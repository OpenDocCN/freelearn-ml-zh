<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Advanced Ways of Improving Models</h1>
                </header>
            
            <article>
                
<p class="mce-root">First, we learned to build a model, then we performed diagnostic analysis on it. Then, we determined how accurate the model was, and in this chapter, we will extend our model-building skills. We will learn how to not view a model as an endpoint, but as a starting position to move forward toward improving models. Basically, we will learn how to improve individual models by building more than one model. We have several ways in which we can do that, and we are going to talk about them in detail.</p>
<p>The topics that will be covered in this chapter are as follows. These are also the ways in which we can improve individual models:</p>
<ul>
<li>Combining models</li>
<li>Propensity scores</li>
<li>Meta-level modeling</li>
<li>Error modeling</li>
<li>Boosting and bagging</li>
<li>Continuous outcomes</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Combining models</h1>
                </header>
            
            <article>
                
<p>There are several ways in which models can be combined. We are going to look at each method in this section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Combining by voting</h1>
                </header>
            
            <article>
                
<p>Let's use an example to understand this method of combining models.</p>
<p>Consider that we have run three models and created a table like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-356 image-border" src="assets/c7806c54-cba4-4c04-be11-4115f2e18485.png" style="width:42.08em;height:5.33em;"/></p>
<p>We have the confidence for each model and its prediction. Let's see how we can combine these models.</p>
<p>If we take a look at the first row, we can see that each of these models is predicting that a person is going to leave. Hence, if we combine the predictions, we are still predicting that the person is going to leave. The confidence value, or the final confidence, is acquired by adding up the confidence values of all the models and dividing by the number of total models, three in our case.</p>
<p>If we look at the second row, we can see that two of these models predict that the person is going to leave; and one model is predicting that the person is going to stay; we can infer that the combined prediction will be that the person will leave. Here, we calculate the confidence values by adding up the confidence of the models that predicted the combined prediction, <span class="packt_screen">Leave</span>, divided by the total number of models, which is three. Hence, the final confidence value is low in the second row.</p>
<p>This is combining models by voting, where only the predictions that occur a number of times are  considered for combining.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Combining by highest confidence</h1>
                </header>
            
            <article>
                
<p>This is another method of combining models. Consider the following table, for example:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-783 image-border" src="assets/4fd27279-8af9-4997-8b9d-50401e224256.png" style="width:41.00em;height:4.83em;"/></p>
<p>In this example, we won't consider what the model is predicting; instead, we will just focus on high confidence values. If we look at the first row, each of the models has predicted <span class="packt_screen">Leave</span>. But <span class="packt_screen">Model 1</span> has the highest confidence, and so the combined prediction is taken as <span class="packt_screen">Leave</span> and the final confidence is the highest confidence acquired.</p>
<p>If we look at the second row, the model with highest confidence is <span class="packt_screen">Model 3</span> and it has predicted that the person is going to stay, and hence, the combined prediction becomes <span class="packt_screen">Stay</span> and the final confidence becomes the highest confidence.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing combining models</h1>
                </header>
            
            <article>
                
<p>Follow these steps to see how we can combine different models:</p>
<ol>
<li>Get <span class="packt_screen">Electronics_Data</span> on Canvas.</li>
<li>Connect the dataset to a <span class="packt_screen">Partition</span> node from the <span class="packt_screen">Field Ops</span> palette.</li>
<li>Split the data into training and testing datasets, we have done before.</li>
<li>Connect the <span class="packt_screen">Partition</span> node to the <span class="packt_screen">Neural Net</span> model and run this model with a random seed set to <kbd>5000</kbd> and run it.</li>
<li>We will now build a <strong>support vector machine</strong> (<strong>SVM</strong>) model. As we are heading towards combining models, we will go to the <span class="packt_screen">Partition</span> node and connect it with an <span class="packt_screen">SVM</span> model from the <span class="packt_screen">Modeling</span> palette.</li>
<li>Run the <span class="packt_screen">SVM</span> model by recalling the edits we had made in the <span class="packt_screen">Expert</span> tab from <a href="f4f20b86-4417-4c0c-a8b2-d0be16f28e20.xhtml">Chapter 2</a>, <em>Getting Started with Machine Learning</em>. Go to the <span class="packt_screen">Expert</span> tab, select the mode as <span class="packt_screen">Expert</span>. Change the <span class="packt_screen">Regularization parameter</span>, <span class="packt_screen">C</span> and set it to <kbd>5</kbd>, the middle value, and change the <span class="packt_screen">Kernel</span> type to <span class="packt_screen">Polynomial</span>, as that's what gave us an accurate and consistent model earlier on using the same data. Also, change the <span class="packt_screen">Degree</span> value to <kbd>2</kbd>. We are changing the parameters to these values because we acquired proper results earlier when we first saw a demonstration of this model in <a href="f4f20b86-4417-4c0c-a8b2-d0be16f28e20.xhtml">Chapter 2</a>, <em>Getting Started with Machine Learning</em>. Click on <span class="packt_screen">Run</span>.</li>
<li>Connect both the <span class="packt_screen">SVM</span> and the <span class="packt_screen">Neural Net</span> model that were generated.</li>
<li>Go to the <span class="packt_screen">Output</span> palette and connect the generated <span class="packt_screen">SVM</span> model to a <span class="packt_screen">Table</span>.</li>
</ol>
<ol start="9">
<li>Run the table using the <span class="packt_screen">Run</span> icon on top. You will see the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-828 image-border" src="assets/a3ce3b75-6b27-4fa9-89f5-aa5333160068.png" style="width:45.33em;height:31.50em;"/></p>
<p style="padding-left: 60px">In this, you can see the results from the partition node, the predictions from the <span class="packt_screen">Neural Net</span> model, its confidence, and even the predictions from the <span class="packt_screen">SVM</span> model and its confidence. You can close this window.</p>
<ol start="10">
<li>We will now analyze the model by connecting the <span class="packt_screen">SVM</span>-generated model to an <span class="packt_screen">Analysis</span> node from the <span class="packt_screen">Output</span> palette.</li>
<li class="CDPAlignLeft CDPAlign">Edit the <span class="packt_screen">Analysis</span> node, check the <span class="packt_screen">Coincidence matrices</span>, and click on <span class="packt_screen">Run</span>. You will see the following results:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-827 image-border" src="assets/b60dd1dc-71dc-4780-a089-f9d88ef0c7c8.png" style="width:33.42em;height:37.58em;"/></p>
<p>We can see how well each of the models has performed. If you scroll down, you can see that the models have agreed 88% of the time on predictions in the training dataset, and about 87% of the time in the testing dataset. When these models agreed, they were actually correct a fair amount of the time. This brings us to evaluate the possibility of combining these two models.</p>
<p>We are now moving on to combine the models. We will first combine using Modeler, but we will also see how we can combine models outside of a modeler.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Combining models in Modeler</h1>
                </header>
            
            <article>
                
<p>For combining models within a modeler, follow these steps:</p>
<ol>
<li>Go to the <span class="packt_screen">SVM</span> model and connect it to the <span class="packt_screen">Ensemble</span> node from the <span class="packt_screen">Field Ops</span> palette.</li>
<li>Let's edit the <span class="packt_screen">Ensemble</span> node. The <span class="packt_screen">Ensemble</span> node knows that it is combining the results of two models as it shows two models in ensemble. Choose the <span class="packt_screen"><span class="packt_screen">Target</span> field for <span class="packt_screen">Ensemble</span></span> as the <span class="packt_screen">Status</span> from the drop-down button on the right. If the <span class="packt_screen">Filter out <span>fields</span> generated by ensemble models</span> is checked, it will filter out the already generated fields from the previous models, hence, we will deselect it. Next, select the <span class="packt_screen">Ensemble method.</span> This is a list 0f ways in which we can combine the model. Here, we will select <span class="packt_screen">Voting</span> as we have already seen this. We will talk about the propensity scores later on in this chapter. Then we have to select what happens when there is a tie; here, we will select <span class="packt_screen">Highest confidence</span> as we have seen this too and click on <span class="packt_screen">OK</span>, as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-829 image-border" src="assets/bd87ac6b-18d5-476e-9b22-8ab613e75eeb.png" style="width:30.58em;height:24.58em;"/></p>
<ol start="3">
<li>Let's see the results of our combination. For this, connect the <span class="packt_screen">Ensemble</span> node to the <span class="packt_screen">Analysis</span> node and click on the <span class="packt_screen">Run</span> button on top. The following will be the results:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f1151314-0192-4a57-9166-298e5263210e.png" style="width:29.50em;height:44.67em;"/></p>
<p>First, we have the results of the <span class="packt_screen">Neural Net</span> model, followed by the results of the <span class="packt_screen">SVM</span> model and then finally, we can see the results of the combined model.</p>
<p>We can see that the overall accuracy in the testing dataset is 82%, which means that there is a slight improvement. We were able to improve the accuracy by combining two models by 2% which is great as a starting point. Let's see how we can combine models from outside of Modeler.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Combining models outside Modeler</h1>
                </header>
            
            <article>
                
<p>This method can be used when you are using any data-mining software other than SPSS Modeler.</p>
<p>Let's see how to do that:</p>
<ol>
<li>Go to the <span class="packt_screen">Field Ops</span> palette and connect the <span class="packt_screen">SVM-</span>generated model to a <span class="packt_screen">Derive</span> node.</li>
<li>We will use the <span class="packt_screen">Derive</span> node to create a new field. We will edit this node and name it <kbd>Combined_Prediction</kbd>.</li>
<li>Derive this field as a <span class="packt_screen">Conditional</span>. You will see an <kbd>if-else</kbd> condition.</li>
<li>Let's tell Modeler that if the predictions from all the models are equal then the combined prediction will be that prediction itself. To do this, let's add an expression in the first <kbd>if</kbd> condition as, the prediction from the <span class="packt_screen">Neural Net</span> model, <span class="packt_screen">$N-Status</span> select <span class="packt_screen">=</span> the prediction of the <span class="packt_screen">SVM</span> model, <span class="packt_screen">$S-Status</span>; go to the <kbd>Then</kbd> condition, click on the expression builder and select, the prediction of the <span class="packt_screen">Neural Net</span> model, <span class="packt_screen">$N-Status</span> or alternatively, you can even select a prediction from the <span class="packt_screen">SVM</span> model.</li>
<li>Write in the <kbd>Else</kbd> condition, this statement: You can select the variable names from the list:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-825 image-border" src="assets/b2e480d6-9a22-49f4-a1ff-3eada9573d36.png" style="width:36.42em;height:25.75em;"/></p>
<p class="mce-root"/>
<p style="padding-left: 60px">This statement means that we will select the <strong>Highest confidence</strong> from any of the models if the predictions of the two models do not match. And if the confidence of the prediction from the <span class="packt_screen">Neural Net</span> model is higher than that of the <span class="packt_screen">SVM</span> model, then we will go with the prediction of the <span class="packt_screen">Neural Net</span> model. Otherwise, if the confidence of the prediction of the <span class="packt_screen">SVM</span> model is higher than the <span class="packt_screen">Neural Net</span> model, then we will go with the <span class="packt_screen">SVM</span> model. But, if both the conditions don't satisfy, then we will put a <kbd>0</kbd>, and then we have to end with an <kbd>endif</kbd> statement. Click on <span class="packt_screen">OK.</span></p>
<ol start="6">
<li>Connect the <span class="packt_screen">Combined_prediction</span> node to the <span class="packt_screen">Table</span> mode and let's see the results take a look at the results, as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-826 image-border" src="assets/2d884fe8-d782-4865-96c7-cbc9412229c0.png" style="width:41.67em;height:28.75em;"/></p>
<p style="padding-left: 60px">Here, in the 12th row, we can see that the neural network predicted a customer as <span class="packt_screen">Churned</span> whereas the <span class="packt_screen">SVM</span> predicted it as <span class="packt_screen">Current</span>, but as the confidence of the <span class="packt_screen">Neural Net</span> prediction was higher, the combined prediction was picked as <span class="packt_screen">Churned</span>.</p>
<ol start="7">
<li>You can analyze this model and see for yourself that the numbers that will be acquired will be similar to the numbers that we had using the <span class="packt_screen">Ensemble</span> node.</li>
</ol>
<p>This is how we combined two models to improve accuracy and we saw how we can get the combined results from the two models. You can try this out with three or more models. You will be amazed at how well combining models can work. We will now see another advanced method to improve the model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using propensity scores</h1>
                </header>
            
            <article>
                
<p>Propensity scores are very useful because they tell you the likelihood of something happening. Confidence values for models reflect confidence in our predictions so a high degree of confidence doesn't help us determine if we're going to have a customer that's going to stay or leave a company, instead it indicates the confidence that we have in our prediction. Sometimes it's helpful to modify the confidence value so that a high confidence value means a prediction that a person is going to leave and a low confidence value indicates that a person is going to stay. Basically, we end up creating a propensity to leave score which would be helpful so that we could make interventions, different marketing efforts, and so on.</p>
<p>Consider this table, for example:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-367 image-border" src="assets/2579696c-ecb3-4400-9829-07f9c8de4441.png" style="width:17.33em;height:7.42em;"/></p>
<p>We have two values for <span class="packt_screen">Leaving</span> and two values for <span class="packt_screen">Staying</span>, each with the confidence values that we have in those predictions. In this example, let's assume that we are trying to calculate the propensity of losing a customer. We will create a propensity score; this means that when a person is predicted to leave, the propensity score is the same thing as a confidence value. So you can see that for the first person, we're predicting they are going to leave, and as we have a high degree of confidence in that prediction, therefore the propensity score is pretty high. For the second person, we're predicting they are also going to leave, but the confidence in that prediction is not quite as high, so therefore, we can see that the propensity score is not quite so high either.</p>
<p class="mce-root"/>
<p>While predicting the opposite, if we are predicting that a third person is going to stay but the confidence in that prediction is not very great, really what we're doing is taking 1 minus the confidence value of the opposite of what we really want, and that ends up being the propensity score. Finally, in the last example, we have a person that we're predicting is going to stay. The confidence in that prediction is extremely high, so therefore, the likelihood of that person leaving is very low.</p>
<p>The following figure sums up the propensity formulas:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-368 image-border" src="assets/e55068c9-388b-4aa2-909a-bc9207fcfeed.png" style="width:21.17em;height:2.92em;"/></p>
<p>In essence, what propensity scores do is modify confidence values so that you can see the likelihood of something happening. So, if you could put them all on some kind of spectrum it would be possible to see, for example, that there are some people for whom there is a high degree of confidence that they are going to leave, so maybe there's not much that we can do about that. We have another group of people for whom we have a high degree of confidence that they're going to stay, so the propensity of them leaving is pretty low. Again, we may not necessarily need to worry about them that much, but maybe the people we need to focus on are the people in the middle, because they're the predictions that are not quite as extreme, and so we cannot be quite as confident about those predictions. Potentially, we can do something with that group. We might be able to change their minds, or something like that, and that's how propensity scores <span>can be used</span>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementations of propensity scores</h1>
                </header>
            
            <article>
                
<p>To see how we can use propensity scores to our advantage, follow these steps:</p>
<ol>
<li>Get your dataset on the canvas and connect it to a <span class="packt_screen">Partition</span> node, dividing the dataset into two parts.</li>
<li>Connect the <span class="packt_screen">Partition</span> node to a <span class="packt_screen">Chaid</span> model from the <span class="packt_screen">Modeling</span> tab. You could use any model here, but let's use this as it will be used in our next example as well.</li>
</ol>
<ol start="3">
<li><span class="packt_screen">Chaid</span> will build a decision tree model. To edit, go to the <span class="packt_screen">Model Options</span> tab, where there is a section that asks for <span class="packt_screen">Propensity Scores</span>. There are two types; a raw propensity score is for the training dataset and the adjusted propensity score is for the testing or validation dataset. We will select the raw propensity score for now:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-790 image-border" src="assets/ea658e0a-6679-404c-bd44-55cdab3879ba.png" style="width:34.25em;height:32.17em;"/></p>
<ol start="4">
<li>Click on <span class="packt_screen">Run</span>. Connect the generated model to a <span class="packt_screen">Table</span> node from the <span class="packt_screen">Output</span> palette and run the <span class="packt_screen">Table</span>. Observe the <span class="packt_screen">Table</span> and see that we have another variable added known as the propensity score, and when a customer is predicted to be churned, and if their confidence score is low, the propensity score is <em>1-confidence</em> of what we really want. But for the <span class="packt_screen">Current</span> customer, we have a propensity score similar to the confidence value.</li>
</ol>
<ol start="5">
<li>If you wish to see a graphical representation of this, connect the generated model, to the <span class="packt_screen">Histogram</span> node from the <span class="packt_screen">Graphs</span> palette. Edit the <span class="packt_screen">Histogram</span>, in the field box, and select the propensity score variable:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-791 image-border" src="assets/008e5975-611e-4434-b90f-bc3b849a06f0.png" style="width:31.08em;height:24.50em;"/></p>
<ol start="6">
<li>Click on <span class="packt_screen">Run</span> to see the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-830 image-border" src="assets/af0b244e-277b-4e36-8912-7227ecc4091b.png" style="width:35.50em;height:29.83em;"/></p>
<p style="padding-left: 60px">Notice that the propensity scores will range from <span class="packt_screen">0.0</span> to <span class="packt_screen">1.0</span>. But the confidence values have only two values, and they have a range from <span class="packt_screen">0.5</span> to <span class="packt_screen">1.0<span>.</span></span> To see this again, go to the histogram and from the <span class="packt_screen">Fields</span> option, select the <span class="packt_screen">Confidence</span> variable, then click on <span class="packt_screen">Run</span>. You will see the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-831 image-border" src="assets/b681f5ae-ebc6-4856-904e-f648f623f24b.png" style="width:35.42em;height:29.92em;"/></p>
<p>Hence, we transformed the confidence values into a propensity score and now that propensity score is giving us information about the likelihood, in this case, of somebody staying as a customer. We could have done it the other way, where we were finding the propensity score for the likelihood of losing a customer, but we could just invert those scores and it would end up creating that for us. In any case, we can use those propensity scores now to do something with them, to see which customers are the most likely ones that we're going to lose, for example, or those which we're going to keep. However you want to look at it, you know which people that are very likely to be lost and so it may not be possible to do anything with them. Those people that were in the middle might not be lost, so maybe a little more could be done for them to try to keep them as customers and try to understand them <span>better</span>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Meta-level modeling</h1>
                </header>
            
            <article>
                
<p>Meta-level modeling is building a model based on predictions or results from another model. In the previous example, we saw how to create propensity scores for our <strong>Chaid</strong> model. In this section, we will see how you can extract results from the <strong>Chaid</strong> model and feed them into a <span class="packt_screen">Neural Net</span> model, and this will enable us to improve the results from a <span class="packt_screen">Neural Net</span> model.</p>
<p>To do this, follow these steps:</p>
<ol>
<li>Connect the partition node to the <span class="packt_screen">Neural Net</span> node from the modeling palette.</li>
<li>Run the <span class="packt_screen">Neural Net</span> model by changing the <span class="packt_screen">Random seed</span> to <kbd>5000</kbd> from the <span class="packt_screen">Advanced</span> options under the <span class="packt_screen">Build</span> tab and click on <span class="packt_screen">Run<span>. </span></span>Now connect the generated Chaid model to the generated Neural Net model.</li>
<li>Now, use the <span class="packt_screen">Analysis</span> node to see the level of accuracy of these models.</li>
<li>You will get the following results:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-792 image-border" src="assets/2651538a-9c6c-4ee8-91de-6ebc5e1320dd.png" style="width:26.67em;height:29.83em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We can see that the accuracy of both the models is somewhat similar, so now we will move on to build a different kind of <span class="packt_screen">Neural Net</span> model.</p>
<p>We will take the results from the <span class="packt_screen">Chaid</span> model and feed them to the <span class="packt_screen">Neural Net</span> model. The <span class="packt_screen">Neural Net</span> model will then use the results, along with all other individual predictors, to try to capture more than <span class="packt_screen">Chaid</span>:</p>
<ol>
<li>Right-click on the generated <span class="packt_screen">Neural Net</span> model and delete it.</li>
<li>Connect the generated <span class="packt_screen">Chaid</span> model to a <span class="packt_screen">Type</span> node from the <span class="packt_screen">Field Ops</span> palette.</li>
<li>Scroll the <span class="packt_screen">Type</span> node edit box to the bottom and set the confidence value variable of the <span class="packt_screen">Chaid</span> model to <span class="packt_screen">None</span>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-373 image-border" src="assets/9e2551b6-5c03-4b33-ba82-8f24d44ffa44.png" style="width:40.25em;height:28.00em;"/></p>
<p class="CDPAlignLeft CDPAlign" style="padding-left: 60px">Click on <span class="packt_screen">OK.</span></p>
<ol start="4">
<li>Connect the <span class="packt_screen">Type</span> node to the <span class="packt_screen">Neural Net</span> model that we already have on the canvas and run it using the <span class="packt_screen">Random seed</span> of <kbd>5000</kbd>.</li>
<li>If you take a look at the results of the new model, you can see that the most important predictors are the propensity scores from the <span class="packt_screen">Chaid</span> model:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-795 image-border" src="assets/662317dc-0e1c-44dd-8227-a0aad09f451b.png" style="width:48.33em;height:27.50em;"/></p>
<ol start="6">
<li>Connect the the generated <span class="packt_screen">Chaid</span> model to <span class="packt_screen">Analysis</span> node and run the analysis to get the following result:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-833 image-border" src="assets/0e52a752-684d-434d-b926-aa86c7e606bb.png" style="width:33.58em;height:37.75em;"/></p>
<p>In this example, we can see that in the <span class="packt_screen">Neural Net</span> model, we got a 1% increase in accuracy if we fed the results from the <span class="packt_screen">Chaid</span> model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Error modeling</h1>
                </header>
            
            <article>
                
<p>Error modeling is another form of meta-level modeling but in this case we will be modeling cases where there were errors in our predictions. In this way, we can increase the accuracy of that prediction. Using an example, we will walk through how to do error modeling.</p>
<p>Consider the following scenario, for example:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-797 image-border" src="assets/cb0683df-a373-4da4-a23a-bc5a6ed14f84.png" style="width:49.25em;height:29.08em;"/></p>
<p>Here, we have a dataset named <kbd>LoyalTrain</kbd>. This is just a training dataset; we have our testing and validation dataset at a different place and will build a model only on the training dataset. Theer is also a <span class="packt_screen">Type</span> node and a <span class="packt_screen">Neural Net</span> model, where we are predicting the variable <span class="packt_screen">loyal</span>. Run the <span class="packt_screen">Analysis</span> node to see the results as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-798 image-border" src="assets/ded52372-c6cc-4205-bd25-f4177e98e3bf.png" style="width:33.08em;height:37.67em;"/></p>
<p>You can see that there are two categories in the outcome variable: people are either predicted to stay or to leave. You can also see that correct predictions were made in 79% of the cases. Mistakes were made in 21% of the cases. In total, there were 236 errors. </p>
<p>From this example, you can also see that the <span class="packt_screen">Neural Net</span> model was copied and placed in another part of the stream. A new variable, <kbd>CORRECT</kbd>, was also made using a <span class="packt_screen">Derive</span> node. Let's take a look at what's happened here, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-799 image-border" src="assets/65e71ad2-fd93-46aa-a021-10af29b15c8b.png" style="width:35.75em;height:33.92em;"/></p>
<p>Here, we have created a new field as <kbd>CORRECT</kbd>, and we have kept its values as <kbd>True</kbd> and <kbd>False</kbd>. We are telling Modeler here that if it finds a variable, <span class="packt_screen">LOYAL</span>, and if it is equal to the prediction of <span><span class="packt_screen">LOYAL</span></span>, then the value is <span class="packt_screen">True</span>; otherwise, it is <span class="packt_screen">False</span>.</p>
<p>If you run the <span class="packt_screen">Distribution</span> node placed above it, you will see the following results:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-800 image-border" src="assets/424119fa-4336-4b65-b2a6-0bca6ad9afdd.png" style="width:46.25em;height:20.08em;"/></p>
<p>Next, we will use the <span class="packt_screen">Type</span> node to instantiate the data, after which we can use a <span class="packt_screen">C5.0</span> decision tree model that looks at the data in a very different way. Here we have built a <span class="packt_screen">C5.0</span> model that is trying to predict if we are getting a correct or an incorrect prediction. Click on the generated <span class="packt_screen">C5.0</span> model to see its results, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-801 image-border" src="assets/f708e93e-ff03-4077-b682-f45bc2d54d56.png" style="width:35.25em;height:25.42em;"/></p>
<p>In this example, we can see that we have 14 rows with <span class="packt_screen">4 rule(s)</span> for a <span class="packt_screen">False</span> prediction, that is, when we are predicting incorrectly, and <span class="packt_screen">10 rule(s)</span> for <span class="packt_screen">True</span> values when we are predicting correctly.</p>
<p>You can expand the rules and click on the <strong><span class="packt_screen">%</span></strong> sign above them to get the following results:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-803 image-border" src="assets/8c58a081-e865-4c1a-9e5a-a2565ae64a1a.png" style="width:36.08em;height:42.42em;"/></p>
<p class="CDPAlignCenter CDPAlign"/>
<p>In this example, the first rule basically states that if you're male and you're using fewer than 1 minute of international calls, fewer than 1 minute of long-distance calls, and your status is single, we are predicting that we will have a value of <span class="packt_screen">False</span>. If you want to see the numbers, click on the <span class="packt_screen">%</span> sign, where you will see the following results:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b1a3a44a-e41d-486b-9ab1-3b8384d450ec.png" style="width:32.50em;height:38.25em;"/></p>
<p> As shown in the preceding screenshot, first rule had 22 people, and the accuracy of predictions relating to them was around 82%.</p>
<p>From this, we can see that there are certain kinds of mistakes cropping up while we are making the predictions. We might therefore need to use another kind of model instead of a <span class="packt_screen">Neural Net</span> model. To do this, click on the <span class="packt_screen">Generate</span> option and select the <span class="packt_screen">Rule Trace Node</span>, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-388 image-border" src="assets/a1f3277c-c8a4-426e-8cbc-8177a04d7f4f.png" style="width:33.83em;height:39.67em;"/></p>
<p class="mce-root"/>
<p>This step created the <span class="packt_screen">FALSE_TRUE</span> node that you can see in the example scenario as the Start icon. This creates all of our rules. If you wish to take a look inside it, click on the Start + icon on the <span class="packt_screen">Tools</span> tab, where you should see the following result:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-804 image-border" src="assets/d42de6f6-3fb3-4c61-8eab-ec70ecae1c8b.png" style="width:48.25em;height:26.83em;"/></p>
<p>Let's now take a look at the first rule. Click on the expression builder in that rule, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-834 image-border" src="assets/f58fb2fe-48ac-48da-b8f5-ebddab672ba6.png" style="width:39.58em;height:38.67em;"/></p>
<p>Here, the rule appears to state that if you're male and you're using fewer than 1 minute of international calls, fewer than 1 minute of long-distance calls, and your status is also single, we're predicting that you're going to have a value of <span class="packt_screen">False</span>. You can see the accuracy in that prediction.</p>
<p>Go back using the Start icon. Here, we have the classify node, <kbd>Split</kbd>. Let's see what we have done so far, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-805 image-border" src="assets/4f68ae73-f951-4022-9aed-dd9c01b17c94.png" style="width:34.42em;height:42.42em;"/></p>
<p>We have taken the variable <span class="packt_screen">RULE</span> and clicked on <span class="packt_screen">Get</span>, which gave us all of these different original values of <span class="packt_screen">False</span>, which we renamed to <span class="packt_screen">Incorrect</span> and all the values of <span class="packt_screen">True</span>, which were renamed to <span class="packt_screen">Correct</span>, and then we had just the <span class="packt_screen">Correct Predictions</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-806 image-border" src="assets/ec547e81-6ddc-42f5-a169-323d8d890d41.png" style="width:29.58em;height:30.25em;"/></p>
<p>We have now built the <span class="packt_screen">Neural Net</span> model. If you run the <span class="packt_screen">Analysis</span> node of the generated <span class="packt_screen">Neural Net</span> model from the <span class="packt_screen">Correct Predictions</span>, you should see the following results:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-808 image-border" src="assets/28266a65-c6fd-40af-9336-4a1d478b1a18.png" style="width:39.42em;height:29.00em;"/></p>
<p>Remember that the overall accuracy of the earlier model was around 79%, which has now improved to around 84%.</p>
<p>We have also done the same thing for incorrect predictions in a separate field from the <span class="packt_screen">Type</span> node. Let's have a look at that, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-809 image-border" src="assets/40117de4-64a4-460f-8f66-48277f984a9e.png" style="width:28.08em;height:25.33em;"/></p>
<p>We built a <span class="packt_screen">C5.0</span> model for incorrect predictions, so let's take a look at its analysis, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-835 image-border" src="assets/dec38d60-e165-4cb6-a00c-cc4a4e5ade56.png" style="width:38.67em;height:28.25em;"/></p>
<p>The <span class="packt_screen">C5.0</span> model has done a great job at predicting the incorrect values where the <span class="packt_screen">Neural Net</span> model didn't work well; we now have an overall accuracy of 89%.</p>
<p>Let's sum up what we did here. We had a dataset that we split into correct and incorrect results and separately modeled each one to give us fewer errors than we used one model.</p>
<p>Now we need to combine the predictions from the two models. For this, go to the <span class="packt_screen">Error 2</span> Stream from the <span class="packt_screen">Streams</span> tab at the right, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-811 image-border" src="assets/265f52b9-f13d-4e27-86bf-33cdaf06c82f.png" style="width:50.17em;height:30.08em;"/></p>
<p>Here, we have combined the predictions of both the models and have used a <span class="packt_screen">Derive</span> node <span class="packt_screen">Prediction</span>, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-812 image-border" src="assets/1322cc1a-1431-48df-87cd-08f70911f8a7.png" style="width:34.33em;height:39.33em;"/></p>
<p>Here, we have specified that if a prediction is correct, the prediction of the <span class="packt_screen">Neural Net</span> model should be opted for. If a prediction is incorrect, we should opt for the prediction of the <span class="packt_screen">C5.0</span> model.</p>
<p>Then, having added the <span class="packt_screen">Matrix</span> node, run the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-813 image-border" src="assets/c07fa06b-e595-4f41-bb43-ec56a34afbc5.png" style="width:35.17em;height:23.42em;"/></p>
<p>What we can see in the preceding screenshot is that we have correctly predicted that <span class="packt_screen">437</span> people will leave with <span class="packt_screen">118</span> errors, and that <span class="packt_screen">498</span> people will stay with just <span class="packt_screen">55</span> errors. This means there is a total number of <span>173</span> errors.<br/>
Our original model made 236 errors, so we have brought down the number of errors by a great extent. Just by using two different models for different groups of people and by combining them with, we have produced an output with 63 fewer errors.</p>
<p>This is error modeling. In error modelling you can build one model, see what the results look like, and then decide from there whether <span>to</span> build two or three models for different types of people, because it can't be assumed that one size fits all. Therefore, we can build different kinds of models, feed different types of data to those models, and then ultimately combine the results of each model to produce a final prediction that can end up having fewer errors in terms of the predictive modeling undertaken.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Boosting and bagging</h1>
                </header>
            
            <article>
                
<p>The idea behind boosting is that by building successive models that are built to predict the misclassifications of earlier models you're performing a form of error modeling. Bagging, on the other hand, is sampling with replacement. With this method, new training datasets are generated which are of the same size as the original dataset. For our example in this section, will be using a bootstrap sample.</p>
<p>In this example, we're going to see how to do boosting and bagging, which are two methods of improving a model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Boosting</h1>
                </header>
            
            <article>
                
<p>Let's see how to do boosting with the following steps:</p>
<ol>
<li>Get your data on a canvas and partition it.</li>
<li>Create a <span class="packt_screen">Neural Net</span> model for the data.</li>
<li>Run the <span class="packt_screen">Neural Net</span> model with a <span class="packt_screen">Random seed</span> set to <kbd>5000</kbd>.</li>
<li>Connect an <span class="packt_screen">Analysis</span> node and run it with <span class="packt_screen">Coincidence matrices</span> checked – you will see that the testing accuracy is 81% and the overall accuracy is 80%.</li>
<li>Now, boost the <span class="packt_screen">Neural Net</span> model. For this, go to the <span class="packt_screen">Neural Net</span> model and edit it. Go to <span class="packt_screen">Objectives</span> under <span class="packt_screen">Build options</span> and click on <span class="packt_screen">Enhance model accuracy</span> (boosting). B<span>oosting can be used with any size of dataset. The idea here is that we're building successive models that are built to predict the misclassifications of earlier models. So, basically, we end up building a model. There'll be some errors, so a second model should be built where the errors of the first model are given more weight so that we're able to understand them better. Then, when we build a second model, there are going to be errors, so we end up building a third model where the errors of the second model are given more weight again so that we try to understand them better, and so forth. Whenever you're doing boosting and bagging, you always have to make sure you have a training and a testing dataset because there's a very good chance that you're going to capitalize on chance, and that you might find sample-specific information because we're focusing on the errors that we're finding within that specific sample. W</span>e'll now click on <span class="packt_screen">Run</span>.</li>
</ol>
<ol start="6">
<li>Let's take a look at our generated model:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-814 image-border" src="assets/5ce2de68-ad80-4cbd-a5e4-5ce279231e75.png" style="width:58.67em;height:42.17em;"/></p>
<p class="CDPAlignLeft CDPAlign" style="padding-left: 60px">The first tab that we have here in our generated model is showing us what the Ensemble model looks like: that's combining the 10 models that we've created. You can see its overall accuracy is about 98%: that's the model that's been chosen as the best model. You can also see what the reference model is—that would be the first model that was built—and then you can see the naive model and, really, that's no model, that's <span>just</span> <span>where we're predicting the mode or the most common response.</span></p>
<ol start="7">
<li class="CDPAlignLeft CDPAlign">Let's go down to the second icon:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-815 image-border" src="assets/ddc7969a-8722-4c53-988a-aadd7f4b0c58.png" style="width:58.58em;height:33.08em;"/></p>
<p class="CDPAlignLeft CDPAlign" style="padding-left: 60px">Here, you can see the <span class="packt_screen">Predictor Importance</span>. Across the 10 models that we ended up building, we can see that the <span><span class="packt_screen">Premier</span></span> variable was the most important predictor and then you can see what the other predictors were in terms of their order of importance. This is the same kind of information that we would see typically with a general <span class="packt_screen">Neural Net</span> model, but this information comes from across all the different models that we built.</p>
<ol start="8">
<li class="CDPAlignLeft CDPAlign">If we go down to the next icon we can see <span class="packt_screen">Predictor Frequency</span>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-816 image-border" src="assets/e237b5dd-1edb-4a38-b606-72bb67c75bb2.png" style="width:59.17em;height:33.33em;"/></p>
<p class="CDPAlignLeft CDPAlign" style="padding-left: 60px">This shows us how frequently each one of the different predictors was used in the model. For a <span class="packt_screen">Neural Net</span> model this is not so interesting because <span class="packt_screen">Neural Net</span> models generally do not drop predictors, but if we had a decision tree model, for example, this could be a little more interesting because there you do drop predictors.</p>
<ol start="9">
<li class="CDPAlignLeft CDPAlign">Let's go down to the next tab:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-817 image-border" src="assets/3c80ce0a-da3c-4de0-a374-28f0a6c19119.png" style="width:53.83em;height:30.42em;"/></p>
<p class="CDPAlignLeft CDPAlign" style="padding-left: 60px">The preceding screenshot is showing us the level of accuracy of the model. You can see that it flattens out and, at some point, there's no longer much of an improvement. In this case, it's a gradual increase in terms of accuracy. Sometimes, in some models, you see perhaps five models that there's a huge jump in accuracy and then it just stabilizes. Maybe you wouldn't necessarily need to build any more models. In our case, we ended up building 10 models. Our overall accuracy was extremely high. If we had seen much lower accuracy, perhaps because we saw a gradual increase, maybe we would want to use 15 models instead of 10, for example. That's where you would see this kind of information.</p>
<ol start="10">
<li class="CDPAlignLeft CDPAlign">Let's scroll down a little further and let's see the final table:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-818 image-border" src="assets/02ba2c0d-ac25-4f39-8f20-ff527d686dff.png" style="width:31.67em;height:27.58em;"/></p>
<p class="CDPAlignLeft CDPAlign" style="padding-left: 60px">Here, we can see the number of predictors and we can also see the number of cases that we had in the model as well. Finally, we see the number of synapses, which are basically the number of weights or the number of connections that we have within this model. So, you can see how well each one of these individual models is doing. Each new version of a model is giving more weight to where we had more errors in the data and that's basically the idea here.</p>
<ol start="11">
<li>Run the <span class="packt_screen">Analysis</span> node, finally, and you can see that for the training dataset the overall accuracy was about 98%. But in the testing dataset the overall accuracy was about 80% that's what we really care about, the testing dataset. In this case, we see that there's a big difference between training and testing and that's generally going to be the situation when talking about boosting models.</li>
</ol>
<p>Make sure that whatever result you get is really worth it, and that it's really an improvement over just running the model on its own. In this case, when we just ran one model, remember that the overall accuracy on the testing data set was at about 80%; that's what we have here. So really, boosting didn't do much for us in this particular situation. In other situations, it certainly can, but again you want really <span>to</span> be able to weigh that, and in this case boost seemed to be probably not really worth it for us in this situation.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bagging</h1>
                </header>
            
            <article>
                
<p>Let's go back into the <span class="packt_screen">Neural Net</span> model and, this time, what we're going to do is bagging instead of boosting:</p>
<ol>
<li>Go over to the <span class="packt_screen">Objectives</span> tab and select <span class="packt_screen">Enhance model stability</span> – it's sampling with replacement. Do not do bagging when you have small datasets or outliers. The main idea behind bagging is that new training datasets are generated that are of the same size as the original training dataset and this is done by using sampling with replacement. We're actually bootstrapping in this kind of situation.</li>
<li>Click on <span class="packt_screen">Run</span> for the model.</li>
<li>Let's take a look at our generated model. This is the model that we ended up building:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-819 image-border" src="assets/41820b52-d713-4c53-b7bf-2c47a2c0e7dd.png" style="width:46.33em;height:35.50em;"/></p>
<p class="mce-root"/>
<p style="padding-left: 60px">Notice that the combining rule is achieved by voting but there are other ways in which we can combine models and, in fact, we can choose the option to show all the combining rules. We won’t see the details for all the models because the screenshots are the same as with boosting.</p>
<ol start="4">
<li>Run the <span class="packt_screen">Analysis</span> node to get the following results:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-820 image-border" src="assets/4467c320-7c2c-4511-9d45-ecf3360f8c61.png" style="width:31.08em;height:26.33em;"/></p>
<p>We can see that, by doing the bagging, we got a 4% increase in accuracy.</p>
<p>We will now see how to predict continuous outcomes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Predicting continuous outcomes</h1>
                </header>
            
            <article>
                
<p>Until now, we have spent all of our time talking about categorical outcomes and most of those examples apply to continuous outcomes, but in this section we're going to focus exclusively on continuous outcome variables.</p>
<p>As I mentioned previously, when we're talking about continuous outcome predictions or variables, everything that we've talked about in this book still applies: the main difference, though, is going to be in terms of how we end up combining predictions.</p>
<p>Here, in this example, we can see that we built three models and we have predictions from each one of those models:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-821 image-border" src="assets/ad40d5e6-8fe5-40fd-bc5f-7e856ee27814.png" style="width:20.50em;height:7.75em;"/></p>
<p>When we want to combine the predictions, all we do is take a mathematical average. The mean of these previous models ends up being the combined prediction because we're not predicting individual categories as we were when we had a categorical outcome variable. Instead, we're predicting actual numeric values and if we want to combine predictions from these different models, all we do, simply, is take the average of the models. For example, in the first row, the first model predicted a value of <span class="packt_screen">7</span>, the next model predicted a value of <span class="packt_screen">9</span>, and the third model predicted <span class="packt_screen">8</span>. We take those values, we add them up, we divide them by the number of models, and the combined prediction ends up being a value of <span class="packt_screen">8</span>. That's the way you would combine your models.</p>
<p>But when we have continuous outcome variables, we do not have a confidence value, so we don't need to worry about the actual confidence values for those kind of models.</p>
<p>In this example, we're going to use the bank dataset. Bring it onto the canvas.</p>
<p class="mce-root"/>
<p>Let's just take a look at what that data looks like:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-822 image-border" src="assets/18a8aa3b-bba8-444d-87c8-02eae8cf61f7.png" style="width:41.50em;height:29.08em;"/></p>
<p>As you can see, there are several fields. We will predict a variable, <span class="packt_screen">salnow</span>, that's our target variable b<span>ased on a beginning salary, gender, the amount of time that someone has worked at this organization, their age, their level of education, the number of years that they have worked prior to coming to this organization, the job category that they're in, whether they're from a minority, and then the interaction of race and gender.</span></p>
<p>Partition the dataset and go on to create a <span class="packt_screen">Neural Net</span> model. Run the <span class="packt_screen">Neural Net</span> model with default settings. Take a look at the newly generated model.</p>
<p>Let's also build the <span class="packt_screen">SVM</span> model just as we have done before. We will compare the results of both the models.</p>
<p>This is the output for the <span class="packt_screen">SVM</span> model:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-823 image-border" src="assets/03c2bef9-241f-4f8f-b2a5-117d41fde46f.png" style="width:65.00em;height:39.42em;"/></p>
<p>Connect the two generated models and connect the <span class="packt_screen">SVM</span> model to a <span class="packt_screen">Table</span> and run the table.<br/>
Scroll towards the end and you can see a <span class="packt_screen">Partition</span> node and predictions of the two models. Notice that there are no confidence values for these continuous outcome variables.</p>
<p>This also means that we won't get the propensity scores for these models either. Bring an <span class="packt_screen">Analysis</span> node and connect the <span class="packt_screen">SVM</span> model to the <span class="packt_screen">Analysis</span> node and run it:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-410 image-border" src="assets/58d64813-3dec-4fb2-9ba9-5d2a57a92a97.png" style="width:27.50em;height:31.33em;"/></p>
<p>The analysis is a little bit different than what we used to get with the categorical variables. You can see that there is a <span class="packt_screen">Minimum Error</span>, and a <span class="packt_screen">Maximum Error</span> for both the training and the testing datasets. The model has done a worst job in over- or under-predicting the values.</p>
<p><span class="packt_screen">Mean Error</span> is just averaging of the errors. The best way to look at the accuracy of these models is by looking at the <span class="packt_screen">Mean Absolute Error</span>. As you can see, we have a lower value for the training dataset as compared to the testing dataset. These values need to be similar. You can see the mean absolute value for the <span class="packt_screen">SVM</span> model. You can also see the <span class="packt_screen">Standard Deviation</span>. This needs to be as small as possible because this shows that we have less variation in the model. Another criterion is the correlation coefficient. That is extremely high for both the datasets; these values must be similar to each other. People use linear coefficients to validate the usefulness of a model, but sometimes we don't have linear relationships. So, in such cases, we will use the mean absolute error value as the best measure of assessing how the model is performing. <strong>Occurrences</strong> are the number of cases that we have for each dataset.</p>
<p>Let's now combine the model:</p>
<ol>
<li>Connect the last generated node to an ensemble node from the <span class="packt_screen">Field Ops</span> palette.</li>
<li>Edit the <span class="packt_screen">Ensemble</span> node. Deselect the <span class="packt_screen">Filter out</span> field and click <span class="packt_screen">OK</span>.</li>
<li>Connect the <span class="packt_screen">Ensemble</span> node to the <span class="packt_screen">Table</span> node already present and run the <span class="packt_screen">Table</span>.</li>
<li>You can see that we have predictions from the <span class="packt_screen">SVM</span>, <span class="packt_screen">Neural Net</span>, and if we average those two, we have the predictions from the combined model. We also have our standard errors as well. For those who are not using modeler, a <span class="packt_screen">Derive</span> node can be used to calculate the averages of these models and get a combined result.</li>
<li>To see the results, connect the <span class="packt_screen">Ensembles</span> node to an <span class="packt_screen">A</span><span class="packt_screen">nalysis</span> node and run it:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-412 image-border" src="assets/ce42d8e1-d88d-47ea-90ef-56caf40e7293.png" style="width:29.33em;height:33.33em;"/></p>
<p>This was an example of how we can work with continuous variables.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we saw how we can make additional advance operations on models to get better results. Hopefully, you have a deeper insight into how data is fetched to train a machine and how we can make a better model by training it on different types of data. </p>


            </article>

            
        </section>
    </body></html>