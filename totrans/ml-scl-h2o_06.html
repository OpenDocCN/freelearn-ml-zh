<html><head></head><body>
		<div id="_idContainer028">
			<h1 id="_idParaDest-64"><em class="italic"><a id="_idTextAnchor064"/>Chapter 4</em>: H2O Model Building at Scale – Capability Articulation</h1>
			<p>So far, we have learned the fundamental workflow of how to build H2O models at scale, but that was done using H2O at its barest minimum. In this chapter, we will survey the extremely broad capability set of H2O model building at scale. We will then use our knowledge from this chapter and move on to part two, <em class="italic">Building State-of-the-Art Models on Large Data Volumes Using H2O</em>, where we will get down to business and use advanced techniques to build and explain highly predictive models at scale.</p>
			<p>To conduct this survey, we will break down the chapter into the following main topics:</p>
			<ul>
				<li>Articulating the H2O data capabilities during model building</li>
				<li>Overviewing the H2O machine learning algorithms</li>
				<li>Understanding the H2O modeling capabilities </li>
			</ul>
			<h1 id="_idParaDest-65"><a id="_idTextAnchor065"/>H2O data capabilities during model building</h1>
			<p>Recall that H2O model building<a id="_idIndexMarker170"/> at scale is performed by using H2O 3 or its extension, Sparkling Water, which wraps H2O 3 with Spark capabilities. The H2O 3 API has extensive data capabilities used in the model building process, and the Sparkling Water API inherits these and adds additional capabilities from Spark. These capabilities are broken down into the following three broad categories:</p>
			<ul>
				<li><strong class="bold">Ingesting data</strong> from the source to the H2O cluster</li>
				<li><strong class="bold">Manipulating data</strong> on the H2O cluster</li>
				<li><strong class="bold">Exporting data</strong> from the H2O cluster to an external destination</li>
			</ul>
			<p>As emphasized in previous chapters, the H2O cluster architecture (H2O 3 or Sparkling Water) allows model building at an unlimited scale but is abstracted from the data scientist who builds models by coding H2O in the IDE. </p>
			<p>H2O data capabilities<a id="_idIndexMarker171"/> are overviewed in the following diagram and elaborated subsequently:</p>
			<div>
				<div id="_idContainer025" class="IMG---Figure">
					<img src="image/B17065_04_01.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.1 – The H2O data capabilities</p>
			<p>Let's start with data ingestion.</p>
			<h2 id="_idParaDest-66"><a id="_idTextAnchor066"/>Ingesting data from the source to the H2O cluster</h2>
			<p>The following<a id="_idIndexMarker172"/> data sources<a id="_idIndexMarker173"/> are supported:</p>
			<ul>
				<li><strong class="bold">Local file</strong></li>
				<li><strong class="bold">Remote file</strong></li>
				<li><strong class="bold">AWS S3 </strong></li>
				<li><strong class="bold">MinIO cloud storage</strong></li>
				<li><strong class="bold">Azure Blob and Data Lake </strong></li>
				<li><strong class="bold">Google Cloud Storage</strong></li>
				<li><strong class="bold">HDFS</strong></li>
				<li><strong class="bold">HDFS-like</strong>: Alluxio FS and IBM HDFS</li>
				<li><strong class="bold">Hive</strong> (via Metastore/HDFS or JDBC)</li>
				<li><strong class="bold">JDBC </strong></li>
			</ul>
			<p>The supported<a id="_idIndexMarker174"/> file formats<a id="_idIndexMarker175"/> of source data are as<a id="_idIndexMarker176"/> follows:</p>
			<ul>
				<li><strong class="bold">CSV</strong> (a file with any delimiter, auto-detected or specified)</li>
				<li><strong class="bold">GZipped CSV</strong></li>
				<li><strong class="bold">XLS</strong> or <strong class="bold">XLSX</strong> </li>
				<li><strong class="bold">ORC</strong></li>
				<li><strong class="bold">Parquet</strong></li>
				<li><strong class="bold">Avro</strong></li>
				<li><strong class="bold">ARFF</strong></li>
				<li><strong class="bold">SVMLight</strong></li>
			</ul>
			<p>Some important characteristics<a id="_idIndexMarker177"/> of data ingestion to H2O are as follows:</p>
			<ul>
				<li>Data is ingested directly from the source to the H2O cluster memory and does not pass through the IDE client.</li>
				<li>In all cases, data is partitioned in-memory across the H2O cluster.</li>
				<li>Except for the local file, the remote file, and JDBC sources, data is ingested in parallel to each partition.</li>
				<li>Data on the H2O cluster is represented to the user in the IDE<a id="_idIndexMarker178"/> as a two-dimensional <strong class="bold">H2OFrame</strong>.</li>
			</ul>
			<p>Let's now see how we can manipulate data now that it is ingested into H2O and represented as an H2OFrame.</p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor067"/>Manipulating data in the H2O cluster</h2>
			<p>The H2O 3 API provides extensive data<a id="_idIndexMarker179"/> manipulation capabilities. As mentioned <a id="_idIndexMarker180"/>in the previous bullet list, datasets in memory are distributed on the H2O cluster and represented in the IDE specifically as an H2OFrame after data load and subsequent data manipulations.</p>
			<p>H2OFrames have an extensive list of methods to perform mathematical, logical, and introspection operations at the value, column, row, and full dataset<a id="_idIndexMarker181"/> levels. An H2OFrame<a id="_idIndexMarker182"/> is similar in experience to the <strong class="bold">pandas DataFrame</strong> or <strong class="bold">R data frame</strong>.</p>
			<p>The following examples are just a few data manipulations that can be done on H2Oframes: </p>
			<ul>
				<li>Operations<a id="_idIndexMarker183"/> on <strong class="bold">data columns</strong>:<ul><li>Change the data type (for example, integers from 0 to 7 as categorical values).</li><li>Aggregate a column (group by) by applying mathematical functions.</li><li>Display column names and use as features in a model.</li></ul></li>
				<li> Operations on <strong class="bold">data rows</strong>:<ul><li>Combine rows<a id="_idIndexMarker184"/> from one or more datasets.</li><li>Slice out (filter) rows of a dataset by specifying the row index, the range of rows, or the logical condition.</li></ul></li>
				<li>Operations<a id="_idIndexMarker185"/> on <strong class="bold">datasets</strong>:<ul><li>Merge two datasets on common values of shared column names.</li><li>Transform a dataset by pivoting on a column.</li><li>Split a dataset into two or more datasets (for example, train, validate, and test).</li></ul></li>
				<li>Operations on <strong class="bold">data values</strong>:<ul><li>Fill missing values<a id="_idIndexMarker186"/> forward or backward with adjacent row or column values.</li><li>Fill missing values by imputing with aggregate results (for example, the mean for the column).</li><li>Replace numerical values based on logical conditions.</li><li>Trim values, manipulate strings, return a numerical value sign, and test whether a value is N/A.</li></ul></li>
				<li><strong class="bold">Feature engineering</strong> operations:<ul><li>Date parsing, for example, parsing<a id="_idIndexMarker187"/> one date column into separate columns for year, month, day.</li><li>Derive a new column mathematically and conditionally from other columns, including the use of lambda expressions.</li><li>Perform target encoding (that is, replace a categorical value with the mean of the target variable).</li><li>For <strong class="bold">Natural Language Processing (NLP)</strong> problems, perform <strong class="bold">string tokenizing</strong>, <strong class="bold">Term Frequency-Inverse Document Frequency (TF-IDF)</strong> calculations, and<a id="_idIndexMarker188"/> convert a <strong class="bold">Word2vec</strong> model into<a id="_idIndexMarker189"/> an H2OFrame<a id="_idIndexMarker190"/> for data<a id="_idIndexMarker191"/> manipulations.</li></ul></li>
			</ul>
			<p>For full details of H2O data manipulation<a id="_idIndexMarker192"/> possibilities, see the H2O Python documentation (<a href="http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/frame.html">http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/frame.html</a>) or R<a id="_idIndexMarker193"/> documentation (<a href="http://docs.h2o.ai/h2o/latest-stable/h2o-r/docs/reference/index.html">http://docs.h2o.ai/h2o/latest-stable/h2o-r/docs/reference/index.html</a>). Also, refer to the fourth section of <em class="italic">Machine Learning with Python and H2O</em> (<a href="http://h2o-release.s3.amazonaws.com/h2o/rel-wheeler/2/docs-website/h2o-docs/booklets/PythonBooklet.pdf">http://h2o-release.s3.amazonaws.com/h2o/rel-wheeler/2/docs-website/h2o-docs/booklets/PythonBooklet.pdf</a>) for examples<a id="_idIndexMarker194"/> of data manipulation.</p>
			<p>Manipulating data<a id="_idIndexMarker195"/> is key for preparing it as an input for model building. We may<a id="_idIndexMarker196"/> also want to export our manipulated data for future use. The next section lists the H2O data export capabilities. </p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor068"/>Exporting data out of the H2O cluster</h2>
			<p>H2OFrames in memory<a id="_idIndexMarker197"/> can be exported to external targets. These target<a id="_idIndexMarker198"/> systems are as follows:</p>
			<ul>
				<li><strong class="bold">Local client memory</strong></li>
				<li><strong class="bold">Local filesystem</strong></li>
				<li><strong class="bold">AWS S3</strong></li>
				<li><strong class="bold">MinIO cloud storage</strong></li>
				<li><strong class="bold">Azure Blob and Data Lake</strong></li>
				<li><strong class="bold">Google Cloud Storage</strong></li>
				<li><strong class="bold">HDFS</strong></li>
				<li><strong class="bold">HDFS-like</strong>: Alluxio FS and IBM HDFS</li>
				<li><strong class="bold">Hive tables</strong> (CSV or Parquet, via JDBC)</li>
			</ul>
			<p>The volume of exported data must, of course, be considered. Large volumes of data will not, for example, fit into a local client memory or filesystem.</p>
			<p>Let's now see what additional data capabilities Sparkling Water adds.</p>
			<h2 id="_idParaDest-69"><a id="_idTextAnchor069"/>Additional data capabilities provided by Sparkling Water</h2>
			<p>Sparkling Water inherits<a id="_idIndexMarker199"/> all data capabilities from H2O 3. Importantly, Sparkling Water <a id="_idIndexMarker200"/>adds additional data capabilities by leveraging the Spark DataFrame and Spark SQL APIs, and thus can import, manipulate, and export data accordingly. See the following<a id="_idIndexMarker201"/> reference for full<a id="_idIndexMarker202"/> Spark DataFrame and Spark SQL capabilities: <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html">https://spark.apache.org/docs/latest/sql-programming-guide.html</a>.</p>
			<p>A key pattern in using Sparkling Water is to leverage Spark for advanced data munging capabilities, then convert the resulting Spark DataFrame to an H2Oframe, and then build state-of-the-art models using H2O's machine learning algorithms, as covered in the next section. These algorithms<a id="_idIndexMarker203"/> can be used in either H2O 3 or Sparkling<a id="_idIndexMarker204"/> Water.</p>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor070"/>H2O machine learning algorithms</h1>
			<p>H2O has extensive <strong class="bold">unsupervised</strong> and <strong class="bold">supervised</strong> learning algorithms<a id="_idIndexMarker205"/> with similar reusable<a id="_idIndexMarker206"/> API constructs – for example, similar ways<a id="_idIndexMarker207"/> to set hyperparameters or invoke explainability capabilities. These algorithms are identical from an H2O 3 or Sparkling Water perspective and are overviewed in the following diagram:</p>
			<div>
				<div id="_idContainer026" class="IMG---Figure">
					<img src="image/B17065_04_02.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.2 – H2O algorithms</p>
			<p>Each algorithm has an extensive set of parameters and hyperparameters to set or leverage as defaults. The algorithms accept H2OFrames as data inputs. Remember that an H2OFrame is simply a handle on the IDE client to the distributed in-memory data on the remote H2O cluster where the algorithm processes it.</p>
			<p>Let's take a look at H2O's distributed machine learning algorithms.</p>
			<h2 id="_idParaDest-71"><a id="_idTextAnchor071"/>H2O unsupervised learning algorithms</h2>
			<p>Unsupervised algorithms<a id="_idIndexMarker208"/> do not predict but rather attempt<a id="_idIndexMarker209"/> to find clusters and anomalies in data, or to reduce the dimensionality of a dataset. H2O has the following unsupervised learning algorithms to run at scale:</p>
			<ul>
				<li><strong class="bold">Aggregator</strong></li>
				<li><strong class="bold">Generalized Low Rank Models (GLRM)</strong></li>
				<li><strong class="bold">Isolation Forest</strong></li>
				<li><strong class="bold">Extended Isolation Forest</strong></li>
				<li><strong class="bold">K-Means Clustering</strong></li>
				<li><strong class="bold">Principal Component Analysis (PCA)</strong></li>
			</ul>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor072"/>H2O supervised learning algorithms</h2>
			<p>Supervised learning algorithms predict outcomes by learning from a training dataset labeled with those outcomes. H2O has the following supervised learning algorithms to run at scale:</p>
			<ul>
				<li><strong class="bold">Cox Proportional Hazards (CoxPH)</strong></li>
				<li><strong class="bold">Deep Learning (Artificial Neural Network, or ANN)</strong></li>
				<li><strong class="bold">Distributed Random Forest (DRF)</strong></li>
				<li><strong class="bold">Generalized Linear Model (GLM)</strong></li>
				<li><strong class="bold">Maximum R Square Improvements (MAXR)</strong></li>
				<li><strong class="bold">Generalized Additive Models (GAM)</strong></li>
				<li><strong class="bold">ANOVA GLM</strong></li>
				<li><strong class="bold">Gradient Boosting Machine (GBM)</strong></li>
				<li><strong class="bold">Naïve Bayes Classifier</strong></li>
				<li><strong class="bold">RuleFit</strong></li>
				<li><strong class="bold">Support Vector Machine (SVM)</strong></li>
				<li><strong class="bold">XGBoost</strong></li>
			</ul>
			<h2 id="_idParaDest-73"><a id="_idTextAnchor073"/>Parameters and hyperparameters</h2>
			<p>Each algorithm has a deep set of parameters and hyperparameters for configuration and tuning. Specifying most parameters is optional; if not specified, the default will be used. Parameters include the specification of cross-validation parameters, learning rates, tree depths, weights columns, ignored columns, early stopping parameters, the distribution of response column (for example, Bernoulli), categorical encoding schemes, and many other specifications.</p>
			<p>You can dive deeper into H2O's algorithms<a id="_idIndexMarker210"/> and their parameters in H2O's documentation at <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science.html#algorithms">http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science.html#algorithms</a>. The H2O website also lists tutorials and booklets for its algorithms at <a href="http://docs.h2o.ai/#h2o">http://docs.h2o.ai/#h2o</a>. A full list of algorithm parameters, each with a description, status as a hyperparameter or not, and mapping to algorithms that use the parameter, are found<a id="_idIndexMarker211"/> in H2O's documentation <a href="B16721_Appendix_Final_SK_ePub.xhtml#_idTextAnchor268"><em class="italic">Appendix</em></a> at <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/parameters.html">http://docs.h2o.ai/h2o/latest-stable/h2o-docs/parameters.html</a>.</p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor074"/>H2O extensions of supervised learning</h2>
			<p>H2O extends its supervised learning algorithms by providing <strong class="bold">Automatic Machine Learning (AutoML)</strong> and <strong class="bold">Stacked Ensemble</strong> capabilities. We will take a closer look`<a id="_idIndexMarker212"/> at these in the next section, where we will place H2O algorithms in the larger context of model capabilities. </p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor075"/>Miscellaneous</h2>
			<p>H2O provides utilities to enhance work with its algorithms. <strong class="bold">Target encoding</strong> helps you handle categorical values<a id="_idIndexMarker213"/> and has many configurable<a id="_idIndexMarker214"/> parameters to make this easy. <strong class="bold">TF-IDF</strong> and <strong class="bold">Word2vec</strong> are commonly<a id="_idIndexMarker215"/> used in NLP problems, and they also are nicely configurable. Finally, <strong class="bold">permutation variable importance</strong> is a method to help<a id="_idIndexMarker216"/> understand how strongly your features contribute to the model and can help in evaluating which features to use in your final training dataset. </p>
			<h1 id="_idParaDest-76"><a id="_idTextAnchor076"/>H2O modeling capabilities</h1>
			<p>H2O's supervised learning algorithms<a id="_idIndexMarker217"/> are used to train models on training data, tune them on validation data, and score or predict with them on test or live production data. H2O has extensive capabilities to train, evaluate, explain, score, and inspect models. These are summarized in the following diagram:</p>
			<div>
				<div id="_idContainer027" class="IMG---Figure">
					<img src="image/B17065_04_03.jpg" alt="Figure 4.3 – The H2O supervised learning capabilities&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.3 – The H2O supervised learning capabilities</p>
			<p>Let's take a closer look at the model training capabilities.</p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor077"/>H2O model training capabilities</h2>
			<p>Algorithms<a id="_idIndexMarker218"/> are at the heart of model training, but there are a larger set of capabilities to consider beyond the algorithms themselves. H2O provides the following model training capabilities:</p>
			<ul>
				<li><strong class="bold">AutoML</strong>: An easy-to-use interface and parameter set that automates the process of training and tuning many different models, using multiple algorithms, to create a large number of models in a short amount of time.</li>
				<li><strong class="bold">Cross-validation</strong>: K-fold validation is used to generate performance metrics against folds of the validation split, and parameters such as the number of folds can be specified in the algorithm's training parameters.</li>
				<li><strong class="bold">Checkpointing</strong>: A new model is built as a continuation from a previously trained, checkpointed model as opposed to building the model from scratch; this is useful, for example, in retraining a model with new data.</li>
				<li><strong class="bold">Early stopping</strong>: The parameters to define when an algorithm stops model building early, determined by which of many stopping metrics is specified.</li>
				<li><strong class="bold">Grid search</strong>: Build models for each combination of a range of hyperparameters that are specified and sort the resulting models by a performance metric.</li>
				<li><strong class="bold">Regularization</strong>: Most algorithms have parameter settings to specify regularization techniques to prevent overfitting and increase explainability.</li>
				<li><strong class="bold">Segmented training</strong>: Training data is partitioned into segments based on the same column values, and a separate model is built for each segment.</li>
				<li><strong class="bold">Stacked ensembles</strong>: Combines the results of multiple base models that use the same or different algorithms into a better-performing single model.</li>
			</ul>
			<p>After training a model, we<a id="_idIndexMarker219"/> want to evaluate it to determine whether its predictive performance meets our needs. Let's see what H2O offers in this regard.</p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor078"/>H2O model evaluation capabilities</h2>
			<p>H2O exposes many model<a id="_idIndexMarker220"/> attributes to evaluate model performance. These are summarized as follows:</p>
			<ul>
				<li>The <strong class="bold">leaderboard for AutoML</strong>: The AutoML model results ranked by configured performance metrics or other attributes, such as average prediction speed, with additional metrics shown.</li>
				<li><strong class="bold">Performance metrics for classification problems</strong>: For classification problems, H2O calculates <strong class="bold">GINI coefficient</strong>, <strong class="bold">Absolute Matthew Correlation Coefficient (MCC)</strong>, <strong class="bold">F1,</strong> <strong class="bold">F0.5</strong>, <strong class="bold">F2</strong>, <strong class="bold">Accuracy</strong>, <strong class="bold">Logloss</strong>, <strong class="bold">Area Under the ROC Curve (AUC)</strong>, <strong class="bold">Area Under the Precision-Recall Curve (AUCPR)</strong>, and <strong class="bold">Kolmogorov-Smirnov (KS)</strong> metrics. </li>
				<li><strong class="bold">Performance metrics for regression problems</strong>: For regression problems, H2O calculates <strong class="bold">R Squared (R²)</strong>, <strong class="bold">Mean Squared Error (MSE)</strong>, <strong class="bold">Root Mean Squared Error (RMSE)</strong>, <strong class="bold">Root Mean Squared Logarithmic Error (RMSLE)</strong>, and <strong class="bold">Mean Absolute Error (MAE) </strong>metrics.</li>
				<li><strong class="bold">Prediction metrics</strong>: After a model is built, H2O allows you to predict a leaf node assignment (tree-based models), feature contributions, class probabilities for each stage (GBM models), and feature frequencies on a prediction path (GBM and DRF).</li>
				<li><strong class="bold">Learning curve plot</strong>: This shows a model performance metric as learning progresses to help diagnose<a id="_idIndexMarker221"/> overfitting or underfitting.</li>
			</ul>
			<p>Let's now explore ways to explain H2O models.</p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor079"/>H2O model explainability capabilities</h2>
			<p>H2O presents<a id="_idIndexMarker222"/> a simple and uniform interface to explain either single models or multiple models, which can be a list of separately built models or a reference to those generated from AutoML. On top of that, H2O allows you to generate global (that is, model-level) and local (row- or individual-level) explanations. H2O's explainability capabilities are configurable to your specifications. Output is tabular, graphical, or both, depending on the explanation.</p>
			<p>We will dedicate all of <em class="italic">Chapter 6</em>, <em class="italic">Advanced Model Building – Part II</em>, to explore this important topic in greater detail, but for now, here is a quick list of capabilities:   </p>
			<ul>
				<li><strong class="bold">Residual analysis for regression</strong></li>
				<li><strong class="bold">Confusion matrix for classification</strong></li>
				<li><strong class="bold">Variable importance table and heatmap</strong></li>
				<li><strong class="bold">Model correlation heatmap</strong></li>
				<li><strong class="bold">Shapley values</strong></li>
				<li><strong class="bold">Partial Dependency Plots (PDPs)</strong></li>
				<li><strong class="bold">Individual Conditional Expectation (ICE)</strong></li>
			</ul>
			<p>Let's now complete our survey of H2O's capabilities for modeling at scale by seeing what we can do once our model<a id="_idIndexMarker223"/> is trained, evaluated, and explained.</p>
			<h2 id="_idParaDest-80"><a id="_idTextAnchor080"/>H2O trained model artifacts</h2>
			<p>Once a model is trained, it can be exported<a id="_idIndexMarker224"/> and saved as a scoring artifact. The larger topic of deploying your artifact for production scoring will be treated in <em class="italic">Part 3: Deploying Your Model to Production Environments</em>. Here are the fundamental capabilities of the exported scoring artifact:</p>
			<ul>
				<li><strong class="bold">Predicting with a MOJO</strong>: Models can be saved as self-contained binary Java objects called MOJOs<a id="_idIndexMarker225"/> that can be flexibly implemented as low-latency production scoring artifacts on diverse systems (for example, a REST server, batch database scoring, and Hive UDFs). MOJOs also can be reimported into H2O clusters for purposes described in the next bullet point.</li>
				<li><strong class="bold">Inspecting the model with a MOJO</strong>: An exported MOJO can be re-imported into the H2O cluster and used to score against a dataset, inspect hyperparameters used to train the original model, see the scoring history, and show feature importances.</li>
				<li><strong class="bold">A MOJO compared to a POJO</strong>: The POJO is the precursor to the MOJO and is being deprecated by H2O but<a id="_idIndexMarker226"/> is still required for some algorithms.</li>
			</ul>
			<h1 id="_idParaDest-81"><a id="_idTextAnchor081"/>Summary</h1>
			<p>In this chapter, we conducted a wide survey of H2O capabilities for model building at scale. We learned about the data sources we can ingest into our H2O clusters and the file formats that are supported. We learned how this data moves from the source to the H2O cluster, and how the H2OFrame API provides a single handle in the IDE to represent the distributed in-memory data on the H2O cluster as a single two-dimensional data structure. We then learned the many ways in which we can manipulate data through the H2OFrame API and how to export it to external systems if need be.</p>
			<p>We then surveyed the core of H2O model building at scale – H2O's many state-of-the-art distributed unsupervised and supervised learning algorithms. Then, we put those into context by surveying model capabilities around them, from training, evaluating, and explaining the models, to using model artifacts to retrain, score and inspect models.</p>
			<p>With this map of the landscape firmly in hand, we can now roll up our sleeves and start building state-of-the-art H2O models at scale. In the next chapter, we will start by implementing the advanced model building topics one by one, before later putting it all together in a fully developed use case.</p>
		</div>
	</body></html>