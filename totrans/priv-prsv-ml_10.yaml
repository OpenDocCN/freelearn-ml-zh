- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Federated Learning Benchmarks, Start-Ups, and the Next Opportunity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter focuses on the importance of **Federated Learning** (**FL**) benchmarks
    and highlights products offered by start-up companies in the field.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: 'FL benchmarks:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An introduction to FL benchmarks, including their significance
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Considerations when designing FL benchmarks
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An overview of FL datasets
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A high-level overview of various FL benchmark suites
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting the appropriate FL framework for a project
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: State-of-the-art research in FL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Netxt Opportunity and Key start-up company products in FL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By exploring these topics, you will gain a comprehensive understanding of the
    need for FL benchmarks and the latest advancements in the field. Additionally,
    we will showcase notable products developed by start-up companies that are closely
    related to FL.
  prefs: []
  type: TYPE_NORMAL
- en: FL benchmarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: FL is a machine learning technique that allows multiple devices/clients or servers
    to collaboratively train a model and keep their data private. There has been an
    increasing need for standardized benchmarks to evaluate the performance of different
    FL algorithms and frameworks/platforms.
  prefs: []
  type: TYPE_NORMAL
- en: The IEEE 3652.1-2020 standard, officially titled *IEEE Guide for Architectural
    Framework and Application of Federated Machine Learning*, is a comprehensive guide
    that provides an architectural framework for **Federated Machine Learning** (**FML**).
    More details about the IEEE 3652.1-2020 standard can be found at [https://ieeexplore.ieee.org/document/9382202](https://ieeexplore.ieee.org/document/9382202).
  prefs: []
  type: TYPE_NORMAL
- en: FL benchmarks are datasets and evaluation metrics that are used to compare and
    evaluate the performance of different FL algorithms and frameworks/platforms.
    These benchmarks can help engineers and researchers to identify the strengths
    and weaknesses of different algorithms and the pros and cons of different FL frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will discuss the importance of FL benchmarks and the key considerations
    that should be taken into account when designing them.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of FL benchmarks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: FL benchmarks are essential for various reasons.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, they help researchers evaluate the performance of different FL algorithms
    on standard public datasets, which can help to identify which algorithms are most
    effective for particular use cases. Comparing the performance of different algorithms
    can help drive progress in the field by identifying areas where new algorithms
    are needed, or where existing algorithms need to be improved.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, these benchmarks also help to compare the performance of different
    FL frameworks and identify which framework is best suited to solving business
    use cases using FL.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, benchmarks can help to improve the reproducibility of research results
    by providing a standardized set of metrics and evaluation procedures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Key considerations when designing FL benchmarks: When designing FL benchmarks,
    several key considerations should be taken into account. Some of them are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data distribution**: FL algorithms are designed to work with data that is
    distributed across multiple devices or servers. Therefore, benchmarks should include
    datasets that are representative of the types of data that might be encountered
    in real-world use cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Heterogeneity**: FL involves training models on data that is distributed
    across different devices or servers, each with potentially different hardware
    capabilities and network conditions. Therefore, benchmarks should include datasets
    that are diverse and reflect different types of devices and network conditions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Privacy**: One of the main benefits of FL is its ability to protect the privacy
    of data by keeping it on users’ devices. Therefore, benchmarks should ensure that
    the data used for training is representative of real-world use cases while still
    protecting users’ privacy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluation metrics**: Benchmarks should include standard metrics to evaluate
    the performance of FL algorithms. These metrics should be carefully selected to
    ensure that they accurately reflect the performance of the algorithm and apply
    to the specific use case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reproducibility**: FL benchmarks should be designed to be reproducible, meaning
    that the datasets and evaluation procedures should be clearly documented and made
    available to others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have discussed the key considerations in designing the FL benchmarks. Let’s
    explore further the datasets that need to be considered in FL benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: FL datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the key aspects of FL is the selection of datasets that will be used
    to train the model. Next, we will explore the various considerations that need
    to be taken into account when selecting datasets for FL:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data privacy**: The most critical consideration when selecting datasets for
    FL is data privacy. Since the data is stored locally on each device, it is essential
    to ensure that the data is not exposed to any unauthorized parties. Data privacy
    can be ensured by using encryption techniques or by anonymizing the data. Additionally,
    it is important to have proper consent and permission from the device owners to
    use their data for FL. Support for differential privacy also needs to be considered
    because data anonymization alone may not be sufficient.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data diversity**: Another important consideration when selecting datasets
    for FL is data diversity. The datasets should be diverse enough to capture a wide
    range of data patterns and features. This is essential to ensure that the model
    is not biased toward a particular set of data. For example, if the datasets only
    include data from a particular region or demographic, the model may not be able
    to generalize well to other regions or demographics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data quality**: Data quality is also a critical consideration when selecting
    data sets for FL. The datasets should be clean and free from errors or inconsistencies.
    Data cleaning and preprocessing techniques may be required to ensure that the
    data is suitable for training the model. Additionally, the data should be representative
    of the real-world scenario to ensure that the model can perform well in practical
    situations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data size**: The size of the datasets is also an important consideration
    for FL. The datasets should be large enough to train the model effectively but
    not too large that they become unwieldy to work with. A good balance needs to
    be struck between the size of the datasets and the computational resources available
    to train the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data distribution**: Finally, the distribution of data across devices is
    also an important consideration for FL. The data should be distributed in such
    a way that each device has access to a representative sample of the data. This
    ensures that each device contributes to the training of the model in a meaningful
    way. Additionally, the data distribution should be balanced to avoid any devices
    being overloaded with data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Datasets are critical in benchmarking FL systems. Next, we will explore the
    following popular standard datasets that are used to benchmark FL systems:'
  prefs: []
  type: TYPE_NORMAL
- en: FLAIR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Federated EMNIST
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shakespeare
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CIFAR-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenStreetMap
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Medical image analysis datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FLAIR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Federated Learning Annotated Image Repository** (**FLAIR**) is an open source
    project that aims to provide a centralized repository of annotated images for
    FL. The repository contains a diverse set of images, including medical images,
    satellite images, and natural images, with corresponding annotations. FLAIR provides
    a platform for researchers and developers to evaluate FL algorithms on image-based
    tasks, such as image classification and object detection. The repository allows
    you to share and collaborate on annotated images, reducing the need for individual
    organizations to collect and label their own data. The annotations in FLAIR are
    designed to be privacy-preserving, with sensitive information removed or obfuscated.
    The privacy-preserving annotations and standardized format make FLAIR a valuable
    resource for evaluating FL algorithms on image-based tasks, enabling easy integration
    with popular machine learning frameworks and tools.'
  prefs: []
  type: TYPE_NORMAL
- en: FLAIR has become a popular resource in the FL community, with several FL frameworks
    integrating it into their evaluation pipeline. The repository is continually updated,
    with new datasets and annotations added regularly. It consists of 430,000 images
    from 51,000 Flickr users, which are mapped to 17 coarse-grained labels such as
    art, food, plant, and outdoor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Visit the following GitHub URL for more information: [https://github.com/apple/ml-flair](https://github.com/apple/ml-flair).'
  prefs: []
  type: TYPE_NORMAL
- en: Federated EMNIST
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Federated Extended MNIST** (**Federated EMNIST**) is a benchmark dataset
    for FL developed by Google. It consists of a set of images of handwritten digits
    and letters, similar to the MNIST dataset. The dataset is distributed across multiple
    devices, and the goal is to train a model that can classify images correctly while
    keeping data private.'
  prefs: []
  type: TYPE_NORMAL
- en: Shakespeare
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This dataset consists of a collection of Shakespearean texts, which can be used
    to train a language model for text generation.
  prefs: []
  type: TYPE_NORMAL
- en: CIFAR-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is a widely used benchmark dataset for computer vision tasks, consisting
    of a set of 60,000 color images of 10 different classes. It is used to train a
    model to classify images correctly while keeping data private.
  prefs: []
  type: TYPE_NORMAL
- en: OpenStreetMap
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is a dataset of geographic data, including maps, satellite imagery, and
    GPS coordinates. The dataset can be distributed across multiple devices, and the
    goal is to train a model that can predict traffic patterns or other features of
    the environment while preserving the privacy of data.
  prefs: []
  type: TYPE_NORMAL
- en: Medical image analysis datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are several medical imaging datasets that can be used for FL, such as
    the **Brain Tumor Segmentation** (**BraTS**) challenge dataset and the **Prostate
    MR Image Segmentation** (**PROMISE12**) challenge dataset. These datasets consist
    of medical images that can be used to train models for diagnosis and treatment
    planning while preserving the privacy of patient data.
  prefs: []
  type: TYPE_NORMAL
- en: Frameworks for FL benchmarks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are several FL benchmark frameworks available in the open source community,
    as well as commercially. FL frameworks provide libraries/platforms to benchmark
    FL systems and applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the most popular FL systems and benchmarks are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: LEAF
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FedML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FATE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FedScale
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PySyft
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLCommons – MLPerf
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MedPerf
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow Federated (TFF)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flower (we covered this in the previous chapter so won’t go into detail again
    in this chapter)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll look at these in more detail in the following subsections.
  prefs: []
  type: TYPE_NORMAL
- en: The LEAF FL benchmarks suite
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The LEAF FL benchmarks suite is a project initiated by Carnegie Mellon University,
    which aims to provide a standardized benchmarking suite for FL algorithms ([https://leaf.cmu.edu/](https://leaf.cmu.edu/)).
  prefs: []
  type: TYPE_NORMAL
- en: The LEAF benchmarks enable researchers and developers to evaluate the performance
    of FL algorithms across different domains and configurations. LEAF provides a
    standardized set of tasks and datasets to evaluate the performance of FL algorithms
    on various metrics, including convergence speed, communication efficiency, and
    accuracy. The broad range of tasks includes image classification, text classification,
    and regression tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The datasets used in the benchmarks are designed to be representative of real-world
    scenarios, with varying levels of data distribution, data size, and data complexity.
    The benchmarks are conducted under various settings, such as varying the number
    of clients, the communication rounds, and the amount of data available to each
    client. The benchmarks also evaluate the algorithms’ robustness to adversarial
    attacks and noisy data.
  prefs: []
  type: TYPE_NORMAL
- en: 'LEAF’s core components are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reference implementation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 7.1 – LEAF’s core components and flow](img/B16573_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – LEAF’s core components and flow
  prefs: []
  type: TYPE_NORMAL
- en: More details about the benchmarks of LEAF can be obtained at [https://arxiv.org/abs/1812.01097](https://arxiv.org/abs/1812.01097).
  prefs: []
  type: TYPE_NORMAL
- en: FedML
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: FedML ([https://github.com/FedML-AI/FedML](https://github.com/FedML-AI/FedML))
    is a benchmark suite for FL that includes several datasets and tasks, such as
    image classification, language modeling, and speech recognition. FedML evaluates
    FL algorithms on their robustness in **non-independent and identically distributed**
    (**non-iid**) data distributions, scalability to large datasets, and communication
    efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: The framework supports various types of FL setups, such as horizontal, vertical,
    and federated transfer learning, as well as different types of optimization algorithms,
    including federated averaging, FedProx, and FedAdapt. The FedML framework is designed
    to be modular, flexible, and scalable, allowing users to customize and extend
    the existing algorithms or develop new ones easily. It also provides a set of
    benchmark datasets and evaluation metrics to facilitate the comparison of different
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: The FedML community actively develops and maintains the framework and organizes
    workshops and challenges to promote research and innovation in FL. The community
    also collaborates with industry partners to apply FL to real-world use cases,
    such as healthcare, finance, and telecommunications. FedML is an important initiative
    that addresses the challenges and opportunities of FL and enables the development
    of secure, privacy-preserving, and efficient machine learning models in a distributed
    setting. FedML supports three types of computing platforms – IoT/mobile, distributed
    computing, and standalone simulation.
  prefs: []
  type: TYPE_NORMAL
- en: 'More details can be found at the following reference research URL: [https://arxiv.org/abs/2007.13518](https://arxiv.org/abs/2007.13518).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – FedML architecture](img/B16573_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – FedML architecture
  prefs: []
  type: TYPE_NORMAL
- en: FATE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Federated AI Technology Enabler** (**FATE**) is an open source project that
    provides a secure and privacy-preserving way to collaborate on **artificial intelligence**
    (**AI**) models. FATE is designed to enable FL, which is a distributed machine
    learning approach that allows multiple parties to train a shared model without
    sharing their data. FATE is developed by the WeBank AI department, a subsidiary
    of Tencent. [https://github.com/FederatedAI/FATE](https://github.com/FederatedAI/FATE)'
  prefs: []
  type: TYPE_NORMAL
- en: FATE provides a platform for developers to implement FL techniques in their
    AI applications. It supports different types of FL, such as horizontal and vertical,
    and provides tools to manage the communication, synchronization, and aggregation
    of models across different devices. FATE uses a variety of cryptographic techniques
    to ensure the privacy and security of the FL process. For example, FATE employs
    differential privacy to add noise to training data, preventing the leakage of
    sensitive information about individual users. FATE also uses homomorphic encryption
    to enable computation on encrypted data, which allows parties to collaborate without
    revealing their data to each other. We will learn about homomorphic encryption
    in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: FATE is built on top of Kubernetes, a popular open source container orchestration
    system. Kubernetes provides FATE with the ability to manage containers and automate
    the deployment, scaling, and monitoring of the FL infrastructure. FATE leverages
    cryptographic techniques and Kubernetes to ensure the privacy and security of
    the FL process. FATE provides a web-based **graphical user interface** (**GUI**)
    for developers to interact with the platform, monitor the training process, and
    visualize the results. It has been adopted by various companies and organizations
    to build privacy-preserving AI applications. For example, Tencent has used FATE
    to develop a privacy-preserving recommendation system for its e-commerce platform.
    Huawei has also used FATE to build an FL platform for the healthcare industry.
    FATE’s popularity is expected to grow as FL becomes increasingly important in
    the AI industry.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of FATE
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The following figure shows the FATE architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – FATE architecture](img/B16573_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – FATE architecture
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure was sourced from [https://fate.readthedocs.io/en/latest/architecture/](https://fate.readthedocs.io/en/latest/architecture/).
  prefs: []
  type: TYPE_NORMAL
- en: The FedScale benchmarking platform
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The FedScale benchmark suite includes several datasets and workloads that are
    designed to evaluate the performance of FL algorithms and systems. The datasets
    are selected to reflect the heterogeneity and non-IIDness of real-world data.
    The workloads are designed to simulate the communication overhead and computation
    complexity of real-world FL scenarios, as well as evaluate the performance of
    FL algorithms and systems in these scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: One of the datasets included in the FedScale benchmark suite is the Synthetic
    IID dataset, which consists of synthetic data that is randomly generated and distributed
    evenly among the clients. This dataset is designed to evaluate the scalability
    and efficiency of FL algorithms and systems. Another dataset included in the FedScale
    benchmark suite is the Heterogeneous Image dataset, which consists of images from
    different sources and domains. This dataset is designed to evaluate the robustness
    of FL algorithms and systems to heterogeneous data.
  prefs: []
  type: TYPE_NORMAL
- en: The FedScale benchmark suite also includes more complex workloads, such as the
    Federated Meta-Learning workload, which involves clients learning from each other’s
    data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – The FedScale runtime to run FL benchmarks](img/B16573_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – The FedScale runtime to run FL benchmarks
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure was sourced from [https://github.com/SymbioticLab/FedScale/blob/master/docs/fedscale-sim-mode.png](https://github.com/SymbioticLab/FedScale/blob/master/docs/fedscale-sim-mode.png).
  prefs: []
  type: TYPE_NORMAL
- en: MLCommons
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MLCommons is a non-profit organization that aims to accelerate machine learning
    innovation and development by providing a platform for collaboration and the sharing
    of resources among its members. It was founded in June 2020 by a group of leading
    researchers, engineers, and entrepreneurs in the field of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: The organization’s primary focus is on developing and promoting best practices
    for designing and deploying machine learning systems, including hardware and software
    frameworks, datasets, benchmarks, and evaluation metrics. One of the key initiatives
    of MLCommons is the MLPerf benchmark suite, which is designed to measure the performance
    of machine learning systems across a range of applications, including computer
    vision, natural language processing, and recommendation systems. The benchmarks
    are developed and maintained by a global community of researchers and engineers,
    and they are used to evaluate the performance of hardware and software platforms
    for machine learning. MLPerf has quickly become the industry standard for benchmarking
    machine learning performance and is used by leading technology companies, researchers,
    and government agencies around the world.
  prefs: []
  type: TYPE_NORMAL
- en: MedPerf
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MedPerf is an open benchmarking platform for medical AI using FL. The MLCommons
    team piloted multiple use cases in collaboration with multiple institutions and
    universities to run FL models using MDPerf benchmarks. The use cases are brain
    tumor segmentation, pancreas segmentation, surgical workflow phase recognition,
    and cloud experiments.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16573_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – MedPerf architecture for cloud experiments
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure was sourced from [https://github.com/mlcommons/medperf](https://github.com/mlcommons/medperf).
  prefs: []
  type: TYPE_NORMAL
- en: Selecting an FL framework for a project
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Selecting the best FL framework can be a challenging task, since there are several
    factors to consider, such as the features, scalability, security, ease of use,
    documentation, the machine learning algorithms to support (out of the box), neural
    network support, and the learning curve.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some of the key considerations when selecting an FL framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Features**: It is important to consider the features that the framework offers.
    Some of the critical features to look for include support for different machine
    learning algorithms, distributed training, data privacy, and data validation.
    You should choose a framework that has features that match your use case needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: Another critical consideration is scalability. The framework
    should be able to handle large datasets and a high number of participants. You
    should also consider whether the framework can scale horizontally or vertically.
    In the case of cross-device (mobile, web, etc.), the framework should be able
    to handle a large number of clients.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security and privacy**: FL involves multiple parties sharing data, so it’s
    essential to select a framework that offers robust security features. The framework
    should have encryption and authentication capabilities to protect the data from
    unauthorized access. You should also consider whether the framework has a transparent
    security model, allowing you to audit the security measures in place. You need
    to consider whether or not the framework supports privacy protection by default,
    such as differential privacy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ease of use**: The ease of use of the framework is also a critical consideration.
    The framework should have an intuitive user interface that makes it easy to set
    up, configure, and use. You should also consider whether the framework has good
    documentation and community support, as this can help you troubleshoot any issues
    that arise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning curve**: You should consider the learning curve of the framework
    as well in terms of user documentation, deployment setup, programming language
    support, the examples and samples provided, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compatibility**: Finally, you should consider whether the framework is compatible
    with your existing technology stack and programming language. For example, if
    you’re using TensorFlow for machine learning, you may want to select an FL framework
    that is compatible with TensorFlow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In conclusion, selecting the best framework requires careful consideration of
    several factors, including features, scalability, security, privacy, ease of use,
    the learning curve, and compatibility with your existing technology stack. By
    considering these factors, you can select a framework that meets your requirements
    to solve the business use case using FL.
  prefs: []
  type: TYPE_NORMAL
- en: A comparison of FedScale, FATE, Flower, and TensorFlow Federated
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: FedScale, FATE, Flower, FedML, and **TensorFlow Federated** (**TFF**) are some
    of the popular FL frameworks that offer unique features and capabilities. In the
    following subsections, we will compare these features and capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: FedScale is a framework that supports FL for a wide range of machine learning
    models, including deep neural networks, decision trees, and **Support Vector Machines**
    (**SVMs**). It also supports various optimization algorithms such as federated
    **stochastic gradient descent** (**SGD**) and federated proximal algorithms. FATE,
    conversely, offers a comprehensive suite of FL tools and supports various machine
    learning algorithms, including logistic regression, decision trees, and deep learning
    models. Flower is a lightweight framework that supports FL for various machine
    learning models, including neural networks, decision trees, and logistic regression.
    TFF is a widely used FL framework that supports TensorFlow-based machine learning
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: FedScale and FATE are designed for large-scale deployments and can handle a
    high number of participants. Flower is a lightweight framework that is ideal for
    small to large-scale FL projects. TFF is a scalable framework that can handle
    large-scale FL projects and can be used to deploy FL algorithms in production
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: Security
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All four frameworks offer robust security features such as encryption and authentication
    to protect data during the FL process. FATE has additional features such as differential
    privacy and homomorphic encryption to enhance data privacy. TFF also offers secure
    **multi-party computation** (**MPC**) protocols that enable multiple parties to
    securely collaborate on FL projects.
  prefs: []
  type: TYPE_NORMAL
- en: Ease of use
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: FedScale and FATE have a steeper learning curve than Flower and TFF, as they
    offer more complex features and tools. Flower and TFF are both easy to use, with
    intuitive APIs and documentation that make it easy to get started. TFF also offers
    a high-level API that enables users to build FL algorithms without requiring deep
    knowledge of the underlying details.
  prefs: []
  type: TYPE_NORMAL
- en: Community support
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All four frameworks have active open source contributors that provide support
    and documentation. TFF, being developed by Google, has the largest community and
    support, followed by Flower, FedScale, and FATE.
  prefs: []
  type: TYPE_NORMAL
- en: Each of these frameworks has its unique features and capabilities, making them
    suitable for different use cases. FedScale and FATE are ideal for large-scale
    FL deployments, while Flower and TFF are more suited for small to medium-scale
    projects. TFF has the largest community and support, making it an excellent choice
    for developers looking for an FL framework with extensive support and resources.
    Ultimately, the choice of framework will depend on the specific use case, requirements,
    and familiarity with the framework.
  prefs: []
  type: TYPE_NORMAL
- en: Research papers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Each framework published its own benchmark and results in the research journals,
    with a list of supported features and accuracy metrics. Currently, there are no
    industry standard benchmarks similar to [TPC.Org](http://TPC.Org) or [SPEC.org](http://SPEC.org)
    that are widely accepted.
  prefs: []
  type: TYPE_NORMAL
- en: State-of-the-art research in FL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: FL is a rapidly growing technology, and there are several state-of-the-art research
    directions in this space. The following are some of the recent advances in FL
    research.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a high-level comparison of the different FL frameworks:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **High-level capabilities** | **FATE** | **FedML** | **Flower** | **FedScale**
    | **TFF** |'
  prefs: []
  type: TYPE_TB
- en: '| Regression models | Y | Y | Y | Y | Y |'
  prefs: []
  type: TYPE_TB
- en: '| Neural networks | Y | Y | Y | Y | Y |'
  prefs: []
  type: TYPE_TB
- en: '| Tree-based models | Y | N | N | N | N |'
  prefs: []
  type: TYPE_TB
- en: '| Communication protocol | Customized | MPI | gRPC | gRPC | gRPC |'
  prefs: []
  type: TYPE_TB
- en: '| Support for differential privacy | N | N | Y | Y | Y |'
  prefs: []
  type: TYPE_TB
- en: '| Single host deployment | Y | Y | Y | Y | Y |'
  prefs: []
  type: TYPE_TB
- en: '| Cross-device deployment | N | Y | Y | Y | N |'
  prefs: []
  type: TYPE_TB
- en: '| Research papers | Y | Y | Y | Y | Y |'
  prefs: []
  type: TYPE_TB
- en: Table 7.1 – Comparison of the different FL frameworks
  prefs: []
  type: TYPE_NORMAL
- en: Communication-efficient FL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the main challenges in FL is the high communication cost of transmitting
    the model updates between the devices and the central server. Recent research
    has focused on reducing the communication cost in FL by developing new compression
    techniques, such as quantization, sparsification, and differential compression.
    These techniques can significantly reduce the communication cost without sacrificing
    the model’s accuracy. Several researchers and research groups are actively working
    on **communication-efficient FL** (**CE-FL**) research:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Google Research**: Google Research has been actively working on CE-FL research
    and has developed several techniques to reduce communication costs in FL. One
    of its recent works is *Communication-Efficient Learning of Deep Networks from
    Decentralized Data*, which proposes a new compression technique called **Quantized
    SGD** (**QSGD**) that can significantly reduce the communication cost in FL. This
    is a family of compression schemes that allow the compression of gradient updates
    at each node, while guaranteeing convergence under standard assumptions. See the
    Arxiv paper at [https://arxiv.org/abs/1602.05629](https://arxiv.org/abs/1602.05629).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The Federated Learning Community**: The Federated Learning Community is an
    open community of researchers and practitioners who are working on FL research.
    The community has several working groups, including a CE-FL working group, which
    is focused on developing new techniques to reduce communication costs in FL.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Carnegie Mellon University**: The Machine Learning department at Carnegie
    Mellon University has several researchers who are working on CE-FL research. One
    of its recent works is *Communication-Efficient Distributed Learning with Feature-Selective
    Sampling*, which proposes a new sampling technique that can reduce the communication
    cost in FL.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IBM Research**: IBM researchers and the University of Michigan are working
    on CE-FL research, and their most recent work is *Federated Learning with Matched
    Averaging*, which proposes a new aggregation technique that can reduce the communication
    cost in FL. See the Arxiv paper at [https://arxiv.org/abs/2002.06440](https://arxiv.org/abs/2002.06440).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are just a few examples of the many researchers and research groups who
    are actively working on CE-FL research. As the field of FL continues to evolve,
    it is expected that more researchers and research groups will focus on developing
    new techniques to reduce the communication cost in FL.
  prefs: []
  type: TYPE_NORMAL
- en: Privacy-preserving FL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Privacy is a critical concern in FL, since data is distributed across multiple
    devices, and each device’s owner wants to protect their data privacy. Recent research
    has focused on developing new privacy-preserving techniques, such as differential
    privacy and secure multi-party computation, to protect the data privacy of the
    devices’ owners while enabling collaborative model training. These techniques
    can provide strong privacy guarantees while preserving the model’s accuracy. Several
    researchers and research groups are actively working on **privacy-preserving FL**
    (**PP-FL**) research.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some notable examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '**OpenMined**: OpenMined is an open source community dedicated to advancing
    PP-FL research. It has developed several PP-FL frameworks, including PySyft, which
    is a Python library for PP-FL.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Google Research**: Google Research has been actively working on PP-FL research
    and has developed several techniques to protect the privacy of user data in FL.
    One of its recent works is *Privacy-Preserving Federated Learning with Byzantine
    Robust Aggregation*, which proposes a new aggregation technique that can protect
    user privacy in the presence of malicious actors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Carnegie Mellon University**: The CyLab Security and Privacy Institute at
    Carnegie Mellon University has several researchers who are working on PP-FL research.
    One of its recent works is *Privacy-Preserving Federated Learning via Randomized
    Smoothing*, which proposes a new technique that can protect user privacy by adding
    noise to the user data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The University of Oxford**: The University of Oxford has several researchers
    who are working on PP-FL research. One of their recent works is *Secure Federated
    Learning on Curves*, which proposes a new technique that can protect user privacy
    by using elliptic curves to encrypt user data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Microsoft Research**: Microsoft Research has been actively working on PP-FL
    research and has developed several techniques to protect user privacy in FL. One
    of its recent works is *Private Federated Learning with Secure Aggregation*, which
    proposes a new aggregation technique that can protect user privacy by using homomorphic
    encryption.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are some examples of the many researchers and research groups who are
    actively working on PP-FL research. As the field of FL continues to evolve, it
    is expected that more researchers and research groups will focus on developing
    new techniques to protect the privacy of user data in FL.
  prefs: []
  type: TYPE_NORMAL
- en: Federated Meta-Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Federated Meta-Learning** (**FML**) is a new research direction that combines
    FL and meta-learning to learn from multiple data sources and tasks while preserving
    privacy. FML enables collaborative model training across multiple devices and
    organizations while preserving the data privacy of the devices’ owners. Recent
    research has explored new FML techniques and applications, such as personalized
    medicine and personalized recommendations. FML is a relatively new research direction
    in FL, and only a few research groups are working actively on it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Google Research**: Google Research has been actively working on FML research
    and has developed several techniques to learn from multiple clients’ meta-knowledge
    while preserving their privacy. One of its research works is *Federated Meta-Learning
    for Recommendation*, which proposes a new framework for FML for recommendation
    tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Carnegie Mellon University**: The Machine Learning department at Carnegie
    Mellon University has several researchers who are working on FML research. One
    of its recent works is *Federated Meta-Learning for Fast Model Adaptation in Healthcare*,
    which proposes a new approach for learning from multiple healthcare institutions
    while preserving their privacy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The University of Oxford**: The University of Oxford has several researchers
    who are working on FML research. One of its recent works is *Federated Meta-Learning
    for Recommendation with Private and Communication-Efficient Model Aggregation*,
    which proposes a new approach for FML for recommendation tasks that preserves
    privacy and reduces communication costs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The University of California, Berkeley**: The RISELab at the University of
    California, Berkeley, has several researchers who are working on FML research.
    One of its recent works is *Federated Meta-Learning for Few-Shot Learning Across
    Heterogeneous Devices*, which proposes a new approach for FML that can learn from
    data across heterogeneous devices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are a few examples of the research groups that are actively working on
    FML research. As the field of FML continues to grow, it is expected that more
    researchers and research groups will focus on developing new techniques for FML
    in various applications.
  prefs: []
  type: TYPE_NORMAL
- en: Adaptive FL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In FL, devices’ data distribution can change over time, leading to a concept
    drift problem that can degrade a model’s performance. Recent research has focused
    on developing new adaptive learning techniques that can efficiently update the
    model while preserving privacy. These techniques can adapt the model to the changing
    data distribution and improve the model’s performance over time. **Adaptive federated
    learning** (**AFL**) is an emerging research direction in FL that focuses on dynamically
    adapting the FL system to changing conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some notable examples of researchers and research groups who are working
    actively on AFL:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Carnegie Mellon University**: The Machine Learning department at Carnegie
    Mellon University has several researchers who are working on AFL research. One
    of its recent works is *Adaptive Federated Optimization*, which proposes a new
    framework for AFL that adapts the learning rate and aggregation strategy to network
    conditions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Google Research**: Google Research has been actively working on AFL research
    and has developed several techniques to adapt the FL system to changing conditions.
    One of its recent works is *Adaptive Federated Optimization with Local Descent*,
    which proposes a new approach for AFL that adapts the learning rate and aggregation
    strategy to the client’s local data distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The University of Southern California**: The Department of Computer Science
    at the University of Southern California has several researchers who are working
    on AFL research. One of its recent works is *Decentralized Adaptive Federated
    Learning with Gradient Compression*, which proposes a new approach for AFL that
    adapts the learning rate and aggregation strategy to network conditions while
    compressing the gradients to reduce communication costs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tsinghua University**: The Department of Computer Science and Technology
    at Tsinghua University has several researchers who are working on AFL research.
    One of its recent works is *Adaptive Federated Learning via Second-Order Information
    Exchange*, which proposes a new approach for AFL that adapts the learning rate
    and aggregation strategy based on the second-order information exchange among
    clients.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are a few examples of the research groups that are actively working on
    AFL research. As the field of AFL continues to grow, it is expected that more
    researchers and research groups will focus on developing new techniques to adapt
    the FL system to changing conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Federated reinforcement learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Reinforcement learning** (**RL**) is a machine learning paradigm that enables
    agents to learn through trial and error. **Federated reinforcement learning**
    (**FRL**) is a new research direction that combines FL and RL to enable privacy-preserving
    collaborative learning for decision-making tasks. Recent research has explored
    new FRL algorithms and applications, such as autonomous driving and robotics.'
  prefs: []
  type: TYPE_NORMAL
- en: There are many researchers and organizations actively working on FRL to improve
    machine learning techniques that protect user privacy and data security.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few examples:'
  prefs: []
  type: TYPE_NORMAL
- en: Google’s **Federated Learning of Cohorts** (**FLoC**) team is working on developing
    FRL to create privacy-preserving user models for ad targeting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Researchers at Stanford University have been exploring the use of FRL to train
    autonomous robots. A team of researchers from Carnegie Mellon University is investigating
    FRL as a way to improve the efficiency and privacy of healthcare machine learning
    models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The OpenMined project is a community-driven initiative that aims to create an
    ecosystem for privacy-preserving machine learning, including FRL.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microsoft Research is working on developing FRL methods for distributed robotics
    applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are just a few examples of the many researchers and organizations working
    on FRL, which is a rapidly growing field with many exciting developments and applications.
  prefs: []
  type: TYPE_NORMAL
- en: '**Federated Tumour Segmentation (FeTs**) is a real-world medical FL platform
    developed by Intel and the **University of Pennsylvania** (**UPenn**). They make
    use of OpenFL as a backend for the platform.'
  prefs: []
  type: TYPE_NORMAL
- en: Key company products related to FL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we have seen FL is a rapidly growing field that has gained significant attention
    from start-up companies. Here is a summary of some of the companies that are working
    or providing FL products:'
  prefs: []
  type: TYPE_NORMAL
- en: '**DynamoFL**: DynamoFL is built by privacy and machine learning experts from
    **MIT** (**Massachusetts Institute of Technology**) and Harvard, who built leading
    FL solutions at Google AI and privacy-enhanced technologies at Microsoft. As per
    its website, the current **large language models** (**LLMs**) are not private,
    but the LLMs from DynamoFL are. They provide personalized FL, which is another
    key area of research. LLMs are covered in [*Chapter 10*](B16573_10.xhtml#_idTextAnchor219)
    in the *Privacy-preserved generative* *AI* section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NVIDIA FLARE**: NVIDIA **Federated Learning Application Runtime Environment**
    (**FLARE**) is an SDK for FL that is open sourced by NVIDIA. FLARE supports various
    FL algorithms (FedAvg, FedProx, FedOpt, etc.), neural networks, and ML algorithms.
    It supports differential privacy and homomorphic encryption features as part of
    the security and privacy preservation stack. The SDK also provides a simulator
    that can be used to start servers and clients and execute FL notebooks and FL
    applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OpenMined**: OpenMined is an open source community that provides a platform
    for privacy-preserving ML, including FL tools and libraries for secure aggregation
    and differential privacy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DataFleets**: DataFleets provides a platform for privacy-preserving data
    access and analytics, including FL tools for secure data processing and analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scaleout**: Scaleout provides a platform for distributed ML, including FL
    tools for privacy-preserving and collaborative model training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Edge Impulse**: Edge Impulse provides a platform for developing and deploying
    ML models on edge devices, including FL tools for privacy-preserving model training
    and inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decentralized Machine Learning**: Decentralized Machine Learning provides
    a platform for privacy-preserving and decentralized ML, including FL tools for
    secure aggregation and differential privacy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PySyft**: PySyft is an open source library for FL and secure multi-party
    computation, enabling privacy-preserving and collaborative ML on distributed data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Intellegens**: Intellegens’ product originated from the University of Cambridge
    and Ichnite and is an FL platform product that can be deployed in the cloud or
    on-premises ([https://intellegens.com/](https://intellegens.com/)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These companies are actively contributing to the development and advancement
    of FL, providing tools and platforms for privacy-preserving and collaborative
    ML on distributed data. As FL continues to evolve, it is expected that more start-up
    companies will emerge, contributing to the growth and innovation of this field.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored FL benchmarks, their importance, and how to design
    benchmarks, among other things. We also discussed various FL frameworks and looked
    at what to consider when choosing a framework to implement FL applications. Finally,
    we covered the state-of-the-art research undertaken by various enterprises in
    collaboration with key universities, highlighting some of the key companies that
    are actively working and offering products/platforms to support FL.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about homomorphic encryption and secure multi-party
    computation and how they help in achieving privacy in ML models.
  prefs: []
  type: TYPE_NORMAL
