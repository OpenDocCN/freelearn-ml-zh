- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Federated Learning Benchmarks, Start-Ups, and the Next Opportunity
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联邦学习基准、初创公司和下一个机会
- en: This chapter focuses on the importance of **Federated Learning** (**FL**) benchmarks
    and highlights products offered by start-up companies in the field.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点介绍了**联邦学习**（**FL**）基准的重要性，并突出了该领域初创公司提供的产品。
- en: 'We will cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主要主题：
- en: 'FL benchmarks:'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FL基准：
- en: An introduction to FL benchmarks, including their significance
  id: totrans-5
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: FL基准的介绍，包括其重要性
- en: Considerations when designing FL benchmarks
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计FL基准时的考虑因素
- en: An overview of FL datasets
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: FL数据集概述
- en: A high-level overview of various FL benchmark suites
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各类FL基准套件的高级概述
- en: Selecting the appropriate FL framework for a project
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择适合项目的适当FL框架
- en: State-of-the-art research in FL
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FL领域的最先进研究
- en: Netxt Opportunity and Key start-up company products in FL
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FL的下一个机会和关键初创公司产品
- en: By exploring these topics, you will gain a comprehensive understanding of the
    need for FL benchmarks and the latest advancements in the field. Additionally,
    we will showcase notable products developed by start-up companies that are closely
    related to FL.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 通过探讨这些主题，您将全面了解FL基准的需求以及该领域的最新进展。此外，我们将展示与FL密切相关的一些初创公司开发的显著产品。
- en: FL benchmarks
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FL基准
- en: FL is a machine learning technique that allows multiple devices/clients or servers
    to collaboratively train a model and keep their data private. There has been an
    increasing need for standardized benchmarks to evaluate the performance of different
    FL algorithms and frameworks/platforms.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: FL是一种机器学习技术，允许多个设备/客户端或服务器协作训练模型并保持其数据隐私。对标准化基准的需求不断增加，以评估不同FL算法和框架/平台的表现。
- en: The IEEE 3652.1-2020 standard, officially titled *IEEE Guide for Architectural
    Framework and Application of Federated Machine Learning*, is a comprehensive guide
    that provides an architectural framework for **Federated Machine Learning** (**FML**).
    More details about the IEEE 3652.1-2020 standard can be found at [https://ieeexplore.ieee.org/document/9382202](https://ieeexplore.ieee.org/document/9382202).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: IEEE 3652.1-2020标准，正式名称为*IEEE关于联邦机器学习架构和应用指南*，是一本综合指南，提供了联邦机器学习（**FML**）的架构框架。有关IEEE
    3652.1-2020标准的更多详细信息，请参阅[https://ieeexplore.ieee.org/document/9382202](https://ieeexplore.ieee.org/document/9382202)。
- en: FL benchmarks are datasets and evaluation metrics that are used to compare and
    evaluate the performance of different FL algorithms and frameworks/platforms.
    These benchmarks can help engineers and researchers to identify the strengths
    and weaknesses of different algorithms and the pros and cons of different FL frameworks.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: FL基准是用于比较和评估不同FL算法和框架/平台性能的数据集和评估指标。这些基准可以帮助工程师和研究人员识别不同算法的优缺点以及不同FL框架的利弊。
- en: Next, we will discuss the importance of FL benchmarks and the key considerations
    that should be taken into account when designing them.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论FL基准的重要性以及设计它们时应考虑的关键因素。
- en: The importance of FL benchmarks
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FL基准的重要性
- en: FL benchmarks are essential for various reasons.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: FL基准对于多个原因至关重要。
- en: Firstly, they help researchers evaluate the performance of different FL algorithms
    on standard public datasets, which can help to identify which algorithms are most
    effective for particular use cases. Comparing the performance of different algorithms
    can help drive progress in the field by identifying areas where new algorithms
    are needed, or where existing algorithms need to be improved.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，它们帮助研究人员评估不同FL算法在标准公共数据集上的性能，这有助于确定哪些算法对特定用例最有效。比较不同算法的性能可以帮助通过确定需要新算法或现有算法需要改进的领域来推动该领域的进步。
- en: Secondly, these benchmarks also help to compare the performance of different
    FL frameworks and identify which framework is best suited to solving business
    use cases using FL.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，这些基准还有助于比较不同FL框架的性能，并确定哪个框架最适合使用FL解决业务用例。
- en: Finally, benchmarks can help to improve the reproducibility of research results
    by providing a standardized set of metrics and evaluation procedures.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，基准可以通过提供标准化的指标和评估程序来帮助提高研究结果的再现性。
- en: 'Key considerations when designing FL benchmarks: When designing FL benchmarks,
    several key considerations should be taken into account. Some of them are as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 设计FL基准时的关键考虑因素：在设计FL基准时，应考虑几个关键因素。以下是一些例子：
- en: '**Data distribution**: FL algorithms are designed to work with data that is
    distributed across multiple devices or servers. Therefore, benchmarks should include
    datasets that are representative of the types of data that might be encountered
    in real-world use cases.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据分布**：联邦学习算法旨在处理分布在多个设备或服务器上的数据。因此，基准测试应包括代表可能在实际使用案例中遇到的数据类型的数据集。'
- en: '**Heterogeneity**: FL involves training models on data that is distributed
    across different devices or servers, each with potentially different hardware
    capabilities and network conditions. Therefore, benchmarks should include datasets
    that are diverse and reflect different types of devices and network conditions.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异构性**：联邦学习涉及在不同设备或服务器上训练数据分布的模型，每个设备可能具有不同的硬件能力和网络条件。因此，基准测试应包括多样化的数据集，并反映不同类型的设备和网络条件。'
- en: '**Privacy**: One of the main benefits of FL is its ability to protect the privacy
    of data by keeping it on users’ devices. Therefore, benchmarks should ensure that
    the data used for training is representative of real-world use cases while still
    protecting users’ privacy.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐私**：联邦学习（FL）的主要优势之一是能够通过在用户设备上保留数据来保护数据的隐私。因此，基准测试应确保用于训练的数据能够代表真实世界的使用案例，同时仍然保护用户的隐私。'
- en: '**Evaluation metrics**: Benchmarks should include standard metrics to evaluate
    the performance of FL algorithms. These metrics should be carefully selected to
    ensure that they accurately reflect the performance of the algorithm and apply
    to the specific use case.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估指标**：基准测试应包括标准指标来评估联邦学习算法的性能。这些指标应经过仔细选择，以确保它们能够准确反映算法的性能并适用于特定的使用案例。'
- en: '**Reproducibility**: FL benchmarks should be designed to be reproducible, meaning
    that the datasets and evaluation procedures should be clearly documented and made
    available to others.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可重现性**：联邦学习基准测试应设计成可重现的，这意味着数据集和评估程序应被明确记录并可供他人使用。'
- en: We have discussed the key considerations in designing the FL benchmarks. Let’s
    explore further the datasets that need to be considered in FL benchmarks.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了设计联邦学习基准测试时的关键考虑因素。接下来，我们将进一步探讨在联邦学习基准测试中需要考虑的数据集。
- en: FL datasets
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 联邦学习数据集
- en: 'One of the key aspects of FL is the selection of datasets that will be used
    to train the model. Next, we will explore the various considerations that need
    to be taken into account when selecting datasets for FL:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习的关键方面之一是选择用于训练模型的数据库集。接下来，我们将探讨在选择联邦学习数据集时需要考虑的各种因素：
- en: '**Data privacy**: The most critical consideration when selecting datasets for
    FL is data privacy. Since the data is stored locally on each device, it is essential
    to ensure that the data is not exposed to any unauthorized parties. Data privacy
    can be ensured by using encryption techniques or by anonymizing the data. Additionally,
    it is important to have proper consent and permission from the device owners to
    use their data for FL. Support for differential privacy also needs to be considered
    because data anonymization alone may not be sufficient.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据隐私**：在选择联邦学习数据集时，最关键的考虑因素是数据隐私。由于数据存储在每个设备上，因此确保数据不会被任何未经授权的第三方访问至关重要。可以通过使用加密技术或匿名化数据来确保数据隐私。此外，从设备所有者那里获得适当的同意和许可以使用他们的数据用于联邦学习也很重要。还需要考虑对差分隐私的支持，因为仅仅数据匿名化可能不足以保证。'
- en: '**Data diversity**: Another important consideration when selecting datasets
    for FL is data diversity. The datasets should be diverse enough to capture a wide
    range of data patterns and features. This is essential to ensure that the model
    is not biased toward a particular set of data. For example, if the datasets only
    include data from a particular region or demographic, the model may not be able
    to generalize well to other regions or demographics.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据多样性**：在为联邦学习选择数据集时，数据多样性是另一个重要的考虑因素。数据集应该足够多样化，以捕捉广泛的数据模式和特征。这对于确保模型不会偏向特定数据集是至关重要的。例如，如果数据集只包括来自特定地区或人口统计数据的数据，那么模型可能无法很好地推广到其他地区或人口统计数据。'
- en: '**Data quality**: Data quality is also a critical consideration when selecting
    data sets for FL. The datasets should be clean and free from errors or inconsistencies.
    Data cleaning and preprocessing techniques may be required to ensure that the
    data is suitable for training the model. Additionally, the data should be representative
    of the real-world scenario to ensure that the model can perform well in practical
    situations.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据质量**：在选择联邦学习的数据集时，数据质量也是一个重要的考虑因素。数据集应该是干净的，没有错误或不一致之处。可能需要数据清洗和预处理技术来确保数据适合训练模型。此外，数据应该代表现实世界场景，以确保模型在实际情况下能表现良好。'
- en: '**Data size**: The size of the datasets is also an important consideration
    for FL. The datasets should be large enough to train the model effectively but
    not too large that they become unwieldy to work with. A good balance needs to
    be struck between the size of the datasets and the computational resources available
    to train the model.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据大小**：数据集的大小也是联邦学习的一个重要考虑因素。数据集应该足够大，以便有效地训练模型，但又不能太大，以至于难以处理。需要在数据集的大小和用于训练模型的计算资源之间找到一个良好的平衡。'
- en: '**Data distribution**: Finally, the distribution of data across devices is
    also an important consideration for FL. The data should be distributed in such
    a way that each device has access to a representative sample of the data. This
    ensures that each device contributes to the training of the model in a meaningful
    way. Additionally, the data distribution should be balanced to avoid any devices
    being overloaded with data.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据分布**：最后，数据在设备间的分布也是联邦学习的一个重要考虑因素。数据应该以这种方式分布，使得每个设备都能访问到数据的代表性样本。这确保了每个设备都能以有意义的方式为模型的训练做出贡献。此外，数据分布应该是平衡的，以避免任何设备因数据过载而超负荷。'
- en: 'Datasets are critical in benchmarking FL systems. Next, we will explore the
    following popular standard datasets that are used to benchmark FL systems:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集在基准测试联邦学习系统时至关重要。接下来，我们将探讨以下流行的标准数据集，这些数据集被用于基准测试联邦学习系统：
- en: FLAIR
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FLAIR
- en: Federated EMNIST
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 联邦 EMNIST
- en: Shakespeare
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 莎士比亚
- en: CIFAR-10
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CIFAR-10
- en: OpenStreetMap
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenStreetMap
- en: Medical image analysis datasets
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 医学图像分析数据集
- en: FLAIR
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: FLAIR
- en: '**Federated Learning Annotated Image Repository** (**FLAIR**) is an open source
    project that aims to provide a centralized repository of annotated images for
    FL. The repository contains a diverse set of images, including medical images,
    satellite images, and natural images, with corresponding annotations. FLAIR provides
    a platform for researchers and developers to evaluate FL algorithms on image-based
    tasks, such as image classification and object detection. The repository allows
    you to share and collaborate on annotated images, reducing the need for individual
    organizations to collect and label their own data. The annotations in FLAIR are
    designed to be privacy-preserving, with sensitive information removed or obfuscated.
    The privacy-preserving annotations and standardized format make FLAIR a valuable
    resource for evaluating FL algorithms on image-based tasks, enabling easy integration
    with popular machine learning frameworks and tools.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**联邦学习标注图像库**（**FLAIR**）是一个开源项目，旨在为联邦学习提供一个集中式的标注图像存储库。该存储库包含了一系列多样化的图像，包括医学图像、卫星图像和自然图像，以及相应的标注。FLAIR为研究人员和开发者提供了一个平台，用于评估基于图像的任务上的联邦学习算法，如图像分类和目标检测。该存储库允许您共享和协作标注图像，减少单个组织收集和标注自身数据的需要。FLAIR中的标注旨在保护隐私，敏感信息被移除或模糊化。隐私保护标注和标准化格式使FLAIR成为评估基于图像任务的联邦学习算法的有价值资源，使其能够轻松集成到流行的机器学习框架和工具中。'
- en: FLAIR has become a popular resource in the FL community, with several FL frameworks
    integrating it into their evaluation pipeline. The repository is continually updated,
    with new datasets and annotations added regularly. It consists of 430,000 images
    from 51,000 Flickr users, which are mapped to 17 coarse-grained labels such as
    art, food, plant, and outdoor.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: FLAIR已成为联邦学习社区中流行的资源，几个联邦学习框架已将其集成到它们的评估流程中。该存储库持续更新，定期添加新的数据集和标注。它由来自51,000名Flickr用户的430,000张图像组成，这些图像映射到17个粗粒度标签，如艺术、食物、植物和户外。
- en: 'Visit the following GitHub URL for more information: [https://github.com/apple/ml-flair](https://github.com/apple/ml-flair).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息，请访问以下 GitHub 网址：[https://github.com/apple/ml-flair](https://github.com/apple/ml-flair)。
- en: Federated EMNIST
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 联邦 EMNIST
- en: '**Federated Extended MNIST** (**Federated EMNIST**) is a benchmark dataset
    for FL developed by Google. It consists of a set of images of handwritten digits
    and letters, similar to the MNIST dataset. The dataset is distributed across multiple
    devices, and the goal is to train a model that can classify images correctly while
    keeping data private.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**联邦扩展MNIST**（**Federated EMNIST**）是谷歌开发的一个联邦学习基准数据集。它由一组手写数字和字母的图像组成，类似于MNIST数据集。该数据集分布在多个设备上，目标是训练一个模型，能够在保持数据隐私的同时正确分类图像。'
- en: Shakespeare
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 莎士比亚
- en: This dataset consists of a collection of Shakespearean texts, which can be used
    to train a language model for text generation.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含莎士比亚文本的集合，可用于训练用于文本生成的语言模型。
- en: CIFAR-10
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CIFAR-10
- en: This is a widely used benchmark dataset for computer vision tasks, consisting
    of a set of 60,000 color images of 10 different classes. It is used to train a
    model to classify images correctly while keeping data private.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个广泛使用的计算机视觉任务基准数据集，由10个不同类别的60,000张彩色图像组成。它用于训练一个模型，以在保持数据隐私的同时正确分类图像。
- en: OpenStreetMap
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenStreetMap
- en: This is a dataset of geographic data, including maps, satellite imagery, and
    GPS coordinates. The dataset can be distributed across multiple devices, and the
    goal is to train a model that can predict traffic patterns or other features of
    the environment while preserving the privacy of data.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个地理数据集，包括地图、卫星图像和GPS坐标。该数据集可以在多个设备上分布，目标是训练一个模型，能够在保护数据隐私的同时预测交通模式或其他环境特征。
- en: Medical image analysis datasets
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 医学图像分析数据集
- en: There are several medical imaging datasets that can be used for FL, such as
    the **Brain Tumor Segmentation** (**BraTS**) challenge dataset and the **Prostate
    MR Image Segmentation** (**PROMISE12**) challenge dataset. These datasets consist
    of medical images that can be used to train models for diagnosis and treatment
    planning while preserving the privacy of patient data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个医学图像数据集可以用于联邦学习，例如**脑肿瘤分割**（**BraTS**）挑战数据集和**前列腺磁共振图像分割**（**PROMISE12**）挑战数据集。这些数据集包含可用于训练诊断和治疗规划模型的医学图像，同时保护患者数据的隐私。
- en: Frameworks for FL benchmarks
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 联邦学习基准框架
- en: There are several FL benchmark frameworks available in the open source community,
    as well as commercially. FL frameworks provide libraries/platforms to benchmark
    FL systems and applications.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 开源社区和商业领域都有几个联邦学习基准框架可用。联邦学习框架提供库/平台，用于基准测试联邦学习系统和应用。
- en: 'Some of the most popular FL systems and benchmarks are the following:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一些最受欢迎的联邦学习系统和基准包括以下内容：
- en: LEAF
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LEAF
- en: FedML
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FedML
- en: FATE
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FATE
- en: FedScale
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FedScale
- en: PySyft
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySyft
- en: MLCommons – MLPerf
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLCommons – MLPerf
- en: MedPerf
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MedPerf
- en: TensorFlow Federated (TFF)
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow Federated (TFF)
- en: Flower (we covered this in the previous chapter so won’t go into detail again
    in this chapter)
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花朵（我们在上一章中已经讨论过，所以本章不再详细说明）
- en: We’ll look at these in more detail in the following subsections.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的小节中更详细地探讨这些内容。
- en: The LEAF FL benchmarks suite
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LEAF FL基准测试套件
- en: The LEAF FL benchmarks suite is a project initiated by Carnegie Mellon University,
    which aims to provide a standardized benchmarking suite for FL algorithms ([https://leaf.cmu.edu/](https://leaf.cmu.edu/)).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: LEAF FL基准测试套件是由卡内基梅隆大学发起的一个项目，旨在为联邦学习算法提供一个标准化的基准测试套件（[https://leaf.cmu.edu/](https://leaf.cmu.edu/))。
- en: The LEAF benchmarks enable researchers and developers to evaluate the performance
    of FL algorithms across different domains and configurations. LEAF provides a
    standardized set of tasks and datasets to evaluate the performance of FL algorithms
    on various metrics, including convergence speed, communication efficiency, and
    accuracy. The broad range of tasks includes image classification, text classification,
    and regression tasks.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: LEAF基准使研究人员和开发者能够评估不同领域和配置下联邦学习算法的性能。LEAF提供了一套标准化的任务和数据集，用于评估联邦学习算法在各种指标上的性能，包括收敛速度、通信效率和准确性。广泛的任务包括图像分类、文本分类和回归任务。
- en: The datasets used in the benchmarks are designed to be representative of real-world
    scenarios, with varying levels of data distribution, data size, and data complexity.
    The benchmarks are conducted under various settings, such as varying the number
    of clients, the communication rounds, and the amount of data available to each
    client. The benchmarks also evaluate the algorithms’ robustness to adversarial
    attacks and noisy data.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在基准测试中使用的数据库旨在代表现实世界场景，具有不同的数据分布、数据大小和数据复杂度水平。基准测试在各种设置下进行，例如改变客户端数量、通信轮次以及每个客户端可用的数据量。基准测试还评估算法对对抗攻击和噪声数据的鲁棒性。
- en: 'LEAF’s core components are as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: LEAF 的核心组件如下：
- en: Datasets
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集
- en: Reference implementation
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考实现
- en: Metrics
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指标
- en: '![Figure 7.1 – LEAF’s core components and flow](img/B16573_07_01.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.1 – LEAF 的核心组件和流程](img/B16573_07_01.jpg)'
- en: Figure 7.1 – LEAF’s core components and flow
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 – LEAF 的核心组件和流程
- en: More details about the benchmarks of LEAF can be obtained at [https://arxiv.org/abs/1812.01097](https://arxiv.org/abs/1812.01097).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于 LEAF 基准测试的详细信息可以在 [https://arxiv.org/abs/1812.01097](https://arxiv.org/abs/1812.01097)
    获取。
- en: FedML
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: FedML
- en: FedML ([https://github.com/FedML-AI/FedML](https://github.com/FedML-AI/FedML))
    is a benchmark suite for FL that includes several datasets and tasks, such as
    image classification, language modeling, and speech recognition. FedML evaluates
    FL algorithms on their robustness in **non-independent and identically distributed**
    (**non-iid**) data distributions, scalability to large datasets, and communication
    efficiency.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: FedML ([https://github.com/FedML-AI/FedML](https://github.com/FedML-AI/FedML))
    是一个用于联邦学习的基准套件，包括多个数据集和任务，如图像分类、语言建模和语音识别。FedML 在 **非独立同分布**（**non-iid**）的数据分布、对大数据集的可扩展性和通信效率方面评估联邦学习算法的鲁棒性。
- en: The framework supports various types of FL setups, such as horizontal, vertical,
    and federated transfer learning, as well as different types of optimization algorithms,
    including federated averaging, FedProx, and FedAdapt. The FedML framework is designed
    to be modular, flexible, and scalable, allowing users to customize and extend
    the existing algorithms or develop new ones easily. It also provides a set of
    benchmark datasets and evaluation metrics to facilitate the comparison of different
    algorithms.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 该框架支持各种类型的联邦学习设置，如水平、垂直和联邦迁移学习，以及不同类型的优化算法，包括联邦平均、FedProx 和 FedAdapt。FedML 框架旨在模块化、灵活和可扩展，使用户能够轻松定制和扩展现有算法或开发新算法。它还提供了一套基准数据集和评估指标，以促进不同算法的比较。
- en: The FedML community actively develops and maintains the framework and organizes
    workshops and challenges to promote research and innovation in FL. The community
    also collaborates with industry partners to apply FL to real-world use cases,
    such as healthcare, finance, and telecommunications. FedML is an important initiative
    that addresses the challenges and opportunities of FL and enables the development
    of secure, privacy-preserving, and efficient machine learning models in a distributed
    setting. FedML supports three types of computing platforms – IoT/mobile, distributed
    computing, and standalone simulation.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: FedML 社区积极开发和维护该框架，并组织研讨会和挑战赛，以促进联邦学习的研究和创新。该社区还与行业合作伙伴合作，将联邦学习应用于现实世界的用例，如医疗保健、金融和电信。FedML
    是一个重要的倡议，旨在解决联邦学习的挑战和机遇，并使分布式环境中开发安全、隐私保护和高效的机器学习模型成为可能。FedML 支持三种类型的计算平台 – 物联网/移动、分布式计算和独立模拟。
- en: 'More details can be found at the following reference research URL: [https://arxiv.org/abs/2007.13518](https://arxiv.org/abs/2007.13518).'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 更多详细信息可以在以下参考研究 URL 中找到：[https://arxiv.org/abs/2007.13518](https://arxiv.org/abs/2007.13518)。
- en: '![Figure 7.2 – FedML architecture](img/B16573_07_02.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.2 – FedML 架构](img/B16573_07_02.jpg)'
- en: Figure 7.2 – FedML architecture
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – FedML 架构
- en: FATE
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: FATE
- en: '**Federated AI Technology Enabler** (**FATE**) is an open source project that
    provides a secure and privacy-preserving way to collaborate on **artificial intelligence**
    (**AI**) models. FATE is designed to enable FL, which is a distributed machine
    learning approach that allows multiple parties to train a shared model without
    sharing their data. FATE is developed by the WeBank AI department, a subsidiary
    of Tencent. [https://github.com/FederatedAI/FATE](https://github.com/FederatedAI/FATE)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**联邦人工智能技术赋能器**（**FATE**）是一个开源项目，它提供了一种安全且保护隐私的方式来协作开发**人工智能**（**AI**）模型。FATE旨在实现联邦学习（FL），这是一种分布式机器学习方法，允许多个参与者训练一个共享模型，而无需共享他们的数据。FATE由腾讯的子公司微众银行AI部门开发。[https://github.com/FederatedAI/FATE](https://github.com/FederatedAI/FATE)'
- en: FATE provides a platform for developers to implement FL techniques in their
    AI applications. It supports different types of FL, such as horizontal and vertical,
    and provides tools to manage the communication, synchronization, and aggregation
    of models across different devices. FATE uses a variety of cryptographic techniques
    to ensure the privacy and security of the FL process. For example, FATE employs
    differential privacy to add noise to training data, preventing the leakage of
    sensitive information about individual users. FATE also uses homomorphic encryption
    to enable computation on encrypted data, which allows parties to collaborate without
    revealing their data to each other. We will learn about homomorphic encryption
    in the next chapter.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: FATE为开发者提供了一个平台，使他们能够在自己的AI应用中实现联邦学习技术。它支持不同类型的联邦学习，如横向和纵向，并提供工具来管理不同设备间模型的通信、同步和聚合。FATE采用多种加密技术来确保联邦学习过程的隐私和安全。例如，FATE使用差分隐私向训练数据添加噪声，防止泄露关于单个用户的敏感信息。FATE还使用同态加密来允许对加密数据进行计算，这使得各方可以在不向彼此透露数据的情况下进行协作。我们将在下一章中学习同态加密。
- en: FATE is built on top of Kubernetes, a popular open source container orchestration
    system. Kubernetes provides FATE with the ability to manage containers and automate
    the deployment, scaling, and monitoring of the FL infrastructure. FATE leverages
    cryptographic techniques and Kubernetes to ensure the privacy and security of
    the FL process. FATE provides a web-based **graphical user interface** (**GUI**)
    for developers to interact with the platform, monitor the training process, and
    visualize the results. It has been adopted by various companies and organizations
    to build privacy-preserving AI applications. For example, Tencent has used FATE
    to develop a privacy-preserving recommendation system for its e-commerce platform.
    Huawei has also used FATE to build an FL platform for the healthcare industry.
    FATE’s popularity is expected to grow as FL becomes increasingly important in
    the AI industry.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: FATE建立在Kubernetes之上，这是一个流行的开源容器编排系统。Kubernetes为FATE提供了管理容器和自动化联邦学习基础设施的部署、扩展和监控的能力。FATE利用加密技术和Kubernetes来确保联邦学习过程的隐私和安全。FATE为开发者提供了一个基于Web的**图形用户界面**（**GUI**），以便与平台交互、监控训练过程和可视化结果。它已被各种公司和组织采用，以构建保护隐私的AI应用。例如，腾讯已使用FATE为其电商平台开发了一个保护隐私的推荐系统。华为也使用FATE为医疗保健行业构建了一个联邦学习平台。随着联邦学习在AI行业中的重要性日益增加，FATE的普及度预计将增长。
- en: The architecture of FATE
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: FATE的架构
- en: 'The following figure shows the FATE architecture:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图显示了FATE架构：
- en: '![Figure 7.3 – FATE architecture](img/B16573_07_03.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图7.3 – FATE架构](img/B16573_07_03.jpg)'
- en: Figure 7.3 – FATE architecture
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 – FATE架构
- en: The preceding figure was sourced from [https://fate.readthedocs.io/en/latest/architecture/](https://fate.readthedocs.io/en/latest/architecture/).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图来源于[https://fate.readthedocs.io/en/latest/architecture/](https://fate.readthedocs.io/en/latest/architecture/)。
- en: The FedScale benchmarking platform
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: FedScale基准测试平台
- en: The FedScale benchmark suite includes several datasets and workloads that are
    designed to evaluate the performance of FL algorithms and systems. The datasets
    are selected to reflect the heterogeneity and non-IIDness of real-world data.
    The workloads are designed to simulate the communication overhead and computation
    complexity of real-world FL scenarios, as well as evaluate the performance of
    FL algorithms and systems in these scenarios.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: FedScale基准测试套件包括几个数据集和工作负载，旨在评估联邦学习算法和系统的性能。这些数据集被选中以反映现实世界数据的异构性和非独立同分布（non-IIDness）。工作负载被设计来模拟现实世界联邦学习场景的通信开销和计算复杂性，以及评估这些场景中联邦学习算法和系统的性能。
- en: One of the datasets included in the FedScale benchmark suite is the Synthetic
    IID dataset, which consists of synthetic data that is randomly generated and distributed
    evenly among the clients. This dataset is designed to evaluate the scalability
    and efficiency of FL algorithms and systems. Another dataset included in the FedScale
    benchmark suite is the Heterogeneous Image dataset, which consists of images from
    different sources and domains. This dataset is designed to evaluate the robustness
    of FL algorithms and systems to heterogeneous data.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 包含在 FedScale 基准套件中的数据集之一是 Synthetic IID 数据集，它由随机生成并均匀分布在客户端之间的合成数据组成。该数据集旨在评估联邦学习算法和系统的可扩展性和效率。FedScale
    基准套件中包含的另一个数据集是异构图像数据集，它由来自不同来源和领域的图像组成。该数据集旨在评估联邦学习算法和系统对异构数据的鲁棒性。
- en: The FedScale benchmark suite also includes more complex workloads, such as the
    Federated Meta-Learning workload, which involves clients learning from each other’s
    data.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: FedScale 基准套件还包括更复杂的工作负载，例如联邦元学习工作负载，涉及客户端从彼此的数据中学习。
- en: '![Figure 7.4 – The FedScale runtime to run FL benchmarks](img/B16573_07_04.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.4 – 运行 FL 基准的 FedScale 运行时](img/B16573_07_04.jpg)'
- en: Figure 7.4 – The FedScale runtime to run FL benchmarks
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 – 运行 FL 基准的 FedScale 运行时
- en: The preceding figure was sourced from [https://github.com/SymbioticLab/FedScale/blob/master/docs/fedscale-sim-mode.png](https://github.com/SymbioticLab/FedScale/blob/master/docs/fedscale-sim-mode.png).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图来自 [https://github.com/SymbioticLab/FedScale/blob/master/docs/fedscale-sim-mode.png](https://github.com/SymbioticLab/FedScale/blob/master/docs/fedscale-sim-mode.png)。
- en: MLCommons
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MLCommons
- en: MLCommons is a non-profit organization that aims to accelerate machine learning
    innovation and development by providing a platform for collaboration and the sharing
    of resources among its members. It was founded in June 2020 by a group of leading
    researchers, engineers, and entrepreneurs in the field of machine learning.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: MLCommons 是一个非营利组织，旨在通过为其成员提供一个协作和资源共享的平台来加速机器学习创新和发展。它由机器学习领域的几位领先研究人员、工程师和企业家于
    2020 年 6 月成立。
- en: The organization’s primary focus is on developing and promoting best practices
    for designing and deploying machine learning systems, including hardware and software
    frameworks, datasets, benchmarks, and evaluation metrics. One of the key initiatives
    of MLCommons is the MLPerf benchmark suite, which is designed to measure the performance
    of machine learning systems across a range of applications, including computer
    vision, natural language processing, and recommendation systems. The benchmarks
    are developed and maintained by a global community of researchers and engineers,
    and they are used to evaluate the performance of hardware and software platforms
    for machine learning. MLPerf has quickly become the industry standard for benchmarking
    machine learning performance and is used by leading technology companies, researchers,
    and government agencies around the world.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 该组织的主要关注点是开发和推广设计及部署机器学习系统的最佳实践，包括硬件和软件框架、数据集、基准和评估指标。MLCommons 的一个关键倡议是 MLPerf
    基准套件，该套件旨在衡量机器学习系统在一系列应用中的性能，包括计算机视觉、自然语言处理和推荐系统。这些基准由全球的研究人员和工程师社区开发和维护，并用于评估机器学习硬件和软件平台的性能。MLPerf
    已迅速成为衡量机器学习性能的行业标准，并被全球领先的技术公司、研究人员和政府机构所采用。
- en: MedPerf
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MedPerf
- en: MedPerf is an open benchmarking platform for medical AI using FL. The MLCommons
    team piloted multiple use cases in collaboration with multiple institutions and
    universities to run FL models using MDPerf benchmarks. The use cases are brain
    tumor segmentation, pancreas segmentation, surgical workflow phase recognition,
    and cloud experiments.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: MedPerf 是一个用于医疗人工智能的开放基准平台，使用联邦学习。MLCommons 团队与多个机构和大学合作，试点了多个用例，使用 MDPerf 基准运行联邦学习模型。这些用例包括脑肿瘤分割、胰腺分割、手术流程阶段识别和云实验。
- en: '![](img/B16573_07_05.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B16573_07_05.jpg)'
- en: Figure 7.5 – MedPerf architecture for cloud experiments
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5 – 云实验的 MedPerf 架构
- en: The preceding figure was sourced from [https://github.com/mlcommons/medperf](https://github.com/mlcommons/medperf).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图来自 [https://github.com/mlcommons/medperf](https://github.com/mlcommons/medperf)。
- en: Selecting an FL framework for a project
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择一个联邦学习框架
- en: Selecting the best FL framework can be a challenging task, since there are several
    factors to consider, such as the features, scalability, security, ease of use,
    documentation, the machine learning algorithms to support (out of the box), neural
    network support, and the learning curve.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 选择最佳的联邦学习（FL）框架可能是一项具有挑战性的任务，因为需要考虑多个因素，例如功能、可扩展性、安全性、易用性、文档、支持的学习机器算法（开箱即用）、神经网络支持以及学习曲线。
- en: 'The following are some of the key considerations when selecting an FL framework:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 选择FL框架时的一些关键考虑因素如下：
- en: '**Features**: It is important to consider the features that the framework offers.
    Some of the critical features to look for include support for different machine
    learning algorithms, distributed training, data privacy, and data validation.
    You should choose a framework that has features that match your use case needs.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**功能**：考虑框架提供的功能很重要。一些需要寻找的关键功能包括支持不同的机器学习算法、分布式训练、数据隐私和数据验证。你应该选择一个具有与你的用例需求相匹配的功能的框架。'
- en: '**Scalability**: Another critical consideration is scalability. The framework
    should be able to handle large datasets and a high number of participants. You
    should also consider whether the framework can scale horizontally or vertically.
    In the case of cross-device (mobile, web, etc.), the framework should be able
    to handle a large number of clients.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：另一个关键考虑因素是可扩展性。框架应该能够处理大量数据集和大量参与者。你还应该考虑框架是否能够进行横向或纵向扩展。在跨设备（移动、网页等）的情况下，框架应该能够处理大量客户端。'
- en: '**Security and privacy**: FL involves multiple parties sharing data, so it’s
    essential to select a framework that offers robust security features. The framework
    should have encryption and authentication capabilities to protect the data from
    unauthorized access. You should also consider whether the framework has a transparent
    security model, allowing you to audit the security measures in place. You need
    to consider whether or not the framework supports privacy protection by default,
    such as differential privacy.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全和隐私**：联邦学习涉及多个方共享数据，因此选择一个提供强大安全功能的框架至关重要。框架应该具备加密和认证功能，以保护数据免受未经授权的访问。你还应该考虑框架是否具有透明的安全模型，允许你审计实施的安全措施。你需要考虑框架是否默认支持隐私保护，例如差分隐私。'
- en: '**Ease of use**: The ease of use of the framework is also a critical consideration.
    The framework should have an intuitive user interface that makes it easy to set
    up, configure, and use. You should also consider whether the framework has good
    documentation and community support, as this can help you troubleshoot any issues
    that arise.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**易用性**：框架的易用性也是一个关键考虑因素。框架应该拥有直观的用户界面，使其易于设置、配置和使用。你还应该考虑框架是否具有良好的文档和社区支持，因为这可以帮助你解决出现的任何问题。'
- en: '**Learning curve**: You should consider the learning curve of the framework
    as well in terms of user documentation, deployment setup, programming language
    support, the examples and samples provided, and so on.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习曲线**：你应该考虑框架的学习曲线，包括用户文档、部署设置、编程语言支持、提供的示例和样本等。'
- en: '**Compatibility**: Finally, you should consider whether the framework is compatible
    with your existing technology stack and programming language. For example, if
    you’re using TensorFlow for machine learning, you may want to select an FL framework
    that is compatible with TensorFlow.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**兼容性**：最后，你应该考虑框架是否与你的现有技术堆栈和编程语言兼容。例如，如果你正在使用TensorFlow进行机器学习，你可能希望选择一个与TensorFlow兼容的FL框架。'
- en: In conclusion, selecting the best framework requires careful consideration of
    several factors, including features, scalability, security, privacy, ease of use,
    the learning curve, and compatibility with your existing technology stack. By
    considering these factors, you can select a framework that meets your requirements
    to solve the business use case using FL.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，选择最佳框架需要仔细考虑多个因素，包括功能、可扩展性、安全性、隐私、易用性、学习曲线以及与现有技术堆栈的兼容性。通过考虑这些因素，你可以选择一个满足你要求的框架，以使用联邦学习解决业务用例。
- en: A comparison of FedScale, FATE, Flower, and TensorFlow Federated
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FedScale、FATE、Flower和TensorFlow Federated的比较
- en: FedScale, FATE, Flower, FedML, and **TensorFlow Federated** (**TFF**) are some
    of the popular FL frameworks that offer unique features and capabilities. In the
    following subsections, we will compare these features and capabilities.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: FedScale、FATE、Flower、FedML 和 **TensorFlow Federated**（**TFF**）是一些流行的联邦学习框架，它们提供独特的功能和能力。在以下小节中，我们将比较这些功能和能力。
- en: Features
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 功能
- en: FedScale is a framework that supports FL for a wide range of machine learning
    models, including deep neural networks, decision trees, and **Support Vector Machines**
    (**SVMs**). It also supports various optimization algorithms such as federated
    **stochastic gradient descent** (**SGD**) and federated proximal algorithms. FATE,
    conversely, offers a comprehensive suite of FL tools and supports various machine
    learning algorithms, including logistic regression, decision trees, and deep learning
    models. Flower is a lightweight framework that supports FL for various machine
    learning models, including neural networks, decision trees, and logistic regression.
    TFF is a widely used FL framework that supports TensorFlow-based machine learning
    models.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: FedScale 是一个支持广泛机器学习模型的框架，包括深度神经网络、决策树和**支持向量机**（**SVMs**）。它还支持各种优化算法，如联邦**随机梯度下降**（**SGD**）和联邦近端算法。相反，FATE
    提供了一套全面的联邦学习工具，支持各种机器学习算法，包括逻辑回归、决策树和深度学习模型。Flower 是一个轻量级的框架，支持各种机器学习模型，包括神经网络、决策树和逻辑回归。TFF
    是一个广泛使用的联邦学习框架，支持基于 TensorFlow 的机器学习模型。
- en: Scalability
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可扩展性
- en: FedScale and FATE are designed for large-scale deployments and can handle a
    high number of participants. Flower is a lightweight framework that is ideal for
    small to large-scale FL projects. TFF is a scalable framework that can handle
    large-scale FL projects and can be used to deploy FL algorithms in production
    environments.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: FedScale 和 FATE 是为大规模部署设计的，可以处理大量的参与者。Flower 是一个轻量级的框架，非常适合小型到大型规模的联邦学习项目。TFF
    是一个可扩展的框架，可以处理大型规模的联邦学习项目，并且可以用于在生产环境中部署联邦学习算法。
- en: Security
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安全性
- en: All four frameworks offer robust security features such as encryption and authentication
    to protect data during the FL process. FATE has additional features such as differential
    privacy and homomorphic encryption to enhance data privacy. TFF also offers secure
    **multi-party computation** (**MPC**) protocols that enable multiple parties to
    securely collaborate on FL projects.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 所有四个框架都提供强大的安全功能，如加密和身份验证，以保护在联邦学习过程中的数据。FATE 有额外的功能，如差分隐私和同态加密，以增强数据隐私。TFF
    还提供了安全的**多方计算**（**MPC**）协议，使多个方能够在联邦学习项目中安全地协作。
- en: Ease of use
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 易用性
- en: FedScale and FATE have a steeper learning curve than Flower and TFF, as they
    offer more complex features and tools. Flower and TFF are both easy to use, with
    intuitive APIs and documentation that make it easy to get started. TFF also offers
    a high-level API that enables users to build FL algorithms without requiring deep
    knowledge of the underlying details.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Flower 和 TFF 相比，FedScale 和 FATE 的学习曲线更陡峭，因为它们提供了更复杂的功能和工具。Flower 和 TFF 都易于使用，具有直观的
    API 和文档，使得入门变得容易。TFF 还提供了一个高级 API，使用户能够在不要求深入了解底层细节的情况下构建联邦学习算法。
- en: Community support
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 社区支持
- en: All four frameworks have active open source contributors that provide support
    and documentation. TFF, being developed by Google, has the largest community and
    support, followed by Flower, FedScale, and FATE.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 所有四个框架都有活跃的开源贡献者，提供支持和文档。TFF 由 Google 开发，拥有最大的社区和支持，其次是 Flower、FedScale 和 FATE。
- en: Each of these frameworks has its unique features and capabilities, making them
    suitable for different use cases. FedScale and FATE are ideal for large-scale
    FL deployments, while Flower and TFF are more suited for small to medium-scale
    projects. TFF has the largest community and support, making it an excellent choice
    for developers looking for an FL framework with extensive support and resources.
    Ultimately, the choice of framework will depend on the specific use case, requirements,
    and familiarity with the framework.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这些框架中的每一个都有其独特的功能和能力，使它们适用于不同的用例。FedScale 和 FATE 适用于大规模的联邦学习部署，而 Flower 和 TFF
    更适合小型到中型项目。TFF 拥有最大的社区和支持，是寻找具有广泛支持和资源的联邦学习框架的开发者的绝佳选择。最终，框架的选择将取决于具体的用例、需求和框架的熟悉程度。
- en: Research papers
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 研究论文
- en: Each framework published its own benchmark and results in the research journals,
    with a list of supported features and accuracy metrics. Currently, there are no
    industry standard benchmarks similar to [TPC.Org](http://TPC.Org) or [SPEC.org](http://SPEC.org)
    that are widely accepted.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: State-of-the-art research in FL
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: FL is a rapidly growing technology, and there are several state-of-the-art research
    directions in this space. The following are some of the recent advances in FL
    research.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a high-level comparison of the different FL frameworks:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '| **High-level capabilities** | **FATE** | **FedML** | **Flower** | **FedScale**
    | **TFF** |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
- en: '| Regression models | Y | Y | Y | Y | Y |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
- en: '| Neural networks | Y | Y | Y | Y | Y |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
- en: '| Tree-based models | Y | N | N | N | N |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
- en: '| Communication protocol | Customized | MPI | gRPC | gRPC | gRPC |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
- en: '| Support for differential privacy | N | N | Y | Y | Y |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
- en: '| Single host deployment | Y | Y | Y | Y | Y |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
- en: '| Cross-device deployment | N | Y | Y | Y | N |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
- en: '| Research papers | Y | Y | Y | Y | Y |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
- en: Table 7.1 – Comparison of the different FL frameworks
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Communication-efficient FL
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the main challenges in FL is the high communication cost of transmitting
    the model updates between the devices and the central server. Recent research
    has focused on reducing the communication cost in FL by developing new compression
    techniques, such as quantization, sparsification, and differential compression.
    These techniques can significantly reduce the communication cost without sacrificing
    the model’s accuracy. Several researchers and research groups are actively working
    on **communication-efficient FL** (**CE-FL**) research:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '**Google Research**: Google Research has been actively working on CE-FL research
    and has developed several techniques to reduce communication costs in FL. One
    of its recent works is *Communication-Efficient Learning of Deep Networks from
    Decentralized Data*, which proposes a new compression technique called **Quantized
    SGD** (**QSGD**) that can significantly reduce the communication cost in FL. This
    is a family of compression schemes that allow the compression of gradient updates
    at each node, while guaranteeing convergence under standard assumptions. See the
    Arxiv paper at [https://arxiv.org/abs/1602.05629](https://arxiv.org/abs/1602.05629).'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The Federated Learning Community**: The Federated Learning Community is an
    open community of researchers and practitioners who are working on FL research.
    The community has several working groups, including a CE-FL working group, which
    is focused on developing new techniques to reduce communication costs in FL.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Carnegie Mellon University**: The Machine Learning department at Carnegie
    Mellon University has several researchers who are working on CE-FL research. One
    of its recent works is *Communication-Efficient Distributed Learning with Feature-Selective
    Sampling*, which proposes a new sampling technique that can reduce the communication
    cost in FL.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IBM Research**: IBM researchers and the University of Michigan are working
    on CE-FL research, and their most recent work is *Federated Learning with Matched
    Averaging*, which proposes a new aggregation technique that can reduce the communication
    cost in FL. See the Arxiv paper at [https://arxiv.org/abs/2002.06440](https://arxiv.org/abs/2002.06440).'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are just a few examples of the many researchers and research groups who
    are actively working on CE-FL research. As the field of FL continues to evolve,
    it is expected that more researchers and research groups will focus on developing
    new techniques to reduce the communication cost in FL.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Privacy-preserving FL
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Privacy is a critical concern in FL, since data is distributed across multiple
    devices, and each device’s owner wants to protect their data privacy. Recent research
    has focused on developing new privacy-preserving techniques, such as differential
    privacy and secure multi-party computation, to protect the data privacy of the
    devices’ owners while enabling collaborative model training. These techniques
    can provide strong privacy guarantees while preserving the model’s accuracy. Several
    researchers and research groups are actively working on **privacy-preserving FL**
    (**PP-FL**) research.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some notable examples:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '**OpenMined**: OpenMined is an open source community dedicated to advancing
    PP-FL research. It has developed several PP-FL frameworks, including PySyft, which
    is a Python library for PP-FL.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Google Research**: Google Research has been actively working on PP-FL research
    and has developed several techniques to protect the privacy of user data in FL.
    One of its recent works is *Privacy-Preserving Federated Learning with Byzantine
    Robust Aggregation*, which proposes a new aggregation technique that can protect
    user privacy in the presence of malicious actors.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Carnegie Mellon University**: The CyLab Security and Privacy Institute at
    Carnegie Mellon University has several researchers who are working on PP-FL research.
    One of its recent works is *Privacy-Preserving Federated Learning via Randomized
    Smoothing*, which proposes a new technique that can protect user privacy by adding
    noise to the user data.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The University of Oxford**: The University of Oxford has several researchers
    who are working on PP-FL research. One of their recent works is *Secure Federated
    Learning on Curves*, which proposes a new technique that can protect user privacy
    by using elliptic curves to encrypt user data.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Microsoft Research**: Microsoft Research has been actively working on PP-FL
    research and has developed several techniques to protect user privacy in FL. One
    of its recent works is *Private Federated Learning with Secure Aggregation*, which
    proposes a new aggregation technique that can protect user privacy by using homomorphic
    encryption.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are some examples of the many researchers and research groups who are
    actively working on PP-FL research. As the field of FL continues to evolve, it
    is expected that more researchers and research groups will focus on developing
    new techniques to protect the privacy of user data in FL.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Federated Meta-Learning
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Federated Meta-Learning** (**FML**) is a new research direction that combines
    FL and meta-learning to learn from multiple data sources and tasks while preserving
    privacy. FML enables collaborative model training across multiple devices and
    organizations while preserving the data privacy of the devices’ owners. Recent
    research has explored new FML techniques and applications, such as personalized
    medicine and personalized recommendations. FML is a relatively new research direction
    in FL, and only a few research groups are working actively on it.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '**Google Research**: Google Research has been actively working on FML research
    and has developed several techniques to learn from multiple clients’ meta-knowledge
    while preserving their privacy. One of its research works is *Federated Meta-Learning
    for Recommendation*, which proposes a new framework for FML for recommendation
    tasks.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Carnegie Mellon University**: The Machine Learning department at Carnegie
    Mellon University has several researchers who are working on FML research. One
    of its recent works is *Federated Meta-Learning for Fast Model Adaptation in Healthcare*,
    which proposes a new approach for learning from multiple healthcare institutions
    while preserving their privacy.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The University of Oxford**: The University of Oxford has several researchers
    who are working on FML research. One of its recent works is *Federated Meta-Learning
    for Recommendation with Private and Communication-Efficient Model Aggregation*,
    which proposes a new approach for FML for recommendation tasks that preserves
    privacy and reduces communication costs.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The University of California, Berkeley**: The RISELab at the University of
    California, Berkeley, has several researchers who are working on FML research.
    One of its recent works is *Federated Meta-Learning for Few-Shot Learning Across
    Heterogeneous Devices*, which proposes a new approach for FML that can learn from
    data across heterogeneous devices.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are a few examples of the research groups that are actively working on
    FML research. As the field of FML continues to grow, it is expected that more
    researchers and research groups will focus on developing new techniques for FML
    in various applications.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Adaptive FL
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In FL, devices’ data distribution can change over time, leading to a concept
    drift problem that can degrade a model’s performance. Recent research has focused
    on developing new adaptive learning techniques that can efficiently update the
    model while preserving privacy. These techniques can adapt the model to the changing
    data distribution and improve the model’s performance over time. **Adaptive federated
    learning** (**AFL**) is an emerging research direction in FL that focuses on dynamically
    adapting the FL system to changing conditions.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some notable examples of researchers and research groups who are working
    actively on AFL:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '**Carnegie Mellon University**: The Machine Learning department at Carnegie
    Mellon University has several researchers who are working on AFL research. One
    of its recent works is *Adaptive Federated Optimization*, which proposes a new
    framework for AFL that adapts the learning rate and aggregation strategy to network
    conditions.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Google Research**: Google Research has been actively working on AFL research
    and has developed several techniques to adapt the FL system to changing conditions.
    One of its recent works is *Adaptive Federated Optimization with Local Descent*,
    which proposes a new approach for AFL that adapts the learning rate and aggregation
    strategy to the client’s local data distribution.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The University of Southern California**: The Department of Computer Science
    at the University of Southern California has several researchers who are working
    on AFL research. One of its recent works is *Decentralized Adaptive Federated
    Learning with Gradient Compression*, which proposes a new approach for AFL that
    adapts the learning rate and aggregation strategy to network conditions while
    compressing the gradients to reduce communication costs.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tsinghua University**: The Department of Computer Science and Technology
    at Tsinghua University has several researchers who are working on AFL research.
    One of its recent works is *Adaptive Federated Learning via Second-Order Information
    Exchange*, which proposes a new approach for AFL that adapts the learning rate
    and aggregation strategy based on the second-order information exchange among
    clients.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are a few examples of the research groups that are actively working on
    AFL research. As the field of AFL continues to grow, it is expected that more
    researchers and research groups will focus on developing new techniques to adapt
    the FL system to changing conditions.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Federated reinforcement learning
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Reinforcement learning** (**RL**) is a machine learning paradigm that enables
    agents to learn through trial and error. **Federated reinforcement learning**
    (**FRL**) is a new research direction that combines FL and RL to enable privacy-preserving
    collaborative learning for decision-making tasks. Recent research has explored
    new FRL algorithms and applications, such as autonomous driving and robotics.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: There are many researchers and organizations actively working on FRL to improve
    machine learning techniques that protect user privacy and data security.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few examples:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Google’s **Federated Learning of Cohorts** (**FLoC**) team is working on developing
    FRL to create privacy-preserving user models for ad targeting.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Researchers at Stanford University have been exploring the use of FRL to train
    autonomous robots. A team of researchers from Carnegie Mellon University is investigating
    FRL as a way to improve the efficiency and privacy of healthcare machine learning
    models.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The OpenMined project is a community-driven initiative that aims to create an
    ecosystem for privacy-preserving machine learning, including FRL.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microsoft Research is working on developing FRL methods for distributed robotics
    applications.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are just a few examples of the many researchers and organizations working
    on FRL, which is a rapidly growing field with many exciting developments and applications.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '**Federated Tumour Segmentation (FeTs**) is a real-world medical FL platform
    developed by Intel and the **University of Pennsylvania** (**UPenn**). They make
    use of OpenFL as a backend for the platform.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Key company products related to FL
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we have seen FL is a rapidly growing field that has gained significant attention
    from start-up companies. Here is a summary of some of the companies that are working
    or providing FL products:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '**DynamoFL**: DynamoFL is built by privacy and machine learning experts from
    **MIT** (**Massachusetts Institute of Technology**) and Harvard, who built leading
    FL solutions at Google AI and privacy-enhanced technologies at Microsoft. As per
    its website, the current **large language models** (**LLMs**) are not private,
    but the LLMs from DynamoFL are. They provide personalized FL, which is another
    key area of research. LLMs are covered in [*Chapter 10*](B16573_10.xhtml#_idTextAnchor219)
    in the *Privacy-preserved generative* *AI* section.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NVIDIA FLARE**: NVIDIA **Federated Learning Application Runtime Environment**
    (**FLARE**) is an SDK for FL that is open sourced by NVIDIA. FLARE supports various
    FL algorithms (FedAvg, FedProx, FedOpt, etc.), neural networks, and ML algorithms.
    It supports differential privacy and homomorphic encryption features as part of
    the security and privacy preservation stack. The SDK also provides a simulator
    that can be used to start servers and clients and execute FL notebooks and FL
    applications.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OpenMined**: OpenMined is an open source community that provides a platform
    for privacy-preserving ML, including FL tools and libraries for secure aggregation
    and differential privacy.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DataFleets**: DataFleets provides a platform for privacy-preserving data
    access and analytics, including FL tools for secure data processing and analysis.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scaleout**: Scaleout provides a platform for distributed ML, including FL
    tools for privacy-preserving and collaborative model training.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Edge Impulse**: Edge Impulse provides a platform for developing and deploying
    ML models on edge devices, including FL tools for privacy-preserving model training
    and inference.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Edge Impulse**: Edge Impulse 提供了一个平台，用于在边缘设备上开发和部署机器学习模型，包括用于隐私保护模型训练和推理的联邦学习工具。'
- en: '**Decentralized Machine Learning**: Decentralized Machine Learning provides
    a platform for privacy-preserving and decentralized ML, including FL tools for
    secure aggregation and differential privacy.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**去中心化机器学习**: 去中心化机器学习提供了一个平台，用于隐私保护和去中心化机器学习，包括用于安全聚合和差分隐私的联邦学习工具。'
- en: '**PySyft**: PySyft is an open source library for FL and secure multi-party
    computation, enabling privacy-preserving and collaborative ML on distributed data.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PySyft**: PySyft 是一个用于联邦学习和多方安全计算的开放源代码库，它使分布式数据上的隐私保护和协作机器学习成为可能。'
- en: '**Intellegens**: Intellegens’ product originated from the University of Cambridge
    and Ichnite and is an FL platform product that can be deployed in the cloud or
    on-premises ([https://intellegens.com/](https://intellegens.com/)).'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Intellegens**: Intellegens 的产品起源于剑桥大学和 Ichnite，是一个可以部署在云端或本地环境的联邦学习平台产品（[https://intellegens.com/](https://intellegens.com/))。'
- en: These companies are actively contributing to the development and advancement
    of FL, providing tools and platforms for privacy-preserving and collaborative
    ML on distributed data. As FL continues to evolve, it is expected that more start-up
    companies will emerge, contributing to the growth and innovation of this field.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这些公司积极贡献于联邦学习的发展与进步，为分布式数据上的隐私保护和协作机器学习提供工具和平台。随着联邦学习的持续发展，预计将有更多初创公司涌现，为这一领域的发展和创新做出贡献。
- en: Summary
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored FL benchmarks, their importance, and how to design
    benchmarks, among other things. We also discussed various FL frameworks and looked
    at what to consider when choosing a framework to implement FL applications. Finally,
    we covered the state-of-the-art research undertaken by various enterprises in
    collaboration with key universities, highlighting some of the key companies that
    are actively working and offering products/platforms to support FL.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了联邦学习基准、它们的重要性以及如何设计基准，以及其他内容。我们还讨论了各种联邦学习框架，并探讨了在选择框架以实现联邦学习应用时需要考虑的因素。最后，我们介绍了各企业与关键大学合作进行的尖端研究，突出了一些积极工作并提供产品/平台以支持联邦学习的关键公司。
- en: In the next chapter, we will learn about homomorphic encryption and secure multi-party
    computation and how they help in achieving privacy in ML models.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将了解同态加密和多方安全计算以及它们如何帮助在机器学习模型中实现隐私。
