<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;10.&#xA0;Probabilistic Graphical Models"><div class="book" id="28FAO2-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch10" class="calibre1"/>Chapter 10. Probabilistic Graphical Models</h1></div></div></div><p class="calibre8">Probabilistic graphical models, or simply graphical models as we will refer to them in this chapter, are models that use the representation of a graph to describe the conditional independence relationships between a series of random variables. This topic has received an increasing amount of attention in recent years and probabilistic graphical models have been successfully applied to tasks ranging from medical diagnosis to image segmentation. In this <a id="id717" class="calibre1"/>chapter, we'll present some of the necessary background that will pave the way to understanding the most basic graphical model, the Naïve Bayes classifier. We will then look at a slightly more complicated graphical model, known as the <span class="strong"><strong class="calibre2">Hidden Markov Model</strong></span> (<span class="strong"><strong class="calibre2">HMM</strong></span>). To get started in this field, we must first learn about graphs and why they are useful.</p></div>

<div class="book" title="Chapter&#xA0;10.&#xA0;Probabilistic Graphical Models">
<div class="book" title="A little graph theory"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch10lvl1sec69" class="calibre1"/>A little graph theory</h1></div></div></div><p class="calibre8">Graph theory <a id="id718" class="calibre1"/>is a branch of mathematics that deals with mathematical <a id="id719" class="calibre1"/>objects known as <span class="strong"><strong class="calibre2">graphs</strong></span>. Here, a graph does not have the everyday meaning that we are more used to talking about, in the sense of a diagram or plot with an <span class="strong"><em class="calibre9">x</em></span> and <span class="strong"><em class="calibre9">y</em></span> axis. In graph theory, a graph consists of two sets. The first is a set of vertices, which are also <a id="id720" class="calibre1"/>referred to as <span class="strong"><strong class="calibre2">nodes</strong></span>. We typically use integers to label and <a id="id721" class="calibre1"/>enumerate the vertices. The second set consists of <span class="strong"><strong class="calibre2">edges</strong></span> between these vertices.</p><p class="calibre8">Thus, a graph is nothing more than a description of some points and the connections between them. The <a id="id722" class="calibre1"/>connections can <a id="id723" class="calibre1"/>have a direction so that an edge goes from <a id="id724" class="calibre1"/>the <span class="strong"><strong class="calibre2">source</strong></span> or <span class="strong"><strong class="calibre2">tail vertex</strong></span> to the <span class="strong"><strong class="calibre2">target</strong></span> or <span class="strong"><strong class="calibre2">head vertex</strong></span>. In <a id="id725" class="calibre1"/>this case, we have a <span class="strong"><strong class="calibre2">directed graph</strong></span>. Alternatively, the edges <a id="id726" class="calibre1"/>can have no direction, so that the graph is <span class="strong"><strong class="calibre2">undirected</strong></span>.</p><p class="calibre8">A <a id="id727" class="calibre1"/>common way to describe a graph is via the <span class="strong"><strong class="calibre2">adjacency matrix</strong></span>. If we have <span class="strong"><em class="calibre9">V</em></span> vertices <a id="id728" class="calibre1"/>in the graph, an adjacency matrix is a <span class="strong"><em class="calibre9">V×V</em></span> matrix whose entries are 0 if the vertex represented by the row number is not connected to the vertex represented by the column number. If there is a connection, the entry is 1 (however, when you are using weighted graphs, the entry value is not always 1).</p><p class="calibre8">With undirected graphs, both nodes at each edge are connected to each other so the adjacency matrix is symmetric. For directed graphs, a vertex <span class="strong"><em class="calibre9">v<sub class="calibre14">i</sub></em></span> is connected to a vertex <span class="strong"><em class="calibre9">v<sub class="calibre14">j</sub></em></span> via an edge (<span class="strong"><em class="calibre9">v<sub class="calibre14">i</sub></em></span>,<span class="strong"><em class="calibre9">v<sub class="calibre14">j</sub></em></span>); that is, an edge where <span class="strong"><em class="calibre9">v<sub class="calibre14">i</sub></em></span> is the tail and <span class="strong"><em class="calibre9">v<sub class="calibre14">j</sub></em></span> is the head. Here is an example adjacency matrix for a graph with seven nodes:</p><div class="informalexample"><pre class="programlisting">&gt; adjacency_m
  1 2 3 4 5 6 7
1 0 0 0 0 0 1 0
2 1 0 0 0 0 0 0
3 0 0 0 0 0 0 1
4 0 0 1 0 1 0 1
5 0 0 0 0 0 0 0
6 0 0 0 1 1 0 1
7 0 0 0 0 1 0 0</pre></div><p class="calibre8">This <a id="id729" class="calibre1"/>matrix is not symmetric, so we know that we are dealing with a directed graph. The first <code class="email">1</code> value in the first row of the matrix denotes the fact that there is an edge starting from vertex 1 and ending on vertex 6. When the number of nodes is small, it is easy to visualize a graph. We simply draw circles to represent the vertices and lines between them to represent the edges.</p><p class="calibre8">For directed graphs, we use arrows on the lines to denote the directions of the edges. It is important to note that we can draw the same graph in an infinite number of different ways on the page. This is because the graph tells us nothing about the positioning of the nodes in space; we only care about how they are connected to each other. Here are two different, but equally valid, ways to draw the graph described by the adjacency matrix we just saw:</p><div class="mediaobject"><img src="../images/00167.jpeg" alt="A little graph theory" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Two vertices are said to be connected with each other if there is an edge between them (taking note of the order when talking about directed graphs). If we can move from vertex <span class="strong"><em class="calibre9">v<sub class="calibre14">i</sub></em></span> to vertex <span class="strong"><em class="calibre9">v<sub class="calibre14">j</sub></em></span> by starting at the first vertex and finishing at the second vertex, by moving on the graph along the edges and passing through an arbitrary number of graph vertices, then these <a id="id730" class="calibre1"/>intermediate edges form a <span class="strong"><strong class="calibre2">path</strong></span> between these two vertices. Note that this definition requires that all the vertices and edges along the path are distinct from each <a id="id731" class="calibre1"/>other (with the possible exception of the first and last vertex).</p><p class="calibre8">For example, in our graph, vertex 6 can be reached from vertex 2 by a path leading through vertex 1. Sometimes, there can be many such possible paths through the graph, and we are often <a id="id732" class="calibre1"/>interested in the shortest path, which moves through the fewest number of intermediary vertices. We can define the distance between two nodes in the graph <a id="id733" class="calibre1"/>as the length of the shortest path between them. A path that begins and ends at the same vertex is known as a <span class="strong"><strong class="calibre2">cycle</strong></span>. A graph that does not have any cycles in it <a id="id734" class="calibre1"/>is known as an <span class="strong"><strong class="calibre2">acyclic graph</strong></span>. If an acyclic graph has directed edges, it is known as a <span class="strong"><strong class="calibre2">directed acyclic graph</strong></span>, which is often abbreviated <a id="id735" class="calibre1"/>to <span class="strong"><strong class="calibre2">DAG</strong></span>.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="tip17" class="calibre1"/>Tip</h3><p class="calibre8">There are many excellent references on graph theory available. One such reference that is available online is <span class="strong"><em class="calibre9">Graph Theory</em></span>, <span class="strong"><em class="calibre9">Reinhard Diestel</em></span>, <span class="strong"><em class="calibre9">Springer</em></span>. This landmark reference is now in its <span class="strong"><em class="calibre9">4th</em></span> edition and can be found at <a class="calibre1" href="http://diestel-graph-theory.com/">http://diestel-graph-theory.com/</a>.</p></div><p class="calibre8">It might not seem obvious at first, but it turns out that a large number of real-world situations can be conveniently described using graphs. For example, the network of friendships on social media sites, such as Facebook, or followers on Twitter, can be represented as graphs. On Facebook, the friendship relation is reciprocal, and so the graph is undirected. On Twitter, the follower relation is not, and so the graph is directed.</p><p class="calibre8">Another <a id="id736" class="calibre1"/>graph is the network of websites on the web, where links from one web page to the next form directed edges. Transport networks, communication <a id="id737" class="calibre1"/>networks, and electricity grids can be represented as graphs. For the predictive modeler, it turns out that a special class of models known as <span class="strong"><strong class="calibre2">probabilistic graphical models</strong></span>, or <span class="strong"><strong class="calibre2">graphical models</strong></span> for short, are models that involve a graph structure.</p><p class="calibre8">In a graphical model, the nodes represent random variables and the edges in between represent the dependencies between them. Before we can go into further detail, we'll need to take a short detour in order to visit Bayes' theorem, a classic theorem in statistics that, despite its simplicity, has implications both profound and practical when it comes to statistical inference and prediction.</p></div></div>
<div class="book" title="Bayes' theorem" id="29DRA1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch10lvl1sec70" class="calibre1"/>Bayes' theorem</h1></div></div></div><p class="calibre8">Suppose we <a id="id738" class="calibre1"/>are interested in two events, <span class="strong"><em class="calibre9">A</em></span> and <span class="strong"><em class="calibre9">B</em></span>. In this case, event <span class="strong"><em class="calibre9">A</em></span> might represent the event that a patient has appendicitis and event <span class="strong"><em class="calibre9">B</em></span> might represent a <a id="id739" class="calibre1"/>patient having a high white blood cell count. The <span class="strong"><strong class="calibre2">conditional probability</strong></span> of event <span class="strong"><em class="calibre9">A</em></span> given event <span class="strong"><em class="calibre9">B</em></span> is essentially the probability that event <span class="strong"><em class="calibre9">A</em></span> will occur when we know that event <span class="strong"><em class="calibre9">B</em></span> has already happened.</p><p class="calibre8">Formally, we define the conditional probability of event <span class="strong"><em class="calibre9">A</em></span> given event <span class="strong"><em class="calibre9">B</em></span> as the joint probability of both events occurring divided by the probability of event <span class="strong"><em class="calibre9">B</em></span> occurring:</p><div class="mediaobject"><img src="../images/00168.jpeg" alt="Bayes' theorem" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Note that this is consistent with the way in which we define statistical independence. Statistical independence occurs when the joint probability of two events occurring is just the product of the individual probabilities of the two events. If we substitute this in our previous equation, we have:</p><div class="mediaobject"><img src="../images/00169.jpeg" alt="Bayes' theorem" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">This makes sense intuitively because if we know that two events are independent of each other, knowing that event <span class="strong"><em class="calibre9">B</em></span> has occurred does not change the probability of event <span class="strong"><em class="calibre9">A</em></span> occurring. Now, we can rearrange our equation for conditional probability as follows, and note that we can switch over events <span class="strong"><em class="calibre9">A</em></span> and <span class="strong"><em class="calibre9">B</em></span> to get an alternative form:</p><div class="mediaobject"><img src="../images/00170.jpeg" alt="Bayes' theorem" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">This last step allows us to state Bayes' theorem in its simplest form:</p><div class="mediaobject"><img src="../images/00171.jpeg" alt="Bayes' theorem" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">In the previous equation, <span class="strong"><em class="calibre9">P(A)</em></span> is referred to as the <span class="strong"><strong class="calibre2">prior probability </strong></span>of event <span class="strong"><em class="calibre9">A</em></span>, as it represents <a id="id740" class="calibre1"/>the probability of event <span class="strong"><em class="calibre9">A</em></span> occurring prior to any new information. <span class="strong"><em class="calibre9">P(A|B)</em></span>, which is the conditional probability of event <span class="strong"><em class="calibre9">A</em></span> given that event <span class="strong"><em class="calibre9">B</em></span> has occurred, is <a id="id741" class="calibre1"/>often also referred to as the <span class="strong"><strong class="calibre2">posterior probability</strong></span> of <span class="strong"><em class="calibre9">A</em></span>. It is the probability of event <span class="strong"><em class="calibre9">A</em></span> occurring after receiving some new information; in this case, the fact that event <span class="strong"><em class="calibre9">B</em></span> has occurred.</p><p class="calibre8">All of <a id="id742" class="calibre1"/>this might seem like algebraic trickery, but if we revisit our example of event <span class="strong"><em class="calibre9">A</em></span> representing a patient having appendicitis and event <span class="strong"><em class="calibre9">B</em></span> representing a patient having a high white blood cell count, the usefulness of Bayes' theorem will be revealed. Knowing <span class="strong"><em class="calibre9">P(A|B)</em></span>, the conditional probability of having appendicitis, given that we observe that a patient has a high white blood cell count (and similarly for other symptoms), is knowledge that would be very useful to doctors. This would allow them to make a diagnosis about something that isn't easily observable (appendicitis) using something that is (high white blood cell count).</p><p class="calibre8">Unfortunately, this is something that is very hard to estimate because a high white blood cell count might occur as a symptom of a host of other diseases or pathologies. The reverse probability, <span class="strong"><em class="calibre9">P(B|A)</em></span>, however (namely, the conditional probability of having a high white blood cell count given that a patient already has appendicitis), is much easier to estimate. One simply needs to examine records of past cases with appendicitis and inspect the blood tests of those cases. Bayes' theorem is a fundamental boon to predictive modeling because it allows us to estimate cause by observing effect.</p></div>
<div class="book" title="Conditional independence" id="2ACBS1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch10lvl1sec71" class="calibre1"/>Conditional independence</h1></div></div></div><p class="calibre8">We know <a id="id743" class="calibre1"/>from statistics that the notion of statistical independence says that the joint probability of two random variables, <span class="strong"><em class="calibre9">A</em></span> and <span class="strong"><em class="calibre9">B</em></span>, is just the product of their (marginal) probabilities. Sometimes, two variables may not be statistically independent <a id="id744" class="calibre1"/>of each other to begin with, but observing a third variable, <span class="strong"><em class="calibre9">C</em></span>, might result in them becoming independent of each other. In short, we say that events <span class="strong"><em class="calibre9">A</em></span> and <span class="strong"><em class="calibre9">B</em></span> are <span class="strong"><strong class="calibre2">conditionally independent</strong></span> given <span class="strong"><em class="calibre9">C</em></span>, and we can express this as:</p><div class="mediaobject"><img src="../images/00172.jpeg" alt="Conditional independence" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">For example, suppose that <span class="strong"><em class="calibre9">J</em></span> represents the probability of being given a job offer at a particular company and <span class="strong"><em class="calibre9">G</em></span> represents the probability of being accepted into graduate school at a particular university. Both of these might depend on a variable <span class="strong"><em class="calibre9">U</em></span>, a person's performance on their undergraduate degree. This can be summarized in a graph as follows:</p><div class="mediaobject"><img src="../images/00173.jpeg" alt="Conditional independence" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">When we don't know <span class="strong"><em class="calibre9">U</em></span>, a person's performance on their undergraduate degree, knowing that they <a id="id745" class="calibre1"/>were accepted into graduate school might increase our belief in their chances of getting a job and vice versa. This is because we are inclined to believe that they did well in their undergraduate degree, which influences that person's chances of getting a job. Thus, the two events <span class="strong"><em class="calibre9">J</em></span> and <span class="strong"><em class="calibre9">G</em></span> are not independent of each other.</p><p class="calibre8">If we are told the performance of a person on their undergraduate degree, however, we might assume that the person's chance of getting a job offer might be independent of their chance of getting into graduate school. This is because of other factors that might affect this, such as the person's job interview on a particular day or the quality of other potential candidates for the job, which are not influenced by the person's application to graduate school.</p></div>
<div class="book" title="Bayesian networks" id="2BASE1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch10lvl1sec72" class="calibre1"/>Bayesian networks</h1></div></div></div><p class="calibre8">
<span class="strong"><strong class="calibre2">Bayesian networks</strong></span> are <a id="id746" class="calibre1"/>a type of graphical model that involve a directed acyclic graph structure. We often refer to the tail node of a directed edge in a <a id="id747" class="calibre1"/>graphical model as the <span class="strong"><strong class="calibre2">parent</strong></span> and the head node as the <span class="strong"><strong class="calibre2">child</strong></span> or <span class="strong"><strong class="calibre2">descendant</strong></span>. In fact, we generalize this latter notion so that, if there is a path from node <span class="strong"><em class="calibre9">A</em></span> to node <span class="strong"><em class="calibre9">B</em></span> in the <a id="id748" class="calibre1"/>model, node <span class="strong"><em class="calibre9">B</em></span> is a descendant of node <span class="strong"><em class="calibre9">A</em></span>. We can distinguish the special <a id="id749" class="calibre1"/>case of node <span class="strong"><em class="calibre9">A</em></span> connected to node <span class="strong"><em class="calibre9">B</em></span> by saying that the latter is a <span class="strong"><strong class="calibre2">direct descendant</strong></span>.</p><p class="calibre8">The parent <a id="id750" class="calibre1"/>relationship and the descendant relationship are mutually exclusive in a Bayesian network because it has no cycles. Bayesian networks have the distinguishing property that, given its parents, every node in the network is conditionally independent of all other nodes in the network that are not its descendants. This is sometimes referred to as the <span class="strong"><strong class="calibre2">local Markov property</strong></span>. It is an important property because it means that we can easily factorize the joint probability function of all the random variables in the <a id="id751" class="calibre1"/>model by simply taking note of the edges in the graph.</p><p class="calibre8">To understand <a id="id752" class="calibre1"/>how this works, we will begin with the product rule of probability for three variables that says the following (taking <span class="strong"><em class="calibre9">G</em></span>, <span class="strong"><em class="calibre9">J</em></span>, and <span class="strong"><em class="calibre9">U</em></span> as example variables):</p><div class="mediaobject"><img src="../images/00174.jpeg" alt="Bayesian networks" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">This rule is a general rule and always holds without any loss of generality. Let's return to our student applicant example. This is actually a simple Bayesian network where <span class="strong"><em class="calibre9">G</em></span> and <span class="strong"><em class="calibre9">J</em></span> have <span class="strong"><em class="calibre9">U</em></span> as a parent. Using the local Markov property of Bayesian networks, we can simplify the equation for the joint probability distribution as follows:</p><div class="mediaobject"><img src="../images/00175.jpeg" alt="Bayesian networks" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The ability to factorize a probability distribution in this way is useful as it simplifies the computations we need to make. It can also allow us to represent the entire distribution in a more compact form. Suppose that the distribution of each random variable is discrete and takes on a finite set of values, for example, random variables <span class="strong"><em class="calibre9">G</em></span> and <span class="strong"><em class="calibre9">J</em></span> could each take on the two discrete values {yes, no}. To store a joint probability distribution without factorizing, and taking into account independence relations, we need to consider all possible combinations of every random variable.</p><p class="calibre8">By contrast, if the distribution factorizes into a product of simpler distributions as we saw earlier, the total number of random variable combinations we need to consider are far fewer. For networks with several random variables that take on many values, the savings are very substantial indeed.</p><p class="calibre8">Besides computation and storage, another significant benefit is that when we want to determine the joint probability distribution of our random variables given some data, it becomes much simpler to do so when we can factorize it because of known independence relations. We will see this in detail when, in the next section, we study an important example of a Bayesian network.</p><p class="calibre8">To wrap up this section, we'll note the factorization of the joint probability function of the Bayesian network, represented by the graph we saw in the first diagram in this chapter, and leave it <a id="id753" class="calibre1"/>as an exercise for the reader to verify:</p><div class="mediaobject"><img src="../images/00176.jpeg" alt="Bayesian networks" class="calibre10"/></div><p class="calibre11"> </p></div>

<div id="page" style="height:0pt"/><div class="book" title="The Na&#xEF;ve Bayes classifier"><div class="book" id="2C9D02-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch10lvl1sec73" class="calibre1"/>The Naïve Bayes classifier</h1></div></div></div><p class="calibre8">We now <a id="id754" class="calibre1"/>have the necessary tools to learn about our first and simplest graphical model, the <span class="strong"><strong class="calibre2">Naïve Bayes classifier</strong></span>. This is a directed graphical model that contains a single parent node and a series of child nodes representing random variables that are dependent only on this node with no dependencies between them. Here is an example:</p><div class="mediaobject"><img src="../images/00177.jpeg" alt="The Naïve Bayes classifier" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">We usually interpret our single parent node as the causal node, so in our particular example, the value of the <span class="strong"><em class="calibre9">Sentiment</em></span> node will influence the value of the <span class="strong"><em class="calibre9">sad</em></span> node, the <span class="strong"><em class="calibre9">fun</em></span> node, and so on. As this is a Bayesian network, the local Markov property can be used to explain the core assumption of the model. Given the <span class="strong"><em class="calibre9">Sentiment</em></span> node, all other nodes are independent of each other.</p><p class="calibre8">In practice, we use the Naïve Bayes classifier in a context where we can observe and measure the child nodes and attempt to estimate the parent node as our output. Thus, the child nodes will be the input features of our model, and the parent node will be the output variable. For example, the child nodes may represent various medical symptoms and the parent node might be whether a particular disease is present.</p><p class="calibre8">To understand <a id="id755" class="calibre1"/>how the model works in practice, we make recourse to Bayes' theorem, where <span class="strong"><em class="calibre9">C</em></span> is the parent node and <span class="strong"><em class="calibre9">F<sub class="calibre14">i</sub></em></span> are the children or feature nodes:</p><div class="mediaobject"><img src="../images/00178.jpeg" alt="The Naïve Bayes classifier" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">We can simplify this using the conditional independence assumptions of the network:</p><div class="mediaobject"><img src="../images/00179.jpeg" alt="The Naïve Bayes classifier" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">To make a classifier out of this probability model, our objective is to choose the class <span class="strong"><em class="calibre9">C<sub class="calibre14">i</sub></em></span> which maximizes the posterior probability <span class="strong"><em class="calibre9">P(Ci|F<sub class="calibre14">1</sub></em></span>
<span class="strong"><em class="calibre9">…F<sub class="calibre14">n</sub></em></span>
<span class="strong"><em class="calibre9">)</em></span>; that is, the posterior probability of that class given the observed features. The denominator is the joint probability of the observed features, which is not influenced by the class that is chosen. Consequently, maximizing the posterior class probability amounts to maximizing the numerator of the previous equation:</p><div class="mediaobject"><img src="../images/00180.jpeg" alt="The Naïve Bayes classifier" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Given some data, we can estimate the probabilities, <span class="strong"><em class="calibre9">P(F<sub class="calibre14">i</sub></em></span>
<span class="strong"><em class="calibre9">|C<sub class="calibre14">j</sub></em></span>
<span class="strong"><em class="calibre9">)</em></span>, for all the different values of the feature <span class="strong"><em class="calibre9">F<sub class="calibre14">i</sub></em></span> as the relative proportion of the observations of class <span class="strong"><em class="calibre9">C<sub class="calibre14">j</sub></em></span> that have each different value of feature <span class="strong"><em class="calibre9">F<sub class="calibre14">i</sub></em></span>. We can also estimate <span class="strong"><em class="calibre9">P(C<sub class="calibre14">j</sub></em></span>
<span class="strong"><em class="calibre9">)</em></span> as the relative proportion of the observations that are assigned to class <span class="strong"><em class="calibre9">C<sub class="calibre14">j</sub></em></span>. These are the maximum likelihood estimates. In the next <a id="id756" class="calibre1"/>section, we will see how the Naïve Bayes classifier works in a real example.</p></div>

<div class="book" title="The Na&#xEF;ve Bayes classifier">
<div class="book" title="Predicting the sentiment of movie reviews"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch10lvl2sec88" class="calibre1"/>Predicting the sentiment of movie reviews</h2></div></div></div><p class="calibre8">In a world <a id="id757" class="calibre1"/>of online reviews, forums, and social media, a task that has received, and continues to receive, a growing <a id="id758" class="calibre1"/>amount of interest is the task of <span class="strong"><strong class="calibre2">sentiment analysis</strong></span>. Put simply, the task is to analyze a piece of text to determine the sentiment that is being expressed by the author. A typical scenario involves collecting online reviews, blog posts, or tweets and building a model that predicts whether the user is trying to express a positive or a negative feeling. Sometimes, the task can be framed to capture a wider variety of sentiments, such as a neutral sentiment or the degree of sentiment, such as mildly negative versus very negative.</p><p class="calibre8">In this section, we will limit ourselves to the simpler task of discerning positive from negative sentiments. We will do this by modeling sentiment using a similar Bayesian network to the one that we saw in the previous section. The sentiment is our target output variable, which is either positive or negative. Our input features are all binary features that describe whether a particular word is present in a movie review. The key idea here is that users expressing a negative sentiment will tend to choose from a characteristic set of words in their review that is different from the characteristic set that users would pick from when writing a positive review.</p><p class="calibre8">By using the Naïve Bayes model, our assumption will be that if we know the sentiment being expressed, the presence of each word in the text is independent from all the other words. Of course, this is a very strict assumption to use and doesn't speak at all to the process of how real text is written. Nonetheless, we will show that even under these strict assumptions, we can build a model that performs reasonably well.</p><p class="calibre8">We will use the <span class="strong"><em class="calibre9">Large Movie Review Data Set</em></span>, first presented in the paper titled <span class="strong"><em class="calibre9">Learning Word Vectors for Sentiment Analysis</em></span>, <span class="strong"><em class="calibre9">Andrew L. Maas</em></span>, <span class="strong"><em class="calibre9">Raymond E. Daly</em></span>, <span class="strong"><em class="calibre9">Peter T. Pham</em></span>, <span class="strong"><em class="calibre9">Dan Huang</em></span>, <span class="strong"><em class="calibre9">Andrew Y. Ng</em></span>, and <span class="strong"><em class="calibre9">Christopher Potts</em></span>, published in <span class="strong"><em class="calibre9">The 49th Annual Meeting of the Association for Computational Linguistics</em></span> (<span class="strong"><em class="calibre9">ACL 2011</em></span>). The data is hosted at <a class="calibre1" href="http://ai.stanford.edu/~amaas/data/sentiment/">http://ai.stanford.edu/~amaas/data/sentiment/</a> and is comprised of a training set of 25,000 movie reviews and a test set of another 25,000 movie reviews.</p><p class="calibre8">In order to demonstrate how the model works, we would like to keep the training time of our model low. For this reason, we are going to partition the original training set into a new training and test set, but the reader is very strongly encouraged to repeat the exercise with the larger test dataset that is part of the original data. When downloaded, the data is organized into a <code class="email">train</code> folder and a <code class="email">test</code> folder. The <code class="email">train</code> folder contains a folder called <code class="email">pos</code> that has 12,500 positive movie reviews, each inside a separate text file, and similarly, a folder called <code class="email">neg</code> with 12,500 negative movie reviews.</p><p class="calibre8">Our first task is to load all this information into R and perform some necessary preprocessing. To do this, we are going to install and use the <code class="email">tm</code> package, which is a specialized package <a id="id759" class="calibre1"/>for performing text-mining operations. This package is very useful when working with text data and we will use it again in a subsequent chapter.</p><p class="calibre8">When <a id="id760" class="calibre1"/>working with the <code class="email">tm</code> package, the first task is to organize the various sources of text into a <span class="strong"><strong class="calibre2">corpus</strong></span>. In linguistics, this commonly refers to a collection of documents. In the <code class="email">tm</code> package, it is just a collection of strings representing individual sources of text, along with some metadata that describes some information about them, such as the names of the files from which they were retrieved.</p><p class="calibre8">With the <code class="email">tm</code> package, we build a corpus using the <code class="email">Corpus()</code> function, to which we must provide a source for the various documents we want to import. We could create a vector of strings and pass this as an argument to <code class="email">Corpus()</code> using the <code class="email">VectorSource()</code> function. Instead, as our data source is a series of text files in a directory, we will use the <code class="email">DirSource()</code> function. First, we will create two string variables that will contain the absolute paths to the aforementioned <code class="email">neg</code> and <code class="email">pos</code> folders on our machine (this will depend on where the dataset is downloaded).</p><p class="calibre8">Then, we can use the <code class="email">Corpus()</code> function twice to create two corpora for positive and negative reviews, which will then be merged into a single corpus:</p><div class="informalexample"><pre class="programlisting">&gt; path_to_neg_folder &lt;- "~/aclImdb/train/neg"
&gt; path_to_pos_folder &lt;- "~/aclImdb/train/pos"
&gt; library("tm")
&gt; nb_pos &lt;- Corpus(DirSource(path_to_pos_folder), 
                   readerControl = list(language = "en"))
&gt; nb_neg &lt;- Corpus(DirSource(path_to_neg_folder), 
                  readerControl = list(language = "en"))
&gt; nb_all &lt;- c(nb_pos, nb_neg, recursive = T)</pre></div><p class="calibre8">The second argument to the <code class="email">Corpus()</code> function, <code class="email">readerControl</code>, is a list of optional parameters. We used this to specify that the language of our text files is English. The <code class="email">recursive</code> parameter in the <code class="email">c()</code> function used to merge the two corpora is necessary to maintain the metadata information stored in the corpus objects.</p><p class="calibre8">Note that we can merge the two corpora without actually losing the sentiment label. Each text file representing a movie review is named using the format <code class="email">&lt;counter&gt;_&lt;score&gt;.txt</code>, and this information is stored in the metadata portion of the corpus object created by the <code class="email">Corpus()</code> function. We can see the metadata for the first review in our corpus using the <code class="email">meta()</code> function:</p><div class="informalexample"><pre class="programlisting">&gt; meta(nb_all[[1]])
Metadata:
  author       : character(0)
  datetimestamp: 2015-04-19 09:17:48
  description  : character(0)
  heading      : character(0)
  id           : 0_9.txt
  language     : en
  origin       : character(0)</pre></div><p class="calibre8">The <code class="email">meta()</code> function thus retrieves a metadata object for each entry in our corpus. The <code class="email">ID</code> attribute in this object contains the name of the file. The score part of the name is a number between 0 and 10, where higher numbers denote positive reviews, and low numbers denote negative reviews. In the training data, we only have polar reviews; that is, reviews that are in the ranges 0-4 and 7-10. We can thus use this information to create a vector of document names:</p><div class="informalexample"><pre class="programlisting">&gt; ids &lt;- sapply( 1 : length(nb_all),
                 function(x) meta(nb_all[[x]], "id"))
&gt; head(ids)
[1] "0_9.txt"     "1_7.txt"     "10_9.txt"    "100_7.txt"
[5] "1000_8.txt"  "10000_8.txt"</pre></div><p class="calibre8">From <a id="id761" class="calibre1"/>this list of document names, we'll extract the score component using the <code class="email">sub()</code> function with an appropriate regular expression. If the score of a movie review is less than or equal to 5, it is a negative review and if it is greater, it is a positive review:</p><div class="informalexample"><pre class="programlisting">&gt; scores &lt;- as.numeric(sapply(ids,
            function(x) sub("[0-9]+_([0-9]+)\\.txt", "\\1", x)))
&gt; scores &lt;- factor(ifelse(scores &gt;= 5, "positive", "negative"))
&gt; summary(scores)
negative positive
   12500    12500</pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="tip18" class="calibre1"/>Tip</h3><p class="calibre8">The <code class="email">sub()</code> function is just one of R's functions that uses regular expressions. For readers unfamiliar with the concept, a regular expression is essentially a pattern language for describing strings. Online tutorials for regular expressions are easy to find. An excellent resource for learning about regular expressions' as well as text processing more generally, is <span class="strong"><em class="calibre9">Speech and Language Processing Second Edition</em></span>, <span class="strong"><em class="calibre9">Jurafsky and Martin</em></span>.</p></div><p class="calibre8">The features of our model will be binary features that describe the presence or absence of specific words in the dictionary. Intuitively, we should expect that a movie review containing words such as <span class="strong"><em class="calibre9">boring</em></span>, <span class="strong"><em class="calibre9">cliché</em></span>, and <span class="strong"><em class="calibre9">horrible</em></span> is likely to be a negative review. A movie review with words such as <span class="strong"><em class="calibre9">inspiring</em></span>, <span class="strong"><em class="calibre9">enjoyable</em></span>, <span class="strong"><em class="calibre9">moving</em></span>, and <span class="strong"><em class="calibre9">excellent</em></span> is likely to be a good review.</p><p class="calibre8">When working with text data, we almost always need to perform a series of preprocessing steps. For example, we tend to convert all the words to a lowercase format because we don't want to <a id="id762" class="calibre1"/>have two separate features for the words <span class="strong"><em class="calibre9">Excellent</em></span> and <span class="strong"><em class="calibre9">excellent</em></span>. We also want to remove anything <a id="id763" class="calibre1"/>from our text that will likely be uninformative as features. For this reason, we tend to remove punctuation, numbers, and <span class="strong"><strong class="calibre2">stop words</strong></span>. Stop words are words such as <span class="strong"><em class="calibre9">the</em></span>, <span class="strong"><em class="calibre9">and</em></span>, <span class="strong"><em class="calibre9">in</em></span>, and <span class="strong"><em class="calibre9">he</em></span>, which are very frequently used in the English language and are bound to appear in nearly all of the movie reviews. Finally, because we are removing words from sentences and creating repeated spaces, we will want to remove these in order to assist the process of tokenization (the process of splitting up the text into words).</p><p class="calibre8">The <code class="email">tm</code> package has two functions, <code class="email">tm_map()</code> and <code class="email">content_transformer()</code>, which together can be used to apply text transformations to the content of every entry in our corpus:</p><div class="informalexample"><pre class="programlisting">&gt; nb_all &lt;- tm_map(nb_all, content_transformer(removeNumbers))
&gt; nb_all &lt;- tm_map(nb_all, content_transformer(removePunctuation))
&gt; nb_all &lt;- tm_map(nb_all, content_transformer(tolower))
&gt; nb_all &lt;- tm_map(nb_all, content_transformer(removeWords), 
                           stopwords("english"))
&gt; nb_all &lt;- tm_map(nb_all, content_transformer(stripWhitespace))</pre></div><p class="calibre8">Now that we have preprocessed our corpus, we are ready to compute our features. Essentially, what we <a id="id764" class="calibre1"/>need is a data structure known as a <span class="strong"><strong class="calibre2">document term matrix</strong></span>. The rows of the matrix are the documents. The columns of the matrix are the words in our dictionary. Each entry in the matrix is a binary value, with <code class="email">1</code> representing the fact that the word represented by the column number was found inside the review represented by the row number. For example, if the first column corresponds to the word <code class="email">action</code>, the fourth row corresponds to the fourth movie review, and the value of the matrix at position (4,1) is <code class="email">1</code>, this signifies that the fourth movie review contains the word <code class="email">action</code>.</p><p class="calibre8">The <code class="email">tm</code> package provides us with the <code class="email">DocumentTermMatrix()</code> function that takes in a corpus object and builds a document term matrix. The particular matrix built has numerical entries that represent the total number of times a particular word is seen inside a particular text, so we will have to convert these into a binary factor afterward:</p><div class="informalexample"><pre class="programlisting">&gt; nb_dtm &lt;- DocumentTermMatrix(nb_all)
&gt; dim(nb_dtm)
[1]  25000 117473</pre></div><p class="calibre8">Our document term matrix in this case has 117,473 columns, indicating that we have found this number of different words in the corpus. This matrix is very sparse, meaning that most of the entries are 0. This is a very typical scenario when building document term matrices for text documents, especially text documents that are as short as movie reviews. Any particular movie review will only feature a tiny fraction of the words in the vocabulary. Let's examine our matrix to see just how sparse it is:</p><div class="informalexample"><pre class="programlisting">&gt; nb_dtm
&lt;&lt;DocumentTermMatrix (documents: 25000, terms: 117473)&gt;&gt;
Non-/sparse entries: 2493414/2934331586
Sparsity           : 100%
Maximal term length: 64
Weighting          : term frequency (tf)</pre></div><p class="calibre8">From the ratio of non-sparse to sparse entries, we can see that of the 2,936,825,000 entries in the matrix (25000 × 117473), only 2,493,414 are nonzero. At this point, we should reduce the number of columns of this matrix for two reasons. On the one hand, because the words in our vocabulary will become the features in our model, we don't want to build a model that uses 117,473 features. This would take a very long time to train and, at the same time, is unlikely to provide us with a decent fit using only 25,000 data points.</p><p class="calibre8">Another significant reason for us to want to reduce the number of columns is that many words will <a id="id765" class="calibre1"/>appear only once or twice in the whole corpus, and will be as uninformative about the user's sentiment as words that occur in nearly all the documents. Given this, we have a natural way to reduce the dimensions of the document term matrix, namely by dropping the columns (that is, removing certain words from the feature set) that are the sparsest. We can remove all columns that have a certain percentage of sparse elements using the <code class="email">removeSparseTerms()</code> function. The first argument that we must provide this with is a document term matrix, and the second is the maximum degree of column sparseness that we will allow. Choosing the degree of sparseness is tricky because we don't want to throw away too many of the columns that will become our features. We will proceed by running our experiments with 99 percent sparseness, but encourage the reader to repeat with different values to see the effect this has on the number of features and model performance.</p><p class="calibre8">We have 25,000 rows in the matrix corresponding to the total number of documents in our corpus. If we allow a maximum of 99 percent sparseness, we are effectively removing words that do not occur in at least 1 percent of those 25,000 documents; that is, in at least 250 documents:</p><div class="informalexample"><pre class="programlisting">&gt; nb_dtm &lt;- removeSparseTerms(x = nb_dtm, sparse = 0.99)
&gt; dim(nb_dtm)
[1] 25000  1603</pre></div><p class="calibre8">We have now significantly reduced the number of columns down to 1,603. This is a substantially more reasonable number of features for us to work with. Next, we convert all entries to binary, using another function of <code class="email">tm</code>, <code class="email">weightBin()</code>:</p><div class="informalexample"><pre class="programlisting">&gt; nb_dtm &lt;- weightBin(nb_dtm)</pre></div><p class="calibre8">As the document term matrix is, in general, a very sparse matrix, R uses a compact data structure to store the information. To peek inside this matrix and examine the first few terms, we will use the <code class="email">inspect()</code> function on a small slice of this matrix:</p><div class="informalexample"><pre class="programlisting">&gt; inspect(nb_dtm[10:16, 1:6])
&lt;&lt;DocumentTermMatrix (documents: 7, terms: 6)&gt;&gt;
Non-/sparse entries: 2/40
Sparsity           : 95%
Maximal term length: 10
Weighting          : binary (bin)
             Terms
Docs          ability able absolute absolutely absurd academy
  10004_8.txt       0    1        0          0      0       0
  10005_7.txt       0    0        0          0      0       0
  10006_7.txt       0    0        0          0      0       0
  10007_7.txt       0    0        0          0      0       0
  10008_7.txt       0    0        0          0      0       1
  10009_9.txt       0    0        0          0      0       0
  1001_8.txt        0    0        0          0      0       0</pre></div><p class="calibre8">It looks like the word <code class="email">ability</code> does not appear in the first six documents and the word <code class="email">able</code> appears in the document <code class="email">10004_8.txt</code>. We now have both our features and our output vector. The next step is to convert our document term matrix into a data frame. This is needed <a id="id766" class="calibre1"/>by the function that will train our Naïve Bayes model. Then, before we train the model, we will split our data into a training set with 80 percent of the documents and a test set with 20 percent of the documents, as follows:</p><div class="informalexample"><pre class="programlisting">&gt; nb_df &lt;- as.data.frame(as.matrix(nb_dtm))
&gt; library(caret)
&gt; set.seed(443452342)
&gt; nb_sampling_vector &lt;- createDataPartition(scores, p = 0.80, 
                                            list = FALSE)
&gt; nb_df_train &lt;- nb_df[nb_sampling_vector,]
&gt; nb_df_test &lt;- nb_df[-nb_sampling_vector,]
&gt; scores_train = scores[nb_sampling_vector]
&gt; scores_test = scores[-nb_sampling_vector]</pre></div><p class="calibre8">To train a Naïve Bayes model, we will use the <code class="email">naiveBayes()</code> function in the <code class="email">e1071</code> package that we saw earlier. The first argument we will provide it with is our feature data frame, and the second argument is our vector of output labels:</p><div class="informalexample"><pre class="programlisting">&gt; library("e1071")
&gt; nb_model &lt;- naiveBayes(nb_dtm_train, scores_train)</pre></div><p class="calibre8">We can use the <code class="email">predict()</code> function to obtain predictions on our training data:</p><div class="informalexample"><pre class="programlisting">&gt; nb_train_predictions &lt;- predict(nb_model, nb_df_train)
&gt; mean(nb_train_predictions == scores_train)
[1] 0.83015
&gt; table(actual = scores_train, predictions = nb_train_predictions)
          predictions
actual     negative positive
  negative     8442     1558
  positive     1839     8161</pre></div><p class="calibre8">We have hit over 83 percent training accuracy with our simple Naïve Bayes model, which, admittedly, is not bad for such a simple model with an independence assumption that we know is not realistic for our data. Let's repeat the same on our test data:</p><div class="informalexample"><pre class="programlisting">&gt; nb_test_predictions &lt;- predict(nb_model, nb_df_test)
&gt; mean(nb_test_predictions == scores_test)
[1] 0.8224
&gt; table(actual = scores_test, predictions = nb_test_predictions)
          predictions
actual     negative positive
  negative     2090      410
  positive      478     2022</pre></div><p class="calibre8">The test <a id="id767" class="calibre1"/>accuracy of over 82 percent is comparable to what we saw on our training data. There are a number <a id="id768" class="calibre1"/>of potential avenues for improvement here. The first involves noticing that words such as <span class="strong"><em class="calibre9">movie</em></span> and <span class="strong"><em class="calibre9">movies</em></span> are treated differently, even though they are the same word but inflected. In linguistics, <span class="strong"><strong class="calibre2">inflection</strong></span> is the process by which the base form or <span class="strong"><strong class="calibre2">lemma</strong></span> of <a id="id769" class="calibre1"/>a word is modified to agree with another word on attributes such <a id="id770" class="calibre1"/>as tense, case, gender, and number. For example, in English, verbs must agree with their subject. </p><p class="calibre8">The <code class="email">tm</code> package supports <span class="strong"><strong class="calibre2">stemming</strong></span>, a process of removing the inflected part of a word in order to keep just a stem or root word. This is not always the <a id="id771" class="calibre1"/>same as retrieving what is known as the <span class="strong"><strong class="calibre2">morphological lemma</strong></span> of a word, which is what we look up in a dictionary, but is a rough approximation. The <code class="email">tm</code> package <a id="id772" class="calibre1"/>uses the well-known <span class="strong"><strong class="calibre2">Porter Stemmer</strong></span>.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note39" class="calibre1"/>Note</h3><p class="calibre8">
<span class="strong"><em class="calibre9">Martin Porter</em></span>, the author of the Porter Stemmer, maintains a website at <a class="calibre1" href="http://tartarus.org/martin/PorterStemmer/">http://tartarus.org/martin/PorterStemmer/</a>, which is a great source of information on his famous algorithm.</p></div><p class="calibre8">To apply <a id="id773" class="calibre1"/>stemming to our corpus, we need to add a final transformation to our corpus using <code class="email">tm_map()</code> and then recompute our document term matrix anew, as the columns (the word features) are now word stems:</p><div class="informalexample"><pre class="programlisting">&gt; nb_all &lt;- tm_map(nb_all, stemDocument, language = "english")
&gt; nb_dtm &lt;- DocumentTermMatrix(nb_all) 
&gt; nb_dtm &lt;- removeSparseTerms(x = nb_dtm, sparse = 0.99)
&gt; nb_dtm &lt;- weightBin(nb_dtm)
&gt; nb_df_train &lt;- nb_df[nb_sampling_vector,]
&gt; nb_df_test &lt;- nb_df[-nb_sampling_vector,]
&gt; dim(nb_dtm)
[1] 25000  1553</pre></div><p class="calibre8">Note that we have fewer columns that match our criterion of 99 percent maximum sparsity. We can use this new document term matrix to train another Naïve Bayes classifier and then measure the accuracy on our test set:</p><div class="informalexample"><pre class="programlisting">&gt; nb_model_stem &lt;- naiveBayes(nb_df_train, scores_train)
&gt; nb_test_predictions_stem &lt;- predict(nb_model_stem, nb_df_test)
&gt; mean(nb_test_predictions_stem == scores_test)
[1] 0.8
&gt; table(actual = scores_test, predictions = 
                              nb_test_predictions_stem)
          predictions
actual     negative positive
  negative     2067      433
  positive      567     1933</pre></div><p class="calibre8">The result, 80 percent, is slightly lower than what we observed without stemming, although we are <a id="id774" class="calibre1"/>using slightly fewer features than before. Stemming is not always guaranteed to be a good idea, as in some problems it may improve performance, whereas in others it will make no difference or even make things worse. It is, however, a common transformation that is worth trying when working with text data.</p><p class="calibre8">A second <a id="id775" class="calibre1"/>possible improvement is to use <span class="strong"><strong class="calibre2">additive smoothing</strong></span> (also known as <span class="strong"><strong class="calibre2">laplacian smoothing</strong></span>) during the training of our Naïve Bayes model. This is <a id="id776" class="calibre1"/>actually a form of regularization and it works by adding a fixed number to all the counts of feature and class combinations during training. Using our original document term matrix, we can compute a Naïve Bayes model with additive smoothing by specifying the <code class="email">laplace</code> parameter. For our particular dataset, however, we did not witness any improvements by doing this.</p><p class="calibre8">There are a few more avenues of approach that we might try with a Naïve Bayes model, and we will propose them here for the reader to experiment with. The first of these is that it is often worth manually curating the list of words used as features for the model. When we study the terms selected by our document term matrix, we may find that some words are frequent in our training data, but we do not expect them to be frequent in general, or representative of the overall population. Furthermore, we may only want to experiment with words that we know are suggestive of emotion and sentiment. This can be done by specifying a specific dictionary of terms to use when constructing our document term matrix. Here is an example:</p><div class="informalexample"><pre class="programlisting">&gt; emotion_words &lt;- c("good", "bad", "enjoyed", "hated", "like")
&gt; nb_dtm &lt;- DocumentTermMatrix(nb_all, list(dictionary = 
                                            emotion_words))</pre></div><p class="calibre8">It is relatively straightforward to find examples of such lists on the internet. Another common preprocessing step that is used with a Naïve Bayes model is to remove correlations between features. One way of doing this is to perform PCA, as we saw in <a class="calibre1" title="Chapter 1. Gearing Up for Predictive Modeling" href="part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7">Chapter 1</a>, <span class="strong"><em class="calibre9">Gearing Up for Predictive Modeling</em></span>. Furthermore, this method also allows us to begin with a slightly more sparse document term matrix with a larger number of terms, as we know we will be reducing the overall number of features with PCA.</p><p class="calibre8">Potential <a id="id777" class="calibre1"/>model improvements notwithstanding, it is important to be aware of the limitations that the Naïve Bayes model imposes that impede our ability to train a highly accurate sentiment analyzer. Assuming that all the words in a movie review are independent of each other, once we know the sentiment involved, is quite an unrealistic assumption. Our model completely disregards sentence structure and word order. For example, the phrase <span class="strong"><em class="calibre9">not bad</em></span> in a review might indicate a positive sentiment, but because we look at words in isolation, we will tend to correlate the word <span class="strong"><em class="calibre9">bad</em></span> with a negative sentiment. </p><p class="calibre8">Negation in general is one of the hardest problems to handle in text processing. Our model also cannot handle common patterns of language, such as sarcasm, irony, quoted passages that include other people's thoughts, and other such linguistic devices.</p><p class="calibre8">The next section will introduce a more powerful graphical model.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note40" class="calibre1"/>Note</h3><p class="calibre8">
<span class="strong"><strong class="calibre2">Hidden Markov models</strong></span>
</p><p class="calibre8">A good reference to study for the Naïve Bayes classifier is <span class="strong"><em class="calibre9">An empirical study of the Naïve Bayes classifier</em></span>, <span class="strong"><em class="calibre9">I. Rish</em></span>, presented in the 2001 IJCAI workshop on <span class="strong"><em class="calibre9">Empirical Methods in AI</em></span>. For sentiment analysis, we recommend the slides from <span class="strong"><em class="calibre9">Bing Liu</em></span>'s AAAI 2011 tutorial at (as of this writing) <a class="calibre1" href="https://www.researchgate.net/profile/Irina_Rish/publication/228845263_An_Empirical_Study_of_the_Naive_Bayes_Classifier/links/00b7d52dc3ccd8d692000000/An-Empirical-Study-of-the-Naive-Bayes-Classifier.pdf">https://www.researchgate.net/profile/Irina_Rish/publication/228845263_An_Empirical_Study_of_the_Naive_Bayes_Classifier/links/00b7d52dc3ccd8d692000000/An-Empirical-Study-of-the-Naive-Bayes-Classifier.pdf</a>.</p></div><p class="calibre8">A <span class="strong"><strong class="calibre2">Hidden Markov model</strong></span>, often abbreviated to <span class="strong"><strong class="calibre2">HMM</strong></span>, which we will use here, is a Bayesian <a id="id778" class="calibre1"/>network with a repeating structure that is commonly used to model and predict sequences. In this section, we'll see two applications of this model: one to model DNA gene sequences, and another to model the sequences of letters that make up English text. The basic diagram for an HMM is shown here:</p><div class="mediaobject"><img src="../images/00181.jpeg" alt="Predicting the sentiment of movie reviews" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">As we <a id="id779" class="calibre1"/>can see in the diagram, the sequence flows from left to right and we <a id="id780" class="calibre1"/>have a pair of nodes for every entry in the sequence that we are trying <a id="id781" class="calibre1"/>to model. Nodes labeled <span class="strong"><em class="calibre9">Ci</em></span> are known as <span class="strong"><strong class="calibre2">latent states</strong></span>, <span class="strong"><strong class="calibre2">hidden states</strong></span>, or merely <span class="strong"><strong class="calibre2">states</strong></span>, as they are typically nodes that are not observable. The nodes <a id="id782" class="calibre1"/>labeled <span class="strong"><em class="calibre9">Oi</em></span> are <span class="strong"><strong class="calibre2">observed states</strong></span> or <span class="strong"><strong class="calibre2">observations</strong></span>. We will use <a id="id783" class="calibre1"/>the terms <span class="strong"><em class="calibre9">states</em></span> and <span class="strong"><em class="calibre9">observations</em></span>.</p><p class="calibre8">Now, as this is a Bayesian network, we can immediately identify some key properties. All the observations are independent of each other given their corresponding state. Also, every state <a id="id784" class="calibre1"/>is independent of every other state earlier on in the sequence history, given the state that preceded it (which is its parent in the network). The key idea behind an HMM, therefore, is that the model moves in a linear fashion from one state to the next state.</p><p class="calibre8">In each <a id="id785" class="calibre1"/>latent state, it produces an observation, which is also known as an <span class="strong"><strong class="calibre2">emitted symbol</strong></span>. These symbols are the observed part of the sequence. Hidden Markov models are very common in natural language processing, and a good example <a id="id786" class="calibre1"/>is their application to <span class="strong"><strong class="calibre2">part of speech tagging</strong></span>. The task of a part of speech tagger is to read a sentence and return the sequence of corresponding part of speech labels for the words in that sentence. For example, given the previous sentence, a part of speech tagger might return <span class="strong"><em class="calibre9">determiner</em></span> for the word <span class="strong"><em class="calibre9">The</em></span>, <span class="strong"><em class="calibre9">singular noun</em></span> for the word <span class="strong"><em class="calibre9">task</em></span>, and so on.</p><p class="calibre8">To model this using an HMM, we would have the words be the emitted symbols, and the part of speech tags be the latent states, as the former are observable and the latter are what we <a id="id787" class="calibre1"/>want to determine. There are many other sequence labeling tasks in natural language processing to which Hidden Markov models have been applied, such as <span class="strong"><strong class="calibre2">named entity recognition</strong></span>, where the goal is to identify the words in a sentence that refer to names of individuals, locations, organizations, and other entities.</p><p class="calibre8">A Hidden Markov model is comprised of five core components. The first of these is the set of possible latent class labels. For the part of speech tagger example, this might be a list of all the part of speech tags that we will use. The second component is the set of all possible emitted symbols. For an English part of speech tagger, this is the dictionary of English words.</p><p class="calibre8">The next <a id="id788" class="calibre1"/>three components involve probabilities. The <span class="strong"><strong class="calibre2">starting probability vector</strong></span> is a vector of probabilities <a id="id789" class="calibre1"/>that tells us the probability of starting in each latent state. For part of speech tagging, we may, for example, have a high probability of starting <a id="id790" class="calibre1"/>with a determiner such as <span class="strong"><em class="calibre9">the</em></span>. The <span class="strong"><strong class="calibre2">transition probability matrix</strong></span> is a matrix that tells us the probability of going to state <span class="strong"><em class="calibre9">C<sub class="calibre14">j</sub></em></span> when the current state is <span class="strong"><em class="calibre9">C<sub class="calibre14">i</sub></em></span>. Thus, this contains the probability of moving from a determiner to a noun for <a id="id791" class="calibre1"/>our part of speech example. Finally, the <span class="strong"><strong class="calibre2">emission probability matrix</strong></span> tells us the probability of emitting every symbol in our dictionary for every state that we can be in. Note that some words (such as <span class="strong"><em class="calibre9">bank</em></span>, which is both a noun and a verb) can be labeled with more than one part of speech tag, and so will have nonzero probabilities of being emitted from more than one state.</p><p class="calibre8">In circumstances such as part of speech tagging, we usually have a collection of labeled sequences <a id="id792" class="calibre1"/>so that our data contains both sequences of observations as well as their corresponding states. In this case, similar to the Naïve Bayes model, we use relative frequency counts to populate the probability components of our model.</p><p class="calibre8">For example, to find a suitable starting probability vector, we could tabulate the starting state for every sequence in our dataset and use this to get the relative frequency of beginning in each state. When all we have are unlabeled sequences, the task is significantly harder because we might not even know how many states we need to include in our model. One method to assign states to unlabeled observation sequences in training data is known as the <span class="strong"><strong class="calibre2">Baum-Welch algorithm</strong></span>.</p><p class="calibre8">Once we <a id="id793" class="calibre1"/>know the parameters of our model, the question becomes how to predict the most likely sequence of states behind a sequence of observations. Given an unlabeled sentence in English, a part of speech tagger based on an HMM must predict the sequence of part of speech labels. The most commonly used algorithm for this is <a id="id794" class="calibre1"/>based on a programming technique known as <span class="strong"><strong class="calibre2">dynamic programming</strong></span> and is known as the <span class="strong"><strong class="calibre2">Viterbi algorithm</strong></span>.</p><p class="calibre8">The algorithms we have discussed for the Hidden Markov model are beyond the scope of this book, but are quite intuitive and well worth studying. Given a basic understanding of the core components of the model and its assumptions, our next goal is to see how we can apply them to some real-world situations. We will first see an example with labeled sequences and later, an example with unlabeled sequences.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="tip19" class="calibre1"/>Tip</h3><p class="calibre8">Perhaps the most definitive and thorough introduction to Hidden Markov models is the seminal paper titled <span class="strong"><em class="calibre9">A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition</em></span>, <span class="strong"><em class="calibre9">L. R. Rabiner</em></span>, published in the <span class="strong"><em class="calibre9">Proceedings of the IEEE, 1989</em></span>. The <span class="strong"><em class="calibre9">Jurafsky</em></span> and <span class="strong"><em class="calibre9">Martin</em></span> textbook we mentioned earlier is also an ideal reference to learn more about HMMs, including details on the Baum-Welch and Viterbi algorithms, as well as applications such as part of speech tagging and named entity recognition.</p></div></div></div>

<div class="book" title="The Na&#xEF;ve Bayes classifier">
<div class="book" title="Predicting promoter gene sequences"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch10lvl2sec89" class="calibre1"/>Predicting promoter gene sequences</h2></div></div></div><p class="calibre8">The first <a id="id795" class="calibre1"/>application we will study in detail comes from the field of biology. There, we learn that the basic <a id="id796" class="calibre1"/>building blocks of DNA molecules are actually four fundamental molecules known as <span class="strong"><strong class="calibre2">nucleotides</strong></span>. These are called <span class="strong"><em class="calibre9">Thymine</em></span>, <span class="strong"><em class="calibre9">Cytosine</em></span>, <span class="strong"><em class="calibre9">Adenine</em></span>, and <span class="strong"><em class="calibre9">Guanine</em></span>, and it is the order in which these molecules appear in a DNA strand that encodes the genetic information carried by the DNA.</p><p class="calibre8">An interesting <a id="id797" class="calibre1"/>problem in molecular biology is finding <span class="strong"><strong class="calibre2">promoter sequences</strong></span> within a larger DNA strand. These are special sequences of nucleotides that <a id="id798" class="calibre1"/>play an important role in regulating a genetic process known as <span class="strong"><strong class="calibre2">gene transcription</strong></span>. This is the first step in the mechanism by which information in the DNA is read.</p><p class="calibre8">The <span class="strong"><em class="calibre9">molecular biology (promoter gene sequences) dataset</em></span>, hosted by the UCI Machine Learning repository at <a class="calibre1" href="https://archive.ics.uci.edu/ml/datasets/Molecular+Biology+(Promoter+Gene+Sequences)">https://archive.ics.uci.edu/ml/datasets/Molecular+Biology+(Promoter+Gene+Sequences)</a>, contains <a id="id799" class="calibre1"/>a number of gene sequences from DNA belonging to the bacterium <span class="strong"><em class="calibre9">E. Coli</em></span>.</p><p class="calibre8">The predictive task at hand is to build a model that will discern promoter gene sequences from non-promoter gene sequences. We will approach this problem using HMMs. Specifically, we will build an HMM for promoters and an HMM for non-promoters, and we will pick the model that gives us the highest probability for a test sequence in order to label that sequence:</p><div class="informalexample"><pre class="programlisting">&gt; promoters &lt;- read.csv("promoters.data", header = F, dec = ",", 
               strip.white = TRUE, stringsAsFactors = FALSE)
&gt; promoters[1,]
  V1  V2                                                        V3
1  + S10 tactagcaatacgcttgcgttcggtggttaagtatgtataatgcgcgggcttgtcgt</pre></div><p class="calibre8">Note that it is important to strip whitespace using the <code class="email">strip.white = TRUE</code> parameter setting in the call to <code class="email">read.csv()</code>, as some fields have leading tab characters. The first column in the data frame contains a <code class="email">+</code> or <code class="email">-</code> to denote promoters or non-promoters respectively. The second column is just an identifier for the particular sequence and the third column is the sequence of nucleotides itself. We'll begin by separating the data into positive and negative observations of promoter sequences using the first column:</p><div class="informalexample"><pre class="programlisting">&gt; positive_observations &lt;- subset(promoters, V1 == '+', 3)
&gt; negative_observations &lt;- subset(promoters, V1 == '-', 3)</pre></div><p class="calibre8">In order to train our HMMs, we want to concatenate all the observations from each class into a single observation. We do, however, want to store information about the start and end of each sequence. Consequently, we will prepend each sequence with the character <code class="email">S</code> to denote the start of a sequence and append each sequence with the character <code class="email">X</code> to denote the end of a sequence:</p><div class="informalexample"><pre class="programlisting">&gt; positive_observations &lt;- sapply(positive_observations, 
                           function(x) paste("S", x, "X", sep=""))
&gt; negative_observations &lt;- sapply(negative_observations, 
                           function(x) paste("S", x, "X", sep=""))
&gt; positive_observations[1]
[1] "StactagcaatacgcttgcgttcggtggttaagtatgtataatgcgcgggcttgtcgtX"</pre></div><p class="calibre8">Next, we <a id="id800" class="calibre1"/>will split each observation from a string into a vector of characters using the <code class="email">strsplit()</code> function, which takes a string to split as the first argument and the character to use as the split points (delimiter). Here, we use an empty character on which to split, so that the whole string is broken up into single characters:</p><div class="informalexample"><pre class="programlisting">&gt; positive_observations &lt;- strsplit(positive_observations, "")
&gt; negative_observations &lt;- strsplit(negative_observations, "")
&gt; head(positive_observations[[1]], n = 15)
 [1] "S" "t" "a" "c" "t" "a" "g" "c" "a" "a" "t" "a" "c" "g" "c"</pre></div><p class="calibre8">Now we have to specify the probability matrices for the HMMs that we want to train. In this particular situation, the states have a one-to-one correspondence with the emitted symbols, so in fact this type of problem can be simplified to a visible Markov model, which in this case is just a Markov chain. Nonetheless, the process we will follow for modeling this problem as an HMM is the same that we would follow in the more general case of having multiple symbols assigned to each state. We are going to assume that both positive and negative HMMs involve four states corresponding to the four nucleotides. Although both models will emit the same symbols in each state, they will differ in their transition probabilities from one state to the next.</p><p class="calibre8">Apart from the four states we mentioned earlier, we created a special terminating state at the end of each sequence using the symbol <span class="strong"><em class="calibre9">X</em></span>. We also created a special starting state, which we called <span class="strong"><em class="calibre9">S</em></span>, so that the starting probability of all the other states is 0. In addition, the emission probabilities are trivial to compute as only one symbol is emitted per state. Due to the one-to-one correspondence between states and symbols, we will use the same alphabet to represent the states and the symbols they emit:</p><div class="informalexample"><pre class="programlisting">&gt; states &lt;- c("S", "X", "a", "c", "g", "t")
&gt; symbols &lt;- c("S", "X", "a", "c", "g", "t")
&gt; startingProbabilities &lt;- c(1,0,0,0,0,0)
&gt; emissionProbabilities &lt;- diag(6)
&gt; colnames(emissionProbabilities) &lt;- states
&gt; rownames(emissionProbabilities) &lt;- symbols
&gt; emissionProbabilities
  S X a c g t
S 1 0 0 0 0 0
X 0 1 0 0 0 0
a 0 0 1 0 0 0
c 0 0 0 1 0 0
g 0 0 0 0 1 0
t 0 0 0 0 0 1</pre></div><p class="calibre8">Computing the transition probability matrix requires us to do a bit more work. Thus, we defined our own function for this: <code class="email">calculateTransitionProbabilities()</code>. The input to this function is a single vector of training sequences concatenated with each other, along with a vector containing the names of the states.</p><p class="calibre8">The function <a id="id801" class="calibre1"/>first computes an empty transition probability matrix. By cycling over each consecutive pair of states, it tallies up counts of state transitions. After all the data has been traversed, we normalize the transition probability matrix by dividing each row of the matrix by the sum of the elements in that row. This is done because the rows of this matrix must sum to one. We use the <code class="email">sweep()</code> function, which allows us to apply a function on every element of the matrix using a summary statistic. Here is <code class="email">calculateTransitionProbabilities()</code>:</p><div class="informalexample"><pre class="programlisting">calculateTransitionProbabilities &lt;- function(data, states) {
  transitionProbabilities &lt;- matrix(0, length(states), length(states))
  colnames(transitionProbabilities) &lt;- states
  rownames(transitionProbabilities) &lt;- states
  for (index in 1:(length(data) - 1)) {
    current_state &lt;- data[index]
    next_state &lt;- data[index + 1]
    transitionProbabilities[current_state, next_state] &lt;- 
           transitionProbabilities[current_state, next_state] + 1
  }
  transitionProbabilities &lt;- sweep(transitionProbabilities, 1, 
           rowSums(transitionProbabilities), FUN = "/")
  return(transitionProbabilities)
}</pre></div><p class="calibre8">Now we are ready to train our models. The key observation to make on this dataset is that we have very few observations, just 53 of each class in fact. This dataset is too small to set a portion aside for testing. Instead, we will implement leave-one-out cross validation to estimate the accuracy of our models. To do this, we will begin by leaving an observation out from the positive observations. This leaves all the negative observations available for computing the transition probability matrix for our negative HMM:</p><div class="informalexample"><pre class="programlisting">&gt; negative_observation&lt;-Reduce(function(x, y) c(x, y), 
                               negative_observations, c())
&gt; (transitionProbabilitiesNeg &lt;- 
   calculateTransitionProbabilities(negative_observation, states))
  S          X         a         c         g         t
S 0 0.00000000 0.2264151 0.2830189 0.1320755 0.3584906
X 1 0.00000000 0.0000000 0.0000000 0.0000000 0.0000000
a 0 0.02168022 0.2113821 0.2696477 0.2506775 0.2466125
c 0 0.01256983 0.2500000 0.1634078 0.2667598 0.3072626
g 0 0.01958225 0.3133159 0.2480418 0.1919060 0.2271540
t 0 0.01622971 0.1885144 0.2434457 0.2946317 0.2571785</pre></div><p class="calibre8">When in <a id="id802" class="calibre1"/>the start state (<code class="email">S</code>), we can randomly move to a nucleotide state, but have zero probability of moving to the stop state (<code class="email">X</code>) or staying in the start state. When in the nucleotide states, we can randomly transition to any state except back to the start state. Finally, the only valid transition from a stop state is to the start state for a new sequence.</p><p class="calibre8">We now introduce the <code class="email">HMM</code> package in R, which is for working with Hidden Markov models, as the name implies. We can initialize an HMM with a specific set of parameters using the <code class="email">initHMM()</code> function. As expected, this takes five inputs corresponding to the five components of a Hidden Markov model, which we discussed earlier:</p><div class="informalexample"><pre class="programlisting">&gt; library("HMM")
&gt; negative_hmm &lt;- initHMM(states, symbols, startProbs = 
  startingProbabilities, transProbs = transitionProbabilitiesNeg,
  emissionProbs = emissionProbabilities)</pre></div><p class="calibre8">The next step is to build the positive HMM, but we will have to do this multiple times, leaving out one observation for testing. This test observation will then be processed by the negative HMM we trained earlier and the positive HMM that was trained without that observation. If the positive HMM predicts a higher probability for the test observation than the negative HMM, our model will correctly classify the test observation. The following block of code performs a loop of these calculations for every positive observation:</p><div class="informalexample"><pre class="programlisting">&gt; incorrect &lt;- 0
&gt; for (obs in 1 : length(positive_observations)) {
     positive_observation &lt;- Reduce(function(x, y) c(x, y), 
                             positive_observations[-obs], c())
     transitionProbabilitiesPos  &lt;- 
    calculateTransitionProbabilities(positive_observation, states)
     positive_hmm &lt;- initHMM(states, symbols, 
                          startProbs = startingProbabilities, 
                          transProbs = transitionProbabilitiesPos, 
                          emissionProbs = emissionProbabilities)
     test_observation &lt;- positive_observations[[obs]]
     final_index &lt;- length(test_observation)
     pos_probs &lt;- exp(forward(positive_hmm, test_observation))
     neg_probs &lt;- exp(forward(negative_hmm, test_observation))
     pos_seq_prob &lt;- sum(pos_probs[, final_index])
     neg_seq_prob &lt;- sum(neg_probs[, final_index])
     if (pos_seq_prob &lt; neg_seq_prob) incorrect &lt;- incorrect + 1
 }</pre></div><p class="calibre8">We'll now walk through the previous code block. Firstly, we keep track of any mistakes we make using the <code class="email">incorrect</code> variable. For every observation in our positive observations list, we'll train a positive HMM without this observation. This observation then becomes our test observation.</p><p class="calibre8">To find <a id="id803" class="calibre1"/>the probability of a particular sequence given a particular HMM, we used the <code class="email">forward()</code> function, which computes a matrix containing the logarithm of all the forward probabilities for every step in the observation sequence. The final column in this matrix, whose numerical index is just the length of the sequence, contains the forward probability for the whole sequence. We compute the positive sequence probability using the positive HMM that we trained and use the <code class="email">exp()</code> function to undo the logarithm operation (although not strictly necessary in this case, where we just need a comparison). We repeat this for the negative sequence probability using the negative HMM. As our test observation was one of the positive observations, we will misclassify only if the negative sequence probability is greater than the positive sequence probability. After our code block completes its execution, we can see how many mistakes we have made:</p><div class="informalexample"><pre class="programlisting">&gt; incorrect
[1] 13</pre></div><p class="calibre8">This means that out of the 53 positive observations, we misclassified 13 and correctly classified 40. We are not done yet, though, as we need to do a similar loop with the negative observations. This time, we will train a positive HMM once with all the positive observations:</p><div class="informalexample"><pre class="programlisting">&gt; positive_observation &lt;- Reduce(function(x, y) c(x, y), 
                                 positive_observations, c())
&gt; transitionProbabilitiesPos  &lt;- 
  calculateTransitionProbabilities(positive_observation, states)
&gt; positive_hmm = initHMM(states, symbols, startProbs = 
  startingProbabilities, transProbs = transitionProbabilitiesPos, 
  emissionProbs = emissionProbabilities)</pre></div><p class="calibre8">Next, we are going to iterate over all the negative observations. We will train a negative model by leaving one observation out as the test observation. We will then process this observation with both the positive HMM we just trained and the negative HMM trained without this observation in its training data.</p><p class="calibre8">Finally, we will compare the predicted sequence probability for this test observation produced by <a id="id804" class="calibre1"/>the two HMMs and classify the test observation according to which model produced the higher probability. In essence, we are doing exactly the same process as we did earlier when we were iterating over the positive observations. The following code block will continue to update our <code class="email">incorrect</code> variable and should be self-explanatory:</p><div class="informalexample"><pre class="programlisting">&gt; for (obs in 1:length(negative_observations)) {
     negative_observation&lt;-Reduce(function(x, y) c(x, y), 
                           negative_observations[-obs], c())
     transitionProbabilitiesNeg &lt;- 
    calculateTransitionProbabilities(negative_observation, states)
     negative_hmm &lt;- initHMM(states, symbols, 
                         startProbs = startingProbabilities, 
                         transProbs = transitionProbabilitiesNeg, 
                         emissionProbs = emissionProbabilities)
     test_observation &lt;- negative_observations[[obs]]
     final_index &lt;- length(test_observation)
     pos_probs &lt;- exp(forward(positive_hmm,test_observation))
     neg_probs &lt;- exp(forward(negative_hmm,test_observation))
     pos_seq_prob &lt;- sum(pos_probs[, final_index])
     neg_seq_prob &lt;- sum(neg_probs[, final_index])
     if (pos_seq_prob &gt; neg_seq_prob) incorrect &lt;- incorrect+1
 }</pre></div><p class="calibre8">The overall number of misclassifications in the cross-validation is stored in the <code class="email">incorrect</code> variable:</p><div class="informalexample"><pre class="programlisting">&gt; incorrect
[1] 25
&gt; (cross_validation_accuracy &lt;- 1 - (incorrect/nrow(promoters)))
 [1] 0.7641509</pre></div><p class="calibre8">Our overall cross-validation accuracy is roughly 76 percent. Given that we are using the leave-one-out approach, and that the overall size of the training data is so small, we expect this estimate to have a relatively high variance.</p><p class="calibre8">In our <a id="id805" class="calibre1"/>HMM, the Markov property essentially makes the assumption that only the previous nucleotide determines the choice of the next nucleotide in the sequence. We can reasonably expect that there are longer-range dependencies at work and, as a result, we are limited <a id="id806" class="calibre1"/>in accuracy by the assumptions of our model. For this reason, there are models, such as the <span class="strong"><strong class="calibre2">Trigram HMM</strong></span>, that take into account additional states in the past other than the current state.</p><p class="calibre8">In the next section, we will study an example where we train a Hidden Markov model using unlabeled data. We will manually define the number of hidden states and use the Baum-Welch algorithm to train an HMM while estimating both state transitions and emissions.</p></div></div>

<div class="book" title="The Na&#xEF;ve Bayes classifier">
<div class="book" title="Predicting letter patterns in English words"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch10lvl2sec90" class="calibre1"/>Predicting letter patterns in English words</h2></div></div></div><p class="calibre8">In this <a id="id807" class="calibre1"/>section, we will model the patterns of letters that form English words. Beyond having different words, and sometimes alphabets, languages differ from each other in the patterns of letters that are used to form words. English words have a characteristic distribution of letters and letter sequences and, in this section, we will try to model the process of word formation in a very simplistic way by using a Hidden Markov model.</p><p class="calibre8">The emitted symbols of our model will be the letters themselves but this time, we don't know what the states could be as we are using unlabeled data. For this reason, we are going to provide just the number of states that we want our model to have, and then use the Baum-Welch algorithm to train the parameters of our HMM.</p><p class="calibre8">All we need for this task is a corpus of text in English. Earlier in this chapter, we studied movie reviews with the Naïve Bayes classifier, so we will use these for convenience, although other sources of English text could be used as well. We shall begin by reloading our movie reviews and will use the <code class="email">tm</code> package to transform them all to lowercase:</p><div class="informalexample"><pre class="programlisting">&gt; library("tm")
&gt; nb_pos &lt;- Corpus(DirSource(path_to_pos_folder), 
                   readerControl = list(language = "en"))
&gt; nb_neg &lt;- Corpus(DirSource(path_to_neg_folder), 
                  readerControl = list(language = "en"))
&gt; nb_all &lt;- c(nb_pos, nb_neg, recursive = T)
&gt; nb_all &lt;- tm_map(nb_all, content_transformer(tolower))</pre></div><p class="calibre8">Next, we will read the text from every review and collect these in a single vector:</p><div class="informalexample"><pre class="programlisting">&gt; texts &lt;- sapply(1 : length(nb_all), function(x) nb_all[[x]])</pre></div><p class="calibre8">To simplify our task, aside from the individual letters, we will consider a category with all the whitespace characters (spaces, tabs, and so on) and represent these with the uppercase letter <code class="email">W</code>. We will do the same for numerical digits with the uppercase character <code class="email">N</code>, all punctuation marks with the uppercase character <code class="email">P</code>, and use the uppercase character <code class="email">O</code> for anything that is left. We use regular expressions for this:</p><div class="informalexample"><pre class="programlisting">&gt; texts &lt;- sapply(texts, function(x) gsub("\\s", "W", x))
&gt; texts &lt;- sapply(texts, function(x) gsub("[0-9]", "N", x))
&gt; texts &lt;- sapply(texts, function(x) gsub("[[:punct:]]", "P", x))
&gt; texts &lt;- sapply(texts, function(x) gsub("[^a-zWNP]", "O", x))</pre></div><p class="calibre8">Once we have transformed all our text, we'll pick out a sample and split each review into characters. The sequences of characters from each review will then be concatenated with each <a id="id808" class="calibre1"/>other to create one long character sequence. This works quite well in this context as the corpus of reviews contains complete sentences and concatenating them amounts to joining up complete sentences. We've chosen to use a sample of 100 movie reviews. We can use more, but the time taken to train the model would be longer:</p><div class="informalexample"><pre class="programlisting">&gt; big_text_splits &lt;- lapply(texts[1:100], 
                            function(x) strsplit(x, ""))
&gt; big_text_splits &lt;- unlist(big_text_splits, use.names = F)</pre></div><p class="calibre8">Next, we'll want to initialize our HMM. In this example, we'll consider a model with three states, which we'll arbitrarily name <code class="email">s1</code>, <code class="email">s2</code>, and <code class="email">s3</code>. For emitted symbols, we have the lowercase alphabet and the four uppercase characters that, as we saw earlier, are being used to represent four special character categories such as numbers. R holds a vector of lowercase letters in the variable <code class="email">letters</code>, which is very convenient for us:</p><div class="informalexample"><pre class="programlisting">&gt; states &lt;- c("s1", "s2", "s3")
&gt; numstates &lt;- length(states)
&gt; symbols &lt;- c(letters, "W", "N", "P", "O")
&gt; numsymbols &lt;- length(symbols)</pre></div><p class="calibre8">Next, we'll create random starting, emission, and transmission probability matrices. We'll generate random entries in the [0,1] interval using the <code class="email">runif()</code> function. We will need to normalize every row in these matrices in order to ensure that the entries correspond to probabilities. To achieve this, we'll use the <code class="email">sweep()</code> function as we did earlier:</p><div class="informalexample"><pre class="programlisting">&gt; set.seed(124124) 
&gt; startingProbabilities &lt;- matrix(runif(numstates), 1, numstates)
&gt; startingProbabilities &lt;- sweep(startingProbabilities, 1, 
                           rowSums(startingProbabilities), FUN = "/")
&gt; set.seed(454235) 
&gt; transitionProbabilities &lt;- matrix(runif(numstates * numstates), 
                      numstates, numstates)
&gt; transitionProbabilities &lt;- sweep(transitionProbabilities, 1, 
                      rowSums(transitionProbabilities), FUN = "/")
&gt; set.seed(923501) 
&gt; emissionProbabilities &lt;- matrix(runif(numstates * numsymbols), 
                      numstates, numsymbols)
&gt; emissionProbabilities &lt;- sweep(emissionProbabilities, 1, 
                      rowSums(emissionProbabilities), FUN = "/")</pre></div><p class="calibre8">We now <a id="id809" class="calibre1"/>initialize and train the HMM using the large character sequence we obtained earlier. This will take several minutes to run depending on the computational resources available, and this is the main reason we drew only a sample of the text earlier on:</p><div class="informalexample"><pre class="programlisting">&gt; hmm &lt;- initHMM(states, symbols,  startProbs =  
      startingProbabilities, transProbs = transitionProbabilities, 
      emissionProbs = emissionProbabilities)
&gt; hmm_trained &lt;- baumWelch(hmm, big_text_splits)</pre></div><p class="calibre8">We trained our model in a completely unsupervised way by simply providing it with character sequences. We don't have a meaningful test dataset on which to assess the performance of our model; rather, this exercise is worthwhile in that it produces an HMM that has interesting properties. It is instructive to take a peek at the symbol emission probabilities for each state. These are accessible via the <code class="email">hmm$emissionProbs</code> attribute on the <code class="email">hmm_trained</code> object:</p><div class="mediaobject"><img src="../images/00182.jpeg" alt="Predicting letter patterns in English words" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Let's examine <a id="id810" class="calibre1"/>these states carefully. All states have a relatively high probability of emitting a whitespace character. State 3 is very interesting as, besides whitespace, it seems to have grouped together punctuation and vowels. The HMM has successfully managed to group together the letters <span class="strong"><em class="calibre9">a</em></span>, <span class="strong"><em class="calibre9">e</em></span>, <span class="strong"><em class="calibre9">i</em></span>, <span class="strong"><em class="calibre9">o</em></span>, and <span class="strong"><em class="calibre9">u</em></span> in the same category without any prior information about the English language.</p><p class="calibre8">This state also emits two consonants with a noticeable probability. The consonant <span class="strong"><em class="calibre9">y</em></span> is emitted, which we know occasionally does behave like a vowel in words such as <span class="strong"><em class="calibre9">rhythm</em></span> and <span class="strong"><em class="calibre9">phylum</em></span>, for example. The consonant <span class="strong"><em class="calibre9">s</em></span> is also emitted, and because it is often used to form the plural of nouns, we find this at the end of words just like punctuation marks. So, we see that this state seems to have grouped two main themes.</p><p class="calibre8">By contrast, state 1 tends to emit consonants and not vowels. In fact, only the vowel <span class="strong"><em class="calibre9">u</em></span> seems to have a small probability of being emitted from this state. State 2 has a mix of vowels and consonants, but it is the only state in which the consonant <span class="strong"><em class="calibre9">h</em></span> has a high probability. This is very interesting, as <span class="strong"><em class="calibre9">h</em></span> is another letter of the alphabet that has vowel-like properties in pronunciation (it is often silent or part of a diphthong). We can learn more by examining the transition probabilities between the states:</p><div class="informalexample"><pre class="programlisting">&gt; (trained_transition_probabilities &lt;- hmm_trained$hmm$transProbs)
    to
from           s1           s2         s3
  s1 1.244568e-01 5.115204e-01 0.36402279
  s2 7.739387e-05 2.766151e-01 0.72330746   s3 9.516911e-01 5.377194e-06 0.04830349</pre></div><p class="calibre8">Again, we can discover a wealth of interesting properties. For example, when we are in state 3, the vowel state, we have a 95 percent chance of going to state 1, the consonant state. This is quite intuitive, in that English rarely has consecutive vowels. When we are in state 1, we have a 36 percent chance of going to the vowel state and a 51 percent chance of going to state 2.</p><p class="calibre8">Now we <a id="id811" class="calibre1"/>can begin to understand what state 2 represents. It primarily represents the state that emits the second consonant when we have two consecutive consonants. This is why the letter <span class="strong"><em class="calibre9">h</em></span> has such a high probability in this state, as it participates in very common diphthongs, such as <span class="strong"><em class="calibre9">ch</em></span>, <span class="strong"><em class="calibre9">sh</em></span>, and <span class="strong"><em class="calibre9">th</em></span>, the latter of course being found in very frequent words such as <span class="strong"><em class="calibre9">the</em></span>. From this state, the most common successor state, with 72 percent probability, is the vowel state, as expected after two consecutive consonants.</p><p class="calibre8">This experiment is worth repeating with different conditions. If we use different seeds or sample a different number of movie reviews, we may see different results, as the Baum-Welch algorithm is sensitive to initial conditions and is unsupervised. Specifically, our Hidden Markov model might learn a completely different set of states.</p><p class="calibre8">For example, on some iterations, we noticed that all punctuation and numerical digits are grouped into one state, another state becomes the vowel state, and the third state is a pure consonant state. We can reproduce this behavior if, in the previous code, we sample 40 texts and use the numbers 1816, 1817, and 1818 for the three seeds. There are many more possibilities—some of which are easier to interpret than others.</p><p class="calibre8">Another <a id="id812" class="calibre1"/>parameter that is worth varying here is the number of states. If we use two states, then the split tends to be between vowels and consonants. If we increase the number of states, we will often continue to find results that are interpretable for as many as 10 states. Hidden Markov models are often also referred to as <span class="strong"><strong class="calibre2">generative models</strong></span> because we can use them to generate examples of states and observations once they have been trained. We can do this with the <code class="email">simHMM()</code> function by providing our model and the length of the sequence we want to generate:</p><div class="informalexample"><pre class="programlisting">&gt; set.seed(987987)
&gt; simHMM(hmm_trained$hmm, 30)
$states
 [1] "s2" "s3" "s1" "s3" "s3" "s1" "s3" "s3" "s1" "s1" "s2" "s3"
[13] "s3" "s1" "s2" "s3" "s1" "s2" "s2" "s2" "s3" "s1" "s2" "s2"
[25] "s3" "s1" "s2" "s3" "s1" "s2"
$observation
 [1] "h" "o" "P" "P" "a" "n" "W" "i" "r" "r" "h" "e" "i" "n" "h"
[16] "o" "n" "l" "W" "h" "e" "s" "t" "W" "e" "t" "c" "e" "P" "W"</pre></div><p class="calibre8">As a final <a id="id813" class="calibre1"/>point, we can download and use the <code class="email">markovchain</code> package, take our learned transition probability matrix, and find out in the long run how much time our model spends in each state. This is done <a id="id814" class="calibre1"/>using a <span class="strong"><strong class="calibre2">steady state calculation</strong></span>, the mathematics of which we will not explore in this book. Thankfully, the <code class="email">markovchain</code> package has a simple way to initialize a Markov chain when we know the probabilities that are involved. It does this by using the <code class="email">simpleMc()</code> function, and we can use the <code class="email">steadyStates()</code> function on our Markov chain to find out the steady state distribution:</p><div class="informalexample"><pre class="programlisting">&gt; library("markovchain")
&gt; simpleMc&lt;-new("markovchain", states = c("s1", "s2", "s3"), 
                transitionMatrix = trained_transition_probabilities, 
                name = "simpleMc")
&gt; steadyStates(simpleMc)
            s1       s2        s3
[1,] 0.3806541 0.269171 0.3501748</pre></div><p class="calibre8">In the long term, we spend 38 percent of our time in state 1, the first consonant state; 27 percent in state 2, the second consonant state; and 35 percent of our time in state 3, the main vowel state.</p></div></div>
<div class="book" title="Summary" id="2D7TI1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch10lvl1sec74" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">In this chapter, we introduced ourselves to one of the very active areas of research in machine learning, namely the field of probabilistic graphical models. These models involve using a graphical structure to encode conditional independence relations between random variables. We saw how Bayes' theorem, a very simple formula that essentially tells us how we can predicate cause by observing effect, can be used to build a simple classifier known as the Naïve Bayes classifier. This is a simple model where we are trying to predict an output class that best explains a set of observed features, all of which are assumed to be independent of each other given the output class.</p><p class="calibre8">We used this model to predict user sentiment on a set of movie reviews where the features were the words that were present in the reviews. Although we obtained reasonable accuracy, we found that the assumptions in our model are quite strict and prevent us from doing substantially better. Often, a Naïve Bayes model is built during the modeling process to provide us with a baseline performance that we know we should exceed with more sophisticated models.</p><p class="calibre8">We also studied Hidden Markov models, which are models typically used to label and predict sequences. Every position in the sequence is comprised of a hidden state and an observation emitted from that state. The key assumption of the model is that every state is independent of the entire sequence history, given the state that immediately preceded it. In addition, all observations are independent of each other as well as all other states in the sequence, given the state from which they were emitted.</p><p class="calibre8">When we have labeled sequences, we can train a Hidden Markov model by using state transition and symbol emission counts obtained from the data itself. It is also possible to train an unsupervised HMM using a very smart algorithm known as the Baum-Welch algorithm. Even though we did not dive into the algorithmic details, we saw an example of how this works in practice by training an HMM on sequences of characters in English words.</p><p class="calibre8">From this, we saw that the resulting model picked up on some interesting properties of language. Incidentally, even though we did not mention it, it is also possible to train a Naïve Bayes model with missing class labels, this time using the <span class="strong"><strong class="calibre2">EM algorithm</strong></span>. Despite also having relatively strict independence assumptions, HMMs are quite powerful and have been successfully applied to a wide variety of applications from speech processing to molecular biology.</p><p class="calibre8">In the next chapter, we will look at analyzing and making predictions on time series. Many real-world applications involve taking measurements over a particular period of time and using them to make predictions about the future. For example, we might want to predict tomorrow's weather based on the weather today, or tomorrow's stock market index based on market fluctuations over the past few weeks.</p></div></body></html>