- en: 'Chapter 8: Use Case – Customer Churn Prediction'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we discussed the alternatives to the **Feast** feature
    store available on the market. We looked at a few feature store offerings from
    cloud providers that are part of **Machine Learning** (**ML**) platform offerings,
    namely, SageMaker, Vertex AI, and Databricks. We also looked at a couple of other
    vendors that offer managed feature stores that can be used with your cloud provider,
    namely, Tecton and Hopsworks, of which Hopsworks is also open source. To get a
    feel for a managed feature store, we tried out an exercise on the SageMaker Feature
    Store and also briefly discussed ML best practices.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss an end-to-end use case of customer churn using
    a telecom dataset. We will walk through data cleaning, feature engineering, feature
    ingestion, model training, deployment, and monitoring. For this exercise, we will
    use a managed feature store – Amazon SageMaker. The reason for choosing SageMaker
    over other alternatives that we discussed in the last chapter is simply the easy
    accessibility to the trial version of the software.
  prefs: []
  type: TYPE_NORMAL
- en: The aim of this chapter is to go through a customer churn prediction ML use
    case, step by step, using a managed feature store. This should give you an idea
    of how it differs from self-managed feature stores and also basic feature monitoring
    and model monitoring aspects the feature store helps with.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will discuss the following topics in order:'
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure setup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to the problem and the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data processing and feature engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature group definitions and ingestion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run through the examples and to get a better understanding of this chapter,
    an understanding of the topics covered in previous chapters will be useful but
    is not required. To follow the code examples in this chapter, you need familiarity
    with a notebook environment, which could be a local setup such as the Jupyter
    Notebook or an online notebook environment such as Google Colab, Kaggle, or SageMaker.
    You will also need an AWS account with full access to SageMaker and the Glue console.
    You can create a new account and use all the services for free during the trial
    period. You can find the code examples of the book at the following GitHub link:'
  prefs: []
  type: TYPE_NORMAL
- en: https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter08
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the exercises in this chapter, we will need an S3 bucket to store data,
    an IAM role, and an IAM user that has access to both the SageMaker Feature Store
    and the S3 bucket. Since we have already gone through creating all these resources,
    I will skip through this. Please refer to [*Chapter 4*](B18024_04_ePub.xhtml#_idTextAnchor065),
    *Adding Feature Store to ML Models*, for S3 bucket creation, and [*Chapter 7*](B18024_07_ePub.xhtml#_idTextAnchor113),
    *Feast Alternatives and ML Best Practices*, for IAM role and IAM user creation.
    That is all we need, in terms of initial setup for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: I am trying to use as few resources of AWS SageMaker as possible since it will
    incur costs if your free trial has come to an end. You can use SageMaker Studio
    for a better experience with notebooks and also the UI of the feature store.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to the problem and the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this exercise, we will use the telecom customer churn dataset, which is
    available on Kaggle at the URL https://www.kaggle.com/datasets/blastchar/telco-customer-churn.
    The aim of the exercise is to use this dataset, prepare the data for model training,
    and train an XGBoost model to predict customer churn. The dataset has 21 columns
    and the column names are self-explanatory. The following is a preview of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Telecom dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_08_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.1 – Telecom dataset
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8.1* shows the labeled telecom customer churn dataset. The `customerID`
    column is the ID of the customers. All other columns except `Churn` represent
    the set of attributes, and the `Churn` column is the target column.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's get our hands dirty and perform feature engineering next.
  prefs: []
  type: TYPE_NORMAL
- en: Data processing and feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, let''s use the telecom customer churn dataset and generate
    the features that can be used for training the model. Let''s create a notebook,
    call it `feature-engineering.ipynb`, and install the required dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Once the installation of the libraries is complete, read the data. For this
    exercise, I have downloaded the data from Kaggle and saved it in a location where
    it is accessible from the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command reads the data from S3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: If you are executing the notebook outside AWS, then set the user credentials
    using the environment variables.
  prefs: []
  type: TYPE_NORMAL
- en: If you preview that dataset, there are a few columns that need to be reformatted,
    converted into a categorical column, or have empty values removed. Let's perform
    those transformations one after the other.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `TotalCharges` column contains a few empty strings. Let''s remove the rows
    that contain empty or null values for `TotalCharges`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code block replaces all the empty strings with `np.nan` and drops
    all the rows that contain null in the `TotalCharges` column.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s look at the `tenure` column. This one has integer values that
    represent the tenure of the customer in months. Along with the value, we can also
    group the customers into three groups: short (0-24 months), mid (24-48 months),
    and long (greater than 48 months).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code adds the customer `tenure_group` column with the defined
    groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code block creates the categorical column `tenure_group`, which
    will have three values, `0-24`, `24-48`, and `48-end`, depending on the length
    of the customer tenure.
  prefs: []
  type: TYPE_NORMAL
- en: A few columns in the dataset are dependent on others. For example, `OnlineSecurity`
    depends on whether the customer has `InternetService` or not. Hence, some of these
    columns, namely, `OnlineSecurity`, `OnlineBackup`, `DeviceProtection`, `TechSupport`,
    `StreamingTV`, and `StreamingMovies` have `No internet service` as the value instead
    of `No`. Let's replace `No internet service` with `No` in those columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block performs the replacement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: We have done a set of data cleaning operations so far. Let's preview the dataset
    once before we proceed and do further transformations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code samples the `churn_data` DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code outputs a sample preview as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Churn dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_08_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.2 – Churn dataset
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in *Figure 8.2*, the dataset is clean and only has categorical
    or numerical columns. The next step is to covert these categorical values into
    numerical encoding. Let's look at the dataset and see which ones are categorical
    and which ones are numerical.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code calculates the unique values in every column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code displays the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Unique value count of every column'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_08_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.3 – Unique value count of every column
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in *Figure 8.3*, except `MonthlyCharges`, `tenure`, and `TotalCharges`,
    all other columns are categorical.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the dataset, there are binary columns and multi-value category columns.
    Let''s find out which ones are binary and which ones are multi-value columns.
    The following code block checks if the column is binary from the list of columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have the list of binary columns, let's transform them into 0s and
    1s using the label encoder.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code uses the label encoder to perform the transformation on
    the binary columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The next step is to transform the multi-value categorical columns into 0s and
    1s. To do that, let's filter out the multi-value column names first.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block selects the multi-value columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code block filters out all the categorical columns first and filters
    out the binary columns so that we are left with only the multi-value columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block transforms the multi-value columns into binary encodings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The last part is transforming the numerical values. Since numerical columns
    can have different ranges, scaling the columns to a standard range can be beneficial
    for ML algorithms. It also helps algorithms converge faster. Hence, let's scale
    the number columns to a standard range.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block uses `StandardScaler` to scale all the numerical columns
    to a standard range:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code block scales the numerical columns: `tenure`, `MonthlyCharges`,
    and `TotalCharges`. Now that our feature engineering is complete, let''s preview
    the final feature set and ingest it into the SageMaker Feature Store.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block shows the feature set preview:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code block formats the column names as lowercase and replaces
    all the separators in the string, such as spaces and hyphens, with an underscore.
    The final features are shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Feature set'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_08_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.4 – Feature set
  prefs: []
  type: TYPE_NORMAL
- en: The final feature set has 33 columns as shown in *Figure 8.4*. If you recall
    in [*Chapter 4*](B18024_04_ePub.xhtml#_idTextAnchor065), *Adding Feature Store
    to ML Models*, while creating feature definitions, we identified entities and
    grouped features based on their entities or logical groups. Though these features
    can be grouped into multiple groups, we will be creating a single feature group
    and ingesting all the features into it.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, let's create the feature definitions and ingest the data.
  prefs: []
  type: TYPE_NORMAL
- en: Feature group definitions and feature ingestion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have the feature set ready for ingestion, let's create the feature
    definitions and ingest the features into a feature store. For this exercise, as
    mentioned before, we will be using the SageMaker Feature Store. If you recall
    from the previous chapters, we always kept feature definitions in a separate notebook,
    as it is a one-time activity. In this exercise, we are going to try a different
    method, which is using a conditional statement to create a feature group if it
    doesn't exist. You can use either of the approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s continue in the same notebook and initialize the boto3 session and check
    whether our feature group exists already or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code block queries SageMaker to check whether the feature group
    with the name `telcom-customer-features` exists or not and sets a Boolean based
    on that. We will use this Boolean to either create the feature group or to skip
    creation and just ingest the data into the feature store.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block initializes the objects required for interacting with
    the SageMaker Feature Store:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Use the IAM role created in the earlier section, in the preceding code block.
    The IAM role should have **AmazonSageMakerFullAccess** and **AmazonS3FullAccess**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to initialize the `FeatureGroup` object. The following code
    initializes the feature group object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will use the Boolean that was set earlier to conditionally create the
    feature group if the feature group doesn''t exist. The following code block loads
    the feature definitions and calls `create` if the feature group doesn''t exist:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: In the `load_feature_definitions` call, if you notice, I'm loading all the feature
    definition columns except the `customerid` column and manually adding the `customerid`
    column to the feature definitions list in the following line. The reason for this
    is the `sagemaker` library fails to figure out the `string` data type as the pandas
    `dtype` for `string` is `object`.
  prefs: []
  type: TYPE_NORMAL
- en: The `create` feature group call is straightforward. I am disabling the online
    store by passing `enable_online_store` as `False`, since we will be trying out
    the batch pipeline and I will leave the online model as an exercise. Once the
    preceding code block executes, based on the conditional statement, for the first
    time, it will create the feature group and for the subsequent runs, it will skip
    the feature group creation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final step is to ingest the DataFrame. The following code block performs
    the ingestion and prints any failures:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: If you only have the batch use case, SageMaker has a Spark library that can
    be used to ingest to the offline store directly, which is also cost-effective.
  prefs: []
  type: TYPE_NORMAL
- en: That completes the feature engineering and ingestion. In the next section, let's
    look at model training.
  prefs: []
  type: TYPE_NORMAL
- en: Model training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As before, for the model training, the feature store is the source. Hence,
    let''s create our model training notebook and install and initialize the required
    objects for querying the feature store. Here is the link to the notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter08/Ch8_model_training.ipynb](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter08/Ch8_model_training.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block installs the required libraries for model training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'After installing the required libraries, initialize the SageMaker session and
    the required objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code block initializes the SageMaker session and initializes the
    feature group object. The `name` of the feature group should be the same as the
    `name` of the feature group that we created in our feature engineering notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Assign the IAM role that was created earlier to the `role` variable. Also, if
    you are running the notebook outside AWS, you need to uncomment and set up AWS
    credentials in the preceding code block.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next part is to query the historical store to generate the training data.
    Unlike Feast, we don''t need an entity DataFrame here. Instead, we use SQL queries
    to fetch the historical data. It has the same time travel capabilities as Feast
    has. For this exercise, let''s fetch the latest features for all customers, using
    a similar query to one that we used in the last chapter, during the SageMaker
    overview:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: If you recall correctly, we used a similar nested query in the last chapter.
    The preceding code block fetches all the customers and their latest features.
    The output of the query will be written to a specific S3 location as mentioned
    in the `run` API call.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the query runs successfully, the dataset can be fetched using the following
    code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Please note that, we will perform the same steps for model prediction and feature
    monitoring as we have in this section (*Model training*) from the beginning till
    the preceding code block.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding code block fetches the dataset and drops the unwanted columns.
    The fetched dataset is similar to the data shown in *Figure 8.4* with additional
    columns: `write_time`, `api_invocation_time`, `is_deleted`, and `row_num`. The
    first three are additional metadata columns added by SageMaker during ingestion
    and `row_num` is the column that we created in the query for fetching the latest
    features for every customer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the dataset, let''s split it for training and testing. The
    following code block drops the columns unwanted for training from the dataset
    and splits the data for training and testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code block leaves out the ID column and performs a 75/25 split
    for training and testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of it is straightforward, which is basically training an XGBoost model,
    parameter tuning, and comparing the performance. The following is an example code
    block for training, sample analysis, and logging the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE150]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE151]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE152]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE153]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE154]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE155]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE156]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code block also logs the model to a specific location in S3\.
    This is a crude way of doing it. It is always better to use an experiment training
    tool for logging the performance and the model.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the model training is complete, let's look at model scoring.
  prefs: []
  type: TYPE_NORMAL
- en: Model prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned in the last note in the previous section, as this is a batch model,
    the steps are similar for model scoring for fetching the data from the offline
    store. However, depending on which customers need to be scored (maybe all), you
    might filter out the dataset. Once you filter out the dataset, the rest of the
    steps are again straightforward, which is to load the model, run predictions,
    and store the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a sample code block for loading the model, running predictions,
    and also storing the results back in S3 for consumption:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE157]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE158]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE159]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE160]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE161]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE162]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE163]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE164]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE165]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE166]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE167]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE168]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code block downloads the model from S3, loads the model, scores
    it against the data fetched from the historical store, and also stores the result
    in the S3 bucket for consumption.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The library versions of XGBoost, Joblib, and scikit-learn should be the same
    as what was used while saving the model, otherwise loading of the model might
    fail.
  prefs: []
  type: TYPE_NORMAL
- en: To productionize this ML pipeline, we can use orchestration similar to what
    we did in [*Chapter 6*](B18024_06_ePub.xhtml#_idTextAnchor096), *Model to Production
    and Beyond*. I will leave that as an exercise since it's duplicate content. Let's
    look at an example of feature monitoring next.
  prefs: []
  type: TYPE_NORMAL
- en: Feature monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have discussed how important feature monitoring is in an ML system a few
    times in the book. We have also talked about how a feature store standardizes
    feature monitoring. In this section, let's look at an example of feature monitoring
    that can be useful for any model. As feature monitoring is calculating a set of
    statistics on feature data and notifying the data scientist or data engineer of
    changes, it needs the latest features used by the model.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, let's calculate the summary stats on the feature data and also
    feature correlation, which can be run on a schedule and sent to people of interest
    regularly so that they can take action based on it. As mentioned in the last note
    of the *Model training* section, the steps to fetch the features are the same
    as what was done in that section. Once you have all the features, the next step
    is to calculate the required stats.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Please note you may have to install additional libraries. Here is the URL for
    the notebook: [https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter08/Ch8_feature_monitoring.ipynb](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter08/Ch8_feature_monitoring.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block calculates the summary stats on the feature data and
    also plots the correlation metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE169]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE170]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE171]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE172]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE173]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE174]'
  prefs: []
  type: TYPE_PRE
- en: The preceding line of code produces a descriptive statistic of the dataset,
    which includes min, max, count, standard deviation, and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from the descriptive statistics, the correlation matrix of the features
    is another thing that could be useful for all the ML models. The following code
    block calculates the correlation matrix of the features and plots a heatmap:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE175]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE176]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE177]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE178]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE179]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE180]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE181]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE182]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE183]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE184]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE185]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE186]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE187]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE188]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE189]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE190]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE191]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE192]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE193]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE194]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE195]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code block outputs the following heatmap:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Feature correlation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_08_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.5 – Feature correlation
  prefs: []
  type: TYPE_NORMAL
- en: 'You can add more statistics, in comparison to the previous run, alerts through
    emails, Slack notifications, and more. This could be in another notebook/Python
    script, which can be scheduled at the same or at a lesser frequency than the feature
    engineering notebook and have automated reports sent to you. Here is the link
    to the complete notebook: [https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter08/Ch8_feature_monitoring.ipynb](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter08/Ch8_feature_monitoring.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: This is just an example of feature monitoring. There are more sophisticated
    statistics and metrics that can be used to determine the health of the features.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at model monitoring next.
  prefs: []
  type: TYPE_NORMAL
- en: Model monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another important aspect of ML is model monitoring. There are different aspects
    of model monitoring: one could be system monitoring in the case of online models,
    where you monitor the latency, CPU, memory utilization, requests per minute of
    the model, and more. The other aspect is performance monitoring of the model.
    Again, there are many different ways of measuring performance. In this example,
    we will look at a simple classification report and the accuracy of the model.'
  prefs: []
  type: TYPE_NORMAL
- en: To generate the classification report and calculate the accuracy of the live
    model, you need the prediction data and also the ground truth of the live data.
    For this example, let's say that the churn model is run once a week to generate
    the churn prediction and the ground truth will be available every 4 weeks from
    the day the model is run. That means if the model predicts customer x's churn
    as `True`, and within the next 4 weeks, if we lose the customer for any reason,
    the model predicted correctly; otherwise, the prediction was wrong. Hence, for
    every run of the model prediction, we need to wait for 4 weeks to have the ground
    truth.
  prefs: []
  type: TYPE_NORMAL
- en: For the simplicity of this exercise, let's also assume that the ground truth
    is filled back into the feature store every week. That means the feature store
    always has the latest ground truth. Now, our job is to fetch the prediction results
    that correspond to the latest features in the feature store (the prediction that
    was run 4 weeks ago) and calculate the required metrics. Let's do that next.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned before, the steps to fetch the latest feature from the feature
    store are the same as what we did in the last three sections. Once you have fetched
    the data from the feature store, the following code fetches the corresponding
    prediction results and merges the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE196]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE197]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE198]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE199]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE200]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE201]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE202]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE203]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE204]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE205]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code block should produce an output similar to the following snapshot.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – Merged data for model monitoring'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_08_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.6 – Merged data for model monitoring
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: As we have assumed the prediction and ground truth to be 4 weeks apart, the
    previous code block tries to fetch the data that is 4 weeks from today. For the
    exercise, you can replace the `file_name` variable with the prediction output
    Parquet file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have the DataFrame in *Figure 8.6*, the following code block uses
    the `predicted_churn` and `churn` columns to produce the classification report
    and accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE206]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE207]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE208]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE209]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE210]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE211]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE212]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE213]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE214]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE215]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE216]'
  prefs: []
  type: TYPE_PRE
- en: The previous code block produces output similar to the following.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – Classification report'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_08_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.7 – Classification report
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned before, this is sample monitoring. This can be scheduled at the
    same interval as the feature engineering notebook, though it would fail for the
    first four iterations due to the unavailability of the prediction data. Also make
    sure you adjust the prediction filename appropriately for your needs. Here is
    the URL for the complete notebook: [https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter08/Ch8_model_monitoring.ipynb](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter08/Ch8_model_monitoring.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: With that, let's summarize what we have learned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we set out with the aim of trying out a use case, namely telecom
    customer churn prediction using a dataset available from Kaggle. For this use
    case, we used a managed SageMaker Feature Store, which was introduced in the last
    chapter. In the exercise, we went through the different stages of ML, such as
    data processing, feature engineering, model training, and model prediction. We
    also looked at a feature monitoring and model monitoring example. The aim of this
    chapter was not model building but to showcase how to use a managed feature store
    for model building and the opportunities it opens for monitoring. To learn more
    about feature stores, the apply conference ([https://www.applyconf.com/](https://www.applyconf.com/))
    and feature store forum ([https://www.featurestore.org/](https://www.featurestore.org/))
    are good resources. To stay updated with new developments in ML and how other
    firms are solving similar problems, there are a few interesting podcasts, such
    as TWIML AI ([https://twimlai.com/](https://twimlai.com/)) and Data Skeptic (https://dataskeptic.com/).
    These resources should help you find more resources based on your area of interest
    in ML. With that, let's end this chapter and the book. I hope I was effective
    in conveying the importance of feature stores in the ML process and it was a good
    use of your time, and mine. Thank you!
  prefs: []
  type: TYPE_NORMAL
