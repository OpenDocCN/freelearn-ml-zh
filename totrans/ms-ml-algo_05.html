<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">EM Algorithm and Applications</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we are going to introduce a very important algorithmic framework for many statistical learning tasks: the EM algorithm. Contrary to its name, this is not a method to solve a single problem, but a methodology that can be applied in several contexts. Our goal is to explain the rationale and show the mathematical derivation, together with some practical examples. In particular, we are going to discuss the following topics:</p>
<ul>
<li><strong>Maximum Likelihood Estimation</strong> (<strong>MLE</strong>) and <span><strong>Maximum A Posteriori </strong>(<strong>MAP</strong>) learning approaches</span></li>
<li>The EM algorithm with a simple application for the estimation of unknown parameters</li>
<li>The Gaussian mixture algorithm, which is one the most famous EM applications</li>
<li>Factor analysis</li>
<li><strong>Principal Component Analysis</strong> (<strong>PCA</strong>)</li>
<li><strong>Independent Component Analysis</strong> (<strong>ICA</strong>)</li>
<li>A brief explanation of the <strong>Hidden Markov Models</strong> (<strong>HMMs</strong>) forward-backward algorithm considering the EM steps</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">MLE and MAP learning</h1>
                </header>
            
            <article>
                
<p><span>Let's suppose we have a data generating process <em>p</em></span><sub><em>data</em>, </sub><span>used to draw a dataset <em>X</em>:</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e94722b0-f5c4-4f12-bcd0-aa81d515c695.png" style="width:19.92em;height:1.75em;"/></div>
<p class="CDPAlignCenter CDPAlign CDPAlignLeft">In many statistical learning tasks, our goal is to find the optimal parameter set <em>θ</em> according to a maximization criterion. The most common approach is based on the likelihood and is called MLE. In this case, the optimal set <em>θ</em> is found as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1491b637-4841-422a-9351-f60118dc1a36.png" style="width:22.17em;height:1.42em;"/></div>
<p>This approach has the advantage of being unbiased by wrong preconditions, but, at the same time, it excludes any possibility of incorporating prior knowledge into the model. It simply looks for the best <em>θ</em> <span>in a wider subspace,</span> so that <em>p(X|</em><span><em>θ)</em> is maximized. Even if this approach is almost unbiased, there's a higher probability of finding a sub-optimal solution that can also be quite different from a reasonable (even if not sure) prior. After all, several models are too complex to allow us to define a suitable prior probability (think, for example, of reinforcement learning strategies where there's a huge number of complex states). Therefore, MLE offers the most reliable solution. Moreover, it's possible to prove that the MLE of a parameter <em>θ</em> converges in probability to the real value:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8205fc93-f5bf-4691-a1d2-c04a54ffa35c.png" style="width:24.83em;height:1.83em;"/></div>
<p>On the other hand, if we consider Bayes' theorem, we can derive the following relation:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2542d3e7-10b1-4595-9d67-96821f5f411d.png" style="width:10.75em;height:1.33em;"/></div>
<p>The posterior probability, <em>p(θ|X),</em> is obtained using both the likelihood and a prior probability, <em>p(θ),</em> and hence takes into account existing knowledge encoded in <em>p(</em><span><em>θ)</em>. The choice to maximize <em>p(θ|X)</em> is called the MAP approach and it's often a good alternative to MLE when it's possible to formulate trustworthy priors or, as in the case of <strong>Latent Dirichlet Allocation</strong> (<strong>LDA</strong>), where the model is on purpose based on some specific prior assumptions.</span></p>
<p><span>Unfortunately, a wrong or incomplete prior distribution can bias the model leading to unacceptable results. For this reason, MLE is often the default choice even when it's possible to formulate reasonable assumptions on the structure of <em>p(θ)</em>. To understand the impact of a prior on an estimation, let's consider to have observed <em>n</em>=1000 binomial distributed (<em>θ</em> corresponds to the parameter <em>p</em>) experiments and <em>k</em>=800 had a successful outcome. The likelihood is as follows:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/da8aa927-3117-4921-939d-87d3174fc50b.png" style="width:13.92em;height:3.08em;"/></div>
<p>For simplicity, let's compute the log-likelihood:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ee4937fe-54cc-40b4-9dcf-47683ff96d06.png" style="width:26.17em;height:3.08em;"/></p>
<p>If we compute the derivative with respect to <em>θ</em> and set it equal to zero, we get the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5f3d1309-2ddf-4a0c-8675-a76cd87dcf78.png" style="width:29.17em;height:4.08em;"/></div>
<p>So the MLE for <em>θ</em> is 0.8, which is coherent with the observations (we can say that after observing 1000 experiments with 800 successful outcomes, <em>p(X|Success)=0.8</em>). If we have only the data <em>X</em>, we could say that a success is more likely than a failure because 800 out of 1000 experiments are positive.</p>
<p>However, after this simple exercise, an expert can tell us that, considering the largest possible population, the marginal probability <em>p(Success)=0.001</em> (Bernoulli distributed with <em>p(Failure) = 1 - P(success)</em>) and our sample is not representative. If we trust the expert, we need to compute the posterior probability using Bayes' theorem:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3de50cde-852d-42fd-9b8b-360d5c41e81b.png" style="width:44.08em;height:5.83em;"/></div>
<p><span>Surprisingly, the posterior probability is very close to zero and we should reject our initial hypothesis! At this point, there are two options: if we want to build a model based only on our data, the MLE is the only reasonable choice, because, considering the posterior, we need to accept we have a very poor dataset (this is probably a bias when drawing the samples from the data generating process <em>p<sub>data</sub></em>).</span></p>
<p><span>On the other hand, if we really trust the expert, we have a few options for managing the problem:</span></p>
<ul>
<li>Checking the sampling process in order to assess its quality (we can discover that a better sampling leads to a very lower <em>k</em> value)</li>
<li>Increasing the number of samples</li>
<li>Computing the MAP estimation of <em>θ</em></li>
</ul>
<p><span>I suggest that the reader tries both approaches with simple models, to be able to compare the relative accuracies. </span><span>In this book, we're always going to adopt the MLE when it's necessary to estimate the parameters of a model with a statistical approach. This choice is based on the assumption that our datasets are correctly sampled from <em>p<sub>data</sub></em>. If this is not possible (think about an image classifier that must distinguish between</span> <span>horses, dogs, and cats, built with a dataset where there are pictures of 500 horses, 500 dogs, and 5 cats), we should expand our dataset or use data augmentation techniques to create artificial samples.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">EM algorithm</h1>
                </header>
            
            <article>
                
<p>The EM algorithm is a generic framework that can be employed in the optimization of many generative models. It was originally proposed in <em>Maximum likelihood from incomplete data via the em algorithm</em>, <em>Dempster A. P.</em>, <em>Laird N. M.</em>, <em>Rubin D. B</em><em>.</em>, <em>Journal of the Royal Statistical Society</em>, <em>B</em>, <em>39(1):1–38</em>, <em>11/1977</em>, where the authors also proved its convergence at different levels of genericity.</p>
<p>For our purposes, we are going to consider a dataset, <em>X,</em> and a set of latent variables, <em>Z,</em> that we cannot observe. They can be part of the original model or introduced artificially as a trick to simplify the problem. A generative model parameterized with the vector <em>θ</em> has a log-likelihood equal to the following:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1cdb05ce-95bf-4574-aadd-e6d994da6abc.png" style="width:11.75em;height:1.25em;"/></div>
<p>Of course, a large log-likelihood implies that the model is able to generate the original distribution with a small error. Therefore, our goal is to find the optimal set of parameters <span><em>θ</em> that maximizes the marginalized log-likelihood (we need to sum—or integrate out for continuous variables—the latent variables out because we cannot observe them):</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f8411ee6-ac8d-4de1-b727-988d89b2bc7a.png" style="width:34.92em;height:2.92em;"/></div>
<p>Theoretically, this operation is correct, but, unfortunately, it's almost always impracticable because of its complexity (in particular, the logarithm of a sum is often very problematic to manage). However, the presence of the latent variables can help us in finding a good proxy that is easy to compute and whose maximization corresponds to the maximization of the original log-likelihood. Let's start by rewriting the expression of the likelihood using the chain rule:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c215c707-e6f0-4c87-9c6e-8be1bcc24287.png" style="width:23.33em;height:2.83em;"/></div>
<p>If we consider an iterative process, our goal is to find a procedure that satisfies the following condition:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ba47f7df-0d6f-415f-8610-f9a0327754cd.png" style="width:32.33em;height:1.50em;"/></div>
<p>We can start by considering a generic step:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/35b2da74-5532-41aa-ad22-6b10fa4aa19e.png" style="width:33.17em;height:2.92em;"/></div>
<p>The first problem to solve is the logarithm of the sum. Fortunately, we can employ the <em>Jensen's inequality</em>, which allows us to move the logarithm inside the summation. Let's first define the concept of a <em>convex function</em>: a function, <em>f(x),</em> defined on a convex set, <em>D,</em> is said to be convex if the following applies: </p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/65a9f769-a0d2-47e1-bca0-94f5240a15e4.png" style="width:33.33em;height:1.25em;"/></div>
<p>If the inequality is strict, the function is said to be <em>strictly convex</em>. Intuitively, and considering a function of a single variable <em>f(x)</em>, the previous definition states that the function is never above the segment that connects two points (<em>x<sub>1</sub></em>, <em>f(x<sub>1</sub>)</em>) and <span>(<em>x</em></span><em><sub>2</sub></em><span>, <em>f(</em></span><em><span>x</span><sub>2</sub></em><span><em>)</em>). In the case of strict convexity, <em>f(x)</em> is always below the segment. Inverting these definitions, we obtain the conditions for a function to be <em>concave</em> or <em>strictly concave</em>.</span></p>
<p>If a function <em>f(x)</em> is concave in <em>D</em>, the function <em>-f(x)</em> is convex in <em>D</em>; therefore, as <em>log(x)</em> is concave in <em>[0, ∞)</em> (or with an equivalent notation in <em>[0, <span>∞[)</span>, -log(x)</em> is convex in <em>[0, ∞)</em>, as shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/330d64e8-255f-4350-8f61-16e03189581b.png" style="width:43.33em;height:20.25em;"/></div>
<p>The <em>Jensen's inequality</em> (the proof is omitted but further details can be found in <em>Jensen's</em> <em>Operator Inequality</em>, <em>Hansen F.</em>, <em>Pedersen G. K.</em>, arXiv:math/0204049 [math.OA] states that if <em>f(x)</em> is a convex function defined on a convex set <em>D</em>, if we select n points <em>x<sub>1</sub></em>, <em>x<sub>2</sub></em>, ..., <em>x<sub>n</sub></em> ∈ <em>D</em> and <em>n</em> constants <em>λ<sub>1</sub></em>, <em>λ<sub>2</sub></em>, ..., <em>λ<sub>n</sub></em> ≥ <em>0</em> satisfying the condition <em><span>λ</span><sub>1</sub> + λ<sub>2</sub> + ... + λ<sub>n</sub> = 1</em>, then the following applies:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ee100e07-caba-41fc-a796-d54acf0a812f.png" style="width:16.58em;height:4.42em;"/></div>
<p>Therefore, considering that <em>-log(x)</em> is convex, the <em>Jensen's inequality</em> for <em>log(x)</em> becomes as follows:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/302a6644-870e-4feb-ab0b-d3e09eda66e6.png" style="width:19.17em;height:4.58em;"/></div>
<p><span>Hence, the generic iterative step can be rewritten, as follows:</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d651914c-a165-404e-bba1-fde1c04afc0d.png" style="width:49.83em;height:6.58em;"/></div>
<p>Applying the Jensen's inequality, we obtain the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5bdbe909-f1e6-48f0-8f6a-736a1343bc18.png" style="width:51.75em;height:7.58em;"/></div>
<p>All the conditions are met, because the terms <em>P(z<sub>i</sub>|X, θ<sub>t</sub>)</em> are, by definition, bounded between [0, 1] and the sum over all <em>z</em> must always be equal to 1 (laws of probability). The previous expression implies that the following is true:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/95860f42-dbac-4803-bc80-bcea6c8258b0.png" style="width:32.67em;height:3.67em;"/></div>
<p>Therefore, if we maximize the right side of the inequality, we also maximize the log-likelihood. However, the problem can be further simplified, considering that we are optimizing only the parameter vector <em>θ</em> and we can remove all the terms that don't depend on it. Hence, we can define a <em>Q function</em> (there are no relationships with the Q-Learning that we're going to discuss in <a href="51bcd684-080e-4354-b2ed-60430bd15f6d.xhtml">Chapter 14</a>, <em>Introduction to Reinforcement Learning</em>) whose expression is as follows:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1f6510d2-dec4-4501-be01-9854f147482f.png" style="width:59.50em;height:3.08em;"/></div>
<p><em>Q</em> is the expected value of the log-likelihood considering the complete data <em>Y</em> = (<em>X</em>, <em>Z</em>) and the current iteration parameter <em>set θ<sub>t</sub></em>. At each iteration, <em>Q</em> is computed considering the current estimation <em>θ</em><sub><em>t</em> </sub>and it's maximized considering the variable <em>θ</em><span>.</span> It's now clearer why the latent variables can be often artificially introduced: they allow us to apply the <em>Jensen's inequality</em> and transform the original expression into an expected value that is easy to evaluate and optimize.</p>
<p><span>At this point, we can formalize the</span> EM algorithm:</p>
<ol>
<li>Set a threshold <em>Thr</em> (for example, <em>Thr</em> = 0.01)</li>
<li>Set a random parameter vector <em>θ<sub>0.</sub></em></li>
<li>While <em>|L(<span>θ</span><sub>t</sub>|X, Z) - <span>L</span>(<span>θ</span><sub>t-1</sub>|<span>X, Z</span>)| &gt;</em> <span><em>Thr</em>:</span>
<ul>
<li><strong>E-Step</strong>: Compute the <em>Q</em>(<em>θ</em>|<em><span>θ</span><sub>t</sub></em>). In general, this step consists in computing the conditional probability <em>p</em>(<em>z</em>|<em>X</em>, <em><span>θ</span><sub>t</sub></em>) or some of its moments (sometimes, the sufficient statistics are limited to mean and covariance) using the current parameter estimation <em><span>θ</span><sub>t</sub></em>. </li>
<li><strong>M-Step</strong>: Find <em>θ<sub>t+1</sub></em> = <em>argmax<sub>θ</sub> <span>Q</span></em><span>(</span><em><span>θ</span></em><span>|</span><em><span>θ</span><sub>t</sub></em><span>). The new parameter estimation is computed to maximize the <em>Q</em> function.</span></li>
</ul>
</li>
</ol>
<p>The procedure ends when the log-likelihood stops increasing or after a fixed number of iterations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An example of parameter estimation</h1>
                </header>
            
            <article>
                
<p>In this example, we see how it's possible to apply the EM algorithm for the estimation of unknown parameters (inspired by an example discussed in the original paper <em>Maximum likelihood from incomplete data via the em algorithm</em>,<em> </em><em>Dempster A. P.</em>, <em>Laird N. M.</em>, <em>Rubin D. B.</em>, <em>Journal of the Royal Statistical Society</em>, <em>B, 39(1):1–38</em>, <em>11/1977</em>).</p>
<p>Let's consider a sequence of <em>n</em> independent experiments modeled with a multinomial distribution with three possible outcomes <em>x<sub>1</sub></em>, <em>x<sub>2</sub></em>, <em>x<sub>3</sub></em> and corresponding probabilities <em>p<sub>1</sub></em>, <em>p<sub>2</sub></em> and <em>p<sub>3</sub></em>. The probability mass function is as follows:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/da7e3274-4ca4-400f-b3a2-99677ba9686d.png" style="width:22.92em;height:4.00em;"/></div>
<p>Let's suppose that we can observe <em>z<sub>1</sub> = x<sub>1 </sub>+ x<sub>2</sub></em> and <em>x<sub>3</sub></em>, but we don't have any direct access to the single values <em>x<sub>1</sub></em> and <em>x<sub>2</sub></em>. Therefore, <em>x<sub>1</sub></em> and <em>x<sub>2</sub></em> are latent variables, while <em>z<sub>1</sub></em> and <em>x<sub>3</sub></em> are observed ones. The probability vector <em>p</em> is parameterized in the following way:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c06aa4ef-5882-43be-b878-b0686432ead3.png" style="width:21.25em;height:1.92em;"/></div>
<p>Our goal is to find the MLE for <em>θ</em> given <em>n</em>, <em>z<sub>1</sub></em>, and <em>x<sub>3</sub></em>. Let's start computing the log-likelihood:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ce429bb1-7dc7-4769-8ce5-c289b6351bb5.png" style="width:49.67em;height:7.08em;"/></div>
<p>We can derive the expression for the corresponding <em>Q</em> function, exploiting the linearity of the expected value operator <em>E</em>[<em>•</em>]:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/cc3db94c-f9a9-44ce-ae49-d20c9fdcacd9.png" style="width:40.42em;height:3.33em;"/></div>
<p class="mce-root">The variables <em>x<sub>1</sub></em> and <em>x<sub>2,</sub></em> given <em>z<sub>1,</sub></em> are binomially distributed and can be expressed as a function of <em><span>θ</span><sub>t</sub></em> (we need to recompute them at each iteration). Hence, the expected value of <em>x<sub>1</sub></em><sup>(t<em>+</em>1<em>)</em></sup> becomes as follows:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/53dfda53-ce60-490a-9575-e6d0570eee65.png" style="width:38.42em;height:5.00em;"/></div>
<p>While the expected value of <em>x<sub>2</sub></em><sup>(<em>t</em>+<em>1</em>)</sup> is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8d8e8892-8cc4-4ad8-a3d1-9bacb29a205f.png" style="width:37.50em;height:4.75em;"/></div>
<p class="mce-root CDPAlignLeft CDPAlign">If we apply these expressions in <img class="fm-editor-equation" src="assets/b0a1acac-af44-42fd-b0a8-56201c4e3774.png" style="width:3.25em;height:1.25em;"/> and compute the derivative with respect to <span><em>θ</em>, we get the following:</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/270056ef-2322-4a4d-8314-9950dc7eb4ae.png" style="width:28.33em;height:3.25em;"/></div>
<p>Therefore, solving for <em>θ</em>, we get the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e02ea8ad-1f45-4036-8fc1-8e7f65f7228f.png" style="width:18.92em;height:4.42em;"/></div>
<p>At this point, we can derive the iterative expression for <span><em>θ</em>:</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/59099387-badd-4905-8cc6-6fa09249e47e.png" style="width:25.33em;height:4.17em;"/></div>
<p>Let's compute the value of <span><em>θ</em> for <em>z<sub>1</sub></em> = 50 and <em>x<sub>3</sub></em> = 10:</span></p>
<pre>def theta(theta_prev, z1=50.0, x3=10.0):<br/>    num = (8.0 * z1 * theta_prev) + (4.0 * x3 * (12.0 - theta_prev))<br/>    den = (z1 + x3) * (12.0 - theta_prev)<br/>    return num / den<br/><br/>theta_v = 0.01<br/><br/>for i in range(1000):<br/>    theta_v = theta(theta_v)<br/><br/>print(theta_v)<br/>1.999999999999999<br/><br/>p = [theta_v/6.0, (1-(theta_v/4.0)), theta_v/12.0]<br/><br/>print(p)<br/>[0.33333333333333315, 0.5000000000000002, 0.16666666666666657]</pre>
<p>In this example, we have parameterized all probabilities and, considering that <em><span>z</span><sub>1</sub><span> </span></em><span>=</span> <em><span>x</span><sub>1 </sub></em><span>+ x</span><em><sub>2</sub></em>, we have one degree of freedom for the choice of <em>θ</em>. The reader can repeat the example by setting the value of one of <em><span>p</span><sub>1</sub></em> or <em><span>p</span><sub>2</sub></em> and leaving the other probabilities as functions of <em>θ</em>. The computation is almost identical but in this case, there are no degrees of freedom.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Gaussian mixture</h1>
                </header>
            
            <article>
                
<p class="mce-root">In <a href="f6860c85-a228-4e63-96ac-22a0caf1a767.xhtml">Chapter 2</a>, <em>Introduction to Semi-Supervised Learning</em>, we discussed the generative Gaussian mixture model in the context of semi-supervised learning. In this paragraph, we're going to apply the EM algorithm to derive the formulas for the parameter updates.</p>
<p>Let's start considering a dataset, <em>X,</em> drawn from a data generating process, <em>p<sub>data</sub></em>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c86d60ac-0692-4c32-b3fe-7134cafd9798.png" style="width:21.33em;height:1.58em;"/></div>
<p>We assume that the whole distribution is generated by the sum of <em>k</em> Gaussian distributions so that the probability of each sample can be expressed as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/43b2677f-f55c-4bf4-8601-260e4bfb251b.png" style="width:29.50em;height:3.92em;"/></div>
<p>In the previous expression, the term <em>w<sub>j</sub></em> = <em>P</em>(<em>N</em>=j) is the relative weight of the <em>j<sup>th</sup></em> Gaussian, while <em>μ</em><sub><em>j</em> </sub>and <em>Σ<sub>j</sub></em> are the mean and the covariance matrix. For consistency with the laws of probability, we also need to impose the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3154b0c1-e8e4-418f-92e6-4bc338d9c902.png" style="width:5.00em;height:2.75em;"/></div>
<p>Unfortunately, if we try to solve the problem directly, we need to manage the logarithm of a sum and the procedure becomes very complex. However, we have learned that it's possible to use latent variables as helpers, whenever this trick can simplify the solution.</p>
<p>Let's consider a single parameter set <em>θ</em>=(<em>w<sub>j</sub></em>, <em><span>μ</span><sub>j</sub></em>, <em><span>Σ<sub>j</sub></span></em>) and a latent indicator matrix <em>Z</em> where each element <em>z<sub>ij</sub></em> is equal to 1 if the point <em>x<sub>i</sub></em> has been generated by the <em>j<sup>th</sup></em> Gaussian, and 0 otherwise. Therefore, each <em>z<sub>ij</sub></em> is Bernoulli distributed with parameters equal to <em>p</em>(<em>j</em>|<em>x<sub>i</sub></em>, <em>θ<sub>t</sub></em>).</p>
<p>The joint log-likelihood can hence be expressed using the exponential-indicator notation, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/dc57de2f-997f-44c5-b090-a609d0362c2f.png" style="width:32.75em;height:3.08em;"/></div>
<p>The index, <em>i,</em> is referred to the samples, while <em>j</em> refers to the Gaussian distributions. If we apply the chain rule and the properties of a logarithm, the expression becomes as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1c1df3d3-f8aa-41d2-bcac-024e140109db.png" style="width:39.25em;height:2.83em;"/></div>
<p>The first term represents the probability of <em>x<sub>i</sub></em> under the <em>j<sup>th</sup></em> Gaussian, while the second one is the relative weight of the <em>j<sup>th</sup></em> Gaussian. We can now compute the <em>Q</em>(<em>θ</em>;<em>θ<sub>t</sub></em>) function using the joint log-likelihood:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0dbc339d-5608-4170-a1ff-fb330ba49ab6.png" style="width:41.75em;height:3.92em;"/></div>
<p>Exploiting the linearity of <em>E[•]</em>, the previous expression becomes as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/45c1ddc3-5b20-4f8e-b97f-4827eca10a48.png" style="width:50.00em;height:6.42em;"/></div>
<p>The term <span><em>p</em>(<em>j</em>|<em>x</em></span><em><sub>i</sub></em><span>, </span><em><span>θ</span><sub>t</sub></em><span>)</span> corresponds to the expected value of <em>z<sub>ij</sub></em> considering the complete data, and expresses the probability of the <em>j<sup>th</sup></em> Gaussian given the sample <em>x<sub>i</sub></em>. It can be simplified considering Bayes' theorem:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/682b9783-ce0e-41b0-b56b-5109d6ec3b7c.png" style="width:19.00em;height:1.58em;"/></div>
<p>The first term is the probability of <em><span>x</span><sub>i</sub></em> under the <em>j<sup>th</sup></em> Gaussian with parameters <em>θ<sub>t</sub></em>, while the second one is the weight of the <em>j<sup>th</sup></em> Gaussian considering the same parameter set <em><span>θ</span><sub>t</sub></em>. In order to derive the iterative expressions for the parameters, it's useful to write the complete formula for the logarithm of a multivariate Gaussian distribution:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/631ee1a8-e22e-4b3c-b0a1-9dc90d98eb87.png" style="width:62.33em;height:7.83em;"/></div>
<p>To simplify this expression, we use the trace trick. In fact, as (<em>x<sub>i</sub></em> -<em> μ<sub>j</sub></em>)<em><sup>T</sup></em> Σ<sup>-1</sup> <span>(<em>x</em></span><em><sub>i</sub></em><span> - </span><em><span>μ</span><sub>j</sub></em><span>) is a scalar, we can exploit the properties <em>tr</em>(<em>AB</em>) = <em>tr</em>(<em>BA</em>) and <em>tr</em>(<em>c</em>) = <em>c</em> where <em>A</em> and <em>B</em> are matrices and <em>c</em> ∈ <em>ℜ</em>:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2b77a9af-5e3d-464e-9d0f-083c7e443d24.png" style="width:36.83em;height:2.42em;"/></div>
<p>Let's start considering the estimation of the mean (only the first term of <span><em>Q</em>(<em>θ</em>;<em>θ</em></span><em><sub>t</sub></em><span>)</span> depends on mean and covariance):</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d1b6f81d-ad1a-4f23-aed8-002f1167444d.png" style="width:49.58em;height:6.83em;"/></div>
<p>Setting the derivative equal to zero, we get the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/049ce298-27ae-4414-94ac-31321c8701ff.png" style="width:11.08em;height:3.08em;"/></div>
<p>In the same way, we obtain the expression of the covariance matrix:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d0f30780-0542-45a2-be5b-060feb432d82.png" style="width:23.50em;height:4.33em;"/></div>
<p>To obtain the iterative expressions for the weights, the procedure is a little bit more complex, because we need to use the Lagrange multipliers<strong> </strong>(further information can be found in <a href="http://www.slimy.com/~steuard/teaching/tutorials/Lagrange.html" target="_blank">http://www.slimy.com/~steuard/teaching/tutorials/Lagrange.html</a>). Considering that the sum of the weights must always be equal to 1, it's possible to write the following equation:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/158892a8-78cb-4e03-a49b-195cf5ed898a.png" style="width:31.75em;height:3.42em;"/></div>
<p>Setting both derivatives equal to zero, from the first one, considering that <em>wj</em> = <em>p</em>(<em>j</em>|<em>θ</em>), we get the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/61be7b2e-d780-412a-b991-adcd21e196a1.png" style="width:44.00em;height:6.00em;"/></div>
<p class="mce-root">While from the second derivative, we obtain the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1a87943e-41b8-46b4-9c6b-1b6f923ca193.png" style="width:29.92em;height:3.33em;"/></div>
<p>The last step derives from the fundamental condition: </p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/fe93fa4e-ed32-4de1-bdc9-5136d9e73d7e.png" style="width:9.83em;height:3.00em;"/></div>
<p>Therefore, the final expression of the weights is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ee17e911-2233-4dec-8113-ea506de214be.png" style="width:10.00em;height:2.67em;"/></div>
<p>At this point, we can formalize the Gaussian mixture algorithm:</p>
<ul>
<li>Set random initial values for <em>w<sub>j</sub></em><sup>(<em>0</em>)</sup>, <em>θ</em><sup>(<em>0</em>)</sup><em><sub>j</sub></em> and <em>Σ</em><sup>(0)</sup><em><sub>j</sub></em></li>
<li><strong>E-Step</strong>: Compute <em>p</em>(<em>j</em>|<em>x<sub>i</sub></em>, <em>θ<sub>t</sub></em>) using Bayes' theorem: <span><em>p</em>(<em>j</em>|<em>x</em></span><em><sub>i</sub></em><span>, </span><span><em>θ<sub>t</sub></em><span>) = <em>α w</em><sup>(<em>t</em>)</sup><sub>j</sub> <em>p</em>(<em>x<sub>i</sub></em>|<em>j,</em> <em>θ<sub>t</sub></em>)</span></span></li>
<li><strong>M-Step</strong>: Compute <em><span>w</span><sub>j</sub></em><sup>(<em>t+1</em>)</sup><span>,<em> θ</em></span><sup>(<em>t+1</em>)</sup><sub>j</sub><span> and Σ</span><sup>(t+1)</sup><em><sub>j</sub></em> using the formulas provided previously</li>
</ul>
<p>The process must be iterated until the parameters become stable. In general, the best practice is using both a threshold and a maximum number of iterations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An example of Gaussian Mixtures using Scikit-Learn</h1>
                </header>
            
            <article>
                
<p>We can now implement the Gaussian mixture algorithm using the Scikit-Learn implementation. The direct approach has already been shown in <a href="f6860c85-a228-4e63-96ac-22a0caf1a767.xhtml">Chapter 2</a>, <em>Introduction to Semi-Supervised Learning</em>. The dataset is generated to have three cluster centers and a moderate overlap due to a standard deviation equal to 1.5:</p>
<pre>from sklearn.datasets import make_blobs<br/><br/>nb_samples = 1000<br/>X, Y = make_blobs(n_samples=nb_samples, n_features=2, centers=3, cluster_std=1.5, random_state=1000)</pre>
<p>The corresponding plot is shown in the following diagram:</p>
<div class="CDPAlignCenter packt_figref CDPAlign"><img src="assets/a02e97c3-25bf-4a5a-9687-191ed0bac26c.png" style="width:41.08em;height:23.00em;"/></div>
<p>The Scikit-Learn implementation is based on the <kbd>GaussianMixture</kbd> class<span> </span>, which accepts as parameters the number of Gaussians (<kbd>n_components</kbd>), the type of covariance (<kbd>covariance_type</kbd>), which can be <kbd>full</kbd> (the default value), if all components have their own matrix, <kbd>tied</kbd> if the matrix is shared, <kbd>diag</kbd> if all components have their own diagonal matrix (this condition imposes an uncorrelation among the features), and <kbd>spherical</kbd> when each Gaussian is symmetric in every direction. The other parameters allow setting regularization and initialization factors (for further information, the reader can directly check the documentation). Our implementation is based on full covariance:</p>
<pre>from sklearn.mixture import GaussianMixture<br/><br/>gm = GaussianMixture(n_components=3)<br/>gm.fit(X)</pre>
<p>After fitting the model, it's possible to access to the learned parameters through the instance variables <kbd>weights_</kbd>, <kbd>means_</kbd>, and <kbd>covariances_</kbd>:</p>
<pre>print(gm.weights_)<br/><br/>[ 0.32904743  0.33027731  0.34067526]<br/><br/>print(gm.means_)<br/><br/>[[ 3.03902183 -7.69186648]
 [ 9.04414279 -0.37455175]
 [ 7.37103878 -5.77496152]]<br/><br/>print(gm.covariances_)<br/><br/>[[[ 2.34943036  0.08492009]
  [ 0.08492009  2.36467211]]

 [[ 2.10999633  0.02602279]
  [ 0.02602279  2.21533635]]

 [[ 2.71755196 -0.0100434 ]
  [-0.0100434   2.39941067]]]</pre>
<p>Considering the covariance matrices, we can already understand that the features are very uncorrelated and the Gaussians are almost spherical. The final plot can be obtained by assigning each point to the corresponding cluster (Gaussian distribution) through the <kbd>Yp = gm.transform(X)</kbd> command:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e4962263-c815-4c52-9303-449380b931e0.png" style="width:45.00em;height:25.17em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Labeled dataset obtained through the application of a Gaussian mixture with three components</div>
<p>The reader should have noticed a strong analogy between Gaussian mixture and k-means<strong> </strong>(which we're going to discuss in <a href="59f765c2-2ad0-4605-826e-349080f85f1f.xhtml">Chapter 7</a>, <em>Clustering Algorithms</em>). In particular, we can state that K-means is a particular case of spherical Gaussian mixture with a covariance <em>Σ</em> → 0. This condition transforms the approach from a soft clustering, where each sample belongs to all clusters with a precise probability distribution, into a hard clustering, where the assignment is done by considering the shortest distance between sample and centroid (or mean). For this reason, in some books, the Gaussian mixture algorithm is also called soft K-means.<strong> </strong>A conceptually similar approach that we are going to present is Fuzzy K-means, which is based on assignments characterized by membership functions, which are analogous to probability distributions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Factor analysis</h1>
                </header>
            
            <article>
                
<p>Let's suppose we have a Gaussian data generating process, <em>p</em><sub><em>data</em> </sub>∼ <em>N</em>(<em>0</em>, <em>Σ</em>), and <em>M</em> n-dimensional zero-centered samples drawn from it:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/dd2c1b29-1765-4463-86b2-e448460c1598.png" style="width:22.17em;height:1.58em;"/></div>
<p><span>If <em>p</em></span><em><sub>data</sub></em><span> has a mean <em>μ</em> ≠ <em>0</em>, it's also possible to use this model, but it's necessary to account for this non-null value with slight changes in some formulas. As the zero-centering normally has no drawbacks, it's easier to remove the mean to simplify the model.</span></p>
<p>One of the most common problems in unsupervised learning is finding a lower dimensional distribution <em>p<sub>lower</sub></em> such that the Kullback-Leibler divergence with <em>p<sub>data</sub></em> is minimized. When performing a <strong>factor analysis </strong>(<strong>FA</strong>), following the original proposal published in <em>EM algorithms for ML factor analysis</em>, <em>Rubin D.</em>, <em>Thayer D.</em>, <em>Psychometrika</em>, 47/1982, Issue 1, and <em>The EM algorithm for Mixtures of Factor Analyzers</em>, <em>Ghahramani Z.</em>, <em>Hinton G. E.</em>, CRC-TG-96-1, 05/1996, we start from the assumption to model the generic sample <em>x</em> as a linear combination of Gaussian latent variables, <em>z,</em> (whose dimension <em>p</em> is normally <em>p</em> &lt; <em>n</em>) plus an additive and decorrelated Gaussian noise term, <em>ν</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3ce65823-73b9-42bc-978d-55aafab340f7.png" style="width:45.58em;height:1.67em;"/></div>
<p class="mce-root">The matrix, A, is called a <em>factor loading matrix</em> because it determines the contribution of each latent variable (factor) to the reconstruction of <em>x</em>. Factors and input data are assumed to be statistically independent. Instead, considering the last term, if <em>ω<sub>0</sub><sup>2</sup></em> ≠ <em><span>ω</span><sub>1</sub><sup>2</sup></em><span> ≠ ... ≠ <em>ω<sub>n</sub><sup>2</sup></em> the noise is called <em>heteroscedastic</em>, while it's defined <em>homoscedastic</em> if the variances are equal <em>ω<sub>0</sub><sup>2</sup></em> = <em>ω<sub>1</sub><sup>2</sup></em> = ... = <em>ω<sub>n</sub><sup>2</sup></em> = <em>ω<sup>2</sup></em>. To understand the difference between these two kinds of noise, think about a signal x which is the sum of two identical voices, recorded in different places (for example, an airport and a wood). In this case, we can suppose to also have different noise variances (the first one should be higher than the second considering the number of different noise sources). If instead both voices are recorded in a soundproofed room or even in the same airport, homoscedastic noise is surely more likely (we're not considering the power, but the difference between the variances).</span></p>
<p class="mce-root"><span>One of the most important strengths of FA in respect to other methods (such as PCA) is its intrinsic robustness to heteroscedastic noise. In fact, including the noise term in the model (with only the constraint to be decorrelated) allows partial denoising filtering based on the single components, while one of the </span><span>preconditions for the PCA is to impose only homoscedastic noise (which, in many cases, is very similar to the total absence of noise). Considering the previous example, we could make the assumption to have the first variance be <em>ω<sub>0</sub><sup>2</sup></em> = <em>k ω<sub>1</sub><sup>2</sup></em> with <em>k</em> &gt; <em>1</em>. In this way, the model will be able to understand that a high variance in the first component should be considered (with a higher probability) as the product of the noise and not an intrinsic property of the component.</span></p>
<p class="mce-root">Let's now analyze the linear relation:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/08e836ef-686e-4155-b7ff-4020e1d5948a.png" style="width:6.42em;height:1.17em;"/></div>
<p class="mce-root">Considering the properties of Gaussian distributions, we know that <em>x</em> ∼ <em>N</em>(<em>μ</em>, <em>Σ</em>) and it's easy to determine either the mean or the covariance matrix:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/aec4d0d8-e9e6-4b5b-b8bf-7765455b39fe.png" style="width:42.42em;height:2.83em;"/></div>
<p class="mce-root">Therefore, in order to solve the problem, we need to find the best <em>θ</em>=(<em>A</em>, Ω) so that <em>AA<sup>T</sup></em> + <span>Ω ≈ <em>Σ</em> (with a zero-centered dataset, the estimation is limited to the input covariance matrix <em>Σ</em>).The ability to cope with noisy variables should be clearer now. If <em>AA<sup>T</sup></em> + Ω is exactly equal to <em>Σ</em> and the estimation of Ω is correct, the algorithm will optimize the factor loading matrix A, excluding the interference produced by the noise term; therefore, the components will be approximately denoised.</span></p>
<p class="mce-root"><span>In order to adopt the EM algorithm, we need to determine the joint probability <em>p</em>(<em>X</em>, <em>z</em>; <em>θ</em>) = <em>p</em>(<em>X</em>|<em>z</em>; <em>θ</em>)<em>p</em>(<em>z</em>|<em>θ</em>). The first term on the right side can be easily determined, considering that <em>x</em> - <em>Az</em> ∼ <em>N</em>(<em>0</em>, Ω); therefore, we get the following:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f1c90edf-a50f-4b9b-8791-c77fc50ec9ef.png" style="width:62.50em;height:10.08em;"/></div>
<p class="mce-root">We can now determine the <em>Q</em>(<em>θ</em>;<em>θ<sub>t</sub></em>) function, discarding the constant term (<em>2π</em>)<em><sup>k</sup></em> and term <em>z<sup>T</sup>z</em>, which don't depend on <em>θ</em> (in this particular case, as we're going to see, we don't need to compute the probability <span><em>p</em>(<em>z</em>|<em>X</em>;<em>θ</em>) because it's enough to obtain sufficient statistics for expected value and second moment</span>). Moreover, it's useful to expand the multiplication in the exponential:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/191ed831-9326-49b6-80dd-2df8c532f7e5.png" style="width:55.92em;height:6.50em;"/></div>
<p class="mce-root">Using the trace trick with the last term (which is a scalar), we can rewrite it as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/69496cf8-3686-40ab-ae89-9cf8bc033dbd.png" style="width:30.50em;height:1.75em;"/></div>
<p>Exploiting the linearity of <em>E</em>[•], we obtain the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e3c5f66d-411b-4405-8e12-01fa576f4014.png" style="width:63.83em;height:4.83em;"/></div>
<p>This expression is similar to what we have seen in the Gaussian mixture model, but in this case, we need to compute the conditional expectation and the conditional second moment of <em>z</em>. Unfortunately, we cannot do this directly, but it's possible to compute them exploiting the joint normality of <em>x</em> and <em>z</em>. In particular, using a classic theorem, we can partition the full joint probability <em>p</em>(<em>z</em>, <em>x</em>), considering the following relations:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/99948ccc-10dd-4167-9e12-bbaad7488ddf.png" style="width:46.08em;height:3.50em;"/></div>
<p class="mce-root">The conditional distribution <em>p</em>(<em>z</em>|<em>x</em>=<em>x<sub>i</sub></em>) has a mean equal to the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ccb3c890-8624-4897-9aad-ab99e431cb9d.png" style="width:43.75em;height:1.92em;"/></div>
<p>The conditional variance is as follows:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/89e7ea57-3852-4d17-aa49-dd31424899d3.png" style="width:39.92em;height:2.00em;"/></div>
<p>Therefore, the conditional second moment is equal to the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ee93199c-6dae-48cc-8bfe-0f99fb74b19e.png" style="width:73.00em;height:4.50em;"/></div>
<p>If we define the auxiliary matrix <em>K</em> = (<em>AA<sup>T</sup></em> + Ω)<em><sup>-1</sup></em>, the previous expressions become as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/adaa0e9e-b648-40d5-b776-bb6dd3612434.png" style="width:47.33em;height:3.42em;"/></div>
<p class="mce-root">The reader in search of further details about this technique can read <em>Preview</em><br/>
<em>Introduction to Statistical Decision Theory</em>, <em>Pratt J.</em>, <em>Raiffa H.</em>, <em>Schlaifer R.</em>, <em>The MIT Press</em>.</p>
<p>Using the previous expression, it's possible to build the inverse model (sometimes called a <em>recognition model </em>because it starts with the effects and rebuilds the causes), which is still Gaussian distributed:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a559961b-f113-49c4-831e-707757c833ea.png" style="width:47.25em;height:1.75em;"/></div>
<p class="mce-root">We are now able to maximize <span><em>Q</em>(<em>θ</em>;<em>θ</em></span><em><sub>t</sub></em><span>) with respect to <em>A</em> and Ω, considering <em>θ<sub>t</sub></em>=(<em>A<sub>t</sub></em>, Ω<em><sub>t</sub></em>) and both the conditional expectation and the second moment computed according to the previous estimation <em>θ<sub>t-1</sub></em>=(<em>A<sub>t-1</sub></em>, Ω<em><sub>t-1</sub>)</em>. For this reason, they are not involved in the derivation process. We are adopting the convention that the term subject to maximization is computed at time <em>t</em>, while all the others are obtained through the previous estimations (<em>t</em> - <em>1</em>):</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/286253c5-2e8b-49d8-9748-c066e1c65a9a.png" style="width:40.58em;height:4.17em;"/></div>
<p>The expression for <em>A</em><sub><em>t</em> </sub>is therefore as follows (<em>Q</em> is the biased input covariance matrix <em>E</em>[<em>X<sup>T</sup>X</em>] for a zero-centered dataset):</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/fb5e1019-bad7-4da0-909e-6632d6c7de11.png" style="width:55.00em;height:6.92em;"/></div>
<p class="mce-root">In the same way, we can obtain an expression for Ω<em><sub>t</sub></em> by computing the derivative with respect to Ω<em><sup>-1</sup></em> (this choice simplifies the calculation and doesn't affect the result, because we must set the derivative equal to zero):</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/08c62f0b-649d-4300-9f88-91bb6883e290.png" style="width:52.50em;height:8.42em;"/></div>
<p>The derivative of the first term, which is the determinant of a real diagonal matrix, is obtained using the adjugate matrix <em>Adj</em>(Ω) and exploiting the properties of the inverse matrix <em>T<sup>-1</sup></em> = <em>det(T)<sup>-1</sup>Adj(T)</em> and the properties <em>det(T)<sup>-1</sup> = det(T<sup>-1</sup>)</em> and <em><span>det(T</span><sup>T</sup><span>) = det(T)</span></em>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3cefb8de-78af-42e0-80a6-f87cb9177396.png" style="width:44.50em;height:5.42em;"/></div>
<p>The expression for Ω<sub>t</sub> (imposing the diagonality constraint) is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/36b470f5-fcf0-441e-81e0-9f5ffdf6d963.png" style="width:51.83em;height:6.17em;"/></div>
<p>Summarizing the steps, we can define the complete<strong> </strong>FA algorithm:</p>
<ol>
<li>Set random initial values for <em>A<sup>(0)</sup></em> and Ω<em><sup>(0)</sup></em></li>
<li>Compute the biased input covariance matrix <em>Q = E[X<sup>T</sup>X]</em></li>
<li>E-Step: Compute <em>A<sup>(t)</sup></em>, <span>Ω</span><em><sup>(t)</sup></em>, and <em>K<sup>(t)</sup></em></li>
<li>M-Step: <span>Compute A</span><sup>(t+1)</sup><span>, </span><span>Ω</span><em><sup>(t+1)</sup></em>,<span> and <em>K</em></span><em><sup>(t+1)</sup></em> using the previous estimations and the formulas provided previously</li>
<li>Compute the matrices <em>B</em> and <em>Ψ</em> for the inverse model</li>
</ol>
<p>The process must be repeated until <em><span>A</span><sup>(t)</sup></em><span>, </span><span>Ω</span><em><sup>(t)</sup></em>,<span> and <em>K</em></span><em><sup>(t)</sup></em> stop modifying their values (using a threshold) together with a constraint on the maximum number of iterations. The factors can be easily obtained using the inverse model <em>z = Bx + λ</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An example of factor analysis with Scikit-Learn</h1>
                </header>
            
            <article>
                
<p>We can now make an example of <span><span>FA</span></span> with Scikit-Learn using the MNIST handwritten digits dataset (70,000 28 × 28 grayscale images) in the original version and with added heteroscedastic noise (<em>ω<sub>i</sub></em> randomly selected from [0, 0.75]).</p>
<p>The first step is to load and zero-center the original dataset (I'm using the functions defined in the first chapter, <a href="acff0775-f21c-4b6d-8ef2-c78713e21364.xhtml" target="_blank">Chapter 1</a>, <em>Machine Learning Model Fundamentals</em>):</p>
<pre>import numpy as np<br/><br/>from sklearn.datasets import fetch_mldata<br/><br/>digits = fetch_mldata('MNIST original')<br/>X = zero_center(digits['data'].astype(np.float64))<br/>np.random.shuffle(X)<br/><br/>Omega = np.random.uniform(0.0, 0.75, size=X.shape[1])<br/>Xh = X + np.random.normal(0.0, Omega, size=X.shape)</pre>
<p>After this step, the  <kbd>X </kbd>variable<span> </span> will contain the zero-center original dataset, while <kbd>Xh</kbd> is the noisy version. The following screenshot shows a random selection of samples from both versions:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/34920020-dfd7-48dd-b23b-9599754ded36.png" style="width:37.25em;height:19.08em;"/></div>
<p>We can perform FA on both datasets using the Scikit-Learn <kbd>FactorAnalysis</kbd> class with the <kbd>n_components=64</kbd> parameter and check the score (the average log-likelihood over all samples). If the noise variance is known (or there's a good estimation), it's possible to include the starting point through the <kbd>noise_variance_init</kbd> parameter; otherwise, it will be initialized with the identity matrix:</p>
<pre>from sklearn.decomposition import FactorAnalysis<br/><br/>fa = FactorAnalysis(n_components=64, random_state=1000)<br/>fah = FactorAnalysis(n_components=64, random_state=1000)<br/><br/>Xfa = fa.fit_transform(X)<br/>Xfah = fah.fit_transform(Xh)<br/><br/>print(fa.score(X))<br/>-2162.70193446<br/><br/>print(fah.score(Xh))<br/>-3046.19385694</pre>
<p>As expected, the presence of noise has reduced the final accuracy (MLE). Following an example provided by <em>A. Gramfort</em> and <em>D. A. Engemann</em> in the original Scikit-Learn documentation, we can create a benchmark for the MLE using the <em>Lodoit-Wolf</em> algorithm (a shrinking method for improving the condition of the covariance that is beyond the scope of this book.</p>
<p>For further information, read <em>A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices</em>, <em>Ledoit O.</em>, <em>Wolf M.</em>, <em>Journal of Multivariate Analysis</em>, 88, 2/2004":</p>
<pre>from sklearn.covariance import LedoitWolf<br/><br/>ldw = LedoitWolf()<br/>ldwh = LedoitWolf()<br/><br/>ldw.fit(X)<br/>ldwh.fit(Xh)<br/><br/>print(ldw.score(X))<br/>-2977.12971009<br/><br/>print(ldwh.score(Xh))<br/>-2989.27874799</pre>
<p>With the original dataset, FA performs much better than the benchmark, while it's slightly worse in the presence of heteroscedastic noise. The reader can try other combinations using the grid search with different numbers of components and noise variances, and experiment with the effect of removing the zero-centering step. It's possible to plot the extracted components using the <kbd>components_</kbd> instance variable:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8e09541a-e1bf-4653-9b59-b9c0ba0f3647.png" style="width:24.83em;height:25.08em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">A plot of the 64 components extracted with the factor analysis on the original dataset</div>
<p>A careful analysis shows that the components are a superimposition of many low-level visual features. This is a consequence of the assumption to have a Gaussian prior distribution over the components (<em>z</em> ∼ <em>N(0, I)</em>). In fact, one of the disadvantages of this distribution is its intrinsic denseness (the probability of sampling values far from the mean is often too high, while in some case, it would be desirable to have a peaked distribution that discourages values not close to its mean, to be able to observe more selective components). Moreover, considering the distribution <em>p[Z|X; θ]</em>, the covariance matrix <em>ψ</em> could not be diagonal (trying to impose this constraint can lead to an unsolvable problem), leading to a resulting multivariate Gaussian distribution, which isn't normally made up of independent components. In general, the single variables <em>z<sub>i,</sub></em> (conditioned to an input sample, <em>x<sub>i</sub></em>) are statistically dependent and the reconstruction <em>x<sub>i,</sub></em> is obtained with the participation of almost all extracted features. In all these cases, we say that the <em>coding is dense</em> and the dictionary of features in <em>under-complete</em> (the dimensionality of the components is lower than <em>dim(x<sub>i</sub>)</em>).</p>
<p>The lack of independence can be also an issue considering that any orthogonal transformation <em>Q</em> applied to <em>A</em> (the factor loading matrix) don't affect the distribution <em>p[X|Z, θ]</em>. In fact, as <em>QQ<sup>T</sup>=I</em>, the following applies:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/14717969-bbbd-4109-bb03-de12efafb53e.png" style="width:14.08em;height:1.50em;"/></div>
<p>In other words, any feature rotation <em>(x = AQz + ν)</em> is always a solution to the original problem and it's impossible to decide which is the real loading matrix. All these conditions lead to the further conclusion that the mutual information among components is not equal to zero and neither close to a minimum (in this case, each of them carries a specific portion of information). On the other side, our main goal was to reduce the dimensionality. Therefore, it's not surprising to have dependent components because we aim to preserve the maximum amount of original information contained in <em>p(X)</em> (remember that the amount of information is related to the entropy and the latter is proportional to the variance).</p>
<p>The same phenomenon can be observed in the PCA (which is still based on the Gaussian assumption), but in the last paragraph, we're going to discuss a technique, called<strong> </strong>ICA, whose goal is to create a representation <span>of each sample </span>(without the constraint of the dimensionality reduction) after starting from a set of statistically independent features. This approach, even if it has its peculiarities, belongs to a large family of algorithms called <em>sparse coding</em>. In this scenario, if the corresponding dictionary has <em>dim(z<sub>i</sub>) &gt; dim(x<sub>i</sub>),</em>it is called <em>over-complete</em> (of course, the main goal is no longer the dimensionality reduction).</p>
<p>However, we're going to consider only the case when the dictionary is at most complete <em><span>dim(z</span><sub>i</sub><span>) = dim(x</span><sub>i</sub></em><span><em>)</em>, because ICA with over-complete dictionaries requires a more complex approach. The level of sparsity, of course, is proportional to <em>dim(z<sub>i</sub>)</em> and with ICA, it's always achieved as a secondary goal (the primary one is always the independence between components).</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Principal Component Analysis</h1>
                </header>
            
            <article>
                
<p>Another common approach to the problem of reducing the dimensionality of a high-dimensional dataset is based on the assumption that, normally, the total variance is not explained equally by all components. If <em>p<sub>data</sub></em> is a multivariate Gaussian distribution with covariance matrix <em><span>Σ</span></em>, then the entropy (which is a measure of the amount of information contained in the distribution) is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/779ac8db-243b-486a-8f23-8a7426e7c89e.png" style="width:11.50em;height:2.25em;"/></div>
<p class="mce-root">Therefore, if some components have a very low variance, they also have a limited contribution to the entropy, providing little additional information. Hence, they can be removed without a high loss of accuracy.</p>
<p class="mce-root">Just as we've done with FA, let's consider a dataset drawn from <em>p<sub>data</sub></em> ∼ <em>N(0, Σ)</em><sub> </sub>(for simplicity, we assume that it's zero-centered, even if it's not necessary):</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/14849ea4-4ed0-4929-ba73-832ab5135d7e.png" style="width:22.17em;height:1.58em;"/></div>
<p>Our goal is to define a linear transformation, <em>z = A<sup>T</sup>x</em> (a vector is normally considered a column, therefore <em>x</em> has a shape <em>(n × 1)</em>), such as the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/871b2980-2211-4a6d-93df-a285b3aee07e.png" style="width:10.58em;height:3.00em;"/></div>
<p>As we want to find out the directions where the variance is higher, we can build our transformation matrix, <em>A,</em> starting from the eigen decomposition of the input covariance matrix, <span><em>Σ</em> (which is real, symmetric, and positive definite):</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/685886b8-fd80-4df1-9944-b1dd724b8e7c.png" style="width:5.75em;height:1.25em;"/></div>
<p><em>V</em> is an <em>(n × n)</em> matrix containing the eigenvectors (as columns), while Ω is a diagonal matrix containing the eigenvalues. Moreover, <em>V</em> is also orthogonal, hence the eigenvectors constitute a basis. An alternative approach is based on the <strong>singular value decomposition</strong> (<strong>SVD</strong>), which has an incremental variant and there are algorithms that can perform a decomposition truncated at an arbitrary number of components, speeding up the convergence process (such as the Scikit-Learn implementation <kbd>TruncatedSVD</kbd>).</p>
<p>In this case, it's immediately noticeable that the sample covariance is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/b19af4dc-fb3a-46f9-a690-6bcedab0fcc3.png" style="width:28.33em;height:2.83em;"/></div>
<p>If we apply the SVD to the matrix <em>X</em> (each row represents a single sample with a shape <em>(1, n)</em>), we obtain the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/80bb6690-725d-46c3-b8fc-889d102f8832.png" style="width:34.00em;height:1.67em;"/></div>
<p><em>U</em> is a unitary matrix containing (as rows) the left singular vectors (the eigenvectors of <em>XX<sup>T</sup></em>), <em>V</em> (also unitary) contains (as rows) the right singular vectors (corresponding to the eigenvectors of <em>X<sup>T</sup>X</em>), while <em>Λ</em> is a diagonal matrix containing the singular values of <em>Σ<sub>s</sub></em> (which are the square roots of the eigenvalues of both <em>XX<sup>T </sup></em>and <em>X<sup>T</sup>X</em>). Conventionally, the eigenvalues are sorted by descending order and the eigenvectors are rearranged to match the corresponding position.</p>
<p>Hence, we can directly use the matrix <span><em>Λ</em> to select the most relevant eigenvalues (the square root is an increasing function and doesn't change the order) and the matrix <em>V</em> to retrieve the corresponding eigenvectors (the factor <em>1/M</em> is a proportionality constant). In this way, we don't need to compute and eigen decompose the covariance matrix <em>Σ</em> (contains <em>n × n</em> elements) and we can exploit some very fast approximate algorithms that work only with the dataset (without computing <em>X<sup>T</sup>X</em>). Using the SVD, t</span><span>he transformation of <em>X</em> can be done directly, considering that <em>U</em> and <em>V</em> are unitary matrices (this means that <em>UU<sup>T</sup></em> = <em>U<sup>T</sup>U</em> = <em>I</em>; therefore, the conjugate transpose is also the inverse):</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/9b9e58f1-76cf-4488-b718-eb9cc004ae7c.png" style="width:12.50em;height:1.25em;"/></div>
<p>Right now, <em>X</em> has only been projected in the eigenvector space (it has been simply rotated) and its dimensionality hasn't changed. However, from the definition of the eigenvector, we know that the following is true:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2c9a5609-fbda-4231-b881-0c02e8f1ded0.png" style="width:4.42em;height:1.17em;"/></div>
<p>If <em>λ</em> is large, the projection of <em>v</em> will be amplified proportionally to the variance explained by the direction of the corresponding eigenvector. Therefore, if it has not been already done, we can sort (and rename) the eigenvalues and the corresponding eigenvectors to have the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8a9575b3-b16c-49aa-8b86-97e648bb289d.png" style="width:11.67em;height:1.50em;"/></div>
<p>If we select the first top <em>k</em> eigenvalues, we can build a transformation matrix based on the corresponding eigenvectors (principal components) that projects <em>X</em> onto a subspace of the original eigenvector space:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/fa6a4060-db56-4110-9299-7892d99b2b60.png" style="width:23.92em;height:1.75em;"/></div>
<p>Using the SVD, instead of <em>A<sub>k</sub></em>, we can directly truncate <em>U</em> and <em>Λ</em>, creating the matrices <em>U<sub>k</sub></em> (which contains only the top k eigenvectors) and <em><span>Λ</span><sub>k</sub></em>, a diagonal matrix with the top k eigenvalues.</p>
<p>When choosing the value for <em>k</em>, we are assuming that the following is true:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/fbf9019c-eeb6-4ec9-b961-a4c0cfe08aa6.png" style="width:24.67em;height:1.33em;"/></div>
<p>To achieve this goal, it is normally necessary to compare the performances with a different number of components. In the following graph, there's a plot where the variance ratio (variance explained by component n/total variance) and the cumulative variance are plotted as functions of the components:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/911c3e26-23fd-4da1-9c5a-f0d5a3fcffa0.png" style="width:56.75em;height:22.92em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Explained variance per component (left) and cumulative variance per component (right)</div>
<p>In this case, the first 10 components are able to explain 80% of the total variance. The remaining 25 components have a slighter and slighter impact and could be removed. However, the choice must be always based on the specific context, considering the loss of value induced by the loss of information.</p>
<div class="packt_tip">A trick for determining the right number of components is based on the analysis of the eigenvalues of X. After sorting them, it's possible to consider the differences between subsequent values d = {λ<sub>1</sub> - λ<sub>2</sub>, λ<sub>2</sub> - λ<sub>3</sub>, ..., λ<sub>n-1</sub> - λ<sub>n</sub>}. The highest difference<span> </span><em><span>λ</span><sub>k</sub><span> - λ</span><sub>k+1</sub></em> determines the index <em>k</em> of a potential optimal reduction (obviously, it's necessary to consider a constraint on the minimum value, because normally <em><span>λ</span><sub>1</sub><span> - λ</span><sub>2</sub></em> is the highest difference). For example, if d = {4, 4, 3, 0.2, 0.18, 0.05} the original dimensionality is n=6; however, <em><span>λ</span><sub>4</sub><span> - λ</span></em><sub><em>5</em> </sub>is the smallest difference, so, it's reasonable to reduce the dimensionality to <em>(n + 1) - k = 3</em>. The reason is straightforward, the eigenvalues determine the magnitude of each component, but we need a relative measure because the scale changes. In the example, the last three eigenvectors point to directions where the explained variance is negligible when compared to the first three components. </div>
<p>Once we've defined the transformation matrix <em>A<sub>k</sub></em>, it's possible to perform the actual projection of the original vectors in the new subspace, through the relation:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5ef33cb7-a521-48bb-8f78-5ac18de9fb4b.png" style="width:31.00em;height:1.92em;"/></div>
<p>The complete transformation of the whole dataset is simply obtained as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c18c569e-5d93-4006-8ffa-3673914f2e42.png" style="width:9.50em;height:1.25em;"/></div>
<p class="mce-root">Now, let's analyze the new covariance matrix <em>E[Z<sup>T</sup>Z]</em>. If the original distribution <em>p<sub>data</sub> <span>x ∼ N(0, Σ)</span></em>, <em>p(z)</em> will also be Gaussian with mean and covariance:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ba582816-1287-4ec2-b1a5-c23c22c33cc2.png" style="width:45.00em;height:3.25em;"/></div>
<p>We know that <em>Σ</em> is orthogonal; therefore, <em>v<sub>i</sub> • v<sub>j</sub> = 0</em> if <em>i ≠ j</em>. If we analyze the term <em>A<sup>T</sup>V</em>, we get the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/30a5b689-ff6a-4f72-98cb-1316a6659234.png" style="width:50.75em;height:15.33em;"/></div>
<p class="mce-root">Considering that Ω is diagonal, the resulting matrix <em>Σ<sub>z</sub></em> will be diagonal as well. This means that the PCA decorrelates the transformed covariance matrix. At the same time, we can state that every algorithm that decorrelates the input covariance matrix performs a PCA (with or without dimensionality reduction). For example, the <em>whitening process</em> is a particular PCA without dimensionality reduction, while Isomap (see <a href="c23d1792-167f-416e-a848-fa7a10777697.xhtml">Chapter 3</a>,<a href="c23d1792-167f-416e-a848-fa7a10777697.xhtml"/> <em>Graph-Based Semi-Supervised Learning</em>) performs the same operation working with the Gram matrix with a more geometric approach. This result will be used in <a href="caf0e7e4-42e3-4807-a97d-6f9968eddb29.xhtml">Chapter 6</a>, <em>Hebbian Learning</em>, to show how some particular neural networks can perform a PCA without eigen decomposing <em>Σ</em>.</p>
<p>Let's now consider a FA with homoscedastic noise. We have seen that the covariance matrix of the conditional distribution, <em>p(X|Z; θ),</em> is equal to <em>AA<sup>T</sup> + Ω</em>. In the case of homoscedastic noise, it becomes <em>AA<sup>T</sup> + ωI</em>. For a generic covariance matrix, <em>Σ</em>, it's possible to prove that adding a constant diagonal matrix <em>(<span>Σ + aI)</span></em> doesn't modify the original eigenvectors and shifts the eigenvalues by the same quantity:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/84cf4d44-0842-4919-bdb7-5ceffdf8b011.png" style="width:29.83em;height:1.58em;"/></div>
<p>Therefore, we can consider the generic case of absence of noise without loss of generality. We know that the goal of FA (with Ω = (0)) is finding the matrix, <em>A,</em> so that <em>AA<sup>T</sup> ≈ Q</em> (the input covariance). Hence, thanks to the symmetry and imposing the asymptotic equality, we can write the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/812743e5-3e18-42d4-8efa-47a614ff5e97.png" style="width:39.67em;height:3.58em;"/></div>
<p class="mce-root">This result implies that the FA is a more generic (and robust) way to manage the dimensionality reduction in the presence of heteroscedastic noise, and the PCA is a restriction to homoscedastic noise. When a PCA is performed on datasets affected by heteroscedastic noise, the MLE worsens because the different noise components, altering the magnitude of the eigenvalues at different levels, can drive to the selection of eigenvectors that, in the original dataset, explain only a low percentage of the variance (and in a noiseless scenario, it would be normally discarded in favor of more important directions). If you think of the example discussed at the beginning of the previous paragraph, we know that the noise is strongly heteroscedastic, but we don't have any tools to inform the PCA to cope with it and the variance of the first component will be much higher than expected, considering that the two sources are identical. Unfortunately, in a real- life scenario, the noise is correlated and neither a factor nor a PCA can efficiently solve the problem when the noise power is very high. In all those cases, more sophisticated denoising techniques must be employed. Whenever, instead, it's possible to define an approximate diagonal noise covariance matrix, FA is surely more robust and efficient than PCA. The latter should be considered only in noiseless or <em>quasi-</em>noiseless scenarios. In both cases, the results can never lead to well-separated features. For this reason, the ICA has been studied and many different strategies have been engineered.</p>
<p>The complete algorithm for the PCA is as follows:</p>
<ol>
<li>Create a matrix <em>X<sup>(M × n)</sup></em><span> </span>containing all the samples x<sub>i</sub><span> </span>as rows
<ol>
<li>Eigen decomposition version:<br/>
<ol>
<li>Compute the covariance matrix <em>Σ = [X<sup>T</sup>X]</em></li>
<li>Eigen decompose <em>Σ = VΩV<sup>T</sup></em></li>
</ol>
</li>
<li>SVD version:
<ol>
<li>Compute the SVD on the matrix <em>X = UΛV<sup>T</sup></em></li>
</ol>
</li>
<li>Select the top k eigenvalues (from Ω or Λ) and the corresponding eigenvectors (from V)</li>
<li>Create the matrix A with shape <em>(n × k),</em> whose columns are the top k eigenvectors (each of them has a shape (n <span>× 1))</span></li>
<li>Project the dataset into the low-dimensional space Z = XA (eigen decomposition) or <em>Z = U<span>Λ (SVD)</span></em></li>
</ol>
</li>
</ol>
<div class="packt_infobox">Some packages (such as Scipy, which is the backend for many NumPy function, such as<kbd>np.linalg.svd()</kbd>) return the matrix V (right singular vectors) already transposed. In this case, it's necessary to use <em>V<sup>T</sup></em> instead of V in step 3 of the algorithm. I suggest always checking the documentation when implementing these kinds of algorithms.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An example of PCA with Scikit-Learn</h1>
                </header>
            
            <article>
                
<p>We can repeat the same experiment made with the FA and heteroscedastic noise to assess the MLE score of the PCA. We are going to use the <kbd>PCA</kbd> class with the same number of components (<kbd>n_components=64</kbd>). To achieve the maximum accuracy, we also set the  <kbd>svd_solver='full' </kbd><span>parameter</span>, to force Scikit-Learn to apply a full SVD instead of the truncated version. In this way, the top eigenvalues are selected only after the decomposition, avoiding the risk of imprecise estimations:</p>
<pre>from sklearn.decomposition import PCA<br/><br/>pca = PCA(n_components=64, svd_solver='full', random_state=1000)<br/>Xpca = pca.fit_transform(Xh)<br/><br/>print(pca.score(Xh))<br/>-3772.7483580391995</pre>
<p>The result is not surprising: the MLE is much lower than FA, because of the wrong estimations made due to the heteroscedastic noise. I invite the reader to compare the results with different datasets and noise levels, considering that the training performance of PCA is normally higher than FA. Therefore, when working with large datasets, a good trade-off is surely desirable. As with FA, it's possible to retrieve the components through the  <kbd>components_ </kbd><span>instance variable</span>.</p>
<p>It's interesting to check the total explained variance (as a fraction of the total input variance) through the component-wise instance array <kbd>explained_variance_ratio_</kbd>:</p>
<pre>print(np.sum(pca.explained_variance_ratio_))<br/>0.862522337381</pre>
<p>With 64 components, we are explaining 86% of the total input variance. Of course, it's also useful to compare the explained variance using a plot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/01a974ed-00a8-41bd-aabe-e99aedde239f.png" style="width:42.17em;height:26.08em;"/></div>
<p>As usual, the first components explain the largest part of the variance; however, after about the twentieth component, each contribution becomes lower than 1% (decreasing till about 0%). This analysis suggests two observations: it's possible to further reduce the number of components with an acceptable loss (using the previous snippet, it's easy to extend the sum only the first <em>n</em> components and compare the results) and, at the same time, the PCA will be able to overcome a higher threshold (such as 95%) only by adding a large number of new components. In this particular case, we know that the dataset is made up of handwritten digits; therefore, we can suppose that the tail is due to secondary differences (a line slightly longer than average, a marked stroke, and so on); hence, we can drop all the components with n &gt; 64 (or less) without problems (it's also easy to verify visually a rebuilt image using the <kbd>inverse_transform()</kbd> method). However, it is always best practice to perform a complete analysis before moving on to further processing steps, particularly when the dimensionality of X is high.</p>
<div class="packt_infobox">Another interesting approach to determine the optimal number of components has been proposed by Minka (<em>Automatic Choice of Dimensionality for PCA</em>, <em>Minka T.P.</em>, <em>NIPS</em> 2000"<span>) and it's based on the Bayesian model selection. The idea is to use the MLE to optimize the likelihood <em>p(X|k)</em> where k is a parameter indicating the number of components. In other words, it doesn't start analyzing the explained variance, but determines a value of <em>k &lt; n</em> so that the likelihood keeps being the highest possible (implicitly, k will explain the maximum possible variance under the constraint of <em>max(k) = k<sub>max</sub></em>). The theoretical foundation (with tedious mathematical derivations) of the method is presented in the previously mentioned paper however, it's possible to use this method with Scikit-Learn by setting the <kbd>n_components='mle'</kbd> and <kbd>svd_solver='full'</kbd> parameters.<br/></span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Independent component analysis</h1>
                </header>
            
            <article>
                
<p>We have seen that the factors extracted by a PCA are decorrelated, but not independent. A classic example is the <em>cocktail party:</em> we have a recording of many overlapped voices and we would like to separate them. Every single voice can be modeled as a random process and it's possible to assume that they are statistically independent (this means that the joint probability can be factorized using the marginal probabilities of each source). Using FA or PCA, we are able to find uncorrelated factors, but there's no way to assess whether they are also independent (normally, they aren't). In this section, we are going to study a model that is able to produce sparse representations (when the dictionary isn't under-complete) with a set of statistically independent components.</p>
<p>Let's assume we have a zero-centered and whitened dataset <em>X</em> sampled from <em>N(0, I)</em> and noiseless linear transformation:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/019cead7-d719-4320-97b3-2464b44ea74b.png" style="width:30.83em;height:3.08em;"/></div>
<p class="mce-root">In this case, the prior over, <em>z,</em> is modeled as a product of independent variables (<em>α</em> is the normalization factor), each of them represented as a generic exponential where the function <em>f<sub>k</sub>(z)</em> must be non-quadratic, that is, <em>p(z; θ)</em> cannot be Gaussian. Furthermore, we assume that the variance of <em>z<sub>i</sub></em> is equal to 1, therefore, <em>p(x|z; </em><span><em>θ) ∼ N(Az, AA<sup>T</sup>)</em>. The joint probability <em>p(X, z; θ) = p(X|z; θ)p(z|θ)</em> is equal to the following:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4246f391-0499-415a-9f92-52bd8ce9c61b.png" style="width:48.42em;height:4.67em;"/></div>
<p>If <em>X</em> has been whitened, <em>A</em> is orthogonal (the proof is straightforward); hence, the previous expression can be simplified. However, applying the EM algorithm requires determining <em>p(z|X; θ)</em> and this is quite difficult. The process could be easier after choosing a suitable prior distribution for <em>z</em>, that is, <em>f<sub>k</sub>(z)</em>, but as we discussed at the beginning of the chapter, this assumption can have dramatic consequences if the real factors are distributed differently. For these reasons, other strategies have been studied.</p>
<p>The main concept that we need to enforce is having a non-Gaussian distribution of the factors. In particular, we'd like to have a peaked distribution (inducing sparseness) with heavy tails. From the theory, we know that the standardized fourth moment (also called <em>Kurtosis</em>) is a perfect measure:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f63dbea7-b46f-4bd9-9ee8-6c69aa3fe559.png" style="width:15.58em;height:3.50em;"/></div>
<p class="mce-root">For a Gaussian distribution, <em>Kurt[X]</em> is equal to three (which is often considered as the reference point, determining the so called <em>Excess Kurtosis</em> = <em>Kurtosis - 3</em>), while it's larger for a family of distributions, called <em>Leptokurtotic</em> or super-Gaussian, which are peaked and heavy-tailed (also, the distributions with <em>Kurt[X] &lt; 3</em>, called <em>Platykurtotic</em> or sub-Gaussian, can be good candidates, but they are less peaked and normally only the super-Gaussian distributions are taken into account). However, even if accurate, this measure is very sensitive to outliers because of the fourth power. For example, if <em>x ∼ N(0, 1)</em> and <em>z = x + ν</em>, where <span><em>ν</em> is a noise term that alters a few samples, increasing their value to two, the result can be a super-Gaussian</span> (<em>Kurt[x] &gt; 3</em>) <span>even if, after filtering the outliers out, the distribution has <em>Kurt[x] = 3 (Gaussian)</em>.</span></p>
<p>To overcome this problem, <span>Hyvärinen and Oja</span> (<em>Independent Component Analysis: Algorithms and Applications</em>, <em>Hyvarinen A.</em>, <em>Oja E.</em>, Neural Networks 13/2000) proposed a solution based on another measure, the <em>negentropy</em>. We know that the entropy is proportional to the variance and, given the variance, the Gaussian distribution has the maximum entropy (for further information, read <em>Mathematical Foundations of Information Theory</em>, <em>Khinchin A. I., Dover Publications</em>); therefore, we can define the measure:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7966184b-0868-4d64-bf41-803973421c7f.png" style="width:19.67em;height:1.92em;"/></div>
<p>Formally, the negentropy of <em>X</em> is the difference between the entropy of a Gaussian distribution with the same covariance and the entropy of <em>X</em> (we are assuming both zero-centered). It's immediately possible to understand that <em>H<sub>N</sub>(X) ≥ 0</em>, hence the only way to maximize it is by reducing <em>H(X)</em>. In this way, <em>X</em> becomes less random, concentrating the probability around the mean (in other words, it becomes super-Gaussian). However, the previous expression cannot be easily adapted to closed-form solutions, because <em>H(X)</em> needs to be computed over all the distribution of <em>X</em>, which must be estimated. For this reason, the same authors proposed an approximation based on non-quadratic functions (remember that in the context of ICA, a quadratic function can be never be employed because it would lead to a Gaussian distribution) that is useful to derive a fixed-point iterative algorithm called <em>FastICA</em><strong> </strong>(indeed, it's really faster than EM).</p>
<p>Using k functions <em>f<sub>k</sub>(x)</em>, the approximation becomes as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0993dabf-0638-4931-8d07-0e436cb9c81a.png" style="width:37.83em;height:3.83em;"/></div>
<p class="mce-root">In many real-life scenarios, a single function is enough to achieve a reasonable accuracy and one of the most common choices for f(x) is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/b421b596-abdb-46d4-a009-bf899ca38553.png" style="width:24.67em;height:3.08em;"/></div>
<p class="mce-root">In the aforementioned paper, the reader can find some alternatives that can be employed when this function fails in forcing statistical independence between components.</p>
<p class="mce-root">If we invert the model, we get <em>z = Wx</em> with <em>W = A<sup>-1</sup></em>; therefore, considering a single sample, the approximation becomes as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/13fc3cb9-3bc8-40c8-b86c-562573caee5e.png" style="width:28.08em;height:1.92em;"/></div>
<p>Clearly, the second term doesn't depend on <em>w</em> (in fact, it's only a reference) and can be excluded from the optimization. Moreover, considering the initial assumptions, <em>E[Z<sup>T</sup>Z]=W E[X<sup>T</sup>X] W<sup>T</sup> = I</em>, therefore <em>WW<sup>T</sup> = I, i.e. ||w||<sup>2</sup> = 1</em>. Hence, our goal is to find the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5e460161-d1a1-47ac-bb9e-9b966292ef27.png" style="width:27.08em;height:1.75em;"/></div>
<p>In this way, we are forcing the matrix <em>W</em> to transform the input vector <em>x</em>, so that <em>z</em> has the lowest possible entropy; therefore, it's super-Gaussian. The maximization process is based on convex optimization techniques that are beyond the scope of this book (the reader can find all the details of Lagrange theorems in <em>Luenberger D. G., Optimization by Vector Space Methods, Wiley</em>); therefore, we directly provide the iterative step that must be performed:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8463cdf0-16e2-421f-9612-23fb5740d83f.png" style="width:22.42em;height:1.83em;"/></div>
<p>Of course, to ensure <em><span>||w||</span><sup>2</sup></em><span><em> = 1</em>,</span> after each step, the weight vector w must be normalized <em>(w<sub>t+1</sub> = w<sub>t+1</sub> / ||w<sub>t+1</sub>||)</em>.</p>
<p>In a more general context, the matrix <em>W</em> contains more than one weight vector and, if we apply the previous rule to find out the independent factors, it can happen that some elements, <em>w<sub>i</sub><sup>T</sup>x,</em> are correlated. A strategy to avoid this problem is based on the gram-schmidt orthonormalization process, which decorrelates the components one by one, subtracting the projections of the current component <em>(w<sub>n</sub>)</em> onto all the previous ones <em>(w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>n-1</sub>)</em> to <em><span>w</span><sub>n</sub></em>. In this way, <em><span>w</span><sub>n</sub></em> is forced to be orthogonal to all the other components.</p>
<p>Even if this method is simple and doesn't require much effort, it's preferable a global approach that can work directly with the matrix <em>W</em> at the end of an iteration (so that the order of the weights is not fixed). As explained in <em>Fast and robust fixedpoint</em><br/>
<em>algorithms for independent component analysis</em>, <em>Hyvarinen A.</em>, <em>IEEE Transactions on Neural Networks</em> this result can be achieved with a simple sub-algorithm that we are including in the final <em>FastICA</em> algorithm:</p>
<ol>
<li><span>Set random initial values for <em>W<sub>0</sub></em></span></li>
<li>Set a threshold <em>Thr</em> (for example 0.001) 
<ol>
<li>Independent component extraction</li>
<li>For each <em>w</em> in <em>W</em>:
<ol>
<li>While <em>||w<sub>t+1</sub> - w<sub>t</sub>|| &gt; Thr</em>:
<ol>
<li>Compute <em>w<sub>t+1</sub> = E[x · f<sup>'</sup>(w<sub>t</sub><sup>T</sup>x)] - <span>E[f</span><sup>''</sup><span>(w</span><sub>t</sub><sup>T</sup><span>x)] w<sub>t</sub></span></em></li>
<li><em><span>w<sub>t+1</sub> = w<sub>t+1</sub> / ||w<sub>t+1</sub>||</span></em></li>
</ol>
</li>
</ol>
</li>
<li>Orthonormalization</li>
<li>While <em>||W<sub>t+1</sub> - W<sub>t</sub>||<sub>F</sub> &gt; Thr</em>:
<ol>
<li><em>W<sub>t</sub> = W<sub>t</sub> / sqrt(||W<sub>t</sub>W<sub>t</sub><sup>T</sup>||)</em></li>
<li><em><span>W</span><sub>t</sub><sub>+1</sub><span> = (3/2)W<sub>t</sub> - (1/2)WW<sup>T</sup>W</span></em></li>
</ol>
</li>
</ol>
</li>
</ol>
<p class="mce-root">This process can be also iterated for a fixed number of times, but the best approach is based on using both a threshold and a maximum number of iterations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An example of FastICA with Scikit-Learn</h1>
                </header>
            
            <article>
                
<p>Using the same dataset, we can now test the performance of the ICA. However, in this case, as explained, we need to zero-center and whiten the dataset, but fortunately these preprocessing steps are done by the Scikit-Learn implementation (if the parameter <kbd>whiten=True</kbd> is omitted).</p>
<p>To perform the ICA on the MNIST dataset, we're going to instantiate the  <kbd>FastICA<span> </span></kbd>class, passing the arguments <kbd>n_components=64</kbd> and the maximum number of iterations <kbd>max_iter=5000</kbd>. It's also possible to specify which function will be used to approximate the negentropy; however, the default is <em>log cosh(x)</em>, which is normally a good choice:</p>
<pre>from sklearn.decomposition import FastICA<br/><br/>fastica = FastICA(n_components=64, max_iter=5000, random_state=1000)<br/>fastica.fit(X)</pre>
<p>At this point, we can visualize the components (which are always available through the  <kbd>components_ </kbd><span>instance variance</span>):</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/50c13e9a-68a3-4d63-88fd-56de1116c432.png" style="width:20.42em;height:20.25em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Independent components of the MNIST dataset extracted by the FastICA algorithm (64 components)</div>
<p>There are still some redundancies (the reader can try to increase the number of components) and background noise; however, it's now possible to distinguish some low-level features (such as oriented stripes) that are common to many digits. This representation isn't very sparse yet. In fact, we're always using 64 components (like for FA and PCA); therefore, the dictionary is under-complete (the input dimensionality is 28 × 28 = 784). To see the difference, we can repeat the experiment with a dictionary ten times larger, setting <kbd>n_components=640</kbd>:</p>
<pre>fastica = FastICA(n_components=640, max_iter=5000, random_state=1000)<br/>fastica.fit(Xs)</pre>
<p>A subset of the new components (100) is shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f2ba3dfa-30ae-4a9d-aaa4-b8b58c52b910.png" style="width:20.83em;height:20.75em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><span>Independent components of the MNIST dataset extracted by the FastICA algorithm (640 components)</span></div>
<p>The structure of these components is almost elementary. They represent oriented stripes and positional dots. To check how an input is rebuilt, we can consider the mixing matrix <em>A</em> (which is available as the <kbd>mixing_<span> </span></kbd>instance variable). Considering the first input sample, we can check how many factors have a weight less than half of the average:</p>
<pre>M = fastica.mixing_<br/>M0 = M[0] / np.max(M[0])<br/><br/>print(len(M0[np.abs(M0) &lt; (np.mean(np.abs(M0)) / 2.0)]))<br/>233</pre>
<p>The sample is rebuilt using approximately 410 components. The level of sparsity is higher, but considering the granularity of the factors, it's easy to understand that many of them are needed to rebuild even a single structure (like the image of a 1) where long lines are present. However, this is not a drawback because, as already mentioned, the main goal of the ICA is to extract independent components. Considering an analogy with the <em>cocktail party</em> example, we could deduce that each component represents a phoneme, not the complete sound of a word or a sentence.</p>
<p>The reader can test a different number of components and compare the results with the ones achieved by other sparse coding algorithms (such as Dictionary Learning or Sparse PCA).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Addendum to HMMs</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we discussed how it's possible to train a HMM using the forward-backward algorithm<strong> </strong>and we have seen that it is a particular application of the EM algorithm. The reader can now understand the internal dynamic in terms of E and M steps. In fact, the procedure starts with randomly initialized A and B matrices and proceeds in an alternating manner:</p>
<ul>
<li><strong>E-Step</strong>:
<ul>
<li>The estimation of the probability <em>α<sup>t</sup><sub>ij</sub></em> that the HMM is in the state <em>i</em> at time <em>t</em> and in the state <em>j</em> at time <em>t+1</em> given the observations and the current parameter estimations (A and B)</li>
<li>The estimation of the probability <em><span>β</span><sup>t</sup></em><sub><em>i</em> </sub>that the HMM is in the state <em>i</em> at time <em>t</em> <span>given the observations and the current parameter estimations (A and B)</span></li>
</ul>
</li>
<li><strong>M-Step</strong>:
<ul>
<li>Computing the new estimation for the transition probabilities <em>a<sub>ij</sub> (A)</em> and for the emission probabilities <em>b<sub>ip</sub> (B)</em></li>
</ul>
</li>
</ul>
<p>The procedure is repeated until the convergence is reached. Even if there's no explicit definition of a Q function, the E-step determines a split expression for the expected complete data likelihood of the model given the observations (using both the Forward and Backward algorithms), while the M-Step corrects parameters A and B to maximize this likelihood.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we presented the EM algorithm, explaining the reasons that justify its application in many statistical learning contexts. We also discussed the fundamental role of hidden (latent) variables, in order to derive an expression that is easier to maximize (the Q function).</p>
<p>We applied the EM algorithm to solve a simple parameter estimation problem and afterward to prove the Gaussian Mixture estimation formulas. We showed how it's possible to employ the Scikit-Learn implementation instead of writing the whole procedure from scratch (like in <a href="f6860c85-a228-4e63-96ac-22a0caf1a767.xhtml">Chapter 2</a>, <em>Introduction to Semi-Supervised Learning</em>).</p>
<p>Afterward, we analyzed three different approaches to component extraction. FA assumes that we have a small number of Gaussian latent variables and a Gaussian decorrelated noise term. The only restriction on the noise is to have a diagonal covariance matrix, so two different scenarios are possible. When we are in the presence of heteroscedastic noise, the process is an actual FA. When, instead, the noise is homoscedastic, the algorithm becomes the equivalent of a PCA. In this case, the process is equivalent to check the sample space in order to find the directions where the variance is higher. Selecting only the most important directions, we can project the original dataset onto a low-dimensional subspace, where the covariance matrix becomes decorrelated.</p>
<p>One of the problems of both FA and PCA is their assumption to model the latent variables with Gaussian distributions. This choice simplifies the model, but at the same time, yields dense representations where the single components are statistically dependent. For this reason, we have investigated how it's possible to force the factor distribution to become sparse. The resulting algorithm, which is generally faster and more accurate than the MLE, is called FastICA and its goal is to extract a set of statistically independent components with the maximization of an approximation of the negentropy.</p>
<p>In the end, we provided a brief explanation of the HMM forward-backward algorithm (discussed in the previous chapter) considering the subdivision into E and M steps. Other EM-specific applications will be discussed in the next chapters.</p>
<p>In the next chapter, we are going to introduce the fundamental concepts of Hebbian learning and self-organizing maps, which are still very useful to solve many specific problems, such as principal component extraction, and have a strong neurophysiological foundation.</p>


            </article>

            
        </section>
    </body></html>