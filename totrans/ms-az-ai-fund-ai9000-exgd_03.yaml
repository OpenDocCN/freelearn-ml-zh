- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Identify Common Machine Learning Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we’ve introduced you to AI technologies (such as computer vision or
    generative AI) as well as Microsoft’s principles for responsible AI. Now, it’s
    time to start talking about the substance of AI.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most important questions you might have about AI is how AI manages
    to know what it does. Just as humans learn, AI systems have been designed to be
    capable of learning. And, just like humans learn through a variety of mechanisms
    (such as memorization and practice or repetition), AI systems also learn through
    different techniques and scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be honest, though, the term **machine learning** is a bit of a misnomer.
    Since computers aren’t exactly sentient at this point, one might argue that they’re
    not capable of really learning. What they are capable of, however, is something
    quite useful: examining vast data sets to establish patterns and predict outcomes.
    Humans can sometimes be pretty good at recognizing patterns for small data sets.
    Once the data set includes tens of thousands or millions of data points, it becomes
    much more difficult for a human to keep up—and this is where machine learning
    excels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The core idea of machine learning is looking at these vast data sets and predicting
    outcomes or values of similar actions or scenarios. Examples of machine learning
    might include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A power company combining historical weather patterns and historical energy
    usage to estimate the load on an electric grid
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An insurance company using miles driven, whether driven hours are daylight or
    nighttime, and driver age to predict the likelihood of an accident
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A biologist researcher using visual data of animals to automate the identification
    of known species observed on cameras and highlight potentially unknown species
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In each of these cases, an AI system is trained on known data sets and then
    either asked questions or exposed to new data and is instructed to apply its past
    observations on the new data or queries to come up with new outputs or results.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll cover some of the high-level concepts related to machine
    learning. The objectives and skills we’ll cover in this chapter include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify regression machine learning scenarios
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify classification machine learning scenarios
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify clustering machine learning scenarios
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify features of deep learning techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you should be able to identify and describe some
    of the common machine learning scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s establish a little background information on machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding machine learning terminology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you’ve already learned, machine learning is another way to think about predicting
    outcomes based on observed data sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Machine learning models are essentially software applications that use mathematical
    functions to calculate output values based on input values. This process involves
    two main phases: **training** and **inferencing**.'
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During training, the model learns to predict output values based on input values
    by analyzing past observations. These past observations include both the **features**
    (input values) and **labels** (output values).
  prefs: []
  type: TYPE_NORMAL
- en: In a typical scenario, features are represented as variables denoted by *x*,
    while labels are denoted by *y*. Features can consist of multiple values, forming
    a **vector** represented by *[x1, x2, x3, ...],y*. For example, in predicting
    bottled water sales based on weather, weather measurements are features (*x*)
    and the number of bottles sold is the label (*y*).
  prefs: []
  type: TYPE_NORMAL
- en: An **algorithm** is then applied to the data to establish a relationship between
    the features and labels, creating a calculation to predict the label based on
    the features. The choice of algorithm depends on the type of predictive problem
    being addressed. The outcome is a **model** represented by a function *f*, where
    *y =* *f(x)*.
  prefs: []
  type: TYPE_NORMAL
- en: What are vectors and algorithms, anyway?
  prefs: []
  type: TYPE_NORMAL
- en: Vectors, foundational units of algebra, are essentially **tuples** (or collections)
    of values. These groups of values can be thought of as being similar to an array,
    though tuples can contain mixed data types such as strings, floating point numbers,
    and integers.
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, an algorithm is a set of functions, rules, formulas, or processes
    that an AI system uses to analyze data, discover insights, and predict outcomes.
    Algorithms represent the math that enables machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'The learning part of machine learning can be divided into two types: **supervised**
    and **unsupervised**. Each of these types has a variety of algorithms and processes
    associated with them.'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised machine learning techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Supervised machine learning encompasses algorithms that learn from data containing
    both input features and corresponding target labels. The goal is to uncover patterns
    that link the input features to their outcomes, enabling the model to forecast
    outcomes for new, unseen data. Let’s look at types of supervised learning techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Regression**-based learning is designed to identify and understand the relationship
    between independent and dependent variables and is frequently used to make business
    projections. Regression is a type of supervised learning where the output is a
    continuous number. Examples include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Predicting bottled water sales based on weather conditions such as temperature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating a dealership’s vehicle sales prices from amount of available inventory
    and previous sales prices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Classification** uses an algorithm to assign the training data to categories.
    Classification identifies or recognizes entities in the training data and attempts
    to draw conclusions about how the entities are defined or labeled. Classification
    involves categorizing data points into distinct classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Binary classification**: This predicts one of two possible outcomes. Examples
    include diagnosing diabetes from health metrics, assessing loan default risk from
    financial history, and predicting marketing response from consumer profiles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multiclass classification**: This predicts which one of several classes an
    observation belongs to. For instance, classifying an animal species based on physical
    features or a movie’s genre from its production details. Unlike binary classification,
    a single observation in multiclass classification is assigned to one exclusive
    category.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As you’ve already learned, algorithms are the mathematical formulas used to
    process data. From a supervised learning perspective, these algorithms are commonly
    used:'
  prefs: []
  type: TYPE_NORMAL
- en: '**AdaBoost** and **gradient boosting**: These methods enhance the accuracy
    of simple models by aggregating them into a more robust model. By sequentially
    correcting errors of a basic model using additional weak models, they collectively
    improve prediction accuracy. Boosting models can be applied to both classification
    and regression problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Artificial Neural Networks** (**ANNs**): ANNs are inspired by the human brain’s
    neural networks and are foundational to deep learning. They process data through
    interconnected units called neurons, learning to recognize patterns and make decisions
    over time. ANNs are used in a variety of contexts, such as natural language processing,
    speech and image recognition, and game-playing (such as chess and Go).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decision trees**: These algorithms predict outcomes or classify data by breaking
    down decisions into a tree-like structure of choices. Decision trees are transparent,
    making them easier to understand and validate compared to more opaque models such
    as neural networks. You might picture a decision tree as a type of flow chart,
    as shown in *Figure 3**.1*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Example of a simple decision tree](img/B22207_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – Example of a simple decision tree
  prefs: []
  type: TYPE_NORMAL
- en: '**Dimensionality reduction**: This technique reduces the complexity of data
    by decreasing the number of input features, focusing on retaining only the most
    relevant information. Principal component analysis is a common method used for
    this purpose.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**K-Nearest Neighbor** (**KNN**): KNN classifies data points based on the closest
    neighboring points in the data set. It calculates the distance (often Euclidean)
    between points and assigns a category based on the most common category among
    its nearest neighbors. KNN makes predictions based on the majority or average
    value of the nearest data points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linear regression**: This approach models the relationship between a dependent
    variable and one or more independent variables to predict continuous outcomes.
    Simple linear regression involves just one independent and one dependent variable.
    An example of a linear regression might be predicting house prices based on its
    square footage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logistic regression**: Used for binary outcomes (e.g., yes/no, true/false),
    logistic regression models the probability of a categorical dependent variable
    based on one or more independent variables, ideal for binary classification tasks.
    Logistic regression can be used in spam email detection by building a model based
    on the features of messages such as keywords that indicate spam content, source
    IP address, length of the email, volume of misspelled words, or other characteristics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Naïve Bayes**: Based on Bayes’ theorem, this technique assumes independence
    among predictors and is effective for text classification, spam detection, and
    recommendation systems. Variants include multinomial, Bernoulli, and Gaussian
    Naïve Bayes. Bayes algorithms are frequently used in text classification tasks
    such as spam detection and sentiment analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random forests**: This ensemble method uses multiple decision trees to make
    more reliable and accurate predictions by averaging their results, effectively
    reducing overfitting and variance in predictions. A common use case of a random
    forest algorithm might be detecting if a customer is likely to leave a subscription
    service (or churn) based on a number of features (number of calls made to support,
    length of calls, length of subscription service, telemetry data of how often the
    service used). Each of those features can be evaluated in a decision tree and
    then used together to predict a customer’s churn potential.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Support Vector Machines** (**SVM**): SVMs are used for classification and
    regression by finding the optimal boundary (hyperplane) that maximizes the margin
    or distance between different classes of data points, enhancing the model’s discriminative
    power. Imagine you have a collection of points on a graph that you need to divide
    into two groups. An SVM would determine how to draw a line that best separates
    them. The optimal line path would be one that divides the points on the plane,
    maximizing the gaps between the line and points. See *Figure 3**.2* for a very
    simple example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Simple representation of a support vector machine model](img/B22207_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – Simple representation of a support vector machine model
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs: []
  type: TYPE_NORMAL
- en: While you won’t see all of these individual algorithms on the AI-900 exam, they’re
    neat to learn about. You can explore the basic mathematical concepts behind many
    of these algorithms at sites such as [https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/](https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/)
    and [https://www.kdnuggets.com/a-beginner-guide-to-the-top-10-machine-learning-algorithms](https://www.kdnuggets.com/a-beginner-guide-to-the-top-10-machine-learning-algorithms).
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised machine learning techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unsupervised learning models are trained on data without labels, aiming to find
    underlying patterns or groupings in the data based on similarities.
  prefs: []
  type: TYPE_NORMAL
- en: '**Clustering** is a primary technique in unsupervised learning that groups
    data points based on feature similarities. Examples include categorizing different
    types of flowers or segmenting customers by purchasing habits. Unlike classification,
    clustering does not require pre-defined categories; the algorithm identifies these
    groups autonomously. Clustering can also be a preliminary step to define classes
    for a subsequent classification model, such as segmenting customers into categories
    for targeted marketing strategies.'
  prefs: []
  type: TYPE_NORMAL
- en: Semi-supervised machine learning techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Semi-supervised learning occupies a space between supervised and unsupervised
    learning. This technique combines both aspects of supervised learning (providing
    labeled input data) as well as unsupervised learning (training with unlabeled
    data).
  prefs: []
  type: TYPE_NORMAL
- en: Inferencing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the training phase is complete, the model can be used for **inferencing**
    or making predictions. The model acts as a software program encapsulating the
    learned function, allowing users to input feature values and receive predictions
    of corresponding labels. The predicted label is represented by *ŷ* (pronounced
    “y-hat”) to distinguish it from observed values.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding machine learning involves grasping these fundamental concepts
    of training and inferencing, as well as recognizing the role of algorithms in
    establishing predictive relationships between features and labels. By applying
    mathematical functions to data, machine learning models can make predictions and
    facilitate decision-making in various domains, from weather forecasting to medical
    diagnosis.
  prefs: []
  type: TYPE_NORMAL
- en: Identify regression machine learning scenarios
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regression models aim to forecast numerical outcomes using training data that
    encompasses input features along with their corresponding target values. The development
    of a regression model, as with any supervised learning approach, unfolds through
    several cycles. In each cycle, you select a suitable algorithm—often configurable
    with various parameters—to build the model. You then assess how well the model
    predicts outcomes and adjust it by experimenting with alternative algorithms and
    tuning the parameters. This iterative process continues until the model reaches
    a satisfactory level of prediction accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall process for regression training is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Divide the training data randomly to form a training set for model development,
    reserving a portion for model validation. For example, consider setting aside
    30-50% of the training data to test against later.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Employ a fitting algorithm, such as linear regression for regression models,
    to construct the model based on the training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, use the set aside validation data from *step 1* to evaluate the model’s
    effectiveness by making predictions and comparing these predicted values against
    the actual labels in the validation set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summarize the discrepancies between predicted and actual values to derive a
    performance metric reflecting the model’s prediction accuracy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Iterate this train–validate–evaluate cycle, experimenting with various algorithms
    and settings, until the model’s performance matches your expectations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Through these steps, you can build regression models to predict a number of
    real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Earlier, we discussed an example of predicting bottled water sales based on
    how warm it is. To see how regression training works, let’s dive into the bottled
    water sales example.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say you want to predict, based on the outside temperature, how many bottles
    of water you anticipate selling. This would be important to you as a vendor since
    it helps you understand how much you need to stock to meet the demand.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you need to gather historical data that will be used to train the model
    (as well as validate the model later). Take the following sample data set in *Table
    3.1*—it captures two critical pieces of data: how many bottles of water sold (*y*)
    at a given temperature (*x*):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Sample** | **Temperature (x)** | **Bottled water** **sales (y)** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 50 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 53 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 62 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 63 | 7 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 65 | 9 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 68 | 12 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 70 | 18 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 74 | 22 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 77 | 28 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 84 | 36 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 64 | 7 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 78 | 33 |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | 81 | 34 |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | 79 | 31 |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | 54 | 2 |'
  prefs: []
  type: TYPE_TB
- en: Table 3.1 – Bottled water sales
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to select the amount of data that we’ll use for training and
    the amount we’ll set aside for validation and testing. Let’s go ahead and take
    the first 10 rows for training our fictional model, leaving the last 5 rows for
    validation.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, an easy way to understand the relationship between temperature
    and bottles of water sold is to plot them on a simple graph, as shown in *Figure
    3**.3*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Temperature and bottles of water sold plotted on a graph](img/B22207_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – Temperature and bottles of water sold plotted on a graph
  prefs: []
  type: TYPE_NORMAL
- en: 'Through the training process, an algorithm applies a formula or function to
    calculate the value of *y* from the value of *x*. In this case, the algorithm
    used would be one of linear regression—one that calculates a straight line through
    the points. See *Figure 3**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Linear regression](img/B22207_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – Linear regression
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the graph, for every degree of change, the historical data
    trends upward. The slope can be expressed using the equation *y* = 1.11*x* – 60.02,
    where *x* is the temperature and *y* is the number of water bottles sold. Put
    another way, starting at 60 degrees, for every 1 degree increase in temperature,
    the number of water bottles sold increases by 1.11.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given this formula that’s been developed, the next step is to test the formula
    based on the remaining data in the training set. In this example, the additional
    data left in the training set has been plotted on the same graph and a linear
    regression run against them as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Validation data set composited on to original graph](img/B22207_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – Validation data set composited on to original graph
  prefs: []
  type: TYPE_NORMAL
- en: To test the formula, you could take a value from the held back training data
    (such as 81 degrees) and project the number of water bottles sold by calculating
    *y* = 1.11(81) – 60.02, which results in 30 water bottles (or 29.91 rounded up
    to the next whole number).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can repeat that process for every temperature value held back in the data
    set, using the function to calculate a prediction for the number of water bottles
    sold. See *Table 3.2* for an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Sample** | **Temperature** | **Bottles of** **water sold** | **Prediction**
    |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 64 | 7 | 12 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 78 | 33 | 27 |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | 81 | 34 | 29 |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | 79 | 31 | 28 |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | 54 | 2 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Table 3.2 – Predicting the water bottles sold with the training data
  prefs: []
  type: TYPE_NORMAL
- en: How do you express the accuracy of the model? If you’re not sure how accurate
    your model is, read on! There are a few metrics that can be used to help understand
    how accurate (or inaccurate) the model and its predictions are.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Evaluation metrics are statistical formulas used to evaluate the validity of
    predictions against a data set.
  prefs: []
  type: TYPE_NORMAL
- en: Mean Absolute Error (MAE)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This **mean absolute error** (**MAE**) represents how many units of variance
    there are (either positive or negative) on average. For example, in sample *11*
    from *Table 3.2*, the prediction was to sell 12 bottles of water. The actual value
    sold (based on the training data) was seven, meaning that the prediction was five
    units higher than actual. This variance is known as the **absolute error**. In
    sample 12, the prediction was for 27 bottles to be sold, but the actual number
    sold was 33 (6 units higher, or an absolute error of 6).
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate the MAE, add up all the absolute error values and divide by the
    number of samples in the validation set. In this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Sample** | **Bottles of** **water sold** | **Prediction** **(ŷ)** | **Absolute
    error** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 7 | 12 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 33 | 27 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | 34 | 29 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | 31 | 28 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | 2 | 0 | 2 |'
  prefs: []
  type: TYPE_TB
- en: Table 3.3 – Absolute error table
  prefs: []
  type: TYPE_NORMAL
- en: The total of all of the absolute errors is 22, and the number of items in the
    validation set was 5, resulting in an MAE for the validation set of 4.4 (22 divided
    by 5).
  prefs: []
  type: TYPE_NORMAL
- en: Mean squared error
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the drawbacks of the mean absolute error is that it treats all discrepancies
    equally. While the overall average error rate may be acceptable, there are industries
    or scenarios where it’s more desirable to have more (but smaller) errors as opposed
    to fewer (but larger) errors. For example, with fresh produce, it’s very undesirable
    to overstock because you have a higher likelihood of having to throw away larger
    quantities of expired food.
  prefs: []
  type: TYPE_NORMAL
- en: The **mean squared error** (**MSE**) functions as a measure of the quality of
    the model itself. This metric gives individual errors more weight—and larger error
    predictions in the training data result in a much higher MSE.
  prefs: []
  type: TYPE_NORMAL
- en: To calculate this, each absolute error value is squared, and then the sum of
    those values is averaged. Using the training data output in *Table 3.3*, the mean
    squared error value is 21.2, highlighting the fact that the model may need tweaking
    or more training data.
  prefs: []
  type: TYPE_NORMAL
- en: Root Mean Squared Error (RMSE)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The next metric is **root mean squared error**, which is the square root of
    the MSE. While the MSE provides a measure of the quality of the predictor function,
    the RMSE is converted back to the original unit, making it a little easier to
    interpret and communicate the model’s performance. RMSE is sensitive to outliers
    and gives relatively high weight to large errors. Like MSE, a smaller RMSE indicates
    a better fit.
  prefs: []
  type: TYPE_NORMAL
- en: With our validation data set, the RMSE is 4.6.
  prefs: []
  type: TYPE_NORMAL
- en: Coefficient of determination
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **coefficient of determination**, known as *R*2, measures how well a statistical
    model predicts the actual outcome. It’s a score between 0 and 1 that tells us
    the percentage of the variation in our target variable (what we’re trying to predict)
    that can be explained by the model. A score of 1 means the model predicts perfectly,
    with no difference between predicted and actual values, while a score of 0 means
    the model doesn’t explain any of the variation.
  prefs: []
  type: TYPE_NORMAL
- en: An *R*2 closer to 1 indicates a model that fits our data well, suggesting a
    strong relationship between our input variables and the outcome. However, a high
    *R*2 alone doesn’t guarantee that the model is accurate or useful for making predictions,
    especially in complex models with many variables. It’s essential to look beyond
    *R*2 to ensure the model isn’t just fitting the noise in our data (overfitting),
    which could lead to misleading results. So, while *R*2 is a helpful indicator
    of model fit, it’s just one piece of the puzzle in evaluating a model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: The coefficient of determination for our sample validation data set is 0.898—essentially
    89.8% accurate.
  prefs: []
  type: TYPE_NORMAL
- en: Based on that, it appears that our model may be pretty good (for bottled water).
    Since bottled water isn’t as perishable of a product as say, strawberries, stocking
    a little extra water likely won’t result in a product loss.
  prefs: []
  type: TYPE_NORMAL
- en: If, however, we were talking about more volatile products, you would probably
    perform some **iterative training** (that is repeated training sessions) by varying
    the input data sets, regression algorithms, and algorithm **hyperparameters**
    (parameters that control how the algorithm works, as opposed to the data supplied
    to the algorithm) to come up with a function that more accurately predicts the
    sales outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So where is regression machine learning useful? As you can see from the bottled
    water example, it’s useful for making a number of predictions, especially when
    projecting sales, quotas, housing prices, stock prices, and other forecasting
    activities. It’s also useful in analyzing user trends for advertising or media
    services.
  prefs: []
  type: TYPE_NORMAL
- en: Identify classification machine learning scenarios
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Classification is a supervised machine learning technique that essentially
    puts values into groups (classes) based on a criteria. There are two main types
    of classification techniques: binary and multiclass. Let’s look at both of them.'
  prefs: []
  type: TYPE_NORMAL
- en: Binary classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may have heard the terminology **binary** before and know that it’s the
    language of ones and zeroes that computers use to process information. Binary
    simply means that a data item can be set to one of two values. For example, 0
    or 1 and true or false are binary selections.
  prefs: []
  type: TYPE_NORMAL
- en: In the machine learning context, binary classification works similarly—using
    the value of a feature (*x*, just like in regression machine learning), the model
    predicts whether a label (*y*) is 0 or 1\. Binary classification categories data
    into mutually exclusive groups based on the feature data.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s look at a sample data set that might be used for training a binary classification
    model on whether a patient might be at risk of heart disease based on their LDL
    cholesterol level. Just like the regressive data set we looked at earlier, we’ll
    have a feature (*x*) column containing data measurements and a corresponding label
    (*y*):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Patient ID** | **LDL cholesterol level (x) (measured** **in mg/DL)** |
    **Heart** **disease (y)****0 = no, 1 =** **yes** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 100 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 87 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 132 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 159 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 152 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 171 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 188 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 161 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 118 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 141 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 102 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 144 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | 155 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | 167 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | 142 | 1 |'
  prefs: []
  type: TYPE_TB
- en: Table 3.4 – Binary classification data for heart disease patients
  prefs: []
  type: TYPE_NORMAL
- en: 'Just as with the regression model techniques, there are many algorithms that
    can be used with binary classification. One popular algorithm is *logistic regression*
    (which, despite its name, is not an algorithm for regression-based models), which
    is typically identified by its sigmoid (S-shaped) function graph depicting values
    between 0 and 1\. See *Figure 3**.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – An example of a Sigmoid function graph](img/B22207_03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – An example of a Sigmoid function graph
  prefs: []
  type: TYPE_NORMAL
- en: 'Like a regression algorithm, the resultant function can be expressed mathematically.
    In the graph, the *y* axis represents the probability of a label being true (ranked
    0 to 1), using the following expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '*f(x) = P(y=1 |* *x)*'
  prefs: []
  type: TYPE_NORMAL
- en: The training data set in *Table 3.4* shows seven patients that definitely do
    not have heart disease and eight patients that do. The graph depicted in *Figure
    3**.6* also shows an optional horizontal line that can indicate the **threshold**
    at which a prediction switches from false to true.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with the regression model training, you should divide the dataset into two
    selections: one piece to be used for training and the other for validation.'
  prefs: []
  type: TYPE_NORMAL
- en: Plugging the data into a simple binary classification function, you may plot
    a graph similar to the one in *Figure 3**.7*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 – A sample binary classification graph](img/B22207_03_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – A sample binary classification graph
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 3**.7*, the plotted points indicate the patient’s LDL cholesterol
    reading (plotted along the *x* axis)., and their location on the *y* axis indicates
    if the patient had heart disease or not, where 0 represents “no” and 1 represents
    “yes.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the same method for validating the algorithm with the reserved training
    data as we did with regression training, you can do the same thing with binary
    classification. With the model created, predicting heart disease based on the
    features (LDL cholesterol levels) should be easy:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Patient ID** | **LDL cholesterol level (x) (measured** **in mg/DL)** |
    **Prediction (ŷ)** | **Heart** **disease (y)****0 = no, 1 =** **yes** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 102 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 144 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | 155 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | 167 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | 142 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: Table 3.5 – Predictions for heart disease based on cholesterol
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s look at how to evaluate the data.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When evaluating a model, it’s important to be able to determine not only *where*
    the predictions were right and wrong but to understand *how* they were right or
    wrong. These results can be expressed in a graph called a **confusion matrix**
    or **confusion diagram**, as shown in *Figure 3**.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8 – Confusion matrix](img/B22207_03_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 – Confusion matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'The data is broken into four categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Actual Yes**: The training data said the patient had heart disease'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Actual No**: The training data said the patient did not have heart disease'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predicted Yes**: The model predicted based on the cholesterol level that
    the patient would have heart disease'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predicted No**: The model predicted based on the cholesterol level that the
    patient would not have heart disease'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Those are then laid out on a quadrant, and the intersections are labelled accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Predicted No** | **Predicted Yes** |'
  prefs: []
  type: TYPE_TB
- en: '| **Actual No** | True negative | False positive |'
  prefs: []
  type: TYPE_TB
- en: '| **Actual Yes** | False negative | True positive |'
  prefs: []
  type: TYPE_TB
- en: Table 3.6 – Confusion matrix table
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how to interpret the values:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True Negatives** (**TN**): This is the number of instances where the model
    correctly predicted the absence of the condition (class 0). In our case, this
    would represent the number of patients correctly identified as not being at risk
    for heart disease.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Positives** (**FP**): This is the number of instances where the model
    incorrectly predicted the presence of the condition (class 1). In our case, this
    would represent the number of patients incorrectly identified as being at risk
    for heart disease when they are not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Negatives** (**FN**): This is the number of instances where the model
    incorrectly predicted the absence of the condition (class 0). In our case, this
    would represent the number of patients who are actually at risk for heart disease
    but were incorrectly identified as not being at risk.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True Positives** (**TP**): This is the number of instances where the model
    correctly predicted the presence of the condition (class 1). In our case, this
    would represent the number of patients correctly identified as being at risk for
    heart disease.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These data points (the true negatives, true positives, false positives, and
    false negatives) can then be used to further evaluate how accurate the model is
    using several metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Accuracy** is used to describe how often the model is correct in general.
    The formula or computing accuracy is *(TP + TN) / (total)*. In this case, the
    accuracy of the model is 60%.'
  prefs: []
  type: TYPE_NORMAL
- en: Recall
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Recall** (sometimes called **sensitivity**) indicates how well the model
    identifies the positive class (1, or in this case, patients at risk of heart disease).
    The formula is *TP / (TP + FN)*. The recall for class 1 (with heart disease) is
    50%.'
  prefs: []
  type: TYPE_NORMAL
- en: Precision
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Precision** is an indicator of how often the model is right for each class
    prediction. For the positive class, this formula is TP / (TP + FP). For class
    1 (heart disease), the model correctly predicts 100% of the time.'
  prefs: []
  type: TYPE_NORMAL
- en: Specificity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Specificity** measures how often a model is right for the negative class
    (0, or in this case, patients without heart disease). The formula is *TN / (TN
    + FP)*. This model’s specificity for the negative class is 33%, indicating that
    it is only correct 33% of the time when predicting that a patient will not have
    heart disease.'
  prefs: []
  type: TYPE_NORMAL
- en: If there were discrepancies in prediction versus actual data, it could suggest
    areas where the model might need improvement, such as collecting more diverse
    data, using a different model, or tuning the existing model. Understanding where
    the model fails can help in reducing false positives (incorrectly predicting risk
    where there is none) and false negatives (missing out on identifying actual risk),
    which are particularly critical in medical diagnostics.
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Binary classifications are most useful when there are a limited number of factors
    influencing a true or false outcome. It is common in fields such as medicine,
    biology, technology, and chemistry when you’re trying to determine a yes/no or
    true/false result based on a small set of variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Common real-world examples of binary classification include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Email spam detection**: Classifying emails as either spam or not spam. This
    is one of the most common applications of binary classification, used by email
    services to filter out unwanted messages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Medical diagnosis**: Diagnosing patients with a disease or condition as either
    positive (having the disease) or negative (not having the disease). For example,
    binary classification models can be used to detect the presence of a tumor as
    malignant or benign based on medical imaging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Credit approval**: Deciding whether to approve or decline a credit application.
    Financial institutions use binary classification algorithms to predict whether
    an applicant is likely to default on a loan.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Churn prediction**: Predicting whether a customer will churn (leave) or stay
    with a company or service. Companies use binary classification to identify at-risk
    customers and develop strategies to retain them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fraud detection**: Identifying transactions as fraudulent or legitimate.
    Banks and financial institutions use binary classification models to detect suspicious
    activities and prevent fraud.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentiment analysis**: Determining whether a piece of text (such as a product
    review or social media post) expresses a positive or negative sentiment. This
    is widely used in marketing and customer service to gauge public opinion and customer
    satisfaction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Malware detection**: Classifying files or programs as malicious or safe,
    used by cybersecurity systems to protect computers and networks from viruses and
    other malicious software.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiclass classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Shifting gears a little bit, let’s look at multiclass classification. Like binary
    classification, it’s a probability method that assigns a classification based
    on a feature. However, instead of using a single feature or a binary label (true/false,
    0/1), it can use multiple features and multiple classifications. The underlying
    ideas are the same, but let’s just look at a quick example of how it works.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this example, we’ll be evaluating peppers and their heat (or Scoville heat
    unit rating). Every pepper has a heat rating, ranked in Scoville units, that describes
    how spicy it is.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data in *Table 3.7* depicts a variety of peppers and their average Scoville
    heat units:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Scoville** **heat units** | **Pepper** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1,500,000 – 2,200,000 | Carolina Reaper |'
  prefs: []
  type: TYPE_TB
- en: '| 1,000,000 – 1,500,000 | Trinidad Scorpion |'
  prefs: []
  type: TYPE_TB
- en: '| 855,000 – 1,000,000 | Ghost pepper |'
  prefs: []
  type: TYPE_TB
- en: '| 350,000 – 577,000 | Red Savina Habanero |'
  prefs: []
  type: TYPE_TB
- en: '| 100,000 – 350,000 | Habanero |'
  prefs: []
  type: TYPE_TB
- en: '| 70,000 – 100,000 | Charleston hot |'
  prefs: []
  type: TYPE_TB
- en: '| 30,000 – 50,000 | Cayenne pepper |'
  prefs: []
  type: TYPE_TB
- en: '| 10,000 – 23,000 | Serrano pepper |'
  prefs: []
  type: TYPE_TB
- en: '| 8,000 – 10,000 | Hungarian pepper |'
  prefs: []
  type: TYPE_TB
- en: '| 2,000 – 7,000 | Jalapeno pepper |'
  prefs: []
  type: TYPE_TB
- en: '| 0-100 | Bell pepper |'
  prefs: []
  type: TYPE_TB
- en: Table 3.7 – Selected peppers and their Scoville heat unit ratings
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, just like the binary classification training, let’s generate a sample
    table of features (Scoville heat unit ratings) and labels (peppers):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **ID** | **Scoville heat** **unit (x)** | **Pepper (y)** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1,900,000 | Carolina reaper |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 10 | Bell |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 2,850 | Jalapeno |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 447,700 | Red Savina |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 8,700 | Hungarian |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 127,000 | Habanero |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 88,000 | Charleston hot |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 289,000 | Habanero |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 0 | Bell |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 900,000 | Ghost |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 1,250,000 | Scorpion |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 11,000 | Serrano |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | 42,000 | Cayenne |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | 7,000 | Jalapeno |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | 2,200,000 | Carolina Reaper |'
  prefs: []
  type: TYPE_TB
- en: Table 3.8 – Scoville rating sample data
  prefs: []
  type: TYPE_NORMAL
- en: Once the training data is assembled, it’s time to use an algorithm to fit the
    training data to a function that will calculate the probability for our classes.
    There are 11 possible answers (classes) for our data set based on our training
    data. They’re zero-indexed, meaning they’re numbered from 0 to 10.
  prefs: []
  type: TYPE_NORMAL
- en: The most common algorithms in multiclass classification models are **one-vs-rest**
    (**OvR**) and **multinominal** algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '**One-vs-Rest** (**OvR**) or **One-vs-All** (**OvA**): With this algorithm,
    you train a binary classification function for each class individually. Each function
    targets a specific class compared to any of the other classes in the set. In this
    case, since there are 11 pepper types represented, the algorithm would create
    11 binary classification functions. Like the binary classifications previously,
    the algorithm produces a sigmoid function. The outcome of this model predicts
    the class for the function that results in the highest probability output. OvR
    or OvA strategies could be applied, for example, in a Carolina Reaper vs. all
    classifier—essentially determining if an element is a Carolina Reaper or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multinomial**: This algorithm approaches the problem differently, creating
    a single function with a multivalued (or vector) output. This vector can contain
    a probability distribution across all the potential classes, though it’s really
    designed for mutually exclusive data sets (for example, if a pepper’s Scoville
    rating can’t expand into another pepper’s range). With a vector containing multiple
    elements, each class element is scored individually between 0 and 1, with the
    total adding up to 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These types of models may need more information to reliably predict. For example,
    review the sample data in *Table 3.7* for the Scoville pepper ratings. As you
    can see, there are a few instances where a pepper might have a similar heat rating
    to another pepper (either milder or spicier). A larger training data set would
    help the model understand concepts of frequency to help more accurately predict
    and select an appropriate classification strategy (depending on the factors you
    want to identify) would help you be most successful in this classification methodology.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since multiclass classification can really be looked at as an extension of binary
    classification (in many cases), the same techniques and terminology apply.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, with the training data, you could develop a confusion matrix.
    The layout is very similar to the confusion matrix for binary classification—it’s
    just got more labels to deal with, as shown in *Figure 3**.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – Multiclass confusion matrix based on Scoville pepper data](img/B22207_03_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 – Multiclass confusion matrix based on Scoville pepper data
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Multiclass classification refers to the scenarios where you need to classify
    items into more than two categories. Unlike binary classification, which differentiates
    between two classes (such as yes/no or true/false), multiclass classification
    deals with situations where there are three or more classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Potential use cases include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Predicting animal types**: Suppose you have a dataset containing images of
    animals and you want to classify each image as a dog, cat, bird, or fish. This
    is a multiclass classification problem with four classes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weather forecasting**: If you’re predicting whether the weather will be sunny,
    cloudy, rainy, or snowy, you’re dealing with multiclass classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Handwritten numeral recognition**: A classic example is the MNIST dataset,
    where the task is to classify images of handwritten digits into 10 classes (0
    through 9).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Medical diagnosis**: Suppose a particular diagnostic test can indicate one
    of several different diseases. If you’re developing a model to predict which specific
    disease (out of a possible set) a patient might have based on their symptoms and
    test results, you’re engaging in multiclass classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In multiclass classification, techniques and metrics are slightly different
    from binary classification in that you have to consider how the model performs
    across all the different classes (not just two), but overall the process is very
    similar. Metrics such as precision and recall are calculated for each class and
    then averaged in some way (such as micro, macro, or weighted average) to understand
    the overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: A classification’s F1 score is a measure used to summarize the overall accuracy
    of a class. Previously, you learned about **precision** (ratio of correctly predicted
    positive observations to the total positives using the formula *P = TP / (TP+FP)*)
    and recall (ratio of correctly predicted positive observations to all the observations
    in the class, calculated using the formula *R = TP / (**TP +FN)*).
  prefs: []
  type: TYPE_NORMAL
- en: An F1 score is calculated using the formula *F1 = 2 * (P * R) / (P +* *R)*.
  prefs: []
  type: TYPE_NORMAL
- en: Up to this point, you’ve been learning about **supervised learning** scenarios
    such as binary and multiclass classification. Next, we’ll be shifting to unsupervised
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Identify clustering machine learning scenarios
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering is an unsupervised machine learning scenario where algorithms are
    employed to try to identify patterns in data. Unlike supervised learning, where
    training data has labels and features, unsupervised learning does not. The main
    goal of clustering is to be able to let the machine learning algorithms discover
    natural groupings within the data based on the similarities in the data points
    themselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just as supervised learning had its algorithms, there are several popular algorithms
    available to use with clustering scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '**K-means clustering**: This algorithm partitions the data into *K* distinct,
    non-overlapping subsets (or clusters) based on the mean distance from the centroid
    of each cluster. The value of *K* needs to be specified beforehand.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hierarchical clustering**: Builds a hierarchy of clusters either with a bottom-up
    approach (agglomerative) or a top-down approach (divisive). It does not require
    pre-specification of the number of clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Density-Based Spatial Clustering of Applications with Noise** (**DBSCAN**):
    Forms clusters based on the density of data points, capable of discovering clusters
    of arbitrary shape and handling noise and outliers effectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s say we have some sample data from grocery shoppers; 15 shoppers have
    put the items (or features, in this case) depicted in *Table 3.9* in their baskets:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Basket Id** | **Bread (x1)** | **Milk (x2)** | **Eggs (x3)** | **Bananas
    (x4)** | **Chicken (x5)** | **Apples (x6)** | **Cheese (x7)** | **Tomatoes (x8)**
    | **Potatoes (x9)** | **Onions (x10)** | **Coffee (x11)** | **Lettuce (x12)**
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 1 | 1 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1 | 1 | 0 | 1 | 0 | 1 | 0 | 1 | 0 | 1 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1 | 1 | 1 | 0 | 1 | 1 | 1 | 0 | 1 | 1 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 0 | 1 | 1 | 0 | 1 | 0 | 1 | 1 | 0 | 1 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 1 | 1 | 0 | 1 | 1 | 0 | 1 | 1 | 0 | 1 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 1 | 0 | 1 | 0 | 1 | 0 | 1 | 0 | 1 | 1 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 0 | 0 | 1 | 1 | 1 | 0 | 0 | 1 | 1 | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 1 | 1 | 1 | 0 | 1 | 1 | 0 | 1 | 1 | 1 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 1 | 0 | 1 | 0 | 1 | 0 | 0 | 1 | 0 | 1 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 1 | 1 | 0 | 1 | 1 | 0 | 1 | 1 | 0 | 0 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | 1 | 1 | 1 | 0 | 0 | 0 | 1 | 1 | 1 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | 0 | 1 | 1 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Table 3.9 – Example shopping baskets of food
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we can instruct a K-means algorithm to partition the data
    into three groups of shopping baskets. Based on the output, we end up with the
    following three clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cluster 0: Baskets 2, 5, 12, 15'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cluster 1: Baskets 5, 9, 11, 14'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cluster 2: Baskets 1, 3, 4, 6, 8, 10, 13'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s how the K-means algorithm was applied to the dataset of grocery items
    purchased by shoppers:'
  prefs: []
  type: TYPE_NORMAL
- en: First, the grocery items (such as bread, milk, eggs, etc.) were represented
    in a format suitable for machine learning algorithms. This was done using a binary
    representation where each item was encoded as 0 (not purchased) or 1 (purchased),
    as shown in *Table 3.9*. This binary matrix forms the dataset where each row represents
    a shopping basket and each column represents a different grocery item.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we chose a number of clusters (or groups) that would be used for grouping
    the items. In this case, we set *K* to three, meaning we decided to group the
    shopping baskets into three distinct clusters based on the items they contained.
    The choice of *K* can be influenced by domain knowledge, experimentation, or other
    techniques and algorithms.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the prerequisites for the process are set, we then choose some random points
    on a graph. These points, called **centroids**, are the center points of the clusters
    being formed. In our case, three shopping baskets were randomly chosen as the
    initial centroids.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each shopping basket (or row of our dataset) was then assigned to the nearest
    centroid. The “nearest” is typically determined by calculating the distance between
    the basket and each centroid. Each basket is assigned to the cluster whose centroid
    is closest to it, forming three initial clusters based on the current centroids.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go the distance
  prefs: []
  type: TYPE_NORMAL
- en: There are many different methods for determining distance in clustering. The
    three most common types of distance calculations are **Euclidean**, **Hamming**,
    **Manhattan**, **Minkowski**, and **Jaccard**. Each type of distance is used for
    different types of data (for example, binary data versus linear or continuous
    numerical data). The good news is that none of these things appear on the exam,
    so you don’t need to learn them. However, if you want to explore different mathematical
    foundations for determining clustering distance, see [https://www.displayr.com/understanding-cluster-analysis-a-comprehensive-guide/](https://www.displayr.com/understanding-cluster-analysis-a-comprehensive-guide/).
  prefs: []
  type: TYPE_NORMAL
- en: Once all baskets have been assigned to clusters, the centroids of these clusters
    are recalculated. This is done by taking the mean (or average) of all baskets
    in each cluster. Since our data is binary, this average may not be exactly 0 or
    1; it represents the proportion of baskets in the cluster that contain each item.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The steps of assigning baskets to the nearest centroid and then updating the
    centroids based on the current cluster memberships were repeated. With each iteration,
    baskets might shift from one cluster to another as the centroids change.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This process was repeated until the centroids no longer changed significantly
    between iterations, meaning that the clusters had stabilized and the algorithm
    had converged. This is the stopping criterion for K-means.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the algorithm converged, each shopping basket was assigned to one of the
    three clusters based on the items it contained. The final clusters represent groups
    of shopping baskets that are similar to each other based on the presence or absence
    of certain items.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the context of our grocery item dataset, applying K-means allowed us to group
    shopping baskets into clusters that could potentially reflect different types
    of shopping patterns or preferences among customers. For example, one cluster
    might represent weekly staple shopping (including items such as bread, milk, and
    eggs), another might represent fresh produce shopping (including items such as
    fruits and vegetables), and another might represent specialty item shopping (such
    as coffee or cheese).
  prefs: []
  type: TYPE_NORMAL
- en: This type of information is frequently used by merchants to help understand
    buying habits and suggestive or upselling opportunities. If you’ve ever wondered
    how Amazon determines how items show up as suggested for you to buy or why you
    get certain coupons in the mail, clustering machine learning models were very
    likely involved at some point.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since there are no labels to compare against, evaluating the performance of
    clustering is less straightforward than supervised learning. However, metrics
    such as **silhouette score** (or **silhouette coefficient**), the **Davies-Bouldin
    index**, and the **Calinski-Harabasz index** can be used to assess the quality
    of clustering by measuring the distance between clusters and the density of the
    clusters themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Silhouette score
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The silhouette score or silhouette coefficient measures how well each data point
    fits within its final assigned cluster. The score ranges from -1 to 1, where values
    closer to 1 indicate better-defined, less-overlapping clusters.
  prefs: []
  type: TYPE_NORMAL
- en: A score close to 1 indicates that the clusters are well apart from each other
    and clearly defined
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A score of 0 indicates that the clusters are overlapping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A score close to -1 indicates that the clusters are assigned inappropriately
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, this model’s silhouette score is 0.128, which suggests that the
    clusters are overlapping or might not be distinctly separated. That can be expected
    with this type of data (a grocery shopping list), since shoppers use items at
    different rates and have different needs than each other, leading them to complex
    purchase decisions that might result in them buying the same things but for different
    reasons.
  prefs: []
  type: TYPE_NORMAL
- en: Davies-Bouldin index
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **Davies-Bouldin index** is used to measure the quality of clustering, where
    lower scores indicate better clustering.
  prefs: []
  type: TYPE_NORMAL
- en: The Davies-Bouldin index score for our grocery basket K-means clustering model
    is approximately 1.65\. The score essentially evaluates the average similarity
    between each cluster and its most similar cluster, where similarity is a measure
    that combines the compactness (how close the points in a cluster are to each other)
    and the separation (how far apart different clusters are from each other) of the
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: A lower Davies-Bouldin index indicates that the clusters are compact (i.e.,
    the points within each cluster are close to each other) and well-separated (i.e.,
    the clusters are far apart from each other). Conversely, a higher score suggests
    less distinct clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, scores closer to 0 are more desirable.
  prefs: []
  type: TYPE_NORMAL
- en: Calinski-Barabasz index
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **Calinski-Harabasz index**, also known as the Variance Ratio Criterion,
    does not have a fixed range such as some other metrics (such as the silhouette
    score, which goes from -1 to 1). Instead, its value depends on the dataset’s characteristics,
    including the number of samples, dimensions, and the inherent cluster structure.
    The Calinski-Harabasz index score for this model is 3.10.
  prefs: []
  type: TYPE_NORMAL
- en: A higher Calinski-Harabasz index score indicates that the clusters are dense
    (meaning points within a cluster are close to each other) and well-separated (meaning
    clusters are far apart from each other). This is interpreted as a model with a
    better-defined cluster structure. Unlike metrics such as accuracy, which ranges
    from 0 to 1, or the silhouette score, which has a theoretical range from -1 to
    1, the Calinski-Harabasz index does not have a maximum value and is not bounded
    in a fixed range. Its absolute value is less informative without context.
  prefs: []
  type: TYPE_NORMAL
- en: The index is most useful when it’s being used to compare different clustering
    models or configurations on the same dataset. For example, comparing the scores
    obtained from clustering the same data with different numbers of clusters (*K*)
    can help in choosing the best *K* by selecting the one with the highest Calinski-Harabasz
    score. In this example, we told the model to group the grocery baskets into three
    clusters. To effectively use the Calinski-Harabasz score, it would be useful to
    re-run the model breaking the baskets into two, four, or five clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, after rerunning the K-means algorithm with parameters for two,
    four, and five clusters, the following scores were achieved:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Two clusters: 2.997'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Four clusters: 2.73'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Five clusters: 2.504'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In practice, because there is no fixed “good” threshold, the best approach is
    to use the Calinski-Harabasz index to compare the effectiveness of different clustering
    solutions on the same data and choose the one with the highest score. However,
    this score should be used alongside other metrics such as the silhouette score
    and the Davies-Bouldin index for a more comprehensive evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Clustering has a lot of real-world applications, from social sciences to marketing
    and threat modeling. Example applications include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Market segmentation**: Grouping customers based on purchasing behavior, interests,
    or demographic profiles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anomaly detection**: Identifying unusual data points by finding which ones
    do not fit into any cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data organization**: Organizing large volumes of data into manageable and
    meaningful groups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pattern discovery**: Finding hidden patterns or intrinsic structures in data,
    such as grouping genes with similar expression patterns in bioinformatics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customer behavior analysis**: Understanding different customer groups and
    their purchasing patterns can help in optimizing product placements, store layouts,
    and inventory management.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recommendation systems**: Clustering can help identify groups of similar
    items or users, which can then be used to recommend items to users based on the
    preferences of others in their group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Social network analysis**: Clustering can help identify communities or groups
    within large networks of individuals based on their interactions or shared interests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These applications show how versatile and powerful clustering can be in extracting
    meaningful patterns from vast amounts of data across different fields.
  prefs: []
  type: TYPE_NORMAL
- en: Identify features of deep learning techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning is an advanced subset of machine learning that mimics the human
    brain’s way of learning through an artificial neural network structure. These
    networks consist of multiple layers of neurons that process data in a hierarchical
    manner, which is why the models are called **deep neural networks** (**DNNs**).
    Deep learning automates feature extraction from large volumes of unstructured
    data, such as images and text, significantly enhancing machine learning tasks’
    accuracy and efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike traditional machine learning, which relies on manual feature extraction,
    deep learning models learn to identify and differentiate data features automatically.
    This learning process requires significant computational power and data, utilizing
    backpropagation and optimization algorithms such as stochastic gradient descent
    to adjust neuron connections and minimize prediction errors.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning applications include regression, classification, natural language
    processing, and computer vision, revolutionizing fields such as autonomous driving,
    virtual assistants, facial recognition, and recommendation systems. The training
    process involves fitting data to predict outcomes based on features, iteratively
    adjusting the model to improve accuracy. This technology represents a significant
    advancement in machines’ abilities to perform complex tasks by learning from data.
  prefs: []
  type: TYPE_NORMAL
- en: Just like other machine learning techniques discussed in this chapter, deep
    learning involves fitting training data to a function that can predict a label
    (*y*) based on the value of one or more features (*x*). The function (*f*(*x*))
    is the outer layer of a nested function in which each layer of the neural network
    encapsulates functions that operate on *x* and the weight (*w*) values associated
    with them. The algorithm used to train the model involves iteratively feeding
    the feature values (*x*) in the training data forward through the layers to calculate
    output values for *ŷ*, validating the model to evaluate how far off the calculated
    *ŷ* values are from the known *y* values (which quantifies the level of error,
    or loss, in the model), and then modifying the weights (*w*) to reduce the loss.
    The trained model includes the final weight values that result in the most accurate
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: So how does deep learning work? The foundation of deep learning is a structure
    called the **neural network**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Neural networks refer to computational systems designed to mimic the human
    brain and nervous system’s structure and function. Similar to the brain’s neurons,
    artificial neural networks consist of units called neurons or nodes, interconnected
    with one another. These connections are characterized by weights and biases, activating
    subsequent nodes once input values surpass predefined thresholds. Visualize a
    neural network as a series of nodes organized in layers, where each node connects
    to several others in the neighboring layer, with the output of each node affecting
    the inputs of nodes in the next layer, kind of like a 3D flowchart where each
    condition and action can connect to other conditions and actions. *Figure 3**.10*
    depicts an example of a simple neural network design:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10 – Example of a simple neural network](img/B22207_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10 – Example of a simple neural network
  prefs: []
  type: TYPE_NORMAL
- en: Each input layer node is connected to two nodes in the hidden layer and the
    resultant data is connected to the output layer. The input layer of a neural network
    processes raw data and passes it to the nodes of hidden layers, which classify
    the data points according to the target criteria. As data progresses through successive
    hidden layers, the target value’s focus becomes more refined, leading to more
    precise assumptions.
  prefs: []
  type: TYPE_NORMAL
- en: A **loss function** is used to compare the predicted *ŷ* values against the
    known values. The function compiles those differences, resulting in the aggregate
    variance for multiple cases. This total variance is summarized as the **loss**.
    An optimization function may be employed to evaluate the weight of the various
    losses and determine how to adjust to minimize the loss. These changes are **backpropagated**
    throughout the neural network to replace the original values and the model re-learns
    based on the updated values. Each iteration of this process is known as an **epoch**;
    epochs are repeated until the loss and predictions fall within an acceptable threshold.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the output layer utilizes the information from the hidden layers to
    determine the most likely label.
  prefs: []
  type: TYPE_NORMAL
- en: When you hear people talk about **deep learning**, it’s really about expanding
    the concept of neural networks with more hidden layers (representing more dimensions
    of data classification), resulting in more precise predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An example of deep learning in action is an image identification scenario where
    the goal is to predict animal types in images. In this case, a deep learning model,
    typically a **Convolutional Neural Network** (**CNN**), would be trained on a
    large dataset of animal images. Each image in the dataset is labeled with the
    type of animal it contains (e.g., dog, cat, tiger, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: During training, the CNN learns to recognize patterns and features in the images,
    such as shapes, textures, and colors, that distinguish one animal from another.
    The network consists of various layers, including input, hidden, and output layers.
    The input layer receives the raw image data, while the hidden layers process the
    data through a series of filters, identifying increasingly complex features at
    each layer. The output layer then uses the information extracted by the hidden
    layers to classify the image according to the type of animal it most likely represents.
  prefs: []
  type: TYPE_NORMAL
- en: Once the model is trained, it can be used to predict the type of animal in new,
    unseen images by processing the images through the same network and producing
    a prediction based on the features it has learned to recognize.
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This ability to automatically and accurately classify images makes deep learning
    a powerful tool for tasks such as wildlife monitoring, pet identification, and
    even medical image analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning has many of the same applications as clustering (as its goals
    are very similar—the identification and classification or grouping of previously
    unlabeled data). As such, real-world applications include everything that clustering
    can do, as well as many others:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image recognition and computer vision**: Deep learning models, especially
    CNNs, are widely used in image recognition tasks due to the way they are structured,
    mimicking human visual perception. They can identify faces, objects, scenes, and
    actions in images and videos. This technology underpins various applications,
    including security surveillance, medical imaging diagnosis, and autonomous vehicles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Natural Language Processing** (**NLP**): Deep learning has significantly
    advanced the capabilities of NLP, enabling applications such as language translation,
    sentiment analysis, and chatbots. Models such as transformers and **Recurrent
    Neural Networks** (**RNNs**) have been pivotal in these advancements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speech recognition and generation**: Deep learning models are at the heart
    of voice-activated systems such as virtual assistants (e.g., Siri, Alexa), speech-to-text
    transcription services, and voice-enabled customer service systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recommendation systems**: Deep learning is used to power recommendation engines
    on platforms such as Netflix, YouTube, and Amazon, enhancing user experience by
    personalizing content, products, and services based on individual preferences
    and past behavior.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Autonomous vehicles**: Deep learning models process and interpret the complex
    visual environment required for autonomous navigation, including recognizing traffic
    signs, signals, pedestrians, and other vehicles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fraud detection**: Financial institutions use deep learning to detect unusual
    patterns and prevent fraudulent activities in real-time, significantly reducing
    the risk of financial losses for both financial institutions and consumers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Drug discovery and genomics**: In the field of biotechnology, deep learning
    aids in the discovery of new drugs and the understanding of genetic sequences,
    contributing to personalized medicine and the treatment of complex diseases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Content generation**: Deep learning techniques are used to create realistic
    images, videos, text, and voice, enabling applications such as virtual reality,
    game development, and the creation of art and music.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentiment analysis**: Companies use deep learning to analyze customer feedback,
    social media comments, and reviews to gauge public sentiment, improve products,
    and tailor services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These applications demonstrate the versatility and transformative potential
    of deep learning across different domains, driving innovation and improving efficiency,
    accuracy, and user experience.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about many different types of machine learning
    scenarios such as regression, classification, and deep learning. You learned about
    both supervised and unsupervised learning, and where each of those is appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning is helpful in a variety of real-world scenarios, from weather
    forecasting to medical imaging analysis. You learned about applications for each
    type of machine learning technology and even how to compute several metrics to
    determine how accurate trained models are.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll start talking about core machine learning concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Exam Readiness Drill – Chapter Review Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apart from a solid understanding of key concepts, being able to think quickly
    under time pressure is a skill that will help you ace your certification exam.
    That is why working on these skills early on in your learning journey is key.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter review questions are designed to improve your test-taking skills progressively
    with each chapter you learn and review your understanding of key concepts in the
    chapter at the same time. You’ll find these at the end of each chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Before You Proceed
  prefs: []
  type: TYPE_NORMAL
- en: If you don't have a Packt Library subscription or you haven't purchased this
    book from the Packt store, you will need to unlock the online resources to access
    the exam readiness drills. Unlocking is free and needs to be done only once. To
    learn how to do that, head over to the chapter titled [*Chapter 12*](B22207_12.xhtml#_idTextAnchor228)*,
    Accessing the* *Online Resources*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To open the Chapter Review Questions for this chapter, perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Click the link – [https://packt.link/AI-900_CH03](https://packt.link/AI-900_CH03).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Alternatively, you can scan the following QR code (*Figure 3**.11*):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.11– QR code that opens Chapter Review Questions for logged-in users](img/B22207_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.11– QR code that opens Chapter Review Questions for logged-in users
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you log in, you’ll see a page similar to the one shown in *Figure 3**.12*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.12 – Chapter Review Questions for Chapter 3](img/B22207_03_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.12 – Chapter Review Questions for Chapter 3
  prefs: []
  type: TYPE_NORMAL
- en: Once ready, start the following practice drills, re-attempting the quiz multiple
    times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exam Readiness Drill
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the first three attempts, don’t worry about the time limit.
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first time, aim for at least **40%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix your learning gaps.
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The second time, aim for at least **60%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix any remaining learning
    gaps.
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The third time, aim for at least **75%**. Once you score 75% or more, you start
    working on your timing.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: You may take more than **three** attempts to reach 75%. That’s okay. Just review
    the relevant sections in the chapter till you get there.
  prefs: []
  type: TYPE_NORMAL
- en: Working On Timing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Your aim is to keep the score the same while trying to answer these questions
    as quickly as possible. Here’s an example of how your next attempts should look
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Attempt** | **Score** | **Time Taken** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Attempt 5 | 77% | 21 mins 30 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| Attempt 6 | 78% | 18 mins 34 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| Attempt 7 | 76% | 14 mins 44 seconds |'
  prefs: []
  type: TYPE_TB
- en: Table 3.10 – Sample timing practice drills on the online platform
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The time limits shown in the above table are just examples. Set your own time
    limits with each attempt based on the time limit of the quiz on the website.
  prefs: []
  type: TYPE_NORMAL
- en: With each new attempt, your score should stay above **75%** while your “time
    taken” to complete should “decrease”. Repeat as many attempts as you want till
    you feel confident dealing with the time pressure.
  prefs: []
  type: TYPE_NORMAL
