- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training Fully Custom ML Models with Vertex AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we learned about training no-code (Auto-ML) as well
    as low-code (BQML) **Machine Learning** (**ML**) models with minimum technical
    expertise required. These solutions are really handy when it comes to solving
    common ML problems. However, sometimes the problem or data itself is so complex
    that it requires the development of custom **Artificial Intelligence** (**AI**)
    models, in most cases large deep learning-based models. Working on custom models
    requires a significant level of technical expertise in the fields of ML, deep
    learning, and AI. Sometimes, even with this expertise, it becomes really difficult
    to manage training and experiments of large-scale custom deep learning models
    due to a lack of resources, compute, and proper metadata tracking mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make the lives of ML developers easier, Vertex AI provides a managed environment
    for launching large-scale custom training jobs. Vertex AI-managed jobs let us
    track useful metadata, monitor jobs through the Google Cloud console UI, and launch
    large-scale batch inference jobs without the need to actively monitor them. In
    this chapter, we will learn how to work with custom deep learning-based models
    on Google Vertex AI. Specifically, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Building a basic deep learning model with TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Packaging a model to submit to Vertex AI as a training job
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring model training progress
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating trained models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter requires basic-level knowledge of the deep learning framework TensorFlow
    and neural networks. Code artifacts can be found in the following GitHub repo
    – [https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/tree/main/Chapter07](https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/tree/main/Chapter07)
  prefs: []
  type: TYPE_NORMAL
- en: Building a basic deep learning model with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**TensorFlow**, or **TF** for short, is an end-to-end platform for building
    ML models. The main focus of the TensorFlow framework is to simplify the development,
    training, evaluation, and deployment of deep neural networks. When it comes to
    working with unstructured data (such as images, videos, audio, etc.), neural network-based
    solutions have achieved significantly better results than traditional ML approaches
    that mostly rely on handcrafted features. Deep neural networks are good at understanding
    complex patterns from high-dimensional data points (for example, an image with
    millions of pixels). In this section, we will develop a basic neural network-based
    model using TensorFlow. In the next few sections, we will see how Vertex AI can
    help with setting up scalable and systemic training/tuning of such custom models.'
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that TensorFlow is not the only ML framework that Vertex
    AI supports. Vertex AI supports many different ML frameworks and open-source projects
    including Pytorch, Spark and XGBoost. Pytorch is one of the fastest growing ML
    frameworks and with Vertex AI’s Pytorch integrations, we can easily train, deploy
    and orchestrate PyTorch models in production. Vertex AI provides prebuilt training
    and serving containers and also supports optimized distributed training of PyTorch
    models. Similarly, Vertex AI provides prebuilt training, serving and explainability
    features for multiple ML frameworks including XGBoost, TensorFlow, Pytorch and
    Scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: Experiment – converting black-and-white images into color images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this experiment, we will develop a TensorFlow-based deep learning model that
    takes black-and-white images as input and converts them into color images. As
    this exercise, requires developing a custom model, we will start our initial development
    work on a Jupyter Notebook. The first step is to create a user-managed Jupyter
    Notebook inside Vertex AI Workbench using a preconfigured TensorFlow image. More
    details on how to successfully create a Vertex AI Workbench notebook instance
    can be found in [*Chapter 4*](B17792_04.xhtml#_idTextAnchor056), *Vertex AI Workbench*.
    Next, let’s launch one Jupyter Notebook from the JupyterLab application. We are
    now all set to start working on our experiment.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start with importing useful libraries (prebuilt Python packages) in
    the first cell of our notebook. In this experiment, we will be using the following
    Python libraries – `numpy` for multi-dimensional array manipulation, TensorFlow
    for developing a deep learning model, OpenCV (or `cv2`) for image manipulation,
    and `matplotlib` for plotting images or graphs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We will need an image dataset with at least a few thousand images to train
    and test our model. In this experiment, we will work with the **Oxford-IIIT Pet**
    dataset, which is a public and free-to-use dataset. This dataset consists of around
    7k pet images from more than 30 different annotated categories. The dataset can
    be downloaded from the following website: [https://www.robots.ox.ac.uk/~vgg/data/pets/](https://www.robots.ox.ac.uk/~vgg/data/pets/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also download this dataset using the following commands in our terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the downloads are complete, put the zipped files in the same directory
    as our notebook. Now, let’s create a `!` sign in a notebook cell lets us run terminal
    commands from within Jupyter Notebook):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As our current experiment is concerned with converting black-and-white images
    into colored versions, we will not be using annotations. Now, let’s quickly verify
    in a new cell whether we have all the images successfully copied into the **data**
    folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the `glob` module helps us by listing all the `.jpg` image paths inside
    the data directory. The length of this list will be equal to the number of images.
    The preceding code should print the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have successfully downloaded and extracted data, let’s check a
    few images to be sure everything is fine. The following code block will plot a
    few random images with their annotations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are extracting the image class (or annotation) from the image path
    name itself as all images have the pet category in their filenames. The output
    of the preceding code should look something like shown in *Figure 7**.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – A few samples from the pet dataset](img/B17792_07_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – A few samples from the pet dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have verified our dataset, let’s split these images into three
    sets – train, validation, and test – as we usually do for training/validating
    and testing ML models. We will keep 60% of the images for training, 20% for validation,
    and the remaining 20% for testing. One simple way to do these splits is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The main focus of this experiment is to develop a deep learning model that converts
    black-and-white images into color images. To learn this mapping, the model will
    require pairs of black-and-white and corresponding color versions in order to
    learn this mapping. Our dataset already has color images. We will utilize the
    OpenCV library to convert them into grayscale (black-and-white) images and use
    them as input in our model. We will compare the output with their color versions.
    Another important thing to keep in mind is that our deep learning model will take
    fixed-size images as inputs, so we also need to bring all input images into a
    common resolution. In our experiment, we will change all of our images to have
    an 80x80 resolution. We already have training, validation, and test splits of
    the image paths. We can now read those files and prepare data for training, validation,
    and testing purposes. The following code blocks can be used to prepare the dataset
    as described previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first define empty lists for storing training, validation, and test data,
    respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we read training images, resize them to the required size, create a black-and-white
    version of each of them, and store them as target images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we repeat the same process for validation files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, prepare test files as well in a similar way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Note that images are represented with pixel values ranging from 0 to 255\. We
    are normalizing pixel values and bringing them into the range [-1, 1] by subtracting
    and dividing by 127.5\. Data normalization makes the optimization of deep learning
    models smoother and more stable.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have successfully prepared our dataset with train, validation, and
    test set splits, let’s check a few samples to confirm that the data has been prepared
    correctly. The following code block chooses some random training set images and
    plots them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s plot some input images to get a sense of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also be plotting the output versions (colored versions) of these randomly
    chosen images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: If everything is correct, we should see input-output pair images very similar
    to as in *Figure 7**.2*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17792_07_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Sample input-output pairs for data verification
  prefs: []
  type: TYPE_NORMAL
- en: 'As we are dealing with image data here, we will be working with a **Convolutional
    Neural Network** (**CNN**)-based model so that we can extract useful features
    from image data. The current research shows that CNNs can be very useful in extracting
    features and other useful information from image data. As we will be working with
    CNNs here, we need to convert our image dataset into NumPy arrays, and also add
    one channel dimension to each black-and-white input image (CNNs accept image input
    as a three-dimensional array, one dimension each for width, height, and channels).
    A colored image will already have three channels, one for each color value – R,
    G, and B. The following code block prepares our final dataset as per the steps
    described previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, once again, let’s check the dimensions of our dataset splits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'It should print something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Everything looks great from a data perspective. Let’s jump into defining our
    neural network architecture.
  prefs: []
  type: TYPE_NORMAL
- en: In this experiment, we will define a TensorFlow-based CNN that takes black-and-white
    images as input and predicts their colored variants as output. The model architecture
    can be broadly divided into two parts – **encoder** and **decoder**. The encoder
    part of the model takes a black-and-white image as input and extracts useful features
    from it by passing it through four down-sampling convolutional layers. Each convolutional
    layer is followed by layers of **LeakyReLU** activation and **batch normalization**
    except for the last layer, which has a **dropout** layer in place of batch normalization.
    After passing through the encoder model, an input image with the dimensions (80,
    80, 1) changes into a feature vector with the dimensions (5, 5, 256).
  prefs: []
  type: TYPE_NORMAL
- en: The second part of the model is called the decoder. The decoder part takes the
    feature vector from the encoder output and converts it back into a colored version
    of the corresponding input image. The decoder is made up of four transpose-convolutional
    or up-sampling layers. Each decoder layer is followed by layers of ReLU activation
    and batch normalization except for the last layer, which has tanh activation and
    does not have a normalization layer. tanh activation restricts final output vector
    values into the range [-1,1], which is desired for our output image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code blocks define the TensorFlow model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The encoder part starts from here, within the same function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The decoder part starts from here, within the same function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, add tanh activation to get the required output image in colored format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s create a TensorFlow model object and print the summary of our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This should print the model summary as shown in *Figure 7**.3*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – TensorFlow model summary (see full summary on Github)](img/B17792_07_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – TensorFlow model summary (see full summary on GitHub)
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see from the summary, our model has roughly 1.1 million trainable
    parameters. The next step is to compile the TensorFlow model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We are using the Adam optimizer with a learning rate of 0.0002 and the `beta_1`
    parameter with a value of 0.5\. Here, `beta_1` represents the value for the exponential
    decay rate for the first-moment estimates and the learning rate tells the optimizer
    the rate of updating the model parameter values during training. The rest of the
    parameter values are kept as the default. The idea is to pass a black-and-white
    image and reconstruct its colored version, so we will be using the **Mean Squared
    Error** (**MSE**) loss function as a reconstruction loss on the pixel level.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are all set to start the training now. We will train our model for about
    100 epochs, with a batch size of 128 for this experiment, and check the results.
    The following code snippet starts the training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The output logs should look something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'To check whether our training went smoothly, we can have a look at the loss
    charts from the `history` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The preceding snippet will plot the training and validation loss as a line chart
    for all the training epochs. The output graphs should look something like *Figure
    7**.4*. As we can see, training and validation loss are consistently decreasing
    as training progresses. It is reassuring that our training is going in the right
    direction.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Training and validation loss](img/B17792_07_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – Training and validation loss
  prefs: []
  type: TYPE_NORMAL
- en: 'The final step is now to check the results on an unseen test dataset. The following
    code chooses some random samples from `test_set` and generates model outputs for
    them. We’ve also plotted input images, model-generated colored images, and actual
    colored images for understanding purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot a few test images to verify the model outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the model generates a colored version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, plot the real colored version for reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: When plotting model outputs or input images, we add 1.0 to the image array and
    divide by 2.0\. We are doing this because during data preprocessing, we normalized
    image pixel values into the range [-1,1]. But ideally, image pixel values can’t
    be negative, so we need to inverse our transformation for image plotting purposes.
    So, adding 1.0 and dividing by 2.0 brings pixel values into the range [0,1], which
    is supported by Matplotlib for plotting. See *Figure 7**.5*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – Black-and-white to color model output](img/B17792_07_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – Black-and-white to color model output
  prefs: []
  type: TYPE_NORMAL
- en: As we can see from the preceding outputs, our model is learning some kind of
    colorization, which is, of course, not ideal but still looks pretty good. An interesting
    thing to notice is that it is not filling the color gradient randomly; we can
    clearly spot the main objects as they have a different contrast from the background.
    Given that we had a very small model and small training dataset, this performance
    is quite promising.
  prefs: []
  type: TYPE_NORMAL
- en: However, this is not the best model architecture for solving the image colorization
    problem. Nowadays, generative models such as **Generative Adversarial Networks**
    (**GANs**) provide the best results for such problems. We will study GANs later
    in this book, but for now, let’s stick to this simple experiment. Next, we will
    work with other Vertex AI tools that will make our lives easier when it comes
    to experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: Packaging a model to submit it to Vertex AI as a training job
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous section demonstrated a small image colorization experiment on a
    Vertex AI Workbench notebook. Notebooks are great for small-scale and quick experiments,
    but when it comes to large-scale experiments (with more compute and/or memory
    requirements), it is advised to launch them as a Vertex AI job and specify desired
    machine specifications (accelerators such as GPU or TPU if needed) for optimal
    experimentation. Vertex AI jobs also let us execute tons of experiments in parallel
    without waiting for the results of a single experiment. Experiment tracking is
    also quite easy with Vertex AI jobs, so it becomes easier to compare your latest
    experiments with past experiments with the help of saved metadata and the Vertex
    AI UI. Now, let’s use our model experimentation setup from the previous section
    and launch it as a Vertex AI training job.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Vertex AI jobs run in a containerized environment, so in order to launch an
    experiment, we must package our entire code (including reading data, preprocessing,
    model building, training, and evaluation) into a single script to be launched
    within the container. Google Cloud provides tons of prebuilt container images
    for training and evaluation (with dependencies pre-installed for desired frameworks
    such as TensorFlow, PyTorch, etc.). Plus, we also have the flexibility of defining
    our own custom container with any kind of dependencies that we may need.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the experiment from the previous section, as we downloaded open source
    data into our Jupyter environment, this data is not yet present in **Google Cloud
    Storage** (**GCS**) (i.e., a GCS bucket or BigQuery). So, first, we need to store
    this data somewhere such that our Vertex AI training job can read it from within
    the training container. To make things easier for us, we will upload our pre-processed
    data into a storage bucket. This will save us the effort of preparing data again
    within the job container. We can use the following script to save our prepared
    data into a GCS bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Note that before executing this code, we must create a bucket where we want
    to store these NumPy arrays. In this case, we have already created one bucket
    with the name `data-bucket-417812395597`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can read these NumPy arrays in any number of training jobs/experiments using
    the following script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Our data requirements are now all set. Next, let’s work on setting up our Vertex
    AI training job.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will install some useful packages required to define and launch Vertex
    AI jobs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Once package installation is done, we will move to a new notebook and import
    useful libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will define our project configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Note that we have created a bucket with the name `my-training-artifacts` to
    store all the intermediate metadata and artifacts as a result of our Vertex AI
    job.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s initialize the Vertex AI SDK with our project configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'For our experimentations, we will be using prebuilt TensorFlow images as our
    model is also based on TensorFlow. Let’s define the images to be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we will just launch a simple training job. In the next sections,
    we will also deploy and test our trained models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s define some command-line arguments for our training (these can
    be modified on a per-need basis):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We should also provide a meaningful job name; it will help us distinguish our
    experiment from other experiments running in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to write down our entire training script – starting from reading
    data, defining the model, training, and saving the model into a single file. We
    will write down our entire code from the previous section into a file with the
    name `task.py`. The following are the contents of our `task.py` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The following part of the file parses command-line arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we print some version and environment configurations to keep track of
    current settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we define a training strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we prepare the dataset for training, validation, and testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we define our TensorFlow model as discussed before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the definition of Encoder part of the TF model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The Encoder part is now done. Next we define the decoder part of the model
    within the same function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are ready to build and compile our TensorFlow model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The following block launches training with the defined settings and saves the
    trained model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have all the configurations set up and our training script, `task.py`,
    is ready, we are all set to define and launch our custom training job on Vertex
    AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s define our custom Vertex AI training job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The final step is to launch the job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: This setup launches a Vertex AI custom training job on an `n1-standard-16` machine
    as defined as a parameter in the preceding `job.run` method. When we launch the
    job in a notebook cell, it gives us a URL to the Google Cloud console UI. By clicking
    on it, we can monitor our job logs within the Vertex AI UI.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Vertex AI training job looks something like *Figure 7**.6* in the Google
    Cloud console UI. Here, we can re-verify the configurations and parameters of
    our job that we had defined at the time of launch:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Vertex AI training job](img/B17792_07_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – Vertex AI training job
  prefs: []
  type: TYPE_NORMAL
- en: 'The Vertex AI UI lets us monitor near real-time logs of all the training/custom
    jobs. We can monitor our training within the UI, and it looks something like *Figure
    7**.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Real-time logs for Vertex AI training job on Google Cloud console](img/B17792_07_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – Real-time logs for Vertex AI training job on Google Cloud console
  prefs: []
  type: TYPE_NORMAL
- en: Going through the logs may not be the best way to monitor training progress
    as we may want to track a few parameters, such as loss and accuracy. In the next
    section, we will learn about how to set up TensorBoard-based live monitoring of
    training progress. However, these logs can be really handy for debugging purposes;
    if our pipeline fails in between before completing the execution successfully,
    we can always check these logs to identify the root cause.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring model training progress
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we saw how easy it is to launch a Vertex AI custom
    training job with desired configurations and machine types. These Vertex AI training
    jobs are really useful for running large-scale experiments where training uses
    high compute (multiple GPUs or TPUs) and also may run for a few days. Such long-running
    experiments are not very feasible to run in a Jupyter Notebook-based environment.
    Another great thing about launching Vertex AI jobs is that all the metadata and
    lineage are tracked in a systematic way so that we can come back later and look
    into our past experiments and compare them with the latest ones in an easy and
    accurate way.
  prefs: []
  type: TYPE_NORMAL
- en: Another important aspect is monitoring the live progress of training jobs (including
    metrics such as loss and accuracy). For this purpose, we can easily set up Vertex
    AI TensorBoard within our Vertex AI job and track the progress in a near real-time
    fashion. In this section, we will set up a TensorBoard instance for our previous
    experiment.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the code/scripts will be similar to the previous section. Here, we will
    just examine the modifications needed to set up TensorBoard monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, we need to make small changes in the `task.py` file to account for
    TensorFlow callbacks as we want to monitor training loss. To keep things clean,
    we will modify a copy of the `task.py` file that we have renamed to `task2.py`.
    The following are the changes in the `model.fit` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding script, we have just defined a TensorFlow callback object and
    also passed it into the `model.fit` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Working with TensorBoard requires a service account to be in place (instead
    of individual user accounts). If we already don’t have a service account set up,
    we can use the following script to quickly set up a service account. A service
    account is used to grant permissions to services, VMs, and other tooling on Google
    Cloud:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'If we are working with colab, the following code snippet will create a service
    account accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The next step is to create a Vertex AI TensorBoard instance that we will use
    for monitoring our training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Set up the TensorBoard instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'We need a staging bucket for our Vertex AI job so that it can write event logs
    into that location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now all set to define our custom training job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now launch the Vertex AI job using the following script. Here, we can
    choose the machine type and also specify the `replica_count` parameter, which
    controls the number of replicas to run for the current job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Once we launch the job, it will give us the URL for locating the Vertex AI job
    in the Google Cloud console UI like in the previous section; but this time, it
    will also give us a URL to the Vertex TensorBoard UI. Using this URL, we will
    be able to monitor our training in a near real-time fashion.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is how it looks for our little experiment (see *Figure 7**.8*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Vertex TensorBoard for real-time monitoring of experiments](img/B17792_07_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – Vertex TensorBoard for real-time monitoring of experiments
  prefs: []
  type: TYPE_NORMAL
- en: We can configure it to show more desired metrics for our experiments. Now that
    we are able to launch Vertex AI training, monitor it, and also save our TensorFlow-trained
    model, let’s move on to the model evaluation part.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating trained models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will take the already trained model from the previous section
    and launch a batch inference job on the test data. The first step here will be
    to load our test data into a Jupyter Notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to create a JSON payload of instances from our test data and
    save it in a cloud storage location. The batch inference module will be able to
    read these instances and perform inference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we convert the input images to a serializable format so that the prediction
    service can accept input as a JSON file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Now our test dataset instances are ready in a cloud storage bucket. We can
    launch batch prediction over them, and the batch inference module will save the
    output results into a new folder inside the same bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we call the batch prediction service using the SDK:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: We can also monitor the progress of the batch prediction job within the Google
    Cloud console UI if needed. Once this job finishes, we can check the outputs inside
    the defined destination folder.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the chapter, we learned how to work with a Vertex AI-based managed training
    environment and launch custom training jobs. Launching custom training jobs on
    Vertex AI comes with a number of advantages, such as managed metadata tracking,
    no need to actively monitor jobs, and the ability to launch any number of experiments
    in parallel, choose your desired machine specifications to run your experiments,
    monitor training progress and results in near-real time fashion using the Cloud
    console UI, and run managed batch inference jobs on a saved model. It is also
    tighly integrated with other GCP products.
  prefs: []
  type: TYPE_NORMAL
- en: After reading this chapter, you should be able to develop and run custom deep
    learning models (using frameworks such as TensorFlow) on Vertex AI Workbench notebooks.
    Secondly, you should be able to launch long-running Vertex AI custom training
    jobs and also understand the advantages of the managed Vertex AI training framework.
    The managed Google Cloud console interface and TensorBoard make it easy to monitor
    and evaluate various Vertex AI training jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a good understanding of training models using Vertex AI on
    GCP, we will learn about model explainability in the next chapter.
  prefs: []
  type: TYPE_NORMAL
