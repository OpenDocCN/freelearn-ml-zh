- en: Chapter 12. Reinforcement learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章。强化学习
- en: 'We have covered supervised and unsupervised learning methods in-depth in [Chapter
    5](ch05.html "Chapter 5. Decision Tree based learning"), *Decision Tree based
    learning*, with various algorithms. In this chapter, we will be covering a new
    learning technique that is different from both supervised and unsupervised learning
    called **Reinforcement Learning** (**RL**). Reinforcement Learning is a particular
    type of Machine learning where the learning is driven by the feedback from the
    environment, and the learning technique is iterative and adaptive. RL is believed
    to be closer to human learning. The primary goal of RL is decision making and
    at the heart of it lies **Markov''s Decision Process** (**MDP**). In this chapter,
    we will cover some basic Reinforcement Learning methods like **Temporal Difference**
    (**TD**), certainty equivalence, policy gradient, dynamic programming, and more.
    The following figure depicts different data architecture paradigms that will be
    covered in this chapter:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第5章](ch05.html "第5章。基于决策树的学习")*基于决策树的学习*中深入探讨了监督学习和无监督学习方法，并介绍了各种算法。在本章中，我们将介绍一种不同于监督学习和无监督学习的新学习技术，称为**强化学习**（**RL**）。强化学习是一种特定的机器学习方法，其学习过程由环境的反馈驱动，学习技术是迭代和自适应的。强化学习被认为更接近人类学习。强化学习的主要目标是决策，其核心是**马尔可夫决策过程**（**MDP**）。在本章中，我们将介绍一些基本的强化学习方法，如**时间差分**（**TD**）、确定性等价、策略梯度、动态规划等。以下图展示了本章将涵盖的不同数据架构范例：
- en: '![Reinforcement learning](img/B03980_12_01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![强化学习](img/B03980_12_01.jpg)'
- en: 'The following topics are covered in depth in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将深入探讨以下主题：
- en: Recap of supervised, semi-supervised, and unsupervised learning, and the context
    of Reinforcement Learning.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习、半监督学习和无监督学习的回顾，以及强化学习的背景。
- en: 'Understanding MDP is key to Reinforcement Learning. Regarding this, the following
    topics are covered in this chapter:'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解马尔可夫决策过程（MDP）对于强化学习至关重要。关于这一点，本章将涵盖以下主题：
- en: What does MDP mean, key attributes, states, reward, actions, and transitions
    (discounts)
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: MDP代表什么，关键属性，状态，奖励，动作和转移（折扣）
- en: The underlying process of MDP and how it helps in the decision process
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: MDP的底层过程以及它如何帮助决策过程
- en: Policies and value functions (also called utilities, as in a group of rewards)
    and how we assign value to an infinite sequence of rewards
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略和价值函数（也称为效用，如在奖励组中）以及我们如何对无限序列的奖励进行赋值
- en: '**Bellman Equation**—the value iteration and policy iteration'
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**贝尔曼方程**——值迭代和政策迭代'
- en: 'Regarding Reinforcement Learning, we will cover the following:'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于强化学习，我们将涵盖以下内容：
- en: Planning and learning in MDP
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在MDP中的规划和学习
- en: Connection planning and functional approximation in RL
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习中的连接规划和功能逼近
- en: Different RL methods and approaches to RL, such as simple decision theory, the
    **temporal difference** (**TD**), dynamic programming, policy gradient, certainty
    equivalence, and eligibility traces
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的强化学习方法和对强化学习的不同方法，例如简单的决策理论、**时间差分**（**TD**）、动态规划、策略梯度、确定性等价和资格痕迹
- en: Key algorithms such as Q-learning, Sarsa, and others
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如Q-learning、Sarsa等关键算法
- en: Reinforcement learning applications
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习应用
- en: Reinforcement Learning (RL)
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习（RL）
- en: Let's do a recap of supervised, semi-supervised, and unsupervised learning,
    and set the context for Reinforcement Learning. In [Chapter 1](ch01.html "Chapter 1. Introduction
    to Machine learning"), *Introduction to Machine Learning*, we covered the basic
    definitions of supervised, semi-supervised, and unsupervised learning. Inductive
    learning is a reasoning process that uses the results of one experiment to run
    the next set of experiments and iteratively evolve a model from specific information.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下监督学习、半监督学习和无监督学习，并为强化学习设定背景。在[第1章](ch01.html "第1章。机器学习简介")*机器学习简介*中，我们介绍了监督学习、半监督学习和无监督学习的基本定义。归纳学习是一种推理过程，它使用一个实验的结果来运行下一组实验，并从具体信息中迭代地进化模型。
- en: 'The following figure depicts various subfields of Machine learning. These subfields
    are one of the ways the Machine learning algorithms are classified:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了机器学习的各个子领域。这些子领域是机器学习算法分类的一种方式：
- en: '![Reinforcement Learning (RL)](img/B03980_12_30.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![强化学习（RL）](img/B03980_12_30.jpg)'
- en: 'Supervised learning is all about operating to a known expectation, and in this
    case, what needs to be analyzed from the data being defined. The input datasets
    in this context are also referred to as **labeled** datasets. Algorithms classified
    under this category focus on establishing a relationship between the input and
    output attributes and uses this relationship speculatively to generate an output
    for new input data points. In the preceding section, the example defined for the
    classification problem is also an example of supervised learning. Labeled data
    helps build reliable models and are usually expensive and limited. The following
    diagram depicts the workflow for supervised learning:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习主要涉及根据已知期望进行操作，在这种情况下，需要从定义的数据中进行分析。在此背景下，输入数据集也被称为**标记**数据集。属于这一类别的算法专注于建立输入和输出属性之间的关系，并利用这种关系推测性地为新输入数据点生成输出。在前一节中，为分类问题定义的示例也是监督学习的一个例子。标记数据有助于构建可靠的模型，通常成本高昂且有限。以下图表展示了监督学习的工作流程：
- en: '![Reinforcement Learning (RL)](img/B03980_12_03.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![强化学习 (RL)](img/B03980_12_03.jpg)'
- en: 'So, this is a function approximation, where given the *x*, *y* pairs, our goal
    is to find the function *f* that maps the new *x* to a proper *y*:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这是一个函数逼近，给定 *x* 和 *y* 对，我们的目标是找到将新的 *x* 映射到适当的 *y* 的函数 *f*：
- en: '*y = f(x)*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*y = f(x)*'
- en: In some of the learning problems, we do not have any specific target in mind
    to solve for; this kind of learning is specifically called unsupervised analysis
    or learning. The goal, in this case, is to decipher structures in data as against
    build mapping between input and output attributes of data, and in fact, the output
    attributes are not defined. These learning algorithms operate on an **unlabelled**
    dataset for this reason.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些学习问题中，我们并没有特定的目标来解决；这种学习被称为无监督分析或学习。在这种情况下，目标是解析数据中的结构，而不是在数据的输入和输出属性之间建立映射，实际上，输出属性并未定义。这些学习算法由于这个原因在**无标记**数据集上操作。
- en: '![Reinforcement Learning (RL)](img/B03980_12_04.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![强化学习 (RL)](img/B03980_12_04.jpg)'
- en: 'So, given a bunch of *x*''s, the goal here is to define a function *f* that
    can give a concise description for a set of *x*''s. Hence, this is called clustering:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，给定一系列 *x*，这里的目的是定义一个函数 *f*，它可以对一组 *x* 提供简洁的描述。因此，这被称为聚类：
- en: '*f(x)*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*f(x)*'
- en: Semi-supervised learning is about using both labeled and unlabeled data to learn
    better models. It is important that there are appropriate assumptions for the
    unlabeled data as any incorrect assumptions can invalidate the model. Semi-supervised
    learning takes its motivation from the human way of learning.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习涉及使用标记和无标记数据来学习更好的模型。对于无标记数据，必须存在适当的假设，因为任何错误的假设都可能使模型无效。半监督学习从人类的学习方式中汲取灵感。
- en: The context of Reinforcement Learning
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习的背景
- en: Reinforcement Learning is about learning that is focused on maximizing the rewards
    from the result. For example, while teaching toddlers new habits, rewarding toddlers
    every time they follow instructions works very well. In fact, they figure out
    what behavior helps them earn rewards. This is exactly what Reinforcement Learning
    is it is also called as credit assessment learning.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习关注的是最大化结果带来的奖励。例如，在教导幼儿新习惯时，每次幼儿遵循指示就给予奖励非常有效。事实上，他们发现哪些行为有助于他们获得奖励。这正是强化学习所做的事情，它也被称为信用评估学习。
- en: The most important thing in Reinforcement Learning is that, the model is additionally
    responsible for making decisions for which a periodic reward is received. The
    results, in this case, unlike supervised learning, are not immediate and may require
    a sequence of steps to be executed before the final result is seen. Ideally, the
    algorithm will generate a sequence of decisions that will help achieve the highest
    reward or utility.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习最重要的地方在于，模型还需要负责做出决策，而这些决策会收到周期性的奖励。在这种情况下，与监督学习不同，结果不是立即的，可能需要执行一系列步骤才能看到最终结果。理想情况下，算法将生成一系列决策，以帮助实现最高的奖励或效用。
- en: '![The context of Reinforcement Learning](img/B03980_12_05.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![强化学习的背景](img/B03980_12_05.jpg)'
- en: The goal of this learning technique is to measure the trade-offs effectively
    by exploring and exploiting the data. For example, when a person has to travel
    from a point A to point B, there will be many ways that include traveling by air,
    water, road, or on foot, and there is a significant value in considering this
    data measuring the trade-offs for each of these options. Another important aspect
    is, what would a delay in the rewards mean? Moreover, how it would affect learning?
    For example, in games like chess, any delay in reward identification may change
    or impact the result.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这种学习技术的目标是通过对数据和利用进行探索来有效地衡量权衡。例如，当一个人需要从点A到点B旅行时，会有许多方式，包括空中、水上、路上或步行，考虑这些选项的权衡数据具有很大的价值。另一个重要方面是，奖励的延迟意味着什么？此外，它会如何影响学习？例如，在象棋等游戏中，奖励识别的任何延迟都可能改变或影响结果。
- en: 'So, the representation is very similar to supervised learning, the difference
    being that the input is not *x*, *y* pairs but *x*, *z* pairs. The goal is to
    find a function *f* that identifies a *y*, given *x* and *z*. In the following
    sections, we will explore more of what the *z* is. The equation for definition
    of the goal function is as given here:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，表示与监督学习非常相似，区别在于输入不是 *x*、*y* 对，而是 *x*、*z* 对。目标是找到一个函数 *f*，给定 *x* 和 *z* 识别一个
    *y*。在接下来的章节中，我们将进一步探讨 *z* 是什么。目标函数的定义方程如下：
- en: '*y = f(x)* given *z*.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*y = f(x)* 给定 *z*。'
- en: 'A formal definition of Reinforcement Learning is as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的一个正式定义如下：
- en: '|   | *"Reinforcement Learning is defined as a way of programming agents by
    reward and punishment without needing to specify how the task is to be achieved."*
    |   |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|   | *"强化学习被定义为通过奖励和惩罚来编程代理的方法，而不需要指定如何完成任务。" |   |'
- en: '|   | --*Kaelbling, Littman, & Moore, 96* |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|   | --*凯尔宾、利特曼、摩尔，96* |'
- en: So, overall, RL is neither a type of neural network nor is an alternative to
    neural networks, but an orthogonal approach for Machine learning with emphasis
    being on learning feedback that is used for evaluating the learner's performance
    with no standard behavioral targets against which the performance is measured,
    for example, learning to ride a bicycle.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，总的来说，强化学习既不是一种神经网络类型，也不是神经网络的替代品，而是一种正交的机器学习方法，重点在于学习反馈，用于评估学习者的表现，没有标准的行为目标来衡量表现，例如学习骑自行车。
- en: Let's now look at the formal or basic RL model and understand different elements
    in action. As a first step, let's understand some basic terms.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在看看正式或基本的强化学习模型，并了解动作中的不同元素。作为第一步，让我们了解一些基本术语。
- en: '![The context of Reinforcement Learning](img/B03980_12_06.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![强化学习的环境](img/B03980_12_06.jpg)'
- en: '**Agent**: An agent is an entity that is a learner as well as a decision maker,
    typically an intelligent program in this case.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代理**：代理是一个既是学习者又是决策者的实体，在这种情况下通常是一个智能程序。'
- en: '**Environment**: An environment is an entity that is responsible for producing
    a new situation given an action performed by the agent. It gives rewards or feedback
    for the action. So, in short, the environment is everything other than an agent.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**环境**：环境是一个实体，负责在代理执行动作后产生新的情况。它为动作提供奖励或反馈。所以，简而言之，环境是除了代理之外的一切。'
- en: '**State**: A state is a situation that an action lands an entity in.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态**：状态是动作使实体所处的情形。'
- en: '**Action**: An action is a step executed by an agent that results in a change
    in state.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动作**：动作是代理执行的一步，导致状态的变化。'
- en: '**Policy**: A policy is a definition of how an agent behaves at a given point
    in time. It elaborates the mapping between the states and actions and is usually
    a simple business rule or a function.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**政策**：政策是在特定时间点代理行为的定义。它阐述了状态和动作之间的映射，通常是一个简单的业务规则或函数。'
- en: '**Reward**: A reward lays down short-term benefit of an action that helps in
    reaching the goal.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励**：奖励规定了动作的短期利益，有助于达到目标。'
- en: '**Value:** There is another important element in Reinforcement Learning, and
    that is a value function. While reward function is all about the short-term or
    immediate benefit of an action, a value function is about the good in long run.
    This value is an accumulation of rewards an agent is expected to get from the
    time the world started.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**价值**：在强化学习中还有一个重要的元素，那就是价值函数。虽然奖励函数是关于动作的短期或即时利益，但价值函数是关于长期的好处。这个价值是代理从世界开始时预期获得的奖励的累积。'
- en: Examples of Reinforcement Learning
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 强化学习的例子
- en: The easiest way to understand Reinforcement Learning is to look at some of the
    practical and real-world applications of it. In this section, we will list down
    and understand some of them.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 理解强化学习的最简单方法之一是看看它的实际和现实世界应用。在本节中，我们将列出并理解其中的一些。
- en: '**Chess game**: In the game of chess, a player makes a move; this move is driven
    by an informed selection of an action that comes with a set of counter moves from
    the opponent player. The next action of the player is determined by what moves
    the opponent takes.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**棋类游戏**：在棋类游戏中，玩家进行一步棋；这一步棋是由一个经过信息选择的动作驱动的，该动作伴随着对手玩家的反制动作。玩家的下一步动作取决于对手采取了哪些动作。'
- en: '**Elevator Scheduling**: Let''s take an example of a building with many floors
    and many elevators. The key optimization requirement here is to choose which elevator
    should be sent to which floor and is categorized as a control problem. The input
    here is a set of buttons pressed (inside and outside the lift) across the floors,
    locations of the elevators, and a set of floors. The reward, in this case, is
    the least waiting time of the people wanting to use the lift. Here, the system
    learns how to control the elevators again; through learning in a simulation of
    the building, the system learns to control the elevators through the estimates
    of the value of actions from the past.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**电梯调度**：让我们以一栋有许多楼层和许多电梯的建筑为例。这里的关键优化需求是选择哪个电梯应该被派往哪个楼层，这被归类为一个控制问题。这里的输入是一组楼层上（电梯内外）按下的按钮、电梯的位置和一组楼层。在这种情况下，奖励是最少等待时间，即想要使用电梯的人的等待时间。在这里，系统通过在建筑模拟中的学习，再次学习如何控制电梯；通过从过去动作的价值估计中学习。'
- en: '**Network packet routing**: This is a case of defining a routing policy for
    dynamically changing networks. Q-learning techniques (covered a little later in
    the chapter) are used to identify which adjacent node the packet should be routed
    to. In this case, each node has a queue, and one packet is dispatched at a time.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络数据包路由**：这是一个为动态变化的网络定义路由策略的案例。Q学习技术（在本章稍后部分将简要介绍）用于确定数据包应该路由到哪个相邻节点。在这种情况下，每个节点都有一个队列，并且一次只发送一个数据包。'
- en: '**Mobile robot behavior**: A mobile robot needs to decide between it reaching
    the recharge point or the next trash point depending on how quickly it has been
    able to find a recharge point in the past.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**移动机器人行为**：移动机器人需要根据它过去能够多快找到充电点的速度来决定是前往充电点还是下一个垃圾点。'
- en: Evaluative Feedback
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估反馈
- en: One of the key features that differentiates Reinforcement Learning from the
    other learning types is that it uses the information to evaluate the impact of
    a particular action than instructing blindly what action needs to be taken. Evaluative
    feedback on one hand indicates how good the action taken is while instructive
    feedback indicates what the correct action is irrespective of whether the action
    is taken or not. Although these two mechanisms are different in their way, there
    are some cases where techniques are employed in conjunction. In this section,
    we will explore some evaluative feedback methods that will lay the foundation
    for the rest of the chapter.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习与其他学习类型的关键区别之一在于，它使用信息来评估特定动作的影响，而不是盲目地指示需要采取哪些动作。一方面，评估反馈表明采取的动作有多好，另一方面，指导反馈则表明正确的动作是什么，无论动作是否被采取。尽管这两种机制在方式上有所不同，但在某些情况下，会同时采用一些技术。在本节中，我们将探讨一些评估反馈方法，这将为本章的其余部分奠定基础。
- en: n-Armed Bandit problem
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: n臂老虎机问题
- en: 'A formal definition of this problem with the original gambler analogy is given
    as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的正式定义，与原始的赌徒类比如下：
- en: Note
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: According to Wikipedia, n-armed bandit problem is an issue where the "gambler"
    decides which machine to play, the order of play and the duration of play, he
    then plays and collects the reward that is unique for a machine with a goal to
    maximize the overall rewards.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 根据维基百科，n臂老虎机问题是一个“赌徒”决定玩哪个机器、游戏的顺序和持续时间的问题，然后他开始玩并收集奖励，以最大化总奖励的问题。
- en: Let's consider a case where there are thousands of actions that can be taken.
    Each action fetches a reward, and our goal is to ensure that we take actions in
    such a way that the total of the rewards is maximized over a period. The selection
    of a particular action is called a *play*.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个有数千个动作可以采取的情况。每个动作都会获得一个奖励，我们的目标是确保我们以这种方式采取动作，使得在一段时间内奖励的总和最大化。选择特定动作的行为被称为
    *play*。
- en: An example case that explains the analogy of n-Armed Bandit the problem is that
    of a doctor who needs to choose from a series of options to treat a serious ailment
    where the survival of the patient becomes the reward for the choice of action
    (in this case, the treatment). In this problem, each action is associated with
    an expected reward for a selected action; this is called *value*. If the value
    of each action was known to us, solving n-armed bandit problem is easy as we will
    choose those actions that have a maximum value. It is only possible that we have
    the estimates of the values and are not certain about the actual values.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一个解释 n-Armed Bandit 问题类比案例的例子是一位医生需要从一系列选项中选择来治疗严重疾病，其中患者的生存成为选择动作（在这种情况下，治疗）的奖励。在这个问题中，每个动作都与选择动作的预期奖励相关联；这被称为
    *价值*。如果我们知道每个动作的价值，解决 n-Armed Bandit 问题就很容易，我们将选择那些具有最大价值的动作。只有可能的是，我们有价值的估计，而不是实际的价值。
- en: Now let us see the difference between exploration and exploitation.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看一下探索和利用之间的区别。
- en: Assuming we maintain the estimates of the values, if we choose an action with
    the greatest value (this action is called a greedy action), this situation is
    called exploitation, as we are best using the current knowledge on hand. Moreover,
    all the cases where any non-greedy action is chosen, it would be more of exploring,
    and it would help improve the estimates of the non-greedy action. While exploitation
    helps maximize the expected reward, exploration helps increase the total reward
    in the longer run. The short-term rewards are lower in the case of exploration
    while there might be better long-term total reward. For every action, exploration
    or exploitation approach can be chosen, and what works is a fine balance between
    these two techniques.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们保持对价值的估计，如果我们选择具有最大价值的动作（这种动作被称为贪婪动作），这种情况被称为利用，因为我们正在最好地使用手头的当前知识。此外，所有选择任何非贪婪动作的情况，更多的是探索，这有助于提高非贪婪动作的估计。虽然利用有助于最大化预期奖励，但探索有助于在长期内增加总奖励。在探索的情况下，短期奖励较低，而长期总奖励可能会更好。对于每个动作，可以选择探索或利用的方法，而有效的方法是这两种技术的良好平衡。
- en: So, with this, we will now look at some techniques to best estimate the values
    for actions and to choose the best-suited actions.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，有了这个，我们现在将探讨一些最佳估计动作值和选择最适合动作的技术。
- en: Action-value methods
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 动作值方法
- en: 'If value of an action *a* is *Q*(a)*, then the assessed value of the *t*^(th)
    play is *Q*[t]*(a) I*, the mean of the rewards given that action is chosen, the
    following equation represents this:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果动作 *a* 的值是 *Q*(a)*，那么第 *t* 次游戏的评估值是 *Q*[t]*(a) I*，即在选择了该动作的情况下给出的奖励的平均值，以下方程表示这一点：
- en: '*Q*[t]*(a) = (r1+r2+ … r*[ka] *) / ka*, where *r* is the reward and *ka* is
    the number of times the action *a* is chosen. This is one way of estimating action
    value and not necessarily the best way. Let''s live with this and now look at
    the methods to choose actions.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*Q*[t]*(a) = (r1+r2+ … r*[ka] *) / ka*，其中 *r* 是奖励，*ka* 是动作 *a* 被选择的次数。这是估计动作值的一种方法，但不一定是最好的方法。让我们接受这一点，现在来看看选择动作的方法。'
- en: 'The easiest action selection rule is to select an action or one of the actions,
    *a*, that has the highest estimated action value. So, for a given play *t*, choosing
    a greedy action *a** can be shown as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的动作选择规则是选择一个动作或动作之一，即 *a*，它具有最高的估计动作值。所以，对于给定的游戏 *t*，选择贪婪动作 *a* 可以表示如下：
- en: Q[t](a*) = max[a] Q[t](a)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Q[t](a*) = max[a] Q[t](a)
- en: This method by definition exploits the current knowledge with a little focus
    on whether the action is a better option. As an alternative to this method, we
    can choose to be greedy most of the time, and once in a while, select an action
    independent of the value estimation. With a probability of ԑ, this method is called
    the ԑ**-greedy method**.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法根据定义利用了当前的知识，同时稍微关注一下动作是否是一个更好的选择。作为这种方法的替代，我们可以选择大多数时候采取贪婪策略，偶尔选择一个与价值估计无关的动作。以概率
    ԑ，这种方法被称为 ԑ**-贪婪方法**。
- en: Reinforcement comparison methods
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 强化比较方法
- en: We have been seeing in most of the selection methods that an action that has
    the largest reward has a higher likelihood of being selected than an action with
    a lesser reward. The important question is how to qualify whether a reward is
    big or small. We will always need to have a reference number that qualifies if
    a reward has a high value or a low value. This reference value is called **reference
    reward**. A reference reward, to start with, can be an average of previously received
    rewards. Learning methods that use this idea are called comparison reinforcement
    methods. These methods are more efficient than actor-value methods and form a
    basis for an actor-critic method that we will discuss in the sections to come.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数选择方法中，我们看到具有最大奖励的动作比具有较小奖励的动作更有可能被选中。重要的问题是如何判断奖励是大还是小。我们始终需要有一个参考数字来判断奖励是高值还是低值。这个参考值被称为**参考奖励**。首先，参考奖励可以是之前收到的奖励的平均值。使用这个想法的学习方法被称为比较强化方法。这些方法比actor-value方法更有效，是我们将在后续章节中讨论的actor-critic方法的基础。
- en: The Reinforcement Learning problem – the world grid example
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 强化学习问题 – 世界网格示例
- en: 'We will try to understand the Reinforcement Learning problem using a famous
    example: the grid world. This particular grid world is a 3X4 grid, as shown in
    the following screenshot, and is an approximation of the complexity of the world:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将尝试使用一个著名的例子来理解强化学习问题：网格世界。这个特定的网格世界是一个3X4网格，如下面的截图所示，是对世界复杂性的近似：
- en: '![The Reinforcement Learning problem – the world grid example](img/B03980_12_07.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![强化学习问题 – 世界网格示例](img/B03980_12_07.jpg)'
- en: This example assumes the world is kind of a game where you start with a state
    called start state (from the location *1,1*). Let's assume four actions can be
    taken that include moving left, right, up, and down. The goal is to ensure using
    these actions that we move towards the goal that is represented in the location
    *4,3*. We need to avoid the red box that is shown in the location *4,2* like it
    is shown in the next image.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子假设世界是一种游戏，你从一个称为起始状态的状态（从位置 *1,1*）开始。让我们假设可以采取四种动作，包括向左、向右、向上和向下移动。目标是确保使用这些动作，我们能够移动到表示在位置
    *4,3* 的目标。我们需要避免像下一张图片中显示的那样在位置 *4,2* 显示的红框。
- en: '**Start state**: position *(1,1) -->* The world starts here.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**起始状态**：位置 *(1,1) -->* 世界从这里开始。'
- en: '**Success state**: position *(4,3) -->* The world ends here in a success state.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成功状态**：位置 *(4,3) -->* 世界在这里以成功状态结束。'
- en: '**Failure state**: position *(4,2) -->* The world ends here in a failure state.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**失败状态**：位置 *(4,2) -->* 世界在这里以失败状态结束。'
- en: When the world ends, we need to start over again.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当世界结束时，我们需要重新开始。
- en: '**Wall**: There is a roadblock or a wall shown in the position *(2,2)*. This
    position cannot be navigated:![The Reinforcement Learning problem – the world
    grid example](img/B03980_12_08.jpg)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**墙壁**：在位置 *(2,2)* 显示了一个障碍物或墙壁。这个位置无法通过：![强化学习问题 – 世界网格示例](img/B03980_12_08.jpg)'
- en: To reach the goal *(4,3)* from the start point *(1,1)*, steps can be taken in
    the following directions:![The Reinforcement Learning problem – the world grid
    example](img/B03980_12_09.jpg)
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要从起点 *(1,1)* 达到目标 *(4,3)*，可以采取以下方向的步骤：![强化学习问题 – 世界网格示例](img/B03980_12_09.jpg)
- en: Every step in a direction moves you from one position to another (position here
    is nothing but the state). For example, a movement in the *UP* direction from
    the position *(1,1)* will take you to the position *(1,2)* and so on.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个方向的每一步都会将你从一个位置移动到另一个位置（这里的“位置”只是状态）。例如，从位置 *(1,1)* 向*上*移动将带你到位置 *(1,2)*，依此类推。
- en: All directions cannot be taken from a given position. Let us consider the example
    shown in the following screenshot. From the position *(3,2)*, only *UP*, *DOWN*,
    and *RIGHT* can be taken. *LEFT* movement will hit the wall and hence cannot be
    taken. That said, only *UP* and *DOWN* movements make sense, as *RIGHT* will make
    a move to the danger position that results in failure in reaching the goal.![The
    Reinforcement Learning problem – the world grid example](img/B03980_12_10.jpg)
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从一个给定的位置不能采取所有方向。让我们考虑以下截图中的示例。从位置 *(3,2)*，只能采取*上*、*下*和*右*。*左*移动会撞到墙壁，因此不能采取。话虽如此，只有*上*和*下*的移动是有意义的，因为*右*会将你移动到危险位置，导致无法达到目标。![强化学习问题
    – 世界网格示例](img/B03980_12_10.jpg)
- en: Similarly, any of the positions in the boundaries of the grid will have limitations,
    for example, the position *(1,3)* allows *RIGHT* and *DOWN* movements and any
    other movements do not alter the position.![The Reinforcement Learning problem
    – the world grid example](img/B03980_12_11.jpg)
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似地，网格边界上的任何位置都将有限制，例如，位置 *(1,3)* 允许 *向右* 和 *向下* 移动，其他任何移动都不会改变位置。![强化学习问题 –
    世界网格示例](img/B03980_12_11.jpg)
- en: 'Let''s now look at the shortest path the from *Start (1,1)* to the *Goal (4,3)*.
    There are two solutions:'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在我们来看从 *起点 (1,1)* 到 *目标 (4,3)* 的最短路径。有两种解决方案：
- en: '**Solution 1**: *RIGHT* --> *RIGHT* --> *UP* --> *UP* --> *RIGHT* (5 steps)![The
    Reinforcement Learning problem – the world grid example](img/B03980_12_12.jpg)'
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解决方案 1**：*向右* --> *向右* --> *向上* --> *向上* --> *向右* (5 步)![强化学习问题 – 世界网格示例](img/B03980_12_12.jpg)'
- en: '**Solution 2**: *UP* --> *UP* --> *RIGHT* --> *RIGHT* --> *RIGHT* (5 steps)'
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解决方案 2**：*向上* --> *向上* --> *向右* --> *向右* --> *向右* (5 步)'
- en: '![The Reinforcement Learning problem – the world grid example](img/B03980_12_13.jpg)'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![强化学习问题 – 世界网格示例](img/B03980_12_13.jpg)'
- en: 'In the real world, not all actions get executed as expected. There is a reliability
    factor that affects the performance, or rather, there is uncertainty. If we add
    a small caveat to the example and say that every time there is an action to move
    from one position to another, the probability that the movement is correct is
    0.8\. This means there is an 80% possibility that a movement executes as expected.
    In this case, if we want to measure the probability of Solution 1, (*R*-->*R*-->*U*-->*U*-->*R*)
    is succeeding:'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在现实世界中，并非所有动作都能按预期执行。存在一个可靠性因素，它会影响性能，或者说，存在不确定性。如果我们对示例稍作修改，并说每次从一位置移动到另一位置的动作，移动正确的概率是
    0.8。这意味着有 80% 的可能性移动会按预期执行。在这种情况下，如果我们想测量解决方案 1 (*R*-->*R*-->*U*-->*U*-->*R*)
    成功的概率：
- en: Probability of actions happening as expected + Probability of actions not happening
    as expected
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 行为发生的预期概率 + 行为未按预期发生的概率
- en: '*= 0.8 x 0.8 x 0.8 x 0.8 x 0.8 + 0.1 x 0.1 x 0.1 x 0.1 x 0.8*'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*= 0.8 x 0.8 x 0.8 x 0.8 x 0.8 + 0.1 x 0.1 x 0.1 x 0.1 x 0.8*'
- en: '*= 0.32768 + 0.00008 = 0.32776*'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*= 0.32768 + 0.00008 = 0.32776*'
- en: As we see, the element of uncertainty does change the result. In the next section,
    we will discuss the decision process framework that captures these uncertainties.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如我们所见，不确定性的元素确实会改变结果。在下一节中，我们将讨论捕捉这些不确定性的决策过程框架。
- en: Markov Decision Process (MDP)
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程 (MDP)
- en: Markov's Decision Process is an essential framework or process to make decisions,
    and we will be bringing it up in most of the sections that follow on Reinforcement
    Learning.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程是做出决策的一个基本框架或过程，我们将在后续的强化学习章节中多次提到它。
- en: '**Markov property** is core to the Markov Decision Process, and it states that
    what matters is the present or current state and that the situation is stationary,
    which means that the rules do not change.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**马尔可夫属性**是马尔可夫决策过程的核心，它指出重要的是当前或当前状态，并且情况是稳定的，这意味着规则不会改变。'
- en: 'MDP tries to capture the world that we discussed in the preceding section that
    has the following features:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: MDP 尝试捕捉我们在前面章节中讨论的世界，它具有以下特征：
- en: '![Markov Decision Process (MDP)](img/B03980_12_14.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![马尔可夫决策过程 (MDP)](img/B03980_12_14.jpg)'
- en: '**States**: In the preceding example, every grid position denotes a state'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态**：在前面的例子中，每个网格位置代表一个状态'
- en: '**Model** (Transition function): A Transition function includes three attributes:
    given state, action, and the destination state. It describes the probability of
    an end state *s*, given the current state *s* and action *a*:'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型**（转移函数）：转移函数包括三个属性：给定状态、动作和目标状态。它描述了在当前状态 *s* 和动作 *a* 下，结束状态 *s* 的概率：'
- en: T (s, a,s') ~ P(s'/s, a)
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: T (s, a,s') ~ P(s'/s, a)
- en: '**Actions**: A(s), A In the preceding example, *A (1, 2)* = *UP* in the *UP*,
    *UP* *RIGHT*, *RIGHT*, *RIGHT* solution'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动作**：A(s)，A 在前面的例子中，*A (1, 2)* = *向上* 在 *向上*、*向上* *向右*、*向右*、*向右* 解决方案中'
- en: '**Rewards**: *R(s)*, *R(s,a)*, *R(s,a,s1)* Rewards tell us the usefulness of
    entering into a state'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励**：*R(s)*，*R(s,a)*，*R(s,a,s1)* 奖励告诉我们进入状态的有用性'
- en: '*R(s)*: Reward for entering a state *s*'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*R(s)*：进入状态 *s* 的奖励'
- en: '*R(s, a)*: Reward for the opening of a state *s* for an action *a*'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*R(s, a)*：打开状态 *s* 的动作 *a* 的奖励'
- en: '*R(s, a,s1)*: Reward for the opening of a state *s1* for an action *a* given
    that you were in state *s*'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*R(s, a,s1)*：在状态 *s* 下执行动作 *a* 打开状态 *s1* 的奖励'
- en: State, Action, Model and Rewards make the problem statement of MDP
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态、动作、模型和奖励构成了 MDP 的问题陈述
- en: '**Policy**: It is a solution for a problem; it says what action should be taken
    given a state:'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略**：它是对问题的解决方案；它说明了在给定状态下应该采取什么行动：'
- en: '*π(s)* --> *a*'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*π(s)* --> *a*'
- en: Basic RL model – agent-environment interface
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基本强化学习模型 – 代理-环境接口
- en: As we have discovered, an RL problem is a straightforward way of learning from
    interaction to achieve a goal. Agent is the learner or decision-maker, and it
    interacts with the environment, and that is everything outside in this environment
    gives rise to rewards. The thing it interacts with, comprising everything outside
    the agent, is called the environment. The complete specification of an environment
    is defined as a task—a single instance of Reinforcement Learning problem.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所发现的，强化学习问题是一种直接从交互中学习以实现目标的方法。代理是学习者或决策者，它与环境互动，而这个环境中的所有外部因素都会产生奖励。它与交互的事物，包括代理之外的所有事物，被称为环境。环境的完整规范定义为任务——强化学习问题的单个实例。
- en: 'The following model depicts the agent-environment interface:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 以下模型描述了代理-环境接口：
- en: '![Basic RL model – agent-environment interface](img/B03980_12_15.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![基本强化学习模型 – 代理-环境接口](img/B03980_12_15.jpg)'
- en: An environment model here means a context where an agent uses anything to predict
    the behavior of the environment for a given action. This environment model produces
    a prediction of the next state and the reward, given the action and the current
    state. There will be multiple next state rewards possible in case the model is
    stochastic. Again, these models can be distributed or sample-based. The distributed
    models identify all the potential probabilities, while a sample model produces
    the probability given that sample.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，环境模型意味着一个代理使用任何东西来预测给定动作的环境行为的上下文。这个环境模型根据动作和当前状态产生对下一个状态和奖励的预测。如果模型是随机的，则可能存在多个下一个状态奖励。再次强调，这些模型可以是分布式的或基于样本的。分布式模型识别所有潜在的概率，而基于样本的模型在给定样本的情况下产生概率。
- en: 'Finally, the goals of Reinforcement Learning can be defined as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，强化学习的目标可以定义为以下内容：
- en: 'In an environment where every action taken results in a new situation, RL is
    about how to take actions. The following can be the actions:'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个每个动作都会导致新情况出现的环境中，强化学习（RL）关注的是如何采取行动。以下是一些可能的动作：
- en: Define a policy that maps the action and the resultant situation
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义一个策略，将动作和结果情况映射
- en: Identify the policy that results in highest rewards being given
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定导致最高奖励的策略
- en: 'Steps in Reinforcement Learning are as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的步骤如下：
- en: The agent observes the input state.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理观察输入状态。
- en: By applying the policy that is a decision-making function, an action is identified.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过应用策略，这是一个决策函数，可以确定一个动作。
- en: The action is executed that results in a state change.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行动作导致状态变化。
- en: As a consequence of this action, the agent receives a significant reward from
    the environment.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于这个动作，代理从环境中获得了一个显著的奖励。
- en: The details of the reward, given the change in the state, are recorded.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据状态的变化记录奖励的详细信息。
- en: Delayed rewards
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 延迟奖励
- en: One of the aspects that differentiate Reinforcement Learning from supervised
    learning is *rewards*. In this section, we will explore what delayed rewards mean.
    As we know, every action that results in a particular state change results in
    a reward. The realization of this reward in some cases is not immediate. Let's
    look at an example a chess game. Let's assume it took 65 steps to end a chess
    game and only at the end of 65 steps or moves we get to know if we have won the
    game or lost it. Which of the 65 steps or moves were the cause of the success
    or failure is what is complex here. So, the reward is not known until the end
    of the game or the sequence of actions. Technically, we are looking at identifying
    which sequence of actions resulted in gaining the reward that was seen. This process
    is called **Temporal Credit Assignment**.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 区分强化学习与监督学习的一个方面是*奖励*。在本节中，我们将探讨延迟奖励的含义。正如我们所知，每个导致特定状态变化的动作都会产生奖励。在某些情况下，这种奖励的实现并不是立即的。让我们看看一个棋盘游戏的例子。假设棋盘游戏需要65步才能结束，只有在第65步或移动结束时，我们才能知道我们是否赢得了游戏或输了。哪一步或移动导致了成功或失败在这里是复杂的。因此，奖励直到游戏结束或动作序列结束时才可知。技术上，我们正在寻找识别哪个动作序列导致了所看到的奖励。这个过程被称为**时间信用分配**。
- en: Now, in this journey of achieving the ultimate reward that is a success (+1)
    or failure (-1), every step or move or action would fetch a reward. Let's assume
    every step in solution 1 of the grid world problem fetches a reward of -0.4\.
    The collective rewards that take to success or failure will determine long-term
    reward.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在这个实现最终奖励（成功+1或失败-1）的旅程中，每一步或移动或行动都会获得奖励。假设网格世界问题解决方案1中的每一步都获得奖励-0.4。导致成功或失败的总奖励将决定长期奖励。
- en: The policy
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 策略
- en: 'An optimal policy is a policy or solution that maximizes the expected long-term
    reward and can be represented by the following formula:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳策略是一种最大化预期长期奖励的策略或解决方案，可以用以下公式表示：
- en: '![The policy](img/B03980_12_26.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![策略](img/B03980_12_26.jpg)'
- en: 'Now, let''s measure the utility of a particular state(*s*) that depends on
    the policy (*π*):'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们衡量一个特定状态（*s*）的效用，这取决于策略（*π*）：
- en: '![The policy](img/B03980_12_27.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![策略](img/B03980_12_27.jpg)'
- en: A reward to enter a state(*s*) (this is an immediate benefit) is not equal to
    the utility of that state (this is a long-term benefit of entering the state).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 进入状态（*s*）的奖励（这是一种即时利益）不等于该状态的效用（这是进入该状态的长远利益）。
- en: 'Now, we can define the optimal policy using the utility of the state value:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用状态价值的效用来定义最佳策略：
- en: '![The policy](img/B03980_12_28.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![策略](img/B03980_12_28.jpg)'
- en: 'Now, if we have to define the utility of being in a state(*s*), it is equal
    to the reward for getting into that state, discounting the reward that we get
    from that point on:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们必须定义处于状态（*s*）的效用，它等于进入该状态的奖励，减去从该点开始获得的奖励：
- en: '![The policy](img/B03980_12_29.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![策略](img/B03980_12_29.jpg)'
- en: This is called **Bellman Equation**.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为**贝尔曼方程**。
- en: '*V** is a value function for a policy, and the following is the Bellman optimality
    equation that expresses the fact that the value of a state with an optimal policy
    is the same as the best expected return from the best action for that state:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*V* 是策略的价值函数，以下是将最优策略的价值与该状态的最佳预期回报相等的贝尔曼最优方程：'
- en: '![The policy](img/B03980_12_16.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![策略](img/B03980_12_16.jpg)'
- en: Reinforcement Learning – key features
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习 – 关键特性
- en: Reinforcement Learning is not a set of techniques but is a set of problems that
    focuses on what the task is as, against how the task should be addressed.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习不是一系列技术，而是一系列问题，它关注的是任务本身，而不是如何处理任务。
- en: Reinforcement Learning is considered as a tool for machines to learn using the
    rewards and punishments that are more trial-and-error driven.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习被视为一种工具，机器可以通过更多基于试错法的奖励和惩罚来学习。
- en: Reinforcement Learning employs evaluative feedback. Evaluative feedback measures
    how effective the action taken is as against measuring the action if it is best
    or worst. (Note that supervised learning is more of an instructive learning and
    determines the correctness of an action irrespective of the action being executed.)
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习采用评估反馈。评估反馈衡量采取的行动的有效性，而不是衡量该行动是否为最佳或最差。（注意，监督学习更侧重于指导性学习，并确定行动的正确性，而不考虑执行的行动。）
- en: The tasks in Reinforcement Learning are more of related tasks. Associative tasks
    are dependent on the situation where actions that suit best to the given situation
    are identified and executed. Non-associative tasks are those that are independent
    of the particular situation, and the learner finds the best action when the task
    is stationary.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习中的任务更多的是相关任务。关联任务依赖于情况，其中识别并执行最适合给定情况的动作。非关联任务是指独立于特定情况的任务，当任务处于静止状态时，学习者找到最佳动作。
- en: Reinforcement learning solution methods
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习解决方案方法
- en: In this section, we will discuss in detail some of the methods to solve Reinforcement
    Learning problems. Specifically, dynamic programming (DP), Monte Carlo method,
    and temporal-difference (TD) learning. These methods address the problem of delayed
    rewards as well.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将详细讨论解决强化学习问题的一些方法。特别是动态规划（DP）、蒙特卡洛方法和时序差分（TD）学习。这些方法也解决了延迟奖励的问题。
- en: Dynamic Programming (DP)
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动态规划（DP）
- en: 'DP is a set of algorithms that are used to compute optimal policies given a
    model of environment like Markov Decision Process. Dynamic programming models
    are both computationally expensive and assume perfect models; hence, they have
    low adoption or utility. Conceptually, DP is a basis for many algorithms or methods
    used in the following sections:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 动态规划（DP）是一组算法，用于在给定环境模型（如马尔可夫决策过程）的情况下计算最优策略。动态规划模型在计算上昂贵，并假设完美模型；因此，它们的采用或效用较低。从概念上讲，DP是以下章节中使用的许多算法或方法的基础：
- en: '**Evaluating the policy**: A policy can be assessed by computing the value
    function of the policy in an iterative manner. Computing value function for a
    policy helps find better policies.'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**评估策略**：可以通过迭代方式计算策略的价值函数来评估策略。计算策略的价值函数有助于找到更好的策略。'
- en: '**Improving the policy**: Policy improvement is a process of computing the
    revised policy using its value function information.'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**改进策略**：策略改进是一个使用其价值函数信息计算修订策略的过程。'
- en: '**Value iteration and Policy Iteration**: Policy evaluation and improvement
    together derive value and policy iteration. These are two of the most popular
    DP methods that are used to compute the optimal policies and value functions given
    complete knowledge of the MDPs.'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**值迭代和策略迭代**：策略评估和改进同时推导出值和策略迭代。这些是两种最流行的动态规划方法，用于在完全了解马尔可夫决策过程（MDPs）的情况下计算最优策略和价值函数。'
- en: 'The following algorithm depicts the iteration policy process:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 以下算法描述了迭代策略过程：
- en: '![Dynamic Programming (DP)](img/B03980_12_17.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![动态规划（DP）](img/B03980_12_17.jpg)'
- en: 'Value iteration combines a solid policy improvement and process evaluation.
    The following are the steps involved:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 值迭代结合了坚实的策略改进和过程评估。以下涉及到的步骤：
- en: '![Dynamic Programming (DP)](img/B03980_12_18.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![动态规划（DP）](img/B03980_12_18.jpg)'
- en: Generalized Policy Iteration (GPI)
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 广义策略迭代（GPI）
- en: GPI is a way of categorizing Dynamic Programming (DP) methods. GPI involves
    interaction between two processes—one around the approximate policy, and the other
    around the approximate value.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: GPI是一种对动态规划（DP）方法进行分类的方法。GPI涉及两个过程之间的交互——一个围绕近似策略，另一个围绕近似价值。
- en: In the first case, the process picks the policy as it is and performs policy
    evaluation to identify the true or exact value function associated with the policy.
    The other process picks the value function as the input and uses that to change
    the policy such that it improves the policy, which is its total reward. If you
    observe, each process changes the basis for the other process, and they work in
    conjunction to find a joint solution that results in an optimal policy and value
    function.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一种情况下，过程直接选择策略并执行策略评估以确定与策略相关联的真实或确切的价值函数。另一种过程以价值函数作为输入，并使用它来改变策略，以便改进策略，即其总奖励。如果你观察，每个过程都改变了另一个过程的基础，并且它们协同工作以找到导致最优策略和价值函数的联合解决方案。
- en: Monte Carlo methods
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蒙特卡洛方法
- en: 'Monte Carlo methods in Reinforcement Learning learn policies and values from
    experience as samples. Monte Carlo methods have additional advantages over Dynamic
    Programming methods because of the following:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，蒙特卡洛方法通过经验样本学习策略和价值。由于以下原因，蒙特卡洛方法在动态规划方法之上具有额外的优势：
- en: Learning optimal behavior happens directly from the interactions with the environment
    without any model that simulates model dynamics.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接从与环境交互中学习最优行为，而不需要任何模拟模型动态的模型。
- en: These methods can be used on simulated data or sample models; this feature becomes
    paramount in real-world applications.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些方法可以用于模拟数据或样本模型；这一特性在现实世界的应用中变得至关重要。
- en: With Monte Carlo methods, we can easily focus on smaller sets of states, and
    we can explore a region of interest without necessarily going into complete state
    set.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用蒙特卡洛方法，我们可以轻松地关注更小的状态集，并且可以在不必要进入完整状态集的情况下探索感兴趣的区域。
- en: Monte Carlo methods are least impacted for any violation of Markov's property
    because the estimation for the value is not updated using any of the successor
    states. This also means that they do not bootstrap.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蒙特卡洛方法对于任何违反马尔可夫属性的情况影响最小，因为价值的估计不是使用任何后续状态来更新的。这也意味着它们不需要自举。
- en: Monte Carlo methods are designed by the **Generalized Policy Iteration** (**GPI**)
    method. These methods provide an alternative way of evaluating the policies. For
    each state, instead of independently computing the value, an average value of
    the returns for starting at that state is taken, and this can be a good approximation
    of the value of that state. The focus is to apply action-value functions to improve
    the policies as this does not require environment's transition changes. Monte
    Carlo methods mix policy evaluation and improvement methods and can be implemented
    on a step-by-step basis.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡洛方法是由**广义策略迭代**（**GPI**）方法设计的。这些方法提供了一种评估策略的替代方法。对于每个状态，而不是独立计算价值，取从该状态开始返回的平均值，这可以很好地近似该状态的价值。重点是应用动作值函数来改进策略，因为这不需要环境转换的变化。蒙特卡洛方法混合了策略评估和改进方法，可以逐步实现。
- en: How much of exploration is good enough? This is a crucial question to answer
    in Monte Carlo methods. It is not sufficient to select actions that are best based
    on their value; it is also important to know how much of this is contributing
    to ending reward.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 探索多少是足够的？这是蒙特卡洛方法中需要回答的关键问题。仅仅根据价值选择最佳动作是不够的；了解这些动作中有多少有助于结束奖励也很重要。
- en: Two methods can be used in this case—**on-policy** or **off-policy** methods.
    In an on-policy method, the agent is responsible for finding the optimal policy
    using exploration technique; and in off-policy methods, agent exploration is not
    central, but along with it learns a deterministic optimal policy that need not
    be related to the policy followed. In short, off-policy learning methods are all
    about learning behavior through behavior.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下可以使用两种方法——**在策略**或**离策略**方法。在在策略方法中，智能体负责使用探索技术找到最优策略；而在离策略方法中，智能体的探索不是核心，但与之学习一个确定的最优策略，这个策略不必与遵循的策略相关。简而言之，离策略学习方法都是关于通过行为学习行为。
- en: Temporal difference (TD) learning
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时序差分（TD）学习
- en: 'TD learning is one of the unique techniques of Reinforcement Learning. Temporal
    difference learning is a combination of Monte Carlo methods and dynamic programming
    methods. The most discussed technique in Reinforcement Learning is the relationship
    between temporal difference (TD), dynamic programming (DP), and Monte Carlo methods:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: TD学习是强化学习中的独特技术之一。时序差分学习是蒙特卡洛方法和动态规划方法的结合。在强化学习中讨论最多的技术是时序差分（TD）、动态规划（DP）和蒙特卡洛方法之间的关系：
- en: Evaluate a policy that includes estimating the value function *V*π for a given
    policy *π*.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估一个包含估计给定策略π的价值函数Vπ的策略。
- en: Select an optimal policy. For policy selection, all the DP, TD, and Monte Carlo
    methods use a variation of generalized policy iteration (GPI). Hence, the difference
    in these three methods is nothing but these variations in GPI.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个最优策略。对于策略选择，所有DP、TD和蒙特卡洛方法都使用广义策略迭代（GPI）的变体。因此，这三者之间的区别不过是GPI中的这些变体。
- en: TD methods follow bootstrapping technique to derive an estimate; they fall back
    on the successor states and similar estimates.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: TD方法遵循自举技术来得出估计；它们会回退到后续状态和类似的估计。
- en: 'Let''s now look at some advantages of TD over DP and Monte Carlo methods. We
    will cover this in brief and without delving into too much of complexities. Following
    are the key benefits:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看TD方法相对于DP和蒙特卡洛方法的优点。我们将简要介绍，而不深入复杂性。以下是一些关键好处：
- en: TD methods do not require the model of the environment and probability distributions
    of the next states and rewards
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TD方法不需要环境模型以及下一状态和奖励的概率分布
- en: TD methods can easily and elegantly run in an online and incremental manner
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TD方法可以轻松优雅地以在线和增量方式运行
- en: Sarsa - on-Policy TD
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Sarsa - 在策略TD
- en: 'Let''s look at using TD methods for the control problems. We will continue
    to use GPI techniques, but now in conjunction with TD methods for the evaluation
    and prediction. While we need to have a balance between exploration and exploitation
    options, we have the option to choose on-policy or off-policy learning methods
    in here. We will stick to the on-policy method:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用TD方法来解决控制问题。我们将继续使用GPI技术，但现在结合TD方法进行评估和预测。虽然我们需要在探索和利用选项之间保持平衡，但我们有选择在策略或离策略学习方法中的选项。我们将坚持使用在策略方法：
- en: Learn the action-value function in relation to the state-value function. We
    will define *Q*^π*(s, a)* for the policy *π*:![Sarsa - on-Policy TD](img/B03980_12_19.jpg)
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 学习与状态值函数相关的动作值函数。我们将为策略 *π* 定义 *Q*^π*(s, a)*：![Sarsa - on-Policy TD](img/B03980_12_19.jpg)
- en: Learn the value of transition from one state-action pair to another state-action
    pair. This is computed iteratively as follows:![Sarsa - on-Policy TD](img/B03980_12_20.jpg)
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 学习从一个状态-动作对转换到另一个状态-动作对的值。这通过以下方式迭代计算：![Sarsa - on-Policy TD](img/B03980_12_20.jpg)
- en: This is defined as a Sarsa prediction method and is on-policy as the agent uses
    the identified policy to do this.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这被定义为 Sarsa 预测方法，并且是 on-policy 的，因为智能体使用识别的策略来完成这个任务。
- en: 'The Sarsa algorithm is stated as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: Sarsa 算法表述如下：
- en: '![Sarsa - on-Policy TD](img/B03980_12_21.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![Sarsa - on-Policy TD](img/B03980_12_21.jpg)'
- en: Q-Learning – off-Policy TD
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Q-Learning – off-Policy TD
- en: 'The Q-Learning technique that employs the off-policy learning method is one
    of the groundbreaking strategies of TD. This control algorithm called Q-learning
    (Watkins, 1989) in a simple form is defined as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 使用离策略学习方法的 Q-Learning 技术是 TD 的突破性策略之一。这种称为 Q-learning 的控制算法（Watkins，1989）以简单形式定义如下：
- en: '![Q-Learning – off-Policy TD](img/B03980_12_22.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![Q-Learning – off-Policy TD](img/B03980_12_22.jpg)'
- en: We can see the optimal action-value function *Q** is directly approximated using
    the learned action-value function *Q* irrespective of the policy that it follows.
    This makes it an off-policy method.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到最优动作值函数 *Q** 是直接使用学习到的动作值函数 *Q* 近似，而不考虑它遵循的策略。这使得它成为一个 off-policy 方法。
- en: There is still a small effect on the policy seen as the policy value functions
    are used and updated. Moreover, updates to all the pairs diligently mark convergence.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 由于策略值函数被使用和更新，因此对策略仍有一定的影响。此外，对所有对的勤奋标记标志着收敛。
- en: 'Based on this understanding, the Q-learning algorithm can be depicted as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这种理解，Q-learning 算法可以描述如下：
- en: '![Q-Learning – off-Policy TD](img/B03980_12_23.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![Q-Learning – off-Policy TD](img/B03980_12_23.jpg)'
- en: Actor-critic methods (on-policy)
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Actor-critic methods (on-policy)
- en: 'Actor-critic methods are temporal difference learning methods that ensure policy
    and value independence using a separate memory structure. In this case, the policy
    structure is called as an *actor,* and the value structure is called as a *critic*.
    The name critic comes from the fact that it criticizes the value of the policy.
    Since this critic always criticizes the value of the policy, it is also called
    the TD error. The following screenshot shows the actor-critic method flow:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Actor-critic 方法是时间差分学习方法，它使用单独的记忆结构确保策略和值的独立性。在这种情况下，策略结构被称为 *actor*，值结构被称为
    *critic*。名称 critic 来自于它批评策略的价值。由于这个 critic 总是批评策略的价值，它也被称为 TD 错误。以下截图显示了 actor-critic
    方法流程：
- en: '![Actor-critic methods (on-policy)](img/B03980_12_24.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![Actor-critic methods (on-policy)](img/B03980_12_24.jpg)'
- en: R Learning (Off-policy)
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: R Learning (Off-policy)
- en: 'R-learning is an advanced Reinforcement Learning technique that is used in
    cases where there are no discounts with definitive and finite returns. The algorithm
    is as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: R-learning 是一种高级强化学习技术，用于没有折扣且有确定和有限回报的情况。算法如下：
- en: '![R Learning (Off-policy)](img/B03980_12_25.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![R Learning (Off-policy)](img/B03980_12_25.jpg)'
- en: Implementing Reinforcement Learning algorithms
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现强化学习算法
- en: Refer to the source code provided for this chapter to implement Reinforcement
    learning algorithms. (Source code path `.../chapter12/...` under each of the folder
    for the technology.)
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考本章提供的源代码来实现强化学习算法。（源代码路径 `.../chapter12/...` 在每个技术文件夹下。）
- en: Using Mahout
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Mahout
- en: Refer to the folder `.../mahout/chapter12/rlexample/`.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹 `.../mahout/chapter12/rlexample/`。
- en: Using R
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 R
- en: Refer to the folder `.../r/chapter12/rlexample/`.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹 `.../r/chapter12/rlexample/`。
- en: Using Spark
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Spark
- en: Refer to the folder `.../spark/chapter12/rlexample/`.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹 `.../spark/chapter12/rlexample/`。
- en: Using Python (Scikit-learn)
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Python (Scikit-learn)
- en: Refer to the folder `.../python-scikit-learn/chapter12/rlexample/`.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹 `.../python-scikit-learn/chapter12/rlexample/`。
- en: Using Julia
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Julia
- en: Refer to the folder `.../julia/chapter12/rlexample/`.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹 `.../julia/chapter12/rlexample/`。
- en: Summary
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored a new learning technique called Reinforcement Learning.
    We saw how this was different from traditional supervised and unsupervised learning
    techniques. The goal of Reinforcement Learning is decision making and at the heart
    of it is MDP. We explored the elements of MDP and learned about it using an example.
    We then covered some fundamental Reinforcement Learning techniques that are on-policy
    and off-policy, and some of them are indirect and direct methods of learning.
    We covered dynamic programming (DP) methods, Monte Carlo methods, and some key
    temporal difference (TD) methods like Q-learning, Sarsa, R-learning, and actor-critic
    methods. Finally, we had hands-on implementations for some of these algorithms
    using our standard technology stack identified for this book. In the next chapter,
    we will cover ensemble learning methods.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探索了一种名为强化学习的新学习技术。我们看到了它与传统的监督学习和无监督学习技术的不同之处。强化学习的目标是决策，其核心是马尔可夫决策过程（MDP）。我们探讨了MDP的要素，并通过一个例子来了解它。然后，我们介绍了强化学习的某些基本技术，包括策略学习和非策略学习，其中一些是间接和直接的学习方法。我们涵盖了动态规划（DP）方法、蒙特卡洛方法，以及一些关键的时序差分（TD）方法，如Q学习、Sarsa、R学习和演员-评论家方法。最后，我们使用为本书确定的标准技术栈对这些算法进行了一些实际应用。在下一章中，我们将介绍集成学习方法。
