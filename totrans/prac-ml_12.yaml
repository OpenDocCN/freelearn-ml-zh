- en: Chapter 12. Reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have covered supervised and unsupervised learning methods in-depth in [Chapter
    5](ch05.html "Chapter 5. Decision Tree based learning"), *Decision Tree based
    learning*, with various algorithms. In this chapter, we will be covering a new
    learning technique that is different from both supervised and unsupervised learning
    called **Reinforcement Learning** (**RL**). Reinforcement Learning is a particular
    type of Machine learning where the learning is driven by the feedback from the
    environment, and the learning technique is iterative and adaptive. RL is believed
    to be closer to human learning. The primary goal of RL is decision making and
    at the heart of it lies **Markov''s Decision Process** (**MDP**). In this chapter,
    we will cover some basic Reinforcement Learning methods like **Temporal Difference**
    (**TD**), certainty equivalence, policy gradient, dynamic programming, and more.
    The following figure depicts different data architecture paradigms that will be
    covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reinforcement learning](img/B03980_12_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following topics are covered in depth in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Recap of supervised, semi-supervised, and unsupervised learning, and the context
    of Reinforcement Learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Understanding MDP is key to Reinforcement Learning. Regarding this, the following
    topics are covered in this chapter:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What does MDP mean, key attributes, states, reward, actions, and transitions
    (discounts)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The underlying process of MDP and how it helps in the decision process
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Policies and value functions (also called utilities, as in a group of rewards)
    and how we assign value to an infinite sequence of rewards
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bellman Equation**—the value iteration and policy iteration'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Regarding Reinforcement Learning, we will cover the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Planning and learning in MDP
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Connection planning and functional approximation in RL
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Different RL methods and approaches to RL, such as simple decision theory, the
    **temporal difference** (**TD**), dynamic programming, policy gradient, certainty
    equivalence, and eligibility traces
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Key algorithms such as Q-learning, Sarsa, and others
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning applications
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement Learning (RL)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's do a recap of supervised, semi-supervised, and unsupervised learning,
    and set the context for Reinforcement Learning. In [Chapter 1](ch01.html "Chapter 1. Introduction
    to Machine learning"), *Introduction to Machine Learning*, we covered the basic
    definitions of supervised, semi-supervised, and unsupervised learning. Inductive
    learning is a reasoning process that uses the results of one experiment to run
    the next set of experiments and iteratively evolve a model from specific information.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts various subfields of Machine learning. These subfields
    are one of the ways the Machine learning algorithms are classified:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reinforcement Learning (RL)](img/B03980_12_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Supervised learning is all about operating to a known expectation, and in this
    case, what needs to be analyzed from the data being defined. The input datasets
    in this context are also referred to as **labeled** datasets. Algorithms classified
    under this category focus on establishing a relationship between the input and
    output attributes and uses this relationship speculatively to generate an output
    for new input data points. In the preceding section, the example defined for the
    classification problem is also an example of supervised learning. Labeled data
    helps build reliable models and are usually expensive and limited. The following
    diagram depicts the workflow for supervised learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reinforcement Learning (RL)](img/B03980_12_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'So, this is a function approximation, where given the *x*, *y* pairs, our goal
    is to find the function *f* that maps the new *x* to a proper *y*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y = f(x)*'
  prefs: []
  type: TYPE_NORMAL
- en: In some of the learning problems, we do not have any specific target in mind
    to solve for; this kind of learning is specifically called unsupervised analysis
    or learning. The goal, in this case, is to decipher structures in data as against
    build mapping between input and output attributes of data, and in fact, the output
    attributes are not defined. These learning algorithms operate on an **unlabelled**
    dataset for this reason.
  prefs: []
  type: TYPE_NORMAL
- en: '![Reinforcement Learning (RL)](img/B03980_12_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'So, given a bunch of *x*''s, the goal here is to define a function *f* that
    can give a concise description for a set of *x*''s. Hence, this is called clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '*f(x)*'
  prefs: []
  type: TYPE_NORMAL
- en: Semi-supervised learning is about using both labeled and unlabeled data to learn
    better models. It is important that there are appropriate assumptions for the
    unlabeled data as any incorrect assumptions can invalidate the model. Semi-supervised
    learning takes its motivation from the human way of learning.
  prefs: []
  type: TYPE_NORMAL
- en: The context of Reinforcement Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reinforcement Learning is about learning that is focused on maximizing the rewards
    from the result. For example, while teaching toddlers new habits, rewarding toddlers
    every time they follow instructions works very well. In fact, they figure out
    what behavior helps them earn rewards. This is exactly what Reinforcement Learning
    is it is also called as credit assessment learning.
  prefs: []
  type: TYPE_NORMAL
- en: The most important thing in Reinforcement Learning is that, the model is additionally
    responsible for making decisions for which a periodic reward is received. The
    results, in this case, unlike supervised learning, are not immediate and may require
    a sequence of steps to be executed before the final result is seen. Ideally, the
    algorithm will generate a sequence of decisions that will help achieve the highest
    reward or utility.
  prefs: []
  type: TYPE_NORMAL
- en: '![The context of Reinforcement Learning](img/B03980_12_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The goal of this learning technique is to measure the trade-offs effectively
    by exploring and exploiting the data. For example, when a person has to travel
    from a point A to point B, there will be many ways that include traveling by air,
    water, road, or on foot, and there is a significant value in considering this
    data measuring the trade-offs for each of these options. Another important aspect
    is, what would a delay in the rewards mean? Moreover, how it would affect learning?
    For example, in games like chess, any delay in reward identification may change
    or impact the result.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the representation is very similar to supervised learning, the difference
    being that the input is not *x*, *y* pairs but *x*, *z* pairs. The goal is to
    find a function *f* that identifies a *y*, given *x* and *z*. In the following
    sections, we will explore more of what the *z* is. The equation for definition
    of the goal function is as given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y = f(x)* given *z*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A formal definition of Reinforcement Learning is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | *"Reinforcement Learning is defined as a way of programming agents by
    reward and punishment without needing to specify how the task is to be achieved."*
    |   |'
  prefs: []
  type: TYPE_TB
- en: '|   | --*Kaelbling, Littman, & Moore, 96* |'
  prefs: []
  type: TYPE_TB
- en: So, overall, RL is neither a type of neural network nor is an alternative to
    neural networks, but an orthogonal approach for Machine learning with emphasis
    being on learning feedback that is used for evaluating the learner's performance
    with no standard behavioral targets against which the performance is measured,
    for example, learning to ride a bicycle.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now look at the formal or basic RL model and understand different elements
    in action. As a first step, let's understand some basic terms.
  prefs: []
  type: TYPE_NORMAL
- en: '![The context of Reinforcement Learning](img/B03980_12_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Agent**: An agent is an entity that is a learner as well as a decision maker,
    typically an intelligent program in this case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Environment**: An environment is an entity that is responsible for producing
    a new situation given an action performed by the agent. It gives rewards or feedback
    for the action. So, in short, the environment is everything other than an agent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**State**: A state is a situation that an action lands an entity in.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Action**: An action is a step executed by an agent that results in a change
    in state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Policy**: A policy is a definition of how an agent behaves at a given point
    in time. It elaborates the mapping between the states and actions and is usually
    a simple business rule or a function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reward**: A reward lays down short-term benefit of an action that helps in
    reaching the goal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Value:** There is another important element in Reinforcement Learning, and
    that is a value function. While reward function is all about the short-term or
    immediate benefit of an action, a value function is about the good in long run.
    This value is an accumulation of rewards an agent is expected to get from the
    time the world started.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples of Reinforcement Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The easiest way to understand Reinforcement Learning is to look at some of the
    practical and real-world applications of it. In this section, we will list down
    and understand some of them.
  prefs: []
  type: TYPE_NORMAL
- en: '**Chess game**: In the game of chess, a player makes a move; this move is driven
    by an informed selection of an action that comes with a set of counter moves from
    the opponent player. The next action of the player is determined by what moves
    the opponent takes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Elevator Scheduling**: Let''s take an example of a building with many floors
    and many elevators. The key optimization requirement here is to choose which elevator
    should be sent to which floor and is categorized as a control problem. The input
    here is a set of buttons pressed (inside and outside the lift) across the floors,
    locations of the elevators, and a set of floors. The reward, in this case, is
    the least waiting time of the people wanting to use the lift. Here, the system
    learns how to control the elevators again; through learning in a simulation of
    the building, the system learns to control the elevators through the estimates
    of the value of actions from the past.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Network packet routing**: This is a case of defining a routing policy for
    dynamically changing networks. Q-learning techniques (covered a little later in
    the chapter) are used to identify which adjacent node the packet should be routed
    to. In this case, each node has a queue, and one packet is dispatched at a time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mobile robot behavior**: A mobile robot needs to decide between it reaching
    the recharge point or the next trash point depending on how quickly it has been
    able to find a recharge point in the past.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluative Feedback
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the key features that differentiates Reinforcement Learning from the
    other learning types is that it uses the information to evaluate the impact of
    a particular action than instructing blindly what action needs to be taken. Evaluative
    feedback on one hand indicates how good the action taken is while instructive
    feedback indicates what the correct action is irrespective of whether the action
    is taken or not. Although these two mechanisms are different in their way, there
    are some cases where techniques are employed in conjunction. In this section,
    we will explore some evaluative feedback methods that will lay the foundation
    for the rest of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: n-Armed Bandit problem
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A formal definition of this problem with the original gambler analogy is given
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: According to Wikipedia, n-armed bandit problem is an issue where the "gambler"
    decides which machine to play, the order of play and the duration of play, he
    then plays and collects the reward that is unique for a machine with a goal to
    maximize the overall rewards.
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider a case where there are thousands of actions that can be taken.
    Each action fetches a reward, and our goal is to ensure that we take actions in
    such a way that the total of the rewards is maximized over a period. The selection
    of a particular action is called a *play*.
  prefs: []
  type: TYPE_NORMAL
- en: An example case that explains the analogy of n-Armed Bandit the problem is that
    of a doctor who needs to choose from a series of options to treat a serious ailment
    where the survival of the patient becomes the reward for the choice of action
    (in this case, the treatment). In this problem, each action is associated with
    an expected reward for a selected action; this is called *value*. If the value
    of each action was known to us, solving n-armed bandit problem is easy as we will
    choose those actions that have a maximum value. It is only possible that we have
    the estimates of the values and are not certain about the actual values.
  prefs: []
  type: TYPE_NORMAL
- en: Now let us see the difference between exploration and exploitation.
  prefs: []
  type: TYPE_NORMAL
- en: Assuming we maintain the estimates of the values, if we choose an action with
    the greatest value (this action is called a greedy action), this situation is
    called exploitation, as we are best using the current knowledge on hand. Moreover,
    all the cases where any non-greedy action is chosen, it would be more of exploring,
    and it would help improve the estimates of the non-greedy action. While exploitation
    helps maximize the expected reward, exploration helps increase the total reward
    in the longer run. The short-term rewards are lower in the case of exploration
    while there might be better long-term total reward. For every action, exploration
    or exploitation approach can be chosen, and what works is a fine balance between
    these two techniques.
  prefs: []
  type: TYPE_NORMAL
- en: So, with this, we will now look at some techniques to best estimate the values
    for actions and to choose the best-suited actions.
  prefs: []
  type: TYPE_NORMAL
- en: Action-value methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If value of an action *a* is *Q*(a)*, then the assessed value of the *t*^(th)
    play is *Q*[t]*(a) I*, the mean of the rewards given that action is chosen, the
    following equation represents this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Q*[t]*(a) = (r1+r2+ … r*[ka] *) / ka*, where *r* is the reward and *ka* is
    the number of times the action *a* is chosen. This is one way of estimating action
    value and not necessarily the best way. Let''s live with this and now look at
    the methods to choose actions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest action selection rule is to select an action or one of the actions,
    *a*, that has the highest estimated action value. So, for a given play *t*, choosing
    a greedy action *a** can be shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Q[t](a*) = max[a] Q[t](a)
  prefs: []
  type: TYPE_NORMAL
- en: This method by definition exploits the current knowledge with a little focus
    on whether the action is a better option. As an alternative to this method, we
    can choose to be greedy most of the time, and once in a while, select an action
    independent of the value estimation. With a probability of ԑ, this method is called
    the ԑ**-greedy method**.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement comparison methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We have been seeing in most of the selection methods that an action that has
    the largest reward has a higher likelihood of being selected than an action with
    a lesser reward. The important question is how to qualify whether a reward is
    big or small. We will always need to have a reference number that qualifies if
    a reward has a high value or a low value. This reference value is called **reference
    reward**. A reference reward, to start with, can be an average of previously received
    rewards. Learning methods that use this idea are called comparison reinforcement
    methods. These methods are more efficient than actor-value methods and form a
    basis for an actor-critic method that we will discuss in the sections to come.
  prefs: []
  type: TYPE_NORMAL
- en: The Reinforcement Learning problem – the world grid example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will try to understand the Reinforcement Learning problem using a famous
    example: the grid world. This particular grid world is a 3X4 grid, as shown in
    the following screenshot, and is an approximation of the complexity of the world:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Reinforcement Learning problem – the world grid example](img/B03980_12_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This example assumes the world is kind of a game where you start with a state
    called start state (from the location *1,1*). Let's assume four actions can be
    taken that include moving left, right, up, and down. The goal is to ensure using
    these actions that we move towards the goal that is represented in the location
    *4,3*. We need to avoid the red box that is shown in the location *4,2* like it
    is shown in the next image.
  prefs: []
  type: TYPE_NORMAL
- en: '**Start state**: position *(1,1) -->* The world starts here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Success state**: position *(4,3) -->* The world ends here in a success state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Failure state**: position *(4,2) -->* The world ends here in a failure state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the world ends, we need to start over again.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Wall**: There is a roadblock or a wall shown in the position *(2,2)*. This
    position cannot be navigated:![The Reinforcement Learning problem – the world
    grid example](img/B03980_12_08.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To reach the goal *(4,3)* from the start point *(1,1)*, steps can be taken in
    the following directions:![The Reinforcement Learning problem – the world grid
    example](img/B03980_12_09.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every step in a direction moves you from one position to another (position here
    is nothing but the state). For example, a movement in the *UP* direction from
    the position *(1,1)* will take you to the position *(1,2)* and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All directions cannot be taken from a given position. Let us consider the example
    shown in the following screenshot. From the position *(3,2)*, only *UP*, *DOWN*,
    and *RIGHT* can be taken. *LEFT* movement will hit the wall and hence cannot be
    taken. That said, only *UP* and *DOWN* movements make sense, as *RIGHT* will make
    a move to the danger position that results in failure in reaching the goal.![The
    Reinforcement Learning problem – the world grid example](img/B03980_12_10.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, any of the positions in the boundaries of the grid will have limitations,
    for example, the position *(1,3)* allows *RIGHT* and *DOWN* movements and any
    other movements do not alter the position.![The Reinforcement Learning problem
    – the world grid example](img/B03980_12_11.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s now look at the shortest path the from *Start (1,1)* to the *Goal (4,3)*.
    There are two solutions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Solution 1**: *RIGHT* --> *RIGHT* --> *UP* --> *UP* --> *RIGHT* (5 steps)![The
    Reinforcement Learning problem – the world grid example](img/B03980_12_12.jpg)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Solution 2**: *UP* --> *UP* --> *RIGHT* --> *RIGHT* --> *RIGHT* (5 steps)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![The Reinforcement Learning problem – the world grid example](img/B03980_12_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'In the real world, not all actions get executed as expected. There is a reliability
    factor that affects the performance, or rather, there is uncertainty. If we add
    a small caveat to the example and say that every time there is an action to move
    from one position to another, the probability that the movement is correct is
    0.8\. This means there is an 80% possibility that a movement executes as expected.
    In this case, if we want to measure the probability of Solution 1, (*R*-->*R*-->*U*-->*U*-->*R*)
    is succeeding:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Probability of actions happening as expected + Probability of actions not happening
    as expected
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*= 0.8 x 0.8 x 0.8 x 0.8 x 0.8 + 0.1 x 0.1 x 0.1 x 0.1 x 0.8*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*= 0.32768 + 0.00008 = 0.32776*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As we see, the element of uncertainty does change the result. In the next section,
    we will discuss the decision process framework that captures these uncertainties.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Markov Decision Process (MDP)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Markov's Decision Process is an essential framework or process to make decisions,
    and we will be bringing it up in most of the sections that follow on Reinforcement
    Learning.
  prefs: []
  type: TYPE_NORMAL
- en: '**Markov property** is core to the Markov Decision Process, and it states that
    what matters is the present or current state and that the situation is stationary,
    which means that the rules do not change.'
  prefs: []
  type: TYPE_NORMAL
- en: 'MDP tries to capture the world that we discussed in the preceding section that
    has the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Markov Decision Process (MDP)](img/B03980_12_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**States**: In the preceding example, every grid position denotes a state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model** (Transition function): A Transition function includes three attributes:
    given state, action, and the destination state. It describes the probability of
    an end state *s*, given the current state *s* and action *a*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: T (s, a,s') ~ P(s'/s, a)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Actions**: A(s), A In the preceding example, *A (1, 2)* = *UP* in the *UP*,
    *UP* *RIGHT*, *RIGHT*, *RIGHT* solution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rewards**: *R(s)*, *R(s,a)*, *R(s,a,s1)* Rewards tell us the usefulness of
    entering into a state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*R(s)*: Reward for entering a state *s*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*R(s, a)*: Reward for the opening of a state *s* for an action *a*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*R(s, a,s1)*: Reward for the opening of a state *s1* for an action *a* given
    that you were in state *s*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: State, Action, Model and Rewards make the problem statement of MDP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Policy**: It is a solution for a problem; it says what action should be taken
    given a state:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*π(s)* --> *a*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Basic RL model – agent-environment interface
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we have discovered, an RL problem is a straightforward way of learning from
    interaction to achieve a goal. Agent is the learner or decision-maker, and it
    interacts with the environment, and that is everything outside in this environment
    gives rise to rewards. The thing it interacts with, comprising everything outside
    the agent, is called the environment. The complete specification of an environment
    is defined as a task—a single instance of Reinforcement Learning problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following model depicts the agent-environment interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Basic RL model – agent-environment interface](img/B03980_12_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: An environment model here means a context where an agent uses anything to predict
    the behavior of the environment for a given action. This environment model produces
    a prediction of the next state and the reward, given the action and the current
    state. There will be multiple next state rewards possible in case the model is
    stochastic. Again, these models can be distributed or sample-based. The distributed
    models identify all the potential probabilities, while a sample model produces
    the probability given that sample.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the goals of Reinforcement Learning can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In an environment where every action taken results in a new situation, RL is
    about how to take actions. The following can be the actions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define a policy that maps the action and the resultant situation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify the policy that results in highest rewards being given
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Steps in Reinforcement Learning are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The agent observes the input state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By applying the policy that is a decision-making function, an action is identified.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The action is executed that results in a state change.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As a consequence of this action, the agent receives a significant reward from
    the environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The details of the reward, given the change in the state, are recorded.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Delayed rewards
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the aspects that differentiate Reinforcement Learning from supervised
    learning is *rewards*. In this section, we will explore what delayed rewards mean.
    As we know, every action that results in a particular state change results in
    a reward. The realization of this reward in some cases is not immediate. Let's
    look at an example a chess game. Let's assume it took 65 steps to end a chess
    game and only at the end of 65 steps or moves we get to know if we have won the
    game or lost it. Which of the 65 steps or moves were the cause of the success
    or failure is what is complex here. So, the reward is not known until the end
    of the game or the sequence of actions. Technically, we are looking at identifying
    which sequence of actions resulted in gaining the reward that was seen. This process
    is called **Temporal Credit Assignment**.
  prefs: []
  type: TYPE_NORMAL
- en: Now, in this journey of achieving the ultimate reward that is a success (+1)
    or failure (-1), every step or move or action would fetch a reward. Let's assume
    every step in solution 1 of the grid world problem fetches a reward of -0.4\.
    The collective rewards that take to success or failure will determine long-term
    reward.
  prefs: []
  type: TYPE_NORMAL
- en: The policy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An optimal policy is a policy or solution that maximizes the expected long-term
    reward and can be represented by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The policy](img/B03980_12_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s measure the utility of a particular state(*s*) that depends on
    the policy (*π*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![The policy](img/B03980_12_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A reward to enter a state(*s*) (this is an immediate benefit) is not equal to
    the utility of that state (this is a long-term benefit of entering the state).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can define the optimal policy using the utility of the state value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The policy](img/B03980_12_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, if we have to define the utility of being in a state(*s*), it is equal
    to the reward for getting into that state, discounting the reward that we get
    from that point on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The policy](img/B03980_12_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is called **Bellman Equation**.
  prefs: []
  type: TYPE_NORMAL
- en: '*V** is a value function for a policy, and the following is the Bellman optimality
    equation that expresses the fact that the value of a state with an optimal policy
    is the same as the best expected return from the best action for that state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The policy](img/B03980_12_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Reinforcement Learning – key features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reinforcement Learning is not a set of techniques but is a set of problems that
    focuses on what the task is as, against how the task should be addressed.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement Learning is considered as a tool for machines to learn using the
    rewards and punishments that are more trial-and-error driven.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement Learning employs evaluative feedback. Evaluative feedback measures
    how effective the action taken is as against measuring the action if it is best
    or worst. (Note that supervised learning is more of an instructive learning and
    determines the correctness of an action irrespective of the action being executed.)
  prefs: []
  type: TYPE_NORMAL
- en: The tasks in Reinforcement Learning are more of related tasks. Associative tasks
    are dependent on the situation where actions that suit best to the given situation
    are identified and executed. Non-associative tasks are those that are independent
    of the particular situation, and the learner finds the best action when the task
    is stationary.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning solution methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss in detail some of the methods to solve Reinforcement
    Learning problems. Specifically, dynamic programming (DP), Monte Carlo method,
    and temporal-difference (TD) learning. These methods address the problem of delayed
    rewards as well.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic Programming (DP)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'DP is a set of algorithms that are used to compute optimal policies given a
    model of environment like Markov Decision Process. Dynamic programming models
    are both computationally expensive and assume perfect models; hence, they have
    low adoption or utility. Conceptually, DP is a basis for many algorithms or methods
    used in the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Evaluating the policy**: A policy can be assessed by computing the value
    function of the policy in an iterative manner. Computing value function for a
    policy helps find better policies.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Improving the policy**: Policy improvement is a process of computing the
    revised policy using its value function information.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Value iteration and Policy Iteration**: Policy evaluation and improvement
    together derive value and policy iteration. These are two of the most popular
    DP methods that are used to compute the optimal policies and value functions given
    complete knowledge of the MDPs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following algorithm depicts the iteration policy process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dynamic Programming (DP)](img/B03980_12_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Value iteration combines a solid policy improvement and process evaluation.
    The following are the steps involved:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dynamic Programming (DP)](img/B03980_12_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Generalized Policy Iteration (GPI)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GPI is a way of categorizing Dynamic Programming (DP) methods. GPI involves
    interaction between two processes—one around the approximate policy, and the other
    around the approximate value.
  prefs: []
  type: TYPE_NORMAL
- en: In the first case, the process picks the policy as it is and performs policy
    evaluation to identify the true or exact value function associated with the policy.
    The other process picks the value function as the input and uses that to change
    the policy such that it improves the policy, which is its total reward. If you
    observe, each process changes the basis for the other process, and they work in
    conjunction to find a joint solution that results in an optimal policy and value
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Monte Carlo methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Monte Carlo methods in Reinforcement Learning learn policies and values from
    experience as samples. Monte Carlo methods have additional advantages over Dynamic
    Programming methods because of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Learning optimal behavior happens directly from the interactions with the environment
    without any model that simulates model dynamics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These methods can be used on simulated data or sample models; this feature becomes
    paramount in real-world applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With Monte Carlo methods, we can easily focus on smaller sets of states, and
    we can explore a region of interest without necessarily going into complete state
    set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monte Carlo methods are least impacted for any violation of Markov's property
    because the estimation for the value is not updated using any of the successor
    states. This also means that they do not bootstrap.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monte Carlo methods are designed by the **Generalized Policy Iteration** (**GPI**)
    method. These methods provide an alternative way of evaluating the policies. For
    each state, instead of independently computing the value, an average value of
    the returns for starting at that state is taken, and this can be a good approximation
    of the value of that state. The focus is to apply action-value functions to improve
    the policies as this does not require environment's transition changes. Monte
    Carlo methods mix policy evaluation and improvement methods and can be implemented
    on a step-by-step basis.
  prefs: []
  type: TYPE_NORMAL
- en: How much of exploration is good enough? This is a crucial question to answer
    in Monte Carlo methods. It is not sufficient to select actions that are best based
    on their value; it is also important to know how much of this is contributing
    to ending reward.
  prefs: []
  type: TYPE_NORMAL
- en: Two methods can be used in this case—**on-policy** or **off-policy** methods.
    In an on-policy method, the agent is responsible for finding the optimal policy
    using exploration technique; and in off-policy methods, agent exploration is not
    central, but along with it learns a deterministic optimal policy that need not
    be related to the policy followed. In short, off-policy learning methods are all
    about learning behavior through behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Temporal difference (TD) learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TD learning is one of the unique techniques of Reinforcement Learning. Temporal
    difference learning is a combination of Monte Carlo methods and dynamic programming
    methods. The most discussed technique in Reinforcement Learning is the relationship
    between temporal difference (TD), dynamic programming (DP), and Monte Carlo methods:'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate a policy that includes estimating the value function *V*π for a given
    policy *π*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select an optimal policy. For policy selection, all the DP, TD, and Monte Carlo
    methods use a variation of generalized policy iteration (GPI). Hence, the difference
    in these three methods is nothing but these variations in GPI.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: TD methods follow bootstrapping technique to derive an estimate; they fall back
    on the successor states and similar estimates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now look at some advantages of TD over DP and Monte Carlo methods. We
    will cover this in brief and without delving into too much of complexities. Following
    are the key benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: TD methods do not require the model of the environment and probability distributions
    of the next states and rewards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TD methods can easily and elegantly run in an online and incremental manner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sarsa - on-Policy TD
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s look at using TD methods for the control problems. We will continue
    to use GPI techniques, but now in conjunction with TD methods for the evaluation
    and prediction. While we need to have a balance between exploration and exploitation
    options, we have the option to choose on-policy or off-policy learning methods
    in here. We will stick to the on-policy method:'
  prefs: []
  type: TYPE_NORMAL
- en: Learn the action-value function in relation to the state-value function. We
    will define *Q*^π*(s, a)* for the policy *π*:![Sarsa - on-Policy TD](img/B03980_12_19.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Learn the value of transition from one state-action pair to another state-action
    pair. This is computed iteratively as follows:![Sarsa - on-Policy TD](img/B03980_12_20.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is defined as a Sarsa prediction method and is on-policy as the agent uses
    the identified policy to do this.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Sarsa algorithm is stated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sarsa - on-Policy TD](img/B03980_12_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Q-Learning – off-Policy TD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Q-Learning technique that employs the off-policy learning method is one
    of the groundbreaking strategies of TD. This control algorithm called Q-learning
    (Watkins, 1989) in a simple form is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Q-Learning – off-Policy TD](img/B03980_12_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can see the optimal action-value function *Q** is directly approximated using
    the learned action-value function *Q* irrespective of the policy that it follows.
    This makes it an off-policy method.
  prefs: []
  type: TYPE_NORMAL
- en: There is still a small effect on the policy seen as the policy value functions
    are used and updated. Moreover, updates to all the pairs diligently mark convergence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on this understanding, the Q-learning algorithm can be depicted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Q-Learning – off-Policy TD](img/B03980_12_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Actor-critic methods (on-policy)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Actor-critic methods are temporal difference learning methods that ensure policy
    and value independence using a separate memory structure. In this case, the policy
    structure is called as an *actor,* and the value structure is called as a *critic*.
    The name critic comes from the fact that it criticizes the value of the policy.
    Since this critic always criticizes the value of the policy, it is also called
    the TD error. The following screenshot shows the actor-critic method flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Actor-critic methods (on-policy)](img/B03980_12_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: R Learning (Off-policy)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'R-learning is an advanced Reinforcement Learning technique that is used in
    cases where there are no discounts with definitive and finite returns. The algorithm
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![R Learning (Off-policy)](img/B03980_12_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Implementing Reinforcement Learning algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the source code provided for this chapter to implement Reinforcement
    learning algorithms. (Source code path `.../chapter12/...` under each of the folder
    for the technology.)
  prefs: []
  type: TYPE_NORMAL
- en: Using Mahout
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Refer to the folder `.../mahout/chapter12/rlexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using R
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Refer to the folder `.../r/chapter12/rlexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using Spark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Refer to the folder `.../spark/chapter12/rlexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using Python (Scikit-learn)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Refer to the folder `.../python-scikit-learn/chapter12/rlexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using Julia
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Refer to the folder `.../julia/chapter12/rlexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored a new learning technique called Reinforcement Learning.
    We saw how this was different from traditional supervised and unsupervised learning
    techniques. The goal of Reinforcement Learning is decision making and at the heart
    of it is MDP. We explored the elements of MDP and learned about it using an example.
    We then covered some fundamental Reinforcement Learning techniques that are on-policy
    and off-policy, and some of them are indirect and direct methods of learning.
    We covered dynamic programming (DP) methods, Monte Carlo methods, and some key
    temporal difference (TD) methods like Q-learning, Sarsa, R-learning, and actor-critic
    methods. Finally, we had hands-on implementations for some of these algorithms
    using our standard technology stack identified for this book. In the next chapter,
    we will cover ensemble learning methods.
  prefs: []
  type: TYPE_NORMAL
