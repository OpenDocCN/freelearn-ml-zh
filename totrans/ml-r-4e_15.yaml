- en: '15'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '15'
- en: Making Use of Big Data
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用大数据
- en: Although today’s most exciting machine learning research is found in the realm
    of big data—computer vision, natural language processing, autonomous vehicles,
    and so on—most business applications are much smaller scale, using what might
    be termed, at best, “*medium*” data. As noted in *Chapter 12*, *Advanced Data
    Preparation*, true big data work requires access to datasets and computing facilities
    generally found only at very large tech companies or research universities. Even
    then, the actual job of using these resources is often primarily a feat of data
    engineering, which simplifies the data greatly before its use in conventional
    business applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管今天最令人激动的机器学习研究是在大数据领域——计算机视觉、自然语言处理、自动驾驶汽车等——但大多数商业应用规模较小，使用的数据最多只能称为“*中等*”数据。正如*第12章*，*高级数据准备*中所述，真正的大数据工作需要访问数据集和计算设施，这些设施通常只有在非常大的科技公司或研究型大学才能找到。即便如此，使用这些资源的实际工作通常主要是数据工程的壮举，它在数据用于传统商业应用之前极大地简化了数据。
- en: 'The good news is that the headline-making research conducted at big data companies
    eventually trickles down and can be applied in simpler forms to more traditional
    machine learning tasks. This chapter covers a variety of approaches for making
    use of such big data methods in R. You will learn:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，大数据公司进行的引人注目的研究最终会逐渐渗透到更传统的机器学习任务中。本章涵盖了在R中使用这些大数据方法的多种方法。您将学习：
- en: How to borrow from the deep learning models developed at big data companies
    and apply them to conventional modeling tasks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何借鉴大数据公司开发的深度学习模型并将其应用于传统的建模任务
- en: Strategies for reducing the complexity of large and unstructured big data formats
    like text and images so that they can be used for prediction
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少大型和非结构化大数据格式（如文本和图像）的复杂性的策略，以便它们可用于预测
- en: Cutting-edge packages and approaches for accessing and modeling big datasets
    that may be too large to fit into memory
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问和建模大数据集的尖端包和途径，这些数据集可能太大而无法装入内存
- en: Despite R’s reputation for being ill equipped for big data projects, the efforts
    of the R community have gradually transformed it into a tool capable of tackling
    a surprising number of advanced tasks. The goal of this chapter is to demonstrate
    R’s ability to remain relevant in the era of deep learning and big data. Even
    though R is unlikely to be found at the heart of the biggest big data projects,
    and despite facing increasing competition from Python and cloud-based tools, R’s
    strengths keep it on the desktops of many practicing data scientists.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管R以其不适合大数据项目而闻名，但R社区的努力逐渐将其转变为一种能够处理大量高级任务的工具。本章的目标是展示R在深度学习和大数据时代保持相关性的能力。尽管R不太可能成为最大大数据项目的核心，并且尽管面临来自Python和基于云的工具的日益激烈的竞争，R的优势仍然使其保持在许多实践数据科学家的桌面上。
- en: Practical applications of deep learning
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习的实际应用
- en: Deep learning has received a great deal of attention lately due to its successes
    in tackling machine learning tasks that have been notoriously difficult to solve
    with conventional methods. Using sophisticated neural networks to teach computers
    to think more like a human has allowed machines to catch up with or even surpass
    human performance on many tasks that humans once held a seemingly insurmountable
    lead. Perhaps more importantly, even if humans still perform better at certain
    tasks, the upsides of machine learning—workers that never tire, never get bored,
    and require no salary—turn even imperfect automatons into useful tools for many
    tasks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习最近因其成功解决传统方法难以解决的机器学习任务而受到广泛关注。使用复杂的神经网络教会计算机更像人类思考，使得机器在许多人类曾经占据压倒性优势的任务上能够赶上甚至超越人类的表现。也许更重要的是，即使人类在某些任务上仍然表现更好，机器学习的优势——工人从不疲倦、从不厌倦，且无需工资——甚至将不完美的自动机变成了许多任务的实用工具。
- en: Unfortunately, for those of us working outside of large technology companies
    and research organizations, it is not always easy to take advantage of deep learning
    methods. Training a deep learning model generally requires not only state-of-the-art
    computing hardware but also large volumes of training data. In fact, as mentioned
    in *Chapter 12*, *Advanced Data Preparation*, most practical machine learning
    applications in the business setting are in the so-called small or medium data
    regimes, and here deep learning methods might perform no better and possibly even
    worse than conventional machine learning approaches like regression and decision
    trees. Thus, many organizations that are investing heavily in deep learning are
    doing so as a result of hype rather than true need.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，对于我们这些在大科技公司和研究机构外工作的人来说，利用深度学习方法并不总是那么容易。训练深度学习模型通常不仅需要最先进的计算硬件，还需要大量的训练数据。事实上，正如在第
    12 章 *《高级数据准备》* 中提到的，在商业环境中，大多数实际机器学习应用都是在所谓的少量或中等数据规模下进行的，而在这里，深度学习方法可能并不比传统的机器学习方法（如回归和决策树）表现得更好，甚至可能更差。因此，许多在深度学习上投入大量资金的组织，这样做更多的是因为炒作而非真正的需求。
- en: Even though some of the buzz around deep learning is surely based on its novelty
    as well as excitement from business leaders with visions of artificial intelligence
    replacing costly human workers, there are in fact practical applications of the
    technique that can be used in combination with, rather than as a replacement for,
    conventional machine learning methods. The purpose of this chapter is therefore
    not to provide a start-to-finish tutorial for building deep neural networks, but
    rather to show how deep learning’s successes can be incorporated into conventional
    machine learning projects including those outside the big data regime.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管围绕深度学习的一些炒作无疑是基于其新颖性以及商业领袖对人工智能取代昂贵的人工工人的愿景所带来的兴奋，但实际上，这项技术有一些实际应用，可以与传统的机器学习方法结合使用，而不是作为替代品。因此，本章的目的不是提供一个从头到尾的构建深度神经网络的教程，而是展示如何将深度学习的成功融入传统的机器学习项目中，包括那些在大数据环境之外的项目。
- en: 'Packt Publishing offers numerous resources on deep learning, such as *Hands-On
    Deep Learning with R: A practical guide to designing, building, and improving
    neural network models using R* (2020) by M. Pawlus and R. Devine, *Advanced Deep
    Learning with R* (2019) by B. Rai, and *Deep Learning with R Cookbook* (2020)
    by S. Gupta, R. A. Ansari, and D. Sarkar.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Packt Publishing 提供了关于深度学习的众多资源，例如 M. Pawlus 和 R. Devine 所著的 *《使用 R 进行深度学习实战：设计、构建和改进神经网络模型的实际指南》（2020
    年）*，B. Rai 所著的 *《使用 R 进行高级深度学习》（2019 年）*，以及 S. Gupta、R. A. Ansari 和 D. Sarkar
    所著的 *《深度学习 R 烹饪书》（2020 年）*。
- en: Beginning with deep learning
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从深度学习开始
- en: In recent years, with a new cohort of data science practitioners having been
    trained in the age of deep learning, a form of “generation gap” has developed
    in the machine learning community. Prior to the development of deep learning,
    the field was staffed primarily by those who were trained in statistics or computer
    science. Especially in the earliest years, machine learning practitioners carried
    with them the metaphorical baggage of their prior domain, and the software and
    algorithms they used fell into camps along party lines. Statisticians typically
    preferred regression-based techniques and software like R, whereas computer scientists
    favored iterative and heuristic-based algorithms like decision trees written in
    languages like Python and Java. Deep learning has blurred the line between these
    camps, and the next generation of data scientists may seem somewhat foreign to
    the prior generations as if they speak a different language.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，随着新一代在深度学习时代受过训练的数据科学实践者的出现，机器学习社区中形成了一种“代沟”。在深度学习发展之前，该领域主要由受过统计学或计算机科学培训的人士组成。特别是在最初几年，机器学习从业者带着他们先前领域的隐喻性负担，他们所使用的软件和算法沿着党派路线分成了阵营。统计学家通常更喜欢基于回归的技术和软件，如
    R，而计算机科学家则更喜欢用 Python 和 Java 等语言编写的迭代和启发式算法，如决策树。深度学习模糊了这些阵营之间的界限，下一代数据科学家可能对前辈们显得有些陌生，就像他们说的是另一种语言。
- en: 'The rift seems to have come out of nowhere, despite being able to see its origins
    clearly with the benefit of hindsight. As the famous author Ernest Hemingway once
    wrote, it happened “gradually, then suddenly.” Just as machine learning itself
    was only possible as the result of the simultaneous evolution of computing power,
    statistical methods, and available data, it makes sense that the next big evolutionary
    leap would arise out of a series of smaller evolutions in each of the same three
    components. Recalling the similar image presented in *Chapter 1*, *Introducing
    Machine Learning*, a revised cycle of advancement diagram depicting today’s state-of-the-art
    machine learning cycle illustrates the environment in which deep learning was
    developed:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然事后我们可以清楚地看到其起源，但这次分歧似乎是从无中来，正如著名作家欧内斯特·海明威曾经写的那样，“它逐渐发生，然后突然发生。” 正如机器学习本身只有在计算能力、统计方法和可用数据的同步演变下才成为可能一样，下一个大的进化飞跃从这三个相同组件的一系列较小进化中产生，这是有道理的。回忆一下*第一章*中提出的类似图像，*介绍机器学习*，一个修订后的进步周期图描绘了当今最先进的机器学习周期，它说明了深度学习发展的环境：
- en: '![](img/B17290_15_01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_15_01.png)'
- en: 'Figure 15.1: A combination of factors in the cycle of advancement led to the
    development of deep learning'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.1：进步周期中的多种因素共同导致了深度学习的发展
- en: It is no surprise that deep learning arose out of the big data era, while also
    being provided the requisite computing hardware—**graphics processing units**
    (**GPUs**) and cloud-based parallel processing tools, which will be covered later
    in this chapter—necessary to process datasets that are both very long and very
    wide. What is less obvious, and therefore easy to take for granted, is the academic
    and research environment that was also necessary for this evolution. Without a
    strong data science community comprising researchers whose expertise spans both
    statistics and computer science, in addition to applied data scientists motivated
    to solve practical business problems on large and complex real-world datasets,
    it is unlikely that deep learning would have arisen as it has. Stated differently,
    the fact that data science now exists as a focused academic discipline has undoubtedly
    accelerated the cycle of advancement. To borrow an analogy from science fiction,
    the system is much like a robot that becomes self-aware and learns much more quickly
    now that it has learned how to learn!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习起源于大数据时代，同时得到了必要的计算硬件——**图形处理单元**（**GPUs**）和基于云的并行处理工具——这些将在本章后面进行介绍，用于处理既长又宽的数据集，这并不令人惊讶。但更不明显的是，这种演变也离不开必要的学术和研究环境。如果没有一个强大的数据科学社区，其中研究人员在统计学和计算机科学方面都有专业知识，再加上那些致力于解决大型和复杂真实世界数据集上实际业务问题的应用数据科学家，深度学习可能就不会以这种方式出现。换句话说，数据科学现在作为一个专门的学术学科存在，无疑加速了进步周期。借用科幻小说中的类比，系统现在就像一个机器人，它学会了如何学习，因此现在它变得自我意识，并且学习得更快！
- en: The rapid development of deep learning has contributed to the previously mentioned
    generation gap, but it is not the only factor. As you will soon learn, deep learning
    not only offers impressive performance on big data tasks, but it can also perform
    much like conventional learning methods on smaller tasks. This has led some to
    focus almost exclusively on the technique, much like earlier generations of machine
    learning practitioners focused exclusively on regression or decision trees. The
    fact that deep learning also utilizes specialized software tools and mathematical
    terminology to perform these tasks means that its practitioners are, in some cases,
    literally speaking another language to describe the same series of steps. As has
    been said many times before, “there is no free lunch” in the field of machine
    learning, so as you continue your machine learning journey, it is best to see
    it as one of many useful tools—and not the *only* tool for the job.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的快速发展导致了之前提到的代沟，但这并不是唯一因素。正如你很快就会学到的那样，深度学习不仅在大型数据任务上提供了令人印象深刻的性能，而且在较小任务上也能像传统学习方法一样执行。这导致一些人几乎完全专注于这项技术，就像早期机器学习实践者只专注于回归或决策树一样。深度学习还利用专门的软件工具和数学术语来执行这些任务，这意味着在某些情况下，其从业者实际上是在用另一种语言描述相同的步骤。正如以前多次说过的，“在机器学习领域没有免费的午餐”，因此在你继续机器学习之旅时，最好将其视为许多有用工具之一——而不是这项工作的*唯一*工具。
- en: The terminology used by deep learning practitioners, even for simpler methods
    like linear regression, includes phrases like “cost function,” “gradient descent,”
    and “optimization.” This is a good reminder that although deep learning can approximate
    regression and other machine learning methods, the means by which it finds the
    solution is completely different.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习实践者使用的术语，即使是像线性回归这样的简单方法，也包括“代价函数”、“梯度下降”和“优化”等短语。这是一个很好的提醒，尽管深度学习可以近似回归和其他机器学习方法，但它找到解决方案的方式是完全不同的。
- en: Choosing appropriate tasks for deep learning
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择适合深度学习的任务
- en: As mentioned in *Chapter 7*, *Black-Box Methods – Neural Networks and Support
    Vector Machines*, neural networks with at least one hidden layer can act as universal
    function approximators. Elaborating on this principle, one might say that given
    enough training data, a cleverly designed neural network can learn to mimic the
    output of any other function.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如同在*第七章*中提到的，*黑盒方法 – 神经网络和支持向量机*，至少包含一个隐藏层的神经网络可以作为通用函数逼近器。对此原理进行阐述，我们可能会说，在给定足够多的训练数据的情况下，一个设计巧妙的神经网络可以学会模仿任何其他函数的输出。
- en: This implies that the conventional learning methods covered throughout this
    book can likewise be approximated by well-designed neural networks. In fact, it
    is quite trivial to design a neural network that almost exactly matches linear
    or logistic regression, and with a bit more work it is possible to approximate
    techniques like k-nearest neighbors and naive Bayes. Given enough data, a neural
    network can get closer and closer to the performance of even the best tree-based
    algorithms like random forests or gradient boosting machines.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着本书中涵盖的传统学习方法同样可以被设计良好的神经网络所近似。事实上，设计一个几乎完全匹配线性或逻辑回归的神经网络是非常简单的，通过更多的工作，也有可能近似k-最近邻和朴素贝叶斯等技术。在数据足够的情况下，神经网络可以越来越接近甚至最好的基于树的算法，如随机森林或梯度提升机。
- en: Why not apply deep learning to every problem, then? Indeed, a neural network’s
    ability to mimic all other learning approaches appears to be a violation of the
    “no free lunch” theorem, which, put simply, suggests that there is no machine
    learning algorithm that can perform best across all potential modeling tasks.
    There are a couple of key reasons why the theorem remains safe despite deep learning’s
    magic. First, the ability of a neural network to approximate a function is related
    to how much training data it has. In the small data regime, conventional techniques
    can perform better, especially when combined with careful feature engineering.
    Second, to reduce the amount of data the neural network needs for training, the
    network must have a topology that facilitates its ability to learn the underlying
    function. Of course, if the person building the model knows what topology to use,
    then it may be preferable to use the simpler conventional model in the first place.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 那为什么不将深度学习应用于每一个问题呢？确实，神经网络模仿所有其他学习方法的特性似乎违反了“没有免费午餐”定理，这个定理简单来说就是指出没有一种机器学习算法可以在所有潜在的建模任务中表现最佳。有几个关键原因使得这个定理在深度学习的魔力下依然安全。首先，神经网络逼近函数的能力与其拥有的训练数据量相关。在小数据规模下，传统技术可以表现得更好，尤其是在与仔细的特征工程相结合时。其次，为了减少神经网络训练所需的数据量，网络必须具有便于学习潜在函数的拓扑结构。当然，如果构建模型的人知道应该使用哪种拓扑结构，那么最初使用更简单的传统模型可能更可取。
- en: People using deep learning for conventional learning tasks are likely to prefer
    the black box approach, which works in the big data regime. Big data, however,
    is not merely the presence of many rows of data, but also many features. Most
    conventional learning tasks, including those with many millions of rows of data,
    are in the medium data regime, where conventional learning algorithms still perform
    well. In this case, whether the neural network performs better will ultimately
    come down to the balance of overfitting and underfitting—a balance that is sometimes
    challenging to find with a neural network, as the method tends to somewhat easily
    overfit the training data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用深度学习进行传统学习任务的人们可能会倾向于选择黑盒方法，这种方法在大数据环境中效果显著。然而，大数据并不仅仅是数据行数的增加，还包括许多特征。大多数传统学习任务，包括那些拥有数百万行数据的情况，都处于中等数据规模，在这种规模下，传统学习算法仍然表现良好。在这种情况下，神经网络是否表现更好最终将取决于过拟合和欠拟合之间的平衡——这种平衡有时在神经网络中是难以找到的，因为该方法容易对训练数据进行过拟合。
- en: Perhaps for this reason, deep learning is not well suited for racking up wins
    in machine learning competitions. If you ask a Kaggle Grandmaster, they are likely
    to tell you that neural networks don’t work on standard, real-life problems, and
    on traditional supervised learning tasks, gradient boosting wins. One can also
    see proof of this by browsing the leaderboards and noting the absence of deep
    learning. Perhaps a clever team will use neural networks for feature engineering
    and blend the deep learning model with other models in an ensemble to boost their
    performance, but even this is rare. Deep learning’s strengths are elsewhere. As
    a rule of thumb, tree-based ensemble methods win on structured, tabular data,
    while neural networks win on unstructured data, like image, sound, and text.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 可能正因为如此，深度学习不适合在机器学习竞赛中取得胜利。如果你问一个Kaggle大师，他们可能会告诉你神经网络在标准、现实生活中的问题以及传统监督学习任务上不起作用，梯度提升法才是赢家。人们也可以通过浏览排行榜并注意深度学习的缺席来证明这一点。也许一个聪明的团队会使用神经网络进行特征工程，并将深度学习模型与其他模型结合成集成模型以提高性能，但这种做法很少见。深度学习的优势在于其他方面。一般来说，基于树的集成方法在结构化、表格数据上获胜，而神经网络在图像、声音和文本等非结构化数据上获胜。
- en: 'Reading recent news about research breakthroughs and technology startup companies,
    one is likely to encounter deep learning applications that utilize the method’s
    unique ability to solve unconventional tasks. In general, these unconventional
    learning tasks fall into one of three categories: computer vision, natural language
    processing, or tasks involving unusual data formats like repeated measurements
    over time or having an exceptionally large number of interrelated predictors.
    A selection of specific successes for each category is listed in the following
    table:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读关于研究突破和技术初创公司的最新新闻，可能会遇到利用该方法独特能力解决非常规任务的深度学习应用。一般来说，这些非常规学习任务可以分为三类：计算机视觉、自然语言处理或涉及时间重复测量或具有异常大量相互关联预测因子的不寻常数据格式。以下表格列出了每个类别的具体成功案例：
- en: '| **Challenging machine learning tasks** | **Deep learning successes** |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| **具有挑战性的机器学习任务** | **深度学习成功案例** |'
- en: '| Computer vision tasks involving classifying images found in still pictures
    or video data |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 涉及对静止图片或视频数据中图像进行分类的计算机视觉任务 |'
- en: Identifying faces in security camera footage
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在安全摄像头录像中识别人脸
- en: Categorizing plants or animals for ecological monitoring
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对植物或动物进行分类以进行生态监测
- en: Diagnosing medical images such as X-rays, MRI, or CT scans
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 诊断X射线、MRI或CT扫描等医疗图像
- en: Measuring the activity of athletes on the sporting field
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量运动员在运动场上的活动
- en: Autonomous driving
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动驾驶
- en: '|'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Natural language applications requiring an understanding of the meaning of
    words in context |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 需要理解上下文中词语含义的自然语言应用 |'
- en: Processing social media posts to filter out fake news or hate speech
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理社交媒体帖子以过滤掉假新闻或仇恨言论
- en: Monitoring Twitter or customer support emails for consumer sentiment or other
    marketing insights
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监测Twitter或客户支持电子邮件以了解消费者情绪或其他市场洞察
- en: Examining electronic health records for patients at risk of adverse outcomes
    or for eligibility for new treatments
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查处于不良结果风险中的患者或符合新治疗方案资格的患者的电子健康记录
- en: '|'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '|  |  |'
- en: '| Predictive analysis involving many repeated measurements or very large numbers
    of predictors |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 涉及许多重复测量或大量预测因子的预测分析 |'
- en: Predicting the price of goods or equities in open markets
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测公开市场上商品或股票的价格
- en: Estimating energy, resource, or healthcare utilization
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估算能源、资源或医疗保健利用
- en: Forecasting survival or other medical outcomes using insurance billing codes
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用保险账单代码预测生存或其他医疗结果
- en: '|'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Even though some people certainly do use deep learning for conventional learning
    problems, this chapter focuses only on tasks that cannot be solved via conventional
    modeling techniques. Deep learning is highly adept at tapping into the unstructured
    data types that characterize the big data era, such as images and text, which
    are extremely difficult to model with traditional approaches.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有些人确实使用深度学习来解决传统学习问题，但本章仅关注无法通过传统建模技术解决的问题。深度学习非常擅长挖掘大数据时代特征的数据类型，如图像和文本，这些数据类型用传统方法难以建模。
- en: Unlocking these capabilities requires the use of specialized software and specialized
    data structures, which you will learn about in the next section.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 要解锁这些功能，需要使用专门的软件和专门的数据结构，这些内容你将在下一节中学习。
- en: The TensorFlow and Keras deep learning frameworks
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TensorFlow 和 Keras 深度学习框架
- en: Perhaps no software tool has contributed as much to the rapid growth in deep
    learning as **TensorFlow** ([https://www.tensorflow.org](https://www.tensorflow.org)),
    an open-source mathematical library developed at Google for advanced machine learning.
    TensorFlow provides a computing interface using directed graphs that “flow” data
    structures through a sequence of mathematical operations.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 或许没有哪个软件工具像 **TensorFlow** ([https://www.tensorflow.org](https://www.tensorflow.org))
    这样对深度学习的快速发展做出了如此大的贡献，TensorFlow 是一个开源的数学库，由谷歌开发，用于高级机器学习。TensorFlow 提供了一个使用有向图进行数据结构“流动”的数学运算序列的计算接口。
- en: Packt Publishing offers many books on TensorFlow. To search the current offerings,
    visit [https://subscription.packtpub.com/search?query=tensorflow](https://subscription.packtpub.com/search?query=tensorflow).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Packt Publishing 提供了许多关于 TensorFlow 的书籍。要搜索当前提供的书籍，请访问 [https://subscription.packtpub.com/search?query=tensorflow](https://subscription.packtpub.com/search?query=tensorflow)。
- en: 'The fundamental TensorFlow data structure is unsurprisingly known as a **tensor**,
    which is an array of zero or more dimensions. Building upon the simplest 0-D and
    1-D tensors, which represent a single value and a sequence of values, respectively,
    adding additional dimensions allows more complex data structures to be represented.
    Note that because we typically analyze sets of structures, the first dimension
    is generally reserved to allow multiple objects to be stacked; the first dimension
    then refers to the batch or sample number for each structure. For example:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 的基本数据结构不出所料地被称为 **张量**，它是一个零个或多个维度的数组。在 0-D 和 1-D 张量（分别代表单个值和值序列）的基础上构建，增加额外的维度允许表示更复杂的数据结构。请注意，因为我们通常分析结构集，第一个维度通常保留以允许堆叠多个对象；第一个维度因此指的是每个结构的批次或样本编号。例如：
- en: 'A set of 1-D tensors, collecting feature values for a set of people, is a 2-D
    tensor analogous to a data frame in R: `[person_id, feature_values]`'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组一维张量，收集一组人的特征值，是一个二维张量，类似于 R 中的数据框：`[person_id, feature_values]`
- en: 'For measurements repeated over time, 2-D tensors can be stacked as a 3-D tensor:
    `[person_id, time_sequence, feature values]`'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于随时间重复测量的情况，二维张量可以堆叠成三维张量：`[person_id, time_sequence, feature values]`
- en: '2-D images are represented by a 4-D tensor, with the fourth dimension storing
    the color values for each pixel in the 2-D grid: `[image_id, row, column, color_values]`'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2-D 图像由一个 4-D 张量表示，第四维存储 2-D 网格中每个像素的颜色值：`[image_id, row, column, color_values]`
- en: 'Video or animated images are represented in 5-D with an additional time dimension:
    `[image_id, time_sequence, row, column, color_values]`'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视频或动画图像以 5 维表示，并增加一个时间维度：`[image_id, time_sequence, row, column, color_values]`
- en: Most tensors are rectangular matrices completely filled with numeric data, but
    more complex structures like ragged tensors and sparse tensors are available for
    use with text data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数张量是完全填充数字数据的矩形矩阵，但还有更复杂的结构，如稀疏张量和稀疏张量，可用于文本数据。
- en: For an in-depth look at TensorFlow’s tensor objects, the documentation is available
    at [https://www.tensorflow.org/guide/tensor](https://www.tensorflow.org/guide/tensor).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入了解 TensorFlow 的张量对象，文档可在 [https://www.tensorflow.org/guide/tensor](https://www.tensorflow.org/guide/tensor)
    查找。
- en: TensorFlow’s graph, which can be more specifically termed a **dataflow graph**,
    uses nodes connected by directional arrows known as edges to represent dependencies
    between data structures, mathematical operations on these data structures, and
    the output. The nodes represent mathematical operations and the edges represent
    tensors flowing between operations. The graph aids in the parallelization of the
    work, since it is clear what steps must be completed in sequence versus those
    that may be completed simultaneously.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 的图，更具体地可以称为 **数据流图**，使用由称为边的方向箭头连接的节点来表示数据结构之间的依赖关系，这些数据结构上的数学运算以及输出。节点代表数学运算，边代表在运算之间流动的张量。该图有助于并行化工作，因为它清楚地表明哪些步骤必须按顺序完成，哪些步骤可以同时完成。
- en: 'A dataflow graph can be visualized if desired, which produces something like
    the idealized graph depicted in *Figure 15.2*. Although this is a highly simplified
    representation and real-world TensorFlow graphs tend to be much more complex,
    the diagram here shows that after completing the first operation, the second and
    fourth operations can begin in parallel:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，可以可视化数据流图，它会产生类似于*图15.2*中描述的理想化图。尽管这是一个高度简化的表示，现实世界的TensorFlow图通常要复杂得多，但此图表明，在完成第一个操作后，第二个和第四个操作可以并行开始：
- en: '![](img/B17290_15_02.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_15_02.png)'
- en: 'Figure 15.2: A simplified representation of a TensorFlow graph'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.2：TensorFlow图的简化表示
- en: As tensors flow through the graph, they are transformed by the series of operations
    represented by the nodes. The steps are defined by the person building the diagram,
    with each step moving closer to the end goal of accomplishing some sort of mathematical
    task. Some steps in the flow graph may apply simple mathematical transformations
    like normalization, smoothing, or bucketing to the data; others may attempt to
    train a model by iterating repeatedly while monitoring a **loss function**, which
    measures the fit of the model’s predictions to the true values.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当张量在图中流动时，它们会被节点表示的操作序列所转换。步骤由构建图表的人定义，每个步骤都使目标更接近完成某种数学任务。流程图中的某些步骤可能对数据进行简单的数学转换，如归一化、平滑或分桶；其他步骤可能通过迭代重复训练模型，同时监控一个**损失函数**，该函数衡量模型预测与真实值之间的拟合度。
- en: R interfaces to TensorFlow have been developed by the team at RStudio. The `tensorflow`
    package provides access to the core API, while the `tfestimators` package provides
    access to higher-level machine learning functionality. Note that TensorFlow’s
    directed graph approach can be used to implement many different machine learning
    models, including some of those discussed in this book. However, to do so requires
    a thorough understanding of the matrix mathematics that defines each model, and
    thus is outside the scope of this text. For more information about these packages
    and RStudio’s ability to interface with TensorFlow, visit [https://tensorflow.rstudio.com](https://tensorflow.rstudio.com).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: R接口的TensorFlow是由RStudio团队开发的。`tensorflow`包提供了对核心API的访问，而`tfestimators`包提供了对高级机器学习功能的访问。请注意，TensorFlow的定向图方法可以用来实现许多不同的机器学习模型，包括本书中讨论的一些模型。然而，要这样做需要彻底理解定义每个模型的矩阵数学，因此超出了本文的范围。有关这些包和RStudio与TensorFlow接口能力的更多信息，请访问[https://tensorflow.rstudio.com](https://tensorflow.rstudio.com)。
- en: Because TensorFlow relies so heavily on complex mathematical operations that
    must be programmed carefully by hand, the **Keras** library ([https://keras.io](https://keras.io))
    was developed to provide a higher-level interface to TensorFlow and allow deep
    neural networks to be built more easily. Keras was developed in Python and is
    typically paired with TensorFlow as the back-end computing engine. Using Keras,
    it is possible to do deep learning in just a few lines of code—even for challenging
    applications such as image classification, as you will discover in the example
    later in this chapter.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由于TensorFlow严重依赖于必须由手编程的复杂数学运算，因此开发了**Keras**库([https://keras.io](https://keras.io))，以提供对TensorFlow的高级接口，并允许更轻松地构建深度神经网络。Keras是用Python开发的，通常与TensorFlow配对作为后端计算引擎。使用Keras，只需几行代码就可以进行深度学习——即使是像图像分类这样的挑战性应用，你将在本章后面的示例中看到。
- en: Packt Publishing offers numerous books and videos to learn Keras. To search
    current offerings, visit [https://subscription.packtpub.com/search?query=keras](https://subscription.packtpub.com/search?query=keras).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Packt Publishing提供了许多书籍和视频来学习Keras。要搜索当前提供的内容，请访问[https://subscription.packtpub.com/search?query=keras](https://subscription.packtpub.com/search?query=keras)。
- en: The `keras` package, developed by RStudio founder J. J. Allaire, allows R to
    interface with Keras. Although there is very little code required to use the package,
    developing useful deep learning models from scratch requires extensive knowledge
    of neural networks as well as familiarity with TensorFlow and the Keras API. For
    these reasons, a tutorial is outside the scope of this book. Instead, refer to
    the RStudio TensorFlow documentation or the book *Deep Learning with R* (2018),
    which was co-authored by Francois Chollet and J. J. Allaire—the creators of Keras
    and the `keras` package, respectively. Given their credentials, there is no better
    place to begin learning about this tool.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 由RStudio创始人J. J. Allaire开发的`keras`包允许R与Keras接口。尽管使用此包所需的代码非常少，但要从头开始开发有用的深度学习模型，需要广泛了解神经网络以及熟悉TensorFlow和Keras
    API。因此，教程超出了本书的范围。相反，请参阅RStudio TensorFlow文档或由Keras和`keras`包的创造者Francois Chollet和J.
    J. Allaire合著的书籍《深度学习与R》（2018年），这是开始学习这个工具的绝佳起点。
- en: Although the combination of Keras and TensorFlow is arguably the most popular
    deep learning toolkit, it is not the only tool for the task. The **PyTorch** framework
    developed at Facebook has rapidly gained popularity, especially in the academic
    research community, as an easy-to-use alternative. For more information, see [https://pytorch.org](https://pytorch.org).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Keras和TensorFlow的组合可能是最受欢迎的深度学习工具包，但这并不是唯一的工具。Facebook开发的**PyTorch**框架迅速获得了人气，尤其是在学术研究社区中，作为一个易于使用的替代品。更多信息，请参阅[https://pytorch.org](https://pytorch.org)。
- en: TensorFlow’s innovative idea to represent complex mathematical functions using
    a simple graph abstraction, combined with the Keras framework to make it easier
    to specify the network topology, has enabled the construction of deeper and more
    complex neural networks, such as those described in the next section. Keras even
    makes it easy to adapt pre-built neural networks to new tasks with no more than
    a few lines of code.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow使用简单的图抽象来表示复杂数学函数的创新理念，结合Keras框架使其更容易指定网络拓扑，这已经使得构建更深更复杂的神经网络成为可能，如下一节所述。Keras甚至使得仅用几行代码就能轻松地将预构建的神经网络适应新任务。
- en: Understanding convolutional neural networks
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解卷积神经网络
- en: Neural networks have been studied for over 60 years, and even though deep learning
    has only recently become widespread, the concept of a deep neural network has
    been known for decades. As first introduced in *Chapter 7*, *Black-Box Methods
    – Neural Networks and Support Vector Machines*, a **deep neural network** (**DNN**)
    is simply a neural network with more than one hidden layer.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络已经被研究超过60年，尽管深度学习最近才变得普遍，但深度神经网络的概念已经存在了几十年。正如在*第7章*，*黑盒方法 – 神经网络和支持向量机*中首次介绍的那样，**深度神经网络**（**DNN**）只是一个具有多个隐藏层的神经网络。
- en: This understates what deep learning is in practice, as typical DNNs are substantially
    more complex than the types of neural networks we’ve built previously. It’s not
    enough to add a few extra nodes in a new hidden layer and call it “deep learning.”
    Instead, typical DNNs use extremely complex but purposefully designed topologies
    to facilitate learning on big data, and in doing so are capable of human-like
    performance on challenging learning tasks.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这低估了深度学习在实践中是什么，因为典型的DNN比我们之前构建的神经网络类型要复杂得多。仅仅在新的隐藏层中添加几个额外的节点并称之为“深度学习”是不够的。相反，典型的DNN使用极其复杂但故意设计的拓扑结构来促进在大数据上的学习，并且在这个过程中能够在具有挑战性的学习任务上实现类似人类的性能。
- en: A turning point for deep learning came in 2012, when a team called SuperVision
    used deep learning to win the ImageNet Large Scale Visual Recognition Challenge.
    This annual competition tests the ability to classify a subset of 10 million hand-labeled
    images across 10,000 categories of objects. In the early years of the competition,
    humans vastly outperformed computers, but the performance of the SuperVision model
    closed the gap significantly. Today, computers are nearly as good as humans at
    visual classification, and, in some specific cases, are even better. Humans tend
    to be better at identifying small, thin, or distorted items, while computers have
    a greater ability to distinguish among specific types of items such as dog breeds.
    Before long, it is likely that computers will outperform humans on both types
    of visual tasks.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的一个转折点出现在2012年，当时一个名为SuperVision的团队使用深度学习赢得了ImageNet大规模视觉识别挑战赛。这项年度竞赛测试了分类100万张手标注图像的能力，这些图像分布在10,000个物体类别中。在竞赛的早期年份，人类在视觉分类方面远远优于计算机，但SuperVision模型的性能显著缩小了差距。如今，计算机在视觉分类方面几乎与人类一样好，在某些特定情况下甚至更好。人类在识别小、细或变形物品方面往往更擅长，而计算机在区分特定类型的物品，如狗的品种方面具有更大的能力。不久的将来，计算机很可能在两种视觉任务类型上都优于人类。
- en: An innovative network topology designed specifically for image recognition is
    responsible for the surge in performance. A **convolutional neural network** (**CNN**)
    is a deep feed-forward network used for visual tasks that independently learns
    the important distinguishing image features rather than requiring such feature
    engineering beforehand. For example, to classify road signs like “stop” or “yield,”
    a traditional learning algorithm would require pre-engineered features like the
    shape and color of the sign. In contrast, a CNN requires only the raw data for
    each of the image pixels, and the network will learn how to distinguish important
    features like shape and color on its own.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一种专门为图像识别设计的创新网络拓扑结构，是性能激增的原因。**卷积神经网络**（**CNN**）是一种用于视觉任务的深度前馈网络，它独立地学习重要的区分图像特征，而不是在事先需要这样的特征工程。例如，为了对“停止”或“让行”等路标进行分类，传统的学习算法需要预先设计好的特征，如标志的形状和颜色。相比之下，CNN只需要每个图像像素的原始数据，网络将自行学习如何区分形状和颜色等重要特征。
- en: 'Learning features like these is made possible due to the huge increase in dimensionality
    when using raw image data. A traditional learning algorithm would use one row
    per image, in a form like (*stop sign*, *red, octagon*), while a CNN uses data
    in the form (*stop sign*, *x*, *y*, *color*), where *x* and *y* are pixel coordinates
    and *color* is the color data for the given pixel. This may seem like an increase
    of only one dimension but note that even a very small image is made of many (*x*,
    *y*) combinations and color is often specified as a combination of RGB (*red*,
    *green*, *blue*) values. This means that a more accurate representation of a single
    row of training data would be:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 由于使用原始图像数据时维度的大幅增加，这些特征的提取成为可能。传统的学习算法会为每张图像使用一行，形式如（*停止标志*，*红色，八边形*），而CNN使用的数据形式为（*停止标志*，*x*，*y*，*颜色*），其中*x*和*y*是像素坐标，*颜色*是给定像素的颜色数据。这看起来只是维度增加了一个，但请注意，即使是非常小的图像也由许多(*x*，*y*)组合构成，颜色通常指定为RGB（*红色*，*绿色*，*蓝色*）值的组合。这意味着单个训练数据行的更准确表示将是：
- en: (*stop sign*, *x*[1]*y*[1]*r*, *x*[1]*x*[1]*g*, *x*[1]*y*[1]*b*, *x*[2]*y*[1]*r*,
    *x*[2]*y*[1]*g*, *x*[2]*y*[1]*b*, …, *x*[n]*y*[n]*r*, *x*[n]*x*[n]*g*, *x*[n]*y*[n]*b*)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: （*停止标志*，*x*[1]*y*[1]*r*，*x*[1]*x*[1]*g*，*x*[1]*y*[1]*b*，*x*[2]*y*[1]*r*，*x*[2]*y*[1]*g*，*x*[2]*y*[1]*b*，……，*x*[n]*y*[n]*r*，*x*[n]*x*[n]*g*，*x*[n]*y*[n]*b*）
- en: Each of the predictors refers to the level of red, green, or blue at the specified
    combination of (*x*, *y*), and *r*, *g*, or *b* values. Thus, the dimensionality
    increases greatly, and the dataset becomes much wider as the image becomes larger.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 每个预测因子都指的是在指定的(*x*，*y*)组合中红色、绿色或蓝色的程度，以及*r*、*g*或*b*值。因此，维度大大增加，随着图像变大，数据集也变得更大。
- en: A small 100x100 pixel image would have *100x100x3 = 30,000* predictors. Even
    this is small compared to the SuperVision team, which used over 60 million parameters
    when it won the visual recognition challenge in 2012!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一个100x100像素的小图像将有*100x100x3 = 30,000*个预测因子。即使这样，与2012年赢得视觉识别挑战赛的SuperVision团队使用的超过6000万个参数相比，这仍然很小！
- en: '*Chapter 12*, *Advanced Data Preparation*, noted that if a model is overparameterized,
    it reaches an interpolation threshold at which there are enough parameters to
    memorize and perfectly classify all the training samples. The ImageNet Challenge
    dataset, which contained 10 million images, is much smaller than the 60 million
    parameters the winning team used. Intuitively, this makes sense; assuming there
    are no completely identical pictures in the database, at least one of the pixels
    will vary for every image. Thus, an algorithm could simply memorize every image
    to achieve the perfect classification of the training data. The problem is that
    the model will be evaluated on a dataset of unseen data, and thus the severe overfitting
    to the training data will lead to a massive generalization error.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*第12章*，*高级数据准备*指出，如果一个模型过度参数化，它将达到一个插值阈值，此时有足够的参数来记忆并完美地分类所有训练样本。包含1000万张图片的ImageNet挑战数据集比获胜团队使用的6000万个参数要小得多。直观上看，这是有道理的；假设数据库中没有完全相同的图片，至少每个图像的一个像素会有所不同。因此，算法可以简单地记忆每一张图片以实现训练数据的完美分类。问题是模型将在未见过的数据集上评估，因此对训练数据的严重过拟合将导致巨大的泛化误差。'
- en: 'The topology of a CNN prevents this from happening. We won’t be diving too
    deeply into the black box of the CNN, but we will understand it as a series of
    layers in the following categories:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的拓扑结构阻止了这种情况的发生。我们不会深入探讨CNN的黑盒，但我们将它理解为一组以下类别的层：
- en: '**Convolutional layers** are placed early in the network and usually comprise
    the most computationally intensive step in the network because they are the only
    layers to process the raw image data directly; we can understand convolution as
    passing the raw data through a filter creating a set of tiles that represent small,
    overlapping portions of the full area'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积层**在网络中放置得较早，通常构成了网络中最计算密集的步骤，因为它们是唯一直接处理原始图像数据的层；我们可以将卷积理解为将原始数据通过一个过滤器，创建一组代表整个区域的小型重叠部分的瓦片'
- en: '**Pooling layers**, also known as **downsampling** or **subsampling** layers,
    gather the output signals from a cluster of neurons in one layer, and summarize
    them into a single neuron for the next layer, usually by taking the maximum or
    average value among those being summarized'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**池化层**，也称为**下采样**或**子采样**层，从一层中的一簇神经元中收集输出信号，并将它们总结为下一层的单个神经元，通常是通过取被总结的信号中的最大值或平均值来实现'
- en: '**Fully connected layers** are much like the layers in a traditional multilayer
    perceptron, and are used near the end of the CNN to build the model that makes
    predictions'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全连接层**与传统的多层感知器中的层非常相似，通常在CNN的末尾用于构建预测模型'
- en: The convolutional and pooling layers in the network serve the interrelated purposes
    of identifying important features of the images to be learned, as well as reducing
    the dimensionality of the dataset before hitting the fully connected layers that
    make predictions. In other words, the early stages of the network perform feature
    engineering, while the later stages use the constructed features to make predictions.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 网络中的卷积层和池化层服务于识别要学习的图像的重要特征以及在使用进行预测的全连接层之前降低数据集的维度的相关目的。换句话说，网络的早期阶段执行特征工程，而后期阶段使用构建的特征进行预测。
- en: To better understand the layers in a CNN, see *An Interactive Node-Link Visualization
    of Convolutional Neural Networks* by Adam W. Harley at [https://adamharley.com/nn_vis/](https://adamharley.com/nn_vis/).
    The interactive tool has you draw a number from zero to nine, which is then classified
    using a neural network. The 2D and 3D convolutional network visualizations clearly
    show how the digit you drew passes through the convolutional, downsampling, and
    fully connected layers before reaching the output layer where the prediction is
    made. Neural networks for general image classification work much the same way,
    but using a substantially larger and more complex network.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解CNN中的层，请参阅Adam W. Harley在[https://adamharley.com/nn_vis/](https://adamharley.com/nn_vis/)上发表的《卷积神经网络的交互式节点-链接可视化》*。该交互式工具让您从零到九画一个数字，然后使用神经网络对其进行分类。二维和三维卷积网络可视化清楚地显示了您所画的数字是如何通过卷积、下采样和全连接层，最终到达输出层进行预测的。用于通用图像分类的神经网络以类似的方式工作，但使用的是一个更大、更复杂的网络。
- en: Transfer learning and fine tuning
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 迁移学习和微调
- en: Building a CNN from scratch requires a tremendous amount of data, expertise,
    and computing power. Thankfully, many large organizations that have access to
    data and computing resources have built image, text, and audio classification
    models, and have shared them with the data science community. Through a process
    of **transfer learning**, a deep learning model can be adapted from one context
    to another. Not only is it possible to apply the saved model to similar types
    of problems as it was trained on, but it may also be useful for problems outside
    the original domain. For instance, a neural network that was trained to recognize
    an endangered species of elephants in satellite photos may also be useful for
    identifying the position of tanks in infrared drone images taken above a war zone.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 从头开始构建CNN需要大量的数据、专业知识和计算能力。幸运的是，许多拥有数据和计算资源的大型组织已经构建了图像、文本和音频分类模型，并将它们与数据科学社区共享。通过**迁移学习**的过程，深度学习模型可以从一个上下文适应到另一个上下文。不仅可以将保存的模型应用于与训练时相似类型的问题，而且它也可能对原始领域之外的问题有用。例如，一个在卫星照片中训练以识别濒危象种的神经网络，也可能有助于识别在战区上空拍摄的红外无人机图像中坦克的位置。
- en: If the knowledge doesn’t transfer directly to the new task, it is possible to
    hone a pre-trained neural network using additional training in a process known
    as **fine tuning**. Taking a model that was trained generally, such as a general
    image classification model that can identify 10,000 classes of objects, and fine
    tuning it to be good at identifying a single type of object not only reduces the
    amount of training data and computing power needed but also may offer improved
    generalization over a model trained on a single class of images.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果知识不能直接迁移到新任务，可以通过**微调**过程使用额外的训练来磨练预训练的神经网络。从一个一般性的模型开始训练，例如一个可以识别10,000类对象的通用图像分类模型，并将其微调以擅长识别单一类型的对象，这不仅减少了所需的训练数据和计算能力，还可能比仅在单一类图像上训练的模型提供更好的泛化能力。
- en: Keras can be used for both transfer learning and fine tuning by downloading
    neural networks with pre-trained weights. A list of available pre-trained models
    is available at [https://keras.io/api/applications/](https://keras.io/api/applications/)
    and an example of fine tuning an image processing model to better predict cats
    and dogs can be found at [https://tensorflow.rstudio.com/guides/keras/transfer_learning](https://tensorflow.rstudio.com/guides/keras/transfer_learning).
    In the next section, we will apply a pre-trained image model to real-world images.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Keras可以通过下载带有预训练权重的神经网络来进行迁移学习和微调。可用的预训练模型列表可在[https://keras.io/api/applications/](https://keras.io/api/applications/)找到，一个将图像处理模型微调以更好地预测猫和狗的示例可在[https://tensorflow.rstudio.com/guides/keras/transfer_learning](https://tensorflow.rstudio.com/guides/keras/transfer_learning)找到。在下一节中，我们将应用一个预训练的图像模型到现实世界的图像上。
- en: Example – classifying images using a pre-trained CNN in R
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例 - 使用预训练的CNN在R中分类图像
- en: 'R may not be the right tool for the heaviest deep learning jobs, but with the
    right set of packages, we can apply pre-trained CNNs to perform tasks, such as
    image recognition, that conventional machine learning algorithms have trouble
    solving. The predictions generated by the R code can then be used directly for
    image recognition tasks like filtering obscene profile pictures, determining whether
    an image depicts a cat or a dog, or even identifying stop signs inside a simple
    autonomous vehicle. Perhaps more commonly, the predictions could be used as predictors
    in an ensemble that includes conventional machine learning models using tabular-structured
    data in addition to the deep learning neural network that consumes the unstructured
    image data. You may recall that *Chapter 14*, *Building Better Learners*, described
    a potential stacked ensemble that combined image, text, and traditional machine
    learning models in this way to predict elements of a Twitter user’s future behavior.
    The following pictures illustrate hypothetical Twitter profile pictures, which
    we will classify using a deep neural network shortly:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: R可能不是处理最重深度学习任务的正确工具，但有了合适的包集，我们可以将预训练的CNN应用于执行图像识别等任务，这些任务传统机器学习算法难以解决。R代码生成的预测可以直接用于图像识别任务，如过滤不雅的资料照片、确定图像是否描绘了猫或狗，甚至识别简单自动驾驶车辆内的停车标志。也许更常见的是，这些预测可以用作包含使用表格结构数据的传统机器学习模型以及消耗非结构化图像数据的深度学习神经网络的集成模型的预测因子。你可能还记得，*第14章*，*构建更好的学习者*，描述了一种潜在的堆叠集成，它以这种方式结合了图像、文本和传统机器学习模型，以预测Twitter用户未来行为的元素。以下图片展示了假设的Twitter个人资料图片，我们将使用深度神经网络对其进行分类：
- en: '![](img/B17290_15_03.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_15_03.png)'
- en: 'Figure 15.3: A pre-trained neural network can be used in R to identify the
    subject of images like these'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.3：可以在R中使用预训练的神经网络识别这类图像
- en: First, before using a pre-trained model, it is important to consider the dataset
    that was used to train the neural network. Most publicly available image networks
    were trained on huge image databases comprising a variety of everyday objects
    and animals, such as cars, dogs, houses, various tools, and so on. This is appropriate
    if the desired task is to distinguish among everyday objects, but more specific
    tasks may require more specific training datasets. For instance, a facial recognition
    tool or an algorithm identifying stop signs would be more effectively trained
    on datasets of faces and road signs, respectively. With transfer learning, it
    is possible to fine-tune a deep neural network trained on a variety of images
    to be better at a more specific task—it could become very good at identifying
    pictures of cats, for example—but it is hard to imagine a network trained on faces
    or road signs ever becoming very good at identifying cats, even with additional
    tuning!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在使用预训练模型之前，考虑用于训练神经网络的训练数据集非常重要。大多数公开可用的图像网络都是在包含各种日常物体和动物（如汽车、狗、房屋、各种工具等）的巨大图像数据库上训练的。如果目标是区分日常物体，这是合适的，但对于更具体的工作可能需要更具体的训练数据集。例如，面部识别工具或识别停车标志的算法可能需要在面部和道路标志的数据集上分别进行更有效的训练。通过迁移学习，可以将训练于各种图像的深度神经网络微调以更好地完成更具体的工作——例如，它可能非常擅长识别猫的图片——但很难想象一个在面部或道路标志上训练的网络，即使经过额外的调整，也能非常擅长识别猫！
- en: For this exercise, we will classify our small set of images using a CNN called
    **ResNet-50**, which is a 50-layer deep network that has been trained on a large
    and comprehensive variety of labeled images. This model, which was introduced
    by a group of researchers in 2015 as a state-of-the-art, competition-winning computer
    vision algorithm, has since been surpassed by more sophisticated approaches, but
    continues to be extremely popular and effective due to its ease of use and integration
    with tools like R and Keras.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用名为**ResNet-50**的CNN对我们的小图像集进行分类，这是一个在大量和综合的标记图像上训练的50层深度网络。这个模型由一组研究人员在2015年作为最先进的、获奖的计算机视觉算法引入，尽管后来被更复杂的方法所超越，但由于其易用性和与R和Keras等工具的集成，它仍然非常受欢迎和有效。
- en: For more information about ResNet-50, see *Deep Residual Learning for Image
    Recognition, He, K., Zhang, X., Ren, S., and Sun, J., 2015,* [https://arxiv.org/abs/1512.03385v1](https://arxiv.org/abs/1512.03385v1).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 关于ResNet-50的更多信息，请参阅*《深度残差学习用于图像识别》，He, K.，Zhang, X.，Ren, S.，和Sun, J.，2015年*
    [https://arxiv.org/abs/1512.03385v1](https://arxiv.org/abs/1512.03385v1)。
- en: The **ImageNet database** ([https://www.image-net.org](https://www.image-net.org))
    that was used to train the ResNet-50 model is the same database used for the ImageNet
    Visual Recognition Challenge and has contributed greatly to computer vision since
    its introduction in 2010\. Composed of over 14 million hand-labeled images and
    consuming many gigabytes of storage (or even terabytes in the case of the full,
    academic version), it is fortunate that there is no need to download this resource
    and train the model from scratch. Instead, we simply download the neural network
    weights for the ResNet-50 model that researchers trained on the full database,
    saving us a tremendous amount of computational expense.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练 ResNet-50 模型的 **ImageNet 数据库** ([https://www.image-net.org](https://www.image-net.org))
    与用于 ImageNet 视觉识别挑战的数据库相同，自 2010 年推出以来对计算机视觉做出了巨大贡献。它由超过 1400 万张手工标注的图像组成，消耗了数
    GB 的存储空间（在完整、学术版本的情况下甚至达到 TB 级别），幸运的是，我们无需下载此资源从头开始训练模型。相反，我们只需下载研究人员在完整数据库上训练的
    ResNet-50 模型的神经网络权重，从而节省了大量计算开销。
- en: 'To begin the process, we’ll need to add the `tensorflow` and `keras` packages
    to R as well as the various dependencies. Most of these steps must only be performed
    once. The `devtools` package adds tools to develop R packages and use packages
    that are in active development, so we’ll begin by installing and loading this
    as usual:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始这个过程，我们需要将 `tensorflow` 和 `keras` 包添加到 R 中，以及各种依赖项。大多数这些步骤只需执行一次。`devtools`
    包为开发 R 包和使用处于开发中的包添加了工具，因此我们将像往常一样安装并加载这个包：
- en: '[PRE0]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we’ll use the `devtools` package to obtain the latest version of the
    `tensorflow` package from GitHub. Typically, we install packages from CRAN, but
    for packages in active development, it can be better to install directly from
    the latest source code. The command to install the `tensorflow` package from its
    GitHub path is:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 `devtools` 包从 GitHub 获取 `tensorflow` 包的最新版本。通常，我们从 CRAN 安装包，但对于处于开发中的包，直接从最新源代码安装可能更好。从其
    GitHub 路径安装 `tensorflow` 包的命令是：
- en: '[PRE1]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This points R to the RStudio GitHub account, which stores the source code for
    the package. To read the documentation and see the code for yourself on the web,
    visit [https://github.com/rstudio/tensorflow](https://github.com/rstudio/tensorflow)
    in a web browser.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这将 R 指向 RStudio 的 GitHub 账户，该账户存储了包的源代码。要在线阅读文档并查看代码，请在网络浏览器中访问 [https://github.com/rstudio/tensorflow](https://github.com/rstudio/tensorflow)。
- en: 'After installing the `tensorflow` package, there are several dependencies that
    are required to begin using TensorFlow in R. In particular, the `tensorflow` package
    is merely an interface between R and TensorFlow, so we must first install TensorFlow
    itself. Perhaps ironically, Python and several of its packages are required for
    this. Thus, the R `reticulate` package ([https://rstudio.github.io/reticulate/](https://rstudio.github.io/reticulate/))
    is used to manage the interface between R and Python. As confusing as this seems,
    the complete installation process is driven by a single command from the `tensorflow`
    package as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 `tensorflow` 包后，有几个依赖项是开始使用 TensorFlow 在 R 中所必需的。特别是，`tensorflow` 包仅仅是 R
    和 TensorFlow 之间的接口，因此我们必须首先安装 TensorFlow 本身。也许有些讽刺，Python 及其一些包是完成此任务所必需的。因此，我们使用
    R 的 `reticulate` 包 ([https://rstudio.github.io/reticulate/](https://rstudio.github.io/reticulate/))
    来管理 R 和 Python 之间的接口。尽管这听起来很令人困惑，但完整的安装过程是由 `tensorflow` 包的单个命令驱动的，如下所示：
- en: '[PRE2]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'While the command is running, you should see R working to install a large collection
    of Python tools and packages. If all goes well, you can proceed to install the
    Keras package from GitHub:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 当命令运行时，你应该看到 R 正在安装大量 Python 工具和包。如果一切顺利，你可以继续从 GitHub 安装 Keras 包：
- en: '[PRE3]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the case of a problem, keep in mind that although the code for this example
    was tested on multiple platforms and R versions, it is quite possible for something
    to go wrong among the many dependencies required to have R interface with Python
    and TensorFlow. Don’t be afraid to search the web for a particular error message
    or check the Packt Publishing GitHub repository for the updated R code for this
    chapter.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果出现问题，请记住，尽管此示例的代码已在多个平台和 R 版本上进行了测试，但在 R 与 Python 和 TensorFlow 交互所需的众多依赖项中，出现问题的可能性相当大。不要害怕在网络上搜索特定的错误消息，或者检查
    Packt Publishing 的 GitHub 仓库以获取本章更新的 R 代码。
- en: 'With the necessary packages installed, Keras can help load the ResNet-50 model
    with the weights trained on the ImageNet database:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装了必要的包之后，Keras可以帮助加载在ImageNet数据库上训练的ResNet-50模型：
- en: '[PRE4]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Our 50-layer deep image classification model trained on millions of everyday
    images is now ready to start making predictions; however, the ease at which we
    loaded the model conceals the work to come.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在数百万张日常图像上训练的50层深度图像分类模型现在可以开始做出预测了；然而，我们加载模型时的轻松程度掩盖了即将到来的工作。
- en: The greater challenge with using a pre-trained model is transforming our unstructured
    image data, which we hope to classify, into the same structured format that it
    saw during training. ResNet-50 used square images of 224-by-224 pixels with each
    pixel reflecting a color composed of three channels, red, green, and blue, each
    having 255 levels of brightness. All images we hope to classify must be transformed
    from their original formats, such as PNG, GIF, or JPEG, into a 3-D tensor using
    this representation. We’ll see this in practice using the previously depicted
    `cat.jpg`, `ice_cream.jpg`, and `pizza.jpg` files found in the R code folder for
    this chapter, but the process will work similarly for any image.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练模型的一个更大挑战是将我们希望分类的无结构图像数据转换成它在训练期间看到的相同结构格式。ResNet-50使用了224x224像素的方形图像，每个像素反映由三个通道组成的颜色，红色、绿色和蓝色，每个通道都有255个亮度级别。我们希望分类的所有图像都必须使用这种表示从其原始格式（如PNG、GIF或JPEG）转换为3-D张量。我们将通过之前描述的`cat.jpg`、`ice_cream.jpg`和`pizza.jpg`文件来实际看到这一点，这些文件位于本章R代码文件夹中，但这个过程对任何图像都是类似的。
- en: 'The `image_load()` function in the `keras` package will get the process started.
    Simply provide the file name and desired target dimensions as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`keras`包中的`image_load()`函数将启动这个过程。只需提供文件名和所需的目标尺寸，如下所示：'
- en: '[PRE5]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This creates an image object, but we need one more command to convert it into
    a 3-D tensor:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个图像对象，但我们还需要一个额外的命令将其转换为3-D张量：
- en: '[PRE6]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To prove it to ourselves, we can examine the dimensions and structure of the
    object as follows. As expected, the object is a numeric matrix with *224* x *224*
    x *3* as the dimensions:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明这一点，我们可以检查对象的大小和结构，如下所示。正如预期的那样，对象是一个数值矩阵，其维度为*224* x *224* x *3*：
- en: '[PRE7]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The first few values in the matrix are all 255, which isn’t very meaningful:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵中的前几个值都是255，这并没有什么意义：
- en: '[PRE9]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let’s do some investigation to better understand these data structures. Because
    of R’s row, column matrix format, the matrix coordinates are (*y*, *x*), with
    (*1*, *1*) representing the top-left pixel in the image and (*1*, *224*) the top-right
    pixel. To illustrate this, let’s obtain each of the three color channels for a
    couple of pixels in the ice cream image:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行一些调查，以便更好地理解这些数据结构。由于R的行列矩阵格式，矩阵坐标是(*y*, *x*)，其中(*1*, *1*)代表图像的左上角像素，(*1*,
    *224*)代表右上角像素。为了说明这一点，让我们获取冰淇淋图像中几个像素的三个颜色通道：
- en: '[PRE11]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The pixel at (*1*, *224*) has (*r*, *g*, *b*) colors of (*253*, *253*, *255*),
    which corresponds to nearly the brightest possible white, while the pixel at (*40*,
    *145*) has color values (*149*, *23*, *34*) translating to a dark red—a piece
    of strawberry in the ice cream. These coordinates are illustrated in the following
    figure:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 像素(*1*, *224*)的颜色为(*r*, *g*, *b*)，颜色值为(*253*, *253*, *255*)，这几乎是最亮的白色，而像素(*40*,
    *145*)的颜色值为(*149*, *23*, *34*)，翻译成深红色——冰淇淋中的一块草莓。这些坐标在以下图中进行了说明：
- en: '![](img/B17290_15_04.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17290_15_04.png)'
- en: 'Figure 15.4: The picture of ice cream has been reduced from a matrix of 1,000x1,000
    pixels to 224x224; each pixel in the image has three color channels'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.4：冰淇淋的图片已经从1,000x1,000像素的矩阵减少到224x224；图像中的每个像素都有三个颜色通道
- en: 'One additional complication is that the ResNet-50 model expects a four-dimensional
    tensor, with the fourth dimension representing the batch. With only one image
    to classify, we have no need for this parameter, so we’ll simply assign it a constant
    value of 1 to create a matrix of *1x224x224x3*. The command `c(1, dim(x))` defines
    the new matrix in this format, and then the `array_reshape()` function fills this
    matrix with the contents of `x` using the Python-style row-by-row ordering used
    by TensorFlow rather than the R-style column-by-column filling. The full command
    is as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个额外的复杂性是，ResNet-50模型期望一个四维张量，其中第四维代表批次。由于只有一个图像需要分类，我们不需要这个参数，因此我们将简单地将其分配一个常数值1，以创建一个`1x224x224x3`的矩阵。命令`c(1,
    dim(x))`以这种格式定义新的矩阵，然后`array_reshape()`函数使用TensorFlow使用的Python风格行行顺序而不是R风格的列列填充，将`x`的内容填充到这个矩阵中。完整的命令如下：
- en: '[PRE15]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To confirm that `x` has the correct dimensions, we can use the `dim()` command:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认`x`具有正确的维度，我们可以使用`dim()`命令：
- en: '[PRE16]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, we run the `imagenet_preprocess_input()` function to normalize the
    color values to match the ImageNet database:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们运行`imagenet_preprocess_input()`函数来将颜色值归一化，以匹配ImageNet数据库：
- en: '[PRE18]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The primary function of this transformation is to zero-center each color with
    respect to the database, essentially treating each color value as being greater
    than or less than the average of ImageNet images on that color. For example, the
    red pixel in the ice cream at (`40`, `145`) had color values of 149, 23, and 34
    before; now, it has very different values:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这种转换的主要功能是将每个颜色相对于数据库进行零中心化，基本上是将每个颜色值视为大于或小于ImageNet图像中该颜色的平均值。例如，在冰淇淋中，位于(`40`,
    `145`)的红色像素在之前具有149、23和34的颜色值；现在，它具有非常不同的值：
- en: '[PRE19]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Negative values indicate a color level less than the ImageNet average for that
    color, and positive values indicate higher. The preprocessing step also inverts
    the red-green-blue format to blue-green-red, so only the red channel is above
    the average ImageNet level, which is not terribly surprising, as a strawberry
    is quite red!
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 负值表示该颜色的颜色级别低于ImageNet的平均值，而正值表示更高。预处理步骤还将红绿蓝格式反转为蓝绿红，因此只有红色通道高于ImageNet的平均水平，这并不令人特别惊讶，因为草莓非常红！
- en: 'Now, let’s see what the ResNet-50 network thinks is depicted in the image.
    We’ll first use the `predict()` function on the model object and the image matrix,
    and then use the `keras` function `imagenet_decode_predictions()` to convert the
    network’s predicted probabilities to text-based labels that categorize each of
    the ImageNet images. Because there are 1,000 categories of images in the ImageNet
    database, the `preds` object contains 1,000 predicted probabilities—one for each
    possibility. The decoding function allows us to limit the output to the top *N*
    most probable possibilities—ten, in this case:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看ResNet-50网络认为图像中描绘了什么。我们将首先在模型对象和图像矩阵上使用`predict()`函数，然后使用`keras`函数`imagenet_decode_predictions()`将网络的预测概率转换为基于文本的标签，这些标签将分类ImageNet中的每个图像。由于ImageNet数据库中有1,000种图像类别，`preds`对象包含1,000个预测概率——每个可能性一个。解码函数允许我们将输出限制为最可能的前*N*个可能性——在这个例子中是十个：
- en: '[PRE21]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The `c_resnet50` object is a list which contains the top ten predictions for
    our lone image. To see the predictions, we simply type the name of the list to
    discover that the network correctly identified the image as ice cream with about
    99.6 percent probability:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`c_resnet50`对象是一个列表，其中包含我们单个图像的前十个预测。要查看预测结果，我们只需输入列表的名称，就可以发现网络正确地将图像识别为冰淇淋，概率约为99.6%：'
- en: '[PRE22]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Although none of the other potential classifications had a predicted probability
    much greater than zero, some of the other top predictions make a bit of sense;
    it is not difficult to see why they were considered as possibilities. Eggnog is
    in the correct category of foods, while an ice cream cone might look a bit like
    a cup, or a thimble.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管其他潜在分类的预测概率并没有远大于零，但一些其他顶级预测还是有点道理；不难理解为什么它们被视为可能性。例如，蛋酒属于正确的食物类别，而冰淇淋锥可能看起来有点像杯子，或者像顶针。
- en: The model even listed strawberry as the sixth most likely option, which is the
    correct flavor of ice cream.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 模型甚至将草莓列为第六个最有可能的选项，这是正确的冰淇淋口味。
- en: 'As an exercise, we’ll do the same process with the other two images. The following
    sequence of steps uses the `lapply()` function to apply the image processing steps
    to the pair of images, each time creating a new list to supply to the subsequent
    function. The last step supplies the list containing two prepared image arrays
    to the `lapply()` function, which applies the `predict()` command to each image:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，我们将对另外两张图片执行相同的过程。以下步骤序列使用`lapply()`函数将图像处理步骤应用于图像对，每次创建一个新的列表以供后续函数使用。最后一步将包含两个准备好的图像数组的列表提供给`lapply()`函数，该函数对每个图像应用`predict()`命令：
- en: '[PRE24]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Finally, the `sapply()` function is used to apply the decoding function to
    each of the two sets of predictions, while simplifying the result. The `lapply()`
    function would also work here, but because `imagenet_decode_predictions()` returns
    a list, the result is a sub-list of length one within a list; `sapply()` recognizes
    that this is redundant and unnecessary, and will eliminate the additional level
    of hierarchy:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用`sapply()`函数将解码函数应用于两个预测集的每个集合，同时简化结果。`lapply()`函数在这里也可以工作，但由于`imagenet_decode_predictions()`返回一个列表，结果是列表中的一个长度为1的子列表；`sapply()`识别出这是多余的，并且将消除额外的层次结构：
- en: '[PRE25]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Typing the name of the resulting object shows the top three predictions for
    each of the two images:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 输入结果的名称将显示两张图像各自的前三个预测：
- en: '[PRE26]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The ResNet-50 algorithm didn’t merely classify the images correctly; it also
    correctly identified the cat picture as a tabby. This demonstrates the ability
    of neural networks to surpass human specificity for certain tasks; many or most
    people might have simply labeled the image as a cat, whereas the computer can
    determine the specific type of cat. On the other hand, humans remain better at
    identifying objects in less-than-optimal conditions. For example, a cat obscured
    by darkness or camouflaged in the weeds would present a greater challenge to a
    computer than to a human in most cases. Even so, the computer’s ability to work
    tirelessly gives it a huge advantage for automating artificial intelligence tasks.
    As stated previously, applied to a large dataset such as Twitter profile images,
    the predictions from this type of computer vision model could be used in an ensemble
    model predicting countless different user behaviors.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet-50算法不仅正确地分类了图像；它还正确地将猫图片识别为虎斑猫。这证明了神经网络在某些任务上超越人类特定性的能力；许多人或大多数人可能只是将图像标记为猫，而计算机可以确定猫的具体类型。另一方面，人类在识别非最佳条件下的物体方面仍然更胜一筹。例如，在黑暗中或杂草中隐藏的猫对计算机来说可能比对人更具挑战性。尽管如此，计算机不知疲倦的能力使其在自动化人工智能任务方面具有巨大的优势。如前所述，应用于大型数据集，如Twitter个人资料图片，此类计算机视觉模型的预测可以用于预测无数不同用户行为的集成模型。
- en: Unsupervised learning and big data
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习和大数据
- en: The previous section illustrated how a deep neural network could be used to
    classify a limitless supply of input images as an instance of everyday creatures
    or objects. From another perspective, one might also understand this as a machine
    learning task that takes the highly dimensional input of image pixel data and
    reduces it to a lower-dimensional set of image labels. It is important to note,
    however, that the deep learning neural network is a supervised learning technique,
    which means that the machine can only learn what the humans tell it to learn—in
    other words, it can only learn from something that has been previously labeled.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节说明了如何使用深度神经网络将无限供应的输入图像分类为日常生物或物体的实例。从另一个角度来看，这也可能被理解为一种机器学习任务，它将图像像素数据的高度维输入降低到一组低维度的图像标签。然而，值得注意的是，深度学习神经网络是一种监督学习技术，这意味着机器只能学习人类告诉它学习的内容——换句话说，它只能从已经被标记的内容中学习。
- en: The purpose of this section is to present useful applications of unsupervised
    learning techniques in the context of big data. These applications are in many
    ways similar to the techniques covered in *Chapter 9*, *Finding Groups of Data
    – Clustering with k-means*. However, where previous unsupervised learning techniques
    leaned heavily on humans to interpret the results, in the context of big data,
    the machine can go a step further than before and provide a deeper, richer understanding
    of the data and the implications of the connections the algorithm discovers.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的目的在于介绍在大数据背景下无监督学习技术的有用应用。这些应用在很多方面与第9章中介绍的技术相似，即*寻找数据组 - 使用k-means进行聚类*。然而，在先前无监督学习技术中，人类在解释结果方面承担了很大的责任，而在大数据的背景下，机器可以比以前更进一步，提供对数据及其发现算法连接的更深、更丰富的理解。
- en: 'To put this in practical terms, imagine a deep neural network that can learn
    to identify a cat without ever having been told what a cat is. Of course, without
    being given the label beforehand, the computer may not explicitly label it a “cat”
    *per se*, but it may understand that a cat has certain consistent relationships
    to other things that appear in pictures along with the cat: people, litter boxes,
    mice, balls of yarn—but rarely or never dogs! Such associations help form a conceptualization
    of cat as something that relates closely to people, litter boxes, and yarn, but
    is perhaps in opposition to another thing with four legs and a tail. Given enough
    pictures, it is possible the neural network could eventually associate its impression
    of cats with the English-language word “cat” by identifying cats near bags of
    cat food or in the internet’s countless cat-based memes!'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 用实际的话来说，想象一个深度神经网络，它可以学会识别猫，而从未被告知猫是什么。当然，如果没有事先给出标签，计算机可能不会明确地将它标记为“猫”本身，但它可能理解猫与图片中出现的其他事物之间存在某些一致的关系：人、猫砂盆、老鼠、一团毛线——但很少或从不包括狗！这样的关联有助于形成对猫的概念，认为它与人类、猫砂盆和毛线密切相关，但可能与另一种有四条腿和尾巴的东西相对立。如果有足够的图片，神经网络最终可能会通过识别猫粮袋附近的猫或互联网上无数的以猫为主题的梗，将对其印象与英语单词“猫”联系起来！
- en: Developing such a sophisticated model of cats would take more data and computing
    power than most machine learning practitioners have access to, but it is certainly
    possible to develop simpler models or to borrow from big data companies that do
    have access to such resources. These techniques provide yet another way to incorporate
    unstructured data sources into more conventional learning tasks, as the machine
    can reduce the complexity of big data into something much more digestible.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 开发这样一个复杂的猫的模型需要比大多数机器学习从业者所能获取的数据和计算能力更多，但当然有可能开发更简单的模型，或者借鉴那些确实能获取这些资源的大数据公司。这些技术提供了另一种将非结构化数据源纳入更传统学习任务的方法，因为机器可以将大数据的复杂性降低到更易于消化的程度。
- en: Representing highly dimensional concepts as embeddings
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将高维概念表示为嵌入
- en: 'The things we encounter in everyday life can be described by a limitless number
    of attributes. Moreover, not only are there countless data points that can be
    used to describe each object, but the nature of human subjectivity makes it unlikely
    that any two people would describe an object in the same way. For example, if
    you ask a few people to describe typical horror films, one might say they imagine
    slasher films with blood and gore, another might think of zombie or vampire movies,
    and another one might think of spooky ghost stories and haunted houses. These
    descriptions could be represented using the following statements:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在日常生活中遇到的事物可以用无数个属性来描述。此外，不仅存在无数可以用来描述每个对象的数据点，而且人类主观性的本质使得任何两个人都不太可能以相同的方式描述一个对象。例如，如果你问一些人描述典型的恐怖电影，一个人可能会想象血腥和血腥的砍杀电影，另一个人可能会想到僵尸或吸血鬼电影，还有一个人可能会想到阴森的鬼故事和闹鬼的房子。这些描述可以用以下陈述来表示：
- en: '*horror* = *killer* + *blood* + *gore*'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*恐怖* = *杀手* + *血液* + *血腥*'
- en: '*horror* = *creepy* + *zombies* + *vampires*'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*恐怖* = *诡异* + *僵尸* + *吸血鬼*'
- en: '*horror* = *spooky* + *ghosts* + *haunted*'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*恐怖* = *阴森* + *鬼魂* + *闹鬼*'
- en: If we were to program these definitions into a computer, it could substitute
    any of the representations of horror for one another and thus use a wider general
    concept of “horror” rather than the more specific features like “gore,” “creepy,”
    or “spooky” to make predictions. For instance, a learning algorithm could discover
    that a social media user that writes any of these horror-related terms is more
    likely to click on an advertisement for the new *Scream* movie.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将这些定义编程到计算机中，它就可以相互替换任何关于恐怖的表示，从而使用更广泛的“恐怖”概念，而不是像“血腥”、“诡异”或“阴森”这样的更具体特征来进行预测。例如，一个学习算法可能会发现，任何写有这些恐怖相关术语的社交媒体用户更有可能点击观看新片
    *Scream* 的广告。
- en: Unfortunately, if a user posts “I just love a good scary movie!” or “The Halloween
    season is my favorite time of year!” then the algorithm will be unable to relate
    the text to the prior conceptualizations of horror, and thus will be unable to
    realize that a horror movie advertisement should be displayed. This is likewise
    true for any of the hundreds of horror-related keywords that the computer had
    not previously seen verbatim, including many that will seem obvious to a human
    observer, like witches, demons, graveyards, spiders, skeletons, and so on. What
    is needed is a way to generalize the concept of horror to the almost limitless
    number of ways that it can be described.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，如果用户发布“我简直喜欢一部好恐怖的电影！”或“万圣节季节是我一年中最喜欢的时光！”这样的帖子，算法将无法将文本与先前的恐怖概念联系起来，因此将无法意识到应该显示恐怖电影的广告。这同样适用于计算机之前未曾直接看到的数百个与恐怖相关的关键词，包括许多对人类观察者来说显然的关键词，如女巫、恶魔、墓地、蜘蛛、骷髅等等。所需要的，是一种将恐怖概念推广到几乎无限的描述方式的方法。
- en: An **embedding** is a mathematical concept referring to the ability to represent
    a higher-dimensional vector using fewer dimensions; in machine learning, the embedding
    is purposefully constructed such that dimensions that are correlated in the high-dimensional
    space are positioned more closely in the lower-dimensional space.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**嵌入**是一个数学概念，指的是使用更少的维度来表示高维向量的能力；在机器学习中，嵌入是有意构建的，使得在高维空间中相关联的维度在低维空间中位置更接近。'
- en: If the embedding is constructed well, the low-dimensional space will retain
    the semantics, or meaning, of the higher dimensions while being a more compact
    representation that can be used for classification tasks. The core challenge of
    creating an embedding is the unsupervised learning task of modeling the semantic
    meaning embedded in highly dimensional unstructured or semi-structured datasets.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果嵌入构建得当，低维空间将保留高维的语义或意义，同时成为一个更紧凑的表示，可用于分类任务。创建嵌入的核心挑战是建模高度维度的非结构化或半结构化数据集中嵌入的语义意义，这是一个无监督学习任务。
- en: Humans are quite adept at constructing low-dimensional representations of concepts,
    as we do this intuitively whenever we assign labels to objects or phenomena that
    are similar in broad strokes but may vary in the details. We do this when we label
    movies as comedy, science fiction, or horror; when we talk about categories of
    music like hip-hop, pop, or rock and roll; or when we create taxonomies of foods,
    animals, or illnesses. In *Chapter 9*, *Finding Groups of Data – Clustering with
    k-means*, we saw how the machine learning process of clustering can mimic this
    human process of labeling by grouping diverse but similar items through a process
    of “unsupervised classification.” However, even though this approach reduces the
    dimensionality of a dataset, it requires a structured dataset with the same specific
    features for each example before it can associate like-with-like. For something
    unstructured like a textual description of a movie, the features are too numerous
    and sparse to allow clustering.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 人类在构建概念的低维表示方面非常擅长，因为我们总是在为粗略上相似但在细节上可能有所不同的对象或现象分配标签时直觉地这样做。当我们给电影贴上喜剧、科幻或恐怖的标签时；当我们谈论像嘻哈、流行或摇滚这样的音乐类别时；或者当我们创建食物、动物或疾病的分类法时，我们就是这样做的。在
    *第9章*，*寻找数据组 - 使用 k-means 进行聚类* 中，我们看到了机器学习过程中的聚类如何通过“无监督分类”的过程模仿人类标签化过程，通过将不同但相似的项目分组。然而，尽管这种方法减少了数据集的维度，但在它能够将类似的项目关联起来之前，它需要一个具有每个示例相同特定特征的具有结构的数据集。对于像电影文本描述这样的非结构化事物，特征太多且稀疏，无法进行聚类。
- en: Instead, what if we wanted to mimic the human ability to learn via association?
    Specifically, a human can watch a series of movies and associate like-with-like
    without having specific measurable features for each movie; we can classify one
    set of movies as horror films without needing to see the identical clichéd storyline
    or to count the number of screams each film elicited. The trick is that a human
    doesn’t need a concrete definition of “horror” because we intuit it as a concept
    relative to the others in a set. Just like a cat suddenly jumping into frame can
    be used as slapstick humor in a comedy movie or paired with suspenseful music
    to provoke a jump scare, the semantic meaning of horror is always determined by
    its context.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 反之，如果我们想模仿人类通过联想学习的能力呢？具体来说，人类可以观看一系列电影，并将类似的电影进行分类，而不需要为每部电影提供具体的可测量特征；我们可以将一组电影归类为恐怖片，而无需看到相同的陈词滥调的故事情节或计算每部电影引发的尖叫次数。诀窍在于人类不需要“恐怖”的明确定义，因为我们将其作为一个相对于集合中其他元素的概念来直观地理解。就像一只猫突然跳入画面可以在喜剧电影中用作滑稽幽默，或者与悬疑音乐搭配来引发惊吓一样，恐怖的语义含义总是由其上下文决定的。
- en: In much the same way, learning algorithms can construct embeddings via context.
    Each of the thousands of movies that Hollywood has produced can be understood
    relative to others, and without studying exactly what features the movies *Halloween*
    and *Night of the Living Dead* have in common, an algorithm can observe that they
    appear in similar contexts and are somewhat substitutable for one another across
    contexts. This notion of substitutability is the basis for most embedding algorithms,
    and has indeed been used to construct embeddings for use in movie recommendation
    algorithms and other domains. In the next section, we’ll see how a popular language
    embedding algorithm uses substitutability to discover the semantic meanings of
    words.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，学习算法可以通过上下文来构建嵌入。好莱坞制作的成千上万部电影都可以相对于其他电影来理解，而且无需研究电影《万圣节》和《活死人之夜》有哪些共同特征，算法可以观察到它们出现在相似上下文中，并且在不同的上下文中可以相互替代。这种可替代性的概念是大多数嵌入算法的基础，并且确实被用来构建用于电影推荐算法和其他领域的嵌入。在下一节中，我们将看到一种流行的语言嵌入算法是如何使用可替代性来发现词语的语义含义的。
- en: Understanding word embeddings
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解词嵌入
- en: If there are roughly a million words in the English language, the feature space
    for a language-based model would be roughly a million dimensions wide even before
    phrases and word order are considered! This clearly would be far too large and
    sparse for most conventional learning algorithms to find a meaningful signal.
    A bag-of-words approach, as described in *Chapter 4*, *Probabilistic Learning
    – Classification Using Naive Bayes*, might work with enough computing power, but
    it would also require a tremendous amount of training data to associate words
    with the desired outcome. What if, instead, we could use a language embedding
    that has been pre-trained on big data?
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果英语中大约有一百万个单词，那么基于语言模型的特征空间在考虑短语和词序之前就已经有大约一百万维！这显然对于大多数传统学习算法来说太大且稀疏，以至于无法找到有意义的信号。正如在第4章“概率学习——使用朴素贝叶斯进行分类”中描述的，词袋方法在足够的计算能力下可能可行，但它也需要大量的训练数据来将单词与期望的结果关联起来。那么，如果我们能够使用在大数据上预训练的语言嵌入会怎样呢？
- en: 'To illustrate the upside of this alternative approach, let’s imagine the machine
    learning task of deciding whether to display an advertisement for a lunchtime
    café to users posting on a social media website. Consider the following posts
    made by hypothetical users:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这种替代方法的优点，让我们设想一个机器学习任务：决定是否向在社交媒体网站上发帖的用户展示午餐咖啡馆的广告。考虑以下由假设用户发表的帖子：
- en: I ate bacon and eggs in the morning for the most important meal of the day!
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我在早上吃了培根和鸡蛋，这是一天中最重要的餐食！
- en: I am going to grab a quick sandwich this afternoon before hitting the gym.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我下午要去健身房之前，打算快速吃个三明治。
- en: Can anyone provide restaurant recommendations for my date tonight?
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谁能为我今晚的约会推荐一些餐厅？
- en: For a naive Bayes approach, we would first need many of these types of sentences,
    but because the algorithm is a supervised learner, we would also need a target
    feature that indicates whether or not the user writing the sentence is interested
    in purchasing lunch from the café. We could then train the model to recognize
    which words are predictive of buying lunch.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: In comparison, a human reading these sentences can easily form a reasonable
    guess as to which of the three users is most likely to be interested in buying
    lunch today. The human’s guess is not based on being trained to predict lunch
    buying behavior specifically, but rather is based on an understanding of the embedded
    meaning in each of the sentences’ words. In other words, because a human understands
    the meaning of the users’ words, it becomes unnecessary to guess their behavior
    as we can instead just listen to what they are telling us they plan to do.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: The most effective language models do more than merely look at the meaning of
    words; they also look at the meaning of words in relation to others. The use of
    grammar and phrasing can completely change the implications of a sentence. For
    example, the sentence “I skipped breakfast today, so I can stuff myself at lunch”
    is very different from “I stuffed myself at breakfast, so I need to skip lunch
    today” despite containing almost exactly the same words!
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'Ignoring for now how this might be constructed, suppose we have a very simple
    language embedding that captures the meaning of all English-language words in
    two dimensions: a “lunch” dimension that measures how related a term is to lunch,
    and a “food” dimension that indicates whether the term is related to food. In
    this model, the semantic meaning that was once delivered by unique and specific
    terms like “soup” and “salad” is instead represented by the position of these
    concepts in the 2-D space, as illustrated for a selection of words in *Figure
    15.5*:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, receipt, screenshot  Description automatically
    generated](img/B17290_15_05.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.5: A very simple embedding reduces the highly dimensional meaning
    of various words into two dimensions that a machine can use to understand the
    subjective concepts of “food” and “lunch”'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'The embedding itself is a mapping of a word to coordinates in a lower-dimensional
    space. Thus, a lookup function can provide the values for a specific word. For
    instance, using the 2-D word embedding above, we can obtain coordinates for a
    selection of terms that may have appeared in social media posts:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '*f(sandwich)* = *(0.97, 0.54)*'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*f(bacon)* = *(-0.88, 0.75)*'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*f(apple)* = *(0.63, 0.25)*'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*f(orange)* = *(-0.38, 0.13)*'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Terms with higher values of the first dimension are more specifically related
    to lunch (and lunch alone) while lower values indicate terms that are specifically
    not related to lunch. For example, the word “sandwich” has a high lunch value
    while “bacon” has a low lunch value, due to their close association with lunch
    and breakfast, respectively. In much the same way, terms with higher or lower
    values of the second dimension are more or less likely to be foods. The words
    “orange” and “apple” can both be foods, but the former can also represent a color
    while the latter can represent a computer, so they are near the middle of the
    food dimension. In contrast, the words “bacon” and “sandwich” are higher in this
    dimension but are lower than “burrito” or “spaghetti” due to their meanings outside
    of the culinary context; someone can “bring home the bacon” (that is, they can
    earn money) or an item can be “sandwiched” between other items.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'An interesting and useful property of this type of embedding is that words
    can be related to one another via simple mathematics and nearest-neighbor-style
    distance calculations. In the 2-D plot, we can observe this property by examining
    the terms that are mirror images across the horizontal or vertical axis or those
    that are close neighbors. This leads to observations such as:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: An apple is a more lunch-related version of an orange
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beef is like chicken, but not as associated with lunch
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pitas and tacos are somewhat similar, as are kebabs and sandwiches
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Soup and salad are closely related and are the lunch versions of eggs and pasta
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heavy and light are opposites with respect to lunch, as are afternoon and evening
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The term “brown bag” is lunch-ish like “apple” but less food-ish
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although this is a simple, contrived example, the word embeddings developed
    using big data have similar mathematical properties—albeit with a much higher
    number of dimensions. As you will soon see, these additional dimensions allow
    additional aspects of word meaning to be modeled, and enrich the embedding far
    beyond the “lunch” and “food” dimensions illustrated so far.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Example – using word2vec for understanding text in R
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The previous sections introduced the idea of embedding as a means of encoding
    a highly dimensional concept in a lower-dimensional space. We’ve also learned
    that, conceptually, the process involves training a computer to learn about the
    substitutability of various terms by applying a human-like process of learning
    by association. But so far, we haven’t explored the algorithm that performs this
    feat. There are several such methods, which have been developed by big data companies
    or research universities and shared with the public.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps one of the most widely used word embedding techniques is **word2vec**,
    which was published in 2013 by a team of researchers at Google and, as the name
    suggests, literally transforms words to vectors. According to the authors, it
    is not a single algorithm so much as it is a collection of methods that can be
    used for natural language processing tasks. Although there have been many new
    methods published in the time since word2vec was published, it remains popular
    and is well studied even today. Understanding the full scope of word2vec is outside
    the scope of this chapter, but understanding some of its key components will provide
    a foundation upon which many other natural language processing techniques can
    be understood.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: For a deep dive into the word2vec approach, see *Efficient Estimation of Word
    Representations in Vector Space by Mikolov, T., Chen, K., Corrado, G., and Dean,
    J., 2013* at [https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781).
    Another early but widely used approach for word embeddings is called the **GloVe
    algorithm**, which was published in 2014 by a team at Stanford University and
    uses a similar set of methods. For more information on GloVe, see [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Consider a computer attempting to learn from reading a large corpus of text
    such as a web page or textbook. To begin learning which words are associated and
    are substitutable for one another, the computer will need a formal definition
    of “context” to limit the scope to something more reasonable than the entire text,
    particularly if the text is large. To this end, the word2vec technique defines
    a **window size** parameter that dictates how many words of context will be used
    when attempting to understand a single word. A smaller window size guarantees
    a tight association between words in context, but because related words can appear
    much later in the sentence, making the window too small may lead to missing important
    relationships among words and ideas. Balance is required, because making the window
    too large can pull in unrelated ideas much earlier or later in the text. Typically,
    the window is set to approximately the length of a sentence, or about five to
    ten words, with useless stop words like “and,” “but,” and “the” excluded.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Given contexts comprising approximately sentence-length sets of words, the word2vec
    process proceeds with one of two methodologies. The **continuous bag-of-words**
    (**CBOW**) methodology trains a model to predict each word from its context; the
    **skip-gram** approach does the inverse and attempts to guess the surrounding
    contextual words when provided with a single input word. Although the underlying
    process is nearly identical for both approaches, there are mathematical nuances
    that lead to different results depending on which one is used.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Because we are merely understanding the methods conceptually, it suffices to
    say that the CBOW methodology tends to create embeddings favoring words that are
    nearly identical replacements or true synonyms for one another, such as “apple”
    and “apples” or “burger” and “hamburger,” while the skip-gram method favors terms
    that are conceptually similar, like “apple” and “fruit” or “burger” and “fries.”
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'For both CBOW and skip-gram, the process of developing the embedding is similar
    and can be understood as follows. Beginning from a sentence like “an apple is
    a fruit I eat for lunch,” a model is constructed that attempts to relate a word
    like “apple” to its context, like “fruit,” “eat,” and “lunch.” By iterating over
    huge volumes of such sentences—like “a banana is a fruit people eat for breakfast”
    or “an orange is both a fruit and a color” and so on—the values of the embedding
    can be determined, such that the embedding minimizes the prediction error between
    the word and its context. Words that appear consistently in similar contexts will
    thus have similar values for the embedding and can therefore be treated as similar,
    interchangeable concepts:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17290_15_06.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.6: The word2vec process creates an embedding that relates each term
    to its context'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Technically speaking, the word2vec approach is not considered “deep learning”
    even though it is in many ways analogous to deep learning. As depicted in the
    figure that follows, the embedding itself can be imagined as a hidden layer in
    a neural network, here represented with four nodes. In the CBOW approach, the
    input layer is a one-hot encoding of the input term, with one node for each possible
    word in the vocabulary, but only a single node with a value of 1 and the remaining
    nodes set to 0 values. The output layer also has one node per term in the vocabulary
    but can have multiple “1” values—each representing a word appearing in the context
    of the input term.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that for the skip-gram approach, this arrangement would be reversed:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram, schematic  Description automatically generated](img/B17290_15_07.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.7: Developing an embedding involves training a model in a process
    analogous to deep learning'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Varying the number of nodes in the hidden layer affects the complexity of the
    network as well as the depth of the model’s semantic understanding of each term.
    A greater number of nodes leads to a richer understanding of each term in its
    context but becomes much more computationally expensive to train and requires
    much more training data. Each additional node adds an additional dimension from
    which each term can be distinguished. Too few nodes and the model will have insufficient
    dimensionality to capture the many nuances of how each term can be used—the word
    “orange” as a color versus “orange” as a food, for instance—but using too many
    dimensions may increase the risk of the model being distracted by noise, or worse,
    being useless for the embedding’s initial intended purpose of dimensionality reduction!
    As you will soon see firsthand, even though the embeddings presented so far used
    just a few dimensions for simplicity and illustrative purposes, actual word embeddings
    used in practice typically have hundreds of dimensions and require huge amounts
    of training data and computational power to train.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: In R, installing the `word2vec` package by Jan Wijffels will provide a wrapper
    for the C++ implementation of the word2vec algorithm. If desired, the package
    can train a word embedding if given a corpus of text data, but it is often preferable
    to use pre-trained embeddings that can be downloaded from the web. Here, we’ll
    use an embedding that was trained using a Google News archive consisting of 100
    billion written words.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting embedding contains 300-dimensional vectors for 3 million words
    and simple phrases, and is available for download at the Google word2vec project
    page as follows: [https://code.google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/).
    To follow along with the example, look for the link to the `GoogleNews-vectors-negative300.bin.gz`
    file, then download, unzip, and save the file to your R project folder before
    proceeding.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: As a word of warning, the Google News embedding is quite large at about 1.5
    GB compressed (3.4 GB after unzipping) and unfortunately cannot be distributed
    with the code for this chapter. Furthermore, the file can be somewhat hard to
    find on the project website. Try a find command (*Ctrl* + *F* or *Command* + *F*)
    in your web browser to search the page for the file name if needed. Depending
    on your platform, you may need an additional program to unzip files with the Gzip
    compression algorithm (`.gz` file extension).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the code that follows, to read the Google News embedding into R,
    we’ll load the `word2vec` package and use the `read.word2vec()` function. Ensure
    you have downloaded and installed the `word2vec` package and Google News embedding
    before attempting this step:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'If the embedding loaded correctly, the `str()` command will show details about
    this pre-trained model:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: As expected, the embedding has 300 dimensions for each of the 3 million terms.
    We can obtain these dimensions for a term (or terms) using `predict()` as a lookup
    function on the model object. The `type = "embedding"` parameter requests the
    embedding vector for the term, as opposed to the most similar terms, which will
    be demonstrated shortly.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we’ll request the word vectors for a few terms related to breakfast,
    lunch, and dinner:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The previous commands created matrices named `foods` and `meals`, with rows
    reflecting the terms and columns representing the 300 dimensions of the embedding.
    We can examine the first few values of a single word vector for *cereal* as follows:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Alternatively, we can examine the first few columns for all foods:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Although we have no idea what each of the five dimensions represents (nor any
    of the remaining 295 dimensions not shown), we would expect similar, more substitutable
    foods and concepts to be closer neighbors in the 300-dimensional space. We can
    take advantage of this to measure the relatedness of the foods to the three main
    meals of the day using the `word2vec_similarity()` function as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: In this output, higher values indicate greater similarity between the foods
    and each of the three mealtimes, according to the 300-dimension word embedding.
    Unsurprisingly, breakfast foods like cereal, bacon, and eggs are closer to the
    word *breakfast* than they are to *lunch* or *dinner*. Sandwiches and salads are
    closest to lunch, while steak and spaghetti are closest to dinner.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Although it was not used in the previous example, it is a popular convention
    to use the **cosine similarity** measure, which considers only the direction of
    the compared vectors, rather than the default Euclidean distance-like measure,
    which considers both direction and magnitude. The cosine similarity can be obtained
    by specifying `type = "cosine"` when calling the `word2vec_similarity()` function.
    Here, it is not likely to substantially affect the results because the Google
    News vectors were normalized when they were loaded into R.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'For a more practical application of word2vec concepts, let’s revisit the hypothetical
    social media posts presented earlier and attempt to determine whether to present
    the users with a breakfast, lunch, or dinner advertisement. We’ll start by creating
    a `user_posts` character vector, which stores the raw text of each of the posts:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Importantly, there is a substantial hurdle we must pass before applying word2vec
    to each of the user posts; specifically, each post is a sentence composed of multiple
    terms, and word2vec is only designed to return vectors for single words. Unfortunately,
    there is no perfect solution to this problem, and choosing the correct solution
    may depend on the desired use case. For instance, if the application is intended
    merely to identify people that post about a particular subject, it may suffice
    to iterate over each word in the post and determine whether any of the words meet
    a similarity threshold.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: More complex alternative solutions exist for solving the problem of applying
    word2vec to longer strings of text. A common but somewhat crude solution involves
    simply averaging the word2vec vectors across all words in the sentence, but this
    often results in poor results for much the same reason that mixing too many colors
    of paint results in an ugly shade of brown. As sentences grow longer, averaging
    across all words creates a muddy mess due to the fact that some words will inevitably
    have vectors in opposite directions and the resulting average is meaningless.
    Moreover, as sentences grow in complexity, it is more likely that word order and
    grammar will affect the meaning of the words in the sentence.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: An approach called doc2vec attempts to address this by adapting the training
    of word2vec to longer blocks of text, called documents, which need not be full
    documents but may be paragraphs or sentences. The premise of doc2vec is to create
    an embedding for each document based on the words appearing in the document. Document
    vectors can then be compared to determine the overall similarity between two documents.
    In our case, the goal would be to compare whether two documents (that is, sentences)
    are conveying similar ideas—for instance, is a user’s post like other sentences
    that were about breakfast, lunch, or dinner?
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, we do not have access to a doc2vec model to use this more sophisticated
    approach, but we can apply the `word2vec` package’s `doc2vec()` function to create
    a document vector for each user post and treat the document vector as if it were
    a single word. As stated previously, for longer sentences this may create a muddied
    vector, but because social media posts are often short and to the point, this
    issue may be mitigated.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll begin by loading the `tm` package, which was introduced in *Chapter 4*,
    *Probabilistic Learning – Classification Using Naive Bayes*, as a collection of
    tools for processing text data. The package provides a `stopwords()` function,
    which can be combined with its `removeWords()` function to remove unhelpful terms
    from social media posts. Then, the `txt_clean_word2vec()` function is used to
    prepare the posts for use with `doc2vec`:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'To see the result of this processing, let’s look at the first cleaned user
    post:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'As expected, the text has been standardized and all unhelpful words have been
    removed. We can then supply the posts to the `doc2vec()` function, along with
    the pre-trained Google News word2vec model as follows:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The result of this operation is a matrix with three rows (one for each document)
    and 300 columns (one for each dimension in the embedding). The `str()` command
    shows the first few values of this matrix:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We’ll need to compare these pseudo-document vectors to the word vectors for
    breakfast, lunch, and dinner. These vectors were created previously using the
    `predict()` function and the word2vec model, but the code is repeated here for
    clarity:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Finally, we can compute the similarity between the two. Each row represents
    a user’s post, and the column values indicate the similarity between that post’s
    document vector and the corresponding term:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Unsurprisingly, the user post about bacon and eggs is most similar to the word
    breakfast, while the post with sandwiches is most similar to lunch, and the evening
    date is most related to dinner. We could use the maximum similarity per row to
    determine whether to display a breakfast, lunch, or dinner advertisement to each
    user.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Document vectors can also be used directly as predictors in supervised machine
    learning tasks. For example, *Chapter 14*, *Building Better Learners*, described
    a theoretical model for predicting a Twitter user’s gender or future purchasing
    behavior based on the user’s basic profile data, profile picture, and social media
    post text.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'The chapter proposed ensembling a traditional machine learning model with a
    deep learning model for the image data and a naive Bayes text model for the user
    posts. Alternatively, it is possible to use document vectors as is by treating
    the 300 dimensions as 300 individual predictors that the supervised learning algorithm
    can use to determine which are relevant to predicting the user’s gender:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, device, gauge  Description automatically generated](img/B17290_15_08.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.8: The values for a document vector resulting from unstructured text
    data can be used in a predictive model side by side with the more conventional
    predictors'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: This strategy of creating a document vector for an unstructured block of text
    and using the resultant embedding values as predictors for supervised learning
    is quite generalizable as a means of enhancing the performance of a conventional
    machine learning approach. Many datasets include unstructured text fields that
    go unused in conventional models due to their complexity or the inability to train
    a language model. However, a relatively simple transformation made possible by
    a pre-trained word embedding allows the text data to be used in the model alongside
    the other predictors. Thus, there is little excuse not to incorporate this approach
    and provide the learning algorithm with an infusion of big data the next time
    you encounter this type of machine learning task.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing highly dimensional data
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data exploration is one of the five key steps involved in any machine learning
    project, and thus is not immune to the so-called curse of dimensionality—the tendency
    of a project to become increasingly challenging as the number of features increases.
    Visualization techniques that work on simpler datasets may become useless as the
    number of dimensions grows unmanageable; for example, a scatterplot matrix may
    help identify relationships for a dozen or so features, but as the number grows
    bigger to dozens or hundreds of features, then what was once a helpful visualization
    may quickly turn into information overload.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Likewise, we can interpret a 2-D or even a three-dimensional plot without too
    much difficulty, but if we hope to understand the relationship among four or more
    dimensions, an entirely different approach is needed.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Though physics suggests there are ten or eleven dimensions of the universe,
    we only experience four, and only interact directly with three of them. Perhaps
    for this reason, our brains are attuned to understanding visuals in at most three
    dimensions; moreover, because most of our intellectual work is on 2-D surfaces
    like blackboards, whiteboards, paper, or computer screens, we are accustomed to
    seeing data represented in at most two dimensions. One day, as virtual or augmented
    reality computer interfaces become more prevalent, we may see an explosion of
    innovation in three-dimensional visualizations, but until that day comes, there
    is a need for tools that can aid the display of highly dimensional relationships
    in no more than two dimensions.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'Reducing the dimensionality of a highly dimensional visualization to just two
    dimensions may seem like an impossibility, but the premise guiding the process
    is surprisingly straightforward: points that are closely positioned in the highly
    dimensional space need to be positioned closely in the 2-D space. If you are thinking
    that this idea sounds somewhat familiar, you would not be wrong; this is the same
    concept that guides embeddings, as described earlier in this chapter. The key
    difference is that while an embedding technique like word2vec reduces highly dimensional
    data down to a few hundred dimensions, embeddings for visualization must reduce
    the dimensionality even further to only two dimensions.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: The limitations of using PCA for big data visualization
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Principal component analysis** (**PCA**), which was introduced in *Chapter
    13*, *Challenging Data – Too Much, Too Little, Too Complex*, is one approach capable
    of reducing a highly dimensional dataset to two dimensions. You may recall that
    PCA works by expressing the covariance of multiple correlated attributes as a
    single vector. In this way, from the larger set of features, a smaller number
    of new features, called components, can be synthesized. If the number of components
    is set to two, a high-dimensional dataset can then be visualized in a simple scatterplot.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll apply this visualization technique to the 36-dimension social media profile
    dataset first introduced in *Chapter 9*, *Finding Groups of Data – Clustering
    with k-means*. The first few steps are straightforward; we use the tidyverse to
    read the data and select the 36 columns of interest, set the random seed to `123456`
    to ensure your results match the book, then use the `prcomp_irlba()` function
    from the `irlba` package to find the two principal components of the dataset:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The `sns_pca$x` object contains a transformed version of the original dataset
    in which the 36 original dimensions have been reduced to 2\. Because this is stored
    as a matrix, we’ll first convert it to a data frame before piping it into a `ggplot()`
    function to create a scatterplot:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The resulting visualization appears as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_15_09.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.9: Principal component analysis (PCA) can be used to create 2-D visualizations
    of highly dimensional datasets, but the results are not always especially helpful'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, this scatterplot reveals a limitation of using PCA for data exploration,
    which is that the two principal components often create little visual separation
    among the points in 2-D space. Based on our prior work in *Chapter 9*, *Finding
    Groups of Data – Clustering with k-means*, we know that there are clusters of
    social media users that use similar keywords on their social media profiles. These
    clusters ought to be visible as distinct groupings in the scatterplot, but instead,
    we see one large group of points and a scattering of apparent outliers around
    the perimeter. The disappointing result here is not specific to the dataset used
    here and is typical of PCA when used in this way. Thankfully, there is another
    algorithm that is better suited to data exploration, which will be introduced
    in the next section.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the t-SNE algorithm
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The underlying math of the PCA technique utilizes covariance matrices to perform
    a linear dimensionality reduction, and the resulting principal components are
    intended to capture the overall variance of the dataset. The effect is like a
    compression algorithm that reduces the dimensionality of a dataset by eliminating
    redundant information. While this is obviously an important and useful attribute
    for a dimensionality reduction technique, it is less helpful for data visualization.
    As we observed in the previous section, this tendency of PCA to “compress” the
    dimensions may obscure important relationships in the data—the exact type of relationships
    we hope to discover when performing big data exploration.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: A technique called **t-Distributed Stochastic Neighbor Embedding**, or **t-SNE**
    for short, is designed precisely as a tool for the visualization of high-dimensional
    datasets and thus addresses the previously mentioned shortcomings of PCA. The
    t-SNE approach was published in 2008 by Laurens van der Maaten, and it has quickly
    become a de facto standard for big data visualization for high-dimensional real-world
    datasets. Van der Maaten and others have published and presented numerous case
    studies contrasting PCA and t-SNE, and illustrating the strengths of the latter.
    However, because the math that drives t-SNE is highly complex, we will focus on
    understanding it conceptually and comparing it to other related methods covered
    previously.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: For a deep dive into the mechanics of the t-SNE algorithm, see the original
    publication, *Visualizing Data using t-SNE, van der Maaten, L. and Hinton, G.,
    Journal of Machine Learning Research 9, 2008, pp. 2579-2606*.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Just like with any technique for visualizing highly dimensional datasets, the
    goal of t-Distributed Stochastic Neighbor Embedding is to ensure that points or
    “neighbors” that are close in the high-dimensional space are positioned closely
    in the low-dimensional (2-D or 3-D) space.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: The word *embedding* in the t-SNE name highlights the close connection between
    this and the more general task of constructing an embedding, as described in prior
    sections. However, as will be apparent shortly, t-SNE uses an approach unlike
    the deep learning analogue that is used for creating a word embedding. For starters,
    the word *stochastic* in the t-SNE name describes the non-deterministic nature
    of the algorithm, which implies that there is a relatively large degree of randomness
    in the output. But there are also more fundamental differences.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: To begin to understand the t-SNE algorithm, imagine if the task were merely
    to reduce from three dimensions to two. In this case, if the data points were
    somehow depicted as small balls suspended in the air in three-dimensional space,
    and the same number of data points were placed randomly as flat discs on the ground
    in 2-D space, then a human could perform the dimensionality reduction by observing
    each ball in 3-D space, identifying its set of neighbors, and then carefully moving
    the discs in 2-D space to place neighbors closer together. Of course, this is
    more challenging than it sounds, because moving discs closer together and further
    apart in the flat space may inadvertently create or eliminate groupings relative
    to the 3-D space. For instance, moving point A to be closer to its neighbor point
    B may also move A closer to point C, when A and C should be distant according
    to the higher-dimensional space. For this reason, it would be important to iterate,
    observing each 3-D point’s neighborhood and shifting its 2-D neighbors until the
    overall 2-D representation is relatively stable.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: The same basic process can be performed algorithmically in a much larger number
    of dimensions using a series of mathematical steps. First, the similarity of each
    point in high-dimensional space is computed—traditionally, using the familiar
    metric of Euclidian distance as with k-means and k-nearest neighbors in earlier
    chapters. This similarity metric is used to define a conditional probability distribution
    stating that similar points are proportionally more probable to be neighbors in
    the high-dimensional space. Likewise, a similar distance metric and conditional
    probability distribution is defined for the low-dimensional space. With these
    two metrics defined, the algorithm must then optimize the entire system such that
    the overall error for the high- and low-dimensional probability distributions
    is minimized. Keep in mind that the two are inseparably linked by the fact they
    rely on the same set of examples; the coordinates are known for the high-dimensional
    space, so it is essentially solving for a way to transform the high-dimensional
    coordinates into a low-dimensional space while preserving the similarity as much
    as possible.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that the t-SNE algorithm is so different than PCA, it is no surprise
    that there are many differences in how they perform. An overall comparison of
    the two approaches is presented in the following table:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '| **PCA** | **t-SNE** |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Tends to compress the visualization
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Global (overall) variance is depicted
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deterministic algorithm will produce the same result each run
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does not have hyperparameters to be set
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relatively fast (for datasets that can fit in memory)
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Involves linear transformations
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Useful as a general dimensionality reduction technique by creating additional
    principal components
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Tends to cluster the visualization
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Local variance is more apparent
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stochastic algorithm introduces randomness into the result
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Result can be sensitive to hyperparameters
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relatively slow (but faster approximations exist)
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Involves non-linear transformations
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typically used only as a data visualization technique (two or three dimensions)
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: As a rule of thumb, t-SNE is generally the more appropriate tool for big data
    visualization, but it is worth noting a few differences that can be weaknesses
    or present challenges in certain circumstances. First, we have observed that PCA
    can do a poor job at depicting natural clusters in the data, but t-SNE is so apt
    at presenting clusters that it can occasionally even form clusters in a dataset
    without these types of natural divisions. This fault is compounded by the fact
    that t-SNE is a non-deterministic algorithm that is often quite sensitive to the
    values of its hyperparameters; setting these parameters poorly is more likely
    to create false clusters or obscure real ones. Lastly, the t-SNE algorithm involves
    iterating repeatedly over a relatively slow process, but stopping too early often
    produces a poor result or creates a false sense of the dataset’s structure; unfortunately,
    it is also possible that too many iterations will lead to the same problems!
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: These challenges are not listed here to imply that t-SNE is more work than it
    is worth, but rather to encourage treating the output with a degree of skepticism
    until it has been thoroughly explored. This may mean testing various hyperparameter
    combinations, or it may involve a qualitative examination of the visualization,
    such as investigating the identified clusters by hand in order to determine what
    features the neighborhood has in common. We’ll see some of these potential pitfalls
    in practice in the next section, which applies t-SNE to a familiar real-world
    dataset.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Example – visualizing data’s natural clusters with t-SNE
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To illustrate the ability of t-SNE to depict a dataset’s natural clusters,
    we’ll apply the method to the same 36-dimensional social media profile dataset
    used previously with PCA. Beginning as before, we’ll read the raw data into R
    using the tidyverse, but because t-SNE is somewhat computationally expensive,
    we use the `slice_sample()` command to limit the dataset to a random sample of
    5,000 users. This is not strictly necessary but will speed up the execution time
    and make the visualization less dense and thus easier to read. Don’t forget to
    use the `set.seed(123)` command to ensure your results match those that follow:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Even with a relatively small sample, the standard t-SNE implementation can still
    be rather slow. Instead, we will use a faster version called the **Barnes-Hut
    implementation**. The Barnes-Hut algorithm was originally developed to simulate
    the so-called “*n*-body” problem—the complex system of gravitational relationships
    that arises among a set of *n* celestial bodies. Because every object exerts a
    force on every other object, exactly computing the net force for each body requires
    *n* *× n = n*² calculations. This becomes computationally infeasible at an astronomical
    scale due to the scope of the universe and the virtually limitless numbers of
    objects within. Barnes-Hut simplifies this problem using a heuristic that treats
    more distant objects as a group identified by its center of mass, and only performs
    the exact calculations for objects closer than a threshold represented by the
    Greek letter *theta*. Larger values of theta drastically reduce the number of
    calculations needed to perform the simulation, while setting theta to zero performs
    the exact calculation.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: Because the role of t-SNE can be imagined as an *n*-body problem of positioning
    points in space, with each point’s force of attraction to other points in the
    2-D space based on how similar it is to the same points in the high-dimensional
    space, the Barnes-Hut simplification can be applied to simplify the computation
    of the system’s gravity-like forces. This provides a t-SNE implementation that
    is much faster and scales much better on large datasets.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: The `Rtsne` package, which you should install if you have not done so already,
    provides a wrapper for the C++ implementation of Barnes-Hut t-SNE. It also includes
    other optimizations for use with very large-dimensional datasets. One of these
    optimizations includes an initial PCA step, which by default reduces the dataset
    to its first 50 principal components.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Admittedly, it may seem odd to use PCA as part of the t-SNE process, but the
    two have complementary strengths and weaknesses. While t-SNE tends to struggle
    with the curse of dimensionality, PCA is strong at dimensionality reduction; likewise,
    while PCA tends to obscure local variance, t-SNE highlights the data’s natural
    structures. Using PCA to reduce the dimensionality and following this with the
    t-SNE process applies both techniques’ strengths. In our case, with a dataset
    having only 36 dimensions, the PCA step does not meaningfully affect the result.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll begin by running a t-SNE process with the default parameters. After setting
    the random seed, the 5,000-row sample is piped into a `select()` command to choose
    only the 36 columns that measure the counts of various terms used on each user’s
    profile. This is then piped into the `Rtsne()` function with `check_duplicates
    = FALSE` to prevent an error message that occurs when the dataset has duplicate
    rows. Duplicate rows are found in the social media dataset chiefly because there
    are many users who have counts of zero for all 36 terms. There is no reason that
    the t-SNE method cannot handle these duplicates, but including them may lead to
    unexpected or unsightly results in the visualization when the algorithm attempts
    to arrange such a tightly clustered set of points. For social media users, seeing
    this cluster will be helpful, so we will override the `Rtsne()` function’s default
    as follows:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Piping a dataset into the `distinct()` function will eliminate duplicate rows
    and can be used prior to the `Rtsne()` command.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: 'The 2-D representation of the 36-dimensional dataset is stored as a matrix
    named `Y` in the `sns_tsne` list object created by the `Rtsne()` function. This
    has 5,000 rows representing the social media users, and two columns representing
    the (*x*, *y*) coordinates of each user. After converting the matrix to a data
    frame, we can pipe these values into a `ggplot()` function to visualize the t-SNE
    result as follows:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Displayed side by side with the earlier PCA visualization, it’s remarkable
    to see the vast improvement in visual clarity that the t-SNE technique provides.
    Distinct clusters of users can be observed, reflecting these users’ similarities
    in the 36-dimensional space:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_15_10.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.10: Compared to PCA, the t-SNE technique tends to create more useful
    visualizations that depict the data’s natural clusters'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Of course, it is somewhat unusual for a t-SNE visualization to work as nicely
    as this one did on the first try. If your results are disappointing, it is possible
    that merely setting a different random seed will generate better-looking results
    due to t-SNE’s use of randomization. Additionally, the `perplexity` and `max_iter`
    parameters of the `Rtsne()` function can be adjusted to affect the size and density
    of the resulting plot. The perplexity governs the number of nearest neighbors
    to consider during the adjustment from high-to-low dimensions, and changing the
    maximum number of iterations (`max_iter`) up or down may lead the algorithm to
    arrive at a completely different solution.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, there are very few rules of thumb for tuning these parameters,
    and thus it often requires some trial and error to get things just right. The
    creator of t-SNE, Laurens van der Maaten, offers a few words of wisdom:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: …one could say that a larger / denser dataset requires a larger perplexity.
    Typical values for the perplexity range between 5 and 50… [seeing a “ball” with
    uniformly distributed points] usually indicates you set your perplexity way too
    high. [If you continue to see bad results after tuning] maybe there is not very
    much nice structure in your data in the first place.
  id: totrans-323
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-324
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Source: [https://lvdmaaten.github.io/tsne/](https://lvdmaaten.github.io/tsne/)'
  id: totrans-325
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Be warned that the `Rtsne()` function parameters like `perplexity` and `max_iter`
    can drastically affect the amount of time it takes for the t-SNE algorithm to
    converge. If you’re not careful, you may need to force the process to quit rather
    than wait endlessly. Setting `verbose = TRUE` in the `Rtsne()` function call may
    provide insight into how the work has progressed.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: For an outstanding treatment of t-SNE’s parameters and hyperparameters with
    interactive visualizations that show the impact of adjustments to each, see *How
    to Use t-SNE Effectively, Wattenberg, M., Viégas, F., and Johnson, I., 2016*,
    `https://distill.pub/2016/misread-tsne/`.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: Because t-SNE is an unsupervised method, aside from the remarkably large and
    round cluster in the top right of the visualization—which we can reasonably assume
    is composed of identical users with no social media keywords in their profile—we
    have no idea what the other clusters represent. This being said, it is possible
    to probe the data to investigate the clusters by labeling points with different
    colors or shapes based on their underlying values.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we can confirm the hypothesis about the top-right cluster by creating
    a categorical measure of how many keywords were used on each user’s page. The
    following tidyverse code begins by using `bind_cols()` to append the t-SNE coordinates
    onto the original dataset. Next, it uses the `rowwise()` function to change the
    behavior of `dplyr` so that the commands work on rows rather than columns. Thus,
    we can use the `sum()` function to count the number of terms each user had on
    their profile, using `c_across()` to select the columns with word counts. After
    using `ungroup()` to remove the rowwise behavior, this count is transformed into
    a two-outcome categorical variable using the `if_else()` function:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Using the result of this series of steps, we’ll again plot the t-SNE data,
    but change the shape and color of the points according to the number of terms
    used:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The resulting figure confirms our assumption, as the users with zero terms
    used in their social media profile (denoted by circles) comprise the dense cluster
    in the top right of the figure, while the users with one or more terms used (denoted
    by triangles) are scattered elsewhere in the plot:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_15_11.png)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.11: Adding color or changing the point style can help understand
    the clusters depicted in the t-SNE visualization'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: The t-SNE technique is more than just a tool to make pretty pictures, although
    it does tend to also do that well! For one, it may be helpful for determining
    the value of *k* to be used for k-means clustering. The t-SNE technique can also
    be used after clustering has been performed, with the points colored according
    to their cluster assignments to illustrate the clusters for presentation purposes.
    Stakeholders are more likely to trust a model with results that can be seen in
    a PowerPoint presentation. Similarly, t-SNE can be used to qualitatively gauge
    the performance of an embedding such as word2vec; if the embedding is meaningful,
    plotting the 300-dimensional vectors in 2-D space will reveal clusters of words
    with related meanings. With so many useful applications of t-SNE, it is no wonder
    that it has quickly become a popular tool in the data science toolkit.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 'For a fun application using both word2vec and t-SNE in which computers learned
    the meaning of emoji, see *emoji2vec: Learning Emoji Representations from their
    Description, Eisner, B., Rocktäschel, T., Augenstein, I., Bošnjak, M., and Riedel,
    S., 2016, in Proceedings of the 4th International Workshop on Natural Language
    Processing for Social Media at EMNLP 2016*.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: While tools like word2vec and t-SNE provide means for understanding big data,
    they are of no use if R is unable to handle the workload. The remainder of this
    chapter will equip you with additional tools for loading, processing, and modeling
    such large data sources.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: Adapting R to handle large datasets
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although the phrase “big data” means more than just the number of rows or the
    amount of memory a dataset consumes, sometimes working with a large volume of
    data can be a challenge in itself. Large datasets can cause computers to freeze
    or slow to a crawl when system memory runs out, or models cannot be built in a
    reasonable amount of time. Many real-world datasets are very large even if they
    are not truly “big,” and thus you are likely to encounter some of these issues
    on future projects. In doing so, you may find that the task of turning data into
    action is more difficult than it first appeared.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: Thankfully, there are packages that make it easier to work with large datasets
    even while remaining in the R environment. We’ll begin by looking at the functionality
    that allows R to connect to databases and work with datasets that may exceed available
    system memory, as well as packages allowing R to work in parallel, and some that
    utilize modern machine learning frameworks in the cloud.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: Querying data in SQL databases
  id: totrans-342
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large datasets are often stored in a **database management system** (**DBMS**)
    such as Oracle, MySQL, PostgreSQL, Microsoft SQL, or SQLite. These systems allow
    the datasets to be accessed using the **Structured Query Language** (**SQL**),
    a programming language designed to pull data from databases.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: The tidy approach to managing database connections
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'RStudio version 1.1, which was released in 2017, introduced a graphical approach
    for connecting to databases. The **Connections** tab in the top-right portion
    of the interface provides the ability to interact with database connections found
    on your system. Upon clicking the **New Connection** button within this interface
    tab, you will see a window with the available connection options. The following
    screenshot depicts some of the possible connection types, but your own system
    is likely to have a different selection than those shown here:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_15_12.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.12: The “New Connection” button in RStudio v1.1 or greater opens
    an interface that will assist you with connecting to any predefined data sources'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: The creation of these connections is typically performed by a database administrator
    and is specific to the type of database as well as the operating system. For instance,
    on Microsoft Windows, you may need to install the appropriate database drivers
    as well as use the ODBC Data Source Administrator application; on macOS and Unix/Linux,
    you may need to install the drivers and edit an `odbc.ini` file. Complete documentation
    about the potential connection types and installation instructions is available
    at [https://solutions.posit.co/connections/db/](https://solutions.posit.co/connections/db/).
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: Behind the scenes, the graphical interface uses a variety of R packages to manage
    the connections to these data sources. At the core of this functionality is the
    `DBI` package, which provides a tidyverse-compliant front-end interface to the
    database. The `DBI` package also manages the back-end database driver, which must
    be provided by another R package. Such packages let R connect to Oracle (`ROracle`),
    MySQL (`RMySQL`), PostgreSQL (`RPostgreSQL`), and SQLite (`RSQLite`), among many
    others.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate this functionality, we’ll use the `DBI` and `RSQLite` packages
    to connect to a SQLite database containing the credit dataset used previously.
    SQLite is a simple database that doesn’t require running a server. It simply connects
    to a database file on a machine, which here is named `credit.sqlite3`. Before
    starting, be sure you’ve installed both required packages and saved the database
    file into your R working directory. After doing this, you can connect to the database
    using the following command:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'To prove the connection has succeeded, we can list the database tables to confirm
    the credit table exists as expected:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'From here, we can send SQL query commands to the database and return records
    as R data frames. For instance, to return the loan applicants with an age of 45
    years or greater, we would query the database as follows:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The entire result set can be fetched as a data frame using the following command:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'To confirm that it worked, we’ll examine the summary statistics, which confirm
    that the ages begin at 45 years:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'When our work is done, it is advisable to clear the query result set and close
    the database connection to free these resources:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: In addition to SQLite and the database-specific R packages, the `odbc` package
    allows R to connect to many different types of databases using a single protocol
    known as the **Open Database Connectivity** (**ODBC**) standard. The ODBC standard
    can be used regardless of operating system or DBMS.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have previously connected to an ODBC database, you may have referred
    to it via its **data source name** (**DSN**). You can use the DSN to create a
    database connection with a single line of R code:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'If you have a more complicated setup, or want to specify the connection properties
    manually, you can specify a full connection string as arguments to the DBI package
    `dbConnect()` function as follows:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: With the connection established, queries can be sent to the ODBC database and
    tables can be returned as data frames using the same functions that were used
    for the SQLite example previously.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: Due to security and firewall settings, the instructions for configuring an ODBC
    network connection are highly specific to each situation. If you are having trouble
    setting up the connection, check with your database administrator. The Posit team
    (formerly known as RStudio) also provides helpful information at [https://solutions.posit.co/connections/db/best-practices/drivers/](https://solutions.posit.co/connections/db/best-practices/drivers/).
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: Using a database backend for dplyr with dbplyr
  id: totrans-371
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using the tidyverse’s `dplyr` functions with an external database is no more
    difficult than using it with a traditional data frame. The `dbplyr` package (short
    for “database plyr”) allows any database supported by the `DBI` package to be
    used transparently as a backend for `dplyr`. The connection allows tibble objects
    to be pulled from the database. Generally, one does not need to do more than merely
    install the `dbplyr` package, and `dplyr` can then take advantage of its functionality.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s connect to the SQLite `credit.sqlite3` database used previously,
    then save its `credit` table as a tibble object using the `tbl()` function as
    follows:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Because `dplyr` has been routed through a database, the `credit_tbl` object
    here is not stored as a local R object, but rather is a table within a database.
    In spite of this, `credit_tbl` will act exactly like an ordinary tibble and will
    gain all the other benefits of the `dplyr` package, with the exception that the
    computational work will occur within the database rather than in R. This means
    that if the SQLite database were replaced with a database residing across a network
    on a more traditional SQL server, work could be offloaded to machines with more
    computational power rather than being performed on your local machine.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to query the database and display the age summary statistics for
    credit applicants that are at least 45 years old, we can pipe the tibble through
    the following sequence of functions:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The result is as follows:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Note that the `dbplyr` functions are “lazy,” which means that no work is done
    in the database until it is necessary. Thus, the `collect()` function forces `dplyr`
    to retrieve the results from the “server” (in this case, a SQLite instance, but
    more typically a powerful database server) so that the summary statistics may
    be calculated. If the `collect()` statement is omitted, the code will fail as
    the `summary()` function cannot work directly with the database connection object.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a database connection, most `dplyr` commands will be translated seamlessly
    into SQL on the backend. To see how this works, we can ask `dbplyr` to show the
    SQL code that is generated for a series of `dplyr` steps. Let’s build a slightly
    more complex sequence of commands to show the average loan amount after filtering
    for ages 45 and older, and grouping by loan default status:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The output shows that those that defaulted tended to request larger loan amounts
    on average:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Note that this looks different from a normal `dplyr` output and includes information
    about the database used, since the work was performed in the database rather than
    R. To see the SQL code that was generated to perform this analysis, simply pipe
    the steps into the `show_query()` function:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The output shows the SQL query that was run on the SQLite database:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Using the `dbplyr` functionality, the same R code that is used on smaller data
    frames can also be used to prepare larger datasets stored in SQL databases—the
    heavy lifting is done on the remote server, rather than your local laptop or desktop
    machine. In this way, learning the tidyverse suite of packages ensures your code
    will apply to any type of project from small to massive. Of course, there are
    even more ways to enable R to work with large datasets, as you will see in the
    sections that follow.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: Doing work faster with parallel processing
  id: totrans-390
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the early days of computing, computer processors always executed instructions
    in **serial**, which meant that they were limited to performing a single task
    at a time. In serial computing, the next instruction cannot be started until the
    previous instruction is complete:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17290_15_13.png)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.13: In serial computing, tasks cannot begin until prior tasks have
    been completed'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: 'Although it was widely known that many tasks could be completed more efficiently
    by completing steps simultaneously, the technology simply did not exist. This
    was addressed by the development of **parallel computing** methods, which use
    a set of two or more processors or computers to perform tasks simultaneously:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17290_15_14.png)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.14: Parallel computing allows several tasks to occur simultaneously,
    which can speed up processing, but the results must be combined at the end'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: Many modern computers are designed for parallel computing. Even in the case
    that they have a single processor, they often have two or more cores that work
    in parallel. A core is essentially a processor within a processor, which allows
    computations to occur even if the other cores are busy with another task.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: Networks of multiple computers called **clusters** can also be used for parallel
    computing. A large cluster may include a variety of hardware and be separated
    over large distances. In this case, the cluster is known as a **grid**. Taken
    to an extreme, a cluster or grid of hundreds or thousands of computers running
    commodity hardware could be a very powerful system. Cloud computing systems like
    Amazon Web Services (AWS) and Microsoft Azure make it easier than ever to use
    clusters for data science projects.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: The catch, however, is that not every problem can be parallelized. Certain problems
    are more conducive to parallel execution than others. One might expect that adding
    100 processors would result in 100 times the work being accomplished in the same
    amount of time (that is, the overall execution time would be 1/100), but this
    is typically not the case. The reason is that it takes effort to manage the workers.
    Work must be divided into equal, non-overlapping tasks, and each of the workers’
    results must be combined into one final answer.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: So-called **embarrassingly parallel** problems are the ideal. These tasks are
    easy to reduce into non-overlapping blocks of work, and the results are easy to
    recombine. An example of an embarrassingly parallel machine learning task would
    be 10-fold cross-validation; once the 10 samples are divided, each of the 10 blocks
    of work is independent, meaning that they do not affect the others. As you will
    soon see, this task can be sped up quite dramatically using parallel computing.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: Measuring R’s execution time
  id: totrans-401
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Efforts to speed up R will be wasted if it is not possible to systematically
    measure how much time was saved. Although a stopwatch is one option, an easier
    solution is to wrap the offending code in a `system.time()` function.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, on the author’s laptop, the `system.time()` function notes that
    it takes about 0.026 seconds to generate a million random numbers:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: The same function can be used to evaluate improvement in performance, obtained
    with the methods that were just described or any R function.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: For what it’s worth, when the first edition of this book was published, generating
    a million random numbers took 0.130 seconds; the same took about 0.093 seconds
    for the second edition and 0.067 seconds for the third edition. Here, it takes
    only 0.026 seconds. Although I’ve used a slightly more powerful computer each
    time, this reduction of about 80 percent of the processing time over the course
    of about ten years illustrates just how quickly computer hardware and software
    are improving!
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: Enabling parallel processing in R
  id: totrans-408
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `parallel` package, included with R version 2.14.0 and later, has lowered
    the entry barrier to deploying parallel algorithms by providing a standard framework
    for setting up worker processes that can complete tasks simultaneously. It does
    this by including components of the `multicore` and `snow` packages, which each
    take a different approach to multitasking.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: 'If your computer is reasonably recent, you are likely to be able to use parallel
    processing. To determine the number of cores your machine has, use the `detectCores()`
    function as follows. Note that your output will differ depending on your hardware
    specifications:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: The `multicore` package was developed by Simon Urbanek and allows parallel processing
    on a single machine that has multiple processors or processor cores. It utilizes
    the multitasking capabilities of a computer’s operating system to **fork**, or
    create a copy of, additional R sessions that share the same memory, and is perhaps
    the simplest way to get started with parallel processing in R.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: Note that because the Microsoft Windows operating system does not support forking,
    the `multicore` example works only on macOS or Linux machines. For a Windows-ready
    solution, skip ahead to the next section on `foreach` and `doParallel`.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: 'An easy way to get started with the `multicore` functionality is to use the
    `mclapply()` function, which is a multicore version of `lapply()`. For instance,
    the following blocks of code illustrate how the task of generating 10 million
    random numbers can be divided across 1, 2, 4, and 8 cores. The `unlist()` function
    is used to combine the parallel results (a list) into a single vector after each
    core has completed its chunk of work:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Notice how as the number of cores increases, the elapsed time decreases, though
    the benefit tapers off and may even be detrimental once too many cores have been
    added. Though this is a simple example, it can be adapted easily to many other
    tasks.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: 'The `snow` package (Simple Network of Workstations) by Luke Tierney, A. J.
    Rossini, Na Li, and H. Sevcikova allows parallel computing on multicore or multiprocessor
    machines as well as on a network of multiple machines. It is slightly more difficult
    to use but offers much more power and flexibility. The `snow` functionality is
    included in the `parallel` package, so to set up a cluster on a single machine,
    use the `makeCluster()` function with the number of cores to be used:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Because `snow` communicates via network traffic, depending on your operating
    system, you may receive a message to approve access through your firewall.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: 'To confirm the cluster is operational, we can ask each node to report back
    its hostname. The `clusterCall()` function executes a function on each machine
    in the cluster. In this case, we’ll define a function that simply calls the `Sys.info()`
    function and returns the `nodename` parameter:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Unsurprisingly, since all four nodes are running on a single machine, they
    report back the same hostname. To have the four nodes run a different command,
    supply them with a unique parameter via the `clusterApply()` function. Here, we’ll
    supply each node with a different letter. Each node will then perform a simple
    function on its letter in parallel:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'When we’re done with the cluster, it’s important to terminate the processes
    it spawned. This will free up the resources each node is using:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Using these simple commands, it is possible to speed up many machine learning
    tasks. For the largest big data problems, much more complex `snow` configurations
    are possible. For instance, you may attempt to configure a **Beowulf cluster**—a
    network of many consumer-grade machines. In academic and industry research settings
    with dedicated computing clusters, `snow` can use the `Rmpi` package to access
    these high-performance **message-passing interface** (**MPI**) servers. Working
    with such clusters requires knowledge of network configurations and computing
    hardware outside the scope of this book.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: 'For a much more detailed introduction to `snow`, including some information
    on how to configure parallel computing on several computers over a network, see
    the following lecture by Luke Tierney: [http://homepage.stat.uiowa.edu/~luke/classes/295-hpc/notes/snow.pdf](http://homepage.stat.uiowa.edu/~luke/classes/295-hpc/notes/snow.pdf).'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: Taking advantage of parallel with foreach and doParallel
  id: totrans-431
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `foreach` package by Rich Calaway and Steve Weston provides perhaps the
    easiest way to get started with parallel computing, especially if you are running
    R on the Windows operating system, as some of the other packages are platform
    specific.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: The core of the package is a `foreach` looping construct. If you have worked
    with other programming languages, this may be familiar. Essentially, it allows
    looping over a set of items, without explicitly counting the number of items;
    in other words, *for each* item in the set, *do* something.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re thinking that R already provides a set of apply functions to loop
    over sets of items (for example, `apply()`, `lapply()`, `sapply()`, and so on),
    you are correct. However, the `foreach` loop has an additional benefit: iterations
    of the loop can be completed in parallel using a very simple syntax. Let’s see
    how this works.'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall the command we’ve been using to generate millions of random numbers.
    To make this more challenging, let’s increase the count to a hundred million,
    which causes the process to take about 2.5 seconds:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'After the `foreach` package has been installed, the same task can be expressed
    with a loop that combines four sets of 25,000,000 randomly generated numbers.
    The `.combine` parameter is an optional setting that tells `foreach` which function
    it should use to combine the final set of results from each loop iteration. In
    this case, since each iteration generates a set of random numbers, we simply use
    the `c()` concatenate function to create a single, combined vector:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  id: totrans-439
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: If you noticed that this function didn’t result in a speed improvement, good
    catch! In fact, the process was slower. The reason is that by default, the `foreach`
    package runs each loop iteration in serial, and the function adds a small amount
    of computational overhead to the process. The sister package `doParallel` provides
    a parallel backend for `foreach` that utilizes the `parallel` package included
    with R, described earlier in this chapter.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: 'Before parallelizing this work, it is wise to confirm the number of cores available
    on your system as follows:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: Your results will differ depending on your system capabilities.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, after installing and loading the `doParallel` package, simply register
    the desired number of cores and swap the `%do%` command with the `%dopar%` operator.
    Here, we only need at most four cores, as there are only four groups of random
    numbers to combine:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: As shown in the output, this results in a performance increase, cutting the
    execution time by about 40 percent.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: '**Warning**: if the `cores` parameter is set to a number greater than the available
    cores on your system, or if the combined work exceeds the free memory on your
    computer, R may crash! In this case, the vector of random numbers is nearly a
    gigabyte of data, so systems with low RAM may be especially prone to crashing
    here.'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: 'To close the `doParallel` cluster, simply type:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: Though the cluster will be closed automatically at the conclusion of the R session,
    it is better form to do so explicitly.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: Training and evaluating models in parallel with caret
  id: totrans-454
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `caret` package by Max Kuhn (covered previously in *Chapter 10*, *Evaluating
    Model Performance*, and *Chapter 14*, *Building Better Learners*) will transparently
    utilize a parallel backend if one has been registered with R using the `foreach`
    package described previously.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at a simple example in which we attempt to train a random forest
    model on the credit dataset. Without parallelization, the model takes about 65
    seconds to train:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'On the other hand, if we use the `doParallel` package to register eight cores
    to be used in parallel (be sure to lower this number if you have fewer than eight
    cores available), the model takes about 10 seconds to build—less than one-sixth
    of the time—and we didn’t need to change the remaining `caret` code:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '[PRE95]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: Many of the tasks involved in training and evaluating models, such as creating
    random samples and repeatedly testing predictions for 10-fold cross-validation,
    are embarrassingly parallel and ripe for performance improvements. With this in
    mind, it is wise to always register multiple cores before beginning a `caret`
    project.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: 'Configuration instructions and a case study of the performance improvements
    for enabling parallel processing in `caret` are available on the project’s website:
    [https://topepo.github.io/caret/parallel-processing.html](https://topepo.github.io/caret/parallel-processing.html).'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing specialized hardware and algorithms
  id: totrans-464
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Base R has a reputation for being slow and memory inefficient, a reputation
    that is at least somewhat earned. These faults are largely unnoticed on a modern
    PC for datasets of many thousands of records, but datasets with millions of records
    or more can exceed the limits of what is currently possible with consumer-grade
    hardware. The problem is worsened if the dataset contains many features or if
    complex learning algorithms are being used.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: CRAN has a high-performance computing task view that lists packages pushing
    the boundaries of what is possible in R at [http://cran.r-project.org/web/views/HighPerformanceComputing.html](http://cran.r-project.org/web/views/HighPerformanceComputing.html).
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: Packages that extend R past the capabilities of the base package are being developed
    rapidly. These packages allow R to work faster, perhaps by spreading the work
    over additional computers or processors, by utilizing specialized computer hardware,
    or by providing machine learning optimized to big data problems.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: Parallel computing with MapReduce concepts via Apache Spark
  id: totrans-468
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The **MapReduce** programming model was developed at Google to process its
    data on a large cluster of networked computers. MapReduce conceptualizes parallel
    programming as a two-step process:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: A **map** step, in which a problem is divided into smaller tasks that are distributed
    across the computers in the cluster
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **reduce** step, in which the results of the small chunks of work are collected
    and synthesized into a solution to the original problem
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A popular open-source alternative to the proprietary MapReduce framework is
    **Apache Hadoop**. The Hadoop software comprises the MapReduce concept plus a
    distributed filesystem capable of storing large amounts of data across a cluster
    of computers. Hadoop requires somewhat specialized programming skills to take
    advantage of its capabilities and to perform even basic machine learning tasks.
    Additionally, although Hadoop is excellent at working with extremely large amounts
    of data, it may not always be the fastest option because it keeps all data on
    disk rather than utilizing available memory.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: '**Apache Spark** is a cluster-computing framework for big data, offering solutions
    to these issues with Hadoop. Spark takes advantage of the cluster’s available
    memory to process data approximately 100x faster than Hadoop. Additionally, it
    provides easy-to-use libraries for many common data processing, analysis, and
    modeling tasks. These include the SparkSQL data query language, the MLlib machine
    learning library, GraphX for graph and network analysis, and the Spark Streaming
    library for processing real-time data streams. For these reasons, Spark is perhaps
    the current standard for open-source big data processing.'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: Packt Publishing has published many books on Spark. To search their current
    offerings, visit [https://subscription.packtpub.com/search?query=spark](https://subscription.packtpub.com/search?query=spark).
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark is often run remotely on a cloud-hosted cluster of virtual machines,
    but its benefits can also be seen running on your own hardware. In either case,
    the `sparklyr` package connects to the cluster and provides a `dplyr` interface
    for analyzing the data using Spark.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: More detailed instructions for using Spark with R can be found at [https://spark.rstudio.com](https://spark.rstudio.com),
    but the basic instructions for getting up and running are straightforward.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the fundamentals, let’s build a random forest model on the credit
    dataset to predict loan defaults. To begin, you’ll need to install and load the
    `sparklyr` package. Then, the first time you use Spark, you’ll need to run the
    `spark_install()` function, which downloads Spark onto your computer. Note that
    this is a sizeable download at about 220 megabytes, as it includes the full Spark
    environment:'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  id: totrans-478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Additionally, Spark itself requires a Java installation, which can be downloaded
    from `http://www.java.com` if you do not already have it. Once Spark and Java
    have been installed, you can instantiate a Spark cluster on your local machine
    using the following command:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  id: totrans-480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Next, we’ll load the loan dataset from the `credit.csv` file on our local machine
    into the Spark instance, then use the Spark function `sdf_random_split()` to randomly
    assign 75 and 25 percent of the data to the training and test sets, respectively.
    The `seed` parameter is the random seed to ensure the results are identical each
    time this code is run:'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  id: totrans-482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'Lastly, we’ll pipe the training data into the random forest model function,
    make predictions, and use the classification evaluator to compute the AUC on the
    test set:'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  id: totrans-484
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '[PRE100]'
  id: totrans-485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'We’ll then disconnect from the cluster:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  id: totrans-487
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: With just a few lines of R code, we’ve built a random forest model using Spark
    that could expand to model millions of records. If even more computing power is
    needed, the code can be run in the cloud using a massively parallel Spark cluster
    simply by pointing the `spark_connect()` function to the correct hostname. The
    code can also be easily adapted to other modeling approaches using one of the
    supervised learning functions in the Spark Machine Learning Library (MLlib) listed
    at [https://spark.rstudio.com/mlib/](https://spark.rstudio.com/mlib/).
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps the easiest way to get started using Spark is with Databricks, a cloud
    platform developed by the creators of Spark that makes it easy to manage and scale
    clusters via a web-based interface. The free “Community Edition” provides a small
    cluster for you to try tutorials or even experiment with your own data. Check
    it out at [https://databricks.com](https://databricks.com).
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: Learning via distributed and scalable algorithms with H2O
  id: totrans-490
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **H2O project** ([https://h2o.ai](https://h2o.ai)) is a big data framework
    that provides fast in-memory implementations of machine learning algorithms, which
    can also operate in a cluster-computing environment. It includes functions for
    many of the methods covered in this book, including Naive Bayes, regression, deep
    neural networks, k-means clustering, ensemble methods, and random forests, among
    many others.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: H2O uses heuristics to find approximate solutions to machine learning problems
    by iterating repeatedly over smaller chunks of the data. This gives the user the
    control to determine exactly how much of a massive dataset the learner should
    use. For some problems, a quick solution may be acceptable, but for others, the
    complete set may be required, which will require additional training time.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: H2O is usually substantially faster and performs better on very massive datasets
    relative to Spark’s machine learning functions, which are already much faster
    than base R. However, because Apache Spark is a commonly used cluster-computing
    and big data preparation environment, H2O can be run on Apache Spark using the
    **Sparkling Water** software. With Sparkling Water, data scientists have the best
    of both worlds—the benefits of Spark for data preparation, and the benefits of
    H2O for machine learning.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: The `h2o` package provides functionality for accessing an H2O instance from
    within the R environment. A full tutorial on H2O is outside the scope of this
    book, and documentation is available at `http://docs.h2o.ai`, but the basics are
    straightforward.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, be sure you have Java installed on your computer ([http://www.java.com](http://www.java.com))
    and install the `h2o` package in R. Then, initialize a local H2O instance using
    the following code:'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  id: totrans-496
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'This starts an H2O server on your computer, which can be viewed via H2O Flow
    at `http://localhost:54321`. The H2O Flow web application allows you to administer
    and send commands to the H2O server, or even build and evaluate models using a
    simple, browser-based interface:'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_15_15.png)'
  id: totrans-498
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.15: H2O Flow is a web application for interacting with the H2O instance'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: 'Although you could complete an analysis within this interface, let’s go back
    to R and use H2O on the loan default data that we examined previously. First,
    we need to upload the `credit.csv` dataset to this instance using the following
    command:'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  id: totrans-501
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: Note that the `.hex` extension is used to refer to an H2O data frame.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll apply H2O’s random forest implementation to this dataset using
    the following command:'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  id: totrans-504
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'The output of this command includes information on the out-of-bag estimates
    of model performance:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  id: totrans-506
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: Although the credit dataset used here is not very large, the H2O code used here
    would scale to datasets of almost any size. Additionally, the code would be virtually
    unchanged if it were to be run in the cloud—simply point the `h2o.init()` function
    to the remote host.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: GPU computing
  id: totrans-508
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An alternative to parallel processing uses a computer’s **graphics processing
    unit** (**GPU**) to increase the speed of mathematical calculations. A GPU is
    a specialized processor that is optimized for rapidly displaying images on a computer
    screen. Because a computer often needs to display complex 3D graphics (particularly
    for video games), many GPUs use hardware designed for parallel processing and
    extremely efficient matrix and vector calculations.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: 'A side benefit is that they can be used to efficiently solve certain types
    of mathematical problems. As depicted in the following illustration, where a typical
    laptop or desktop computer processor may have on the order of 16 cores, a typical
    GPU may have thousands or even tens of thousands:'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing shoji, building  Description automatically generated](img/B17290_15_16.png)'
  id: totrans-511
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.16: A graphics processing unit (GPU) has many times more cores than
    the typical central processing unit (CPU)'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: The downside of GPU computing is that it requires specific hardware that is
    not included with many computers. In most cases, a GPU from the manufacturer NVIDIA
    is required, as it provides a proprietary framework called **Complete Unified
    Device Architecture** (**CUDA**) that makes the GPU programmable using common
    languages such as C++.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: For more information on NVIDIA’s role in GPU computing, go to [https://www.nvidia.com/en-us/deep-learning-ai/solutions/machine-learning/](https://www.nvidia.com/en-us/deep-learning-ai/solutions/machine-learning/).
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
- en: The `gputools` package by Josh Buckner, Mark Seligman, and Justin Wilson implements
    several R functions, such as matrix operations, clustering, and regression modeling
    using the NVIDIA CUDA toolkit. The package requires a CUDA 1.3 or higher GPU and
    the installation of the NVIDIA CUDA toolkit. This package was once the standard
    approach for GPU computing in R, but appears to have gone without an update since
    2017 and has since been removed from the CRAN repository.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, it appears that GPU work has transitioned to the TensorFlow mathematical
    library. The RStudio team provides information about using a local or cloud GPU
    on the following pages:'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: '[https://tensorflow.rstudio.com/install/local_gpu](https://tensorflow.rstudio.com/install/local_gpu)'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://tensorflow.rstudio.com/install/cloud_server_gpu](https://tensorflow.rstudio.com/install/cloud_server_gpu)'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the time of publication, a typical GPU used for deep learning is priced at
    several hundred US dollars for entry-level models and around $1,000-$3,000 for
    moderately priced units with greater performance. High-end units may cost many
    thousands of dollars.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: Rather than spending this much up front, many people rent server time by the
    hour on cloud providers like AWS and Microsoft Azure, where it costs approximately
    $1 per hour for a minimal GPU instance—just don’t forget to shut it down when
    your work completes, as it can get expensive quite quickly!
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-521
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is certainly an exciting time to be studying machine learning. Ongoing work
    on the relatively uncharted frontiers of parallel and distributed computing offers
    great potential for tapping the knowledge found in the deluge of big data. And
    the burgeoning data science community is facilitated by the free and open-source
    R programming language, which provides a very low barrier to entry—you simply
    need to be willing to learn.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
- en: The topics you have learned, in both this chapter as well as previous chapters,
    provide the foundation for understanding more advanced machine learning methods.
    It is now your responsibility to keep learning and adding tools to your arsenal.
    Along the way, be sure to keep in mind the no free lunch theorem—no learning algorithm
    rules them all, and they all have varying strengths and weaknesses. For this reason,
    there will always be a human element to machine learning, adding subject-specific
    knowledge and the ability to match the appropriate algorithm to the task at hand.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
- en: In the coming years, it will be interesting to see how the human side changes
    as the line between machine learning and human learning blurs. Services such as
    Amazon’s Mechanical Turk provide crowd-sourced intelligence, offering a cluster
    of human minds ready to perform simple tasks at a moment’s notice. Perhaps one
    day, just as we have used computers to perform tasks that human beings cannot
    do easily, computers will employ human beings to do the reverse. What interesting
    food for thought!
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  id: totrans-525
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 4000 people at:'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/r](https://packt.link/r)'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/r.jpg)'
  id: totrans-528
  prefs: []
  type: TYPE_IMG
