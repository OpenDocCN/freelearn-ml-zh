- en: '15'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '15'
- en: Making Use of Big Data
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用大数据
- en: Although today’s most exciting machine learning research is found in the realm
    of big data—computer vision, natural language processing, autonomous vehicles,
    and so on—most business applications are much smaller scale, using what might
    be termed, at best, “*medium*” data. As noted in *Chapter 12*, *Advanced Data
    Preparation*, true big data work requires access to datasets and computing facilities
    generally found only at very large tech companies or research universities. Even
    then, the actual job of using these resources is often primarily a feat of data
    engineering, which simplifies the data greatly before its use in conventional
    business applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管今天最令人激动的机器学习研究是在大数据领域——计算机视觉、自然语言处理、自动驾驶汽车等——但大多数商业应用规模较小，使用的数据最多只能称为“*中等*”数据。正如*第12章*，*高级数据准备*中所述，真正的大数据工作需要访问数据集和计算设施，这些设施通常只有在非常大的科技公司或研究型大学才能找到。即便如此，使用这些资源的实际工作通常主要是数据工程的壮举，它在数据用于传统商业应用之前极大地简化了数据。
- en: 'The good news is that the headline-making research conducted at big data companies
    eventually trickles down and can be applied in simpler forms to more traditional
    machine learning tasks. This chapter covers a variety of approaches for making
    use of such big data methods in R. You will learn:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，大数据公司进行的引人注目的研究最终会逐渐渗透到更传统的机器学习任务中。本章涵盖了在R中使用这些大数据方法的多种方法。您将学习：
- en: How to borrow from the deep learning models developed at big data companies
    and apply them to conventional modeling tasks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何借鉴大数据公司开发的深度学习模型并将其应用于传统的建模任务
- en: Strategies for reducing the complexity of large and unstructured big data formats
    like text and images so that they can be used for prediction
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少大型和非结构化大数据格式（如文本和图像）的复杂性的策略，以便它们可用于预测
- en: Cutting-edge packages and approaches for accessing and modeling big datasets
    that may be too large to fit into memory
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问和建模大数据集的尖端包和途径，这些数据集可能太大而无法装入内存
- en: Despite R’s reputation for being ill equipped for big data projects, the efforts
    of the R community have gradually transformed it into a tool capable of tackling
    a surprising number of advanced tasks. The goal of this chapter is to demonstrate
    R’s ability to remain relevant in the era of deep learning and big data. Even
    though R is unlikely to be found at the heart of the biggest big data projects,
    and despite facing increasing competition from Python and cloud-based tools, R’s
    strengths keep it on the desktops of many practicing data scientists.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管R以其不适合大数据项目而闻名，但R社区的努力逐渐将其转变为一种能够处理大量高级任务的工具。本章的目标是展示R在深度学习和大数据时代保持相关性的能力。尽管R不太可能成为最大大数据项目的核心，并且尽管面临来自Python和基于云的工具的日益激烈的竞争，R的优势仍然使其保持在许多实践数据科学家的桌面上。
- en: Practical applications of deep learning
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习的实际应用
- en: Deep learning has received a great deal of attention lately due to its successes
    in tackling machine learning tasks that have been notoriously difficult to solve
    with conventional methods. Using sophisticated neural networks to teach computers
    to think more like a human has allowed machines to catch up with or even surpass
    human performance on many tasks that humans once held a seemingly insurmountable
    lead. Perhaps more importantly, even if humans still perform better at certain
    tasks, the upsides of machine learning—workers that never tire, never get bored,
    and require no salary—turn even imperfect automatons into useful tools for many
    tasks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习最近因其成功解决传统方法难以解决的机器学习任务而受到广泛关注。使用复杂的神经网络教会计算机更像人类思考，使得机器在许多人类曾经占据压倒性优势的任务上能够赶上甚至超越人类的表现。也许更重要的是，即使人类在某些任务上仍然表现更好，机器学习的优势——工人从不疲倦、从不厌倦，且无需工资——甚至将不完美的自动机变成了许多任务的实用工具。
- en: Unfortunately, for those of us working outside of large technology companies
    and research organizations, it is not always easy to take advantage of deep learning
    methods. Training a deep learning model generally requires not only state-of-the-art
    computing hardware but also large volumes of training data. In fact, as mentioned
    in *Chapter 12*, *Advanced Data Preparation*, most practical machine learning
    applications in the business setting are in the so-called small or medium data
    regimes, and here deep learning methods might perform no better and possibly even
    worse than conventional machine learning approaches like regression and decision
    trees. Thus, many organizations that are investing heavily in deep learning are
    doing so as a result of hype rather than true need.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，对于我们这些在大科技公司和研究机构外工作的人来说，利用深度学习方法并不总是那么容易。训练深度学习模型通常不仅需要最先进的计算硬件，还需要大量的训练数据。事实上，正如在第
    12 章 *《高级数据准备》* 中提到的，在商业环境中，大多数实际机器学习应用都是在所谓的少量或中等数据规模下进行的，而在这里，深度学习方法可能并不比传统的机器学习方法（如回归和决策树）表现得更好，甚至可能更差。因此，许多在深度学习上投入大量资金的组织，这样做更多的是因为炒作而非真正的需求。
- en: Even though some of the buzz around deep learning is surely based on its novelty
    as well as excitement from business leaders with visions of artificial intelligence
    replacing costly human workers, there are in fact practical applications of the
    technique that can be used in combination with, rather than as a replacement for,
    conventional machine learning methods. The purpose of this chapter is therefore
    not to provide a start-to-finish tutorial for building deep neural networks, but
    rather to show how deep learning’s successes can be incorporated into conventional
    machine learning projects including those outside the big data regime.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管围绕深度学习的一些炒作无疑是基于其新颖性以及商业领袖对人工智能取代昂贵的人工工人的愿景所带来的兴奋，但实际上，这项技术有一些实际应用，可以与传统的机器学习方法结合使用，而不是作为替代品。因此，本章的目的不是提供一个从头到尾的构建深度神经网络的教程，而是展示如何将深度学习的成功融入传统的机器学习项目中，包括那些在大数据环境之外的项目。
- en: 'Packt Publishing offers numerous resources on deep learning, such as *Hands-On
    Deep Learning with R: A practical guide to designing, building, and improving
    neural network models using R* (2020) by M. Pawlus and R. Devine, *Advanced Deep
    Learning with R* (2019) by B. Rai, and *Deep Learning with R Cookbook* (2020)
    by S. Gupta, R. A. Ansari, and D. Sarkar.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Packt Publishing 提供了关于深度学习的众多资源，例如 M. Pawlus 和 R. Devine 所著的 *《使用 R 进行深度学习实战：设计、构建和改进神经网络模型的实际指南》（2020
    年）*，B. Rai 所著的 *《使用 R 进行高级深度学习》（2019 年）*，以及 S. Gupta、R. A. Ansari 和 D. Sarkar
    所著的 *《深度学习 R 烹饪书》（2020 年）*。
- en: Beginning with deep learning
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从深度学习开始
- en: In recent years, with a new cohort of data science practitioners having been
    trained in the age of deep learning, a form of “generation gap” has developed
    in the machine learning community. Prior to the development of deep learning,
    the field was staffed primarily by those who were trained in statistics or computer
    science. Especially in the earliest years, machine learning practitioners carried
    with them the metaphorical baggage of their prior domain, and the software and
    algorithms they used fell into camps along party lines. Statisticians typically
    preferred regression-based techniques and software like R, whereas computer scientists
    favored iterative and heuristic-based algorithms like decision trees written in
    languages like Python and Java. Deep learning has blurred the line between these
    camps, and the next generation of data scientists may seem somewhat foreign to
    the prior generations as if they speak a different language.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，随着新一代在深度学习时代受过训练的数据科学实践者的出现，机器学习社区中形成了一种“代沟”。在深度学习发展之前，该领域主要由受过统计学或计算机科学培训的人士组成。特别是在最初几年，机器学习从业者带着他们先前领域的隐喻性负担，他们所使用的软件和算法沿着党派路线分成了阵营。统计学家通常更喜欢基于回归的技术和软件，如
    R，而计算机科学家则更喜欢用 Python 和 Java 等语言编写的迭代和启发式算法，如决策树。深度学习模糊了这些阵营之间的界限，下一代数据科学家可能对前辈们显得有些陌生，就像他们说的是另一种语言。
- en: 'The rift seems to have come out of nowhere, despite being able to see its origins
    clearly with the benefit of hindsight. As the famous author Ernest Hemingway once
    wrote, it happened “gradually, then suddenly.” Just as machine learning itself
    was only possible as the result of the simultaneous evolution of computing power,
    statistical methods, and available data, it makes sense that the next big evolutionary
    leap would arise out of a series of smaller evolutions in each of the same three
    components. Recalling the similar image presented in *Chapter 1*, *Introducing
    Machine Learning*, a revised cycle of advancement diagram depicting today’s state-of-the-art
    machine learning cycle illustrates the environment in which deep learning was
    developed:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然事后我们可以清楚地看到其起源，但这次分歧似乎是从无中来，正如著名作家欧内斯特·海明威曾经写的那样，“它逐渐发生，然后突然发生。” 正如机器学习本身只有在计算能力、统计方法和可用数据的同步演变下才成为可能一样，下一个大的进化飞跃从这三个相同组件的一系列较小进化中产生，这是有道理的。回忆一下*第一章*中提出的类似图像，*介绍机器学习*，一个修订后的进步周期图描绘了当今最先进的机器学习周期，它说明了深度学习发展的环境：
- en: '![](img/B17290_15_01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_15_01.png)'
- en: 'Figure 15.1: A combination of factors in the cycle of advancement led to the
    development of deep learning'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.1：进步周期中的多种因素共同导致了深度学习的发展
- en: It is no surprise that deep learning arose out of the big data era, while also
    being provided the requisite computing hardware—**graphics processing units**
    (**GPUs**) and cloud-based parallel processing tools, which will be covered later
    in this chapter—necessary to process datasets that are both very long and very
    wide. What is less obvious, and therefore easy to take for granted, is the academic
    and research environment that was also necessary for this evolution. Without a
    strong data science community comprising researchers whose expertise spans both
    statistics and computer science, in addition to applied data scientists motivated
    to solve practical business problems on large and complex real-world datasets,
    it is unlikely that deep learning would have arisen as it has. Stated differently,
    the fact that data science now exists as a focused academic discipline has undoubtedly
    accelerated the cycle of advancement. To borrow an analogy from science fiction,
    the system is much like a robot that becomes self-aware and learns much more quickly
    now that it has learned how to learn!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习起源于大数据时代，同时得到了必要的计算硬件——**图形处理单元**（**GPUs**）和基于云的并行处理工具——这些将在本章后面进行介绍，用于处理既长又宽的数据集，这并不令人惊讶。但更不明显的是，这种演变也离不开必要的学术和研究环境。如果没有一个强大的数据科学社区，其中研究人员在统计学和计算机科学方面都有专业知识，再加上那些致力于解决大型和复杂真实世界数据集上实际业务问题的应用数据科学家，深度学习可能就不会以这种方式出现。换句话说，数据科学现在作为一个专门的学术学科存在，无疑加速了进步周期。借用科幻小说中的类比，系统现在就像一个机器人，它学会了如何学习，因此现在它变得自我意识，并且学习得更快！
- en: The rapid development of deep learning has contributed to the previously mentioned
    generation gap, but it is not the only factor. As you will soon learn, deep learning
    not only offers impressive performance on big data tasks, but it can also perform
    much like conventional learning methods on smaller tasks. This has led some to
    focus almost exclusively on the technique, much like earlier generations of machine
    learning practitioners focused exclusively on regression or decision trees. The
    fact that deep learning also utilizes specialized software tools and mathematical
    terminology to perform these tasks means that its practitioners are, in some cases,
    literally speaking another language to describe the same series of steps. As has
    been said many times before, “there is no free lunch” in the field of machine
    learning, so as you continue your machine learning journey, it is best to see
    it as one of many useful tools—and not the *only* tool for the job.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的快速发展导致了之前提到的代沟，但这并不是唯一因素。正如你很快就会学到的那样，深度学习不仅在大型数据任务上提供了令人印象深刻的性能，而且在较小任务上也能像传统学习方法一样执行。这导致一些人几乎完全专注于这项技术，就像早期机器学习实践者只专注于回归或决策树一样。深度学习还利用专门的软件工具和数学术语来执行这些任务，这意味着在某些情况下，其从业者实际上是在用另一种语言描述相同的步骤。正如以前多次说过的，“在机器学习领域没有免费的午餐”，因此在你继续机器学习之旅时，最好将其视为许多有用工具之一——而不是这项工作的*唯一*工具。
- en: The terminology used by deep learning practitioners, even for simpler methods
    like linear regression, includes phrases like “cost function,” “gradient descent,”
    and “optimization.” This is a good reminder that although deep learning can approximate
    regression and other machine learning methods, the means by which it finds the
    solution is completely different.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习实践者使用的术语，即使是像线性回归这样的简单方法，也包括“代价函数”、“梯度下降”和“优化”等短语。这是一个很好的提醒，尽管深度学习可以近似回归和其他机器学习方法，但它找到解决方案的方式是完全不同的。
- en: Choosing appropriate tasks for deep learning
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择适合深度学习的任务
- en: As mentioned in *Chapter 7*, *Black-Box Methods – Neural Networks and Support
    Vector Machines*, neural networks with at least one hidden layer can act as universal
    function approximators. Elaborating on this principle, one might say that given
    enough training data, a cleverly designed neural network can learn to mimic the
    output of any other function.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如同在*第七章*中提到的，*黑盒方法 – 神经网络和支持向量机*，至少包含一个隐藏层的神经网络可以作为通用函数逼近器。对此原理进行阐述，我们可能会说，在给定足够多的训练数据的情况下，一个设计巧妙的神经网络可以学会模仿任何其他函数的输出。
- en: This implies that the conventional learning methods covered throughout this
    book can likewise be approximated by well-designed neural networks. In fact, it
    is quite trivial to design a neural network that almost exactly matches linear
    or logistic regression, and with a bit more work it is possible to approximate
    techniques like k-nearest neighbors and naive Bayes. Given enough data, a neural
    network can get closer and closer to the performance of even the best tree-based
    algorithms like random forests or gradient boosting machines.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着本书中涵盖的传统学习方法同样可以被设计良好的神经网络所近似。事实上，设计一个几乎完全匹配线性或逻辑回归的神经网络是非常简单的，通过更多的工作，也有可能近似k-最近邻和朴素贝叶斯等技术。在数据足够的情况下，神经网络可以越来越接近甚至最好的基于树的算法，如随机森林或梯度提升机。
- en: Why not apply deep learning to every problem, then? Indeed, a neural network’s
    ability to mimic all other learning approaches appears to be a violation of the
    “no free lunch” theorem, which, put simply, suggests that there is no machine
    learning algorithm that can perform best across all potential modeling tasks.
    There are a couple of key reasons why the theorem remains safe despite deep learning’s
    magic. First, the ability of a neural network to approximate a function is related
    to how much training data it has. In the small data regime, conventional techniques
    can perform better, especially when combined with careful feature engineering.
    Second, to reduce the amount of data the neural network needs for training, the
    network must have a topology that facilitates its ability to learn the underlying
    function. Of course, if the person building the model knows what topology to use,
    then it may be preferable to use the simpler conventional model in the first place.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 那为什么不将深度学习应用于每一个问题呢？确实，神经网络模仿所有其他学习方法的特性似乎违反了“没有免费午餐”定理，这个定理简单来说就是指出没有一种机器学习算法可以在所有潜在的建模任务中表现最佳。有几个关键原因使得这个定理在深度学习的魔力下依然安全。首先，神经网络逼近函数的能力与其拥有的训练数据量相关。在小数据规模下，传统技术可以表现得更好，尤其是在与仔细的特征工程相结合时。其次，为了减少神经网络训练所需的数据量，网络必须具有便于学习潜在函数的拓扑结构。当然，如果构建模型的人知道应该使用哪种拓扑结构，那么最初使用更简单的传统模型可能更可取。
- en: People using deep learning for conventional learning tasks are likely to prefer
    the black box approach, which works in the big data regime. Big data, however,
    is not merely the presence of many rows of data, but also many features. Most
    conventional learning tasks, including those with many millions of rows of data,
    are in the medium data regime, where conventional learning algorithms still perform
    well. In this case, whether the neural network performs better will ultimately
    come down to the balance of overfitting and underfitting—a balance that is sometimes
    challenging to find with a neural network, as the method tends to somewhat easily
    overfit the training data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用深度学习进行传统学习任务的人们可能会倾向于选择黑盒方法，这种方法在大数据环境中效果显著。然而，大数据并不仅仅是数据行数的增加，还包括许多特征。大多数传统学习任务，包括那些拥有数百万行数据的情况，都处于中等数据规模，在这种规模下，传统学习算法仍然表现良好。在这种情况下，神经网络是否表现更好最终将取决于过拟合和欠拟合之间的平衡——这种平衡有时在神经网络中是难以找到的，因为该方法容易对训练数据进行过拟合。
- en: Perhaps for this reason, deep learning is not well suited for racking up wins
    in machine learning competitions. If you ask a Kaggle Grandmaster, they are likely
    to tell you that neural networks don’t work on standard, real-life problems, and
    on traditional supervised learning tasks, gradient boosting wins. One can also
    see proof of this by browsing the leaderboards and noting the absence of deep
    learning. Perhaps a clever team will use neural networks for feature engineering
    and blend the deep learning model with other models in an ensemble to boost their
    performance, but even this is rare. Deep learning’s strengths are elsewhere. As
    a rule of thumb, tree-based ensemble methods win on structured, tabular data,
    while neural networks win on unstructured data, like image, sound, and text.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 可能正因为如此，深度学习不适合在机器学习竞赛中取得胜利。如果你问一个Kaggle大师，他们可能会告诉你神经网络在标准、现实生活中的问题以及传统监督学习任务上不起作用，梯度提升法才是赢家。人们也可以通过浏览排行榜并注意深度学习的缺席来证明这一点。也许一个聪明的团队会使用神经网络进行特征工程，并将深度学习模型与其他模型结合成集成模型以提高性能，但这种做法很少见。深度学习的优势在于其他方面。一般来说，基于树的集成方法在结构化、表格数据上获胜，而神经网络在图像、声音和文本等非结构化数据上获胜。
- en: 'Reading recent news about research breakthroughs and technology startup companies,
    one is likely to encounter deep learning applications that utilize the method’s
    unique ability to solve unconventional tasks. In general, these unconventional
    learning tasks fall into one of three categories: computer vision, natural language
    processing, or tasks involving unusual data formats like repeated measurements
    over time or having an exceptionally large number of interrelated predictors.
    A selection of specific successes for each category is listed in the following
    table:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读关于研究突破和技术初创公司的最新新闻，可能会遇到利用该方法独特能力解决非常规任务的深度学习应用。一般来说，这些非常规学习任务可以分为三类：计算机视觉、自然语言处理或涉及时间重复测量或具有异常大量相互关联预测因子的不寻常数据格式。以下表格列出了每个类别的具体成功案例：
- en: '| **Challenging machine learning tasks** | **Deep learning successes** |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| **具有挑战性的机器学习任务** | **深度学习成功案例** |'
- en: '| Computer vision tasks involving classifying images found in still pictures
    or video data |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 涉及对静止图片或视频数据中图像进行分类的计算机视觉任务 |'
- en: Identifying faces in security camera footage
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在安全摄像头录像中识别人脸
- en: Categorizing plants or animals for ecological monitoring
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对植物或动物进行分类以进行生态监测
- en: Diagnosing medical images such as X-rays, MRI, or CT scans
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 诊断X射线、MRI或CT扫描等医疗图像
- en: Measuring the activity of athletes on the sporting field
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量运动员在运动场上的活动
- en: Autonomous driving
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动驾驶
- en: '|'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Natural language applications requiring an understanding of the meaning of
    words in context |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 需要理解上下文中词语含义的自然语言应用 |'
- en: Processing social media posts to filter out fake news or hate speech
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理社交媒体帖子以过滤掉假新闻或仇恨言论
- en: Monitoring Twitter or customer support emails for consumer sentiment or other
    marketing insights
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监测Twitter或客户支持电子邮件以了解消费者情绪或其他市场洞察
- en: Examining electronic health records for patients at risk of adverse outcomes
    or for eligibility for new treatments
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查处于不良结果风险中的患者或符合新治疗方案资格的患者的电子健康记录
- en: '|'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '|  |  |'
- en: '| Predictive analysis involving many repeated measurements or very large numbers
    of predictors |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 涉及许多重复测量或大量预测因子的预测分析 |'
- en: Predicting the price of goods or equities in open markets
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测公开市场上商品或股票的价格
- en: Estimating energy, resource, or healthcare utilization
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估算能源、资源或医疗保健利用
- en: Forecasting survival or other medical outcomes using insurance billing codes
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用保险账单代码预测生存或其他医疗结果
- en: '|'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Even though some people certainly do use deep learning for conventional learning
    problems, this chapter focuses only on tasks that cannot be solved via conventional
    modeling techniques. Deep learning is highly adept at tapping into the unstructured
    data types that characterize the big data era, such as images and text, which
    are extremely difficult to model with traditional approaches.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有些人确实使用深度学习来解决传统学习问题，但本章仅关注无法通过传统建模技术解决的问题。深度学习非常擅长挖掘大数据时代特征的数据类型，如图像和文本，这些数据类型用传统方法难以建模。
- en: Unlocking these capabilities requires the use of specialized software and specialized
    data structures, which you will learn about in the next section.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 要解锁这些功能，需要使用专门的软件和专门的数据结构，这些内容你将在下一节中学习。
- en: The TensorFlow and Keras deep learning frameworks
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TensorFlow 和 Keras 深度学习框架
- en: Perhaps no software tool has contributed as much to the rapid growth in deep
    learning as **TensorFlow** ([https://www.tensorflow.org](https://www.tensorflow.org)),
    an open-source mathematical library developed at Google for advanced machine learning.
    TensorFlow provides a computing interface using directed graphs that “flow” data
    structures through a sequence of mathematical operations.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 或许没有哪个软件工具像 **TensorFlow** ([https://www.tensorflow.org](https://www.tensorflow.org))
    这样对深度学习的快速发展做出了如此大的贡献，TensorFlow 是一个开源的数学库，由谷歌开发，用于高级机器学习。TensorFlow 提供了一个使用有向图进行数据结构“流动”的数学运算序列的计算接口。
- en: Packt Publishing offers many books on TensorFlow. To search the current offerings,
    visit [https://subscription.packtpub.com/search?query=tensorflow](https://subscription.packtpub.com/search?query=tensorflow).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Packt Publishing 提供了许多关于 TensorFlow 的书籍。要搜索当前提供的书籍，请访问 [https://subscription.packtpub.com/search?query=tensorflow](https://subscription.packtpub.com/search?query=tensorflow)。
- en: 'The fundamental TensorFlow data structure is unsurprisingly known as a **tensor**,
    which is an array of zero or more dimensions. Building upon the simplest 0-D and
    1-D tensors, which represent a single value and a sequence of values, respectively,
    adding additional dimensions allows more complex data structures to be represented.
    Note that because we typically analyze sets of structures, the first dimension
    is generally reserved to allow multiple objects to be stacked; the first dimension
    then refers to the batch or sample number for each structure. For example:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 的基本数据结构不出所料地被称为 **张量**，它是一个零个或多个维度的数组。在 0-D 和 1-D 张量（分别代表单个值和值序列）的基础上构建，增加额外的维度允许表示更复杂的数据结构。请注意，因为我们通常分析结构集，第一个维度通常保留以允许堆叠多个对象；第一个维度因此指的是每个结构的批次或样本编号。例如：
- en: 'A set of 1-D tensors, collecting feature values for a set of people, is a 2-D
    tensor analogous to a data frame in R: `[person_id, feature_values]`'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组一维张量，收集一组人的特征值，是一个二维张量，类似于 R 中的数据框：`[person_id, feature_values]`
- en: 'For measurements repeated over time, 2-D tensors can be stacked as a 3-D tensor:
    `[person_id, time_sequence, feature values]`'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于随时间重复测量的情况，二维张量可以堆叠成三维张量：`[person_id, time_sequence, feature values]`
- en: '2-D images are represented by a 4-D tensor, with the fourth dimension storing
    the color values for each pixel in the 2-D grid: `[image_id, row, column, color_values]`'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2-D 图像由一个 4-D 张量表示，第四维存储 2-D 网格中每个像素的颜色值：`[image_id, row, column, color_values]`
- en: 'Video or animated images are represented in 5-D with an additional time dimension:
    `[image_id, time_sequence, row, column, color_values]`'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视频或动画图像以 5 维表示，并增加一个时间维度：`[image_id, time_sequence, row, column, color_values]`
- en: Most tensors are rectangular matrices completely filled with numeric data, but
    more complex structures like ragged tensors and sparse tensors are available for
    use with text data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数张量是完全填充数字数据的矩形矩阵，但还有更复杂的结构，如稀疏张量和稀疏张量，可用于文本数据。
- en: For an in-depth look at TensorFlow’s tensor objects, the documentation is available
    at [https://www.tensorflow.org/guide/tensor](https://www.tensorflow.org/guide/tensor).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入了解 TensorFlow 的张量对象，文档可在 [https://www.tensorflow.org/guide/tensor](https://www.tensorflow.org/guide/tensor)
    查找。
- en: TensorFlow’s graph, which can be more specifically termed a **dataflow graph**,
    uses nodes connected by directional arrows known as edges to represent dependencies
    between data structures, mathematical operations on these data structures, and
    the output. The nodes represent mathematical operations and the edges represent
    tensors flowing between operations. The graph aids in the parallelization of the
    work, since it is clear what steps must be completed in sequence versus those
    that may be completed simultaneously.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 的图，更具体地可以称为 **数据流图**，使用由称为边的方向箭头连接的节点来表示数据结构之间的依赖关系，这些数据结构上的数学运算以及输出。节点代表数学运算，边代表在运算之间流动的张量。该图有助于并行化工作，因为它清楚地表明哪些步骤必须按顺序完成，哪些步骤可以同时完成。
- en: 'A dataflow graph can be visualized if desired, which produces something like
    the idealized graph depicted in *Figure 15.2*. Although this is a highly simplified
    representation and real-world TensorFlow graphs tend to be much more complex,
    the diagram here shows that after completing the first operation, the second and
    fourth operations can begin in parallel:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，可以可视化数据流图，它会产生类似于*图15.2*中描述的理想化图。尽管这是一个高度简化的表示，现实世界的TensorFlow图通常要复杂得多，但此图表明，在完成第一个操作后，第二个和第四个操作可以并行开始：
- en: '![](img/B17290_15_02.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_15_02.png)'
- en: 'Figure 15.2: A simplified representation of a TensorFlow graph'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.2：TensorFlow图的简化表示
- en: As tensors flow through the graph, they are transformed by the series of operations
    represented by the nodes. The steps are defined by the person building the diagram,
    with each step moving closer to the end goal of accomplishing some sort of mathematical
    task. Some steps in the flow graph may apply simple mathematical transformations
    like normalization, smoothing, or bucketing to the data; others may attempt to
    train a model by iterating repeatedly while monitoring a **loss function**, which
    measures the fit of the model’s predictions to the true values.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当张量在图中流动时，它们会被节点表示的操作序列所转换。步骤由构建图表的人定义，每个步骤都使目标更接近完成某种数学任务。流程图中的某些步骤可能对数据进行简单的数学转换，如归一化、平滑或分桶；其他步骤可能通过迭代重复训练模型，同时监控一个**损失函数**，该函数衡量模型预测与真实值之间的拟合度。
- en: R interfaces to TensorFlow have been developed by the team at RStudio. The `tensorflow`
    package provides access to the core API, while the `tfestimators` package provides
    access to higher-level machine learning functionality. Note that TensorFlow’s
    directed graph approach can be used to implement many different machine learning
    models, including some of those discussed in this book. However, to do so requires
    a thorough understanding of the matrix mathematics that defines each model, and
    thus is outside the scope of this text. For more information about these packages
    and RStudio’s ability to interface with TensorFlow, visit [https://tensorflow.rstudio.com](https://tensorflow.rstudio.com).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: R接口的TensorFlow是由RStudio团队开发的。`tensorflow`包提供了对核心API的访问，而`tfestimators`包提供了对高级机器学习功能的访问。请注意，TensorFlow的定向图方法可以用来实现许多不同的机器学习模型，包括本书中讨论的一些模型。然而，要这样做需要彻底理解定义每个模型的矩阵数学，因此超出了本文的范围。有关这些包和RStudio与TensorFlow接口能力的更多信息，请访问[https://tensorflow.rstudio.com](https://tensorflow.rstudio.com)。
- en: Because TensorFlow relies so heavily on complex mathematical operations that
    must be programmed carefully by hand, the **Keras** library ([https://keras.io](https://keras.io))
    was developed to provide a higher-level interface to TensorFlow and allow deep
    neural networks to be built more easily. Keras was developed in Python and is
    typically paired with TensorFlow as the back-end computing engine. Using Keras,
    it is possible to do deep learning in just a few lines of code—even for challenging
    applications such as image classification, as you will discover in the example
    later in this chapter.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由于TensorFlow严重依赖于必须由手编程的复杂数学运算，因此开发了**Keras**库([https://keras.io](https://keras.io))，以提供对TensorFlow的高级接口，并允许更轻松地构建深度神经网络。Keras是用Python开发的，通常与TensorFlow配对作为后端计算引擎。使用Keras，只需几行代码就可以进行深度学习——即使是像图像分类这样的挑战性应用，你将在本章后面的示例中看到。
- en: Packt Publishing offers numerous books and videos to learn Keras. To search
    current offerings, visit [https://subscription.packtpub.com/search?query=keras](https://subscription.packtpub.com/search?query=keras).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Packt Publishing提供了许多书籍和视频来学习Keras。要搜索当前提供的内容，请访问[https://subscription.packtpub.com/search?query=keras](https://subscription.packtpub.com/search?query=keras)。
- en: The `keras` package, developed by RStudio founder J. J. Allaire, allows R to
    interface with Keras. Although there is very little code required to use the package,
    developing useful deep learning models from scratch requires extensive knowledge
    of neural networks as well as familiarity with TensorFlow and the Keras API. For
    these reasons, a tutorial is outside the scope of this book. Instead, refer to
    the RStudio TensorFlow documentation or the book *Deep Learning with R* (2018),
    which was co-authored by Francois Chollet and J. J. Allaire—the creators of Keras
    and the `keras` package, respectively. Given their credentials, there is no better
    place to begin learning about this tool.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 由RStudio创始人J. J. Allaire开发的`keras`包允许R与Keras接口。尽管使用此包所需的代码非常少，但要从头开始开发有用的深度学习模型，需要广泛了解神经网络以及熟悉TensorFlow和Keras
    API。因此，教程超出了本书的范围。相反，请参阅RStudio TensorFlow文档或由Keras和`keras`包的创造者Francois Chollet和J.
    J. Allaire合著的书籍《深度学习与R》（2018年），这是开始学习这个工具的绝佳起点。
- en: Although the combination of Keras and TensorFlow is arguably the most popular
    deep learning toolkit, it is not the only tool for the task. The **PyTorch** framework
    developed at Facebook has rapidly gained popularity, especially in the academic
    research community, as an easy-to-use alternative. For more information, see [https://pytorch.org](https://pytorch.org).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Keras和TensorFlow的组合可能是最受欢迎的深度学习工具包，但这并不是唯一的工具。Facebook开发的**PyTorch**框架迅速获得了人气，尤其是在学术研究社区中，作为一个易于使用的替代品。更多信息，请参阅[https://pytorch.org](https://pytorch.org)。
- en: TensorFlow’s innovative idea to represent complex mathematical functions using
    a simple graph abstraction, combined with the Keras framework to make it easier
    to specify the network topology, has enabled the construction of deeper and more
    complex neural networks, such as those described in the next section. Keras even
    makes it easy to adapt pre-built neural networks to new tasks with no more than
    a few lines of code.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow使用简单的图抽象来表示复杂数学函数的创新理念，结合Keras框架使其更容易指定网络拓扑，这已经使得构建更深更复杂的神经网络成为可能，如下一节所述。Keras甚至使得仅用几行代码就能轻松地将预构建的神经网络适应新任务。
- en: Understanding convolutional neural networks
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解卷积神经网络
- en: Neural networks have been studied for over 60 years, and even though deep learning
    has only recently become widespread, the concept of a deep neural network has
    been known for decades. As first introduced in *Chapter 7*, *Black-Box Methods
    – Neural Networks and Support Vector Machines*, a **deep neural network** (**DNN**)
    is simply a neural network with more than one hidden layer.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络已经被研究超过60年，尽管深度学习最近才变得普遍，但深度神经网络的概念已经存在了几十年。正如在*第7章*，*黑盒方法 – 神经网络和支持向量机*中首次介绍的那样，**深度神经网络**（**DNN**）只是一个具有多个隐藏层的神经网络。
- en: This understates what deep learning is in practice, as typical DNNs are substantially
    more complex than the types of neural networks we’ve built previously. It’s not
    enough to add a few extra nodes in a new hidden layer and call it “deep learning.”
    Instead, typical DNNs use extremely complex but purposefully designed topologies
    to facilitate learning on big data, and in doing so are capable of human-like
    performance on challenging learning tasks.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这低估了深度学习在实践中是什么，因为典型的DNN比我们之前构建的神经网络类型要复杂得多。仅仅在新的隐藏层中添加几个额外的节点并称之为“深度学习”是不够的。相反，典型的DNN使用极其复杂但故意设计的拓扑结构来促进在大数据上的学习，并且在这个过程中能够在具有挑战性的学习任务上实现类似人类的性能。
- en: A turning point for deep learning came in 2012, when a team called SuperVision
    used deep learning to win the ImageNet Large Scale Visual Recognition Challenge.
    This annual competition tests the ability to classify a subset of 10 million hand-labeled
    images across 10,000 categories of objects. In the early years of the competition,
    humans vastly outperformed computers, but the performance of the SuperVision model
    closed the gap significantly. Today, computers are nearly as good as humans at
    visual classification, and, in some specific cases, are even better. Humans tend
    to be better at identifying small, thin, or distorted items, while computers have
    a greater ability to distinguish among specific types of items such as dog breeds.
    Before long, it is likely that computers will outperform humans on both types
    of visual tasks.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的一个转折点出现在2012年，当时一个名为SuperVision的团队使用深度学习赢得了ImageNet大规模视觉识别挑战赛。这项年度竞赛测试了分类100万张手标注图像的能力，这些图像分布在10,000个物体类别中。在竞赛的早期年份，人类在视觉分类方面远远优于计算机，但SuperVision模型的性能显著缩小了差距。如今，计算机在视觉分类方面几乎与人类一样好，在某些特定情况下甚至更好。人类在识别小、细或变形物品方面往往更擅长，而计算机在区分特定类型的物品，如狗的品种方面具有更大的能力。不久的将来，计算机很可能在两种视觉任务类型上都优于人类。
- en: An innovative network topology designed specifically for image recognition is
    responsible for the surge in performance. A **convolutional neural network** (**CNN**)
    is a deep feed-forward network used for visual tasks that independently learns
    the important distinguishing image features rather than requiring such feature
    engineering beforehand. For example, to classify road signs like “stop” or “yield,”
    a traditional learning algorithm would require pre-engineered features like the
    shape and color of the sign. In contrast, a CNN requires only the raw data for
    each of the image pixels, and the network will learn how to distinguish important
    features like shape and color on its own.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一种专门为图像识别设计的创新网络拓扑结构，是性能激增的原因。**卷积神经网络**（**CNN**）是一种用于视觉任务的深度前馈网络，它独立地学习重要的区分图像特征，而不是在事先需要这样的特征工程。例如，为了对“停止”或“让行”等路标进行分类，传统的学习算法需要预先设计好的特征，如标志的形状和颜色。相比之下，CNN只需要每个图像像素的原始数据，网络将自行学习如何区分形状和颜色等重要特征。
- en: 'Learning features like these is made possible due to the huge increase in dimensionality
    when using raw image data. A traditional learning algorithm would use one row
    per image, in a form like (*stop sign*, *red, octagon*), while a CNN uses data
    in the form (*stop sign*, *x*, *y*, *color*), where *x* and *y* are pixel coordinates
    and *color* is the color data for the given pixel. This may seem like an increase
    of only one dimension but note that even a very small image is made of many (*x*,
    *y*) combinations and color is often specified as a combination of RGB (*red*,
    *green*, *blue*) values. This means that a more accurate representation of a single
    row of training data would be:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 由于使用原始图像数据时维度的大幅增加，这些特征的提取成为可能。传统的学习算法会为每张图像使用一行，形式如（*停止标志*，*红色，八边形*），而CNN使用的数据形式为（*停止标志*，*x*，*y*，*颜色*），其中*x*和*y*是像素坐标，*颜色*是给定像素的颜色数据。这看起来只是维度增加了一个，但请注意，即使是非常小的图像也由许多(*x*，*y*)组合构成，颜色通常指定为RGB（*红色*，*绿色*，*蓝色*）值的组合。这意味着单个训练数据行的更准确表示将是：
- en: (*stop sign*, *x*[1]*y*[1]*r*, *x*[1]*x*[1]*g*, *x*[1]*y*[1]*b*, *x*[2]*y*[1]*r*,
    *x*[2]*y*[1]*g*, *x*[2]*y*[1]*b*, …, *x*[n]*y*[n]*r*, *x*[n]*x*[n]*g*, *x*[n]*y*[n]*b*)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: （*停止标志*，*x*[1]*y*[1]*r*，*x*[1]*x*[1]*g*，*x*[1]*y*[1]*b*，*x*[2]*y*[1]*r*，*x*[2]*y*[1]*g*，*x*[2]*y*[1]*b*，……，*x*[n]*y*[n]*r*，*x*[n]*x*[n]*g*，*x*[n]*y*[n]*b*）
- en: Each of the predictors refers to the level of red, green, or blue at the specified
    combination of (*x*, *y*), and *r*, *g*, or *b* values. Thus, the dimensionality
    increases greatly, and the dataset becomes much wider as the image becomes larger.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 每个预测因子都指的是在指定的(*x*，*y*)组合中红色、绿色或蓝色的程度，以及*r*、*g*或*b*值。因此，维度大大增加，随着图像变大，数据集也变得更大。
- en: A small 100x100 pixel image would have *100x100x3 = 30,000* predictors. Even
    this is small compared to the SuperVision team, which used over 60 million parameters
    when it won the visual recognition challenge in 2012!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一个100x100像素的小图像将有*100x100x3 = 30,000*个预测因子。即使这样，与2012年赢得视觉识别挑战赛的SuperVision团队使用的超过6000万个参数相比，这仍然很小！
- en: '*Chapter 12*, *Advanced Data Preparation*, noted that if a model is overparameterized,
    it reaches an interpolation threshold at which there are enough parameters to
    memorize and perfectly classify all the training samples. The ImageNet Challenge
    dataset, which contained 10 million images, is much smaller than the 60 million
    parameters the winning team used. Intuitively, this makes sense; assuming there
    are no completely identical pictures in the database, at least one of the pixels
    will vary for every image. Thus, an algorithm could simply memorize every image
    to achieve the perfect classification of the training data. The problem is that
    the model will be evaluated on a dataset of unseen data, and thus the severe overfitting
    to the training data will lead to a massive generalization error.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*第12章*，*高级数据准备*指出，如果一个模型过度参数化，它将达到一个插值阈值，此时有足够的参数来记忆并完美地分类所有训练样本。包含1000万张图片的ImageNet挑战数据集比获胜团队使用的6000万个参数要小得多。直观上看，这是有道理的；假设数据库中没有完全相同的图片，至少每个图像的一个像素会有所不同。因此，算法可以简单地记忆每一张图片以实现训练数据的完美分类。问题是模型将在未见过的数据集上评估，因此对训练数据的严重过拟合将导致巨大的泛化误差。'
- en: 'The topology of a CNN prevents this from happening. We won’t be diving too
    deeply into the black box of the CNN, but we will understand it as a series of
    layers in the following categories:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的拓扑结构阻止了这种情况的发生。我们不会深入探讨CNN的黑盒，但我们将它理解为一组以下类别的层：
- en: '**Convolutional layers** are placed early in the network and usually comprise
    the most computationally intensive step in the network because they are the only
    layers to process the raw image data directly; we can understand convolution as
    passing the raw data through a filter creating a set of tiles that represent small,
    overlapping portions of the full area'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积层**在网络中放置得较早，通常构成了网络中最计算密集的步骤，因为它们是唯一直接处理原始图像数据的层；我们可以将卷积理解为将原始数据通过一个过滤器，创建一组代表整个区域的小型重叠部分的瓦片'
- en: '**Pooling layers**, also known as **downsampling** or **subsampling** layers,
    gather the output signals from a cluster of neurons in one layer, and summarize
    them into a single neuron for the next layer, usually by taking the maximum or
    average value among those being summarized'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**池化层**，也称为**下采样**或**子采样**层，从一层中的一簇神经元中收集输出信号，并将它们总结为下一层的单个神经元，通常是通过取被总结的信号中的最大值或平均值来实现'
- en: '**Fully connected layers** are much like the layers in a traditional multilayer
    perceptron, and are used near the end of the CNN to build the model that makes
    predictions'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全连接层**与传统的多层感知器中的层非常相似，通常在CNN的末尾用于构建预测模型'
- en: The convolutional and pooling layers in the network serve the interrelated purposes
    of identifying important features of the images to be learned, as well as reducing
    the dimensionality of the dataset before hitting the fully connected layers that
    make predictions. In other words, the early stages of the network perform feature
    engineering, while the later stages use the constructed features to make predictions.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 网络中的卷积层和池化层服务于识别要学习的图像的重要特征以及在使用进行预测的全连接层之前降低数据集的维度的相关目的。换句话说，网络的早期阶段执行特征工程，而后期阶段使用构建的特征进行预测。
- en: To better understand the layers in a CNN, see *An Interactive Node-Link Visualization
    of Convolutional Neural Networks* by Adam W. Harley at [https://adamharley.com/nn_vis/](https://adamharley.com/nn_vis/).
    The interactive tool has you draw a number from zero to nine, which is then classified
    using a neural network. The 2D and 3D convolutional network visualizations clearly
    show how the digit you drew passes through the convolutional, downsampling, and
    fully connected layers before reaching the output layer where the prediction is
    made. Neural networks for general image classification work much the same way,
    but using a substantially larger and more complex network.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解CNN中的层，请参阅Adam W. Harley在[https://adamharley.com/nn_vis/](https://adamharley.com/nn_vis/)上发表的《卷积神经网络的交互式节点-链接可视化》*。该交互式工具让您从零到九画一个数字，然后使用神经网络对其进行分类。二维和三维卷积网络可视化清楚地显示了您所画的数字是如何通过卷积、下采样和全连接层，最终到达输出层进行预测的。用于通用图像分类的神经网络以类似的方式工作，但使用的是一个更大、更复杂的网络。
- en: Transfer learning and fine tuning
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 迁移学习和微调
- en: Building a CNN from scratch requires a tremendous amount of data, expertise,
    and computing power. Thankfully, many large organizations that have access to
    data and computing resources have built image, text, and audio classification
    models, and have shared them with the data science community. Through a process
    of **transfer learning**, a deep learning model can be adapted from one context
    to another. Not only is it possible to apply the saved model to similar types
    of problems as it was trained on, but it may also be useful for problems outside
    the original domain. For instance, a neural network that was trained to recognize
    an endangered species of elephants in satellite photos may also be useful for
    identifying the position of tanks in infrared drone images taken above a war zone.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 从头开始构建CNN需要大量的数据、专业知识和计算能力。幸运的是，许多拥有数据和计算资源的大型组织已经构建了图像、文本和音频分类模型，并将它们与数据科学社区共享。通过**迁移学习**的过程，深度学习模型可以从一个上下文适应到另一个上下文。不仅可以将保存的模型应用于与训练时相似类型的问题，而且它也可能对原始领域之外的问题有用。例如，一个在卫星照片中训练以识别濒危象种的神经网络，也可能有助于识别在战区上空拍摄的红外无人机图像中坦克的位置。
- en: If the knowledge doesn’t transfer directly to the new task, it is possible to
    hone a pre-trained neural network using additional training in a process known
    as **fine tuning**. Taking a model that was trained generally, such as a general
    image classification model that can identify 10,000 classes of objects, and fine
    tuning it to be good at identifying a single type of object not only reduces the
    amount of training data and computing power needed but also may offer improved
    generalization over a model trained on a single class of images.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果知识不能直接迁移到新任务，可以通过**微调**过程使用额外的训练来磨练预训练的神经网络。从一个一般性的模型开始训练，例如一个可以识别10,000类对象的通用图像分类模型，并将其微调以擅长识别单一类型的对象，这不仅减少了所需的训练数据和计算能力，还可能比仅在单一类图像上训练的模型提供更好的泛化能力。
- en: Keras can be used for both transfer learning and fine tuning by downloading
    neural networks with pre-trained weights. A list of available pre-trained models
    is available at [https://keras.io/api/applications/](https://keras.io/api/applications/)
    and an example of fine tuning an image processing model to better predict cats
    and dogs can be found at [https://tensorflow.rstudio.com/guides/keras/transfer_learning](https://tensorflow.rstudio.com/guides/keras/transfer_learning).
    In the next section, we will apply a pre-trained image model to real-world images.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Keras可以通过下载带有预训练权重的神经网络来进行迁移学习和微调。可用的预训练模型列表可在[https://keras.io/api/applications/](https://keras.io/api/applications/)找到，一个将图像处理模型微调以更好地预测猫和狗的示例可在[https://tensorflow.rstudio.com/guides/keras/transfer_learning](https://tensorflow.rstudio.com/guides/keras/transfer_learning)找到。在下一节中，我们将应用一个预训练的图像模型到现实世界的图像上。
- en: Example – classifying images using a pre-trained CNN in R
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例 - 使用预训练的CNN在R中分类图像
- en: 'R may not be the right tool for the heaviest deep learning jobs, but with the
    right set of packages, we can apply pre-trained CNNs to perform tasks, such as
    image recognition, that conventional machine learning algorithms have trouble
    solving. The predictions generated by the R code can then be used directly for
    image recognition tasks like filtering obscene profile pictures, determining whether
    an image depicts a cat or a dog, or even identifying stop signs inside a simple
    autonomous vehicle. Perhaps more commonly, the predictions could be used as predictors
    in an ensemble that includes conventional machine learning models using tabular-structured
    data in addition to the deep learning neural network that consumes the unstructured
    image data. You may recall that *Chapter 14*, *Building Better Learners*, described
    a potential stacked ensemble that combined image, text, and traditional machine
    learning models in this way to predict elements of a Twitter user’s future behavior.
    The following pictures illustrate hypothetical Twitter profile pictures, which
    we will classify using a deep neural network shortly:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: R可能不是处理最重深度学习任务的正确工具，但有了合适的包集，我们可以将预训练的CNN应用于执行图像识别等任务，这些任务传统机器学习算法难以解决。R代码生成的预测可以直接用于图像识别任务，如过滤不雅的资料照片、确定图像是否描绘了猫或狗，甚至识别简单自动驾驶车辆内的停车标志。也许更常见的是，这些预测可以用作包含使用表格结构数据的传统机器学习模型以及消耗非结构化图像数据的深度学习神经网络的集成模型的预测因子。你可能还记得，*第14章*，*构建更好的学习者*，描述了一种潜在的堆叠集成，它以这种方式结合了图像、文本和传统机器学习模型，以预测Twitter用户未来行为的元素。以下图片展示了假设的Twitter个人资料图片，我们将使用深度神经网络对其进行分类：
- en: '![](img/B17290_15_03.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_15_03.png)'
- en: 'Figure 15.3: A pre-trained neural network can be used in R to identify the
    subject of images like these'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.3：可以在R中使用预训练的神经网络识别这类图像
- en: First, before using a pre-trained model, it is important to consider the dataset
    that was used to train the neural network. Most publicly available image networks
    were trained on huge image databases comprising a variety of everyday objects
    and animals, such as cars, dogs, houses, various tools, and so on. This is appropriate
    if the desired task is to distinguish among everyday objects, but more specific
    tasks may require more specific training datasets. For instance, a facial recognition
    tool or an algorithm identifying stop signs would be more effectively trained
    on datasets of faces and road signs, respectively. With transfer learning, it
    is possible to fine-tune a deep neural network trained on a variety of images
    to be better at a more specific task—it could become very good at identifying
    pictures of cats, for example—but it is hard to imagine a network trained on faces
    or road signs ever becoming very good at identifying cats, even with additional
    tuning!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在使用预训练模型之前，考虑用于训练神经网络的训练数据集非常重要。大多数公开可用的图像网络都是在包含各种日常物体和动物（如汽车、狗、房屋、各种工具等）的巨大图像数据库上训练的。如果目标是区分日常物体，这是合适的，但对于更具体的工作可能需要更具体的训练数据集。例如，面部识别工具或识别停车标志的算法可能需要在面部和道路标志的数据集上分别进行更有效的训练。通过迁移学习，可以将训练于各种图像的深度神经网络微调以更好地完成更具体的工作——例如，它可能非常擅长识别猫的图片——但很难想象一个在面部或道路标志上训练的网络，即使经过额外的调整，也能非常擅长识别猫！
- en: For this exercise, we will classify our small set of images using a CNN called
    **ResNet-50**, which is a 50-layer deep network that has been trained on a large
    and comprehensive variety of labeled images. This model, which was introduced
    by a group of researchers in 2015 as a state-of-the-art, competition-winning computer
    vision algorithm, has since been surpassed by more sophisticated approaches, but
    continues to be extremely popular and effective due to its ease of use and integration
    with tools like R and Keras.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用名为**ResNet-50**的CNN对我们的小图像集进行分类，这是一个在大量和综合的标记图像上训练的50层深度网络。这个模型由一组研究人员在2015年作为最先进的、获奖的计算机视觉算法引入，尽管后来被更复杂的方法所超越，但由于其易用性和与R和Keras等工具的集成，它仍然非常受欢迎和有效。
- en: For more information about ResNet-50, see *Deep Residual Learning for Image
    Recognition, He, K., Zhang, X., Ren, S., and Sun, J., 2015,* [https://arxiv.org/abs/1512.03385v1](https://arxiv.org/abs/1512.03385v1).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 关于ResNet-50的更多信息，请参阅*《深度残差学习用于图像识别》，He, K.，Zhang, X.，Ren, S.，和Sun, J.，2015年*
    [https://arxiv.org/abs/1512.03385v1](https://arxiv.org/abs/1512.03385v1)。
- en: The **ImageNet database** ([https://www.image-net.org](https://www.image-net.org))
    that was used to train the ResNet-50 model is the same database used for the ImageNet
    Visual Recognition Challenge and has contributed greatly to computer vision since
    its introduction in 2010\. Composed of over 14 million hand-labeled images and
    consuming many gigabytes of storage (or even terabytes in the case of the full,
    academic version), it is fortunate that there is no need to download this resource
    and train the model from scratch. Instead, we simply download the neural network
    weights for the ResNet-50 model that researchers trained on the full database,
    saving us a tremendous amount of computational expense.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练 ResNet-50 模型的 **ImageNet 数据库** ([https://www.image-net.org](https://www.image-net.org))
    与用于 ImageNet 视觉识别挑战的数据库相同，自 2010 年推出以来对计算机视觉做出了巨大贡献。它由超过 1400 万张手工标注的图像组成，消耗了数
    GB 的存储空间（在完整、学术版本的情况下甚至达到 TB 级别），幸运的是，我们无需下载此资源从头开始训练模型。相反，我们只需下载研究人员在完整数据库上训练的
    ResNet-50 模型的神经网络权重，从而节省了大量计算开销。
- en: 'To begin the process, we’ll need to add the `tensorflow` and `keras` packages
    to R as well as the various dependencies. Most of these steps must only be performed
    once. The `devtools` package adds tools to develop R packages and use packages
    that are in active development, so we’ll begin by installing and loading this
    as usual:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始这个过程，我们需要将 `tensorflow` 和 `keras` 包添加到 R 中，以及各种依赖项。大多数这些步骤只需执行一次。`devtools`
    包为开发 R 包和使用处于开发中的包添加了工具，因此我们将像往常一样安装并加载这个包：
- en: '[PRE0]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we’ll use the `devtools` package to obtain the latest version of the
    `tensorflow` package from GitHub. Typically, we install packages from CRAN, but
    for packages in active development, it can be better to install directly from
    the latest source code. The command to install the `tensorflow` package from its
    GitHub path is:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 `devtools` 包从 GitHub 获取 `tensorflow` 包的最新版本。通常，我们从 CRAN 安装包，但对于处于开发中的包，直接从最新源代码安装可能更好。从其
    GitHub 路径安装 `tensorflow` 包的命令是：
- en: '[PRE1]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This points R to the RStudio GitHub account, which stores the source code for
    the package. To read the documentation and see the code for yourself on the web,
    visit [https://github.com/rstudio/tensorflow](https://github.com/rstudio/tensorflow)
    in a web browser.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这将 R 指向 RStudio 的 GitHub 账户，该账户存储了包的源代码。要在线阅读文档并查看代码，请在网络浏览器中访问 [https://github.com/rstudio/tensorflow](https://github.com/rstudio/tensorflow)。
- en: 'After installing the `tensorflow` package, there are several dependencies that
    are required to begin using TensorFlow in R. In particular, the `tensorflow` package
    is merely an interface between R and TensorFlow, so we must first install TensorFlow
    itself. Perhaps ironically, Python and several of its packages are required for
    this. Thus, the R `reticulate` package ([https://rstudio.github.io/reticulate/](https://rstudio.github.io/reticulate/))
    is used to manage the interface between R and Python. As confusing as this seems,
    the complete installation process is driven by a single command from the `tensorflow`
    package as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 `tensorflow` 包后，有几个依赖项是开始使用 TensorFlow 在 R 中所必需的。特别是，`tensorflow` 包仅仅是 R
    和 TensorFlow 之间的接口，因此我们必须首先安装 TensorFlow 本身。也许有些讽刺，Python 及其一些包是完成此任务所必需的。因此，我们使用
    R 的 `reticulate` 包 ([https://rstudio.github.io/reticulate/](https://rstudio.github.io/reticulate/))
    来管理 R 和 Python 之间的接口。尽管这听起来很令人困惑，但完整的安装过程是由 `tensorflow` 包的单个命令驱动的，如下所示：
- en: '[PRE2]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'While the command is running, you should see R working to install a large collection
    of Python tools and packages. If all goes well, you can proceed to install the
    Keras package from GitHub:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 当命令运行时，你应该看到 R 正在安装大量 Python 工具和包。如果一切顺利，你可以继续从 GitHub 安装 Keras 包：
- en: '[PRE3]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the case of a problem, keep in mind that although the code for this example
    was tested on multiple platforms and R versions, it is quite possible for something
    to go wrong among the many dependencies required to have R interface with Python
    and TensorFlow. Don’t be afraid to search the web for a particular error message
    or check the Packt Publishing GitHub repository for the updated R code for this
    chapter.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果出现问题，请记住，尽管此示例的代码已在多个平台和 R 版本上进行了测试，但在 R 与 Python 和 TensorFlow 交互所需的众多依赖项中，出现问题的可能性相当大。不要害怕在网络上搜索特定的错误消息，或者检查
    Packt Publishing 的 GitHub 仓库以获取本章更新的 R 代码。
- en: 'With the necessary packages installed, Keras can help load the ResNet-50 model
    with the weights trained on the ImageNet database:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装了必要的包之后，Keras可以帮助加载在ImageNet数据库上训练的ResNet-50模型：
- en: '[PRE4]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Our 50-layer deep image classification model trained on millions of everyday
    images is now ready to start making predictions; however, the ease at which we
    loaded the model conceals the work to come.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在数百万张日常图像上训练的50层深度图像分类模型现在可以开始做出预测了；然而，我们加载模型时的轻松程度掩盖了即将到来的工作。
- en: The greater challenge with using a pre-trained model is transforming our unstructured
    image data, which we hope to classify, into the same structured format that it
    saw during training. ResNet-50 used square images of 224-by-224 pixels with each
    pixel reflecting a color composed of three channels, red, green, and blue, each
    having 255 levels of brightness. All images we hope to classify must be transformed
    from their original formats, such as PNG, GIF, or JPEG, into a 3-D tensor using
    this representation. We’ll see this in practice using the previously depicted
    `cat.jpg`, `ice_cream.jpg`, and `pizza.jpg` files found in the R code folder for
    this chapter, but the process will work similarly for any image.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练模型的一个更大挑战是将我们希望分类的无结构图像数据转换成它在训练期间看到的相同结构格式。ResNet-50使用了224x224像素的方形图像，每个像素反映由三个通道组成的颜色，红色、绿色和蓝色，每个通道都有255个亮度级别。我们希望分类的所有图像都必须使用这种表示从其原始格式（如PNG、GIF或JPEG）转换为3-D张量。我们将通过之前描述的`cat.jpg`、`ice_cream.jpg`和`pizza.jpg`文件来实际看到这一点，这些文件位于本章R代码文件夹中，但这个过程对任何图像都是类似的。
- en: 'The `image_load()` function in the `keras` package will get the process started.
    Simply provide the file name and desired target dimensions as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`keras`包中的`image_load()`函数将启动这个过程。只需提供文件名和所需的目标尺寸，如下所示：'
- en: '[PRE5]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This creates an image object, but we need one more command to convert it into
    a 3-D tensor:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个图像对象，但我们还需要一个额外的命令将其转换为3-D张量：
- en: '[PRE6]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To prove it to ourselves, we can examine the dimensions and structure of the
    object as follows. As expected, the object is a numeric matrix with *224* x *224*
    x *3* as the dimensions:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明这一点，我们可以检查对象的大小和结构，如下所示。正如预期的那样，对象是一个数值矩阵，其维度为*224* x *224* x *3*：
- en: '[PRE7]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The first few values in the matrix are all 255, which isn’t very meaningful:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵中的前几个值都是255，这并没有什么意义：
- en: '[PRE9]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let’s do some investigation to better understand these data structures. Because
    of R’s row, column matrix format, the matrix coordinates are (*y*, *x*), with
    (*1*, *1*) representing the top-left pixel in the image and (*1*, *224*) the top-right
    pixel. To illustrate this, let’s obtain each of the three color channels for a
    couple of pixels in the ice cream image:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行一些调查，以便更好地理解这些数据结构。由于R的行列矩阵格式，矩阵坐标是(*y*, *x*)，其中(*1*, *1*)代表图像的左上角像素，(*1*,
    *224*)代表右上角像素。为了说明这一点，让我们获取冰淇淋图像中几个像素的三个颜色通道：
- en: '[PRE11]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The pixel at (*1*, *224*) has (*r*, *g*, *b*) colors of (*253*, *253*, *255*),
    which corresponds to nearly the brightest possible white, while the pixel at (*40*,
    *145*) has color values (*149*, *23*, *34*) translating to a dark red—a piece
    of strawberry in the ice cream. These coordinates are illustrated in the following
    figure:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 像素(*1*, *224*)的颜色为(*r*, *g*, *b*)，颜色值为(*253*, *253*, *255*)，这几乎是最亮的白色，而像素(*40*,
    *145*)的颜色值为(*149*, *23*, *34*)，翻译成深红色——冰淇淋中的一块草莓。这些坐标在以下图中进行了说明：
- en: '![](img/B17290_15_04.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17290_15_04.png)'
- en: 'Figure 15.4: The picture of ice cream has been reduced from a matrix of 1,000x1,000
    pixels to 224x224; each pixel in the image has three color channels'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.4：冰淇淋的图片已经从1,000x1,000像素的矩阵减少到224x224；图像中的每个像素都有三个颜色通道
- en: 'One additional complication is that the ResNet-50 model expects a four-dimensional
    tensor, with the fourth dimension representing the batch. With only one image
    to classify, we have no need for this parameter, so we’ll simply assign it a constant
    value of 1 to create a matrix of *1x224x224x3*. The command `c(1, dim(x))` defines
    the new matrix in this format, and then the `array_reshape()` function fills this
    matrix with the contents of `x` using the Python-style row-by-row ordering used
    by TensorFlow rather than the R-style column-by-column filling. The full command
    is as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个额外的复杂性是，ResNet-50模型期望一个四维张量，其中第四维代表批次。由于只有一个图像需要分类，我们不需要这个参数，因此我们将简单地将其分配一个常数值1，以创建一个`1x224x224x3`的矩阵。命令`c(1,
    dim(x))`以这种格式定义新的矩阵，然后`array_reshape()`函数使用TensorFlow使用的Python风格行行顺序而不是R风格的列列填充，将`x`的内容填充到这个矩阵中。完整的命令如下：
- en: '[PRE15]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To confirm that `x` has the correct dimensions, we can use the `dim()` command:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认`x`具有正确的维度，我们可以使用`dim()`命令：
- en: '[PRE16]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, we run the `imagenet_preprocess_input()` function to normalize the
    color values to match the ImageNet database:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们运行`imagenet_preprocess_input()`函数来将颜色值归一化，以匹配ImageNet数据库：
- en: '[PRE18]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The primary function of this transformation is to zero-center each color with
    respect to the database, essentially treating each color value as being greater
    than or less than the average of ImageNet images on that color. For example, the
    red pixel in the ice cream at (`40`, `145`) had color values of 149, 23, and 34
    before; now, it has very different values:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这种转换的主要功能是将每个颜色相对于数据库进行零中心化，基本上是将每个颜色值视为大于或小于ImageNet图像中该颜色的平均值。例如，在冰淇淋中，位于(`40`,
    `145`)的红色像素在之前具有149、23和34的颜色值；现在，它具有非常不同的值：
- en: '[PRE19]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Negative values indicate a color level less than the ImageNet average for that
    color, and positive values indicate higher. The preprocessing step also inverts
    the red-green-blue format to blue-green-red, so only the red channel is above
    the average ImageNet level, which is not terribly surprising, as a strawberry
    is quite red!
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 负值表示该颜色的颜色级别低于ImageNet的平均值，而正值表示更高。预处理步骤还将红绿蓝格式反转为蓝绿红，因此只有红色通道高于ImageNet的平均水平，这并不令人特别惊讶，因为草莓非常红！
- en: 'Now, let’s see what the ResNet-50 network thinks is depicted in the image.
    We’ll first use the `predict()` function on the model object and the image matrix,
    and then use the `keras` function `imagenet_decode_predictions()` to convert the
    network’s predicted probabilities to text-based labels that categorize each of
    the ImageNet images. Because there are 1,000 categories of images in the ImageNet
    database, the `preds` object contains 1,000 predicted probabilities—one for each
    possibility. The decoding function allows us to limit the output to the top *N*
    most probable possibilities—ten, in this case:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看ResNet-50网络认为图像中描绘了什么。我们将首先在模型对象和图像矩阵上使用`predict()`函数，然后使用`keras`函数`imagenet_decode_predictions()`将网络的预测概率转换为基于文本的标签，这些标签将分类ImageNet中的每个图像。由于ImageNet数据库中有1,000种图像类别，`preds`对象包含1,000个预测概率——每个可能性一个。解码函数允许我们将输出限制为最可能的前*N*个可能性——在这个例子中是十个：
- en: '[PRE21]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The `c_resnet50` object is a list which contains the top ten predictions for
    our lone image. To see the predictions, we simply type the name of the list to
    discover that the network correctly identified the image as ice cream with about
    99.6 percent probability:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`c_resnet50`对象是一个列表，其中包含我们单个图像的前十个预测。要查看预测结果，我们只需输入列表的名称，就可以发现网络正确地将图像识别为冰淇淋，概率约为99.6%：'
- en: '[PRE22]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Although none of the other potential classifications had a predicted probability
    much greater than zero, some of the other top predictions make a bit of sense;
    it is not difficult to see why they were considered as possibilities. Eggnog is
    in the correct category of foods, while an ice cream cone might look a bit like
    a cup, or a thimble.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管其他潜在分类的预测概率并没有远大于零，但一些其他顶级预测还是有点道理；不难理解为什么它们被视为可能性。例如，蛋酒属于正确的食物类别，而冰淇淋锥可能看起来有点像杯子，或者像顶针。
- en: The model even listed strawberry as the sixth most likely option, which is the
    correct flavor of ice cream.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 模型甚至将草莓列为第六个最有可能的选项，这是正确的冰淇淋口味。
- en: 'As an exercise, we’ll do the same process with the other two images. The following
    sequence of steps uses the `lapply()` function to apply the image processing steps
    to the pair of images, each time creating a new list to supply to the subsequent
    function. The last step supplies the list containing two prepared image arrays
    to the `lapply()` function, which applies the `predict()` command to each image:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，我们将对另外两张图片执行相同的过程。以下步骤序列使用`lapply()`函数将图像处理步骤应用于图像对，每次创建一个新的列表以供后续函数使用。最后一步将包含两个准备好的图像数组的列表提供给`lapply()`函数，该函数对每个图像应用`predict()`命令：
- en: '[PRE24]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Finally, the `sapply()` function is used to apply the decoding function to
    each of the two sets of predictions, while simplifying the result. The `lapply()`
    function would also work here, but because `imagenet_decode_predictions()` returns
    a list, the result is a sub-list of length one within a list; `sapply()` recognizes
    that this is redundant and unnecessary, and will eliminate the additional level
    of hierarchy:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用`sapply()`函数将解码函数应用于两个预测集的每个集合，同时简化结果。`lapply()`函数在这里也可以工作，但由于`imagenet_decode_predictions()`返回一个列表，结果是列表中的一个长度为1的子列表；`sapply()`识别出这是多余的，并且将消除额外的层次结构：
- en: '[PRE25]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Typing the name of the resulting object shows the top three predictions for
    each of the two images:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 输入结果的名称将显示两张图像各自的前三个预测：
- en: '[PRE26]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The ResNet-50 algorithm didn’t merely classify the images correctly; it also
    correctly identified the cat picture as a tabby. This demonstrates the ability
    of neural networks to surpass human specificity for certain tasks; many or most
    people might have simply labeled the image as a cat, whereas the computer can
    determine the specific type of cat. On the other hand, humans remain better at
    identifying objects in less-than-optimal conditions. For example, a cat obscured
    by darkness or camouflaged in the weeds would present a greater challenge to a
    computer than to a human in most cases. Even so, the computer’s ability to work
    tirelessly gives it a huge advantage for automating artificial intelligence tasks.
    As stated previously, applied to a large dataset such as Twitter profile images,
    the predictions from this type of computer vision model could be used in an ensemble
    model predicting countless different user behaviors.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet-50算法不仅正确地分类了图像；它还正确地将猫图片识别为虎斑猫。这证明了神经网络在某些任务上超越人类特定性的能力；许多人或大多数人可能只是将图像标记为猫，而计算机可以确定猫的具体类型。另一方面，人类在识别非最佳条件下的物体方面仍然更胜一筹。例如，在黑暗中或杂草中隐藏的猫对计算机来说可能比对人更具挑战性。尽管如此，计算机不知疲倦的能力使其在自动化人工智能任务方面具有巨大的优势。如前所述，应用于大型数据集，如Twitter个人资料图片，此类计算机视觉模型的预测可以用于预测无数不同用户行为的集成模型。
- en: Unsupervised learning and big data
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习和大数据
- en: The previous section illustrated how a deep neural network could be used to
    classify a limitless supply of input images as an instance of everyday creatures
    or objects. From another perspective, one might also understand this as a machine
    learning task that takes the highly dimensional input of image pixel data and
    reduces it to a lower-dimensional set of image labels. It is important to note,
    however, that the deep learning neural network is a supervised learning technique,
    which means that the machine can only learn what the humans tell it to learn—in
    other words, it can only learn from something that has been previously labeled.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节说明了如何使用深度神经网络将无限供应的输入图像分类为日常生物或物体的实例。从另一个角度来看，这也可能被理解为一种机器学习任务，它将图像像素数据的高度维输入降低到一组低维度的图像标签。然而，值得注意的是，深度学习神经网络是一种监督学习技术，这意味着机器只能学习人类告诉它学习的内容——换句话说，它只能从已经被标记的内容中学习。
- en: The purpose of this section is to present useful applications of unsupervised
    learning techniques in the context of big data. These applications are in many
    ways similar to the techniques covered in *Chapter 9*, *Finding Groups of Data
    – Clustering with k-means*. However, where previous unsupervised learning techniques
    leaned heavily on humans to interpret the results, in the context of big data,
    the machine can go a step further than before and provide a deeper, richer understanding
    of the data and the implications of the connections the algorithm discovers.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的目的在于介绍在大数据背景下无监督学习技术的有用应用。这些应用在很多方面与第9章中介绍的技术相似，即*寻找数据组 - 使用k-means进行聚类*。然而，在先前无监督学习技术中，人类在解释结果方面承担了很大的责任，而在大数据的背景下，机器可以比以前更进一步，提供对数据及其发现算法连接的更深、更丰富的理解。
- en: 'To put this in practical terms, imagine a deep neural network that can learn
    to identify a cat without ever having been told what a cat is. Of course, without
    being given the label beforehand, the computer may not explicitly label it a “cat”
    *per se*, but it may understand that a cat has certain consistent relationships
    to other things that appear in pictures along with the cat: people, litter boxes,
    mice, balls of yarn—but rarely or never dogs! Such associations help form a conceptualization
    of cat as something that relates closely to people, litter boxes, and yarn, but
    is perhaps in opposition to another thing with four legs and a tail. Given enough
    pictures, it is possible the neural network could eventually associate its impression
    of cats with the English-language word “cat” by identifying cats near bags of
    cat food or in the internet’s countless cat-based memes!'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 用实际的话来说，想象一个深度神经网络，它可以学会识别猫，而从未被告知猫是什么。当然，如果没有事先给出标签，计算机可能不会明确地将它标记为“猫”本身，但它可能理解猫与图片中出现的其他事物之间存在某些一致的关系：人、猫砂盆、老鼠、一团毛线——但很少或从不包括狗！这样的关联有助于形成对猫的概念，认为它与人类、猫砂盆和毛线密切相关，但可能与另一种有四条腿和尾巴的东西相对立。如果有足够的图片，神经网络最终可能会通过识别猫粮袋附近的猫或互联网上无数的以猫为主题的梗，将对其印象与英语单词“猫”联系起来！
- en: Developing such a sophisticated model of cats would take more data and computing
    power than most machine learning practitioners have access to, but it is certainly
    possible to develop simpler models or to borrow from big data companies that do
    have access to such resources. These techniques provide yet another way to incorporate
    unstructured data sources into more conventional learning tasks, as the machine
    can reduce the complexity of big data into something much more digestible.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 开发这样一个复杂的猫的模型需要比大多数机器学习从业者所能获取的数据和计算能力更多，但当然有可能开发更简单的模型，或者借鉴那些确实能获取这些资源的大数据公司。这些技术提供了另一种将非结构化数据源纳入更传统学习任务的方法，因为机器可以将大数据的复杂性降低到更易于消化的程度。
- en: Representing highly dimensional concepts as embeddings
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将高维概念表示为嵌入
- en: 'The things we encounter in everyday life can be described by a limitless number
    of attributes. Moreover, not only are there countless data points that can be
    used to describe each object, but the nature of human subjectivity makes it unlikely
    that any two people would describe an object in the same way. For example, if
    you ask a few people to describe typical horror films, one might say they imagine
    slasher films with blood and gore, another might think of zombie or vampire movies,
    and another one might think of spooky ghost stories and haunted houses. These
    descriptions could be represented using the following statements:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在日常生活中遇到的事物可以用无数个属性来描述。此外，不仅存在无数可以用来描述每个对象的数据点，而且人类主观性的本质使得任何两个人都不太可能以相同的方式描述一个对象。例如，如果你问一些人描述典型的恐怖电影，一个人可能会想象血腥和血腥的砍杀电影，另一个人可能会想到僵尸或吸血鬼电影，还有一个人可能会想到阴森的鬼故事和闹鬼的房子。这些描述可以用以下陈述来表示：
- en: '*horror* = *killer* + *blood* + *gore*'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*恐怖* = *杀手* + *血液* + *血腥*'
- en: '*horror* = *creepy* + *zombies* + *vampires*'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*恐怖* = *诡异* + *僵尸* + *吸血鬼*'
- en: '*horror* = *spooky* + *ghosts* + *haunted*'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*恐怖* = *阴森* + *鬼魂* + *闹鬼*'
- en: If we were to program these definitions into a computer, it could substitute
    any of the representations of horror for one another and thus use a wider general
    concept of “horror” rather than the more specific features like “gore,” “creepy,”
    or “spooky” to make predictions. For instance, a learning algorithm could discover
    that a social media user that writes any of these horror-related terms is more
    likely to click on an advertisement for the new *Scream* movie.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将这些定义编程到计算机中，它就可以相互替换任何关于恐怖的表示，从而使用更广泛的“恐怖”概念，而不是像“血腥”、“诡异”或“阴森”这样的更具体特征来进行预测。例如，一个学习算法可能会发现，任何写有这些恐怖相关术语的社交媒体用户更有可能点击观看新片
    *Scream* 的广告。
- en: Unfortunately, if a user posts “I just love a good scary movie!” or “The Halloween
    season is my favorite time of year!” then the algorithm will be unable to relate
    the text to the prior conceptualizations of horror, and thus will be unable to
    realize that a horror movie advertisement should be displayed. This is likewise
    true for any of the hundreds of horror-related keywords that the computer had
    not previously seen verbatim, including many that will seem obvious to a human
    observer, like witches, demons, graveyards, spiders, skeletons, and so on. What
    is needed is a way to generalize the concept of horror to the almost limitless
    number of ways that it can be described.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，如果用户发布“我简直喜欢一部好恐怖的电影！”或“万圣节季节是我一年中最喜欢的时光！”这样的帖子，算法将无法将文本与先前的恐怖概念联系起来，因此将无法意识到应该显示恐怖电影的广告。这同样适用于计算机之前未曾直接看到的数百个与恐怖相关的关键词，包括许多对人类观察者来说显然的关键词，如女巫、恶魔、墓地、蜘蛛、骷髅等等。所需要的，是一种将恐怖概念推广到几乎无限的描述方式的方法。
- en: An **embedding** is a mathematical concept referring to the ability to represent
    a higher-dimensional vector using fewer dimensions; in machine learning, the embedding
    is purposefully constructed such that dimensions that are correlated in the high-dimensional
    space are positioned more closely in the lower-dimensional space.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**嵌入**是一个数学概念，指的是使用更少的维度来表示高维向量的能力；在机器学习中，嵌入是有意构建的，使得在高维空间中相关联的维度在低维空间中位置更接近。'
- en: If the embedding is constructed well, the low-dimensional space will retain
    the semantics, or meaning, of the higher dimensions while being a more compact
    representation that can be used for classification tasks. The core challenge of
    creating an embedding is the unsupervised learning task of modeling the semantic
    meaning embedded in highly dimensional unstructured or semi-structured datasets.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果嵌入构建得当，低维空间将保留高维的语义或意义，同时成为一个更紧凑的表示，可用于分类任务。创建嵌入的核心挑战是建模高度维度的非结构化或半结构化数据集中嵌入的语义意义，这是一个无监督学习任务。
- en: Humans are quite adept at constructing low-dimensional representations of concepts,
    as we do this intuitively whenever we assign labels to objects or phenomena that
    are similar in broad strokes but may vary in the details. We do this when we label
    movies as comedy, science fiction, or horror; when we talk about categories of
    music like hip-hop, pop, or rock and roll; or when we create taxonomies of foods,
    animals, or illnesses. In *Chapter 9*, *Finding Groups of Data – Clustering with
    k-means*, we saw how the machine learning process of clustering can mimic this
    human process of labeling by grouping diverse but similar items through a process
    of “unsupervised classification.” However, even though this approach reduces the
    dimensionality of a dataset, it requires a structured dataset with the same specific
    features for each example before it can associate like-with-like. For something
    unstructured like a textual description of a movie, the features are too numerous
    and sparse to allow clustering.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 人类在构建概念的低维表示方面非常擅长，因为我们总是在为粗略上相似但在细节上可能有所不同的对象或现象分配标签时直觉地这样做。当我们给电影贴上喜剧、科幻或恐怖的标签时；当我们谈论像嘻哈、流行或摇滚这样的音乐类别时；或者当我们创建食物、动物或疾病的分类法时，我们就是这样做的。在
    *第9章*，*寻找数据组 - 使用 k-means 进行聚类* 中，我们看到了机器学习过程中的聚类如何通过“无监督分类”的过程模仿人类标签化过程，通过将不同但相似的项目分组。然而，尽管这种方法减少了数据集的维度，但在它能够将类似的项目关联起来之前，它需要一个具有每个示例相同特定特征的具有结构的数据集。对于像电影文本描述这样的非结构化事物，特征太多且稀疏，无法进行聚类。
- en: Instead, what if we wanted to mimic the human ability to learn via association?
    Specifically, a human can watch a series of movies and associate like-with-like
    without having specific measurable features for each movie; we can classify one
    set of movies as horror films without needing to see the identical clichéd storyline
    or to count the number of screams each film elicited. The trick is that a human
    doesn’t need a concrete definition of “horror” because we intuit it as a concept
    relative to the others in a set. Just like a cat suddenly jumping into frame can
    be used as slapstick humor in a comedy movie or paired with suspenseful music
    to provoke a jump scare, the semantic meaning of horror is always determined by
    its context.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 反之，如果我们想模仿人类通过联想学习的能力呢？具体来说，人类可以观看一系列电影，并将类似的电影进行分类，而不需要为每部电影提供具体的可测量特征；我们可以将一组电影归类为恐怖片，而无需看到相同的陈词滥调的故事情节或计算每部电影引发的尖叫次数。诀窍在于人类不需要“恐怖”的明确定义，因为我们将其作为一个相对于集合中其他元素的概念来直观地理解。就像一只猫突然跳入画面可以在喜剧电影中用作滑稽幽默，或者与悬疑音乐搭配来引发惊吓一样，恐怖的语义含义总是由其上下文决定的。
- en: In much the same way, learning algorithms can construct embeddings via context.
    Each of the thousands of movies that Hollywood has produced can be understood
    relative to others, and without studying exactly what features the movies *Halloween*
    and *Night of the Living Dead* have in common, an algorithm can observe that they
    appear in similar contexts and are somewhat substitutable for one another across
    contexts. This notion of substitutability is the basis for most embedding algorithms,
    and has indeed been used to construct embeddings for use in movie recommendation
    algorithms and other domains. In the next section, we’ll see how a popular language
    embedding algorithm uses substitutability to discover the semantic meanings of
    words.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，学习算法可以通过上下文来构建嵌入。好莱坞制作的成千上万部电影都可以相对于其他电影来理解，而且无需研究电影《万圣节》和《活死人之夜》有哪些共同特征，算法可以观察到它们出现在相似上下文中，并且在不同的上下文中可以相互替代。这种可替代性的概念是大多数嵌入算法的基础，并且确实被用来构建用于电影推荐算法和其他领域的嵌入。在下一节中，我们将看到一种流行的语言嵌入算法是如何使用可替代性来发现词语的语义含义的。
- en: Understanding word embeddings
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解词嵌入
- en: If there are roughly a million words in the English language, the feature space
    for a language-based model would be roughly a million dimensions wide even before
    phrases and word order are considered! This clearly would be far too large and
    sparse for most conventional learning algorithms to find a meaningful signal.
    A bag-of-words approach, as described in *Chapter 4*, *Probabilistic Learning
    – Classification Using Naive Bayes*, might work with enough computing power, but
    it would also require a tremendous amount of training data to associate words
    with the desired outcome. What if, instead, we could use a language embedding
    that has been pre-trained on big data?
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果英语中大约有一百万个单词，那么基于语言模型的特征空间在考虑短语和词序之前就已经有大约一百万维！这显然对于大多数传统学习算法来说太大且稀疏，以至于无法找到有意义的信号。正如在第4章“概率学习——使用朴素贝叶斯进行分类”中描述的，词袋方法在足够的计算能力下可能可行，但它也需要大量的训练数据来将单词与期望的结果关联起来。那么，如果我们能够使用在大数据上预训练的语言嵌入会怎样呢？
- en: 'To illustrate the upside of this alternative approach, let’s imagine the machine
    learning task of deciding whether to display an advertisement for a lunchtime
    café to users posting on a social media website. Consider the following posts
    made by hypothetical users:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这种替代方法的优点，让我们设想一个机器学习任务：决定是否向在社交媒体网站上发帖的用户展示午餐咖啡馆的广告。考虑以下由假设用户发表的帖子：
- en: I ate bacon and eggs in the morning for the most important meal of the day!
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我在早上吃了培根和鸡蛋，这是一天中最重要的餐食！
- en: I am going to grab a quick sandwich this afternoon before hitting the gym.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我下午要去健身房之前，打算快速吃个三明治。
- en: Can anyone provide restaurant recommendations for my date tonight?
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谁能为我今晚的约会推荐一些餐厅？
- en: For a naive Bayes approach, we would first need many of these types of sentences,
    but because the algorithm is a supervised learner, we would also need a target
    feature that indicates whether or not the user writing the sentence is interested
    in purchasing lunch from the café. We could then train the model to recognize
    which words are predictive of buying lunch.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 对于朴素贝叶斯方法，我们首先需要许多这类句子，但由于该算法是一个监督学习器，我们还需要一个目标特征，用来指示撰写句子的用户是否对从咖啡馆购买午餐感兴趣。然后我们可以训练模型来识别哪些单词可以预测购买午餐。
- en: In comparison, a human reading these sentences can easily form a reasonable
    guess as to which of the three users is most likely to be interested in buying
    lunch today. The human’s guess is not based on being trained to predict lunch
    buying behavior specifically, but rather is based on an understanding of the embedded
    meaning in each of the sentences’ words. In other words, because a human understands
    the meaning of the users’ words, it becomes unnecessary to guess their behavior
    as we can instead just listen to what they are telling us they plan to do.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 与此相比，一个阅读这些句子的人类可以很容易地猜测出三位用户中哪一位最有可能对今天购买午餐感兴趣。人类的猜测并不是基于专门训练来预测午餐购买行为，而是基于对每个句子中单词嵌入意义的理解。换句话说，因为人类理解了用户单词的意义，所以我们不需要猜测他们的行为，我们只需倾听他们告诉我们他们计划做什么。
- en: The most effective language models do more than merely look at the meaning of
    words; they also look at the meaning of words in relation to others. The use of
    grammar and phrasing can completely change the implications of a sentence. For
    example, the sentence “I skipped breakfast today, so I can stuff myself at lunch”
    is very different from “I stuffed myself at breakfast, so I need to skip lunch
    today” despite containing almost exactly the same words!
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 最有效的语言模型不仅仅是查看单词的意义；它们还会考虑单词与其他单词之间的关系。语法和句式的使用可以完全改变句子的含义。例如，“我今天跳过了早餐，所以午餐可以大吃一顿”这句话与“我今天早餐大吃一顿，所以今天需要跳过午餐”这句话虽然几乎包含完全相同的单词，但意义却大相径庭！
- en: 'Ignoring for now how this might be constructed, suppose we have a very simple
    language embedding that captures the meaning of all English-language words in
    two dimensions: a “lunch” dimension that measures how related a term is to lunch,
    and a “food” dimension that indicates whether the term is related to food. In
    this model, the semantic meaning that was once delivered by unique and specific
    terms like “soup” and “salad” is instead represented by the position of these
    concepts in the 2-D space, as illustrated for a selection of words in *Figure
    15.5*:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在先不考虑这种结构是如何构建的，假设我们有一个非常简单的语言嵌入，它可以在两个维度上捕捉所有英语单词的意义：一个“午餐”维度，用来衡量一个术语与午餐的相关性，以及一个“食物”维度，用来表示一个术语是否与食物相关。在这个模型中，曾经由独特的、具体的术语如“汤”和“沙拉”传达的语义意义，现在由这些概念在二维空间中的位置来表示，如图15.5所示：
- en: '![A picture containing text, receipt, screenshot  Description automatically
    generated](img/B17290_15_05.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本、收据、屏幕截图的图片，描述自动生成](img/B17290_15_05.png)'
- en: 'Figure 15.5: A very simple embedding reduces the highly dimensional meaning
    of various words into two dimensions that a machine can use to understand the
    subjective concepts of “food” and “lunch”'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.5：一个非常简单的嵌入将各种单词高度维度的意义简化为机器可以用来理解“食物”和“午餐”主观概念的二维空间
- en: 'The embedding itself is a mapping of a word to coordinates in a lower-dimensional
    space. Thus, a lookup function can provide the values for a specific word. For
    instance, using the 2-D word embedding above, we can obtain coordinates for a
    selection of terms that may have appeared in social media posts:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入本身是将一个单词映射到低维空间中的坐标。因此，查找函数可以提供特定单词的值。例如，使用上述2维单词嵌入，我们可以获得可能出现在社交媒体帖子中的术语的坐标：
- en: '*f(sandwich)* = *(0.97, 0.54)*'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*f(三明治)* = *(0.97, 0.54)*'
- en: '*f(bacon)* = *(-0.88, 0.75)*'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*f(培根)* = *(-0.88, 0.75)*'
- en: '*f(apple)* = *(0.63, 0.25)*'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*f(苹果)* = *(0.63, 0.25)*'
- en: '*f(orange)* = *(-0.38, 0.13)*'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*f(橙子)* = *(-0.38, 0.13)*'
- en: Terms with higher values of the first dimension are more specifically related
    to lunch (and lunch alone) while lower values indicate terms that are specifically
    not related to lunch. For example, the word “sandwich” has a high lunch value
    while “bacon” has a low lunch value, due to their close association with lunch
    and breakfast, respectively. In much the same way, terms with higher or lower
    values of the second dimension are more or less likely to be foods. The words
    “orange” and “apple” can both be foods, but the former can also represent a color
    while the latter can represent a computer, so they are near the middle of the
    food dimension. In contrast, the words “bacon” and “sandwich” are higher in this
    dimension but are lower than “burrito” or “spaghetti” due to their meanings outside
    of the culinary context; someone can “bring home the bacon” (that is, they can
    earn money) or an item can be “sandwiched” between other items.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 第一维数值较高的术语与午餐（仅限于午餐）有更具体的关联性，而较低的数值表示与午餐具体不相关的术语。例如，“三明治”有较高的午餐值，而“培根”有较低的午餐值，因为它们分别与午餐和早餐有紧密的关联。同样，第二维数值较高或较低的术语更有可能或不太可能是食物。单词“橙子”和“苹果”都可以是食物，但前者还可以代表一种颜色，而后者可以代表计算机，所以它们在食物维度上接近中间位置。相比之下，单词“培根”和“三明治”在这个维度上较高，但低于“玉米卷”或“意大利面”，因为它们在烹饪语境之外的意义；有人可以“带回家培根”（即他们可以赚钱）或一个物品可以被“夹在”其他物品之间。
- en: 'An interesting and useful property of this type of embedding is that words
    can be related to one another via simple mathematics and nearest-neighbor-style
    distance calculations. In the 2-D plot, we can observe this property by examining
    the terms that are mirror images across the horizontal or vertical axis or those
    that are close neighbors. This leads to observations such as:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这种嵌入类型的一个有趣且有用的特性是，单词可以通过简单的数学和最近邻样式的距离计算相互关联。在二维图中，我们可以通过检查水平或垂直轴上的镜像词或邻近词来观察这一特性。这导致以下观察：
- en: An apple is a more lunch-related version of an orange
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 苹果是比橙子更与午餐相关的版本
- en: Beef is like chicken, but not as associated with lunch
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 牛肉像鸡肉，但与午餐的关联性不如鸡肉
- en: Pitas and tacos are somewhat similar, as are kebabs and sandwiches
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 皮塔饼和玉米卷在某种程度上是相似的，烤肉串和三明治也是如此
- en: Soup and salad are closely related and are the lunch versions of eggs and pasta
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 汤和沙拉密切相关，是鸡蛋和意面的午餐版本
- en: Heavy and light are opposites with respect to lunch, as are afternoon and evening
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在午餐方面，“重”和“轻”是相对的，下午和晚上也是如此
- en: The term “brown bag” is lunch-ish like “apple” but less food-ish
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “棕色纸袋”像“苹果”一样有午餐的感觉，但食物感较弱
- en: Although this is a simple, contrived example, the word embeddings developed
    using big data have similar mathematical properties—albeit with a much higher
    number of dimensions. As you will soon see, these additional dimensions allow
    additional aspects of word meaning to be modeled, and enrich the embedding far
    beyond the “lunch” and “food” dimensions illustrated so far.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这是一个简单、人为编造的例子，但使用大数据开发出的词嵌入具有类似的数学特性——尽管维度数量要高得多。正如你很快就会看到的，这些额外的维度允许对词义的其他方面进行建模，并极大地丰富了嵌入，远远超出了迄今为止所展示的“午餐”和“食物”维度。
- en: Example – using word2vec for understanding text in R
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例——在R中使用word2vec理解文本
- en: The previous sections introduced the idea of embedding as a means of encoding
    a highly dimensional concept in a lower-dimensional space. We’ve also learned
    that, conceptually, the process involves training a computer to learn about the
    substitutability of various terms by applying a human-like process of learning
    by association. But so far, we haven’t explored the algorithm that performs this
    feat. There are several such methods, which have been developed by big data companies
    or research universities and shared with the public.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 前几节介绍了将嵌入作为一种在低维空间中编码高度概念的方法。我们还了解到，从概念上讲，这个过程涉及训练计算机通过应用类似人类的学习联想过程来了解各种术语的可替换性。但到目前为止，我们还没有探讨执行这一壮举的算法。有几种这样的方法，这些方法是由大数据公司或研究大学开发的，并已与公众分享。
- en: Perhaps one of the most widely used word embedding techniques is **word2vec**,
    which was published in 2013 by a team of researchers at Google and, as the name
    suggests, literally transforms words to vectors. According to the authors, it
    is not a single algorithm so much as it is a collection of methods that can be
    used for natural language processing tasks. Although there have been many new
    methods published in the time since word2vec was published, it remains popular
    and is well studied even today. Understanding the full scope of word2vec is outside
    the scope of this chapter, but understanding some of its key components will provide
    a foundation upon which many other natural language processing techniques can
    be understood.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 可能最广泛使用的词嵌入技术之一是**word2vec**，该技术于2013年由谷歌研究团队发布，正如其名所示，它将词汇直接转换为向量。根据作者的说法，它不是一个单一的算法，而是一系列可用于自然语言处理任务的方法集合。尽管自word2vec发布以来已经有许多新的方法被提出，但它仍然很受欢迎，并且至今仍被广泛研究。理解word2vec的全貌超出了本章的范围，但了解其一些关键组件将为理解许多其他自然语言处理技术提供一个基础。
- en: For a deep dive into the word2vec approach, see *Efficient Estimation of Word
    Representations in Vector Space by Mikolov, T., Chen, K., Corrado, G., and Dean,
    J., 2013* at [https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781).
    Another early but widely used approach for word embeddings is called the **GloVe
    algorithm**, which was published in 2014 by a team at Stanford University and
    uses a similar set of methods. For more information on GloVe, see [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入了解word2vec方法，请参阅Mikolov, T.，Chen, K.，Corrado, G.和Dean, J.于2013年发表的*《在向量空间中高效估计词表示》*，链接为[https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781)。另一个早期但广泛使用的词嵌入方法是**GloVe算法**，该算法于2014年由斯坦福大学的研究团队发布，并使用了一套类似的方法。有关GloVe的更多信息，请参阅[https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)。
- en: Consider a computer attempting to learn from reading a large corpus of text
    such as a web page or textbook. To begin learning which words are associated and
    are substitutable for one another, the computer will need a formal definition
    of “context” to limit the scope to something more reasonable than the entire text,
    particularly if the text is large. To this end, the word2vec technique defines
    a **window size** parameter that dictates how many words of context will be used
    when attempting to understand a single word. A smaller window size guarantees
    a tight association between words in context, but because related words can appear
    much later in the sentence, making the window too small may lead to missing important
    relationships among words and ideas. Balance is required, because making the window
    too large can pull in unrelated ideas much earlier or later in the text. Typically,
    the window is set to approximately the length of a sentence, or about five to
    ten words, with useless stop words like “and,” “but,” and “the” excluded.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一台计算机试图通过阅读大量文本（如网页或教科书）来学习。为了开始学习哪些词汇是相互关联并可互相替换的，计算机需要一个关于“上下文”的正式定义，以将范围限制在比整个文本更合理的东西上，尤其是如果文本很大。为此，word2vec技术定义了一个**窗口大小**参数，该参数决定了在尝试理解单个词汇时将使用多少上下文词汇。较小的窗口大小保证了上下文中词汇之间的紧密关联，但由于相关词汇可以出现在句子中的较后位置，窗口太小可能会导致错过词汇和思想之间的重要关系。需要平衡，因为窗口太大可能会在文本的较早或较晚位置引入无关的思想。通常，窗口被设置为大约句子的长度，即大约五到十个单词，不包括像“和”、“但是”和“the”这样的无意义停用词。
- en: Given contexts comprising approximately sentence-length sets of words, the word2vec
    process proceeds with one of two methodologies. The **continuous bag-of-words**
    (**CBOW**) methodology trains a model to predict each word from its context; the
    **skip-gram** approach does the inverse and attempts to guess the surrounding
    contextual words when provided with a single input word. Although the underlying
    process is nearly identical for both approaches, there are mathematical nuances
    that lead to different results depending on which one is used.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 给定由大约句子长度的词汇集组成的上下文，word2vec过程采用两种方法之一。**连续词袋模型**（**CBOW**）方法训练一个模型来预测上下文中的每个词汇；**跳字模型**则相反，当提供一个输入词汇时，尝试猜测周围的上下文词汇。尽管两种方法的基本过程几乎相同，但数学上的细微差别会导致使用不同方法时产生不同的结果。
- en: Because we are merely understanding the methods conceptually, it suffices to
    say that the CBOW methodology tends to create embeddings favoring words that are
    nearly identical replacements or true synonyms for one another, such as “apple”
    and “apples” or “burger” and “hamburger,” while the skip-gram method favors terms
    that are conceptually similar, like “apple” and “fruit” or “burger” and “fries.”
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们仅仅是概念上理解这些方法，所以可以说CBOW方法倾向于创建偏好于彼此几乎相同替换或真正同义词的嵌入，例如“apple”（苹果）和“apples”（苹果），或者“burger”（汉堡）和“hamburger”（汉堡包），而skip-gram方法则偏好概念上相似的术语，如“apple”（苹果）和“fruit”（水果）或“burger”（汉堡）和“fries”（薯条）。
- en: 'For both CBOW and skip-gram, the process of developing the embedding is similar
    and can be understood as follows. Beginning from a sentence like “an apple is
    a fruit I eat for lunch,” a model is constructed that attempts to relate a word
    like “apple” to its context, like “fruit,” “eat,” and “lunch.” By iterating over
    huge volumes of such sentences—like “a banana is a fruit people eat for breakfast”
    or “an orange is both a fruit and a color” and so on—the values of the embedding
    can be determined, such that the embedding minimizes the prediction error between
    the word and its context. Words that appear consistently in similar contexts will
    thus have similar values for the embedding and can therefore be treated as similar,
    interchangeable concepts:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CBOW和skip-gram，开发嵌入的过程是相似的，可以理解为以下。从一个句子如“an apple is a fruit I eat for lunch”（我午餐吃苹果是一种水果）开始，构建一个模型，试图将一个词如“apple”（苹果）与其上下文如“fruit”（水果）、“eat”（吃）和“lunch”（午餐）联系起来。通过迭代大量这样的句子——如“a
    banana is a fruit people eat for breakfast”（香蕉是一种人们早餐吃的水果）或“an orange is both
    a fruit and a color”（橙子既是水果也是颜色）等等——可以确定嵌入的值，使得嵌入最小化单词与其上下文之间的预测误差。因此，在相似上下文中一致出现的单词将具有相似的嵌入值，因此可以被视为相似、可互换的概念：
- en: '![Diagram  Description automatically generated](img/B17290_15_06.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![图 描述自动生成](img/B17290_15_06.png)'
- en: 'Figure 15.6: The word2vec process creates an embedding that relates each term
    to its context'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.6：word2vec过程创建了一个将每个术语与其上下文相关联的嵌入
- en: Technically speaking, the word2vec approach is not considered “deep learning”
    even though it is in many ways analogous to deep learning. As depicted in the
    figure that follows, the embedding itself can be imagined as a hidden layer in
    a neural network, here represented with four nodes. In the CBOW approach, the
    input layer is a one-hot encoding of the input term, with one node for each possible
    word in the vocabulary, but only a single node with a value of 1 and the remaining
    nodes set to 0 values. The output layer also has one node per term in the vocabulary
    but can have multiple “1” values—each representing a word appearing in the context
    of the input term.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上来说，尽管word2vec方法在许多方面与深度学习类似，但它不被认为是“深度学习”。如图所示，嵌入本身可以想象成神经网络中的一个隐藏层，这里用四个节点表示。在CBOW方法中，输入层是输入项的一个one-hot编码，每个可能的词汇都有一个节点，但只有一个节点值为1，其余节点设置为0值。输出层也有每个词汇中的一个节点，但可以有多个“1”值——每个值代表出现在输入项上下文中的单词。
- en: 'Note that for the skip-gram approach, this arrangement would be reversed:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于skip-gram方法，这种安排将会相反：
- en: '![Diagram, schematic  Description automatically generated](img/B17290_15_07.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![图，示意图 描述自动生成](img/B17290_15_07.png)'
- en: 'Figure 15.7: Developing an embedding involves training a model in a process
    analogous to deep learning'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.7：开发嵌入涉及训练一个模型，其过程类似于深度学习
- en: Varying the number of nodes in the hidden layer affects the complexity of the
    network as well as the depth of the model’s semantic understanding of each term.
    A greater number of nodes leads to a richer understanding of each term in its
    context but becomes much more computationally expensive to train and requires
    much more training data. Each additional node adds an additional dimension from
    which each term can be distinguished. Too few nodes and the model will have insufficient
    dimensionality to capture the many nuances of how each term can be used—the word
    “orange” as a color versus “orange” as a food, for instance—but using too many
    dimensions may increase the risk of the model being distracted by noise, or worse,
    being useless for the embedding’s initial intended purpose of dimensionality reduction!
    As you will soon see firsthand, even though the embeddings presented so far used
    just a few dimensions for simplicity and illustrative purposes, actual word embeddings
    used in practice typically have hundreds of dimensions and require huge amounts
    of training data and computational power to train.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 调整隐藏层中节点的数量会影响网络的复杂性和模型对每个术语语义理解的深度。节点数量越多，对每个术语在其上下文中的理解就越丰富，但训练成本会大幅增加，并且需要更多的训练数据。每个额外的节点都为区分每个术语提供了一个额外的维度。节点过少时，模型将没有足够的维度来捕捉每个术语使用的许多细微差别——例如，“orange”作为颜色与“orange”作为食物之间的区别——但使用过多的维度可能会增加模型被噪声分散注意力的风险，或者更糟，使得模型对于嵌入的初始目的——降维——变得无用！正如你很快就会亲身体验到的，尽管到目前为止展示的嵌入仅使用了几个维度以保持简单和说明性，但实际中使用的词嵌入通常具有数百个维度，并且需要大量的训练数据和计算能力来训练。
- en: In R, installing the `word2vec` package by Jan Wijffels will provide a wrapper
    for the C++ implementation of the word2vec algorithm. If desired, the package
    can train a word embedding if given a corpus of text data, but it is often preferable
    to use pre-trained embeddings that can be downloaded from the web. Here, we’ll
    use an embedding that was trained using a Google News archive consisting of 100
    billion written words.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在R中，通过Jan Wijffels安装的`word2vec`包将提供对word2vec算法C++实现的封装。如果需要，该包可以在提供文本数据语料库的情况下训练词嵌入，但通常更倾向于使用可以从网络上下载的预训练嵌入。在这里，我们将使用一个使用包含1000亿个书面单词的Google新闻存档进行训练的嵌入。
- en: 'The resulting embedding contains 300-dimensional vectors for 3 million words
    and simple phrases, and is available for download at the Google word2vec project
    page as follows: [https://code.google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/).
    To follow along with the example, look for the link to the `GoogleNews-vectors-negative300.bin.gz`
    file, then download, unzip, and save the file to your R project folder before
    proceeding.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 结果嵌入包含300维向量，用于300万个单词和简单短语，可在以下Google word2vec项目页面下载：[https://code.google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/)。为了跟随示例，查找`GoogleNews-vectors-negative300.bin.gz`文件的链接，然后下载、解压并将文件保存到您的R项目文件夹中，然后再继续。
- en: As a word of warning, the Google News embedding is quite large at about 1.5
    GB compressed (3.4 GB after unzipping) and unfortunately cannot be distributed
    with the code for this chapter. Furthermore, the file can be somewhat hard to
    find on the project website. Try a find command (*Ctrl* + *F* or *Command* + *F*)
    in your web browser to search the page for the file name if needed. Depending
    on your platform, you may need an additional program to unzip files with the Gzip
    compression algorithm (`.gz` file extension).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一句警告，Google新闻嵌入相当大，大约为1.5 GB的压缩文件（解压后为3.4 GB），并且不幸的是，不能与本章的代码一起分发。此外，该文件在项目网站上可能难以找到。如果需要，请在您的网络浏览器中使用查找命令（*Ctrl*
    + *F* 或 *Command* + *F*）搜索页面上的文件名。根据您的平台，您可能需要一个额外的程序来解压使用Gzip压缩算法（`.gz`文件扩展名）的文件。
- en: 'As shown in the code that follows, to read the Google News embedding into R,
    we’ll load the `word2vec` package and use the `read.word2vec()` function. Ensure
    you have downloaded and installed the `word2vec` package and Google News embedding
    before attempting this step:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如以下代码所示，要将Google新闻嵌入读取到R中，我们将加载`word2vec`包并使用`read.word2vec()`函数。在尝试此步骤之前，请确保您已下载并安装了`word2vec`包和Google新闻嵌入：
- en: '[PRE28]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'If the embedding loaded correctly, the `str()` command will show details about
    this pre-trained model:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 如果嵌入加载正确，`str()`命令将显示有关此预训练模型的相关细节：
- en: '[PRE29]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: As expected, the embedding has 300 dimensions for each of the 3 million terms.
    We can obtain these dimensions for a term (or terms) using `predict()` as a lookup
    function on the model object. The `type = "embedding"` parameter requests the
    embedding vector for the term, as opposed to the most similar terms, which will
    be demonstrated shortly.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，嵌入向量对于每个300万个术语都有300个维度。我们可以使用`predict()`作为模型对象的查找函数来获取术语（或术语）的这些维度。`type
    = "embedding"`参数请求术语的嵌入向量，而不是最相似的术语，这将在稍后演示。
- en: 'Here, we’ll request the word vectors for a few terms related to breakfast,
    lunch, and dinner:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将请求与早餐、午餐和晚餐相关的一些术语的词向量：
- en: '[PRE31]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The previous commands created matrices named `foods` and `meals`, with rows
    reflecting the terms and columns representing the 300 dimensions of the embedding.
    We can examine the first few values of a single word vector for *cereal* as follows:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的命令创建了一个名为`foods`和`meals`的矩阵，行反映了术语，列表示嵌入的300个维度。我们可以如下检查单个词向量*cereal*的前几个值：
- en: '[PRE32]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Alternatively, we can examine the first few columns for all foods:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以检查所有食物的前几列：
- en: '[PRE34]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Although we have no idea what each of the five dimensions represents (nor any
    of the remaining 295 dimensions not shown), we would expect similar, more substitutable
    foods and concepts to be closer neighbors in the 300-dimensional space. We can
    take advantage of this to measure the relatedness of the foods to the three main
    meals of the day using the `word2vec_similarity()` function as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们不知道这五个维度代表什么（也不了解未显示的其余295个维度），但我们预计相似、可替代性更强的食物和概念在300维空间中会更接近。我们可以利用这一点，使用`word2vec_similarity()`函数来测量食物与一天三餐的相关性，如下所示：
- en: '[PRE36]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: In this output, higher values indicate greater similarity between the foods
    and each of the three mealtimes, according to the 300-dimension word embedding.
    Unsurprisingly, breakfast foods like cereal, bacon, and eggs are closer to the
    word *breakfast* than they are to *lunch* or *dinner*. Sandwiches and salads are
    closest to lunch, while steak and spaghetti are closest to dinner.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个输出中，更高的值表示食物与三个用餐时间之间的相似性更高，根据300维词嵌入。不出所料，像谷物、培根和鸡蛋这样的早餐食品比午餐或晚餐更接近单词*breakfast*。三明治和沙拉最接近午餐，而牛排和意大利面最接近晚餐。
- en: Although it was not used in the previous example, it is a popular convention
    to use the **cosine similarity** measure, which considers only the direction of
    the compared vectors, rather than the default Euclidean distance-like measure,
    which considers both direction and magnitude. The cosine similarity can be obtained
    by specifying `type = "cosine"` when calling the `word2vec_similarity()` function.
    Here, it is not likely to substantially affect the results because the Google
    News vectors were normalized when they were loaded into R.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然它在前面的例子中没有使用，但使用**余弦相似度**度量是一个流行的约定，它只考虑比较向量的方向，而不是默认的类似于欧几里得距离的度量，后者考虑方向和大小。可以通过在调用`word2vec_similarity()`函数时指定`type
    = "cosine"`来获得余弦相似度。在这里，它不太可能对结果产生重大影响，因为当Google新闻向量被加载到R中时，它们已经被归一化了。
- en: 'For a more practical application of word2vec concepts, let’s revisit the hypothetical
    social media posts presented earlier and attempt to determine whether to present
    the users with a breakfast, lunch, or dinner advertisement. We’ll start by creating
    a `user_posts` character vector, which stores the raw text of each of the posts:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更实际地应用word2vec概念，让我们回顾一下之前提出的假设社交媒体帖子，并尝试确定是否向用户展示早餐、午餐或晚餐广告。我们将首先创建一个`user_posts`字符向量，该向量存储每篇帖子的原始文本：
- en: '[PRE38]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Importantly, there is a substantial hurdle we must pass before applying word2vec
    to each of the user posts; specifically, each post is a sentence composed of multiple
    terms, and word2vec is only designed to return vectors for single words. Unfortunately,
    there is no perfect solution to this problem, and choosing the correct solution
    may depend on the desired use case. For instance, if the application is intended
    merely to identify people that post about a particular subject, it may suffice
    to iterate over each word in the post and determine whether any of the words meet
    a similarity threshold.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，在将word2vec应用于每个用户帖子之前，我们必须克服一个重大的障碍；具体来说，每个帖子是由多个术语组成的句子，而word2vec仅设计用于返回单个单词的向量。不幸的是，这个问题没有完美的解决方案，选择正确的解决方案可能取决于预期的用例。例如，如果应用程序的目的是仅识别发布特定主题的人，那么遍历帖子中的每个单词并确定是否有任何单词达到相似度阈值可能就足够了。
- en: More complex alternative solutions exist for solving the problem of applying
    word2vec to longer strings of text. A common but somewhat crude solution involves
    simply averaging the word2vec vectors across all words in the sentence, but this
    often results in poor results for much the same reason that mixing too many colors
    of paint results in an ugly shade of brown. As sentences grow longer, averaging
    across all words creates a muddy mess due to the fact that some words will inevitably
    have vectors in opposite directions and the resulting average is meaningless.
    Moreover, as sentences grow in complexity, it is more likely that word order and
    grammar will affect the meaning of the words in the sentence.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 存在更多复杂的替代方案来解决将word2vec应用于较长的文本字符串的问题。一个常见的但相对粗糙的解决方案是简单地平均句子中所有单词的word2vec向量，但这种方法往往会导致较差的结果，原因与混合过多的油漆颜色导致难看的棕色色调相似。随着句子的变长，对所有单词的平均处理会由于一些单词的向量不可避免地指向相反方向而造成混乱，导致平均结果毫无意义。此外，随着句子的复杂性增加，单词顺序和语法更有可能影响句子中单词的意义。
- en: An approach called doc2vec attempts to address this by adapting the training
    of word2vec to longer blocks of text, called documents, which need not be full
    documents but may be paragraphs or sentences. The premise of doc2vec is to create
    an embedding for each document based on the words appearing in the document. Document
    vectors can then be compared to determine the overall similarity between two documents.
    In our case, the goal would be to compare whether two documents (that is, sentences)
    are conveying similar ideas—for instance, is a user’s post like other sentences
    that were about breakfast, lunch, or dinner?
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 一种称为doc2vec的方法试图通过调整word2vec的训练以适应更长的文本块，称为文档，这些文档不必是完整的文档，但可能是段落或句子。doc2vec的原理是基于文档中出现的单词为每个文档创建一个嵌入。然后，可以通过比较文档向量来确定两个文档的整体相似度。在我们的案例中，目标将是比较两个文档（即句子）是否传达了相似的想法——例如，用户的帖子是否与其他关于早餐、午餐或晚餐的句子相似？
- en: Unfortunately, we do not have access to a doc2vec model to use this more sophisticated
    approach, but we can apply the `word2vec` package’s `doc2vec()` function to create
    a document vector for each user post and treat the document vector as if it were
    a single word. As stated previously, for longer sentences this may create a muddied
    vector, but because social media posts are often short and to the point, this
    issue may be mitigated.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们无法访问doc2vec模型来使用这种更复杂的方法，但我们可以应用`word2vec`包的`doc2vec()`函数为每个用户帖子创建一个文档向量，并将文档向量视为一个单独的单词。正如之前所述，对于较长的句子，这可能会创建一个混乱的向量，但由于社交媒体帖子通常简短且直接，这个问题可能得到缓解。
- en: 'We’ll begin by loading the `tm` package, which was introduced in *Chapter 4*,
    *Probabilistic Learning – Classification Using Naive Bayes*, as a collection of
    tools for processing text data. The package provides a `stopwords()` function,
    which can be combined with its `removeWords()` function to remove unhelpful terms
    from social media posts. Then, the `txt_clean_word2vec()` function is used to
    prepare the posts for use with `doc2vec`:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先加载`tm`包，该包在*第4章*，*概率学习 - 使用朴素贝叶斯进行分类*中介绍，作为处理文本数据的一系列工具。该包提供了一个`stopwords()`函数，可以与它的`removeWords()`函数结合使用，从社交媒体帖子中删除无用的术语。然后，使用`txt_clean_word2vec()`函数为使用`doc2vec`做准备：
- en: '[PRE39]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'To see the result of this processing, let’s look at the first cleaned user
    post:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看这种处理的成果，让我们看看第一个清理过的用户帖子：
- en: '[PRE40]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'As expected, the text has been standardized and all unhelpful words have been
    removed. We can then supply the posts to the `doc2vec()` function, along with
    the pre-trained Google News word2vec model as follows:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，文本已经被标准化，并且所有无用的词汇都被移除了。然后我们可以将帖子提供给`doc2vec()`函数，并附带预训练的Google News
    word2vec模型，如下所示：
- en: '[PRE42]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The result of this operation is a matrix with three rows (one for each document)
    and 300 columns (one for each dimension in the embedding). The `str()` command
    shows the first few values of this matrix:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 这个操作的结果是包含三行（每行代表一个文档）和300列（每列代表嵌入中的每个维度）的矩阵。`str()`命令显示了该矩阵的前几个值：
- en: '[PRE43]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We’ll need to compare these pseudo-document vectors to the word vectors for
    breakfast, lunch, and dinner. These vectors were created previously using the
    `predict()` function and the word2vec model, but the code is repeated here for
    clarity:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将这些伪文档向量与早餐、午餐和晚餐的词向量进行比较。这些向量之前使用`predict()`函数和word2vec模型创建，但在此处重复代码以保持清晰：
- en: '[PRE45]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Finally, we can compute the similarity between the two. Each row represents
    a user’s post, and the column values indicate the similarity between that post’s
    document vector and the corresponding term:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以计算这两个向量之间的相似度。每一行代表一个用户的帖子，列值表示该帖子的文档向量与相应术语之间的相似度：
- en: '[PRE46]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Unsurprisingly, the user post about bacon and eggs is most similar to the word
    breakfast, while the post with sandwiches is most similar to lunch, and the evening
    date is most related to dinner. We could use the maximum similarity per row to
    determine whether to display a breakfast, lunch, or dinner advertisement to each
    user.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 出乎意料的是，关于培根和鸡蛋的用户帖子与早餐这个词最相似，而关于三明治的帖子与午餐最相似，而晚上的约会与晚餐最相关。我们可以使用每行的最大相似度来确定是否向每个用户显示早餐、午餐或晚餐广告。
- en: Document vectors can also be used directly as predictors in supervised machine
    learning tasks. For example, *Chapter 14*, *Building Better Learners*, described
    a theoretical model for predicting a Twitter user’s gender or future purchasing
    behavior based on the user’s basic profile data, profile picture, and social media
    post text.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 文档向量也可以直接用作监督机器学习任务中的预测器。例如，*第14章*，*构建更好的学习者*，描述了一个基于用户的基线数据、个人资料图片和社交媒体帖子文本预测Twitter用户性别或未来购买行为的理论模型。
- en: 'The chapter proposed ensembling a traditional machine learning model with a
    deep learning model for the image data and a naive Bayes text model for the user
    posts. Alternatively, it is possible to use document vectors as is by treating
    the 300 dimensions as 300 individual predictors that the supervised learning algorithm
    can use to determine which are relevant to predicting the user’s gender:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 该章节提出了将传统机器学习模型与用于图像数据的深度学习模型以及用于用户帖子的朴素贝叶斯文本模型进行集成。或者，也可以直接使用文档向量，将300个维度视为300个单独的预测器，监督学习算法可以使用这些预测器来确定哪些与预测用户的性别相关：
- en: '![A picture containing text, device, gauge  Description automatically generated](img/B17290_15_08.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本、设备和仪表的图片，自动生成描述](img/B17290_15_08.png)'
- en: 'Figure 15.8: The values for a document vector resulting from unstructured text
    data can be used in a predictive model side by side with the more conventional
    predictors'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.8：来自非结构化文本数据的文档向量的值可以与更传统的预测器并排用于预测模型
- en: This strategy of creating a document vector for an unstructured block of text
    and using the resultant embedding values as predictors for supervised learning
    is quite generalizable as a means of enhancing the performance of a conventional
    machine learning approach. Many datasets include unstructured text fields that
    go unused in conventional models due to their complexity or the inability to train
    a language model. However, a relatively simple transformation made possible by
    a pre-trained word embedding allows the text data to be used in the model alongside
    the other predictors. Thus, there is little excuse not to incorporate this approach
    and provide the learning algorithm with an infusion of big data the next time
    you encounter this type of machine learning task.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 这种为无结构的文本块创建文档向量，并使用结果嵌入值作为监督学习预测器的策略，作为一种增强传统机器学习性能的方法，相当具有通用性。许多数据集包括未使用的非结构化文本字段，因为它们的复杂性或无法训练语言模型。然而，通过预训练的词嵌入实现的相对简单的转换使得文本数据可以在模型中与其他预测器一起使用。因此，没有理由不采用这种方法，并在下次遇到此类机器学习任务时为学习算法提供大数据的注入。
- en: Visualizing highly dimensional data
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化高维数据
- en: Data exploration is one of the five key steps involved in any machine learning
    project, and thus is not immune to the so-called curse of dimensionality—the tendency
    of a project to become increasingly challenging as the number of features increases.
    Visualization techniques that work on simpler datasets may become useless as the
    number of dimensions grows unmanageable; for example, a scatterplot matrix may
    help identify relationships for a dozen or so features, but as the number grows
    bigger to dozens or hundreds of features, then what was once a helpful visualization
    may quickly turn into information overload.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 数据探索是任何机器学习项目中涉及的五个关键步骤之一，因此不会免受所谓的维度诅咒——随着特征数量的增加，项目变得越来越具有挑战性的趋势。在处理更简单数据集上的可视化技术可能随着维度的增加而变得无用；例如，散点图矩阵可能有助于识别十几个特征之间的关系，但当特征数量增加到几十或几百时，曾经有帮助的可视化可能迅速变成信息过载。
- en: Likewise, we can interpret a 2-D or even a three-dimensional plot without too
    much difficulty, but if we hope to understand the relationship among four or more
    dimensions, an entirely different approach is needed.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以没有太多困难地解释二维甚至三维的图表，但如果我们希望理解四个或更多维度之间的关系，则需要一种完全不同的方法。
- en: Though physics suggests there are ten or eleven dimensions of the universe,
    we only experience four, and only interact directly with three of them. Perhaps
    for this reason, our brains are attuned to understanding visuals in at most three
    dimensions; moreover, because most of our intellectual work is on 2-D surfaces
    like blackboards, whiteboards, paper, or computer screens, we are accustomed to
    seeing data represented in at most two dimensions. One day, as virtual or augmented
    reality computer interfaces become more prevalent, we may see an explosion of
    innovation in three-dimensional visualizations, but until that day comes, there
    is a need for tools that can aid the display of highly dimensional relationships
    in no more than two dimensions.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然物理学表明宇宙有十个或十一个维度，但我们只体验到四个，并且只直接与其中三个互动。也许正因为如此，我们的大脑适应了最多在三个维度上理解视觉；此外，因为我们的大部分智力工作都是在黑板、白板、纸张或计算机屏幕这样的二维表面上进行的，所以我们习惯于看到最多在两个维度上表示的数据。有一天，随着虚拟或增强现实计算机界面的更加普及，我们可能会看到三维可视化创新的爆炸式增长，但直到那一天到来之前，我们需要工具来帮助在最多两个维度内展示高度维度的关系。
- en: 'Reducing the dimensionality of a highly dimensional visualization to just two
    dimensions may seem like an impossibility, but the premise guiding the process
    is surprisingly straightforward: points that are closely positioned in the highly
    dimensional space need to be positioned closely in the 2-D space. If you are thinking
    that this idea sounds somewhat familiar, you would not be wrong; this is the same
    concept that guides embeddings, as described earlier in this chapter. The key
    difference is that while an embedding technique like word2vec reduces highly dimensional
    data down to a few hundred dimensions, embeddings for visualization must reduce
    the dimensionality even further to only two dimensions.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 将高度维度的可视化降低到仅两个维度似乎是不可能的，但指导这一过程的原理却出奇地简单：在高度维度空间中位置接近的点需要在二维空间中保持接近的位置。如果你认为这个想法听起来有些熟悉，你并不会错；这正是本章前面描述的嵌入所指导的相同概念。关键的区别在于，虽然像word2vec这样的嵌入技术可以将高度维度的数据降低到几百维，但用于可视化的嵌入必须进一步降低维度，仅保留两个维度。
- en: The limitations of using PCA for big data visualization
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用PCA进行大数据可视化的局限性
- en: '**Principal component analysis** (**PCA**), which was introduced in *Chapter
    13*, *Challenging Data – Too Much, Too Little, Too Complex*, is one approach capable
    of reducing a highly dimensional dataset to two dimensions. You may recall that
    PCA works by expressing the covariance of multiple correlated attributes as a
    single vector. In this way, from the larger set of features, a smaller number
    of new features, called components, can be synthesized. If the number of components
    is set to two, a high-dimensional dataset can then be visualized in a simple scatterplot.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**），在第13章“挑战性数据 – 过多、过少、过于复杂”中介绍，是一种能够将高度维度的数据集降低到二维的方法。你可能还记得，PCA通过将多个相关属性的协方差表示为一个单一向量来工作。通过这种方式，从更大的特征集中，可以合成较少的新特征，称为成分。如果将成分的数量设置为两个，那么高度维度的数据集就可以通过简单的散点图进行可视化。'
- en: 'We’ll apply this visualization technique to the 36-dimension social media profile
    dataset first introduced in *Chapter 9*, *Finding Groups of Data – Clustering
    with k-means*. The first few steps are straightforward; we use the tidyverse to
    read the data and select the 36 columns of interest, set the random seed to `123456`
    to ensure your results match the book, then use the `prcomp_irlba()` function
    from the `irlba` package to find the two principal components of the dataset:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先将这种可视化技术应用于在第9章*寻找数据组 – 使用k-means进行聚类*中首次介绍过的36维社交媒体个人资料数据集。前几个步骤很简单；我们使用tidyverse读取数据并选择感兴趣的36列，设置随机种子为`123456`以确保你的结果与书中的一致，然后使用`irlba`包中的`prcomp_irlba()`函数找到数据集的两个主成分：
- en: '[PRE48]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The `sns_pca$x` object contains a transformed version of the original dataset
    in which the 36 original dimensions have been reduced to 2\. Because this is stored
    as a matrix, we’ll first convert it to a data frame before piping it into a `ggplot()`
    function to create a scatterplot:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '`sns_pca$x` 对象包含原始数据集的转换版本，其中36个原始维度已减少到2。由于这是以矩阵形式存储的，我们首先将其转换为数据框，然后再将其传递到`ggplot()`函数中创建散点图：'
- en: '[PRE49]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The resulting visualization appears as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 结果可视化如下：
- en: '![](img/B17290_15_09.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_15_09.png)'
- en: 'Figure 15.9: Principal component analysis (PCA) can be used to create 2-D visualizations
    of highly dimensional datasets, but the results are not always especially helpful'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.9：主成分分析（PCA）可以用于创建高维数据集的二维可视化，但结果并不总是特别有帮助
- en: Unfortunately, this scatterplot reveals a limitation of using PCA for data exploration,
    which is that the two principal components often create little visual separation
    among the points in 2-D space. Based on our prior work in *Chapter 9*, *Finding
    Groups of Data – Clustering with k-means*, we know that there are clusters of
    social media users that use similar keywords on their social media profiles. These
    clusters ought to be visible as distinct groupings in the scatterplot, but instead,
    we see one large group of points and a scattering of apparent outliers around
    the perimeter. The disappointing result here is not specific to the dataset used
    here and is typical of PCA when used in this way. Thankfully, there is another
    algorithm that is better suited to data exploration, which will be introduced
    in the next section.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这个散点图揭示了使用PCA进行数据探索的一个局限性，即两个主成分通常在二维空间中在点之间产生很少的视觉分离。根据我们在*第9章*，*寻找数据组
    – 使用k-means进行聚类*中的先前工作，我们知道存在使用社交媒体用户在社交媒体个人资料中使用相似关键词的集群。这些集群应该作为不同的分组在散点图中可见，但相反，我们看到一个大的点群和围绕边缘的明显异常值的散布。这里令人失望的结果并不仅限于这里使用的数据集，而且在这种方式使用PCA时是典型的。幸运的是，还有一种更适合数据探索的算法，将在下一节中介绍。
- en: Understanding the t-SNE algorithm
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解t-SNE算法
- en: The underlying math of the PCA technique utilizes covariance matrices to perform
    a linear dimensionality reduction, and the resulting principal components are
    intended to capture the overall variance of the dataset. The effect is like a
    compression algorithm that reduces the dimensionality of a dataset by eliminating
    redundant information. While this is obviously an important and useful attribute
    for a dimensionality reduction technique, it is less helpful for data visualization.
    As we observed in the previous section, this tendency of PCA to “compress” the
    dimensions may obscure important relationships in the data—the exact type of relationships
    we hope to discover when performing big data exploration.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: PCA技术的底层数学利用协方差矩阵执行线性降维，并且得到的主成分旨在捕捉数据集的整体方差。这种效果就像是一种压缩算法，通过消除冗余信息来减少数据集的维度。虽然这显然是降维技术的一个重要且有用的属性，但对于数据可视化来说帮助不大。正如我们在上一节中观察到的，PCA“压缩”维度的这种趋势可能会掩盖数据中的重要关系——这正是我们在进行大数据探索时希望发现的关系的确切类型。
- en: A technique called **t-Distributed Stochastic Neighbor Embedding**, or **t-SNE**
    for short, is designed precisely as a tool for the visualization of high-dimensional
    datasets and thus addresses the previously mentioned shortcomings of PCA. The
    t-SNE approach was published in 2008 by Laurens van der Maaten, and it has quickly
    become a de facto standard for big data visualization for high-dimensional real-world
    datasets. Van der Maaten and others have published and presented numerous case
    studies contrasting PCA and t-SNE, and illustrating the strengths of the latter.
    However, because the math that drives t-SNE is highly complex, we will focus on
    understanding it conceptually and comparing it to other related methods covered
    previously.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 一种称为**t-Distributed Stochastic Neighbor Embedding**的技术，简称**t-SNE**，正是为了精确地作为高维数据集可视化的工具，因此解决了之前提到的PCA的不足。t-SNE方法由Laurens
    van der Maaten于2008年发表，并迅速成为高维现实数据集大数据可视化的实际标准。Van der Maaten及其他人发表了大量的案例研究，对比PCA和t-SNE，并说明了后者的优势。然而，由于驱动t-SNE的数学非常复杂，我们将专注于从概念上理解它，并将其与其他之前介绍的相关方法进行比较。
- en: For a deep dive into the mechanics of the t-SNE algorithm, see the original
    publication, *Visualizing Data using t-SNE, van der Maaten, L. and Hinton, G.,
    Journal of Machine Learning Research 9, 2008, pp. 2579-2606*.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入了解t-SNE算法的机制，请参阅原始出版物，《使用t-SNE可视化数据》，作者van der Maaten, L.和Hinton, G.，发表于《机器学习研究》第9卷，2008年，第2579-2606页。
- en: Just like with any technique for visualizing highly dimensional datasets, the
    goal of t-Distributed Stochastic Neighbor Embedding is to ensure that points or
    “neighbors” that are close in the high-dimensional space are positioned closely
    in the low-dimensional (2-D or 3-D) space.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 就像任何用于可视化高维数据集的技术一样，t-Distributed Stochastic Neighbor Embedding的目标是确保在多维空间中靠近的点或“邻居”在低维（2-D或3-D）空间中也是紧密排列的。
- en: The word *embedding* in the t-SNE name highlights the close connection between
    this and the more general task of constructing an embedding, as described in prior
    sections. However, as will be apparent shortly, t-SNE uses an approach unlike
    the deep learning analogue that is used for creating a word embedding. For starters,
    the word *stochastic* in the t-SNE name describes the non-deterministic nature
    of the algorithm, which implies that there is a relatively large degree of randomness
    in the output. But there are also more fundamental differences.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE名称中的单词*embedding*突出了其与更一般任务之间的紧密联系，即构建嵌入，如前几节所述。然而，正如很快就会显现的，t-SNE使用的方法与用于创建词嵌入的深度学习类似物不同。首先，t-SNE名称中的单词*stochastic*描述了算法的非确定性，这意味着输出中存在相当大的随机性。但还有更多根本性的差异。
- en: To begin to understand the t-SNE algorithm, imagine if the task were merely
    to reduce from three dimensions to two. In this case, if the data points were
    somehow depicted as small balls suspended in the air in three-dimensional space,
    and the same number of data points were placed randomly as flat discs on the ground
    in 2-D space, then a human could perform the dimensionality reduction by observing
    each ball in 3-D space, identifying its set of neighbors, and then carefully moving
    the discs in 2-D space to place neighbors closer together. Of course, this is
    more challenging than it sounds, because moving discs closer together and further
    apart in the flat space may inadvertently create or eliminate groupings relative
    to the 3-D space. For instance, moving point A to be closer to its neighbor point
    B may also move A closer to point C, when A and C should be distant according
    to the higher-dimensional space. For this reason, it would be important to iterate,
    observing each 3-D point’s neighborhood and shifting its 2-D neighbors until the
    overall 2-D representation is relatively stable.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始理解t-SNE算法，想象一下如果任务仅仅是把三维降低到二维。在这种情况下，如果数据点以某种方式在三维空间中描绘为悬挂在空中的小球，而在二维空间中放置相同数量的数据点作为平面的圆盘，那么人类可以通过观察三维空间中的每个球，识别其邻居集，然后仔细地将二维空间中的圆盘移动到使邻居更靠近的位置来完成降维。当然，这比听起来要困难得多，因为在平面上移动圆盘使其更靠近或更远可能会无意中在三维空间中创建或消除分组。例如，将点A移动到更靠近其邻居点B的位置时，也可能使A更靠近点C，而根据高维空间，A和C应该是遥远的。因此，迭代观察每个三维点的邻居并移动其二维邻居，直到整体二维表示相对稳定，这一点非常重要。
- en: The same basic process can be performed algorithmically in a much larger number
    of dimensions using a series of mathematical steps. First, the similarity of each
    point in high-dimensional space is computed—traditionally, using the familiar
    metric of Euclidian distance as with k-means and k-nearest neighbors in earlier
    chapters. This similarity metric is used to define a conditional probability distribution
    stating that similar points are proportionally more probable to be neighbors in
    the high-dimensional space. Likewise, a similar distance metric and conditional
    probability distribution is defined for the low-dimensional space. With these
    two metrics defined, the algorithm must then optimize the entire system such that
    the overall error for the high- and low-dimensional probability distributions
    is minimized. Keep in mind that the two are inseparably linked by the fact they
    rely on the same set of examples; the coordinates are known for the high-dimensional
    space, so it is essentially solving for a way to transform the high-dimensional
    coordinates into a low-dimensional space while preserving the similarity as much
    as possible.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的基本过程可以通过一系列数学步骤在更多的维度上算法化执行。首先，计算高维空间中每个点的相似性——传统上，使用熟悉的欧几里得距离度量为标准，如前几章中的k-means和k-最近邻。这个相似性度量用于定义一个条件概率分布，表明相似点在更高维空间中成为邻居的可能性成比例更高。同样，为低维空间定义了类似的距离度量和条件概率分布。定义了这两个度量后，算法必须优化整个系统，使得高维和低维概率分布的整体误差最小化。记住，这两个度量通过它们依赖于相同的一组示例而不可分割地联系在一起；高维空间的坐标是已知的，因此本质上是在寻找一种方法，将高维坐标转换到低维空间，同时尽可能保留相似性。
- en: 'Given that the t-SNE algorithm is so different than PCA, it is no surprise
    that there are many differences in how they perform. An overall comparison of
    the two approaches is presented in the following table:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 由于t-SNE算法与PCA如此不同，它们在性能上的许多差异也就不足为奇了。以下表格展示了这两种方法的总体比较：
- en: '| **PCA** | **t-SNE** |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| **PCA** | **t-SNE** |'
- en: '|'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Tends to compress the visualization
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 倾向于压缩可视化
- en: Global (overall) variance is depicted
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 展示全局（总体）方差
- en: Deterministic algorithm will produce the same result each run
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定性算法每次运行都会产生相同的结果
- en: Does not have hyperparameters to be set
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有需要设置的超参数
- en: Relatively fast (for datasets that can fit in memory)
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相对较快（对于可以放入内存的数据集）
- en: Involves linear transformations
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 涉及线性变换
- en: Useful as a general dimensionality reduction technique by creating additional
    principal components
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过创建额外的主成分，可以作为通用的降维技术使用
- en: '|'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Tends to cluster the visualization
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 倾向于将可视化聚类
- en: Local variance is more apparent
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 局部方差更为明显
- en: Stochastic algorithm introduces randomness into the result
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机算法将随机性引入结果
- en: Result can be sensitive to hyperparameters
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果可能对超参数敏感
- en: Relatively slow (but faster approximations exist)
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相对较慢（但存在更快的近似方法）
- en: Involves non-linear transformations
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 涉及非线性变换
- en: Typically used only as a data visualization technique (two or three dimensions)
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常仅作为数据可视化技术（二维或三维）使用
- en: '|'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: As a rule of thumb, t-SNE is generally the more appropriate tool for big data
    visualization, but it is worth noting a few differences that can be weaknesses
    or present challenges in certain circumstances. First, we have observed that PCA
    can do a poor job at depicting natural clusters in the data, but t-SNE is so apt
    at presenting clusters that it can occasionally even form clusters in a dataset
    without these types of natural divisions. This fault is compounded by the fact
    that t-SNE is a non-deterministic algorithm that is often quite sensitive to the
    values of its hyperparameters; setting these parameters poorly is more likely
    to create false clusters or obscure real ones. Lastly, the t-SNE algorithm involves
    iterating repeatedly over a relatively slow process, but stopping too early often
    produces a poor result or creates a false sense of the dataset’s structure; unfortunately,
    it is also possible that too many iterations will lead to the same problems!
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 按照惯例，t-SNE 通常是大数据可视化的更合适工具，但值得注意的是一些差异，这些差异在某些情况下可能是弱点或挑战。首先，我们观察到主成分分析（PCA）在描绘数据中的自然聚类方面可能做得不好，但
    t-SNE 在呈现聚类方面非常擅长，有时甚至可以在没有这些自然划分的数据集中形成聚类。这种错误由于 t-SNE 是一个非确定性算法，通常对超参数的值非常敏感而加剧；设置这些参数不当更有可能创建虚假的聚类或掩盖真实的聚类。最后，t-SNE
    算法涉及反复迭代一个相对较慢的过程，但过早停止通常会产生较差的结果或产生对数据集结构的错误感觉；不幸的是，过多的迭代也可能导致相同的问题！
- en: These challenges are not listed here to imply that t-SNE is more work than it
    is worth, but rather to encourage treating the output with a degree of skepticism
    until it has been thoroughly explored. This may mean testing various hyperparameter
    combinations, or it may involve a qualitative examination of the visualization,
    such as investigating the identified clusters by hand in order to determine what
    features the neighborhood has in common. We’ll see some of these potential pitfalls
    in practice in the next section, which applies t-SNE to a familiar real-world
    dataset.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 列出这些挑战并不是为了暗示 t-SNE 的工作量大于其价值，而是为了鼓励在彻底探索之前对输出持一定程度的怀疑态度。这可能意味着测试各种超参数组合，或者可能涉及对可视化进行定性检查，例如通过手动调查已识别的聚类来确定邻域有哪些共同特征。我们将在下一节中看到一些这些潜在陷阱的实际应用，该节将
    t-SNE 应用于熟悉的现实世界数据集。
- en: Example – visualizing data’s natural clusters with t-SNE
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例 - 使用 t-SNE 可视化数据的自然聚类
- en: 'To illustrate the ability of t-SNE to depict a dataset’s natural clusters,
    we’ll apply the method to the same 36-dimensional social media profile dataset
    used previously with PCA. Beginning as before, we’ll read the raw data into R
    using the tidyverse, but because t-SNE is somewhat computationally expensive,
    we use the `slice_sample()` command to limit the dataset to a random sample of
    5,000 users. This is not strictly necessary but will speed up the execution time
    and make the visualization less dense and thus easier to read. Don’t forget to
    use the `set.seed(123)` command to ensure your results match those that follow:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明 t-SNE 描绘数据集自然聚类的能力，我们将该方法应用于之前与 PCA 一起使用的相同的 36 维社交媒体配置文件数据集。像之前一样，我们将使用
    tidyverse 将原始数据读入 R，但由于 t-SNE 在计算上有些昂贵，我们使用 `slice_sample()` 命令将数据集限制为 5,000 个用户的随机样本。这并非绝对必要，但可以加快执行时间并使可视化不那么密集，从而更容易阅读。别忘了使用
    `set.seed(123)` 命令以确保你的结果与以下结果匹配：
- en: '[PRE50]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Even with a relatively small sample, the standard t-SNE implementation can still
    be rather slow. Instead, we will use a faster version called the **Barnes-Hut
    implementation**. The Barnes-Hut algorithm was originally developed to simulate
    the so-called “*n*-body” problem—the complex system of gravitational relationships
    that arises among a set of *n* celestial bodies. Because every object exerts a
    force on every other object, exactly computing the net force for each body requires
    *n* *× n = n*² calculations. This becomes computationally infeasible at an astronomical
    scale due to the scope of the universe and the virtually limitless numbers of
    objects within. Barnes-Hut simplifies this problem using a heuristic that treats
    more distant objects as a group identified by its center of mass, and only performs
    the exact calculations for objects closer than a threshold represented by the
    Greek letter *theta*. Larger values of theta drastically reduce the number of
    calculations needed to perform the simulation, while setting theta to zero performs
    the exact calculation.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是相对较小的样本，标准的t-SNE实现也可能相当慢。相反，我们将使用一个名为**Barnes-Hut实现**的更快版本。Barnes-Hut算法最初是为了模拟所谓的“*n*-body”问题——一组*n*个天体之间出现的复杂引力关系系统。由于每个物体都对其他每个物体施加力，精确计算每个物体的总力需要*n*
    *× n = n*²次计算。由于宇宙的规模和其中几乎无限数量的物体，这在天文尺度上变得计算上不可行。Barnes-Hut通过使用一种启发式方法简化了这个问题，该方法将更远的物体视为一个以其质心为标识的组，并且只对距离小于由希腊字母*theta*表示的阈值的物体进行精确计算。theta值越大，所需的计算次数就越少，而将theta设置为零则执行精确计算。
- en: Because the role of t-SNE can be imagined as an *n*-body problem of positioning
    points in space, with each point’s force of attraction to other points in the
    2-D space based on how similar it is to the same points in the high-dimensional
    space, the Barnes-Hut simplification can be applied to simplify the computation
    of the system’s gravity-like forces. This provides a t-SNE implementation that
    is much faster and scales much better on large datasets.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 因为t-SNE的作用可以想象为在空间中定位点的*n*-body问题，其中每个点对其他点在2-D空间中的吸引力基于它与高维空间中相同点的相似程度，Barnes-Hut简化可以应用于简化系统类似重力作用的计算。这提供了一个在大型数据集上运行更快且扩展性更好的t-SNE实现。
- en: The `Rtsne` package, which you should install if you have not done so already,
    provides a wrapper for the C++ implementation of Barnes-Hut t-SNE. It also includes
    other optimizations for use with very large-dimensional datasets. One of these
    optimizations includes an initial PCA step, which by default reduces the dataset
    to its first 50 principal components.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '`Rtsne` 包，如果你还没有安装，应该安装它，它提供了一个对C++实现的Barnes-Hut t-SNE的包装。它还包括用于处理高维数据集的其他优化。这些优化之一包括一个初始的PCA步骤，默认情况下将数据集减少到其前50个主成分。'
- en: Admittedly, it may seem odd to use PCA as part of the t-SNE process, but the
    two have complementary strengths and weaknesses. While t-SNE tends to struggle
    with the curse of dimensionality, PCA is strong at dimensionality reduction; likewise,
    while PCA tends to obscure local variance, t-SNE highlights the data’s natural
    structures. Using PCA to reduce the dimensionality and following this with the
    t-SNE process applies both techniques’ strengths. In our case, with a dataset
    having only 36 dimensions, the PCA step does not meaningfully affect the result.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然使用PCA作为t-SNE过程的一部分可能看起来有些奇怪，但这两个方法各有互补的优势和劣势。t-SNE往往难以处理维度诅咒，而PCA在降维方面很强；同样，PCA往往掩盖局部方差，而t-SNE突出了数据的天然结构。使用PCA来降低维度，然后跟随t-SNE过程应用了两种技术的优势。在我们的案例中，由于数据集只有36个维度，PCA步骤对结果没有实质性影响。
- en: 'We’ll begin by running a t-SNE process with the default parameters. After setting
    the random seed, the 5,000-row sample is piped into a `select()` command to choose
    only the 36 columns that measure the counts of various terms used on each user’s
    profile. This is then piped into the `Rtsne()` function with `check_duplicates
    = FALSE` to prevent an error message that occurs when the dataset has duplicate
    rows. Duplicate rows are found in the social media dataset chiefly because there
    are many users who have counts of zero for all 36 terms. There is no reason that
    the t-SNE method cannot handle these duplicates, but including them may lead to
    unexpected or unsightly results in the visualization when the algorithm attempts
    to arrange such a tightly clustered set of points. For social media users, seeing
    this cluster will be helpful, so we will override the `Rtsne()` function’s default
    as follows:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先使用默认参数运行 t-SNE 过程。在设置随机种子后，5,000 行样本通过 `select()` 命令被导入，以仅选择每个用户资料中使用的各种术语计数的
    36 列。然后，这些数据通过 `Rtsne()` 函数导入，其中 `check_duplicates = FALSE` 以防止当数据集存在重复行时出现的错误信息。在社交媒体数据集中发现重复行主要是因为许多用户对所有
    36 个术语的计数为零。没有理由认为 t-SNE 方法不能处理这些重复项，但包括它们可能导致算法在尝试排列如此紧密的点集时出现意外或不美观的结果。对于社交媒体用户来说，看到这个簇将是有帮助的，因此我们将覆盖
    `Rtsne()` 函数的默认设置如下：
- en: '[PRE51]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Piping a dataset into the `distinct()` function will eliminate duplicate rows
    and can be used prior to the `Rtsne()` command.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据集导入 `distinct()` 函数将消除重复行，可以在 `Rtsne()` 命令之前使用。
- en: 'The 2-D representation of the 36-dimensional dataset is stored as a matrix
    named `Y` in the `sns_tsne` list object created by the `Rtsne()` function. This
    has 5,000 rows representing the social media users, and two columns representing
    the (*x*, *y*) coordinates of each user. After converting the matrix to a data
    frame, we can pipe these values into a `ggplot()` function to visualize the t-SNE
    result as follows:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 36 维数据集的二维表示存储在 `sns_tsne` 列表对象中，名为 `Y` 的矩阵由 `Rtsne()` 函数创建。这个矩阵有 5,000 行，代表社交媒体用户，两列代表每个用户的
    (*x*, *y*) 坐标。在将矩阵转换为数据框后，我们可以将这些值导入 `ggplot()` 函数，如下所示可视化 t-SNE 结果：
- en: '[PRE52]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Displayed side by side with the earlier PCA visualization, it’s remarkable
    to see the vast improvement in visual clarity that the t-SNE technique provides.
    Distinct clusters of users can be observed, reflecting these users’ similarities
    in the 36-dimensional space:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 与早期的 PCA 可视化并排显示，可以看到 t-SNE 技术提供的视觉清晰度的巨大改进。可以观察到不同的用户簇，反映了这些用户在 36 维空间中的相似性：
- en: '![](img/B17290_15_10.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17290_15_10.png)'
- en: 'Figure 15.10: Compared to PCA, the t-SNE technique tends to create more useful
    visualizations that depict the data’s natural clusters'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.10：与 PCA 相比，t-SNE 技术倾向于创建更多有用的可视化，这些可视化描绘了数据的自然簇
- en: Of course, it is somewhat unusual for a t-SNE visualization to work as nicely
    as this one did on the first try. If your results are disappointing, it is possible
    that merely setting a different random seed will generate better-looking results
    due to t-SNE’s use of randomization. Additionally, the `perplexity` and `max_iter`
    parameters of the `Rtsne()` function can be adjusted to affect the size and density
    of the resulting plot. The perplexity governs the number of nearest neighbors
    to consider during the adjustment from high-to-low dimensions, and changing the
    maximum number of iterations (`max_iter`) up or down may lead the algorithm to
    arrive at a completely different solution.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，t-SNE 可视化像这样第一次就工作得如此之好是相当不寻常的。如果你的结果令人失望，可能只是设置一个不同的随机种子就会因为 t-SNE 的随机化而产生更好的结果。此外，`Rtsne()`
    函数的 `perplexity` 和 `max_iter` 参数可以调整以影响结果的尺寸和密度。`perplexity` 控制在从高维到低维调整时考虑的最近邻的数量，上下调整最大迭代次数
    (`max_iter`) 可能会导致算法得出完全不同的解决方案。
- en: 'Unfortunately, there are very few rules of thumb for tuning these parameters,
    and thus it often requires some trial and error to get things just right. The
    creator of t-SNE, Laurens van der Maaten, offers a few words of wisdom:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 很遗憾，调整这些参数的经验法则非常少，因此通常需要一些尝试和错误才能得到恰到好处。t-SNE 的创造者 Laurens van der Maaten 提供了一些智慧的话语：
- en: …one could say that a larger / denser dataset requires a larger perplexity.
    Typical values for the perplexity range between 5 and 50… [seeing a “ball” with
    uniformly distributed points] usually indicates you set your perplexity way too
    high. [If you continue to see bad results after tuning] maybe there is not very
    much nice structure in your data in the first place.
  id: totrans-323
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: …可以说，更大的/更密集的数据集需要更大的困惑度。困惑度的典型值介于 5 到 50 之间…[看到一个“球”中均匀分布的点]通常表明你设置的困惑度过高。
    [如果你在调整后仍然看到不良结果]可能最初你的数据中并没有太多好的结构。
- en: ''
  id: totrans-324
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Source: [https://lvdmaaten.github.io/tsne/](https://lvdmaaten.github.io/tsne/)'
  id: totrans-325
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://lvdmaaten.github.io/tsne/](https://lvdmaaten.github.io/tsne/)
- en: Be warned that the `Rtsne()` function parameters like `perplexity` and `max_iter`
    can drastically affect the amount of time it takes for the t-SNE algorithm to
    converge. If you’re not careful, you may need to force the process to quit rather
    than wait endlessly. Setting `verbose = TRUE` in the `Rtsne()` function call may
    provide insight into how the work has progressed.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：`Rtsne()` 函数的参数，如 `perplexity` 和 `max_iter`，会极大地影响 t-SNE 算法收敛所需的时间。如果你不小心，你可能需要强制终止进程而不是无限期地等待。在
    `Rtsne()` 函数调用中设置 `verbose = TRUE` 可能会提供关于工作进展的见解。
- en: For an outstanding treatment of t-SNE’s parameters and hyperparameters with
    interactive visualizations that show the impact of adjustments to each, see *How
    to Use t-SNE Effectively, Wattenberg, M., Viégas, F., and Johnson, I., 2016*,
    `https://distill.pub/2016/misread-tsne/`.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解 t-SNE 的参数和超参数的杰出处理，以及展示每个调整影响的交互式可视化，请参阅 *如何有效地使用 t-SNE，Wattenberg, M.，Viégas,
    F.，和 Johnson, I.，2016*，`https://distill.pub/2016/misread-tsne/`。
- en: Because t-SNE is an unsupervised method, aside from the remarkably large and
    round cluster in the top right of the visualization—which we can reasonably assume
    is composed of identical users with no social media keywords in their profile—we
    have no idea what the other clusters represent. This being said, it is possible
    to probe the data to investigate the clusters by labeling points with different
    colors or shapes based on their underlying values.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 t-SNE 是一种无监督方法，除了可视化右上角显著大且圆的簇——我们可以合理地假设它由没有社交媒体关键词的相同用户组成——我们不知道其他簇代表什么。尽管如此，我们可以通过根据其基础值用不同颜色或形状标记点来调查数据，以探究这些簇。
- en: 'For example, we can confirm the hypothesis about the top-right cluster by creating
    a categorical measure of how many keywords were used on each user’s page. The
    following tidyverse code begins by using `bind_cols()` to append the t-SNE coordinates
    onto the original dataset. Next, it uses the `rowwise()` function to change the
    behavior of `dplyr` so that the commands work on rows rather than columns. Thus,
    we can use the `sum()` function to count the number of terms each user had on
    their profile, using `c_across()` to select the columns with word counts. After
    using `ungroup()` to remove the rowwise behavior, this count is transformed into
    a two-outcome categorical variable using the `if_else()` function:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以通过创建每个用户页面上使用的关键词数量的分类度量来验证关于右上角簇的假设。以下 tidyverse 代码首先使用 `bind_cols()`
    将 t-SNE 坐标附加到原始数据集上。接下来，它使用 `rowwise()` 函数改变 `dplyr` 的行为，使命令作用于行而不是列。因此，我们可以使用
    `sum()` 函数计算每个用户在其个人资料中使用的术语数量，使用 `c_across()` 选择包含词频的列。在 `ungroup()` 移除行行为后，这个计数通过
    `if_else()` 函数转换为一个两结果分类变量：
- en: '[PRE53]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Using the result of this series of steps, we’ll again plot the t-SNE data,
    but change the shape and color of the points according to the number of terms
    used:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这一系列步骤的结果，我们将再次绘制 t-SNE 数据，但根据使用的术语数量改变点的形状和颜色：
- en: '[PRE54]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The resulting figure confirms our assumption, as the users with zero terms
    used in their social media profile (denoted by circles) comprise the dense cluster
    in the top right of the figure, while the users with one or more terms used (denoted
    by triangles) are scattered elsewhere in the plot:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图证实了我们的假设，因为在其社交媒体个人资料中未使用任何术语的用户（用圆圈表示）构成了图右上角的密集簇，而使用一个或多个术语的用户（用三角形表示）散布在图的其余部分：
- en: '![](img/B17290_15_11.png)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_15_11.png)'
- en: 'Figure 15.11: Adding color or changing the point style can help understand
    the clusters depicted in the t-SNE visualization'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.11：添加颜色或更改点样式可以帮助理解 t-SNE 可视化中描述的簇
- en: The t-SNE technique is more than just a tool to make pretty pictures, although
    it does tend to also do that well! For one, it may be helpful for determining
    the value of *k* to be used for k-means clustering. The t-SNE technique can also
    be used after clustering has been performed, with the points colored according
    to their cluster assignments to illustrate the clusters for presentation purposes.
    Stakeholders are more likely to trust a model with results that can be seen in
    a PowerPoint presentation. Similarly, t-SNE can be used to qualitatively gauge
    the performance of an embedding such as word2vec; if the embedding is meaningful,
    plotting the 300-dimensional vectors in 2-D space will reveal clusters of words
    with related meanings. With so many useful applications of t-SNE, it is no wonder
    that it has quickly become a popular tool in the data science toolkit.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE技术不仅仅是一个制作精美图片的工具，尽管它在这方面也做得很好！首先，它可能有助于确定用于k-means聚类的*k*值。t-SNE技术也可以在聚类完成后使用，根据点的聚类分配给它们上色，以展示聚类以便于展示。利益相关者更可能信任那些可以在PowerPoint演示中看到结果的模型。同样，t-SNE可以用来定性评估嵌入（如word2vec）的性能；如果嵌入是有意义的，将300维向量绘制在2维空间中将会揭示具有相关意义的单词簇。鉴于t-SNE有如此多的实用应用，它迅速成为数据科学工具箱中的流行工具也就不足为奇了。
- en: 'For a fun application using both word2vec and t-SNE in which computers learned
    the meaning of emoji, see *emoji2vec: Learning Emoji Representations from their
    Description, Eisner, B., Rocktäschel, T., Augenstein, I., Bošnjak, M., and Riedel,
    S., 2016, in Proceedings of the 4th International Workshop on Natural Language
    Processing for Social Media at EMNLP 2016*.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '对于一个有趣的应用，使用word2vec和t-SNE，其中计算机学习了表情符号的意义，请参阅*emoji2vec: Learning Emoji Representations
    from their Description, Eisner, B., Rocktäschel, T., Augenstein, I., Bošnjak,
    M., and Riedel, S., 2016, in Proceedings of the 4th International Workshop on
    Natural Language Processing for Social Media at EMNLP 2016*。'
- en: While tools like word2vec and t-SNE provide means for understanding big data,
    they are of no use if R is unable to handle the workload. The remainder of this
    chapter will equip you with additional tools for loading, processing, and modeling
    such large data sources.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管word2vec和t-SNE等工具提供了理解大数据的方法，但如果R无法处理工作负载，它们就没有用处。本章的剩余部分将为您提供额外的工具，用于加载、处理和建模如此大的数据源。
- en: Adapting R to handle large datasets
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 适应R以处理大型数据集
- en: Although the phrase “big data” means more than just the number of rows or the
    amount of memory a dataset consumes, sometimes working with a large volume of
    data can be a challenge in itself. Large datasets can cause computers to freeze
    or slow to a crawl when system memory runs out, or models cannot be built in a
    reasonable amount of time. Many real-world datasets are very large even if they
    are not truly “big,” and thus you are likely to encounter some of these issues
    on future projects. In doing so, you may find that the task of turning data into
    action is more difficult than it first appeared.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然短语“大数据”不仅仅意味着数据集的行数或数据集消耗的内存量，但有时处理大量数据本身就是一个挑战。当系统内存耗尽时，大型数据集可能导致计算机冻结或速度减慢到几乎不动，或者模型无法在合理的时间内构建。即使它们不是真正的“大”，许多现实世界的数据集也非常大，因此你可能在未来的项目中遇到这些问题。在这样做的时候，你可能会发现将数据转化为行动的任务比最初看起来更困难。
- en: Thankfully, there are packages that make it easier to work with large datasets
    even while remaining in the R environment. We’ll begin by looking at the functionality
    that allows R to connect to databases and work with datasets that may exceed available
    system memory, as well as packages allowing R to work in parallel, and some that
    utilize modern machine learning frameworks in the cloud.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一些包使得即使在R环境中也能更容易地处理大型数据集。我们将从查看允许R连接到数据库并处理可能超过可用系统内存的数据集的功能开始，以及允许R并行工作的包，还有一些利用云中现代机器学习框架的包。
- en: Querying data in SQL databases
  id: totrans-342
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在SQL数据库中查询数据
- en: Large datasets are often stored in a **database management system** (**DBMS**)
    such as Oracle, MySQL, PostgreSQL, Microsoft SQL, or SQLite. These systems allow
    the datasets to be accessed using the **Structured Query Language** (**SQL**),
    a programming language designed to pull data from databases.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 大型数据集通常存储在**数据库管理系统**（**DBMS**）中，如Oracle、MySQL、PostgreSQL、Microsoft SQL或SQLite。这些系统允许使用**结构化查询语言**（**SQL**）访问数据集，这是一种旨在从数据库中提取数据的编程语言。
- en: The tidy approach to managing database connections
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 管理数据库连接的整洁方法
- en: 'RStudio version 1.1, which was released in 2017, introduced a graphical approach
    for connecting to databases. The **Connections** tab in the top-right portion
    of the interface provides the ability to interact with database connections found
    on your system. Upon clicking the **New Connection** button within this interface
    tab, you will see a window with the available connection options. The following
    screenshot depicts some of the possible connection types, but your own system
    is likely to have a different selection than those shown here:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 2017 年发布的 RStudio 版本 1.1 引入了一种连接到数据库的图形方法。界面右上角的 **连接** 选项卡提供了与系统上找到的数据库连接交互的能力。在此界面选项卡中单击
    **新建连接** 按钮时，您将看到一个包含可用连接选项的窗口。以下截图显示了某些可能的连接类型，但您自己的系统可能具有与这里显示不同的选择：
- en: '![](img/B17290_15_12.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_15_12.png)'
- en: 'Figure 15.12: The “New Connection” button in RStudio v1.1 or greater opens
    an interface that will assist you with connecting to any predefined data sources'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.12：RStudio v1.1 或更高版本中的“新建连接”按钮打开一个界面，该界面将帮助您连接到任何预定义的数据源
- en: The creation of these connections is typically performed by a database administrator
    and is specific to the type of database as well as the operating system. For instance,
    on Microsoft Windows, you may need to install the appropriate database drivers
    as well as use the ODBC Data Source Administrator application; on macOS and Unix/Linux,
    you may need to install the drivers and edit an `odbc.ini` file. Complete documentation
    about the potential connection types and installation instructions is available
    at [https://solutions.posit.co/connections/db/](https://solutions.posit.co/connections/db/).
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 这些连接的创建通常由数据库管理员执行，并且特定于数据库类型以及操作系统。例如，在 Microsoft Windows 上，您可能需要安装适当的数据库驱动程序以及使用
    ODBC 数据源管理员应用程序；在 macOS 和 Unix/Linux 上，您可能需要安装驱动程序并编辑 `odbc.ini` 文件。有关潜在连接类型和安装说明的完整文档可在
    [https://solutions.posit.co/connections/db/](https://solutions.posit.co/connections/db/)
    找到。
- en: Behind the scenes, the graphical interface uses a variety of R packages to manage
    the connections to these data sources. At the core of this functionality is the
    `DBI` package, which provides a tidyverse-compliant front-end interface to the
    database. The `DBI` package also manages the back-end database driver, which must
    be provided by another R package. Such packages let R connect to Oracle (`ROracle`),
    MySQL (`RMySQL`), PostgreSQL (`RPostgreSQL`), and SQLite (`RSQLite`), among many
    others.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，图形界面使用各种 R 包来管理连接到这些数据源。此功能的核心是 `DBI` 包，它提供了一个符合 tidyverse 标准的前端界面到数据库。`DBI`
    包还管理后端数据库驱动程序，这必须由另一个 R 包提供。这样的包让 R 可以连接到 Oracle (`ROracle`)、MySQL (`RMySQL`)、PostgreSQL
    (`RPostgreSQL`) 和 SQLite (`RSQLite`) 等多种数据库。
- en: 'To illustrate this functionality, we’ll use the `DBI` and `RSQLite` packages
    to connect to a SQLite database containing the credit dataset used previously.
    SQLite is a simple database that doesn’t require running a server. It simply connects
    to a database file on a machine, which here is named `credit.sqlite3`. Before
    starting, be sure you’ve installed both required packages and saved the database
    file into your R working directory. After doing this, you can connect to the database
    using the following command:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一功能，我们将使用 `DBI` 和 `RSQLite` 包连接到一个包含之前使用的信用数据集的 SQLite 数据库。SQLite 是一个简单的数据库，不需要运行服务器。它只需连接到机器上的数据库文件，在这里命名为
    `credit.sqlite3`。在开始之前，请确保您已安装了所需的两个包并将数据库文件保存到您的 R 工作目录中。完成此操作后，您可以使用以下命令连接到数据库：
- en: '[PRE55]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'To prove the connection has succeeded, we can list the database tables to confirm
    the credit table exists as expected:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明连接已成功建立，我们可以列出数据库表以确认预期的信用表存在：
- en: '[PRE56]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'From here, we can send SQL query commands to the database and return records
    as R data frames. For instance, to return the loan applicants with an age of 45
    years or greater, we would query the database as follows:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，我们可以向数据库发送 SQL 查询命令，并将记录作为 R 数据框返回。例如，为了返回年龄为 45 岁或以上的贷款申请人，我们将按以下方式查询数据库：
- en: '[PRE58]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The entire result set can be fetched as a data frame using the following command:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下命令获取整个结果集作为数据框：
- en: '[PRE59]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'To confirm that it worked, we’ll examine the summary statistics, which confirm
    that the ages begin at 45 years:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认其工作正常，我们将检查摘要统计信息，这些信息确认年龄从 45 岁开始：
- en: '[PRE60]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'When our work is done, it is advisable to clear the query result set and close
    the database connection to free these resources:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的工作完成时，建议清除查询结果集并关闭数据库连接以释放这些资源：
- en: '[PRE62]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: In addition to SQLite and the database-specific R packages, the `odbc` package
    allows R to connect to many different types of databases using a single protocol
    known as the **Open Database Connectivity** (**ODBC**) standard. The ODBC standard
    can be used regardless of operating system or DBMS.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 SQLite 和特定数据库的 R 包之外，`odbc` 包允许 R 使用称为 **开放数据库连接**（**ODBC**）标准的单一协议连接到许多不同类型的数据库。无论操作系统或
    DBMS 如何，都可以使用 ODBC 标准。
- en: 'If you have previously connected to an ODBC database, you may have referred
    to it via its **data source name** (**DSN**). You can use the DSN to create a
    database connection with a single line of R code:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前已经连接到 ODBC 数据库，你可能通过其 **数据源名称**（**DSN**）来引用它。你可以使用 DSN 通过一行 R 代码创建数据库连接：
- en: '[PRE63]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'If you have a more complicated setup, or want to specify the connection properties
    manually, you can specify a full connection string as arguments to the DBI package
    `dbConnect()` function as follows:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个更复杂的设置，或者想要手动指定连接属性，你可以将完整的连接字符串作为 DBI 包 `dbConnect()` 函数的参数指定，如下所示：
- en: '[PRE64]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: With the connection established, queries can be sent to the ODBC database and
    tables can be returned as data frames using the same functions that were used
    for the SQLite example previously.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 建立连接后，可以将查询发送到 ODBC 数据库，并使用与之前 SQLite 示例中相同的函数将表作为数据框返回。
- en: Due to security and firewall settings, the instructions for configuring an ODBC
    network connection are highly specific to each situation. If you are having trouble
    setting up the connection, check with your database administrator. The Posit team
    (formerly known as RStudio) also provides helpful information at [https://solutions.posit.co/connections/db/best-practices/drivers/](https://solutions.posit.co/connections/db/best-practices/drivers/).
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 由于安全和防火墙设置，配置 ODBC 网络连接的说明非常具体，针对每种情况。如果你在设置连接时遇到困难，请咨询你的数据库管理员。Posit 团队（以前称为
    RStudio）也在 [https://solutions.posit.co/connections/db/best-practices/drivers/](https://solutions.posit.co/connections/db/best-practices/drivers/)
    提供了有用的信息。
- en: Using a database backend for dplyr with dbplyr
  id: totrans-371
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 dbplyr 为 dplyr 提供数据库后端
- en: Using the tidyverse’s `dplyr` functions with an external database is no more
    difficult than using it with a traditional data frame. The `dbplyr` package (short
    for “database plyr”) allows any database supported by the `DBI` package to be
    used transparently as a backend for `dplyr`. The connection allows tibble objects
    to be pulled from the database. Generally, one does not need to do more than merely
    install the `dbplyr` package, and `dplyr` can then take advantage of its functionality.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 tidyverse 的 `dplyr` 函数与外部数据库相比，与传统的数据框使用并没有更难。`dbplyr` 包（简称“数据库 plyr”）允许使用
    `DBI` 包支持的任何数据库作为 `dplyr` 的后端进行透明使用。这个连接允许从数据库中提取 tibble 对象。通常，你不需要做更多的事情，只需安装
    `dbplyr` 包，然后 `dplyr` 就可以利用其功能。
- en: 'For example, let’s connect to the SQLite `credit.sqlite3` database used previously,
    then save its `credit` table as a tibble object using the `tbl()` function as
    follows:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们连接到之前使用的 SQLite `credit.sqlite3` 数据库，然后使用 `tbl()` 函数将其 `credit` 表保存为 tibble
    对象，如下所示：
- en: '[PRE65]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Because `dplyr` has been routed through a database, the `credit_tbl` object
    here is not stored as a local R object, but rather is a table within a database.
    In spite of this, `credit_tbl` will act exactly like an ordinary tibble and will
    gain all the other benefits of the `dplyr` package, with the exception that the
    computational work will occur within the database rather than in R. This means
    that if the SQLite database were replaced with a database residing across a network
    on a more traditional SQL server, work could be offloaded to machines with more
    computational power rather than being performed on your local machine.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 `dplyr` 已经通过数据库进行路由，所以这里的 `credit_tbl` 对象并不是以本地 R 对象的形式存储，而是一个数据库内的表。尽管如此，`credit_tbl`
    将会像普通的 tibble 一样工作，并且会获得 `dplyr` 包的所有其他好处，唯一的例外是计算工作将在数据库内部而不是在 R 中进行。这意味着，如果将
    SQLite 数据库替换为位于网络另一端的更传统的 SQL 服务器上的数据库，工作可以卸载到计算能力更强的机器上，而不是在本地机器上执行。
- en: 'For example, to query the database and display the age summary statistics for
    credit applicants that are at least 45 years old, we can pipe the tibble through
    the following sequence of functions:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，为了查询数据库并显示至少 45 岁的信用申请人的年龄汇总统计信息，我们可以通过以下函数序列将 tibble 管道化：
- en: '[PRE66]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The result is as follows:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE67]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Note that the `dbplyr` functions are “lazy,” which means that no work is done
    in the database until it is necessary. Thus, the `collect()` function forces `dplyr`
    to retrieve the results from the “server” (in this case, a SQLite instance, but
    more typically a powerful database server) so that the summary statistics may
    be calculated. If the `collect()` statement is omitted, the code will fail as
    the `summary()` function cannot work directly with the database connection object.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`dbplyr`函数是“惰性的”，这意味着在必要时才在数据库中执行工作。因此，`collect()`函数迫使`dplyr`从“服务器”（在这种情况下是一个SQLite实例，但更常见的是强大的数据库服务器）检索结果，以便计算汇总统计量。如果省略`collect()`语句，代码将失败，因为`summary()`函数不能直接与数据库连接对象一起工作。
- en: 'Given a database connection, most `dplyr` commands will be translated seamlessly
    into SQL on the backend. To see how this works, we can ask `dbplyr` to show the
    SQL code that is generated for a series of `dplyr` steps. Let’s build a slightly
    more complex sequence of commands to show the average loan amount after filtering
    for ages 45 and older, and grouping by loan default status:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个数据库连接，大多数`dplyr`命令将无缝地在后端转换为SQL。要了解这是如何工作的，我们可以要求`dbplyr`显示为一系列`dplyr`步骤生成的SQL代码。让我们构建一个稍微复杂一些的命令序列，以显示在过滤年龄为45岁及以上并按贷款违约状态分组后的平均贷款金额：
- en: '[PRE68]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The output shows that those that defaulted tended to request larger loan amounts
    on average:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示，那些违约的人平均倾向于要求更大的贷款金额：
- en: '[PRE69]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Note that this looks different from a normal `dplyr` output and includes information
    about the database used, since the work was performed in the database rather than
    R. To see the SQL code that was generated to perform this analysis, simply pipe
    the steps into the `show_query()` function:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这与正常的`dplyr`输出不同，因为它包含了关于所使用数据库的信息，因为工作是在数据库中而不是在R中完成的。要查看执行此分析生成的SQL代码，只需将步骤通过`show_query()`函数管道传输即可：
- en: '[PRE70]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The output shows the SQL query that was run on the SQLite database:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了在SQLite数据库上运行的SQL查询：
- en: '[PRE71]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Using the `dbplyr` functionality, the same R code that is used on smaller data
    frames can also be used to prepare larger datasets stored in SQL databases—the
    heavy lifting is done on the remote server, rather than your local laptop or desktop
    machine. In this way, learning the tidyverse suite of packages ensures your code
    will apply to any type of project from small to massive. Of course, there are
    even more ways to enable R to work with large datasets, as you will see in the
    sections that follow.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`dbplyr`功能，在较小的数据框上使用的相同R代码也可以用来准备存储在SQL数据库中的大型数据集——繁重的工作是在远程服务器上完成的，而不是在您的本地笔记本电脑或台式机上。通过这种方式，学习tidyverse套件确保您的代码适用于从小型到大型任何类型的项目。当然，还有更多方法可以启用R与大型数据集一起工作，您将在接下来的章节中看到。
- en: Doing work faster with parallel processing
  id: totrans-390
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用并行处理更快地完成工作
- en: 'In the early days of computing, computer processors always executed instructions
    in **serial**, which meant that they were limited to performing a single task
    at a time. In serial computing, the next instruction cannot be started until the
    previous instruction is complete:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机的早期阶段，计算机处理器总是**顺序**执行指令，这意味着它们一次只能执行一个任务。在顺序计算中，下一个指令不能开始，直到前一个指令完成：
- en: '![Diagram  Description automatically generated](img/B17290_15_13.png)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
  zh: '![图描述自动生成](img/B17290_15_13.png)'
- en: 'Figure 15.13: In serial computing, tasks cannot begin until prior tasks have
    been completed'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.13：在顺序计算中，任务不能开始，直到先前任务完成
- en: 'Although it was widely known that many tasks could be completed more efficiently
    by completing steps simultaneously, the technology simply did not exist. This
    was addressed by the development of **parallel computing** methods, which use
    a set of two or more processors or computers to perform tasks simultaneously:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管众所周知，许多任务可以通过同时完成步骤来更有效地完成，但这项技术当时并不存在。这个问题通过开发**并行计算**方法得到了解决，这些方法使用一组两个或更多的处理器或计算机来同时执行任务：
- en: '![Diagram  Description automatically generated](img/B17290_15_14.png)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
  zh: '![图描述自动生成](img/B17290_15_14.png)'
- en: 'Figure 15.14: Parallel computing allows several tasks to occur simultaneously,
    which can speed up processing, but the results must be combined at the end'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.14：并行计算允许同时执行多个任务，这可以加快处理速度，但最终必须将结果合并
- en: Many modern computers are designed for parallel computing. Even in the case
    that they have a single processor, they often have two or more cores that work
    in parallel. A core is essentially a processor within a processor, which allows
    computations to occur even if the other cores are busy with another task.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现代计算机都是为并行计算而设计的。即使在它们只有一个处理器的情况下，它们通常也有两个或更多个并行工作的核心。核心本质上是一个处理器内的处理器，它允许即使在其他核心忙于其他任务时也能进行计算。
- en: Networks of multiple computers called **clusters** can also be used for parallel
    computing. A large cluster may include a variety of hardware and be separated
    over large distances. In this case, the cluster is known as a **grid**. Taken
    to an extreme, a cluster or grid of hundreds or thousands of computers running
    commodity hardware could be a very powerful system. Cloud computing systems like
    Amazon Web Services (AWS) and Microsoft Azure make it easier than ever to use
    clusters for data science projects.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 由多台计算机组成的**集群**也可以用于并行计算。一个大型的集群可能包括各种硬件，并且分布在很大的距离上。在这种情况下，该集群被称为**网格**。如果将集群或网格扩展到数百或数千台运行通用硬件的计算机，这将是一个非常强大的系统。像Amazon
    Web Services (AWS)和Microsoft Azure这样的云计算系统使得使用集群进行数据科学项目变得比以往任何时候都容易。
- en: The catch, however, is that not every problem can be parallelized. Certain problems
    are more conducive to parallel execution than others. One might expect that adding
    100 processors would result in 100 times the work being accomplished in the same
    amount of time (that is, the overall execution time would be 1/100), but this
    is typically not the case. The reason is that it takes effort to manage the workers.
    Work must be divided into equal, non-overlapping tasks, and each of the workers’
    results must be combined into one final answer.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，并不是每个问题都可以并行化。某些问题比其他问题更适合并行执行。人们可能会预期增加100个处理器会在相同的时间内完成100倍的工作（也就是说，总执行时间将是1/100），但通常并非如此。原因是管理工作者需要付出努力。工作必须分成相等且不重叠的任务，并且每个工作者的结果必须合并成一个最终答案。
- en: So-called **embarrassingly parallel** problems are the ideal. These tasks are
    easy to reduce into non-overlapping blocks of work, and the results are easy to
    recombine. An example of an embarrassingly parallel machine learning task would
    be 10-fold cross-validation; once the 10 samples are divided, each of the 10 blocks
    of work is independent, meaning that they do not affect the others. As you will
    soon see, this task can be sped up quite dramatically using parallel computing.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 所说的**令人尴尬的并行**问题是最理想的。这些任务很容易被简化为不重叠的工作块，并且结果很容易重新组合。一个令人尴尬的并行机器学习任务的例子是10折交叉验证；一旦将10个样本分开，每个工作块都是独立的，这意味着它们不会相互影响。正如你很快就会看到的，这个任务可以通过并行计算大大加快。
- en: Measuring R’s execution time
  id: totrans-401
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测量R的执行时间
- en: Efforts to speed up R will be wasted if it is not possible to systematically
    measure how much time was saved. Although a stopwatch is one option, an easier
    solution is to wrap the offending code in a `system.time()` function.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 如果无法系统地测量节省了多少时间，那么加快R的努力将是徒劳的。尽管秒表是一个选择，但一个更容易的解决方案是将有问题的代码包裹在`system.time()`函数中。
- en: 'For example, on the author’s laptop, the `system.time()` function notes that
    it takes about 0.026 seconds to generate a million random numbers:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在作者的笔记本电脑上，`system.time()`函数记录生成一百万个随机数大约需要0.026秒：
- en: '[PRE72]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: The same function can be used to evaluate improvement in performance, obtained
    with the methods that were just described or any R function.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用相同的函数来评估使用刚刚描述的方法或任何R函数获得的性能改进。
- en: For what it’s worth, when the first edition of this book was published, generating
    a million random numbers took 0.130 seconds; the same took about 0.093 seconds
    for the second edition and 0.067 seconds for the third edition. Here, it takes
    only 0.026 seconds. Although I’ve used a slightly more powerful computer each
    time, this reduction of about 80 percent of the processing time over the course
    of about ten years illustrates just how quickly computer hardware and software
    are improving!
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 就其价值而言，当这本书的第一版出版时，生成一百万个随机数需要0.130秒；第二版需要大约0.093秒，第三版需要0.067秒。在这里，只需要0.026秒。尽管我每次都使用了一台稍微更强大的计算机，但在这大约十年的过程中，处理时间减少了大约80%，这仅仅说明了计算机硬件和软件的进步是多么迅速！
- en: Enabling parallel processing in R
  id: totrans-408
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在R中启用并行处理
- en: The `parallel` package, included with R version 2.14.0 and later, has lowered
    the entry barrier to deploying parallel algorithms by providing a standard framework
    for setting up worker processes that can complete tasks simultaneously. It does
    this by including components of the `multicore` and `snow` packages, which each
    take a different approach to multitasking.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: R版本2.14.0及以后的版本中包含的`parallel`包，通过提供一个标准框架来设置可以同时完成任务的工作进程，降低了部署并行算法的入门门槛。它是通过包含`multicore`和`snow`包的组件来实现的，每个包都采用不同的多任务处理方法。
- en: 'If your computer is reasonably recent, you are likely to be able to use parallel
    processing. To determine the number of cores your machine has, use the `detectCores()`
    function as follows. Note that your output will differ depending on your hardware
    specifications:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的计算机相对较新，你很可能能够使用并行处理。要确定你的机器有多少核心，可以使用以下`detectCores()`函数。请注意，你的输出将取决于你的硬件规格：
- en: '[PRE74]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: The `multicore` package was developed by Simon Urbanek and allows parallel processing
    on a single machine that has multiple processors or processor cores. It utilizes
    the multitasking capabilities of a computer’s operating system to **fork**, or
    create a copy of, additional R sessions that share the same memory, and is perhaps
    the simplest way to get started with parallel processing in R.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 由Simon Urbanek开发的`multicore`包允许在具有多个处理器或处理器核心的单台机器上进行并行处理。它利用计算机操作系统的多任务处理能力来**fork**，或创建共享相同内存的额外R会话的副本，可能是开始使用R进行并行处理的最简单方法。
- en: Note that because the Microsoft Windows operating system does not support forking,
    the `multicore` example works only on macOS or Linux machines. For a Windows-ready
    solution, skip ahead to the next section on `foreach` and `doParallel`.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于Microsoft Windows操作系统不支持fork，`multicore`示例只能在macOS或Linux机器上运行。对于Windows兼容的解决方案，请跳到下一节关于`foreach`和`doParallel`。
- en: 'An easy way to get started with the `multicore` functionality is to use the
    `mclapply()` function, which is a multicore version of `lapply()`. For instance,
    the following blocks of code illustrate how the task of generating 10 million
    random numbers can be divided across 1, 2, 4, and 8 cores. The `unlist()` function
    is used to combine the parallel results (a list) into a single vector after each
    core has completed its chunk of work:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 开始使用`multicore`功能的一个简单方法是使用`mclapply()`函数，它是`lapply()`的多核版本。例如，以下代码块说明了如何将生成1000万个随机数的任务分配到1、2、4和8个核心。在每台核心完成其工作块后，使用`unlist()`函数将并行结果（一个列表）组合成一个单一的向量：
- en: '[PRE76]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Notice how as the number of cores increases, the elapsed time decreases, though
    the benefit tapers off and may even be detrimental once too many cores have been
    added. Though this is a simple example, it can be adapted easily to many other
    tasks.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，随着核心数量的增加，经过的时间会减少，尽管这种好处会逐渐减弱，一旦添加了过多的核心，甚至可能产生负面影响。尽管这是一个简单的例子，但它可以很容易地适应许多其他任务。
- en: 'The `snow` package (Simple Network of Workstations) by Luke Tierney, A. J.
    Rossini, Na Li, and H. Sevcikova allows parallel computing on multicore or multiprocessor
    machines as well as on a network of multiple machines. It is slightly more difficult
    to use but offers much more power and flexibility. The `snow` functionality is
    included in the `parallel` package, so to set up a cluster on a single machine,
    use the `makeCluster()` function with the number of cores to be used:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 由Luke Tierney、A. J. Rossini、Na Li和H. Sevcikova开发的`snow`包（简单工作站网络）允许在多核或多处理器机器以及多个机器的网络上进行并行计算。它使用起来稍微困难一些，但提供了更多的功能和灵活性。`snow`功能包含在`parallel`包中，因此要在单台机器上设置集群，请使用带有要使用核心数的`makeCluster()`函数：
- en: '[PRE77]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Because `snow` communicates via network traffic, depending on your operating
    system, you may receive a message to approve access through your firewall.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`snow`通过网络流量进行通信，根据你的操作系统，你可能会收到一条消息，要求你批准通过防火墙的访问。
- en: 'To confirm the cluster is operational, we can ask each node to report back
    its hostname. The `clusterCall()` function executes a function on each machine
    in the cluster. In this case, we’ll define a function that simply calls the `Sys.info()`
    function and returns the `nodename` parameter:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认集群正在运行，我们可以要求每个节点报告其主机名。`clusterCall()`函数在集群中的每台机器上执行一个函数。在这种情况下，我们将定义一个简单的函数，该函数仅调用`Sys.info()`函数并返回`nodename`参数：
- en: '[PRE78]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Unsurprisingly, since all four nodes are running on a single machine, they
    report back the same hostname. To have the four nodes run a different command,
    supply them with a unique parameter via the `clusterApply()` function. Here, we’ll
    supply each node with a different letter. Each node will then perform a simple
    function on its letter in parallel:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 出乎意料的是，由于所有四个节点都在同一台机器上运行，它们报告的宿主名称相同。为了使四个节点运行不同的命令，可以通过`clusterApply()`函数为它们提供唯一的参数。在这里，我们将为每个节点提供不同的字母。然后，每个节点将并行地对它的字母执行一个简单函数：
- en: '[PRE80]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'When we’re done with the cluster, it’s important to terminate the processes
    it spawned. This will free up the resources each node is using:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们完成集群操作后，终止它所启动的进程是很重要的。这将释放每个节点所使用的资源：
- en: '[PRE82]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Using these simple commands, it is possible to speed up many machine learning
    tasks. For the largest big data problems, much more complex `snow` configurations
    are possible. For instance, you may attempt to configure a **Beowulf cluster**—a
    network of many consumer-grade machines. In academic and industry research settings
    with dedicated computing clusters, `snow` can use the `Rmpi` package to access
    these high-performance **message-passing interface** (**MPI**) servers. Working
    with such clusters requires knowledge of network configurations and computing
    hardware outside the scope of this book.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些简单的命令，可以加速许多机器学习任务。对于最大的大数据问题，可能的`snow`配置更加复杂。例如，你可能尝试配置一个**Beowulf集群**——一个由许多消费级机器组成的网络。在具有专用计算集群的学术和工业研究环境中，`snow`可以使用`Rmpi`包来访问这些高性能的**消息传递接口**（**MPI**）服务器。与这样的集群一起工作需要了解超出本书范围的网络配置和计算硬件知识。
- en: 'For a much more detailed introduction to `snow`, including some information
    on how to configure parallel computing on several computers over a network, see
    the following lecture by Luke Tierney: [http://homepage.stat.uiowa.edu/~luke/classes/295-hpc/notes/snow.pdf](http://homepage.stat.uiowa.edu/~luke/classes/295-hpc/notes/snow.pdf).'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`snow`的更详细介绍，包括如何在多台计算机上配置网络并行计算的一些信息，请参阅Luke Tierney的以下讲座：[http://homepage.stat.uiowa.edu/~luke/classes/295-hpc/notes/snow.pdf](http://homepage.stat.uiowa.edu/~luke/classes/295-hpc/notes/snow.pdf)。
- en: Taking advantage of parallel with foreach and doParallel
  id: totrans-431
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 利用foreach和doParallel进行并行计算
- en: The `foreach` package by Rich Calaway and Steve Weston provides perhaps the
    easiest way to get started with parallel computing, especially if you are running
    R on the Windows operating system, as some of the other packages are platform
    specific.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: Rich Calaway和Steve Weston的`foreach`包提供了开始并行计算的最简单方法，特别是如果你在Windows操作系统上运行R，因为其他一些包是特定平台的。
- en: The core of the package is a `foreach` looping construct. If you have worked
    with other programming languages, this may be familiar. Essentially, it allows
    looping over a set of items, without explicitly counting the number of items;
    in other words, *for each* item in the set, *do* something.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 该包的核心是一个`foreach`循环结构。如果你使用过其他编程语言，这可能会很熟悉。本质上，它允许遍历一组项目，而不需要显式地计数项目数量；换句话说，对集合中的每个项目，都执行某些操作。
- en: 'If you’re thinking that R already provides a set of apply functions to loop
    over sets of items (for example, `apply()`, `lapply()`, `sapply()`, and so on),
    you are correct. However, the `foreach` loop has an additional benefit: iterations
    of the loop can be completed in parallel using a very simple syntax. Let’s see
    how this works.'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你认为R已经提供了一套apply函数来遍历项目集（例如，`apply()`、`lapply()`、`sapply()`等），你是正确的。然而，`foreach`循环有一个额外的优点：循环的迭代可以使用非常简单的语法并行完成。让我们看看这是如何工作的。
- en: 'Recall the command we’ve been using to generate millions of random numbers.
    To make this more challenging, let’s increase the count to a hundred million,
    which causes the process to take about 2.5 seconds:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我们用来生成数百万随机数的命令。为了使这个任务更具挑战性，让我们将计数增加到一亿，这将使进程大约需要2.5秒：
- en: '[PRE83]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'After the `foreach` package has been installed, the same task can be expressed
    with a loop that combines four sets of 25,000,000 randomly generated numbers.
    The `.combine` parameter is an optional setting that tells `foreach` which function
    it should use to combine the final set of results from each loop iteration. In
    this case, since each iteration generates a set of random numbers, we simply use
    the `c()` concatenate function to create a single, combined vector:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装了`foreach`包之后，可以使用一个循环来表示相同的任务，该循环结合了四组2500万个随机生成的数字。`.combine`参数是一个可选设置，它告诉`foreach`应该使用哪个函数来组合每个循环迭代的最终结果集。在这种情况下，由于每个迭代都生成一组随机数，我们只需使用`c()`连接函数来创建一个单一的、组合的向量：
- en: '[PRE85]'
  id: totrans-439
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: If you noticed that this function didn’t result in a speed improvement, good
    catch! In fact, the process was slower. The reason is that by default, the `foreach`
    package runs each loop iteration in serial, and the function adds a small amount
    of computational overhead to the process. The sister package `doParallel` provides
    a parallel backend for `foreach` that utilizes the `parallel` package included
    with R, described earlier in this chapter.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您注意到这个函数没有导致速度提升，那是个很好的发现！实际上，这个过程更慢。原因是默认情况下，`foreach`包以串行方式运行每个循环迭代，并且该函数会给过程增加少量的计算开销。姐妹包`doParallel`为`foreach`提供了一个并行后端，它利用了R中包含的`parallel`包，这在本章前面已经描述过。
- en: 'Before parallelizing this work, it is wise to confirm the number of cores available
    on your system as follows:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 在并行化这项工作之前，明智的做法是确认您系统上可用的核心数，如下所示：
- en: '[PRE87]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: Your results will differ depending on your system capabilities.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 您的结果将取决于您的系统能力。
- en: 'Next, after installing and loading the `doParallel` package, simply register
    the desired number of cores and swap the `%do%` command with the `%dopar%` operator.
    Here, we only need at most four cores, as there are only four groups of random
    numbers to combine:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在安装和加载`doParallel`包之后，只需注册所需的核心数，并将`%do%`命令与`%dopar%`运算符交换。在这里，我们最多只需要四个核心，因为只有四组随机数需要组合：
- en: '[PRE89]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: As shown in the output, this results in a performance increase, cutting the
    execution time by about 40 percent.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 如输出所示，这导致性能提升，将执行时间减少了大约40%。
- en: '**Warning**: if the `cores` parameter is set to a number greater than the available
    cores on your system, or if the combined work exceeds the free memory on your
    computer, R may crash! In this case, the vector of random numbers is nearly a
    gigabyte of data, so systems with low RAM may be especially prone to crashing
    here.'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '**警告**：如果将`cores`参数设置为大于您系统上可用核心数的数字，或者如果总工作量超过了您计算机上的空闲内存，R可能会崩溃！在这种情况下，随机数向量几乎是一个GB的数据，因此具有低RAM的系统可能在这里特别容易崩溃。'
- en: 'To close the `doParallel` cluster, simply type:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 要关闭`doParallel`集群，只需输入以下命令：
- en: '[PRE91]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: Though the cluster will be closed automatically at the conclusion of the R session,
    it is better form to do so explicitly.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管集群将在R会话结束时自动关闭，但明确地这样做是更好的做法。
- en: Training and evaluating models in parallel with caret
  id: totrans-454
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用caret并行训练和评估模型
- en: The `caret` package by Max Kuhn (covered previously in *Chapter 10*, *Evaluating
    Model Performance*, and *Chapter 14*, *Building Better Learners*) will transparently
    utilize a parallel backend if one has been registered with R using the `foreach`
    package described previously.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: Max Kuhn的`caret`包（在*第10章* *评估模型性能*和*第14章* *构建更好的学习者*中已有介绍）如果使用前面描述的`foreach`包在R中注册了并行后端，将透明地利用并行后端。
- en: 'Let’s look at a simple example in which we attempt to train a random forest
    model on the credit dataset. Without parallelization, the model takes about 65
    seconds to train:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个简单的例子，我们尝试在信用数据集上训练一个随机森林模型。在没有并行化的情况下，模型大约需要65秒来训练：
- en: '[PRE92]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'On the other hand, if we use the `doParallel` package to register eight cores
    to be used in parallel (be sure to lower this number if you have fewer than eight
    cores available), the model takes about 10 seconds to build—less than one-sixth
    of the time—and we didn’t need to change the remaining `caret` code:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果我们使用`doParallel`包将八个核心注册为并行使用（如果您有少于八个核心可用，请确保降低此数字），模型大约需要10秒来构建——不到六分之一的时间——而且我们不需要更改剩余的`caret`代码：
- en: '[PRE94]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '[PRE95]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: Many of the tasks involved in training and evaluating models, such as creating
    random samples and repeatedly testing predictions for 10-fold cross-validation,
    are embarrassingly parallel and ripe for performance improvements. With this in
    mind, it is wise to always register multiple cores before beginning a `caret`
    project.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练和评估模型的过程中涉及到的许多任务，例如创建随机样本和重复测试预测以进行10折交叉验证，都是可以并行处理的，非常适合性能提升。因此，在开始一个`caret`项目之前，始终注册多个核心是明智的。
- en: 'Configuration instructions and a case study of the performance improvements
    for enabling parallel processing in `caret` are available on the project’s website:
    [https://topepo.github.io/caret/parallel-processing.html](https://topepo.github.io/caret/parallel-processing.html).'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 在项目的网站上提供了配置说明和启用`caret`中并行处理性能提升的案例研究：[https://topepo.github.io/caret/parallel-processing.html](https://topepo.github.io/caret/parallel-processing.html)。
- en: Utilizing specialized hardware and algorithms
  id: totrans-464
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用专用硬件和算法
- en: Base R has a reputation for being slow and memory inefficient, a reputation
    that is at least somewhat earned. These faults are largely unnoticed on a modern
    PC for datasets of many thousands of records, but datasets with millions of records
    or more can exceed the limits of what is currently possible with consumer-grade
    hardware. The problem is worsened if the dataset contains many features or if
    complex learning algorithms are being used.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 基础R以其速度慢和内存效率低而闻名，这种声誉至少在一定程度上是应得的。对于包含数万条记录的数据集，这些错误在很大程度上是不被注意到的，但包含数百万条记录或更多记录的数据集可能会超过当前消费级硬件所能实现的极限。如果数据集包含许多特征或正在使用复杂的机器学习算法，问题会更加严重。
- en: CRAN has a high-performance computing task view that lists packages pushing
    the boundaries of what is possible in R at [http://cran.r-project.org/web/views/HighPerformanceComputing.html](http://cran.r-project.org/web/views/HighPerformanceComputing.html).
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: CRAN有一个高性能计算任务视图，列出了在[http://cran.r-project.org/web/views/HighPerformanceComputing.html](http://cran.r-project.org/web/views/HighPerformanceComputing.html)上推动R可能性的边界包。
- en: Packages that extend R past the capabilities of the base package are being developed
    rapidly. These packages allow R to work faster, perhaps by spreading the work
    over additional computers or processors, by utilizing specialized computer hardware,
    or by providing machine learning optimized to big data problems.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 正在快速开发扩展R基础包功能的包。这些包允许R运行得更快，可能通过在额外的计算机或处理器上分散工作，利用专用计算机硬件，或者通过提供针对大数据问题优化的机器学习来实现。
- en: Parallel computing with MapReduce concepts via Apache Spark
  id: totrans-468
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过Apache Spark实现具有MapReduce概念的并行计算
- en: 'The **MapReduce** programming model was developed at Google to process its
    data on a large cluster of networked computers. MapReduce conceptualizes parallel
    programming as a two-step process:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: '**MapReduce**编程模型是在谷歌开发的，用于在大型网络计算机集群上处理其数据。MapReduce将并行编程概念化为一个两步过程：'
- en: A **map** step, in which a problem is divided into smaller tasks that are distributed
    across the computers in the cluster
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**映射**步骤，其中将问题分解为更小的任务，这些任务分布在集群中的计算机上'
- en: A **reduce** step, in which the results of the small chunks of work are collected
    and synthesized into a solution to the original problem
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**归约**步骤，其中将小块工作的结果收集并综合成解决原始问题的解决方案'
- en: A popular open-source alternative to the proprietary MapReduce framework is
    **Apache Hadoop**. The Hadoop software comprises the MapReduce concept plus a
    distributed filesystem capable of storing large amounts of data across a cluster
    of computers. Hadoop requires somewhat specialized programming skills to take
    advantage of its capabilities and to perform even basic machine learning tasks.
    Additionally, although Hadoop is excellent at working with extremely large amounts
    of data, it may not always be the fastest option because it keeps all data on
    disk rather than utilizing available memory.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 作为专有MapReduce框架的流行开源替代品是**Apache Hadoop**。Hadoop软件包括MapReduce概念以及一个能够存储大量数据并在计算机集群中分布的分布式文件系统。Hadoop需要一定的专用编程技能来利用其功能，以及执行甚至基本的机器学习任务。此外，尽管Hadoop在处理极大量数据方面非常出色，但它可能不是最快的选项，因为它将所有数据都保存在磁盘上，而不是利用可用的内存。
- en: '**Apache Spark** is a cluster-computing framework for big data, offering solutions
    to these issues with Hadoop. Spark takes advantage of the cluster’s available
    memory to process data approximately 100x faster than Hadoop. Additionally, it
    provides easy-to-use libraries for many common data processing, analysis, and
    modeling tasks. These include the SparkSQL data query language, the MLlib machine
    learning library, GraphX for graph and network analysis, and the Spark Streaming
    library for processing real-time data streams. For these reasons, Spark is perhaps
    the current standard for open-source big data processing.'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '**Apache Spark** 是一个用于大数据的集群计算框架，通过 Hadoop 解决这些问题。Spark 利用集群的可用内存，将数据处理速度提高约
    100 倍于 Hadoop。此外，它还提供了易于使用的库，用于许多常见的数据处理、分析和建模任务。这些包括 SparkSQL 数据查询语言、MLlib 机器学习库、GraphX
    用于图和网络分析，以及 Spark Streaming 库用于处理实时数据流。因此，Spark 可能是当前开源大数据处理的行业标准。'
- en: Packt Publishing has published many books on Spark. To search their current
    offerings, visit [https://subscription.packtpub.com/search?query=spark](https://subscription.packtpub.com/search?query=spark).
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: Packt Publishing 出版了许多关于 Spark 的书籍。要搜索他们的当前产品，请访问 [https://subscription.packtpub.com/search?query=spark](https://subscription.packtpub.com/search?query=spark)。
- en: Apache Spark is often run remotely on a cloud-hosted cluster of virtual machines,
    but its benefits can also be seen running on your own hardware. In either case,
    the `sparklyr` package connects to the cluster and provides a `dplyr` interface
    for analyzing the data using Spark.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 通常在远程运行的云托管虚拟机集群上运行，但它的好处也可以在您自己的硬件上看到。在两种情况下，`sparklyr` 包连接到集群，并为使用
    Spark 分析数据提供 `dplyr` 接口。
- en: More detailed instructions for using Spark with R can be found at [https://spark.rstudio.com](https://spark.rstudio.com),
    but the basic instructions for getting up and running are straightforward.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [https://spark.rstudio.com](https://spark.rstudio.com) 可以找到使用 Spark 与 R 的更详细说明，但启动和运行的基本说明很简单。
- en: 'To illustrate the fundamentals, let’s build a random forest model on the credit
    dataset to predict loan defaults. To begin, you’ll need to install and load the
    `sparklyr` package. Then, the first time you use Spark, you’ll need to run the
    `spark_install()` function, which downloads Spark onto your computer. Note that
    this is a sizeable download at about 220 megabytes, as it includes the full Spark
    environment:'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明基础知识，让我们在信用数据集上构建一个随机森林模型来预测贷款违约。首先，您需要安装并加载 `sparklyr` 包。然后，您第一次使用 Spark
    时，需要运行 `spark_install()` 函数，该函数将 Spark 下载到您的计算机上。请注意，这是一个大约 220 兆字节的较大下载，因为它包括了完整的
    Spark 环境：
- en: '[PRE96]'
  id: totrans-478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Additionally, Spark itself requires a Java installation, which can be downloaded
    from `http://www.java.com` if you do not already have it. Once Spark and Java
    have been installed, you can instantiate a Spark cluster on your local machine
    using the following command:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Spark 本身需要安装 Java，如果您还没有安装，可以从 `http://www.java.com` 下载。一旦 Spark 和 Java 安装完成，您可以使用以下命令在本地机器上实例化一个
    Spark 集群：
- en: '[PRE97]'
  id: totrans-480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Next, we’ll load the loan dataset from the `credit.csv` file on our local machine
    into the Spark instance, then use the Spark function `sdf_random_split()` to randomly
    assign 75 and 25 percent of the data to the training and test sets, respectively.
    The `seed` parameter is the random seed to ensure the results are identical each
    time this code is run:'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将从本地机器上的 `credit.csv` 文件加载贷款数据集到 Spark 实例中，然后使用 Spark 函数 `sdf_random_split()`
    分别随机分配 75% 和 25% 的数据到训练集和测试集。`seed` 参数是随机种子，以确保每次运行此代码时结果都相同：
- en: '[PRE98]'
  id: totrans-482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'Lastly, we’ll pipe the training data into the random forest model function,
    make predictions, and use the classification evaluator to compute the AUC on the
    test set:'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将训练数据管道输入到随机森林模型函数中，进行预测，并使用分类评估器在测试集上计算 AUC：
- en: '[PRE99]'
  id: totrans-484
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '[PRE100]'
  id: totrans-485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'We’ll then disconnect from the cluster:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将从集群断开连接：
- en: '[PRE101]'
  id: totrans-487
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: With just a few lines of R code, we’ve built a random forest model using Spark
    that could expand to model millions of records. If even more computing power is
    needed, the code can be run in the cloud using a massively parallel Spark cluster
    simply by pointing the `spark_connect()` function to the correct hostname. The
    code can also be easily adapted to other modeling approaches using one of the
    supervised learning functions in the Spark Machine Learning Library (MLlib) listed
    at [https://spark.rstudio.com/mlib/](https://spark.rstudio.com/mlib/).
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 仅用几行 R 语言代码，我们就构建了一个使用 Spark 的随机森林模型，该模型可以扩展以处理数百万条记录。如果需要更多的计算能力，只需将 `spark_connect()`
    函数指向正确的主机名，代码就可以在云中使用大规模并行 Spark 集群运行。代码也可以轻松地适应 Spark 机器学习库（MLlib）中列出的其他建模方法，这些方法包括在
    [https://spark.rstudio.com/mlib/](https://spark.rstudio.com/mlib/) 中的监督学习函数。
- en: Perhaps the easiest way to get started using Spark is with Databricks, a cloud
    platform developed by the creators of Spark that makes it easy to manage and scale
    clusters via a web-based interface. The free “Community Edition” provides a small
    cluster for you to try tutorials or even experiment with your own data. Check
    it out at [https://databricks.com](https://databricks.com).
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 可能开始使用 Spark 的最简单方式是通过 Databricks，这是一个由 Spark 的创造者开发的云平台，它通过基于网络的界面使管理集群和扩展集群变得容易。免费的“社区版”提供了一个小型集群，您可以尝试教程，甚至可以对自己的数据进行实验。请访问
    [https://databricks.com](https://databricks.com) 了解详情。
- en: Learning via distributed and scalable algorithms with H2O
  id: totrans-490
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 H2O 通过分布式和可扩展算法进行学习
- en: The **H2O project** ([https://h2o.ai](https://h2o.ai)) is a big data framework
    that provides fast in-memory implementations of machine learning algorithms, which
    can also operate in a cluster-computing environment. It includes functions for
    many of the methods covered in this book, including Naive Bayes, regression, deep
    neural networks, k-means clustering, ensemble methods, and random forests, among
    many others.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: '**H2O 项目** ([https://h2o.ai](https://h2o.ai)) 是一个大数据框架，它提供了机器学习算法的快速内存实现，这些算法也可以在集群计算环境中运行。它包括本书中涵盖的许多方法，包括朴素贝叶斯、回归、深度神经网络、k-means
    聚类、集成方法和随机森林等。'
- en: H2O uses heuristics to find approximate solutions to machine learning problems
    by iterating repeatedly over smaller chunks of the data. This gives the user the
    control to determine exactly how much of a massive dataset the learner should
    use. For some problems, a quick solution may be acceptable, but for others, the
    complete set may be required, which will require additional training time.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: H2O 通过对数据的小块进行重复迭代来寻找机器学习问题的近似解。这使用户能够精确地确定学习者应该使用多少大规模数据集。对于某些问题，快速解决方案可能是可以接受的，但对于其他问题，可能需要完整的集合，这将需要额外的训练时间。
- en: H2O is usually substantially faster and performs better on very massive datasets
    relative to Spark’s machine learning functions, which are already much faster
    than base R. However, because Apache Spark is a commonly used cluster-computing
    and big data preparation environment, H2O can be run on Apache Spark using the
    **Sparkling Water** software. With Sparkling Water, data scientists have the best
    of both worlds—the benefits of Spark for data preparation, and the benefits of
    H2O for machine learning.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 相比于 Spark 的机器学习函数，H2O 通常在处理非常大的数据集时速度更快，性能也更好，而 Spark 的机器学习函数已经比基础 R 语言快得多。然而，由于
    Apache Spark 是一个常用的集群计算和大数据准备环境，H2O 可以通过 **Sparkling Water** 软件在 Apache Spark
    上运行。使用 Sparkling Water，数据科学家可以同时享受到 Spark 在数据准备方面的优势和 H2O 在机器学习方面的优势。
- en: The `h2o` package provides functionality for accessing an H2O instance from
    within the R environment. A full tutorial on H2O is outside the scope of this
    book, and documentation is available at `http://docs.h2o.ai`, but the basics are
    straightforward.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '`h2o` 包提供了在 R 环境中访问 H2O 实例的功能。关于 H2O 的完整教程超出了本书的范围，文档可在 `http://docs.h2o.ai`
    查找，但基本概念是直接的。'
- en: 'To get started, be sure you have Java installed on your computer ([http://www.java.com](http://www.java.com))
    and install the `h2o` package in R. Then, initialize a local H2O instance using
    the following code:'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用，请确保您的计算机上已安装 Java ([http://www.java.com](http://www.java.com))，并在 R 中安装
    `h2o` 包。然后，使用以下代码初始化一个本地 H2O 实例：
- en: '[PRE102]'
  id: totrans-496
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'This starts an H2O server on your computer, which can be viewed via H2O Flow
    at `http://localhost:54321`. The H2O Flow web application allows you to administer
    and send commands to the H2O server, or even build and evaluate models using a
    simple, browser-based interface:'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在您的计算机上启动一个H2O服务器，您可以通过H2O Flow在`http://localhost:54321`查看它。H2O Flow网络应用程序允许您管理并向H2O服务器发送命令，甚至可以使用简单的基于浏览器的界面构建和评估模型：
- en: '![](img/B17290_15_15.png)'
  id: totrans-498
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17290_15_15.png)'
- en: 'Figure 15.15: H2O Flow is a web application for interacting with the H2O instance'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.15：H2O Flow是一个用于与H2O实例交互的Web应用程序
- en: 'Although you could complete an analysis within this interface, let’s go back
    to R and use H2O on the loan default data that we examined previously. First,
    we need to upload the `credit.csv` dataset to this instance using the following
    command:'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然您可以在该界面内完成分析，但让我们回到R，并使用之前检查过的贷款违约数据应用H2O。首先，我们需要使用以下命令将`credit.csv`数据集上传到此实例：
- en: '[PRE103]'
  id: totrans-501
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: Note that the `.hex` extension is used to refer to an H2O data frame.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`.hex`扩展名用于指代H2O数据帧。
- en: 'Next, we’ll apply H2O’s random forest implementation to this dataset using
    the following command:'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用以下命令将H2O的随机森林实现应用于此数据集：
- en: '[PRE104]'
  id: totrans-504
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'The output of this command includes information on the out-of-bag estimates
    of model performance:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令的输出包括模型性能的袋外估计信息：
- en: '[PRE105]'
  id: totrans-506
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: Although the credit dataset used here is not very large, the H2O code used here
    would scale to datasets of almost any size. Additionally, the code would be virtually
    unchanged if it were to be run in the cloud—simply point the `h2o.init()` function
    to the remote host.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这里使用的信用数据集并不大，但这里使用的H2O代码可以扩展到几乎任何大小的数据集。此外，如果要在云中运行，代码几乎不会改变——只需将`h2o.init()`函数指向远程主机即可。
- en: GPU computing
  id: totrans-508
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GPU计算
- en: An alternative to parallel processing uses a computer’s **graphics processing
    unit** (**GPU**) to increase the speed of mathematical calculations. A GPU is
    a specialized processor that is optimized for rapidly displaying images on a computer
    screen. Because a computer often needs to display complex 3D graphics (particularly
    for video games), many GPUs use hardware designed for parallel processing and
    extremely efficient matrix and vector calculations.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 并行处理的一个替代方案是使用计算机的**图形处理单元**（**GPU**）来提高数学计算的效率。GPU是一种专门处理器，它针对快速在计算机屏幕上显示图像进行了优化。由于计算机通常需要显示复杂的3D图形（尤其是视频游戏），许多GPU使用了专为并行处理和极其高效的矩阵和向量计算设计的硬件。
- en: 'A side benefit is that they can be used to efficiently solve certain types
    of mathematical problems. As depicted in the following illustration, where a typical
    laptop or desktop computer processor may have on the order of 16 cores, a typical
    GPU may have thousands or even tens of thousands:'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 一个附带的好处是，它们可以用来高效地解决某些类型的数学问题。如图所示，一个典型的笔记本电脑或台式计算机处理器可能有大约16个核心，而一个典型的GPU可能有数千甚至数万个：
- en: '![A picture containing shoji, building  Description automatically generated](img/B17290_15_16.png)'
  id: totrans-511
  prefs: []
  type: TYPE_IMG
  zh: '![包含障子、建筑物的图片，描述自动生成](img/B17290_15_16.png)'
- en: 'Figure 15.16: A graphics processing unit (GPU) has many times more cores than
    the typical central processing unit (CPU)'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.16：一个图形处理单元（GPU）的核心数量比典型的中央处理器（CPU）多得多
- en: The downside of GPU computing is that it requires specific hardware that is
    not included with many computers. In most cases, a GPU from the manufacturer NVIDIA
    is required, as it provides a proprietary framework called **Complete Unified
    Device Architecture** (**CUDA**) that makes the GPU programmable using common
    languages such as C++.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: GPU计算的一个缺点是它需要特定的硬件，而这并不是许多计算机都包含的。在大多数情况下，需要NVIDIA制造商的GPU，因为它提供了一个专有的框架，称为**完整统一设备架构**（**CUDA**），这使得GPU可以使用如C++等通用语言进行编程。
- en: For more information on NVIDIA’s role in GPU computing, go to [https://www.nvidia.com/en-us/deep-learning-ai/solutions/machine-learning/](https://www.nvidia.com/en-us/deep-learning-ai/solutions/machine-learning/).
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 有关NVIDIA在GPU计算中作用的更多信息，请访问[https://www.nvidia.com/en-us/deep-learning-ai/solutions/machine-learning/](https://www.nvidia.com/en-us/deep-learning-ai/solutions/machine-learning/)。
- en: The `gputools` package by Josh Buckner, Mark Seligman, and Justin Wilson implements
    several R functions, such as matrix operations, clustering, and regression modeling
    using the NVIDIA CUDA toolkit. The package requires a CUDA 1.3 or higher GPU and
    the installation of the NVIDIA CUDA toolkit. This package was once the standard
    approach for GPU computing in R, but appears to have gone without an update since
    2017 and has since been removed from the CRAN repository.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: Josh Buckner、Mark Seligman和Justin Wilson开发的`gputools`包实现了几个R函数，例如使用NVIDIA CUDA工具包进行矩阵运算、聚类和回归建模。该包需要CUDA
    1.3或更高版本的GPU以及NVIDIA CUDA工具包的安装。这个包曾经是R中GPU计算的标准方法，但似乎自2017年以来没有更新，并且已经被从CRAN存储库中移除。
- en: 'Instead, it appears that GPU work has transitioned to the TensorFlow mathematical
    library. The RStudio team provides information about using a local or cloud GPU
    on the following pages:'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，GPU工作似乎已经转向了TensorFlow数学库。RStudio团队在以下页面提供了有关在本地或云上使用GPU的信息：
- en: '[https://tensorflow.rstudio.com/install/local_gpu](https://tensorflow.rstudio.com/install/local_gpu)'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://tensorflow.rstudio.com/install/local_gpu](https://tensorflow.rstudio.com/install/local_gpu)'
- en: '[https://tensorflow.rstudio.com/install/cloud_server_gpu](https://tensorflow.rstudio.com/install/cloud_server_gpu)'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://tensorflow.rstudio.com/install/cloud_server_gpu](https://tensorflow.rstudio.com/install/cloud_server_gpu)'
- en: At the time of publication, a typical GPU used for deep learning is priced at
    several hundred US dollars for entry-level models and around $1,000-$3,000 for
    moderately priced units with greater performance. High-end units may cost many
    thousands of dollars.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 在出版时，用于深度学习的典型GPU，入门级模型的价格为几百美元，而中等价格、性能更高的单元的价格在1000-3000美元之间。高端单元可能要花费数千美元。
- en: Rather than spending this much up front, many people rent server time by the
    hour on cloud providers like AWS and Microsoft Azure, where it costs approximately
    $1 per hour for a minimal GPU instance—just don’t forget to shut it down when
    your work completes, as it can get expensive quite quickly!
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是一开始就花费这么多，许多人选择在AWS和Microsoft Azure等云服务提供商按小时租用服务器时间，那里一个最小的GPU实例每小时大约花费1美元——只是别忘了当你的工作完成后关闭它，因为它可能会很快变得很昂贵！
- en: Summary
  id: totrans-521
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: It is certainly an exciting time to be studying machine learning. Ongoing work
    on the relatively uncharted frontiers of parallel and distributed computing offers
    great potential for tapping the knowledge found in the deluge of big data. And
    the burgeoning data science community is facilitated by the free and open-source
    R programming language, which provides a very low barrier to entry—you simply
    need to be willing to learn.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 研究机器学习无疑是激动人心的时刻。在相对未知的并行和分布式计算前沿领域的研究，为挖掘大数据洪流中的知识提供了巨大的潜力。而数据科学社区的蓬勃发展得益于免费开源的R编程语言，它为入门提供了非常低的门槛——你只需要愿意学习。
- en: The topics you have learned, in both this chapter as well as previous chapters,
    provide the foundation for understanding more advanced machine learning methods.
    It is now your responsibility to keep learning and adding tools to your arsenal.
    Along the way, be sure to keep in mind the no free lunch theorem—no learning algorithm
    rules them all, and they all have varying strengths and weaknesses. For this reason,
    there will always be a human element to machine learning, adding subject-specific
    knowledge and the ability to match the appropriate algorithm to the task at hand.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 你在本章以及之前章节中学到的主题，为理解更高级的机器学习方法提供了基础。现在，你的责任是继续学习和添加工具到你的工具箱中。在这个过程中，务必牢记没有免费午餐定理——没有学习算法适用于所有情况，它们都有各自的优势和劣势。因此，机器学习中始终会有人类因素，这包括特定领域的知识以及将适当的算法匹配到当前任务的能力。
- en: In the coming years, it will be interesting to see how the human side changes
    as the line between machine learning and human learning blurs. Services such as
    Amazon’s Mechanical Turk provide crowd-sourced intelligence, offering a cluster
    of human minds ready to perform simple tasks at a moment’s notice. Perhaps one
    day, just as we have used computers to perform tasks that human beings cannot
    do easily, computers will employ human beings to do the reverse. What interesting
    food for thought!
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来几年里，将很有趣地看到随着机器学习和人类学习之间的界限变得模糊，人类方面将如何改变。像Amazon的Mechanical Turk这样的服务提供了众包智能，提供了一群随时准备执行简单任务的人类大脑。也许有一天，就像我们使用计算机来完成人类难以轻易完成的任务一样，计算机将利用人类来完成相反的任务。这真是一个有趣的思考！
- en: Join our book’s Discord space
  id: totrans-525
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 4000 people at:'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区，与志同道合的人相聚，并和超过 4000 人一起学习，详情请访问：
- en: '[https://packt.link/r](https://packt.link/r)'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/r](https://packt.link/r)'
- en: '![](img/r.jpg)'
  id: totrans-528
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/r.jpg)'
