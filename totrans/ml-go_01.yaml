- en: Gathering and Organizing Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Polls have shown that 90% or more of a data scientist's time is spent gathering
    data, organizing it, and cleaning it, not training/tuning their sophisticated
    machine learning models. Why is this? Isn't the machine learning part the fun
    part? Why do we need to care so much about the state of our data? Firstly, without
    data, our machine learning models can't learn. This might seem obvious. However,
    we need to realize that part of the strength of the models that we build is in
    the data that we feed them. As the common phrase goes, *garbage in, garbage out*.
    We need to make sure that we gather relevant, clean data to power our machine
    learning models, such that they can operate on the data as expected and produce
    valuable results.
  prefs: []
  type: TYPE_NORMAL
- en: Not all types of data are appropriate when using certain types of models. For
    example, certain models do not perform well when we have high-dimensional data
    (for example, text data), and other models assume that variables are normally
    distributed, which is definitely not always the case. Thus, we must take care
    in gathering data that fits our use case and make sure that we understand how
    our data and models will interact.
  prefs: []
  type: TYPE_NORMAL
- en: Another reason why gathering and organizing data consumes so much of a data
    scientist's time is that data is often messy and hard to aggregate. In most organizations,
    data might be housed in various systems and formats, and have various access control
    policies. We can't assume that supplying a training set to our model will be as
    easy as specifying a file path; this is often not the case.
  prefs: []
  type: TYPE_NORMAL
- en: To form a training/test set or to supply variables to a model for predictions,
    we will likely need to deal with various formats of data, such as CSV, JSON, database
    tables, and so on, and we will likely need to transform individual values. Common
    transformations include parsing date times, converting categorical data to numerical
    data, normalizing values, and applying some function across values. However, we
    can't always assume that all values of a certain variable are present or able
    to be parsed in a similar manner.
  prefs: []
  type: TYPE_NORMAL
- en: Often data includes missing values, mixed types, or corrupted values. How we
    handle each of these scenarios will directly influence the quality of the models
    that we build, and thus, we have to be willing to carefully gather, organize,
    and understand our data.
  prefs: []
  type: TYPE_NORMAL
- en: Even though much of this book will be focused on various modeling techniques,
    you should always consider data gathering, parsing, and organization as a (or
    maybe the) key component of a successful data science project. If this part of
    your project is not carefully developed with a high level of integrity, you are
    setting yourself up for trouble in the long run.
  prefs: []
  type: TYPE_NORMAL
- en: Handling data - Gopher style
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In comparison to many other languages that are used for data science/analysis,
    Go provides a very strong foundation for data manipulation and parsing. Although
    other languages (for example, Python or R) may allow users to quickly explore
    data interactively, they often promote integrity-breaking convenience, that is,
    dynamic and interactive data exploration often results in code that behaves strangely
    when applied more generally.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take, for instance, this simple CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'It is true that, very quickly, we can write some Python code to parse this
    CSV and output the maximum value from the integer column without even knowing
    what types are in the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This simple program will print the correct result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We now remove one of the integer values to produce a missing value, as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The Python program consequently has a complete breakdown in integrity; specifically,
    the program still runs, doesn''t tell us that anything went differently, still
    produces a value, and produces a value of a different type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This is unacceptable. All but one of our integer values could disappear, and
    we wouldn't have any insight into the changes. This could produce profound changes
    in our modeling, but they would be extremely hard to track down. Generally, when
    we opt for the conveniences of dynamic types and abstraction, we are accepting
    this sort of variability in behavior.
  prefs: []
  type: TYPE_NORMAL
- en: The important thing here is not that you cannot handle such behavior in Python,
    because Python, experts will quickly recognize that you can properly handle such
    behavior. The point is that such conveniences do not promote integrity by default,
    and thus, it is very easy to shoot yourself in the foot.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, we can leverage Go''s static typing and explicit error handling
    to ensure that our data is parsed as expected. In this small example, we can also
    write some Go code, without too much trouble, to parse our CSV (don''t worry about
    the details right now):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce the same correct result for the CSV file with all the integer
    values present:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'But in contrast to our previous Python code, our Go code will inform us when
    we encounter something that we don''t expect in the input CSV (for the case when
    we remove the value 3):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have maintained integrity, and we can ensure that we can handle missing
    values in a manner that is appropriate for our use case.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices for gathering and organizing data with Go
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you can see in the preceding section, Go itself provides us with an opportunity
    to maintain high levels of integrity in our data gathering, parsing, and organization.
    We want to ensure that we leverage Go's unique properties whenever we are preparing
    our data for machine learning workflows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, Go data scientists/analysts should follow the following best practices
    when gathering and organizing data. These best practices are meant to help you
    maintain integrity in your applications, and been able you to reproduce any analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Check for and enforce expected types**: This might seem obvious, but it is
    too often overlooked when using dynamically typed languages. Although it is slightly
    verbose, explicitly parsing data into expected types and handling related errors
    can save you big headaches down the road.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Standardize and simplify your data ingress/egress**: There are many third-party
    packages for handling certain types of data or interactions with certain sources
    of data (some of which we will cover in this book). However, if you standardize
    the ways you are interacting with data sources, particularly centered around the
    use of `stdlib`, you can develop predictable patterns and maintain consistency
    within your team. A good example of this is a choice to utilize `database/sql`
    for database interactions rather than using various third-party APIs and DSLs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Version your data**: Machine learning models produce extremely different
    results depending on the training data you use, your choice of parameters, and
    input data. Thus, it is impossible to reproduce results without versioning both
    your code and data. We will discuss the appropriate techniques for data versioning
    later in this chapter.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you start to stray from these general principles, you should stop immediately.
    You are likely to sacrifice integrity for the sake of convenience, which is a
    dangerous road. We will let these principles guide us through the book and as
    we consider various data formats/sources in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: CSV files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CSV files might not be a go-to format for big data, but as a data scientist
    or developer working in machine learning, you are sure to encounter this format.
    You might need a mapping of zip codes to latitude/longitude and find this as a
    CSV file on the internet, or you may be given sales figures from your sales team
    in a CSV format. In any event, we need to understand how to parse these files.
  prefs: []
  type: TYPE_NORMAL
- en: The main package that we will utilize in parsing CSV files is `encoding/csv`
    from Go's standard library. However, we will also discuss a couple of packages
    that allow us to quickly manipulate or transform CSV data--`github.com/kniren/gota/dataframe`
    and `go-hep.org/x/hep/csvutil`.
  prefs: []
  type: TYPE_NORMAL
- en: Reading in CSV data from a file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s consider a simple CSV file, which we will return to later, named `iris.csv`
    (available here: [https://archive.ics.uci.edu/ml/datasets/iris](https://archive.ics.uci.edu/ml/datasets/iris)).
    This CSV file includes four float columns of flower measurements and a string
    column with the corresponding flower species:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'With `encoding/csv` imported, we first open the CSV file and create a CSV reader
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can read in all of the records (corresponding to rows) of the CSV file.
    These records are imported as `[][]string`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also read in records one at a time in an infinite loop. Just make sure
    that you check for the end of the file (`io.EOF`) so that the loop ends after
    reading in all of your data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: If your CSV file is not delimited by commas and/or if your CSV file contains
    commented rows, you can utilize the `csv.Reader.Comma` and `csv.Reader.Comment`
    fields to properly handle uniquely formatted CSV files. In cases where the fields
    in your CSV file are single-quoted, you may need to add in a helper function to
    trim the single quotes and parse the values.
  prefs: []
  type: TYPE_NORMAL
- en: Handling unexpected fields
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The preceding methods work fine with clean CSV data, but, in general, we don''t
    encounter clean data. We have to parse messy data. For example, you might find
    unexpected fields or numbers of fields in your CSV records. This is why `reader.FieldsPerRecord`
    exists. This field of the reader value lets us easily handle messy data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This version of the `iris.csv` file has an extra field in one of the rows.
    We know that each record should have five fields, so let''s set our `reader.FieldsPerRecord`
    value to `5`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then as we are reading in records from the CSV file, we can check for unexpected
    fields and maintain the integrity of our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have chosen to handle the error by logging the error, and we only collect
    successfully parsed records into `rawCSVData`. The reader will note that this
    error could be handled in many different ways. The important thing is that we
    are forcing ourselves to check for an expected property of the data and increasing
    the integrity of our application.
  prefs: []
  type: TYPE_NORMAL
- en: Handling unexpected types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We just saw that CSV data is read into Go as `[][]string`. However, Go is statically
    typed, which allows us to enforce strict checks for each of the CSV fields. We
    can do this as we parse each field for further processing. Consider some messy
    data that has random fields that don''t match the type of the other values in
    a column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'To check the types of the fields in our CSV records, let''s create a `struct`
    variable to hold successfully parsed values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, before we loop over the records, let''s initialize a slice of these values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now as we loop over the records, we can parse into the relevant type for that
    record, catch any errors, and log as needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Manipulating CSV data with data frames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you can see, manually parsing many different fields and performing row-by-row
    operations can be rather verbose and tedious. This is definitely *not* an excuse
    to increase complexity and import a bunch of non standard functionalities. You
    should still default to the use of `encoding/csv` in most cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, manipulation of data frames has proven to be a successful and somewhat
    standardized way (in the data science community) of dealing with tabular data.
    Thus, in some cases, it is worth employing some third-party functionality to manipulate
    tabular data, such as CSV data. For example, data frames and the corresponding
    functionality can be very useful when you are trying to filter, subset, and select
    portions of tabular datasets. In this section, we will introduce `github.com/kniren/gota/dataframe`,
    a wonderful `dataframe` package for Go:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'To create a data frame from a CSV file, we open a file with `os.Open()` and
    then supply the returned pointer to the `dataframe.ReadCSV()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'If we compile and run this Go program, we will see a nice, pretty-printed version
    of our data with the types that were inferred during parsing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have the data parsed into a `dataframe`, we can filter, subset, and
    select our data easily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This is really only scratching the surface of the `github.com/kniren/gota/dataframe`
    package. You can merge datasets, output to other formats, and even process JSON
    data. For more information about this package, you should visit the auto generated
    GoDocs at [https://godoc.org/github.com/kniren/gota/dataframe](https://godoc.org/github.com/kniren/gota/dataframe),
    which is good practice, in general, for any packages we discuss in the book.
  prefs: []
  type: TYPE_NORMAL
- en: JSON
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a world in which the majority of data is accessed via the web, and most engineering
    organizations implement some number of microservices, we are going to encounter
    data in JSON format fairly frequently. We may only need to deal with it when pulling
    some random data from an API, or it might actually be the primary data format
    that drives our analytics and machine learning workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, JSON is used when ease of use is the primary goal of data interchange.
    Since JSON is human readable, it is easy to debug if something breaks. Remember
    that we want to maintain the integrity of our data handling as we process data
    with Go, and part of that process is ensuring that, when possible, our data is
    interpretable and readable. JSON turns out to be very useful in achieving these
    goals (which is why it is also used for logging, in many cases).
  prefs: []
  type: TYPE_NORMAL
- en: Go offers really great JSON functionality in its standard library with `encoding/json`.
    We will utilize this standard library functionality throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing JSON
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To understand how to parse (that is, unmarshal) JSON data in Go, we will be
    using some data from the Citi Bike API ([https://www.citibikenyc.com/system-data](https://www.citibikenyc.com/system-data)),
    a bike-sharing service operating in New York City. Citi Bike provides frequently
    updated operational information about its network of bike sharing stations in
    JSON format at [https://gbfs.citibikenyc.com/gbfs/en/station_status.json](https://gbfs.citibikenyc.com/gbfs/en/station_status.json):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'To parse the import and this type of data in Go, we first need to import `encoding/json`
    (along with a couple of other things from a standard library, such as `net/http`,
    because we are going to pull this data off of the previously mentioned website).
    We will also define `struct` that mimics the structure of the JSON shown in the
    preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Note a couple of things here: (i) we have followed Go idioms by avoiding the
    `struct` field name with underscores, but (ii) we have utilized the `json` struct
    tags to label the `struct` fields with the corresponding expected fields in the
    JSON data.'
  prefs: []
  type: TYPE_NORMAL
- en: Note, to properly parse JSON data, the struct fields need to be exported fields.
    That is, the fields need to begin with a capital letter. `encoding/json` does
    cannot view fields using reflect unless they are exported.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can get the JSON data from the URL and unmarshal it into a new `stationData`
    value. This will produce a `struct` variable with the respective fields filled
    with the data in the tagged JSON data fields. We can check it by printing out
    some data associated with one of the stations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run this, we can see that our `struct` contains the parsed data from
    the URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: JSON output
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let''s say that we have the Citi Bike station data in our `stationData`
    struct value and we want to save that data out to a file. We can do this with
    `json.marshal`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: SQL-like databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although there is a good bit of hype around interesting NoSQL databases and
    key-value stores, SQL-like databases are still ubiquitous. Every data scientist
    will, at some point, be processing data from an SQL-like database, such as Postgres,
    MySQL, or SQLite.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we may be required to query one or more tables in a Postgres database
    to generate a set of features for model training. After using that model to make
    predictions or identify anomalies, we may send results to another database table
    that drives a dashboard or other reporting tool.
  prefs: []
  type: TYPE_NORMAL
- en: Go, of course, interacts nicely with all the popular data stores, such as SQL,
    NoSQL, key-value, and so on, but here, we will focus on SQL-like interactions.
    We will utilize `database/sql` for these interactions throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to an SQL database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first thing we need do before connecting to an SQL-like database is identify
    the particular database that we will be interacting with and import a corresponding
    driver. In the following examples, we will be connecting to a Postgres database
    and will utilize the `github.com/lib/pq` database driver for `database/sql`. This
    driver can be loaded via an empty import (with a corresponding comment):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s assume that you have exported the Postgres connection string to
    an environmental variable `PGURL`. We can easily create an `sql.DB` value for
    our connection via the follow code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Note that we need to defer the `close` method on this value. Also, note that
    creating this value does not mean that you have made a successful connection to
    the database. This is merely a value used by `database/sql` to connect to the
    database when triggered to do so by certain operations (such as a query).
  prefs: []
  type: TYPE_NORMAL
- en: 'To ensure that we can make a successful connection to the database, we can
    use the `Ping` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Querying the database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we know how to connect to the database, let''s see how we can get
    data out of the database. We won''t cover the specifics of SQL queries and statements
    in this book. If you are not familiar with SQL, I would highly recommend that
    you learn how to query, insert, and so on, but for our purposes here, you should
    know that there are basically two types of operations we want to perform as related
    to SQL databases:'
  prefs: []
  type: TYPE_NORMAL
- en: A `Query` operation selects, groups, or aggregates data in the database and
    returns rows of data to us
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An `Exec` operation updates, inserts, or otherwise modifies the state of the
    database without an expectation that portions of the data stored in the database
    should be returned
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As you might expect, to get data out of our database, we will use a `Query`
    operation. To do this, we need to query the database with an SQL statement string.
    For example, imagine we have a database storing a bunch of iris flower measurements
    (petal length, petal width, and so on), we could query some of that data related
    to a particular iris species as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that this returns a pointer to an `sql.Rows` value, and we need to defer
    the closing of this rows value. Then we can loop over our rows and parse the data
    into values of expected type. We utilize the `Scan` method on rows to parse out
    the columns returned by the SQL query and print them to standard out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we need to check for any errors that might have occurred while processing
    our rows. We want to maintain the integrity of our data handling, and we cannot
    assume that we looped over all the rows without encountering an error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Modifying the database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned earlier, there is another flavor of interaction with the database
    called `Exec`. With these types of statements, we are concerned with updating,
    adding to, or otherwise modifying the state of one or more tables in the database.
    We use the same type of database connection, but instead of calling `db.Query`,
    we will call `db.Exec`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s say we want to update some of the values in our iris database
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'But how do we know whether we were successful and changed something? Well,
    the `res` function returned here allows us to see how many rows of our table were
    affected by our update:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Caching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, our machine learning algorithms will be trained by and/or given input
    for prediction via data from external sources (for example, APIs), that is, data
    that isn't local to the application running our modeling or analysis. Further,
    we might have various sets of data that are being accessed frequently, may be
    accessed again soon, or may need to be made available while the application is
    running.
  prefs: []
  type: TYPE_NORMAL
- en: In at least some of these cases, it might make sense to cache data in memory
    or embed the data locally where the application is running. For example, if you
    are reaching out to a government API (typically having high latency) for census
    data frequently, you may consider maintaining a local or in-memory cache of the
    census data being used so that you can avoid constantly reaching out to the API.
  prefs: []
  type: TYPE_NORMAL
- en: Caching data in memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To cache a series of values in memory, we will use `github.com/patrickmn/go-cache`.
    With this package, we can create an in-memory cache of keys and corresponding
    values. We can even specify things, such as the time to live, in the cache for
    specific key-value pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a new in-memory cache and set a key-value pair in the cache, we do
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'To then retrieve the value for `mykey` out of the cache, we just need to use
    the `Get` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Caching data locally on disk
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The caching we just saw is in memory. That is, the cached data exists and is
    accessible while your application is running, but as soon as your application
    exits, your data disappears. In some cases, you may want your cached data to stick
    around when your application restarts or exits. You may also want to back up your
    cache such that you don't have to start applications from scratch without a cache
    of relevant data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In these scenarios, you may consider using a local, embedded cache, such as
    `github.com/boltdb/bolt`. BoltDB, as it is referred to, is a very popular project
    for these sorts of applications, and basically consists of a local key-value store.
    To initialize one of these local key-value stores, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: You can, of course, have multiple different buckets of data in your BoltDB and
    use a filename other than `embedded.db`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s say you had a map of string values in memory that you need to
    cache in BoltDB. To do this, you would range over the keys and values in the map,
    updating your BoltDB:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, to get values out of BoltDB, you can view your data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Data versioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned, machine learning models produce extremely different results depending
    on the training data you use, the choices of parameters, and the input data. It
    is essential to be able to reproduce results for collaborative, creative, and
    compliance reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Collaboration**: Despite what you see on social media, there are no data
    science and machine learning unicorns (that is, people with knowledge and capabilities
    in every area of data science and machine learning). We need to have our colleagues''
    reviews and improve on our work, and this is impossible if they aren''t able to
    reproduce our model results and analyses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Creativity**: I don''t know about you, but I have trouble remembering even
    what I did yesterday. We can''t trust ourselves to always remember our reasoning
    and logic, especially when we are dealing with machine learning workflows. We
    need to track exactly what data we are using, what results we created, and how
    we created them. This is the only way we will be able to continually improve our
    models and techniques.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compliance**: Finally, we may not have a choice regarding data versioning
    and reproducibility in machine learning very soon. Laws are being passed around
    the world (for example, the **General Data Protection Regulation** (**GDPR**)
    in the European Union) that give users a right to an explanation for algorithmically
    made decisions. We simply cannot hope to comply with these rulings if we don''t
    have a robust way of tracking what data we are processing and what results we
    are producing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are multiple open source data versioning projects. Some of these are
    focused on security and peer-to-peer distributed storage of data. Others are focused
    on data science workflows. In this book, we will focus on and utilize Pachyderm
    ([http://pachyderm.io/](http://pachyderm.io/)), an open source framework for data
    versioning and data pipelining. Some of the reasons for this will be clear later
    in the book when we talk about production deploys and managing ML pipelines. For
    now, I will just summarize some of the features of Pachyderm that make it an attractive
    choice for data versioning in Go-based (and other) ML projects:'
  prefs: []
  type: TYPE_NORMAL
- en: It has an convenient Go client, `github.com/pachyderm/pachyderm/src/client`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to version any type and format of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A flexible object store backing for the versioned data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration with a data pipelining system for driving versioned ML workflows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pachyderm jargon
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Think about versioning data in Pachyderm kind of like versioning code in Git.
    The primitives are similar:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Repositories**: These are versioned collections of data, similar to having
    versioned collections of code in Git repositories'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Commits**: Data is versioned in Pachyderm by making commits of that data
    into data repositories'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Branches**: These lightweight points to certain commits or sets of commits
    (for example, master points to the latest HEAD commit)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Files**: Data is versioned at the file level in Pachyderm, and Pachyderm
    automatically employs strategies, such as de-duplication, to keep your versioned
    data space efficient'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even though versioning data with Pachyderm feels similar to versioning code
    with Git, there are some major differences. For example, merging data doesn't
    exactly make sense. If there are merge conflicts on petabytes of data, no human
    could resolve these. Furthermore, the Git protocol would not be space efficient
    in general for large sets of data. Pachyderm uses its own internal logic to perform
    the versioning and work with versioned data, and the logic is both space efficient
    and processing efficient in terms of caching.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying/installing Pachyderm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will be using Pachyderm in various other places in the book to both version
    data and create distributed ML workflows. Pachyderm itself is an app that runs
    on top of Kubernetes ([https://kubernetes.io/](https://kubernetes.io/)), and is
    backed by an object store of your choice. For the purposes of this book, development,
    and experimentation, you can easily install and run Pachyderm locally. It should
    take 5-10 minutes to install and doesn't require much effort. The instructions
    for the local installation can be found in the Pachyderm documentation at [http://docs.pachyderm.io](http://docs.pachyderm.io).
  prefs: []
  type: TYPE_NORMAL
- en: When you are ready to run your workflows in production or your deploy model,
    you can easily deploy a production-ready Pachyderm cluster that will behave the
    same exact way as your local installation. Pachyderm can be deployed to any cloud,
    or even on premises.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, Pachyderm is an open source project and has an active group of
    users. If you have questions or need help, you can join the public Pachyderm Slack
    channel by visiting [http://slack.pachyderm.io/](http://slack.pachyderm.io/).
    The active Pachyderm users and the Pachyderm team itself will be able to respond
    very quickly to your questions there.
  prefs: []
  type: TYPE_NORMAL
- en: Creating data repositories for data versioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you followed the local installation of Pachyderm specified in the Pachyderm
    documentation, you should have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes running in a Minikube VM on your machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `pachctl` command line tool installed and connected to your Pachyderm cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, if you have a production cluster running in a cloud, the following
    steps still apply. Your `pachctl` would just be connected to the remote cluster.
  prefs: []
  type: TYPE_NORMAL
- en: We will be demonstrating data versioning functionality with the `pachctl` **Command-line
    Interface** (**CLI**) tool below (which is a Go program). However, as mentioned
    above, Pachyderm has a full-fledged Go client. You can create repositories, commit
    data, and much more directly from your Go programs. This functionality will be
    demonstrated later in Chapter 9, *Deploying and distributing Analyses and Models*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a repository of data called `myrepo`, you can run this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'You can then confirm that the repository exists with `list-repo`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: This `myrepo` repository is a collection of data that we have defined and is
    ready for housing-versioned data. Right now, there is no data in the repository,
    because we haven't put any data there yet.
  prefs: []
  type: TYPE_NORMAL
- en: Putting data into data repositories
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s say that we have a simple text file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'If this file is part of the data we are utilizing in our ML workflow, we should
    version it. To version this file in our repository, `myrepo`, we just need to
    commit it into that repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The `-c` flag specifies that we want Pachyderm to open a new commit, insert
    the file we are referencing, and close the commit all in one shot. The `-f` flag
    specifies that we are providing a file.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we are committing a single file to the master branch of a single repository
    here. However, the Pachyderm API is incredibly flexible. We can commit, delete,
    or otherwise modify many versioned files in a single commit or over multiple commits.
    Further, these files could be versioned via a URL, object store link, database
    dump, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a sanity check, we can confirm that our file was versioned in the repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Getting data out of versioned data repositories
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have versioned data in Pachyderm, we probably want to know how to
    interact with that data. The primary way is via Pachyderm data pipelines (which
    will be discussed later in this book). The mechanism for interacting with versioned
    data when using pipelines is a simple file I/O.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if we manually want to pull certain sets of versioned data out of
    Pachyderm, analyze them interactively, then we can use the `pachctl` CLI to get
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'CSV data:'
  prefs: []
  type: TYPE_NORMAL
- en: '`encoding/csv` docs: [https://golang.org/pkg/encoding/csv/](https://golang.org/pkg/encoding/csv/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`github.com/kniren/gota/dataframe` docs: [https://godoc.org/github.com/kniren/gota/dataframe](https://godoc.org/github.com/kniren/gota/dataframe)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'JSON data:'
  prefs: []
  type: TYPE_NORMAL
- en: '`encoding/json` docs: [https://golang.org/pkg/encoding/json/](https://golang.org/pkg/encoding/json/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bill Kennedy''s blog post of JSON decoding: [https://www.goinggo.net/2014/01/decode-json-documents-in-go.html](https://www.goinggo.net/2014/01/decode-json-documents-in-go.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ben Johnson''s blog post Go Walkthrough: `encoding/json` package: [https://medium.com/go-walkthrough/go-walkthrough-encoding-json-package-9681d1d37a8f](https://medium.com/go-walkthrough/go-walkthrough-encoding-json-package-9681d1d37a8f)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Caching:'
  prefs: []
  type: TYPE_NORMAL
- en: '`github.com/patrickmn/go-cache` docs: [https://godoc.org/github.com/patrickmn/go-cache](https://godoc.org/github.com/patrickmn/go-cache)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`github.com/boltdb/bolt` docs: [https://godoc.org/github.com/boltdb/bolt](https://godoc.org/github.com/boltdb/bolt)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Information about and motivation for BoltDB: [https://npf.io/2014/07/intro-to-boltdb-painless-performant-persistence/](https://npf.io/2014/07/intro-to-boltdb-painless-performant-persistence/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pachyderm:'
  prefs: []
  type: TYPE_NORMAL
- en: 'General documentation: [http://docs.pachyderm.io](http://docs.pachyderm.io)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Go client docs: [https://godoc.org/github.com/pachyderm/pachyderm/src/client](https://godoc.org/github.com/pachyderm/pachyderm/src/client)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Public users Slack team registration: [http://docs.pachyderm.io](http://docs.pachyderm.io)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to gather, organize, and parse data. This is
    the first step, and one of the most important step, in developing machine learning
    models, but having data does not get us very far if we do not gain some intuition
    about our data and put it into a standard form for processing. Next, we will tackle
    some techniques for further structuring our data (matrices) and for understanding
    our data (statistics and probability).
  prefs: []
  type: TYPE_NORMAL
