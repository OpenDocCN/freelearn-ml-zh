<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer050" class="Content">
			<h1 class="chapter-number"><a id="_idTextAnchor447"/>4<a id="_idTextAnchor448"/><a id="_idTextAnchor449"/><a id="_idTextAnchor450"/></h1>
			<h1 id="_idParaDest-88"><a id="_idTextAnchor451"/>Data Preparation and Transformation</h1>
			<p>You have probably heard that data scientists spend most of their time working on data-preparation-related activities. It is now time to explain why that happens and what types of activities they <span class="No-Break">work on.</span></p>
			<p>In this chapter, you will learn how to deal with categorical and numerical features, as well as how to apply different techniques to transform your data, such as one-hot encoding, binary encoders, ordinal encoding, binning, and text transformations. You will also learn how to handle missing values and outliers in your data, which are two important tasks you can implement to build good <strong class="bold">machine learning</strong> (<span class="No-Break"><strong class="bold">ML</strong></span><span class="No-Break">) models.</span></p>
			<p>In this chapter, you will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Identifying types <span class="No-Break">of features</span></li>
				<li>Dealing with <span class="No-Break">categorical features</span></li>
				<li>Dealing with <span class="No-Break">numerical features</span></li>
				<li>Understanding <span class="No-Break">data distributions</span></li>
				<li>Handling <span class="No-Break">missing values</span></li>
				<li>Dealing <span class="No-Break">with outliers</span></li>
				<li>Dealing with <span class="No-Break">unbalanced datasets</span></li>
				<li>Dealing with <span class="No-Break">text data</span></li>
			</ul>
			<p>This chapter is a little longer than the others and will require more patience. Knowing about these topics in detail will put you in a good position for the AWS Machine Learning <span class="No-Break">Specialty exam.</span></p>
			<h1 id="_idParaDest-89"><a id="_idTextAnchor452"/><a id="_idTextAnchor453"/>Identifying types of features</h1>
			<p>You <em class="italic">cannot</em> start <a id="_idTextAnchor454"/>modeling without<a id="_idTextAnchor455"/> knowing what a feature is and what type of information it can store. You have already read about the different processes that deal with features. For example, you know that feature engineering is related to the task of building and preparing features for your models; you also know that feature selection is related to the task of choosing the best set of features to feed a particular algorithm. These two tasks have one behavior in common: they may vary according to the types of features they <span class="No-Break">are processing.</span></p>
			<p>It is very<a id="_idTextAnchor456"/> important to understand this behavior (feature type versus applicable transformations) because it will help you eliminate invalid answers during <a id="_idTextAnchor457"/>your exam (and, most importantly, you will become a better <span class="No-Break">data scientist).</span></p>
			<p>By <em class="italic">types of features</em>, you refer to the data type that a particular feature is supposed to store. <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.1</em> shows how you could potentially describe the different types of features of <span class="No-Break">a model.</span></p>
			<div>
				<div id="_idContainer034" class="IMG---Figure">
					<img src="image/B21197_04_01.jpg" alt="Figure 4.1 – Feature types" width="1442" height="819"/>
				</div>
			</div>
			<p class="IMG---Figure"><a id="_idTextAnchor458"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1 – Feature types</p>
			<p>In <a href="B21197_01.xhtml#_idTextAnchor018"><span class="No-Break"><em class="italic">Chapter 1</em></span></a><em class="italic">, Machine Learning Fundamentals</em>, you were introduced to the feature classification shown in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.1</em>. Now, look at some real examples, so that you can eliminate any remaining questions you <span class="No-Break">may have:</span></p>
			<p class="IMG---Figure"><a id="_idTextAnchor459"/></p>
			<table id="table001-3" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Feature type</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Feature sub-type</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Definition</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Example</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Categorical</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Nominal</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Labelled variables with no <span class="No-Break">quantitative value</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Cloud provider: AWS, <span class="No-Break">MS, Google</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Categorical</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Ordinal</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Adds the sense of order to the <span class="No-Break">labelled variable</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Job title: junior data scientist, senior data scientist, chief <span class="No-Break">data scientist</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Categorical</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Binary</span></p>
						</td>
						<td class="No-Table-Style">
							<p>A variable with only two <span class="No-Break">allowed values</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Fraud classification: fraud, <span class="No-Break">not fraud</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Numerical</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Discrete</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Individual and <span class="No-Break">countable items</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Number of <span class="No-Break">students: 100</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Numerical</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Continuous</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Infinite number of possible measurements and they often carry <span class="No-Break">decimal points</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Total <span class="No-Break">amount: $150.35</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.1 – Real examples of feature values</p>
			<p>Although <a id="_idTextAnchor460"/>looking at the values of the variable may help you find its type, you <a id="_idTextAnchor461"/>should never rely only on this approach. The nature of the<a id="_idTextAnchor462"/> variable is also very important for making such decisions. For example, someone could encode the cloud provider variable shown in <em class="italic">Table 4.1</em> as follows: 1 (AWS), 2 (MS), 3 (Google). In that case, the variable is still a nominal feature, even if it is now represented by <span class="No-Break">discrete numbers.</span></p>
			<p>If you are building an ML model and you don’t tell your algorithm that this variable is not a discrete number but is instead a nominal variable, the algorithm will treat it as a number and the model won’t be <span class="No-Break">interpretable anymore.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Before feeding any ML algorithm with data, make sure your feature types have been <span class="No-Break">properly identified.</span></p>
			<p>In theory, if you are happy with your features and have properly classified each of them, you should be ready to go into the modeling phase of the CRISP-DM methodology, shouldn’t you? Well, maybe not. There are many reasons you may want to spend a little more time on data preparation, even after you have correctly classified <span class="No-Break">your features:</span></p>
			<ul>
				<li>Some ML libraries, such as <strong class="source-inline">scikit-learn</strong>, may not accept string values on your <span class="No-Break">categorical features.</span></li>
				<li>The data<a id="_idTextAnchor463"/> distribution of your variable may not be the most <a id="_idTextAnchor464"/>optimal distribution for <span class="No-Break">your algorithm.</span></li>
				<li>Your ML algorithm may be impacted by the scale of <span class="No-Break">your data.</span></li>
				<li>Some observations of your variable may be missing information and you will have to fix it. These are also known as <span class="No-Break">missing values.</span></li>
				<li>You may find outlier values of your variable that can potentially add bias to <span class="No-Break">your model.</span></li>
				<li>Your variable may be storing different types of information and you may only be interested in a few of them (for example, a date variable can store the day of the week or the week of <span class="No-Break">the month).</span></li>
				<li>You might want to find a mathematical representation for a <span class="No-Break">text variable.</span></li>
				<li>Believe me, this list has no <span class="No-Break">real end!</span></li>
			</ul>
			<p>In the following sections, you will understand how to address all these points, starting with <span class="No-Break">categorical features.</span></p>
			<h1 id="_idParaDest-90"><a id="_idTextAnchor465"/><a id="_idTextAnchor466"/>Dealing with categorical features</h1>
			<p>Data transformation <a id="_idTextAnchor467"/>methods for categorical features will vary according to the sub-type of your variable. In the upcoming sections, you will understand how to transform nominal and <span class="No-Break">ordinal features.</span></p>
			<h2 id="_idParaDest-91"><a id="_idTextAnchor468"/><a id="_idTextAnchor469"/>Transforming nominal features</h2>
			<p>You may have to create<a id="_idTextAnchor470"/> numerical representations of your <a id="_idTextAnchor471"/>categorical features before applying ML algorithms to them. Some libraries may have embedded logic to handle that transformation for you, but most of them <span class="No-Break">do not.</span></p>
			<p>The first transformation you will learn is<a id="_idTextAnchor472"/> known as label encoding. A label encoder <a id="_idTextAnchor473"/>is suitable for categorical/nominal variables, and it will just associate a number with each distinct label of your variables. <em class="italic">Table 4.2</em> shows how a label <span class="No-Break">encoder works:</span></p>
			<table id="table002-2" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold"><a id="_idTextAnchor474"/></strong><span class="No-Break"><strong class="bold">Countr</strong></span><span class="No-Break"><strong class="bold">y</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Label encoding</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">India</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Canada</span></p>
						</td>
						<td class="No-Table-Style">
							<p>2</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Brazil</span></p>
						</td>
						<td class="No-Table-Style">
							<p>3</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Australia</span></p>
						</td>
						<td class="No-Table-Style">
							<p>4</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">India</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.2 – Label encoder in action</p>
			<p>A label encoder will always ensure that a unique number is associated with each distinct label. In the preceding table, although “India” appears twice, the same number was assigned <span class="No-Break">to it.</span></p>
			<p>You now have a numerical representation of each country, but this does not mean you can use that numerical representation in your models! In this particular case, you are transforming a nominal feature, <em class="italic">which does not have </em><span class="No-Break"><em class="italic">an order</em></span><span class="No-Break">.</span></p>
			<p>According to <em class="italic">Table 4.2</em>, if you pass the encoded version of the <em class="italic">country</em> variable to a model, it will make assumptions such as “Brazil (3) is greater than Canada (2),” which does not make <span class="No-Break">any sense.</span></p>
			<p>One possible solution for that scenario is applying another type of transformation on top of “<em class="italic">country”</em>: <strong class="bold">one-hot encoding</strong>. This <a id="_idTextAnchor475"/>transformation will represent all the categories from the original feature as individual features (also known as dummy <strong class="bold">variables</strong>), which <a id="_idTextAnchor476"/>will store the presence or absence of each category. <em class="italic">Table 4.3</em> is transforming the same information from <em class="italic">Table 4.2</em>, but this time it’s applying <span class="No-Break">one-hot encoding:</span></p>
			<table id="table003-1" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold"><a id="_idTextAnchor477"/></strong><span class="No-Break"><strong class="bold">Country</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">India</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Canada</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Brazil</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Australia</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">India</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Canada</span></p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Brazil</span></p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Australia</span></p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">India</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.3 – One-hot encoding in action</p>
			<p>You can now use the <a id="_idTextAnchor478"/>one-hot encoded version of the <em class="italic">country</em> variable as a feature of an ML model. However, your work as a skeptical data scientist is <a id="_idTextAnchor479"/>never done, and your critical thinking ability will be tested in the AWS Machine Learning <span class="No-Break">Specialty exam.</span></p>
			<p>Suppose you have 150 distinct countries in your dataset. How many dummy variables would you come up with? 150, right? Here, you just ran into a potential issue: apart from adding complexity to your model (which is not a desired characteristic of any model at all), dummy variables also add <strong class="bold">sparsity</strong> to <span class="No-Break">your data.</span></p>
			<p>A sparse dataset has a lot of variables filled with zeros. Often, it is hard to fit this type of data structure into memory (you can easily run out of memory), and it is very time-consuming for ML algorithms to process <span class="No-Break">sparse structures.</span></p>
			<p>You can work around the sparsity problem by grouping your original data and reducing the number of categories, and you can even use custom libraries that compress your sparse data and make it easier to manipulate (such as <strong class="source-inline">scipy.sparse.csr_matrix</strong>, <span class="No-Break">from Python).</span></p>
			<p>Therefore, during the exam, remember that one-hot encoding is definitely the right way to go when you need to transform categorical/nominal data to feed ML models; however, take the number of unique categories of your original feature into account and think about whether it makes sense to create dummy variables for all of them (it might not make sense if you have a very large number of <span class="No-Break">unique categories).</span></p>
			<h2 id="_idParaDest-92"><a id="_idTextAnchor480"/><a id="_idTextAnchor481"/>Applying binary encoding</h2>
			<p>For those types of<a id="_idTextAnchor482"/> variables with a higher number of unique <a id="_idTextAnchor483"/>categories, a potential approach to creating a numerical representation for them is applying binary encoding. In this approach, the goal is transforming a categorical variable into multiple binary <a id="_idTextAnchor484"/>columns, but minimizing the number of <span class="No-Break">new columns.</span></p>
			<p>This process consists of three <span class="No-Break">basic steps:</span></p>
			<ol>
				<li>The categorical data is converted into numerical data after being passed through an <span class="No-Break">ordinal encoder.</span></li>
				<li>The resulting number is then converted into a <span class="No-Break">binary value.</span></li>
				<li>The binary value is split into <span class="No-Break">different columns.</span></li>
			</ol>
			<p><em class="italic">Table 4.4</em> shows how to convert the data from <em class="italic">Table 4.2</em> into a <span class="No-Break">binary variable.</span></p>
			<table id="table004" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold"><a id="_idTextAnchor485"/></strong><span class="No-Break"><strong class="bold">Country</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Label encoder</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Binary</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Col1</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Col2</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Col3</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">India</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">001</span></p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Canada</span></p>
						</td>
						<td class="No-Table-Style">
							<p>2</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">010</span></p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Brazil</span></p>
						</td>
						<td class="No-Table-Style">
							<p>3</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">011</span></p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Australia</span></p>
						</td>
						<td class="No-Table-Style">
							<p>4</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">100</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">India</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">001</span></p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.4 – Binary encoding in action</p>
			<p>As you can see, there are now three columns (Col1, Col2, and Col3) instead of four columns from the one-hot encoding transformation in <span class="No-Break"><em class="italic">Table 4.3</em></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor486"/><a id="_idTextAnchor487"/>Transforming ordinal features</h2>
			<p>Ordinal features <a id="_idTextAnchor488"/>have a very specific characteristic: <em class="italic">they have an order</em>. Because they have this quality, it does <em class="italic">not</em> make sense to apply one-hot encoding to them; if you do so, the underlying algorithm that is used to train your model will not be able to differentiate the implicit order of the data points associated with <span class="No-Break">this feature.</span></p>
			<p>The most common <a id="_idTextAnchor489"/>transformation for this type of variable is <a id="_idTextAnchor490"/>known as <strong class="bold">ordinal</strong> <strong class="bold">encoding</strong>. An <a id="_idTextAnchor491"/>ordinal encoder will associate a number with each distinct label of your variable, just like a label encoder does, but this time, it will respect the order of each category. The following table shows how an ordinal <span class="No-Break">encoder works:</span></p>
			<table id="table005" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold"><a id="_idTextAnchor492"/></strong><span class="No-Break"><strong class="bold">Education</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Ordinal encoding</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Trainee</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Junior <span class="No-Break">data analyst</span></p>
						</td>
						<td class="No-Table-Style">
							<p>2</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Senior <span class="No-Break">data analyst</span></p>
						</td>
						<td class="No-Table-Style">
							<p>3</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Chief <span class="No-Break">data scientist</span></p>
						</td>
						<td class="No-Table-Style">
							<p>4</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.5 – Ordinal encoding in action</p>
			<p>You can now pass the encoded variable to ML models and they will be able to handle this variable properly, with no need to apply one-hot encoding transformations. This time, comparisons such as “senior data analyst is greater than junior data analyst” make <span class="No-Break">total sense.</span></p>
			<h2 id="_idParaDest-94"><a id="_idTextAnchor493"/><a id="_idTextAnchor494"/>Avoiding confusion in our train and test datasets</h2>
			<p>Do not forget the <a id="_idTextAnchor495"/>following statement: encoders are fitted on training data and transformed on test and production data. This is how your ML pipeline <span class="No-Break">should work.</span></p>
			<p>Suppose you have created a one-hot encoder that fits the data from <em class="italic">Table 4.2</em> and returns data according to <em class="italic">Table 4.3</em>. In this example, assume this is your training data. Once you have completed your training process, you may want to apply the same one-hot encoding transformation to your testing data to check the <span class="No-Break">model’s results.</span></p>
			<p>In the scenario that was just described (which is a very common situation in modeling pipelines), you <em class="italic">cannot</em> retrain your encoder on top of the testing data! You should just reuse the previous encoder object that you have created on top of the training data. Technically, you shouldn’t use the <strong class="source-inline">fit</strong> method again but the <strong class="source-inline">transform</strong> <span class="No-Break">method instead.</span></p>
			<p>You may already <a id="_idTextAnchor496"/>know the reasons why you should follow this <a id="_idTextAnchor497"/>rule, but just as a reminder: the testing data was created to extract the performance metrics of your model, so you should not use it to extract any other knowledge. If you do so, your performance metrics will be biased by the testing data, and you cannot infer that the same performance (shown in the test data) is likely to happen in production (when new data will <span class="No-Break">come in).</span></p>
			<p>Alright, all good so far. However, what if your testing set has a new category that was not present in the training set? How are you supposed to transform <span class="No-Break">this data?</span></p>
			<p>Going back to the one-hot encoding example in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.3</em> (input data) and <em class="italic">Table 4.3</em> (output data), this encoder knows how to transform the following countries: Australia, Brazil, Canada, and India. If you had a different country in the testing set, the encoder would not know how to transform it, and that’s why you need to define how it will behave in scenarios where there <span class="No-Break">are exceptions.</span></p>
			<p>Most ML libraries provide specific parameters for these situations. In the previous example, you could configure the encoder to either raise an error or set all zeros on our dummy variables, as shown in <span class="No-Break"><em class="italic">Table 4.6</em></span><span class="No-Break">.</span></p>
			<table id="table006" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold"><a id="_idTextAnchor498"/></strong><span class="No-Break"><strong class="bold">Country</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">India</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Canada</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Brazil</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Australia</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">India</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Canada</span></p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Brazil</span></p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Australia</span></p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">India</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Portugal</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">0</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">0</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">0</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">0</strong></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.6 – Handling unknown values on one-hot encoding transformations</p>
			<p>As you can see, Portugal was not present in the training set (<em class="italic">Table 4.2</em>), so during the transformation, it will keep the same list of known countries and say that Portugal <em class="italic">is not</em> among them (<span class="No-Break">all zeros).</span></p>
			<p>As the very<a id="_idTextAnchor499"/> good skeptical data scientist you are becoming, should you be concerned about the fact that you have a particular category that has not been used during training? Well, maybe. This type of analysis really depends on your <span class="No-Break">problem domain.</span></p>
			<p>Handling unknown values is very common and something that you should expect to do in your ML pipeline. However, you should also ask yourself, due to the fact that you did not use that particular category during your training process, whether your model can be extrapolated <span class="No-Break">and generalized.</span></p>
			<p>Remember, your testing data must follow the same data distribution as your training data, and you are very likely to find all (or at least most) of the categories (of a categorical feature) either in the training or testing sets. Furthermore, if you are facing overfitting issues (doing well in the training, but poorly in the testing set) and, at the same time, you realize that your categorical encoders are transforming a lot of unknown values in the test set, guess what? It’s likely that your training and testing samples are not following the same distribution, invalidating your <span class="No-Break">model entirely.</span></p>
			<p>As you can see, slowly, you are getting there. You are learning about bias and investigation strategies in fine-grained detail – that is so exciting! Now, move on and look at performing transformations on numerical features. Yes, each type of data matters and drives <span class="No-Break">your decisions.</span></p>
			<h1 id="_idParaDest-95"><a id="_idTextAnchor500"/><a id="_idTextAnchor501"/>Dealing with numerical features</h1>
			<p>In terms of <a id="_idTextAnchor502"/>numerical features (discrete and continuous), you can think of transformations that rely on the training data and others that rely purely on the (individual) observation <span class="No-Break">being transformed.</span></p>
			<p>Those who rely on the training data will use the training set to learn the necessary parameters during <strong class="source-inline">fit</strong>, and then use them to transform any test or new data. The logic is pretty much the same as what you just learned for categorical features; however, this time, the encoder will learn <span class="No-Break">different parameters.</span></p>
			<p>On the other hand, those that rely purely on (individual) observations do not depend on training or testing sets. They will simply perform a mathematical computation on top of an individual value. For example, you could apply an exponential transformation to a particular variable by squaring its value. There is no dependency on learned parameters from anywhere – just get the value and <span class="No-Break">square it.</span></p>
			<p>At this point, you<a id="_idTextAnchor503"/> might be thinking about dozens of available transformations for numerical features! Indeed, there are so many options, and you will not learn all of them here. However, you are not supposed to know all of them for the AWS Machine Learning Specialty exam. You will learn the most important ones (for the exam), but you should not limit your modeling skills: take a moment to think about the unlimited options you have by creating custom transformations according to your <span class="No-Break">use case.</span></p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor504"/><a id="_idTextAnchor505"/>Data normalization</h2>
			<p>Applying data <strong class="bold">normalization</strong> means <a id="_idTextAnchor506"/>changing the scale of the data. For example, your feature may store employee salaries that range between 20,000 and 200,000 dollars/year and you want to put this data in the range of 0 and 1; where 20,000 (the minimum observed value) will be transformed as 0; and 200,000 (the maximum observed value) will be transformed <span class="No-Break">as 1.</span></p>
			<p>This type of technique is especially important when you want to fit your training data on top of certain types of algorithms that are impacted by the scale/magnitude of the underlying data. For instance, you can think about those algorithms that use the dot product of the<a id="_idTextAnchor507"/> input variables (such as neural networks or linear regression) and those algorithms that rely on<a id="_idTextAnchor508"/> distance measures (such as k-<strong class="bold">nearest neighbors (KNN)</strong> <span class="No-Break">or </span><span class="No-Break"><strong class="bold">k-means</strong></span><span class="No-Break">).</span></p>
			<p>On the other hand, applying data normalization will not result in performance improvements for rule-based algorithms, such as decision trees, since they will be able to check the predictive <a id="_idTextAnchor509"/>power of the<a id="_idTextAnchor510"/> features (either via entropy or information gain analysis), regardless of the scale of <span class="No-Break">the data.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">You will learn about these algorithms, along with the appropriate details, in the later chapters of this book. For instance, you can look at entropy and information gain as two types of metrics used by decision trees to check feature importance. Knowing the predictive power of each feature helps the algorithm define the optimal root, intermediaries, and leaf nodes of <span class="No-Break">the tree.</span></p>
			<p>Take a moment and use the following example to <a id="_idTextAnchor511"/>understand why data normalization will help those types of algorithms. You already know that the goal of a clustering algorithm is to find groups or clusters in your data, and one of the most used clustering algorithms is known <span class="No-Break">as k-means.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.2</em> shows how different scales of the variable could change the hyper plan’s projection of <span class="No-Break">k-means clustering:</span></p>
			<div>
				<div id="_idContainer035" class="IMG---Figure">
					<img src="image/B21197_04_02.jpg" alt="Figure 4.2 – Plotting data of different scales in a hyper plan" width="1650" height="529"/>
				</div>
			</div>
			<p class="IMG---Figure"><a id="_idTextAnchor512"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2 – Plotting data of different scales in a hyper plan</p>
			<p>On the left-hand side of <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.2</em>, you can see a single data point plotted in a hyper plan that has three dimensions (x, y, and z). All three dimensions (also known as features) were normalized to the scale of 0 and 1. On the right-hand side, you can see the same data point, but this time, the x dimension was <em class="italic">not</em> normalized. You can clearly see that the hyper plan <span class="No-Break">has changed.</span></p>
			<p>In a real scenario, you would have far more dimensions and data points. The difference in the scale of the data would change the centroids of each cluster and could potentially change the assigned clusters of some points. This same problem will happen on other algorithms that rely on distance calculation, such <span class="No-Break">as KNN.</span></p>
			<p>Other algorithms, such as neural networks and linear regression, will compute weighted sums using your input data. Usually, these types of algorithms will perform operations such as <em class="italic">W1*X1 + W2*X2 + Wi*Xi</em>, where <em class="italic">Xi</em> and <em class="italic">Wi</em> refer to a particular feature value and its weight, respectively. Again, you will learn details of neural networks and linear models later, but can you see the data scaling problem by just looking at the calculations that were just described? It can easily come up with very large values if <em class="italic">X</em> (feature) and <em class="italic">W</em> (weight) are large numbers. That will make the algorithm’s optimizations much more complex. In neural networks, this problem is known as <span class="No-Break">gradient exploding.</span></p>
			<p>You now have a v<a id="_idTextAnchor513"/>ery good understanding of the reasons you should apply data normalization (and when you should not). Data normalization is often implemented in ML <a id="_idTextAnchor514"/>libraries as <strong class="bold">Min Max Scaler</strong>. If you find this term in the exam, then remember that it is the same as <span class="No-Break">data normalization.</span></p>
			<p>Additionally, data normalization does not necessarily need to transform your feature into a range between 0 and 1. In reality, you can transform the feature into any range you want. <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.3</em> shows how normalization is <span class="No-Break">formally defined.</span></p>
			<div>
				<div id="_idContainer036" class="IMG---Figure">
					<img src="image/B21197_04_03.jpg" alt="Figure 4.3 – Normalization formula" width="1650" height="152"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor515"/>Figure 4.3 – Normalization formula</p>
			<p>Here, <em class="italic">Xmin</em> and <em class="italic">Xmax</em> are the lower and upper values of the range; <em class="italic">X</em> is the value of the feature. Apart from data normalization, there is another very important technique regarding numerical transformations that you <em class="italic">must</em> be aware of, not only for the exam but also for your data science career. You’ll look at this in the <span class="No-Break">next section.</span></p>
			<h2 id="_idParaDest-97">Da<a id="_idTextAnchor516"/><a id="_idTextAnchor517"/>ta standardization</h2>
			<p>Data standardization is an<a id="_idTextAnchor518"/>other scaling method that transforms the distribution of the data, so that the mean will become 0 and the standard deviation will become 1. <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.4</em> formally describes this scaling technique, where <em class="italic">X</em> represents the value to be transformed, <em class="italic">µ</em> refers to the mean of <em class="italic">X</em>, and <em class="italic">σ</em> is the standard deviation <span class="No-Break">of </span><span class="No-Break"><em class="italic">X</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer037" class="IMG---Figure">
					<img src="image/B21197_04_04.jpg" alt="Figure 4.4 – Standardization formula" width="1650" height="123"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">F<a id="_idTextAnchor519"/>igure 4.4 – Standardization formula</p>
			<p>Unlike normalization, data standardization will <em class="italic">not</em> result in a predefined range of values. Instead, it will transform your data into a sta<a id="_idTextAnchor520"/>ndard Gaussian distribution, where your transformed values will represent the number of standard deviations of each value to the mean of <span class="No-Break">the distribution.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The Gaussian distribution, also known as the normal distribution, is one of the most used distributions in statistical models. This is a continuous distribution with two main controlled parameters: <em class="italic">µ</em> (mean) and <em class="italic">σ</em> (standard deviation). Normal distributions are symmetric around the mean. In other words, most of the values will be close to the mean of <span class="No-Break">the distribution.</span></p>
			<p>Data standardization is often referred to as the<a id="_idTextAnchor521"/> zscore and is widely used to identify outliers on your variable, which you will see later in this chapter. For the sake of demonstration, <em class="italic">Table 4.7</em> simulates the data standardization of a small dataset. The input value is shown in the Age column, while the scaled value is shown in the <span class="No-Break">Zscore column:</span></p>
			<table id="table007" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold"><a id="_idTextAnchor522"/></strong><span class="No-Break"><strong class="bold">Age</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Mean</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Standard deviation</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Zscore</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>5</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">31,83</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">25,47</span></p>
						</td>
						<td class="No-Table-Style">
							<p>-<span class="No-Break">1,05</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">20</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">31,83</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">25,47</span></p>
						</td>
						<td class="No-Table-Style">
							<p>-<span class="No-Break">0,46</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">24</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">31,83</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">25,47</span></p>
						</td>
						<td class="No-Table-Style">
							<p>-<span class="No-Break">0,31</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">32</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">31,83</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">25,47</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0,01</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">30</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">31,83</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">25,47</span></p>
						</td>
						<td class="No-Table-Style">
							<p>-<span class="No-Break">0,07</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">80</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">31,83</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">25,47</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">1,89</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.7 – Data standardization in action</p>
			<p>Make sure you are confident when applying normalization and standardization by hand in the AWS Machine Learning Specialty exam. They might provide a list of values, as well as mean and standard deviation, and ask you for the scaled value of each element in <span class="No-Break">the list.</span></p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor523"/><a id="_idTextAnchor524"/>Applying binning and discretization</h2>
			<p><strong class="bold">Binning</strong> is a<a id="_idTextAnchor525"/> technique where you can group a set of values into a bucket or bin – for example, grouping people between 0 and 14 years old into a bucket named “children,” another group of people between 15 and 18 years old into a bucket named “teenager,” and <span class="No-Break">so on.</span></p>
			<p><strong class="bold">Discretization</strong> is the<a id="_idTextAnchor526"/> process of transforming a continuous variable into discrete or nominal attributes. These continuous values can be discretized by multiple<a id="_idTextAnchor527"/> strategies, such <a id="_idTextAnchor528"/>as <strong class="bold">equal-width</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="bold">equal-frequency</strong></span><span class="No-Break">.</span></p>
			<p>An equal-width strategy will split your data across multiple bins of the same width. Equal-frequency will split your data across multiple bins with the same number <span class="No-Break">of frequencies.</span></p>
			<p>Look at the following example. Suppose you have the following list containing 16 numbers: 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 90. As you can see, this list ranges between 10 and 90. Assuming you want to create four bins using an equal-width strategy, you could come up with the <span class="No-Break">following bins:</span></p>
			<ul>
				<li>Bin &gt;= 10 &lt;= 30 &gt; 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, <span class="No-Break">23, 24</span></li>
				<li>Bin &gt; 30 &lt;= <span class="No-Break">50 &gt;</span></li>
				<li>Bin &gt; 50 &lt;= <span class="No-Break">70 &gt;</span></li>
				<li>Bin &gt; 71 &lt;= 90 &gt; <span class="No-Break">90</span></li>
			</ul>
			<p>In this case, the <a id="_idTextAnchor529"/>width of each bin is the same (20 units), but the observations are not equally distributed. Now, the next example simulates an <span class="No-Break">equal-frequency strategy:</span></p>
			<ul>
				<li>Bin &gt;= 10 &lt;= 13 &gt; 10, 11, <span class="No-Break">12, 13</span></li>
				<li>Bin &gt; 13 &lt;= 17 &gt; 14, 15, <span class="No-Break">16, 17</span></li>
				<li>Bin &gt; 17 &lt;= 21 &gt; 18, 19, <span class="No-Break">20, 21</span></li>
				<li>Bin &gt; 21 &lt;= 90 &gt; 22, 23, <span class="No-Break">24, 90</span></li>
			</ul>
			<p>In this case, all the bins have the same frequency of observations, although they have been built with different bin widths to make <span class="No-Break">that possible.</span></p>
			<p>Once you have computed your bins, you should be wondering what’s next, right? Here, you have <span class="No-Break">some options:</span></p>
			<ul>
				<li>You can name your bins and use them as a nominal feature on your model! Of course, as a nominal variable, you should think about applying one-hot encoding before feeding an ML model with <span class="No-Break">this data.</span></li>
				<li>You might want to order your bins and use them as an <span class="No-Break">ordinal feature.</span></li>
				<li>Maybe you want to remove some noise from your feature by averaging the minimum and maximum values of each bin and using that value as your <span class="No-Break">transformed feature.</span></li>
			</ul>
			<p><a id="_idTextAnchor530"/>Take a look at <em class="italic">Table 4.8</em> to understand these approaches using our <span class="No-Break">equal-frequency example:</span></p>
			<table id="table008" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold"><a id="_idTextAnchor531"/>Ordinal </strong><span class="No-Break"><strong class="bold">value</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Bin</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Transforming to a </strong><span class="No-Break"><strong class="bold">nominal feature</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Transforming to an </strong><span class="No-Break"><strong class="bold">ordinal feature</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Removing noise</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">10</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>Bin &gt;= 10 &lt;= <span class="No-Break">13</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Bin A</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">11,5</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">11</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>Bin &gt;= 10 &lt;= <span class="No-Break">13</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Bin A</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">11,5</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">12</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>Bin &gt;= 10 &lt;= <span class="No-Break">13</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Bin A</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">11,5</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">13</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>Bin &gt;= 10 &lt;= <span class="No-Break">13</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Bin A</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">11,5</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">14</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>Bin &gt; 13 &lt;= <span class="No-Break">17</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Bin B</span></p>
						</td>
						<td class="No-Table-Style">
							<p>2</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">15,5</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">15</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>Bin &gt; 13 &lt;= <span class="No-Break">17</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Bin B</span></p>
						</td>
						<td class="No-Table-Style">
							<p>2</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">15,5</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">16</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>Bin &gt; 13 &lt;= <span class="No-Break">17</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Bin B</span></p>
						</td>
						<td class="No-Table-Style">
							<p>2</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">15,5</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">17</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>Bin &gt; 13 &lt;= <span class="No-Break">17</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Bin B</span></p>
						</td>
						<td class="No-Table-Style">
							<p>2</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">15,5</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">18</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>Bin &gt; 17 &lt;= <span class="No-Break">21</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Bin C</span></p>
						</td>
						<td class="No-Table-Style">
							<p>3</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">19,5</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">19</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>Bin &gt; 17 &lt;= <span class="No-Break">21</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Bin C</span></p>
						</td>
						<td class="No-Table-Style">
							<p>3</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">19,5</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">20</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>Bin &gt; 17 &lt;= <span class="No-Break">21</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Bin C</span></p>
						</td>
						<td class="No-Table-Style">
							<p>3</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">19,5</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">21</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>Bin &gt; 17 &lt;= <span class="No-Break">21</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Bin C</span></p>
						</td>
						<td class="No-Table-Style">
							<p>3</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">19,5</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">22</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>Bin &gt; 21 &lt;= <span class="No-Break">90</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Bin D</span></p>
						</td>
						<td class="No-Table-Style">
							<p>4</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">55,5</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">23</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>Bin &gt; 21 &lt;= <span class="No-Break">90</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Bin D</span></p>
						</td>
						<td class="No-Table-Style">
							<p>4</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">55,5</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">24</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>Bin &gt; 21 &lt;= <span class="No-Break">90</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Bin D</span></p>
						</td>
						<td class="No-Table-Style">
							<p>4</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">55,5</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">90</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>Bin &gt; 21 &lt;= <span class="No-Break">90</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Bin D</span></p>
						</td>
						<td class="No-Table-Style">
							<p>4</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">55,5</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.8 – Different approaches to working with bins and discretization</p>
			<p>Again, playing <a id="_idTextAnchor532"/>with different binning strategies will give you different results and you should analyze/test the best approach for your dataset. There is no standard answer here – it is all about <span class="No-Break">data exploration!</span></p>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor533"/><a id="_idTextAnchor534"/>Applying other types of numerical transformations</h2>
			<p>Normalization and <a id="_idTextAnchor535"/>standardization rely on your training data to fit their parameters: minimum and maximum values, in the case of normalization, and mean and standard deviation in the case of standard scaling. This also means you must fit those parameters using <em class="italic">only</em> your training data and never the <span class="No-Break">testing data.</span></p>
			<p>However, there are other types of numerical transformations that do not require parameters from training data to be applied. These types of transformations rely purely on mathematical <a id="_idTextAnchor536"/>computations. For example, one of these transformations is known as logarithmic transformation. This is a very common type of transformation<a id="_idTextAnchor537"/> in ML models and is especially beneficial for skewed features. If you don’t know what a skewed distribution is, take a look at <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">.</span></p>
			<div>
				<div id="_idContainer038" class="IMG---Figure">
					<img src="image/B21197_04_05.jpg" alt="Figure 4.5 – Skewed distributions" width="1650" height="736"/>
				</div>
			</div>
			<p class="IMG---Figure"><a id="_idTextAnchor538"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.5 – Skewed distributions</p>
			<p>In the middle, you h<a id="_idTextAnchor539"/>ave a normal distribution (or Gaussian distribution). On the left- and right-hand sides, you have skewed distributions. In terms of skewed features, there will be some values far away from the mean in one single direction (either left or right). Such behavior will push both the median and mean values of this distribution in the same direction, that of the long tail you can see in <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">.</span></p>
			<p>One very clear example of data that used to be skewed is the annual salaries of a particular group of professionals in a given region, such as senior data scientists working in Florida, US. This type of variable usually has most of its values close to the others (because people used to earn an average salary) and just has a few very high values (because a small group of people makes much more money <span class="No-Break">than others).</span></p>
			<p>Hopefully, you can now easily understand why the mean and median values will move to the tail direction, right? The big salaries will push them in <span class="No-Break">that direction.</span></p>
			<p>Alright, but why will a logarithmic transformation be beneficial for this type of feature? The answer to this question can be explained by the math <span class="No-Break">behind it:</span></p>
			<div>
				<div id="_idContainer039" class="IMG---Figure">
					<img src="image/B21197_04_06.jpg" alt="Figure 4.6 – Logarithmic properties" width="1650" height="74"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor540"/>Figure 4.6 – Logarithmic properties</p>
			<p>Computing the log of a n<a id="_idTextAnchor541"/>umber is the inverse of the exponential function. Log transformation will then reduce the scale of your number according to a given base (such as base 2, base 10, or base e, in the case of a natural logarithm). Looking at the salary distribution from the previous example, you would bring all those numbers down so that the higher the number, the higher the reduction; however, you would do this in a log scale and not in a linear fashion. Such behavior will remove the outliers of this distribution (making it closer to a normal distribution), which is beneficial for many ML algorithms, such as linear regression. <em class="italic">Table 4.9</em> shows you some of the differences when transforming a number in a linear scale versus a <span class="No-Break">log scale:</span></p>
			<table id="table009" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold"><a id="_idTextAnchor542"/></strong><span class="No-Break"><strong class="bold">Ordinal value</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Linear </strong><span class="No-Break"><strong class="bold">scale (normalization)</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Log scale (</strong><span class="No-Break"><strong class="bold">base 10)</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">10</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.0001</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">1,000</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.01</span></p>
						</td>
						<td class="No-Table-Style">
							<p>3</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">10,000</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.1</span></p>
						</td>
						<td class="No-Table-Style">
							<p>4</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">100,000</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p>5</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.9 – Differences between linear transformation and log transformation</p>
			<p>As you can see, the linear transformation kept the original magnitude of the data (you can still see outliers, but in another scale), while the log transformation removed those differences of magnitude and still kept the order of <span class="No-Break">the values.</span></p>
			<p>Would you be able to think about another type of mathematical transformation that follows the same behavior of <em class="italic">log</em> (making the distribution closer to Gaussian)? OK, here you have another: square root. Take the square root of those numbers shown in <em class="italic">Table 4.9</em> and <span class="No-Break">see yourself!</span></p>
			<p>Now, pay attention to this: both log and square root belong to a set of transformations known as power transformations, and there<a id="_idTextAnchor543"/> is a very popular method, which is likely to be mentioned on your AWS exam, that can perform a range of power transformations like those you have seen. This method was proposed by George Box and David Cox and its <a id="_idTextAnchor544"/>name <span class="No-Break">is </span><span class="No-Break"><strong class="bold">Box-Cox</strong></span><span class="No-Break">.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">During your exam, if you see questions about the Box-Cox transformation, remember that it is a method that can perform many power transformations (according to a lambda parameter), and its end goal is to make the original distribution closer to a <span class="No-Break">normal distribution.</span></p>
			<p>Just to conclude this discussion regarding why mathematical transformations can really make a difference to ML models, here <a id="_idTextAnchor545"/>is an example of <span class="No-Break"><strong class="bold">exponential transformations.</strong></span></p>
			<p>Suppose you have a set of data points, such as those on the left-hand side of <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.7</em>. Your goal is to draw a line that will perfectly split blue and red points. Just by looking at the original data (again, on the left-hand side), you know that your best guess for performing this linear task would be the one you can see in the same figure. However, the science (not magic) happens on the right-hand side of the figure! By squaring those numbers and plotting them in another hyper plan, you can perfectly separate each group <span class="No-Break">of points.</span></p>
			<p class="IMG---Figure"><a id="_idTextAnchor546"/></p>
			<div>
				<div id="_idContainer040" class="IMG---Figure">
					<img src="image/B21197_04_07.jpg" alt="Figure 4.7 – Exponential transformation in action" width="1650" height="695"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.7 – Exponential transformation in action</p>
			<p>You might be t<a id="_idTextAnchor547"/>hinking that there are infinite ways in which you can deal with your data. Although this is true, you should always take the business scenario you are working on into account and plan the work accordingly. Remember that model improvements or exploration is always possible, but you have to define your goals (remember the CRISP-DM methodology) and <span class="No-Break">move on.</span></p>
			<p>By the way, data transformation is important, but it is just one piece of your work as a data scientist. Your modeling journey still needs to move to other important topics, such as missing values and outliers handling. However, before that, you may have noticed that you were introduced to Gaussian distributions during this section, so why not go deeper <span class="No-Break">into it?</span></p>
			<h1 id="_idParaDest-100">U<a id="_idTextAnchor548"/><a id="_idTextAnchor549"/>nderstanding data distributions</h1>
			<p>Although the Gaussian distribution <a id="_idTextAnchor550"/>is probably the most common distribution for statistical and machine learning models, you should be aware that it is not the only one. There are other types of data distributions, such as the Bernoulli, binomial, and <span class="No-Break">Poisson distributions.</span></p>
			<p>The Bernoulli distribution <a id="_idTextAnchor551"/>is a very simple one, as there are only two types of possible events: success or failure. The success event has a probability <em class="italic">p</em> of happening, while the failure one has a probability <span class="No-Break">of </span><span class="No-Break"><em class="italic">1-p</em></span><span class="No-Break">.</span></p>
			<p>Some examples that follow a Bernoulli distribution are rolling a six-sided die or flipping a coin. In both cases, you must define the event of success and the event of failure. For example, assume the following success and failure events when rolling <span class="No-Break">a die:</span></p>
			<ul>
				<li>Success: Getting a <span class="No-Break">number 6</span></li>
				<li>Failure: Getting any <span class="No-Break">other number</span></li>
			</ul>
			<p>You can then say that there is a <em class="italic">p</em> probability of success (1/6 = 0.16 = 16%) and a <em class="italic">1-p</em> probability of failure (1 - 0.16 = 0.84 = <span class="No-Break">84%).</span></p>
			<p>The binomial distribution <a id="_idTextAnchor552"/>generalizes the Bernoulli distribution. The Bernoulli distribution has only one repetition of an event, while the binomial distribution allows the event to be repeated many times, and you must count the number of successes. Continue with the prior example, that is, counting the number of times you got a 6 out of our 10 dice rolls. Due to the nature of this example, binomial distribution has two parameters, <em class="italic">n</em> and <em class="italic">p</em>, where <em class="italic">n</em> is the number of repetitions and <em class="italic">p</em> is the probability of success in <span class="No-Break">every repetition.</span></p>
			<p>Finally, a Poisson d<a id="_idTextAnchor553"/>istribution <a id="_idTextAnchor554"/>allows you to find a number of events in a period of time, given the number of times an event occurs in an interval. It has three parameters: lambda, <em class="italic">e</em>, and <em class="italic">k</em>, where lambda is the average number of events per interval, <em class="italic">e</em> is the Euler number, and <em class="italic">k</em> is the number of times an event occurs in <span class="No-Break">an interval.</span></p>
			<p>With all those distributions, including the Gaussian one, it is possible to compute the expected mean value and variance based on their parameters. This information is usually used in hypothesis tests to check whether some sample data follows a given distribution, by comparing the mean and variance <strong class="bold">of the sample</strong> against the <strong class="bold">expected </strong>mean and variance of <span class="No-Break">the distribution.</span></p>
			<p>You are now more familiar with data distributions, not only Gaussian distributions. You will keep learning about data distributions throughout this book. For now, it is time to move on to missing values and <span class="No-Break">outlier detection.</span></p>
			<h1 id="_idParaDest-101">H<a id="_idTextAnchor555"/><a id="_idTextAnchor556"/>andling missing values</h1>
			<p>As the name suggests, missing values r<a id="_idTextAnchor557"/>efer to the absence of data. Such absences are usually represented by tokens, which may or may not be implemented in a <span class="No-Break">standard way.</span></p>
			<p>Although using tokens is standard, the way those tokens are displayed may vary across different platforms. For example, relational databases represent missing data with <em class="italic">NULL</em>, core Python <a id="_idTextAnchor558"/>code will use <em class="italic">None</em>, and some Python libraries will represent missing numbers as <strong class="bold">Not a </strong><span class="No-Break"><strong class="bold">Number (NaN)</strong></span><span class="No-Break">.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">For numerical fields, don’t replace those standard missing tokens with <em class="italic">zeros</em>. By default, zero is not a missing value, but <span class="No-Break">another number.</span></p>
			<p>However, in real business scenarios, you may or may not find those standard tokens. For example, a software engineering team might have designed the system to automatically fill missing data with specific tokens, such as “unknown” for strings or “-1” for numbers. In that case, you would have to search by those two tokens to find missing data. People can <span class="No-Break">set anything.</span></p>
			<p>In the previous example, the software engineering team was still kind enough to give you standard tokens. However, there are many cases where legacy systems do not add any data quality layer in front of the user, and you may find an address field filled with, “I don’t want to share,” or a phone number field filled with, “Don’t call me.” This is clearly missing data, but not as standard as the <span class="No-Break">previous example.</span></p>
			<p>There are many more nuances that you will learn regarding missing data, all of which you will learn in this section, but be advised: before you start making decisions about missing values, you should prepare a good data exploration and make sure you find those values. You can either compute data frequencies or use missing plots, but please do something. Never assume that your missing data is represented only by those handy <span class="No-Break">standard tokens.</span></p>
			<p>Why should you care about this type of data? Well, first, because most algorithms (apart from decision trees implemented on very specific ML libraries) will raise errors when they find a missing value. Second (and maybe most important), by grouping all the missing data in the same bucket, you are assuming that they are all the same, but in reality, you don’t <span class="No-Break">know that.</span></p>
			<p>Such a decision will <a id="_idTextAnchor559"/>not only add bias to your model – it will reduce its interpretability, as you will be unable to explain the missing data. Once you know why you want to treat the missing values, then you can analyze <span class="No-Break">your options.</span></p>
			<p>Theoretically, you can classify missing values into two main groups: <strong class="bold">MCAR </strong>or <strong class="bold">MNAR</strong>. MCAR stands for <strong class="bold">Missing Completely at Random</strong> and states that there is no pattern associated with the missing data. On the other hand, MNAR <a id="_idTextAnchor560"/>stands for <strong class="bold">Missing Not at Random</strong> and means that the underlying process used to generate the data is strictly connected to the <span class="No-Break">missing values.</span></p>
			<p>Look at the following example about MNAR missing values. Suppose you are collecting user feedback about a particular product in an online survey. Your process of asking questions is dynamic and depends on user answers. When a user specifies an age lower than 18 years old, you never ask his/her marital status. In this case, missing values of marital status are connected to the age of the <span class="No-Break">user (MNAR).</span></p>
			<p>Knowing the class of missing values that you are dealing with will help you understand whether you have any control over the underlying process that generates the data. Sometimes, you can come back to the source process and, somehow, complete your <span class="No-Break">missing data.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Although, in real scenarios, you usually need to treat missing data via exclusion or imputation, never forget that you can always try to look at the source process and check if you can retrieve (or, at least, better understand) the missing data. You may face this option in <span class="No-Break">the exam.</span></p>
			<p>If you don’t have an opportunity to <a id="_idTextAnchor561"/>recover your missing data from anywhere, then you should move <a id="_idTextAnchor562"/>on to other approaches, such as<strong class="bold"> listwise deletion</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="bold">imputation</strong></span><span class="No-Break">.</span></p>
			<p>Listwise deletion r<a id="_idTextAnchor563"/>efers to the process of discarding some data, which is the downside of this choice. This may happen at the row level or the column level. For example, suppose you have a DataFrame containing four columns and one of them has 90% of its data missing. In such cases, what usually makes more sense is dropping the entire feature (column), since you don’t have that information for the majority of your <span class="No-Break">observations (rows).</span></p>
			<p>From a row p<a id="_idTextAnchor564"/>erspective, you may have a DataFrame with a small number of observations (rows) containing missing data in one of its features (columns). In such scenarios, instead of removing the entire feature, what makes more sense is removing only those <span class="No-Break">few observations.</span></p>
			<p>The benefit of using this method is the simplicity of dropping a row or a column. Again, the downside is losing information. If you don’t want to lose information while handling your missing data, then you should go for an <span class="No-Break">imputation strategy.</span></p>
			<p>Imputation i<a id="_idTextAnchor565"/>s also known as replacement, where you will replace missing values by substituting a value. The most common approach to imputation is replacing the missing value with the mean of the feature. Please take note of this approach because it is likely to appear in <span class="No-Break">your exam:</span></p>
			<table id="table010" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold"><a id="_idTextAnchor566"/></strong><span class="No-Break"><strong class="bold">Age</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">35</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">30</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">25</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">80</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">75</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.10 – Replacing missing values with the mean or median</p>
			<p><em class="italic">Table 4.10</em> shows a very simple dataset with one single feature and five observations, where the third observation has a missing value. If you decide to replace that missing data with the mean value of the feature, you will come up with 49. Sometimes, when there are outliers in the data, the median might be more appropriate (in this case, the median would <span class="No-Break">be 35):</span></p>
			<table id="table011" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Age</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Job status</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">35</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Employee</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">30</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Employee</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><span class="No-Break">Retired</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">25</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Employee</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">80</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Retired</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">75</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Retired</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.11 – Replacing missing values with the mean or median of the group</p>
			<p>If you want to go deeper, you could find the mean or median value according to a given group of features. For example, <em class="italic">Table 4.11</em> expanded the previous dataset from <em class="italic">Table 4.10</em> by adding the <em class="italic">Job status</em> column. Now, there is some evidence that the initial approach of changing the missing value by using the overall median (35 years old) was likely to be wrong (since that person <span class="No-Break">is retired).</span></p>
			<p>What you can do now is replace the missing value with the mean or median of the other observations that belong to the same job status. Using this new approach, you can change the missing information to 77.5. Considering that the person is retired, 77.5 makes more sense than 35 <span class="No-Break">years old.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">In the case of categorical variables, you can replace the missing data with the value that has the highest occurrence in your dataset. The same logic of grouping the dataset according to specific features is <span class="No-Break">still applicable.</span></p>
			<p>You can also use more sophisticated <a id="_idTextAnchor567"/>methods of imputation, including constructing an ML model to predict the value of your missing data. The downside of these imputation approaches (either by averaging or predicting the value) is that you are making inferences about the data that are not necessarily right and will add bias to <span class="No-Break">the dataset.</span></p>
			<p>To sum this up, the trade-off while dealing with missing data is having a balance between losing data or adding bias to the dataset. Unfortunately, there is no scientific recipe that you can follow, whatever your problem is. To decide on what you are going to do, you must look at your success criteria, explore your data, run experiments, and then make <span class="No-Break">your decisions.</span></p>
			<p>You will now move to another headache for many ML <span class="No-Break">algorithms: outliers.</span></p>
			<h1 id="_idParaDest-102"><a id="_idTextAnchor568"/><a id="_idTextAnchor569"/>Dealing with outliers</h1>
			<p>You are not on this studying journey just to pass the AWS Machine Learning Specialty exam but also to become a better data scientist. There are many different ways to look at the outlier problem purely from a mathematical perspective; however, the datasets used in real life are derived from the underlying business process, so you must include a business perspective during an <span class="No-Break">outlier analysis.</span></p>
			<p>An outlier is an <a id="_idTextAnchor570"/>atypical data point in a set of data. For example, <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.8</em> shows some data points that have been plotted in a two-dimension plan; that is, x and y. The red point is an outlier since it is an atypical value in this series <span class="No-Break">of data.</span></p>
			<div>
				<div id="_idContainer041" class="IMG---Figure">
					<img src="image/B21197_04_08.jpg" alt="Figure 4.8 – Identifying an outlier" width="1650" height="473"/>
				</div>
			</div>
			<p class="IMG---Figure"><a id="_idTextAnchor571"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.8 – Identifying an outlier</p>
			<p>It is important to treat outlier values because some statistical methods are impacted by them. Still, in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.8</em>, you can see this behavior in action. On the left-hand side, there has been drawn a line that best fits those data points, ignoring the red point. On the right-hand side, the same line was drawn, but including the <span class="No-Break">red point.</span></p>
			<p>You can visually conclude that, by ignoring the outlier point, you will come up with a better solution on the plan of the left-hand side of the preceding chart since it was able to pass closer to most of the values. You can also prove this by computing an associated error for each line (which you will learn later in <span class="No-Break">this book).</span></p>
			<p>It is worth reminding that you have also seen the outlier issue in action in another situation in this book: specifically, in <em class="italic">Table 4.10</em>, while dealing with missing values. In that example, the median was used to work around the problem. Feel free to go back and read it again, but what should be very clear at this point is that median values are less impacted by outliers than average (<span class="No-Break">mean) values.</span></p>
			<p>You now know w<a id="_idTextAnchor572"/>hat outliers are and why you should treat them. You should always consider your business perspective while dealing with outliers, but there are mathematical methods to find them. Now, you are ready to move on and look at some methods for <span class="No-Break">outlier detection.</span></p>
			<p>You have already learned about the most common method: zscore. In <em class="italic">Table 4.7</em>, you saw a table containing a set of ages. Refer to it again to refresh your memory. In the last column of that table, it was computed the z<a id="_idTextAnchor573"/>score of each age, according to the equation shown in <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">.</span></p>
			<p>There is no well-defined range for those zscore values; however, in a normal distribution <em class="italic">without</em> outliers, they will mostly range between -3 and 3. Remember: zscore will give you the number of standard deviations from the mean of the distribution. <em class="italic">Table 4.10</em> shows some of the properties of a <span class="No-Break">normal distribution:</span></p>
			<div>
				<div id="_idContainer042" class="IMG---Figure">
					<img src="image/B21197_04_09.jpg" alt="Figure 4.9 – Normal distribution properties. Image adapted from  https://pt.wikipedia.org/wiki/Ficheiro:The_Normal_Distribution.svg" width="717" height="413"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor574"/>Figure 4.9 – Normal distribution properties. Image adapted from https://pt.wikipedia.org/wiki/Ficheiro:The_Normal_Distribution.svg</p>
			<p>According to the normal distribution properties, 95% of values will belong to the range of -2 and 2 standard deviations from the mean, while 99% of the values will belong to the range of -3 and 3. Coming back to the outlier detection context, you can set thresholds on top of those z<a id="_idTextAnchor575"/>score values to specify whether a data point is an outlier <span class="No-Break">or not!</span></p>
			<p>There is no st<a id="_idTextAnchor576"/>andard threshold that you can use to classify outliers. Ideally, you should look at your data and see what makes more sense for you… usually (this is not a rule), you will use some number between 2 and 3 standard deviations from the mean to flag outliers, since less than 5% of your data will be selected by this rule (again, this is just a reference threshold, so that you can select some data from further scructizing). You may remember that there are outliers <em class="italic">below</em> and <em class="italic">above</em> the mean value of the distribution, as shown in <em class="italic">Table 4.12</em>, where the outliers were flagged with an <strong class="bold">absolute </strong>zscore greater than 3 (the value column is hidden for the sake of <span class="No-Break">this demonstration).</span></p>
			<table id="table012" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Value</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Zscore</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Is outlier?</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>...</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">1.3</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">NO</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>...</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.8</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">NO</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>...</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">3.1</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">YES</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>...</p>
						</td>
						<td class="No-Table-Style">
							<p>-<span class="No-Break">2.9</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">NO</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>...</p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">-</strong><span class="No-Break"><strong class="bold">3.5</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">YES</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>...</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">1.0</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">NO</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>...</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">1.1</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">NO</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.12 – Flagging outliers according to the zscore value</p>
			<p>Two <a id="_idTextAnchor577"/>outliers were found in <em class="italic">Table 4.12</em>: row number three and row number five. Another way to find outliers in the data is by applying the box plot logic. When you look at a numerical variable, it is possible to extract many descriptive statistics from it, not only the mean, median, minimum, and maximum values, as you have seen previously. Another property that’s present in data distributions is known <span class="No-Break">as quantiles.</span></p>
			<p>Quantiles are <a id="_idTextAnchor578"/>cut-off points that are established at regular intervals from the cumulative distribution function of a random variable. Those regular intervals, also known as <em class="italic">q-quantiles</em>, will be nearly the same size and will receive special names in <span class="No-Break">some situations:</span></p>
			<ul>
				<li>The 4-quantiles are <span class="No-Break">called quartiles.</span></li>
				<li>The 10-quantiles are <span class="No-Break">called deciles.</span></li>
				<li>The 100-quantiles are <span class="No-Break">called percentiles.</span></li>
			</ul>
			<p>For example, the 20th percentile (of a 100-quantile regular interval) specifies that 20% of the data is below that point. In a box plot, you can use regular intervals of 4-quantiles (also known as <em class="italic">quartiles</em>) to expose the distribution of the data (Q1 and Q3), as shown in <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.10.</em></span></p>
			<div>
				<div id="_idContainer043" class="IMG---Figure">
					<img src="image/B21197_04_10.jpg" alt="Figure 4.10 – Box plot definition" width="1650" height="670"/>
				</div>
			</div>
			<p class="IMG---Figure"><a id="_idTextAnchor579"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.10 – Box plot definition</p>
			<p>Q1 is also known as the lower quartile or 25th quartile, and this means that 25% of the data is below that point in the distribution. Q3 is also known as the upper quartile or 75th quartile, and this means that 75% of the data is below that point in <span class="No-Break">the distribution.</span></p>
			<p>Computing the d<a id="_idTextAnchor580"/>ifference between Q1 and Q3 will give you the<strong class="bold"> interquartile range (IQR)</strong> value, which you can then use to compute the limits of the box plot, shown by the “minimum” and “maximum” labels in the <span class="No-Break">preceding diagram.</span></p>
			<p>After all, you can finally infer that anything below the “minimum” value or above the “maximum” value of the box plot will be flagged as <span class="No-Break">an outlier.</span></p>
			<p>You have now learned about two different ways you can flag outliers on your data: zscore and box plot. You can decide whether you are going to remove these points from your dataset, transform them, or create another variable to specify that they exist (as shown in <span class="No-Break"><em class="italic">Table 4.11</em></span><span class="No-Break">).</span></p>
			<p>Moving further on this journey of data preparation and transformation, you will face other types of problems in real life. Next, you will learn that several use cases contain something known as <strong class="bold">rare events</strong>, which makes <a id="_idTextAnchor581"/>ML algorithms focus on the wrong side of the problem and propose bad solutions. Luckily, you will learn how to either tune hyperparameters or prepare the data to facilitate algorithm convergence while fitting <span class="No-Break">rare events.</span></p>
			<h1 id="_idParaDest-103">D<a id="_idTextAnchor582"/><a id="_idTextAnchor583"/>ealing with unbalanced datasets</h1>
			<p>At this point, you <a id="_idTextAnchor584"/>might have realized why data preparation is probably the longest part of the data scientist’s work. You have learned about data transformation, missing data values, and outliers, but the list of problems goes on. Don’t worry – you are on the right journey to master <span class="No-Break">this topic!</span></p>
			<p>Another well-known problem with ML models, specifically with classification problems, is unbalanced classes. In a classification model, you can say that a dataset is unbalanced when most of its observations belong to one (or some) of the classes (<span class="No-Break">target variable).</span></p>
			<p>This is very common in fraud identification systems: for example, where most of the events belong to a regular operation, while a very small number of events belong to a fraudulent operation. In this case, you can also say that fraud is a <span class="No-Break">rare event.</span></p>
			<p>There is no strong rule for defining whether a dataset is unbalanced or not, it really depends on the context of your business domain. Most challenge problems will contain more than 99% of the observations in the <span class="No-Break">majority class.</span></p>
			<p>The problem with unbalanced datasets is very simple: ML algorithms will try to find the best fit in the training data to maximize their accuracy. In a dataset where 99% of the cases belong to one single class, without any tuning, the algorithm is likely to prioritize the assertiveness of the majority class. In the worst-case scenario, it will classify all the observations as the majority class and ignore the minority one, which is usually our interest when modeling <span class="No-Break">rare events.</span></p>
			<p>To deal with unbalanced datasets, you have two major directions to follow: tuning the algorithm to handle the issue or resampling the data to make it <span class="No-Break">more balanced.</span></p>
			<p>By tuning the algorithm, you have to specify the weight of each class under classification. This class weight configuration belongs to the algorithm, not to the training data, so it is a hyperparameter setting. It is important to keep in mind that not all algorithms will have that type of configuration, and that not all ML frameworks will expose it, either. As a quick reference, the <strong class="source-inline">DecisionTreeClassifier</strong> class, from the scikit-learn ML library, is a good example that does implement the class <span class="No-Break">weight hyperparameter.</span></p>
			<p>Another way to work a<a id="_idTextAnchor585"/>round unbalanced problems is changing the training dataset by applying <strong class="bold">undersampling</strong> or <strong class="bold">oversampling</strong>. If you decide to apply undersampling, all you <a id="_idTextAnchor586"/>have to do is remove s<a id="_idTextAnchor587"/>ome observations from the majority class until you get a more balanced dataset. Of course, the downside of this approach is that you may lose important information about the majority class that you are removing <span class="No-Break">observations from.</span></p>
			<p>The most common approach for undersampling is known as random undersampling, which is a naïve resampling approach where you randomly remove some observations from the <span class="No-Break">training set.</span></p>
			<p>On the other hand, you can decide to go for oversampling, where you will create new observations/samples of the minority class. The simplest approach is the naïve one, where you randomly select observations from the training set (with replacement) for duplication. The downside of this method is the potential issue of overfitting, since you will be duplicating/highlighting the observed pattern of the <span class="No-Break">minority class.</span></p>
			<p>To either underfit or overfit your model, you should always test the fitted model on your <span class="No-Break">testing set.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The testing set cannot be under/oversampled: only the training set should pass through these <span class="No-Break">resampling techniques.</span></p>
			<p>You can also oversample the training set by applying synthetic sampling techniques. Random oversample does not add any new information to the training set: it just duplicates the existing ones. By creating synthetic samples, you are deriving those new observations from the existing ones (instead of simply copying them). This is a type of data augmentation technique <a id="_idTextAnchor588"/>known as the <strong class="bold">Synthetic Minority Oversampling </strong><span class="No-Break"><strong class="bold">Technique (SMOTE).</strong></span></p>
			<p>Technically, what SMOTE does is plot a line in the feature space of the minority class and extract points that are close to <span class="No-Break">that line.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">You may find questions in your exam where the term SMOTE has been used. If that happens, keep in mind the context where this term is <span class="No-Break">applied: oversampling.</span></p>
			<p>Alright – in the next section, you will learn how to prepare text data for <span class="No-Break">ML models.</span></p>
			<h1 id="_idParaDest-104">D<a id="_idTextAnchor589"/><a id="_idTextAnchor590"/>ealing with text data</h1>
			<p>You have already learned <a id="_idTextAnchor591"/>how to transform categorical features into numerical representations, either using label encoders, ordinal encoders, or one-hot encoding. However, what if you have fields containing long pieces of text in your dataset? How are you s<a id="_idTextAnchor592"/>upposed to provide a mathematical representation for them in order to properly feed ML algorithms? This is a common issue in <strong class="bold">Natural Language Processing (NLP),</strong> a subfield <span class="No-Break">of AI.</span></p>
			<p>NLP models aim to extract knowledge from texts; for example, translating text between languages, identifying entities in a corpus of text (also known as <strong class="bold">Name Entity Recognition</strong>, or <strong class="bold">NER</strong> for short), classifying s<a id="_idTextAnchor593"/>entiments from a user review, and many <span class="No-Break">other applications.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">In <a href="B21197_08.xhtml#_idTextAnchor1058"><span class="No-Break"><em class="italic">Chapter 8</em></span></a><em class="italic">, AWS Application Services for AI/ML</em>, you will learn about some AWS application services that apply NLP to their solutions, such as Amazon Translate and Amazon Comprehend. During the exam, you might be asked to think about the fastest or easiest way (with the least development effort) to build certain types of NLP applications. The fastest or easiest way is usually to use those out-of-the-box AWS services, since they offer pre-trained models for some use cases (especially machine translation, sentiment analysis, topic modeling, document classification, and <span class="No-Break">entity recognition).</span></p>
			<p class="callout">In a few chapters’ time, you will also l<a id="_idTextAnchor594"/>earn about some built-in AWS algorithms for NLP applications, such as BlazingText, <strong class="bold">Latent Dirichlet Allocation </strong>(<strong class="bold">LDA</strong>), <strong class="bold">Neural Topic Modeling </strong>(<strong class="bold">NTM</strong>), and the <strong class="bold">Sequence-to-Sequence</strong> algorithm. Those algorithms a<a id="_idTextAnchor595"/>lso let you create the same NLP solutions that are created by those out-of-the-box services; however, you have to use them on SageMaker and write your own solution. In other words, they offer more flexibility but demand more <span class="No-Break">development effort.</span></p>
			<p class="callout">Keep that in mind for <span class="No-Break">your exam!</span></p>
			<p>Although AWS o<a id="_idTextAnchor596"/>ffers many out-of-the-box services and built-in algorithms that allow you to create NLP applications, you will not look at those AWS product features now (you will do in <a href="B21197_06.xhtml#_idTextAnchor708"><span class="No-Break"><em class="italic">Chapter 6</em></span></a><em class="italic">, Applying Machine Learning Algorithms</em>, and <a href="B21197_08.xhtml#_idTextAnchor1058"><span class="No-Break"><em class="italic">Chapter 8</em></span></a><em class="italic">, AWS Application Services for AI/ML</em>). You will finish this chapter by looking at some data preparation techniques that are extremely important for preparing your data <span class="No-Break">for NLP.</span></p>
			<h2 id="_idParaDest-105">B<a id="_idTextAnchor597"/><a id="_idTextAnchor598"/>ag of words</h2>
			<p>The first one you will learn is k<a id="_idTextAnchor599"/>nown as bag of words (BoW). This is a very common and simple technique, applied to text data, that creates matrix representations to describe the number of words within the text. BoW consists of two main steps: creating a vocabulary and creating a representation of the presence of those known words from the vocabulary in the text. These steps can be seen in <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.11.</em></span></p>
			<div>
				<div id="_idContainer044" class="IMG---Figure">
					<img src="image/B21197_04_11.jpg" alt="Figure 4.11 – BoW in action" width="1650" height="880"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor600"/>Figure 4.11 – BoW in action</p>
			<p>First things first, you us<a id="_idTextAnchor601"/>ually can’t use raw text to prepare a BoW representation. There is a data cleansing step where you lowercase the text; split each work into tokens; remove punctuation, non-alphabetical, and stop words; and, whenever necessary, apply any other custom cleansing techniques you <span class="No-Break">may want.</span></p>
			<p>Once you have cleansed your raw text, you can add each word to a global vocabulary. Technically, this is usually a dictionary of tuples, in the form {(word, number of occurrences)} – for example, {(apple, 10), (watermelon, 20)}. As I mentioned previously, this is a global dictionary, and you should consider all the texts you <span class="No-Break">are analyzing.</span></p>
			<p>Now, with the cleansed text and updated vocabulary, you can build your text representation in the form of a matrix, where each column represents one word from the global vocabulary and each row represents a text you have analyzed. The way you represent those texts on each row may vary according to different strategies, such as binary, frequency, and count. Next, you will learn these strategies a <span class="No-Break">little more.</span></p>
			<p>In <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.11</em>, a single piece of text is being processed with the three different strategies for BoW. That’s why you can see three rows on that table, instead of just one (in a real scenario, you have to choose one of them <span class="No-Break">for implementation).</span></p>
			<p>In the first row, it was used a binary strategy, which will assign 1 if the word exists in the global vocabulary and 0 if not. Because the vocabulary was built on a single text, all the words from that text belong to the vocabulary (the reason you can only see 1s in the <span class="No-Break">binary strategy).</span></p>
			<p>In the second row, it was used a f<a id="_idTextAnchor602"/>requency strategy, which will check the number of occurrences of each word within the text and divide it by the total number of words within the text. For example, the word “this” appears just once (1) and there are seven other words in the text (7), so 1/7 is equal <span class="No-Break">to 0.14.</span></p>
			<p>Finally, in the third row, it was used a count strategy, which is a simple count of occurrences of each word within <span class="No-Break">the text.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">This note is really important – you are likely to find it in your exam. You may have noticed that the BoW matrix co<a id="_idTextAnchor603"/>ntains unique words in the <em class="italic">columns</em> and each text representation is in the <em class="italic">rows</em>. If you have 100 long pieces of text with only 50 unique words across them, your BoW matrix will have 50 columns and 100 rows. During your exam, you are likely to receive a list of pieces of text and be asked to prepare the <span class="No-Break">BoW matrix.</span></p>
			<p>There is one more extremely important co<a id="_idTextAnchor604"/>ncept you should know about BoW, which is the n-gram configuration. The term n-gram is used to describe the way you would like to look at your vocabulary, either via single words (uni-gram), groups of two words (bi-gram), groups of three words (tri-gram), or even groups of <em class="italic">n</em> words (n-gram). So far, you have seen BoW representations using a uni-gram approach, but more sophisticated representations of BoW may use bi-grams, tri-grams, <span class="No-Break">or n-grams.</span></p>
			<p>The main logic itself does not change, but you need to know how to represent n-grams in BoW. Still using the example from <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.11</em>, a bi-gram approach would combine those words in the following way: [this movie, movie really, really good, good although, although old, old production]. Make sure you understand this before taking <span class="No-Break">the exam.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The power and simplicity of BoW come from the fact that you can easily come up with a training set to train your algorithms. If you look at <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.11</em>, can you see that having more data and just adding a classification column to that table, such as good or bad review, would allow us to train a binary classification model to <span class="No-Break">predict sentiments?</span></p>
			<p>Alright – you might have noticed that many of the awesome techniques that you have been introduced to come with some downsides. The problem with BoW is the challenge of maintaining its vocabulary. You can easily see that, in a huge corpus of texts, the vocabulary size tends to become bigger and bigger and the matrices’ representations tend to be sparse (yes – the sparsity <span class="No-Break">issue again).</span></p>
			<p>One possible way to solve the vocabulary size i<a id="_idTextAnchor605"/>ssue is by using word hashing (also known in ML as the <strong class="bold">hashing</strong> <strong class="bold">trick</strong>). Hash functions map data of arbitrary sizes to data of a fixed size. This means you can use the hash trick to represent each text with a fixed number of features (regardless of the vocabulary’s size). Technically, this hashing space allows collisions (different texts represented by the same features), so this is something to take into account when you are implementing <span class="No-Break">feature hashing.</span></p>
			<h2 id="_idParaDest-106">TF<a id="_idTextAnchor606"/><a id="_idTextAnchor607"/>-IDF</h2>
			<p>Another problem that comes with BoW, especially when you use the frequency strategy to build the feature space, is that more frequent words will strongly boost their scores due to the high number of occurrences within the document. It turns out that, often, those words with high occurrences are not the key words of the document, but just other words that <em class="italic">also</em> appear many times in several <span class="No-Break">other documents.</span></p>
			<p><strong class="bold">Term Frequency-Inverse Document Frequency (TF-IDF)</strong> helps pe<a id="_idTextAnchor608"/>nalize these types of words, by checking how frequent they are in other documents and using that information to rescale the frequency of the words within <span class="No-Break">the document.</span></p>
			<p>At the end of the process, TF-IDF t<a id="_idTextAnchor609"/>ends to give more importance to words that are unique to the document (document-specific words). Next, let’s look at a concrete example so that you can understand it <span class="No-Break">in depth.</span></p>
			<p>Consider that you have a text corpus containing 100 words and the word “Amazon” appears three times. The Term Frequency (TF) of th<a id="_idTextAnchor610"/>is word would be 3/100, which is equal to 0.03. Now, suppose you have other 1,000 documents and that the word “Amazon” appears in 50 of these. In this case, the Inverse Document Frequency (IDF) would be<a id="_idTextAnchor611"/> given by the log as 1,000/50, which is equal to 1.30. The TF-IDF score of the word “Amazon,” in that specific document under analysis, will be the product of TF * IDF, which is 0.03 * <span class="No-Break">1.30 (</span><span class="No-Break"><em class="italic">0.039</em></span><span class="No-Break">).</span></p>
			<p>Suppose that instead of 50 documents, the word “Amazon” had also appeared on another 750 documents – in other words, much more frequently than in the prior scenario. In this case, the TF part of this equation will not change – it is still 0.03. However, the IDF piece will change a little, since this time it will be log 1,000/750, which is equal to <em class="italic">0.0036</em>. As you can see, now the word “Amazon” has much less importance than in the <span class="No-Break">previous example.</span></p>
			<h2 id="_idParaDest-107">Wo<a id="_idTextAnchor612"/><a id="_idTextAnchor613"/>rd embedding</h2>
			<p>Unlike traditional approaches, such as BoW and TD-IDF, modern methods of text representation will take care of the context of the information, as well as the presence or frequency of words. One very popular and powerful approach that follows this concept is known as <strong class="bold">word embedding.</strong> Word embeddings c<a id="_idTextAnchor614"/>reate a dense vector of a fixed length that can store information about the context and meaning of <span class="No-Break">the document.</span></p>
			<p>Each word is represented by a data point in a multidimensional hyper plan, which is known as embedding space. This embedding space will have <em class="italic">n</em> dimensions, where each of these dimensions refers to a particular position of this <span class="No-Break">dense vector.</span></p>
			<p>Although it may sound confusing, the concept is actually pretty simple. Suppose you have a list of four words, and you want to plot them in an embedding space of five dimensions. The words are king, queen, live, and castle. <em class="italic">Table 4.13</em> shows how to <span class="No-Break">do this.</span></p>
			<table id="table013" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Dim 1</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Dim 2</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Dim 3</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Dim 4</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Dim 5</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">King</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.22</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.76</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.77</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.44</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.33</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Queen</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.98</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.09</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.67</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.89</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.56</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Live</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.13</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.99</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.88</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.01</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.55</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Castle</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.01</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.89</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.34</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.02</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.90</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.13 – An embedding space representation</p>
			<p>Forget the hypothetical <a id="_idTextAnchor615"/>numbers in <em class="italic">Table 4.13</em> and focus on the data structure; you will see that each word is now represented by <em class="italic">n</em> dimensions in the embedding space. This process of transforming words into vectors can be performed by many different methods, but the<a id="_idTextAnchor616"/> most popular<a id="_idTextAnchor617"/> ones are word2vec <span class="No-Break">and GloVe.</span></p>
			<p>Once you have each word represented as a vector of a fixed length, you can apply many other techniques to do whatever you need. One very common task is plotting those “words” (actually, their dimensions) in a hyper plan and visually checking how close they are to <span class="No-Break">each other!</span></p>
			<p>Technically, you don’t use this to plot them as-is, since human brains cannot interpret more than three dimensions. Furthermore, you usually apply a dimensionality reduction technique (such as principal component analysis, which you will learn about later) to reduce the number of dimensions to two, and finally plot the words in a Cartesian plan. That’s why you might have seen pictures like the one at the bottom of <em class="italic">Table 4.15</em>. Have you ever asked yourself how it is possible to plot words on <span class="No-Break">a graph?</span></p>
			<p class="IMG---Figure"><img src="image/B21197_04_12.png" alt="Figure 4.12 – Plotting words" width="896" height="987"/><a id="_idTextAnchor618"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.12 – Plotting words</p>
			<p>Next, you will learn how the five dimensions shown in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.12</em> were built. Again, there are different methods to do this, but you will learn the most popular, which uses a co-occurrence matrix with a fixed <span class="No-Break">context window.</span></p>
			<p>First, you have to come up with <a id="_idTextAnchor619"/>some logic to represent each word, keeping in mind that you also have to take their context into consideration. To s<a id="_idTextAnchor620"/>olve the context requirement, you need to define a <strong class="bold">fixed context window</strong>, which is going to be responsible for specifying how many words will be grouped together for context learning. For instance, assume this fixed context window <span class="No-Break">as 2.</span></p>
			<p>Next, you will create a co-occurrence matrix, which will count the number of occurrences of each pair of words, according to t<a id="_idTextAnchor621"/>he pre-defined context window. Consider the following text: “I will pass this exam, you will see. I will <span class="No-Break">pass it.”</span></p>
			<p>The context window of the first word “pass” would be the ones in <em class="italic">bold</em>: “<em class="italic">I will</em> pass <em class="italic">this exam</em>, you will see. I will pass it.” Considering this logic, have a look at how many times each pair of words appears in the context window (<span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.13</em></span><span class="No-Break">).</span></p>
			<div>
				<div id="_idContainer046" class="IMG---Figure">
					<img src="image/B21197_04_13.jpg" alt="Figure 4.13 – Co-occurrence matrix" width="719" height="415"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor622"/>Figure 4.13 – Co-occurrence matrix</p>
			<p>As you can see, the pair of words “I will” appears th<a id="_idTextAnchor623"/>ree times when a context window of size 2 <span class="No-Break">is used:</span></p>
			<ol>
				<li><em class="italic">I will</em> pass this exam, you will see. I will <span class="No-Break">pass it.</span></li>
				<li>I will pass this exam, you <em class="italic">will</em> see. <em class="italic">I</em> will <span class="No-Break">pass it.</span></li>
				<li>I will pass this exam, you will see. <em class="italic">I will</em> <span class="No-Break">pass it.</span></li>
			</ol>
			<p>Looking at <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.13</em>, the same logic should be applied to all other pairs of words, replacing “…” with the associated number of occurrences. You now have a numerical representation for <span class="No-Break">each word!</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">You should be aware that there are many alternatives to co-occurrence matrices with a fixed context window, such as using TD-IDF vectorization or even simpler counters of words per document. The most important message here is that, somehow, you must come up with a numerical representation for <span class="No-Break">each word.</span></p>
			<p>The last step is finally finding those dimensions shown in <em class="italic">Table 4.13</em>. You can do this by creating a multilayer model, usually based on neural networks, where the hidden layer will represent your embedding space. <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.14 </em>shows a simplified example where you could potentially compress those words shown in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.13 </em>into an embedding space of <span class="No-Break">five dimensions:</span></p>
			<div>
				<div id="_idContainer047" class="IMG---Figure">
					<img src="image/B21197_04_14.jpg" alt="Figure 4.14 – Building embedding spaces with neural networks" width="1296" height="673"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">F<a id="_idTextAnchor624"/>igure 4.14 – Building embedding spaces with neural networks</p>
			<p>You will learn about neural networks in more detail later in this book. For now, understanding where the embedding vector comes from is already an <span class="No-Break">awesome achievement!</span></p>
			<p>Another important thing you should keep in mind while modeling natural language problems is that you can reuse a pre-trained embedding space in your models. Some companies have created modern neural network architectures, trained on billions of documents, which have become the state of the ar<a id="_idTextAnchor625"/>t in this field. For your reference, take a look at Bidirectional Encoder Representations from Transformers (BERT), which was proposed by Google and has been widely used by the data science community <span class="No-Break">and industry.</span></p>
			<p>You have now reached the end of this long – but very important – chapter about data preparation and transformation. Take this opportunity to do a quick recap of the awesome things you <span class="No-Break">have learned.</span></p>
			<h1 id="_idParaDest-108">Sum<a id="_idTextAnchor626"/><a id="_idTextAnchor627"/>mary</h1>
			<p>First, you were introduced to the different types of features that you might have to work with. Identifying the type of variable you’ll be working with is very important for defining the types of transformations and techniques that can be applied to <span class="No-Break">each case.</span></p>
			<p>Then, you learned how to deal with categorical features. You saw that, sometimes, categorical variables do have an order (such as the ordinal ones), while other times, they don’t (such as the nominal ones). You learned that one-hot encoding (or dummy variables) is probably the most common type of transformation for nominal features; however, depending on the number of unique categories, after applying one-hot encoding, your data might suffer from sparsity issues. Regarding ordinal features, you shouldn’t create dummy variables on top of them, since you would be losing the information about the order that has been incorporated into the variable. In those cases, ordinal encoding is the most <span class="No-Break">appropriate transformation.</span></p>
			<p>You continued your journey by looking at numerical features, where you learned how to deal with continuous and discrete data. You walked through the most important types of transformations, such as normalization, standardization, binning, and discretization. You saw that some types of transformation rely on the underlying data to find their parameters, so it is very important to avoid using the testing set to learn anything from the data (it must strictly be used only <span class="No-Break">for testing).</span></p>
			<p>You have also seen that you can even apply pure math to transform your data; for example, you learned that power transformations can be used to reduce the skewness of your feature and make it more similar to a <span class="No-Break">normal distribution.</span></p>
			<p>After that, you looked at missing data and got a sense of how important this task is. When you are modeling, you <em class="italic">can’t</em> look at the missing values as a simple computational problem, where you just have to replace <em class="italic">x</em> with <em class="italic">y</em>. This is a much bigger problem, and you need to start solving it by exploring your data and then checking whether your missing data was generated at random <span class="No-Break">or not.</span></p>
			<p>When you are making the decision to remove or replace missing data, you must be aware that you are either losing information or adding bias to the dataset, respectively. Remember to review all the important notes of this chapter, since they are likely to be relevant to <span class="No-Break">your exam.</span></p>
			<p>You also learned about outlier detection. You looked at different ways to find outliers, such as the zscore and box plot approaches. Most importantly, you learned that you can either flag or <span class="No-Break">smooth them.</span></p>
			<p>At the beginning, you were advised that this chapter would be a long but worthwhile journey about data preparation. You have also learned how to deal with rare events, since this is one of the most challenging problems in ML. Now you are aware that, sometimes, your data might be unbalanced, and you must either trick your algorithm (by changing the class weights) or resample your data (applying undersampling <span class="No-Break">or oversampling).</span></p>
			<p>Finally, you learned how to deal with text data for NLP. You should now be able to manually compute BoW and TF-IDF matrices! You went even deeper and learned how word embedding works. During this subsection, you learned that you can either create your own embedding space (using many different methods) or reuse a pre-trained one, such <span class="No-Break">as BERT.</span></p>
			<p>You are done! In the next chapter, you will dive into data <span class="No-Break">visualization techniques.</span></p>
			<h1 id="_idParaDest-109"><a id="_idTextAnchor628"/>Exam Readiness Drill – Chapter Review Questions</h1>
			<p>Apart from a solid understanding of key concepts, being able to think quickly under time pressure is a skill that will help you ace your certification exam. That is why working on these skills early on in your learning journey <span class="No-Break">is key.</span></p>
			<p>Chapter review questions are designed to improve your test-taking skills progressively with each chapter you learn and review your understanding of key concepts in the chapter at the same time. You’ll find these at the end of <span class="No-Break">each chapter.</span></p>
			<p class="callout-heading">How To Access These Resources</p>
			<p class="callout">To learn how to access these resources, head over to the chapter titled <a href="B21197_11.xhtml#_idTextAnchor1477"><span class="No-Break"><em class="italic">Chapter 11</em></span></a>, <em class="italic">Accessing the Online </em><span class="No-Break"><em class="italic">Practice Resources</em></span><span class="No-Break">.</span></p>
			<p>To open the Chapter Review Questions for this chapter, perform the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>Click the link – <a href="https://packt.link/MLSC01E2_CH04"><span class="No-Break">https://packt.link/MLSC01E2_CH04</span></a><span class="No-Break">.</span><p class="list-inset">Alternatively, you can scan the following <strong class="bold">QR code</strong> (<span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.15</em></span><span class="No-Break">):</span></p></li>
			</ol>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="image/B21197_04_15.jpg" alt="Figure 4.15 – QR code that opens Chapter Review Questions for logged-in users" width="550" height="150"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.15 – QR code that opens Chapter Review Questions for logged-in users</p>
			<ol>
				<li value="2">Once you log in, you’ll see a page similar to the one shown in <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.16</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/B21197_04_16.jpg" alt="Figure 4.16 – Chapter Review Questions for Chapter 4" width="1416" height="1350"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.16 – Chapter Review Questions for Chapter 4</p>
			<ol>
				<li value="3">Once ready, start the following practice drills, re-attempting the quiz <span class="No-Break">multiple times.</span></li>
			</ol>
			<h2 id="_idParaDest-110"><a id="_idTextAnchor629"/>Exam Readiness Drill</h2>
			<p>For the first three attempts, don’t worry about the <span class="No-Break">time limit.</span></p>
			<h3 id="_idParaDest-111"><a id="_idTextAnchor630"/>ATTEMPT 1</h3>
			<p>The first time, aim for at least <strong class="bold">40%</strong>. Look at the answers you got wrong and read the relevant sections in the chapter again to fix your <span class="No-Break">learning gaps.</span></p>
			<h3 id="_idParaDest-112"><a id="_idTextAnchor631"/>ATTEMPT 2</h3>
			<p>The second time, aim for at least <strong class="bold">60%</strong>. Look at the answers you got wrong and read the relevant sections in the chapter again to fix any remaining <span class="No-Break">learning gaps.</span></p>
			<h3 id="_idParaDest-113"><a id="_idTextAnchor632"/>ATTEMPT 3</h3>
			<p>The third time, aim for at least <strong class="bold">75%</strong>. Once you score 75% or more, you start working on <span class="No-Break">your timing.</span></p>
			<p class="callout-heading">Tip</p>
			<p class="callout">You may take more than <strong class="bold">three</strong> attempts to reach 75%. That’s okay. Just review the relevant sections in the chapter till you <span class="No-Break">get there.</span></p>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor633"/>Working On Timing</h1>
			<p>Target: Your aim is to keep the score the same while trying to answer these questions as quickly as possible. Here’s an example of how your next attempts should <span class="No-Break">look like:</span></p>
			<table id="table014" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Attempt</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Score</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Time Taken</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Attempt 5</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">77%</span></p>
						</td>
						<td class="No-Table-Style">
							<p>21 mins <span class="No-Break">30 seconds</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Attempt 6</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">78%</span></p>
						</td>
						<td class="No-Table-Style">
							<p>18 mins <span class="No-Break">34 seconds</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Attempt 7</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">76%</span></p>
						</td>
						<td class="No-Table-Style">
							<p>14 mins <span class="No-Break">44 seconds</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.14 – Sample timing practice drills on the online platform</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The time limits shown in the above table are just examples. Set your own time limits with each attempt based on the time limit of the quiz on <span class="No-Break">the website.</span></p>
			<p>With each new attempt, your score should stay above <strong class="bold">75%</strong> while your “time taken” to complete should “decrease”. Repeat as many attempts as you want till you feel confident dealing with the <span class="No-Break">time pressure.</span></p>
		</div>
	</div>
</div>
</body></html>