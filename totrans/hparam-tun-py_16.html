<html><head></head><body>
		<div id="_idContainer365">
			<h1 id="_idParaDest-119"><em class="italic"><a id="_idTextAnchor125"/>Chapter 13</em><span class="superscript">: Tracking Hyperparameter Tuning Experiments</span></h1>
			<p><span class="superscript">Working with a lot of experiments can sometimes be overwhelming. Many iterations of experiments will need to be done. It will become even more complicated when we are experimenting with many ML models.</span></p>
			<p><span class="superscript">In this chapter, you will be introduced to the importance of tracking hyperparameter tuning experiments, along with the usual practices. You will also be introduced to several open source packages that are available and learn how to utilize each of them in practice.</span></p>
			<p><span class="superscript">By the end of this chapter, you will be able to utilize your favorite package to track your hyperparameter tuning experiment. Being able to track your hyperparameter tuning experiment will boost the effectiveness of your workflow.</span></p>
			<p><span class="superscript">In this chapter, we will cover the following topics:</span></p>
			<ul>
				<li><span class="superscript">Revisiting the usual practices</span></li>
				<li><span class="superscript">Exploring Neptune</span></li>
				<li><span class="superscript">Exploring Scikit-Optimize</span></li>
				<li><span class="superscript">Exploring Optuna</span></li>
				<li><span class="superscript">Exploring Microsoft NNI</span></li>
				<li><span class="superscript">Exploring MLflow</span></li>
			</ul>
			<h1 id="_idParaDest-120"><span class="superscript"><a id="_idTextAnchor126"/>Technical re</span><span class="superscript">quirements</span></h1>
			<p><span class="superscript">In this chapter, we will learn how to track hyperparameter tuning experiments with various packages. To ensure that you can reproduce the code examples in this chapter, you will require the following:</span></p>
			<ul>
				<li><span class="superscript">The Python 3 (version 3.7 or above)</span></li>
				<li><span class="superscript">The </span><strong class="source-inline">pandas</strong> package (version 1.3.4 or above)</li>
				<li><span class="superscript">The </span><strong class="source-inline">NumPy</strong> package (version 1.21.2 or above)</li>
				<li><span class="superscript">The </span><strong class="source-inline">scikit-learn</strong> package (version 1.0.1 or above)</li>
				<li><span class="superscript">The </span><strong class="source-inline">matplotlib</strong> package (version 3.5.0 or above)</li>
				<li><span class="superscript">The </span><strong class="source-inline">Plotly</strong> package (version 4.0.0 or above)</li>
				<li><span class="superscript">The </span><strong class="source-inline">Neptune-client</strong> package (version 0.16.3 or above)</li>
				<li><span class="superscript">The </span><strong class="source-inline">Neptune-optuna</strong> package (version 0.9.14 or above)</li>
				<li><span class="superscript">The </span>Scikit-Optimize package (version 0.9.0 or above)</li>
				<li><span class="superscript">The </span><strong class="source-inline">TensorFlow</strong> package (version 2.4.1 or above)</li>
				<li><span class="superscript">The </span><strong class="source-inline">Optuna</strong> package (version 2.10.0 or above)</li>
				<li><span class="superscript">The </span><strong class="source-inline">MLflow</strong> package (version 1.27.0 or above)</li>
			</ul>
			<p>All the code examples for this chapter can be found on GitHub at <a href="https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python">https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python</a>.</p>
			<h1 id="_idParaDest-121"><a id="_idTextAnchor127"/>Revisiting the usual practices</h1>
			<p>Conducting <a id="_idIndexMarker631"/>hyperparameter tuning experiments in a small-scale project may seem straightforward. We can easily do several iterations of experiments and write all the results in a separate document. We can log the details of the best set of hyperparameter values (or the tested set of hyperparameters if we perform a manual search method, as shown in <a href="B18753_03_ePub.xhtml#_idTextAnchor031"><em class="italic">Chapter 3</em></a><em class="italic">, Exhaustive Search</em>), along with the evaluation metric, in each experiment iteration. By having an experiment log, we can learn from the history and define a better hyperparameter space in the next iteration of the experiment. </p>
			<p>When we adopt the automated hyperparameter tuning method (all the methods we’ve discussed <a id="_idIndexMarker632"/>so far besides the manual search method), we can get the final best set of hyperparameter values directly. However, this is not the case when we adopt the manual search method. We need to test numerous sets of hyperparameters manually. Several practices are adopted by the community when performing manual searches. Let’s take a look.</p>
			<h2 id="_idParaDest-122"><a id="_idTextAnchor128"/>Using a built-in Python dictionary</h2>
			<p>This is the most straightforward approach since we just need to create a Python dictionary <a id="_idIndexMarker633"/>that stores all the hyperparameter values that need to be tested. Although this practice is very simple, it has drawbacks. For example, we may not notice if we overwrite some of the <a id="_idIndexMarker634"/>hyperparameter values and forget to log the correct set of hyperparameter values. The following example of utilizing a built-in Python dictionary to store all of the hyperparameter values needs to be tested in a particular manual search iteration:</p>
			<pre class="source-code">hyperparameters = {</pre>
			<pre class="source-code">'n_estimators': 30,</pre>
			<pre class="source-code">'max_features': 10,</pre>
			<pre class="source-code">'criterion': 'gini',</pre>
			<pre class="source-code">'max_depth': 5,</pre>
			<pre class="source-code">'min_samples_split': 0.03,</pre>
			<pre class="source-code">'min_samples_leaf': 1,</pre>
			<pre class="source-code">}</pre>
			<p>Next, let’s look at configuration files.</p>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor129"/>Using a configuration file</h2>
			<p>Whether <a id="_idIndexMarker635"/>it is a JSON, YAML, or CFG file, configuration files are <a id="_idIndexMarker636"/>another option. We can put all the hyperparameter details within this configuration file, along with other additional information, including (but not limited to) project name, author name, and data pre-processing pipeline methods. Once you have created the configuration file, you can load it into your Python script or Jupyter notebook, and treat it like a <a id="_idIndexMarker637"/>standard Python dictionary. The main advantage of using a configuration file is that all the important <a id="_idIndexMarker638"/>parameters are located within a single file, so it will be very easy to reuse the previously saved configuration files and increase the readability of your code. However, utilizing configuration files when working with a big project or huge code base can sometimes confuse us since we have to maintain several configuration files.</p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor130"/>Using additional modules</h2>
			<p>The <strong class="source-inline">argparse</strong> and <strong class="source-inline">Click</strong> modules come in handy if you want to specify the hyperparameter <a id="_idIndexMarker639"/>values or <a id="_idIndexMarker640"/>any other training arguments via the <strong class="bold">Command Line Interface</strong> (<strong class="bold">CLI</strong>). These <a id="_idIndexMarker641"/>modules can be utilized when we write our code in a Python script, not in a Jupyter notebook. </p>
			<h3>Using argparse</h3>
			<p>The following <a id="_idIndexMarker642"/>code shows how to utilize <strong class="source-inline">argparse</strong> in a <a id="_idIndexMarker643"/>Python script:</p>
			<pre class="source-code">import argparse</pre>
			<pre class="source-code">parser = argparse.ArgumentParser(description='Hyperparameter Tuning')</pre>
			<pre class="source-code">parser.add_argument('--n_estimators, type=int, default=30, help='number of estimators')</pre>
			<pre class="source-code">parser.add_argument('--max_features, type=int, default=20, help='number of randomly sampled features for choosing the best splitting point')</pre>
			<pre class="source-code">parser.add_argument('--criterion, type=str, default='gini', help='homogeneity measurement method')</pre>
			<pre class="source-code">parser.add_argument('--max_depth, type=int, default=5, help='maximum tree depth')</pre>
			<pre class="source-code">parser.add_argument('--min_samples_split, type=float, default=0.03, help='minimum samples to split internal node')</pre>
			<pre class="source-code">parser.add_argument('--min_samples_leaf, type=int, default=1, help='minimum number of samples in a leaf node')</pre>
			<pre class="source-code">parser.add_argument('--data_dir, type=str, required=True, help='maximum tree depth')</pre>
			<p>The following <a id="_idIndexMarker644"/>code shows how to access the values <a id="_idIndexMarker645"/>from the CLI:</p>
			<pre class="source-code">args = parser.parse_args()</pre>
			<pre class="source-code">print(args.n_estimators)</pre>
			<pre class="source-code">print(args.max_features)</pre>
			<pre class="source-code">print(args.criterion)</pre>
			<pre class="source-code">print(args.max_depth)</pre>
			<pre class="source-code">print(args.min_samples_split)</pre>
			<pre class="source-code">print(args.min_samples_leaf)</pre>
			<pre class="source-code">print(args.data_dir)</pre>
			<p>You can run the Python script with specified parameters, as follows:</p>
			<pre class="source-code">python main.py --n_estimators 35 -–criterion "entropy" -–data_dir "/path/to/my/data"</pre>
			<p>It is worth noting that the default values of the hyperparameters will be used if you don’t specify them when calling the Python script. </p>
			<h3>Using click</h3>
			<p>The following <a id="_idIndexMarker646"/>code shows how to utilize <strong class="source-inline">click</strong> in a Python script. Note that <a id="_idIndexMarker647"/><strong class="source-inline">click</strong> is very similar to <strong class="source-inline">argparse</strong> with a simpler implementation. We just need to add decorations on top of a particular function:</p>
			<pre class="source-code">import click</pre>
			<pre class="source-code">@click.command()</pre>
			<pre class="source-code">@click.option("--n_estimators, type=int, default=30, help='number of estimators")</pre>
			<pre class="source-code">@click.option("--max_features, type=int, default=20, help='number of randomly sampled features for choosing the best splitting point")</pre>
			<pre class="source-code">@click.option("--criterion, type=str, default='gini', help='homogeneity measurement method")</pre>
			<pre class="source-code">@click.option("--max_depth, type=int, default=5, help='maximum tree depth")</pre>
			<pre class="source-code">@click.option("--min_samples_split, type=float, default=0.03, help='minimum samples to split internal node")</pre>
			<pre class="source-code">@click.option("--data_dir, type=str, required=True, help='maximum tree depth")</pre>
			<pre class="source-code">def hyperparameter_tuning(n_estimators, max_features, criterion, max_depth, min_samples_split, data_dir):</pre>
			<pre class="source-code">#write your code here</pre>
			<p>Similar to <strong class="source-inline">argparse</strong>, you can run the Python script with specified parameters, as shown <a id="_idIndexMarker648"/>here. The default hyperparameter values will be used if you don’t <a id="_idIndexMarker649"/>specify them when calling the Python script:</p>
			<pre class="source-code">python main.py --n_estimators 35 -–criterion "entropy" -–data_dir "/path/to/my/data"</pre>
			<p>While experimenting with either <strong class="source-inline">argparse</strong> or <strong class="source-inline">click</strong> is very easy to do, it is worth noting that neither saves values anywhere. Hence, it requires extra effort to log all of the experimented hyperparameter values in each trial.</p>
			<p>Regardless of whether we are adopting manual search or other automated hyperparameter tuning methods, it will require a lot of effort if we have to log the resulting experiment’s details manually. It can be overwhelming, especially when we are working with larger-scale experiments where we have to test several different ML models, data <a id="_idIndexMarker650"/>pre-processing pipelines, and other experiment setups. That’s <a id="_idIndexMarker651"/>why, in the coming sections, you will be introduced to several packages that can help you track your hyperparameter tuning experiments so that you have a more effective workflow.</p>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor131"/>Exploring Neptune</h1>
			<p><strong class="bold">Neptune</strong> is a <a id="_idIndexMarker652"/>Python (and R) package that acts as a metadata store for MLOps. This package supports a lot of features for working with the model-building metadata. We can utilize Neptune for tracking our experiments, not only hyperparameter <a id="_idIndexMarker653"/>tuning experiments but also other model-building-related experiments. We can log, visualize, organize, and manage our experiments just by using a single package. Furthermore, it also supports model registry and live monitors our ML jobs.</p>
			<p>Installing Neptune is very easy – you can just use <strong class="source-inline">pip install neptune-client</strong> or <strong class="source-inline">conda install -c conda-forge neptune-client</strong>. Once it has been installed, you need to sign up for an account to get the API token. Neptune is free for an individual plan within the quota limit, but you need to pay if you want to utilize Neptune for commercial team usage. Further information about registering yourself for Neptune can be found <a id="_idIndexMarker654"/>on their official website: https://neptune.ai/register.</p>
			<p>Using Neptune to help track your hyperparameter tuning experiments is straightforward, as shown in the following steps:</p>
			<ol>
				<li>Create a new project from your Neptune account’s home page:</li>
			</ol>
			<div>
				<div id="_idContainer344" class="IMG---Figure">
					<img src="image/B18753_13_001.jpg" alt="Figure 13.1 – Creating a new Neptune project&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.1 – Creating a new Neptune project</p>
			<ol>
				<li value="2">Enter a <a id="_idIndexMarker655"/>name and description for your project:</li>
			</ol>
			<div>
				<div id="_idContainer345" class="IMG---Figure">
					<img src="image/B18753_13_002.jpg" alt="Figure 13.2 – Entering the project’s details&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.2 – Entering the project’s details</p>
			<ol>
				<li value="3">Write the hyperparameter tuning experiment script. Neptune provides several boilerplate code options based on the framework you want to use, including (but not limited to) Optuna, PyTorch, Keras, TensorFlow, scikit-learn, and XGBoost. You can <a id="_idIndexMarker656"/>just copy the provided boilerplate code and customize it based on your needs. For example, let’s use the provided boilerplate code for Optuna (see <em class="italic">Figure 13.3</em>) and save the training script as <strong class="source-inline">train_optuna.py</strong>. Please see the full code in this book’s GitHub repository, which was provided in the <em class="italic">Technical requirements</em> section:</li>
			</ol>
			<div>
				<div id="_idContainer346" class="IMG---Figure">
					<img src="image/B18753_13_003.jpg" alt="Figure 13.3 – Creating the hyperparameter tuning experiment script&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.3 – Creating the hyperparameter tuning experiment script</p>
			<ol>
				<li value="4">Run the hyperparameter tuning script (<strong class="source-inline">python train_optuna.py</strong>) and look at the metadata of the experiments on your Neptune project page. Every run will be stored as a new experiment ID in Neptune, so you don’t have to worry about <a id="_idIndexMarker657"/>the experiment versioning since Neptune will handle it automatically for you:</li>
			</ol>
			<div>
				<div id="_idContainer347" class="IMG---Figure">
					<img src="image/B18753_13_004.jpg" alt="Figure 13.4 – Neptune’s experiment runs table&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.4 – Neptune’s experiment runs table</p>
			<p>You can also see all the metadata for each of the experiment runs, including (but not limited to) the tested hyperparameters, source code, CPU/GPU usage, metric charts, artifacts (data, model, or any other related files), and figures (for example, confusion matrices), as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer348" class="IMG---Figure">
					<img src="image/B18753_13_005.jpg" alt="Figure 13.5 – Metadata stored in Neptune&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.5 – Metadata stored in Neptune</p>
			<ol>
				<li value="5">Analyze the <a id="_idIndexMarker658"/>experiment results. Neptune can not only help you log all of the metadata for each experiment run, but it can also compare several different runs using several types of comparison strategies. You can see the hyperparameter values comparison via parallel plot or line charts. You can also compare all of the experiment details via a <strong class="bold">Side-by-side</strong> comparison strategy (see <em class="italic">Figure 13.6</em>). Furthermore, Neptune also enables us to compare the logged images or artifacts between each run:</li>
			</ol>
			<div>
				<div id="_idContainer349" class="IMG---Figure">
					<img src="image/B18753_13_006.jpg" alt="Figure 13.6 – Comparing the experiment runs and their results&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.6 – Comparing the experiment runs and their results</p>
			<p>For more <a id="_idIndexMarker659"/>information regarding what you can log and display in Neptune, please <a id="_idIndexMarker660"/>refer to the official documentation page: https://docs.neptune.ai/you-should-know/what-can-you-log-and-display. </p>
			<p class="callout-heading">Integrations in Neptune</p>
			<p class="callout">Neptune provides <a id="_idIndexMarker661"/>numerous integrations for ML-related experiments in general and also for specific hyperparameter tuning-related tasks. Three integrations are supported by Neptune for hyperparameter tuning tasks: Optuna, Keras, and Scikit-Optimize. For more information, please refer to the official documentation page: https://docs.neptune.ai/integrations-and-supported-tools/intro.  </p>
			<p class="callout-heading">More examples</p>
			<p class="callout">Neptune is a very powerful package that can be utilized for other ML experiment-related tasks, too. For more examples of how to utilize Neptune in general, please refer to the official documentation page: https://docs.neptune.ai/getting-started/examples. </p>
			<p>In this section, you <a id="_idIndexMarker662"/>were introduced to Neptune and how to utilize it to help you track your hyperparameter tuning experiments. In the next section, you will learn how to utilize the famous Scikit-Optimize package for hyperparameter tuning experiment tracking purposes.</p>
			<h1 id="_idParaDest-126"><a id="_idTextAnchor132"/>Exploring scikit-optimize</h1>
			<p>You were introduced <a id="_idIndexMarker663"/>to the <strong class="bold">Scikit-Optimize</strong> package in <a href="B18753_07_ePub.xhtml#_idTextAnchor062"><em class="italic">Chapter 7</em></a>, <em class="italic">Hyperparameter Tuning via Scikit</em>, to conduct a hyperparameter tuning experiment. In this section, we will learn how to utilize this package to track <a id="_idIndexMarker664"/>all hyperparameter tuning experiments conducted using this package. </p>
			<p>Scikit-Optimize provides very nice visualization plots that summarize the tested hyperparameter values, the objective function scores, and the relationship between them. Three plots are available in this package, as shown here. Please see the full code in this book’s GitHub repository for more details. The following plots were generated based on the same experimental setup that was provided in <a href="B18753_07_ePub.xhtml#_idTextAnchor062"><em class="italic">Chapter 7</em></a>, <em class="italic">Hyperparameter Tuning via Scikit</em>, for the BOGP hyperparameter tuning method:</p>
			<ul>
				<li><strong class="source-inline">plot_convergence</strong>: This <a id="_idIndexMarker665"/>is used to visualize the hyperparameter tuning optimization progress for each iteration:</li>
			</ul>
			<div>
				<div id="_idContainer350" class="IMG---Figure">
					<img src="image/B18753_13_007.jpg" alt="Figure 13.7 – Convergence plot&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.7 – Convergence plot</p>
			<ul>
				<li><strong class="source-inline">plot_evaluations</strong>: This is used to visualize the optimization evolution process <a id="_idIndexMarker666"/>history. In other words, it shows the order in which hyperparameter values were sampled during the optimization process. For each hyperparameter, a histogram of explored hyperparameter values is generated. For each pair of hyperparameters, the scatter plot of tested hyperparameter values is visualized and equipped with colors to act as the legend of the evolution history (from blue to yellow):</li>
			</ul>
			<div>
				<div id="_idContainer351" class="IMG---Figure">
					<img src="image/B18753_13_008.jpg" alt="Figure 13.8 – Evaluation plot&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.8 – Evaluation plot</p>
			<ul>
				<li><strong class="source-inline">plot_objective</strong>: This is used to visualize the pairwise dependence plot of the <a id="_idIndexMarker667"/>objective function. This visualization helps us gain information regarding the relationship between the tested hyperparameter values and the objective function scores. From this plot, you can see which subspace needs more attention and which subspace, or even which hyperparameter, needs to be removed from the original space in the next trial:</li>
			</ul>
			<div>
				<div id="_idContainer352" class="IMG---Figure">
					<img src="image/B18753_13_009.jpg" alt="Figure 13.9 – Pairwise dependence plot&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.9 – Pairwise dependence plot</p>
			<p class="callout-heading">Integration with Neptune</p>
			<p class="callout">Scikit-Optimize provides very informative visualization modules. However, it does not support any experiment versioning capabilities, unlike the Neptune package. To get the best of both worlds, we can integrate Scikit-Optimize with Neptune via its integration <a id="_idIndexMarker668"/>module. For more information about this, please refer to the official documentation page: https://docs-legacy.neptune.ai/integrations/skopt.html. </p>
			<p>In this section, you <a id="_idIndexMarker669"/>learned how to utilize the Scikit-Optimize package to help you track your hyperparameter tuning experiments. In the next section, you will learn how to utilize the Optuna package for hyperparameter tuning experiment tracking purposes.</p>
			<h1 id="_idParaDest-127"><a id="_idTextAnchor133"/>Exploring Optuna</h1>
			<p><strong class="bold">Optuna</strong> is a <a id="_idIndexMarker670"/>hyperparameter tuning package in Python that provides several hyperparameter tuning methods. We discussed how to utilize Optuna to conduct a <a id="_idIndexMarker671"/>hyperparameter tuning experiment in <a href="B18753_09_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 9</em></a>, <em class="italic">Hyperparameter Tuning via Optuna</em>. Here, we will discuss how to utilize this package to track those experiments. </p>
			<p>Similar to Scikit-Optimize, Optuna provides very nice visualization modules to help us track the hyperparameter tuning experiments and as a guide for us to decide which subspace to search in the next trial. Four visualization modules can be utilized, as shown here. All of them expect the <strong class="source-inline">study</strong> object (see <em class="italic">Chapter 9</em>, <em class="italic">Hyperparameter Tuning via Optuna</em>) as input. Please see the full code in this book’s GitHub repository:</p>
			<ul>
				<li><strong class="source-inline">plot_contour</strong>: This is <a id="_idIndexMarker672"/>used to visualize the relationship between hyperparameters (as well as the objective function scores) in the form of contour plots:</li>
			</ul>
			<div>
				<div id="_idContainer353" class="IMG---Figure">
					<img src="image/B18753_13_010.jpg" alt="Figure 13.10 – Contour plot&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.10 – Contour plot</p>
			<ul>
				<li><strong class="source-inline">plot_optimization_history</strong>: This <a id="_idIndexMarker673"/>is used to visualize the hyperparameter tuning optimization progress for each iteration:</li>
			</ul>
			<div>
				<div id="_idContainer354" class="IMG---Figure">
					<img src="image/B18753_13_011.jpg" alt="Figure 13.11 – Optimization history plot&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.11 – Optimization history plot</p>
			<ul>
				<li><strong class="source-inline">plot_parallel_coordinate</strong>: This is used to visualize the relationship between <a id="_idIndexMarker674"/>hyperparameters (as well as the objective function scores) in the form of a parallel coordinate plot:</li>
			</ul>
			<div>
				<div id="_idContainer355" class="IMG---Figure">
					<img src="image/B18753_13_012.jpg" alt="Figure 13.12 – Parallel coordinate plot&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.12 – Parallel coordinate plot</p>
			<ul>
				<li><strong class="source-inline">plot_slice</strong>: This is used to visualize the hyperparameter tuning method’s search evolution. You <a id="_idIndexMarker675"/>can see what hyperparameter values have been tested in the experiment and which subspace is getting more attention during the search process:</li>
			</ul>
			<div>
				<div id="_idContainer356" class="IMG---Figure">
					<img src="image/B18753_13_013.jpg" alt="Figure 13.13 – Slice plot&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.13 – Slice plot</p>
			<p>The nice thing <a id="_idIndexMarker676"/>about all the visualization modules in Optuna is that they are all interactive charts since they are created using the <strong class="source-inline">Plotly</strong> visualization package. You can zoom in on a specific area in the charts and use other interactive features.</p>
			<p class="callout-heading">Integration with Neptune</p>
			<p class="callout">Similar to Scikit-Optimize, Optuna provides very informative visualization modules. However, it does not support any experiment versioning capability, unlike the Neptune package. We can integrate Optuna with Neptune via its integration module. For more <a id="_idIndexMarker677"/>information about this, please refer to the official documentation page: https://docs-legacy.neptune.ai/integrations/optuna.html. </p>
			<p>In this section, you learned how to utilize the Optuna package to track your hyperparameter tuning experiments. In the next section, you will learn how to utilize the Microsoft NNI package for hyperparameter tuning experiment tracking purposes.</p>
			<h1 id="_idParaDest-128"><a id="_idTextAnchor134"/>Exploring Microsoft NNI</h1>
			<p><strong class="bold">Neural Network Intelligence</strong> (<strong class="bold">NNI</strong>) is <a id="_idIndexMarker678"/>a package that is developed by Microsoft and can be utilized not only for hyperparameter tuning tasks <a id="_idIndexMarker679"/>but also for neural architecture search, model compression, and feature engineering. We discussed how to utilize NNI to conduct hyperparameter tuning experiments in <a href="B18753_10_ePub.xhtml#_idTextAnchor092"><em class="italic">Chapter 10</em></a>, <em class="italic">Advanced Hyperparameter Tuning with DEAP and Microsoft NNI</em>. </p>
			<p>In this section, we will discuss how to utilize this package to track those experiments. All of the experiment tracking modules provided by NNI are located in the <em class="italic">web portal</em>. You learned about the web portal in <a href="B18753_10_ePub.xhtml#_idTextAnchor092"><em class="italic">Chapter 10</em></a>, <em class="italic">Advanced Hyperparameter Tuning with DEAP and Microsoft NNI</em>. However, we haven’t discussed it in depth and there are many useful features you should know about. </p>
			<p>The web portal can be utilized to visualize all of the hyperparameter tuning experiment’s metadata, including (but not limited to) tuning and training progress, evaluation metrics, and error logs. It can also be utilized to update the experiment’s concurrency and duration, and retry the failed trials. The following is a list of all the important modules in the NNI web portal that can be utilized to help us track our hyperparameter tuning experiments. The following plots have been generated based on the same experimental setup that was stated in <a href="B18753_10_ePub.xhtml#_idTextAnchor092"><em class="italic">Chapter 10</em></a>, <em class="italic">Advanced Hyperparameter Tuning with DEAP and Microsoft NNI</em>, for the Random Search method. Please see the full code in this book’s GitHub repository:</p>
			<ul>
				<li>The <strong class="bold">Overview</strong> page shows an overview of our hyperparameter tuning experiment, including its name and ID, status, start and end time, best metric, elapsed duration, number of trials faceted by the status, as well as the experiment path, training platform, and tuner details. Here, you can also change the maximum duration, the maximum number of trials, and the experiment’s concurrency. There is also a dedicated module that shows the top-performing trials: </li>
			</ul>
			<div>
				<div id="_idContainer357" class="IMG---Figure">
					<img src="image/B18753_13_014.jpg" alt="Figure 13.14 – The Overview page&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.14 – The Overview page</p>
			<ul>
				<li>The <strong class="bold">Trials detail</strong> page shows every detail regarding the experiment’s trials, including <a id="_idIndexMarker680"/>a visualization of all the metrics (see <em class="italic">Figure 13.15</em>), a hyperparameter values parallel plot (see <em class="italic">Figure 13.16</em>), a bar chart of the duration of all the trials (see <em class="italic">Figure 13.17</em>), and a line chart of all intermediate results that shows the trend of each trial during the intermediate steps. We can also see the details of each trial via the <strong class="bold">Trial jobs</strong> module, including (but not limited to) the trial’s ID, duration, status, metric, hyperparameter value details, and log files (see <em class="italic">Figure 13.18</em>):</li>
			</ul>
			<div>
				<div id="_idContainer358" class="IMG---Figure">
					<img src="image/B18753_13_015.jpg" alt="Figure 13.15 – The Trials detail page&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.15 – The Trials detail page</p>
			<p>The following <a id="_idIndexMarker681"/>is a parallel plot that shows different hyperparameter values that had been tested in the experiment:</p>
			<div>
				<div id="_idContainer359" class="IMG---Figure">
					<img src="image/B18753_13_016.jpg" alt="Figure 13.16 – Hyperparameter values parallel plot&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.16 – Hyperparameter values parallel plot</p>
			<p>The following is a bar chart containing information about the duration of all the trials in the experiment:</p>
			<div>
				<div id="_idContainer360" class="IMG---Figure">
					<img src="image/B18753_13_017.jpg" alt="Figure 13.17 – Trials duration bar chart&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.17 – Trials duration bar chart</p>
			<p>Finally, there’s <a id="_idIndexMarker682"/>the <strong class="bold">Trial jobs</strong> module:</p>
			<div>
				<div id="_idContainer361" class="IMG---Figure">
					<img src="image/B18753_13_018.jpg" alt="Figure 13.18 – The Trial jobs module&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.18 – The Trial jobs module</p>
			<p>The Trial jobs module includes the following:</p>
			<ul>
				<li><strong class="bold">Sidebar</strong>: We can access all the information related to the search space, config, and log files in the sidebar:</li>
			</ul>
			<div>
				<div id="_idContainer362" class="IMG---Figure">
					<img src="image/B18753_13_019.jpg" alt="Figure 13.19 – Sidebar&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.19 – Sidebar</p>
			<ul>
				<li>The <strong class="bold">Auto refresh</strong> button: We <a id="_idIndexMarker683"/>can also change the refresh interval of the web portal by using the <strong class="bold">Auto refresh</strong> button:</li>
			</ul>
			<div>
				<div id="_idContainer363" class="IMG---Figure">
					<img src="image/B18753_13_020.jpg" alt="Figure 13.20 – The Auto refresh button&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.20 – The Auto refresh button</p>
			<ul>
				<li>The <strong class="bold">Experiment summary</strong> button: By clicking this button, you can view all the summaries for the current experiment:</li>
			</ul>
			<div>
				<div id="_idContainer364" class="IMG---Figure">
					<img src="image/B18753_13_021.jpg" alt="Figure 13.21 – The Experiment summary button&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.21 – The Experiment summary button</p>
			<p>In this section, you <a id="_idIndexMarker684"/>learned how to utilize the Microsoft NNI package to track your hyperparameter tuning experiments. In the next section, you will learn how to utilize the MLflow package for hyperparameter-tuning experiment tracking purposes.</p>
			<h1 id="_idParaDest-129"><a id="_idTextAnchor135"/>Exploring MLflow</h1>
			<p><strong class="bold">MLflow</strong> can be utilized <a id="_idIndexMarker685"/>to manage the whole end-to-end ML pipeline. It is available in Python, R, Java, and via the REST API. The primary functions of MLflow include experiment tracking, ML code packaging, ML model deployment management, and centralized model storing and versioning. In this section, we will learn how to utilize this package to track our hyperparameter tuning experiments. Installing MLflow is very easy; you can just use the <strong class="source-inline">pip install mlflow</strong> command.</p>
			<p>To track our hyperparameter tuning experiments with MLflow, we simply need to add several logging functions to our code base. Once we’ve added the required logging function, we can go to the provided UI by simply entering the <strong class="source-inline">mlflow ui</strong> command in the command line and opening it at <a href="http://localhost:5000">http://localhost:5000</a>. Many logging functions are provided by MLflow, and the following are some of the main important logging functions you need to be aware of. Please see the full example c</p>
			<p>ode in this book’s GitHub repository:</p>
			<ul>
				<li><strong class="source-inline">create_experiment()</strong>: This function is used to create a new experiment. You can specify the name of the experiment, tags, and the path to store the experiment artifacts.</li>
				<li><strong class="source-inline">set_experiment()</strong>: This function is used to set the given experiment name or ID as the current active experiment.</li>
				<li><strong class="source-inline">start_run()</strong>: This function is used to start a new MLflow run under the current active experiment. It is suggested to use this function as a context manager within a <strong class="source-inline">with</strong> block. </li>
				<li><strong class="source-inline">log_metric()</strong>: This function is used to log a single metric within the currently active run. If you want to do bulk logging, you can also use the <strong class="source-inline">log_metrics()</strong> function by passing a dictionary of metrics.</li>
				<li><strong class="source-inline">log_param()</strong>: This function is used to log a parameter or hyperparameter within the currently active run. If you want to do bulk logging, you can also use the <strong class="source-inline">log_params()</strong> function by passing a dictionary of metrics.</li>
				<li><strong class="source-inline">log_artifact()</strong>: This function is used to log a file or directory as an artifact of the currently active run. If you want to log all the contents of a local directory, you can also use the <strong class="source-inline">log_artifacts()</strong> function.</li>
				<li><strong class="source-inline">set_tag()</strong>: This function is <a id="_idIndexMarker686"/>used to set a tag for the currently active run. You must provide the key and value of the tag. For example, you can set the key as <strong class="source-inline">“release_version”</strong> and the value as <strong class="source-inline">“1.0.0”</strong>. </li>
				<li><strong class="source-inline">log_figure()</strong>: This function is used to log a figure as an artifact of the currently active run. This function supports the <strong class="source-inline">matplotlib</strong> and <strong class="source-inline">pyplot</strong> figure object types.</li>
				<li><strong class="source-inline">log_image()</strong>: This function is used to log an image as an artifact of the currently active run. This function supports the <strong class="source-inline">numpy.ndarray</strong> and <strong class="source-inline">PIL.image.image</strong> object types.</li>
			</ul>
			<p class="callout-heading">MLflow Logging Functions</p>
			<p class="callout">For more <a id="_idIndexMarker687"/>information regarding all the available logging functions in MLfLow, please refer to the official documentation page: https://www.mlflow.org/docs/latest/tracking.html#logging-functions. </p>
			<p class="callout-heading">MLflow Integrations</p>
			<p class="callout">MLflow also supports integrations with many well-known open source packages, including (but not limited to) scikit-learn, TensorFlow, XGBoost, PyTorch, and Spark. You can do automatic logging by utilizing the provided integrations. For more information, please <a id="_idIndexMarker688"/>refer to the official documentation page: https://www.mlflow.org/docs/latest/tracking.html#automatic-logging. </p>
			<p class="callout-heading">Examples of Hyperparameter Tuning Use Cases</p>
			<p class="callout">The author of MLflow has provided example code for hyperparameter tuning use cases. For more information, please refer to the official GitHub repository: https://github.com/mlflow/mlflow/tree/master/examples/hyperparam. </p>
			<p>In this section, you learned <a id="_idIndexMarker689"/>how to utilize the MLflow package to track your hyperparameter tuning experiments. You can start exploring this package by yourself to get a better understanding of how this package works and how powerful it is.</p>
			<h1 id="_idParaDest-130"><a id="_idTextAnchor136"/>Summary</h1>
			<p>In this chapter, we discussed the importance of tracking hyperparameter tuning experiments, along with the usual practices. You were also introduced to several open source packages that are available and learned how to utilize each of them in practice, including Neptune, Scikit-Optimize, Optuna, Microsoft NNI, and MLflow. At this point, you should be able to utilize your favorite package to track your hyperparameter tuning experiment, which will boost the effectiveness of your workflow.</p>
			<p>In the next chapter, we’ll conclude all the topics we have discussed throughout this book. We’ll also discuss the next steps you can take to expand your hyperparameter tuning knowledge.</p>
		</div>
		<div>
			<div id="_idContainer366">
			</div>
		</div>
	</body></html>