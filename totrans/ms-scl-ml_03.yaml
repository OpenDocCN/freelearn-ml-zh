- en: Chapter 3. Working with Spark and MLlib
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we are powered with the knowledge of where and how statistics and machine
    learning fits in the global data-driven enterprise architecture, let's stop at
    the specific implementations in Spark and MLlib, a machine learning library on
    top of Spark. Spark is a relatively new member of the big data ecosystem that
    is optimized for memory usage as opposed to disk. The data can be still spilled
    to disk as necessary, but Spark does the spill only if instructed to do so explicitly,
    or if the active dataset does not fit into the memory. Spark stores lineage information
    to recompute the active dataset if a node goes down or the information is erased
    from memory for some other reason. This is in contrast to the traditional MapReduce
    approach, where the data is persisted to the disk after each map or reduce task.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Spark is particularly suited for iterative or statistical machine learning algorithms
    over a distributed set of nodes and can scale out of core. The only limitation
    is the total memory and disk space available across all Spark nodes and the network
    speed. I will cover the basics of Spark architecture and implementation in this
    chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: One can direct Spark to execute data pipelines either on a single node or across
    a set of nodes with a simple change in the configuration parameters. Of course,
    this flexibility comes at a cost of slightly heavier framework and longer setup
    times, but the framework is very parallelizable and as most of modern laptops
    are already multithreaded and sufficiently powerful, this usually does not present
    a big issue.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Installing and configuring Spark if you haven't done so yet
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning the basics of Spark architecture and why it is inherently tied to the
    Scala language
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning why Spark is the next technology after sequential implementations and
    Hadoop MapReduce
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning about Spark components
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking at the simple implementation of word count in Scala and Spark
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking at the streaming word count implementation
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seeing how to create Spark DataFrames from either a distributed file or a distributed
    database
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning about Spark performance tuning
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up Spark
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you haven''t done so yet, you can download the pre-build Spark package from
    [http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html).
    The latest release at the time of writing is **1.6.1**:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![Setting up Spark](img/B04935_03_01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
- en: Figure 03-1\. The download site at http://spark.apache.org with recommended
    selections for this chapter
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, you can build the Spark by downloading the full source distribution
    from [https://github.com/apache/spark](https://github.com/apache/spark):'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The command will download the necessary dependencies and create the `spark-2.0.0-SNAPSHOT-bin-alex-spark-build-2.6-yarn.tgz`
    file in the Spark directory; the version is 2.0.0, as it is the next release version
    at the time of writing. In general, you do not want to build from trunk unless
    you are interested in the latest features. If you want a released version, you
    can checkout the corresponding tag. Full list of available versions is available
    via the `git branch –r` command. The `spark*.tgz` file is all you need to run
    Spark on any machine that has Java JRE.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令将下载必要的依赖项，并在Spark目录中创建`spark-2.0.0-SNAPSHOT-bin-alex-spark-build-2.6-yarn.tgz`文件；版本号为2.0.0，因为它是在撰写本文时的下一个发布版本。通常情况下，除非你对最新功能感兴趣，否则你不想从主干分支构建。如果你想获取发布版本，可以检出相应的标签。可通过`git
    branch –r`命令查看可用的完整版本列表。`spark*.tgz`文件是你在任何安装了Java JRE的机器上运行Spark所需的所有内容。
- en: The distribution comes with the `docs/building-spark.md` document that describes
    other options for building Spark and their descriptions, including incremental
    Scala compiler, zinc. Full Scala 2.11 support is in the works for the next Spark
    2.0.0 release.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 伴随`docs/building-spark.md`文档的分布，该文档描述了构建Spark的其他选项及其描述，包括增量Scala编译器zinc。下一个Spark
    2.0.0版本的发布正在努力实现对Scala 2.11的全支持。
- en: Understanding Spark architecture
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Spark架构
- en: A parallel execution involves splitting the workload into subtasks that are
    executed in different threads or on different nodes. Let's see how Spark does
    this and how it manages execution and communication between the subtasks.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 并行执行涉及将工作负载拆分为在不同线程或不同节点上执行的子任务。让我们看看Spark是如何做到这一点的，以及它是如何管理子任务之间的执行和通信的。
- en: Task scheduling
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 任务调度
- en: 'Spark workload splitting is determined by the number of partitions for **Resilient
    Distributed Dataset** (**RDD**), the basic abstraction in Spark, and the pipeline
    structure. An RDD represents an immutable, partitioned collection of elements
    that can be operated on in parallel. While the specifics might depend on the mode
    in which Spark runs, the following diagram captures the Spark task/resource scheduling:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Spark工作负载拆分由Spark的基本抽象**弹性分布式数据集**（**RDD**）的分区数量以及管道结构决定。RDD表示一个不可变、分区的元素集合，可以在并行操作上执行。虽然具体细节可能取决于Spark运行的模式，以下图表捕捉了Spark任务/资源调度的过程：
- en: '![Task scheduling](img/B04935_03_02.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![任务调度](img/B04935_03_02.jpg)'
- en: Figure 03-2\. A generic Spark task scheduling diagram. While not shown explicitly
    in the figure, Spark Context opens an HTTP UI, usually on port 4040 (the concurrent
    contexts will open 4041, 4042, and so on), which is present during a task execution.
    Spark Master UI is usually 8080 (although it is changed to 18080 in CDH) and Worker
    UI is usually 7078\. Each node can run multiple executors, and each executor can
    run multiple tasks.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-2。一个通用的Spark任务调度图。虽然图示中没有明确显示，但Spark Context会打开一个HTTP UI，通常在端口4040（并发上下文将打开4041、4042等），在任务执行期间存在。Spark
    Master UI通常为8080（尽管在CDH中已更改为18080），Worker UI通常为7078。每个节点可以运行多个执行器，每个执行器可以运行多个任务。
- en: Tip
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: You will find that Spark, as well as Hadoop, has a lot of parameters. Some of
    them are specified as environment variables (refer to the `$SPARK_HOME/conf/spark-env.sh`
    file), and yet some can be given as a command-line parameter. Moreover, some files
    with pre-defined names can contain parameters that will change the Spark behavior,
    such as `core-site.xml`. This might be confusing, and I will cover as much as
    possible in this and the following chapters. If you are working with **Hadoop
    Distributed File System** (**HDFS**), then the `core-site.xml` and `hdfs-site.xml`
    files will contain the pointer and specifications for the HDFS master. The requirement
    for picking this file is that it has to be on `CLASSPATH` Java process, which,
    again, may be set by either specifying `HADOOP_CONF_DIR` or `SPARK_CLASSPATH`
    environment variables. As is usual with open source, you need to grep the code
    sometimes to understand how various parameters work, so having a copy of the source
    tree on your laptop is a good idea.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Each node in the cluster can run one or more executors, and each executor can
    schedule a sequence of tasks to perform the Spark operations. Spark driver is
    responsible for scheduling the execution and works with the cluster scheduler,
    such as Mesos or YARN to schedule the available resources. Spark driver usually
    runs on the client machine, but in the latest release, it can also run in the
    cluster under the cluster manager. YARN and Mesos have the ability to dynamically
    manage the number of executors that run concurrently on each node, provided the
    resource constraints.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Standalone mode, **Spark Master** does the work of the cluster scheduler—it
    might be less efficient in allocating resources, but it''s better than nothing
    in the absence of preconfigured Mesos or YARN. Spark standard distribution contains
    shell scripts to start Spark in Standalone mode in the `sbin` directory. Spark
    Master and driver communicate directly with one or several Spark workers that
    run on individual nodes. Once the master is running, you can start Spark shell
    with the following command:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Tip
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that you can always run Spark in local mode, which means that all tasks
    will be executed in a single JVM, by specifying `--master local[2]`, where `2`
    is the number of threads that have to be at least `2`. In fact, we will be using
    the local mode very often in this book for running small examples.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark shell is an application from the Spark point of view. Once you start
    a Spark application, you will see it under **Running Applications** in the Spark
    Master UI (or in the corresponding cluster manager), which can redirect you to
    the Spark application HTTP UI at port 4040, where one can see the subtask execution
    timeline and other important properties such as environment setting, classpath,
    parameters passed to the JVM, and information on resource usage (refer to *Figure
    3-3*):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![Task scheduling](img/B04935_03_03.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: Figure 03-3\. Spark Driver UI in Standalone mode with time decomposition
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'As we saw, with Spark, one can easily switch between local and cluster mode
    by providing the `--master` command-line option, setting a `MASTER` environment
    variable, or modifying `spark-defaults.conf`, which should be on the classpath
    during the execution, or even set explicitly using the `setters` method on the
    `SparkConf` object directly in Scala, which will be covered later:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '| Cluster Manager | MASTER env variable | Comments |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
- en: '| Local (single node, multiple threads) | `local[n]` | *n* is the number of
    threads to use, should be greater than or equal to *2*. If you want Spark to communicate
    with other Hadoop tools such as Hive, you still need to point it to the cluster
    by either setting the `HADOOP_CONF_DIR` environment variable or copying the Hadoop
    `*-site.xml` configuration files into the `conf` subdirectory. |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
- en: '| Standalone (Daemons running on the nodes) | `spark:// master-address>:7077`
    | This has a set of start/stop scripts in the `$SPARK_HOME/sbin` directory. This
    also supports the HA mode. More details can be found at [https://spark.apache.org/docs/latest/spark-standalone.html](https://spark.apache.org/docs/latest/spark-standalone.html).
    |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
- en: '| Mesos | `mesos://host:5050` or `mesos://zk://host:2181`(multimaster) | Here,
    you need to set `MESOS_NATIVE_JAVA_LIBRARY=<path to libmesos.so>` and `SPARK_EXECUTOR_URI=<URL
    of spark-1.5.0.tar.gz>`. The default is fine-grained mode, where each Spark task
    runs as a separate Mesos task. Alternatively, the user can specify the coarse-grained
    mode, where the Mesos tasks persists for the duration of the application. The
    advantage is lower total start-up costs. This can use dynamic allocation (refer
    to the following URL) in coarse-grained mode. More details can be found at [https://spark.apache.org/docs/latest/running-on-mesos.html](https://spark.apache.org/docs/latest/running-on-mesos.html).
    |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
- en: '| YARN | `yarn` | Spark driver can run either in the cluster or on the client
    node, which is managed by the `--deploy-mode` parameter (cluster or client, shell
    can only run in the client mode). Set `HADOOP_CONF_DIR` or `YARN_CONF_DIR` to
    point to the YARN config files. Use the `--num-executors` flag or `spark.executor.instances`
    property to set a fixed number of executors (default).Set `spark.dynamicAllocation.enabled`
    to `true` to dynamically create/kill executors depending on the application demand.
    More details are available at [https://spark.apache.org/docs/latest/running-on-yarn.html](https://spark.apache.org/docs/latest/running-on-yarn.html).
    |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
- en: 'The most common ports are 8080, the master UI, and 4040, the application UI.
    Other Spark ports are summarized in the following table:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '| Standalone ports |   |   |   |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
- en: '| From | To | Default Port | Purpose | Configuration Setting |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
- en: '| Browser | Standalone Master | 8080 | Web UI | `spark.master.ui.port /SPARK_MASTER_WEBUI_PORT`
    |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
- en: '| Browser | Standalone worker | 8081 | Web UI | `spark.worker.ui.port /SPARK_WORKER_WEBUI_PORT`
    |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
- en: '| Driver / Standalone worker | Standalone Master | 7077 | Submit job to cluster
    / Join cluster | `SPARK_MASTER_PORT` |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
- en: '| Standalone master | Standalone worker | (random) | Schedule executors | `SPARK_WORKER_PORT`
    |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
- en: '| Executor / Standalone master | Driver | (random) | Connect to application
    / Notify executor state changes | `spark.driver.port` |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
- en: '| **Other ports** |   |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
- en: '| **From** | **To** | **Default Port** | **Purpose** | **Configuration Setting**
    |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
- en: '| Browser | Application | 4040 | Web UI | `spark.ui.port` |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
- en: '| Browser | History server | 18080 | Web UI | `spark.history.ui.port` |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
- en: '| Driver | Executor | (random) | Schedule tasks | `spark.executor.port` |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
- en: '| Executor | Driver | (random) | File server for files and jars | `spark.fileserver.port`
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
- en: '| Executor | Driver | (random) | HTTP broadcast | `spark.broadcast.port` |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
- en: Also, some of the documentation is available with the source distribution in
    the `docs` subdirectory, but may be out of date.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Spark components
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since the emergence of Spark, multiple applications that benefit from Spark''s
    ability to cache RDDs have been written: Shark, Spork (Pig on Spark), graph libraries
    (GraphX, GraphFrames), streaming, MLlib, and so on; some of these will be covered
    here and in later chapters.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, I will cover major architecture components to collect, store,
    and analyze the data in Spark. While I will cover a more complete data life cycle
    architecture in [Chapter 2](ch02.xhtml "Chapter 2. Data Pipelines and Modeling"),
    *Data Pipelines and Modeling*, here are Spark-specific components:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '![Spark components](img/B04935_03_04.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
- en: Figure 03-4\. Spark architecture and components.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: MQTT, ZeroMQ, Flume, and Kafka
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All of these are different ways to reliably move data from one place to another
    without loss and duplication. They usually implement a publish-subscribe model,
    where multiple writers and readers can write and read from the same queues with
    different guarantees. Flume stands out as a first distributed log and event management
    implementation, but it is slowly replaced by Kafka, a fully functional publish-subscribe
    distributed message queue optionally persistent across a distributed set of nodes
    developed at LinkedIn. We covered Flume and Kafka briefly in the previous chapter.
    Flume configuration is file-based and is traditionally used to deliver messages
    from a Flume source to one or several Flume sinks. One of the popular sources
    is `netcat`—listening on raw data over a port. For example, the following configuration
    describes an agent receiving data and then writing them to HDFS every 30 seconds
    (default):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This file is included as part of the code provided with this book in the `chapter03/conf`
    directory. Let''s download and start Flume agent (check the MD5 sum with one provided
    at [http://flume.apache.org/download.html](http://flume.apache.org/download.html)):'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, in a separate window, you can type a `netcat` command to send text to
    the Flume agent:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The Flume agent will first create a `*.tmp` file and then rename it to a file
    without extension (the file extension can be used to filter out files being written
    to):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, each row is a Unix time in milliseconds and data received. In this case,
    we put the data into HDFS, from where they can be analyzed by a Spark/Scala program,
    we can exclude the files being written to by the `*.tmp` filename pattern. However,
    if you are really interested in up-to-the-last-minute values, Spark, as well as
    some other platforms, supports streaming, which I will cover in a few sections.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: HDFS, Cassandra, S3, and Tachyon
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'HDFS, Cassandra, S3, and Tachyon are the different ways to get the data into
    persistent storage and compute nodes as necessary with different guarantees. HDFS
    is a distributed storage implemented as a part of Hadoop, which serves as the
    backend for many products in the Hadoop ecosystem. HDFS divides each file into
    blocks, which are 128 MB in size by default, and stores each block on at least
    three nodes. Although HDFS is reliable and supports HA, a general complain about
    HDFS storage is that it is slow, particularly for machine learning purposes. Cassandra
    is a general-purpose key/value storage that also stores multiple copies of a row
    and can be configured to support different levels of consistency to optimize read
    or write speeds. The advantage that Cassandra over HDFS model is that it does
    not have a central master node; the reads and writes are completed based on the
    consensus algorithm. This, however, may sometimes reflect on the Cassandra stability.
    S3 is the Amazon storage: The data is stored off-cluster, which affects the I/O
    speed. Finally, the recently developed Tachyon claims to utilize node''s memory
    to optimize access to working sets across the nodes.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, new backends are being constantly developed, for example, Kudu
    from Cloudera ([http://getkudu.io/kudu.pdf](http://getkudu.io/kudu.pdf)) and **Ignite
    File System** (**IGFS**) from GridGain ([http://apacheignite.gridgain.org/v1.0/docs/igfs)](http://apacheignite.gridgain.org/v1.0/docs/igfs).
    Both are open source and Apache-licensed.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Mesos, YARN, and Standalone
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we mentioned before, Spark can run under different cluster resource schedulers.
    These are various implementations to schedule Spark''s containers and tasks on
    the cluster. The schedulers can be viewed as cluster kernels, performing functions
    similar to the operating system kernel: resource allocation, scheduling, I/O optimization,
    application services, and UI.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Mesos is one of the original cluster managers and is built using the same principles
    as the Linux kernel, only at a different level of abstraction. A Mesos slave runs
    on every machine and provides API's for resource management and scheduling across
    entire datacenter and cloud environments. Mesos is written in C++.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: YARN is a more recent cluster manager developed by Yahoo. Each node in YARN
    runs a **Node Manager**, which communicates with the **Resource Manager** which
    may run on a separate node. The resource manager schedules the task to satisfy
    memory and CPU constraints. The Spark driver itself can run either in the cluster,
    which is called the cluster mode for YARN. Otherwise, in the client mode, only
    Spark executors run in the cluster and the driver that schedules Spark pipelines
    runs on the same machine that runs Spark shell or submit program. The Spark executors
    will talk to the local host over a random open port in this case. YARN is written
    in Java with the consequences of unpredictable GC pauses, which might make latency's
    long tail fatter.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Finally, if none of these resource schedulers are available, the standalone
    deployment mode starts a `org.apache.spark.deploy.worker.Worker` process on each
    node that communicates with the Spark Master process run as `org.apache.spark.deploy.master.Master`.
    The worker process is completely managed by the master and can run multiple executors
    and tasks (refer to *Figure 3-2*).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: In practical implementations, it is advised to track the program parallelism
    and required resources through driver's UI and adjust the parallelism and available
    memory, increasing the parallelism if necessary. In the following section, we
    will start looking at how Scala and Scala in Spark address different problems.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's consider a few practical examples and libraries in Spark/Scala starting
    with a very traditional problem of word counting.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Word count
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most modern machine learning algorithms require multiple passes over data. If
    the data fits in the memory of a single machine, the data is readily available
    and this does not present a performance bottleneck. However, if the data becomes
    too large to fit into RAM, one has a choice of either dumping pieces of the data
    on disk (or database), which is about 100 times slower, but has a much larger
    capacity, or splitting the dataset between multiple machines across the network
    and transferring the results. While there are still ongoing debates, for most
    practical systems, analysis shows that storing the data over a set of network
    connected nodes has a slight advantage over repeatedly storing and reading it
    from hard disks on a single node, particularly if we can split the workload effectively
    between multiple CPUs.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An average disk has bandwidth of about 100 MB/sec and transfers with a few mms
    latency, depending on the rotation speed and caching. This is about 100 times
    slower than reading the data from memory, depending on the data size and caching
    implementation again. Modern data bus can transfer data at over 10 GB/sec. While
    the network speed still lags behind the direct memory access, particularly with
    standard TCP/IP kernel networking layer overhead, specialized hardware can reach
    tens of GB/sec and if run in parallel, it can be potentially as fast as reading
    from the memory. In practice, the network-transfer speeds are somewhere between
    1 to 10 GB/sec, but still faster than the disk in most practical systems. Thus,
    we can potentially fit the data into combined memory of all the cluster nodes
    and perform iterative machine learning algorithms across a system of them.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'One problem with memory, however, is that it is does not persist across node
    failures and reboots. A popular big data framework, Hadoop, made possible with
    the help of the original Dean/Ghemawat paper (Jeff Dean and Sanjay Ghemawat, *MapReduce:
    Simplified Data Processing on Large Clusters*, OSDI, 2004.), is using exactly
    the disk layer persistence to guarantee fault tolerance and store intermediate
    results. A Hadoop MapReduce program would first run a `map` function on each row
    of a dataset, emitting one or more key/value pairs. These key/value pairs then
    would be sorted, grouped, and aggregated by key so that the records with the same
    key would end up being processed together on the same reducer, which might be
    running on same or another node. The reducer applies a `reduce` function that
    traverses all the values that were emitted for the same key and aggregates them
    accordingly. The persistence of intermediate results would guarantee that if a
    reducer fails for one or another reason, the partial computations can be discarded
    and the reduce computation can be restarted from the checkpoint-saved results.
    Many simple ETL-like applications traverse the dataset only once with very little
    information preserved as state from one record to another.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, one of the traditional applications of MapReduce is word count.
    The program needs to count the number of occurrences of each word in a document
    consisting of lines of text. In Scala, the word count is readily expressed as
    an application of the `foldLeft` method on a sorted list of words:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If I run this program, the output will be a list of (word, count) tuples. The
    program splits the lines into words, sorts the words, and then matches each word
    with the latest entry in the list of (word, count) tuples. The same computation
    in MapReduce would be expressed as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: First, we need to process each line of the text by splitting the line into words
    and generation `(word, 1)` pairs. This task is easily parallelized. Then, to parallelize
    the global count, we need to split the counting part by assigning a task to do
    the count for a subset of words. In Hadoop, we compute the hash of the word and
    divide the work based on the value of the hash.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Once the map task finds all the entries for a given hash, it can send the key/value
    pairs to the reducer, the sending part is usually called shuffle in MapReduce
    vernacular. A reducer waits until it receives all the key/value pairs from all
    the mappers, combines the values—a partial combine can also happen on the mapper,
    if possible—and computes the overall aggregate, which in this case is just sum.
    A single reducer will see all the values for a given word.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the log output of the word count operation in Spark (Spark is
    very verbose by default, you can manage the verbosity level by modifying the `conf/log4j.properties`
    file by replacing `INFO` with `ERROR` or `FATAL`):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'At this stage, the only thing that happened is metadata manipulations, Spark
    has not touched the data itself. Spark estimates that the size of the dataset
    and the number of partitions. By default, this is the number of HDFS blocks, but
    we can specify the minimum number of partitions explicitly with the `minPartitions`
    parameter:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We just defined another RDD derived from the original `linesRdd`:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Word count over 2 GB of text data—40,291 lines and 353,087 words—took under
    a second to read, split, and group by words.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'With extended logging, you could see the following:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Spark opens a few ports to communicate with the executors and users
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark UI runs on port 4040 on `http://localhost:4040`
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can read the file either from local or distributed storage (HDFS, Cassandra,
    and S3)
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark will connect to Hive if Spark is built with Hive support
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark uses lazy evaluation and executes the pipeline only when necessary or
    when output is required
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark uses internal scheduler to split the job into tasks, optimize the execution,
    and execute the tasks
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The results are stored into RDDs, which can either be saved or brought into
    RAM of the node executing the shell with collect method
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The art of parallel performance tuning is to split the workload between different
    nodes or threads so that the overhead is relatively small and the workload is
    balanced.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Streaming word count
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spark supports listening on incoming streams, partitioning it, and computing
    aggregates close to real-time. Currently supported sources are Kafka, Flume, HDFS/S3,
    Kinesis, Twitter, as well as the traditional MQs such as ZeroMQ and MQTT. In Spark,
    streaming is implemented as micro-batches. Internally, Spark divides input data
    into micro-batches, usually from subseconds to minutes in size and performs RDD
    aggregation operations on these micro-batches.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s extend the Flume example that we covered earlier. We''ll
    need to modify the Flume configuration file to create a Spark polling sink. Instead
    of HDFS, replace the sink section:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, instead of writing to HDFS, Flume will wait for Spark to poll for data:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'To run the program, start the Flume agent in one window:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then run the `FlumeWordCount` object in another:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, any text typed to the `netcat` connection will be split into words and
    counted every two seconds for a six second sliding window:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Spark/Scala allows to seamlessly switch between the streaming sources. For
    example, the same program for Kafka publish/subscribe topic model looks similar
    to the following:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To start the Kafka broker, first download the latest binary distribution and
    start ZooKeeper. ZooKeeper is a distributed-services coordinator and is required
    by Kafka even in a single-node deployment:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In another window, start the Kafka server:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Run the `KafkaWordCount` object:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, publishing the stream of words into the Kafka topic will produce the window
    counts:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As you see, the programs output every two seconds. Spark streaming is sometimes
    called **micro-batch processing**. Streaming has many other applications (and
    frameworks), but this is too big of a topic to be entirely considered here and
    needs to be covered separately. I'll cover some ML on streams of data in [Chapter
    5](ch05.xhtml "Chapter 5. Regression and Classification"), *Regression and Classification*.
    Now, let's get back to more traditional SQL-like interfaces.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Spark SQL and DataFrame
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DataFrame was a relatively recent addition to Spark, introduced in version 1.3,
    allowing one to use the standard SQL language for data analysis. We already used
    some SQL commands in [Chapter 1](ch01.xhtml "Chapter 1. Exploratory Data Analysis"),
    *Exploratory Data Analysis* for the exploratory data analysis. SQL is really great
    for simple exploratory analysis and data aggregations.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: According to the latest poll results, about 70% of Spark users use DataFrame.
    Although DataFrame recently became the most popular framework for working with
    tabular data, it is a relatively heavyweight object. The pipelines that use DataFrames
    may execute much slower than the ones that are based on Scala's vector or LabeledPoint,
    which will be discussed in the next chapter. The evidence from different developers
    is that the response times can be driven to tens or hundreds of milliseconds depending
    on the query, from submillisecond on simpler objects.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark implements its own shell for SQL, which can be invoked in addition to
    the standard Scala REPL shell: `./bin/spark-sql` can be used to access the existing
    Hive/Impala or relational DB tables:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In standard Spark''s REPL, the same query can be performed by running the following
    command:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ML libraries
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark, particularly with memory-based storage systems, claims to substantially
    improve the speed of data access within and between nodes. ML seems to be a natural
    fit, as many algorithms require multiple passes over the data, or repartitioning.
    MLlib is the open source library of choice, although private companies are catching,
    up with their own proprietary implementations.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'As I will chow in [Chapter 5](ch05.xhtml "Chapter 5. Regression and Classification"),
    *Regression and Classification*, most of the standard machine learning algorithms
    can be expressed as an optimization problem. For example, classical linear regression
    minimizes the sum of squares of *y* distance between the regression line and the
    actual value of *y*:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '![ML libraries](img/B04935_03_01F.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![ML libraries](img/B04935_03_02F.jpg) are the predicted values according
    to the linear expression:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '![ML libraries](img/B04935_03_03F.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
- en: '*A* is commonly called the slope, and *B* the intercept. In a more generalized
    formulation, a linear optimization problem is to minimize an additive function:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '![ML libraries](img/B04935_03_04F.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![ML libraries](img/B04935_03_05F.jpg) is a loss function and ![ML libraries](img/B04935_03_06F.jpg)
    is a regularization function. The regularization function is an increasing function
    of model complexity, for example, the number of parameters (or a natural logarithm
    thereof). Most common loss functions are given in the following table:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Loss function L | Gradient |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: '| Linear | ![ML libraries](img/B04935_03_07F.jpg) | ![ML libraries](img/B04935_03_08F.jpg)
    |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
- en: '| Logistic | ![ML libraries](img/B04935_03_09F.jpg) | ![ML libraries](img/B04935_03_10F.jpg)
    |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
- en: '| Hinge | ![ML libraries](img/B04935_03_11F.jpg) | ![ML libraries](img/B04935_03_13F.jpg)
    |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
- en: 'The purpose of the regularizer is to penalize more complex models to avoid
    overfitting and improve generalization error: more MLlib currently supports the
    following regularizers:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Regularizer R | Gradient |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
- en: '| L2 | ![ML libraries](img/B04935_03_14F.jpg) | ![ML libraries](img/B04935_03_15F.jpg)
    |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
- en: '| L1 | ![ML libraries](img/B04935_03_16F.jpg) | ![ML libraries](img/B04935_03_17F.jpg)
    |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
- en: '| Elastic net | ![ML libraries](img/B04935_03_18F.jpg) | ![ML libraries](img/B04935_03_19F.jpg)
    |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
- en: Here, *sign(w)* is the vector of the signs of all entries of *w*.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, MLlib includes implementation of the following algorithms:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'Basic statistics:'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary statistics
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Correlations
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Stratified sampling
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Hypothesis testing
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming significance testing
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Random data generation
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Classification and regression:'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear models (SVMs, logistic regression, and linear regression)
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Naive Bayes
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensembles of trees (Random Forests and Gradient-Boosted Trees)
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Isotonic regression
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Collaborative filtering:'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alternating least squares** (**ALS**)'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clustering:'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k-means
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaussian mixture
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Power Iteration Clustering** (**PIC**)'
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Latent Dirichlet allocation** (**LDA**)'
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Bisecting k-means
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming k-means
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dimensionality reduction:'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Singular Value Decomposition** (**SVD**)'
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Principal Component Analysis** (**PCA**)'
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature extraction and transformation
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frequent pattern mining:'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'FP-growth     Association rules'
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: PrefixSpan
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Optimization:'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stochastic Gradient Descent** (**SGD**)'
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limited-Memory BFGS** (**L-BFGS**)'
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: I will go over some of the algorithms in [Chapter 5](ch05.xhtml "Chapter 5. Regression
    and Classification"), *Regression and Classification*. More complex non-structured
    machine learning methods will be considered in [Chapter 6](ch06.xhtml "Chapter 6. Working
    with Unstructured Data"), *Working with Unstructured Data*.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: SparkR
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: R is an implementation of popular S programming language created by John Chambers
    while working at Bell Labs. R is currently supported by the **R Foundation for
    Statistical Computing**. R's popularity has increased in recent years according
    to polls. SparkR provides a lightweight frontend to use Apache Spark from R. Starting
    with Spark 1.6.0, SparkR provides a distributed DataFrame implementation that
    supports operations such as selection, filtering, aggregation, and so on, which
    is similar to R DataFrames, dplyr, but on very large datasets. SparkR also supports
    distributed machine learning using MLlib.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: SparkR required R version 3 or higher, and can be invoked via the `./bin/sparkR`
    shell. I will cover SparkR in [Chapter 8](ch08.xhtml "Chapter 8. Integrating Scala
    with R and Python"), *Integrating Scala with R and Python*.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Graph algorithms – GraphX and GraphFrames
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Graph algorithms are one of the hardest to correctly distribute between nodes,
    unless the graph itself is naturally partitioned, that is, it can be represented
    by a set of disconnected subgraphs. Since the social networking analysis on a
    multi-million node scale became popular due to companies such as Facebook, Google,
    and LinkedIn, researches have been coming up with new approaches to formalize
    the graph representations, algorithms, and types of questions asked.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'GraphX is a modern framework for graph computations described in a 2013 paper
    (*GraphX: A Resilient Distributed Graph System on Spark* by Reynold Xin, Joseph
    Gonzalez, Michael Franklin, and Ion Stoica, GRADES (SIGMOD workshop), 2013). It
    has graph-parallel frameworks such as Pregel, and PowerGraph as predecessors.
    The graph is represented by two RDDs: one for vertices and another one for edges.
    Once the RDDs are joined, GraphX supports either Pregel-like API or MapReduce-like
    API, where the map function is applied to the node''s neighbors and reduce is
    the aggregation step on top of the map results.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of writing, GraphX includes the implementation for the following
    graph algorithms:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: PageRank
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connected components
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Triangle counting
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Label propagation
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVD++ (collaborative filtering)
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strongly connected components
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As GraphX is an open source library, changes to the list are expected. GraphFrames
    is a new implementation from Databricks that fully supports the following three
    languages: Scala, Java, and Python, and is build on top of DataFrames. I''ll discuss
    specific implementations in [Chapter 7](ch07.xhtml "Chapter 7. Working with Graph
    Algorithms"), *Working with Graph Algorithms*.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Spark performance tuning
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While efficient execution of the data pipeline is prerogative of the task scheduler,
    which is part of the Spark driver, sometimes Spark needs hints. Spark scheduling
    is primarily driven by the two parameters: CPU and memory. Other resources, such
    as disk and network I/O, of course, play an important part in Spark performance
    as well, but neither Spark, Mesos or YARN can currently do anything to actively
    manage them.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: The first parameter to watch is the number of RDD partitions, which can be specified
    explicitly when reading the RDD from a file. Spark usually errs on the side of
    too many partitions as it provides more parallelism, and it does work in many
    cases as the task setup/teardown times are relatively small. However, one might
    experiment with decreasing the number of partitions, especially if one does aggregations.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: The default number of partitions per RDD and the level of parallelism is determined
    by the `spark.default.parallelism` parameter, defined in the `$SPARK_HOME/conf/spark-defaults.conf`
    configuration file. The number of partitions for a specific RDD can also be explicitly
    changed by the `coalesce()` or `repartition()` methods.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: The total number of cores and available memory is often the reason for deadlocks
    as the tasks cannot proceed further. One can specify the number of cores for each
    executor with the `--executor-cores` flag when invoking spark-submit, spark-shell,
    or PySpark from the command line. Alternatively, one can set the corresponding
    parameters in the `spark-defaults.conf` file discussed earlier. If the number
    of cores is set too high, the scheduler will not be able to allocate resources
    on the nodes and will deadlock.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: In a similar way, `--executor-memory` (or the `spark.executor.memory` property)
    specifies the requested heap size for all the tasks (the default is 1g). If the
    executor memory is specified too high, again, the scheduler may be deadlocked
    or will be able to schedule only a limited number of executors on a node.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'The implicit assumption in Standalone mode when counting the number of cores
    and memory is that Spark is the only running application—which may or may not
    be true. When running under Mesos or YARN, it is important to configure the cluster
    scheduler that it has the resources available to schedule the executors requested
    by the Spark Driver. The relevant YARN properties are: `yarn.nodemanager.resource.cpu-vcores`
    and `yarn.nodemanager.resource.memory-mb`. YARN may round the requested memory
    up a little. YARN''s `yarn.scheduler.minimum-allocation-mb` and `yarn.scheduler.increment-allocation-mb`
    properties control the minimum and increment request values respectively.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: JVMs can also use some memory off heap, for example, for interned strings and
    direct byte buffers. The value of the `spark.yarn.executor.memoryOverhead` property
    is added to the executor memory to determine the full memory request to YARN for
    each executor. It defaults to max (*384, .07 * spark.executor.memory*).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Since Spark can internally transfer the data between executors and client node,
    efficient serialization is very important. I will consider different serialization
    frameworks in [Chapter 6](ch06.xhtml "Chapter 6. Working with Unstructured Data"),
    *Working with Unstructured Data*, but Spark uses Kryo serialization by default,
    which requires the classes to be registered explicitly in a static method. If
    you see a serialization error in your code, it is likely because the corresponding
    class has not been registered or Kryo does not support it, as it happens with
    too nested and complex data types. In general, it is recommended to avoid complex
    objects to be passed between the executors unless the object serialization can
    be done very efficiently.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'Driver has similar parameters: `spark.driver.cores`, `spark.driver.memory`,
    and `spark.driver.maxResultSize`. The latter one sets the limit for the results
    collected from all the executors with the `collect` method. It is important to
    protect the driver process from out-of-memory exceptions. The other way to avoid
    out-of-memory exceptions and consequent problems are to either modify the pipeline
    to return aggregated or filtered results or use the `take` method instead.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Running Hadoop HDFS
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A distributed processing framework wouldn't be complete without distributed
    storage. One of them is HDFS. Even if Spark is run on local mode, it can still
    use a distributed file system at the backend. Like Spark breaks computations into
    subtasks, HDFS breaks a file into blocks and stores them across a set of machines.
    For HA, HDFS stores multiple copies of each block, the number of copies is called
    replication level, three by default (refer to *Figure 3-5*).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '**NameNode** is managing the HDFS storage by remembering the block locations
    and other metadata such as owner, file permissions, and block size, which are
    file-specific. **Secondary Namenode** is a slight misnomer: its function is to
    merge the metadata modifications, edits, into fsimage, or a file that serves as
    a metadata database. The merge is required, as it is more practical to write modifications
    of fsimage to a separate file instead of applying each modification to the disk
    image of the fsimage directly (in addition to applying the corresponding changes
    in memory). Secondary **Namenode** cannot serve as a second copy of the **Namenode**.
    A **Balancer** is run to move the blocks to maintain approximately equal disk
    usage across the servers—the initial block assignment to the nodes is supposed
    to be random, if enough space is available and the client is not run within the
    cluster. Finally, the **Client** communicates with the **Namenode** to get the
    metadata and block locations, but after that, either reads or writes the data
    directly to the node, where a copy of the block resides. The client is the only
    component that can be run outside the HDFS cluster, but it needs network connectivity
    with all the nodes in the cluster.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'If any of the node dies or disconnects from the network, the **Namenode** notices
    the change, as it constantly maintains the contact with the nodes via heartbeats.
    If the node does not reconnect to the **Namenode** within 10 minutes (by default),
    the **Namenode** will start replicating the blocks in order to achieve the required
    replication level for the blocks that were lost on the node. A separate block
    scanner thread in the **Namenode** will scan the blocks for possible bit rot—each
    block maintains a checksum—and will delete corrupted and orphaned blocks:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '![Running Hadoop HDFS](img/B04935_03_05.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
- en: Figure 03-5\. This is the HDFS architecture. Each block is stored in three separate
    locations (the replication level).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'To start HDFS on your machine (with replication level 1), download a Hadoop
    distribution, for example, from [http://hadoop.apache.org](http://hadoop.apache.org):'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'To get the minimal HDFS configuration, modify the `core-site.xml` and `hdfs-site.xml`
    files, as follows:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This will put the Hadoop HDFS metadata and data directories under the `/tmp/hadoop-$USER`
    directories. To make this more permanent, we can add the `dfs.namenode.name.dir`,
    `dfs.namenode.edits.dir`, and `dfs.datanode.data.dir` parameters, but we will
    leave these out for now. For a more customized distribution, one can download
    a Cloudera version from [http://archive.cloudera.com/cdh](http://archive.cloudera.com/cdh).
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'First, we need to write an empty metadata:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then start the `namenode`, `secondarynamenode`, and `datanode` Java processes
    (I usually open three different command-line windows to see the logs, but in a
    production environment, these are usually daemonized):'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We are now ready to create the first HDFS file:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Of course, in this particular case, the actual file is stored only on one node,
    which is the same node we run `datanode` on (localhost). In my case, it is the
    following:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The Namenode UI can be found at `http://localhost:50070` and displays a host
    of information, including the HDFS usage and the list of DataNodes, the slaves
    of the HDFS Master node as follows:![Running Hadoop HDFS](img/B04935_03_06.jpg)
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 03-6\. A snapshot of HDFS NameNode UI.
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The preceding figure shows HDFS Namenode HTTP UI in a single node deployment
    (usually, `http://<namenode-address>:50070`). The **Utilities** | **Browse the
    file system** tab allows you to browse and download the files from HDFS. Nodes
    can be added by starting DataNodes on a different node and pointing to the Namenode
    with the `fs.defaultFS=<namenode-address>:8020` parameter. The Secondary Namenode
    HTTP UI is usually at `http:<secondarynamenode-address>:50090`.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Scala/Spark by default will use the local file system. However, if the `core-site/xml`
    file is on the classpath or placed in the `$SPARK_HOME/conf` directory, Spark
    will use HDFS as the default.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, I covered the Spark/Hadoop and their relationship with Scala
    and functional programming at a very high level. I considered a classic word count
    example and it's implementation in Scala and Spark. I also provided high-level
    components of Spark ecosystem with specific examples of word count and streaming.
    I now have all the components to start looking at the specific implementation
    of classic machine learning algorithms in Scala/Spark. In the next chapter, I
    will start by covering supervised and unsupervised learning—a traditional division
    of learning algorithms for structured data.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
