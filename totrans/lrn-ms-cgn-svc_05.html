<html><head></head><body><div class="chapter" title="Chapter&#xA0;5.&#xA0;Speaking with Your Application"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Speaking with Your Application</h1></div></div></div><p>In the previous chapter, we learned how to discover and understand the intent of a user, based on utterances. In this chapter, we will learn how to add audio capabilities to our applications, convert text to speech and speech to text, and learn how to identify the person speaking. Throughout this chapter, we will learn how you can utilize spoken audio to verify a person. Finally, we will briefly touch on how to customize speech recognition to make it unique for your application's usage.</p><p>By the end of this chapter, we will have covered the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Converting spoken audio to text and text to spoken audio</li><li class="listitem" style="list-style-type: disc">Recognizing intent from spoken audio by utilizing LUIS</li><li class="listitem" style="list-style-type: disc">Verifying that the speaker is who they claim to be</li><li class="listitem" style="list-style-type: disc">Identifying the speaker</li><li class="listitem" style="list-style-type: disc">Tailoring the Speaker Recognition API to recognize custom speaking styles and environments</li></ul></div><div class="section" title="Converting text to audio and vice versa"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec29"/>Converting text to audio and vice versa</h1></div></div></div><p>In <a class="link" href="ch01.html" title="Chapter 1. Getting Started with Microsoft Cognitive Services">Chapter 1</a>, <span class="emphasis"><em>Getting Started with Microsoft Cognitive Services</em></span>, we utilized a part of the Bing Speech API. We gave the <a class="indexterm" id="id294"/>example application the ability to say <a class="indexterm" id="id295"/>sentences to us. We will use the code that we created in that example now, but we will dive a bit deeper into the details.</p><p>We will also go through<a class="indexterm" id="id296"/> the other feature of Bing <a class="indexterm" id="id297"/>Speech API, that is, converting spoken audio to text. The idea is that we can speak to the smart-house application, which will recognize what we are saying. Using the textual output, the application will use LUIS to gather the intent of our sentence. If LUIS needs more information, the application will politely ask us for more via audio.</p><p>To get started, we want to modify the build definition of the smart-house application. We need to specify whether we<a class="indexterm" id="id298"/> are running it on a 32-bit or 64-bit OS. To utilize<a class="indexterm" id="id299"/> speech-to-text conversion, we want to install the Bing Speech NuGet client package. Search for <code class="literal">Microsoft.ProjectOxford.SpeechRecognition</code> and install either the 32-bit version or the 64-bit version, depending on your system.</p><p>Further on, we need to add references to <code class="literal">System.Runtime.Serialization</code> and <code class="literal">System.Web</code>. These are needed so that we are able to make web requests and deserialize response data from the APIs.</p><div class="section" title="Speaking to the application"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec39"/>Speaking to the application</h2></div></div></div><p>Add a new file<a class="indexterm" id="id300"/> to the <code class="literal">Model</code> folder, called <code class="literal">SpeechToText.cs</code>. Beneath the automatically created <code class="literal">SpeechToText</code> class, we want to add an <code class="literal">enum</code> type variable called <code class="literal">SttStatus</code>. It should have two values, <code class="literal">Success</code> and <code class="literal">Error</code>.</p><p>In addition, we want to define an <code class="literal">EventArgs</code> class for events that we will raise during execution. Add the following class at the bottom of the file:</p><div class="informalexample"><pre class="programlisting">    public class SpeechToTextEventArgs : EventArgs
    {
        public SttStatus Status { get; private set; }
        public string Message { get; private set; }
        public List&lt;string&gt; Results { get; private set; }

        public SpeechToTextEventArgs(SttStatus status, 
        string message, List&lt;string&gt; results = null)
        {
            Status = status;
            Message = message;
            Results = results;
        }
    }</pre></div><p>As you can see, the <code class="literal">event</code> argument will hold the operation status, a message of any kind, and a list of strings. This will be a list with potential speech-to-text conversions.</p><p>The <code class="literal">SpeechToText</code> class needs to implement <code class="literal">IDisposable</code>. This is done so that we can clean up the resources used for recording spoken audio and shut down the application properly. We will add the details presently, so for now, just make sure to add the <code class="literal">Dispose</code> function.</p><p>Now, we need to define a few private members in the class, as well as an event:</p><div class="informalexample"><pre class="programlisting">    public event EventHandler&lt;SpeechToTextEventArgs&gt; OnSttStatusUpdated;

    private DataRecognitionClient _dataRecClient;
    private MicrophoneRecognitionClient _micRecClient;
    private SpeechRecognitionMode _speechMode = SpeechRecognitionMode.ShortPhrase;

    private string _language = "en-US";
    private bool _isMicRecording = false;</pre></div><p>The <code class="literal">OnSttStatusUpdated</code> event will be triggered whenever we have a new operation status. <code class="literal">DataRecognitionClient</code> and <code class="literal">MicrophoneRecognitionClient</code> are the two objects that we can use to call the Bing Speech API. We will look at how they are created presently.</p><p>We define <code class="literal">SpeechRecognitionMode</code> as <code class="literal">ShortPhrase</code>. This means that we do not expect any<a class="indexterm" id="id301"/> spoken sentences longer than 15 seconds. The alternative is <code class="literal">LongDictation</code>, which means that we can convert spoken sentences to be up to 2 minutes long.</p><p>Finally, we specify the language to be English, and define a <code class="literal">bool</code> type variable, which indicates whether or not we are currently recording anything.</p><p>In our constructor, we accept the Bing Speech API key as a parameter. We will use this in the creation of our API clients:</p><div class="informalexample"><pre class="programlisting">    public SpeechToText(string bingApiKey)
    {
        _dataRecClient = SpeechRecognitionServiceFactory.CreateDataClientWithIntentUsingEndpointUrl(_language, bingApiKey, "LUIS_ROOT_URI");

        _micRecClient = SpeechRecognitionServiceFactory.CreateMicrophoneClient(_speechMode, _language, bingApiKey);

        Initialize();
    }</pre></div><p>As you can see, we create both <code class="literal">_dataRecClient</code> and <code class="literal">_micRecClient</code> by calling <code class="literal">SpeechRecognitionServiceFactory</code>. For the first client, we state that we want to use intent recognition as well. The parameters required are the language, Bing API key, the LUIS app ID, and the LUIS API key. By using a <code class="literal">DataRecognitionClient</code> object, we can upload audio files with speech.</p><p>By using <code class="literal">MicrophoneRecognitionClient</code>, we can use a microphone for real-time conversion. For this, we do not want intent detection, so we call <code class="literal">CreateMicrophoneClient</code>. In this case, we only need to specify the speech mode, the language, and the Bing Speech API key.</p><p>Before leaving the constructor, we call the <code class="literal">Initialize</code> function. In this, we subscribe to certain events <a class="indexterm" id="id302"/>on each of the clients:</p><div class="informalexample"><pre class="programlisting">    private void Initialize()
    {
        _micRecClient.OnMicrophoneStatus += OnMicrophoneStatus;
        _micRecClient.OnPartialResponseReceived += OnPartialResponseReceived;
        _micRecClient.OnResponseReceived += OnResponseReceived;
        _micRecClient.OnConversationError += OnConversationErrorReceived;

        _dataRecClient.OnIntent += OnIntentReceived;
        _dataRecClient.OnPartialResponseReceived +=
        OnPartialResponseReceived;
        _dataRecClient.OnConversationError += OnConversationErrorReceived;
        _dataRecClient.OnResponseReceived += OnResponseReceived;
    }</pre></div><p>As you can see, there are quite a few similarities between the two clients. The two differences are that <code class="literal">_dataRecClient</code> will get intents through the <code class="literal">OnIntent</code> event, and <code class="literal">_micRecClient</code> will get the microphone status through the <code class="literal">OnMicrophoneStatus</code> event.</p><p>We do not really care about partial responses. However, they may be useful in some cases, as they will continuously give the currently completed conversion:</p><div class="informalexample"><pre class="programlisting">    private void OnPartialResponseReceived(object sender, PartialSpeechResponseEventArgs e)
    {
        Debug.WriteLine($"Partial response received:{e.PartialResult}");
    } </pre></div><p>For our application, we will choose to output it to the debug console window. In this case, <code class="literal">PartialResult</code> is a string with the partially converted text:</p><div class="informalexample"><pre class="programlisting">private void OnMicrophoneStatus(object sender, MicrophoneEventArgs e)
{
    Debug.WriteLine($"Microphone status changed to recording: {e.Recording}");
}</pre></div><p>We do not care about the current microphone status, either. Again, we output the status to the debug console window.</p><p>Before moving on, add a helper function, called <code class="literal">RaiseSttStatusUpdated</code>. This should raise <code class="literal">OnSttStatusUpdated</code> when called.</p><p>When we are calling <code class="literal">_dataRecClient</code>, we may recognize intents from LUIS. In these cases, we want<a class="indexterm" id="id303"/> to raise an event, where we output the recognized intent. This is done with the following code:</p><div class="informalexample"><pre class="programlisting">private void OnIntentReceived(object sender, SpeechIntentEventArgs e)
{
    SpeechToTextEventArgs args = new SpeechToTextEventArgs(SttStatus.Success, $"Intent received: {e.Intent.ToString()}.\n Payload: {e.Payload}");
    RaiseSttStatusUpdated(args);
}</pre></div><p>We choose to print out intent information and the <code class="literal">Payload</code>. This is a string containing recognized entities, intents, and actions that are triggered from LUIS.</p><p>If any errors occur during the conversion, there are several things we will want to do. First and foremost, we want to stop any microphone recordings that may be running. There is really no point in trying to convert more in the current operation if it has failed:</p><div class="informalexample"><pre class="programlisting">    private void OnConversationErrorReceived(object sender, SpeechErrorEventArgs e)
    {
        if (_isMicRecording) StopMicRecording();</pre></div><p>We will create <code class="literal">StopMicRecording</code> presently.</p><p>In addition, we want to notify any subscribers that the conversion failed. In such cases, we want to give details about error codes and error messages:</p><div class="informalexample"><pre class="programlisting">        string message = $"Speech to text failed with status code:{e.SpeechErrorCode.ToString()}, and error message: {e.SpeechErrorText}";
  
        SpeechToTextEventArgs args = new SpeechToTextEventArgs(SttStatus.Error, message);

        RaiseSttStatusUpdated(args);
    }</pre></div><p>The <code class="literal">OnConversationError</code> event does, fortunately, provide us with detailed information about any errors.</p><p>Now, let's look at the <code class="literal">StopMicRecording</code> method:</p><div class="informalexample"><pre class="programlisting">    private void StopMicRecording()
    {
        _micRecClient.EndMicAndRecognition();
        _isMicRecording = false;
    }</pre></div><p>This is a simple function that calls <code class="literal">EndMicAndRecognition</code> on the <code class="literal">_micRecClient MicrophoneRecognitionClient</code> object. When this is called, we stop the client from recording.</p><p>The final event handler that we need to create is the <code class="literal">OnResponseReceived</code> handler. This will be triggered whenever we receive a complete, converted response from the service.</p><p>Again, we<a class="indexterm" id="id304"/> want to make sure we do not record any more if we are currently recording:</p><div class="informalexample"><pre class="programlisting">    private void OnResponseReceived(object sender, SpeechResponseEventArgs e)
    {
        if (_isMicRecording) StopMicRecording();</pre></div><p>The <code class="literal">SpeechResponseEventArgs</code> argument contains a <code class="literal">PhraseResponse</code> object. This contains an array of <code class="literal">RecognizedPhrase</code>, which we want to access. Each item in this array contains the confidence of correct conversion. It also contains the converted phrases as <code class="literal">DisplayText</code>. This uses inverse text normalization, proper capitalization, and punctuation, and it masks profanities with asterisks:</p><div class="informalexample"><pre class="programlisting">    RecognizedPhrase[] recognizedPhrases = e.PhraseResponse.Results;
    List&lt;string&gt; phrasesToDisplay = new List&lt;string&gt;();

    foreach(RecognizedPhrase phrase in recognizedPhrases)
    {
        phrasesToDisplay.Add(phrase.DisplayText);
    }</pre></div><p>We may also get the converted phrases in other formats, as described in the following table:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Format</p>
</th><th style="text-align: left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">LexicalForm</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is the raw, unprocessed recognition result.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">InverseTextNormalizationResult</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This displays phrases such as <span class="emphasis"><em>one two three four</em></span> as <span class="emphasis"><em>1234</em></span>, so it is ideal for usages such as <span class="emphasis"><em>go to second street</em></span>.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">MaskedInverseTextNormalizationResult</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Inverse text normalization and the profanity mask. No capitalization or punctuation is applied.</p>
</td></tr></tbody></table></div><p>For our use, we are just interested in the <code class="literal">DisplayText</code>. With a populated list of recognized phrases, we raise the status update event:</p><div class="informalexample"><pre class="programlisting">        SpeechToTextEventArgs args = new SpeechToTextEventArgs(SttStatus.Success, $"STT completed with status: {e.PhraseResponse.RecognitionStatus.ToString()}", phrasesToDisplay);

        RaiseSttStatusUpdated(args);
    }</pre></div><p>To be able to use this class, we need a couple of public functions so that we can start speech recognition:</p><div class="informalexample"><pre class="programlisting">    public void StartMicToText()
    {
        _micRecClient.StartMicAndRecognition();
        _isMicRecording = true;
    }</pre></div><p>The <code class="literal">StartMicToText</code> method will call the <code class="literal">StartMicAndRecognition</code> method on the <code class="literal">_micRecClient</code> object. This will allow us to use the microphone to convert spoken audio. This function will be our main way of accessing this API:</p><div class="informalexample"><pre class="programlisting">    public void StartAudioFileToText(string audioFileName) {
        using (FileStream fileStream = new FileStream(audioFileName, FileMode.Open, FileAccess.Read))
        {
            int bytesRead = 0;
            byte[] buffer = new byte[1024];</pre></div><p>The second function will require a<a class="indexterm" id="id305"/> filename for the audio file, with the audio we want to convert. We open the file, with read access, and are ready to read it:</p><div class="informalexample"><pre class="programlisting">    try {
        do {
            bytesRead = fileStream.Read(buffer, 0, buffer.Length);
            _dataRecClient.SendAudio(buffer, bytesRead);
        } while (bytesRead &gt; 0);
    }</pre></div><p>As long as we have data available, we read from the file. We will fill up the <code class="literal">buffer</code>, and call the <code class="literal">SendAudio</code> method. This will then trigger a recognition operation in the service.</p><p>If any exceptions occur, we make sure to output the exception message to a debug window. Finally, we need to call the <code class="literal">EndAudio</code> method so that the service does not wait for any more data:</p><div class="informalexample"><pre class="programlisting">    catch(Exception ex) {
        Debug.WriteLine($"Exception caught: {ex.Message}");
    }
    finally {
        _dataRecClient.EndAudio();
    }</pre></div><p>Before leaving this class, we need to dispose of our API clients. Add the following in the <code class="literal">Dispose</code> function:</p><div class="informalexample"><pre class="programlisting">    if (_micRecClient != null) {
        _micRecClient.EndMicAndRecognition();
        _micRecClient.OnMicrophoneStatus -= OnMicrophoneStatus;
        _micRecClient.OnPartialResponseReceived -= OnPartialResponseReceived;
        _micRecClient.OnResponseReceived -= OnResponseReceived;
        _micRecClient.OnConversationError -= OnConversationErrorReceived;

       _micRecClient.Dispose();
       _micRecClient = null;
    }

    if(_dataRecClient != null) {
        _dataRecClient.OnIntent -= OnIntentReceived;
        _dataRecClient.OnPartialResponseReceived -= OnPartialResponseReceived;
        _dataRecClient.OnConversationError -= OnConversationErrorReceived;
        _dataRecClient.OnResponseReceived -= OnResponseReceived;

        _dataRecClient.Dispose();
        _dataRecClient = null;
    }</pre></div><p>We stop microphone<a class="indexterm" id="id306"/> recording, unsubscribe from all events, and dispose and clear the client objects.</p><p>Make sure that the application compiles before moving on. We will look at how to use this class presently.</p></div><div class="section" title="Letting the application speak back"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec40"/>Letting the application speak back</h2></div></div></div><p>We have already<a class="indexterm" id="id307"/> seen how to make the application speak back to us. We are going to use the same classes we created in <a class="link" href="ch01.html" title="Chapter 1. Getting Started with Microsoft Cognitive Services">Chapter 1</a>, <span class="emphasis"><em>Getting Started with Microsoft Cognitive Services</em></span>. Copy <code class="literal">Authentication.cs</code> and <code class="literal">TextToSpeech.cs</code> from the example project from <a class="link" href="ch01.html" title="Chapter 1. Getting Started with Microsoft Cognitive Services">Chapter 1</a>, <span class="emphasis"><em>Getting Started with Microsoft Cognitive Services,</em></span> into the <code class="literal">Model</code> folder. Make sure that the namespaces are changed accordingly.</p><p>As we have been through the code already, we will not go through it again. We will instead look at some <a class="indexterm" id="id308"/>of the details left out in <a class="link" href="ch01.html" title="Chapter 1. Getting Started with Microsoft Cognitive Services">Chapter 1</a>, <span class="emphasis"><em>Getting Started with Microsoft Cognitive Services</em></span>.</p><div class="section" title="Audio output format"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec43"/>Audio output format</h3></div></div></div><p>The audio output <a class="indexterm" id="id309"/>format can be one of the following formats:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">raw-8khz-8bit-mono-mulaw</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">raw-16khz-16bit-mono-pcm</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">riff-8khz-8bit-mono-mulaw</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">riff-16khz-16bit-mono-pcm</code></li></ul></div></div><div class="section" title="Error codes"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec44"/>Error codes</h3></div></div></div><p>There are four possible <a class="indexterm" id="id310"/>error codes that can occur in calls to the API. These are described in the following table:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Code</p>
</th><th style="text-align: left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">400 / BadRequest</code>
</p>
</td><td style="text-align: left" valign="top">
<p>A required parameter is missing, empty, or null. Alternatively, a parameter is invalid. An example may be a string that's longer than the allowed length.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">401 / Unauthorized</code>
</p>
</td><td style="text-align: left" valign="top">
<p>The request is not authorized.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">413 / RequestEntityTooLarge</code>
</p>
</td><td style="text-align: left" valign="top">
<p>The SSML input is larger than what's supported.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">502 / BadGateway</code>
</p>
</td><td style="text-align: left" valign="top">
<p>A network-related or server-related issue.</p>
</td></tr></tbody></table></div></div><div class="section" title="Supported languages"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec45"/>Supported languages</h3></div></div></div><p>The following languages<a class="indexterm" id="id311"/> are supported:</p><p>English (Australia), English (United Kingdom), English (United States), English (Canada), English (India), Spanish, Mexican Spanish, German, Arabic (Egypt), French, Canadian French, Italian, Japanese, Portuguese, Russian, Chinese (S), Chinese (Hong Kong), and Chinese (T).</p></div></div><div class="section" title="Utilizing LUIS based on spoken commands"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec41"/>Utilizing LUIS based on spoken commands</h2></div></div></div><p>To utilize the features<a class="indexterm" id="id312"/> that we have just added, we are going to modify <code class="literal">LuisView</code> and <code class="literal">LuisViewModel</code>. Add a new <code class="literal">Button</code> in the View, which will make sure that we record commands. Add a corresponding <code class="literal">ICommand</code> in the ViewModel.</p><p>We also need to add a few more members to the class:</p><div class="informalexample"><pre class="programlisting">    private SpeechToText _sttClient;
    private TextToSpeech _ttsClient;
    private string _bingApiKey = "BING_SPEECH_API_KEY";</pre></div><p>The first two will be used to convert between spoken audio and text. The third is the API key for the Bing Speech API.</p><p>Make the ViewModel implement <code class="literal">IDisposable</code>, and explicitly dispose the <code class="literal">SpeechToText</code> object.</p><p>Create the <a class="indexterm" id="id313"/>objects by adding the following in the constructor:</p><div class="informalexample"><pre class="programlisting">_sttClient = new SpeechToText(_bingApiKey);
_sttClient.OnSttStatusUpdated += OnSttStatusUpdated;

_ttsClient = new TextToSpeech();
_ttsClient.OnAudioAvailable += OnTtsAudioAvailable;
_ttsClient.OnError += OnTtsError;
GenerateHeaders();</pre></div><p>This will create the client objects and subscribe to the required events. Finally, it will call a function to generate authentication tokens for the REST API calls. This function should look like this:</p><div class="informalexample"><pre class="programlisting">private async void GenerateHeaders()
{
   if (await _ttsClient.GenerateAuthenticationToken(_bingApiKey))
   _ttsClient.GenerateHeaders();
}</pre></div><p>If we receive any errors from <code class="literal">_ttsClient</code>, we want to output it to the debug console:</p><div class="informalexample"><pre class="programlisting">    private void OnTtsError(object sender, AudioErrorEventArgs e)
    {
        Debug.WriteLine($"Status: Audio service failed - {e.ErrorMessage}");
    }</pre></div><p>We do not need to output this to the UI, as this is a nice-to-have feature.</p><p>If we have audio available, we want to make sure that we play it. We do so by creating a <code class="literal">SoundPlayer</code> object:</p><div class="informalexample"><pre class="programlisting">    private void OnTtsAudioAvailable(object sender, AudioEventArgs e)
    {
        SoundPlayer player = new SoundPlayer(e.EventData);
        player.Play();
        e.EventData.Dispose();
    }</pre></div><p>Using the <a class="indexterm" id="id314"/>audio stream we got from the event arguments, we can play the audio to the user.</p><p>If we have a status update from <code class="literal">_sttClient</code>, we want to display this in the textbox.</p><p>If we have successfully recognized spoken audio, we want to show the <code class="literal">Message</code> string if it is available:</p><div class="informalexample"><pre class="programlisting">    private void OnSttStatusUpdated(object sender, SpeechToTextEventArgs e) {
        Application.Current.Dispatcher.Invoke(() =&gt;  {
            StringBuilder sb = new StringBuilder();

            if(e.Status == SttStatus.Success) {
               if(!string.IsNullOrEmpty(e.Message)) {
                   sb.AppendFormat("Result message: {0}\n\n", e.Message);
                }</pre></div><p>We also want to show all recognized phrases. Using the first available phrase, we make a call to LUIS:</p><div class="informalexample"><pre class="programlisting">        if(e.Results != null &amp;&amp; e.Results.Count != 0) {
            sb.Append("Retrieved the following results:\n");
                foreach(string sentence in e.Results) {
                    sb.AppendFormat("{0}\n\n", sentence);
                }
                sb.Append("Calling LUIS with the top result\n");
                CallLuis(e.Results.FirstOrDefault());
            }
        }</pre></div><p>If the recognition failed, we print out any error messages that we may have. Finally, we make sure that the <code class="literal">ResultText</code> is updated with the new data:</p><div class="informalexample"><pre class="programlisting">            else {
                sb.AppendFormat("Could not convert speech to text:{0}\n", e.Message);
            }

            sb.Append("\n");
            ResultText = sb.ToString();
        });
    }</pre></div><p>The newly created <code class="literal">ICommand</code> needs to have a function to start the recognition process:</p><div class="informalexample"><pre class="programlisting">    private void RecordUtterance(object obj) {
        _sttClient.StartMicToText();
    }</pre></div><p>The function starts the microphone recording.</p><p>Finally, we <a class="indexterm" id="id315"/>need to make some modifications to <code class="literal">OnLuisUtteranceResultUpdated</code>. Make the following modifications, where we output any <code class="literal">DialogResponse</code>:</p><div class="informalexample"><pre class="programlisting">    if (e.RequiresReply &amp;&amp; !string.IsNullOrEmpty(e.DialogResponse))
    {
        await _ttsClient.SpeakAsync(e.DialogResponse, CancellationToken.None);
        sb.AppendFormat("Response: {0}\n", e.DialogResponse);
        sb.Append("Reply in the left textfield");
 
        RecordUtterance(sender);
    }
    else
    {
        await _ttsClient.SpeakAsync($"Summary: {e.Message}", CancellationToken.None);
    }</pre></div><p>This will play the <code class="literal">DialogResponse</code> if it exists. The application will ask you for more information if required. It will then start the recording, so we can answer without clicking any buttons.</p><p>If no <code class="literal">DialogResponse</code> exists, we simply make the application say the summary to us. This will contain data on intents, entities, and actions from LUIS.</p></div></div></div>
<div class="section" title="Knowing who is speaking"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec30"/>Knowing who is speaking</h1></div></div></div><p>Using the <span class="strong"><strong>Speaker Recognition</strong></span> API, we can identify who is speaking. By defining one or more speaker profiles with<a class="indexterm" id="id316"/> corresponding samples, we can identify whether any <a class="indexterm" id="id317"/>of them are speaking at any time.</p><p>To be able to utilize this feature, we need to go through a few steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">We need to add one or more speaker profiles to the service.</li><li class="listitem">Each speaker profile enrolls several spoken samples.</li><li class="listitem">We call the service to identify a speaker based on audio input.</li></ol></div><p>If you have not already done so, sign up for an<a class="indexterm" id="id318"/> API key for the Speaker Recognition API at <a class="ulink" href="https://portal.azure.com">https://portal.azure.com</a>.</p><p>Start by adding a new NuGet package to your smart-house application. Search for and add <code class="literal">Microsoft.ProjectOxford.SpeakerRecognition</code>.</p><p>Add a new class called <code class="literal">SpeakerIdentification</code> to the <code class="literal">Model</code> folder of your project. This class will hold all of the functionality related to speaker identification.</p><p>Beneath the class, we will add another class, containing <code class="literal">EventArgs</code> for status updates:</p><div class="informalexample"><pre class="programlisting">    public class SpeakerIdentificationStatusUpdateEventArgs : EventArgs
    {
        public string Status { get; private set; }
        public string Message { get; private set; }
        public Identification IdentifiedProfile { get; set; }

        public SpeakerIdentificationStatusUpdateEventArgs (string status, string message)
       { 
           Status = status;
           Message = message;
       }
    }</pre></div><p>The two first properties should be self-explanatory. The last one, <code class="literal">IdentificationProfile</code>, will hold the results<a class="indexterm" id="id319"/> of a successful identification process. We will look at what information this contains presently.</p><p>We also want to send<a class="indexterm" id="id320"/> events for errors, so let's add an <code class="literal">EventArgs</code> class for the required information:</p><div class="informalexample"><pre class="programlisting">    public class SpeakerIdentificationErrorEventArgs : EventArgs {
        public string ErrorMessage { get; private set; }

        public SpeakerIdentificationErrorEventArgs(string errorMessage)
        {
            ErrorMessage = errorMessage;
        }
    }</pre></div><p>Again, the property should be self-explanatory.</p><p>In the <code class="literal">SpeakerIdentification</code> class, add two events and one private member at the top of the class:</p><div class="informalexample"><pre class="programlisting">    public event EventHandler &lt;SpeakerIdentificationStatusUpdateEventArgs&gt;
        OnSpeakerIdentificationStatusUpdated;
    public event EventHandler &lt;SpeakerIdentificationErrorEventArgs&gt;
        OnSpeakerIdentificationError;

    private ISpeakerIdentificationServiceClient _speakerIdentificationClient;</pre></div><p>The events will <a class="indexterm" id="id321"/>be triggered if we have any status updates, a successful identification, or errors. The <code class="literal">ISpeakerIdentificationServiceClient</code> object is the access point for the Speaker Recognition API. Inject this object through the constructor.</p><p>To make it easier to<a class="indexterm" id="id322"/> raise events, add two helper functions, one for each event. Call these <code class="literal">RaiseOnIdentificationStatusUpdated</code> and <code class="literal">RaiseOnIdentificationError</code>. They should accept the corresponding <code class="literal">EventArgs</code> object as a parameter and trigger the corresponding event.</p><div class="section" title="Adding speaker profiles"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec42"/>Adding speaker profiles</h2></div></div></div><p>To be able to<a class="indexterm" id="id323"/> identify speakers, we need to add profiles. Each profile can be seen as a unique person who we can identify later.</p><p>At the time of writing, each subscription allows for 1,000 speaker profiles to be created. This also includes profiles that are created for verification, which we will look at presently.</p><p>To facilitate creating profiles, we need to add some elements to our <code class="literal">AdministrationView</code> and <code class="literal">AdministrationViewModel</code> properties, so open these files.</p><p>In the View, add a new button for adding speaker profiles. Also, add a list box, which will show all of our profiles. How you lay out the UI is up to you.</p><p>The ViewModel will need a new <code class="literal">ICommand</code> property for the button. It will also need an <code class="literal">ObservableObject</code> property for our profile list; make sure it is of type <code class="literal">Guid</code>. We will also need to be able to select a profile, so add a <code class="literal">Guid</code> property for the selected profile.</p><p>Additionally, we need to add a new member to the ViewModel:</p><div class="informalexample"><pre class="programlisting">    private SpeakerIdentification _speakerIdentification;</pre></div><p>This is the reference to the class we created earlier. Create this object in the constructor, passing on an <code class="literal">ISpeakerIdentificationServiceClient</code> object, which you inject via the ViewModel's constructor. In the constructor, you should also subscribe to the events we created:</p><div class="informalexample"><pre class="programlisting">    _speakerIdentification.OnSpeakerIdentificationError += OnSpeakerIdentificationError;
    _speakerIdentification.OnSpeakerIdentificationStatusUpdated += OnSpeakerIdentificationStatusUpdated;</pre></div><p>Basically, we want both event handles to update the status text with the message they carry:</p><div class="informalexample"><pre class="programlisting">    Application.Current.Dispatcher.Invoke(() =&gt; 
    {
        StatusText = e.Message;
    });</pre></div><p>The preceding<a class="indexterm" id="id324"/> code is for <code class="literal">OnSpeakerIdentificationStatusUpdated</code>. The same should be used for <code class="literal">OnSpeakerIdentificationError</code>, but set <code class="literal">StatusText</code> to be <code class="literal">e.ErrorMessage</code> instead.</p><p>In the function created for our <code class="literal">ICommand</code> property, we do the following to create a new profile:</p><div class="informalexample"><pre class="programlisting">    private async void AddSpeaker(object obj)
    {
        Guid speakerId = await _speakerIdentification.CreateSpeakerProfile();</pre></div><p>We make a call to our <code class="literal">_speakerIdentification</code> object's <code class="literal">CreateSpeakerProfile</code> function. This function will return a <code class="literal">Guid</code>, which is the unique ID of that speaker. In our example, we do not do anything further with this. In a real-life application, I would recommend mapping this ID to a name in some way. As you will see presently, identifying people through GUIDs is for machines, not people:</p><div class="informalexample"><pre class="programlisting">        GetSpeakerProfiles();
    }</pre></div><p>We finish this function by calling a <code class="literal">GetSpeakerProfile</code> function, which we will create next. This will fetch a list of all the profiles we have created so that we can use these throught the further process:</p><div class="informalexample"><pre class="programlisting">    private async void GetSpeakerProfiles()
    {
        List&lt;Guid&gt; profiles = await _speakerIdentification.ListSpeakerProfiles();

        if (profiles == null) return;</pre></div><p>In our <code class="literal">GetSpeakerProfiles</code> function, we call <code class="literal">ListSpeakerProfiles</code> on our <code class="literal">_speakerIdentification</code> object. This will, as we will see presently, fetch a list of GUIDs, containing the profile IDs. If this list is null, there is no point in moving on:</p><div class="informalexample"><pre class="programlisting">        foreach(Guid profile in profiles)
        {
            SpeakerProfiles.Add(profile);
        }
    }</pre></div><p>If the list does contain anything, we add these IDs to our <code class="literal">SpeakerProfiles</code>, which is the <code class="literal">ObservableCollection</code> property. This will show all of our profiles in the UI.</p><p>This function should <a class="indexterm" id="id325"/>also be called from the <code class="literal">Initialize</code> function, so we populate the list when we start the application.</p><p>Back in the <code class="literal">SpeakerIdentification</code> class, create a new function called <code class="literal">CreateSpeakerProfile</code>. This should have the return type <code class="literal">Task&lt;Guid&gt;</code> and be marked as <code class="literal">async</code>:</p><div class="informalexample"><pre class="programlisting">    public async Task&lt;Guid&gt; CreateSpeakerProfile()
    {
        try
        {
            CreateProfileResponse response = await _speakerIdentificationClient.CreateProfileAsync("en-US");</pre></div><p>We will then make a call to <code class="literal">CreateProfileAsync</code> on the API object. We need to specify the locale, which is used for the speaker profile. At the time of writing, <code class="literal">en-US</code> is the only valid option.</p><p>If the call is successful, we get a <code class="literal">CreateProfileResponse</code> object in response. This contains the ID of the newly created speaker profile:</p><div class="informalexample"><pre class="programlisting">       if (response == null)
      {
         RaiseOnIdentificationError(
            new SpeakerIdentificationErrorEventArgs
               ("Failed to create speaker profile."));
         return Guid.Empty;
      }

      return response.ProfileId;
   }</pre></div><p>If the <code class="literal">response</code> is null, we raise an error event. If it contains data, we return the <code class="literal">ProfileId</code> to the caller.</p><p>Add the corresponding <code class="literal">catch</code> clause to finish the function.</p><p>Create a new function called <code class="literal">ListSpeakerProfile</code>. This should return <code class="literal">Task&lt;List&lt;Guid&gt;&gt;</code> and be marked as <code class="literal">async</code>:</p><div class="informalexample"><pre class="programlisting">    public async Task&lt;List&lt;Guid&gt;&gt; ListSpeakerProfiles()
    {
        try
        {
            List&lt;Guid&gt; speakerProfiles = new List&lt;Guid&gt;();

            Profile[] profiles = await _speakerIdentificationClient.GetProfilesAsync();</pre></div><p>We will then create a list of type <code class="literal">Guid</code>, which is the list of speaker profiles we will return. Then, we call the <code class="literal">GetProfilesAsync</code> method on our <code class="literal">_speakerIdentificationClient</code> object. This will get us an array of type <code class="literal">Profile</code>, which contains<a class="indexterm" id="id326"/> information on each profile. This is information such as creation time, enrollment status, last modified, and so on. We are interested in the IDs of each profile:</p><div class="informalexample"><pre class="programlisting">            if (profiles == null || profiles.Length == 0)
            {
                RaiseOnIdentificationError(new SpeakerIdentificationErrorEventArgs("No profiles exist"));
                return null;
            }

            foreach (Profile profile in profiles)
            {
                speakerProfiles.Add(profile.ProfileId);
            }

            return speakerProfiles;
        }</pre></div><p>If any profiles are returned, we loop through the array and add each <code class="literal">profileId</code> to the previously created list. This list is then returned to the caller, which in our case will be the ViewModel.</p><p>End the function with the corresponding <code class="literal">catch</code> clause. Make sure that the code compiles and executes as expected before continuing. This means that you should now be able to add speaker profiles to the service and get the created profiles displayed in the UI.</p><p>To delete a speaker profile, we will need to add a new function to <code class="literal">SpeakerIdentification</code>. Call this function <code class="literal">DeleteSpeakerProfile</code>, and let it accept a <code class="literal">Guid</code> as its parameter. This will be the ID of the given profile we want to delete. Mark the function as <code class="literal">async</code>. The function should look as follows:</p><div class="informalexample"><pre class="programlisting">public async void DeleteSpeakerProfile(Guid profileId)
{
   try
   {
      await _speakerIdentificationClient.DeleteProfileAsync(profileId);
   }
   catch (IdentificationException ex)
   {
      RaiseOnIdentificationError(new SpeakerIdentificationErrorEventArgs($"Failed to
      delete speaker profile: {ex.Message}"));
   }
   catch (Exception ex)
   {
      RaiseOnIdentificationError(new SpeakerIdentificationErrorEventArgs($"Failed to
      delete speaker profile: {ex.Message}"));
   }
}</pre></div><p>As you can see, the call to the <code class="literal">DeleteProfileAsync</code> method expects a <code class="literal">Guid</code> type, <code class="literal">profileId</code>. There is no return value and, as such, when we call this function, we need to call the <code class="literal">GetSpeakerProfile</code> method in our ViewModel.</p><p>To facilitate the <a class="indexterm" id="id327"/>deletion of speaker profiles, add a new button to the UI and a corresponding <code class="literal">ICommand</code> property in the ViewModel.</p></div><div class="section" title="Enrolling a profile"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec43"/>Enrolling a profile</h2></div></div></div><p>With a speaker profile in place, we need to associate<a class="indexterm" id="id328"/> spoken audio with the profile. We do this through<a class="indexterm" id="id329"/> a process called <span class="strong"><strong>enrolling</strong></span>. For speaker identification, enrolling is text-independent. This means that you can use whatever sentence you want for enrollment. Once the voice is recorded, a number of features will be extracted to form a unique voice-print.</p><p>When enrolling, the audio file you are using must be 5 seconds at least and 5 minutes at most. Best practice states that you should accumulate at least 30 seconds of speech. This is 30 seconds <span class="emphasis"><em>after</em></span> silence has been removed, so several audio files may be required. This recommendation can be avoided by specifying an extra parameter, as we will see presently.</p><p>How you choose to upload the audio file is up to you. In the smart-house application, we will use a microphone to record live<a class="indexterm" id="id330"/> audio. To do so, we will need to add a new NuGet package called <span class="strong"><strong>NAudio</strong></span>. This is an audio library for .NET, which simplifies audio work.</p><p>We will also need a class to deal with recording, which is out of the scope of this book. As such, I recommend you copy the <code class="literal">Recording.cs</code> file, which can be found in the sample project in the <code class="literal">Model</code> folder.</p><p>In the <code class="literal">AdministrationViewModel</code> ViewModel, add a private member for the newly copied class. Create the class and subscribe to the events defined in the <code class="literal">Initialize</code> function:</p><div class="informalexample"><pre class="programlisting">    _recorder = new Recording();
    _recorder.OnAudioStreamAvailable += OnRecordingAudioStreamAvailable;
    _recorder.OnRecordingError += OnRecordingError;</pre></div><p>We have an event for errors and one for available audio stream. Let <code class="literal">OnRecordingError</code> print the <code class="literal">ErrorMessage</code> to the status text field.</p><p>In <code class="literal">OnAudioStreamAvailable</code>, add the following:</p><div class="informalexample"><pre class="programlisting">    Application.Current.Dispatcher.Invoke(() =&gt; 
    {               
        _speakerIdentification.CreateSpeakerEnrollment(e.AudioStream, SelectedSpeakerProfile);
    });</pre></div><p>Here, we call <code class="literal">CreateSpeakerEnrollment</code> on the <code class="literal">_speakerIdentification</code> object. We will cover this<a class="indexterm" id="id331"/> function presently. The parameters we pass on are the <code class="literal">AudioStream</code>, from the recording, as well as the ID of the selected profile.</p><p>To be able to get audio files for enrollment, we need to start and stop the recording. This can be done by simply adding two new buttons, one for start and one for stop. They will then need to execute one of the following:</p><div class="informalexample"><pre class="programlisting">    _recorder.StartRecording();
    _recorder.StopRecording();</pre></div><p>Back in the <code class="literal">SpeakerIdentification.cs</code> file, we need to create a new function, <code class="literal">CreateSpeakerEnrollment</code>. This should accept <code class="literal">Stream</code> and <code class="literal">Guid</code> as parameters, and be marked as <code class="literal">async</code>:</p><div class="informalexample"><pre class="programlisting">    public async void CreateSpeakerEnrollment(Stream audioStream, Guid profileId) {
        try {
            OperationLocation location = await _speakerIdentificationClient.EnrollAsync(audioStream, profileId);</pre></div><p>In this function, we call the <code class="literal">EnrollAsync</code> function on <code class="literal">_speakerIdentificationClient</code>. This function requires both the <code class="literal">audioStream</code> and <code class="literal">profileId</code> as parameters. An optional third parameter is a <code class="literal">bool</code> type variable, which lets you decide whether or not you would like to use the recommended speech length or not. The default is <code class="literal">false</code>, meaning that you use the recommended setting of at least 30 seconds of speech.</p><p>If the call is successful, we get an <code class="literal">OperationLocation</code> object back. This holds a URL that we can query for the enrollment status, which is precisely what we will do:</p><div class="informalexample"><pre class="programlisting">        if (location == null) {
            RaiseOnIdentificationError(new SpeakerIdentificationErrorEventArgs("Failed to start enrollment process."));
            return;
        }

        GetEnrollmentOperationStatus(location);
    }</pre></div><p>First, we make sure that we have the <code class="literal">location</code> data. Without it, there is no point in moving on. If we <a class="indexterm" id="id332"/>do have the <code class="literal">location</code> data, we call a function, <code class="literal">GetEnrollmentOperationStatus</code>, specifying the <code class="literal">location</code> as the parameter.</p><p>Add the corresponding <code class="literal">catch</code> clause to finish the function.</p><p>The <code class="literal">GetEnrollmentOperationStatus</code> method accepts <code class="literal">OperationLocation</code> as a parameter. When we enter the function, we move into a <code class="literal">while</code> loop, which will run until the operation completes. We call <code class="literal">CheckEnrollmentStatusAsync</code>, specifying the <code class="literal">location</code> as the parameter. If this call is successful, it will return an <code class="literal">EnrollmentOperation</code> object, which contains data such as status, enrollment speech time, and an estimation of the time of enrollment left:</p><div class="informalexample"><pre class="programlisting">    private async void GetEnrollmentOperationStatus(OperationLocation location) {
        try {
            while(true) { 
                EnrollmentOperation result = await _speakerIdentificationClient.CheckEnrollmentStatusAsync(location);</pre></div><p>When we have retrieved the result, we check to see if the status is running or not. If it isn't, the operation has either failed, succeeded, or not started. In any case, we do not want to check any further, so we send an update with the status and break out of the loop:</p><div class="informalexample"><pre class="programlisting">                if(result.Status != Status.Running)
                {
                    RaiseOnIdentificationStatusUpdated(new SpeakerIdentificationStatusUpdateEventArgs(result.Status.ToString(),
                    $"Enrollment finished. Enrollment status: {result.ProcessingResult.EnrollmentStatus.ToString()}"));
                    break;
                }

                RaiseOnIdentificationStatusUpdated(new SpeakerIdentificationStatusUpdateEventArgs(result.Status.ToString(), "Enrolling..."));
                await Task.Delay(1000);
            }
        }</pre></div><p>If the status is still running, we update the status and wait for 1 second before trying again.</p><p>With enrollment completed, there may be times when we need to reset the enrollment for a given profile. We can do so by creating a new function in <code class="literal">SpeakerIdentification</code>. Name it <code class="literal">ResetEnrollments</code>, and let it accept a <code class="literal">Guid</code> as a parameter. This should be the profile ID of the speaker profile to reset. Execute the following inside a <code class="literal">try</code> clause:</p><div class="informalexample"><pre class="programlisting">        await _speakerIdentificationClient .ResetEnrollmentsAsync(profileId);</pre></div><p>This will delete<a class="indexterm" id="id333"/> all audio files associated with the given profile and also reset the enrollment status. To call this function, add a new button to the UI and the corresponding <code class="literal">ICommand</code> property in the ViewModel.</p><p>If you compile and run the application, you may get a result similar to the following screenshot:</p><div class="mediaobject"><img alt="Enrolling a profile" src="graphics/B12373_05_01.jpg"/></div></div><div class="section" title="Identifying the speaker"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec44"/>Identifying the speaker</h2></div></div></div><p>The last step is<a class="indexterm" id="id334"/> to identify the speaker, which we will do in the <code class="literal">HomeView</code> and corresponding <code class="literal">HomeViewModel</code>. We do not need to modify the UI much, but we do need to add two buttons in order to start and stop the recording. Alternatively, if you are not using a microphone, you can get away with one button for browsing an audio file. Either way, add the corresponding <code class="literal">ICommand</code> properties in the ViewModel.</p><p>We also need to add private members for the <code class="literal">Recording</code> and <code class="literal">SpeakerIdentification</code> classes. Both should be created in the constructor, where we should inject <code class="literal">ISpeakerIdentificationServiceClient</code> as well.</p><p>In the <code class="literal">Initialize</code> function, subscribe to the required events:</p><div class="informalexample"><pre class="programlisting">    _speakerIdentification.OnSpeakerIdentificationError += OnSpeakerIdentificationError;          
    _speakerIdentification.OnSpeakerIdentificationStatusUpdated += OnSpeakerIdentificationStatusReceived; 

    _recording.OnAudioStreamAvailable += OnSpeakerRecordingAvailable;
    _recording.OnRecordingError += OnSpeakerRecordingError;</pre></div><p>For both of the error event handlers, <code class="literal">OnSpeakerRecordingError</code> and <code class="literal">OnSpeakerIdentificationError</code>, we do not wish to print the error message here. For simplicity, we just output it to the debug console window.</p><p>The <code class="literal">OnSpeakerRecordingAvailable</code> event will be triggered when we have recorded some audio. This is the event handler that will trigger an attempt to identify the person speaking.</p><p>The first thing we need to do is get a list of speaker profile IDs. We do so by calling <code class="literal">ListSpeakerProfiles</code>, which we looked at earlier:</p><div class="informalexample"><pre class="programlisting">    private async void OnSpeakerRecordingAvailable(object sender, RecordingAudioAvailableEventArgs e)
    {
        try
        {
            List&lt;Guid&gt; profiles = await _speakerIdentification.ListSpeakerProfiles();</pre></div><p>With the list of <a class="indexterm" id="id335"/>speaker profiles, we call the <code class="literal">IdentifySpeaker</code> method on the <code class="literal">_speakerIdentification</code> object. We pass on the recorded audio stream and the profile list, as an array, as parameters to the function:</p><div class="informalexample"><pre class="programlisting">            _speakerIdentification.IdentifySpeaker(e.AudioStream, profiles.ToArray());
        }</pre></div><p>Finish the event handler by adding the corresponding <code class="literal">catch</code> clause.</p><p>Back in the <code class="literal">SpeakerIdentification.cs</code> file, we add the new function, <code class="literal">IdentifySpeaker</code>:</p><div class="informalexample"><pre class="programlisting">    public async void IdentifySpeaker(Stream audioStream, Guid[] speakerIds)
    {
        try
        {
            OperationLocation location = await _speakerIdentificationClient.IdentifyAsync(audioStream, speakerIds);</pre></div><p>The function should be marked as <code class="literal">async</code> and accept a <code class="literal">Stream</code> and an array of <code class="literal">Guid</code> as parameters. To identify a speaker, we make a call to the <code class="literal">IdentifyAsync</code> function on the <code class="literal">_speakerIdentificationClient</code> object. This requires an audio file, in the form of a <code class="literal">Stream</code>, as well as an array of profile IDs. An optional third parameter is a <code class="literal">bool</code>, which you can use to indicate whether or not you want to deviate from the recommended speech length.</p><p>If the call succeeds, we get an <code class="literal">OperationLocation</code> object back. This contains a URL that we can use to retrieve the status of the current identification process:</p><div class="informalexample"><pre class="programlisting">        if (location == null)
        { 
            RaiseOnIdentificationError(new SpeakerIdentificationErrorEventArgs ("Failed to identify speaker."));
            return;
        }
        GetIdentificationOperationStatus(location);                
    }</pre></div><p>If the<a class="indexterm" id="id336"/> resulting data contains nothing, we do not want to bother doing anything else. If it does contain data, we pass it on as a parameter to the <code class="literal">GetIdentificationOperationStatus</code> method:</p><div class="informalexample"><pre class="programlisting">     private async void GetIdentificationOperationStatus (OperationLocation location)
     {
         try
         { 
             while (true)
             {
                 IdentificationOperation result = await _speakerIdentificationClient.CheckIdentificationStatusAsync(location);</pre></div><p>This function is quite similar to <code class="literal">GetEnrollmentOperationStatus</code>. We go into a <code class="literal">while</code> loop, which will run until the operation completes. We call <code class="literal">CheckIdentificationStatusAsync</code>, passing on the <code class="literal">location</code> as a parameter, getting <code class="literal">IdentificationOperation</code> as a result. This will contain data, such as a status, the identified profiles ID, and the confidence of a correct result.</p><p>If the operation is not running, we raise the event with the status message and the <code class="literal">ProcessingResult</code>. If the operation is still running, we update the status and wait for 1 second before trying again:</p><div class="informalexample"><pre class="programlisting">            if (result.Status != Status.Running)
            {
                RaiseOnIdentificationStatusUpdated(new SpeakerIdentificationStatusUpdateEventArgs(result.Status.ToString(), $"Enrollment finished with message:{result.Message}.") { IdentifiedProfile = result.ProcessingResult });
                break;
            } 

            RaiseOnIdentificationStatusUpdated(new SpeakerIdentificationStatusUpdateEventArgs(result.Status.ToString(), "Identifying..."));

            await Task.Delay(1000);
        }
    }</pre></div><p>Add the corresponding <code class="literal">catch</code> clause before heading back to the <code class="literal">HomeViewModel</code>.</p><p>The last piece in the puzzle is to create <code class="literal">OnSpeakerIdentificationStatusReceived</code>. Add the following code inside HomeViewModel:</p><div class="informalexample"><pre class="programlisting">    Application.Current.Dispatcher.Invoke(() =&gt; 
    {
        if (e.IdentifiedProfile == null) return;

        SystemResponse = $"Hi there,{e.IdentifiedProfile.IdentifiedProfileId}";
    });</pre></div><p>We need to check<a class="indexterm" id="id337"/> to see whether or not we have an identified profile. If we do not, we leave the function. If we have an identified profile, we give a response to the screen, stating who it is.</p><p>As with the administrative side of the application, this is a place where it would be convenient to have name-to-profile ID mapping. As you can see from the following resulting screenshot, recognizing one <code class="literal">GUID</code> among many is not that easy:</p><div class="mediaobject"><img alt="Identifying the speaker" src="graphics/B12373_05_02.jpg"/></div></div></div>
<div class="section" title="Verifying a person through speech"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec31"/>Verifying a person through speech</h1></div></div></div><p>The process of verifying<a class="indexterm" id="id338"/> if a person is who they claim to be is quite similar to the identification process. To show how it is done, we will create a new example project, as we do not need this functionality in our smart-house application.</p><p>Add the <code class="literal">Microsoft.ProjectOxford.SpeakerRecognition</code> and <code class="literal">NAudio</code> NuGet packages to the project. We will need the <code class="literal">Recording</code> class that we used earlier, so copy this from the smart-house application's <code class="literal">Model</code> folder.</p><p>Open the <code class="literal">MainView.xaml</code> file. We need a few elements in the UI for the example to work. Add a <code class="literal">Button</code> element to add speaker profiles. Add two <code class="literal">Listbox</code> elements. One will hold available verification phrases while the other will list our speaker profiles.</p><p>Add <code class="literal">Button</code> elements for deleting a profile, starting and stopping enrollment recording, resetting enrollment, and starting/stopping verification recording.</p><p>In the ViewModel, you will need to add two <code class="literal">ObservableCollection</code> properties: one of type <code class="literal">string</code>, the other of type <code class="literal">Guid</code>. One will contain the available verification phrases, while the other will contain the list of speaker profiles. You will also need a property for the selected speaker profile, and we also want a string property to show the status.</p><p>The ViewModel will also need seven <code class="literal">ICommand</code> properties, one for each of our buttons.</p><p>Create a new class in the <code class="literal">Model</code> folder and call this <code class="literal">SpeakerVerification</code>. Add two new classes beneath this one, in the same file.</p><p>The first one is<a class="indexterm" id="id339"/> the event arguments that we will pass on when we raise a status update event. The <code class="literal">Verification</code> property will, if set, hold the verification result, which we will see presently:</p><div class="informalexample"><pre class="programlisting">    public class SpeakerVerificationStatusUpdateEventArgs : EventArgs
    {
        public string Status { get; private set; }
        public string Message { get; private set; }
        public Verification VerifiedProfile { get; set; }

       public SpeakerVerificationStatusUpdateEventArgs(string status,string message)
       {
           Status = status;
           Message = message;
       }
    }</pre></div><p>The next class is a generic event argument, which is used when we raise an error event. In <code class="literal">SpeakerVerification</code> itself, add the following events:</p><div class="informalexample"><pre class="programlisting">    public class SpeakerVerificationErrorEventArgs : EventArgs
    {
        public string ErrorMessage { get; private set; }

        public SpeakerVerificationErrorEventArgs(string errorMessage)
        {
            ErrorMessage = errorMessage;
        }
    }</pre></div><p>For our convenience, add helper functions to raise these. Call them <code class="literal">RaiseOnVerificationStatusUpdated</code> and <code class="literal">RaiseOnVerificationError</code>. Raise the correct event in each of them:</p><div class="informalexample"><pre class="programlisting">    public event EventHandler &lt;SpeakerVerificationStatusUpdateEventArgs&gt; OnSpeakerVerificationStatusUpdated;

    public event EventHandler&lt;SpeakerVerificationErrorEventArgs&gt; OnSpeakerVerificationError;</pre></div><p>We also need to add a private member called <code class="literal">ISpeakerVerificationServiceClient</code>. This will be in charge of calling the API. We inject this through the constructor.</p><p>Add the following functions to the class:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">CreateSpeakerProfile</code>: No parameters, the <code class="literal">async</code> function, and the return type <code class="literal">Task&lt;Guid&gt;</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">ListSpeakerProfile</code>: No parameters, the <code class="literal">async</code> function, and the return type <code class="literal">Task&lt;List&lt;Guid&gt;&gt;</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">DeleteSpeakerProfile</code>: <code class="literal">Guid</code> as the required parameter, the <code class="literal">async</code> function, no returned values</li><li class="listitem" style="list-style-type: disc"><code class="literal">ResetEnrollments</code>: <code class="literal">Guid</code> as the required parameter, the <code class="literal">async</code> function, no returned values</li></ul></div><p>The contents<a class="indexterm" id="id340"/> of these functions can be copied from the corresponding functions in the smart-house application, as they are exactly the same. The only difference is that you need to change the API call from <code class="literal">_speakerIdentificationClient</code> to <code class="literal">_speakerVerificationClient</code>. Also, raising the events will require the newly created event arguments.</p><p>Next, we need a function to list verification phrases. These are phrases that are supported for use with verification. When enrolling a profile, you are required to say one of the sentences in this list.</p><p>Create a function named <code class="literal">GetVerificationPhrase</code>. Have it return <code class="literal">Task&lt;List&lt;string&gt;&gt;</code>, and mark it as <code class="literal">async</code>:</p><div class="informalexample"><pre class="programlisting">    public async Task&lt;List&lt;string&gt;&gt; GetVerificationPhrase()
    {
        try
        {
            List&lt;string&gt; phrases = new List&lt;string&gt;();

            VerificationPhrase[] results = await _speakerVerificationClient.GetPhrasesAsync("en-US");</pre></div><p>We will make a call to <code class="literal">GetPhrasesAsync</code>, specifying the language we want the phrases to be in. At the time of writing, English is the only possible choice.</p><p>If this call is successful, we will get an array of <code class="literal">VerificationPhrases</code> in return. Each element in this array contains a string with the following phrase:</p><div class="informalexample"><pre class="programlisting">            foreach(VerificationPhrase phrase in results) {
                phrases.Add(phrase.Phrase);
            }
            return phrases;
        }</pre></div><p>We loop through the array and add the phrases to our list, which we will return to the caller.</p><p>So, we have created a profile and we have the list of possible verification phrases. Now, we need to do the enrollment. To enroll, the service requires at least three enrollments from each speaker. This means that you choose a phrase and enroll it at least three times.</p><p>When you do the enrollment, it is highly recommended to use the same recording device that you will use for verification.</p><p>Create a new function called <code class="literal">CreateSpeakerEnrollment</code>. This should require a <code class="literal">Stream</code> and a <code class="literal">Guid</code>. The first parameter is the audio to use for enrollment. The latter is the ID of the profile we are enrolling. The function should be marked as <code class="literal">async</code>, and have no return value:</p><div class="informalexample"><pre class="programlisting">    public async void CreateSpeakerEnrollment(Stream audioStream, Guid profileId) {
        try {
            Enrollment enrollmentStatus = await _speakerVerificationClient.EnrollAsync(audioStream, profileId);</pre></div><p>When we call <code class="literal">EnrollAsync</code>, we pass on the <code class="literal">audioStream</code> and <code class="literal">profileId</code> parameters. If the call is <a class="indexterm" id="id341"/>successful, we get an <code class="literal">Enrollment</code> object back. This contains the current status of enrollment and specifies the number of enrollments you need to add before completing the process.</p><p>If the <code class="literal">enrollmentStatus</code> is null, we exit the function and notify any subscribers. If we do have status data, we raise the event to notify it that there is a status update, specifying the current status:</p><div class="informalexample"><pre class="programlisting">            if (enrollmentStatus == null) {
                RaiseOnVerificationError(new SpeakerVerificationErrorEventArgs("Failed to start enrollment process."));
                return;
            }

           RaiseOnVerificationStatusUpdate(new SpeakerVerificationStatusUpdateEventArgs("Succeeded", $"Enrollment status:{enrollmentStatus.EnrollmentStatus}"));
       }</pre></div><p>Add the corresponding <code class="literal">catch</code> clause to finish up the function.</p><p>The last function we need in this class is a function for verification. To verify a speaker, you need to send in an audio file. This file must be at least 1 second and at most 15 seconds long. You will need to record the same phrase that you used for enrollment.</p><p>Call the <code class="literal">VerifySpeaker</code> function and make it require a <code class="literal">Stream</code> and <code class="literal">Guid</code>. The stream is the audio file we will use for verification. The <code class="literal">Guid</code> is the ID of the profile we wish to verify. The function should be <code class="literal">async</code> and have no return type:</p><div class="informalexample"><pre class="programlisting">    public async void VerifySpeaker(Stream audioStream, Guid speakerProfile) {
        try {
            Verification verification = await _speakerVerificationClient.VerifyAsync(audioStream, speakerProfile);</pre></div><p>We will make a call to <code class="literal">VerifyAsync</code> from <code class="literal">_speakerVerificationClient</code>. The required parameters are <code class="literal">audioStream</code> and <code class="literal">speakerProfile</code>.</p><p>A successful <a class="indexterm" id="id342"/>API call will result in a <code class="literal">Verification</code> object in response. This will contain the verification results, as well as the confidence of the results being correct:</p><div class="informalexample"><pre class="programlisting">            if (verification == null) {
                RaiseOnVerificationError(new SpeakerVerificationErrorEventArgs("Failed to verify speaker."));
                return;
            }

            RaiseOnVerificationStatusUpdate(new SpeakerVerificationStatusUpdateEventArgs("Verified", "Verified speaker") { VerifiedProfile = verification });             
        }</pre></div><p>If we do have a verification result, we raise the status update event. Add the corresponding <code class="literal">catch</code> clause to complete the function.</p><p>Back in the ViewModel, we need to wire up the commands and event handlers. This is done in a similar manner as for speaker identification, and as such we will not cover the code in detail.</p><p>With the code compiling and running, the result may look similar to the following screenshot:</p><div class="mediaobject"><img alt="Verifying a person through speech" src="graphics/B12373_05_03.jpg"/></div><p>Here, we can see<a class="indexterm" id="id343"/> that we have created a speaker profile. We have also completed the enrollment and are ready to verify the speaker.</p><p>Verifying the speaker profile may result in the following:</p><div class="mediaobject"><img alt="Verifying a person through speech" src="graphics/B12373_05_04.jpg"/></div><p>As you <a class="indexterm" id="id344"/>can see, the verification was accepted with high confidence.</p><p>If we try to verify this using a different phrase or let someone else try to verify as a particular speaker profile, we may end up with the following result:</p><div class="mediaobject"><img alt="Verifying a person through speech" src="graphics/B12373_05_05.jpg"/></div><p>Here, we can <a class="indexterm" id="id345"/>see that the verification has been rejected.</p></div>
<div class="section" title="Customizing speech recognition"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec32"/>Customizing speech recognition</h1></div></div></div><p>When we use speech<a class="indexterm" id="id346"/> recognition systems, there are several components that are working together. Two of the more important components are acoustic and language models. The first one labels short fragments of audio into sound units. The second helps the system decide the words, based on the likelihood of a given word appearing in certain sequences.</p><p>Although Microsoft has done a great job of creating comprehensive acoustic and language models, there may still be times when you need to customize these models.</p><p>Imagine that you have an application that is supposed to be used in a factory environment. Using speech recognition will require acoustic training of that environment so that the recognition can separate it from usual factory noises.</p><p>Another example is if your <a class="indexterm" id="id347"/>application is used by a specific group of people, say, an application for search, where programming is the main topic. You would typically use words such as <span class="emphasis"><em>object-oriented</em></span>, <span class="emphasis"><em>dot net</em></span>, or <span class="emphasis"><em>debugging</em></span>. This can be recognized by customizing language models.</p><div class="section" title="Creating a custom acoustic model"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec45"/>Creating a custom acoustic model</h2></div></div></div><p>To create custom <a class="indexterm" id="id348"/>acoustic models, you will need <a class="indexterm" id="id349"/>audio files and transcripts. Each audio file must be stored as a WAV and be between 100 ms and 1 minute in length. It is recommended that there is at least 100 ms of silence at the start and end of the file. Typically, this will be between 500 ms and 1 second. With a lot of background noise, it is recommended to have silences in-between content.</p><p>Each file should contain one sentence or utterance. Files should be uniquely named, and an entire set of files can be up to 2 GB. This translates to about 17 to 34 hours of audio, depending on the sampling rate. All files in one set should be placed in a zipped folder, which then can be uploaded.</p><p>Accompanying the audio files is a single file with the transcript. This should name the file and have the sentence next to the name. The filename and sentence should be separated by a tab.</p><p>Uploading the audio files and transcript will make CRIS process it. When this process is done, you will get a report stating which sentences have failed or succeeded. If anything fails, you will get the reason for the failure.</p><p>When the dataset has been uploaded, you can create the acoustic model. This will be associated with the dataset you select. When the model has been created, you can start the process to train it. Once the training is completed, you can deploy the model.</p></div><div class="section" title="Creating a custom language model"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec46"/>Creating a custom language model</h2></div></div></div><p>Creating custom<a class="indexterm" id="id350"/> language models will also require a dataset. This set is a <a class="indexterm" id="id351"/>single plain text file containing sentences or utterances unique to your model. Each new line marks a new utterance. The maximum file size is 2 GB.</p><p>Uploading the file will make CRIS process it. Once the processing is done, you will get a report, which will print any errors, with the reason of failure.</p><p>With the processing done, you can create a custom language model. Each model will be associated with a given dataset of your selection. Once created, you can train the model, and when the<a class="indexterm" id="id352"/> training complete, you can deploy it.</p></div><div class="section" title="Deploying the application"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec47"/>Deploying the application</h2></div></div></div><p>To deploy and use<a class="indexterm" id="id353"/> the custom models, you will need to create a deployment. Here, you will name and describe the application. You can select acoustic models and language models. Be aware that you can only select one of each per deployed application.</p><p>Once created, the deployment will start. This process can take up to 30 minutes to complete, so be patient. When the deployment completes, you can get the required information by clicking on the application name. You will be given URLs you can use, as well as subscription keys to use.</p><p>To use the custom models with the Bing Speech API, you can overload <code class="literal">CreateDataClientWithIntent</code> and <code class="literal">CreateMicrophoneClient</code>. The overloads you will want to use specify both the primary and secondary API keys. You need to use the ones supplied by CRIS. Additionally, you need to specify the supplied URL as the last parameter.</p><p>Once this is done, you are able to use customized recognition models.</p></div></div>
<div class="section" title="Translating speech on the fly"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec33"/>Translating speech on the fly</h1></div></div></div><p>Using the <span class="strong"><strong>Translator Speech</strong></span> API, you can add automatic end-to-end translation for speech. Utilizing this API, one <a class="indexterm" id="id354"/>can submit an audio stream of speech and retrieve a textual and audio version of translated text. It uses silent detection to detect when speech has ended. Results will be streamed back once the pause is detected.</p><p>For a comprehensive list of <a class="indexterm" id="id355"/>supported languages, please visit the following site: <a class="ulink" href="https://www.microsoft.com/en-us/translator/business/languages/">https://www.microsoft.com/en-us/translator/business/languages/</a>.</p><p>The result recieved from the API, will contain a stream of audio- and text-based results. The results contain the source text in its original language and the translation in the target language.</p><p>For a thorough<a class="indexterm" id="id356"/> example on how to use the <span class="strong"><strong>Translator Speech</strong></span> API, please visit the following sample at GitHub: <a class="ulink" href="https://github.com/MicrosoftTranslator/SpeechTranslator">https://github.com/MicrosoftTranslator/SpeechTranslator</a>.</p></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec34"/>Summary</h1></div></div></div><p>Throughout this chapter, we have focused on speech. We started by looking at how we can convert spoken audio to text and text to spoken audio. Using this, we modified our LUIS implementation so that we can say commands and have conversations with the smart-house application. From there, we moved on to see how we can identify a person speaking using the Speaker Recognition API. Using the same API, we also learned how to verify that a person is who they claim to be. We briefly looked at the core functionality of the Custom Speech Service. Finally, we briefly covered an introduction to the Translator Speech API.</p><p>In the following chapter, we will move back to textual APIs, where we will learn how to explore and analyze text in different ways.</p></div></body></html>