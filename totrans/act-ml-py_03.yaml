- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Designing Query Strategy Frameworks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计查询策略框架
- en: '**Query strategies** act as the engine that drives active ML and determines
    which data points get selected for labeling. In this chapter, we aim to provide
    a comprehensive and detailed explanation of the most widely used and highly effective
    query strategy frameworks that are employed in active ML. These frameworks play
    a crucial role in the field of active ML, aiding in selecting informative and
    representative data points for labeling. The strategies that we will delve into
    include uncertainty sampling, query-by-committee, **expected model change** (**EMC**),
    **expected error reduction** (**EER**), and density-weighted methods. By thoroughly
    understanding these frameworks and the underlying principles, you can make informed
    decisions when designing and implementing active ML algorithms.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**查询策略**作为驱动主动机器学习的引擎，并决定哪些数据点被选中进行标记。在本章中，我们旨在提供对主动机器学习中应用最广泛、最有效的查询策略框架的全面和详细解释。这些框架在主动机器学习领域发挥着至关重要的作用，有助于选择具有信息性和代表性的数据点进行标记。我们将深入探讨的策略包括不确定性采样、委员会查询、**预期模型变化**（**EMC**）、**预期误差减少**（**EER**）和密度加权方法。通过彻底理解这些框架及其背后的原理，你可以在设计和实现主动机器学习算法时做出明智的决策。'
- en: In this chapter, you will gain skills that will equip you to design and deploy
    query strategies that extract maximum value from labeling efforts. You will gain
    intuition for matching strategies to datasets and use cases when building active
    ML systems.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将获得设计并部署查询策略的技能，这些策略可以从标记工作中提取最大价值。你将获得在构建主动机器学习系统时将匹配策略与数据集和用例相匹配的直觉。
- en: 'We will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主题：
- en: Exploring uncertainty sampling methods
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索不确定性采样方法
- en: Understanding query-by-committee approaches
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解委员会查询方法
- en: Labeling with EMC sampling
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用EMC采样进行标记
- en: Sampling with EER
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用EER进行采样
- en: Understanding density-weighted sampling methods
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解密度加权采样方法
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'For the code examples demonstrated in this chapter, we have used Python 3.9.6
    with the following packages:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章中演示的代码示例，我们使用了Python 3.9.6以及以下包：
- en: '`numpy` (version 1.23.5)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy`（版本1.23.5）'
- en: '`scikit-learn` (version 1.2.2)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scikit-learn`（版本1.2.2）'
- en: '`matplotlib` (version 3.7.1)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matplotlib`（版本3.7.1）'
- en: Exploring uncertainty sampling methods
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索不确定性采样方法
- en: '**Uncertainty sampling** refers to querying data points for which the model
    is least certain about their prediction. These are samples the model finds most
    ambiguous and cannot confidently label on its own. Getting these high-uncertainty
    points labeled allows the model to clarify where its knowledge is lacking.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**不确定性采样**指的是查询模型对其预测最不确定的数据点。这些是模型认为最模糊且无法自信地自行标记的样本。将这些高不确定性点标记出来，允许模型阐明其知识不足之处。'
- en: In uncertainty sampling, the active ML system queries instances for which the
    current model’s predictions exhibit *high uncertainty*. The goal is to select
    data points that are *near the decision boundary* between classes. Labeling these
    ambiguous examples helps the model gain confidence in areas where its knowledge
    is weakest.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在不确定性采样中，主动机器学习系统查询当前模型预测表现出**高不确定性**的实例。目标是选择位于类别之间的**决策边界**附近的数据点。标记这些模糊示例有助于模型在知识最薄弱的领域建立信心。
- en: Uncertainty sampling methods select data points close to the **decision boundary**
    because points near this boundary exhibit the highest prediction ambiguity. The
    decision boundary is defined as the point where the model shows the most uncertainty
    in distinguishing between different classes for a given input. Points on the boundary
    represent the most ambiguous, uncertain cases for a model.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性采样方法选择靠近**决策边界**的数据点，因为这些点表现出最高的预测不确定性。决策边界定义为模型在区分给定输入的不同类别时表现出最大不确定性的点。边界上的点代表模型最模糊、最不确定的情况。
- en: '*Figure 2**.1* illustrates the difference between uncertainty sampling and
    random sampling:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2**.1*展示了不确定性采样与随机采样的区别：'
- en: '![](img/B21789_02_01.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21789_02_01.jpg)'
- en: Figure 2.1 – Uncertainty sampling versus random sampling
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 – 不确定性采样与随机采样的比较
- en: For data points that are located far away from the decision boundary (*Figure
    2**.1*) within a class region (labeled as A or B), the model will exhibit a high
    level of confidence in assigning them to that class (for example, >95%). These
    points are considered certain and will not be selected when employing uncertainty
    sampling. However, there is a possibility that some of these points may be chosen
    when using random sampling. For data points that are extremely close to or directly
    on the decision boundary, the model will struggle to distinguish between the classes.
    The predicted class probabilities will be more evenly distributed, with the top
    predictions being very close to each other. Therefore, these points are considered
    uncertain and will be selected when using uncertainty sampling. These important
    data points might have been overlooked when using random sampling. As a result,
    the distance to the boundary correlates to uncertainty – the closest points will
    have the lowest max confidence, the smallest margin between top classes, and the
    highest entropy over the class probabilities.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于位于类区域（标记为A或B）内远离决策边界（*图2**.1*）的数据点，模型将非常自信地将它们分配到该类（例如，>95%）。这些点被认为是确定的，在采用不确定性采样时不会被选中。然而，当使用随机采样时，可能选择其中的一些点。对于非常接近或直接位于决策边界上的数据点，模型将难以区分这些类别。预测的类概率将更加均匀分布，顶级预测非常接近。因此，这些点被认为是不确定的，在采用不确定性采样时将被选中。这些重要的数据点在使用随机采样时可能会被忽略。因此，到边界的距离与不确定性相关联——最接近边界的点将具有最低的最大置信度，顶级类别之间的最小边缘，以及类概率的最高熵。
- en: Therefore, by selecting points based on metrics such as low confidence, low
    margin, and high entropy, uncertainty sampling queries the instances nearest to
    the decision boundary. We will discuss these metrics in detail in the upcoming
    sections of this chapter. Labeling these provides information to help clarify
    class regions and refine the boundary. The model is unlikely to gain much information
    from examples it can already predict correctly with high confidence. However,
    querying data points that the model is very uncertain about directly provides
    useful information about its gaps. Uncertainty sampling takes advantage of this
    by targeting points with high prediction ambiguity.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过根据低置信度、低边缘和高熵度等指标选择点，不确定性采样查询了最接近决策边界的实例。我们将在本章接下来的部分详细讨论这些指标。对这些实例进行标注提供了帮助，有助于明确类区域并细化边界。模型不太可能从它已经可以高置信度正确预测的例子中获得很多信息。然而，直接查询模型非常不确定的数据点直接提供了关于其差距的有用信息。不确定性采样通过针对具有高预测模糊性的点来利用这一点。
- en: For example, an image classifier’s least confident predictions likely correspond
    to challenging out-of-distribution examples that traditional sampling would miss.
    By querying these unusual cases for labels, the model rapidly improves at classifying
    edge cases. Now, let’s discuss some of the common methods that are used for uncertainty
    sampling.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，图像分类器的最小置信度预测很可能对应于具有挑战性的分布外示例，传统采样会错过这些示例。通过查询这些异常情况以获取标签，模型可以快速提高对边缘案例的分类能力。现在，让我们讨论一些用于不确定性采样的常用方法。
- en: First, we will talk about **least-confidence sampling**, where the data points
    are ranked according to their least-confidence score. This score is obtained by
    subtracting the most confident prediction label for each item from 1, which represents
    100% confidence. To facilitate understanding, it is beneficial to convert the
    uncertainty scores into a range of 0-1, where 1 signifies the highest level of
    uncertainty. The magnitude of the score that’s assigned to each data point lies
    in its association with the uncertainty of the model’s prediction. Consequently,
    data samples with the highest least-confident scores should be given priority
    for annotation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将讨论**最小置信度采样**，其中数据点根据它们的最小置信度得分进行排序。这个得分是通过从每个项目的最自信预测标签中减去1来获得的，这代表100%的置信度。为了便于理解，将不确定性得分转换为0-1的范围是有益的，其中1表示最高水平的不确定性。分配给每个数据点的分数的大小在于它与模型预测的不确定性之间的关联。因此，具有最高最小置信度得分的数据样本应该优先进行标注。
- en: 'The most informative sample, x, can be computed as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最具信息量的样本x可以按以下方式计算：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi
    mathvariant="normal">argmax</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>](img/1.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi
    mathvariant="normal">argmax</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>](img/1.png)'
- en: 'Here, we have the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们有以下内容：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi
    mathvariant="normal">argmax</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>](img/2.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi
    mathvariant="normal">argmax</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>](img/2.png)'
- en: 'For instance, let’s say we have a model that classifies samples into three
    different classes. Now, we are trying to rank two samples using least-confidence
    sampling. The predicted probabilities of the two samples are `[0.05, 0.85, 0.10]`
    for sample 1 and `[0.35, 0.15, 0.50]` for sample 2\. Let’s find out which sample
    is the most informative when using the least-confidence sampling method by using
    the following Python code:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一个将样本分类为三个不同类别的模型。现在，我们正在尝试使用最小置信度采样方法对两个样本进行排序。样本1和样本2的预测概率分别为 `[0.05,
    0.85, 0.10]` 和 `[0.35, 0.15, 0.50]`。让我们通过以下Python代码找出使用最小置信度采样方法时哪个样本最有信息量：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output of the preceding code snippet is as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段的输出如下：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Thus, the most informative sample is sample 2.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最有信息量的样本是样本2。
- en: In this case, we can see that sample 2 is chosen when using the least-confidence
    sampling approach because the model’s predictions were the least confident for
    that sample.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们可以看到当使用最小置信度采样方法时，选择了样本2，因为模型对该样本的预测最不自信。
- en: 'Next, we will discuss **margin sampling**. This method is designed to identify
    and select data points that have the smallest disparity in probability between
    the top two predicted classes. By focusing on data points with minimal margin
    between classes, we can effectively prioritize the annotation of data samples
    that result in a higher level of confusion for the model. Therefore, the model’s
    level of uncertainty is higher when it encounters data points with a lower margin
    score, making them ideal candidates for annotation. The formula to calculate the
    score of the most informative data point with the margin sampling method is as
    follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论**边缘采样**。这种方法旨在识别和选择在两个预测类别之间概率差异最小的数据点。通过关注类别之间最小边缘的数据点，我们可以有效地优先标注导致模型产生更高混淆程度的数据样本。因此，当模型遇到边缘分数较低的数据点时，其不确定性水平更高，这使得它们成为理想的标注候选。使用边缘采样方法计算最有信息量数据点分数的公式如下：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi
    mathvariant="normal">argmin</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>](img/3.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi
    mathvariant="normal">argmin</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>](img/3.png)'
- en: 'Let’s use the samples from our previous example again:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次使用我们之前的示例中的样本：
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output of the preceding script is as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 上述脚本的输出如下：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: With the margin sampling method, sample 2 is selected as well because it has
    the smallest disparity in probability between the top two predicted classes.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用边缘采样方法，样本2也被选中，因为它在两个预测类别之间的概率差异最小。
- en: In the **ratio of confidence** method, the data points that have the smallest
    ratio between the probability of the top predicted class and the probability of
    the second most likely class are selected. This targets examples where the model’s
    top two predictions are closest in likelihood. A lower ratio indicates that the
    model is less confident in the top class relative to the second class. By querying
    points with the minimum ratio between the top two class probabilities, this technique
    focuses on cases where the model is nearly equivocal between two classes. Getting
    these boundary points labeled will push the model to gain greater confidence in
    the true class. A lower ratio means higher ambiguity, so the ratio of confidence
    sampling finds points where the model is most unsure of which class is correct.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在**置信度比率**方法中，选择具有最小概率比的数据点，即最高预测类别的概率与第二可能类别的概率之间的比率。这针对的是模型的前两个预测在可能性上最接近的例子。较低的比率表示模型对第二类相对于最高类别的置信度较低。通过查询最高两个类别概率之间的最小比率点，这种技术专注于模型在两个类别之间几乎无法区分的情况。将这些边界点标记出来将推动模型对真实类别有更大的信心。较低的比率意味着更高的歧义，因此置信度采样比率找到模型最不确定哪个类别是正确的点。
- en: 'We can calculate the score of the most informative data point using the ratio
    of confidence sampling method via the following equation:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用置信度采样方法通过以下方程计算最有信息量的数据点的得分：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi
    mathvariant="normal">argmin</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>](img/4.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi
    mathvariant="normal">argmin</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>](img/4.png)'
- en: 'Once again, we’ll utilize the samples that we used previously for this method:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们将使用之前用于此方法的样本：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output of this script is as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本的输出如下：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: So, sample 2 is selected when using the ratio of confidence sampling method
    as it has the smallest ratio between the probability of the top predicted class
    and the probability of the second most likely class.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当使用置信度采样方法时，选择样本2，因为它具有最高预测类别概率与第二可能类别概率之间的最小比率。
- en: Another method is **entropy sampling**. This method selects data points that
    have the highest entropy across the probability distribution over classes. Entropy
    represents the overall uncertainty in the predicted class probabilities. Higher
    entropy means the model is more uncertain, with a more uniform probability spread
    over classes. Lower entropy indicates confidence, with probability concentrated
    on one class.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是**熵采样**。这种方法选择在类别概率分布上具有最高熵的数据点。熵表示预测类别概率的整体不确定性。熵值越高，模型的不确定性越大，类别上的概率分布越均匀。熵值越低，表示信心，概率集中在单个类别上。
- en: 'By querying points with maximum entropy, this technique targets instances where
    the model’s predicted class probabilities are most evenly distributed. These highly
    uncertain points provide the most information gain since the model cannot strongly
    favor one class – its predictions are maximally unsure. Getting these high entropy
    points labeled enables the model to gain more confidence in areas in which it
    is the most uncertain. Overall, entropy sampling finds points with the highest
    total ambiguity. The formula to calculate the score of the most informative data
    point with the entropy sampling method is as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查询具有最大熵的点，这种技术针对模型预测的类别概率分布最均匀的实例。这些高度不确定的点提供了最多的信息增益，因为模型不能强烈偏向一个类别——其预测是最不确定的。获取这些高熵点进行标记使模型能够对其最不确定的领域获得更多信心。总的来说，熵采样找到具有最高总模糊度的点。使用熵采样方法计算最有信息数据点得分的公式如下：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi
    mathvariant="normal">argmax</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo>-</mml:mo><mml:mrow><mml:munder><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mi
    mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">g</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>](img/5.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi
    mathvariant="normal">argmax</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo>-</mml:mo><mml:mrow><mml:munder><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mi
    mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">g</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>](img/5.png)'
- en: 'Let’s use our sample examples again with this method:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次使用这个方法来使用我们的样本示例：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This script outputs the following results:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本输出以下结果：
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Using entropy sampling, sample 2 was chosen as it has the highest entropy across
    the probability distribution over classes.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用熵采样，样本2被选中，因为它在类别概率分布上具有最高的熵。
- en: These common uncertainty sampling techniques provide simple but effective strategies
    to identify highly ambiguous points to query.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这些常见的不确定性采样技术提供了简单但有效的策略来识别高度模糊的点进行查询。
- en: 'Now, let’s explore the key benefits that uncertainty sampling provides for
    active ML:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们探索不确定性采样为主动机器学习提供的关键好处：
- en: Uncertainty sampling is a conceptually intuitive query strategy that is efficient
    to compute. Metrics such as confidence, margin, ratio, and entropy have clear
    uncertainty interpretations and can be calculated quickly.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不确定性采样是一种概念上直观且计算效率高的查询策略。如置信度、间隔、比率和熵等指标具有明确的不确定性解释，并且可以快速计算。
- en: It actively enhances model confidence in areas where it is uncertain, expanding
    knowledge boundaries. For example, a sentiment classifier can gain more certainty
    on ambiguous reviews containing rare phrases by querying the most uncertain cases
    for their true sentiment.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过查询最不确定的案例以获取其真实情感，积极增强模型在不确定区域中的信心，扩大知识边界。例如，情感分类器可以通过查询最不确定的案例来获得包含罕见短语的模糊评论的更多确定性。
- en: Uncertainty sampling is widely applicable across classification tasks and model
    types such as **support vector machines** (**SVMs**), logistic regression, random
    forests, and **neural networks** (**NNs**). Uncertainty applies broadly to classification
    tasks.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不确定性采样在分类任务和各种模型类型（如**支持向量机**（**SVMs**）、逻辑回归、随机森林和**神经网络**（**NNs**））中具有广泛的应用。不确定性在分类任务中具有广泛的应用。
- en: Uncertainty sampling is useful for anomaly detection by finding ambiguous outliers
    the model cannot explain. Uncertainty highlights unusual cases.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不确定性采样通过寻找模型无法解释的模糊异常值来用于异常检测。不确定性突出了不寻常的案例。
- en: It can identify labeling errors by seeking points with inconsistent predictions
    between models. High uncertainty may indicate noisy data.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以通过寻找模型之间预测不一致的点来识别标签错误。高不确定性可能表明数据有噪声。
- en: Overall, uncertainty sampling is a highly effective and versatile active ML
    method. It can be used in various domains and is intuitive and efficient. It helps
    expand a model’s capabilities and discover unknown points. Whether it’s used for
    classification, regression, or other ML tasks, uncertainty sampling consistently
    improves model performance. By selecting uncertain data points for annotation,
    the model learns from informative examples and improves predictions. It has proven
    useful in natural language processing, computer vision, and data mining. Uncertainty
    sampling actively acquires new knowledge and enhances ML models. While uncertainty
    sampling focuses on points the model is individually unsure of, query-by-committee
    approaches aim to add diversity by identifying points where an ensemble of models
    disagrees. We will discuss query-by-committee approaches in the next section.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，不确定性采样是一种高度有效和通用的主动机器学习方法。它可用于各种领域，直观且高效。它有助于扩展模型的能力并发现未知点。无论是用于分类、回归还是其他机器学习任务，不确定性采样都能持续提高模型性能。通过选择不确定数据点进行标注，模型从信息丰富的例子中学习并改进预测。它在自然语言处理、计算机视觉和数据挖掘中已被证明是有用的。不确定性采样积极获取新知识并增强机器学习模型。虽然不确定性采样关注模型个体不确定的点，但查询委员会方法旨在通过识别模型集成分歧的点来增加多样性。我们将在下一节中讨论查询委员会方法。
- en: Understanding query-by-committee approaches
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解查询委员会方法
- en: '**Query-by-committee** aims to add diversity by querying points where an ensemble
    of models disagrees the most. Different models will disagree where the data is
    most uncertain or ambiguous.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**查询委员会**旨在通过查询模型集成分歧最大的点来增加多样性。不同的模型会在数据最不确定或模糊的地方产生分歧。'
- en: In the query-by-committee approach, a group of models is trained using a labeled
    set of data. By doing so, the ensemble can work together and provide a more robust
    and accurate prediction.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在查询委员会方法中，一组模型使用标记数据集进行训练。通过这样做，集成模型可以协同工作并提供更稳健和准确的预测。
- en: One interesting aspect of this approach is that it identifies the data point
    that causes the most disagreement among the ensemble members. This data point
    is then chosen to be queried to obtain a label.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个有趣方面是，它确定了导致集成成员之间最大分歧的数据点。然后，这个数据点被选中进行查询以获取标签。
- en: 'The reason why this method works well is because different models tend to have
    the most disagreement on difficult and boundary examples, as depicted in *Figure
    2**.2*. These are the instances where there is ambiguity or uncertainty, and by
    focusing on these points of maximal disagreement, the ensemble can gain consensus
    and make more confident predictions:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法之所以有效，是因为不同的模型往往在难以处理和边界示例上存在最大分歧，如图*2**.2所示。这些是存在歧义或不确定性的实例，通过关注这些最大分歧的点，集成模型可以达成共识并做出更自信的预测：
- en: '![Figure 2.2 – Query-by-committee sampling with five unlabeled data points](img/B21789_02_02.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图2.2 – 使用五个未标记数据点的查询委员会采样](img/B21789_02_02.jpg)'
- en: Figure 2.2 – Query-by-committee sampling with five unlabeled data points
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 – 使用五个未标记数据点的查询委员会采样
- en: '*Figure 2**.2* reveals a disagreement between models 1 and 4, as opposed to
    models 2 and 3, regarding data point 2\. A similar pattern can be observed with
    data point 4\. Therefore, data points 2 and 4 have been chosen to be sent to the
    oracle for labeling.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2.2*揭示了模型1和4之间的分歧，而不是模型2和3，关于数据点2。类似的模式也可以在数据点4上观察到。因此，数据点2和4已被选中发送到预言机进行标记。'
- en: Query-by-committee is a widely used and effective active ML strategy that addresses
    the limitations of uncertainty sampling. While uncertainty sampling can be biased
    toward the current learner and may overlook crucial examples that are not within
    its estimator’s focus, query-by-committee overcomes these challenges. This approach
    involves maintaining multiple hypotheses simultaneously and selecting queries
    that lead to disagreements among these hypotheses. By doing so, it ensures a more
    comprehensive and diverse exploration of the data, ultimately enhancing the learning
    process. For instance, a committee of image classifiers may heavily disagree on
    ambiguous images that traditional sampling fails to capture. By querying labels
    for images with maximal disagreement, such as varied predictions for an unusual
    object, the committee collectively improves.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 查询委员会是一种广泛使用且有效的活跃机器学习策略，它解决了不确定性采样的局限性。虽然不确定性采样可能会偏向当前的学习者，并可能忽略其估计者关注范围之外的至关重要示例，但查询委员会克服了这些挑战。这种方法涉及同时维护多个假设，并选择导致这些假设之间产生不一致的查询。通过这样做，它确保了对数据的更全面和多样化的探索，从而增强了学习过程。例如，一个图像分类器的委员会可能会在传统采样无法捕捉到的模糊图像上产生很大的分歧。通过查询具有最大分歧的图像标签，例如对不寻常物体的不同预测，委员会可以集体改进。
- en: Some of the common techniques for query-by-committee sampling include **maximum
    disagreement**, **vote entropy**, and **average KL divergence**, all of which
    we will discuss now.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 查询委员会采样的常见技术包括**最大不一致**、**投票熵**和**平均KL散度**，我们将在下面讨论这些内容。
- en: Maximum disagreement
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最大不一致
- en: This simple approach looks at direct disagreement in the predicted labels between
    committee members. The data points where most ensemble members disagree on the
    label are queried. For example, if a three-model committee’s label votes for a
    point are (1, 2, 3), this exemplifies maximum disagreement as each model predicts
    a different class. Querying the points with the most label conflicts helps us
    focus only on cases that divide the committee. Maximum disagreement target instances
    create the largest rifts within the ensemble. Getting these high disagreement
    points labeled will aim to resolve the core differences between models.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这种简单的方法关注委员会成员之间预测标签的直接不一致。在标签上大多数集成成员存在分歧的数据点被查询。例如，如果一个三模型委员会的标签投票为（1，2，3），这表明最大不一致，因为每个模型预测了不同的类别。查询具有最多标签冲突的点有助于我们只关注分裂委员会的案例。最大不一致的目标实例在集成中造成了最大的分歧。对这些高分歧点进行标记的目的是解决模型之间的核心差异。
- en: 'Let’s explore a numerical example of the query-by-committee maximum disagreement
    method in active ML. For this example, we will consider a pool of 10 unlabeled
    data points, as shown in *Figure 2**.3*, that we want to label to train a classifier.
    We create two committee members (models) called M1 and M2\. We evaluate each unlabeled
    data point using M1 and M2 to get the following predicted labels:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个活跃机器学习中的查询委员会最大不一致方法的数值示例来探讨。在这个例子中，我们将考虑一个包含10个未标记数据点的集合，如图*图2.3*所示，我们希望对这些数据进行标记以训练一个分类器。我们创建了两个委员会成员（模型），分别称为M1和M2。我们使用M1和M2评估每个未标记数据点，以获得以下预测标签：
- en: '![Figure 2.3 – A numerical example to illustrate the query-by-committee maximum
    disagreement method](img/B21789_02_03.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图2.3 – 用数值示例说明查询委员会最大不一致方法](img/B21789_02_03.jpg)'
- en: Figure 2.3 – A numerical example to illustrate the query-by-committee maximum
    disagreement method
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 – 用数值示例说明查询委员会最大不一致方法
- en: Then, we select the data point with the maximum disagreement between the two
    committee members. Here, data points 1, 4, 5, 8, and 9 have different predicted
    labels by M1 and M2\. We select one of these points, say point 4, to query the
    true label from an oracle. We then add the newly labeled point to retrain the
    committee members.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们选择两个委员会成员之间分歧最大的数据点。在这里，数据点1、4、5、8和9被M1和M2预测为不同的标签。我们选择这些点中的一个，比如点4，来查询来自先知者的真实标签。然后，我们将新标记的点添加到重新训练委员会成员。
- en: 'We can do this with a simple Python script:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用一个简单的Python脚本来实现：
- en: '[PRE8]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This returns the following output:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回以下输出：
- en: '[PRE9]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This process is then repeated, querying points with maximum disagreement in
    labels predicted by the committee, until we reach sufficient performance. The
    most informative points surface through the maximum disagreement of the committee
    members.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 然后重复此过程，查询委员会预测标签分歧最大的点，直到达到足够的性能。最有信息量的点通过委员会成员的最大分歧显现出来。
- en: Vote entropy
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 投票熵
- en: 'This technique calculates the entropy over the label votes from each model
    in the ensemble committee. **Entropy** represents the overall uncertainty, where
    higher entropy means the models have a wider spread of predictions. Lower entropy
    indicates the models largely agree on the label. Querying the data points with
    maximum entropy in the vote distribution helps target the instances where the
    committee displays the highest collective uncertainty and disagreement. Getting
    these maximally entropic points labeled will push the ensemble toward greater
    consensus. Overall, vote entropy identifies cases that divide the committee the
    most, focusing labeling on their disagreements. If we go back to using a numerical
    example to better understand how the query-by-committee vote entropy method works,
    we can once again use a pool of 10 unlabeled data points, as shown in *Figure
    2**.4*, and a committee of two models, M1 and M2\. We get the following predicted
    probabilities for each class on the data points:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术计算了集成委员会中每个模型的标签投票的熵。**熵**代表整体不确定性，熵值越高意味着模型预测的分布越广。熵值较低表明模型在标签上大体上达成一致。查询投票分布中熵值最大的数据点有助于定位委员会显示最高集体不确定性和分歧的实例。对具有最大熵的点进行标记将推动集成向更大的一致性发展。总的来说，投票熵识别出最分裂委员会的案例，将标记重点放在它们的分歧上。如果我们回到使用数值示例来更好地理解基于委员会的投票熵方法的工作原理，我们可以再次使用一个包含10个未标记数据点的池，如图*图2**.4*所示，以及一个由两个模型M1和M2组成的委员会。我们对数据点上的每个类别得到以下预测概率：
- en: '![Figure 2.4 – A numerical example to illustrate the query-by-committee vote
    entropy method](img/B21789_02_04.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图2.4 – 用一个数值示例来说明基于委员会的投票熵方法](img/B21789_02_04.jpg)'
- en: Figure 2.4 – A numerical example to illustrate the query-by-committee vote entropy
    method
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 – 用一个数值示例来说明基于委员会的投票熵方法
- en: 'We calculate the vote entropy for each point as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按照以下方式计算每个点的投票熵：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>H</mi><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mo>=</mo><mo>−</mo><mrow><munder><mo>∑</mo><mi>i</mi></munder><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>y</mi><mo>|</mo><mi>x</mi></mrow></mfenced><mi mathvariant="normal">l</mi><mi
    mathvariant="normal">o</mi><mi mathvariant="normal">g</mi><mo>(</mo><mi>y</mi><mo>|</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></mrow></math>](img/6.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>H</mi><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mo>=</mo><mo>−</mo><mrow><munder><mo>∑</mo><mi>i</mi></munder><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>y</mi><mo>|</mo><mi>x</mi></mrow></mfenced><mi mathvariant="normal">l</mi><mi
    mathvariant="normal">o</mi><mi mathvariant="normal">g</mi><mo>(</mo><mi>y</mi><mo>|</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></mrow></math>](img/6.png)'
- en: Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/7.png)
    is averaged over the committee members.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/7.png)是平均每个委员会成员的。
- en: 'The probabilities from the committee members are averaged when calculating
    the vote entropy because we want to measure the total uncertainty or disagreement
    of the entire committee on a data point. By averaging, we essentially get the
    *vote* of the full committee on the probabilities of each class, rather than just
    considering individual members’ predictions. This allows us to select the data
    points where the committee has the most uncertainty or disagrees the most with
    its predictions:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算投票熵时，委员会成员的概率会被平均，因为我们想测量整个委员会对一个数据点的总不确定性或分歧。通过平均，我们实际上得到了整个委员会对每个类别的概率的“投票”，而不仅仅是考虑个别成员的预测。这使我们能够选择委员会在数据点上最不确定或与其预测分歧最大的数据点：
- en: '![Figure 2.5 – A numerical example to illustrate the query-by-committee vote
    entropy method with averages per class and entropy calculated](img/B21789_02_05.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.5 – 用每类的平均值和计算出的熵来展示查询委员会投票熵方法的一个数值示例](img/B21789_02_05.jpg)'
- en: Figure 2.5 – A numerical example to illustrate the query-by-committee vote entropy
    method with averages per class and entropy calculated
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5 – 用每类的平均值和计算出的熵来展示查询委员会投票熵方法的一个数值示例
- en: The points with maximum entropy will have the most disagreement among models.
    In *Figure 2**.5*, points 1, 4, 5, 8, and 9 have the highest entropy, so we query
    their labels. The next step would be to retrain the models and repeat the process.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 具有最大熵的点将在模型之间产生最多的分歧。在 *图 2**.5 中，点 1、4、5、8 和 9 具有最高的熵，因此我们查询它们的标签。下一步将是重新训练模型并重复此过程。
- en: 'We can write this with some Python code as well:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以用一些 Python 代码来写这个：
- en: '[PRE10]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output of this script is as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本的输出如下：
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Next, we will move our focus to computing predictions using the KL divergence
    method.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把重点转移到使用 KL 散度方法进行预测的计算上。
- en: Average KL divergence
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 平均 KL 散度
- en: This method measures the **Kullback-Leibler divergence** (**KL divergence**)
    between each committee member’s predicted label distribution and the average predicted
    distribution across all members.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法测量每个委员会成员预测的标签分布与所有成员的平均预测分布之间的**Kullback-Leibler 散度（KL 散度**）。
- en: 'KL divergence is defined as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: KL 散度定义为以下：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo>(</mo><mi>P</mi><mo>|</mo><mfenced
    open="|" close=")"><mi>Q</mi></mfenced><mo>=</mo><mo>−</mo><mrow><munder><mo>∑</mo><mrow><mi>x</mi><mo>∈</mo><mi>X</mi></mrow></munder><mrow><mi>P</mi><mo>(</mo><mi>x</mi><mo>)</mo><mi
    mathvariant="normal">l</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">g</mi><mo>(</mo><mfrac><mrow><mi>Q</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mrow><mi>P</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac><mo>)</mo></mrow></mrow></mrow></mrow></mrow></math>](img/8.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo>(</mo><mi>P</mi><mo>|</mo><mfenced
    open="|" close=")"><mi>Q</mi></mfenced><mo>=</mo><mo>−</mo><mrow><munder><mo>∑</mo><mrow><mi>x</mi><mo>∈</mo><mi>X</mi></mrow></munder><mrow><mi>P</mi><mo>(</mo><mi>x</mi><mo>)</mo><mi
    mathvariant="normal">l</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">g</mi><mo>(</mo><mfrac><mrow><mi>Q</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mrow><mi>P</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac><mo>)</mo></mrow></mrow></mrow></mrow></mrow></math>](img/8.png)'
- en: Here, P and Q are two probability distributions.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，P 和 Q 是两个概率分布。
- en: 'The data points with the highest average KL divergence are then queried. A
    higher KL divergence indicates a larger difference between a model’s predictions
    and the committee consensus. Querying points with maximum divergence targets instances
    where individual models strongly disagree with the overall ensemble. Labeling
    these high-divergence points will bring the individual models closer to the committee
    average. Average KL divergence identifies cases with outlying model predictions
    to focus labeling on reconciliation. Let’s take a look at our numerical example
    for the query-by-committee average KL divergence method. Again, we’re using the
    pool of 10 unlabeled data points, as shown in *Figure 2**.6*, and a committee
    of two models, M1 and M2\. We get the predicted class probabilities on each data
    point from M1 and M2 and calculate the KL divergence between M1’s and M2’s predictions
    for each point:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 具有最高平均KL散度的数据点将被查询。更高的KL散度表明模型预测与委员会共识之间的差异更大。查询最大差异的点旨在针对个体模型与整体集成强烈不一致的实例。标记这些高差异点将使个体模型更接近委员会的平均值。平均KL散度识别出具有异常模型预测的案例，以便将标记重点放在协调上。让我们看一下我们的查询委员会平均KL散度方法的数值示例。同样，我们使用*图2**.6*中显示的10个未标记数据点的池，以及一个由两个模型M1和M2组成的委员会。我们从M1和M2对每个数据点进行预测，并计算M1和M2预测之间的KL散度：
- en: '![Figure 2.6 – A numerical example to illustrate the query-by-committee average
    KL divergence method](img/B21789_02_06.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图2.6 – 一个数值示例，说明查询委员会平均KL散度方法](img/B21789_02_06.jpg)'
- en: Figure 2.6 – A numerical example to illustrate the query-by-committee average
    KL divergence method
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6 – 一个数值示例，说明查询委员会平均KL散度方法
- en: We average the KL divergence between M1 and M2, and M2 and M1\. We calculate
    the KL divergence in both directions (KL(M1||M2) and KL(M2||M1)) – because KL
    divergence is asymmetric, it will give different values depending on the direction.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算M1和M2之间以及M2和M1之间的KL散度平均值。我们计算两个方向的KL散度（KL(M1||M2)和KL(M2||M1)）——因为KL散度是非对称的，它将根据方向给出不同的值。
- en: The KL divergence from M1 to M2, KL(M1||M2), measures how well M2’s distribution
    approximates M1’s. On the other hand, KL(M2||M1) measures how well M1’s distribution
    approximates M2’s.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 从M1到M2的KL散度KL(M1||M2)衡量M2的分布如何近似M1的分布。另一方面，KL(M2||M1)衡量M1的分布如何近似M2的分布。
- en: In query-by-committee, we want to measure the total disagreement between the
    two committee members’ distributions. Just using KL(M1||M2) or just using KL(M2||M1)
    will not capture the full divergence. By taking the average of KL(M1||M2) and
    KL(M2||M1), we get a symmetric measure of the total divergence between the two
    distributions. This gives a better indication of the overall disagreement between
    the two committee members on a data point.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在查询委员会中，我们想要衡量两位委员会成员分布之间的总差异。仅仅使用KL(M1||M2)或仅仅使用KL(M2||M1)将无法捕捉到完整的差异。通过取KL(M1||M2)和KL(M2||M1)的平均值，我们得到两个分布之间总差异的对称度量。这为两个委员会成员在数据点上的整体差异提供了更好的指示。
- en: Taking the average KL in both directions ensures we select the points with maximum
    mutual divergence between the two models’ predictions. This surfaces the most
    informative points for labeling to resolve the committee’s uncertainty. The point
    with maximum average KL divergence has the most disagreement. So, here, points
    1, 4, 5, and 9 have the highest average KL divergence. We would then query these
    labels, retrain the models, and repeat the process.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个方向上取平均KL散度确保我们选择两个模型预测之间最大互变异的点。这揭示了标记以解决委员会不确定性的最有信息量的点。具有最大平均KL散度的点是分歧最大的点。因此，这里点1、4、5和9具有最高的平均KL散度。然后我们会查询这些标签，重新训练模型，并重复此过程。
- en: 'We can write this with some Python code as well:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以用一些Python代码来实现这一点：
- en: '[PRE12]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Here’s the output we get:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的输出结果：
- en: '[PRE13]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Upon exploring different methods that are used to perform query-by-committee
    sampling, we’ve noticed that it can be quite computationally intensive. Indeed,
    as shown in *Figure 2**.7*, the query-by-committee technique is an iterative approach
    that requires retraining the models from the committee every time new labeled
    data is added to the training set:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索用于执行查询委员会采样的不同方法时，我们发现这可能会非常计算密集。确实，如图*图2**.7*所示，查询委员会技术是一种迭代方法，每次向训练集中添加新的标记数据时，都需要重新训练委员会中的模型：
- en: '![Figure 2.7 – The iteration process in the query-by-committee sampling technique](img/B21789_02_07.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图2.7 – 查询委员会采样技术中的迭代过程](img/B21789_02_07.jpg)'
- en: Figure 2.7 – The iteration process in the query-by-committee sampling technique
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7 – 查询委员会采样技术中的迭代过程
- en: 'In conclusion, this technique is designed to identify informative query points
    by quantifying the level of disagreement among the ensemble models and offers
    several key advantages:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，这项技术旨在通过量化集成模型之间的一致性水平来识别信息查询点，并提供了几个关键优势：
- en: It promotes diversity by finding points that various models interpret differently.
    The techniques can effectively prioritize query points that are likely to provide
    valuable information, thus improving the overall quality of the query-based learning
    process.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过寻找各种模型解释不同的点来促进多样性。这些技术可以有效地优先处理可能提供有价值信息的查询点，从而提高基于查询的学习过程的整体质量。
- en: It encourages exploration by actively seeking out query points that are less
    predictable or commonly known, allowing for a more comprehensive understanding
    of the dataset.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过积极寻找不太可预测或常见的查询点来鼓励探索，从而对数据集有更全面的理解。
- en: It provides the ability to construct committees with an array of distinct models
    such as SVMs, NNs, and many others. This versatility allows for a diverse range
    of strategies and approaches to be employed when making important decisions. By
    leveraging these various models, you can gain deeper insights and improve the
    overall performance of your committee. The query-by-committee approach differs
    from traditional techniques such as bagging and boosting. Its main objective is
    to choose the most informative unlabeled data points for labeling and inclusion
    in the training set. On the other hand, traditional ensemble methods such as bagging
    and boosting focus on combining multiple models to enhance overall predictive
    performance. Indeed, query-by-committee methods calculate disagreement between
    committee members to find optimal queries for labeling, as we have seen previously.
    Traditional ensembles combine predictions through voting or averaging to produce
    a unified prediction.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供了构建包含SVMs、NNs等多种不同模型的委员会的能力。这种多功能性允许在做出重要决策时采用多样化的策略和方法。通过利用这些不同的模型，你可以获得更深入的见解并提高委员会的整体性能。查询委员会方法与传统技术（如bagging和boosting）不同。其主要目标是选择最有信息的未标记数据点进行标记并包含在训练集中。另一方面，传统的集成方法（如bagging和boosting）侧重于结合多个模型以增强整体预测性能。确实，查询委员会方法通过计算委员会成员之间的不一致性来找到最佳的查询点，正如我们之前所看到的。传统的集成通过投票或平均来组合预测以产生统一的预测。
- en: It is very useful in situations where the unlabeled data pool contains a limited
    representation of the underlying distribution. By combining the opinions and predictions
    of different committee members, query-by-committee methods can effectively address
    the challenges posed by poorly covered distributions in unlabeled data pools.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在未标记数据池中，底层分布的表示有限的情况下，它非常有用。通过结合不同委员会成员的意见和预测，查询委员会方法可以有效地解决未标记数据池中覆盖不良分布所提出的挑战。
- en: By leveraging the varying opinions of the committee, query-by-committee enhances
    the overall performance of the learning system.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用委员会的不同意见，查询委员会增强了学习系统的整体性能。
- en: We will now delve deeper into the implementation of EMC strategies and explore
    their potential benefits.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将更深入地探讨EMC策略的实施及其潜在优势。
- en: Labeling with EMC sampling
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用EMC采样进行标记
- en: EMC aims to query points that will induce the greatest change in the current
    model when labeled and trained on. This focuses labeling on points with the highest
    expected impact.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: EMC旨在查询那些在被标记和训练后将对当前模型产生最大变化的点。这侧重于对具有最高预期影响的点进行标记。
- en: EMC techniques involve selecting a specific data point to label and learn from
    to cause the most significant alteration to the current model’s parameters and
    predictions. The core idea is to query the point that would impact the maximum
    change to the model’s parameters if we knew its label. By carefully identifying
    this particular data point, the EMC method aims to maximize the impact on the
    model and improve its overall performance. The process involves assessing various
    factors and analyzing the potential effects of each data point, ultimately choosing
    the one that is expected to yield the most substantial changes to the model, as
    depicted in *Figure 2**.8*. The goal is to enhance the model’s accuracy and make
    it more effective in making predictions.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: EMC 技术涉及选择一个特定的数据点进行标记和学习，以对当前模型的参数和预测造成最大的改变。核心思想是查询如果知道其标签，将对模型参数的最大变化产生影响的点。通过仔细识别这个特定的数据点，EMC
    方法旨在最大化对模型的影响并提高其整体性能。这个过程涉及评估各种因素和分析每个数据点的潜在影响，最终选择预期将对模型产生最大变化的那个，如图*图 2*.8*所示。目标是提高模型的准确性并使其在做出预测时更加有效。
- en: 'When we refer to querying points that lead to larger updates in the model targets,
    what we are discussing is identifying highly informative examples located in uncertain
    areas of the input space. These examples play a crucial role in influencing and
    enhancing the model’s performance. By paying attention to these specific instances,
    we can gain deeper insights into the complexities and subtleties of the input
    space, resulting in a more thorough understanding and improved overall outcomes:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们提到查询导致模型目标更新更大的点时，我们讨论的是在输入空间的不确定区域中识别高度信息性的示例。这些示例在影响和提升模型性能方面起着至关重要的作用。通过关注这些特定实例，我们可以更深入地了解输入空间的复杂性和微妙之处，从而实现更全面的理解和改进的整体结果：
- en: '![Figure 2.8 – EMC sampling](img/B21789_02_08.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.8 – EMC 采样](img/B21789_02_08.jpg)'
- en: Figure 2.8 – EMC sampling
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.8 – EMC 采样
- en: The initial model is trained using the training dataset. Then, the unlabeled
    samples are evaluated based on the changes in model outputs after including them
    in the training set. In *Figure 2**.8*, the graphs show the resulting **model
    output change** (**MOC**) for three example samples. The sample that leads to
    the largest output change, when considering all data, is chosen to be labeled
    next.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 初始模型使用训练数据集进行训练。然后，根据将未标记样本包括在训练集中后模型输出的变化来评估这些样本。在*图 2*.8*中，图表显示了三个示例样本导致的**模型输出变化**（**MOC**）。在考虑所有数据的情况下，导致最大输出变化的样本被选中进行标记。
- en: In other words, in EMC sampling, the process begins by ranking the unlabeled
    examples. This ranking is determined by estimating the expected change that would
    occur in the model’s predictions if each example were to be labeled. This estimation
    takes into account various factors and considerations, ultimately providing a
    basis for the prioritization of labeling.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，在 EMC 采样中，过程从对未标记的示例进行排序开始。这种排序是通过估计如果每个示例都被标记，模型预测中预期会发生的变化来确定的。这种估计考虑了各种因素和考虑事项，最终为标记优先级提供了依据。
- en: 'This estimation is typically based on calculating the **expected gradient length**
    (**EGL**). The EGL method estimates the expected length of the gradient of the
    loss function if the model was trained on the newly labeled point. When training
    discriminative probabilistic models, gradient-based optimization is commonly used.
    To assess the *change* in the model, we can examine the size of the training gradient.
    This gradient refers to the vector that is employed to update the parameter values
    during the training process. In simpler terms, the learner should choose the instance,
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>G</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">*</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/9.png),
    that, when labeled and included in the labeled dataset, (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="script">L</mml:mi></mml:math>](img/10.png)),
    would result in the greatest magnitude for the new training gradient:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这种估计通常基于计算**预期梯度长度**（**EGL**）。EGL方法估计如果模型在新的标记点上训练，损失函数梯度的预期长度。在训练判别概率模型时，基于梯度的优化通常被使用。为了评估模型的**变化**，我们可以检查训练梯度的尺寸。这个梯度指的是在训练过程中用来更新参数值的向量。简单来说，学习者应该选择实例，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>G</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">*</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/9.png)，当标记并包含在标记数据集中时，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="script">L</mml:mi></mml:math>](img/10.png))，将导致新的训练梯度具有最大的幅度：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msubsup><mi>x</mi><mrow><mi>E</mi><mi>G</mi><mi>L</mi></mrow><mi
    mathvariant="normal">*</mi></msubsup><mo>=</mo><munder><mi>argmax</mi><mi>x</mi></munder><mrow><munder><mo>∑</mo><mi>i</mi></munder><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>|</mo><mi>x</mi><mo>;</mo><mi>θ</mi></mrow></mfenced><mfenced
    open="‖" close="‖"><mrow><mo>∇</mo><mi mathvariant="script">l</mi><mo>(</mo><mi
    mathvariant="script">L</mi><mo>∪</mo><mfenced open="〈" close="〉"><mrow><mi>x</mi><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></mfenced><mo>;</mo><mi>θ</mi><mo>)</mo></mrow></mfenced></mrow></mrow></mrow></mrow></math>](img/11.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msubsup><mi>x</mi><mrow><mi>E</mi><mi>G</mi><mi>L</mi></mrow><mi
    mathvariant="normal">*</mi></msubsup><mo>=</mo><munder><mi>argmax</mi><mi>x</mi></munder><mrow><munder><mo>∑</mo><mi>i</mi></munder><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>|</mo><mi>x</mi><mo>;</mo><mi>θ</mi></mrow></mfenced><mfenced
    open="‖" close="‖"><mrow><mo>∇</mo><mi mathvariant="script">l</mi><mo>(</mo><mi
    mathvariant="script">L</mi><mo>∪</mo><mfenced open="〈" close="〉"><mrow><mi>x</mi><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></mfenced><mo>;</mo><mi>θ</mi><mo>)</mo></mrow></mfenced></mrow></mrow></mrow></mrow></math>](img/11.png)'
- en: Here, ||.|| is the Euclidean norm of each resulting gradient vector, θ is the
    model parameters, x is an unlabeled point, y is the predicted label for x, and
    ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mo>∇</mo><mi
    mathvariant="script">l</mi><mo>(</mo><mi mathvariant="script">L</mi><mo>∪</mo><mfenced
    open="〈" close="〉"><mrow><mi>x</mi><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></mfenced><mo>;</mo><mi>θ</mi><mo>)</mo></mrow></mrow></mrow></math>](img/12.png)
    is the new gradient that would be obtained by adding the training tuple, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    open="⟨" close="⟩" separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/13.png),
    to the labeled dataset, (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="script">L</mml:mi></mml:math>](img/14.png)).
    Data points that result in a longer expected gradient are prioritized for querying.
    A longer gradient indicates a greater expected change in the model parameters
    during training. By selecting points with high expected gradient length, the model
    focuses on samples that will highly influence the model once labeled. This targets
    points in uncertain regions that will have an outsized impact on updating model
    predictions. In short, EGL identifies data points that are likely to substantially
    reshape the decision boundary.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，||.|| 是每个结果梯度向量的欧几里得范数，θ 是模型参数，x 是一个未标记的点，y 是对 x 的预测标签，![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mo>∇</mo><mi
    mathvariant="script">l</mi><mo>(</mo><mi mathvariant="script">L</mi><mo>∪</mo><mfenced
    open="〈" close="〉"><mrow><mi>x</mi><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></mfenced><mo>;</mo><mi>θ</mi><mo>)</mo></mrow></mrow></mrow></math>](img/12.png)
    是通过将训练元组 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    open="⟨" close="⟩" separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/13.png)
    添加到标记数据集 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="script">L</mml:mi></mml:math>](img/14.png) 中所获得的新梯度。具有更长预期梯度的数据点被优先用于查询。更长的梯度表示在训练过程中模型参数的预期变化更大。通过选择具有高预期梯度长度的点，模型专注于一旦标记将对模型产生重大影响的样本。这针对的是那些将对模型预测更新产生巨大影响的不确定区域中的点。简而言之，EGL
    识别出那些可能显著重塑决策边界的数据点。
- en: By employing this technique, the algorithm can pinpoint and identify the specific
    data points that are anticipated to yield substantial alterations in the model’s
    predictions. The selected examples are subsequently sent for labeling as they
    are believed to possess the most value in terms of training the model effectively.
    Once designated, these informative samples are seamlessly integrated into the
    existing training data, thereby facilitating the necessary updates and improvements
    to the model’s overall performance and accuracy.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 通过采用这种技术，算法可以精确地识别出预计将对模型预测产生重大改变的具体数据点。选定的示例随后被发送进行标记，因为它们被认为在有效训练模型方面具有最大的价值。一旦指定，这些信息样本就会无缝地集成到现有的训练数据中，从而促进对模型整体性能和准确性的必要更新和改进。
- en: 'There are several key advantages of the EMC method:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: EMC 方法有几个关键优势：
- en: Its ability to actively seek out influential and under-represented regions within
    the input space. This means that it can effectively identify areas that may have
    been overlooked or not given enough attention in a traditional modeling approach.
    Suppose we are training a model to predict housing prices. The input features
    are things such as square footage, number of bedrooms, location, and so on. Using
    a traditional modeling approach, we may collect a random sample of houses to train
    the model on. However, this could lead to certain neighborhoods or house styles
    being under-represented if they are less common. The EMC method would analyze
    the current model and identify areas where new training data would likely lead
    to the largest change in the model predictions. For example, it may find that
    adding more samples from older houses could better calibrate the model’s understanding
    of how age affects price, or gathering data from a new suburban development that
    is under-represented could improve the performance of houses in that area. By
    actively seeking these influential regions, EMC can make the model more robust
    with fewer overall training examples. It reduces the risk of underfitting certain
    areas of the input space compared to passive or random data collection. It can
    help uncover hidden patterns or relationships that may not be immediately apparent,
    further enhancing the overall understanding of the dataset.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它能够在输入空间内积极寻找有影响力和代表性不足的区域。这意味着它可以有效地识别在传统建模方法中可能被忽视或未得到足够关注的区域。假设我们正在训练一个预测房价的模型。输入特征包括平方英尺、卧室数量、位置等等。使用传统的建模方法，我们可能会收集一个随机样本的房屋来训练模型。然而，如果某些社区或房屋风格不太常见，这可能会导致某些社区或房屋风格代表性不足。EMC方法会分析当前模型，并确定新训练数据可能导致模型预测最大变化的地方。例如，它可能会发现添加更多旧房屋的样本可以更好地校准模型对年龄影响价格的理解，或者从代表性不足的新郊区开发中收集数据可以提高该地区房屋的性能。通过积极寻找这些有影响力的区域，EMC可以使模型在更少的整体训练示例下更加稳健。与被动或随机数据收集相比，它降低了输入空间中某些区域欠拟合的风险。它可以帮助揭示可能不是立即显而易见的隐藏模式或关系，从而进一步增强对数据集的整体理解。
- en: It is compatible with probabilistic and kernel-based models. By leveraging the
    probabilistic nature of these models, the EMC method can provide insightful and
    accurate predictions. Additionally, its compatibility with kernel-based models
    allows for an enhanced understanding of complex data patterns and relationships.
    This combination of features makes the EMC method a powerful tool for analyzing
    and interpreting data in a wide range of domains.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它与概率模型和基于核的模型兼容。通过利用这些模型的概率性质，EMC方法可以提供有洞察力和准确的预测。此外，它与基于核的模型的兼容性允许对复杂数据模式和相关关系有更深入的理解。这些特性的结合使EMC方法成为分析和解释广泛领域数据的有力工具。
- en: It allows for estimation without the need for full retraining at each step.
    This means that the process can be more efficient and less time-consuming as it
    eliminates the need to repeatedly train the model from scratch. Instead, the method
    enables model changes to be estimated by focusing on the expected changes in the
    model’s parameters. By utilizing this approach, you can save valuable time and
    resources while still obtaining accurate and reliable estimations.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它允许在每一步无需完全重新训练的情况下进行估计。这意味着过程可以更加高效且节省时间，因为它消除了反复从头开始训练模型的需求。相反，该方法通过关注模型参数的预期变化来估计模型的变化。通过利用这种方法，你可以在仍然获得准确和可靠估计的同时，节省宝贵的时间和资源。
- en: In summary, EMC queries aim to identify points with the highest potential impact
    on the model. It selects those with the maximum expected impact. This method is
    widely discussed in literature but not implemented in practice due to its high
    computational cost.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，EMC查询旨在识别对模型影响最大的点。它选择那些预期影响最大的点。这种方法在文献中广泛讨论，但由于其高计算成本，在实践中并未得到实施。
- en: Next, we’ll explore EER sampling. This technique reduces the model’s error by
    selecting points that are expected to contribute the most to error reduction.
    By strategically sampling these points, we can improve overall model accuracy.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨EER采样。这种技术通过选择预期对错误减少贡献最大的点来减少模型的误差。通过战略性地采样这些点，我们可以提高整体模型的准确性。
- en: Sampling with EER
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用EER进行采样
- en: 'EER focuses on measuring the potential decrease in generalization error instead
    of the expected change in the model, as seen in the previous approach. The goal
    is to estimate the anticipated future error of a model by training it with the
    current labeled set and the remaining unlabeled samples. EER can be defined as
    follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: EER 关注于衡量泛化误差的潜在下降，而不是像先前方法中看到的那样，关注模型期望的变化。目标是通过对当前标记集和剩余未标记样本进行训练来估计模型预期的未来误差。EER
    可以定义为以下：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>E</mi><msub><mover><mi>P</mi><mo
    stretchy="true">ˆ</mo></mover><mi mathvariant="script">L</mi></msub></msub><mo>=</mo><mrow><msub><mo>∫</mo><mi>x</mi></msub><mrow><mi>L</mi><mfenced
    open="(" close=")"><mrow><mi>P</mi><mfenced open="(" close=")"><mrow><mi>y</mi><mo>|</mo><mi>x</mi></mrow></mfenced><mo>,</mo><mover><mi>P</mi><mo
    stretchy="true">ˆ</mo></mover><mfenced open="(" close=")"><mrow><mi>y</mi><mo>|</mo><mi>x</mi></mrow></mfenced></mrow></mfenced><mi>P</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></mrow></math>](img/15.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>E</mi><msub><mover><mi>P</mi><mo
    stretchy="true">ˆ</mo></mover><mi mathvariant="script">L</mi></msub></msub><mo>=</mo><mrow><msub><mo>∫</mo><mi>x</mi></msub><mrow><mi>L</mi><mfenced
    open="(" close=")"><mrow><mi>P</mi><mfenced open="(" close=")"><mrow><mi>y</mi><mo>|</mo><mi>x</mi></mrow></mfenced><mo>,</mo><mover><mi>P</mi><mo
    stretchy="true">ˆ</mo></mover><mfenced open="(" close=")"><mrow><mi>y</mi><mo>|</mo><mi>x</mi></mrow></mfenced></mrow></mfenced><mi>P</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></mrow></math>](img/15.png)'
- en: Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="script">L</mml:mi></mml:math>](img/16.png) is the pool of paired
    labeled data, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/17.png),
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mover
    accent="true"><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi
    mathvariant="script">L</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/18.png)
    is the estimated output distribution. L is a chosen loss function that measures
    the error between the true distribution, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/19.png),
    and the learner’s prediction, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mover
    accent="true"><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi
    mathvariant="script">L</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/20.png).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="script">L</mml:mi></mml:math>](img/16.png) 是成对标记数据的集合，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/17.png)，和![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mover
    accent="true"><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi
    mathvariant="script">L</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/18.png)
    是估计的输出分布。L 是一个选择的损失函数，用于衡量真实分布 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/19.png)
    和学习者的预测 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mover
    accent="true"><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi
    mathvariant="script">L</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/20.png)
    之间的误差。
- en: This involves selecting the instance that is expected to have the lowest future
    error (referred to as *risk*) for querying. This focuses active ML on reducing
    long-term generalization errors rather than just immediate training performance.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这涉及到选择预计未来误差（称为*风险*）最低的实例进行查询。这使主动机器学习专注于减少长期泛化误差，而不仅仅是立即的训练性能。
- en: In other words, EER selects unlabeled data points that, when queried and learned
    from, are expected to significantly reduce the model’s errors on new data points
    from the same distribution. By focusing on points that minimize future expected
    errors, as shown in *Figure 2**.9*, EER aims to identify valuable training examples
    that will enhance the model’s ability to generalize effectively. This technique
    targets high-value training examples that will improve the model’s performance
    by minimizing incorrect predictions.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，EER选择那些在查询和学习后预计将显著减少模型在相同分布的新数据点上错误的数据点。通过关注如图*图2**.9所示的最小化未来预期误差的点，EER旨在识别有价值的训练示例，这将增强模型有效泛化的能力。这项技术针对高价值的训练示例，通过最小化错误预测来提高模型的表现。
- en: 'This approach helps prevent short-term overfitting by avoiding the inclusion
    of redundant similar examples and instead focusing on diverse edge cases that
    better span the feature space. For instance, in the case of an image classifier,
    the technique may prioritize the inclusion of diverse edge cases that capture
    a wide range of features rather than including redundant similar examples:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法通过避免包含冗余的相似示例，而是专注于更好地覆盖特征空间的多样化边缘情况，有助于防止短期过拟合。例如，在图像分类器的情况下，该技术可能会优先考虑包含能够捕捉广泛特征的多样化边缘情况，而不是包含冗余的相似示例：
- en: '![Figure 2.9 – EER sampling](img/B21789_02_09.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图2.9 – EER采样](img/B21789_02_09.jpg)'
- en: Figure 2.9 – EER sampling
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9 – EER采样
- en: Computing the expected model’s prediction error can be done using various loss
    functions, such as the log loss, which is defined as ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>L</mi><mo>=</mo><mrow><msub><mo>∑</mo><mrow><mi>y</mi><mo>∈</mo><mi>Y</mi></mrow></msub><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>y</mi><mo>|</mo><mi>x</mi></mrow></mfenced><mi mathvariant="normal">l</mi><mi
    mathvariant="normal">o</mi><mi mathvariant="normal">g</mi><mo>(</mo><msub><mover><mi>P</mi><mo
    stretchy="true">ˆ</mo></mover><mi mathvariant="script">L</mi></msub><mfenced open="("
    close=")"><mrow><mi>y</mi><mo>|</mo><mi>x</mi></mrow></mfenced><mo>)</mo></mrow></mrow></mrow></mrow></math>](img/21.png),
    or the 0/1 loss, which is defined as ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>L</mi><mo>=</mo><mn>1</mn><mo>−</mo><munder><mi>max</mi><mrow><mi>y</mi><mo>∈</mo><mi>Y</mi></mrow></munder><msub><mover><mi>P</mi><mo
    stretchy="true">ˆ</mo></mover><mi mathvariant="script">L</mi></msub><mfenced open="("
    close=")"><mrow><mi>y</mi><mo>|</mo><mi>x</mi></mrow></mfenced></mrow></mrow></math>](img/22.png).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用各种损失函数来计算预期模型的预测误差，例如对数损失，其定义为 ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>L</mi><mo>=</mo><mrow><msub><mo>∑</mo><mrow><mi>y</mi><mo>∈</mo><mi>Y</mi></mrow></msub><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>y</mi><mo>|</mo><mi>x</mi></mrow></mfenced><mi mathvariant="normal">l</mi><mi
    mathvariant="normal">o</mi><mi mathvariant="normal">g</mi><mo>(</mo><msub><mover><mi>P</mi><mo
    stretchy="true">ˆ</mo></mover><mi mathvariant="script">L</mi></msub><mfenced open="("
    close=")"><mrow><mi>y</mi><mo>|</mo><mi>x</mi></mrow></mfenced><mo>)</mo></mrow></mrow></mrow></mrow></math>](img/21.png)，或者0/1损失，其定义为
    ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>L</mi><mo>=</mo><mn>1</mn><mo>−</mo><munder><mi>max</mi><mrow><mi>y</mi><mo>∈</mo><mi>Y</mi></mrow></munder><msub><mover><mi>P</mi><mo
    stretchy="true">ˆ</mo></mover><mi mathvariant="script">L</mi></msub><mfenced open="("
    close=")"><mrow><mi>y</mi><mo>|</mo><mi>x</mi></mrow></mfenced></mrow></mrow></math>](img/22.png).
- en: 'EER incurs a significant time cost due to the estimation of error reduction.
    To calculate the expected generalization error, the classifier must be re-optimized
    for each data point, considering its possible labels. Additionally, it is necessary
    to re-infer the labels of other data points. However, this technique offers a
    couple of key advantages:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 由于误差减少的估计，EER会带来显著的时间成本。为了计算预期的泛化误差，分类器必须针对每个数据点重新优化，考虑其可能的标签。此外，还需要重新推断其他数据点的标签。然而，这项技术提供了一些关键优势：
- en: It allows direct optimization of the true objective of reducing the generalization
    error instead of solely focusing on improving training performance. By prioritizing
    the reduction of the generalization error, EER allows for more accurate and reliable
    predictions in real-world scenarios. This not only enhances the overall performance
    of the model but also ensures that it can effectively generalize to unseen data.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它允许直接优化减少泛化误差的真实目标，而不是仅仅关注提高训练性能。通过优先考虑减少泛化误差，EER允许在现实世界场景中进行更准确和可靠的预测。这不仅提高了模型的总体性能，还确保了它能够有效地推广到未见过的数据。
- en: It considers the impact on unseen data points, going beyond just the training
    set. By doing so, EER helps to mitigate overfitting, which is a common challenge
    in ML and statistical modeling. Overfitting occurs when a model performs exceedingly
    well on the training data but fails to generalize well to new, unseen data. EER
    tackles this issue head-on by incorporating a comprehensive evaluation of potential
    errors and their reduction. This ensures that the model’s performance is not limited
    to the training set and instead extends to real-world scenarios, making it a valuable
    tool in data-driven decision-making.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它考虑了对未见数据点的影响，而不仅仅是训练集。通过这样做，EER有助于减轻过拟合，这是机器学习和统计建模中常见的挑战。过拟合发生在模型在训练数据上表现极好，但无法很好地推广到新的、未见过的数据时。EER通过综合评估潜在错误及其减少来直接应对这个问题。这确保了模型的表现不仅限于训练集，而是扩展到现实世界场景，使其成为数据驱动决策的有价值工具。
- en: EER is a predictive and robust query framework that focuses on maximally reducing
    the model’s generalization error. Similar to the EMC method, the EER method is
    a topic of extensive discussion in literature. However, it has not been widely
    adopted in practical applications primarily because of the significant computational
    resources it demands.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: EER是一个预测性和鲁棒的查询框架，专注于最大限度地减少模型的一般化误差。与EMC方法类似，EER方法在文献中是一个广泛讨论的话题。然而，它并没有在实用应用中得到广泛采用，主要是因为它对计算资源的需求非常显著。
- en: The next sampling method that we will explore, density-weighted sampling, aims
    to improve diversity by selecting representative points from all density regions.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要探索的下一个采样方法是密度加权采样，旨在通过从所有密度区域中选择代表性点来提高多样性。
- en: Understanding density-weighted sampling methods
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解密度加权采样方法
- en: '**Density-weighted methods** are approaches that aim to carefully choose points
    that accurately represent the densities of their respective local neighborhoods.
    By doing so, these methods prioritize the labeling of diverse cluster centers,
    ensuring a comprehensive and inclusive representation of the data.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '**密度加权方法**是旨在仔细选择能够准确代表其各自局部邻域密度的点的途径。通过这样做，这些方法优先考虑对多样化聚类中心的标记，确保数据的全面和包容性表示。'
- en: Density-weighted techniques are highly beneficial and effective when it comes
    to querying points. These techniques utilize a clever combination of an informativeness
    measure and a density weight. An **informativeness measure** provides a score
    of how useful a data point would be for improving the model if we queried its
    label. Higher informativeness indicates the point is more valuable to label and
    add to the training set. In this chapter, we have explored several informativeness
    measures, such as uncertainty and disagreement. In density-weighted methods, the
    informativeness score is combined with a density weight to ensure we select representative
    and diverse queries across different regions of the input space. This is done
    by assigning a weight to each data point based on both its density and its informativeness.
    Data points with higher informativeness and lower density will be given a higher
    weight, and will therefore be more likely to be selected for labeling. Points
    in dense clusters receive lower weights. The density and informativeness are combined
    through multiplicative, exponential, or additive formulations. This balances informativeness
    with density to achieve diversity.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 密度加权技术在查询点时非常有用和有效。这些技术利用了信息性度量与密度权重的一个巧妙组合。一个 **信息性度量** 提供了一个分数，表示如果我们查询数据点的标签，它对改进模型有多有用。信息性越高，表示该点对标记和添加到训练集越有价值。在本章中，我们探讨了几个信息性度量，如不确定性和不一致性。在密度加权方法中，信息性分数与密度权重相结合，以确保我们在输入空间的不同区域选择具有代表性且多样化的查询。这是通过根据数据点的密度和信息性为其分配权重来实现的。信息性高且密度低的数据点将获得更高的权重，因此更有可能被选中进行标记。密集聚类中的点将获得较低的权重。密度和信息性通过乘法、指数或加法公式相结合。这平衡了信息性和密度，以实现多样性。
- en: 'The density weight represents the density of the local neighborhood surrounding
    each point, allowing for a more comprehensive and accurate sampling of points
    from different densities, as shown in *Figure 2**.10*. This approach avoids the
    pitfall of solely focusing on dense clusters, which could result in redundant
    points. By taking into account the density weight, these techniques guarantee
    that the selected points effectively capture the overall distribution of the dataset.
    As a result, the obtained results are more meaningful and provide deeper insights
    into the data:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 密度权重表示每个点周围局部邻域的密度，允许从不同密度中更全面和准确地采样点，如图 *图 2**.10 所示。这种方法避免了仅仅关注密集聚类的陷阱，这可能导致冗余点。通过考虑密度权重，这些技术保证所选点能够有效地捕捉数据集的整体分布。因此，获得的结果更有意义，并提供了对数据的更深入洞察：
- en: '![Figure 2.10 – Density-weighted sampling](img/B21789_02_10.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.10 – 密度加权采样](img/B21789_02_10.jpg)'
- en: Figure 2.10 – Density-weighted sampling
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.10 – 密度加权采样
- en: In *Figure 2**.10*, we can observe the significance of sample density in the
    active ML process. With instance 1 being positioned near the decision boundary,
    this makes it a prime candidate for being chosen as the most uncertain one. However,
    if we analyze the situation more closely, it becomes apparent that selecting instance
    2 would be more advantageous in terms of enhancing the overall quality of the
    model. This is because instance 2 not only represents itself accurately but also
    acts as a representative for other instances within the data distribution. Therefore,
    its inclusion in the active ML process can lead to more comprehensive and reliable
    model improvements.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 2**.10 中，我们可以观察到样本密度在主动机器学习过程中的重要性。由于实例 1 位于决策边界附近，这使得它成为被选为最不确定实例的理想候选者。然而，如果我们更仔细地分析情况，很明显，选择实例
    2 在提高模型整体质量方面会更有利。这是因为实例 2 不仅能够准确代表自身，而且还能代表数据分布中的其他实例。因此，将其纳入主动机器学习过程可以导致更全面和可靠的模型改进。
- en: Implementing queries that would select instance 2 in the preceding example can
    be done with various density-weighted sampling methods, such as kNN density, **kernel
    density estimation** (**KDE**), K-means clustering, and **maximum mean** **discrepancy**
    (**MMD**).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，通过各种密度加权采样方法（如 kNN 密度、**核密度估计**（**KDE**）、K-均值聚类和**最大均值差异**（**MMD**））实现选择实例
    2 的查询可以完成。
- en: 'Let’s start with the imports and generating the dummy data:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从导入和生成虚拟数据开始：
- en: '[PRE14]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, let’s understand and apply different density-based techniques for our
    `X` sample data:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们了解并应用不同的基于密度的技术来处理我们的 `X` 样本数据：
- en: '**kNN density** calculates the local density around each data point using its
    *k-nearest neighbors*. The density is estimated by taking the inverse of the average
    distance to the k-closest points. Denser points have higher density, while isolated
    points have lower density. The estimated density is then used as a weight. When
    combined with informativeness criteria such as uncertainty, points in sparser
    neighborhoods get higher density weights, increasing their priority. kNN density
    weighting provides an efficient way to increase sample diversity and avoid over-sampling
    clusters when querying:'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**kNN密度**通过每个数据点的**k个最近邻**来计算其周围的局部密度。密度是通过取到k个最近点平均距离的倒数来估计的。密度较高的点具有更高的密度，而孤立点具有较低的密度。估计的密度随后用作权重。当与信息性标准（如不确定性）结合时，稀疏邻域中的点会获得更高的密度权重，从而提高它们的优先级。kNN密度加权提供了一种有效的方法来增加样本多样性，并在查询时避免对簇进行过度采样：'
- en: '[PRE15]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**KDE** estimates the local density around each point using a kernel function
    centered on the point. Typically, a Gaussian kernel is used. The densities from
    the kernels of nearby points are summed to get the overall estimated density.
    As with kNN density, points in sparser regions will have lower kernel density
    compared to crowded areas. These density values can be used as weights when combined
    with informativeness criteria. Points in isolated clusters will be up-weighted,
    increasing their query priority. KDE provides a smooth, probabilistic estimate
    of local density, as opposed to the discrete clusters of kNN. It is more computationally
    expensive than kNN but can be implemented efficiently in high dimensions. KDE
    weighting focuses sampling on representative low-density points:'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**核密度估计（KDE**）使用以点为中心的核函数来估计每个点周围的局部密度。通常使用高斯核。通过将附近点的核密度求和来获得整体估计密度。与kNN密度类似，稀疏区域的点与拥挤区域相比将具有较低的核密度。这些密度值可以与信息性标准结合使用作为权重。孤立簇中的点将被赋予更高的权重，从而提高它们的查询优先级。KDE提供了一种平滑、概率性的局部密度估计，与kNN的离散簇相对。它比kNN计算成本更高，但在高维中可以高效实现。KDE加权将采样集中在代表性低密度点上：'
- en: '[PRE16]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**K-means density** clusters the unlabeled data points using k-means into k
    clusters. The size of each cluster indicates its density – smaller clusters correspond
    to sparser regions. This cluster density can be used as a weight when combined
    with informativeness criteria. Points in smaller, tighter clusters get increased
    weight, making them more likely to be queried. This balances sampling across varying
    densities. K-means provides a simple way to estimate density and identify representative
    points from all densities. It is fast and scales well to large datasets. One limitation
    is determining the number of clusters, k, upfront. K-means density weighting focuses
    active ML on diverse cases from all densities equally:'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**K-means密度**使用k-means将未标记的数据点聚类成k个簇。每个簇的大小表示其密度——较小的簇对应于稀疏区域。这种簇密度可以与信息性标准结合使用作为权重。较小、更紧密的簇中的点会获得更高的权重，从而使其更有可能被查询。这平衡了不同密度下的采样。K-means提供了一种简单的方法来估计密度并从所有密度中识别代表性点。它速度快，并且在大数据集上具有良好的扩展性。一个限制是预先确定簇的数量，k。K-means密度加权将主动机器学习集中在所有密度的多样化案例上：'
- en: '[PRE17]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**MMD** measures the *distance* between distributions to identify points in
    low-density regions. The MMD between a point’s neighborhood distribution and the
    overall data distribution is calculated. A higher MMD indicates that the local
    region is very different from the overall distribution, so it is likely a low-density
    area. These MMD density scores are then used as weights when combined with informativeness
    measures. Points in sparse, isolated regions with high MMD get increased priority
    for querying. This results in balanced sampling across varying densities, thereby
    avoiding cluster oversampling. MMD provides a principled way to estimate density
    that captures useful nonlinear variations. MMD density weighting focuses active
    ML on representative low-density areas:'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最大均值差异（MMD**）测量分布之间的**距离**以识别低密度区域的点。计算一个点的邻域分布与整体数据分布之间的MMD。较高的MMD表明局部区域与整体分布非常不同，因此它很可能是一个低密度区域。然后，将这些MMD密度分数与信息性度量结合使用作为权重。具有高MMD的稀疏、孤立区域中的点在查询时将获得更高的优先级。这导致在变化密度上实现平衡采样，从而避免簇过度采样。MMD提供了一种原则性的方法来估计密度，它捕捉了有用的非线性变化。MMD密度加权将主动机器学习集中在代表性低密度区域上：'
- en: '[PRE18]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, let’s visualize these density-weight sampling methods:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们可视化这些密度加权采样方法：
- en: '[PRE19]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The resulting graph is presented in *Figure 2**.11*:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图展示在 *图 2**.11* 中：
- en: '![Figure 2.11 – A comparison of different density-weighted sampling methods](img/B21789_02_11.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.11 – 不同密度加权采样方法的比较](img/B21789_02_11.jpg)'
- en: Figure 2.11 – A comparison of different density-weighted sampling methods
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.11 – 不同密度加权采样方法的比较
- en: 'Density-weighted sampling methods, including the ones mentioned previously,
    offer diverse advantages:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 包括之前提到的在内的密度加权采样方法提供了多种优势：
- en: They can help improve the performance of the ML model by selecting samples that
    are more likely to be informative. This is because samples that are in high-density
    regions of the data are more likely to be representative of the underlying distribution
    and therefore more likely to be informative for the model.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们可以帮助通过选择更有可能提供信息的样本来提高机器学习模型的表现。这是因为位于数据高密度区域的样本更有可能代表基础分布，因此更有可能对模型提供信息。
- en: They help reduce the number of labeled samples needed to train the model. This
    is because density-weighted sampling can help focus the labeling effort on the
    most informative samples, which can lead to faster convergence of the model.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们有助于减少训练模型所需的标注样本数量。这是因为密度加权采样可以帮助将标注工作集中在最有信息的样本上，这可能导致模型更快收敛。
- en: They can be used with any type of data. Density-weighted sampling does not make
    any assumptions about the data distribution, so it can be used with any type of
    data, including structured and unstructured data.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们可以与任何类型的数据一起使用。密度加权采样不对数据分布做出任何假设，因此可以与任何类型的数据一起使用，包括结构化和非结构化数据。
- en: To conclude, density weighting, which is a technique that’s used to increase
    the diversity and coverage of samples, offers an effective and efficient approach.
    By assigning weights to each sample based on their density, this method ensures
    that the resulting sample set represents the underlying population more accurately.
    With this approach, you can obtain a more comprehensive understanding of the data,
    allowing for better decision-making and analysis. Overall, density weighting is
    a valuable tool in research and statistical analysis, allowing you to highlight
    hidden patterns and trends that might otherwise be overlooked.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，密度加权，这是一种用于增加样本多样性和覆盖率的技巧，提供了一种有效且高效的方法。通过根据每个样本的密度为其分配权重，这种方法确保了结果样本集更准确地代表基础人群。采用这种方法，您可以获得对数据的更全面理解，从而进行更好的决策和分析。总的来说，密度加权是研究和统计分析中的一个宝贵工具，可以帮助您突出那些可能被忽视的隐藏模式和趋势。
- en: 'Now that we have discussed several query strategies, let’s compare them to
    understand how they fare against each other:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了几种查询策略，让我们比较它们以了解它们之间的表现：
- en: '![Figure 2.12 – Comparison chart for query strategies](img/B21789_02_12.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.12 – 查询策略比较图](img/B21789_02_12.jpg)'
- en: Figure 2.12 – Comparison chart for query strategies
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.12 – 查询策略比较图
- en: '*Figure 2**.12* summarizes and compares various query strategies that have
    been discussed in this chapter.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2**.12* 总结并比较了本章中讨论的各种查询策略。'
- en: Summary
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered key techniques such as uncertainty sampling, query-by-committee,
    EMC, EER, and density weighting for designing effective active ML query strategies.
    Moving forward, in the next chapter, our focus will shift toward exploring strategies
    for managing the human in the loop. It is essential to optimize the interactions
    with the oracle labeler to ensure maximum efficiency in the active ML process.
    By understanding the intricacies of human interaction and leveraging this knowledge
    to streamline the labeling process, we can significantly enhance the efficiency
    and effectiveness of active ML algorithms.In the next chapter we will discuss
    how to manage the role of human labelers in active ML.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了设计有效的主动机器学习查询策略的关键技术，如不确定性采样、委员会查询、EMC、EER 和密度加权。展望未来，在下一章中，我们的重点将转向探索管理闭环中人类角色的策略。优化与专家标签器的交互对于确保主动机器学习过程的最大效率至关重要。通过理解人类交互的复杂性并利用这些知识来简化标注过程，我们可以显著提高主动机器学习算法的效率和效果。在下一章中，我们将讨论如何管理主动机器学习中人类标注员的角色。
