- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Designing Query Strategy Frameworks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计查询策略框架
- en: '**Query strategies** act as the engine that drives active ML and determines
    which data points get selected for labeling. In this chapter, we aim to provide
    a comprehensive and detailed explanation of the most widely used and highly effective
    query strategy frameworks that are employed in active ML. These frameworks play
    a crucial role in the field of active ML, aiding in selecting informative and
    representative data points for labeling. The strategies that we will delve into
    include uncertainty sampling, query-by-committee, **expected model change** (**EMC**),
    **expected error reduction** (**EER**), and density-weighted methods. By thoroughly
    understanding these frameworks and the underlying principles, you can make informed
    decisions when designing and implementing active ML algorithms.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**查询策略**作为驱动主动机器学习的引擎，并决定哪些数据点被选中进行标记。在本章中，我们旨在提供对主动机器学习中应用最广泛、最有效的查询策略框架的全面和详细解释。这些框架在主动机器学习领域发挥着至关重要的作用，有助于选择具有信息性和代表性的数据点进行标记。我们将深入探讨的策略包括不确定性采样、委员会查询、**预期模型变化**（**EMC**）、**预期误差减少**（**EER**）和密度加权方法。通过彻底理解这些框架及其背后的原理，你可以在设计和实现主动机器学习算法时做出明智的决策。'
- en: In this chapter, you will gain skills that will equip you to design and deploy
    query strategies that extract maximum value from labeling efforts. You will gain
    intuition for matching strategies to datasets and use cases when building active
    ML systems.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将获得设计并部署查询策略的技能，这些策略可以从标记工作中提取最大价值。你将获得在构建主动机器学习系统时将匹配策略与数据集和用例相匹配的直觉。
- en: 'We will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主题：
- en: Exploring uncertainty sampling methods
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索不确定性采样方法
- en: Understanding query-by-committee approaches
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解委员会查询方法
- en: Labeling with EMC sampling
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用EMC采样进行标记
- en: Sampling with EER
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用EER进行采样
- en: Understanding density-weighted sampling methods
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解密度加权采样方法
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'For the code examples demonstrated in this chapter, we have used Python 3.9.6
    with the following packages:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章中演示的代码示例，我们使用了Python 3.9.6以及以下包：
- en: '`numpy` (version 1.23.5)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy`（版本1.23.5）'
- en: '`scikit-learn` (version 1.2.2)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scikit-learn`（版本1.2.2）'
- en: '`matplotlib` (version 3.7.1)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matplotlib`（版本3.7.1）'
- en: Exploring uncertainty sampling methods
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索不确定性采样方法
- en: '**Uncertainty sampling** refers to querying data points for which the model
    is least certain about their prediction. These are samples the model finds most
    ambiguous and cannot confidently label on its own. Getting these high-uncertainty
    points labeled allows the model to clarify where its knowledge is lacking.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**不确定性采样**指的是查询模型对其预测最不确定的数据点。这些是模型认为最模糊且无法自信地自行标记的样本。将这些高不确定性点标记出来，允许模型阐明其知识不足之处。'
- en: In uncertainty sampling, the active ML system queries instances for which the
    current model’s predictions exhibit *high uncertainty*. The goal is to select
    data points that are *near the decision boundary* between classes. Labeling these
    ambiguous examples helps the model gain confidence in areas where its knowledge
    is weakest.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在不确定性采样中，主动机器学习系统查询当前模型预测表现出**高不确定性**的实例。目标是选择位于类别之间的**决策边界**附近的数据点。标记这些模糊示例有助于模型在知识最薄弱的领域建立信心。
- en: Uncertainty sampling methods select data points close to the **decision boundary**
    because points near this boundary exhibit the highest prediction ambiguity. The
    decision boundary is defined as the point where the model shows the most uncertainty
    in distinguishing between different classes for a given input. Points on the boundary
    represent the most ambiguous, uncertain cases for a model.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性采样方法选择靠近**决策边界**的数据点，因为这些点表现出最高的预测不确定性。决策边界定义为模型在区分给定输入的不同类别时表现出最大不确定性的点。边界上的点代表模型最模糊、最不确定的情况。
- en: '*Figure 2**.1* illustrates the difference between uncertainty sampling and
    random sampling:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2**.1*展示了不确定性采样与随机采样的区别：'
- en: '![](img/B21789_02_01.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21789_02_01.jpg)'
- en: Figure 2.1 – Uncertainty sampling versus random sampling
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 – 不确定性采样与随机采样的比较
- en: For data points that are located far away from the decision boundary (*Figure
    2**.1*) within a class region (labeled as A or B), the model will exhibit a high
    level of confidence in assigning them to that class (for example, >95%). These
    points are considered certain and will not be selected when employing uncertainty
    sampling. However, there is a possibility that some of these points may be chosen
    when using random sampling. For data points that are extremely close to or directly
    on the decision boundary, the model will struggle to distinguish between the classes.
    The predicted class probabilities will be more evenly distributed, with the top
    predictions being very close to each other. Therefore, these points are considered
    uncertain and will be selected when using uncertainty sampling. These important
    data points might have been overlooked when using random sampling. As a result,
    the distance to the boundary correlates to uncertainty – the closest points will
    have the lowest max confidence, the smallest margin between top classes, and the
    highest entropy over the class probabilities.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于位于类区域（标记为A或B）内远离决策边界（*图2**.1*）的数据点，模型将非常自信地将它们分配到该类（例如，>95%）。这些点被认为是确定的，在采用不确定性采样时不会被选中。然而，当使用随机采样时，可能选择其中的一些点。对于非常接近或直接位于决策边界上的数据点，模型将难以区分这些类别。预测的类概率将更加均匀分布，顶级预测非常接近。因此，这些点被认为是不确定的，在采用不确定性采样时将被选中。这些重要的数据点在使用随机采样时可能会被忽略。因此，到边界的距离与不确定性相关联——最接近边界的点将具有最低的最大置信度，顶级类别之间的最小边缘，以及类概率的最高熵。
- en: Therefore, by selecting points based on metrics such as low confidence, low
    margin, and high entropy, uncertainty sampling queries the instances nearest to
    the decision boundary. We will discuss these metrics in detail in the upcoming
    sections of this chapter. Labeling these provides information to help clarify
    class regions and refine the boundary. The model is unlikely to gain much information
    from examples it can already predict correctly with high confidence. However,
    querying data points that the model is very uncertain about directly provides
    useful information about its gaps. Uncertainty sampling takes advantage of this
    by targeting points with high prediction ambiguity.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过根据低置信度、低边缘和高熵度等指标选择点，不确定性采样查询了最接近决策边界的实例。我们将在本章接下来的部分详细讨论这些指标。对这些实例进行标注提供了帮助，有助于明确类区域并细化边界。模型不太可能从它已经可以高置信度正确预测的例子中获得很多信息。然而，直接查询模型非常不确定的数据点直接提供了关于其差距的有用信息。不确定性采样通过针对具有高预测模糊性的点来利用这一点。
- en: For example, an image classifier’s least confident predictions likely correspond
    to challenging out-of-distribution examples that traditional sampling would miss.
    By querying these unusual cases for labels, the model rapidly improves at classifying
    edge cases. Now, let’s discuss some of the common methods that are used for uncertainty
    sampling.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，图像分类器的最小置信度预测很可能对应于具有挑战性的分布外示例，传统采样会错过这些示例。通过查询这些异常情况以获取标签，模型可以快速提高对边缘案例的分类能力。现在，让我们讨论一些用于不确定性采样的常用方法。
- en: First, we will talk about **least-confidence sampling**, where the data points
    are ranked according to their least-confidence score. This score is obtained by
    subtracting the most confident prediction label for each item from 1, which represents
    100% confidence. To facilitate understanding, it is beneficial to convert the
    uncertainty scores into a range of 0-1, where 1 signifies the highest level of
    uncertainty. The magnitude of the score that’s assigned to each data point lies
    in its association with the uncertainty of the model’s prediction. Consequently,
    data samples with the highest least-confident scores should be given priority
    for annotation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将讨论**最小置信度采样**，其中数据点根据它们的最小置信度得分进行排序。这个得分是通过从每个项目的最自信预测标签中减去1来获得的，这代表100%的置信度。为了便于理解，将不确定性得分转换为0-1的范围是有益的，其中1表示最高水平的不确定性。分配给每个数据点的分数的大小在于它与模型预测的不确定性之间的关联。因此，具有最高最小置信度得分的数据样本应该优先进行标注。
- en: 'The most informative sample, x, can be computed as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最具信息量的样本x可以按以下方式计算：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi
    mathvariant="normal">argmax</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>](img/1.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
- en: 'Here, we have the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi
    mathvariant="normal">argmax</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>](img/2.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
- en: 'For instance, let’s say we have a model that classifies samples into three
    different classes. Now, we are trying to rank two samples using least-confidence
    sampling. The predicted probabilities of the two samples are `[0.05, 0.85, 0.10]`
    for sample 1 and `[0.35, 0.15, 0.50]` for sample 2\. Let’s find out which sample
    is the most informative when using the least-confidence sampling method by using
    the following Python code:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output of the preceding code snippet is as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Thus, the most informative sample is sample 2.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we can see that sample 2 is chosen when using the least-confidence
    sampling approach because the model’s predictions were the least confident for
    that sample.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will discuss **margin sampling**. This method is designed to identify
    and select data points that have the smallest disparity in probability between
    the top two predicted classes. By focusing on data points with minimal margin
    between classes, we can effectively prioritize the annotation of data samples
    that result in a higher level of confusion for the model. Therefore, the model’s
    level of uncertainty is higher when it encounters data points with a lower margin
    score, making them ideal candidates for annotation. The formula to calculate the
    score of the most informative data point with the margin sampling method is as
    follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi
    mathvariant="normal">argmin</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>](img/3.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: 'Let’s use the samples from our previous example again:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output of the preceding script is as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: With the margin sampling method, sample 2 is selected as well because it has
    the smallest disparity in probability between the top two predicted classes.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: In the **ratio of confidence** method, the data points that have the smallest
    ratio between the probability of the top predicted class and the probability of
    the second most likely class are selected. This targets examples where the model’s
    top two predictions are closest in likelihood. A lower ratio indicates that the
    model is less confident in the top class relative to the second class. By querying
    points with the minimum ratio between the top two class probabilities, this technique
    focuses on cases where the model is nearly equivocal between two classes. Getting
    these boundary points labeled will push the model to gain greater confidence in
    the true class. A lower ratio means higher ambiguity, so the ratio of confidence
    sampling finds points where the model is most unsure of which class is correct.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'We can calculate the score of the most informative data point using the ratio
    of confidence sampling method via the following equation:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi
    mathvariant="normal">argmin</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>](img/4.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: 'Once again, we’ll utilize the samples that we used previously for this method:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output of this script is as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: So, sample 2 is selected when using the ratio of confidence sampling method
    as it has the smallest ratio between the probability of the top predicted class
    and the probability of the second most likely class.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Another method is **entropy sampling**. This method selects data points that
    have the highest entropy across the probability distribution over classes. Entropy
    represents the overall uncertainty in the predicted class probabilities. Higher
    entropy means the model is more uncertain, with a more uniform probability spread
    over classes. Lower entropy indicates confidence, with probability concentrated
    on one class.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'By querying points with maximum entropy, this technique targets instances where
    the model’s predicted class probabilities are most evenly distributed. These highly
    uncertain points provide the most information gain since the model cannot strongly
    favor one class – its predictions are maximally unsure. Getting these high entropy
    points labeled enables the model to gain more confidence in areas in which it
    is the most uncertain. Overall, entropy sampling finds points with the highest
    total ambiguity. The formula to calculate the score of the most informative data
    point with the entropy sampling method is as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi
    mathvariant="normal">argmax</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo>-</mml:mo><mml:mrow><mml:munder><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mi
    mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">g</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>](img/5.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: 'Let’s use our sample examples again with this method:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This script outputs the following results:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Using entropy sampling, sample 2 was chosen as it has the highest entropy across
    the probability distribution over classes.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: These common uncertainty sampling techniques provide simple but effective strategies
    to identify highly ambiguous points to query.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s explore the key benefits that uncertainty sampling provides for
    active ML:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Uncertainty sampling is a conceptually intuitive query strategy that is efficient
    to compute. Metrics such as confidence, margin, ratio, and entropy have clear
    uncertainty interpretations and can be calculated quickly.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不确定性采样是一种概念上直观且计算效率高的查询策略。如置信度、间隔、比率和熵等指标具有明确的不确定性解释，并且可以快速计算。
- en: It actively enhances model confidence in areas where it is uncertain, expanding
    knowledge boundaries. For example, a sentiment classifier can gain more certainty
    on ambiguous reviews containing rare phrases by querying the most uncertain cases
    for their true sentiment.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过查询最不确定的案例以获取其真实情感，积极增强模型在不确定区域中的信心，扩大知识边界。例如，情感分类器可以通过查询最不确定的案例来获得包含罕见短语的模糊评论的更多确定性。
- en: Uncertainty sampling is widely applicable across classification tasks and model
    types such as **support vector machines** (**SVMs**), logistic regression, random
    forests, and **neural networks** (**NNs**). Uncertainty applies broadly to classification
    tasks.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不确定性采样在分类任务和各种模型类型（如**支持向量机**（**SVMs**）、逻辑回归、随机森林和**神经网络**（**NNs**））中具有广泛的应用。不确定性在分类任务中具有广泛的应用。
- en: Uncertainty sampling is useful for anomaly detection by finding ambiguous outliers
    the model cannot explain. Uncertainty highlights unusual cases.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不确定性采样通过寻找模型无法解释的模糊异常值来用于异常检测。不确定性突出了不寻常的案例。
- en: It can identify labeling errors by seeking points with inconsistent predictions
    between models. High uncertainty may indicate noisy data.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以通过寻找模型之间预测不一致的点来识别标签错误。高不确定性可能表明数据有噪声。
- en: Overall, uncertainty sampling is a highly effective and versatile active ML
    method. It can be used in various domains and is intuitive and efficient. It helps
    expand a model’s capabilities and discover unknown points. Whether it’s used for
    classification, regression, or other ML tasks, uncertainty sampling consistently
    improves model performance. By selecting uncertain data points for annotation,
    the model learns from informative examples and improves predictions. It has proven
    useful in natural language processing, computer vision, and data mining. Uncertainty
    sampling actively acquires new knowledge and enhances ML models. While uncertainty
    sampling focuses on points the model is individually unsure of, query-by-committee
    approaches aim to add diversity by identifying points where an ensemble of models
    disagrees. We will discuss query-by-committee approaches in the next section.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，不确定性采样是一种高度有效和通用的主动机器学习方法。它可用于各种领域，直观且高效。它有助于扩展模型的能力并发现未知点。无论是用于分类、回归还是其他机器学习任务，不确定性采样都能持续提高模型性能。通过选择不确定数据点进行标注，模型从信息丰富的例子中学习并改进预测。它在自然语言处理、计算机视觉和数据挖掘中已被证明是有用的。不确定性采样积极获取新知识并增强机器学习模型。虽然不确定性采样关注模型个体不确定的点，但查询委员会方法旨在通过识别模型集成分歧的点来增加多样性。我们将在下一节中讨论查询委员会方法。
- en: Understanding query-by-committee approaches
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解查询委员会方法
- en: '**Query-by-committee** aims to add diversity by querying points where an ensemble
    of models disagrees the most. Different models will disagree where the data is
    most uncertain or ambiguous.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**查询委员会**旨在通过查询模型集成分歧最大的点来增加多样性。不同的模型会在数据最不确定或模糊的地方产生分歧。'
- en: In the query-by-committee approach, a group of models is trained using a labeled
    set of data. By doing so, the ensemble can work together and provide a more robust
    and accurate prediction.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在查询委员会方法中，一组模型使用标记数据集进行训练。通过这样做，集成模型可以协同工作并提供更稳健和准确的预测。
- en: One interesting aspect of this approach is that it identifies the data point
    that causes the most disagreement among the ensemble members. This data point
    is then chosen to be queried to obtain a label.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个有趣方面是，它确定了导致集成成员之间最大分歧的数据点。然后，这个数据点被选中进行查询以获取标签。
- en: 'The reason why this method works well is because different models tend to have
    the most disagreement on difficult and boundary examples, as depicted in *Figure
    2**.2*. These are the instances where there is ambiguity or uncertainty, and by
    focusing on these points of maximal disagreement, the ensemble can gain consensus
    and make more confident predictions:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法之所以有效，是因为不同的模型往往在难以处理和边界示例上存在最大分歧，如图*2**.2所示。这些是存在歧义或不确定性的实例，通过关注这些最大分歧的点，集成模型可以达成共识并做出更自信的预测：
- en: '![Figure 2.2 – Query-by-committee sampling with five unlabeled data points](img/B21789_02_02.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图2.2 – 使用五个未标记数据点的查询委员会采样](img/B21789_02_02.jpg)'
- en: Figure 2.2 – Query-by-committee sampling with five unlabeled data points
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 – 使用五个未标记数据点的查询委员会采样
- en: '*Figure 2**.2* reveals a disagreement between models 1 and 4, as opposed to
    models 2 and 3, regarding data point 2\. A similar pattern can be observed with
    data point 4\. Therefore, data points 2 and 4 have been chosen to be sent to the
    oracle for labeling.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2.2*揭示了模型1和4之间的分歧，而不是模型2和3，关于数据点2。类似的模式也可以在数据点4上观察到。因此，数据点2和4已被选中发送到预言机进行标记。'
- en: Query-by-committee is a widely used and effective active ML strategy that addresses
    the limitations of uncertainty sampling. While uncertainty sampling can be biased
    toward the current learner and may overlook crucial examples that are not within
    its estimator’s focus, query-by-committee overcomes these challenges. This approach
    involves maintaining multiple hypotheses simultaneously and selecting queries
    that lead to disagreements among these hypotheses. By doing so, it ensures a more
    comprehensive and diverse exploration of the data, ultimately enhancing the learning
    process. For instance, a committee of image classifiers may heavily disagree on
    ambiguous images that traditional sampling fails to capture. By querying labels
    for images with maximal disagreement, such as varied predictions for an unusual
    object, the committee collectively improves.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 查询委员会是一种广泛使用且有效的活跃机器学习策略，它解决了不确定性采样的局限性。虽然不确定性采样可能会偏向当前的学习者，并可能忽略其估计者关注范围之外的至关重要示例，但查询委员会克服了这些挑战。这种方法涉及同时维护多个假设，并选择导致这些假设之间产生不一致的查询。通过这样做，它确保了对数据的更全面和多样化的探索，从而增强了学习过程。例如，一个图像分类器的委员会可能会在传统采样无法捕捉到的模糊图像上产生很大的分歧。通过查询具有最大分歧的图像标签，例如对不寻常物体的不同预测，委员会可以集体改进。
- en: Some of the common techniques for query-by-committee sampling include **maximum
    disagreement**, **vote entropy**, and **average KL divergence**, all of which
    we will discuss now.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 查询委员会采样的常见技术包括**最大不一致**、**投票熵**和**平均KL散度**，我们将在下面讨论这些内容。
- en: Maximum disagreement
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最大不一致
- en: This simple approach looks at direct disagreement in the predicted labels between
    committee members. The data points where most ensemble members disagree on the
    label are queried. For example, if a three-model committee’s label votes for a
    point are (1, 2, 3), this exemplifies maximum disagreement as each model predicts
    a different class. Querying the points with the most label conflicts helps us
    focus only on cases that divide the committee. Maximum disagreement target instances
    create the largest rifts within the ensemble. Getting these high disagreement
    points labeled will aim to resolve the core differences between models.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这种简单的方法关注委员会成员之间预测标签的直接不一致。在标签上大多数集成成员存在分歧的数据点被查询。例如，如果一个三模型委员会的标签投票为（1，2，3），这表明最大不一致，因为每个模型预测了不同的类别。查询具有最多标签冲突的点有助于我们只关注分裂委员会的案例。最大不一致的目标实例在集成中造成了最大的分歧。对这些高分歧点进行标记的目的是解决模型之间的核心差异。
- en: 'Let’s explore a numerical example of the query-by-committee maximum disagreement
    method in active ML. For this example, we will consider a pool of 10 unlabeled
    data points, as shown in *Figure 2**.3*, that we want to label to train a classifier.
    We create two committee members (models) called M1 and M2\. We evaluate each unlabeled
    data point using M1 and M2 to get the following predicted labels:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个活跃机器学习中的查询委员会最大不一致方法的数值示例来探讨。在这个例子中，我们将考虑一个包含10个未标记数据点的集合，如图*图2.3*所示，我们希望对这些数据进行标记以训练一个分类器。我们创建了两个委员会成员（模型），分别称为M1和M2。我们使用M1和M2评估每个未标记数据点，以获得以下预测标签：
- en: '![Figure 2.3 – A numerical example to illustrate the query-by-committee maximum
    disagreement method](img/B21789_02_03.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图2.3 – 用数值示例说明查询委员会最大不一致方法](img/B21789_02_03.jpg)'
- en: Figure 2.3 – A numerical example to illustrate the query-by-committee maximum
    disagreement method
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 – 用数值示例说明查询委员会最大不一致方法
- en: Then, we select the data point with the maximum disagreement between the two
    committee members. Here, data points 1, 4, 5, 8, and 9 have different predicted
    labels by M1 and M2\. We select one of these points, say point 4, to query the
    true label from an oracle. We then add the newly labeled point to retrain the
    committee members.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'We can do this with a simple Python script:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This returns the following output:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This process is then repeated, querying points with maximum disagreement in
    labels predicted by the committee, until we reach sufficient performance. The
    most informative points surface through the maximum disagreement of the committee
    members.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Vote entropy
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This technique calculates the entropy over the label votes from each model
    in the ensemble committee. **Entropy** represents the overall uncertainty, where
    higher entropy means the models have a wider spread of predictions. Lower entropy
    indicates the models largely agree on the label. Querying the data points with
    maximum entropy in the vote distribution helps target the instances where the
    committee displays the highest collective uncertainty and disagreement. Getting
    these maximally entropic points labeled will push the ensemble toward greater
    consensus. Overall, vote entropy identifies cases that divide the committee the
    most, focusing labeling on their disagreements. If we go back to using a numerical
    example to better understand how the query-by-committee vote entropy method works,
    we can once again use a pool of 10 unlabeled data points, as shown in *Figure
    2**.4*, and a committee of two models, M1 and M2\. We get the following predicted
    probabilities for each class on the data points:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – A numerical example to illustrate the query-by-committee vote
    entropy method](img/B21789_02_04.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 – A numerical example to illustrate the query-by-committee vote entropy
    method
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'We calculate the vote entropy for each point as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>H</mi><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mo>=</mo><mo>−</mo><mrow><munder><mo>∑</mo><mi>i</mi></munder><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>y</mi><mo>|</mo><mi>x</mi></mrow></mfenced><mi mathvariant="normal">l</mi><mi
    mathvariant="normal">o</mi><mi mathvariant="normal">g</mi><mo>(</mo><mi>y</mi><mo>|</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></mrow></math>](img/6.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
- en: Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/7.png)
    is averaged over the committee members.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'The probabilities from the committee members are averaged when calculating
    the vote entropy because we want to measure the total uncertainty or disagreement
    of the entire committee on a data point. By averaging, we essentially get the
    *vote* of the full committee on the probabilities of each class, rather than just
    considering individual members’ predictions. This allows us to select the data
    points where the committee has the most uncertainty or disagrees the most with
    its predictions:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5 – A numerical example to illustrate the query-by-committee vote
    entropy method with averages per class and entropy calculated](img/B21789_02_05.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 – A numerical example to illustrate the query-by-committee vote entropy
    method with averages per class and entropy calculated
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: The points with maximum entropy will have the most disagreement among models.
    In *Figure 2**.5*, points 1, 4, 5, 8, and 9 have the highest entropy, so we query
    their labels. The next step would be to retrain the models and repeat the process.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 'We can write this with some Python code as well:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output of this script is as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Next, we will move our focus to computing predictions using the KL divergence
    method.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Average KL divergence
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This method measures the **Kullback-Leibler divergence** (**KL divergence**)
    between each committee member’s predicted label distribution and the average predicted
    distribution across all members.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'KL divergence is defined as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo>(</mo><mi>P</mi><mo>|</mo><mfenced
    open="|" close=")"><mi>Q</mi></mfenced><mo>=</mo><mo>−</mo><mrow><munder><mo>∑</mo><mrow><mi>x</mi><mo>∈</mo><mi>X</mi></mrow></munder><mrow><mi>P</mi><mo>(</mo><mi>x</mi><mo>)</mo><mi
    mathvariant="normal">l</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">g</mi><mo>(</mo><mfrac><mrow><mi>Q</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mrow><mi>P</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac><mo>)</mo></mrow></mrow></mrow></mrow></mrow></math>](img/8.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
- en: Here, P and Q are two probability distributions.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'The data points with the highest average KL divergence are then queried. A
    higher KL divergence indicates a larger difference between a model’s predictions
    and the committee consensus. Querying points with maximum divergence targets instances
    where individual models strongly disagree with the overall ensemble. Labeling
    these high-divergence points will bring the individual models closer to the committee
    average. Average KL divergence identifies cases with outlying model predictions
    to focus labeling on reconciliation. Let’s take a look at our numerical example
    for the query-by-committee average KL divergence method. Again, we’re using the
    pool of 10 unlabeled data points, as shown in *Figure 2**.6*, and a committee
    of two models, M1 and M2\. We get the predicted class probabilities on each data
    point from M1 and M2 and calculate the KL divergence between M1’s and M2’s predictions
    for each point:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6 – A numerical example to illustrate the query-by-committee average
    KL divergence method](img/B21789_02_06.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 – A numerical example to illustrate the query-by-committee average
    KL divergence method
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: We average the KL divergence between M1 and M2, and M2 and M1\. We calculate
    the KL divergence in both directions (KL(M1||M2) and KL(M2||M1)) – because KL
    divergence is asymmetric, it will give different values depending on the direction.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: The KL divergence from M1 to M2, KL(M1||M2), measures how well M2’s distribution
    approximates M1’s. On the other hand, KL(M2||M1) measures how well M1’s distribution
    approximates M2’s.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: In query-by-committee, we want to measure the total disagreement between the
    two committee members’ distributions. Just using KL(M1||M2) or just using KL(M2||M1)
    will not capture the full divergence. By taking the average of KL(M1||M2) and
    KL(M2||M1), we get a symmetric measure of the total divergence between the two
    distributions. This gives a better indication of the overall disagreement between
    the two committee members on a data point.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Taking the average KL in both directions ensures we select the points with maximum
    mutual divergence between the two models’ predictions. This surfaces the most
    informative points for labeling to resolve the committee’s uncertainty. The point
    with maximum average KL divergence has the most disagreement. So, here, points
    1, 4, 5, and 9 have the highest average KL divergence. We would then query these
    labels, retrain the models, and repeat the process.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'We can write this with some Python code as well:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Here’s the output we get:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Upon exploring different methods that are used to perform query-by-committee
    sampling, we’ve noticed that it can be quite computationally intensive. Indeed,
    as shown in *Figure 2**.7*, the query-by-committee technique is an iterative approach
    that requires retraining the models from the committee every time new labeled
    data is added to the training set:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7 – The iteration process in the query-by-committee sampling technique](img/B21789_02_07.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 – The iteration process in the query-by-committee sampling technique
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'In conclusion, this technique is designed to identify informative query points
    by quantifying the level of disagreement among the ensemble models and offers
    several key advantages:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: It promotes diversity by finding points that various models interpret differently.
    The techniques can effectively prioritize query points that are likely to provide
    valuable information, thus improving the overall quality of the query-based learning
    process.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It encourages exploration by actively seeking out query points that are less
    predictable or commonly known, allowing for a more comprehensive understanding
    of the dataset.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides the ability to construct committees with an array of distinct models
    such as SVMs, NNs, and many others. This versatility allows for a diverse range
    of strategies and approaches to be employed when making important decisions. By
    leveraging these various models, you can gain deeper insights and improve the
    overall performance of your committee. The query-by-committee approach differs
    from traditional techniques such as bagging and boosting. Its main objective is
    to choose the most informative unlabeled data points for labeling and inclusion
    in the training set. On the other hand, traditional ensemble methods such as bagging
    and boosting focus on combining multiple models to enhance overall predictive
    performance. Indeed, query-by-committee methods calculate disagreement between
    committee members to find optimal queries for labeling, as we have seen previously.
    Traditional ensembles combine predictions through voting or averaging to produce
    a unified prediction.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is very useful in situations where the unlabeled data pool contains a limited
    representation of the underlying distribution. By combining the opinions and predictions
    of different committee members, query-by-committee methods can effectively address
    the challenges posed by poorly covered distributions in unlabeled data pools.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By leveraging the varying opinions of the committee, query-by-committee enhances
    the overall performance of the learning system.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: We will now delve deeper into the implementation of EMC strategies and explore
    their potential benefits.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Labeling with EMC sampling
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: EMC aims to query points that will induce the greatest change in the current
    model when labeled and trained on. This focuses labeling on points with the highest
    expected impact.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: EMC techniques involve selecting a specific data point to label and learn from
    to cause the most significant alteration to the current model’s parameters and
    predictions. The core idea is to query the point that would impact the maximum
    change to the model’s parameters if we knew its label. By carefully identifying
    this particular data point, the EMC method aims to maximize the impact on the
    model and improve its overall performance. The process involves assessing various
    factors and analyzing the potential effects of each data point, ultimately choosing
    the one that is expected to yield the most substantial changes to the model, as
    depicted in *Figure 2**.8*. The goal is to enhance the model’s accuracy and make
    it more effective in making predictions.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'When we refer to querying points that lead to larger updates in the model targets,
    what we are discussing is identifying highly informative examples located in uncertain
    areas of the input space. These examples play a crucial role in influencing and
    enhancing the model’s performance. By paying attention to these specific instances,
    we can gain deeper insights into the complexities and subtleties of the input
    space, resulting in a more thorough understanding and improved overall outcomes:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8 – EMC sampling](img/B21789_02_08.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 – EMC sampling
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: The initial model is trained using the training dataset. Then, the unlabeled
    samples are evaluated based on the changes in model outputs after including them
    in the training set. In *Figure 2**.8*, the graphs show the resulting **model
    output change** (**MOC**) for three example samples. The sample that leads to
    the largest output change, when considering all data, is chosen to be labeled
    next.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: In other words, in EMC sampling, the process begins by ranking the unlabeled
    examples. This ranking is determined by estimating the expected change that would
    occur in the model’s predictions if each example were to be labeled. This estimation
    takes into account various factors and considerations, ultimately providing a
    basis for the prioritization of labeling.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'This estimation is typically based on calculating the **expected gradient length**
    (**EGL**). The EGL method estimates the expected length of the gradient of the
    loss function if the model was trained on the newly labeled point. When training
    discriminative probabilistic models, gradient-based optimization is commonly used.
    To assess the *change* in the model, we can examine the size of the training gradient.
    This gradient refers to the vector that is employed to update the parameter values
    during the training process. In simpler terms, the learner should choose the instance,
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>G</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">*</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/9.png),
    that, when labeled and included in the labeled dataset, (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="script">L</mml:mi></mml:math>](img/10.png)),
    would result in the greatest magnitude for the new training gradient:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msubsup><mi>x</mi><mrow><mi>E</mi><mi>G</mi><mi>L</mi></mrow><mi
    mathvariant="normal">*</mi></msubsup><mo>=</mo><munder><mi>argmax</mi><mi>x</mi></munder><mrow><munder><mo>∑</mo><mi>i</mi></munder><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>|</mo><mi>x</mi><mo>;</mo><mi>θ</mi></mrow></mfenced><mfenced
    open="‖" close="‖"><mrow><mo>∇</mo><mi mathvariant="script">l</mi><mo>(</mo><mi
    mathvariant="script">L</mi><mo>∪</mo><mfenced open="〈" close="〉"><mrow><mi>x</mi><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></mfenced><mo>;</mo><mi>θ</mi><mo>)</mo></mrow></mfenced></mrow></mrow></mrow></mrow></math>](img/11.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
- en: Here, ||.|| is the Euclidean norm of each resulting gradient vector, θ is the
    model parameters, x is an unlabeled point, y is the predicted label for x, and
    ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mo>∇</mo><mi
    mathvariant="script">l</mi><mo>(</mo><mi mathvariant="script">L</mi><mo>∪</mo><mfenced
    open="〈" close="〉"><mrow><mi>x</mi><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></mfenced><mo>;</mo><mi>θ</mi><mo>)</mo></mrow></mrow></mrow></math>](img/12.png)
    is the new gradient that would be obtained by adding the training tuple, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    open="⟨" close="⟩" separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/13.png),
    to the labeled dataset, (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="script">L</mml:mi></mml:math>](img/14.png)).
    Data points that result in a longer expected gradient are prioritized for querying.
    A longer gradient indicates a greater expected change in the model parameters
    during training. By selecting points with high expected gradient length, the model
    focuses on samples that will highly influence the model once labeled. This targets
    points in uncertain regions that will have an outsized impact on updating model
    predictions. In short, EGL identifies data points that are likely to substantially
    reshape the decision boundary.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: By employing this technique, the algorithm can pinpoint and identify the specific
    data points that are anticipated to yield substantial alterations in the model’s
    predictions. The selected examples are subsequently sent for labeling as they
    are believed to possess the most value in terms of training the model effectively.
    Once designated, these informative samples are seamlessly integrated into the
    existing training data, thereby facilitating the necessary updates and improvements
    to the model’s overall performance and accuracy.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several key advantages of the EMC method:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Its ability to actively seek out influential and under-represented regions within
    the input space. This means that it can effectively identify areas that may have
    been overlooked or not given enough attention in a traditional modeling approach.
    Suppose we are training a model to predict housing prices. The input features
    are things such as square footage, number of bedrooms, location, and so on. Using
    a traditional modeling approach, we may collect a random sample of houses to train
    the model on. However, this could lead to certain neighborhoods or house styles
    being under-represented if they are less common. The EMC method would analyze
    the current model and identify areas where new training data would likely lead
    to the largest change in the model predictions. For example, it may find that
    adding more samples from older houses could better calibrate the model’s understanding
    of how age affects price, or gathering data from a new suburban development that
    is under-represented could improve the performance of houses in that area. By
    actively seeking these influential regions, EMC can make the model more robust
    with fewer overall training examples. It reduces the risk of underfitting certain
    areas of the input space compared to passive or random data collection. It can
    help uncover hidden patterns or relationships that may not be immediately apparent,
    further enhancing the overall understanding of the dataset.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is compatible with probabilistic and kernel-based models. By leveraging the
    probabilistic nature of these models, the EMC method can provide insightful and
    accurate predictions. Additionally, its compatibility with kernel-based models
    allows for an enhanced understanding of complex data patterns and relationships.
    This combination of features makes the EMC method a powerful tool for analyzing
    and interpreting data in a wide range of domains.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It allows for estimation without the need for full retraining at each step.
    This means that the process can be more efficient and less time-consuming as it
    eliminates the need to repeatedly train the model from scratch. Instead, the method
    enables model changes to be estimated by focusing on the expected changes in the
    model’s parameters. By utilizing this approach, you can save valuable time and
    resources while still obtaining accurate and reliable estimations.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, EMC queries aim to identify points with the highest potential impact
    on the model. It selects those with the maximum expected impact. This method is
    widely discussed in literature but not implemented in practice due to its high
    computational cost.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll explore EER sampling. This technique reduces the model’s error by
    selecting points that are expected to contribute the most to error reduction.
    By strategically sampling these points, we can improve overall model accuracy.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Sampling with EER
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'EER focuses on measuring the potential decrease in generalization error instead
    of the expected change in the model, as seen in the previous approach. The goal
    is to estimate the anticipated future error of a model by training it with the
    current labeled set and the remaining unlabeled samples. EER can be defined as
    follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>E</mi><msub><mover><mi>P</mi><mo
    stretchy="true">ˆ</mo></mover><mi mathvariant="script">L</mi></msub></msub><mo>=</mo><mrow><msub><mo>∫</mo><mi>x</mi></msub><mrow><mi>L</mi><mfenced
    open="(" close=")"><mrow><mi>P</mi><mfenced open="(" close=")"><mrow><mi>y</mi><mo>|</mo><mi>x</mi></mrow></mfenced><mo>,</mo><mover><mi>P</mi><mo
    stretchy="true">ˆ</mo></mover><mfenced open="(" close=")"><mrow><mi>y</mi><mo>|</mo><mi>x</mi></mrow></mfenced></mrow></mfenced><mi>P</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></mrow></math>](img/15.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
- en: Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="script">L</mml:mi></mml:math>](img/16.png) is the pool of paired
    labeled data, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/17.png),
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mover
    accent="true"><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi
    mathvariant="script">L</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/18.png)
    is the estimated output distribution. L is a chosen loss function that measures
    the error between the true distribution, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/19.png),
    and the learner’s prediction, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mover
    accent="true"><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi
    mathvariant="script">L</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/20.png).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: This involves selecting the instance that is expected to have the lowest future
    error (referred to as *risk*) for querying. This focuses active ML on reducing
    long-term generalization errors rather than just immediate training performance.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: In other words, EER selects unlabeled data points that, when queried and learned
    from, are expected to significantly reduce the model’s errors on new data points
    from the same distribution. By focusing on points that minimize future expected
    errors, as shown in *Figure 2**.9*, EER aims to identify valuable training examples
    that will enhance the model’s ability to generalize effectively. This technique
    targets high-value training examples that will improve the model’s performance
    by minimizing incorrect predictions.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach helps prevent short-term overfitting by avoiding the inclusion
    of redundant similar examples and instead focusing on diverse edge cases that
    better span the feature space. For instance, in the case of an image classifier,
    the technique may prioritize the inclusion of diverse edge cases that capture
    a wide range of features rather than including redundant similar examples:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9 – EER sampling](img/B21789_02_09.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 – EER sampling
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Computing the expected model’s prediction error can be done using various loss
    functions, such as the log loss, which is defined as ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>L</mi><mo>=</mo><mrow><msub><mo>∑</mo><mrow><mi>y</mi><mo>∈</mo><mi>Y</mi></mrow></msub><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>y</mi><mo>|</mo><mi>x</mi></mrow></mfenced><mi mathvariant="normal">l</mi><mi
    mathvariant="normal">o</mi><mi mathvariant="normal">g</mi><mo>(</mo><msub><mover><mi>P</mi><mo
    stretchy="true">ˆ</mo></mover><mi mathvariant="script">L</mi></msub><mfenced open="("
    close=")"><mrow><mi>y</mi><mo>|</mo><mi>x</mi></mrow></mfenced><mo>)</mo></mrow></mrow></mrow></mrow></math>](img/21.png),
    or the 0/1 loss, which is defined as ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>L</mi><mo>=</mo><mn>1</mn><mo>−</mo><munder><mi>max</mi><mrow><mi>y</mi><mo>∈</mo><mi>Y</mi></mrow></munder><msub><mover><mi>P</mi><mo
    stretchy="true">ˆ</mo></mover><mi mathvariant="script">L</mi></msub><mfenced open="("
    close=")"><mrow><mi>y</mi><mo>|</mo><mi>x</mi></mrow></mfenced></mrow></mrow></math>](img/22.png).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'EER incurs a significant time cost due to the estimation of error reduction.
    To calculate the expected generalization error, the classifier must be re-optimized
    for each data point, considering its possible labels. Additionally, it is necessary
    to re-infer the labels of other data points. However, this technique offers a
    couple of key advantages:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: It allows direct optimization of the true objective of reducing the generalization
    error instead of solely focusing on improving training performance. By prioritizing
    the reduction of the generalization error, EER allows for more accurate and reliable
    predictions in real-world scenarios. This not only enhances the overall performance
    of the model but also ensures that it can effectively generalize to unseen data.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It considers the impact on unseen data points, going beyond just the training
    set. By doing so, EER helps to mitigate overfitting, which is a common challenge
    in ML and statistical modeling. Overfitting occurs when a model performs exceedingly
    well on the training data but fails to generalize well to new, unseen data. EER
    tackles this issue head-on by incorporating a comprehensive evaluation of potential
    errors and their reduction. This ensures that the model’s performance is not limited
    to the training set and instead extends to real-world scenarios, making it a valuable
    tool in data-driven decision-making.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EER is a predictive and robust query framework that focuses on maximally reducing
    the model’s generalization error. Similar to the EMC method, the EER method is
    a topic of extensive discussion in literature. However, it has not been widely
    adopted in practical applications primarily because of the significant computational
    resources it demands.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: The next sampling method that we will explore, density-weighted sampling, aims
    to improve diversity by selecting representative points from all density regions.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Understanding density-weighted sampling methods
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Density-weighted methods** are approaches that aim to carefully choose points
    that accurately represent the densities of their respective local neighborhoods.
    By doing so, these methods prioritize the labeling of diverse cluster centers,
    ensuring a comprehensive and inclusive representation of the data.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Density-weighted techniques are highly beneficial and effective when it comes
    to querying points. These techniques utilize a clever combination of an informativeness
    measure and a density weight. An **informativeness measure** provides a score
    of how useful a data point would be for improving the model if we queried its
    label. Higher informativeness indicates the point is more valuable to label and
    add to the training set. In this chapter, we have explored several informativeness
    measures, such as uncertainty and disagreement. In density-weighted methods, the
    informativeness score is combined with a density weight to ensure we select representative
    and diverse queries across different regions of the input space. This is done
    by assigning a weight to each data point based on both its density and its informativeness.
    Data points with higher informativeness and lower density will be given a higher
    weight, and will therefore be more likely to be selected for labeling. Points
    in dense clusters receive lower weights. The density and informativeness are combined
    through multiplicative, exponential, or additive formulations. This balances informativeness
    with density to achieve diversity.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'The density weight represents the density of the local neighborhood surrounding
    each point, allowing for a more comprehensive and accurate sampling of points
    from different densities, as shown in *Figure 2**.10*. This approach avoids the
    pitfall of solely focusing on dense clusters, which could result in redundant
    points. By taking into account the density weight, these techniques guarantee
    that the selected points effectively capture the overall distribution of the dataset.
    As a result, the obtained results are more meaningful and provide deeper insights
    into the data:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10 – Density-weighted sampling](img/B21789_02_10.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 – Density-weighted sampling
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 2**.10*, we can observe the significance of sample density in the
    active ML process. With instance 1 being positioned near the decision boundary,
    this makes it a prime candidate for being chosen as the most uncertain one. However,
    if we analyze the situation more closely, it becomes apparent that selecting instance
    2 would be more advantageous in terms of enhancing the overall quality of the
    model. This is because instance 2 not only represents itself accurately but also
    acts as a representative for other instances within the data distribution. Therefore,
    its inclusion in the active ML process can lead to more comprehensive and reliable
    model improvements.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Implementing queries that would select instance 2 in the preceding example can
    be done with various density-weighted sampling methods, such as kNN density, **kernel
    density estimation** (**KDE**), K-means clustering, and **maximum mean** **discrepancy**
    (**MMD**).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the imports and generating the dummy data:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, let’s understand and apply different density-based techniques for our
    `X` sample data:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '**kNN density** calculates the local density around each data point using its
    *k-nearest neighbors*. The density is estimated by taking the inverse of the average
    distance to the k-closest points. Denser points have higher density, while isolated
    points have lower density. The estimated density is then used as a weight. When
    combined with informativeness criteria such as uncertainty, points in sparser
    neighborhoods get higher density weights, increasing their priority. kNN density
    weighting provides an efficient way to increase sample diversity and avoid over-sampling
    clusters when querying:'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**KDE** estimates the local density around each point using a kernel function
    centered on the point. Typically, a Gaussian kernel is used. The densities from
    the kernels of nearby points are summed to get the overall estimated density.
    As with kNN density, points in sparser regions will have lower kernel density
    compared to crowded areas. These density values can be used as weights when combined
    with informativeness criteria. Points in isolated clusters will be up-weighted,
    increasing their query priority. KDE provides a smooth, probabilistic estimate
    of local density, as opposed to the discrete clusters of kNN. It is more computationally
    expensive than kNN but can be implemented efficiently in high dimensions. KDE
    weighting focuses sampling on representative low-density points:'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**K-means density** clusters the unlabeled data points using k-means into k
    clusters. The size of each cluster indicates its density – smaller clusters correspond
    to sparser regions. This cluster density can be used as a weight when combined
    with informativeness criteria. Points in smaller, tighter clusters get increased
    weight, making them more likely to be queried. This balances sampling across varying
    densities. K-means provides a simple way to estimate density and identify representative
    points from all densities. It is fast and scales well to large datasets. One limitation
    is determining the number of clusters, k, upfront. K-means density weighting focuses
    active ML on diverse cases from all densities equally:'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**MMD** measures the *distance* between distributions to identify points in
    low-density regions. The MMD between a point’s neighborhood distribution and the
    overall data distribution is calculated. A higher MMD indicates that the local
    region is very different from the overall distribution, so it is likely a low-density
    area. These MMD density scores are then used as weights when combined with informativeness
    measures. Points in sparse, isolated regions with high MMD get increased priority
    for querying. This results in balanced sampling across varying densities, thereby
    avoiding cluster oversampling. MMD provides a principled way to estimate density
    that captures useful nonlinear variations. MMD density weighting focuses active
    ML on representative low-density areas:'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, let’s visualize these density-weight sampling methods:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The resulting graph is presented in *Figure 2**.11*:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.11 – A comparison of different density-weighted sampling methods](img/B21789_02_11.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
- en: Figure 2.11 – A comparison of different density-weighted sampling methods
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'Density-weighted sampling methods, including the ones mentioned previously,
    offer diverse advantages:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: They can help improve the performance of the ML model by selecting samples that
    are more likely to be informative. This is because samples that are in high-density
    regions of the data are more likely to be representative of the underlying distribution
    and therefore more likely to be informative for the model.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They help reduce the number of labeled samples needed to train the model. This
    is because density-weighted sampling can help focus the labeling effort on the
    most informative samples, which can lead to faster convergence of the model.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They can be used with any type of data. Density-weighted sampling does not make
    any assumptions about the data distribution, so it can be used with any type of
    data, including structured and unstructured data.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To conclude, density weighting, which is a technique that’s used to increase
    the diversity and coverage of samples, offers an effective and efficient approach.
    By assigning weights to each sample based on their density, this method ensures
    that the resulting sample set represents the underlying population more accurately.
    With this approach, you can obtain a more comprehensive understanding of the data,
    allowing for better decision-making and analysis. Overall, density weighting is
    a valuable tool in research and statistical analysis, allowing you to highlight
    hidden patterns and trends that might otherwise be overlooked.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have discussed several query strategies, let’s compare them to
    understand how they fare against each other:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.12 – Comparison chart for query strategies](img/B21789_02_12.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
- en: Figure 2.12 – Comparison chart for query strategies
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2**.12* summarizes and compares various query strategies that have
    been discussed in this chapter.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered key techniques such as uncertainty sampling, query-by-committee,
    EMC, EER, and density weighting for designing effective active ML query strategies.
    Moving forward, in the next chapter, our focus will shift toward exploring strategies
    for managing the human in the loop. It is essential to optimize the interactions
    with the oracle labeler to ensure maximum efficiency in the active ML process.
    By understanding the intricacies of human interaction and leveraging this knowledge
    to streamline the labeling process, we can significantly enhance the efficiency
    and effectiveness of active ML algorithms.In the next chapter we will discuss
    how to manage the role of human labelers in active ML.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
