- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Conformal Prediction for Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter dives deeper into the topic of conformal prediction for classification
    problems. We will explore the concept of classifier calibration and demonstrate
    how conformal prediction compares to other calibration methods before introducing
    Venn-ABERS predictors as specialized techniques within conformal prediction. Additionally,
    we will provide an overview of open source tools that can be utilized to implement
    conformal prediction for classifier calibration.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Classifier calibration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating calibration performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various approaches to classifier calibration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conformal prediction for classifier calibration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open source tools for conformal prediction in classification problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifier calibration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most statistical, machine learning, and deep learning models output predicted
    class labels, and the models are typically evaluated in terms of their accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy is a prevalent measure for assessing the performance of a machine learning
    classification model. It quantifies the ratio of instances that are correctly
    identified to the overall count in the dataset. In other words, accuracy tells
    us how often the model’s predictions align with the true labels of the data.
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy score measures how often the model’s predictions match the true
    observed labels. It is calculated as the fraction of correct predictions out of
    all predictions made. Accuracy scores between 0 and 1 quantify how accurate the
    model’s predictions are compared to the ground truth data. A higher accuracy score
    close to 1 signifies that the model is performing very accurately overall, with
    most of its predictions being correct. A lower accuracy approaching 0 indicates
    poor performance, with the majority of the model’s predictions being incorrect
    compared to the true labels.
  prefs: []
  type: TYPE_NORMAL
- en: The closer the accuracy is to 1, the better the model is performing. The closer
    it is to 0, the worse the model is at predicting the true labels in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy is a straightforward and intuitive metric that is easy to understand
    and interpret. However, it may not always be the most suitable metric, especially
    when dealing with imbalanced datasets. In imbalanced datasets, where the number
    of instances in different classes is significantly different, accuracy alone may
    be misleading. In an imbalanced dataset, a classifier that consistently predicts
    the majority class can attain a high accuracy based on the class distribution,
    even if it doesn’t identify the minority class.
  prefs: []
  type: TYPE_NORMAL
- en: In these scenarios, it’s crucial to look at other evaluation measures, such
    as precision, recall, F1 score, or ROC-AUC, to gain a fuller insight into the
    model’s efficacy.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the specific problem and the requirements, other metrics and considerations,
    such as the cost of false positives or false negatives, might be more relevant.
    Therefore, it is essential to assess the model’s performance using multiple evaluation
    metrics and consider the context in which the classification model will be applied.
  prefs: []
  type: TYPE_NORMAL
- en: 'Accuracy alone may be insufficient, particularly in critical applications,
    for several reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Imbalanced datasets**: In scenarios where the dataset is imbalanced, accuracy
    can be misleading. If the majority class dominates the dataset, a model that predicts
    only the majority class can achieve high accuracy but fails to capture the minority
    class effectively. This can be problematic in critical applications where correctly
    identifying rare events or detecting anomalies is crucial.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost of errors**: In many real-world applications, the cost of false positives
    and false negatives can vary significantly. Accuracy treats all errors equally
    and does not consider the consequences of misclassifications. For instance, in
    a medical diagnosis, a false negative (failing to detect a disease) can be far
    more critical than a false positive. In such cases, accuracy alone does not provide
    sufficient information about the model’s performance in terms of the actual impact
    on decision-making and outcomes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Probability estimation**: Accuracy does not take into account the confidence
    or uncertainty of the model’s predictions. It is essential to assess the model’s
    ability to provide well-calibrated probability estimates. Calibration refers to
    the alignment between predicted probabilities and the true probabilities of events.
    A poorly calibrated model may provide overly confident or unreliable probability
    estimates, which can lead to incorrect decisions or the misinterpretation of risks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decision threshold**: Accuracy does not consider the decision threshold used
    for classification. Different decision thresholds can result in varying trade-offs
    between precision and recall. Depending on the application, certain misclassification
    errors may be more tolerable than others. Evaluating only accuracy does not provide
    insights into the model’s performance at different decision thresholds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s get to understanding the concepts of classifier calibration next.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the concepts of classifier calibration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous chapters, we defined and discussed the concept of classifier
    calibration.
  prefs: []
  type: TYPE_NORMAL
- en: Classifier calibration involves adjusting the predicted probabilities from a
    classification model so that they better reflect the true likelihood of each class.
    The goal is to make the predictions better calibrated.
  prefs: []
  type: TYPE_NORMAL
- en: A well-calibrated classifier is one where the predicted probabilities match
    the empirical probabilities. For example, if the model predicts “class A” with
    60% probability across 100 examples, then class A should occur approximately 60
    times out of those 100 predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'More formally, a well calibrated classifier satisfies the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: P(actual class is c | predicted probability of c is p) ≈ p
  prefs: []
  type: TYPE_NORMAL
- en: This means the observed frequency of class c should be close to p when the model
    predicts class c with probability p.
  prefs: []
  type: TYPE_NORMAL
- en: Calibration adjustment ensures the predicted probabilities are aligned with
    the relative frequencies in the actual data. The predictions are calibrated to
    the empirical evidence so that a predicted probability of `0.7` corresponds to
    a 70% chance based on the data. This calibration is essential for probability
    estimates to be meaningful and reliable.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, consider a binary classifier that predicts whether an email is
    spam or not. For each email, it might predict a probability, say, `0.8`, which
    means it believes there’s an 80% chance that the email is spam. If the classifier
    is well calibrated, then out of all emails that it assigns a spam probability
    of `0.8`, about 80% should be spam.
  prefs: []
  type: TYPE_NORMAL
- en: Without calibration, the output probabilities of a classifier might not correspond
    to the true likelihood of the predicted class, which can be problematic for decision-making.
    Calibration methods adjust these probabilities to better reflect reality. The
    goal is to have the output probabilities of the classifier be as close as possible
    to the true probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Model calibration is crucial because of the following aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reliable probability estimates**: Calibrated classifiers provide accurate
    and reliable probability estimates for the predicted classes. Probability estimates
    reflect the model’s confidence in its predictions and can be interpreted as the
    likelihood of a particular class being correct. In many real-world applications,
    such as medical diagnosis, risk assessment, or fraud detection, having well-calibrated
    probability estimates is crucial for making informed decisions and assessing the
    level of uncertainty associated with the predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reliable risk assessment**: In many domains, accurate risk assessment is
    paramount. Calibrated classifiers provide well-calibrated probability estimates
    that reflect the true likelihood of events. This allows for more accurate and
    reliable risk assessment, enabling decision-makers to allocate resources, prioritize
    actions, or estimate the impact of certain events more effectively. For instance,
    in credit scoring, a calibrated classifier can provide accurate estimates of the
    probability of default, aiding in better risk management.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decision threshold determination**: In classification tasks, decisions are
    often made by setting a threshold on the predicted probabilities. This threshold
    determines the trade-off between precision and recall, or equivalently, between
    false positives and false negatives. Calibrated classifiers help in selecting
    an appropriate decision threshold by aligning the probability estimates with the
    desired trade-off, considering the specific costs or consequences associated with
    different types of errors. This ensures that decision-making aligns with the objectives
    and requirements of the application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interpretability and trust**: Calibration enhances the interpretability of
    the model’s predictions. Calibrated probability estimates can be used to understand
    the level of confidence the model has in its predictions. This transparency helps
    in building trust with users, stakeholders, and regulatory authorities, particularly
    in domains where decision-making is critical and must be justified. By providing
    well-calibrated probability estimates, the model’s predictions can be better understood
    and validated, instilling confidence in its reliability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Improved fairness**: Calibrated classifiers can contribute to fairness in
    decision-making processes. By providing well-calibrated probability estimates,
    they can help in identifying and mitigating biases that may arise from the underlying
    training data or model assumptions. This allows for fairer and more equitable
    predictions, ensuring that different groups are treated consistently and without
    undue bias.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is essential to evaluate model calibration to ensure that the model’s predictions
    align with the underlying uncertainties in the data. This evaluation helps in
    making informed decisions, understanding the model’s limitations, and managing
    the risks associated with misclassifications or incorrect probability estimates.
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, many classifiers, such as logistic regression or support vector
    machines, generate probability estimates based on their internal models. However,
    these probability estimates are not always accurate or well calibrated, leading
    to overconfidence or under confidence in the predictions. For instance, a classifier
    may assign probabilities close to `1.0` or `0.0` to certain examples when it should
    have assigned probabilities closer to `0.7` or `0.3`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: To address this issue, various techniques have been proposed to calibrate classifiers
    and improve the reliability of their probability estimates. These techniques aim
    to map the original probability scores to more accurate and calibrated probabilities.
    The goal is to ensure that, on average, the predicted probabilities match the
    observed frequencies or likelihoods of the predicted events.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating calibration performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Evaluating the calibration performance of a classifier is crucial to assessing
    the reliability and accuracy of its probability estimates. Calibration evaluation
    allows us to determine how well the predicted probabilities align with the true
    probabilities or likelihoods of the predicted events. Here are some commonly used
    techniques for evaluating the calibration performance of classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Calibration plot**: A calibration plot visually assesses how well a classifier’s
    predicted probabilities match the true class frequencies. The *x* axis shows the
    predicted probabilities for each class, while the *y* axis shows the empirically
    observed frequencies for those predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a well-calibrated model, the calibration curve should closely match the
    diagonal, representing a 1:1 relationship between predicted and actual probabilities.
    Deviations from the diagonal indicate miscalibration, where the predictions are
    inconsistent with empirical evidence.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calibration plots provide an intuitive way to identify if a classifier is over
    confident or under confident in its estimates across different probability ranges.
    The closer the curve aligns with the diagonal, the better calibrated the predicted
    probabilities are. Significant deviations signal that recalibration is needed
    for the model’s outputs to be reliable:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Calibration plot](img/B19925_06_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Calibration plot
  prefs: []
  type: TYPE_NORMAL
- en: '**Calibration error**: Calibration error measures the average difference between
    the predicted probabilities and the actual probabilities of the forecasted events.
    It’s determined by the mean absolute deviation between the estimated probabilities
    and the observed probabilities. Lower calibration error values indicate better
    calibration performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Calibration metrics**: Several metrics can be used to evaluate the calibration
    performance of a classifier. Commonly used metrics include the **expected calibration
    error** (**ECE**), log loss, and the Brier score. ECE measures the calibration
    error by partitioning the predicted probabilities into bins and calculating the
    difference between the average predicted probabilities and the average empirical
    probabilities within each bin. The Brier score assesses the overall accuracy of
    the predicted probabilities, considering both calibration and resolution (sharpness)
    of the probability estimates. The Brier score is a commonly used scoring rule
    for assessing the calibration of probabilistic forecasts. It measures the mean
    squared difference between the predicted probabilities and the actual outcomes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a set of *N* predictions, the Brier score is calculated as BS =  1 _ N ∑ t=1 N  
    (f t − o t) 2, where *ft,i* is the forecasted probability for an event, *i*, at
    time *t*, and *ot,i* is the actual outcome of an event, *i,* at time *t* (0 or
    1).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Squaring the errors gives more weight to large mistakes. The average squared
    error is then taken across all predictions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A lower Brier score indicates better calibration, with a minimum of 0 for a
    perfect probabilistic forecaster. It penalizes both inaccurate and over/underconfident
    predictions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Cross-validation**: Cross-validation is a technique for estimating the calibration
    performance. It does this by partitioning the dataset into multiple folds and
    training the model on one fold while evaluating calibration on the remaining folds.
    This helps in assessing the calibration performance across different subsets of
    the data and provides a more robust evaluation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When evaluating the calibration performance, it is important to compare the
    results against an appropriate baseline. A well-calibrated classifier should outperform
    random or uncalibrated probability estimates.
  prefs: []
  type: TYPE_NORMAL
- en: Various approaches to classifier calibration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before exploring how conformal prediction can provide calibrated probabilities,
    we will first discuss some common non-conformal calibration techniques and their
    strengths and weaknesses. These include histogram binning, Platt scaling, and
    isotonic regression.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that the following methods are not part of the conformal
    prediction framework. We are covering them to build intuition about calibration
    and highlight some of the challenges with conventional calibration approaches.
    This background will motivate the need for and benefits of the conformal prediction
    perspective so that we can obtain reliable probability estimates.
  prefs: []
  type: TYPE_NORMAL
- en: The calibration techniques we will explore, including histogram binning, Platt
    scaling, and isotonic regression, represent widely used approaches for adjusting
    classifier confidence values. However, as we will discuss, they have certain limitations
    regarding model flexibility, computational expense, and generalization.
  prefs: []
  type: TYPE_NORMAL
- en: By first understanding these existing calibration methods and their drawbacks,
    we will be equipped to better comprehend the value of conformal prediction’s inherent
    calibration properties. This background provides context into the calibration
    problem before presenting conformal prediction as an attractive modern solution.
  prefs: []
  type: TYPE_NORMAL
- en: Histogram binning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Histogram binning is a technique that’s commonly used in classifier calibration
    to improve the calibration performance of probability estimates. It involves dividing
    the predicted probabilities into bins or intervals and mapping them to more accurate
    and reliable probabilities based on the empirical frequencies or observed proportions
    of the predicted events within each bin. The goal of histogram binning is to align
    the predicted probabilities with the true probabilities of the events, resulting
    in a better-calibrated classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of histogram binning can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Partitioning**: The predicted probabilities are divided into a predefined
    number of bins or intervals. The number of bins can vary based on the dataset
    and the desired granularity of calibration.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Bin assignment**: Each instance in the dataset is assigned to the corresponding
    bin based on its predicted probability. For example, if we have five bins with
    equal width intervals (for example, *0-0.2*, *0.2-0.4*, *0.4-0.6*, *0.6-0.8*,
    and *0.8-1.0*), an instance with a predicted probability of 0.45 would be assigned
    to the third bin.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`0.7`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Mapping to calibrated probabilities**: The predicted probabilities within
    each bin are then mapped or adjusted to more accurate and calibrated probabilities
    based on the empirical proportions of positives. This mapping can be performed
    using various techniques, such as isotonic regression or Platt scaling.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Overall calibration**: Once the mapping has been applied to all the bins,
    the calibrated probabilities are obtained by combining the probabilities from
    all the bins. The result is a set of calibrated probabilities that better align
    with the true probabilities or likelihoods of the events.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here are some potential disadvantages of using histogram binning for classifier
    calibration:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Inflexibility**: Histogram binning divides the prediction space into fixed
    intervals. It lacks the flexibility to model complex, nonlinear miscalibration
    patterns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data underutilization**: Hard binning discards information within each bin.
    The calibration mapping uses only the bin averages rather than the full distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sensitivity to the binning scheme**: The calibration quality is dependent
    on the specific binning thresholds chosen, which can be arbitrary. Optimal binning
    is often not known beforehand.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Discontinuities**: Adjacent bins may have very different adjustments, leading
    to abrupt discontinuities in the calibration mapping. This can introduce artifacts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Difficulty extrapolating**: The binning calibration is based only on the
    training data distribution. It may not extrapolate well to unseen data with sparse
    or no coverage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Curse of dimensionality**: Histograms do not scale well to high-dimensional
    feature spaces. The data becomes too sparse within each bin.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limited model expressiveness**: Histograms can only represent simple, low-order
    calibration relationships. They cannot model complex miscalibration patterns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Histogram binning can be simple to implement but provides an inflexible, discontinuous
    calibration mapping. More sophisticated dense modeling and smoothing are often
    required for optimal calibration quality.
  prefs: []
  type: TYPE_NORMAL
- en: Platt scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Platt scaling, sometimes referred to as Platt’s method or sigmoid calibration,
    is a post-processing approach that’s employed to refine the output probabilities
    of a binary classification model. It was introduced by John C. Platt in 1999 to
    transform the raw output scores of a support vector machines classifier into well-calibrated
    probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of Platt scaling is to adjust the predicted scores or logits produced
    by the classifier in such a way that they reflect more accurate estimates of the
    true probabilities. This is achieved by fitting a logistic regression model on
    the classifier’s scores while using a labeled validation set or a holdout set.
    The logistic regression model is trained to map the original scores to calibrated
    probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps involved in Platt scaling are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Collect a labeled validation set or a holdout set that is distinct from the
    training data used to train the classifier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the classifier to generate the raw output scores or logits for the instances
    in the validation set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit a logistic regression model on the validation set, treating the raw scores
    as the independent variable and the true class labels as the dependent variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the logistic regression model using standard techniques such as maximum
    likelihood estimation or gradient descent to estimate the model’s parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the logistic regression model has been trained, it can be used as a calibration
    function. Given a new instance, the raw score produced by the classifier is input
    into the logistic regression model, which transforms it into a calibrated probability
    estimate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The logistic regression model essentially learns the transformation from the
    raw scores to calibrated probabilities by estimating the intercept and slope parameters.
    This transformation is represented by the sigmoid function, which maps the scores
    to probabilities between `0` and `1`.
  prefs: []
  type: TYPE_NORMAL
- en: Platt scaling aims to achieve better calibration by adjusting the predicted
    probabilities to match the true probabilities or likelihoods of the events.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that Platt scaling assumes that the relationship between
    the raw scores and the true probabilities can be modeled by a logistic function.
    If the underlying relationship is more complex, other calibration methods such
    as conformal prediction may be more suitable.
  prefs: []
  type: TYPE_NORMAL
- en: 'While Platt scaling can be an effective technique for calibrating classifier
    probabilities, it is important to be aware of its limitations and potential disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Requirement for a separate validation set**: Platt scaling requires a labeled
    validation set or holdout set that is distinct from the training data. This means
    additional data may be needed for calibration, which can be a limitation in situations
    where obtaining labeled data is challenging or costly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Assumption of a logistic relationship**: Platt scaling assumes that the relationship
    between the raw scores and the true probabilities can be accurately modeled by
    a logistic function. If the underlying relationship is more complex or different,
    the logistic regression model may not be able to capture the true calibration
    mapping adequately.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sensitivity to extreme scores**: Platt scaling can be sensitive to extreme
    scores or outliers in the validation set. Outliers may disproportionately influence
    the calibration function, leading to potential overfitting or a suboptimal calibration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lack of flexibility for different calibration shapes**: The logistic regression
    model used in Platt scaling is constrained to fit a sigmoid function, which may
    not be suitable for all calibration shapes. If the desired calibration shape deviates
    significantly from a sigmoid curve, Platt scaling may not achieve optimal calibration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limited applicability to multiclass problems**: Platt scaling is primarily
    designed for binary classification problems. Extending it to multiclass classification
    can be challenging as it requires adapting the calibration mapping to handle multiple
    classes and their respective probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Potential overconfidence in extreme probabilities**: Platt scaling may introduce
    overconfidence in extreme predicted probabilities. The calibrated probabilities
    near the boundaries (close to 0 or 1) might be more extreme than they should be,
    leading to overconfident predictions in those regions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dependence on the quality of the validation set**: The effectiveness of Platt
    scaling is dependent on the quality and representativeness of the labeled validation
    set. If the validation set does not accurately capture the true distribution of
    the target variable, the resulting calibration may be suboptimal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s important to consider these disadvantages and assess whether Platt scaling
    is suitable for a specific application or if alternative calibration methods,
    such as methods based on conformal prediction, may be more appropriate based on
    the specific characteristics of the problem and dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Isotonic regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Isotonic regression is a non-parametric regression technique that’s used for
    calibration and monotonicity modeling. It is commonly applied to adjust the output
    scores or predicted probabilities of a classifier to improve their calibration.
    Isotonic regression seeks to find a monotonic function that maps the original
    scores to calibrated probabilities while preserving the ordering of the scores.
  prefs: []
  type: TYPE_NORMAL
- en: The primary goal of isotonic regression is to determine a non-decreasing function
    that reduces the sum of squared discrepancies between the predicted probabilities
    and the target probabilities or actual occurrences. By fitting a piecewise linear
    or piecewise constant function to the data, isotonic regression ensures that the
    predicted probabilities are monotonically increasing or non-decreasing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps involved in isotonic regression are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Collect a labeled validation set or a holdout set that is separate from the
    training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the classifier to generate the raw output scores or probabilities for the
    instances in the validation set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sort the instances in the validation set based on the raw scores.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the isotonic regression function as the identity function, where
    the initial predicted probabilities are equal to the raw scores.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Iteratively update the isotonic regression function by adjusting the predicted
    probabilities to minimize the squared differences between the predicted probabilities
    and the target probabilities. This adjustment is subject to the constraint of
    non-decreasing probabilities.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Repeat the updating process until convergence or a stopping criterion is reached:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Isotonic regression](img/B19925_06_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Isotonic regression
  prefs: []
  type: TYPE_NORMAL
- en: Once the isotonic regression model has been trained, it can be used to map the
    raw scores of new instances to calibrated probabilities. The model ensures that
    the predicted probabilities are monotonically increasing and better aligned with
    the true probabilities or likelihoods of the events.
  prefs: []
  type: TYPE_NORMAL
- en: 'While isotonic regression is a valuable technique for calibrating classifier
    probabilities, it is important to consider its limitations and potential disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Potential overfitting**: Isotonic regression can suffer from overfitting
    if the calibration function is overly complex or if the calibration dataset is
    small. Regularization techniques, such as using a limited number of segments in
    the piecewise linear function, can help prevent overfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complexity and computational cost**: Isotonic regression can be resource-intensive,
    especially with vast datasets or when navigating high-dimensional features. As
    the number of data points and unique scores or probabilities grow, so does the
    complexity of isotonic regression. It’s crucial to weigh up the computational
    limitations when using isotonic regression for extensive tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sensitivity to outliers**: Isotonic regression can be sensitive to outliers
    in the data. Outliers may have a significant impact on the estimated calibration
    function, potentially leading to suboptimal calibration. Careful data preprocessing
    or outlier detection techniques may be necessary to mitigate this issue.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limited flexibility for complex calibration shapes**: Isotonic regression
    assumes a monotonic relationship between the scores and the probabilities, which
    constrains the calibration function to be piecewise constant or piecewise linear.
    This limits the model’s flexibility to capture more complex or nonlinear calibration
    shapes. If the desired calibration shape deviates significantly from monotonicity,
    isotonic regression may not provide an optimal fit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Need for sufficient data**: Isotonic regression requires a sufficient amount
    of labeled data to estimate the calibration function accurately. If the calibration
    dataset is small or imbalanced, the estimation may be suboptimal. Ensuring a representative
    and adequately sized calibration dataset is important for reliable calibration
    results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Difficulty in handling multiclass problems**: Isotonic regression is inherently
    designed for binary classification problems, so extending it to multiclass problems
    is not straightforward. Adapting isotonic regression to handle multiple classes
    and their respective probabilities requires careful consideration and modification
    of the algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lack of probabilistic interpretation**: Unlike Platt scaling, which explicitly
    models probabilities using logistic regression, isotonic regression does not provide
    a probabilistic interpretation of the calibrated scores. It focuses solely on
    ensuring monotonicity and may not directly estimate well-calibrated probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to evaluate these limitations and consider the characteristics
    of the problem at hand when deciding whether isotonic regression is the most appropriate
    calibration method.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the advantages and disadvantages of different calibration techniques,
    such as conformal prediction and Platt scaling, can help determine the best approach
    for achieving well-calibrated probabilities in a specific application.
  prefs: []
  type: TYPE_NORMAL
- en: Conformal prediction for classifier calibration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Conformal prediction is a powerful framework for probabilistic prediction that
    provides valid and well-calibrated prediction sets and prediction intervals. It
    offers a principled approach to quantify and control the uncertainty associated
    with the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: We have already seen how conformal prediction approaches, such as **inductive
    conformal prediction** (**ICP**) and **transductive conformal prediction** (**TCP**),
    aim to generate sets that have accurate coverage probabilities. To recap, conformal
    prediction computes p-values and constructs prediction sets by comparing the p-values
    of each potential label with a selected significance level.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike Platt scaling, histogram binning, and isotonic regression, which focus
    on calibrating the predicted probabilities or scores, conformal prediction takes
    a more comprehensive approach by providing prediction sets that encompass the
    uncertainty associated with the predictions and enhances the reliability and interpretability
    of predictions by providing valid measures of confidence or significance.
  prefs: []
  type: TYPE_NORMAL
- en: Venn-ABERS conformal prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Classical methods such as Platt scaling was initially developed as parametric
    solutions for calibrating classifiers. However, these methods are becoming somewhat
    outdated and have limitations due to their simplistic assumptions, resulting in
    suboptimal calibration of probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Platt scaling assumes a logistic relationship between scores and probabilities,
    which may not adequately capture the actual calibration shape in practical scenarios.
    It is worth noting that Platt’s original paper in 1999 did not explicitly discuss
    the underlying assumptions of this approach. However, recent research (see *Beta
    calibration: a well-founded and easily implemented improvement on logistic calibration
    for binary classifiers*: https://proceedings.mlr.press/v54/kull17a.html) has revealed
    that these assumptions are essentially equivalent to assuming both normality and
    homoscedasticity, which are overly restrictive assumptions for real-world datasets.
    Real datasets often exhibit more complex and diverse patterns that cannot be accurately
    captured by such assumptions. Therefore, relying solely on Platt scaling with
    its underlying assumptions may result in suboptimal calibration and poorly calibrated
    probabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: Isotonic regression, as an approach, assumes a monotonic relationship between
    scores and probabilities. However, this assumption may not capture the intricate
    nature of the calibration curve in all cases. Furthermore, isotonic regression
    relies on the assumption of perfect ranking (an ROC AUC of 1) on the test dataset,
    which is rarely achievable in real-world datasets. Additionally, it has been demonstrated
    that isotonic regression can overfit when applied to smaller datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The assumption of a monotonic relationship limits the flexibility of isotonic
    regression to model more complex calibration curves that may exhibit non-monotonic
    patterns. Moreover, the requirement of perfect ranking on the test dataset is
    often unrealistic as datasets typically involve inherent noise and uncertainty.
    This assumption can lead to suboptimal calibration results in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the issue of overfitting with isotonic regression becomes more
    prominent when dealing with smaller datasets. When the dataset’s size is limited,
    isotonic regression may overly adjust to the noise or specific characteristics
    of the training data, resulting in poor generalization performance.
  prefs: []
  type: TYPE_NORMAL
- en: Because of their simplistic assumptions, Platt scaling and isotonic regression
    may not achieve optimal calibration and may not deliver well-calibrated probabilities.
    These methods may struggle to capture nonlinear or more intricate calibration
    patterns, limiting their effectiveness in certain applications.
  prefs: []
  type: TYPE_NORMAL
- en: To address the limitations of classical calibrators, such as Platt scaling and
    isotonic regression, a powerful solution called Venn-ABERS has been developed
    by the creator of conformal prediction, Vladimir Vovk , Ivan Petej and Valentina
    Fedorova”, Venn-ABERS is a conformal prediction method that offers mathematical
    guarantees of validity, regardless of the data distribution, dataset size, or
    underlying classification model.
  prefs: []
  type: TYPE_NORMAL
- en: This work is detailed in the NeurIPS paper titled *Large-scale probabilistic
    predictors with and without guarantees of validity* ([https://papers.nips.cc/paper_files/paper/2015/hash/a9a1d5317a33ae8cef33961c34144f84-Abstract.html](https://papers.nips.cc/paper_files/paper/2015/hash/a9a1d5317a33ae8cef33961c34144f84-Abstract.html)).
    For a more mathematical understanding, watch the associated presentation, *Large-Scale
    Probabilistic Prediction With and Without Validity Guarantees*, at [https://www.youtube.com/watch?v=ksrUJdb2tA8](https://www.youtube.com/watch?v=ksrUJdb2tA8).
  prefs: []
  type: TYPE_NORMAL
- en: 'The name Venn-ABERS is derived from a combination of Venn predictors, another
    class of conformal predictors, and the initials of the authors who contributed
    to a classical paper called *An Empirical Distribution Function for Sampling with
    Incomplete Information* (M. Ayer, H.D. Brunk, G.M. Ewing, W.T. Reid, and E. Silverman:
    [https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-26/issue-4/An-Empirical-Distribution-Function-for-Sampling-with-Incomplete-Information/10.1214/aoms/1177728423.full](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-26/issue-4/An-Empirical-Distribution-Function-for-Sampling-with-Incomplete-Information/10.1214/aoms/1177728423.full)).'
  prefs: []
  type: TYPE_NORMAL
- en: So, how do Venn-ABERS predictors work? Rather than constructing isotonic regression
    once, Venn-ABERS fits isotonic regression twice by assuming that each test object
    can have both label 0 and label 1\. This means that each test object is added
    to the calibration set twice, once with label 0 and once with label 1\. Two separate
    isotonic regressions are then fitted, resulting in two probabilities, *p0* and
    *p1*, for each test object.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that both *p0* (lower bound) and *p1* (upper bound)
    represent probabilities of the object belonging to class 1\. These probabilities
    create a prediction interval for the probability of class 1, with mathematical
    guarantees that the actual probability falls within this interval.
  prefs: []
  type: TYPE_NORMAL
- en: Consequently, Venn-ABERS solves the problem beautifully, without requiring assumptions
    about score distributions such as Platt scaling and without suffering from overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: The Venn-ABERS prediction is a multi-predictor, and the width of the interval
    (*p0, p1*) contains valuable information about the confidence of classification.
    In larger datasets, *p0* and *p1* are typically very close to each other. However,
    for smaller and more challenging datasets, *p0* and *p1* may diverge, indicating
    that certain objects are difficult to classify due to factors such as data distribution,
    insufficient data, or the underlying classifier’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: Importantly, in critical situations, the Venn-ABERS predictor not only outputs
    accurate and well-calibrated probabilities but also issues an “alert” by widening
    the (*p0, p1*) interval. This alert indicates that the decision-making process
    should consider the increased uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: For practical decision-making purposes, the probabilities can be combined into
    a single value using *p = p1 / (1 - p0 + p1)*. This combined probability of class
    *1*, *p*, can then be utilized for decision-making tasks such as loan granting
    or determining whether to disable autopilot in autonomous car. With the inclusion
    of *p*, the decision-making process can be successfully concluded.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing calibration methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given the range of calibration methods, you might be wondering how they are
    compared to each other. We have already seen that classical methods such as Platt
    scaling and isotonic regression rely on restrictive assumptions and, unlike the
    conformal prediction Venn-ABERS method, do not have validity guarantees. An interesting
    question is also how the performance of different methods compares empirically
    across a range of datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Such a study was performed, and the results were summarized in the paper *Probabilistic
    Prediction in scikit-learn* ([http://www.diva-portal.org/smash/get/diva2:1603345/FULLTEXT01.pdf](http://www.diva-portal.org/smash/get/diva2:1603345/FULLTEXT01.pdf)).
    In this paper, a large experimental study was conducted to investigate the calibration
    of scikit-learn models out of the box. In addition, the study looked at whether
    calibration techniques such as Platt scaling, isotonic regression, and Venn-ABERs
    can improve calibration.
  prefs: []
  type: TYPE_NORMAL
- en: 'The result of the study showed that of the seven algorithms evaluated (logistic
    regression, random forest, AdaBoost, gradient boosting, kNN, naïve Bayes, and
    decision tree), the only model that obtained well-calibrated predictions was logistic
    regression. Calibration enhances all models, especially decision trees, boosted
    trees (such as XGBoost, LightGBM, and CatBoost), and naïve Bayes. This underscores
    the clear advice for professionals: obtained relatively well-calibrated predictions
    was logistic regression.'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the study uncovered a notable finding that miscalibrated models
    tend to exhibit a high level of overconfidence. Surprisingly, even logistic regression,
    although to a lesser extent compared to other models, displayed systematic optimism
    in its predictions.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, these miscalibrated models tended to assign higher probabilities
    or confidence to their predictions than what was warranted by the actual outcomes.
    This overconfidence could potentially lead to misguided decision-making or misplaced
    trust in the reliability of the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: It is crucial to recognize that while logistic regression demonstrated better
    calibration compared to other models, it still exhibited a certain level of systematic
    optimism. This highlights the importance of thoroughly evaluating and calibrating
    models, even those considered to be well-calibrated, to ensure accurate and reliable
    probabilistic predictions.
  prefs: []
  type: TYPE_NORMAL
- en: When examining the calibration techniques in terms of their benefits for calibration,
    their order of effectiveness is typically observed to be Venn-ABERS, followed
    by Platt scaling and isotonic regression.
  prefs: []
  type: TYPE_NORMAL
- en: Venn-ABERS tends to demonstrate the most significant improvement in calibration,
    providing notable benefits in terms of achieving well-calibrated predictions.
    Its utilization within the conformal prediction framework allows for reliable
    estimation of uncertainty and enhanced calibration performance.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, the comparison of calibration techniques revealed that Venn-ABERS
    tends to yield the most substantial benefits, followed by Platt scaling and isotonic
    regression. It is important to select the appropriate technique based on the specific
    requirements of the problem at hand, considering factors such as the complexity
    of the calibration curve and the desired level of calibration improvement.
  prefs: []
  type: TYPE_NORMAL
- en: The study’s findings emphasized that miscalibrated models often exhibit overconfidence,
    and even logistic regression, although more calibrated than other models, can
    display systematic optimism. This underlines the necessity of assessing and enhancing
    the calibration of models to avoid unwarranted confidence and make informed decisions
    based on accurate probabilistic predictions. The research further indicated that
    uncalibrated models frequently exhibit overconfidence. This includes logistic
    regression, which tends to be systematically optimistic, although to a lesser
    extent.
  prefs: []
  type: TYPE_NORMAL
- en: Open source tools for conformal prediction in classification problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While deep-diving into the intricacies of conformal prediction for classification,
    it has become evident that the right tools can significantly enhance our implementation
    efficiency. Recognizing this, the open source community has made remarkable contributions
    by providing various tools tailored for this purpose. In this section, we will
    explore some of the prominent open source tools for conformal prediction that
    can seamlessly integrate into your projects and elevate your predictive capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Nonconformist
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`nonconformist` ([https://github.com/donlnz/nonconformist](https://github.com/donlnz/nonconformist))
    is a classical conformal prediction package that can be used for conformal prediction
    in classification problems.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s illustrate how to create an ICP using `nonconformist`. You can find the
    Jupyter notebook containing the relevant code at [https://github.com/PacktPublishing/Practical-Guide-to-Applied-Conformal-Prediction/blob/main/Chapter_06.ipynb](https://github.com/PacktPublishing/Practical-Guide-to-Applied-Conformal-Prediction/blob/main/Chapter_06.ipynb):'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the `nonconformist` documentation here: http://donlnz.github.io/nonconformist/index.html.
    First, we will install `nonconformist` using the standard functionality – that
    is, `pip install`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can import the relevant modules as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this case, we created an ICP with a margin nonconformity measure; we looked
    at this in previous chapters. This ICP uses logistic regression as the underlying
    classifier:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we must fit the ICP using the proper training set and calibrate it using
    the calibration set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using the trained model, we can obtain the predicted class scores on the calibration
    and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the notebook, we use a bank marketing dataset ([https://archive.ics.uci.edu/dataset/222/bank+marketing](https://archive.ics.uci.edu/dataset/222/bank+marketing))
    related to the direct marketing campaigns of a Portuguese banking institution.
    This dataset contains the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: '`age` (numeric)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`job`: Type of job (categorical: `admin`, `unknown`, `unemployed`, `management`,
    `housemaid`, `entrepreneur`, `student`, `blue-collar`, `self-employed`, `retired`,
    `technician`, or `services`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`marital`: Marital status (categorical: `married`, `divorced`, or `single`;
    note that `divorced` means divorced or widowed)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`education` (categorical: `unknown`, `secondary`, `primary`, or `tertiary`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`default`: Has credit in default? (binary: `yes` or `no`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`balance`: Average yearly balance, in euros (numeric)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`housing`: Has a housing loan? (binary: `yes` or `no`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`loan`: Has a personal loan? (binary: `yes` or `no`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following features are related to the last contact of the current campaign:'
  prefs: []
  type: TYPE_NORMAL
- en: '`contact`: Contact communication type (categorical: `unknown`, `telephone`,
    and `cellular`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`day`: Last contact day of the month (numeric)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`month`: Last contact month of the year (categorical: `jan`, `feb`, `mar`,
    ..., `nov`, `dec`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`duration`: Last contact duration, in seconds (numeric)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The other attributes are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`campaign`: Number of contacts performed during this campaign and for this
    client (numeric; this includes the last contact)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pdays`: The number of days that passed by after the client was last contacted
    from a previous campaign (numeric; `-1` means that the client was not previously
    contacted)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`previous`: The number of contacts performed before this campaign and for this
    client (numeric)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`poutcome`: The outcome of the previous marketing campaign (categorical: `unknown`,
    `other`, `failure`, or `success`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output variable (the desired target) is `y` – has the client subscribed
    to a term deposit? (Binary: `yes`, `no`.)'
  prefs: []
  type: TYPE_NORMAL
- en: The objective is to predict the `Class` target variable to indicate whether
    the marketing campaign was successful in terms of whether the client subscribed
    to a term deposit. The dataset is mildly imbalanced with ~12% of clients subscribing
    to a term deposit as a result of the marketing campaign.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use OpenML API to access and read the dataset. As discussed in previous
    chapters, ICP requires a separate calibration set that should not be used to train
    the underlying machine learning classifier. In the following code, we’re creating
    three datasets – *the proper training dataset for the classifier*, *calibration
    datasets to calibrate the classifier using ICP*, and *the test dataset that will
    be used to evaluate the performance*. The dataset contains 45,211 instances; we
    must split it so that it has 1,000 instances for each of the training and calibration
    sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can build the underlying classifier using logistic regression and compute
    the accuracy and ROC AUC on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s train logistic regression using standard scikit-learn functionality:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we must use the trained logistic regression classifier model to predict
    class labels and obtain class scores on the calibration and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, compute the classification accuracy and ROC AUC on the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'So far, we have only used standard classification functionality. Now, let’s
    build ICP using `nonconformist`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must create ICP classifiers by using a wrapper from `nonconformist`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This code snippet constructs an ICP using logistic regression as the underlying
    machine learning model. Here’s a breakdown of what’s happening:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`` `LogisticRegression()` ``: This is a classifier from the scikit-learn library
    in Python that’s used for binary classification tasks. It predicts the class score
    of an instance belonging to a particular class.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`` `ClassifierAdapter(LogisticRegression())` ``: This wraps the logistic regression
    model so that it’s compatible with the nonconformity scorer. The adapter makes
    sure that the underlying classifier’s methods align with the expectations of the
    nonconformity scorer.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`` `ClassifierNc(ClassifierAdapter(LogisticRegression()))` ``: Here, a nonconformity
    scorer is created. Nonconformity scorers, in the context of conformal prediction,
    are used to measure how much an instance deviates from the norm according to the
    training data. In this case, `ClassifierNc` is using `ClassifierAdapter` to create
    a scorer that measures nonconformity based on the logistic regression classifier.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`` `MarginErrFunc()` ``: This is the nonconformity measure we have looked at
    in previous chapters. In `nonconformist`, the margin error is defined as 0.5 −
    ˆ P(y i ∣ x) − max y!=y i  ˆ P(y ∣ x)  _____________ 2 .'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`` ` [PRE24]'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: icp.fit(X_train, y_train)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: icp.calibrate(X_calib, y_calib)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Produce predictions for the test set, with confidence 95%
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: prediction = icp.predict(X_test.values, significance=0.05)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '```'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note that `nonconformist` uses classical conformal prediction and outputs prediction
    sets.
  prefs: []
  type: TYPE_NORMAL
- en: '`nonconformist` is a Python library built on top of scikit-learn that focuses
    on implementing conformal prediction methods for classification tasks. It provides
    a comprehensive set of tools and algorithms to generate prediction intervals,
    estimate uncertainty, and enhance calibration in classification models. Here is
    an overview of the main features and capabilities of the `nonconformist` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '`nonconformist` offers various conformal prediction algorithms specifically
    designed for classification. These algorithms include the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inductive conformal classifier** (**ICC**): This constructs a nonconformity
    measure and a prediction region based on the training set'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transductive conformal classifier** (**TCC**): This incorporates the test
    set into the construction of prediction regions'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Venn predictors**: This generates prediction intervals using nested Venn
    regions to control the number of false positives'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random forest conformal predictor**: This utilizes a random forest model
    as the underlying classifier for conformal prediction'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nonconformist` seamlessly integrates with scikit-learn, allowing users to
    leverage scikit-learn’s extensive collection of classifiers. It provides a wrapper
    class that allows scikit-learn classifiers to be used within the conformal prediction
    framework.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nonconformist` also offers tools to assess the calibration quality, such as
    reliability diagrams.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nonconformist` provides evaluation metrics to assess the performance of conformal
    prediction models. These metrics include accuracy, error rate, p-values, and efficiency
    measures, enabling thorough evaluation and comparison of different models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cross-validation support**: The library offers support for performing cross-validation
    with conformal prediction models. This enables robust evaluation and validation
    of the models across different folds of the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nonconformist` is a powerful tool for applying conformal prediction techniques
    to classification problems. With its extensive range of algorithms, compatibility
    with scikit-learn, calibration and uncertainty estimation capabilities, evaluation
    metrics, and cross-validation support, `nonconformist` provides a comprehensive
    framework for implementing and evaluating conformal prediction models in classification
    tasks. It is a valuable resource for researchers and practitioners looking to
    incorporate conformal prediction into their classification projects.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we embarked on an enlightening exploration of conformal prediction
    specifically tailored to classification tasks. We began by underscoring the significance
    of calibration in the realm of classification, emphasizing its role in ensuring
    the reliability and trustworthiness of model predictions. Through our journey,
    we were introduced to various calibration methods, including the various approaches
    to conformal prediction. We observed how conformal prediction uniquely addresses
    the challenges of calibration, providing both a theoretical and practical edge
    over traditional methods.
  prefs: []
  type: TYPE_NORMAL
- en: We also delved into the nuanced realms of Venn-ABERS predictors, shedding light
    on their roles and implications in the calibration process.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we underscored the invaluable contribution of the open source community
    in this domain. We highlighted tools such as the `nonconformist` library, which
    serve as essential resources for practitioners who are keen on implementing conformal
    prediction in their classification challenges.
  prefs: []
  type: TYPE_NORMAL
- en: As we conclude this chapter, it’s evident that calibration, and more specifically
    conformal prediction, plays a pivotal role in enhancing the robustness and reliability
    of classification models. With the tools and knowledge we have at our disposal,
    we’re well equipped to tackle classification problems with greater confidence
    and precision.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover conformal prediction for regression problems.
  prefs: []
  type: TYPE_NORMAL
