<html><head></head><body><div class="chapter" title="Chapter&#xA0;14.&#xA0;New generation data architectures for Machine learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch14"/>Chapter 14. New generation data architectures for Machine learning</h1></div></div></div><p>This is our last chapter, and we will take a detour from our usual learning topics to cover some of the solution aspects of Machine learning. This is in an attempt to complete a practitioner's view on the implementation aspects of Machine learning solutions, covering more on the choice of platform for different business cases. Let's look beyond Hadoop, NoSQL, and other related solutions. The new paradigm is definitely a unified platform architecture that takes care of all the aspects of Machine learning, starting from data collection and preparation until the visualizations, with focus on all the key architecture drivers such as volume, sources, throughput, latency, extensibility, data quality, reliability, security, self-service, and cost.</p><p>The following flowchart depicts different data architecture paradigms that will be covered in this chapter:</p><div class="mediaobject"><img src="graphics/B03980_14_01.jpg" alt="New generation data architectures for Machine learning"/></div><p>The topics listed here are covered in depth in this chapter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A brief history of how traditional data architectures were implemented and why they are found desirable in the current context of big data and analytics.</li><li class="listitem" style="list-style-type: disc">An overview of the new-age data architecture requirements in the context of Machine learning that includes <a id="id1470" class="indexterm"/><span class="strong"><strong>Extract, Transform, and Load</strong></span> (<span class="strong"><strong>ETL</strong></span>), storage, processing and reporting, distribution, and the presentation of the insights.</li><li class="listitem" style="list-style-type: disc">An introduction to Lambda architectures that unifies strategies for batch and real-time processing with some examples.</li><li class="listitem" style="list-style-type: disc">An introduction to Polyglot Persistence and Polymorphic databases that unify data storage strategies that include structured, unstructured, and semi-structured data stores, and centralize the querying approach across data stores. An example of how the Greenplum database supports the same and how it integrates with Hadoop seamlessly.</li><li class="listitem" style="list-style-type: disc">Semantic Data Architectures include Ontologies Evolution, purpose, use cases, and technologies.</li></ul></div><div class="section" title="Evolution of data architectures"><div class="titlepage"><div><div><h1 class="title"><a id="ch14lvl2sec157"/>Evolution of data architectures</h1></div></div></div><p>We will start with<a id="id1471" class="indexterm"/> understanding how data architectures traditionally have been followed by detailing the demands of modern machine learning or analytics platforms in the context of big data.</p><p>Observation 1—Data stores were always for a purpose</p><p>Traditionally, data architectures had a clear segregation of purpose, <span class="strong"><strong>OLTP</strong></span> (<span class="strong"><strong>Online Transaction Processing</strong></span>), typically <a id="id1472" class="indexterm"/>known to be used for transactional needs, and<a id="id1473" class="indexterm"/> <span class="strong"><strong>OLAP</strong></span> (<span class="strong"><strong>Online Analytic Processing</strong></span>) data stores that typically used for reporting and analytical needs. The following table elaborates the general differences:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom"> </th><th style="text-align: left" valign="bottom">
<p>OLTP databases</p>
</th><th style="text-align: left" valign="bottom">
<p>OLAP databases</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Definition</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>This<a id="id1474" class="indexterm"/> involves many small online transactions (INSERT, UPDATE, and DELETE). The fast query processing is the core requirement; maintaining data integrity, concurrency, and effectiveness is measured by the number of transactions per second. It's usually characterized by a high-level of normalization.</p>
</td><td style="text-align: left" valign="top">
<p>This<a id="id1475" class="indexterm"/> involves a relatively small volume of transactions. Complex Queries involves slicing and dicing of data. The data stored is usually aggregated, historical in nature, and mostly stored in multi-dimensional schemas (usually star schema).</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Data type</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>Operational data</p>
</td><td style="text-align: left" valign="top">
<p>Integrated/consolidated/aggregated data</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Source</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>OLTP databases usually are the actual sources of data</p>
</td><td style="text-align: left" valign="top">
<p>OLAP databases consolidate data from various OLTP databases</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Primary purpose</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>This deals with the execution of day-to-day business processes/tasks</p>
</td><td style="text-align: left" valign="top">
<p>This serves decision-support</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>CUD</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>This is<a id="id1476" class="indexterm"/> short, fast inserts and updates initiated by users</p>
</td><td style="text-align: left" valign="top">
<p>Periodic long-running jobs are refreshing the data</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Queries</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>This usually works on smaller volumes of data and executes simpler queries</p>
</td><td style="text-align: left" valign="top">
<p>This often includes complex queries involving aggregations and slicing and dicing in the multi-dimensional structure</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Throughput</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>This is usually very fast due to relatively smaller data volumes and quicker running queries</p>
</td><td style="text-align: left" valign="top">
<p>This <a id="id1477" class="indexterm"/>usually run in batches and in higher volumes, may take several hours depending on volumes</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Storage Capacity</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>Relatively small as historical data is archived</p>
</td><td style="text-align: left" valign="top">
<p>This requires larger storage space due to the volumes that are involved</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Schema Design</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>Highly normalized with many tables</p>
</td><td style="text-align: left" valign="top">
<p>This is typically de-normalized with fewer tables and the use of star and/or snowflake schemas</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Backup and Recovery</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>This requires proper backup religiously; operational data is critical to run the business. Data loss is likely to entail significant monetary loss and legal liability</p>
</td><td style="text-align: left" valign="top">
<p>Instead of regular backups, some environments may consider simply reloading the OLTP data as a recovery method</p>
</td></tr></tbody></table></div><p>Observation 2—Data architectures were shared disk</p><p>Shared disk data <a id="id1478" class="indexterm"/>architecture refers to an architecture where there is a data disk that holds all the data, and each node in the cluster has access to this data for processing. All the data operations can be performed by any node at a given point in time, and in case two nodes attempt at persisting/writing a tuple at the same time, to ensure consistency, a disk-based lock or intended lock communication is passed on, thus affecting the performance. Further with an increase in the number of nodes, contention at the database level increases. These architectures are <span class="emphasis"><em>write</em></span> limited as there is a need to handle the locks across the nodes in the cluster.</p><p>Even in the case of the reads, partitioning should be implemented effectively to avoid complete table scans. All the traditional RDBMS databases are the shared disk data architectures.</p><div class="mediaobject"><img src="graphics/B03980_14_02.jpg" alt="Evolution of data architectures"/></div><p>Observation 3—Traditional ETL architecture<a id="id1479" class="indexterm"/> had limitations. The following <a id="id1480" class="indexterm"/>list provides the details of these limitations:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Onboarding and integrating data were slow and expensive. Most of the ETL logic that exists today is custom coded and is tightly coupled with the database. This tight coupling also resulted in a problem where the existing logic code cannot be reused. Analytics and reporting requirements needed a different set of tuning techniques to be applied. Optimization for analytics was time-consuming and costly.</li><li class="listitem" style="list-style-type: disc">Data provenance was often poorly recorded. The data meaning was <span class="emphasis"><em>lost in translation</em></span>. Post-onboarding, maintenance and analysis cost for the on-boarded data was usually very high. Recreating data lineage was manual, time-consuming, and error-prone. There was no strong auditing or record of the data transformations and were generally tracked in spreadsheets.</li><li class="listitem" style="list-style-type: disc">The target data was difficult to consume. The optimization favors known analytics but was not well-suited to the new requirements. A one-size-fits-all canonical view was used rather than fit-for-purpose views, or lacks a conceptual model to consume easily the target data. It has been difficult to identify what data was available, how to get access, and how to integrate the data to answer a question.</li></ul></div><p>Observation 4—Data was<a id="id1481" class="indexterm"/> usually only structured</p><p>Most of the time, the database was designed to fit the RDBMS models. If the incoming data was not actually structured, the ETLs would build a structure around for being stored in a standard OLTP or OLAP store.</p><p>Observation 5—Performance and scalability</p><p>The optimization of a<a id="id1482" class="indexterm"/> data store or a query was possible to an extent, given the infrastructure, and beyond a certain point, there was a need for a redesign.</p></div></div>
<div class="section" title="Emerging perspectives &amp; drivers for new age data architectures"><div class="titlepage"><div><div><h1 class="title"><a id="ch14lvl2sec158"/>Emerging perspectives &amp; drivers for new age data architectures</h1></div></div></div><p>Driver 1—<span class="emphasis"><em>BIG</em></span> data intervention.</p><p>We have defined big<a id="id1483" class="indexterm"/> data and large dataset <a id="id1484" class="indexterm"/>concepts in <a class="link" href="ch02.html" title="Chapter 2. Machine learning and Large-scale datasets">Chapter 2</a>, <span class="emphasis"><em>Machine learning and Large-scale datasets</em></span>. The data that is now being ingested and needs to be processed typically has the following characteristics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Source</strong></span>: Depending upon the nature of the information, the source may be a real-time stream of data (for example, trade transactions), or batches of data containing updates since the last sync</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Content</strong></span>: The data may represent different types of information. Often, this information is related to other pieces of data and is needed to be connected<p>The following screenshot shows the types of data and different sources that need to be supported:</p><div class="mediaobject"><img src="graphics/B03980_14_03.jpg" alt="Emerging perspectives &amp; drivers for new age data architectures"/></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Volume</strong></span>: Depending upon the nature of the data, the volumes that are being processed may vary. For example, master data or the securities definition data are relatively fixed, whereas the transaction data is enormous compared to the other two.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Lifecycle</strong></span>: Master data has a fixed life and is rarely updated (such as, slowly changing dimensions). However, the transactional data has a very short life but needs to be available for analysis, audit, and so on for longer periods.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Structure</strong></span>: While most of the data is structured, there is an advent of unstructured data in the financial industry. It is becoming increasingly critical for financial systems to incorporate unstructured data as part of their IT architectures.</li></ul></div><p>The next chart <a id="id1485" class="indexterm"/>depicts the complexity, velocity <a id="id1486" class="indexterm"/>volume, and various aspects of each data source:</p><div class="mediaobject"><img src="graphics/B03980_14_04.jpg" alt="Emerging perspectives &amp; drivers for new age data architectures"/><div class="caption"><p>Source: SoftServe</p></div></div><p>Driver 2—Data platform requirements are advanced</p><p>The landscape of the new age-data platform requirements is drastically expanding, and the unified platforms are the happening ones. The next concept map explains it all. The core elements of data architectures include ETL (Extract, Transform, and Load), Storage, Reporting, Analytics, Visualization, and data distribution.</p><div class="mediaobject"><img src="graphics/B03980_14_05.jpg" alt="Emerging perspectives &amp; drivers for new age data architectures"/></div><p>Driver 3—Machine<a id="id1487" class="indexterm"/> learning and analytics <a id="id1488" class="indexterm"/>platforms now have a new purpose and definition</p><p>The evolution of analytics and it repurposing itself is depicted in the following diagram:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Historically, the focus was merely on reporting. Aggregated or preprocessed data is loaded into the warehouse to understand what has happened. This is termed as <span class="strong"><strong>Descriptive analytics</strong></span> <a id="id1489" class="indexterm"/>and was primarily a backward step.</li><li class="listitem" style="list-style-type: disc">With the advent of ad-hoc data inclusion, the need was to understand why certain behavior happened. This is called <span class="strong"><strong>Diagnostic analytics</strong></span><a id="id1490" class="indexterm"/> and is focused on understanding the root cause of the behavior, which is again based on the historical data.</li><li class="listitem" style="list-style-type: disc">Now, the demand has shifted, and the need is to understand what will happen. This is called <a id="id1491" class="indexterm"/><span class="strong"><strong>Predictive analytics,</strong></span> and the focus is to predict the events based on the historical behavior.</li><li class="listitem" style="list-style-type: disc">With the advent of real-time data, the focus is now on do we make it happen? This goes beyond predictive analytics where remediation is a part of something. The ultimate focus is to <span class="emphasis"><em>Make it happen!</em></span> with the advent of real-time event access. The following diagram depicts the evolution of analytics w.r.t. the value and related complexity:</li></ul></div><div class="mediaobject"><img src="graphics/B03980_14_06.jpg" alt="Emerging perspectives &amp; drivers for new age data architectures"/></div><p>The next table <a id="id1492" class="indexterm"/>differentiates the <a id="id1493" class="indexterm"/>traditional analytics (BI) and the new age analytics:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Area</p>
</th><th style="text-align: left" valign="bottom">
<p>Traditional analytics (BI)</p>
</th><th style="text-align: left" valign="bottom">
<p>New age analytics</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Scope</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>Descriptive Analytics</p>
<p>Diagnostic Analytics</p>
</td><td style="text-align: left" valign="top">
<p>Predictive Analytics</p>
<p>Data science</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Data</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>Limited/Controlled volumes</p>
<p>Preprocessed/Validated</p>
<p>Basic models</p>
</td><td style="text-align: left" valign="top">
<p>Large volumes</p>
<p>Diverse formats and heavy on variety</p>
<p>Raw data that are not pre-processed</p>
<p>The growing model complexity</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Result</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>Here, the focus is on retrospection and the root-cause analysis</p>
</td><td style="text-align: left" valign="top">
<p>Here, the focus is on prediction/insights and the accuracy of analysis</p>
</td></tr></tbody></table></div><p>Driver 4—It is<a id="id1494" class="indexterm"/> not all about historical and<a id="id1495" class="indexterm"/> batch, it is real-time and instantaneous insights</p><p>The data coming in lower volumes and higher velocity is what defines <span class="emphasis"><em>real-time</em></span>. The new age analytics systems are expected to handle real-time, batch, and near real-time processing requests (these are scheduled and known as micro batches). The following graph depicts the properties of real-time and batch data characteristics with respect to volume, velocity, and variety being constant.</p><div class="mediaobject"><img src="graphics/B03980_14_07.jpg" alt="Emerging perspectives &amp; drivers for new age data architectures"/></div><p>Driver 5—Traditional ETL is unable to cope with <span class="emphasis"><em>BIG</em></span> data</p><p>The goal is to be able to lay out an ETL architecture strategy that can address the following problematic areas:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Facilitates <a id="id1496" class="indexterm"/>standardization<a id="id1497" class="indexterm"/> in implementation—dealing with the need for one standard</li><li class="listitem" style="list-style-type: disc">Supports building reusable components</li><li class="listitem" style="list-style-type: disc">Building agnostic functions</li><li class="listitem" style="list-style-type: disc">Improving performance and scalability using parallel processing techniques</li><li class="listitem" style="list-style-type: disc">Reducing the <a id="id1498" class="indexterm"/>total overall<span class="strong"><strong> cost of ownership</strong></span> (<span class="strong"><strong>TCO</strong></span>)</li><li class="listitem" style="list-style-type: disc">Building a specialized skill pool</li></ul></div><p>The following table provides a comparative analysis of the key data loading pattern:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom"> </th><th style="text-align: left" valign="bottom">
<p>ETL</p>
<p>Extract, Transform, and Load</p>
</th><th style="text-align: left" valign="bottom">
<p>ELT</p>
<p>Extract, Load, and Transform</p>
</th><th style="text-align: left" valign="bottom">
<p>ETLT</p>
<p>Extract, Transform, Load, and Transform</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Overview</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>This is a<a id="id1499" class="indexterm"/> traditional technique for moving and transforming data in which an ETL engine is either separated from the source or the target DBMS performs the data transformations.</p>
</td><td style="text-align: left" valign="top">
<p>This is <a id="id1500" class="indexterm"/>a technique for moving and transforming data from one location and formatting it to another instance and format. In this style of integration, the target DBMS becomes the transformation engine.</p>
</td><td style="text-align: left" valign="top">
<p>In this <a id="id1501" class="indexterm"/>technique, transformations are partly done by the ETL engine and partly pushed to the destination DBMS.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Highlights</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>A<a id="id1502" class="indexterm"/> heavy work of transformation is done in the ETL engine.</p>
<p>It uses the integrated transformation functions.</p>
<p>Transformation logic can be configured through the GUI.</p>
<p>This is supported by Informatica.</p>
</td><td style="text-align: left" valign="top">
<p>A <a id="id1503" class="indexterm"/>heavy work of transformation is handed over to the DBMS layer.</p>
<p>Transformation logic runs closer to the data.</p>
<p>It is supported by Informatica.</p>
</td><td style="text-align: left" valign="top">
<p>Transformation <a id="id1504" class="indexterm"/>work is split between the ETL engine and the DBMS.</p>
<p>It is supported by Informatica.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Benefits</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>This is<a id="id1505" class="indexterm"/> an easy, GUI-based configuration.</p>
<p>The transformation logic is independent, outside the database, and is reusable</p>
<p>This works very well for granular, simple, function-oriented transformations that do not require any database calls.</p>
<p>Can run on SMP or MPP hardware.</p>
</td><td style="text-align: left" valign="top">
<p>This<a id="id1506" class="indexterm"/> leverages the RDBMS engine hardware for scalability.</p>
<p>It always keeps all the data in RDBMS.</p>
<p>It is parallelized according to the dataset, and the disk I/O is usually optimized at the engine level for faster throughput.</p>
<p>It scales as long as the hardware and the RDBMS engine can continue to scale.</p>
<p>Can achieve 3x to 4x the throughput rates on the appropriately tuned MPP RDBMS platform.</p>
</td><td style="text-align: left" valign="top">
<p>It can <a id="id1507" class="indexterm"/>balance the workload or share the workload with the RDBMS.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Risks</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>This <a id="id1508" class="indexterm"/>requires a higher processing power on the ETL side.</p>
<p>The costs are higher.</p>
<p>It consists of the complex transformations that would need reference data, which will slow down the process.</p>
</td><td style="text-align: left" valign="top">
<p>Transformation<a id="id1509" class="indexterm"/> logic is tied to a database.</p>
<p>The transformations that involve smaller volume and simple in nature will not gain many benefits.</p>
</td><td style="text-align: left" valign="top">
<p>This <a id="id1510" class="indexterm"/>will still have a part of the transformation logic within the database.</p>
</td></tr></tbody></table></div><p>Fact 6—No "one" data <a id="id1511" class="indexterm"/>model fits advanced or complex<a id="id1512" class="indexterm"/> data processing requirements; there is a need for multi-data model platforms</p><p>Different databases are designed to solve different problems. Using a single database engine for all of the requirements usually leads to non-performant solutions. RDBMSs are known to work well-transactional operations, OLAP databases for reporting, NoSQL for high-volume data processing, and storage. Some solutions unify these storages and provide an abstraction for querying across these stores.</p></div>
<div class="section" title="Modern data architectures for Machine learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch14lvl1sec63"/>Modern data architectures for Machine learning</h1></div></div></div><p>From this section onwards, we will <a id="id1513" class="indexterm"/>cover some of the emergent data architectures, challenges that gave rise to architectures of this implementation architecture, some relevant technology stacks, and use cases where these architectures apply (as relevant) in detail.</p><div class="section" title="Semantic data architecture"><div class="titlepage"><div><div><h2 class="title"><a id="ch14lvl2sec159"/>Semantic data architecture</h2></div></div></div><p>Some of the facts covered in the<a id="id1514" class="indexterm"/> emerging perspectives in the previous<a id="id1515" class="indexterm"/> section give rise to the following core architecture drivers to build semantic data model driven data lakes that seamlessly integrate a larger data scope, which is analytics ready. The future of analytics is semantified. The goal here is to create a large-scale, flexible, standards-driven ETL architecture framework that models with the help of tools and other architecture assets to enable the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Enabling a common data architecture that can be a standard architecture.</li><li class="listitem" style="list-style-type: disc">Dovetailing into the Ontology-driven data architecture and data lakes of the future (it is important to tie this architecture strategy with the data aggregation reference architecture). This will ensure there is a single data strategy that takes care of the data quality and data integration.</li><li class="listitem" style="list-style-type: disc">Enabling product groups to integrate rapidly into the data architecture and deliver into and draw from the common data repository.</li><li class="listitem" style="list-style-type: disc">Enabling ad-hoc analytics on need basis.</li><li class="listitem" style="list-style-type: disc">Reducing time needed to implement the new data aggregation, ingestion, and transformation.</li><li class="listitem" style="list-style-type: disc">Enabling <span class="emphasis"><em>any format to any format</em></span> model (a format-agnostic approach that involves data normalization sometimes).</li><li class="listitem" style="list-style-type: disc">Complying with emerging semantic standards. This will bring in the flexibility.</li><li class="listitem" style="list-style-type: disc">Enabling the common IT management and reduction of the TCO.</li><li class="listitem" style="list-style-type: disc">Enabling a consolidated cloud (that can be a proprietary) for the Broadridge Master Business Data Repository.</li><li class="listitem" style="list-style-type: disc">Enabling all the applications and products to "talk to a common language" and building the Broadridge data format.</li><li class="listitem" style="list-style-type: disc">Reduce, and in some cases eliminate, the proliferation of too many licenses, databases, implementations, stacks, and more.</li><li class="listitem" style="list-style-type: disc">Data Semantification: It is important to analyze the underlying schemas in order to unlock the meaning from them. The semantification process is always iterative and evolves over the period of time. The metadata definitions in this context will be elaborated or expanded in this process.<div class="mediaobject"><img src="graphics/B03980_14_08.jpg" alt="Semantic data architecture"/></div></li></ul></div><p>Setting up an <a id="id1516" class="indexterm"/>enterprise-wide <a id="id1517" class="indexterm"/>aggregate data mart is not the solution to problems outlined previously. Even if such a data mart was set up, keeping it updated and in line with the rest of the projects would be a major problem. As stated earlier, the need is to lay out the common reference architecture of a system that can accumulate data from many sources without making any assumptions on how, where, or when this data will be used.</p><p>There are two different advances in the field that we leverage to address the issues at an architecture level. These are the evolution of a data lake as an architecture pattern, and the emergence of Semantic Web and its growing relevance in e-business.</p><div class="section" title="The business data lake"><div class="titlepage"><div><div><h3 class="title"><a id="ch14lvl3sec128"/>The business data lake</h3></div></div></div><p>The enterprise <a id="id1518" class="indexterm"/>data lake gives the concept of an enterprise data warehouse a whole new dimension. While the approach with a data warehouse has always been to design a single schema and aggregate the minimum information needed to fulfill the schema, data lake turns both these premises of traditional data warehouse architectures on its head. The traditional data warehouse is designed for a specific purpose in mind (for example, analytics, reporting, and operational insights). The schema is designed accordingly, and the minimum information needed for the purpose is aggregated. This means that using this warehouse for any other objective is only incidental if at all, but it is not designed for such a use.</p><p>The business data lake promotes the concept of an appropriate schema—the warehouse is not constrained by a fixed, predetermined schema. This allows the data lake to assimilate information as and when it becomes available in the organization. The important direct implication of this is that rather than assimilating the minimum information—the data lake can assimilate all the information produced in the organization. Since there are no assumptions made about what the data is, options remain open to use the information for any purpose in the future. This enables the data lake to power business agility by being able to serve newer ideas with the data that is already available in the data lake.</p><p>The business<a id="id1519" class="indexterm"/> data lake addresses the following concerns:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">How to handle unstructured data?</li><li class="listitem" style="list-style-type: disc">How to link internal and external data?</li><li class="listitem" style="list-style-type: disc">How to adapt to the speed of business change?</li><li class="listitem" style="list-style-type: disc">How to remove the repetitive ETL cycle?</li><li class="listitem" style="list-style-type: disc">How to support different levels of data quality and governance based on differing business demands?</li><li class="listitem" style="list-style-type: disc">How to let local business units take the initiative?</li><li class="listitem" style="list-style-type: disc">How to ensure the deliverance of platform and that it will it be adopted?</li></ul></div></div><div class="section" title="Semantic Web technologies"><div class="titlepage"><div><div><h3 class="title"><a id="ch14lvl3sec129"/>Semantic Web technologies</h3></div></div></div><p>When using external data that are <a id="id1520" class="indexterm"/>most often found on the web, the most important requirement is understanding the precise semantics of the data. Without this, the results cannot be trusted. Here, Semantic Web technologies come to the rescue, as they allow semantics ranging from very simple to very complex to be specified for any available resource. Semantic Web technologies do not only support capturing the passive semantics, but also support active inference and reasoning on the data.</p><p>Semantic Web technologies allow data to be annotated with additional metadata (as RDF). One of the most fundamental capabilities that this adds is the <a id="id1521" class="indexterm"/>
<span class="strong"><strong>AAA principle</strong></span> of Semantic Computing is—<span class="emphasis"><em>Anyone can add anything about anything at any time</em></span>. As the information is made up of metadata, adding more metadata can enrich the information at any time.</p><p>Querying RDF data is done using SPARQL, which allows navigating complex relationship graphs to extract meaningful information from the data store. Reasoner (or an inference engine) works with the RDF metadata to deliver inferences at the top of the data. This allows the system to extract newer insights, which were originally not available in the incoming data.</p><p>Today, enormous amounts of information are becoming available over the web and over corporate and regulatory networks. However, access to all the available information remains limited as long as the information is stored separately without easy means to combine them from different sources.</p><p>This exacerbates the need for suitable methods to combine data from various sources. This is termed as the <span class="emphasis"><em>cooperation of information systems</em></span>. This is defined as the ability to share, combine, and exchange information between heterogeneous sources in a transparent way to the end users. These heterogeneous sources are usually known to have always handled data in silos, and thus, they are inaccessible. To achieve data interoperability, the issues posed by data heterogeneity needs to be eliminated. Data sources can be<a id="id1522" class="indexterm"/> heterogeneous in the following ways:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Syntax</strong></span>: Syntactic heterogeneity is caused by the use of different models or languages</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Schema</strong></span>: Schematic heterogeneity results from structural differences</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Semantics</strong></span>: Semantic heterogeneity is caused by different meanings or interpretations of data in various contexts<div class="mediaobject"><img src="graphics/B03980_14_09.jpg" alt="Semantic Web technologies"/></div></li></ul></div><p>Data integration provides the ability to manipulate data transparently across multiple data sources. Based on the architecture, there are two systems:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Central Data Integration</strong></span>: A <a id="id1523" class="indexterm"/>central data integration system usually has a global schema, which provides the user with a uniform interface to access information stored in the data sources.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Peer-to-peer</strong></span>: In <a id="id1524" class="indexterm"/>contrast, in a peer-to-peer data integration system, there are no general points of control on the data sources (or peers). Instead, any peer can accept user queries for the information distributed in the entire system.</li></ul></div><p>The cooperation<a id="id1525" class="indexterm"/> of information systems is the ability to share, combine, and/or exchange information from multiple information sources, and the ability to access the integrated information by its final receivers transparently. The major problems that hinder the cooperation of information systems are the autonomy, the distribution, the heterogeneity, and the instability of information sources. In particular, we are interested in the heterogeneity problem that can be identified at several levels: the system, the syntactic, the structural, and the semantic heterogeneity. The cooperation of information systems has been extensively studied, and several approaches have been proposed to bridge the gap between heterogeneous information systems, such as: database translation, standardization, federation, mediation, and web services. These approaches provide appropriate solutions to the heterogeneity problem at syntactic and basic levels.</p><p>However, in order to achieve semantic interoperability between heterogeneous information systems, the meaning of the information that is interchanged has to be understood in the systems. Semantic conflicts occur whenever two contexts do not use the same interpretation of the information.</p><p>Therefore, in order to deal with semantic heterogeneity, there is a need for more semantic-specialized approaches, such as ontologies. In this chapter, our focus is to demonstrate how information systems can cooperate using semantics. In the next section, let us look at the constitution of semantic data architecture.</p><div class="section" title="Ontology and data integration"><div class="titlepage"><div><div><h4 class="title"><a id="ch14lvl4sec55"/>Ontology and data integration</h4></div></div></div><p>The diagram here<a id="id1526" class="indexterm"/> represents the reference architecture for Semantic data architecture-based analytics:</p><div class="mediaobject"><img src="graphics/B03980_14_10.jpg" alt="Ontology and data integration"/></div><p>The key features of<a id="id1527" class="indexterm"/> semantic data architecture are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Metadata representation</strong></span>: Each of the sources can be represented as local ontologies supported by a meta-data dictionary to interpret the nomenclature.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Global conceptualization</strong></span>: There will be a global ontology definition that maps the local ontologies and provides a single view or nomenclature for a common view.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Generic querying</strong></span>: There will be a support for querying at a local of a global ontology levels depending on the need and purpose of the consumer / client.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Materialised view</strong></span>: A high level querying strategy that masks querying between nomenclatures and peer sources.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Mapping</strong></span>: There will be support for defining the thesaurus based mapping between ontology attributes and values.</li></ul></div></div></div><div class="section" title="Vendors"><div class="titlepage"><div><div><h3 class="title"><a id="ch14lvl3sec130"/>Vendors</h3></div></div></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Type</p>
</th><th style="text-align: left" valign="bottom">
<p>Product/framework </p>
</th><th style="text-align: left" valign="bottom">
<p>Vendor </p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Open source and commercial versions</p>
</td><td style="text-align: left" valign="top">
<p>MarkLogic 8<a id="id1528" class="indexterm"/> is the NoSQL graph <a id="id1529" class="indexterm"/>store that supports storing and processing RDF data formats and can serve as a triple store.</p>
</td><td style="text-align: left" valign="top">
<p>MarkLogic</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Open source and commercial versions</p>
</td><td style="text-align: left" valign="top">
<p>Stardog<a id="id1530" class="indexterm"/> is the easiest and the most powerful graph database: search, query, reasoning, and constraints in a lightweight, pure Java system.</p>
</td><td style="text-align: left" valign="top">
<p>Stardog</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Open source</p>
</td><td style="text-align: left" valign="top">
<p>4Store<a id="id1531" class="indexterm"/> is an efficient, scalable, and stable RDF database.</p>
</td><td style="text-align: left" valign="top">
<p>Garlik Ltd.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Open source</p>
</td><td style="text-align: left" valign="top">
<p>Jena<a id="id1532" class="indexterm"/> is a free and open source Java framework for building Semantic Web and Linked Data applications.</p>
</td><td style="text-align: left" valign="top">
<p>Apache</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Open source</p>
</td><td style="text-align: left" valign="top">
<p>Sesame<a id="id1533" class="indexterm"/> is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, storing, inferencing, and querying of such data. It offers an easy-to-use API that can be connected to all the leading RDF storage solutions.</p>
</td><td style="text-align: left" valign="top">
<p>GPL v2</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Open Source</p>
</td><td style="text-align: left" valign="top">
<p>Blazegraph<a id="id1534" class="indexterm"/> is SYSTAP's flagship graph database. It is specifically designed to support big graphs offering both Semantic Web (RDF/SPARQL) and graph database (TinkerPop, blueprints, and vertex-centric) APIs.</p>
</td><td style="text-align: left" valign="top">
<p>GPL v2</p>
</td></tr></tbody></table></div></div></div><div class="section" title="Multi-model database architecture / polyglot persistence"><div class="titlepage"><div><div><h2 class="title"><a id="ch14lvl2sec160"/>Multi-model database architecture / polyglot persistence</h2></div></div></div><p>We could never<a id="id1535" class="indexterm"/> have imagined, even five years ago, that relational databases would <a id="id1536" class="indexterm"/>become only a single kind of a database technology and not the database technology. Internet-scale data processing changed the way we process data.</p><p>The new generation architectures such as Facebook, Wikipedia, SalesForce, and so on, are found in principles and paradigms, which are radically different from the well-established theoretical foundations on which the current data management technologies are developed.</p><p>The major architectural <a id="id1537" class="indexterm"/>challenges of these architectures can be characterized next:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Commoditizing information:<p>Apple App Store, SaaS, Ubiquitous Computing, Mobility, and the Cloud-Based Multi-Tenant architectures have unleashed, in business terms, an ability to commoditize information delivery. This model changes almost all the architecture decision making, as we now need to think in terms of what the "units of information" that can be offered and billed as services are, instead of thinking in terms of the TCO of the solution.</p></li><li class="listitem" style="list-style-type: disc">Theoretical limitations of RDBMS:<p>What Michael Stonebraker, an influential Database theorist, has been writing in recent times at the heart of the Internet Scale Architectures is a new theoretical model of data processing and management. The theories of database management are now more than three decades old, and when they were designed, they were designed for the mainframe-type computing environments and very unreliable electronic components. Nature and the capabilities of the systems and applications have since evolved significantly. With reliability becoming a quality attribute of the underlying environment, systems are composed of parallel processing cores, and the nature of data creation and usage has undergone tremendous change. In order to conceptualize solutions for these new environments, we need to approach the designing solution architectures from a computing perspective, not only from an engineering perspective.</p></li></ul></div><p>Six major forces are driving <a id="id1538" class="indexterm"/>the data revolution today. They are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Massive Parallel Processing</li><li class="listitem" style="list-style-type: disc">Commoditized Information Delivery</li><li class="listitem" style="list-style-type: disc">Ubiquitous Computing and Mobile Devices</li><li class="listitem" style="list-style-type: disc">Non-RDBMS and Semantic Databases</li><li class="listitem" style="list-style-type: disc">Community Computing</li><li class="listitem" style="list-style-type: disc">Cloud Computing</li></ul></div><p>Hadoop<a id="id1539" class="indexterm"/> and MapReduce<a id="id1540" class="indexterm"/> have unleashed massive parallel processing of data on a substantial basis and have made complex computing algorithms in a programmatic<a id="id1541" class="indexterm"/> platform. This has changed analytics and BI forever. Similarly, web services and API-driven architectures have made information delivery commoditized on a substantial basis. Today, it is possible to build extremely large systems in such a way that each subsystem or component is a complete platform in itself, hosted and managed by a different entity altogether.</p><p>The previous innovations have changed the traditional Data Architecture completely. Especially, semantic computing and the ontology-driven modeling of information have turned data design on its head.</p><p>Philosophically, the data architecture is going through a factual underpinning. In traditional data models, we first design the <span class="emphasis"><em>data model</em></span>—a fixed, design-time understanding of the world and its future. A data model fixes the meaning of data forever into a fixed structure.</p><p>A table is nothing but a category, a set of something. As a result, data has meaning only if we understand the set/category to which it belongs. For example, if we design an automobile processing system into some categories such as four-wheelers, two-wheelers, commercial vehicles, and so on, this division itself has some significant meaning embedded into it. The data that is stored in each of these categories does not reveal the <span class="emphasis"><em>purpose of the design</em></span> that is embedded in the way the categories are designed. For example, another system might view the world of automobiles in terms of its drivetrain—electric, petroleum powered, nuclear powered, and more. This categorization itself reveals the purpose of the system in some manner, which is impossible to obtain from the attributes of any single record.</p><p>The term <span class="emphasis"><em>polyglot</em></span> <a id="id1542" class="indexterm"/>is typically used to define a person who can speak many languages. In the context of big data, this term refers to a set of applications that use many database technologies where each database technology solves a particular problem. The basic premise of this data architecture is that different database technologies solve various problems and since complex applications have many problems, picking one option to solve a particular issue is better than trying to solve all the problems using one option. When we talk about a data system, it is defined as a system that takes care of storage and querying of data, which has a runtime of several years and needs to address every possible hardware and maintenance complexities.</p><p>Polyglot persistence data<a id="id1543" class="indexterm"/> architecture is used when there is a complex problem, broken down into smaller problems and solved by applying different database models. This is followed by aggregating the results into a hybrid data storage platform followed by analysis. Some factors influencing the choice of database are as follows:</p><p>Factor 1—Data Models:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">What type of data sources do we want to integrate?</li><li class="listitem" style="list-style-type: disc">How would we want to manipulate/analyze the data?</li><li class="listitem" style="list-style-type: disc">What is the volume, variety, and velocity of data?</li><li class="listitem" style="list-style-type: disc">Examples—Relational, Key-Value, Column-Oriented, Document-Oriented, Graph, and so on.</li></ul></div><p>Factor 2—Consistency, availability, and partitioning (CAP):</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Consistency</strong></span>: Only one value of an object to each client (Atomicity)</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Availability</strong></span>: All objects are always available (Low Latency)</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Partition tolerance</strong></span>: Data is split into multiple network partitions (Clustering)</li></ul></div><p>CAP theorem requires us to choose any of the two features depicted here:</p><div class="mediaobject"><img src="graphics/B03980_14_11.jpg" alt="Multi-model database architecture / polyglot persistence"/></div><p>The following diagram is<a id="id1544" class="indexterm"/> an example of a system that has multiple applications with a data model built for its purpose:</p><div class="mediaobject"><img src="graphics/B03980_14_12.jpg" alt="Multi-model database architecture / polyglot persistence"/><div class="caption"><p>Source: ThoughtWorks</p></div></div><p>Some important aspects<a id="id1545" class="indexterm"/> that affect this solution are listed next:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">It is important that the proposed hybrid environment be clearly understood to ensure that it facilitates taking the right decision about data integration, analytics, data visibility, and others, and thus, how the solution fits into the entire big data and analytics implementation umbrella.</li><li class="listitem" style="list-style-type: disc">Since there is more than one data model, there will be a need for a unified platform that can interface with all the databases identified for solution and aggregation. This platform should address some bare minimum big data platform expectations like; fault tolerance high-availability, transactional integrity, data agility and reliability, scalability and performance are addressed.</li><li class="listitem" style="list-style-type: disc">Depending on the specific requirements, it is important for us to know/understand what sort of a data model works both: for the particular problem and the overall solution.</li><li class="listitem" style="list-style-type: disc">Data ingestion strategies address the real-time and batch data updates and how they can be made to work in the context of the multi-model database. Since there will be a variety of data stores, what will the <span class="strong"><strong>System of Record</strong></span> (<span class="strong"><strong>SOR</strong></span>) be? And how do we ensure that data across all the data sources is in sync or up-to-date?</li></ul></div><p>So overall, this is probably a big data challenge at its best. Multiple sources of data with very different structures need to be collected, integrated, and analyzed to solve a particular business problem. Then, the key is to identify whether the data needs to be pushed to the client on-demand or in real-time. And obviously, this type of problem cannot be solved easily or cost-effectively with one type of database technology. There could be some cases where a straightforward RDBMS could work, but in cases where there is non-relational data, there is a need for different persistence engines such as NoSQL. Similarly, for an e-commerce business problem, it is important that we have a highly available and a scalable data store for shopping cart functionality. However, to find products bought by a particular group, the same store cannot help. The need here is to go for a hybrid approach and have multiple data stores used in conjunction that is known as polyglot persistence.</p><div class="section" title="Vendors"><div class="titlepage"><div><div><h3 class="title"><a id="ch14lvl3sec131"/>Vendors</h3></div></div></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Type</p>
</th><th style="text-align: left" valign="bottom">
<p>Product/framework </p>
</th><th style="text-align: left" valign="bottom">
<p>Vendor </p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Commercial</p>
</td><td style="text-align: left" valign="top">
<p>FoundationDB is <a id="id1546" class="indexterm"/>a rock-solid<a id="id1547" class="indexterm"/> database that gives NoSQL (Key-Value store) and SQL access.</p>
</td><td style="text-align: left" valign="top">
<p>FoundationDB</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Open source</p>
</td><td style="text-align: left" valign="top">
<p>ArangoDB<a id="id1548" class="indexterm"/> is an open source NoSQL solution with a flexible data model for documents, graphs, and key-values.</p>
</td><td style="text-align: left" valign="top">
<p>GPL v2</p>
</td></tr></tbody></table></div></div></div><div class="section" title="Lambda Architecture (LA)"><div class="titlepage"><div><div><h2 class="title"><a id="ch14lvl2sec161"/>Lambda Architecture (LA)</h2></div></div></div><p>Lambda Architecture addresses one<a id="id1549" class="indexterm"/> important aspect of Machine learning; that is, providing a unified platform for real-time and batch analytics. Most of the frameworks that we have seen until now support batch architecture (for example, Hadoop), in order to support real-time processing integration with specific frameworks (for example, Storm).</p><p>Nathan Marz introduced the concept of Lambda Architecture for a generic, scalable, and fault-tolerant data processing architecture that addresses a real-time stream-based processing and batch processing as a unified offering.</p><p>Lambda Architecture facilitates a data architecture that is highly fault-tolerant, both: against hardware failures and human mistakes. At the same time, it serves a broad range of uses and workloads, where low-latency reads and updates are required. The resulting system should be linearly scalable, and it should scale out rather than up.</p><p>Here's how it looks from a high-level perspective:</p><div class="mediaobject"><img src="graphics/B03980_14_13.jpg" alt="Lambda Architecture (LA)"/></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Data Layer</strong></span>: All of<a id="id1550" class="indexterm"/> the data entering the system is dispatched to both the batch layer and the speed layer for processing.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Batch layer</strong></span>: This<a id="id1551" class="indexterm"/> manages master data and is responsible for batch pre-computation. It handles heavy volumes.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Speed layer</strong></span>: Speed layer <a id="id1552" class="indexterm"/>is responsible for handling recent data and compensates for the high latency of updates to the serving layer. On an average, this layer does not deal with large volumes.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Serving layer</strong></span>: Serving layer<a id="id1553" class="indexterm"/> handles indexing the batch views and facilitates ad hoc querying demonstrating low-latency.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Query function</strong></span>: This combines <a id="id1554" class="indexterm"/>the results from batch views and real-time views.</li></ul></div><div class="section" title="Vendors"><div class="titlepage"><div><div><h3 class="title"><a id="ch14lvl3sec132"/>Vendors</h3></div></div></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Type</p>
</th><th style="text-align: left" valign="bottom">
<p>Product/framework </p>
</th><th style="text-align: left" valign="bottom">
<p>Vendor </p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Open<a id="id1555" class="indexterm"/> source and commercial</p>
</td><td style="text-align: left" valign="top">
<p>Spring XD<a id="id1556" class="indexterm"/> is a unified platform for a fragmented Hadoop ecosystem. It's built at the top of the battle-tested open source projects, and dramatically simplifies the orchestration of big data workloads and data pipelines.</p>
</td><td style="text-align: left" valign="top">
<p>Pivotal (Spring Source)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Open source</p>
</td><td style="text-align: left" valign="top">
<p>Apache Spark <a id="id1557" class="indexterm"/>is a fast and conventional engine for big data processing with built-in modules for streaming, SQL, Machine learning, and graph processing.</p>
</td><td style="text-align: left" valign="top">
<p>Apache</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Open<a id="id1558" class="indexterm"/> source </p>
</td><td style="text-align: left" valign="top">
<p>Oryx<a id="id1559" class="indexterm"/> is a simple, real-time, and large-scale Machine learning infrastructure.</p>
</td><td style="text-align: left" valign="top">
<p>Apache (Cloudera)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Open source</p>
</td><td style="text-align: left" valign="top">
<p>The storm is a system used to process streaming data in the real time.</p>
</td><td style="text-align: left" valign="top">
<p>Apache (Hortonworks)</p>
</td></tr></tbody></table></div></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch14lvl1sec64"/>Summary</h1></div></div></div><p>In this concluding chapter, our focus has been on the implementation aspects of Machine learning. We have understood what traditional analytics platforms have been and how they cannot fit the modern data requirements. You have also learned the architecture drivers that are promoting the new data architecture paradigms such as Lamda Architectures and polyglot persistence (multi-model database architecture), and how Semantic architectures help seamless data integration. With this chapter, you can assume that you are ready for implementing a Machine learning solution for any domain with an ability to not only identify what algorithms or models are to be applied to solve a learning problem, but also what platform solutions will address it in the best possible way.</p></div></body></html>