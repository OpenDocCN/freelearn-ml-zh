<html><head></head><body>
<div class="Basic-Text-Frame" id="_idContainer225">
<h1 class="chapterNumber"><span class="koboSpan" id="kobo.1.1">16</span></h1>
<h1 class="chapterTitle" id="_idParaDest-425"><span class="koboSpan" id="kobo.2.1">Designing Generative AI Platforms and Solutions</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.3.1">Deploying generative AI at scale in an enterprise introduces new complexities around infrastructure, tooling, and operational processes required to harness its potential while managing risks. </span><span class="koboSpan" id="kobo.3.2">This chapter explores the essential components for building robust generative AI platforms and</span><a id="_idIndexMarker1595"/><span class="koboSpan" id="kobo.4.1"> examines </span><strong class="keyWord"><span class="koboSpan" id="kobo.5.1">Retrieval-Augmented Generation</span></strong><span class="koboSpan" id="kobo.6.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.7.1">RAG</span></strong><span class="koboSpan" id="kobo.8.1">), an effective architecture pattern for generative applications. </span><span class="koboSpan" id="kobo.8.2">Additionally, we highlight near-term generative AI solution opportunities ripe for business adoption across industries. </span><span class="koboSpan" id="kobo.8.3">With the right platform foundations and a pragmatic approach focused on delivering tangible value, enterprises can start realizing benefits from generative AI today while paving the way for increasing innovation as this technology matures. </span><span class="koboSpan" id="kobo.8.4">Readers will gain insight into the practical building blocks and strategies that accelerate generative AI adoption.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.9.1">Specifically, this chapter is going to cover the following topics:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.10.1">Operational considerations for generative AI platforms and solutions</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.11.1">The retrieval-augmented generation pattern</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.12.1">Choosing a </span><strong class="keyWord"><span class="koboSpan" id="kobo.13.1">Large Language Model</span></strong><span class="koboSpan" id="kobo.14.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.15.1">LLM</span></strong><span class="koboSpan" id="kobo.16.1">) adaptation method</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.17.1">End-to-end generative AI platforms</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.18.1">Considerations for deploying generative AI applications in production</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.19.1">Practical generative AI business solutions</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.20.1">Are we close to artificial general intelligence?</span></li>
</ul>
<h1 class="heading-1" id="_idParaDest-426"><span class="koboSpan" id="kobo.21.1">Operational considerations for generative AI platforms and solutions</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.22.1">In an </span><a id="_idIndexMarker1596"/><span class="koboSpan" id="kobo.23.1">enterprise, deploying generative AI solutions at scale requires robust infrastructure, tools, and operations. </span><span class="koboSpan" id="kobo.23.2">Organizations should contemplate establishing a dedicated generative AI platform to meet these evolving project demands.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.24.1">Architecturally and operationally, a generative AI platform builds on top of an ML platform with additional new and enhanced technology infrastructure for large-scale model training, large model hosting, model evaluation, guardrails, and model monitoring. </span><span class="koboSpan" id="kobo.24.2">As such, the core operation and automation requirements for a generative AI platform are similar to those of a traditional MLOps practice. </span><span class="koboSpan" id="kobo.24.3">However, the unique aspects of generative AI projects such as model selection, model tuning, and integration with external data sources, require several new process workflows to be established, and as a result, new technology components need to be incorporated into the operation and automation of the AI/ML platform.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.25.1">In this section, we will delve into several new business and operational workflows involved in building and running generative AI initiatives, and their implication in new technology requirements and new functional roles.</span></p>
<h2 class="heading-2" id="_idParaDest-427"><span class="koboSpan" id="kobo.26.1">New generative AI workflow and processes</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.27.1">One </span><a id="_idIndexMarker1597"/><span class="koboSpan" id="kobo.28.1">of the key steps in building generative AI solutions is to select the right foundation models for further</span><a id="_idIndexMarker1598"/><span class="koboSpan" id="kobo.29.1"> evaluation and/or fine-tuning against the requirements. </span><span class="koboSpan" id="kobo.29.2">Before generative AI, data scientists were mainly concerned with selecting the right algorithms to train models from scratch. </span><span class="koboSpan" id="kobo.29.3">While there were techniques such as transfer learning available, the scope was small, and mainly limited to some computer vision tasks and tuning of language models such as BERT. </span><span class="koboSpan" id="kobo.29.4">With large foundation models, the process of model selection becomes a lot more involved, and the model-tuning process has evolved as well. </span><span class="koboSpan" id="kobo.29.5">While instruction fine-tuning remains similar to traditional supervised learning, </span><strong class="keyWord"><span class="koboSpan" id="kobo.30.1">Reinforcement Learning Human Feedback</span></strong><span class="koboSpan" id="kobo.31.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.32.1">RLHF</span></strong><span class="koboSpan" id="kobo.33.1">) is a brand-new process to be considered.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.34.1">A generative AI project also requires a new process to manage and evaluate different prompt templates against different generative AI models for different use cases. </span><span class="koboSpan" id="kobo.34.2">These templates need to be version-tracked against various versions of different generative AI models as part of experimentation and testing. </span><span class="koboSpan" id="kobo.34.3">Metadata such as descriptions of the templates, their intended use, and limitations need to be documented and tracked. </span><span class="koboSpan" id="kobo.34.4">Testing results for the different combination of templates and models need to be stored and managed. </span><span class="koboSpan" id="kobo.34.5">New capabilities such as prompt template generation and tuning are also required.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.35.1">Some </span><a id="_idIndexMarker1599"/><span class="koboSpan" id="kobo.36.1">generative AI use cases such as document search and retrieval also require a new workflow for generating embeddings for different data modalities. </span><span class="koboSpan" id="kobo.36.2">This requires a new architecture pattern for splitting the source data (e.g., documents) and running it through embedding models to generate embeddings. </span><span class="koboSpan" id="kobo.36.3">These processes often need to be tuned to support different embedding needs. </span><span class="koboSpan" id="kobo.36.4">Moreover, the knowledge encoded in pre-trained foundation models is frozen in time. </span><span class="koboSpan" id="kobo.36.5">To keep the knowledge up to date, continuous incremental pre-training is required with new, up-to-date datasets. </span><span class="koboSpan" id="kobo.36.6">This flow to source, process, and manage the new dataset to hydrate the knowledge of the foundation model requires new technology components to manage it.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.37.1">Lastly, as generative AI technology can generate factually incorrect and sometimes toxic information, a new monitoring capability is needed for detection. </span><span class="koboSpan" id="kobo.37.2">For example, a set of filters might be needed to detect potential adversarial prompting and monitor the output response for incorrect and toxic information.</span></p>
<h2 class="heading-2" id="_idParaDest-428"><span class="koboSpan" id="kobo.38.1">New technology components</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.39.1">From a </span><a id="_idIndexMarker1600"/><span class="koboSpan" id="kobo.40.1">technology component perspective, some new tools such as vector databases and prompt stores, FM evaluation tools, and RLHF workflow tools are now needed to support the additional requirements. </span><span class="koboSpan" id="kobo.40.2">From a model deployment perspective, in addition to the deployment of large generative models themselves, new models are now needed for inspecting the inputs from the users and outputs from the model to ensure responsible AI principles are followed and adversarial attacks are detected and mitigated.</span></p>
<h2 class="heading-2" id="_idParaDest-429"><span class="koboSpan" id="kobo.41.1">New roles</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.42.1">In </span><a id="_idIndexMarker1601"/><span class="koboSpan" id="kobo.43.1">addition to the traditional roles and personas involved in ML projects and ML platforms, some new roles are now also required for building and using generative AI models. </span><span class="koboSpan" id="kobo.43.2">For example, to support RLHF, a new set of data annotators with domain expertise and language and linguistic proficiency is needed to help rate/rank responses. </span><span class="koboSpan" id="kobo.43.3">These annotators need knowledge to detect bias and harmful content, as well as being a good judge of the style and tone </span><a id="_idIndexMarker1602"/><span class="koboSpan" id="kobo.44.1">of texts. </span><span class="koboSpan" id="kobo.44.2">To support the development and testing of prompts, a new role called prompt engineer has also been established.</span></p>
<h2 class="heading-2" id="_idParaDest-430"><span class="koboSpan" id="kobo.45.1">Exploring generative AI platforms</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.46.1">A</span><a id="_idIndexMarker1603"/><span class="koboSpan" id="kobo.47.1"> generative AI platform builds on top of an ML platform, with new technology components to support new workflows such as prompt management and FM benchmarking. </span><span class="koboSpan" id="kobo.47.2">It is a new, emerging concept yet to have commonly agreed-upon architecture patterns. </span><span class="koboSpan" id="kobo.47.3">The following diagram illustrates one example blueprint for building such a platform. </span><span class="koboSpan" id="kobo.47.4">Keep in mind that many of the proposed components will need to be custom-built as there is no managed or even open-source technology available.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.48.1"><img alt="" role="presentation" src="../Images/B20836_16_01.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.49.1">Figure 16.1: Generative AI platform</span></p>
<p class="normal"><span class="koboSpan" id="kobo.50.1">As you can see from the preceding figure, the following new technology components will be needed to enhance the existing MLOps platform to support the needs of generative AI model development and deployment:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.51.1">Prompt management</span></strong><span class="koboSpan" id="kobo.52.1">: As </span><a id="_idIndexMarker1604"/><span class="koboSpan" id="kobo.53.1">one of the most important factors in getting desired responses from FMs, prompts need to be properly managed, tracked, and versioned.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.54.1">FM benchmarking/playground workbench</span></strong><span class="koboSpan" id="kobo.55.1">: As the field continues to accelerate, we</span><a id="_idIndexMarker1605"/><span class="koboSpan" id="kobo.56.1"> expect more FMs to become available. </span><span class="koboSpan" id="kobo.56.2">In addition, organizations will fine-tune existing FMs to create new models. </span><span class="koboSpan" id="kobo.56.3">To quickly determine if any new or fine-tuned FMs should be considered for different use cases, it is important to have an FM benchmark tool to quickly evaluate FMs against required criteria and use cases. </span><span class="koboSpan" id="kobo.56.4">Automation is critical as well as human-in-the-loop review and approval. </span><span class="koboSpan" id="kobo.56.5">Experiment tracking is also critical as part of the new capability to track and measure the performance of various prompts and response pairs.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.57.1">Central foundation model repository</span></strong><span class="koboSpan" id="kobo.58.1">: Unlike a regular model registry, where the </span><a id="_idIndexMarker1606"/><span class="koboSpan" id="kobo.59.1">main purpose is to keep an inventory of all models. </span><span class="koboSpan" id="kobo.59.2">There is a need to maintain a list of approved FMs for the rest of the organization to use. </span><span class="koboSpan" id="kobo.59.3">Both the process of introducing new models into the central repository and the process of adopting it for various downstream tasks such as fine-tuning and additional pre-training need to be properly managed. </span><span class="koboSpan" id="kobo.59.4">In addition, many of the foundation models will come from third-party providers via an API, so the new model repository will need to handle proper listing and access provisioning on behalf of the third-party providers. </span><span class="koboSpan" id="kobo.59.5">Centralizing FM models, especially those from third-party sources, in a shared repository raises additional cybersecurity concerns. </span><span class="koboSpan" id="kobo.59.6">These models may potentially harbor vulnerabilities that could be exploited. </span><span class="koboSpan" id="kobo.59.7">To mitigate these risks, it is crucial to implement robust cybersecurity measures, including thorough security scans, before adding the models to the repository. </span><span class="koboSpan" id="kobo.59.8">This helps ensure the integrity and security of the model repository.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.60.1">Supervised fine-tuning and RLHF</span></strong><span class="koboSpan" id="kobo.61.1">: This component provides support for frequent</span><a id="_idIndexMarker1607"/><span class="koboSpan" id="kobo.62.1"> instruction fine-tuning and domain adaptation incremental pre-training.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.63.1">Enhanced model monitoring and filters</span></strong><span class="koboSpan" id="kobo.64.1">: The platform should provide a common </span><a id="_idIndexMarker1608"/><span class="koboSpan" id="kobo.65.1">set of capabilities to monitor in production both the input and output of models for bias, toxic content, and factually incorrect responses. </span><span class="koboSpan" id="kobo.65.2">These should be configurable capabilities that can meet the needs of different use cases and responsible AI requirements.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.66.1">FM gateway</span></strong><span class="koboSpan" id="kobo.67.1">: Many </span><a id="_idIndexMarker1609"/><span class="koboSpan" id="kobo.68.1">organizations will likely adopt FMs from different internal and external sources and providers. </span><span class="koboSpan" id="kobo.68.2">A governed and secured FM gateway layer for accessing different FMs is a new platform component that needs to be implemented.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.69.1">Now, let’s take a closer look at some of the key components of a generative AI platform.</span></p>
<h3 class="heading-3" id="_idParaDest-431"><span class="koboSpan" id="kobo.70.1">The prompt management component</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.71.1">The </span><a id="_idIndexMarker1610"/><span class="koboSpan" id="kobo.72.1">prompt management component integrates with various generative AI platform components, enriching workflows such as model evaluation, generative AI chat applications, and generative AI agent interactions. </span><span class="koboSpan" id="kobo.72.2">At its core, the prompt management component consists of two main layers: the store and the management layer.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.73.1"><img alt="A diagram of a software application  Description automatically generated" src="../Images/B20836_16_02.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.74.1">Figure 16.2: Prompt management component</span></p>
<p class="normal"><span class="koboSpan" id="kobo.75.1">In the preceding figure, the prompt management component illustrates the role of the management layer in overseeing a catalog of reusable, versioned prompt templates. </span><span class="koboSpan" id="kobo.75.2">This layer not only facilitates template authoring, tagging, and ingestion capabilities but also governs template access and consumption features, including the logging of usage history. </span><span class="koboSpan" id="kobo.75.3">It should also have workflow tools to automate the process for prompt approval and publishing. </span><span class="koboSpan" id="kobo.75.4">Other core functionalities could include a prompt recommendation engine based on context and automated prompt generation based on inputs and model targets.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.76.1">For those opting to build this component on AWS, DynamoDB can serve as a viable choice for constructing the template store and managing usage history and metadata. </span><span class="koboSpan" id="kobo.76.2">The management layer and web UI can be implemented as a custom containerized software stack running on compute services like EKS. </span><span class="koboSpan" id="kobo.76.3">The API management layer can be easily implemented </span><a id="_idIndexMarker1611"/><span class="koboSpan" id="kobo.77.1">using AWS API Gateway.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.78.1">An API Gateway-enabled API access layer will be used to support both prompt store user applications and integration with other downstream and consuming applications and systems.</span></p>
<h3 class="heading-3" id="_idParaDest-432"><span class="koboSpan" id="kobo.79.1">FM benchmark workbench</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.80.1">The </span><a id="_idIndexMarker1612"/><span class="koboSpan" id="kobo.81.1">following diagram illustrates a high-level architecture for building an FM benchmarking solution using AWS services.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.82.1"><img alt="A diagram of a process  Description automatically generated" src="../Images/B20836_16_03.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.83.1">Figure 16.3: FM benchmark workbench</span></p>
<p class="normal"><span class="koboSpan" id="kobo.84.1">Within this architecture, target models sourced from a model registry are loaded into SageMaker endpoints. </span><span class="koboSpan" id="kobo.84.2">The testing process involves the generation of testing prompts by inserting various testing data from databases or S3 into predefined prompt templates, all of which are hosted within DynamoDB. </span><span class="koboSpan" id="kobo.84.3">These generated prompts are subsequently sent to the API endpoint for response generation. </span><span class="koboSpan" id="kobo.84.4">The resulting responses are evaluated using a predefined set of metrics, and the evaluation results are stored in DynamoDB. </span><span class="koboSpan" id="kobo.84.5">This stored data is made accessible to a workbench front-end application for further reference and utilization by the users.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.85.1">Amazon SageMaker</span><a id="_idIndexMarker1613"/><span class="koboSpan" id="kobo.86.1"> now also has built-in model evaluation capabilities with its Clarify component. </span><span class="koboSpan" id="kobo.86.2">SageMaker Clarify’s </span><strong class="keyWord"><span class="koboSpan" id="kobo.87.1">Foundation Model Evaluations</span></strong><span class="koboSpan" id="kobo.88.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.89.1">FMEval</span></strong><span class="koboSpan" id="kobo.90.1">) provides a centralized solution to assess model</span><a id="_idIndexMarker1614"/><span class="koboSpan" id="kobo.91.1"> quality, fairness, and reliability across LLMs. </span><span class="koboSpan" id="kobo.91.2">It has support for both automated model evaluation and workflows for human evaluation. </span><span class="koboSpan" id="kobo.91.3">Some</span><a id="_idIndexMarker1615"/><span class="koboSpan" id="kobo.92.1"> of the main tasks supported by the automated evaluation include:</span></p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1"><strong class="keyWord"><span class="koboSpan" id="kobo.93.1">Open-ended text generation</span></strong><span class="koboSpan" id="kobo.94.1">: For this task, Clarify can automate evaluations for factual knowledge, semantic robustness, prompt stereotyping, and toxicity. </span><span class="koboSpan" id="kobo.94.2">Clarify provides default testing datasets including TREX, CrowS-Pairs, RealToxicityPrompt, and BOLD.</span></li>
<li class="numberedList"><strong class="keyWord"><span class="koboSpan" id="kobo.95.1">Text summarization</span></strong><span class="koboSpan" id="kobo.96.1">: With this task, Clarify can assess accuracy, toxicity, and semantic robustness. </span><span class="koboSpan" id="kobo.96.2">For this evaluation, Clarify has built-in datasets including Government Report dataset, gigaword, and XSum.</span></li>
<li class="numberedList"><strong class="keyWord"><span class="koboSpan" id="kobo.97.1">Question answering</span></strong><span class="koboSpan" id="kobo.98.1">: For question and answering tasks, Clarify also assesses accuracy, toxicity, and semantic robustness, with built-in datasets such as BoolQ, TriviaQA, and Natural Questions.</span></li>
<li class="numberedList"><strong class="keyWord"><span class="koboSpan" id="kobo.99.1">Classification</span></strong><span class="koboSpan" id="kobo.100.1">: For this task, Clarify uses Women’s E-Commerce Clothing Reviews to assess accuracy and semantic robustness.</span></li>
</ol>
<p class="normal"><span class="koboSpan" id="kobo.101.1">With automated evaluation, you can use built-in datasets or bring your own dataset for testing.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.102.1">To perform the human evaluation of a natural language processing model, you must first define the relevant metrics and metric types. </span><span class="koboSpan" id="kobo.102.2">A comparative rating can be used to evaluate multiple models side by side on those metrics. </span><span class="koboSpan" id="kobo.102.3">Individual rating is required for evaluating a single model. </span><span class="koboSpan" id="kobo.102.4">Both rating mechanisms are applicable to any text-related task.</span></p>
<h3 class="heading-3" id="_idParaDest-433"><span class="koboSpan" id="kobo.103.1">Supervised fine-tuning and RLHF</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.104.1">As organizations</span><a id="_idIndexMarker1616"/><span class="koboSpan" id="kobo.105.1"> seek to customize FMs for their specific use cases, they need tools for fine-tuning and aligning FMs with human preferences and use cases. </span><span class="koboSpan" id="kobo.105.2">The core components of FM fine-tuning and RLHF play a pivotal role in facilitating end-to-end adaptation workflows.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.106.1">The core functionalities of these components should include automated supervised fine-tuning on an organization-specific dataset for target use cases while ensuring integration with the underlying training infrastructure. </span><span class="koboSpan" id="kobo.106.2">Additionally, it would also integrate with the FM evaluation service for both automated and human-assisted model evaluation.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.107.1">Another core </span><a id="_idIndexMarker1617"/><span class="koboSpan" id="kobo.108.1">functionality should be the support for a complete RLHF loop, allowing support from the collection and management of human preference data to interactive human evaluation/voting of FM outputs, automated reward model training, and finally, fine-tuning of models through reinforcement learning.</span></p>
<h3 class="heading-3" id="_idParaDest-434"><span class="koboSpan" id="kobo.109.1">FM monitoring</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.110.1">Since FMs </span><a id="_idIndexMarker1618"/><span class="koboSpan" id="kobo.111.1">are trained on extensive public datasets, which can potentially contain harmful and biased content, these models may demonstrate similar problematic behavior. </span><span class="koboSpan" id="kobo.111.2">While many proprietary models have integrated safeguards and filters to screen out harmful responses or inappropriate prompts, organizations may have distinct requirements for filtering, especially when they utilize open-source models that may lack similar built-in safeguards. </span><span class="koboSpan" id="kobo.111.3">The following diagram illustrates an architectural approach for incorporating supplementary filtering components into the inference workflow:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.112.1"><img alt="A diagram of a software system  Description automatically generated" src="../Images/B20836_16_04.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.113.1">Figure 16.4: Implementing model monitoring for generative AI models</span></p>
<p class="normal"><span class="koboSpan" id="kobo.114.1">In this architecture framework, AWS Lambda functions are positioned within the inference workflow to inspect both the input fed into the generative models and the responses generated by these models. </span><span class="koboSpan" id="kobo.114.2">Specialized detectors and classifier models, including toxicity classifiers, bias detectors, and adversarial prompt detectors, can be deployed and hosted as distinct endpoints. </span><span class="koboSpan" id="kobo.114.3">These Lambda functions are responsible for triggering </span><a id="_idIndexMarker1619"/><span class="koboSpan" id="kobo.115.1">these models to execute screening procedures. </span><span class="koboSpan" id="kobo.115.2">Moreover, alongside employing ML models, the Lambda functions can also incorporate rule-based logic to apply supplementary filtering measures.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.116.1">AWS has also implemented FM monitoring support, called guardrails, directly in its Bedrock service. </span><span class="koboSpan" id="kobo.116.2">It provides capabilities such as blocking undesired topics, filtering harmful content, and redacting PII data for the FMs hosted in Bedrock.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.117.1">Generative AI platform architecture and implementation are yet to mature at the time of writing. </span><span class="koboSpan" id="kobo.117.2">Organizations will need to evaluate the specific needs and invest in the building of these technology components to support their new generative AI workload at scale.</span></p>
<h1 class="heading-1" id="_idParaDest-435"><span class="koboSpan" id="kobo.118.1">The retrieval-augmented generation pattern</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.119.1">Foundation </span><a id="_idIndexMarker1620"/><span class="koboSpan" id="kobo.120.1">models are frozen in time and limited to the knowledge they were trained on, lacking access to an organization’s private data or changing public domain information. </span><span class="koboSpan" id="kobo.120.2">To enhance the accuracy of responses, especially when using proprietary or up-to-date data, we require a mechanism to integrate external information into the model’s response generation process.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.121.1">This is where </span><strong class="keyWord"><span class="koboSpan" id="kobo.122.1">retrieval-augmented generation</span></strong><span class="koboSpan" id="kobo.123.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.124.1">RAG</span></strong><span class="koboSpan" id="kobo.125.1">) can step in. </span><span class="koboSpan" id="kobo.125.2">RAG is a new architecture pattern introduced to support generative AI-based solutions such as enterprise knowledge search and document question answering where external data sources are required. </span><span class="koboSpan" id="kobo.125.3">There </span><a id="_idIndexMarker1621"/><span class="koboSpan" id="kobo.126.1">are two main stages to RAG:</span></p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1"><span class="koboSpan" id="kobo.127.1">The indexing stage for preparing a knowledge base with data ingestion and indexes.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.128.1">The query stage for retrieving relevant context from the knowledge base and passing it to the LLM to generate a response.</span></li>
</ol>
<p class="normal"><span class="koboSpan" id="kobo.129.1">Architecturally, RAG architecture consists of the following key components:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.130.1">Knowledge and document store</span></strong><span class="koboSpan" id="kobo.131.1">: This contains enterprise knowledge and documents </span><a id="_idIndexMarker1622"/><span class="koboSpan" id="kobo.132.1">used to provide context and facts for the LLM to generate responses based on real knowledge and facts. </span><span class="koboSpan" id="kobo.132.2">The store can be a knowledge graph, a database, a document store, or an object store.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.133.1">Document chunking component</span></strong><span class="koboSpan" id="kobo.134.1">: Long documents need to be broken up into small chunks containing different knowledge and data, and they can be managed and retrieved based on specific user queries. </span><span class="koboSpan" id="kobo.134.2">This technology component splits the documents into small pieces based on predefined logic and rules (by number of words/characters, by paragraph).</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.135.1">Document embedding component</span></strong><span class="koboSpan" id="kobo.136.1">: This component creates embeddings from the document chunks so these chunks can be effectively searched based on semantic similarity. </span><span class="koboSpan" id="kobo.136.2">This is a key concept for knowledge retrieval.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.137.1">Vector DB</span></strong><span class="koboSpan" id="kobo.138.1">: This component stores the document chunks and associated embeddings and provides the capability for semantic searches using various techniques such as cosine similarity.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.139.1">Retriever and re-ranker</span></strong><span class="koboSpan" id="kobo.140.1">: This component retrieves and re-ranks the top matches from the vector DB depending on specific requirements or context.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.141.1">Query embedding component</span></strong><span class="koboSpan" id="kobo.142.1">: This component creates embeddings of user queries; these queries are then used to look up embeddings from the vector DB.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.143.1">Workflow orchestration</span></strong><span class="koboSpan" id="kobo.144.1">: This component orchestrates the various steps in the chunking, embedding, and query/response flow.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.145.1">Prompt template store</span></strong><span class="koboSpan" id="kobo.146.1">: This store maintains prompt templates to construct appropriate queries against LLMs.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.147.1">Task prompt builder</span></strong><span class="koboSpan" id="kobo.148.1">: This component selects the appropriate prompts from the prompt template store and creates a task prompt using the original query and retrieved document/knowledge as context. </span><span class="koboSpan" id="kobo.148.2">The prompt is formatted to optimize the responses from the LLMs.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.149.1">LLMs</span></strong><span class="koboSpan" id="kobo.150.1">: These LLMs are responsible for constructing responses from templates that use the knowledge/document chunks as part of the context to provide more factual and fluent responses.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.151.1">The following diagram illustrates this architecture and flow:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.152.1"><img alt="A diagram of a vector database  Description automatically generated" src="../Images/B20836_16_05.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.153.1">Figure 16.5: Retrieval-augmented generation architecture</span></p>
<p class="normal"><span class="koboSpan" id="kobo.154.1">During the</span><a id="_idIndexMarker1623"/><span class="koboSpan" id="kobo.155.1"> indexing stage, source documents are broken up by a document-chunking component that breaks them up into small chunks. </span><span class="koboSpan" id="kobo.155.2">These chunks are further processed by an embedding component, whereby a vector representation (embedding) is created for each chunk, capturing the semantic meaning of the text in that chunk. </span><span class="koboSpan" id="kobo.155.3">The embeddings, along with their associated document chunks, are then stored in a vector database.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.156.1">During the query stage, the user’s query (prompt) is first processed by an embedding component to generate a vector representation (embedding) for the query itself. </span><span class="koboSpan" id="kobo.156.2">A retriever component then uses this query embedding to retrieve matching vectors and their associated document chunks from the previously indexed vector database, typically by calculating similarity measures like cosine distance between the query and stored embeddings. </span><span class="koboSpan" id="kobo.156.3">The document chunks corresponding to the most similar embeddings are retrieved. </span><span class="koboSpan" id="kobo.156.4">These retrieved chunks are then incorporated into the original user query as additional context, forming a new, augmented prompt that combines the initial query with the relevant retrieved information. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.157.1">Finally, this augmented </span><a id="_idIndexMarker1624"/><span class="koboSpan" id="kobo.158.1">prompt is sent to an LLM for synthesis, allowing the LLM to generate a response informed by both the original query and the contextual information retrieved from the database.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.159.1">In the following sections, we will first explore leading open-source frameworks for building RAG applications. </span><span class="koboSpan" id="kobo.159.2">We will then discuss the evaluation of the RAG pipeline, followed by advanced RAG patterns. </span><span class="koboSpan" id="kobo.159.3">Finally, we will understand how to build RAG architecture using AWS services. </span><span class="koboSpan" id="kobo.159.4">Let’s get started!</span></p>
<h2 class="heading-2" id="_idParaDest-436"><span class="koboSpan" id="kobo.160.1">Open-source frameworks for RAG</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.161.1">As </span><a id="_idIndexMarker1625"/><span class="koboSpan" id="kobo.162.1">RAG has emerged as an important architecture for many generative AI use cases, multiple technology frameworks have been developed by the open-source community to help streamline the implementation of RAG-based solutions and provide new capabilities. </span><span class="koboSpan" id="kobo.162.2">There are many RAG frameworks such as LangChain, LlamaIndex, REALM, and Haystack. </span><span class="koboSpan" id="kobo.162.3">Here, let’s briefly review LangChain and LlamaIndex, two of the more popular frameworks for RAG.</span></p>
<h3 class="heading-3" id="_idParaDest-437"><span class="koboSpan" id="kobo.163.1">LangChain</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.164.1">One of the</span><a id="_idIndexMarker1626"/><span class="koboSpan" id="kobo.165.1"> key challenges of building an LLM-based application is the coordination of various components, including vector DB, data retrieval, embedding LLM, and response generation LLMs.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.166.1">LangChain is </span><a id="_idIndexMarker1627"/><span class="koboSpan" id="kobo.167.1">an open-source library that provides a capability that provides abstraction for these various RAG components and allows you to orchestrate them. </span><span class="koboSpan" id="kobo.167.2">LangChain supports the concept of a chain whereby a list of dependent tasks and components are connected to perform a function. </span><span class="koboSpan" id="kobo.167.3">For example, you can create a simple chain that takes an input query, reformat it using a predefined prompt template, and then invoke an LLM with the reformatted query. </span><span class="koboSpan" id="kobo.167.4">You can also have a complex chain that takes a document, splits it into small chunks, passes them to an embedding LLM, and then stores the embeddings in a vector DB. </span><span class="koboSpan" id="kobo.167.5">While these orchestrations can be hardcoded programmatically, a LangChain chain provides a dynamic way to define the orchestration and provides modular abstractions for many components, such as the LLMs and vector DBs, to simplify the implementation. </span><span class="koboSpan" id="kobo.167.6">LangChain comes with a list of built-in use-case-specific chains for quick implementation.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.168.1">The following is a simple code example of using LangChain for a question-answering task:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-key ord"><span class="koboSpan" id="kobo.169.1">from</span></span><span class="koboSpan" id="kobo.170.1"> langchain </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.171.1">import</span></span><span class="koboSpan" id="kobo.172.1"> LLMChain, PromptTemplate
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.173.1">from</span></span><span class="koboSpan" id="kobo.174.1"> langchain_community.embeddings </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.175.1">import</span></span><span class="koboSpan" id="kobo.176.1"> BedrockEmbeddings
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.177.1">from</span></span><span class="koboSpan" id="kobo.178.1"> langchain_community.vectorstore </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.179.1">import</span></span><span class="koboSpan" id="kobo.180.1"> OpenSearchVectorSearch
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.181.1">from</span></span><span class="koboSpan" id="kobo.182.1"> langchain_community.llms.bedrock </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.183.1">import</span></span><span class="koboSpan" id="kobo.184.1"> Bedrock
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.185.1">from</span></span><span class="koboSpan" id="kobo.186.1"> langchain.chains </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.187.1">import</span></span><span class="koboSpan" id="kobo.188.1"> RetrievalQA
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.189.1">import</span></span><span class="koboSpan" id="kobo.190.1"> opensearch 
bedrock_client = get_bedrock_client(region, …)
bedrock_llm = create_bedrock_llm(bedrock_client)
opensearch_endpoint = opensearch.get_opensearch_endpoint(index_name, …)
opensearch_vector_search_client = create_opensearch_vector_search_client(index_name, bedrock_embeddings_client, …)
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.191.1"># Define a prompt template </span></span><span class="koboSpan" id="kobo.192.1">
prompt = PromptTemplate(
  input_variables=[</span><span class="hljs-string"><span class="koboSpan" id="kobo.193.1">"context"</span></span><span class="koboSpan" id="kobo.194.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.195.1">"question"</span></span><span class="koboSpan" id="kobo.196.1">],
  template=</span><span class="hljs-string"><span class="koboSpan" id="kobo.197.1">"Answer the question based on the retrieved</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.198.1">  context: {context}</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.199.1">  Question: {question}</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.200.1">  Answer:"</span></span><span class="koboSpan" id="kobo.201.1">)
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.202.1"># Create a chain with Bedrock model and prompt </span></span><span class="koboSpan" id="kobo.203.1">
qa_chain = RetrievalQA.from_chain_type(llm=bedrock_llm, retriever=opensearch_vector_client.as_retriever(), chain_type_kwarges={</span><span class="hljs-string"><span class="koboSpan" id="kobo.204.1">"prompt"</span></span><span class="koboSpan" id="kobo.205.1">: prompt}
)
question = </span><span class="hljs-string"><span class="koboSpan" id="kobo.206.1">"What is the capital of France?"</span></span><span class="koboSpan" id="kobo.207.1">
result = qa_chain(question=question)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.208.1">This code </span><a id="_idIndexMarker1628"/><span class="koboSpan" id="kobo.209.1">sample initializes an OpenSearch and </span><a id="_idIndexMarker1629"/><span class="koboSpan" id="kobo.210.1">Amazon Bedrock client, defines a prompt template, creates a built-in RetrievalQA chain instance, and executes the chain to generate an answer to the question.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.211.1">LangChain also supports the concept of a tool. </span><span class="koboSpan" id="kobo.211.2">A tool is a function that LangChain can invoke to perform an action such as math calculation or searching the web. </span><span class="koboSpan" id="kobo.211.3">This is a critical feature as this extends an LLM’s capability to perform more complex and precise tasks and be more factually correct.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.212.1">In addition, LangChain also has support for the concept of an agent. </span><span class="koboSpan" id="kobo.212.2">An agent helps link LLMs with different tools for dynamic execution, enabling LLMs to select the right tool to perform different tasks based on the query. </span><span class="koboSpan" id="kobo.212.3">For example, you might have a query that requires first searching the internet to get some facts and then performing a mathematical calculation on the result. </span><span class="koboSpan" id="kobo.212.4">In this case, an agent would allow an LLM to dynamically figure out which tool to use to complete the query. </span><span class="koboSpan" id="kobo.212.5">The following diagram illustrates</span><a id="_idIndexMarker1630"/><span class="koboSpan" id="kobo.213.1"> how agents and tools can work together to </span><a id="_idIndexMarker1631"/><span class="koboSpan" id="kobo.214.1">support the workflow.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.215.1"><img alt="A diagram of a tool  Description automatically generated" src="../Images/B20836_16_06.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.216.1">Figure 16.6: Agent and tool workflow</span></p>
<p class="normal"><span class="koboSpan" id="kobo.217.1">Since its initial release in 2022, LangChain has seen rapid uptake across the AI community and companies. </span><span class="koboSpan" id="kobo.217.2">Numerous integrations have been developed to provide a range of capabilities, such as document loaders, vector stores, embedding models, LLMs, and tools.</span></p>
<h3 class="heading-3" id="_idParaDest-438"><span class="koboSpan" id="kobo.218.1">LlamaIndex</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.219.1">LlamaIndex</span><a id="_idIndexMarker1632"/><span class="koboSpan" id="kobo.220.1"> is a data framework for building </span><a id="_idIndexMarker1633"/><span class="koboSpan" id="kobo.221.1">LLM applications. </span><span class="koboSpan" id="kobo.221.2">LlamaIndex offers a comprehensive toolkit encompassing data connectors for diverse data sources and formats (e.g., APIs, PDFs, SQL), data indexes that structure information for efficient consumption by LLMs, and a query interface for sending queries and getting back responses from the underlying indexes.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.222.1">After the </span><a id="_idIndexMarker1634"/><span class="koboSpan" id="kobo.223.1">data is ingested using LlamaIndex, the data is split up into data chunks and an embedding is created for each chunk. </span><span class="koboSpan" id="kobo.223.2">A data chunk and its embedding is called a node in LlamaIndex. </span><span class="koboSpan" id="kobo.223.3">The nodes are stored in the underlying vector databases. </span><span class="koboSpan" id="kobo.223.4">It also supports the concept of a list index where a list of related nodes are linked together; for example, a list of all chunks from a single document. </span><span class="koboSpan" id="kobo.223.5">This can be used for use cases where you want to summarize the entire document instead of returning a piece of relevant information.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.224.1">These engines</span><a id="_idIndexMarker1635"/><span class="koboSpan" id="kobo.225.1"> include query engines for robust knowledge retrieval, chat engines for interactive conversations, and data agents that empower LLM-driven knowledge workers with a range of tools and integrations. </span><span class="koboSpan" id="kobo.225.2">Moreover, LlamaIndex seamlessly integrates with various applications, such as LangChain, Flask, Docker, ChatGPT, and others, ensuring cohesive integration within your broader ecosystem.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.226.1">LlamaIndex has the concept of data agents, which are knowledge workers that perform various tasks including searching and the retrieval of data, and calling external service APIs. </span><span class="koboSpan" id="kobo.226.2">This concept is similar to the agent concept in LangChain. </span><span class="koboSpan" id="kobo.226.3">Given an input query, the data agent uses a reasoning loop to determine which tool to use and in which sequence to invoke the tools.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.227.1">There are some overlaps between LlamaIndex and LangChain in the area of external data connectors and indexing, query management for data retrieval, and interacting with LLMs. </span><span class="koboSpan" id="kobo.227.2">The key difference is that LlamaIndex focuses on building rich capabilities in those overlapping areas, while LangChain is more general-purpose with support for tools, agents, and chains.</span></p>
<h2 class="heading-2" id="_idParaDest-439"><span class="koboSpan" id="kobo.228.1">Evaluating a RAG pipeline</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.229.1">Evaluating</span><a id="_idIndexMarker1636"/><span class="koboSpan" id="kobo.230.1"> the performance of a RAG pipeline is a complex task as there are multiple processes involved, such as knowledge indexing, knowledge retrieval, and response synthesis. </span><span class="koboSpan" id="kobo.230.2">In addition, there is the unpredictable nature of text generation.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.231.1">Evaluating a RAG application involves multiple stages:</span></p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1"><strong class="keyWord"><span class="koboSpan" id="kobo.232.1">Stage 1 – Faithfulness evaluation of response against the context</span></strong><span class="koboSpan" id="kobo.233.1">: This stage tests if the synthesized response faithfully captures the facts and context of the retrieved source documents. </span><span class="koboSpan" id="kobo.233.2">If it does not, then it is a sign of LLM hallucination.</span></li>
<li class="numberedList"><strong class="keyWord"><span class="koboSpan" id="kobo.234.1">Stage 2 – Response relevancy evaluation against the query</span></strong><span class="koboSpan" id="kobo.235.1">: This stage checks if the response matches the retrieved source documents, and then this stage evaluates if the synthesized response answers the query. </span><span class="koboSpan" id="kobo.235.2">If it does not match, then it is an indication that the semantic search and/or embeddings do not function correctly.</span></li>
<li class="numberedList"><strong class="keyWord"><span class="koboSpan" id="kobo.236.1">Stage 3 – Question answering on the source document</span></strong><span class="koboSpan" id="kobo.237.1">: This stage uses an external tool to generate questions from the source document, and test if the LLM can answer questions using data. </span><span class="koboSpan" id="kobo.237.2">If the LLM cannot answer the question correctly, then the LLM is defective.</span></li>
</ol>
<p class="normal"><span class="koboSpan" id="kobo.238.1">There are </span><a id="_idIndexMarker1637"/><span class="koboSpan" id="kobo.239.1">several tools that can be used for RAG evaluation. </span><span class="koboSpan" id="kobo.239.2">For example, LlamaIndex provides a number of modules for evaluating hallucination and relevancy, as well as generating questions from source data. </span><span class="koboSpan" id="kobo.239.3">There are also other open-source tools, such as DeepEval for writing unit tests, and Ragas, which measures the RAG pipeline’s performance against different dimensions, including faithfulness and answer relevancy for response generation, and context precision and recall for the retrieved context against annotated answers.</span></p>
<h2 class="heading-2" id="_idParaDest-440"><span class="koboSpan" id="kobo.240.1">Advanced RAG patterns</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.241.1">While</span><a id="_idIndexMarker1638"/><span class="koboSpan" id="kobo.242.1"> RAG has proven to be highly versatile and effective in solving many generative AI use cases, it also comes with its unique set of challenges that we need to be aware of and address when implementing a RAG solution.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.243.1">A core </span><a id="_idIndexMarker1639"/><span class="koboSpan" id="kobo.244.1">challenge in developing effective RAG solutions is ensuring high-quality retrieval results. </span><span class="koboSpan" id="kobo.244.2">Poor retrieval can stem from various factors. </span><span class="koboSpan" id="kobo.244.3">For instance, retrieved text blocks may lack correlation with the original query, leading to hallucinated or fabricated responses. </span><span class="koboSpan" id="kobo.244.4">Additionally, failure to retrieve all relevant knowledge blocks prevents the model from synthesizing complete, high-quality answers.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.245.1">One way to improve retrieval quality is to improve the indexing of the documents. </span><span class="koboSpan" id="kobo.245.2">There are several methods for improving indexing:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.246.1">Pre-indexing data optimization</span></strong><span class="koboSpan" id="kobo.247.1">: Standardizing text, removing irrelevant content, eliminating</span><a id="_idIndexMarker1640"/><span class="koboSpan" id="kobo.248.1"> redundancies, and validating factual accuracy before indexing. </span><span class="koboSpan" id="kobo.248.2">This reduces noise in the indexed data.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.249.1">Index structure optimization</span></strong><span class="koboSpan" id="kobo.250.1">: Tuning</span><a id="_idIndexMarker1641"/><span class="koboSpan" id="kobo.251.1"> chunk size parameters to balance context preservation against quality. </span><span class="koboSpan" id="kobo.251.2">Strategies like sliding window chunking help retain contextual information across chunks.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.252.1">Metadata enhancement</span></strong><span class="koboSpan" id="kobo.253.1">: Incorporating</span><a id="_idIndexMarker1642"/><span class="koboSpan" id="kobo.254.1"> supplemental metadata like chapter descriptions and dates into chunks provides useful indexing context.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.255.1">Embedding fine-tuning</span></strong><span class="koboSpan" id="kobo.256.1">: Fine-tuning embeddings adds critical in-domain context for </span><a id="_idIndexMarker1643"/><span class="koboSpan" id="kobo.257.1">specialized areas with uncommon terms.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.258.1">Dynamic embedding</span></strong><span class="koboSpan" id="kobo.259.1">: Generating</span><a id="_idIndexMarker1644"/><span class="koboSpan" id="kobo.260.1"> contextualized embeddings improves upon static embedding limitations.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.261.1">Beyond</span><a id="_idIndexMarker1645"/><span class="koboSpan" id="kobo.262.1"> indexing, implementing</span><a id="_idIndexMarker1646"/><span class="koboSpan" id="kobo.263.1"> smarter retrieval pipelines can further enhance RAG performance:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.264.1">Recursive retrieval</span></strong><span class="koboSpan" id="kobo.265.1">: This</span><a id="_idIndexMarker1647"/><span class="koboSpan" id="kobo.266.1"> approach introduces a multi-stage querying approach, first retrieving smaller semantic blocks followed by larger contextual blocks. </span><span class="koboSpan" id="kobo.266.2">This balances efficiency with rich contextual grounding.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.267.1">Subqueries</span></strong><span class="koboSpan" id="kobo.268.1">: Various</span><a id="_idIndexMarker1648"/><span class="koboSpan" id="kobo.269.1"> query strategies can be employed in different scenarios, including using query engines provided by frameworks like LlamaIndex, employing tree queries, utilizing vector queries, or employing the most basic sequential querying of chunks.
    </span><p class="normal"><span class="koboSpan" id="kobo.270.1">Retrieving relevant contextual documents is only the first step. </span><span class="koboSpan" id="kobo.270.2">Preparing the retrieved evidence for input into the LLM presents additional challenges. </span><span class="koboSpan" id="kobo.270.3">Inputting all documents at once risks exceeding the LLM’s context capacity. </span><span class="koboSpan" id="kobo.270.4">Meanwhile, concatenating documents creates lengthy, unfocused prompts. </span><span class="koboSpan" id="kobo.270.5">Advanced techniques are needed to optimize the retrieved evidence for response generation. </span><span class="koboSpan" id="kobo.270.6">With optimized evidence preparation, the LLM can focus on crucial information within its context window for producing high-quality responses grounded in the retrieved data.</span></p></li>
</ul>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.271.1">Re-ranking</span></strong><span class="koboSpan" id="kobo.272.1">: Re-ranking techniques can optimize document order to prioritize the </span><a id="_idIndexMarker1649"/><span class="koboSpan" id="kobo.273.1">most relevant information for synthesizing. </span><span class="koboSpan" id="kobo.273.2">There are different rankers available. </span><span class="koboSpan" id="kobo.273.3">For example, the Diversity Ranker reorders documents to increase diversity within the context window. </span><span class="koboSpan" id="kobo.273.4">The LostInTheMiddleRanker positions the best document at the start and end of the context window alternately. </span><span class="koboSpan" id="kobo.273.5">Through re-ranking, the most informative evidence is strategically placed for the language model to focus on. </span><span class="koboSpan" id="kobo.273.6">This prevents the most relevant details from getting lost in the middle of a lengthy context. </span><span class="koboSpan" id="kobo.273.7">Effective re-ranking is crucial for enabling models to produce high-quality responses grounded in the top retrieved evidence.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.274.1">Prompt compression</span></strong><span class="koboSpan" id="kobo.275.1">: Post-retrieval processing techniques can remove irrelevant context from</span><a id="_idIndexMarker1650"/><span class="koboSpan" id="kobo.276.1"> prompts to reduce noise and improve response quality in RAG systems. </span><span class="koboSpan" id="kobo.276.2">For example, some models calculate mutual information across prompt elements to estimate and filter out unimportant or distracting information. </span><span class="koboSpan" id="kobo.276.3">By stripping away non-essential text, the language model can focus on the truly salient evidence when generating responses. </span><span class="koboSpan" id="kobo.276.4">Careful prompt pruning is an impactful strategy to reduce hallucinations and keep RAG responses grounded in the most critical supporting context.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.277.1">In summary, optimizing </span><a id="_idIndexMarker1651"/><span class="koboSpan" id="kobo.278.1">the indexing, implementing recursive retrieval pipelines, strategically reformatting evidence, and pruning prompts are all impactful strategies to address these challenges. </span><span class="koboSpan" id="kobo.278.2">When combined creatively, these advanced patterns enable models to retrieve the most relevant knowledge at each stage, hone in on the salient context, and synthesize this tailored evidence into high-quality, grounded responses.</span></p>
<h2 class="heading-2" id="_idParaDest-441"><span class="koboSpan" id="kobo.279.1">Designing a RAG architecture on AWS</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.280.1">If you use</span><a id="_idIndexMarker1652"/><span class="koboSpan" id="kobo.281.1"> AWS, there are multiple ready-to-use AWS and third-party services you can use to build a RAG architecture.</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.282.1">SageMaker JumpStart</span></strong><span class="koboSpan" id="kobo.283.1">: SageMaker JumpStart </span><a id="_idIndexMarker1653"/><span class="koboSpan" id="kobo.284.1">provides pre-trained, open-source models for a wide range of problem types. </span><span class="koboSpan" id="kobo.284.2">You can incrementally train and tune these models for specific requirements and host these models using the SageMaker hosting service to support RAG architecture. </span><span class="koboSpan" id="kobo.284.3">Choose SageMaker JumpStart if you like to train and host open-source </span><a id="_idIndexMarker1654"/><span class="koboSpan" id="kobo.285.1">FMs yourself and have the choice of different compute options for model hosting.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.286.1">Amazon Bedrock</span></strong><span class="koboSpan" id="kobo.287.1">: Amazon Bedrock</span><a id="_idIndexMarker1655"/><span class="koboSpan" id="kobo.288.1"> is a fully managed service that makes FMs from Amazon and leading AI startups available through an API. </span><span class="koboSpan" id="kobo.288.2">At the time of writing, Amazon Bedrock provides the Titan FM from Amazon, and FMs from AI21, Stability AI, Cohere, Meta, Mistral, and Anthropic. </span><span class="koboSpan" id="kobo.288.3">For more details about Bedrock, you can visit the AWS public site. </span><span class="koboSpan" id="kobo.288.4">You want to explore Bedrock if you want access to the top proprietary FMs via an API instead of hosting your own. </span><span class="koboSpan" id="kobo.288.5">Also, for workloads with a low volume of transactions, Bedrock can be more cost-effective. </span><span class="koboSpan" id="kobo.288.6">Amazon Bedrock also has built-in support for document indexing and vector storage, known as the Bedrock knowledge base for RAG development. </span><span class="koboSpan" id="kobo.288.7">It also comes with support for agents and tools, called Agents for Amazon Bedrock, for building agent-based workflow applications.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.289.1">Amazon OpenSearch</span></strong><span class="koboSpan" id="kobo.290.1">: Amazon OpenSearch Service provides capabilities for interactive</span><a id="_idIndexMarker1656"/><span class="koboSpan" id="kobo.291.1"> log </span><a id="_idIndexMarker1657"/><span class="koboSpan" id="kobo.292.1">analytics, real-time application monitoring, website search, and more. </span><span class="koboSpan" id="kobo.292.2">It can also be used as a vector database in the RAG architecture. </span><span class="koboSpan" id="kobo.292.3">It allows for the building of indexes for knowledge chunks and embeddings, and it provides proximity-based vector search. </span><span class="koboSpan" id="kobo.292.4">OpenSearch can integrate with LangChain and LlamaIndex.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.293.1">Amazon Kendra</span></strong><span class="koboSpan" id="kobo.294.1">: Amazon Kendra is </span><a id="_idIndexMarker1658"/><span class="koboSpan" id="kobo.295.1">an intelligent search service that can be used as a knowledge store for RAG. </span><span class="koboSpan" id="kobo.295.2">Amazon Kendra also provides a RAG retrieval API that can work with LangChain seamlessly.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.296.1">Amazon Q</span></strong><span class="koboSpan" id="kobo.297.1">: Amazon Q is</span><a id="_idIndexMarker1659"/><span class="koboSpan" id="kobo.298.1"> a relatively new service from AWS. </span><span class="koboSpan" id="kobo.298.2">There are multiple independent services under the Amazon Q product suite, including Amazon Q for Business, Amazon Q for Builder, Amazon Q for QuickSight, and Amazon Q for Connect. </span><span class="koboSpan" id="kobo.298.3">Amazon Q for Business is a fully managed RAG-based assistant that can answer questions, provide summaries, and generate content based on enterprise data.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.299.1">There are other third-party and open-source vector database options available, such as Pinecone and FAISS.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.300.1">The following is an example RAG architecture designed using AWS services:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.301.1"><img alt="A diagram of data lake  Description automatically generated" src="../Images/B20836_16_07.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.302.1">Figure 16.7: RAG architecture on AWS</span></p>
<p class="normal"><span class="koboSpan" id="kobo.303.1">A RAG-based</span><a id="_idIndexMarker1660"/><span class="koboSpan" id="kobo.304.1"> architecture can support many use cases, such as question answering over documents, querying data from databases and knowledge graphs using natural language, interactive chatbots, customer support assistants, medical information search and recommendations, and education and training.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.305.1">As RAG architecture is becoming a common critical component of many LLM application architecture stacks, it is crucial to establish an operational capability and process that is similar to that of a GenAI/ML platform. </span><span class="koboSpan" id="kobo.305.2">However, since this is still a new area, there has not been much development of technical capabilities and tools around RAG management. </span><span class="koboSpan" id="kobo.305.3">Different organizations will need to evaluate needs and develop customer software and infrastructure to enable the operation of a common RAG platform. </span><span class="koboSpan" id="kobo.305.4">Organizations can consider RAG infrastructure as an independent platform or part of the overall generative AI platform.</span></p>
<h1 class="heading-1" id="_idParaDest-442"><span class="koboSpan" id="kobo.306.1">Choosing an LLM adaptation method</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.307.1">We have </span><a id="_idIndexMarker1661"/><span class="koboSpan" id="kobo.308.1">covered various LLM adaptation methods, including prompt engineering, domain adaptation pre-training, fine-tuning, and RAG. </span><span class="koboSpan" id="kobo.308.2">All these methods are intended to get better responses from the pre-trained LLMs. </span><span class="koboSpan" id="kobo.308.3">With all these options, it </span><a id="_idIndexMarker1662"/><span class="koboSpan" id="kobo.309.1">leaves one wondering: how do we choose which method to use?</span></p>
<p class="normal"><span class="koboSpan" id="kobo.310.1">Let’s break down some of the considerations when choosing these different methods.</span></p>
<h2 class="heading-2" id="_idParaDest-443"><span class="koboSpan" id="kobo.311.1">Response quality</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.312.1">Response quality</span><a id="_idIndexMarker1663"/><span class="koboSpan" id="kobo.313.1"> measures how accurately the LLM response is aligned with the intent of the user queries. </span><span class="koboSpan" id="kobo.313.2">The evaluation of response quality can be intricate for different use cases, as there are different considerations for evaluating response quality, such as knowledge domain affinity, task accuracy, up-to-date data, source data transparency, and hallucination.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.314.1">For knowledge domain affinity, domain adaptation pre-training can be used to effectively teach LLM domain-specific knowledge and terminology. </span><span class="koboSpan" id="kobo.314.2">RAG is efficient in retrieving relevant data, but the LLM used for the response synthetization may not capture domain-specific patterns, terminology, and nuance as well as fine-tuned or domain adaptation pre-training models. </span><span class="koboSpan" id="kobo.314.3">If you need strong domain-specific performance, you want to consider domain adaptation pre-training.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.315.1">If you need to maximize accuracy for specific tasks, then fine-tuning is the recommended approach. </span><span class="koboSpan" id="kobo.315.2">Prompt engineering can also help improve task accuracy through single-shot or few-shot prompting techniques, but it is prompt-specific and does not generalize across different prompts.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.316.1">If information freshness in the response is the primary goal, then RAG is the ideal solution since it has access to dynamic external data sources. </span><span class="koboSpan" id="kobo.316.2">Prompt engineering can also help with data freshness when up-to-date knowledge is provided as part of the prompt. </span><span class="koboSpan" id="kobo.316.3">Fine-tuning and domain adaptation pre-training have knowledge cutoffs based on the latest training dataset used.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.317.1">For some applications such as medical diagnosis or financial analysis, knowing how the decisions were made and what data sources were used in making the decision is crucial. </span><span class="koboSpan" id="kobo.317.2">If this is a critical requirement for the use case, then RAG is the clear choice here, as RAG can provide references to the knowledge it used for constructing the response. </span><span class="koboSpan" id="kobo.317.3">Fine-tuning and domain adaptation pre-training behave more like a “black box,” often obscuring what data sources are used for decision-making.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.318.1">As mentioned in the previous chapter, LLMs sometimes generate inaccurate responses that are not grounded in their training data or user input when they encounter unfamiliar queries and hallucinate plausible but false information. </span><span class="koboSpan" id="kobo.318.2">Fine-tuning can reduce </span><a id="_idIndexMarker1664"/><span class="koboSpan" id="kobo.319.1">fabrication by focusing the model on domain-specific knowledge. </span><span class="koboSpan" id="kobo.319.2">However, the risk remains for unfamiliar inputs. </span><span class="koboSpan" id="kobo.319.3">RAG systems better address hallucination risks by anchoring responses to retrieved documents. </span><span class="koboSpan" id="kobo.319.4">The initial retrieval step acts as a fact check, finding relevant passages to ground the response in real data. </span><span class="koboSpan" id="kobo.319.5">Subsequent generation is confined within the context of the retrievals rather than being unconstrained. </span><span class="koboSpan" id="kobo.319.6">This mechanism minimizes fabricated responses not supported by data.</span></p>
<h2 class="heading-2" id="_idParaDest-444"><span class="koboSpan" id="kobo.320.1">Cost of the adaptation</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.321.1">When</span><a id="_idIndexMarker1665"/><span class="koboSpan" id="kobo.322.1"> evaluating LLM adaptation approaches, it is important to consider both initial implementation costs as well as long-term maintenance costs. </span><span class="koboSpan" id="kobo.322.2">With this in mind, let’s compare the costs of the different approaches.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.323.1">Prompt engineering has the lowest overhead, involving simply writing and testing prompts to yield good results from the pre-trained language model. </span><span class="koboSpan" id="kobo.323.2">Maintenance may require occasional prompt updates as the foundation model is updated over time.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.324.1">RAG systems have moderately high startup costs due to requiring multiple components – embeddings, vector stores, retrievers, and language models. </span><span class="koboSpan" id="kobo.324.2">However, these systems are relatively static over time.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.325.1">Full fine-tuning and domain adaptation pre-training can be expensive, needing massive computational resources and time to completely update potentially all parameters of a large foundation model, as well as the cost of dataset preparation. </span><strong class="keyWord"><span class="koboSpan" id="kobo.326.1">Parameter Efficient Fine-Tuning</span></strong><span class="koboSpan" id="kobo.327.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.328.1">PEFT</span></strong><span class="koboSpan" id="kobo.329.1">) can </span><a id="_idIndexMarker1666"/><span class="koboSpan" id="kobo.330.1">be cheaper than full fine-tuning and domain adaptation pre-training. </span><span class="koboSpan" id="kobo.330.2">However, it is still considered more expensive than RAG due to the requirement for high-quality dataset preparation and training resource requirements.</span></p>
<h2 class="heading-2" id="_idParaDest-445"><span class="koboSpan" id="kobo.331.1">Implementation complexity</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.332.1">The implementation complexity </span><a id="_idIndexMarker1667"/><span class="koboSpan" id="kobo.333.1">varies significantly across different techniques, from straightforward to highly advanced configurations.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.334.1">Prompt engineering has relatively low complexity, requiring mainly language skills and few-shot learning familiarity to craft prompts that elicit good performance from the foundation model. </span><span class="koboSpan" id="kobo.334.2">There are minimal requirements for programming skills and science knowledge.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.335.1">RAG systems </span><a id="_idIndexMarker1668"/><span class="koboSpan" id="kobo.336.1">have moderate complexity, needing software engineering to build the pipeline components like retrievers and integrators. </span><span class="koboSpan" id="kobo.336.2">The complexity rises with advanced RAG configurations and infrastructure, such as complex workflows involving agents and tools, and infrastructure components for monitoring, observability, evaluation, and orchestration.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.337.1">PEFT and full model fine-tuning have the highest complexity. </span><span class="koboSpan" id="kobo.337.2">These require deep expertise in deep learning, NLP, and data science to select training data, write tuning scripts, choose hyperparameters like learning rates, loss functions, etc., and ultimately update the model’s internal representations.</span></p>
<h1 class="heading-1" id="_idParaDest-446"><span class="koboSpan" id="kobo.338.1">Bringing it all together</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.339.1">Having delved into the various technical components separately within the generative AI technical stack, let’s now consolidate them into a unified perspective.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.340.1"><img alt="" role="presentation" src="../Images/B20836_16_08.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.341.1">Figure 16.8: Generative AI tech stack</span></p>
<p class="normal"><span class="koboSpan" id="kobo.342.1">In summary, a generative AI platform is an extension of an ML platform by introducing additional capabilities such as prompt management, input/output filtering, and tools for FM evaluation and RLHF workflows. </span><span class="koboSpan" id="kobo.342.2">To accommodate these enhancements, the ML platform’s pipeline capability will need to include new generative AI workflows. </span><span class="koboSpan" id="kobo.342.3">The new RAG infrastructure will form the foundational backbone of RAG-based LLM applications and will be closely integrated with the underlying generative AI platform.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.343.1">The development of generative AI applications will continue to leverage other core application architecture components, including streaming, batch processing, message queuing, and workflow tools.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.344.1">Although many of the core components will likely possess their unique set of security and governance capabilities, there will be an overarching need for comprehensive end-to-end observability, monitoring, security, and governance for generative AI application development and operation at scale.</span></p>
<h1 class="heading-1" id="_idParaDest-447"><span class="koboSpan" id="kobo.345.1">Considerations for deploying generative AI applications in production</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.346.1">Deploying</span><a id="_idIndexMarker1669"/><span class="koboSpan" id="kobo.347.1"> generative AI applications in production environments introduces a new set of challenges that go beyond the considerations for traditional software and ML deployments. </span><span class="koboSpan" id="kobo.347.2">While aspects such as functional correctness, system/application security, security scan of artifacts such as model files and code, infrastructure scalability, documentation, and operational readiness (e.g., observability, change management, incident management, and audit) remain essential, there are additional factors to consider when deploying generative AI models.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.348.1">The following are some of the key additional considerations when deciding on the production deployment of generative AI applications.</span></p>
<h2 class="heading-2" id="_idParaDest-448"><span class="koboSpan" id="kobo.349.1">Model readiness </span></h2>
<p class="normal"><span class="koboSpan" id="kobo.350.1">When </span><a id="_idIndexMarker1670"/><span class="koboSpan" id="kobo.351.1">deciding whether a generative AI model is ready for production deployment, the focus should be on its accuracy for the target use cases. </span><span class="koboSpan" id="kobo.351.2">These models can solve a wide range of problems, but attempting to test for all possible scenarios and use cases would be an endless endeavor, making it challenging to feel confident in the deployment. </span><span class="koboSpan" id="kobo.351.3">Instead, concentrate on designing the application layer to support only the targeted use cases, simplifying the evaluation process.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.352.1">Additionally, when determining if a performance metric is satisfactory, it’s essential to establish a threshold using existing benchmarks as a baseline. </span><span class="koboSpan" id="kobo.352.2">For example, if the error rate for the </span><a id="_idIndexMarker1671"/><span class="koboSpan" id="kobo.353.1">current process is 20% and deemed acceptable for business operations, then a generative AI application that can achieve the same or lower error rate should be considered capable of delivering at least the same or better value. </span><span class="koboSpan" id="kobo.353.2">By adopting this approach, you can confidently proceed with production deployment.</span></p>
<h2 class="heading-2" id="_idParaDest-449"><span class="koboSpan" id="kobo.354.1">Decision-making workflow</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.355.1">When</span><a id="_idIndexMarker1672"/><span class="koboSpan" id="kobo.356.1"> deploying generative AI applications, it’s crucial to consider whether the system will make automated decisions or involve human oversight. </span><span class="koboSpan" id="kobo.356.2">Due to the potential for hallucinations or inaccuracies in these models, the level of testing and evaluation rigor needs to be determined based on the decision-making flow.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.357.1">If the system is designed to make fully automated decisions, you must assess the risk of incorrect decisions being made. </span><span class="koboSpan" id="kobo.357.2">Is this risk tolerable for your use case? </span><span class="koboSpan" id="kobo.357.3">If so, then automated decision-making can proceed after thorough testing against the target use cases and scenarios. </span><span class="koboSpan" id="kobo.357.4">However, if the potential risk is not tolerable, it’s essential to incorporate human oversight into the decision-making process.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.358.1">In cases where human oversight is required, ensure that the individuals involved are well qualified to make informed decisions with the support of the generative AI application. </span><span class="koboSpan" id="kobo.358.2">The system should be designed to provide recommendations or insights to human decision-makers, who can then apply their expertise and judgment to mitigate potential errors or biases from the AI model.</span></p>
<h2 class="heading-2" id="_idParaDest-450"><span class="koboSpan" id="kobo.359.1">Responsible AI assessment</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.360.1">When it comes to</span><a id="_idIndexMarker1673"/><span class="koboSpan" id="kobo.361.1"> responsible AI considerations, such as bias and harmful content, it’s essential to evaluate them on a case-by-case basis for each specific use case. </span><span class="koboSpan" id="kobo.361.2">Different use cases may have varying tolerances for certain types of language or biases. </span><span class="koboSpan" id="kobo.361.3">For example, some use cases could be more tolerant of certain language patterns, while others may have stricter requirements.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.362.1">Similarly, the degree of bias that is considered acceptable can vary depending on the use case and scenarios involved. </span><span class="koboSpan" id="kobo.362.2">What might be considered an acceptable level of bias for one application may be unacceptable for another. </span><span class="koboSpan" id="kobo.362.3">It’s crucial to assess the potential impact of biases within the context of each use case and determine the appropriate thresholds.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.363.1">Instead </span><a id="_idIndexMarker1674"/><span class="koboSpan" id="kobo.364.1">of applying a one-size-fits-all approach, it’s recommended to conduct a thorough evaluation of bias and harmful content considerations for each specific use case. </span><span class="koboSpan" id="kobo.364.2">This targeted assessment will ensure that the deployed generative AI application aligns with the unique requirements and constraints of that particular use case, minimizing potential risks and ensuring responsible and ethical deployment.</span></p>
<h2 class="heading-2" id="_idParaDest-451"><span class="koboSpan" id="kobo.365.1">Guardrails in production environments</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.366.1">Despite thorough testing and evaluation during the development phase, generative AI models can still exhibit unexpected or undesirable behaviors when deployed in live production environments. </span><span class="koboSpan" id="kobo.366.2">To address this challenge, it is essential to establish a comprehensive set of guardrails within the production environment.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.367.1">At the core of these guardrails are robust input validation systems. </span><span class="koboSpan" id="kobo.367.2">These systems scrutinize the data and prompts fed into the generative AI models, ensuring that only appropriate, safe, and intended inputs are used. </span><span class="koboSpan" id="kobo.367.3">This protects the models from being exposed to potentially harmful or adversarial inputs, which could trigger unpredictable or undesirable outputs.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.368.1">Complementing the input validation, organizations must also develop sophisticated output filtering and moderation systems. </span><span class="koboSpan" id="kobo.368.2">These systems review the generated content before it is released or exposed to end-users, detecting and flagging any outputs that may be biased, offensive, sensitive (PII), or otherwise undesirable. </span><span class="koboSpan" id="kobo.368.3">This allows for timely review and intervention, ensuring that potentially problematic content is addressed before it reaches the public.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.369.1">To enable rapid response and intervention, the monitoring and validation systems should be integrated with automated alerting mechanisms. </span><span class="koboSpan" id="kobo.369.2">These mechanisms quickly notify the appropriate teams of any concerning behaviors or outputs detected by the system. </span><span class="koboSpan" id="kobo.369.3">This allows organizations to act swiftly, addressing issues before they escalate or cause harm.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.370.1">Ultimately, the human remains a crucial safeguard. </span><span class="koboSpan" id="kobo.370.2">Generative AI workflows must maintain the ability for experienced operators to override or intervene when necessary. </span><span class="koboSpan" id="kobo.370.3">This serves as a backstop, allowing knowledgeable personnel to make informed decisions when the models exhibit unpredictable or undesirable behaviors that require immediate attention.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.371.1">If you use Amazon Bedrock, you can consider the built-in Guardrails feature which can detect and filter out undesired topics, harmful content, or PII data.</span></p>
<h2 class="heading-2" id="_idParaDest-452"><span class="koboSpan" id="kobo.372.1">External knowledge change management</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.373.1">For</span><a id="_idIndexMarker1675"/><span class="koboSpan" id="kobo.374.1"> generative AI applications that rely on external knowledge retrieval, such as those based on the RAG architecture, it is crucial to consider the dynamic nature of the underlying knowledge sources. </span><span class="koboSpan" id="kobo.374.2">External knowledge can change over time, and for the same query, there could be multiple relevant answers depending on the recency and timeline of the information.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.375.1">To address this challenge, it is essential to either design prompts that accurately capture the desired temporal context or ensure that the knowledge retriever component is aware of the knowledge lineage and timeline. </span><span class="koboSpan" id="kobo.375.2">This way, the system can retrieve and present the most relevant and up-to-date information in response to a given query.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.376.1">For example, if a query pertains to a standard operating procedure, the procedure itself may evolve over time. </span><span class="koboSpan" id="kobo.376.2">Without considering the timeline, the system might retrieve outdated information, potentially leading to incorrect or irrelevant responses. </span><span class="koboSpan" id="kobo.376.3">By incorporating knowledge lineage or explicitly specifying the desired time frame in the prompt, the system can retrieve the appropriate version of the standard operating procedure that aligns with the intended temporal context.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.377.1">Alternatively, the knowledge retriever component can be designed to maintain and leverage a timeline or versioning system for external knowledge sources. </span><span class="koboSpan" id="kobo.377.2">This would allow the system to automatically retrieve the most recent and relevant information based on the query’s context, without relying solely on prompt engineering.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.378.1">Implementing these considerations is crucial for ensuring the accuracy, relevance, and reliability of generative AI applications that depend on external knowledge sources, especially in domains where information evolves rapidly or where temporal context is critical.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.379.1">The list</span><a id="_idIndexMarker1676"/><span class="koboSpan" id="kobo.380.1"> provided earlier serves as a sample of additional considerations. </span><span class="koboSpan" id="kobo.380.2">Organizations should contemplate additional decision points tailored to their specific needs, industry, and regulatory environment. </span><span class="koboSpan" id="kobo.380.3">It is crucial to carefully assess the scope and use case of generative AI, implementing checks and controls within the defined scope to facilitate efficient deployment decision-making. </span><span class="koboSpan" id="kobo.380.4">As generative AI technology advances, evolving requirements will emerge, necessitating ongoing considerations.</span></p>
<h1 class="heading-1" id="_idParaDest-453"><span class="koboSpan" id="kobo.381.1">Practical generative AI business solutions</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.382.1">In the</span><a id="_idIndexMarker1677"/><span class="koboSpan" id="kobo.383.1"> previous chapter, we talked about the business potential of generative AI and potential use cases in various industries. </span><span class="koboSpan" id="kobo.383.2">We then followed that with a detailed discussion of the lifecycle of a generative project from business use case identification to deployment. </span><span class="koboSpan" id="kobo.383.3">In this chapter, we have covered operational considerations, building enterprise generative AI platforms, and one of the most important architecture patterns for building generative AI applications, RAG.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.384.1">In this section, we will highlight some of the more practical generative AI solution opportunities ready for business adoption in the near term. </span><span class="koboSpan" id="kobo.384.2">While research continues on aspirational applications, prudent enterprises should evaluate proven pilot use cases to drive measurable impact from generative AI’s rapid advances. </span><span class="koboSpan" id="kobo.384.3">With these examples, we will present the recommended approach to identify generative AI opportunities by understanding challenges associated with the specific business workflow within several industries.</span></p>
<h2 class="heading-2" id="_idParaDest-454"><span class="koboSpan" id="kobo.385.1">Generative AI-powered semantic search engine</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.386.1">Enterprise</span><a id="_idIndexMarker1678"/><span class="koboSpan" id="kobo.387.1"> search solutions enable organizations to provide powerful search</span><a id="_idIndexMarker1679"/><span class="koboSpan" id="kobo.388.1"> capabilities for their internal data and content. </span><span class="koboSpan" id="kobo.388.2">Companies in sectors like technology, healthcare, finance, and manufacturing have widely adopted enterprise search platforms to improve information discovery for employees. </span><span class="koboSpan" id="kobo.388.3">These tools index structured databases, intranets, document repositories, emails, and more within an organization. </span><span class="koboSpan" id="kobo.388.4">Users can then quickly find relevant content by searching instead of hunting across siloed systems. </span><span class="koboSpan" id="kobo.388.5">Advanced natural language processing, ML algorithms, and cognitive capabilities enable enterprise search solutions to deliver precise, relevant results.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.389.1">With the</span><a id="_idIndexMarker1680"/><span class="koboSpan" id="kobo.390.1"> arrival of generative AI technologies, especially LLMs, enterprise search can be enhanced to provide an improved user experience, more accurate information, and greater specificity. </span><span class="koboSpan" id="kobo.390.2">For example, instead of simply returning a list of search results using keywords, LLMs can take natural language queries and directly provide the answers or a summarized version of an answer in natural language. </span><span class="koboSpan" id="kobo.390.3">LLMs can help understand </span><a id="_idIndexMarker1681"/><span class="koboSpan" id="kobo.391.1">the user intent and context semantically in the user queries for more relevant information retrieval. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.392.1">LLMs can also take query/response history as part of the context when constructing search queries against the underlying knowledge base. </span><span class="koboSpan" id="kobo.392.2">LLMs can also help rewrite the queries with synonyms, related terms, and rephrases to broaden the search scope. </span><span class="koboSpan" id="kobo.392.3">With LLMs, results can be matched based on meaning, relationship, and concepts rather than just keywords.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.393.1">There are multiple technology options and architecture patterns available to build an enterprise search platform powered by generative AI. </span><span class="koboSpan" id="kobo.393.2">You can choose to build your own semantic search engine using a combination of open-source and commercial components. </span><span class="koboSpan" id="kobo.393.3">Building a semantic search engine largely follows the RAG architecture pattern we have discussed previously. </span><span class="koboSpan" id="kobo.393.4">The following architecture shows a semantic search engine using a combination of AWS-managed services and open-source components.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.394.1"><img alt="A diagram of a chat interface  Description automatically generated" src="../Images/B20836_16_09.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.395.1">Figure 16.9: Semantic search engine on AWS</span></p>
<p class="normal"><span class="koboSpan" id="kobo.396.1">With this </span><a id="_idIndexMarker1682"/><span class="koboSpan" id="kobo.397.1">architecture, Kendra takes care of document ingestion and indexing, semantic search, and ranking. </span><span class="koboSpan" id="kobo.397.2">Amazon OpenSearch can be used to build additional alternative knowledge indexes if needed. </span><span class="koboSpan" id="kobo.397.3">The LLMs model from Bedrock or hosted in SageMaker provide query understanding and response generation via the Amazon Lex chat interface.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.398.1">Although </span><a id="_idIndexMarker1683"/><span class="koboSpan" id="kobo.399.1">generative AI significantly improves the enterprise search experience, it is essential to acknowledge its limitations. </span><span class="koboSpan" id="kobo.399.2">In contrast to traditional keyword or semantic search methods, generative AI occasionally yields results that are irrelevant or off-topic, lacking the precision characteristic of conventional approaches. </span><span class="koboSpan" id="kobo.399.3">Moreover, it presents consistency challenges, generating different outputs for the same or similar inputs due to variations in its interpretation of instructions over time and the accuracy of the index retrievers. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.400.1">Additionally, there is the potential for privacy concerns, as generative AI might inadvertently disclose sensitive information, raising issues related to privacy violations if sensitive datasets such as PII/PHI are not masked in the model training/tuning process.</span></p>
<h2 class="heading-2" id="_idParaDest-455"><span class="koboSpan" id="kobo.401.1">Financial data analysis and research workflow</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.402.1">Financial analysts across</span><a id="_idIndexMarker1684"/><span class="koboSpan" id="kobo.403.1"> banking, asset management, and other domains rely heavily on analyzing and synthesizing data to deliver insights. </span><span class="koboSpan" id="kobo.403.2">For example, an investment </span><a id="_idIndexMarker1685"/><span class="koboSpan" id="kobo.404.1">bank analyst might gather information from earnings reports, news, filings, and research coverage to extract details on financials, business outlooks, and corporate actions. </span><span class="koboSpan" id="kobo.404.2">The analyst then performs a comparative valuation analysis, forecasts growth and returns, and advises an investment strategy. </span><span class="koboSpan" id="kobo.404.3">This requires manually crunching numbers, modeling scenarios, and creating reports to communicate findings. </span><span class="koboSpan" id="kobo.404.4">With so much reading, data gathering, and analysis required, the process is often tedious and time-consuming.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.405.1">Generative AI capabilities like natural language processing, data extraction, summarization, and text generation have shown promise in augmenting analysts’ workflows. </span><span class="koboSpan" id="kobo.405.2">Generative AI-powered assistants that automate data aggregation, run comparative analytics, and draft reports could amplify analyst productivity multifold. </span><span class="koboSpan" id="kobo.405.3">The following are several areas in the workflow where generative AI can be applied:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.406.1">Data extraction</span></strong><span class="koboSpan" id="kobo.407.1">: Generative AI</span><a id="_idIndexMarker1686"/><span class="koboSpan" id="kobo.408.1"> provides new capabilities to automatically extract structured data from unstructured documents and output it in usable formats. </span><span class="koboSpan" id="kobo.408.2">Traditional NLP techniques have enabled entity extraction and relation mapping to some extent. </span><span class="koboSpan" id="kobo.408.3">However, LLMs now achieve superior performance in accurately identifying key entities, relationships, and data points in texts. </span><span class="koboSpan" id="kobo.408.4">These models can parse details like financial figures, corporate actions, and business events from earnings reports, filings, news, and other sources. </span><span class="koboSpan" id="kobo.408.5">The extracted data can then be formatted for seamless loading into workflows like Excel financial models and PowerPoint presentations for further analysis. </span><span class="koboSpan" id="kobo.408.6">This alleviates tedious manual data entry and copying for analysts. </span><span class="koboSpan" id="kobo.408.7">With higher accuracy at directly generating structured outputs, generative AI can integrate deep unstructured data understanding into downstream systems. </span><span class="koboSpan" id="kobo.408.8">This fills a major gap in leveraging textual data like reports and articles in quantitative finance workflows.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.409.1">Document QA</span></strong><span class="koboSpan" id="kobo.410.1">: Generative AI </span><a id="_idIndexMarker1687"/><span class="koboSpan" id="kobo.411.1">models enable users to ask freeform questions in natural language to extract additional insights from documents. </span><span class="koboSpan" id="kobo.411.2">For example, an analyst could query, “What were the key revenue drivers last quarter?” </span><span class="koboSpan" id="kobo.411.3">and the model would comprehend the underlying earnings transcript to summarize the major growth factors concisely in a generated response. </span><span class="koboSpan" id="kobo.411.4">Such ad hoc queries allow flexibly extracting only the most relevant points instead of processing the full document. </span><span class="koboSpan" id="kobo.411.5">The model would focus on areas related to the question and ignore superfluous text. </span><span class="koboSpan" id="kobo.411.6">By generating condensed, tailored answers to natural language questions, generative AI provides a powerful capability to slice and dice documents on demand. </span><span class="koboSpan" id="kobo.411.7">Analysts can dynamically explore and analyze long reports by conversing with the AI in plain language to uncover relevant facts, relationships, and conclusions.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.412.1">Enterprise search and data query against internal data sources</span></strong><span class="koboSpan" id="kobo.413.1">: Conversational</span><a id="_idIndexMarker1688"/><span class="koboSpan" id="kobo.414.1"> interfaces powered by generative AI can enable financial analysts to gather data through natural dialog. </span><span class="koboSpan" id="kobo.414.2">Instead of needing to navigate disparate systems and remember specific query languages, analysts </span><a id="_idIndexMarker1689"/><span class="koboSpan" id="kobo.415.1">could simply ask questions in plain language. </span><span class="koboSpan" id="kobo.415.2">For example, “Get me the 3-year sales growth by region from the EMEA market database.” </span><span class="koboSpan" id="kobo.415.3">The model would interpret the intent, translate the required queries for each data source, gather the results, and </span><a id="_idIndexMarker1690"/><span class="koboSpan" id="kobo.416.1">summarize them in a readable format for the analyst. </span><span class="koboSpan" id="kobo.416.2">This conversational ability to retrieve cross-system data on demand has the potential to unify access and accelerate insights. </span><span class="koboSpan" id="kobo.416.3">Analysts could explore connections across internal document stores, financial databases, knowledge bases, and search engines using everyday language.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.417.1">Financial analysis and report generation</span></strong><span class="koboSpan" id="kobo.418.1">: Generative AI enables financial analysts to directly</span><a id="_idIndexMarker1691"/><span class="koboSpan" id="kobo.419.1"> request certain analytical tasks using natural language instructions. </span><span class="koboSpan" id="kobo.419.2">For example, an analyst could ask the model, “Compare the 5-year revenue growth and profitability margins for the top 5 companies in the industry.” </span><span class="koboSpan" id="kobo.419.3">The model would then extract relevant financial figures, compute required ratios, generate suitable visualizations, and summarize key</span><a id="_idIndexMarker1692"/><span class="koboSpan" id="kobo.420.1"> takeaways in an output report. </span><span class="koboSpan" id="kobo.420.2">Where required, it could seamlessly leverage external tools and APIs to augment its analysis. </span><span class="koboSpan" id="kobo.420.3">Unlike rigid commands, natural instructions allow analysts to specify bespoke analysis on demand. </span><span class="koboSpan" id="kobo.420.4">By automating data gathering, financial modeling, and report generation, while coordinating external services, generative AI can dramatically amplify an analyst’s productivity.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.421.1">The following example prompt can help provide a financial analysis across a number of financial dimensions:</span></p>
<pre class="programlisting code"><code class="hljs-code"><code class="codeHighlighted" style="font-weight: bold;"><span class="koboSpan" id="kobo.422.1">Action</span></code><span class="koboSpan" id="kobo.423.1">: Analyze financial reports [from companies X, Y, Z] between 2020-2022 to identify key trends, growth and risk areas.
</span><code class="codeHighlighted" style="font-weight: bold;"><span class="koboSpan" id="kobo.424.1">Context</span></code><span class="koboSpan" id="kobo.425.1">: These companies operate in [industry]. </span><span class="koboSpan" id="kobo.425.2">Focus on major changes in financial KPIs like revenue, costs, profits, debt. </span><span class="koboSpan" id="kobo.425.3">Call out important shifts across business segments, geographies, products. </span><span class="koboSpan" id="kobo.425.4">Highlight growth opportunities but also flag potential risks.
</span><code class="codeHighlighted" style="font-weight: bold;"><span class="koboSpan" id="kobo.426.1">Input Data</span></code><span class="koboSpan" id="kobo.427.1">: [text from actual company financial filings]
</span><code class="codeHighlighted" style="font-weight: bold;"><span class="koboSpan" id="kobo.428.1">Output</span></code><span class="koboSpan" id="kobo.429.1">:
</span><code class="codeHighlighted" style="font-weight: bold;"><span class="koboSpan" id="kobo.430.1">Revenue trend</span></code><span class="koboSpan" id="kobo.431.1">:
</span><code class="codeHighlighted" style="font-weight: bold;"><span class="koboSpan" id="kobo.432.1">Cost trend</span></code><span class="koboSpan" id="kobo.433.1">:
</span><code class="codeHighlighted" style="font-weight: bold;"><span class="koboSpan" id="kobo.434.1">Profitability trend</span></code><span class="koboSpan" id="kobo.435.1">:
</span><code class="codeHighlighted" style="font-weight: bold;"><span class="koboSpan" id="kobo.436.1">Growth areas</span></code><span class="koboSpan" id="kobo.437.1">:
</span><code class="codeHighlighted" style="font-weight: bold;"><span class="koboSpan" id="kobo.438.1">Decline areas</span></code><span class="koboSpan" id="kobo.439.1">:
</span><code class="codeHighlighted" style="font-weight: bold;"><span class="koboSpan" id="kobo.440.1">Major risks</span></code><span class="koboSpan" id="kobo.441.1">:
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.442.1">Architecturally, building generative AI-powered financial analysis and research solutions is mainly based on the RAG architecture and principles. </span><span class="koboSpan" id="kobo.442.2">The following diagram illustrates a conceptual application architecture for such an application.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.443.1"><img alt="A diagram of data lake  Description automatically generated" src="../Images/B20836_16_10.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.444.1">Figure 16.10: Generative AI-powered financial analysis application</span></p>
<p class="normal"><span class="koboSpan" id="kobo.445.1">Finance is a highly precise science and business domain, and as such generative AI solutions for financial analysis require stringent accuracy and factual grounding. </span><span class="koboSpan" id="kobo.445.2">To limit hallucination risks in LLMs, techniques beyond prompt engineering and fine-tuning are necessary. </span><span class="koboSpan" id="kobo.445.3">Advanced information retrieval and embedding approaches can enhance output relevancy and correctness.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.446.1">For example, instead </span><a id="_idIndexMarker1693"/><span class="koboSpan" id="kobo.447.1">of retrieving information just once and then generating the response for a query, the system can implement multi-hop information retrieval by predicting the related queries and retrieving other relevant information for a more complete context. </span><span class="koboSpan" id="kobo.447.2">From an embedding perspective, instead of simply chunking up documents and creating embeddings for the chunks, additional structural, semantic, and domain meta can be combined to create enriched embeddings. </span><span class="koboSpan" id="kobo.447.3">On the retrieval end, instead of returning the top matching fragments as is from a vector DB, a re-ranker can be implemented to post-process the outputs based on unique requirements. </span><span class="koboSpan" id="kobo.447.4">Comprehensive evaluation techniques and processes also need to be implemented to establish high confidence in the system. </span><span class="koboSpan" id="kobo.447.5">Where possible, implement a facts validator to validate responses against known facts.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.448.1">Although advanced LLMs and techniques have showcased remarkable capabilities in automating or aiding various facets of financial analysis tasks, it remains premature to depend solely on LLMs for intricate financial analysis tasks. </span><span class="koboSpan" id="kobo.448.2">The precision demanded in financial decision-making necessitates accurate information, as any inaccuracies in LLM responses could result in substantial negative consequences.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.449.1">The</span><a id="_idIndexMarker1694"/><span class="koboSpan" id="kobo.450.1"> financial services industry as a whole has been actively embracing and implementing generative AI technology to achieve diverse business objectives. </span><span class="koboSpan" id="kobo.450.2">This widespread adoption has the potential to significantly impact various financial functions, ranging from financial analysis and combating financial crimes to the development of new business models, products, and enhanced customer experiences. </span><span class="koboSpan" id="kobo.450.3">However, this increasing trend also gives rise to concerns, particularly in areas such as risk management and transparency. </span><span class="koboSpan" id="kobo.450.4">For instance, in financial analysis, maintaining transparency is crucial for regulatory compliance and effective risk management. </span><span class="koboSpan" id="kobo.450.5">The inherent opacity of LLMs may pose challenges in meeting these regulatory requirements. </span><span class="koboSpan" id="kobo.450.6">Additionally, the demand for explanations in financial decision-making could be a potential hurdle, as comprehending the decision-making process of LLMs may prove challenging.</span></p>
<h2 class="heading-2" id="_idParaDest-456"><span class="koboSpan" id="kobo.451.1">Clinical trial recruiting workflow</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.452.1">Clinical trials </span><a id="_idIndexMarker1695"/><span class="koboSpan" id="kobo.453.1">are lengthy research processes that test new medical treatments like drugs, devices, or interventions on human subjects. </span><span class="koboSpan" id="kobo.453.2">Clinical trials progress through different phases to evaluate a new medical treatment, starting with safety in smaller groups before expanding to measure efficacy and comparisons. </span><span class="koboSpan" id="kobo.453.3">In addition, patient recruiting is an important process in all phases of a clinical trial. </span><span class="koboSpan" id="kobo.453.4">Let’s look at these phases in more detail:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.454.1">In Phase 1 trials, the treatment is given to fewer than 100 people to assess safety and side effects. </span><span class="koboSpan" id="kobo.454.2">Since it focuses on a smaller, healthy group, recruiting patients at this stage is less complex.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.455.1">Phase 2 trials administer the treatment to several hundred participants with the target condition. </span><span class="koboSpan" id="kobo.455.2">Here, researchers continue collecting safety data while gathering preliminary efficacy information. </span><span class="koboSpan" id="kobo.455.3">Recruiting becomes more difficult as patients need to have the specific condition.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.456.1">In Phase 3, the trial expands to 300-3000 participants to further understand safety, efficacy, dosages, and how it compares to existing treatments. </span><span class="koboSpan" id="kobo.456.2">The much larger sample size covering diverse demographics makes recruiting extremely challenging at this advanced stage.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.457.1">Phase 4 trials monitor the approved treatment in broad real-world populations to gather additional long-term safety and efficacy data. </span><span class="koboSpan" id="kobo.457.2">Recruiting for Phase 4 can also be challenging as criteria tend to be more expansive.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.458.1">During recruitment, clinical research coordinators meticulously screen patient medical history across various systems like electronic health records to check if criteria like health status, past conditions, medications, demographics, etc. </span><span class="koboSpan" id="kobo.458.2">match the trial’s eligibility requirements. </span><span class="koboSpan" id="kobo.458.3">This manual and repetitive process of collating data from multiple sources to identify matches is a major bottleneck.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.459.1">Generative AI</span><a id="_idIndexMarker1696"/><span class="koboSpan" id="kobo.460.1"> could automate and accelerate screening by intelligently querying patient EHR data against complex inclusion/exclusion logic specified in natural language. </span><span class="koboSpan" id="kobo.460.2">As EHRs contain comprehensive histories including diagnoses, medications, procedures, and test results, the models can parse criteria and rapidly filter candidates. </span><span class="koboSpan" id="kobo.460.3">For example, a researcher can directly ask the generative AI platform to compare a patient’s EHR records with the inclusion/exclusion criteria to determine if the patient is a match. </span><span class="koboSpan" id="kobo.460.4">Generative AI can also help synthesize and generate a summarized report about the overall selected cohort population and individual patients to help with human review. </span><span class="koboSpan" id="kobo.460.5">This can significantly speed up finding eligible patients for different trial phases.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.461.1">Additionally, combining generative capabilities with predictive analytics can optimize trial performance. </span><span class="koboSpan" id="kobo.461.2">The combined capabilities can forecast enrollment rates, monitor site progress, and trigger recommended interventions when delays or issues occur by generating insights from patient and site data.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.462.1">The following diagram shows a conceptual flow of applying generative AI and traditional predictive analytics to optimize the various tasks within clinical trials:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.463.1"><img alt="A diagram of a computer  Description automatically generated" src="../Images/B20836_16_11.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.464.1">Figure 16.11: Clinical trial optimization</span></p>
<p class="normal"><span class="koboSpan" id="kobo.465.1">The</span><a id="_idIndexMarker1697"/><span class="koboSpan" id="kobo.466.1"> following is an example command for patient search against medical records using a list of inclusion and exclusion criteria:</span></p>
<pre class="programlisting code"><code class="hljs-code"><code class="codeHighlighted" style="font-weight: bold;"><span class="koboSpan" id="kobo.467.1">Action</span></code><span class="koboSpan" id="kobo.468.1">: Search de-identified electronic health records to identify patients meeting the following criteria:
</span><code class="codeHighlighted" style="font-weight: bold;"><span class="koboSpan" id="kobo.469.1">Inclusion Criteria</span></code><span class="koboSpan" id="kobo.470.1">:
</span><code class="inlineCode"><span class="koboSpan" id="kobo.471.1">Age between 40-60</span></code>
<code class="inlineCode"><span class="koboSpan" id="kobo.472.1">Diagnosis of hypertension (ICD-10 code I10)</span></code>
<code class="inlineCode"><span class="koboSpan" id="kobo.473.1">Prescribed beta blockers in 2022</span></code>
<code class="codeHighlighted" style="font-weight: bold;"><span class="koboSpan" id="kobo.474.1">Exclusion Criteria</span></code><span class="koboSpan" id="kobo.475.1">:
</span><code class="inlineCode"><span class="koboSpan" id="kobo.476.1">History of heart failure</span></code>
<code class="inlineCode"><span class="koboSpan" id="kobo.477.1">Currently hospitalized</span></code>
</code></pre>
<p class="normal"><span class="koboSpan" id="kobo.478.1">Behind the scenes of the command, an agent can facilitate the search of patient records against the semantic search engine of EHR records and generate a response with a list of matching patients.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.479.1">While generative AI has the potential to automate and accelerate patient recruitment for clinical trials, it is crucial to keep qualified humans in the loop throughout the process. </span><span class="koboSpan" id="kobo.479.2">AI systems alone cannot fully validate that trial participants meet the complex eligibility criteria, which often involves interpreting medical histories, test results, prior conditions, and more. </span><span class="koboSpan" id="kobo.479.3">Humans with clinical expertise need to review participant profiles surfaced by AI to catch any inaccurate assessments of eligibility and prevent improper enrollment.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.480.1">In addition, human oversight is required to ensure AI-assisted recruiting adheres to ethical guidelines </span><a id="_idIndexMarker1698"/><span class="koboSpan" id="kobo.481.1">around transparency, fairness, and avoiding undue influence. </span><span class="koboSpan" id="kobo.481.2">Participants should comprehend why they were targeted and how their data is used. </span><span class="koboSpan" id="kobo.481.3">Automated processes could lead to opaque and biased recruiting without checks against discrimination. </span><span class="koboSpan" id="kobo.481.4">Experienced clinical research staff need to steward participant interactions to uphold understandability, equitability, and medical appropriateness.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.482.1">Lastly, human involvement lends necessary nuance and discretion on a case-by-case basis. </span><span class="koboSpan" id="kobo.482.2">Factors like availability, transportation needs, and personal situations must be weighed. </span><span class="koboSpan" id="kobo.482.3">AI models alone lack the empathy and adaptability needed. </span><span class="koboSpan" id="kobo.482.4">The clinical trial process ultimately deals with human lives, so the compassion and experience of human recruiters remain indispensable when augmented by AI efficiency gains.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.483.1">In conclusion, generative AI’s ability to deeply understand criteria, reason across data sources, and generate matches and recommendations offers immense potential to transform the protracted patient recruitment process to help advance critical medical research. </span><span class="koboSpan" id="kobo.483.2">It is also important to acknowledge its limitations, such as hallucination, opaqueness, and privacy concerns such as the handling of PII/PHI data. </span><span class="koboSpan" id="kobo.483.3">Furthermore, the regulatory environment around the adoption of generative AI for clinical trials remains unclear, which poses challenges in the adoption of generative AI in this domain.</span></p>
<h2 class="heading-2" id="_idParaDest-457"><span class="koboSpan" id="kobo.484.1">Media entertainment content creation workflow</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.485.1">The</span><a id="_idIndexMarker1699"/><span class="koboSpan" id="kobo.486.1"> content creation process in the media and entertainment industry encompasses multiple stages, spanning from idea generation and scriptwriting to casting, production, post-production editing, sound design, graphics and animation, distribution, marketing, and monetization. </span><span class="koboSpan" id="kobo.486.2">However, this process is accompanied by various challenges that can impact the success of a project. </span><span class="koboSpan" id="kobo.486.3">These challenges include the need to generate unique and resonating ideas, crafting compelling and coherent storylines, ensuring that visual representations effectively convey intended emotions, the time-consuming nature of post-production editing, and the difficulty in selecting or creating </span><a id="_idIndexMarker1700"/><span class="koboSpan" id="kobo.487.1">suitable music and sounds. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.488.1">Additionally, specialized skills are often required for animation and visual effects, and breaking through the competitive market with effective marketing strategies can be a daunting task.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.489.1">Generative AI presents a promising </span><a id="_idIndexMarker1701"/><span class="koboSpan" id="kobo.490.1">solution to address these challenges in practical ways:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.491.1">Script generation</span></strong><span class="koboSpan" id="kobo.492.1">: Generative AI can be used to generate movie or TV show scripts based on human-provided ideas and inputs.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.493.1">Storyboarding</span></strong><span class="koboSpan" id="kobo.494.1">: Generative AI can propose visual representations aligned with intended themes and emotions from the scripts.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.495.1">Production</span></strong><span class="koboSpan" id="kobo.496.1">: Generative AI can be used to generate images and photos needed on film sets.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.497.1">Post-production editing</span></strong><span class="koboSpan" id="kobo.498.1">: Generative AI can help automate post-production editing processes through techniques like text-guided editing. </span><span class="koboSpan" id="kobo.498.2">For example, you can use generative to create special effects, and for editing scenes and images.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.499.1">Media asset search</span></strong><span class="koboSpan" id="kobo.500.1">: Generative AI can help enhance media content search through advanced tagging and semantic matching.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.501.1">Marketing and promotion</span></strong><span class="koboSpan" id="kobo.502.1">: Generative AI can generate compelling marketing messages and visuals for promotional campaigns. </span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.503.1">Engagement</span></strong><span class="koboSpan" id="kobo.504.1">: Generative AI can enhance user engagement experience through personalization and recommendations.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.505.1">The following diagram shows where generative AI can be applied throughout the media lifecycle:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.506.1"><img alt="A diagram of a marketing process  Description automatically generated" src="../Images/B20836_16_12.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.507.1">Figure 16.12: Media content development and distribution flow</span></p>
<p class="normal"><span class="koboSpan" id="kobo.508.1">The </span><a id="_idIndexMarker1702"/><span class="koboSpan" id="kobo.509.1">following is an example of a prompt to generate a movie script:</span></p>
<pre class="programlisting code"><code class="hljs-code"><code class="codeHighlighted" style="font-weight: bold;"><span class="koboSpan" id="kobo.510.1">Action</span></code><span class="koboSpan" id="kobo.511.1">: Write a script for a compelling Law &amp; Order episode in a classic procedural drama style.
</span><code class="codeHighlighted" style="font-weight: bold;"><span class="koboSpan" id="kobo.512.1">Context</span></code><span class="koboSpan" id="kobo.513.1">: This is for Law &amp; Order's flagship show set in New York City following NYPD detectives investigating crimes and the prosecutors who ultimately try the cases. </span><span class="koboSpan" id="kobo.513.2">Base the script on fictional characters and events. </span><span class="koboSpan" id="kobo.513.3">Focus the plot on an intriguing murder mystery that explores moral dilemmas around crime and justice. </span><span class="koboSpan" id="kobo.513.4">Align the tone and dialogue with the show's dramatic yet grounded writing style. </span><span class="koboSpan" id="kobo.513.5">Structure the script in a three-act format with an exciting opening scene, plot twists, and a suspenseful climax leading to a just resolution. </span><span class="koboSpan" id="kobo.513.6">Describe the settings and character actions vividly. </span><span class="koboSpan" id="kobo.513.7">Follow standard screenwriting conventions and formatting.
</span><code class="codeHighlighted" style="font-weight: bold;"><span class="koboSpan" id="kobo.514.1">Script Details</span></code><span class="koboSpan" id="kobo.515.1">:
Approximately 50 scenes
Opening scene introduces the victim and mystery
Detectives pursue leads through witty interrogations
Prosecutors debate legal strategy and ethics
Red herrings before revealing the true perpetrator
Thoughtful closing arguments and courtroom drama
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.516.1">If you run the preceding prompt against ChatGPT, you could get something like the following:</span></p>
<pre class="programlisting code"><code class="hljs-code"><code class="codeHighlighted" style="font-weight: bold;"><span class="koboSpan" id="kobo.517.1">Title</span></code><span class="koboSpan" id="kobo.518.1">: Law &amp; Order Episode: "Shadows of Deceit"
INT. </span><span class="koboSpan" id="kobo.518.2">DARK ALLEY - NIGHT
Rain pours down on a desolate New York City alley. </span><span class="koboSpan" id="kobo.518.3">A shadowy figure lurks in the darkness, holding a gleaming knife. </span><span class="koboSpan" id="kobo.518.4">JASON HARPER (late 30s, rugged detective) and LUCY MARTIN (early 30s, sharp-witted detective) arrive at the crime scene. </span><span class="koboSpan" id="kobo.518.5">Flashing lights and uniformed officers create an eerie atmosphere.
</span><span class="koboSpan" id="kobo.518.6">EXT. </span><span class="koboSpan" id="kobo.518.7">DARK ALLEY - CONTINUOUS
They discover the lifeless body of SARAH WESTON (early 40s, elegant, with a secret), dressed in a pristine white gown, her face frozen in fear.
</span><span class="koboSpan" id="kobo.518.8">JASON
 (whispering) What kind of monster does this?
</span><span class="koboSpan" id="kobo.518.9">INT. </span><span class="koboSpan" id="kobo.518.10">POLICE STATION - INTERROGATION ROOM - MORNING
Jason and Lucy interrogate EDWARD KINGSLEY (50s, wealthy businessman), the victim's secret lover.
</span><span class="koboSpan" id="kobo.518.11">EDWARD
 (sweating) I didn't kill her! </span><span class="koboSpan" id="kobo.518.12">I loved her!
</span><span class="koboSpan" id="kobo.518.13">….
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.519.1">If</span><a id="_idIndexMarker1703"/><span class="koboSpan" id="kobo.520.1"> we want to create visual representations for some of the scenes from the script, we can employ a text-to-image model. </span><span class="koboSpan" id="kobo.520.2">For instance, we can provide the following input to a Stable Diffusion model:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.521.1">Rain pours down on a desolate New York City alley. </span><span class="koboSpan" id="kobo.521.2">A shadowy figure lurks in the darkness, holding a gleaming knife.
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.522.1">The resulting image would resemble the image depicted here:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.523.1"><img alt="" role="presentation" src="../Images/B20836_16_13.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.524.1">Figure 16.13: Storyboarding using text-to-image model</span></p>
<p class="normal"><span class="koboSpan" id="kobo.525.1">It’s</span><a id="_idIndexMarker1704"/><span class="koboSpan" id="kobo.526.1"> still early days for generative AI-created entertainment, but it is already clear that the tremendous opportunities have attracted many companies to build generative AI tools for media use cases such as storyline generation tools. </span><span class="koboSpan" id="kobo.526.2">A generative AI video editing tool from Runway, a company that builds video editing tools, has been used for post-production editing such as in </span><em class="italic"><span class="koboSpan" id="kobo.527.1">Everything, Everywhere, All At Once</span></em><span class="koboSpan" id="kobo.528.1">. </span><span class="koboSpan" id="kobo.528.2">However, this type of tool raises questions about intellectual property. </span><span class="koboSpan" id="kobo.528.3">If AI creates a new character influenced by a well-known figure, who owns the copywrite? </span><span class="koboSpan" id="kobo.528.4">There is also concern about the social impact that this technology will have on creative professionals such as animators, scriptwriters, and visual artists. </span><span class="koboSpan" id="kobo.528.5">Some generative AI technologies have taken a more cautious approach to creative content generation. </span><span class="koboSpan" id="kobo.528.6">For example, the Claude chatbot tool from Anthropic will block the generation of full-length movie scripts to avoid potential negative consequences.</span></p>
<h2 class="heading-2" id="_idParaDest-458"><span class="koboSpan" id="kobo.529.1">Car design workflow</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.530.1">Car design plays </span><a id="_idIndexMarker1705"/><span class="koboSpan" id="kobo.531.1">an essential role in shaping a high-quality automobile. </span><span class="koboSpan" id="kobo.531.2">It encompasses three key domains: exterior design, interior design, and color and trim design. </span><span class="koboSpan" id="kobo.531.3">The design team responsible for the exterior of the vehicle develops the proportions, shape, and surface details of the vehicle. </span><span class="koboSpan" id="kobo.531.4">The interior designer develops the proportions, shape, placement, and surfaces for the instrument panel, seats, door trim panels, headliner, pillar trims, etc. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.532.1">Here, the emphasis is on ergonomics and the comfort of the passengers. </span><span class="koboSpan" id="kobo.532.2">Lastly, the trim designer is responsible for the research, design, and development of all interior and exterior colors and materials used on a vehicle.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.533.1">The design development process for exterior and interior design starts with manual sketches and digital drawings, which form the foundation of concept development. </span><span class="koboSpan" id="kobo.533.2">These sketches and drawings undergo rigorous review and approval processes within various layers of management. </span><span class="koboSpan" id="kobo.533.3">Subsequently, the concept is rendered into a digital format using</span><a id="_idIndexMarker1706"/><span class="koboSpan" id="kobo.534.1"> a </span><strong class="keyWord"><span class="koboSpan" id="kobo.535.1">computer-aided styling</span></strong><span class="koboSpan" id="kobo.536.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.537.1">CAS</span></strong><span class="koboSpan" id="kobo.538.1">) tool to further refine the style, and the design is transformed into vivid images. </span><span class="koboSpan" id="kobo.538.2">After that, industrial plasticine or clay modeling is developed from the images. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.539.1">During the design process, it is imperative to maintain alignment with the designer’s stylistic vision while adhering to stringent criteria encompassing performance, manufacturability, and safety regulations. </span><span class="koboSpan" id="kobo.539.2">This requires product engineering to work concurrently with the designer to ensure the styling is grounded in various engineering constraints.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.540.1">One area generative AI can play a role is concept development. </span><span class="koboSpan" id="kobo.540.2">Designers can use text-guided prompts, using keywords such as “pronounced spoiler,” “futuristic,” or “aerodynamics,” to swiftly generate car design concepts. </span><span class="koboSpan" id="kobo.540.3">General-purpose text-to-image models such as Stable Diffusion, Imagen, Amazon Titan Image Generator and DALLE-2 models have demonstrated immense potential in text-guided concept design. </span><span class="koboSpan" id="kobo.540.4">For example, the following prompt example will generate a car exterior concept:</span></p>
<pre class="programlisting code"><code class="hljs-code"><code class="codeHighlighted" style="font-weight: bold;"><span class="koboSpan" id="kobo.541.1">Action</span></code><span class="koboSpan" id="kobo.542.1">: Generate a 3D rendering of a futuristic electric sports car exterior concept emphasizing aerodynamics and aggressive styling.
</span><code class="codeHighlighted" style="font-weight: bold;"><span class="koboSpan" id="kobo.543.1">Context</span></code><span class="koboSpan" id="kobo.544.1">: This is an electric sports car aimed at the premium market. </span><span class="koboSpan" id="kobo.544.2">Focus on creating an extremely aerodynamic shape with sweeping curves and sharp angles. </span><span class="koboSpan" id="kobo.544.3">Make it appear powerful, nimble, and high-tech. </span><span class="koboSpan" id="kobo.544.4">Incorporate design elements that maximize battery range by reducing drag and turbulence. </span><span class="koboSpan" id="kobo.544.5">The styling should feel bold, exotic, and heavily influenced by fighter jets. </span><span class="koboSpan" id="kobo.544.6">Render the concept in a three-quarters front perspective in vibrant red. </span><span class="koboSpan" id="kobo.544.7">Add dramatic studio lighting to accentuate the design.
</span><code class="codeHighlighted" style="font-weight: bold;"><span class="koboSpan" id="kobo.545.1">Additional Details</span></code><span class="koboSpan" id="kobo.546.1">:
2-door coupe body style
Emphasize the wide and low visual stance
Large air intakes for brake cooling
Futuristic looking wheels
LED accent lighting
Closed grille since it's an EV
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.547.1">The</span><a id="_idIndexMarker1707"/><span class="koboSpan" id="kobo.548.1"> following is one output variation when running the prompt using the Stable Diffusion model from Stability AI:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.549.1"><img alt="A blue sports car on a road  Description automatically generated" src="../Images/B20836_16_14.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.550.1">Figure 16.14: Sports car concept design using generative AI</span></p>
<p class="normal"><span class="koboSpan" id="kobo.551.1">It is important to note, however, that these tools can only provide a source of style inspiration and do not address complex engineering and safety considerations integral to actual car design for production. </span><span class="koboSpan" id="kobo.551.2">These models need to be enhanced to support image generation while also optimizing specific engineering constraints.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.552.1">Progress </span><a id="_idIndexMarker1708"/><span class="koboSpan" id="kobo.553.1">has been made in this area, with examples like drag-guided diffusion models that can render creative car concepts while minimizing drag. </span><span class="koboSpan" id="kobo.553.2">Technically, these techniques aim to minimize an auxiliary loss function tied to a particular constraint, such as drag, during the model training and image generation process. </span><span class="koboSpan" id="kobo.553.3">As a result, when a car design image is generated, it also aligns with optimized constraints, such as minimizing the drag value.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.554.1"><img alt="A diagram of a car  Description automatically generated" src="../Images/B20836_16_15.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.555.1">Figure 16.15: Generative AI-powered car design flow</span></p>
<p class="normal"><span class="koboSpan" id="kobo.556.1">Such tools have the potential to greatly enhance the productivity of both car designers and product engineers, enabling them to avoid investing time in impractical car design concepts that cannot be feasibly produced for the market.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.557.1">The adoption of generative AI in the automotive industry has significant implications across various aspects of the sector. </span><span class="koboSpan" id="kobo.557.2">Generative AI has the potential to revolutionize vehicle design, manufacturing processes, and overall operational efficiency. </span><span class="koboSpan" id="kobo.557.3">It can play a crucial role in the development of autonomous vehicles, enhancing their perception, decision-making, and response capabilities. </span><span class="koboSpan" id="kobo.557.4">However, the adoption of generative AI in the automotive industry also raises challenges and considerations. </span><span class="koboSpan" id="kobo.557.5">Safety and security concerns, ethical considerations related to decision-making algorithms, and regulatory compliance </span><a id="_idIndexMarker1709"/><span class="koboSpan" id="kobo.558.1">are critical aspects that need careful attention. </span><span class="koboSpan" id="kobo.558.2">Striking the right balance between innovation and responsible use of AI is crucial for the successful and sustainable integration of generative AI in the automotive sector.</span></p>
<h2 class="heading-2" id="_idParaDest-459"><span class="koboSpan" id="kobo.559.1">Contact center customer service operation</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.560.1">Generative AI </span><a id="_idIndexMarker1710"/><span class="koboSpan" id="kobo.561.1">has the potential to revolutionize the entire customer operations function, improving the customer experience and agent productivity through digital self-service and enhancing and augmenting agent skills. </span><span class="koboSpan" id="kobo.561.2">The technology has already gained traction in customer service because of its ability to automate interactions with customers using natural language. </span><span class="koboSpan" id="kobo.561.3">Contact centers are critical customer service operations across sectors like finance, telecom, and healthcare. </span><span class="koboSpan" id="kobo.561.4">Key responsibilities include workforce staffing, performance monitoring, agent training, and customer engagement.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.562.1">However, several challenges plague contact center workflows – high call volumes, agent attrition, inconsistent service quality, meeting customer speed and personalization expectations, and agent burnout. </span><span class="koboSpan" id="kobo.562.2">Multilingual support and extracting insights from customer feedback add complexity.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.563.1">Generative AI can address many of these challenges:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.564.1">For agent training, models can synthesize guided learning content from call transcripts and history. </span><span class="koboSpan" id="kobo.564.2">Generative AI can enhance quality assurance and coaching by gathering insights from customer conversations, determining what could be done better, and coaching agents.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.565.1">During customer interactions, AI assistants provide agents with real-time recommendations and answers to boost resolution rates. </span><span class="koboSpan" id="kobo.565.2">For example, generative AI can instantly retrieve data a company has on a specific customer, which can help a human customer service representative more successfully answer questions and resolve issues during an initial interaction. </span><span class="koboSpan" id="kobo.565.3">Generative AI can cut the time a human sales representative spends responding to a customer by providing assistance in real time and recommending the next steps.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.566.1">For self-service, generative AI-fueled chatbots can give immediate and personalized responses to complex customer inquiries regardless of the language or location of the customer. </span><span class="koboSpan" id="kobo.566.2">By improving the quality and effectiveness of interactions via automated channels, generative AI could automate responses to a higher percentage of customer inquiries, enabling customer care teams to take on inquiries that can only be resolved by a human agent.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.567.1">Post-call analysis by generative models identifies areas for improvement from conversation data.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.568.1">Mining dialogs also reveals customer needs and intents to generate cross-sell opportunities.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.569.1">Enabling </span><a id="_idIndexMarker1711"/><span class="koboSpan" id="kobo.570.1">call centers with generative AI capabilities requires the integration of LLMs, semantic search engines, and contact center applications. </span><span class="koboSpan" id="kobo.570.2">The following diagram shows an architecture for enabling Amazon Connect (an AWS call center service) with generative AI and chatbot capabilities.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.571.1"><img alt="A diagram of a phone  Description automatically generated" src="../Images/B20836_16_16.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.572.1">Figure 16.16: Generative AI-powered contact center self-service</span></p>
<p class="normal"><span class="koboSpan" id="kobo.573.1">With careful implementation, generative AI can drive significant operational efficiencies, service quality improvements, and customer experience breakthroughs across the contact center. </span><span class="koboSpan" id="kobo.573.2">With all the possibilities, it is also important to recognize the potential limitations of adopting generative AI for contact centers such as providing factually incorrect answers due to hallucination and misunderstanding of user queries, risk of exposing sensitive and private information, and introducing potentially harmful and biased responses.</span></p>
<h1 class="heading-1" id="_idParaDest-460"><span class="koboSpan" id="kobo.574.1">Are we close to having artificial general intelligence?</span></h1>
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.575.1">Artificial General Intelligence</span></strong><span class="koboSpan" id="kobo.576.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.577.1">AGI</span></strong><span class="koboSpan" id="kobo.578.1">) is </span><a id="_idIndexMarker1712"/><span class="koboSpan" id="kobo.579.1">a field within theoretical AI research working to create AI systems with cognitive functions comparable to human capabilities. </span><span class="koboSpan" id="kobo.579.2">AGI remains a theoretical concept that’s not well defined, and its definition and opinions on its eventual realization vary. </span><span class="koboSpan" id="kobo.579.3">Nevertheless, loosely speaking, AGI involves AI systems/agents equipped with a broad capacity to understand and learn across many diverse domains and address diverse problems in various contexts, not just narrow expertise in one field. </span><span class="koboSpan" id="kobo.579.4">These systems should have the ability to generalize the knowledge they gain, transfer learning from one domain, and apply knowledge and skills to novel situations and problems like humans do.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.580.1">The impressive capabilities displayed by LLMs and diffusion models have generated a lot of excitement about the potential to achieve AGI. </span><span class="koboSpan" id="kobo.580.2">Their ability to perform reasonably well across a wide variety of natural language processing and image generation tasks with minimal fine-tuning seems closer to flexible human-like intelligence than the previous narrow AI systems. </span><span class="koboSpan" id="kobo.580.3">As a result, there is increasingly optimistic speculation among some researchers and the media about whether we are on the brink of achieving true AGI through just the scaling of the data, model, and compute.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.581.1">However, most AI experts caution that we still have a long way to go to realize fully general and human-level intelligence. </span><span class="koboSpan" id="kobo.581.2">While very broad in scope, FMs remain confined to language and visual domains. </span><span class="koboSpan" id="kobo.581.3">Moreover, their knowledge and reasoning abilities remain brittle and narrow compared to humans. </span><span class="koboSpan" id="kobo.581.4">Transferring learning across radically different tasks and knowledge domains remains difficult for current FMs. </span><span class="koboSpan" id="kobo.581.5">Moreover, multimodality is integral to human intelligence, and while significant progress has been made in multimodal AI, it is still early to have seamlessly integrated understanding and reasoning of different modalities like text, image, video, sound, touch, social cues, etc. </span><span class="koboSpan" id="kobo.581.6">Thus, while the capabilities of LLMs like GPT and Anthropic Claude represent notable progress, we are still far from replicating the robustness, flexibility, and multidimensional qualities of human cognition.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.582.1">To help measure the progress of AGI, Google’s DeepMind has published an AGI levels guide using</span><a id="_idIndexMarker1713"/><span class="koboSpan" id="kobo.583.1"> performance and generality (narrow tasks vs. </span><span class="koboSpan" id="kobo.583.2">a range of general tasks) as the two dimensions. </span><span class="koboSpan" id="kobo.583.3">On the performance </span><a id="_idIndexMarker1714"/><span class="koboSpan" id="kobo.584.1">dimension, there are six levels in the guide:</span></p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1"><strong class="keyWord"><span class="koboSpan" id="kobo.585.1">Level 0 – No AI</span></strong><span class="koboSpan" id="kobo.586.1">: At this level, AI is not used for either narrow or general intelligent tasks.</span></li>
<li class="numberedList"><strong class="keyWord"><span class="koboSpan" id="kobo.587.1">Level 1 – Emergent</span></strong><span class="koboSpan" id="kobo.588.1">: At this level, AI is equal to or better than unskilled humans in either some narrow tasks or a range of general tasks. </span><span class="koboSpan" id="kobo.588.2">The guide states that FMs such as GPT, Bard, Llama 2, Claude, and Gemini have achieved this level of maturity.</span></li>
<li class="numberedList"><strong class="keyWord"><span class="koboSpan" id="kobo.589.1">Level 2 – Competent</span></strong><span class="koboSpan" id="kobo.590.1">: At this level, AI is better than at least 50% of skilled adults. </span><span class="koboSpan" id="kobo.590.2">According to the guide, some narrowly focused AI technology has achieved this level on some narrow tasks such as AI assistants like Siri and Alexa, essay writing, and coding. </span><span class="koboSpan" id="kobo.590.3">However, AGI has not achieved this level of progress.</span></li>
<li class="numberedList"><strong class="keyWord"><span class="koboSpan" id="kobo.591.1">Level 3 – Expert</span></strong><span class="koboSpan" id="kobo.592.1">: At this level, AI is better than at least 90% of skilled adults. </span><span class="koboSpan" id="kobo.592.2">Technologies such as AI grammar checkers and image generators have achieved this level on specific narrow tasks, however, no AGI has arrived at this level.</span></li>
<li class="numberedList"><strong class="keyWord"><span class="koboSpan" id="kobo.593.1">Level 4 – Virtuoso</span></strong><span class="koboSpan" id="kobo.594.1">: At this level, AI is better than 99% of skilled adults. </span><span class="koboSpan" id="kobo.594.2">Only a few narrow AI technologies such as AlphaGo have achieved this level of capability on narrow AI tasks.</span></li>
<li class="numberedList"><strong class="keyWord"><span class="koboSpan" id="kobo.595.1">Level 5 – Superhuman</span></strong><span class="koboSpan" id="kobo.596.1">: This is where AI is better than 100% of humans. </span><span class="koboSpan" id="kobo.596.2">Again, only narrow AI such as AlphaFold (a protein folding model), AlphaZero (an AI model that plays the game of Go), and Stockfish (an open-source chess engine) has achieved this level of capability. </span><span class="koboSpan" id="kobo.596.3">So what is the path to achieving AGI? </span><span class="koboSpan" id="kobo.596.4">No one has claimed they know the definitive answer yet, but the pursuit continues through the exploration of various active research areas such as multi-modality understanding, memory management, cross-domain reasoning and planning, and continuous self-learning. </span><span class="koboSpan" id="kobo.596.5">Furthermore, leading AI experts have been proposing diverse theoretical approaches in these domains, ranging from the integration of symbolic reasoning and connectionist architectures to the study of emergent phenomena, and the whole organism approach. </span><span class="koboSpan" id="kobo.596.6">In the subsequent section, let’s explore the symbolic, connectionist, and neural-symbolic approaches.</span></li>
</ol>
<h2 class="heading-2" id="_idParaDest-461"><span class="koboSpan" id="kobo.597.1">The symbolic approach</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.598.1">The </span><a id="_idIndexMarker1715"/><span class="koboSpan" id="kobo.599.1">symbolic approach relies on explicitly representing knowledge and reasoning using symbolic representations, such as logical statements, rules, and structured data formats. </span><span class="koboSpan" id="kobo.599.2">In this paradigm, the knowledge about a particular domain is manually encoded into the system using formal languages and logical formalisms. </span><span class="koboSpan" id="kobo.599.3">An example of symbolic representation could be “dogs have four legs.” </span><span class="koboSpan" id="kobo.599.4">This encoded knowledge forms a knowledge base, which</span><a id="_idIndexMarker1716"/><span class="koboSpan" id="kobo.600.1"> acts as a repository of facts, concepts, relationships, and rules that define how the world works within that domain. </span><span class="koboSpan" id="kobo.600.2">The symbolic AI system then uses an inference engine, which is a component that applies logical operations and rules of reasoning to the knowledge base. </span><span class="koboSpan" id="kobo.600.3">This allows the system to derive new conclusions, make inferences, and solve problems by manipulating and combining the symbolic representations in a structured and logical manner.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.601.1">For example, in a symbolic AI system designed for medical diagnosis, the knowledge base might contain rules that represent the relationships between symptoms, diseases, and treatments. </span><span class="koboSpan" id="kobo.601.2">The inference engine could then use these rules to analyze a patient’s symptoms and logically deduce the most likely diagnosis and appropriate treatment plan.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.602.1">One of the key advantages of symbolic AI is its ability to provide explainable and interpretable reasoning. </span><span class="koboSpan" id="kobo.602.2">Since the knowledge and reasoning processes are explicitly represented, the system’s decision-making process can be traced and understood by humans, which is particularly important in domains such as finance, law, and healthcare, where transparency and accountability are crucial.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.603.1">However, symbolic AI systems also face challenges, such as the knowledge acquisition bottleneck, where manually encoding vast amounts of knowledge can be time-consuming and labor-intensive. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.604.1">Additionally, these systems often struggle with handling ambiguity, uncertainty, and context-dependent knowledge, which are more easily tackled by other AI approaches, such as ML and neural networks.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.605.1"><img alt="" role="presentation" src="../Images/B20836_16_17.png"/></span></figure>
<figure class="mediaobject"><span class="koboSpan" id="kobo.606.1">Figure 16.17: Symbolic approach</span></figure>
<p class="normal"><span class="koboSpan" id="kobo.607.1">One </span><a id="_idIndexMarker1717"/><span class="koboSpan" id="kobo.608.1">prominent example of symbolic projects </span><a id="_idIndexMarker1718"/><span class="koboSpan" id="kobo.609.1">is the Cyc project, which stands as one of the longest-running symbolic AI initiatives. </span><span class="koboSpan" id="kobo.609.2">This </span><a id="_idIndexMarker1719"/><span class="koboSpan" id="kobo.610.1">ambitious endeavor aims to construct a comprehensive ontology and knowledge base that captures common sense rules about how the world operates. </span><span class="koboSpan" id="kobo.610.2">The Cyc project employs formal logic as its primary mechanism for reasoning and inference.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.611.1">While</span><a id="_idIndexMarker1720"/><span class="koboSpan" id="kobo.612.1"> groundbreaking in its scope and ambition, the Cyc project also highlights key challenges inherent to the symbolic AI approach. </span><span class="koboSpan" id="kobo.612.2">These challenges include the knowledge acquisition bottleneck, which refers to the arduous task of manually encoding vast amounts of knowledge in the system. </span><span class="koboSpan" id="kobo.612.3">Additionally, the project grapples with issues of brittleness, where slight deviations from the encoded rules or representations can lead to unexpected or erroneous behavior.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.613.1">Scalability concerns also arise, as the complexity of symbolic systems can rapidly escalate as the knowledge base expands, potentially leading to computational intractability. </span><span class="koboSpan" id="kobo.613.2">Furthermore, the robust handling of nuance, uncertainty, and context-dependent interpretations remains a formidable challenge within the symbolic paradigm.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.614.1">Despite these obstacles, the Cyc project’s pioneering efforts have contributed significantly to the field of symbolic AI, pushing the boundaries of knowledge representation and reasoning capabilities. </span><span class="koboSpan" id="kobo.614.2">Its ongoing development continues to shed light on both the potential and limitations of the symbolic approach in the quest for artificial general intelligence.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.615.1">While symbolic AI laid the crucial groundwork for establishing formal methods for knowledge representation and reasoning, many researchers believe that purely symbolic systems alone are unlikely to be sufficient for realizing the flexibility, robustness, and open-ended generalization required for general intelligence comparable to humans. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.616.1">The debate surrounding the extent to which symbolic approaches </span><a id="_idIndexMarker1721"/><span class="koboSpan" id="kobo.617.1">should be incorporated into AGI systems – whether as a core architecture or in a more complementary role – remains an active area of research with differing perspectives. </span><span class="koboSpan" id="kobo.617.2">Critics argue that the inherent limitations of symbolic systems, such as brittleness, scalability issues, and the knowledge acquisition bottleneck, pose significant challenges in </span><a id="_idIndexMarker1722"/><span class="koboSpan" id="kobo.618.1">capturing the nuanced, context-dependent, and continuously evolving nature of human intelligence. </span><span class="koboSpan" id="kobo.618.2">As a result, many advocate for a synergistic approach that combines the strengths of symbolic methods with other paradigms, leveraging the interpretability and strong generalization capabilities of symbolic reasoning while mitigating its weaknesses through complementary techniques, such as connectionist models or hybrid architectures.</span></p>
<h2 class="heading-2" id="_idParaDest-462"><span class="koboSpan" id="kobo.619.1">The connectionist/neural network approach</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.620.1">This </span><a id="_idIndexMarker1723"/><span class="koboSpan" id="kobo.621.1">approach focuses on constructing systems that emulate the human brain. </span><span class="koboSpan" id="kobo.621.2">Neural network </span><a id="_idIndexMarker1724"/><span class="koboSpan" id="kobo.622.1">systems, like the brain, employ extensive parallel processing across interconnected nodes or “neurons,” distributing knowledge across connections. </span><span class="koboSpan" id="kobo.622.2">Unlike explicit symbolic encodings or rules such as “a dog is an animal,” neural models rely on sub-symbolic distributed representations for the same concept. </span><span class="koboSpan" id="kobo.622.3">In sub-symbolic representation, which is used in approaches like neural networks and connectionist models, knowledge or concepts are represented not through explicit symbols or rules, but through patterns of features and characteristics.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.623.1">Integrated neural network approaches aspire to develop comprehensive end-to-end cognitive architectures covering perception, reasoning, and action. </span><span class="koboSpan" id="kobo.623.2">The hypothesis is that scaling up neural networks in both architecture and training data may induce the emergence of general intelligence, resembling the neural connections in the brain. </span><span class="koboSpan" id="kobo.623.3">However, current neural nets face challenges, as they are often narrow, lack systematicity, and encounter difficulties with transfer learning.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.624.1"><img alt="" role="presentation" src="../Images/B20836_16_18.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.625.1">Figure 16.18: Connectionist or neural network approach </span></p>
<p class="normal"><span class="koboSpan" id="kobo.626.1">With </span><a id="_idIndexMarker1725"/><span class="koboSpan" id="kobo.627.1">the remarkable capabilities demonstrated by LLMs, a compelling question arises: Can these models, arguably the most advanced neural networks to date, pave the way toward AGI? </span><span class="koboSpan" id="kobo.627.2">This</span><a id="_idIndexMarker1726"/><span class="koboSpan" id="kobo.628.1"> question has sparked contrasting viewpoints within the AI community. </span><span class="koboSpan" id="kobo.628.2">Yann LeCun, the chief AI scientist at Meta, contends that the current autoregressive approach employed by LLMs is unlikely to lead to AGI, as these models are primarily trained to predict the next token rather than engage in genuine planning or reasoning processes. </span><span class="koboSpan" id="kobo.628.3">According to LeCun, current LLMs lack the ability to truly comprehend and reason about knowledge; instead, they retrieve and generate information in an approximate manner.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.629.1">On the other hand, Ilya Sutskever, chief scientist at OpenAI, seems to lean toward the perspective that, with sufficient data and increasingly larger architectures, LLMs may indeed develop a profound understanding of semantic meanings, potentially leading to AGI. </span><span class="koboSpan" id="kobo.629.2">Some alternative viewpoints suggest that LLMs, especially multimodal variants capable of integrating diverse information sources, combined with their ability to leverage different tools, could achieve a certain level of general intelligence.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.630.1">As newer and more capable models are developed, and as more innovative systems are created around these models, only time will tell whether the current approach can ultimately lead to AGI. </span><span class="koboSpan" id="kobo.630.2">The ongoing advancements and debates within the field underscore the complexity and uncertainty surrounding this quest, while simultaneously fueling the relentless pursuit of more powerful AI.</span></p>
<h2 class="heading-2" id="_idParaDest-463"><span class="koboSpan" id="kobo.631.1">The neural-symbolic approach</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.632.1">The </span><a id="_idIndexMarker1727"/><span class="koboSpan" id="kobo.633.1">neural-symbolic approach aims to combine the best of both worlds by integrating neural networks with symbolic reasoning systems. </span><span class="koboSpan" id="kobo.633.2">The idea is to create AI systems that can learn patterns and representations from data using neural networks, while also leveraging the explicit knowledge and logical reasoning capabilities of symbolic AI.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.634.1">Here’s a</span><a id="_idIndexMarker1728"/><span class="koboSpan" id="kobo.635.1"> simple analogy: Imagine a young child is learning about the world. </span><span class="koboSpan" id="kobo.635.2">The child’s brain (the neural network component) can learn and recognize patterns, like shapes, colors, and objects, through experience and exposure. </span><span class="koboSpan" id="kobo.635.3">However, to truly understand and reason about the world, the child also needs to learn explicit rules, concepts, and knowledge from teachers, books, and other sources (the symbolic component).</span></p>
<p class="normal"><span class="koboSpan" id="kobo.636.1">In a neural-symbolic AI system, the neural network component would be responsible for learning patterns and representations from data, just like the child’s brain. </span><span class="koboSpan" id="kobo.636.2">At the same time, the symbolic component would provide a structured knowledge base and logical reasoning capabilities, similar to the child learning from books and teachers.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.637.1">By tightly integrating these two components, the AI system can leverage the strengths of both approaches. </span><span class="koboSpan" id="kobo.637.2">It can learn and adapt like neural networks, while also reasoning and making inferences using explicit knowledge and logical rules, much like humans do.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.638.1">The neural-symbolic approach is seen by many researchers as a promising path toward achieving AGI, as it aims to capture the key aspects of human-like intelligence: the ability to learn from experience, reason with structured knowledge, and adapt to new situations.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.639.1"><img alt="" role="presentation" src="../Images/B20836_16_19.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.640.1">Figure 16.19: Neural-symbolic architecture</span></p>
<p class="normal"><span class="koboSpan" id="kobo.641.1">AlphaGeometry, developed by DeepMind, is an innovative AI system that leverages the neural-symbolic </span><a id="_idIndexMarker1729"/><span class="koboSpan" id="kobo.642.1">approach</span><a id="_idIndexMarker1730"/><span class="koboSpan" id="kobo.643.1"> to tackle complex geometric reasoning tasks. </span><span class="koboSpan" id="kobo.643.2">This </span><a id="_idIndexMarker1731"/><span class="koboSpan" id="kobo.644.1">system combines the powerful pattern recognition and learning capabilities of deep neural networks with the structured reasoning and symbolic manipulation of geometric knowledge.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.645.1">At the core of </span><a id="_idIndexMarker1732"/><span class="koboSpan" id="kobo.646.1">AlphaGeometry lies a neural network component that processes visual inputs and generates candidate symbolic expressions representing geometric concepts and relationships. </span><span class="koboSpan" id="kobo.646.2">These symbolic expressions are then passed to a symbolic reasoning engine, which employs logical rules and constraints to validate, refine, and manipulate the expressions. </span><span class="koboSpan" id="kobo.646.3">The symbolic output is then interpreted and used to guide the neural network’s predictions, enabling the system to iteratively improve its geometric understanding.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.647.1">By tightly integrating neural and symbolic components, AlphaGeometry can effectively combine the strengths of both paradigms. </span><span class="koboSpan" id="kobo.647.2">The neural network excels at learning patterns and extracting geometric features from visual data, while the symbolic component provides a structured representation of geometric knowledge and enables logical reasoning over complex relationships.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.648.1">This neural-symbolic approach</span><a id="_idIndexMarker1733"/><span class="koboSpan" id="kobo.649.1"> allows AlphaGeometry to achieve impressive performance on challenging geometric tasks, outperforming previous methods and demonstrating an ability to generalize to novel problems. </span><span class="koboSpan" id="kobo.649.2">It showcases the potential of hybrid systems in advancing AI capabilities, particularly in domains that require both robust pattern recognition and structured reasoning.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.650.1">As the </span><a id="_idIndexMarker1734"/><span class="koboSpan" id="kobo.651.1">pursuit for AGI continues to captivate researchers and pioneers across multiple disciplines, the future holds both immense opportunities and formidable challenges. </span><span class="koboSpan" id="kobo.651.2">While connectionist approaches have demonstrated remarkable pattern recognition and learning capabilities, pushing the boundaries of these models to achieve the breadth and flexibility of human-level intelligence remains an ongoing endeavor. </span><span class="koboSpan" id="kobo.651.3">Symbolic approaches, with their explicit knowledge representation and reasoning prowess, offer a complementary pathway, yet struggles with the knowledge acquisition bottleneck and brittleness issues persist. </span><span class="koboSpan" id="kobo.651.4">The neural-symbolic paradigm, seamlessly fusing the strengths of these two worlds, emerges as a highly promising avenue.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.652.1">Irrespective of the path, the realization of AGI hinges on our ability to synergize diverse approaches, harness the exponential growth of data and computing power, and deepen our understanding of intelligence itself. </span><span class="koboSpan" id="kobo.652.2">As researchers continue their pursuit in this uncharted territory, unprecedented breakthroughs and paradigm shifts await.</span></p>
<h1 class="heading-1" id="_idParaDest-464"><span class="koboSpan" id="kobo.653.1">Summary</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.654.1">We are now coming to the end of this book spanning the breadth of machine learning – from foundational concepts to cutting-edge generative AI. </span><span class="koboSpan" id="kobo.654.2">We started the book by covering core ML techniques, algorithms, and industry applications to provide a strong base. </span><span class="koboSpan" id="kobo.654.3">We then progressed to data architectures, ML tools like TensorFlow and PyTorch, and engineering best practices to put skills into practice. </span><span class="koboSpan" id="kobo.654.4">Architecting robust ML infrastructure on AWS and optimization methods prepared you for real-world systems.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.655.1">Securing and governing AI responsibly is critical, so we delved into risk management. </span><span class="koboSpan" id="kobo.655.2">To guide organizations on the ML journey, we discussed maturity models and evolutionary steps.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.656.1">Closing the chapter by looking at generative AI and AGI, we explored the immense possibilities of the most disruptive new capability currently. </span><span class="koboSpan" id="kobo.656.2">Specifically, we delved into the intricacies of generative AI platforms, RAG architecture, and considerations for generative AI production deployment. </span><span class="koboSpan" id="kobo.656.3">Furthermore, we examined practical generative AI business applications across various industries, showcasing the transformative potential of this technology. </span><span class="koboSpan" id="kobo.656.4">Finally, the chapter concluded with an introduction to various theoretical approaches for achieving artificial general intelligence, providing a glimpse into the future of this rapidly evolving field.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.657.1">I hope you found this book enriching and that it provides a comprehensive foundation to propel your AI learning to new heights. </span><span class="koboSpan" id="kobo.657.2">The concepts and frameworks covered aim to equip you to build practical ML solutions. </span><span class="koboSpan" id="kobo.657.3">Keep learning, practicing, and developing your skills to maximize the value of AI. </span><span class="koboSpan" id="kobo.657.4">The future promises to be exponentially more exciting!</span></p>
<h1 class="heading-1"><span class="koboSpan" id="kobo.658.1">Leave a review!</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.659.1">Enjoyed this book? </span><span class="koboSpan" id="kobo.659.2">Help readers like you by leaving an Amazon review. </span><span class="koboSpan" id="kobo.659.3">Scan the QR code below to get a free eBook of your choice.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.660.1"><img alt="" role="presentation" src="../Images/Review_Copy.png"/></span></p>
<p class="normal"><em class="italic"><span class="koboSpan" id="kobo.661.1">*Limited Offer</span></em></p>
</div>
</body></html>