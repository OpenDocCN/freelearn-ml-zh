<html><head></head><body>
		<div id="_idContainer269" class="Content">
			<h1 id="_idParaDest-312"><em class="italics"><a id="_idTextAnchor315"/>Chapter 8:</em></h1>
		</div>
		<div id="_idContainer270" class="Content">
			<h1 id="_idParaDest-313"><a id="_idTextAnchor316"/>Model Deployment</h1>
		</div>
		<div id="_idContainer271" class="Content">
			<h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Deploy an ML model as an API using the R plumber package</li>
				<li class="bullets">Develop serverless APIs using AWS SageMaker, AWS Lambda, and AWS API Gateway</li>
				<li class="bullets">Create infrastructure from scratch using AWS CloudFormation</li>
				<li class="bullets">Deploy an ML model as an API using Docker containers</li>
			</ul>
			<p>In this chapter, we will learn how to host, deploy, and manage models on AWS and Docker containers.</p>
		</div>
		<div id="_idContainer291" class="Content">
			<h2 id="_idParaDest-314"><a id="_idTextAnchor317"/>Introduction</h2>
			<p>In the previous chapter, we studied model improvements and explored the various techniques within hyperparameter tuning to improve model performance and develop the best model for a given use case. The next step is to deploy the machine learning model into production so that it can be easily consumed by or integrated into a large software product.</p>
			<p>Most data science professionals assume that the process of developing machine learning models ends with hyperparameter tuning when we have the best model in place. In reality, the value and impact delivered by a machine learning model is limited (mostly futile) if it isn't deployed and (or) integrated with other software services/products into a large tech ecosystem. Machine learning and software engineering are definitely two separate disciplines. Most data scientists have limited proficiency in understanding the software engineering ecosystem and, similarly, software engineers have a limited understanding of the machine learning field. Thus, in large enterprises where they build a product where a machine learning use case evolves into a major feature for a software product, there is a need for data scientists and software engineers to collaborate. However, collaboration between software engineers and data scientists in most cases is extremely challenging, as both find each other's fields highly overwhelming to comprehend.</p>
			<p>Over the years, there has been a lot of effort invested in developing tools and resources by large corporations to aid data scientists to easily embrace a few software engineering components and vice versa. These tools have enabled easier collaboration between the two disciplines, accelerating the process of developing large-scale, enterprise-grade machine learning products.</p>
			<p>In this chapter, we will learn about a few approaches to deploying machine learning models as web services that can easily integrate with other services in a large software ecosystem. We will also discuss the pros and cons of the different approaches and best practices for model deployment.</p>
			<h2 id="_idParaDest-315"><a id="_idTextAnchor318"/>What is an API?</h2>
			<p>Before delving into the specifics of model deployment, we need to study an important software engineering topic that simplifies the entire process of model deployment, that is, an <strong class="keyword">Application Program Interface</strong>, commonly referred to as an <strong class="keyword">API</strong>. An API is a set of clearly defined methods for communication between various software components. Software development has been made significantly easier with the advent of APIs. If a developer, say, wanted to develop an iPhone app that would add some filters to an image, they need not write the entire code to capture the image from the phone's camera, save it to the library, and then apply their app-specific filters to it. Instead, they can use the phone camera API, which provides an easy way to communicate with the camera and only focus on writing code that would add filters to an image. In a nutshell, an API is the means for heterogeneous software components to communicate with each other.</p>
			<p>In a large software product, there would be several components that are responsible for a specific task. These components interact with each other through a defined language that ensures the smooth communication of data, events, and alerts.</p>
			<p>Here are some salient features of APIs:</p>
			<ul>
				<li>APIs help in <strong class="bold">modularizing</strong> software applications and enable the building of better products.</li>
				<li>APIs are commonly known by software engineers and are <strong class="bold">language agnostic</strong>. Thus, heterogenous applications developed in a completely different language or system can also effectively communicate with each other.</li>
				<li>Communication between services is also enabled using a common language, that is, <strong class="keyword">JavaScript Object Notation</strong> (short for, <strong class="keyword">JSON</strong>). However, there are other popular languages too, for example, XML.</li>
				<li>They also support HTTP, which means APIs are accessible through web browsers (such as Google Chrome or Mozilla Firefox) or a tool such as <em class="italics">Postman</em>.<h4>Note</h4><p class="callout">Postman is a free tool that offers easy-to-use services in the entire life cycle of an API, such as designing, debugging, testing, documenting, monitoring, and publishing. It is available for download on the Windows, Linux, and macOS platforms. You can learn more about Postman at https://www.getpostman.com/.</p></li>
			</ul>
			<p>We are particularly interested in the development of RESTful APIs, that is, APIs that communicate over HTTP. RESTful APIs are also called <strong class="bold">REST APIs</strong> and have two main types of methods:</p>
			<ul>
				<li><strong class="bold">GET method</strong>: This is used when we want to read data from a service.</li>
				<li><strong class="bold">POST method</strong>: This is used when we want to send data to a service.</li>
			</ul>
			<p>A few other methods are <strong class="bold">head</strong>, <strong class="bold">put</strong>, and <strong class="bold">delete</strong>.</p>
			<p>Deploying machine learning models as (web service) REST APIs eases the process of integrating a service with other services. Software engineers are fond of using REST APIs and since the service is language-agnostic, we have a tremendous advantage in developing the model in the language of our choice. The software engineer could use Python, Java, Ruby, or one of many other languages for the development of other services, whereas we can develop the model in R, Python, Julia, and so on, and yet effectively and effortlessly integrate services into a large software product.</p>
			<p>Now that we have a fair understanding of APIs, let's understand how we can deploy an ML model in R as an API.</p>
			<h2 id="_idParaDest-316"><a id="_idTextAnchor319"/>Introduction to plumber</h2>
			<p><strong class="keyword">Plumber</strong> is an R package that helps in translating R functions into an HTTP API that can be invoked from other machines within a network, enabling communication between systems. By using R plumber, we will be able to achieve the advantages discussed, such as developing modularized, language agnostic, common communication language (JSON) based HTTP rest APIs that provide a defined path of communication between systems. Using plumber is extremely straightforward. With a few lines of code, we can convert our existing R functions into a web service that can be served as an endpoint.</p>
			<p>In this chapter, we will extend the same model and use case we built in <em class="italics">Chapter 7</em>, <em class="italics">Model Improvements</em>, to classify whether a patient is diabetic using the <strong class="inline">PimaIndiasDiabetes</strong> dataset in the <strong class="inline">mlbench</strong> library. Later, we will extend the same use case to deploy the model as a web service using a Docker container and serverless applications.</p>
			<h3 id="_idParaDest-317"><a id="_idTextAnchor320"/>Exercise 98: Developing an ML Model and Deploying It as a Web Service Using Plumber</h3>
			<p>In this exercise, we will develop a logistic regression model using three independent variables and deploy it as a REST API using Plumber. We will create a simple binary classification model and use the <strong class="inline">plumber</strong> package's services to wrap the model as an API by defining the HTTP get and post methods.</p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li>Create an R script named <strong class="inline">model.R</strong> using RStudio or Jupyter Notebook.</li>
				<li>Load the required libraries and build a logistic regression model. Now, define the <strong class="inline">get</strong> methods that accept the input parameters and return the prediction as an outcome:<p class="snippet">#----------Code listing for model.R-----------------------------</p><p class="snippet">library(mlbench)</p><p class="snippet">data(PimaIndiansDiabetes)</p><p class="snippet">df&lt;-PimaIndiansDiabetes</p><h4>Note</h4><p class="callout">This data has been taken from the UCI Repository Of Machine Learning Databases from the following URLs:</p><p class="callout">ftp://ftp.ics.uci.edu/pub/machine-learning-databases</p><p class="callout">http://www.ics.uci.edu/~mlearn/MLRepository.html</p><p class="callout">It was converted to the R format by Friedrich Leisch.</p></li>
				<li>Train a logistic regression model with the <strong class="inline">df</strong> DataFrame object:<p class="snippet">model&lt;-glm(diabet-es~pregnant+glucose+pressure, data=df,</p><p class="snippet">           family=binomial(link='logit'))</p></li>
				<li>Define the API endpoint as a function with the additional <strong class="inline">#' @get /</strong> construct:<p class="snippet">#' @get /predict_data</p><p class="snippet">function(pregnant, glucose, pressure){</p></li>
				<li>Within the function, convert the parameters into <strong class="inline">numeric</strong> values using the <strong class="inline">as.numeric</strong> command:<p class="snippet">  pregnant &lt;- as.numeric(pregnant)</p><p class="snippet">  glucose &lt;- as.numeric(glucose)</p><p class="snippet">  pressure &lt;- as.numeric(pressure)</p></li>
				<li>Then, create a DataFrame with the same column names as we did in the previous step:<p class="snippet">  sample &lt;- data.frame(pregnant=pregnant,                       glucose=glucose,                       pressure=pressure)</p></li>
				<li>Use the newly created <strong class="inline">sample</strong> DataFrame to make predictions on the trained model:<p class="snippet">  y_pred&lt;-ifelse(predict(model,newdata=sample)&gt;0.5,"yes","no")</p></li>
				<li>Package the result as a <strong class="inline">list</strong> for the function's return call and complete/close the function definition with <strong class="inline">}</strong> parentheses:<p class="snippet">  list(Answer=y_pred)</p><p class="snippet">}</p><p>The previous exercise demonstrates regular R model code with one additional construct. The model we developed is rather a simple one with only three independent variables and one dependent variable (unlike the eight independent variables we saw in <em class="italics">Chapter 7</em>, <em class="italics">Model Improvements)</em>. We also created a function that will accept three input parameters, each representing one independent variable for the model. This function will be used as the endpoint when we deploy the model as a REST API. We added one additional construct just before the function (refer to the fourth step of the previous exercise):</p><p class="snippet">#' @get /predict_data</p><p>This construct defines that the <strong class="inline">predict_data</strong> endpoint will be serving the GET requests. The function we defined for this endpoint accepts three parameters with no default values. Let's now install the <strong class="inline">plumber</strong> package and make a call using the web server.</p><p>The final complete <strong class="inline">model.R</strong> file should look like this:</p><p class="snippet">#----------Code listing for model.R-----------------------------</p><p class="snippet">library(mlbench)</p><p class="snippet">data(PimaIndiansDiabetes)</p><p class="snippet">df&lt;-PimaIndiansDiabetes</p><p class="snippet">model&lt;-glm(diabet-es~pregnant+glucose+pressure, data=df,</p><p class="snippet">           family=binomial(link='logit'))</p><p class="snippet">#' @get /predict_data</p><p class="snippet">function(pregnant, glucose, pressure){</p><p class="snippet">  pregnant &lt;- as.numeric(pregnant)</p><p class="snippet">  glucose &lt;- as.numeric(glucose)</p><p class="snippet">  pressure &lt;- as.numeric(pressure)</p><p class="snippet">  sample &lt;- data.frame(pregnant=pregnant,                       glucose=glucose,                       pressure=pressure)</p><p class="snippet">  y_pred&lt;-ifelse(predict(model,newdata=sample)&gt;0.5,"yes","no")</p><p class="snippet">  list(Answer=y_pred)</p><p class="snippet">}</p><h4>Note</h4><p class="callout">You can also refer to the GitHub URL: https://github.com/TrainingByPackt/Applied-Supervised-Learning-with-R/blob/master/Lesson08/Docker/model.R.</p></li>
				<li>Create another R script called <strong class="inline">main.R</strong>. Now, install the <strong class="inline">plumber</strong> package and load it into memory, and then deploy the R function using the <strong class="inline">plumb</strong> function, as illustrated here:<p class="snippet">#-------main.R-----------------</p><p class="snippet">install.packages("plumber")</p><p class="snippet">library(plumber)</p><p class="snippet"># (The previous code snippet is supposed to be save as 'model.R')</p></li>
				<li>Use the following command to pass the R file to the <strong class="inline">plumb</strong> function:<p class="snippet">pr &lt;- plumber::plumb("model.R")</p><p class="snippet">pr$run(port=8080,host='127.0.0.1')</p></li>
				<li>After executing the previous code, the plumber library creates a web server within your <strong class="inline">localhost</strong> and responds to the requests. To test whether the endpoint functions we wrote are functioning in the expected way, we will invoke the endpoint from the browser.<h4>Note</h4><p class="callout">The following format is used to call the API endpoint:</p><p class="callout"><strong class="inline">[host]:[port]/endpoint_name?parameter1=Value1&amp;Parameter2=Value2[&amp;ParameterN=ValueN]</strong></p><p>The name of the <strong class="inline">predict_data</strong> endpoint corresponds to the definition in our code snippet:</p><p class="snippet">#' @get /predict_data</p><p>The parameters are passed to the endpoint after the <strong class="inline">?</strong> symbol, and multiple parameters are separated by the <strong class="inline">&amp;</strong> symbol.</p></li>
				<li>Since we deployed the endpoint to localhost, that is, <strong class="inline">127.0.0.1</strong> on the <strong class="inline">8080</strong> port, we will have the following API definition for the endpoint. Invoke the API using the browser:<p class="snippet">http://127.0.0.1:8080/predict_data?pregnant=1&amp;glucose=3&amp;pressure=1</p></li>
				<li>Execute the previous API definition, which will return the following prediction value:<p class="snippet">{"Answer":["no"]}</p><p>To test the APIs, we can use a better tool, such as Postman, instead of the browser. Postman (https://www.getpostman.com/) is currently one of the most popular tools used in API testing. It is available for free on the Windows, Mac, and Linux platforms. Using Postman is relatively simple and doesn't include any new learning.</p><h4>Note</h4><p class="callout">In case you would like to explore additional details about Postman, you can explore the learning resources provided at https://learning.getpostman.com/docs/postman/launching_postman/installation_and_updates/.</p></li>
				<li>After you download and install Postman for your system, you can test the API endpoint by pasting it in the input window, as shown in the following screenshot:</li>
			</ol>
			<div>
				<div id="_idContainer272" class="IMG---Figure">
					<img src="image/C12624_08_01.jpg" alt="Figure 8.1: Plumber UI&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 8.1: Plumber UI</h6>
			<p>We can execute the API by clicking on the <strong class="bold">Send</strong> button, and the results are displayed in the highlighted area of the previous screenshot. Observing the output from Postman, we can see that our machine learning model has been deployed successfully as an API endpoint.</p>
			<h3 id="_idParaDest-318"><a id="_idTextAnchor321"/>Challenges in Deploying Models with plumber</h3>
			<p>We can see that deploying the model as an API using plumber is simple and can be done easily with a few additional lines of code. The <strong class="inline">plumber</strong> package provides additional features that we have not explored in this exercise. Some important topics that might be interesting to explore are as follows:</p>
			<ul>
				<li><strong class="bold">Filters</strong>: Filters can be used to define a <em class="italics">pipeline</em> with a flow of incoming requests. This functionality helps to further modularize the deployment logic and workflow. You can read more at https://www.rplumber.io/docs/routing-and-input.html#filters.</li>
				<li><strong class="bold">Error handling</strong>: With larger applications, the code base and the complexity of applications increase exponentially. It becomes increasingly important to add exception handlers and ease the process of debugging applications. You can read more about it at https://www.rplumber.io/docs/rendering-and-output.html#error-handling.</li>
			</ul>
			<p>These methods have a major drawback. While it is relatively easy to set up an endpoint in a single host, there might be issues faced when deploying the same solution on a different system. These issues might arise due to the difference in the system architecture, software version, operating system, and so on. To mitigate the conflicts that you may face when deploying the endpoint, one technique is to make the process environment-agnostic, that is, a solution developed in one host system can be deployed without any issues in any other host with a different architecture, platform, or operating system. This can be achieved using <strong class="bold">Docker containers</strong> along with plumber for deployment instead of directly using plumber.</p>
			<h2 id="_idParaDest-319"><a id="_idTextAnchor322"/>A Brief History of the Pre-Docker Era</h2>
			<p>Before diving deep into the Docker tool, let's understand some background and history.</p>
			<p>The challenge of deploying an application in an environment-agnostic framework was achieved earlier using virtualization, that is, the entire application, dependencies, libraries, necessary frameworks, and the operating system itself was virtualized and packaged as a solution that could be deployed on a host. Multiple virtual environments could run on an infrastructure (called a <strong class="bold">hypervisor</strong>), and applications became environment-agnostic. However, this approach has a major trade-off. Packaging the entire operating system into the <strong class="bold">virtual machine</strong> (<strong class="bold">VM</strong>) of an application made the package heavy and often resulted in wasting memory and computing resources.</p>
			<p>A more intuitive approach to this problem was to exclude the operating system from the package and only include the application-related libraries and dependencies. Additionally, enable a mechanism such that the package becomes infrastructure-agnostic, keeping the app lightweight. This is when Docker was introduced. The following visual sheds light on the high-level view of how Docker was improvised to solve problems previously solved by virtual machines:</p>
			<div>
				<div id="_idContainer273" class="IMG---Figure">
					<img src="image/C12624_08_02.jpg" alt="Figure 8.2: The architectural difference between a virtual machine and a Docker container&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 8.2: The architectural difference between a virtual machine and a Docker container</h6>
			<p>Let's now understand Docker containers in more detail.</p>
			<h2 id="_idParaDest-320"><a id="_idTextAnchor323"/>Docker</h2>
			<p>Docker is a simple tool that eases the process of developing, deploying, and executing software applications using containers. A <strong class="bold">container</strong> is analogous to a shipping industry container, and allows a developer to package an entire application with its dependencies, and ship it all out as one package. Once built on a system, the package will work on any other system as well, regardless of the differences in the infrastructure.</p>
			<p>With Docker, we can create a single document (called a <strong class="bold">Dockerfile</strong>) that defines a simplified step for setting up the required environment for the application. The Dockerfile is then used to build a <strong class="bold">Docker image</strong>. A container is an instance of a Docker image. For the same application, we might sometimes have multiple containers that will help in load balancing for high-traffic applications.</p>
			<h3 id="_idParaDest-321"><a id="_idTextAnchor324"/>Deploying the ML Model Using Docker and plumber</h3>
			<p>We will leverage the same plumber application with Docker for environment- and infrastructure-agnostic deployment. First, we will need to download and install Docker on our system. It's free and easy. Create an account and download Docker for your system from https://www.docker.com/get-started. Once installed, you can verify whether Docker is running by executing the <strong class="inline">docker</strong> command in the terminal or Windows PowerShell.</p>
			<h3 id="_idParaDest-322"><a id="_idTextAnchor325"/>Exercise 99: Create a Docker Container for the R plumber Application</h3>
			<p>In this exercise, we will extend the previously created plumber application as a Docker container. In order to develop environment-agnostic models that can be deployed to any production system without any issues, we can deploy a plumber app using Docker containers. These containers can then be deployed on any other machine that supports Docker Engine.</p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li value="1">Define a Dockerfile, as illustrated here:<p class="snippet">FROM rocker/r-ver:3.5.0</p></li>
				<li>Now, install the libraries for the <strong class="inline">plumber</strong> package using the following command:<p class="snippet">RUN apt-get update -qq &amp;&amp; apt-get install -y  libssl-dev  libcurl4-gnutls-dev</p></li>
				<li>Next, install the <strong class="inline">plumber</strong> package, as shown:<p class="snippet">RUN R -e "install.packages(c('plumber','mlbench'))"</p></li>
				<li>Now, use the following command to copy all files from the current directory into the current folder for the container. This will copy our <strong class="inline">model.R</strong> and <strong class="inline">plumber.R</strong> file into a container:<p class="snippet">COPY / /</p></li>
				<li>Next, define a port to be exposed where the container will be deployed:<p class="snippet">EXPOSE 8080</p></li>
				<li>Define the first script to run when the container starts after the build:<p class="snippet">ENTRYPOINT ["Rscript", "main.R"]</p><p>We now have three files in our project folder, as shown in the following diagram. Note that the Dockerfile is a simple text file with no extensions. On running a <strong class="inline">build</strong> command from the terminal within this folder, <strong class="bold">Docker Engine</strong> searches for the Dockerfile and prepares the environment based on the instructions provided within the document:</p><div id="_idContainer274" class="IMG---Figure"><img src="image/C12624_08_03.jpg" alt="Figure 8.3: Project files&#13;&#10;"/></div><h6>Figure 8.3: Project files</h6><p>The final Dockerfile with all the commands, as defined in the previous steps, will look like this:</p><p class="snippet">FROM rocker/r-ver:3.5.0</p><p class="snippet">RUN apt-get update -qq &amp;&amp; apt-get install -y  libssl-dev  libcurl4-gnutls-dev</p><p class="snippet">RUN R -e "install.packages(c('plumber','mlbench'))"</p><p class="snippet">COPY / /</p><p class="snippet">EXPOSE 8080</p><p class="snippet">ENTRYPOINT ["Rscript", "main.R"]</p></li>
				<li>We can now build the Docker image from the Dockerfile using the <strong class="inline">docker build</strong> command, as follows:<p class="snippet">docker build -t r_ml_demo .</p><p>The <strong class="inline">.</strong> after the <strong class="inline">r_ml_demo</strong> indicates that the Dockerfile is present in the current folder. The build process takes a while as it creates the container image with all the necessary dependencies. Once the image is built, we can run the Docker image using the following command by mapping the machine's <strong class="inline">8080</strong> port to the container's published <strong class="inline">8080</strong> port.</p></li>
				<li>Run the Docker image using the following command:<p class="snippet">docker run --rm -p 8080:8080 r_ml_demo</p><h4>Note</h4><p class="callout">You can refer to the complete code from GitHub at https://github.com/TrainingByPackt/Applied-Supervised-Learning-with-R/tree/master/Lesson08/Docker.</p></li>
			</ol>
			<p>The container can be tested again in the same way we tested our plumber application using Postman, and we will get exactly the same result. We can, therefore, deploy an R application using plumber and Docker on any other system, regardless of the operating system and missing libraries.</p>
			<h4>Note</h4>
			<p class="callout">Docker cannot be installed on Windows Home edition. Only Windows Pro editions support Hypervisor.</p>
			<h3 id="_idParaDest-323"><a id="_idTextAnchor326"/>Disadvantages of Using plumber to Deploy R Models</h3>
			<p>While this process comes with a few advantages of easy and fast implementation, it also comes with some disadvantages. The major disadvantage of plumber is scaling the application endpoint for large complex use cases. The scale here refers to the number of times the endpoint is invoked as well as the amount of data that can be invoked through the endpoint.</p>
			<p>One of the major drawbacks of using plumber is that it doesn't directly support passing JSON objects or arrays or lists to the endpoint. This becomes a bottleneck when we are dealing with bigger models with more than 20 independent variables. The previous use case, in <em class="italics">Exercise 2</em>, <em class="italics">Create a Docker Container for the R Plumber Application</em>, was a fairly small and lightweight model with three independent variables. Therefore, the API definition was short and sweet. However, as the number of parameters increases (which is definitely bound to happen for real production models), plumber model endpoint definitions will not be the best ones to use. Also, the plumber framework is not ideal for large complex software use cases. The small community around the framework, lack of proper documentation, and limited support makes it a risky choice for deploying a model into a large scale machine learning product or service.</p>
			<p>Let's take a look at leveraging cloud services for deploying R ML models.</p>
			<h2 id="_idParaDest-324"><a id="_idTextAnchor327"/>Amazon Web Services</h2>
			<p><strong class="keyword">Amazon Web Services</strong> (<strong class="keyword">AWS</strong>) is the leading provider of cloud services. With the advent of the cloud, the tech industry has seen a dramatic shift in the process of building large-scale enterprise applications leveraging cloud services rather than self-hosted services. Other prominent players in the cloud services market are Microsoft, Google, and IBM. While all leading cloud providers have an exhaustive suite of services to build all kinds of software applications, we will focus only on AWS for the scope of this chapter. You are highly encouraged to explore alternative services for similar use cases from other cloud providers (not restricted to Google or Microsoft).</p>
			<p>AWS has a ton of services readily available that can be used to make large, complex enterprise applications of any scale with no upfront commitments. You pay as you go, and there are also a large number of services that you can explore and test for free for one year (with certain limits). For the scope of the set of experiments we will perform in the upcoming exercises, the free tier should suffice. In case you do not already have an AWS account, create one at https://aws.amazon.com/free/activate-your-free-tier-account/.</p>
			<p>You will need a valid credit/debit card for the signup process. We will only leverage the free tier service from AWS for the exercises.</p>
			<p>There are several approaches that we could take to deploy a machine learning model using cloud services. Some may be well-suited for small applications, some for medium-sized and moderately complex applications, and others for large and very complex applications. We will explore the approach that has the least amount of software engineering yet provides effective flexibility and can easily scale into large-scale applications while easily integrating into complex applications.</p>
			<p>The use of an API and delivering the machine learning model as an API makes the entire process of integrating the service into other applications fairly straightforward.</p>
			<h2 id="_idParaDest-325"><a id="_idTextAnchor328"/>Introducing AWS SageMaker</h2>
			<p><strong class="bold">Amazon SageMaker</strong> is a cloud service that provides developers and data scientists with a platform to build, train, and deploy machine learning models quickly. It is an extremely effective service in aiding data scientists with limited development knowledge to deploy highly scalable ML models while abstracting the entire complexities of the infrastructure and underlying services.</p>
			<p>SageMaker automates the entire process of deploying a model as an API with the defined resources and creates an <em class="italics">endpoint </em>that can be used for inferencing within the other AWS services. To enable the endpoint to be inferenced by other external applications, we would need to orchestrate the flow of requests using two other AWS services, called <strong class="keyword">AWS API Gateway</strong> and <strong class="keyword">AWS Lambda</strong>. We will explore these new services later in the chapter.</p>
			<p>Now, let's begin deploying our model using AWS SageMaker.</p>
			<h3 id="_idParaDest-326"><a id="_idTextAnchor329"/>Deploying an ML Model Endpoint Using SageMaker</h3>
			<p>SageMaker, by default, doesn't provide a direct way to create R models, but there is an easy alternative provided by Amazon. AWS provides the functionality of <strong class="bold">Infrastructure as Code</strong> with <strong class="bold">AWS CloudFormation</strong>, that is, a service where we can codify the entire flow of provisioning and the setup of infrastructure resources for a project. With CloudFormation templates, we can automate the process of provisioning a tech stack as per our needs and reuse it any number of times. Amazon has provided a lucid and elaborate guide to get started with R notebooks on SageMaker using the CloudFormation template.</p>
			<p>To find out more, you can refer to the guide at https://aws.amazon.com/blogs/machine-learning/using-r-with-amazon-sagemaker/ for a detailed understanding of the process.</p>
			<h3 id="_idParaDest-327"><a id="_idTextAnchor330"/>Exercise 100: Deploy the ML Model as a SageMaker Endpoint</h3>
			<p>In this exercise, we will deploy an ML model as a SageMaker endpoint.</p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li value="1">Log in to your AWS account and launch the CloudFormation script to create an R notebook.</li>
				<li>Now, access the CloudFormation template from https://amzn.to/2ZzUM28 to create the R Notebook on SageMaker.<p>Next, we will create and launch the stack in AWS using the previous CloudFormation template.</p></li>
				<li>Click on the template; it directly navigates you to the CloudFormation service, as shown in the following screenshot. The cloud formation template (which is a YAML file) is hosted in a public S3 bucket and has already been added to the input box under <strong class="bold">Specify an Amazon S3 template URL</strong>. Click on the <strong class="bold">Next</strong> button and navigate to the <strong class="bold">Details</strong> page:<div id="_idContainer275" class="IMG---Figure"><img src="image/C12624_08_04.jpg" alt="Figure 8.4: CloudFormation—Create Stack page&#13;&#10;"/></div><h6>Figure 8.4: CloudFormation—Create Stack page</h6></li>
				<li>On the next page, specify the SSH key pair that you will use to log in to the EC2 instance. This is a secure way to access the cloud instance or virtual machine that we provision in the cloud. If you do not have a key already created for your account, you can create one using the steps provided on the Amazon website: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair.</li>
				<li>Once the key pair is created or if you already have a pair, it will appear in the dropdown in the highlighted box, as shown in the following screenshot. Select your key pair and click on the <strong class="bold">Next</strong> button:<div id="_idContainer276" class="IMG---Figure"><img src="image/C12624_08_05.jpg" alt="Figure 8.5: Creating a key pair&#13;&#10;"/></div><h6>Figure 8.5: Creating a key pair</h6></li>
				<li>On the next <strong class="bold">Option</strong> page, we can directly click on the <strong class="bold">Next</strong> button and navigate to the next page.</li>
				<li>Lastly, on the review page, select the <strong class="bold">I acknowledge that AWS CloudFormation might create IAM resources with custom names</strong> checkbox and click on the <strong class="bold">Next</strong> button.</li>
				<li>The process will create the stack (it might take a while to show on the screen—refresh the screen after 1-2 minutes). Once created, you will see the stack ready under CloudFormation, as shown in the following screenshot. The output tabs will have the SSH command to be used to log in; copy the value in the highlighted section and run the command in a terminal or Command Prompt:<div id="_idContainer277" class="IMG---Figure"><img src="image/C12624_08_06.jpg" alt="Figure 8.6: Stacks—SSH key&#13;&#10;"/></div><h6>Figure 8.6: Stacks—SSH key</h6></li>
				<li>On running the SSH command from the terminal, it forwards port <strong class="inline">8787</strong> to your computer while connecting to the new instance. Once connected, open a browser window and type <strong class="inline">https://127.0.0.1:8787</strong> in the address bar to open the RStudio login page. The default username and password is set to <strong class="inline">rstudio</strong>. Enter the username and password and click on the <strong class="bold">Sign In</strong> button:<div id="_idContainer278" class="IMG---Figure"><img src="image/C12624_08_07.jpg" alt="Figure 8.7: RStudio—Sign In page&#13;&#10;"/></div><h6>Figure 8.7: RStudio—Sign In page</h6></li>
				<li>Log in to RStudio and create a new R script with any name, say <strong class="inline">Sagemaker.R</strong>, load the necessary libraries, and get the SageMaker session ready:<p class="snippet">library(reticulate)</p><p class="snippet">library(readr)</p></li>
				<li>Start the SageMaker session and define the default bucket as well as the role to be used for the session:<p class="snippet">sagemaker &lt;- import('sagemaker')</p><p class="snippet">session &lt;- sagemaker$Session()</p><p class="snippet">bucket &lt;- session$default_bucket()</p><p class="snippet">role_arn &lt;- session$expand_role('sagemaker-service-role')</p></li>
				<li>Install the <strong class="inline">mlbench</strong> package and load the data for our use case. In the following command, we'll first set the <strong class="inline">seed</strong> for reproducibility:<p class="snippet">set.seed(2019)</p><p class="snippet">install.packages("mlbench")</p><p class="snippet">library(mlbench)</p><p class="snippet">data(PimaIndiansDiabetes) </p><p class="snippet">df&lt;- PimaIndiansDiabetes</p></li>
				<li>To explore SageMaker's automated hyperparameter tuning, we will be developing an XGBoost model instead of a logistic regression model on the same use case as the previous one. Therefore, we need the target variable and all the independent variables in a numeric type. Also, SageMaker expects the data in a form where the first column is the target variable:<p class="snippet">df$diabetes &lt;- ifelse(df$diabetes == "yes",1,0)</p></li>
				<li>Place the target variable as the first column in the dataset:<p class="snippet">df&lt;- df[,c(9,1:8)]</p></li>
				<li>Create 70% train and 30% test datasets using the following command:<p class="snippet">train_index &lt;- sample(seq_len(nrow(df)),floor(0.7 * nrow(df)))</p><p class="snippet">train &lt;- df[train_index,]</p><p class="snippet">test &lt;- df[-train_index,]</p></li>
				<li>Write the train and test data created from the <strong class="inline">df</strong> DataFrame into memory and upload the CSV files into an S3 bucket on AWS. The session has been defined with a default bucket, therefore, we can directly use the <strong class="inline">upload_data</strong> command with the path and the required constructs to upload the dataset on our default S3 bucket:<p class="snippet">write_csv(train, 'diabetes_train.csv', col_names = FALSE)</p><p class="snippet">write_csv(test, 'diabetes_test.csv', col_names = FALSE)</p><p class="snippet">s3_train &lt;- session$upload_data(path = 'diabetes_train.csv', </p><p class="snippet">                                bucket = bucket, </p><p class="snippet">                                key_prefix = 'data')</p><p class="snippet">s3_test &lt;- session$upload_data(path = 'diabetes_test.csv', </p><p class="snippet">                               bucket = bucket, </p><p class="snippet">                               key_prefix = 'data')</p></li>
				<li>Define the train and test dataset (validation data) for the SageMaker session:<p class="snippet">s3_train_input &lt;- sagemaker$s3_input(s3_data = s3_train,</p><p class="snippet">                                     content_type = 'csv')</p><p class="snippet">s3_test_input &lt;- sagemaker$s3_input(s3_data = s3_test,</p><p class="snippet">                                    content_type = 'csv')</p><p>SageMaker provides AWS optimized pre-configured containers that can be leveraged directly for model training. We would need to choose a container base from the same region that our resources are hosted in. In this case, it is <strong class="bold">us-east-1</strong>.</p></li>
				<li>Define the container for the estimator and the output folder in S3:<p class="snippet">containers &lt;- list('us-west-2' = </p><p class="snippet">'433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest',</p><p class="snippet">                   'us-east-1' = </p><p class="snippet">'811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest',</p><p class="snippet">                   'us-east-2' =</p><p class="snippet"> '825641698319.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest',</p><p class="snippet">                   'eu-west-1' =</p><p class="snippet"> '685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:latest')</p></li>
				<li>Select the container for the estimator using the following command:<p class="snippet">container &lt;- containers[session$boto_region_name][[1]]</p></li>
				<li>Define the output folder as illustrated here:<p class="snippet">s3_output &lt;- paste0('s3://', bucket, '/output')</p></li>
				<li>Define the SageMaker estimator, the job, and the input data. Here, we would need to provide the type of instance that we would like to use for the model training process, and we will choose <strong class="inline">ml.m5.large</strong>:<p class="snippet">estimator &lt;- sagemaker$estimator$Estimator(image_name = container,</p><p class="snippet">                                           role = role_arn,</p><p class="snippet">                                           train_instance_count = 1L,</p><p class="snippet">                                           train_instance_type = 'ml.m5.large',</p><p class="snippet">                                           train_volume_size = 30L,</p><p class="snippet">                                           train_max_run = 3600L,</p><p class="snippet">                                           input_mode = 'File',</p><p class="snippet">                                           output_path = s3_output,</p><p class="snippet">                                           output_kms_key = NULL,</p><p class="snippet">                                           base_job_name = NULL,</p><p class="snippet">                                           sagemaker_session = NULL)</p></li>
				<li>Set the hyperparameters of interest for the model and define the training and validation datasets for the model as a list:<p class="snippet">estimator$set_hyperparameters(num_round = 100L)</p><p class="snippet">job_name &lt;- paste('sagemaker-train-xgboost', format(Sys.time(), '%H-%M-%S'), </p><p class="snippet">                             sep = '-')</p><p class="snippet">input_data &lt;- list('train' = s3_train_input,</p><p class="snippet">                   'validation' = s3_test_input)</p><h4>Note</h4><p class="callout">You can read more about the different types of instances that can be used for the purpose of model training at https://aws.amazon.com/sagemaker/pricing/instance-types/.</p></li>
				<li>Train/fit the model we defined. The model training process will take a while (~10-12 minutes). In the background, SageMaker will provision an instance that was defined by us in the model definition, trigger the necessary background operations to orchestrate the training process, and finally, train the model:<p class="snippet">estimator$fit(inputs = input_data, job_name = job_name)</p></li>
				<li>Deploy the train model as an endpoint. We will have to, again, provide the type of instance we would want SageMaker to provision for the model inference. Since this is just a sample model, we can choose the instance with the lowest configuration. This process will also take some time, as SageMaker will orchestrate a series of services in the background to deploy the model as an endpoint:<p class="snippet">model_endpoint &lt;- estimator$deploy(initial_instance_count = 1L,</p><p class="snippet">                                   instance_type = 'ml.t2.medium')</p><p class="snippet">model_endpoint$content_type &lt;- 'text/csv'</p><p class="snippet">model_endpoint$serializer &lt;- sagemaker$predictor$csv_serializer</p><p>After the model endpoint is created, we can test it by invoking it with the right form of test data. Since we are saving the test data as CSV files, we will pass comma-separated text to be serialized into JSON format. Therefore, we specify <strong class="inline">text/csv</strong> and <strong class="inline">csv_serializer</strong> for the endpoint. Let's prepare a sample test data feed for a quick test.</p></li>
				<li>First, make a copy of the test dataset using the following command:<p class="snippet">one_test &lt;- test</p></li>
				<li>Next, delete the target variable:<p class="snippet">one_test$diabetes&lt;-NULL</p></li>
				<li>Create a single test sample using the following command:<p class="snippet">test_sample &lt;- as.matrix(one_test[7, ])</p></li>
				<li>Now, delete the column names:<p class="snippet">dimnames(test_sample)[[2]] &lt;- NULL</p></li>
				<li>Make a prediction using the model endpoint on the sample data that we created in the previous step. Invoke the SageMaker endpoint and pass the test data:<p class="snippet">predictions &lt;- model_endpoint$predict(test_sample)</p></li>
				<li>Now, print the result using the <strong class="inline">print</strong> command:<p class="snippet">print(ifelse(predictions&gt;0.5,"yes",'no'))</p><p>The output is as follows:</p><p class="snippet">1] "no"</p></li>
			</ol>
			<p>This output helps us to understand that the model has been correctly deployed and is functioning as expected. We can check whether the endpoint is created by navigating to the SageMaker service in the AWS account and opening the <em class="italics">Endpoint</em> section from the right-hand-side sidebar:</p>
			<div>
				<div id="_idContainer279" class="IMG---Figure">
					<img src="image/C12624_08_08.jpg" alt="Figure 8.8: Endpoint page&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 8.8: Endpoint page</h6>
			<p>This endpoint can be used directly by other services within AWS to invoke and get predictions. The only requirement is that the input data should be provided as expected. To enable our model endpoint to be accessible by other services outside AWS, we would need to orchestrate the API request using AWS API Gateway and AWS Lambda.</p>
			<h4>Note</h4>
			<p class="callout">You can access the complete code file on GitHub at https://github.com/TrainingByPackt/Applied-Supervised-Learning-with-R/blob/master/Lesson08/RStudio_SageMaker.R.</p>
			<p>Now, let's study these services a bit before delving into the solution.</p>
			<h2 id="_idParaDest-328"><a id="_idTextAnchor331"/>What is Amazon Lambda?</h2>
			<p><strong class="bold">AWS Lambda</strong> is an event-driven, serverless computing platform provided by Amazon as a part of Amazon Web Services. It is a computing service that runs code in response to events and automatically manages the computing resources required by that code. The service enables us to develop serverless applications. The term serverless indicates that we do not actually need to manage and provision the infrastructure resources; instead, they are managed by the cloud service provider, and we only pay for what we use, say, pay per event or execution. AWS Lambda can be configured to execute a defined function in response to a specific event, say, when someone uploads a new file to a defined S3 bucket or invokes another function or another service in AWS.</p>
			<h2 id="_idParaDest-329"><a id="_idTextAnchor332"/>What is Amazon API Gateway?</h2>
			<p><strong class="bold">Amazon API Gateway</strong> is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. With this service, we can develop REST as well as WebSocket APIs that act as a <em class="italics">front door</em> for applications to access data, business logic, or functionality from other backend services, while protecting the backend services within the private network. These backend services could be any applications that are running on AWS.</p>
			<p>The overall flow of our service can be represented as in the following diagram:</p>
			<div>
				<div id="_idContainer280" class="IMG---Figure">
					<img src="image/C12624_08_09.jpg" alt="Figure 8.9: Workflow of API Gateway, AWS Lambda, and SageMaker&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 8.9: Workflow of API Gateway, AWS Lambda, and SageMaker</h6>
			<p>The client (say, a web browser), calls an Amazon API Gateway's defined action and passes the appropriate parameter values. API Gateway passes the request to AWS Lambda, while it also seals the backend so that AWS Lambda stays and executes in a protected private network.</p>
			<p>In our case, we will use Lambda to help us tailor the data received from API Gateway into an appropriate form that can be consumed by the SageMaker endpoint. This is necessary because there is a difference between the structure of data passed through a REST API and that of what SageMaker expects. The SageMaker model performs the prediction and returns the predicted value to AWS Lambda. The Lambda function parses the returned value and sends it back to API Gateway, after which API Gateway responds to the client with the result. This entire flow is orchestrated without us actually provisioning any infrastructure; it is entirely managed by AWS.</p>
			<h4>Note</h4>
			<p class="callout">This workflow is explained in further detail in a blog post by Amazon. You can read more at https://aws.amazon.com/blogs/machine-learning/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda/.</p>
			<p>There is one additional challenge that we will need to tackle. As of today, AWS Lambda doesn't support R for defining functions. It supports Python, Java, Go, and a few others, but R is not on the list as of now. The lambda function will be in charge of transforming the data passed by the API into the required form. We will leverage Python scripts for this task. In the future, we can expect R to be supported by AWS Lambda.</p>
			<p>Now that we have the required context about the necessary services, let's deploy our model on a serverless application.</p>
			<h2 id="_idParaDest-330"><a id="_idTextAnchor333"/>Building Serverless ML Applications</h2>
			<p>Serverless computing is the new paradigm within cloud computing. It allows us to build and run applications and services without thinking about servers. In reality, the application we build still runs on a cloud server, but the entire process for server management is done by the cloud service provider, such as AWS. By leveraging the serverless platform, we can build and deploy robust, large-scale, complex applications by only focusing on our application code instead of worrying about provisioning, configuring, and managing servers.</p>
			<p>We have explored some important components of the AWS serverless platform such as AWS Lambda in this chapter, and we can now leverage these solutions to build a machine learning application where we can only focus on the core ML code and forget about provisioning infrastructure and scaling applications.</p>
			<h3 id="_idParaDest-331"><a id="_idTextAnchor334"/>Exercise 101: Building a Serverless Application Using API Gateway, AWS Lambda, and SageMaker</h3>
			<p>In this exercise, we will build a machine learning model using AWS SageMaker and deploy it as an endpoint (using automated SageMaker functions). To enable the model endpoint to be invoked by any service (within or outside AWS) we define an AWS Lambda function and expose the endpoint to public networks through API Gateway.</p>
			<p>The aim of this exercise is to create a serverless application that will use the SageMaker model endpoint we created in the previous exercise.</p>
			<p>Perform the following steps:</p>
			<ol>
				<li value="1">Create an IAM role in AWS that will allow Lambda to execute endpoints from the SageMaker service. From the AWS dashboard, search for <strong class="inline">IAM</strong>, and click on the <strong class="bold">Roles</strong> option on the IAM page:<div id="_idContainer281" class="IMG---Figure"><img src="image/C12624_08_10.jpg" alt="Figure 8.10: Creating IAM roles&#13;&#10;"/></div><h6>Figure 8.10: Creating IAM roles</h6></li>
				<li>Once the <strong class="bold">Roles</strong> page loads, click on the <strong class="bold">Create role</strong> option.</li>
				<li>On the <strong class="bold">Create role</strong> page, select <strong class="bold">AWS Service</strong> as the type of trusted entity and <strong class="bold">Lambda</strong> as the service that will use this role. Click on the <strong class="bold">Next: Permissions</strong> button to proceed:<div id="_idContainer282" class="IMG---Figure"><img src="image/C12624_08_11.jpg" alt="Figure 8.11: Selecting the AWS service option&#13;&#10;"/></div><h6>Figure 8.11: Selecting the AWS service option</h6></li>
				<li>On the <strong class="bold">Permissions</strong> page, filter policies using the <strong class="inline">sagemaker</strong> keyword, select the <strong class="inline">AmazonSageMakerFullAccess</strong> policy, and click on the <strong class="bold">Next: Tags</strong> option:<div id="_idContainer283" class="IMG---Figure"><img src="image/C12624_08_12.jpg" alt="Figure 8.12: The Create role screen&#13;&#10;"/></div><h6>Figure 8.12: The Create role screen</h6></li>
				<li>On the <strong class="bold">Tags</strong> page, you can directly click on <strong class="bold">Next</strong> and proceed to the final page to name the <strong class="bold">Role</strong>.</li>
				<li>Now, on the final page, add a suitable name (say, <strong class="inline">lambda_sagemaker_execution</strong>) and click on <strong class="bold">Create role</strong>. The role will be created for us to use:<div id="_idContainer284" class="IMG---Figure"><img src="image/C12624_08_13.jpg" alt="Figure 8.13: Review page—creating role&#13;&#10;"/></div><h6>Figure 8.13: Review page—creating role</h6></li>
				<li>In the AWS console, search for AWS Lambda and click on the <strong class="bold">Create function</strong> button. The create function page will have some inputs that need to be defined.</li>
				<li>Select the <strong class="bold">Author from scratch</strong> option and give a name to the function (say, <strong class="inline">lambda_sagemaker_connection</strong>).</li>
				<li>Next, select <strong class="bold">Python 3.6</strong> from the <strong class="bold">Runtime</strong> dropdown. For the <strong class="bold">Execution role</strong>, select the <strong class="bold">Use an existing role</strong> option and then select the role we created in the previous step, that is, <strong class="inline">lambda_sagemaker_execution</strong>. Click on the <strong class="bold">Create function</strong> button:<div id="_idContainer285" class="IMG---Figure"><img src="image/C12624_08_14.jpg" alt="Figure 8.14: Creating a function form&#13;&#10;"/></div><h6>Figure 8.14: Creating a function form</h6></li>
				<li>Define a Python function that will accept the input request from the API, parse the payload, and invoke the SageMaker endpoint:<p class="snippet">import os, io, boto3, json, csv</p><p class="snippet"># grab environment variables</p><p class="snippet">ENDPOINT_NAME = os.environ['ENDPOINT_NAME']</p><p class="snippet">runtime= boto3.client('runtime.sagemaker')</p><h4>Note</h4><p class="callout">The endpoint name will be available in the SageMaker page under the endpoint section.</p></li>
				<li>We will additionally define the environment variable for the function that will store the SageMaker endpoint:<p class="snippet">def lambda_handler(event, context):</p><p class="snippet">    print("Received event: " + json.dumps(event, indent=2))</p><p class="snippet">    </p><p class="snippet">    #Load JSON data from API call</p><p class="snippet">    data = json.loads(json.dumps(event))</p><p class="snippet">    payload = data['data']</p><p class="snippet">    print(payload)</p><p class="snippet">    </p><p class="snippet">    #Invoke Sagemaker endpoint and pass Payload</p><p class="snippet">    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,</p><p class="snippet">                                       ContentType='text/csv',</p><p class="snippet">                                       Body=bytes(str(payload),'utf'))</p><p class="snippet">    </p><p class="snippet">    result = json.loads(response['Body'].read().decode())</p><p class="snippet">    predicted_label = 'yes' if result &gt;0.5 else 'no'</p><p class="snippet">    return predicted_label</p><h4>Note</h4><p class="callout">You can refer to the complete code on GitHub at https://github.com/TrainingByPackt/Applied-Supervised-Learning-with-R/blob/master/Lesson08/Amazon_Lambda_Function.py.</p></li>
				<li>Click on <strong class="bold">Save</strong>, and the Amazon Lambda function is ready. Now, create an API using Amazon API Gateway by searching for <strong class="inline">API Gateway</strong> in the AWS console, and create a new API function by selecting the following highlighted options in the screenshot. Give the API a suitable name, say, <strong class="inline">api_lambda_connect</strong>:<div id="_idContainer286" class="IMG---Figure"><img src="image/C12624_08_15.jpg" alt="Figure 8.15: Amazon API Gateway&#13;&#10;"/></div><h6>Figure 8.15: Amazon API Gateway</h6></li>
				<li>From the <strong class="bold">Actions</strong> dropdown, select <strong class="bold">Create Resource</strong>, add a suitable resource name, and then click on the <strong class="bold">Create Resource</strong> button:<div id="_idContainer287" class="IMG---Figure"><img src="image/C12624_08_16.jpg" alt="Figure 8.16: Creating a new child resource&#13;&#10;"/></div><h6>Figure 8.16: Creating a new child resource</h6></li>
				<li>Again, from the <strong class="bold">Actions</strong> dropdown, select <strong class="bold">Create Method</strong> and select the method type as <strong class="bold">POST</strong>.</li>
				<li>Select the <strong class="bold">Integration Type</strong> as <strong class="bold">Lambda Function</strong> and mention the <strong class="bold">Lambda Function</strong> name in the input label, as shown in the following screenshot. Next, click on the <strong class="bold">Save</strong> button:<div id="_idContainer288" class="IMG---Figure"><img src="image/C12624_08_17.jpg" alt="Figure 8.17: Creating a Lambda function&#13;&#10;"/></div><h6>Figure 8.17: Creating a Lambda function</h6></li>
				<li>Next, select <strong class="bold">Deploy API</strong> from the <strong class="bold">Actions</strong> dropdown. A small window will open asking for details about the deployment stage to be used. Select <strong class="bold">New Stage</strong> as the deployment stage, provide a suitable name, (say, <strong class="inline">test</strong>) and click on the <strong class="bold">Deploy</strong> option. The API will now be deployed and will be ready to use.</li>
				<li>Once the API is deployed, we can find the URL of the API to be invoked by navigating to the <strong class="bold">test</strong> deployment stage we created and clicking on the innermost member of the tree under the <strong class="bold">test</strong> deployment stage. Click on the <strong class="bold">Save Changes</strong> button and copy the URL to test:<div id="_idContainer289" class="IMG---Figure"><img src="image/C12624_08_18.jpg" alt="Figure 8.18: API-Connect stage&#13;&#10;"/></div><h6>Figure 8.18: API-Connect stage</h6><h4>Note</h4><p class="callout">The structure of the URL will be <strong class="inline">https://****amazonaws.com/[deployment-stage]/[resource-name]/</strong>.</p></li>
				<li>Call the API from Postman. Open Postman, select a <strong class="bold">POST</strong> call, and paste the URL we copied from the API Gateway stage. Then, click on <strong class="bold">Body</strong> and add raw data, that is, the JSON formatted test data, in the body, as shown in the following screenshot:<div id="_idContainer290" class="IMG---Figure"><img src="image/C12624_08_19.jpg" alt="Figure 8.19: Calling the API via Postman&#13;&#10;"/></div><h6>Figure 8.19: Callin<a id="_idTextAnchor335"/>g the API via Postman</h6></li>
				<li>Click on the <strong class="bold">Send</strong> button to invoke the API with the provided raw data. The result is showcased in the lower window.</li>
			</ol>
			<p>We received a prediction of "no," which indicates that the model has been successfully deployed as a serverless application. We can now invoke the API from anywhere in the world in a browser or Postman and get predictions for the model. This API call be integrated with other services in a larger product and can be scaled as and when there is more demand.</p>
			<h2 id="_idParaDest-332"><a id="_idTextAnchor336"/>Deleting All Cloud Resources to Stop Billing</h2>
			<p>All the resources we have provisioned will need to be deleted/terminated to ensure that they are no longer billed. The following steps will need to be performed to ensure that all resources created in the book of the exercise are deleted:</p>
			<ol>
				<li value="1">Log in to CloudFormation and click on <strong class="bold">Delete</strong> stack (the one we provisioned for RStudio).</li>
				<li>Log in to SageMaker, open Endpoints from the right-hand-side sidebar, check the endpoint we created for the exercise, and delete it.</li>
				<li>Log in to AWS Lambda and delete the Lambda function we created for the exercise.</li>
				<li>Log in to AWS API Gateway and delete the API we created for the exercise.</li>
			</ol>
			<p><strong class="bold">Further notes on AWS SageMaker</strong></p>
			<p>We leveraged the existing containers of the algorithm provided by Amazon to train the model. This step was followed to keep things simple. We can bring our own custom trained algorithms to SageMaker and leverage the platform to deploy the model as a service. SageMaker takes care of the entire process of orchestrating the background resources to provision instances, configure model artefacts, and build the endpoint. We would, however, need to provide the data and model artifacts in a specific format for SageMaker to deploy it.</p>
			<h4>Note</h4>
			<p class="callout">Additional details on the process of building custom models can be found at https://docs.aws.amazon.com/sagemaker/latest/dg/build-model.html.</p>
			<h3 id="_idParaDest-333"><a id="_idTextAnchor337"/>Activity 13: Deploy an R Model Using plumber</h3>
			<p>In this activity, we will develop a regression model in R and deploy it as an API endpoint using plumber. We will be using another use case for supervised learning in R, and we will build a regression model using a different dataset, that is, <strong class="bold">Boston Housing</strong>. The dataset is available within R's Mlbench library, which we already installed and provides information on the median price of a house within Boston when given a number of house attributes.</p>
			<p>We will write two R scripts: <strong class="inline">model.r</strong> to house the regression model as well as the prediction function and <strong class="inline">plumber.R</strong> to house the necessary functions to deploy the model as an API endpoint.</p>
			<h4>Note</h4>
			<p class="callout">Additional details about the dataset can be explored at https://www.rdocumentation.org/packages/mlbench/versions/2.1-1/topics/BostonHousing.</p>
			<p>Perform the following steps:</p>
			<ol>
				<li value="1">Create a <strong class="inline">model.r</strong> script, which will load the required libraries, data, and fit a regression model and the necessary functions to make predictions on unseen data.</li>
				<li>Load the <strong class="inline">mlbench</strong> library, which has the data for this activity.</li>
				<li>Load the <strong class="inline">BostonHousing</strong> data into a DataFrame, <strong class="inline">df</strong>.</li>
				<li>Create a train dataset using the first <strong class="inline">400</strong> rows of <strong class="inline">df</strong> and test with the remaining.</li>
				<li>Fit a logistic regression model using the <strong class="inline">lm</strong> function with the dependent variable as <strong class="inline">medv</strong> (median value) and <strong class="inline">10</strong> independent variables, such as, <strong class="inline">crim</strong>, <strong class="inline">zn</strong>, <strong class="inline">indus</strong>, <strong class="inline">chas</strong>, <strong class="inline">nox</strong>, <strong class="inline">rm</strong>, <strong class="inline">age</strong>, <strong class="inline">dis</strong>, <strong class="inline">rad</strong>, and <strong class="inline">tax</strong>.</li>
				<li>Define a model endpoint as <strong class="inline">predict_data</strong>; this will be used as the API endpoint for plumber.</li>
				<li>Within the function, convert the parameters to <strong class="inline">numeric</strong> and <strong class="bold">factor</strong> (since the API call will pass them as a string only).</li>
				<li>Wrap the 10 independent features for the model as a DataFrame named <strong class="inline">sample</strong>, with the same name for the columns.</li>
				<li>Pass the <strong class="inline">sample</strong> DataFrame to the predict function with the model (created in step 4), and return the predictions.</li>
				<li>Load the <strong class="inline">plumber</strong> library.</li>
				<li>Create a plumber object using the <strong class="inline">plumb</strong> function and pass the <strong class="inline">model.r</strong> file (created in part 1).</li>
				<li>Run the plumber object by passing the hostname as <strong class="inline">localhost</strong> or <strong class="inline">127.0.0.1</strong> and a port, say <strong class="inline">8080</strong>.</li>
				<li>Test the deployed model using the browser or Postman and invoke the API.<p>API invocation:</p><p>http://127.0.0.1:8080/predict_data?crim=0.01&amp;zn=18&amp;indus=2.3&amp;chas=0&amp;nox=0.5&amp;rm=6&amp;age=65&amp;dis=4&amp;rad=1&amp;tax=242</p><p>The final output is as follows:</p><p class="snippet">{"Answer":[22.5813]}Note</p><h4>Note</h4><p class="callout">The solution for this activity can be found on page 463.</p></li>
			</ol>
			<h2 id="_idParaDest-334"><a id="_idTextAnchor338"/>Summary</h2>
			<p>In this chapter, we studied how to deploy our machine learning models with traditional server-based deployment strategies using R's plumber, and enhanced approaches using plumber for R with Docker containers. We then studied how serverless applications can be built using cloud services and how we can easily scale applications as needed with minimal code.</p>
			<p>We explored various web services, such as Amazon Lambda, Amazon SageMaker, and Amazon API Gateway, and studied how services can be orchestrated to deploy our machine learning model as a serverless application.</p>
			<p>In the next chapter, we will work on a capstone project by taking up one of the latest research papers based on a real-world problem and reproducing the result.</p>
		</div>
	</body></html>