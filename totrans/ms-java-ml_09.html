<html><head></head><body><div class="chapter" title="Chapter&#xA0;9.&#xA0;Big Data Machine Learning &#x2013; The Final Frontier"><div class="titlepage"><div><div><h1 class="title"><a id="ch09"/>Chapter 9. Big Data Machine Learning – The Final Frontier</h1></div></div></div><p>In recent years, we have seen an exponential growth in data generated by humans and machines. Varied sources, including home sensors, healthcare-related monitoring devices, news feeds, conversations on social media, images, and worldwide commerce transactions—an endless list—contribute to the vast volumes of data generated every day.</p><p>Facebook had 1.28 billion daily active users in March 2017 sharing close to four million pieces of unstructured information as text, images, URLs, news, and videos (Source: Facebook). 1.3 billion Twitter users share approximately 500 million tweets a day (Source: Twitter). <span class="strong"><strong>Internet of Things</strong></span> (<span class="strong"><strong>IoT</strong></span>) sensors in lights, thermostats, sensor in cars, watches, smart <a id="id1709" class="indexterm"/>devices, and so on, will grow from 50 billion to 200 billion by 2020 (Source: IDC estimates). YouTube users upload 300 hours of new video content every five minutes. Netflix has 30 million viewers who stream 77,000 hours of video daily. Amazon has sold approximately 480 million products and has approximately 244 million customers. In the financial sector, the volume of transactional data generated by even a single large institution is enormous—approximately 25 million households in the US have Bank of America, a major financial institution, as their primary bank, and together produce petabytes of data annually. Overall, it is estimated that the global Big Data industry will be worth 43 billion US dollars in 2017 (Source: <a class="ulink" href="http://www.statista.com">www.statista.com</a>).</p><p>Each of the aforementioned companies and many more like them face the real problem of storing all this data (structured and unstructured), processing the data, and learning hidden patterns from the data to increase their revenue and to improve customer satisfaction. We will explore how current methods, tools and technology can help us learn from data in Big Data-scale environments and how as practitioners in the field we must recognize challenges unique to this problem space.</p><p>This chapter has the following structure:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">What are the characteristics of Big Data?</li><li class="listitem" style="list-style-type: disc">Big Data Machine Learning<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">General Big Data Framework:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Big data <a id="id1710" class="indexterm"/>cluster deployments frameworks</li><li class="listitem" style="list-style-type: disc">HortonWorks Data Platform (HDP)</li><li class="listitem" style="list-style-type: disc">Cloudera CDH</li><li class="listitem" style="list-style-type: disc">Amazon Elastic MapReduce (EMR)</li><li class="listitem" style="list-style-type: disc">Microsoft <a id="id1711" class="indexterm"/>HDInsight</li></ul></div></li><li class="listitem" style="list-style-type: disc">Data acquisition:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Publish-subscribe framework</li><li class="listitem" style="list-style-type: disc">Source-sink framework</li><li class="listitem" style="list-style-type: disc">SQL framework</li><li class="listitem" style="list-style-type: disc">Message queueing framework</li><li class="listitem" style="list-style-type: disc">Custom framework</li></ul></div></li><li class="listitem" style="list-style-type: disc">Data storage:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Hadoop Distributed File System (HDFS)</li><li class="listitem" style="list-style-type: disc">NoSQL</li></ul></div></li><li class="listitem" style="list-style-type: disc">Data processing and preparation:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Hive and Hive Query Language (HQL)</li><li class="listitem" style="list-style-type: disc">Spark SQL</li><li class="listitem" style="list-style-type: disc">Amazon Redshift</li><li class="listitem" style="list-style-type: disc">Real-time <a id="id1712" class="indexterm"/>stream processing</li></ul></div></li><li class="listitem" style="list-style-type: disc">Machine Learning</li><li class="listitem" style="list-style-type: disc">Visualization and analysis</li></ul></div></li><li class="listitem" style="list-style-type: disc">Batch Big Data Machine Learning<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">H2O:</li><li class="listitem" style="list-style-type: disc">H2O architecture<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Machine learning in H2O</li><li class="listitem" style="list-style-type: disc">Tools and usage</li><li class="listitem" style="list-style-type: disc">Case study</li><li class="listitem" style="list-style-type: disc">Business problems</li><li class="listitem" style="list-style-type: disc">Machine Learning mapping</li><li class="listitem" style="list-style-type: disc">Data collection</li><li class="listitem" style="list-style-type: disc">Data sampling and transformation</li><li class="listitem" style="list-style-type: disc">Experiments, results, and analysis</li></ul></div></li><li class="listitem" style="list-style-type: disc">Spark MLlib:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Spark architecture</li><li class="listitem" style="list-style-type: disc">Machine Learning in MLlib</li><li class="listitem" style="list-style-type: disc">Tools and usage</li><li class="listitem" style="list-style-type: disc">Experiments, results, and analysis</li></ul></div></li></ul></div></li><li class="listitem" style="list-style-type: disc">Real-time Big Data Machine Learning<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Scalable Advanced Massive Online Analysis (SAMOA):<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">SAMOA architecture</li><li class="listitem" style="list-style-type: disc">Machine Learning algorithms</li><li class="listitem" style="list-style-type: disc">Tools and usage</li><li class="listitem" style="list-style-type: disc">Experiments, results, and analysis</li><li class="listitem" style="list-style-type: disc">The <a id="id1713" class="indexterm"/>future of Machine Learning</li></ul></div></li></ul></div></li></ul></div><div class="section" title="What are the characteristics of Big Data?"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec73"/>What are the characteristics of Big Data?</h1></div></div></div><p>There <a id="id1714" class="indexterm"/>are many characteristics of Big Data that are different than normal data. Here we highlight them as four <span class="emphasis"><em>V</em></span>s that characterize Big Data. Each of these <a id="id1715" class="indexterm"/>makes it necessary to use specialized tools, frameworks, and algorithms for data acquisition, storage, processing, and analytics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Volume</strong></span>: One of<a id="id1716" class="indexterm"/> the characteristic of Big Data is the size of the content, structured or unstructured, which doesn't fit the storage capacity or processing power available on a single machine and therefore needs multiple machines.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Velocity</strong></span>: Another <a id="id1717" class="indexterm"/>characteristic of Big Data is the rate at which the content is generated, which contributes to volume but needs to be handled in a time sensitive manner. Social media content and IoT sensor information are the best examples of high velocity Big Data.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Variety</strong></span>: This generally <a id="id1718" class="indexterm"/>refers to multiple formats in which data exists, that is, structured, semi-structured, and unstructured and furthermore, each of them has different forms. Social media content with images, video, audio, text, and structured information about activities, background, networks, and so on, is the best example of where data from various sources must be analyzed.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Veracity</strong></span>: This refers <a id="id1719" class="indexterm"/>to a wide variety of factors such as noise, uncertainty, biases, and abnormality in the data that must be addressed, especially given the volume, velocity, and variety of data. One of the key steps, as we will discuss in the context of Big Data Machine Learning, is processing and cleaning such "unclean" data.</li></ul></div><p>Many have added other characteristics such as value, validity, and volatility to the preceding list, but we believe they are largely derived from the previous four.</p></div></div>
<div class="section" title="Big Data Machine Learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec74"/>Big Data Machine Learning</h1></div></div></div><p>In this section, we will discuss the general flow and components that are required for Big Data <a id="id1720" class="indexterm"/>Machine Learning. Although many of the components, such as data acquisition or storage, are not directly related to Machine Learning methodologies, they inevitably have an impact on the frameworks and processes. Giving a complete catalog of the available components and tools is beyond the scope of this book, but we will discuss the general responsibilities of the tasks involved and give some information on the techniques and tools available to accomplish them.</p><div class="section" title="General Big Data framework"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec150"/>General Big Data framework</h2></div></div></div><p>The general Big Data framework is illustrated in the following figure:</p><div class="mediaobject"><img src="graphics/B05137_09_001.jpg" alt="General Big Data framework"/><div class="caption"><p>Figure 1: Big data framework</p></div></div><p>The <a id="id1721" class="indexterm"/>choice of how the Big Data framework <a id="id1722" class="indexterm"/>is set up and deployed in the cluster is one of the decisions that affects the choice of tools, techniques, and cost. The data acquisition or collection component is the first step and it consists of several techniques, both synchronous and asynchronous, to absorb data into the system. Various techniques ranging from publish-subscribe, source-sink, relational database queries, and custom data connectors are available in the components.</p><p>Data storage choices ranging from distributed filesystems such as HDFS to non-relational databases (NoSQL) are available based on various other functional requirements. NoSQL databases are described in the section on <span class="emphasis"><em>Data Storage</em></span>.</p><p>Data preparation, or transforming the large volume of stored data so that it is consumable by the Machine Learning analytics, is an important processing step. This has some dependencies on the frameworks, techniques, and tools used in storage. It also has some dependency on the next step: the choice of Machine Learning analytics/frameworks that will be used. There are a wide range of choices for processing frameworks that will be discussed in the following sub-section.</p><p>Recall that, in batch learning, the model is trained simultaneously on a number of examples that have been previously collected. In contrast to batch learning, in real-time learning model training is continuous, each new instance that arrives becoming part of a dynamic training set. See <a class="link" href="ch05.html" title="Chapter 5. Real-Time Stream Machine Learning">Chapter 5</a>, <span class="emphasis"><em>Real-Time Stream Machine Learning</em></span> for details. Once the data is collected, stored, and transformed based on the domain requirements, different Machine Learning methodologies can be employed, including batch learning, real-time learning, and batch-real-time mixed learning. Whether one selects supervised learning, unsupervised learning, or a combination of the two also depends on the data, the availability of labels, and label quality. These will be discussed in detail later in this chapter.</p><p>The results of <a id="id1723" class="indexterm"/>analytics during the development stage as well as the <a id="id1724" class="indexterm"/>production or runtime stage also need to be stored and visualized for humans and automated tasks.</p><div class="section" title="Big Data cluster deployment frameworks"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec164"/>Big Data cluster deployment frameworks</h3></div></div></div><p>There <a id="id1725" class="indexterm"/>are many frameworks <a id="id1726" class="indexterm"/>that are built on the core Hadoop (<span class="emphasis"><em>References</em></span> [3]) open source platform. Each of them provides a number of tools for the Big Data components described previously.</p><div class="section" title="Hortonworks Data Platform"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl4sec240"/>Hortonworks Data Platform</h4></div></div></div><p>
<span class="strong"><strong>Hortonworks Data Platform</strong></span> (<span class="strong"><strong>HDP</strong></span>) provides an open source distribution comprising<a id="id1727" class="indexterm"/> various components in its stack, from data acquisition to visualization. Apache Ambari <a id="id1728" class="indexterm"/>is often the user interface used for managing services and provisioning and monitoring clusters. The following screenshot depicts Ambari used for configuring various services and the health-check dashboard:</p><div class="mediaobject"><img src="graphics/B05137_09_002.jpg" alt="Hortonworks Data Platform"/><div class="caption"><p>Figure 2: Ambari dashboard user interface</p></div></div></div><div class="section" title="Cloudera CDH"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl4sec241"/>Cloudera CDH</h4></div></div></div><p>Like HDP, Cloudera CDH (<span class="emphasis"><em>References</em></span> [4]) provides similar services and Cloudera Services <a id="id1729" class="indexterm"/>Manager can be used <a id="id1730" class="indexterm"/>in a similar way to Ambari for cluster management and health checks, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/B05137_09_003.jpg" alt="Cloudera CDH"/><div class="caption"><p>Figure 3: Cloudera Service Manager user interface</p></div></div></div><div class="section" title="Amazon Elastic MapReduce"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl4sec242"/>Amazon Elastic MapReduce</h4></div></div></div><p>Amazon <a id="id1731" class="indexterm"/>Elastic MapReduce (EMR) (<span class="emphasis"><em>References</em></span> [5]) is another Big Data cluster, platform <a id="id1732" class="indexterm"/>similar to HDP and Cloudera, which supports <a id="id1733" class="indexterm"/>a wide variety of frameworks. EMR has two modes—<span class="strong"><strong>cluster mode</strong></span> and <span class="strong"><strong>step execution mode</strong></span>. In cluster mode, you choose the Big Data stack vendor EMR or <a id="id1734" class="indexterm"/>MapR and in step execution mode, you give jobs ranging from JARs to SQL queries for execution. In the following screenshot, we see the interface for configuring a new cluster as well as defining a new job flow:</p><div class="mediaobject"><img src="graphics/B05137_09_004.jpg" alt="Amazon Elastic MapReduce"/><div class="caption"><p>Figure 4: Amazon Elastic MapReduce cluster management user interface</p></div></div></div><div class="section" title="Microsoft Azure HDInsight"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl4sec243"/>Microsoft Azure HDInsight</h4></div></div></div><p>Microsoft <a id="id1735" class="indexterm"/>Azure HDInsight (<span class="emphasis"><em>References</em></span> [6]) is another platform that allows cluster management with most <a id="id1736" class="indexterm"/>of the services that are required, including storage, processing, and Machine Learning. The Azure portal, as shown in the following screenshot, is used to create, manage, and help in learning the statuses of the various components of the cluster:</p><div class="mediaobject"><img src="graphics/B05137_09_005.jpg" alt="Microsoft Azure HDInsight"/><div class="caption"><p>Figure 5: Microsoft Azure HDInsight cluster management user interface</p></div></div></div></div><div class="section" title="Data acquisition"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec165"/>Data acquisition</h3></div></div></div><p>In the <a id="id1737" class="indexterm"/>Big Data framework, the acquisition component <a id="id1738" class="indexterm"/>plays an important role in collecting the data from disparate source systems and storing it in Big Data storage. Based on types of source and volume, velocity, functional, and performance-based requirements, there are a wide variety of acquisition frameworks and tools. We will describe a few of the most well-known frameworks and tools used to give readers some insight.</p><div class="section" title="Publish-subscribe frameworks"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl4sec244"/>Publish-subscribe frameworks</h4></div></div></div><p>In <a id="id1739" class="indexterm"/>publish-subscribe based frameworks, the publishing source pushes the data in different formats to the broker, which <a id="id1740" class="indexterm"/>has different subscribers waiting to consume them. The publisher and subscriber are unaware of each other, with the broker mediating in between.</p><p>
<span class="strong"><strong>Apache Kafka</strong></span> (<span class="emphasis"><em>References</em></span> [9]) and <span class="strong"><strong>Amazon Kinesis</strong></span> are two well-known implementations <a id="id1741" class="indexterm"/>that are based on this model. Apache Kafka  defines the concepts of publishers, consumers, and topics—on which things get published and <a id="id1742" class="indexterm"/>consumed—and a broker to <a id="id1743" class="indexterm"/>manage the topics. Amazon Kinesis <a id="id1744" class="indexterm"/>is built on similar concepts with producers and consumers connected through Kinesis streams, which are similar to topics in Kafka.</p></div><div class="section" title="Source-sink frameworks"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl4sec245"/>Source-sink frameworks</h4></div></div></div><p>In <a id="id1745" class="indexterm"/>source-sink models, sources push the data <a id="id1746" class="indexterm"/>into the framework and the framework pushes the system to the sinks. Apache Flume (<span class="emphasis"><em>References</em></span> [7]) is a well-known implementation of this kind of framework with a variety of sources, channels to buffer the data, and a number of sinks to store the data in the Big Data world.</p></div><div class="section" title="SQL frameworks"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl4sec246"/>SQL frameworks</h4></div></div></div><p>Since <a id="id1747" class="indexterm"/>many traditional data stores are in the form of <a id="id1748" class="indexterm"/>SQL-based RDBMS, SQL-based frameworks provide a generic way to import the data from RDBMS and store it in Big Data, mainly in the HDFS format. Apache Sqoop (<span class="emphasis"><em>References</em></span> [10]) is a well-known implementation that can import data from any JDBC-based RDBMS and store it in HDFS-based systems.</p></div><div class="section" title="Message queueing frameworks"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl4sec247"/>Message queueing frameworks</h4></div></div></div><p>Message <a id="id1749" class="indexterm"/>queueing frameworks are push-pull based frameworks similar to publisher-subscriber systems. Message queues separate the producers and consumers and can store the data in the queue, in an asynchronous <a id="id1750" class="indexterm"/>communication pattern. Many <a id="id1751" class="indexterm"/>protocols have been developed on this such as Advanced Message Queueing Protocol (AMQP) and ZeroMQ Message Transfer Protocol (ZMTP). RabbitMQ, ZeroMQ, Amazon SQS, and so on, are some well-known implementations of this framework.</p></div><div class="section" title="Custom frameworks"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl4sec248"/>Custom frameworks</h4></div></div></div><p>Specialized <a id="id1752" class="indexterm"/>connectors for different sources such <a id="id1753" class="indexterm"/>as IoT, HTTP, WebSockets, and so on, have resulted in many specific connectors such as Amazon IoT Hub, REST-connectors, WebSocket, and so on.</p></div></div><div class="section" title="Data storage"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec166"/>Data storage</h3></div></div></div><p>The <a id="id1754" class="indexterm"/>data storage component plays a key part in connecting <a id="id1755" class="indexterm"/>the acquisition and the rest of the components together. Performance, impact on data processing, cost, high-availability, ease of management, and so on, should be taken into consideration while deciding on data storage. For pure real-time or near real-time systems there are in-memory based frameworks for storage, but for batch-based systems there are mainly distributed File Systems such as HDFS or NoSQL.</p><div class="section" title="HDFS"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl4sec249"/>HDFS</h4></div></div></div><p>HDFS can <a id="id1756" class="indexterm"/>run on a large cluster of nodes and provide all the important features <a id="id1757" class="indexterm"/>such as high-throughput, replications, fail-over, and so on.</p><div class="mediaobject"><img src="graphics/B05137_09_006.jpg" alt="HDFS"/></div><p>The basic architecture of HDFS <a id="id1758" class="indexterm"/>has the following components:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>NameNode</strong></span>: The HDFS <a id="id1759" class="indexterm"/>client always sends the request to the NameNode, which keeps the metadata of the file while the real data is distributed in blocks on the DataNodes. NameNodes are only responsible for handling opening and closing a file while the remaining interactions of reading, writing, and appending happen between clients and the data nodes. The NameNode stores the metadata in two files: <code class="literal">fsimage</code> and <code class="literal">edit</code> files. The <code class="literal">fsimage</code> contains the filesystem metadata as a snapshot, while edit files contain the incremental changes to the metadata.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Secondary NameNode</strong></span>: Secondary <a id="id1760" class="indexterm"/>NameNode provides redundancy to the metadata in the NameNode by keeping a copy of the <code class="literal">fsimage</code> and <code class="literal">edit</code> files at every predefined <a id="id1761" class="indexterm"/>checkpoint.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>DataNode</strong></span>: DataNodes <a id="id1762" class="indexterm"/>manage the actual blocks of data and facilitate read-write operation on these datablocks. DataNodes keep communicating with the NameNodes using heartbeat signals <a id="id1763" class="indexterm"/>indicating they are alive. The data blocks stored <a id="id1764" class="indexterm"/>in DataNodes are also replicated for redundancy. Replication of the data blocks in the DataNodes is governed by the rack-aware placement policy.</li></ul></div></div><div class="section" title="NoSQL"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl4sec250"/>NoSQL</h4></div></div></div><p>Non-relational <a id="id1765" class="indexterm"/>databases, also referred to as NoSQL databases, are <a id="id1766" class="indexterm"/>gaining enormous popularity in the Big Data world. High throughput, better horizontal scaling, improved performance on retrieval, and storage at the cost of weaker consistency models are notable characteristics of most NoSQL databases. We will discuss some important forms of NoSQL database in this section along with implementations.</p><div class="section" title="Key-value databases"><div class="titlepage"><div><div><h5 class="title"><a id="ch09lvl5sec100"/>Key-value databases</h5></div></div></div><p>Key-value <a id="id1767" class="indexterm"/>databases are the most prominent NoSQL <a id="id1768" class="indexterm"/>databases used mostly for semi-structured or unstructured data. As the name suggests, the structure of storage is quite basic, with unique keys associating the data values that can be of any type including string, integer, double precision, and so on—even BLOBS. Hashing the keys for quick lookup and retrieval of the values together with partitioning the data across multiple nodes gives high throughput and scalability. The query capabilities are very limited. Amazon DynamoDB, Oracle NoSQL, MemcacheDB, and so on, are examples of key-value databases. </p></div><div class="section" title="Document databases"><div class="titlepage"><div><div><h5 class="title"><a id="ch09lvl5sec101"/>Document databases</h5></div></div></div><p>Document <a id="id1769" class="indexterm"/>databases store semi-structured data in the form <a id="id1770" class="indexterm"/>of XML, JSON, or YAML documents, to name some of the most popular formats. The documents have unique keys to which they are mapped. Though it is possible to store documents in key-value stores, the query capabilities offered by document stores are greater as the primitives making up the structure of the document—which may include names or attributes—can also be used for retrieval. When the data is ever-changing and has variable numbers or lengths of fields, document databases are often a good choice. Document databases do not offer join capabilities and <a id="id1771" class="indexterm"/>hence all information needs to be captured in the <a id="id1772" class="indexterm"/>document values. MongoDB, ElasticSearch, Apache Solr, and so on, are some well-known implementations of document databases.</p></div><div class="section" title="Columnar databases"><div class="titlepage"><div><div><h5 class="title"><a id="ch09lvl5sec102"/>Columnar databases</h5></div></div></div><p>The <a id="id1773" class="indexterm"/>use of columns as the basic unit of storage with name, value, and often timestamp, differentiates columnar databases from traditional relational <a id="id1774" class="indexterm"/>databases. Columns are further combined to form column families. A row is indexed by the row key and has multiple column families associated with the row. Certain rows can use only column families that are populated, giving it a good storage representation in sparse data. Columnar databases do not have fixed schema-like <a id="id1775" class="indexterm"/>relational databases; new columns and families can be added <a id="id1776" class="indexterm"/>at any time, giving them a significant advantage. <span class="strong"><strong>HBase</strong></span>, <span class="strong"><strong>Cassandra</strong></span>, and <span class="strong"><strong>Parquet</strong></span> are some well-known <a id="id1777" class="indexterm"/>implementations of columnar databases.</p></div><div class="section" title="Graph databases"><div class="titlepage"><div><div><h5 class="title"><a id="ch09lvl5sec103"/>Graph databases</h5></div></div></div><p>In <a id="id1778" class="indexterm"/>many applications, the data has an inherent graph structure <a id="id1779" class="indexterm"/>with nodes and links. Storing such data in graph databases makes it more efficient for storage, retrieval, and queries. The nodes have a set of attributes and <a id="id1780" class="indexterm"/>generally represent entities, while links represent relationships <a id="id1781" class="indexterm"/>between the nodes that can be directed or undirected. <span class="strong"><strong>Neo4J</strong></span>, <span class="strong"><strong>OrientDB</strong></span>, and <span class="strong"><strong>ArangoDB</strong></span> are some<a id="id1782" class="indexterm"/> well-known implementations of graph databases.</p></div></div></div><div class="section" title="Data processing and preparation"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec167"/>Data processing and preparation</h3></div></div></div><p>The <a id="id1783" class="indexterm"/>data preparation step involves various preprocessing <a id="id1784" class="indexterm"/>steps before the data is ready to be consumed by analytics and machine learning algorithms. Some of the key tasks involved are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Data cleansing</strong></span>: Involves <a id="id1785" class="indexterm"/>everything from correcting errors, type <a id="id1786" class="indexterm"/>matching, normalization of elements, and so on, on the raw data.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Data scraping and curating</strong></span>: Converting data elements and normalizing the data from one structure to another.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Data transformation</strong></span>: Many analytical algorithms need features that are aggregates built on raw or historical data. Transforming and computing those extra features are done in this step.</li></ul></div><div class="section" title="Hive and HQL"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl4sec251"/>Hive and HQL</h4></div></div></div><p>Apache Hive (<span class="emphasis"><em>References</em></span> [11]) is a powerful tool for performing various data preparation activities <a id="id1787" class="indexterm"/>in HDFS systems. Hive organizes the underlying HDFS data <a id="id1788" class="indexterm"/>a of structure that is similar to relational <a id="id1789" class="indexterm"/>databases. HQL is like SQL and helps in performing <a id="id1790" class="indexterm"/>various aggregates, transformations, cleanup, and <a id="id1791" class="indexterm"/>normalization, and the data is then serialized back to HDFS. The logical <a id="id1792" class="indexterm"/>tables in Hive are partitioned across and sub-divided into buckets for speed-up. Complex joins and aggregate queries in Hive are automatically converted into MapReduce jobs for throughput and speed-up.</p></div><div class="section" title="Spark SQL"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl4sec252"/>Spark SQL</h4></div></div></div><p>Spark <a id="id1793" class="indexterm"/>SQL, which is a major component of Apache Spark (<span class="emphasis"><em>References</em></span> [1] and [2]), provides SQL-like functionality—similar to what HQL provides—for <a id="id1794" class="indexterm"/>performing changes to the Big Data. Spark SQL can <a id="id1795" class="indexterm"/>work with underlying data storage systems such as Hive or NoSQL databases such as Parquet. We will touch upon some aspects of Spark SQL in the section on Spark later.</p></div><div class="section" title="Amazon Redshift"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl4sec253"/>Amazon Redshift</h4></div></div></div><p>Amazon <a id="id1796" class="indexterm"/>Redshift provides several warehousing <a id="id1797" class="indexterm"/>capabilities especially on Amazon <a id="id1798" class="indexterm"/>EMR setups. It can process petabytes <a id="id1799" class="indexterm"/>of data using its <span class="strong"><strong>massively parallel processing</strong></span> (<span class="strong"><strong>MPP</strong></span>) data warehouse architecture.</p></div><div class="section" title="Real-time stream processing"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl4sec254"/>Real-time stream processing</h4></div></div></div><p>In <a id="id1800" class="indexterm"/>many Big Data deployments, processing <a id="id1801" class="indexterm"/>and performing the transformations <a id="id1802" class="indexterm"/>specified previously must be done on the stream of <a id="id1803" class="indexterm"/>data in real time rather than from stored batch data. There are various <span class="strong"><strong>Stream Processing Engines</strong></span> (<span class="strong"><strong>SPE</strong></span>) such as Apache Storm (<span class="emphasis"><em>References</em></span> [12]) and Apache Samza, and in-memory processing engines such as Spark-Streaming that are used for stream processing.</p></div></div><div class="section" title="Machine Learning"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec168"/>Machine Learning</h3></div></div></div><p>Machine <a id="id1804" class="indexterm"/>learning helps to perform descriptive, predictive, and prescriptive analysis on Big Data. There are two broad extremes <a id="id1805" class="indexterm"/>that will be covered in this chapter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Machine learning can be done on batch historical data and then the learning/models can be applied to new batch/real-time data</li><li class="listitem" style="list-style-type: disc">Machine learning can be done on real-time data and applied simultaneously to the real-time data</li></ul></div><p>Both topics are covered at length in the remainder of this chapter.</p></div><div class="section" title="Visualization and analysis"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec169"/>Visualization and analysis</h3></div></div></div><p>With <a id="id1806" class="indexterm"/>batch learning done at modeling time and <a id="id1807" class="indexterm"/>real-time learning done at runtime, predictions—the output of applying the models to new data—must be stored in some data structure and then analyzed by the users. Visualization tools and other reporting tools are frequently used to extract and present information to the users. Based on the domain and the requirements of the users, the analysis and visualization can be static, dynamic, or interactive. </p><p>Lightning is a framework for performing interactive visualizations on the Web with different binding APIs using REST for Python, R, Scala, and JavaScript languages.</p><p>Pygal and Seaborn are Python-based libraries that help in plotting all possible charts and graphs in Python for analysis, reporting, and visualizations.</p></div></div></div>
<div class="section" title="Batch Big Data Machine Learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec75"/>Batch Big Data Machine Learning</h1></div></div></div><p>Batch <a id="id1808" class="indexterm"/>Big Data Machine Learning involves two basic steps, as discussed in <a class="link" href="ch02.html" title="Chapter 2. Practical Approach to Real-World Supervised Learning">Chapter 2</a>, <span class="emphasis"><em>Practical Approach to Real-World Supervised Learning</em></span>, <a class="link" href="ch03.html" title="Chapter 3. Unsupervised Machine Learning Techniques">Chapter 3</a>, <span class="emphasis"><em>Unsupervised Machine Learning Techniques</em></span>, and <a class="link" href="ch04.html" title="Chapter 4. Semi-Supervised and Active Learning">Chapter 4</a>, <span class="emphasis"><em>Semi-Supervised and Active Learning</em></span>: learning or training data from historical datasets and applying the learned models to unseen future data. The following figure demonstrates the two environments along with the component tasks and some technologies/frameworks that accomplish them:</p><div class="mediaobject"><img src="graphics/B05137_09_007.jpg" alt="Batch Big Data Machine Learning"/><div class="caption"><p>Figure 6: Model time and run time components for Big Data and providers</p></div></div><p>We will discuss two of the most well-known frameworks for doing Machine Learning in the <a id="id1809" class="indexterm"/>context of batch data and will use the case study to highlight either the code or tools to perform modeling.</p><div class="section" title="H2O as Big Data Machine Learning platform"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec151"/>H2O as Big Data Machine Learning platform</h2></div></div></div><p>H2O (<span class="emphasis"><em>References</em></span> [13]) is a leading open source platform for Machine Learning at Big Data scale, with a <a id="id1810" class="indexterm"/>focus on bringing AI <a id="id1811" class="indexterm"/>to the enterprise. The company was founded in 2011 and counts several leading lights in statistical learning theory and optimization among its scientific advisors. It supports programming environments in multiple languages. While the H2O software is freely available, customer service and custom extensions to the product can be purchased.</p><div class="section" title="H2O architecture"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec170"/>H2O architecture</h3></div></div></div><p>The <a id="id1812" class="indexterm"/>following figure gives a high-level architecture of H2O with important components. H2O can access data from various data stores such as HDFS, SQL, NoSQL, and Amazon S3, to name a few. The most popular deployment of H2O is to use one of the deployment stacks discussed earlier with Spark or to run it in a H2O cluster itself.</p><p>The core of H2O is an optimized way of handling Big Data in memory, so that iterative algorithms that go through the same data can be handled efficiently and achieve good performance. Important Machine Learning algorithms in supervised and unsupervised learning are implemented specially to handle horizontal scalability across multiple nodes and JVMs. H2O provides not only its own user interface, known as flow, to manage and run modeling <a id="id1813" class="indexterm"/>tasks, but also has different language bindings and connector APIs to Java, R, Python, and Scala.</p><div class="mediaobject"><img src="graphics/B05137_09_008.jpg" alt="H2O architecture"/><div class="caption"><p>Figure 7: H2O high level architecture</p></div></div><p>Most Machine Learning algorithms, optimization algorithms, and utilities use the concept of fork-join or MapReduce. As shown in <span class="emphasis"><em>Figure 8</em></span>, the entire dataset is considered as a <span class="strong"><strong>Data Frame</strong></span> in H2O, and <a id="id1814" class="indexterm"/> comprises vectors, which are features or columns in the dataset. The rows or instances are made up of one element from each Vector arranged side-by-side. The rows are grouped together to form a processing unit <a id="id1815" class="indexterm"/>known as a <span class="strong"><strong>Chunk</strong></span>. Several chunks are combined in one JVM. Any algorithmic or optimization work begins by sending the information from the topmost JVM to fork on to the next JVM, then on to the next, and so on, similar to the map operation in MapReduce. Each JVM works on the  rows in the chunks to establish the task and finally the results flow back in the reduce operation:</p><div class="mediaobject"><img src="graphics/B05137_09_009.jpg" alt="H2O architecture"/><div class="caption"><p>Figure 8: H2O distributed data processing using chunking</p></div></div></div><div class="section" title="Machine learning in H2O"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec171"/>Machine learning in H2O</h3></div></div></div><p>The <a id="id1816" class="indexterm"/>following figure shows all the Machine Learning algorithms <a id="id1817" class="indexterm"/>supported in H2O v3 for supervised and unsupervised learning:</p><div class="mediaobject"><img src="graphics/B05137_09_010.jpg" alt="Machine learning in H2O"/><div class="caption"><p>Figure 9: H2O v3 Machine learning algorithms</p></div></div></div><div class="section" title="Tools and usage"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec172"/>Tools and usage</h3></div></div></div><p>H2O Flow <a id="id1818" class="indexterm"/>is an interactive web application that helps data scientists to perform <a id="id1819" class="indexterm"/>various tasks from importing data to running complex models using point and click and wizard-based concepts. </p><p>H2O is run in local mode as:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java –Xmx6g –jar h2o.jar</strong></span>
</pre></div><p>The default way to start Flow is to point your browser and go to the following URL: <code class="literal">http://192.168.1.7:54321/</code>. The right-side of Flow captures every user action performed under the tab <span class="strong"><strong>OUTLINE</strong></span>. The actions taken can be edited and saved as named flows for reuse and collaboration, as shown in <span class="emphasis"><em>Figure 10</em></span>:</p><div class="mediaobject"><img src="graphics/B05137_09_011.jpg" alt="Tools and usage"/><div class="caption"><p>Figure 10: H2O Flow in the browser</p></div></div><p>
<span class="emphasis"><em>Figure 11</em></span> shows the interface for importing files from the local filesystem or HDFS and displays detailed summary statistics as well as next actions that can be performed on the dataset. Once the data is imported, it gets a data frame reference in the H2O framework with the extension of <code class="literal">.hex</code>. The summary statistics are useful in understanding the characteristics of data such as <span class="strong"><strong>missing</strong></span>, <span class="strong"><strong>mean</strong></span>, <span class="strong"><strong>max</strong></span>, <span class="strong"><strong>min</strong></span>, and so on. It also has an easy way to transform the features from one type to another, for example, numeric features with a few unique values to categorical/nominal types known as <code class="literal">enum</code> in H2O.</p><p>The actions that can be performed on the datasets are:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Visualize the data.</li><li class="listitem">Split the data into different sets such as training, validation, and testing.</li><li class="listitem">Build supervised and unsupervised models.</li><li class="listitem">Use the models to predict.</li><li class="listitem">Download and export the files in various formats.<div class="mediaobject"><img src="graphics/B05137_09_012.jpg" alt="Tools and usage"/><div class="caption"><p>Figure 11: Importing data as frames, summarizations, and actions that can be performed</p></div></div></li></ol></div><p>Building supervised or unsupervised models in H2O is done through an interactive screen. Every <a id="id1820" class="indexterm"/>modeling algorithm has its parameters classified into three sections: basic, advanced, and expert. Any parameter that supports hyper-parameter searches for <a id="id1821" class="indexterm"/>tuning the model has a checkbox grid next to it, and more than one parameter value can be used.</p><p>Some basic parameters such as <span class="strong"><strong>training_frame</strong></span>, <span class="strong"><strong>validation_frame</strong></span>, and <span class="strong"><strong>response_column</strong></span>, are common to every supervised algorithm; others are specific to model types, such as the choice of solver for GLM, the activation function for deep learning, and so on. All such common parameters are available in the basic section. Advanced parameters are settings that afford greater flexibility and control to the modeler if the default behavior must be overridden. Several of these parameters are also common across some algorithms—two examples are the choice of method for assigning the fold index (if cross-validation was selected in the basic section), and selecting the column containing <a id="id1822" class="indexterm"/>weights (if each example is weighted separately), and so on.</p><p>Expert parameters define more complex elements such as how to handle the missing <a id="id1823" class="indexterm"/>values, model-specific parameters that need more than a basic understanding of the algorithms, and other esoteric variables. In <span class="emphasis"><em>Figure 12</em></span>, GLM, a supervised learning algorithm, is being configured with 10-fold cross-validation, binomial (two-class) classification, efficient LBFGS optimization algorithm, and stratified sampling for cross-validation split:</p><div class="mediaobject"><img src="graphics/B05137_09_013.jpg" alt="Tools and usage"/><div class="caption"><p>Figure 12: Modeling algorithm parameters and validations</p></div></div><p>The model results screen contains a detailed analysis of the results using important evaluation charts, depending on the validation method that was used. At the top of the screen are possible actions that can be taken, such as to run the model on unseen data for prediction, download the model as POJO format, export the results, and so on.</p><p>Some of the charts are algorithm-specific, like the scoring history that shows how the training loss or the objective function changes over the iterations in GLM—this gives the user insight into the speed of convergence as well as into the tuning of the iterations parameter. We see the ROC curves and the Area Under Curve metric on the validation data in addition to the gains and lift charts, which give the cumulative capture rate and cumulative lift over the validation sample respectively.</p><p>
<span class="emphasis"><em>Figure 13</em></span> shows <span class="strong"><strong>SCORING HISTORY</strong></span>, <span class="strong"><strong>ROC CURVE</strong></span>, and <span class="strong"><strong>GAINS/LIFT</strong></span> charts for GLM on 10-fold cross-validation on the <code class="literal">CoverType</code> dataset:</p><div class="mediaobject"><img src="graphics/B05137_09_014.jpg" alt="Tools and usage"/><div class="caption"><p>Figure 13: Modeling and validation ROC curves, objective functions, and lift/gain charts</p></div></div><p>The output <a id="id1824" class="indexterm"/>of validation gives detailed evaluation measures such as accuracy, AUC, err, errors, f1 measure, MCC (Mathews Correlation Coefficient), precision, <a id="id1825" class="indexterm"/>and recall for each validation fold in the case of cross-validation as well as the mean and standard deviation computed across all.</p><div class="mediaobject"><img src="graphics/B05137_09_015.jpg" alt="Tools and usage"/><div class="caption"><p>Figure 14: Validation results and summary</p></div></div><p>The <a id="id1826" class="indexterm"/>prediction action runs the model using unseen held-out data to estimate <a id="id1827" class="indexterm"/>the out-of-sample performance. Important measures such as errors, accuracy, area under curve, ROC plots, and so on, are given as the output of predictions that can be saved or exported.</p><div class="mediaobject"><img src="graphics/B05137_09_016.jpg" alt="Tools and usage"/><div class="caption"><p>Figure 15: Running test data, predictions, and ROC curves</p></div></div></div></div></div>
<div class="section" title="Case study"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec76"/>Case study</h1></div></div></div><p>In <a id="id1828" class="indexterm"/>this case study, we use the <code class="literal">CoverType</code> dataset to demonstrate classification and clustering algorithms from H2O, Apache Spark MLlib, and SAMOA Machine Learning libraries in Java.</p><div class="section" title="Business problem"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec152"/>Business problem</h2></div></div></div><p>The <code class="literal">CoverType</code> dataset available from the UCI machine learning repository (<a class="ulink" href="https://archive.ics.uci.edu/ml/datasets/Covertype">https://archive.ics.uci.edu/ml/datasets/Covertype</a>) contains unscaled cartographic <a id="id1829" class="indexterm"/>data for 581,012 cells <a id="id1830" class="indexterm"/>of forest land 30 x 30 m2 in dimension, accompanied by actual forest cover type labels. In the experiments conducted here, we use the normalized version of the data. Including one-hot encoding of two categorical types, there are a total of 54 attributes in each row.</p></div><div class="section" title="Machine Learning mapping"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec153"/>Machine Learning mapping</h2></div></div></div><p>First, we treat the problem as one of classification using the labels included in the dataset and <a id="id1831" class="indexterm"/>perform several supervised learning experiments. With the models generated, we make predictions about the forest cover type of an unseen held out test dataset.  For the clustering experiments that follow, we ignore the data labels, determine the number of clusters to use, and then report the corresponding cost using various algorithms implemented in H2O and Spark MLLib.</p></div><div class="section" title="Data collection"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec154"/>Data collection</h2></div></div></div><p>This <a id="id1832" class="indexterm"/>dataset was collected using cartographic measurements only and no remote sensing. It was derived from data originally <a id="id1833" class="indexterm"/>collected by the <span class="strong"><strong>US Forest Service</strong></span> (<span class="strong"><strong>USFS</strong></span>) and <a id="id1834" class="indexterm"/>the <span class="strong"><strong>US Geological Survey</strong></span> (<span class="strong"><strong>USGS</strong></span>).</p></div><div class="section" title="Data sampling and transformation"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec155"/>Data sampling and transformation</h2></div></div></div><p>Train <a id="id1835" class="indexterm"/>and test data—The dataset <a id="id1836" class="indexterm"/>was split into two sets <a id="id1837" class="indexterm"/>in the ratio 20% for <a id="id1838" class="indexterm"/>testing and 80% for training.</p><p>The categorical Soil Type designation was represented by 40 binary variable attributes. A value of 1 indicates the presence of a soil type in the observation; a 0 indicates its absence.</p><p>The <a id="id1839" class="indexterm"/>wilderness area designation <a id="id1840" class="indexterm"/>is likewise a categorical <a id="id1841" class="indexterm"/>attribute with four binary columns, with 1 indicating <a id="id1842" class="indexterm"/>presence and 0 absence.</p><p>All continuous value attributes have been normalized prior to use.</p><div class="section" title="Experiments, results, and analysis"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec173"/>Experiments, results, and analysis</h3></div></div></div><p>In the <a id="id1843" class="indexterm"/>first set of experiments in this case study, we used <a id="id1844" class="indexterm"/>the H2O <a id="id1845" class="indexterm"/>framework.</p><div class="section" title="Feature relevance and analysis"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl4sec255"/>Feature relevance and analysis</h4></div></div></div><p>Though <a id="id1846" class="indexterm"/>H2O doesn't have explicit feature selection <a id="id1847" class="indexterm"/>algorithms, many learners such as GLM, random forest, GBT, and so on, give feature importance metrics based on training/validation of the models. In our analysis, we have used GLM for feature selection, as shown in <span class="emphasis"><em>Figure 16</em></span>. It is interesting that the feature <span class="strong"><strong>Elevation</strong></span> emerges as the <a id="id1848" class="indexterm"/>most discriminating feature along with some <a id="id1849" class="indexterm"/>categorical features that are converted into <a id="id1850" class="indexterm"/>numeric/binary such as <span class="strong"><strong>Soil_Type2</strong></span>, <span class="strong"><strong>Soil_Type4</strong></span>, and so <a id="id1851" class="indexterm"/>on. Many <a id="id1852" class="indexterm"/>of the soil type categorical <a id="id1853" class="indexterm"/>features have no relevance and can be dropped from the modeling perspective.</p><p>Learning algorithms included in this set of experiments were: <span class="strong"><strong>Generalized Linear Models</strong></span> (<span class="strong"><strong>GLM</strong></span>), <span class="strong"><strong>Gradient Boosting Machine</strong></span> (<span class="strong"><strong>GBM</strong></span>), <span class="strong"><strong>Random Forest</strong></span> (<span class="strong"><strong>RF</strong></span>), <span class="strong"><strong>Naïve Bayes</strong></span> (<span class="strong"><strong>NB</strong></span>), and <span class="strong"><strong>Deep Learning</strong></span> (<span class="strong"><strong>DL</strong></span>). The deep learning model supported by H2O is the <span class="strong"><strong>multi-layered perceptron</strong></span> (<span class="strong"><strong>MLP</strong></span>).</p><div class="mediaobject"><img src="graphics/B05137_09_017.jpg" alt="Feature relevance and analysis"/><div class="caption"><p>Figure 16: Feature selection using GLM</p></div></div></div><div class="section" title="Evaluation on test data"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl4sec256"/>Evaluation on test data</h4></div></div></div><p>The <a id="id1854" class="indexterm"/>results using all the features are shown in the table:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Algorithm</p>
</th><th style="text-align: left" valign="bottom">
<p>Parameters</p>
</th><th style="text-align: left" valign="bottom">
<p>AUC</p>
</th><th style="text-align: left" valign="bottom">
<p>Max Accuracy</p>
</th><th style="text-align: left" valign="bottom">
<p>Max F1</p>
</th><th style="text-align: left" valign="bottom">
<p>Max Precision</p>
</th><th style="text-align: left" valign="bottom">
<p>Max Recall</p>
</th><th style="text-align: left" valign="bottom">
<p>Max Specificity</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>GLM</strong></span> </p>
</td><td style="text-align: left" valign="top">
<p>Default </p>
</td><td style="text-align: left" valign="top">
<p>0.84</p>
</td><td style="text-align: left" valign="top">
<p>0.79</p>
</td><td style="text-align: left" valign="top">
<p>0.84</p>
</td><td style="text-align: left" valign="top">
<p>0.98</p>
</td><td style="text-align: left" valign="top">
<p>1.0(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.99</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>GBM</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>Default</p>
</td><td style="text-align: left" valign="top">
<p>0.86</p>
</td><td style="text-align: left" valign="top">
<p>0.82</p>
</td><td style="text-align: left" valign="top">
<p>0.86</p>
</td><td style="text-align: left" valign="top">
<p>1.0(1)</p>
</td><td style="text-align: left" valign="top">
<p>1.0(1)</p>
</td><td style="text-align: left" valign="top">
<p>1.0(1)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Random Forest</strong></span> (<span class="strong"><strong>RF</strong></span>)</p>
</td><td style="text-align: left" valign="top">
<p>Default</p>
</td><td style="text-align: left" valign="top">
<p>0.88(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.83(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.87(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.97</p>
</td><td style="text-align: left" valign="top">
<p>1.0(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.99</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Naïve Bayes</strong></span> (<span class="strong"><strong>NB</strong></span>)</p>
</td><td style="text-align: left" valign="top">
<p>Laplace=50</p>
</td><td style="text-align: left" valign="top">
<p>0.66</p>
</td><td style="text-align: left" valign="top">
<p>0.72</p>
</td><td style="text-align: left" valign="top">
<p>0.81</p>
</td><td style="text-align: left" valign="top">
<p>0.68</p>
</td><td style="text-align: left" valign="top">
<p>1.0(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.33</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Deep Learning</strong></span> (<span class="strong"><strong>DL</strong></span>)</p>
</td><td style="text-align: left" valign="top">
<p>Rect,300, 300,Dropout</p>
</td><td style="text-align: left" valign="top">
<p>0.</p>
</td><td style="text-align: left" valign="top">
<p>0.78</p>
</td><td style="text-align: left" valign="top">
<p>0.83</p>
</td><td style="text-align: left" valign="top">
<p>0.88</p>
</td><td style="text-align: left" valign="top">
<p>1.0(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.99</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Deep Learning</strong></span> (<span class="strong"><strong>DL</strong></span>)</p>
</td><td style="text-align: left" valign="top">
<p>300, 300,MaxDropout</p>
</td><td style="text-align: left" valign="top">
<p>0.82</p>
</td><td style="text-align: left" valign="top">
<p>0.8</p>
</td><td style="text-align: left" valign="top">
<p>0.84</p>
</td><td style="text-align: left" valign="top">
<p>1.0(1)</p>
</td><td style="text-align: left" valign="top">
<p>1.0(1)</p>
</td><td style="text-align: left" valign="top">
<p>1.0(1)</p>
</td></tr></tbody></table></div><p>The <a id="id1855" class="indexterm"/>results after removing features not scoring well in feature relevance were:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Algorithm</p>
</th><th style="text-align: left" valign="bottom">
<p>Parameters</p>
</th><th style="text-align: left" valign="bottom">
<p>AUC</p>
</th><th style="text-align: left" valign="bottom">
<p>Max Accuracy</p>
</th><th style="text-align: left" valign="bottom">
<p>Max F1</p>
</th><th style="text-align: left" valign="bottom">
<p>Max</p>
<p>Precision</p>
</th><th style="text-align: left" valign="bottom">
<p>Max Recall</p>
</th><th style="text-align: left" valign="bottom">
<p>Max Specificity</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>GLM</strong></span> </p>
</td><td style="text-align: left" valign="top">
<p>Default</p>
</td><td style="text-align: left" valign="top">
<p>0.84</p>
</td><td style="text-align: left" valign="top">
<p>0.80</p>
</td><td style="text-align: left" valign="top">
<p>0.85</p>
</td><td style="text-align: left" valign="top">
<p>1.0</p>
</td><td style="text-align: left" valign="top">
<p>1.0</p>
</td><td style="text-align: left" valign="top">
<p>1.0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>GBM</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>Default</p>
</td><td style="text-align: left" valign="top">
<p>0.85</p>
</td><td style="text-align: left" valign="top">
<p>0.82</p>
</td><td style="text-align: left" valign="top">
<p>0.86</p>
</td><td style="text-align: left" valign="top">
<p>1.0</p>
</td><td style="text-align: left" valign="top">
<p>1.0</p>
</td><td style="text-align: left" valign="top">
<p>1.0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Random Forest</strong></span> (<span class="strong"><strong>RF</strong></span>)</p>
</td><td style="text-align: left" valign="top">
<p>Default</p>
</td><td style="text-align: left" valign="top">
<p>0.88</p>
</td><td style="text-align: left" valign="top">
<p>0.83</p>
</td><td style="text-align: left" valign="top">
<p>0.87</p>
</td><td style="text-align: left" valign="top">
<p>1.0</p>
</td><td style="text-align: left" valign="top">
<p>1.0</p>
</td><td style="text-align: left" valign="top">
<p>1.0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Naïve Bayes</strong></span> (<span class="strong"><strong>NB</strong></span>)</p>
</td><td style="text-align: left" valign="top">
<p>Laplace=50</p>
</td><td style="text-align: left" valign="top">
<p>0.76</p>
</td><td style="text-align: left" valign="top">
<p>0.74</p>
</td><td style="text-align: left" valign="top">
<p>0.81</p>
</td><td style="text-align: left" valign="top">
<p>0.89</p>
</td><td style="text-align: left" valign="top">
<p>1.0</p>
</td><td style="text-align: left" valign="top">
<p>0.95</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Deep Learning</strong></span> (<span class="strong"><strong>DL</strong></span>)</p>
</td><td style="text-align: left" valign="top">
<p>300,300, RectDropout</p>
</td><td style="text-align: left" valign="top">
<p>0.81</p>
</td><td style="text-align: left" valign="top">
<p>0.79</p>
</td><td style="text-align: left" valign="top">
<p>0.84</p>
</td><td style="text-align: left" valign="top">
<p>1.0</p>
</td><td style="text-align: left" valign="top">
<p>1.0</p>
</td><td style="text-align: left" valign="top">
<p>1.0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Deep Learning</strong></span> (<span class="strong"><strong>DL</strong></span>)</p>
</td><td style="text-align: left" valign="top">
<p>300, 300, MaxDropout</p>
</td><td style="text-align: left" valign="top">
<p>0.85</p>
</td><td style="text-align: left" valign="top">
<p>0.80</p>
</td><td style="text-align: left" valign="top">
<p>0.84</p>
</td><td style="text-align: left" valign="top">
<p>0.89</p>
</td><td style="text-align: left" valign="top">
<p>0.90</p>
</td><td style="text-align: left" valign="top">
<p>1.0</p>
</td></tr></tbody></table></div><p>Table 1: Model evaluation results with all features included</p></div><div class="section" title="Analysis of results"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl4sec257"/>Analysis of results</h4></div></div></div><p>The <a id="id1856" class="indexterm"/>main observations from an analysis of the results obtained are quite instructive and are presented here.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The feature relevance analysis shows how the <span class="strong"><strong>Elevation</strong></span> feature is a highly discriminating feature, whereas many categorical attributes converted to binary features, such as <span class="strong"><strong>SoilType_10</strong></span>, and so on, have near-zero to no relevance.</li><li class="listitem">The results for experiments with all features included, shown in <span class="emphasis"><em>Table 1</em></span>, clearly indicate that the non-linear ensemble technique Random Forest is the best algorithm as shown by the majority of the evaluation metrics including accuracy, F1, AUC, and recall.</li><li class="listitem"><span class="emphasis"><em>Table 1</em></span> also highlights the fact that whereas the faster, linear Naive Bayes algorithm <a id="id1857" class="indexterm"/>may not be best-suited, GLM, which also falls in the category of linear algorithms, demonstrates much better performance—this points to some inter-dependence among features!</li><li class="listitem">As we saw in <a class="link" href="ch07.html" title="Chapter 7. Deep Learning">Chapter 7</a>, <span class="emphasis"><em>Deep Learning,</em></span> algorithms used in deep learning typically need a lot of tuning; however, even with a few small tuning changes, the results from DL are comparable to Random Forest, especially with MaxDropout.</li><li class="listitem"><span class="emphasis"><em>Table 2</em></span> shows the results of all the algorithms after removing low-relevance features from the training set. It can be seen that Naive Bayes—which has the most impact due to multiplication of probabilities based on the assumption of independence between features—gets the most benefit and highest uplift in performance. Most of the other algorithms such as Random Forest have inbuilt feature selection as we discussed in <a class="link" href="ch02.html" title="Chapter 2. Practical Approach to Real-World Supervised Learning">Chapter 2</a>, <span class="emphasis"><em>Practical Approach to Real-World Supervised Learning,</em></span> and as a result removing the unimportant features has little or no effect on their performance.</li></ol></div></div></div></div><div class="section" title="Spark MLlib as Big Data Machine Learning platform"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec156"/>Spark MLlib as  Big Data Machine Learning platform</h2></div></div></div><p>Apache <a id="id1858" class="indexterm"/>Spark, started in 2009 at AMPLab at UC Berkley, was donated to Apache Software <a id="id1859" class="indexterm"/>Foundation in 2013 under <a id="id1860" class="indexterm"/>Apache License 2.0. The core idea of Spark was to build a cluster computing framework that would overcome the issues of Hadoop, especially for iterative and in-memory computations.</p><div class="section" title="Spark architecture"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec174"/>Spark architecture</h3></div></div></div><p>The Spark stack as shown in <span class="emphasis"><em>Figure 17</em></span> can use any kind of data stores such as HDFS, SQL, NoSQL, or local filesystems. It can be deployed on Hadoop, Mesos, or even standalone.</p><p>The <a id="id1861" class="indexterm"/>most important component of Spark is the Spark Core, which provides a framework to handle and manipulate the data in a high-throughput, fault-tolerant, and scalable manner.</p><p>Built on top of Spark core are various libraries each meant for various functionalities needed in processing data and doing analytics in the Big Data world. Spark SQL gives us a language for performing data manipulation in Big Data stores using a querying language very much like SQL, the<span class="emphasis"><em> lingua franca</em></span> of databases. Spark GraphX provides APIs to perform graph-related manipulations and graph-based algorithms on Big Data. Spark Streaming provides APIs to handle real-time operations needed in stream processing ranging from data manipulations to queries on the streams.</p><p>Spark-MLlib is the Machine Learning library that has an extensive set of Machine Learning algorithms to perform supervised and unsupervised tasks from feature selection to modeling. Spark has various language bindings such as Java, R, Scala, and Python. MLlib has a clear advantage running on top of the Spark engine, especially because of caching data in memory across multiple nodes and running MapReduce jobs, thus improving performance as compared to Mahout and other large-scale Machine Learning engines by a significant factor. MLlib also has other advantages such as fault tolerance and scalability without explicitly managing it in the Machine Learning algorithms.</p><div class="mediaobject"><img src="graphics/B05137_09_018.jpg" alt="Spark architecture"/><div class="caption"><p>Figure 17: Apache Spark high level architecture</p></div></div><p>The <a id="id1862" class="indexterm"/>Spark core has the following components:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Resilient Distributed Datasets</strong></span> (<span class="strong"><strong>RDD</strong></span>): RDDs are the basic immutable collection <a id="id1863" class="indexterm"/>of objects that Spark Core knows how to partition and distribute across the <a id="id1864" class="indexterm"/>cluster for performing tasks. RDDs are composed of "partitions", dependent on parent RDDs and metadata about data placement.</li><li class="listitem" style="list-style-type: disc">Two distinct operations are performed on RDDs:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Transformations</strong></span>: Operations that are lazily evaluated and transform one RDD into another. Lazy evaluation defers evaluation as long as possible, which makes some resource optimizations possible.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Action</strong></span>: The actual operation that triggers transformations and returns output values</li></ul></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Lineage graph</strong></span>: The pipeline or flow of data describing the computation for a <a id="id1865" class="indexterm"/>particular task, including different RDDs created in transformations and actions is known as the lineage graph of the task. The lineage graph plays a key role in fault tolerance. <div class="mediaobject"><img src="graphics/B05137_09_019.jpg" alt="Spark architecture"/><div class="caption"><p>Figure 18: Apache Spark lineage graph</p></div></div></li></ul></div><p>Spark is agnostic to the cluster management and can work with several implementations—including YARN and Mesos—for managing the nodes, distributing the work, and communications. The distribution of tasks in Transformations and Actions across the <a id="id1866" class="indexterm"/>cluster is done by the scheduler, starting from the driver node where the Spark context is created, to the many worker nodes as shown in <span class="emphasis"><em>Figure 19</em></span>. When running with YARN, Spark gives the user the choice of the number of executors, heap, and core allocation per JVM at the node level.</p><div class="mediaobject"><img src="graphics/B05137_09_020.jpg" alt="Spark architecture"/><div class="caption"><p>Figure 19: Apace Spark cluster deployment and task distribution</p></div></div></div><div class="section" title="Machine Learning in MLlib"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec175"/>Machine Learning in MLlib</h3></div></div></div><p>Spark MLlib has a <a id="id1867" class="indexterm"/>comprehensive Machine Learning toolkit, offering more algorithms than H2O at the time of writing, as shown in <span class="emphasis"><em>Figure 20</em></span>:</p><div class="mediaobject"><img src="graphics/B05137_09_021.jpg" alt="Machine Learning in MLlib"/><div class="caption"><p>Figure 20: Apache Spark MLlib machine learning algorithms</p></div></div><p>Many extensions have been written for Spark, including Spark MLlib, and the user community continues to contribute more packages.  You can download third-party packages or register your own at <a class="ulink" href="https://spark-packages.org/">https://spark-packages.org/</a>.</p></div><div class="section" title="Tools and usage"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec176"/>Tools and usage</h3></div></div></div><p>Spark <a id="id1868" class="indexterm"/>MLlib provides APIs for other languages in addition to Java, including Scala, Python, and R. When a <code class="literal">SparkContext</code> is created, it launches a monitoring <a id="id1869" class="indexterm"/>and instrumentation web console at port <code class="literal">4040</code>, which lets us see key information about the runtime, including scheduled tasks and their progress, RDD sizes and memory use, and so on. There are also external profiling tools available for use.</p></div><div class="section" title="Experiments, results, and analysis"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec177"/>Experiments, results, and analysis</h3></div></div></div><p>The <a id="id1870" class="indexterm"/>business problem we tackled here is the same as the one <a id="id1871" class="indexterm"/>described earlier for experiments using H2O. We employed <a id="id1872" class="indexterm"/>five learning algorithms using MLlib, in all. The first was k-Means <a id="id1873" class="indexterm"/>with all features using a <span class="emphasis"><em>k</em></span> value determined from computing the cost—specifically, the <span class="strong"><strong>Sum of Squared Errors</strong></span> (<span class="strong"><strong>SSE</strong></span>)—over a large number of values of <span class="emphasis"><em>k</em></span> and selecting the "elbow" of the curve.  Determining the optimal value of <span class="emphasis"><em>k</em></span> is typically not an easy task; often, evaluation measures such as silhouette are compared in order to pick the best <span class="emphasis"><em>k</em></span>. Even though we know the number of classes in the dataset is <span class="emphasis"><em>7</em></span>, it is instructive to see where experiments like this lead if we pretend we did not have labeled data. The optimal <span class="emphasis"><em>k</em></span> found using the elbow method was 27. In the real world, business decisions may often guide the selection of <span class="emphasis"><em>k</em></span>.</p><p>In the following listings, we show how to use different models from the MLlib suite to do cluster analysis and <a id="id1874" class="indexterm"/>classification. The code is based on examples available in the MLlib API Guide (<a class="ulink" href="https://spark.apache.org/docs/latest/mllib-guide.html">https://spark.apache.org/docs/latest/mllib-guide.html</a>). We use the normalized UCI <code class="literal">CoverType</code> dataset in CSV format. Note that it is more natural to use <code class="literal">spark.sql.Dataset</code> with the newer <code class="literal">spark.ml</code> package, whereas the <code class="literal">spark.mllib</code> package works more closely with <code class="literal">JavaRDD</code>. This provides an abstraction over RDDs and allows for optimization of the transformations beneath the covers. In the case of most unsupervised learning algorithms, this means the data must be transformed such that the dataset to be used for training and testing should have a column called features by default that contains all the features of an observation as a vector. A <code class="literal">VectorAssembler</code> object can be used for this transformation. A glimpse into the use of ML pipelines, which is a way to chain tasks together, is given in the source code for training a Random Forest classifier.</p><div class="section" title="k-Means"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl5sec104"/>k-Means</h4></div></div></div><p>The <a id="id1875" class="indexterm"/>following code fragment for the k-Means experiment uses the algorithm from the <code class="literal">org.apache.spark.ml.clustering</code> package. The code includes minimal boilerplate for setting up the <code class="literal">SparkSession</code>, which is the handle to the Spark runtime. Note <a id="id1876" class="indexterm"/>that eight cores have been specified in local mode in the setup:</p><div class="informalexample"><pre class="programlisting">SparkSession spark = SparkSession.builder()
    .master("local[8]")
    .appName("KMeansExpt")
    .getOrCreate();

// Load and parse data
String filePath = "/home/kchoppella/book/Chapter09/data/covtypeNorm.csv";
// Selected K value 
int k =  27;

// Loads data.
Dataset&lt;Row&gt; inDataset = spark.read()
    .format("com.databricks.spark.csv")
    .option("header", "true")
    .option("inferSchema", true)
    .load(filePath);
ArrayList&lt;String&gt; inputColsList = new ArrayList&lt;String&gt;(Arrays.asList(inDataset.columns()));

//Make single features column for feature vectors 
inputColsList.remove("class");
String[] inputCols = inputColsList.parallelStream().toArray(String[]::new);

//Prepare dataset for training with all features in "features" column
VectorAssembler assembler = new VectorAssembler().setInputCols(inputCols).setOutputCol("features");
Dataset&lt;Row&gt; dataset = assembler.transform(inDataset);

KMeans kmeans = new KMeans().setK(k).setSeed(1L);
KMeansModel model = kmeans.fit(dataset);

// Evaluate clustering by computing Within Set Sum of Squared Errors.
double SSE = model.computeCost(dataset);
System.out.println("Sum of Squared Errors = " + SSE);

spark.stop();</pre></div><p>The optimal value for the number of clusters was arrived at by evaluating and plotting the sum of <a id="id1877" class="indexterm"/>squared errors for several different values and choosing the one at the elbow of the curve. The value used here is <span class="emphasis"><em>27</em></span>.</p></div><div class="section" title="k-Means with PCA"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl5sec105"/>k-Means with PCA</h4></div></div></div><p>In the <a id="id1878" class="indexterm"/>second experiment, we used k-Means again, but first we reduced the number of dimensions in the data through PCA. Again, we used a rule of thumb here, which is to select a value for the PCA parameter for the number of dimensions such that at least 85% of the variance in the original dataset is preserved after the reduction in dimensionality. This produced 16 features in the transformed dataset from an initial 54, and this dataset was used in this and subsequent experiments. The following code shows the relevant code for PCA analysis:</p><div class="informalexample"><pre class="programlisting">int numDimensions = 16
PCAModel pca = new PCA()
    .setK(numDimensions)
    .setInputCol("features")
    .setOutputCol("pcaFeatures")
    .fit(dataset);

Dataset&lt;Row&gt; result = pca.transform(dataset).select("pcaFeatures");
KMeans kmeans = new KMeans().setK(k).setSeed(1L);
KMeansModel model = kmeans.fit(dataset);</pre></div></div><div class="section" title="Bisecting k-Means (with PCA)"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl5sec106"/>Bisecting k-Means (with PCA)</h4></div></div></div><p>The <a id="id1879" class="indexterm"/>third experiment used MLlib's Bisecting k-Means algorithm. This algorithm is similar to a top-down hierarchical clustering technique where all instances are in the same cluster at the outset, followed by successive splits:</p><div class="informalexample"><pre class="programlisting">// Trains a bisecting k-Means model.
BisectingKMeans bkm = new BisectingKMeans().setK(k).setSeed(1);
BisectingKMeansModel model = bkm.fit(dataset);</pre></div></div><div class="section" title="Gaussian Mixture Model"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl5sec107"/>Gaussian Mixture Model</h4></div></div></div><p>In the <a id="id1880" class="indexterm"/>next experiment, we used MLlib's <span class="strong"><strong>Gaussian Mixture Model</strong></span> (<span class="strong"><strong>GMM</strong></span>), another clustering model. The assumption inherent <a id="id1881" class="indexterm"/>to this model is that the data distribution in each cluster is Gaussian in nature, with unknown parameters. The same number of clusters is specified here, and default values have been used for the maximum number of iterations and tolerance, which dictate when the algorithm is considered to have converged:</p><div class="informalexample"><pre class="programlisting">GaussianMixtureModel gmm = new GaussianMixture()
    .setK(numClusters)
    .fit(result);
// Output the parameters of the mixture model
for (int k = 0; k &lt; gmm.getK(); k++) {
  String msg = String.format("Gaussian %d:\nweight=%f\nmu=%s\nsigma=\n%s\n\n",
              k, gmm.weights()[k], gmm.gaussians()[k].mean(), 
              gmm.gaussians()[k].cov());
  System.out.printf(msg);
  writer.write(msg + "\n");
  writer.flush();
}</pre></div></div><div class="section" title="Random Forest"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl5sec108"/>Random Forest</h4></div></div></div><p>Finally, we ran <a id="id1882" class="indexterm"/>Random Forest, which is the only available <a id="id1883" class="indexterm"/>ensemble learner that can handle multi-class classification. In the following code, we see that this algorithm needs some preparatory tasks to be performed prior to training. Pre-processing stages are composed into a pipeline of Transformers and Estimators. The <a id="id1884" class="indexterm"/>pipeline is then used to fit the data. You can learn more about Pipelines on the Apache Spark website (<a class="ulink" href="https://spark.apache.org/docs/latest/ml-pipeline.html">https://spark.apache.org/docs/latest/ml-pipeline.html</a>):</p><div class="informalexample"><pre class="programlisting">// Index labels, adding metadata to the label column.
// Fit on whole dataset to include all labels in index.
StringIndexerModel labelIndexer = new StringIndexer()
  .setInputCol("class")
  .setOutputCol("indexedLabel")
  .fit(dataset);
// Automatically identify categorical features, and index them.
// Set maxCategories so features with &gt; 2 distinct values are treated as continuous since we have already encoded categoricals with sets of binary variables.
VectorIndexerModel featureIndexer = new VectorIndexer()
  .setInputCol("features")
  .setOutputCol("indexedFeatures")
  .setMaxCategories(2)
  .fit(dataset);

// Split the data into training and test sets (30% held out for testing)
Dataset&lt;Row&gt;[] splits = dataset.randomSplit(new double[] {0.7, 0.3});
Dataset&lt;Row&gt; trainingData = splits[0];
Dataset&lt;Row&gt; testData = splits[1];

// Train a RF model.
RandomForestClassifier rf = new RandomForestClassifier()
  .setLabelCol("indexedLabel")
  .setFeaturesCol("indexedFeatures")
  .setImpurity("gini")
  .setMaxDepth(5)
  .setNumTrees(20)
  .setSeed(1234);

// Convert indexed labels back to original labels.
IndexToString labelConverter = new IndexToString()
  .setInputCol("prediction")
  .setOutputCol("predictedLabel")
  .setLabels(labelIndexer.labels());

// Chain indexers and RF in a Pipeline.
Pipeline pipeline = new Pipeline()
  .setStages(new PipelineStage[] {labelIndexer, featureIndexer, rf, labelConverter});

// Train model. This also runs the indexers.
PipelineModel model = pipeline.fit(trainingData);

// Make predictions.
Dataset&lt;Row&gt; predictions = model.transform(testData);

// Select example rows to display.
predictions.select("predictedLabel", "class", "features").show(5);

// Select (prediction, true label) and compute test error.
MulticlassClassificationEvaluator evaluator = new MulticlassClassificationEvaluator()
  .setLabelCol("indexedLabel")
  .setPredictionCol("prediction");

evaluator.setMetricName("accuracy");
double accuracy = evaluator.evaluate(predictions);
System.out.printf("Accuracy = %f\n", accuracy); </pre></div><p>The <a id="id1885" class="indexterm"/>sum of squared errors for the experiments using k-Means and <a id="id1886" class="indexterm"/>Bisecting k-Means are given in the following table:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Algorithm</p>
</th><th style="text-align: left" valign="bottom">
<p>k</p>
</th><th style="text-align: left" valign="bottom">
<p>Features</p>
</th><th style="text-align: left" valign="bottom">
<p>SSE</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>k-Means</p>
</td><td style="text-align: left" valign="top">
<p>27</p>
</td><td style="text-align: left" valign="top">
<p>54</p>
</td><td style="text-align: left" valign="top">
<p>214,702</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>k-Means(PCA)</p>
</td><td style="text-align: left" valign="top">
<p>27</p>
</td><td style="text-align: left" valign="top">
<p>16</p>
</td><td style="text-align: left" valign="top">
<p>241,155</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Bisecting k-Means(PCA)</p>
</td><td style="text-align: left" valign="top">
<p>27</p>
</td><td style="text-align: left" valign="top">
<p>16</p>
</td><td style="text-align: left" valign="top">
<p>305,644</p>
</td></tr></tbody></table></div><p>Table 3: Results with k-Means</p><p>The GMM model was used to illustrate the use of the API; it outputs the parameters of the Gaussian mixture for every cluster as well as the cluster weight. Output for all the clusters can be seen at the website for this book.</p><p>For the case of Random Forest these are the results for runs with different numbers of trees. All 54 features were used here:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Number of trees</p>
</th><th style="text-align: left" valign="bottom">
<p>Accuracy</p>
</th><th style="text-align: left" valign="bottom">
<p>F1 measure</p>
</th><th style="text-align: left" valign="bottom">
<p>Weighted precision</p>
</th><th style="text-align: left" valign="bottom">
<p>Weighted recall</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>15</p>
</td><td style="text-align: left" valign="top">
<p>0.6806</p>
</td><td style="text-align: left" valign="top">
<p>0.6489</p>
</td><td style="text-align: left" valign="top">
<p>0.6213</p>
</td><td style="text-align: left" valign="top">
<p>0.6806</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>20</p>
</td><td style="text-align: left" valign="top">
<p>0.6776</p>
</td><td style="text-align: left" valign="top">
<p>0.6470</p>
</td><td style="text-align: left" valign="top">
<p>0.6191</p>
</td><td style="text-align: left" valign="top">
<p>0.6776</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>25</p>
</td><td style="text-align: left" valign="top">
<p>0.5968</p>
</td><td style="text-align: left" valign="top">
<p>0.5325</p>
</td><td style="text-align: left" valign="top">
<p>0.5717</p>
</td><td style="text-align: left" valign="top">
<p>0.5968</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>30</p>
</td><td style="text-align: left" valign="top">
<p>0.6547</p>
</td><td style="text-align: left" valign="top">
<p>0.6207</p>
</td><td style="text-align: left" valign="top">
<p>0.5972</p>
</td><td style="text-align: left" valign="top">
<p>0.6547</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>40</p>
</td><td style="text-align: left" valign="top">
<p>0.6594</p>
</td><td style="text-align: left" valign="top">
<p>0.6272</p>
</td><td style="text-align: left" valign="top">
<p>0.6006</p>
</td><td style="text-align: left" valign="top">
<p>0.6594</p>
</td></tr></tbody></table></div><p>Table 4: Results for Random Forest</p><div class="section" title="Analysis of results"><div class="titlepage"><div><div><h5 class="title"><a id="ch09lvl5sec109"/>Analysis of results</h5></div></div></div><p>As <a id="id1887" class="indexterm"/>can be seen from <span class="emphasis"><em>Table 3</em></span>, there is a small increase in cost when fewer dimensions are used after PCA with the same number of clusters. Varying <span class="emphasis"><em>k</em></span> with PCA might suggest a better <span class="emphasis"><em>k</em></span> for the PCA case. Notice also that in this experiment, for the same <span class="emphasis"><em>k</em></span>, Bisecting K-Means with PCA-derived features has the highest cost of all. The stopping number of clusters used for Bisecting k-Means has simply been picked to be the one determined for basic k-Means, but this need not be so. A similar search for <span class="emphasis"><em>k</em></span> that yields the best cost may be done independently for Bisecting k-Means.</p><p>In the case of Random Forest, we see the best performance when using <span class="emphasis"><em>15</em></span> trees. All trees have a <a id="id1888" class="indexterm"/>depth of three. This hyper-parameter can be varied to tune the model as well. Even though Random Forest is not susceptible to over-fitting due to accounting for variance across trees in the training stages, increasing the value for the number of trees beyond an optimum number can degrade performance.</p></div></div></div><div class="section" title="Real-time Big Data Machine Learning"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl1sec77"/>Real-time Big Data Machine Learning</h3></div></div></div><p>In <a id="id1889" class="indexterm"/>this section, we will discuss the real-time version of Big Data Machine Learning where data arrives in large volumes and is changing at a rapid rate at the same time. Under these conditions, Machine Learning analytics cannot be applied <span class="emphasis"><em>per</em></span> the traditional practice of "batch learning and deploy" (<span class="emphasis"><em>References</em></span> [14]).</p><div class="mediaobject"><img src="graphics/B05137_09_023.jpg" alt="Real-time Big Data Machine Learning"/><div class="caption"><p>Figure 21: Use case for real-time Big Data Machine Learning</p></div></div><p>Let us consider a case where labeled data is available for a short duration, and we perform the appropriate modeling techniques on the data and then apply the most suitable evaluation methods on the resulting models. Next, we select the best model and use it for predictions on unseen data at runtime. We then observe, with some dismay, that model performance drops significantly over time. Repeating the exercise with new data shows a similar degradation in performance! What are we to do now? This quandary, combined with large volumes of data motivates the need for a different approach: real-time Big Data Machine Learning.</p><p>Like the batch learning framework, the real-time framework in big data may have similar components <a id="id1890" class="indexterm"/>up until the data preparation stage. When the computations involved in data preparation must take place on streams or combined stream and batch data, we require specialized computation engines such as <span class="strong"><strong>Spark Streaming</strong></span>. Like <a id="id1891" class="indexterm"/>stream computations, Machine Learning must work across the cluster and perform different Machine Learning tasks on the stream. This adds an additional layer of complexity to the implementations of single machine multi-threaded streaming algorithms.</p><div class="mediaobject"><img src="graphics/B05137_09_024.jpg" alt="Real-time Big Data Machine Learning"/><div class="caption"><p>Figure 22: Real-time big data components and providers</p></div></div><div class="section" title="SAMOA as a real-time Big Data Machine Learning framework"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec178"/>SAMOA as a real-time Big Data Machine Learning framework</h4></div></div></div><p>For a <a id="id1892" class="indexterm"/>single machine, in <a class="link" href="ch05.html" title="Chapter 5. Real-Time Stream Machine Learning">Chapter 5</a>, <span class="emphasis"><em>Real-Time Stream Machine Learning</em></span>, we <a id="id1893" class="indexterm"/>discussed the MOA framework at length. SAMOA is the distributed framework for performing Machine Learning on streams. </p><p>At the <a id="id1894" class="indexterm"/>time of writing, SAMOA is an incubator-level open source project <a id="id1895" class="indexterm"/>with Apache 2.0 license and good integration with different stream processing engines such as <span class="strong"><strong>Apache Storm</strong></span>, <span class="strong"><strong>Samza</strong></span>, and <span class="strong"><strong>S4</strong></span>.</p><div class="section" title="SAMOA architecture"><div class="titlepage"><div><div><h5 class="title"><a id="ch09lvl4sec260"/>SAMOA architecture</h5></div></div></div><p>The SAMOA <a id="id1896" class="indexterm"/>framework offers several key streaming services to an extendable set of stream processing engines, with existing implementations for the most popular engines of today.</p><div class="mediaobject"><img src="graphics/B05137_09_025.jpg" alt="SAMOA architecture"/><div class="caption"><p>Figure 23: SAMOA high level architecture</p></div></div><p>
<code class="literal">TopologyBuilder</code> is the interface that acts as a factory to create different components and connect them together in SAMOA. The core of SAMOA is in building processing elements <a id="id1897" class="indexterm"/>for data streams. The basic unit for processing consists of <code class="literal">ProcessingItem</code> and the <code class="literal">Processor</code> interface, as shown in <span class="emphasis"><em>Figure 24</em></span>. <code class="literal">ProcessingItem</code> is an encapsulated hidden element, while Processor is the core implementation where the logic for handling streams is coded.</p><div class="mediaobject"><img src="graphics/B05137_09_026.jpg" alt="SAMOA architecture"/><div class="caption"><p>Figure 24: SAMOA processing data streams</p></div></div><p>
<span class="strong"><strong>Stream</strong></span> is another <a id="id1898" class="indexterm"/>interface that connects various <span class="strong"><strong>Processors</strong></span> together as the source <a id="id1899" class="indexterm"/>and destination created by <code class="literal">TopologyBuilder</code>. A Stream can have one source and multiple destinations. Stream supports three forms of communication between source and destinations:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>All</strong></span>: In this communication, all messages from source are sent to all the destinations</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Key</strong></span>: In this communication, messages with the same keys are sent to the same processors</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Shuffle</strong></span>: In this communication, messages are randomly sent to the processors</li></ul></div><p>All the messages or events in SAMOA are implementations of the interface <code class="literal">ContentEvent</code>, encapsulating mostly the data in the streams as a value and having some form of key for uniqueness.</p><p>Each stream <a id="id1900" class="indexterm"/>processing engine has an implementation for all the key interfaces as a plugin and integrates with SAMOA. The Apache Storm implementations StormTopology, StormStream, and StormProcessingItem, and so on are shown in the API in <span class="emphasis"><em>Figure 25</em></span>.</p><p>Task is another unit of work in SAMOA, having the responsibility of execution. All the classification or clustering evaluation and validation techniques such as prequential, holdout, and so on, are implemented as Tasks.</p><p>Learner is the interface for implementing all Supervised and Unsupervised Learning capability in SAMOA. Learners can be local or distributed and have different extensions such as <code class="literal">ClassificationLearner</code> and <code class="literal">RegressionLearner</code>.</p></div></div><div class="section" title="Machine Learning algorithms"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec179"/>Machine Learning algorithms</h4></div></div></div><div class="mediaobject"><img src="graphics/B05137_09_027.jpg" alt="Machine Learning algorithms"/></div><div class="mediaobject"><img src="graphics/B05137_09_028.jpg" alt="Machine Learning algorithms"/><div class="caption"><p>Figure 25: SAMOA machine learning algorithms</p></div></div><p>
<span class="emphasis"><em>Figure 25</em></span> shows <a id="id1901" class="indexterm"/>the core components of the SAMOA topology and their implementation for various engines.</p></div><div class="section" title="Tools and usage"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec180"/>Tools and usage</h4></div></div></div><p>We <a id="id1902" class="indexterm"/>continue with the same business problem <a id="id1903" class="indexterm"/>as before. The command line to launch the training job for the <code class="literal">covtype</code> dataset is:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>bin/samoa local target/SAMOA-Local-0.3.0-SNAPSHOT.jar "PrequentialEvaluation -l classifiers.ensemble.Bagging </strong></span>
<span class="strong"><strong>    -s (ArffFileStream -f covtype-train.csv.arff) -f 10000"</strong></span>
</pre></div><div class="mediaobject"><img src="graphics/B05137_09_029.jpg" alt="Tools and usage"/><div class="caption"><p>Figure 25: Bagging model performance</p></div></div><p>When running with Storm, this is the command line:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>bin/samoa storm target/SAMOA-Storm-0.3.0-SNAPSHOT.jar "PrequentialEvaluation -l classifiers.ensemble.Bagging </strong></span>
<span class="strong"><strong>    -s (ArffFileStream -f covtype-train.csv.arff) -f 10000"</strong></span>
</pre></div></div><div class="section" title="Experiments, results, and analysis"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec181"/>Experiments, results, and analysis</h4></div></div></div><p>The <a id="id1904" class="indexterm"/>results of experiments using <a id="id1905" class="indexterm"/>SAMOA as a stream-based learning <a id="id1906" class="indexterm"/>platform for Big Data are given in <span class="emphasis"><em>Table 5</em></span>.</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Algorithm</p>
</th><th style="text-align: left" valign="bottom">
<p>Best Accuracy</p>
</th><th style="text-align: left" valign="bottom">
<p>Final Accuracy</p>
</th><th style="text-align: left" valign="bottom">
<p>Final Kappa Statistic</p>
</th><th style="text-align: left" valign="bottom">
<p>Final Kappa Temporal Statistic</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Bagging</p>
</td><td style="text-align: left" valign="top">
<p>79.16</p>
</td><td style="text-align: left" valign="top">
<p>64.09</p>
</td><td style="text-align: left" valign="top">
<p>37.52</p>
</td><td style="text-align: left" valign="top">
<p>-69.51</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Boosting</p>
</td><td style="text-align: left" valign="top">
<p>78.05</p>
</td><td style="text-align: left" valign="top">
<p>47.82</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>-1215.1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>VerticalHoeffdingTree</p>
</td><td style="text-align: left" valign="top">
<p>83.23</p>
</td><td style="text-align: left" valign="top">
<p>67.51</p>
</td><td style="text-align: left" valign="top">
<p>44.35</p>
</td><td style="text-align: left" valign="top">
<p>-719.51</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>AdaptiveBagging</p>
</td><td style="text-align: left" valign="top">
<p>81.03</p>
</td><td style="text-align: left" valign="top">
<p>64.64</p>
</td><td style="text-align: left" valign="top">
<p>38.99</p>
</td><td style="text-align: left" valign="top">
<p>-67.37</p>
</td></tr></tbody></table></div><p>Table 5: Experimental results with Big Data real-time learning using SAMOA</p><div class="section" title="Analysis of results"><div class="titlepage"><div><div><h5 class="title"><a id="ch09lvl4sec261"/>Analysis of results</h5></div></div></div><p>From <a id="id1907" class="indexterm"/>an analysis of the results, the following observations can be made:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Table 5</em></span> shows that the popular non-linear decision tree-based VHDT on SAMOA is the best performing algorithm according to almost all the metrics.</li><li class="listitem" style="list-style-type: disc">The adaptive bagging algorithm performs better than bagging because it employs Hoeffding Adaptive Trees in the implementation, which are more robust than basic online stream bagging.</li><li class="listitem" style="list-style-type: disc">The online boosting algorithm with its dependency on the weak learners and no adaptability ranked the lowest as expected.</li><li class="listitem" style="list-style-type: disc">The bagging plot in <span class="emphasis"><em>Figure 25</em></span> shows a nice trend of stability achieved as the number of examples increased, validating the general consensus that if the patterns are stationary, more examples lead to robust models.</li></ul></div></div></div></div><div class="section" title="The future of Machine Learning"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl1sec78"/>The future of Machine Learning</h3></div></div></div><p>The <a id="id1908" class="indexterm"/>impact of Machine Learning on businesses, social interactions, and indeed, our day-to-day lives today is undeniable, though not always immediately obvious. In the near future, it will be ubiquitous and inescapable. According to a report by McKinsey Global Institute published in December 2016 (<span class="emphasis"><em>References</em></span> [15]), there is a vast unexploited potential for data and analytics in major industry sectors, especially healthcare and the public sector. Machine Learning is one of the key technologies poised to help exploit that potential. More compute power is at our disposal than ever before. More data is available than ever before, and we have cheaper and greater storage capacity than ever before.</p><p>Already, the unmet demand for data scientists has spurred changes to college curricula worldwide, and has caused an increase of 16% a year in wages for data scientists in the US, in the period 2012-2014. The solution to a wide swathe of problems is within reach with Machine Learning, including resource allocation, forecasting, predictive analytics, predictive maintenance, and price and product optimization.</p><p>The same <a id="id1909" class="indexterm"/>McKinsey report emphasizes the increasing role of Machine Learning, including deep learning in a variety of use cases across industries such as agriculture, pharma, manufacturing, energy, media, and finance. These scenarios run the gamut: predict personalized health outcomes, identify fraud transactions, optimize pricing and scheduling, personalize crops to individual conditions, identify and navigate roads, diagnose disease, and personalize advertising. Deep learning has great potential in automating an increasing number of occupations. Just improving natural language understanding would potentially cause a USD 3 trillion impact on global wages, affecting jobs like customer service and support worldwide.</p><p>Giant strides in image and voice recognition and language processing have made applications such as personal digital assistants commonplace, thanks to remarkable advances in deep learning techniques. The symbolism of AlphaGO's success in defeating Lee Sedol, alluded to in the opening chapter of this book, is enormous, as it is a vivid example of how progress in artificial intelligence is besting our own predictions of milestones in AI advancement. Yet this is the tip of the proverbial iceberg. Recent work in areas such as transfer learning offers the promise of more broadly intelligent systems that will be able to solve a wider range of problems rather than narrowly specializing in just one. General Artificial Intelligence, where AI can develop objective reasoning, proposes a methodology to solve a problem, and learn from its mistakes, is some distance away at this point, but check back in a few years and that distance may well have shrunk beyond our current expectations! Increasingly, the confluence of transformative advances in technologies incrementally enabling each other spells a future of dizzying possibilities that we can already glimpse around us. The role of Machine Learning, it would appear, is to continue to shape that future in profound and extraordinary ways. Of that, there is little doubt.</p></div><div class="section" title="Summary"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl1sec79"/>Summary</h3></div></div></div><p>The final chapter of this book deals with Machine Learning adapted to what is arguably one of the most significant paradigm shifts in information management and analytics to have emerged in the last few decades—Big Data. Much as many other areas of computer science and engineering have seen, AI—and Machine Learning in particular—has benefited from innovative solutions and dedicated communities adapting to face the many challenges posed by Big Data.</p><p>One way to characterize Big Data is by volume, velocity, variety, and veracity. This demands a new set of tools and frameworks to conduct the tasks of effective analytics at large.</p><p>Choosing a Big Data framework involves selecting distributed storage systems, data preparation techniques, batch or real-time Machine Learning, as well as visualization and reporting tools.</p><p>Several open source deployment frameworks are available including Hortonworks Data Platform, Cloudera CDH, Amazon Elastic MapReduce, and Microsoft Azure HDInsight. Each provides a platform with components supporting data acquisition, data preparation, Machine Learning, evaluation, and visualization of results.</p><p>Among the data acquisition components, publish-subscribe is a model offered by Apache Kafka (<span class="emphasis"><em>References</em></span> [8]) and Amazon Kinesis, which involves a broker mediating between subscribers and publishers. Alternatives include source-sink, SQL, message queueing, and other custom frameworks.</p><p>With regard to data storage, several factors contribute to the proper choice for whatever your needs may be. HDFS offers a distributed File System with robust fault tolerance and high throughput. NoSQL databases also offer high throughput, but generally with weak guarantees on consistency. They include key-value, document, columnar, and graph databases.</p><p>Data processing and preparation come next in the flow, which includes data cleaning, scraping, and transformation. Hive and HQL provide these functions in HDFS systems. SparkSQL and Amazon Redshift offer similar capabilities. Real-time stream processing is available from products such as Storm and Samza.</p><p>The learning stage in Big Data analytics can include batch or real-time data.</p><p>A variety of rich visualization and analysis frameworks exist that are accessible from multiple programming environments.</p><p>Two major Machine Learning frameworks on Big Data are H2O and Apache Spark MLlib. Both can access data from various sources such as HDFS, SQL, NoSQL, S3, and others. H2O supports a number of Machine Learning algorithms that can be run in a cluster. For Machine Learning with real-time data, SAMOA is a big data framework with a comprehensive set of stream-processing capabilities.</p><p>The role of Machine Learning in the future is going to be a dominant one, with a wide-ranging impact on healthcare, finance, energy, and indeed on most industries. The expansion in the scope of automation will have inevitable societal effects. Increases in compute power, data, and storage per dollar are opening up great new vistas to Machine Learning applications that have the potential to increase productivity, engender innovation, and dramatically improve living standards the world over.</p></div><div class="section" title="References"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl1sec80"/>References</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Matei Zaharia, Mosharaf Chowdhury, Michael J. Franklin, Scott Shenker, <span class="emphasis"><em>Ion Stoica:Spark: Cluster Computing with Working Sets</em></span>. HotCloud 2010</li><li class="listitem">Matei Zaharia, Reynold S. Xin, Patrick Wendell, Tathagata Das, Michael Armbrust, Ankur Dave, Xiangrui Meng, Josh Rosen, Shivaram Venkataraman, Michael J. Franklin, Ali Ghodsi, Joseph Gonzalez, Scott Shenker, <span class="emphasis"><em>Ion Stoica:Apache Spark: a unified engine for Big Data processing</em></span>. Commun. ACM 59(11): 56-65 (2016)</li><li class="listitem">Apache Hadoop: <a class="ulink" href="https://hadoop.apache.org/">https://hadoop.apache.org/</a>.</li><li class="listitem">Cloudera: <a class="ulink" href="http://www.cloudera.com/">http://www.cloudera.com/</a>.</li><li class="listitem">Hortonworks: <a class="ulink" href="http://hortonworks.com/">http://hortonworks.com/</a>.</li><li class="listitem">Amazon EC2: <a class="ulink" href="http://aws.amazon.com/ec2/">http://aws.amazon.com/ec2/</a>.</li><li class="listitem">Microsoft Azure: <a class="ulink" href="http://azure.microsoft.com/">http://azure.microsoft.com/</a>.</li><li class="listitem">Apache Flume: <a class="ulink" href="https://flume.apache.org/">https://flume.apache.org/</a>.</li><li class="listitem">Apache Kafka: <a class="ulink" href="http://kafka.apache.org/">http://kafka.apache.org/</a>.</li><li class="listitem">Apache Sqoop: <a class="ulink" href="http://sqoop.apache.org/">http://sqoop.apache.org/</a>.</li><li class="listitem">Apache Hive: <a class="ulink" href="http://hive.apache.org/">http://hive.apache.org/</a>.</li><li class="listitem">Apache Storm: <a class="ulink" href="https://storm.apache.org/">https://storm.apache.org/</a>.</li><li class="listitem">H2O: <a class="ulink" href="http://h2o.ai/">http://h2o.ai/</a>.</li><li class="listitem">Shahrivari S, Jalili S. <span class="emphasis"><em>Beyond batch processing: towards real-time and streaming Big Data</em></span>. Computers. 2014;3(4):117–29.</li><li class="listitem"><span class="emphasis"><em>MGI, The Age of Analytics</em></span>–—Executive Summary <a class="ulink" href="http://www.mckinsey.com/~/media/McKinsey/Business%20Functions/McKinsey%20Analytics/Our%20Insights/The%20age%20of%20analytics%20Competing%20in%20a%20data%20driven%20world/MGI-The-Age-of-Analytics-Full-report.ashx">http://www.mckinsey.com/~/media/McKinsey/Business%20Functions/McKinsey%20Analytics/Our%20Insights/The%20age%20of%20analytics%20Competing%20in%20a%20data%20driven%20world/MGI-The-Age-of-Analytics-Full-report.ashx</a>.</li></ol></div></div></div></div></body></html>