- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data-Level Deep Learning Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You learned about various sampling methods in the previous chapters. Collectively,
    we call these methods *data-level methods* in this book. These methods include
    random undersampling, random oversampling, NearMiss, and SMOTE. We also explored
    how these methods work with classical machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll explore how to apply familiar sampling methods to deep
    learning models. Deep learning offers unique opportunities to enhance these methods
    further. We’ll delve into elegant techniques to combine deep learning with oversampling
    and undersampling. Additionally, we’ll learn how to implement various sampling
    methods with a basic neural network. We’ll also cover dynamic sampling, which
    involves adjusting the data sample across multiple training iterations, using
    varying balancing ratios for each iteration. Then, we will learn to use some data
    augmentation techniques for both images and text. We’ll end the chapter by highlighting
    key takeaways from a variety of other data-level techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sampling techniques for deep learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data-level techniques for text classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A discussion of other data-level deep learning methods and their key ideas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is not always straightforward to port methods to handle data imbalance, which
    worked well on classical ML models, into the deep learning world. The challenges
    and opportunities of deep learning models differ from the classical ML models
    primarily because of the difference in the type of data these models have to deal
    with. While classical ML models mostly deal with tabular and structured data,
    deep learning models typically deal with unstructured data, such as images, text,
    audio, and video, which is fundamentally different from tabular data.
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss various techniques to deal with imbalance problems in computer
    vision. In the first part of the chapter, we will focus on various techniques,
    such as sampling and data augmentation, to handle class imbalance when training
    convolutional neural networks on image and text data.
  prefs: []
  type: TYPE_NORMAL
- en: In the latter half of the chapter, we will discuss common data-level techniques
    that can be applied to text problems. A lot of computer vision techniques can
    be successfully applied to NLP problems, too.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to prior chapters, we will continue to utilize common libraries such
    as `torch`, `torchvision`, `numpy`, and `scikit-learn`. We will also use `nlpaug`
    for NLP-related functionalities. The code and notebooks for this chapter are available
    on GitHub at [https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter07](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter07).
    You can open the GitHub notebooks using Google Colab by clicking on the **Open
    in Colab** icon at the top of the chapter’s notebook, or by launching it from
    [https://colab.research.google.com](https://colab.research.google.com), using
    the GitHub URL of the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to use the classic MNIST dataset. This dataset
    contains 28-pixel x 28-pixel images of handwritten digits. The task for the model
    is to take an image as input and identify the digit in the image. We will use
    `PyTorch`, a popular deep-learning library, to demonstrate the algorithms. Let’s
    prepare the data now.
  prefs: []
  type: TYPE_NORMAL
- en: The first step in the process will be to import the libraries. We will need
    NumPy (as we deal with `numpy` arrays), `torchvision` (to load MNIST data), `torch`,
    `random`, and `copy` libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can download the MNIST data from `torchvision.datasets`. The `torchvision`
    library is a part of the `PyTorch` framework, which contains datasets, models,
    and common image transformers for computer vision tasks. The following code will
    download the MNIST dataset from this library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Once the data is downloaded from `torchvision`, we can load it into the `Dataloader`
    utility of `PyTorch`, which creates batches of data and provides us with a Python-style
    iterator over the batches. The following code does exactly that. Here, we are
    creating batches of size 64 for `train_loader`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Since we are interested in imbalanced datasets, we will convert our MNIST dataset,
    which is a balanced dataset, into a long-tailed version of itself by deleting
    examples from various classes. We are omitting that implementation here to save
    space; you can refer to the chapter’s GitHub repository for details. We assume
    that you have the `imbalanced_train_loader` class created from the imbalanced
    trainset.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7**.1* shows the distribution of samples in the imbalanced MNIST dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – A bar chart of the counts of examples from each digit class
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will learn to create a training loop in `PyTorch`.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the training loop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before creating the training loop, we should import the `torch.nn` and `torch.optim`
    packages. The `torch.nn` package provides all the building blocks to create a
    neural network graph, while the `torch.optim` package provides us with most of
    the common optimization algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we will need some hyperparameters, let’s define them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: After setting up the hyperparameters, we can define the `train` function, which
    will take PyTorch’s data loader as input and return a model fitted to the data.
    To create a trained model, we will need a model, a loss criterion, and an optimizer.
    We will use a single-layer linear neural network as the model here. You can design
    your own neural network architecture based on your requirements. For the loss
    criterion, we will use `CrossEntropyLoss`, and we will use **Stochastic Gradient
    Descent** (**SGD**) as the optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will train the model for `num_epochs` epochs. We will discuss how the model
    is trained during a single epoch in the next paragraph. For now, we will abstract
    that part out in the `run_epoch` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'During every epoch, we will train our model over the whole training data once.
    As discussed earlier, dataloader divides the data into multiple batches. First,
    we will have to match the shape of the images in the batch with the input dimension
    of our model. We will take the current batch and do a forward pass over the model,
    calculating the predictions and loss over the predictions in one go. Then, we
    will backpropagate the loss to update the model weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To get a trained model, we can now send the data loader to the `train` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'For all image-related methods in this chapter, we’ll employ the model code
    detailed next. We will create a `PyTorch` neural network named `Net` that features
    two convolutional layers, a dropout mechanism, and a pair of fully connected layers.
    Through the `forward` function, the model seamlessly integrates these layers using
    `ReLU` activations and max-pooling, manipulating the input, `x`. The result is
    `log_softmax` of the computed output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s break down some of these terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Net` class, there are two such layers – `conv1` and `conv2`. The numbers `(1,
    10)` and `(10, 20)` are simply the input and output channels. The term `kernel_size=5`
    means that a 5 x 5 grid (or filter) is used to scan the input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`conv2_drop` is a dropout layer of type `Dropout2d`, specifically designed
    for 2D data (such as images).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fc1` and `fc2`, which further process the patterns recognized by the convolutional
    layers to make predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`F.relu` is an activation function that introduces non-linearity to the model,
    enabling it to learn complex patterns'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`F.max_pool2d` is a pooling function that reduces the spatial dimensions of
    the data while retaining important features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, `F.log_softmax` is an activation function commonly used for classification
    tasks to produce probabilities for each class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In essence, the `Net` class defines a neural network that first detects patterns
    in data using convolutional layers, reduces overfitting using dropout, and then
    makes predictions using fully connected layers. The forward method is a sequence
    of operations that define how data flows through this network.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn how to use the `train` function with oversampling
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling techniques for deep learning models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll explore some sampling methods, such as random oversampling
    and weighted sampling, for deep learning models. We’ll then transition into data
    augmentation techniques, which bolster model robustness and mitigate dataset limitations.
    While large datasets are ideal for deep learning, real-world constraints often
    make them hard to obtain. We will also look at some advanced augmentations, such
    as CutMix and MixUp. We’ll start with standard methods before discussing these
    advanced techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Random oversampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, we will apply the plain old random oversampling we learned in [*Chapter
    2*](B17259_02.xhtml#_idTextAnchor042), *Oversampling Methods*, but using image
    data as input to a neural network. The basic idea is to duplicate samples from
    the minority classes randomly until we end up with an equal number of samples
    from each class. This technique often performs better than no sampling.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to train the model for enough epochs so that it has fully been fitted
    to the data. Under-training will likely lead to suboptimal model performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s spend some time working with the code. There are a few simple steps we
    need to follow. First, we need to convert data from the data loaders into tensors.
    Our `RandomOverSampler` API from `imbalanced-learn` doesn’t work directly with
    data loaders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to reshape the `X` tensor for `RandomOverSampler` to work with
    two-dimensional inputs, as each of our images is a 28 x 28 matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can import the `RandomOverSampler` class from the `imbalanced-learn`
    library, define an `oversampler` object, and resample our data using it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'After resampling the data, we need to reshape it again back to the original
    form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now create a new data loader using the oversampled data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can train our model using the new data loader. For this step, we
    can use the `train` function defined in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: That’s all we need to do to use the random oversampling technique with deep
    learning models.
  prefs: []
  type: TYPE_NORMAL
- en: A similar strategy can be used for `RandomUnderSampling` from the `imbalanced-learn`
    library.
  prefs: []
  type: TYPE_NORMAL
- en: '`PyTorch` provides a `WeightedRandomSampler` API, which is similar to the `sample_weight`
    parameter from `scikit-learn` (found in many of the fit methods in `scikit-learn`
    estimators (such as `RandomForestClassifier` and `LogisticRegression`) and serves
    a similar purpose of assigning a weight to each sample of the training dataset.
    We had a detailed discussion of the differences between `class_weight` and `sample_weight`
    in [*Chapter 5*](B17259_05.xhtml#_idTextAnchor151), *Cost-Sensitive Learning*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can specify `weights` as a parameter to `WeightedRamdomSampler` so that
    it can automatically weigh the examples in the batch, according to the weight
    of each sample. The `weights` parameter values are typically the inverse of the
    frequency of various classes in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '`class_weights` is more for the minority class labels than the majority class
    labels. Let’s compute the `weightedRamdomSampler` values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will learn how to sample data dynamically.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic sampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dynamic sampling [1] is an advanced technique that can self-adjust the sampling
    rate as training progresses. It promises to adapt according to the problem’s complexity
    and class imbalance, with almost no hyperparameter tuning. It is just one more
    tool in your arsenal to try on your dataset, especially when you have imbalanced
    image data at hand, and see whether it gives a better performance than the other
    techniques we’ve discussed so far in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The basic idea of dynamic sampling is to dynamically adjust the sampling rate
    for various classes, depending on whether they are doing well or worse in a particular
    training iteration when compared to the prior iteration. If a class is performing
    comparatively poorly, then the class is oversampled in the next iteration, and
    vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: The details of the algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'These are the core components of dynamic sampling:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Real-time data augmentation**: Apply various kinds of image transformations
    to the images of each training batch. These transformations can be rotation, flipping,
    adjusting brightness, translation, adjusting contrast/color, noise injection,
    and so on. As discussed earlier, this step helps to reduce model overfitting and
    improves generalization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic sampling method**: In each iteration, a sample size (given by a certain
    formula) is chosen, and a model is trained with that sample size. The classes
    with lower F1 scores are sampled at a higher rate in the next iteration, forcing
    the model to focus more on previously misclassified examples. The number of images,
    c j, for the next iteration is updated according to the following formula:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UpdateSampleSize( F1 i, c j) =  1 − f1 i, j  _ ∑ c k∈ C 1 − f1 i,k  × N
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: f1 i, j is the F1-score of class c j in iteration i
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: N = the average number of samples in all classes
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a particular training epoch, let’s say we got the following F1 score for
    each of the three classes, **A**, **B**, and **C**, on our validation dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Class** | **F1 score** |'
  prefs: []
  type: TYPE_TB
- en: '| A | 0.1 |'
  prefs: []
  type: TYPE_TB
- en: '| B | 0.2 |'
  prefs: []
  type: TYPE_TB
- en: '| C | 0.3 |'
  prefs: []
  type: TYPE_TB
- en: Table 7.1 – Sample F1-scores of each class after some epoch
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how we would compute the weight of each class for the next epoch of
    training:'
  prefs: []
  type: TYPE_NORMAL
- en: Weight (class A) =  N * (1 − f1 a)  ______________________  (1 − f1 a)+ (1 −
    f1 b)+ (1 − f1 c)  =  N * 0.9  _ 0.9 + 0.8 + 0.7
  prefs: []
  type: TYPE_NORMAL
- en: = N * 0.375
  prefs: []
  type: TYPE_NORMAL
- en: '*Weight (class B) = N* 0.8/2.4 =* *N*0.33*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Weight (class C) = N* 0.7/2.4 =* *N* 0.29*'
  prefs: []
  type: TYPE_NORMAL
- en: This means that we will sample class A at a higher rate than class B and class
    C, which makes sense because the performance on class A was weaker than that on
    classes B and C.
  prefs: []
  type: TYPE_NORMAL
- en: A second model is trained through transfer learning, without sampling, to prevent
    the minority classes from overfitting. At inference time, the model output is
    a function of both models.
  prefs: []
  type: TYPE_NORMAL
- en: There are additional details about the **DynamicSampling** algorithm that we
    have omitted here due to space constraints. You can find the complete implementation
    code in the corresponding GitHub repository for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – An overall model accuracy comparison of various sampling techniques
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 7.2* shows the per-class model accuracy using various sampling techniques,
    including the baseline, where no sampling is done.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Class** | **Baseline** | **Weighted** **random sampler** | **Dynamic**
    **sampler** | **Random** **oversampling** | **Random** **undersampling** |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | **99.9** | 99.0 | 92.4 | 99.1 | 97.2 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | **99.7** | 99.2 | 96.8 | 99.2 | 90.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | **98.5** | 98.3 | 93.5 | **98.5** | 70.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 97.3 | 97.4 | 96.8 | **98.3** | 74.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 98.3 | 98.0 | 91.9 | **98.6** | 79.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 96.2 | 96.0 | 97.3 | **98.1** | 52.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 94.5 | 97.6 | **98.7** | 97.3 | 77.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 89.7 | 94.7 | **96.5** | 94.1 | 81.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 63.3 | 91.5 | **96.9** | 93.0 | 60.2 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 50.7 | 92.6 | **97.7** | 91.8 | 56.5 |'
  prefs: []
  type: TYPE_TB
- en: Table 7.2 – A per-class model accuracy comparison of various sampling techniques
    (the highest value for a class is in bold)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some insights from the results:'
  prefs: []
  type: TYPE_NORMAL
- en: In terms of overall performance, **Random OverSampling** (**ROS**) performed
    the best, while **Random Undersampling** (**RUS**) did the worst.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although ROS did the best, it can be computationally very expensive due to data
    cloning, making it less suitable for large datasets and industrial settings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic sampling did a little worse than ROS; it did best on the minority classes
    6–9 and would be our preferred choice here. However, due to its increased complexity,
    our second choice will be the weighted random sampler.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The baseline and weighted random sampler techniques are stable across classes;
    RUS is notably variable and performs poorly on most classes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, we can apply SMOTE in the same way as `RandomOverSampler`. Please
    note that while SMOTE can be applied to images, its use of a linear subspace of
    the original data is often limiting.
  prefs: []
  type: TYPE_NORMAL
- en: This ends our discussion of the various sampling techniques. In the next section,
    we will focus on data augmentation techniques specifically designed for images
    to achieve more effective oversampling.
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation techniques for vision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Today, a variety of custom augmentation techniques are used for various kinds
    of data, such as images, audio, video, and even text data. In the vision realm,
    this includes techniques such as rotating, scaling, cropping, blurring, adding
    noise to an image, and a host of other techniques, including combining those techniques
    all at once in some appropriate sequence. Image data augmentation is not really
    a recent innovation. Some image augmentation techniques can also be found in the
    LeNet-5 model paper [2] from 1998, for example. Similarly, the AlexNet model [3]
    from 2012 uses random cropping, flipping, changing the color intensity of RGB
    channels, and so on to reduce errors during model training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s discuss why data augmentation can often be helpful:'
  prefs: []
  type: TYPE_NORMAL
- en: In problems where we have limited data or imbalanced data, it may not always
    be possible to gather more data. This could be because either gathering more data
    is difficult in the first place (for example, waiting for more fraud to occur
    when dealing with credit card fraud, or gathering satellite images where we have
    to pay satellite operators, which can be quite expensive) or labeling the data
    is difficult or expensive (for example, to label medical image datasets, we need
    domain experts).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data augmentation can help reduce overfitting and improve the overall performance
    of a model. One motivation for this practice is that attributes such as lighting,
    noise, color, scale, and focus in the training set may not align with those in
    the real-world images on which we run inference. Additionally, augmentation diversifies
    a dataset to help the model generalize better. For example, if the model is trained
    only on images of cats facing right, it may not perform well on images where the
    cat faces left. Therefore, it’s advisable to always apply valid transformations
    to augment image datasets, as most models gain performance with more diverse data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data augmentation is widely used for computer vision tasks such as object detection,
    classification, and segmentation. It can be very useful for NLP tasks as well.
    In the computer vision world, there are lots of open source libraries that help
    standardize the various image augmentation techniques, while the NLP tools for
    data augmentation space have yet to mature.
  prefs: []
  type: TYPE_NORMAL
- en: While there are several popular open source libraries for image augmentation,
    such as `imgaug`, Facebook’s `AugLy`, and `Albumentations`, we will use `torchvision`
    in this book. As a part of the PyTorch ecosystem, it offers seamless integration
    with PyTorch workflows, a range of common image transformations, as well as pre-trained
    models and datasets, making it a convenient and comprehensive choice for computer
    vision tasks. If you need more advanced augmentations, or if speed is a concern,
    `Albumentations` may be a better choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use `torchvision.transforms.Pad` to add some padding to the image boundaries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B17259_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – The results of applying the Pad function to the image
  prefs: []
  type: TYPE_NORMAL
- en: 'The `torchvision.transforms.FiveCrop` class transforms and crops the given
    image into four corners and the central crop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B17259_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – The results of applying the FiveCrop function on the image
  prefs: []
  type: TYPE_NORMAL
- en: '`torchvision.transforms.CenterCrop` is a similar class to crop images from
    the center.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `torchvision.transforms.ColorJitter` class changes the brightness, saturation,
    and other similar properties of the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B17259_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – The results of applying the ColorJitter function on the image
  prefs: []
  type: TYPE_NORMAL
- en: '`GaussianBlur` can add some blurring to the images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B17259_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – The results of applying the GaussianBlur function on the image
  prefs: []
  type: TYPE_NORMAL
- en: 'The `torchvision.transforms.RandomRotation` class transform rotates an image
    at a random angle:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B17259_07_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – The results of random rotation on the original image (leftmost)
  prefs: []
  type: TYPE_NORMAL
- en: Consider exploring the other image transformation functionalities supported
    by the `torchvision.transforms` class that we didn’t discuss here.
  prefs: []
  type: TYPE_NORMAL
- en: Cutout masks out random square regions of input images during training. While
    it may seem like this technique removes unnecessary portions of the image, it’s
    important to note that the areas to be masked are typically selected at random.
    The primary aim is to force the neural network to generalize better by ensuring
    it does not overly rely on any specific set of pixels within a given image.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_07_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – The result of applying the cutout function on an image
  prefs: []
  type: TYPE_NORMAL
- en: 🚀 Deep learning data-level techniques in production at Etsy/Booking/Wayfair
  prefs: []
  type: TYPE_NORMAL
- en: '**🎯** **Problem** **being solved:**'
  prefs: []
  type: TYPE_NORMAL
- en: Etsy, [Booking.com](http://Booking.com), and Wayfair leveraged user behavior
    to enhance personalization. Etsy focused on item recommendations based on browsing
    history [4], [Booking.com](http://Booking.com) tailored search results to boost
    bookings [5], and Wayfair optimized product image angles to improve CTRs [6].
    All aimed to utilize data-driven strategies for better user experience and performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**⚖️** **Data** **imbalance issue:**'
  prefs: []
  type: TYPE_NORMAL
- en: Etsy, [Booking.com](http://Booking.com), and Wayfair each grappled with data
    imbalance issues in their machine learning projects. Etsy faced a power law distribution
    in user sessions, where most users interacted with only a few listings within
    a one-hour window. [Booking.com](http://Booking.com) dealt with imbalanced classes
    in hotel images, as photos of bedrooms and bathrooms vastly outnumbered those
    of other facilities such as saunas or table tennis. Wayfair encountered an imbalance
    in real-world images of furniture, with a majority of images showing the “front”
    angle, leading to poor performance for other angles. All three companies had to
    address these imbalances to improve the performance and fairness of their models.
  prefs: []
  type: TYPE_NORMAL
- en: '**🎨** **Data** **augmentation strategy:**'
  prefs: []
  type: TYPE_NORMAL
- en: Etsy, [Booking.com](http://Booking.com), and Wayfair each had unique data augmentation
    strategies to address their specific challenges. Etsy used image random rotation,
    translation, zoom, and color contrast transformation to augment their dataset.
    [Booking.com](http://Booking.com) employed a variety of techniques, including
    mirroring, random cropping, affine transformation, aspect ratio distortion, color
    manipulation, and contrast enhancement. They increased their labeled data by 10
    times through these methods, applying distortions on the fly during training.
    Wayfair took a different approach by creating synthetic data with 3D models, generating
    100 views for each 3D model of chairs and sofas, thus providing granular angle
    information for training.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s look at some of the more advanced techniques, such as CutMix, MixUp,
    and AugMix, which are types of **Mixed Sample Data Augmentation** (**MSDA**) techniques.
  prefs: []
  type: TYPE_NORMAL
- en: MSDA is a set of techniques that involve mixing data samples to produce an augmented
    dataset, used to train a model (*Figure 7**.9*).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_07_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – Common MSDA techniques
  prefs: []
  type: TYPE_NORMAL
- en: CutMix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'CutMix [7] is an image data augmentation technique where patches are cut and
    pasted among training images. Specifically, a portion of an image is replaced
    by a portion of another image, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_07_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – The result of applying the CutMix function to the images (image
    1 and image 2)
  prefs: []
  type: TYPE_NORMAL
- en: It’s designed to encourage a model to make more localized, fine-grained predictions,
    thus improving overall generalization. CutMix also enforces consistent predictions
    outside the mixed regions, further enhancing model robustness.
  prefs: []
  type: TYPE_NORMAL
- en: '`torchvision` also offers an in-built API for CutMix, `torchvision.transforms.v2.CutMix`,
    so we don’t have to implement it from scratch.'
  prefs: []
  type: TYPE_NORMAL
- en: The full notebook with CutMix implementation from scratch can be found in the
    GitHub repo.
  prefs: []
  type: TYPE_NORMAL
- en: CutMix often shows improvements over traditional augmentation techniques on
    benchmark datasets, such as CIFAR-10, CIFAR-100, and ImageNet [7].
  prefs: []
  type: TYPE_NORMAL
- en: MixUp
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MixUp [8] creates virtual training examples by forming combinations of pairs
    of inputs and their corresponding labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'If (x i, y i) and (x j, y j) is an arbitrary pair of images in dataset D, where
    x is the image while y is its label, a mixed sample  ~ x ,  ~ y  can be generated
    using the following equations:'
  prefs: []
  type: TYPE_NORMAL
- en: ~ x  = λ x i + (1 − λ) x j
  prefs: []
  type: TYPE_NORMAL
- en: ~ y  = λ y i + (1 − λ) y j
  prefs: []
  type: TYPE_NORMAL
- en: where λ is the mixing factor sampled from the beta distribution. The Beta distribution
    is a flexible, continuous probability distribution defined on the interval [0,
    1].
  prefs: []
  type: TYPE_NORMAL
- en: 'MixUp acts as a regularizer, preventing overfitting and enhancing the generalization
    capabilities of models. The following implementation shuffles the data and targets,
    and then combines them using a weighted average, based on a value sampled from
    the Beta distribution, creating mixed data and targets for augmentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B17259_07_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 – The result of applying MixUp on the images (image 1 and image
    2)
  prefs: []
  type: TYPE_NORMAL
- en: On datasets such as CIFAR-100, MixUp has been found to provide significant gains
    in test accuracy compared to models trained without MixUp [8].
  prefs: []
  type: TYPE_NORMAL
- en: Similar to CutMix, `torchvision` provides a built-in API called `torchvision.transforms.v2.MixUp`,
    eliminating the need for manual implementation.
  prefs: []
  type: TYPE_NORMAL
- en: AugMix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The augmentation techniques that we have studied so far have all been fixed
    augmentations, but deep learning models can memorize them [9] and their performance
    can plateau. This is where AugMix [10] can be helpful, as it produces a diverse
    set of augmented images by performing several random augmentations in a sequence.
  prefs: []
  type: TYPE_NORMAL
- en: AugMix improves model robustness and uncertainty without requiring any changes
    to the model architecture. The full AugMix algorithm also uses a special kind
    of loss function, but we will skip that for simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code presents a simplified version of AugMix’s core logic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: At the end of the function, we combine images by using equal weight for each
    of the four transformed pictures. The actual AugMix implementation uses a Dirichlet
    distribution function to combine the images. A Dirichlet distribution is a generalization
    of the beta distribution that we saw in the MixUp technique.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_07_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.12 – The result of applying the Augmix function to four different images
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 7**.12*, the top row shows the original image, while the bottom row
    shows the result of applying AugMix. Images 1 and 3 don’t seem to have changed,
    but images 2 and 4 have noticeable changes.
  prefs: []
  type: TYPE_NORMAL
- en: According to the AugMix paper [10], in experiments with ImageNet and CIFAR,
    AugMix achieved reduced test errors while providing improved robustness against
    corruption.
  prefs: []
  type: TYPE_NORMAL
- en: We don’t need to create AugMix from scratch, as `torchvision` provides a built-in
    API called `torchvision.transforms.AugMix` for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Remix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Standard data augmentation techniques such as MixUp and CutMix may not be sufficient
    to handle class imbalances, as they do not take the distribution of class labels
    into account. Remix [11] addresses the challenges of training deep learning models
    on imbalanced datasets.
  prefs: []
  type: TYPE_NORMAL
- en: MixUp and CutMix utilize the same mixing factor to combine samples in both the
    feature space and the label space. In the context of imbalanced data, the authors
    of the Remix paper [11] argued that this approach may not be optimal. Therefore,
    they proposed to separate the mixing factors, allowing for more flexibility in
    their application. By doing so, greater weight can be assigned to the minority
    class, enabling the creation of labels that are more favorable to the underrepresented
    class.
  prefs: []
  type: TYPE_NORMAL
- en: 'If (xi, yi; xi, yj) is an arbitrary pair of images in dataset D, a mixed sample
    xRM, y RM can be generated using the following equations:'
  prefs: []
  type: TYPE_NORMAL
- en: x RM = λxxi + (1 − λx)xj
  prefs: []
  type: TYPE_NORMAL
- en: y RM = λyyi + (1 − λy)yj
  prefs: []
  type: TYPE_NORMAL
- en: λx and λy are the mixing factors sampled from the beta distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a simplified implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Combining previous techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It’s possible to combine these methods to introduce even more diversity to
    the training data, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**CutMix and MixUp**: These can be alternated or used in tandem, creating regions
    in images that are replaced with parts of other images while also blending images
    pixel-wise'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sequential**: You could sequentially apply these techniques (e.g., use MixUp
    first and then CutMix) to further diversify the augmented dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When combining these methods, it’s important to carefully manage the probabilities
    and strengths of each method, thus avoiding introducing too much noise or making
    the training data too divergent from the original distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Also, while combining these methods might improve model robustness and generalization
    in certain scenarios, it can also make training more computationally intensive
    and complex. It’s essential to balance the benefits against the potential trade-offs.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, always validate the effectiveness of combined augmentations on a validation
    set to ensure they are beneficial for the task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use a long-tailed version of a different dataset called Fashion-MNIST
    for the techniques we just discussed (*Figure 7**.13*). Fashion-MNIST is another
    MNIST variant, consisting of 60,000 training and 10,000 testing images of 10 different
    clothing items, such as shoes, shirts, and dresses, each represented in a grayscale
    image of 28x28 pixels.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_07_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.13 – Imbalanced FashionMNIST
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7**.14* shows the overall model accuracy when trained using CutMix,
    MixUp, a combination of both, and Remix on the imbalanced FashionMNIST dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_07_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.14 – Overall model accuracy (FashionMNIST)
  prefs: []
  type: TYPE_NORMAL
- en: The difference in the performance of these techniques is more apparent when
    looking at the class-wise accuracy numbers (*Figure 7**.15*).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_07_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.15 – Class-wise model accuracy (the FashionMNIST dataset)
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the given data, here are some insightful conclusions that can be drawn
    for the various techniques, especially in the context of imbalanced data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Overall performance**: **Cutmix** and **Remix** generally offer the highest
    performance across most classes, followed closely by **Mixup** and **Cutmix+Mixup**.
    **Baseline** seems to be the least effective in general.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance on minority classes**: For the minority class labeled as “6,”
    all techniques show relatively low performance compared to other classes. However,
    **Mixup** and **Cutmix+Mixup** offer a slight improvement over the baseline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consistency across classes**: **Cutmix** and **Mixup** are more consistent
    across different classes, excluding class “6,” where they are only marginally
    better. **Baseline**, on the other hand, shows significant variability, performing
    extremely well on some classes (such as “0” and “5”) but poorly on others (such
    as “6”).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Techniques suited for specific classes**: **Cutmix** performs exceptionally
    well for the classes labeled “1” and “8,” where it outperforms all other techniques.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Remix** is particularly strong for the class labeled “2,” where it outshines
    all other techniques.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Complexity versus benefit**: **Cutmix+Mixup** does not offer a significant
    improvement over **Cutmix** or **Mixup** individually, raising questions about
    whether the additional computational complexity is justified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generalizability**: **Cutmix** and **Mixup** appear to be the most robust
    techniques, showing high performance across most classes. These techniques would
    likely perform well on unseen data and are potentially good choices for imbalanced
    datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trade-offs**: **Cutmix** offers high performance but may not be the best
    for minority classes. **Mixup**, although slightly less effective overall, offers
    more balanced performance across classes, including minority ones.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are some points to be careful about while performing these image
    augmentations:'
  prefs: []
  type: TYPE_NORMAL
- en: We must ensure that data augmentations preserve the original labels. For instance,
    rotating digits such as 6 and 9 can be problematic in digit recognition tasks.
    Similarly, cropping an image could invalidate its label, such as removing a cat
    from a “cat” image. This is especially crucial in complex tasks, such as image
    segmentation in self-driving cars, where augmentations can alter output labels
    or masks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While geometric and color transformations often increase memory usage and training
    time, they are not inherently problematic. Although color alterations can sometimes
    remove important details and affect label integrity, smart manipulation can also
    be beneficial. For instance, tweaking the color space to mimic different lighting
    or camera lenses can improve model performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To overcome some of the increased memory, time, and cost issues, as mentioned,
    a technique called `AutoAugment` is available in `PyTorch`, which can automatically
    search for the best augmentation policies on the dataset being used.
  prefs: []
  type: TYPE_NORMAL
- en: 🚀 Deep learning data-level techniques in production at Grab
  prefs: []
  type: TYPE_NORMAL
- en: '**🎯** **Problem** **being solved:**'
  prefs: []
  type: TYPE_NORMAL
- en: Grab, a ride-hailing and food delivery company in South-East Asia, faced the
    primary challenge of anonymizing faces and license plates in images collected
    for their geotagged imagery platform, KartaView [12]. This was essential to ensure
    user privacy.
  prefs: []
  type: TYPE_NORMAL
- en: '**⚖️** **Data** **imbalance issue**:'
  prefs: []
  type: TYPE_NORMAL
- en: The dataset used by Grab was imbalanced, particularly in terms of the object
    sizes. Larger regions of interest, such as close-ups of faces or license plates,
    were underrepresented. This skewed distribution led to poor model performance
    in detecting these larger objects.
  prefs: []
  type: TYPE_NORMAL
- en: '**🎨** **Data** **augmentation strategy:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Grab employed a multi-pronged data augmentation approach to address the imbalance:'
  prefs: []
  type: TYPE_NORMAL
- en: '• **Offline augmentation**: One key method they used was the “image view splitting,”
    where each original image is divided into multiple “views” with predefined properties.
    This was crucial to accommodate different types of images such as perspective,
    wide field of view, and 360-degree equirectangular images. Each “view” was treated
    as a separate image with its tags, which helped the model generalize better. They
    also implemented oversampling for images with larger tags, addressing the imbalance
    in their dataset. This was vital for their anchor-based object detection model,
    as the imbalance was affecting the model’s performance in identifying larger objects.'
  prefs: []
  type: TYPE_NORMAL
- en: '• **Online augmentation**: They used YOLOv4 for object detection, which allowed
    for a variety of online augmentations, such as saturation, exposure, hue, flip,
    and mosaic.'
  prefs: []
  type: TYPE_NORMAL
- en: Modern techniques such as autoencoders and adversarial networks, specifically
    **Generative Adversarial Networks** (**GANs**), have recently gained traction
    in creating synthetic data to enhance image datasets. A GAN comprises two neural
    networks – the generator, which produces synthetic data, and the discriminator,
    which evaluates the authenticity of this data. Together, they work to create realistic
    and high-quality synthetic samples. GANs have also been applied to generate synthetic
    tabular data. For example, they’ve been used to create synthetic medical images
    that significantly improve diagnostic models. We’ll explore these cutting-edge
    techniques in greater detail toward the end of the chapter. In the next section,
    we will learn about applying data-level techniques to NLP problems.
  prefs: []
  type: TYPE_NORMAL
- en: Data-level techniques for text classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data imbalance, wherein certain classes in a dataset are underrepresented, is
    not just an issue confined to image or structured data domains. In NLP, imbalanced
    datasets can lead to biased models that might perform well on the majority class
    but are likely to misclassify underrepresented ones. To address this challenge,
    numerous strategies have been devised.
  prefs: []
  type: TYPE_NORMAL
- en: 'In NLP, data augmentation can boost model performance, especially with limited
    training data. *Table 7.3* categorizes the various data augmentation techniques
    for text data:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Level** | **Method** | **Description** | **Example techniques** |'
  prefs: []
  type: TYPE_TB
- en: '| Character level | Noise | Introducing randomness at the character level |
    Jumbling characters |'
  prefs: []
  type: TYPE_TB
- en: '| Rule-based | Transformations based on predefined rules | Capitalization |'
  prefs: []
  type: TYPE_TB
- en: '| Word level | Noise | Random word changes | “cat” to “dog” |'
  prefs: []
  type: TYPE_TB
- en: '| Synonyms | Replacing words with their synonyms | “happy” to “joyful” |'
  prefs: []
  type: TYPE_TB
- en: '| Embeddings | Using word embeddings for replacement | “king” to “monarch”
    |'
  prefs: []
  type: TYPE_TB
- en: '| Language models | Leveraging advanced language models for word replacement
    | BERT |'
  prefs: []
  type: TYPE_TB
- en: '| Phrase level | Structure | Altering the structure of phrases | Changing word
    order |'
  prefs: []
  type: TYPE_TB
- en: '| Interpolation | Merging features of two phrases | “The cat sat” + “The dog
    barked” = “The cat barked” |'
  prefs: []
  type: TYPE_TB
- en: '| Document level | Translation | Translating the document to another language
    and back | English to French to English |'
  prefs: []
  type: TYPE_TB
- en: '| Generative | Using models to generate new content | GPT-3 |'
  prefs: []
  type: TYPE_TB
- en: Table 7.3 – The categorization of different data augmentation methods (adapted
    from [13])
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation techniques for text can be categorized into character, word,
    phrase, and document levels. Techniques vary from jumbling characters to using
    models such as BERT and GPT-3\. This taxonomy guides us through NLP data augmentation
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 7.3* shows various data augmentation methods used in NLP. We will break
    down the methods based on the level at which the data is manipulated – character,
    word, phrase, and document. Each level has its unique set of methods, such as
    introducing “noise” at the character level or leveraging “language models” at
    the word level. These methods are not just random transformations; they are often
    carefully designed to preserve the semantic meaning of the text while introducing
    variability.'
  prefs: []
  type: TYPE_NORMAL
- en: What sets this categorization apart is its multilayered approach, which allows
    a more targeted application of data augmentation methods. For instance, if you’re
    dealing with short text snippets, methods at the character or word level may be
    more appropriate. On the other hand, if you’re working with longer documents or
    need to generate entirely new content, then methods at the document level, such
    as “generative” techniques, come into play.
  prefs: []
  type: TYPE_NORMAL
- en: In the subsequent sections, we will explore a text classification dataset that
    is imbalanced and illustrate various data augmentation techniques using it. These
    methodologies are designed to synthesize additional data, thereby enhancing a
    model’s ability to learn and generalize from the imbalanced information.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset and baseline model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s take the spam text message classification dataset available on Kaggle
    ([https://www.kaggle.com/datasets/team-ai/spam-text-message-classification](https://www.kaggle.com/datasets/team-ai/spam-text-message-classification)).
    This dataset, primarily used to distinguish spam from legitimate messages, presents
    an imbalance with a majority of “ham” (legitimate) messages and a minority of
    “spam” messages. We are skipping the code here to save space. You can find the
    notebook in the GitHub repo with the name `Data_level_techniques_NLP.ipynb`.
  prefs: []
  type: TYPE_NORMAL
- en: 'With a baseline model, we have the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Random oversampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One basic technique to handle data imbalance is random oversampling, where
    instances of the minority class are replicated to balance out the class distribution.
    While this technique is easy to implement and often shows improved performance,
    it’s essential to be wary of overfitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Random oversampling shows a slight improvement in overall accuracy, rising from
    0.97 to 0.98\. The most notable gain is in the recall for the `spam` class, which
    increased from 0.80 to 0.91, indicating better identification of spam messages.
    However, the precision for `spam` dropped a bit from 0.97 to 0.93.
  prefs: []
  type: TYPE_NORMAL
- en: The macro average F1-score also improved from 0.93 to 0.95, suggesting that
    the model is now better at handling both classes (`ham` and `spam`) more equally.
    The weighted average metrics remain strong, reinforcing that the model’s overall
    performance has improved without sacrificing its ability to correctly classify
    the majority class (`ham`).
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, undersampling can be useful to reduce the size of the majority class,
    particularly by eliminating exact duplicate sentences. For example, you might
    not need 500 copies of “Thanks very much!” However, sentences with similar semantic
    meaning but different wording, such as “Thanks very much!” and “Thanks so much!”,
    should generally be retained. Exact duplicates can be identified, using methods
    such as string matching, while sentences with similar meanings can be detected
    using cosine similarity or the Jaccard similarity of sentence embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 🚀 Deep learning data-level techniques in production at Cloudflare
  prefs: []
  type: TYPE_NORMAL
- en: '**🎯** **Problem** **being solved**:'
  prefs: []
  type: TYPE_NORMAL
- en: Cloudflare [14] aimed to enhance its **Web Application Firewall** (**WAF**)
    to better identify malicious HTTP requests and protect against common attacks,
    such as SQL injection and **cross-site** **scripting** (**XSS**).
  prefs: []
  type: TYPE_NORMAL
- en: '**⚖️** **Data** **imbalance issue**:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a quality dataset to train the WAF model was difficult, due to strict
    privacy regulations and the absence of labeled data for malicious HTTP requests.
    The heterogeneity of samples also presented challenges as the requests came in
    various formats and encodings. There was a significant lack of samples for specific
    types of attacks, making the dataset imbalanced and leading to the risk of false
    positives or negatives.
  prefs: []
  type: TYPE_NORMAL
- en: '**🎨** **Data** **augmentation strategy**:'
  prefs: []
  type: TYPE_NORMAL
- en: To tackle this, Cloudflare employed a combination of data augmentation and generation
    techniques. These included mutating benign content in various ways, generating
    pseudo-random noise samples, and using language models for synthetic data creation.
    The focus was on increasing the diversity of negative samples while maintaining
    the integrity of the content, thereby forcing the model to consider a broader
    spectrum of structural, semantic, and statistical properties for better classification.
  prefs: []
  type: TYPE_NORMAL
- en: '🚀 **Model deployment**:'
  prefs: []
  type: TYPE_NORMAL
- en: The model that they used significantly improved after employing these data augmentation
    techniques, with a remarkable F1 score of 0.99 after augmentation compared to
    0.61 before. The model has been validated against Cloudflare’s signature-based
    WAF and was found to perform comparably, making it production-ready.
  prefs: []
  type: TYPE_NORMAL
- en: Document-level augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In document-level augmentation, entire documents are modified to create new
    examples, in order to preserve the broader semantic context or narrative flow
    of the document. One such technique is back translation.
  prefs: []
  type: TYPE_NORMAL
- en: Back translation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Back translation involves translating a sentence to a different language and
    then reverting it back to the original (*Figure 7**.16*). This produces sentences
    that are syntactically different but semantically similar to the original text,
    providing a form of augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_07_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.16 – Demonstrating the back translation technique
  prefs: []
  type: TYPE_NORMAL
- en: We generate the back-translated text and append it to the original dataset.
    Then, we use the full dataset to train the logistic regression model. Note that
    this can be a time-consuming process, since the translation model binaries are
    resource-intensive. It may also introduce errors, since some words may not be
    exactly translatable across languages. In the GitHub notebook, we used the `BackTranslationAug`
    API from the `nlpaug` library [15].
  prefs: []
  type: TYPE_NORMAL
- en: 'The following results show the classification metrics on the test set. The
    precision of the spam class shows improvement over the random oversampling technique,
    while recall is a bit worse:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Back translation maintains an overall accuracy of 0.98, similar to random oversampling.
    It slightly improves `spam` precision to 0.96 but lowers recall to 0.86\. Both
    methods outperform the baseline, with back translation favoring precision over
    recall for the `spam` class.
  prefs: []
  type: TYPE_NORMAL
- en: Character and word-level augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s briefly go over a few character and word-level augmentation techniques
    that can be applied to NLP problems.
  prefs: []
  type: TYPE_NORMAL
- en: Easy Data Augmentation techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Easy Data Augmentation** (**EDA**) is a suite of data augmentation techniques
    specific to text data. It includes simple operations such as synonym replacement,
    random insertion, random swap, and random deletion. These operations, being simple,
    ensure that the augmented data remains meaningful. The following table shows various
    metrics when using EDA on the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: After applying EDA, the model retains an overall accuracy of 0.98, consistent
    with both random oversampling and back translation. The precision for `spam` is
    high at 0.96, similar to back translation, while the recall is slightly better
    at 0.88 compared to 0.86 with Back Translation. The macro and weighted averages
    remain robust at 0.95 and 0.98, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: EDA offers a balanced improvement in both precision and recall for the `spam`
    class, making it a strong contender among the data augmentation techniques we’ve
    tried.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Precision** | **Recall** | **F1-Score** | **Accuracy** |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline model | **0.97** | 0.80 | 0.88 | 0.97 |'
  prefs: []
  type: TYPE_TB
- en: '| Random oversampling | 0.93 | **0.91** | **0.92** | **0.98** |'
  prefs: []
  type: TYPE_TB
- en: '| Back translation | 0.96 | 0.86 | 0.91 | **0.98** |'
  prefs: []
  type: TYPE_TB
- en: '| EDA | 0.96 | 0.88 | 0.91 | **0.98** |'
  prefs: []
  type: TYPE_TB
- en: Table 7.4 – Comparing the results of the various NLP data-level techniques for
    the spam class (max per metric in bold)
  prefs: []
  type: TYPE_NORMAL
- en: Overall, as we can see in *Table 7.4*, for our dataset, random oversampling
    excels in recall for `spam` but slightly lowers precision. Back translation boosts
    precision at a minor recall trade-off. EDA offers a balanced improvement in both.
    It’s important to note that these results are empirical and specific to the dataset
    used for this analysis. Data augmentation techniques can yield different outcomes,
    depending on the nature of the data, its distribution, and the problem being addressed.
    Therefore, while these techniques show promise in this context, their effectiveness
    may vary when applied to different datasets or NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: We will not be covering phrase-level augmentation techniques in this book due
    to space constraints, but we recommend exploring them on your own.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at some miscellaneous data-level deep learning techniques
    at a high level.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion of other data-level deep learning methods and their key ideas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to the methods previously discussed, there is a rich array of other
    techniques specifically designed to address imbalanced data challenges. This section
    provides a high-level overview of these alternative approaches, each offering
    unique insights and potential advantages. While we will only touch upon their
    key ideas, we encourage you to delve deeper into the literature and explore them
    further if you find these techniques intriguing.
  prefs: []
  type: TYPE_NORMAL
- en: Two-phase learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Two-phase learning [16][17] is a technique designed to enhance the performance
    of minority classes in multi-class classification problems, without compromising
    the performance of majority classes. The process involves two training phases:'
  prefs: []
  type: TYPE_NORMAL
- en: In the first phase, a deep learning model is first trained on the dataset, which
    is balanced with respect to each class. Balancing can be done using sampling techniques
    such as random oversampling or undersampling.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the second phase, we freeze all the layers except the last one, and then
    the model is fine-tuned using the entire dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first phase ensures that all layers are trained on a balanced dataset. The
    second phase calibrates the output probabilities by retraining the last layer
    with the entire dataset, reflecting the original imbalanced class distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The order of the two phases can be reversed – that is, the first model is trained
    on the full imbalanced data and then fine-tuned on a balanced dataset in the second
    phase. This is called **deferred sampling**, since sampling is done later.
  prefs: []
  type: TYPE_NORMAL
- en: Expansive Over-Sampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Introduced in a paper by Damien Dablain et al. [18], **Expansive Over-Sampling**
    (**EOS**) is another data augmentation technique used within a three-phase CNN
    training framework, designed for imbalanced data. It can be considered to incorporate
    both two-phase learning and data augmentation techniques.
  prefs: []
  type: TYPE_NORMAL
- en: EOS works by creating synthetic training instances as combinations between the
    minority class samples and their nearest “enemies” in the embedded space. The
    term “nearest enemies” refers to instances of other classes that are closest in
    the feature space to a given instance. By creating synthetic instances in this
    way, EOS aims to reduce the generalization gap, which is wider for minority classes.
  prefs: []
  type: TYPE_NORMAL
- en: The paper’s authors [18] claimed that this method improves accuracy and efficiency
    over common imbalanced learning techniques, requiring fewer parameters and less
    training time.
  prefs: []
  type: TYPE_NORMAL
- en: Using generative models for oversampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative models, including GANs, **Variational AutoEncoders** (**VAEs**),
    diffusion models, and their derivatives, such as StyleGAN, StyleGAN2, and GPT-based
    models, have become prominent tools for producing data points that resemble training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: VAEs, a specific type of generative model, consist of an encoder and decoder
    that work together to create new instances of data, such as realistic images,
    and can be used to balance imbalanced datasets. On the long-tailed version of
    MNIST, we got a decent performance improvement by using a VAE-augmented model
    when compared to the baseline model on the most imbalanced classes. *Figure 7**.17*
    shows the performance comparison after 50 epochs. You can find the notebook in
    the corresponding chapter of the GitHub repo.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_07_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.17 – VAE-augmented model performance on the long-tailed MNIST dataset
  prefs: []
  type: TYPE_NORMAL
- en: Diffusion models operate by progressively corrupting an image with noise and
    then reconstructing it, with applications in areas such as medical imaging. Examples
    include DALLE-2 and the open source stable diffusion model.
  prefs: []
  type: TYPE_NORMAL
- en: Recent studies [19] highlight the utility of synthetic data in enhancing zero-shot
    and few-shot image classification tasks. Specifically, text-to-image generation
    models, when used in conjunction with large-scale pre-trained models such as DALL-E
    and Stable Diffusion, significantly improve performance in scenarios where real-world
    data is sparse or unavailable. These generative models have gained prominence
    for their ability to create high-quality images based on natural language prompts,
    offering a potential solution for imbalanced datasets. For example, if there is
    a scarcity of images featuring a monkey seated in a car, these models can generate
    hundreds or even thousands of such images to augment training datasets. However,
    it’s worth noting that models trained solely on synthetic data may still underperform
    compared to those trained on real data.
  prefs: []
  type: TYPE_NORMAL
- en: These models often require significant computational resources, making them
    time-consuming and expensive to scale up, especially for vast datasets. Diffusion
    models, in particular, are computationally intensive, and potential overfitting
    can compromise model generalizability. Therefore, it is crucial to balance the
    benefits of data augmentation with the computational cost and potential challenges
    when employing these advanced generative models.
  prefs: []
  type: TYPE_NORMAL
- en: DeepSMOTE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **Deep Synthetic Minority Oversampling** (**DeepSMOTE**) technique [20]
    is essentially SMOTE adapted for deep learning models using an encoder-decoder
    architecture, with minor tweaks for image data. DeepSMOTE consists of three major
    components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**An encoder/decoder framework to handle complex and high-dimensional data**:
    An encoder/decoder framework is used to learn a compact feature representation
    of the image data. It is trained to reconstruct the original images from this
    compact form, ensuring that essential features are captured.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SMOTE-based oversampling for generating synthetic instances**: Once the feature
    representation is learned, SMOTE is applied in this feature space to generate
    synthetic instances of the minority class. This is particularly useful for image
    data where the raw data is high-dimensional and complex. SMOTE creates these synthetic
    instances by finding the *k*-nearest neighbors in the feature space and generating
    new instances that are interpolations between the instance under consideration
    and its neighbors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A dedicated loss function**: DeepSMOTE introduces a specialized loss function
    that not only focuses on the reconstruction error (how well the decoder can reconstruct
    the original image from the encoded form) but also includes a penalty term, ensuring
    that the synthetic instances are useful for the classification task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlike GAN-based oversampling, DeepSMOTE does not require a discriminator. It
    claims to generate high-quality, information-rich synthetic images that can be
    visually inspected.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_07_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.18 – Demonstrating the DeepSMOTE technique (adapted from [20])
  prefs: []
  type: TYPE_NORMAL
- en: Neural style transfer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Neural style transfer is a technique in deep learning that artistically blends
    the content of one image with the style of another (*Figure 7**.19*). While its
    primary application is in art and image processing, the concept of generating
    synthetic data samples can be adapted to address data imbalance in machine learning.
    By drawing inspiration from style transfer, one could potentially generate synthetic
    samples for the minority class, blend features of different classes, or adapt
    domain-specific knowledge. However, care must be taken to ensure that synthetic
    data authentically represents real-world scenarios to avoid overfitting and poor
    generalization of real data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_07_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.19 – Demonstrating the neural style transfer technique
  prefs: []
  type: TYPE_NORMAL
- en: We hope that this provides a thorough understanding of data-level deep learning
    methods to address imbalanced data, including oversampling, data augmentation,
    and various other strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The transition of methods to handle data imbalance from classical machine learning
    models to deep learning models can pose unique challenges, primarily due to the
    distinct types of data that these models have to work with. Classical machine
    learning models typically deal with structured, tabular data, whereas deep learning
    models often grapple with unstructured data, such as images, text, audio, and
    video. This chapter explored how to adapt sampling techniques to work with deep
    learning models. To facilitate this, we used an imbalanced version of the MNIST
    dataset to train a model, which is then employed in conjunction with various oversampling
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating random oversampling with deep learning models involves duplicating
    samples from minority classes randomly, until each class has an equal number of
    samples. This is usually performed using APIs from libraries such as imbalanced-learn,
    Keras, TensorFlow, or PyTorch, which work together seamlessly for this purpose.
    Once data is oversampled, it can be sent for model training in PyTorch or TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter also delved into different data augmentation techniques, which can
    be especially beneficial when dealing with limited or imbalanced data. Augmentation
    techniques include rotating, scaling, cropping, blurring, and adding noise, among
    other advanced techniques such as AugMix, CutMix, and MixUp. However, care must
    be taken to ensure these augmentations preserve the original labels and do not
    inadvertently alter vital information in the data. We discussed other methods,
    such as two-phase learning and dynamic sampling, as potential strategies to improve
    model performance on imbalanced data. We also learned about some data-level techniques
    applicable to text, such as back translation and EDA, while running them on a
    spam/ham dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at some algorithm-based methods to deal with
    imbalanced datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apply Mixup interpolation to the Kaggle spam detection NLP dataset used in
    the chapter. See if Mixup helps to improve the model performance. You can refer
    to the paper *Augmenting Data with Mixup for Sentence Classification: An Empirical
    Study* by Guo et al. ([https://arxiv.org/pdf/1905.08941.pdf](https://arxiv.org/pdf/1905.08941.pdf))
    for further reading.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Refer to the FMix paper [21] and implement the FMix augmentation technique.
    Apply it to the Caltech101 dataset. See whether model performance improves by
    using FMix over the baseline model performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the EOS technique described in the chapter to the CIFAR-10-LT (the long-tailed
    version of CIFAR-10) dataset, and see whether the model performance improves for
    the most imbalanced classes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the MDSA techniques we studied in this chapter to the CIFAR-10-LT dataset,
    and see whether the model performance improves for the most imbalanced classes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Samira Pouyanfar, Yudong Tao, Anup Mohan, Haiman Tian, Ahmed S. Kaseb, Kent
    Gauen, Ryan Dailey, Sarah Aghajanzadeh, Yung-Hsiang Lu, Shu-Ching Chen, and Mei-Ling
    Shyu. 2018\. *Dynamic Sampling in Convolutional Neural Networks for Imbalanced
    Data Classification*. In 2018 IEEE Conference on Multimedia Information Processing
    and Retrieval (MIPR), pages 112–117, Miami, FL, April. IEEE.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'LeNet-5 paper, *Gradient-based learning applied to document* *classification*:
    [http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'AlexNet paper, *ImageNet Classification with Deep Convolutional Neural* *Networks*:
    [https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html](https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Leveraging Real-Time User Actions to Personalize Etsy Ads* (2023): [https://www.etsy.com/codeascraft/leveraging-real-time-user-actions-to-personalize-etsy-ads](https://www.etsy.com/codeascraft/leveraging-real-time-user-actions-to-personalize-etsy-ads).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Automated image tagging at [Booking.com](http://Booking.com) (2017): [https://booking.ai/automated-image-tagging-at-booking-com-7704f27dcc8b](https://booking.ai/automated-image-tagging-at-booking-com-7704f27dcc8b).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Shot Angle Prediction: Estimating Pose Angle with Deep Learning for Furniture
    Items Using Images Generated from 3D Models (**2020)*: [https://www.aboutwayfair.com/tech-innovation/shot-angle-prediction-estimating-pose-angle-with-deep-learning-for-furniture-items-using-images-generated-from-3d-models](https://www.aboutwayfair.com/tech-innovation/shot-angle-prediction-estimating-pose-angle-with-deep-learning-for-furniture-items-using-images-generated-from-3d-models).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'S. Yun, D. Han, S. Chun, S. J. Oh, Y. Yoo, and J. Choe, “*CutMix: Regularization
    Strategy to Train Strong Classifiers With Localizable Features*,” in 2019 IEEE/CVF
    International Conference on Computer Vision (ICCV), Seoul, Korea (South): IEEE,
    Oct. 2019, pp. 6022–6031\. doi: 10.1109/ICCV.2019.00612.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, “*mixup: Beyond Empirical
    Risk Minimization*.” arXiv, Apr. 27, 2018\. Accessed: Feb. 11, 2023\. [Online].
    Available: [http://arxiv.org/abs/1710.09412](http://arxiv.org/abs/1710.09412).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: R. Geirhos, C. R. M. Temme, J. Rauber, H. H. Schütt, M. Bethge, and F. A. Wichmann,
    “*Generalisation in humans and deep* *neural networks*.”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'D. Hendrycks, N. Mu, E. D. Cubuk, B. Zoph, J. Gilmer, and B. Lakshminarayanan,
    “*AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty*.”
    arXiv, Feb. 17, 2020\. Accessed: Aug. 01, 2023\. [Online]. Available: [http://arxiv.org/abs/1912.02781](http://arxiv.org/abs/1912.02781).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'H.-P. Chou, S.-C. Chang, J.-Y. Pan, W. Wei, and D.-C. Juan, “*Remix: Rebalanced
    Mixup*.” arXiv, Nov. 19, 2020\. Accessed: Aug. 15, 2023\. [Online]. Available:
    [http://arxiv.org/abs/2007.03943](http://arxiv.org/abs/2007.03943).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Protecting Personal Data in Grab’s Imagery* (2021): [https://engineering.grab.com/protecting-personal-data-in-grabs-imagery](https://engineering.grab.com/protecting-personal-data-in-grabs-imagery).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'M. Bayer, M.-A. Kaufhold, and C. Reuter, “*A Survey on Data Augmentation for
    Text Classification*,” ACM Comput. Surv., vol. 55, no. 7, pp. 1–39, Jul. 2023,
    doi: 10.1145/3544558.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Improving the accuracy of our machine learning WAF using data augmentation
    and sampling* (2022), Vikram Grover: [https://blog.cloudflare.com/data-generation-and-sampling-strategies/](https://blog.cloudflare.com/data-generation-and-sampling-strategies/).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Data augmentation for* *NLP*: [https://github.com/makcedward/nlpaug](https://github.com/makcedward/nlpaug).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'B. Kang *et al.*, “*Decoupling Representation and Classifier for Long-Tailed
    Recognition*.” arXiv, Feb. 19, 2020\. Accessed: Dec. 15, 2022\. [Online]. Available:
    [http://arxiv.org/abs/1910.09217](http://arxiv.org/abs/1910.09217).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'K. Cao, C. Wei, A. Gaidon, N. Arechiga, and T. Ma, “*Learning Imbalanced Datasets
    with Label-Distribution-Aware Margin Loss*”, [Online]. Available: [https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'D. Dablain, C. Bellinger, B. Krawczyk, and N. Chawla, “*Efficient Augmentation
    for Imbalanced Deep Learning*.” arXiv, Oct. 17, 2022\. Accessed: Jul. 23, 2023\.
    [Online]. Available: [http://arxiv.org/abs/2207.06080](http://arxiv.org/abs/2207.06080).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'R. He *et al.*, “*Is synthetic data from generative models ready for image
    recognition?*” arXiv, Feb. 15, 2023\. Accessed: Aug. 06, 2023\. [Online]. Available:
    [http://arxiv.org/abs/2210.07574](http://arxiv.org/abs/2210.07574).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'D. Dablain, B. Krawczyk, and N. V. Chawla, “*DeepSMOTE: Fusing Deep Learning
    and SMOTE for Imbalanced Data*,” IEEE Transactions on Neural Networks and Learning
    Systems, pp. 1–15, 2022, doi: 10.1109/TNNLS.2021.3136503.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'E. Harris, A. Marcu, M. Painter, M. Niranjan, A. Prügel-Bennett, and J. Hare,
    “*FMix: Enhancing Mixed Sample Data Augmentation*.” arXiv, Feb. 28, 2021\. Accessed:
    Aug. 08, 2023\. [Online]. Available: [http://arxiv.org/abs/2002.12047](http://arxiv.org/abs/2002.12047).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
