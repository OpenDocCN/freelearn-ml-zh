- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Data-Level Deep Learning Methods
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据级别深度学习方法
- en: You learned about various sampling methods in the previous chapters. Collectively,
    we call these methods *data-level methods* in this book. These methods include
    random undersampling, random oversampling, NearMiss, and SMOTE. We also explored
    how these methods work with classical machine learning algorithms.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，你已经了解了各种采样方法。在这本书中，我们将这些方法统称为*数据级别方法*。这些方法包括随机欠采样、随机过采样、NearMiss 和 SMOTE。我们还探讨了这些方法如何与经典机器学习算法协同工作。
- en: In this chapter, we’ll explore how to apply familiar sampling methods to deep
    learning models. Deep learning offers unique opportunities to enhance these methods
    further. We’ll delve into elegant techniques to combine deep learning with oversampling
    and undersampling. Additionally, we’ll learn how to implement various sampling
    methods with a basic neural network. We’ll also cover dynamic sampling, which
    involves adjusting the data sample across multiple training iterations, using
    varying balancing ratios for each iteration. Then, we will learn to use some data
    augmentation techniques for both images and text. We’ll end the chapter by highlighting
    key takeaways from a variety of other data-level techniques.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨如何将熟悉的采样方法应用于深度学习模型。深度学习为这些方法提供了进一步改进的独特机会。我们将深入研究将深度学习与过采样和欠采样相结合的优雅技术。此外，我们还将学习如何使用基本神经网络实现各种采样方法。我们还将介绍动态采样，这涉及到在多个训练迭代中调整数据样本，并为每个迭代使用不同的平衡比率。然后，我们将学习使用一些数据增强技术来处理图像和文本。我们将通过强调来自各种其他数据级别技术的关键要点来结束本章。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Preparing data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备数据
- en: Sampling techniques for deep learning models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习模型的采样技术
- en: Data-level techniques for text classification
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分类的数据级别技术
- en: A discussion of other data-level deep learning methods and their key ideas
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于其他数据级别深度学习方法和其关键思想的讨论
- en: It is not always straightforward to port methods to handle data imbalance, which
    worked well on classical ML models, into the deep learning world. The challenges
    and opportunities of deep learning models differ from the classical ML models
    primarily because of the difference in the type of data these models have to deal
    with. While classical ML models mostly deal with tabular and structured data,
    deep learning models typically deal with unstructured data, such as images, text,
    audio, and video, which is fundamentally different from tabular data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 将处理数据不平衡的方法从在经典机器学习模型上表现良好的方法迁移到深度学习领域并不总是直接的。深度学习模型与经典机器学习模型的主要挑战和机遇不同，主要是因为这些模型必须处理的数据类型不同。虽然经典机器学习模型主要处理表格和结构化数据，但深度学习模型通常处理非结构化数据，如图像、文本、音频和视频，这与表格数据有根本性的不同。
- en: We will discuss various techniques to deal with imbalance problems in computer
    vision. In the first part of the chapter, we will focus on various techniques,
    such as sampling and data augmentation, to handle class imbalance when training
    convolutional neural networks on image and text data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论各种处理计算机视觉中不平衡问题的技术。在本章的第一部分，我们将重点关注各种技术，例如采样和数据增强，以处理在图像和文本数据上训练卷积神经网络时的类别不平衡问题。
- en: In the latter half of the chapter, we will discuss common data-level techniques
    that can be applied to text problems. A lot of computer vision techniques can
    be successfully applied to NLP problems, too.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后半部分，我们将讨论可以应用于文本问题的常见数据级别技术。许多计算机视觉技术也可以成功地应用于自然语言处理问题。
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Similar to prior chapters, we will continue to utilize common libraries such
    as `torch`, `torchvision`, `numpy`, and `scikit-learn`. We will also use `nlpaug`
    for NLP-related functionalities. The code and notebooks for this chapter are available
    on GitHub at [https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter07](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter07).
    You can open the GitHub notebooks using Google Colab by clicking on the **Open
    in Colab** icon at the top of the chapter’s notebook, or by launching it from
    [https://colab.research.google.com](https://colab.research.google.com), using
    the GitHub URL of the notebook.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章类似，我们将继续使用常见的库，如`torch`、`torchvision`、`numpy`和`scikit-learn`。我们还将使用`nlpaug`来实现NLP相关的功能。本章的代码和笔记本可在GitHub上找到，网址为[https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter07](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter07)。您可以通过点击章节笔记本顶部的**在Colab中打开**图标或通过从[https://colab.research.google.com](https://colab.research.google.com)启动它，使用笔记本的GitHub
    URL来打开GitHub笔记本。
- en: Preparing the data
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据
- en: In this chapter, we are going to use the classic MNIST dataset. This dataset
    contains 28-pixel x 28-pixel images of handwritten digits. The task for the model
    is to take an image as input and identify the digit in the image. We will use
    `PyTorch`, a popular deep-learning library, to demonstrate the algorithms. Let’s
    prepare the data now.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用经典的MNIST数据集。该数据集包含28像素x 28像素的手写数字图像。对于模型的任务是接收一个图像作为输入并识别图像中的数字。我们将使用流行的深度学习库`PyTorch`来展示算法。现在让我们准备数据。
- en: The first step in the process will be to import the libraries. We will need
    NumPy (as we deal with `numpy` arrays), `torchvision` (to load MNIST data), `torch`,
    `random`, and `copy` libraries.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程的第一步将是导入库。我们需要NumPy（因为我们处理`numpy`数组）、`torchvision`（用于加载MNIST数据）、`torch`、`random`和`copy`库。
- en: 'Next, we can download the MNIST data from `torchvision.datasets`. The `torchvision`
    library is a part of the `PyTorch` framework, which contains datasets, models,
    and common image transformers for computer vision tasks. The following code will
    download the MNIST dataset from this library:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以从`torchvision.datasets`下载MNIST数据。`torchvision`库是`PyTorch`框架的一部分，它包含用于计算机视觉任务的数据库、模型和常见图像转换器。以下代码将从该库下载MNIST数据集：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Once the data is downloaded from `torchvision`, we can load it into the `Dataloader`
    utility of `PyTorch`, which creates batches of data and provides us with a Python-style
    iterator over the batches. The following code does exactly that. Here, we are
    creating batches of size 64 for `train_loader`.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦从`torchvision`下载了数据，我们就可以将其加载到`PyTorch`的`Dataloader`实用工具中，它创建数据批次并提供对批次的Python风格迭代器。以下代码正是这样做的。在这里，我们为`train_loader`创建大小为64的批次。
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Since we are interested in imbalanced datasets, we will convert our MNIST dataset,
    which is a balanced dataset, into a long-tailed version of itself by deleting
    examples from various classes. We are omitting that implementation here to save
    space; you can refer to the chapter’s GitHub repository for details. We assume
    that you have the `imbalanced_train_loader` class created from the imbalanced
    trainset.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们对不平衡数据集感兴趣，我们将把我们的MNIST数据集，一个平衡数据集，通过删除各个类别的示例转换为它的长尾版本。我们在这里省略了该实现以节省空间；您可以参考章节的GitHub仓库以获取详细信息。我们假设您已经创建了由不平衡训练集生成的`imbalanced_train_loader`类。
- en: '*Figure 7**.1* shows the distribution of samples in the imbalanced MNIST dataset:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.1*显示了不平衡MNIST数据集中样本的分布：'
- en: '![](img/B17259_07_01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_07_01.jpg)'
- en: Figure 7.1 – A bar chart of the counts of examples from each digit class
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 – 每个数字类示例计数的条形图
- en: Next, we will learn to create a training loop in `PyTorch`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习如何在`PyTorch`中创建训练循环。
- en: Creating the training loop
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建训练循环
- en: Before creating the training loop, we should import the `torch.nn` and `torch.optim`
    packages. The `torch.nn` package provides all the building blocks to create a
    neural network graph, while the `torch.optim` package provides us with most of
    the common optimization algorithms.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建训练循环之前，我们应该导入`torch.nn`和`torch.optim`包。`torch.nn`包提供了创建神经网络图的全部构建块，而`torch.optim`包为我们提供了大多数常见的优化算法。
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Since we will need some hyperparameters, let’s define them:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们需要一些超参数，让我们定义它们：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: After setting up the hyperparameters, we can define the `train` function, which
    will take PyTorch’s data loader as input and return a model fitted to the data.
    To create a trained model, we will need a model, a loss criterion, and an optimizer.
    We will use a single-layer linear neural network as the model here. You can design
    your own neural network architecture based on your requirements. For the loss
    criterion, we will use `CrossEntropyLoss`, and we will use **Stochastic Gradient
    Descent** (**SGD**) as the optimizer.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置好超参数之后，我们可以定义`train`函数，该函数将接受PyTorch的数据加载器作为输入，并返回一个拟合数据的模型。为了创建一个训练好的模型，我们需要一个模型、一个损失准则和一个优化器。在这里，我们将使用单层线性神经网络作为模型。你可以根据你的需求设计自己的神经网络架构。对于损失准则，我们将使用`CrossEntropyLoss`，并且我们将使用**随机梯度下降**（**SGD**）作为优化器。
- en: 'We will train the model for `num_epochs` epochs. We will discuss how the model
    is trained during a single epoch in the next paragraph. For now, we will abstract
    that part out in the `run_epoch` function:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练模型`num_epochs`个epochs。我们将在下一段讨论模型在单个epoch中的训练过程。现在，我们将在`run_epoch`函数中抽象出这部分：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'During every epoch, we will train our model over the whole training data once.
    As discussed earlier, dataloader divides the data into multiple batches. First,
    we will have to match the shape of the images in the batch with the input dimension
    of our model. We will take the current batch and do a forward pass over the model,
    calculating the predictions and loss over the predictions in one go. Then, we
    will backpropagate the loss to update the model weights:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个epoch期间，我们将对整个训练数据进行一次模型训练。如前所述，数据加载器将数据分成多个批次。首先，我们必须将批次中图像的形状与模型的输入维度相匹配。我们将对当前批次进行前向传递，一次计算预测和预测的损失。然后，我们将损失反向传播以更新模型权重：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To get a trained model, we can now send the data loader to the `train` function:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得一个训练好的模型，我们现在可以将数据加载器发送到`train`函数：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'For all image-related methods in this chapter, we’ll employ the model code
    detailed next. We will create a `PyTorch` neural network named `Net` that features
    two convolutional layers, a dropout mechanism, and a pair of fully connected layers.
    Through the `forward` function, the model seamlessly integrates these layers using
    `ReLU` activations and max-pooling, manipulating the input, `x`. The result is
    `log_softmax` of the computed output:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的所有与图像相关的方法中，我们将使用下面详细说明的模型代码。我们将创建一个名为`Net`的`PyTorch`神经网络，它具有两个卷积层、一个dropout机制和一对全连接层。通过`forward`函数，模型使用`ReLU`激活和最大池化无缝集成这些层，操作输入`x`。结果是计算输出的`log_softmax`：
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, let’s break down some of these terms:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们分解一些这些术语：
- en: '`Net` class, there are two such layers – `conv1` and `conv2`. The numbers `(1,
    10)` and `(10, 20)` are simply the input and output channels. The term `kernel_size=5`
    means that a 5 x 5 grid (or filter) is used to scan the input.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`Net`类中，有两个这样的层——`conv1`和`conv2`。数字`(1, 10)`和`(10, 20)`仅仅是输入和输出通道。术语`kernel_size=5`意味着使用一个5
    x 5的网格（或过滤器）来扫描输入。
- en: '`conv2_drop` is a dropout layer of type `Dropout2d`, specifically designed
    for 2D data (such as images).'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv2_drop`是一个类型为`Dropout2d`的dropout层，专门设计用于2D数据（如图像）。'
- en: '`fc1` and `fc2`, which further process the patterns recognized by the convolutional
    layers to make predictions.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fc1`和`fc2`，它们进一步处理卷积层识别出的模式，以进行预测。'
- en: '`F.relu` is an activation function that introduces non-linearity to the model,
    enabling it to learn complex patterns'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`F.relu`是一个激活函数，它向模型引入非线性，使其能够学习复杂的模式'
- en: '`F.max_pool2d` is a pooling function that reduces the spatial dimensions of
    the data while retaining important features'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`F.max_pool2d`是一个池化函数，它在保留重要特征的同时减少了数据的空间维度'
- en: Finally, `F.log_softmax` is an activation function commonly used for classification
    tasks to produce probabilities for each class
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，`F.log_softmax`是一个常用于分类任务的激活函数，用于为每个类别生成概率。
- en: In essence, the `Net` class defines a neural network that first detects patterns
    in data using convolutional layers, reduces overfitting using dropout, and then
    makes predictions using fully connected layers. The forward method is a sequence
    of operations that define how data flows through this network.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，`Net`类定义了一个神经网络，它首先使用卷积层在数据中检测模式，使用dropout减少过拟合，然后使用全连接层进行预测。前向方法是一系列操作，定义了数据如何通过这个网络流动。
- en: In the next section, we will learn how to use the `train` function with oversampling
    methods.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何使用oversampling方法与`train`函数一起使用。
- en: Sampling techniques for deep learning models
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习模型的采样技术
- en: In this section, we’ll explore some sampling methods, such as random oversampling
    and weighted sampling, for deep learning models. We’ll then transition into data
    augmentation techniques, which bolster model robustness and mitigate dataset limitations.
    While large datasets are ideal for deep learning, real-world constraints often
    make them hard to obtain. We will also look at some advanced augmentations, such
    as CutMix and MixUp. We’ll start with standard methods before discussing these
    advanced techniques.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨一些采样方法，例如随机过采样和加权采样，用于深度学习模型。然后我们将过渡到数据增强技术，这些技术可以增强模型鲁棒性并减轻数据集的限制。虽然大型数据集对于深度学习来说是理想的，但现实世界的限制往往使得它们难以获得。我们还将探讨一些高级增强技术，如CutMix和MixUp。我们将从标准方法开始，然后讨论这些高级技术。
- en: Random oversampling
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机过采样
- en: Here, we will apply the plain old random oversampling we learned in [*Chapter
    2*](B17259_02.xhtml#_idTextAnchor042), *Oversampling Methods*, but using image
    data as input to a neural network. The basic idea is to duplicate samples from
    the minority classes randomly until we end up with an equal number of samples
    from each class. This technique often performs better than no sampling.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将应用我们在[*第2章*](B17259_02.xhtml#_idTextAnchor042)，“过采样方法”中学习的简单随机过采样，但使用图像数据作为神经网络的输入。基本思想是随机复制少数类的样本，直到每个类别的样本数量相等。这种技术通常比不采样表现得更好。
- en: Tip
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: Make sure to train the model for enough epochs so that it has fully been fitted
    to the data. Under-training will likely lead to suboptimal model performance.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 确保训练模型足够多的轮次，以便它已经完全拟合到数据上。欠拟合可能会导致模型性能不佳。
- en: 'Let’s spend some time working with the code. There are a few simple steps we
    need to follow. First, we need to convert data from the data loaders into tensors.
    Our `RandomOverSampler` API from `imbalanced-learn` doesn’t work directly with
    data loaders:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花些时间来处理代码。我们需要遵循几个简单的步骤。首先，我们需要将数据加载器中的数据转换为张量。我们的`imbalanced-learn`中的`RandomOverSampler`
    API不能直接与数据加载器一起使用：
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We also need to reshape the `X` tensor for `RandomOverSampler` to work with
    two-dimensional inputs, as each of our images is a 28 x 28 matrix:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要重塑`X`张量，以便`RandomOverSampler`可以处理二维输入，因为我们的每个图像都是一个28 x 28的矩阵：
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, we can import the `RandomOverSampler` class from the `imbalanced-learn`
    library, define an `oversampler` object, and resample our data using it:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以从`imbalanced-learn`库中导入`RandomOverSampler`类，定义一个`oversampler`对象，并使用它重新采样我们的数据：
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'After resampling the data, we need to reshape it again back to the original
    form:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在重新采样数据后，我们需要将其再次重塑回原始形式：
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can now create a new data loader using the oversampled data:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用过采样数据创建一个新的数据加载器：
- en: '[PRE12]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, we can train our model using the new data loader. For this step, we
    can use the `train` function defined in the previous section:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用新的数据加载器来训练我们的模型。对于这一步，我们可以使用上一节中定义的`train`函数：
- en: '[PRE13]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: That’s all we need to do to use the random oversampling technique with deep
    learning models.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们使用深度学习模型中的随机过采样技术所需做的所有事情。
- en: A similar strategy can be used for `RandomUnderSampling` from the `imbalanced-learn`
    library.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用类似的策略从`imbalanced-learn`库中的`RandomUnderSampling`进行采样。
- en: '`PyTorch` provides a `WeightedRandomSampler` API, which is similar to the `sample_weight`
    parameter from `scikit-learn` (found in many of the fit methods in `scikit-learn`
    estimators (such as `RandomForestClassifier` and `LogisticRegression`) and serves
    a similar purpose of assigning a weight to each sample of the training dataset.
    We had a detailed discussion of the differences between `class_weight` and `sample_weight`
    in [*Chapter 5*](B17259_05.xhtml#_idTextAnchor151), *Cost-Sensitive Learning*.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`PyTorch`提供了一个`WeightedRandomSampler` API，它与`scikit-learn`中的`sample_weight`参数类似（在`scikit-learn`估计器的许多fit方法中找到，例如`RandomForestClassifier`和`LogisticRegression`），具有为训练数据集中的每个样本分配权重的类似目的。我们在[*第5章*](B17259_05.xhtml#_idTextAnchor151)，“成本敏感学习”中对`class_weight`和`sample_weight`之间的差异进行了详细讨论。'
- en: 'We can specify `weights` as a parameter to `WeightedRamdomSampler` so that
    it can automatically weigh the examples in the batch, according to the weight
    of each sample. The `weights` parameter values are typically the inverse of the
    frequency of various classes in the dataset:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将`weights`参数指定给`WeightedRamdomSampler`，以便它可以根据每个样本的权重自动对批次的示例进行加权。`weights`参数的值通常是数据集中各种类别的频率的倒数：
- en: '[PRE14]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`class_weights` is more for the minority class labels than the majority class
    labels. Let’s compute the `weightedRamdomSampler` values:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`class_weights` 对于少数类标签比多数类标签更重要。让我们计算`weightedRamdomSampler`的值：'
- en: '[PRE15]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the next section, we will learn how to sample data dynamically.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何动态采样数据。
- en: Dynamic sampling
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动态采样
- en: Dynamic sampling [1] is an advanced technique that can self-adjust the sampling
    rate as training progresses. It promises to adapt according to the problem’s complexity
    and class imbalance, with almost no hyperparameter tuning. It is just one more
    tool in your arsenal to try on your dataset, especially when you have imbalanced
    image data at hand, and see whether it gives a better performance than the other
    techniques we’ve discussed so far in this chapter.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 动态采样 [1] 是一种高级技术，可以在训练过程中自我调整采样率。它承诺根据问题的复杂性和类别不平衡进行适应，几乎不需要调整超参数。它是你武器库中另一个可以尝试的工具，尤其是在你手头有不平衡的图像数据时，看看它是否比我们在本章中讨论的其他技术给出更好的性能。
- en: The basic idea of dynamic sampling is to dynamically adjust the sampling rate
    for various classes, depending on whether they are doing well or worse in a particular
    training iteration when compared to the prior iteration. If a class is performing
    comparatively poorly, then the class is oversampled in the next iteration, and
    vice versa.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 动态采样的基本思想是根据它们在特定训练迭代中与先前迭代相比的表现好坏，动态调整各种类别的采样率。如果一个类别的表现相对较差，那么在下一个迭代中该类别将被过度采样，反之亦然。
- en: The details of the algorithm
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 算法的细节
- en: 'These are the core components of dynamic sampling:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是动态采样的核心组件：
- en: '**Real-time data augmentation**: Apply various kinds of image transformations
    to the images of each training batch. These transformations can be rotation, flipping,
    adjusting brightness, translation, adjusting contrast/color, noise injection,
    and so on. As discussed earlier, this step helps to reduce model overfitting and
    improves generalization.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时数据增强**：将各种图像变换应用于每个训练批次的图像。这些变换可以是旋转、翻转、调整亮度、平移、调整对比度/颜色、噪声注入等。如前所述，这一步骤有助于减少模型过拟合并提高泛化能力。'
- en: '**Dynamic sampling method**: In each iteration, a sample size (given by a certain
    formula) is chosen, and a model is trained with that sample size. The classes
    with lower F1 scores are sampled at a higher rate in the next iteration, forcing
    the model to focus more on previously misclassified examples. The number of images,
    c j, for the next iteration is updated according to the following formula:'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动态采样方法**：在每次迭代中，选择一个样本大小（由某个公式给出），并使用该样本大小训练一个模型。在下一个迭代中，具有较低F1分数的类别将以更高的比率进行采样，迫使模型更多地关注先前错误分类的示例。下一个迭代中图像的数量，c j，根据以下公式更新：'
- en: UpdateSampleSize( F1 i, c j) =  1 − f1 i, j  _ ∑ c k∈ C 1 − f1 i,k  × N
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: UpdateSampleSize( F1 i, c j) =  1 − f1 i, j  _ ∑ c k∈ C 1 − f1 i,k  × N
- en: 'Here:'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里：
- en: f1 i, j is the F1-score of class c j in iteration i
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: f1 i, j 是第i次迭代中类c j的F1分数
- en: N = the average number of samples in all classes
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: N = 所有类别中样本数量的平均值
- en: 'For a particular training epoch, let’s say we got the following F1 score for
    each of the three classes, **A**, **B**, and **C**, on our validation dataset:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特定的训练周期，假设我们在验证数据集上对三个类别，**A**、**B**和**C**，得到了以下F1分数：
- en: '| **Class** | **F1 score** |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| **类别** | **F1分数** |'
- en: '| A | 0.1 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| A | 0.1 |'
- en: '| B | 0.2 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| B | 0.2 |'
- en: '| C | 0.3 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| C | 0.3 |'
- en: Table 7.1 – Sample F1-scores of each class after some epoch
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.1 – 某个周期后每个类别的样本F1分数
- en: 'Here is how we would compute the weight of each class for the next epoch of
    training:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们如何计算每个类别的权重以用于下一个训练周期的计算方法：
- en: Weight (class A) =  N * (1 − f1 a)  ______________________  (1 − f1 a)+ (1 −
    f1 b)+ (1 − f1 c)  =  N * 0.9  _ 0.9 + 0.8 + 0.7
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 权重（类A）=  N * (1 − f1 a)  ______________________  (1 − f1 a)+ (1 − f1 b)+ (1
    − f1 c)  =  N * 0.9  _ 0.9 + 0.8 + 0.7
- en: = N * 0.375
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: = N * 0.375
- en: '*Weight (class B) = N* 0.8/2.4 =* *N*0.33*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*（类B的）权重 = N* 0.8/2.4 =* *N*0.33*'
- en: '*Weight (class C) = N* 0.7/2.4 =* *N* 0.29*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*（类C的）权重 = N* 0.7/2.4 =* *N* 0.29*'
- en: This means that we will sample class A at a higher rate than class B and class
    C, which makes sense because the performance on class A was weaker than that on
    classes B and C.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们将以比类B和类C更高的比率采样类A，这是有意义的，因为类A的性能比类B和类C弱。
- en: A second model is trained through transfer learning, without sampling, to prevent
    the minority classes from overfitting. At inference time, the model output is
    a function of both models.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 通过迁移学习训练第二个模型，不进行采样，以防止少数类过拟合。在推理时间，模型的输出是两个模型输出的函数。
- en: There are additional details about the **DynamicSampling** algorithm that we
    have omitted here due to space constraints. You can find the complete implementation
    code in the corresponding GitHub repository for this chapter.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 由于篇幅限制，我们在此省略了关于**动态采样**算法的更多细节。您可以在本章对应的GitHub仓库中找到完整的实现代码。
- en: '![](img/B17259_07_02.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_07_02.jpg)'
- en: Figure 7.2 – An overall model accuracy comparison of various sampling techniques
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 – 使用各种采样技术的整体模型准确率比较
- en: '*Table 7.2* shows the per-class model accuracy using various sampling techniques,
    including the baseline, where no sampling is done.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*表7.2*显示了使用各种采样技术（包括基线，其中不进行采样）的每类模型准确率。'
- en: '| **Class** | **Baseline** | **Weighted** **random sampler** | **Dynamic**
    **sampler** | **Random** **oversampling** | **Random** **undersampling** |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| **类别** | **基线** | **加权随机采样器** | **动态采样器** | **随机过采样** | **随机欠采样** |'
- en: '| 0 | **99.9** | 99.0 | 92.4 | 99.1 | 97.2 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 0 | **99.9** | 99.0 | 92.4 | 99.1 | 97.2 |'
- en: '| 1 | **99.7** | 99.2 | 96.8 | 99.2 | 90.7 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 1 | **99.7** | 99.2 | 96.8 | 99.2 | 90.7 |'
- en: '| 2 | **98.5** | 98.3 | 93.5 | **98.5** | 70.8 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 2 | **98.5** | 98.3 | 93.5 | **98.5** | 70.8 |'
- en: '| 3 | 97.3 | 97.4 | 96.8 | **98.3** | 74.4 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 97.3 | 97.4 | 96.8 | **98.3** | 74.4 |'
- en: '| 4 | 98.3 | 98.0 | 91.9 | **98.6** | 79.6 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 98.3 | 98.0 | 91.9 | **98.6** | 79.6 |'
- en: '| 5 | 96.2 | 96.0 | 97.3 | **98.1** | 52.8 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 96.2 | 96.0 | 97.3 | **98.1** | 52.8 |'
- en: '| 6 | 94.5 | 97.6 | **98.7** | 97.3 | 77.6 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 94.5 | 97.6 | **98.7** | 97.3 | 77.6 |'
- en: '| 7 | 89.7 | 94.7 | **96.5** | 94.1 | 81.1 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 89.7 | 94.7 | **96.5** | 94.1 | 81.1 |'
- en: '| 8 | 63.3 | 91.5 | **96.9** | 93.0 | 60.2 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 63.3 | 91.5 | **96.9** | 93.0 | 60.2 |'
- en: '| 9 | 50.7 | 92.6 | **97.7** | 91.8 | 56.5 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 50.7 | 92.6 | **97.7** | 91.8 | 56.5 |'
- en: Table 7.2 – A per-class model accuracy comparison of various sampling techniques
    (the highest value for a class is in bold)
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.2 – 使用各种采样技术的每类模型准确率比较（类别的最高值用粗体表示）
- en: 'Here are some insights from the results:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一些从结果中得到的见解：
- en: In terms of overall performance, **Random OverSampling** (**ROS**) performed
    the best, while **Random Undersampling** (**RUS**) did the worst.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在整体性能方面，**随机过采样**（**ROS**）表现最佳，而**随机欠采样**（**RUS**）表现最差。
- en: Although ROS did the best, it can be computationally very expensive due to data
    cloning, making it less suitable for large datasets and industrial settings.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然ROS表现最佳，但由于数据克隆，它可能计算成本非常高，这使得它不太适合大型数据集和工业环境。
- en: Dynamic sampling did a little worse than ROS; it did best on the minority classes
    6–9 and would be our preferred choice here. However, due to its increased complexity,
    our second choice will be the weighted random sampler.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态采样比ROS略差；它在少数类别6-9上表现最佳，将是我们的首选选择。然而，由于其复杂性增加，我们的第二选择将是加权随机采样器。
- en: The baseline and weighted random sampler techniques are stable across classes;
    RUS is notably variable and performs poorly on most classes.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基线和加权随机采样器技术在类别间表现稳定；RUS在大多数类别上表现明显不稳定且表现不佳。
- en: Similarly, we can apply SMOTE in the same way as `RandomOverSampler`. Please
    note that while SMOTE can be applied to images, its use of a linear subspace of
    the original data is often limiting.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以像使用`RandomOverSampler`一样应用SMOTE。请注意，虽然SMOTE可以应用于图像，但其对原始数据线性子空间的利用通常受到限制。
- en: This ends our discussion of the various sampling techniques. In the next section,
    we will focus on data augmentation techniques specifically designed for images
    to achieve more effective oversampling.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们对各种采样技术的讨论。在下一节中，我们将专注于专门为图像设计的用于实现更有效过采样的数据增强技术。
- en: Data augmentation techniques for vision
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视觉数据增强技术
- en: Today, a variety of custom augmentation techniques are used for various kinds
    of data, such as images, audio, video, and even text data. In the vision realm,
    this includes techniques such as rotating, scaling, cropping, blurring, adding
    noise to an image, and a host of other techniques, including combining those techniques
    all at once in some appropriate sequence. Image data augmentation is not really
    a recent innovation. Some image augmentation techniques can also be found in the
    LeNet-5 model paper [2] from 1998, for example. Similarly, the AlexNet model [3]
    from 2012 uses random cropping, flipping, changing the color intensity of RGB
    channels, and so on to reduce errors during model training.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，各种定制的增强技术被用于各种类型的数据，如图像、音频、视频，甚至文本数据。在视觉领域，这包括旋转、缩放、裁剪、模糊、向图像添加噪声以及其他许多技术，包括将这些技术一次性以适当的顺序组合在一起。图像数据增强并不是一项最近才出现的创新。例如，一些图像增强技术也可以在1998年的LeNet-5模型论文[2]中找到。同样，2012年的AlexNet模型[3]使用随机裁剪、翻转、改变RGB通道的颜色强度等方法来减少模型训练过程中的错误。
- en: 'Let’s discuss why data augmentation can often be helpful:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一下为什么数据增强通常很有帮助：
- en: In problems where we have limited data or imbalanced data, it may not always
    be possible to gather more data. This could be because either gathering more data
    is difficult in the first place (for example, waiting for more fraud to occur
    when dealing with credit card fraud, or gathering satellite images where we have
    to pay satellite operators, which can be quite expensive) or labeling the data
    is difficult or expensive (for example, to label medical image datasets, we need
    domain experts).
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们数据有限或不平衡的问题中，并不总是能够收集到更多的数据。这可能是因为收集更多数据本身就很困难（例如，在处理信用卡欺诈时等待更多欺诈发生，或者收集卫星图像，我们必须支付卫星运营商的费用，这可能相当昂贵）或者标记数据很困难或昂贵（例如，为了标记医学图像数据集，我们需要领域专家）。
- en: Data augmentation can help reduce overfitting and improve the overall performance
    of a model. One motivation for this practice is that attributes such as lighting,
    noise, color, scale, and focus in the training set may not align with those in
    the real-world images on which we run inference. Additionally, augmentation diversifies
    a dataset to help the model generalize better. For example, if the model is trained
    only on images of cats facing right, it may not perform well on images where the
    cat faces left. Therefore, it’s advisable to always apply valid transformations
    to augment image datasets, as most models gain performance with more diverse data.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据增强可以帮助减少过拟合并提高模型的总体性能。这种做法的一个动机是，训练集中的属性如光照、噪声、颜色、比例和焦点可能与我们运行推理的真实世界图像中的属性不一致。此外，增强多样化数据集可以帮助模型更好地泛化。例如，如果模型仅在面向右的猫的图像上训练，它可能在面向左的猫的图像上表现不佳。因此，始终应用有效的变换来增强图像数据集是明智的，因为大多数模型在更多样化的数据上都能获得性能提升。
- en: Data augmentation is widely used for computer vision tasks such as object detection,
    classification, and segmentation. It can be very useful for NLP tasks as well.
    In the computer vision world, there are lots of open source libraries that help
    standardize the various image augmentation techniques, while the NLP tools for
    data augmentation space have yet to mature.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强在计算机视觉任务中如目标检测、分类和分割等方面被广泛使用。它对于自然语言处理任务也非常有用。在计算机视觉领域，有许多开源库帮助标准化各种图像增强技术，而数据增强在自然语言处理工具方面尚未成熟。
- en: While there are several popular open source libraries for image augmentation,
    such as `imgaug`, Facebook’s `AugLy`, and `Albumentations`, we will use `torchvision`
    in this book. As a part of the PyTorch ecosystem, it offers seamless integration
    with PyTorch workflows, a range of common image transformations, as well as pre-trained
    models and datasets, making it a convenient and comprehensive choice for computer
    vision tasks. If you need more advanced augmentations, or if speed is a concern,
    `Albumentations` may be a better choice.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有几个流行的开源库用于图像增强，如 `imgaug`、Facebook的 `AugLy` 和 `Albumentations`，但在这本书中我们将使用
    `torchvision`。作为PyTorch生态系统的一部分，它提供了与PyTorch工作流程的无缝集成、一系列常见的图像变换、以及预训练模型和数据集，使其成为计算机视觉任务的方便且全面的选项。如果您需要更高级的增强，或者如果速度是一个关注点，`Albumentations`
    可能是一个更好的选择。
- en: 'We can use `torchvision.transforms.Pad` to add some padding to the image boundaries:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `torchvision.transforms.Pad` 来向图像边界添加一些填充：
- en: '[PRE16]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](img/B17259_07_03.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_07_03.jpg)'
- en: Figure 7.3 – The results of applying the Pad function to the image
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 – 应用Pad函数到图像的结果
- en: 'The `torchvision.transforms.FiveCrop` class transforms and crops the given
    image into four corners and the central crop:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchvision.transforms.FiveCrop`类将给定的图像转换并裁剪成四个角落和中央部分：'
- en: '[PRE17]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](img/B17259_07_04.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_07_04.jpg)'
- en: Figure 7.4 – The results of applying the FiveCrop function on the image
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – 应用FiveCrop函数到图像的结果
- en: '`torchvision.transforms.CenterCrop` is a similar class to crop images from
    the center.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchvision.transforms.CenterCrop`是一个类似的类，用于从中心裁剪图像。'
- en: 'The `torchvision.transforms.ColorJitter` class changes the brightness, saturation,
    and other similar properties of the image:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchvision.transforms.ColorJitter`类会改变图像的亮度、饱和度和其他类似属性：'
- en: '[PRE18]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](img/B17259_07_05.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_07_05.jpg)'
- en: Figure 7.5 – The results of applying the ColorJitter function on the image
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 – 应用ColorJitter函数到图像的结果
- en: '`GaussianBlur` can add some blurring to the images:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`GaussianBlur`可以为图像添加一些模糊效果：'
- en: '[PRE19]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](img/B17259_07_06.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_07_06.jpg)'
- en: Figure 7.6 – The results of applying the GaussianBlur function on the image
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 – 应用高斯模糊函数到图像的结果
- en: 'The `torchvision.transforms.RandomRotation` class transform rotates an image
    at a random angle:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchvision.transforms.RandomRotation`类可以将图像随机旋转一个角度：'
- en: '[PRE20]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](img/B17259_07_07.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_07_07.jpg)'
- en: Figure 7.7 – The results of random rotation on the original image (leftmost)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 – 对原始图像（最左侧）进行随机旋转的结果
- en: Consider exploring the other image transformation functionalities supported
    by the `torchvision.transforms` class that we didn’t discuss here.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑探索`torchvision.transforms`类支持的其它图像变换功能，这些功能我们在这里没有讨论。
- en: Cutout masks out random square regions of input images during training. While
    it may seem like this technique removes unnecessary portions of the image, it’s
    important to note that the areas to be masked are typically selected at random.
    The primary aim is to force the neural network to generalize better by ensuring
    it does not overly rely on any specific set of pixels within a given image.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，Cutout会随机遮盖输入图像的随机正方形区域。虽然这种技术看起来像是移除了图像中不必要的部分，但重要的是要注意，要被遮盖的区域通常是随机选择的。主要目的是通过确保神经网络不过度依赖给定图像中的任何特定像素集，来强迫神经网络更好地泛化。
- en: '![](img/B17259_07_08.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_07_08.jpg)'
- en: Figure 7.8 – The result of applying the cutout function on an image
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 – 应用cutout函数到图像的结果
- en: 🚀 Deep learning data-level techniques in production at Etsy/Booking/Wayfair
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 🚀 Etsy/Booking/Wayfair在生产中应用深度学习数据级技术
- en: '**🎯** **Problem** **being solved:**'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**🎯** **问题解决：**'
- en: Etsy, [Booking.com](http://Booking.com), and Wayfair leveraged user behavior
    to enhance personalization. Etsy focused on item recommendations based on browsing
    history [4], [Booking.com](http://Booking.com) tailored search results to boost
    bookings [5], and Wayfair optimized product image angles to improve CTRs [6].
    All aimed to utilize data-driven strategies for better user experience and performance.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Etsy、[Booking.com](http://Booking.com)和Wayfair利用用户行为来增强个性化。Etsy专注于基于浏览历史的商品推荐[4]，[Booking.com](http://Booking.com)定制搜索结果以增加预订[5]，而Wayfair优化产品图像角度以提高点击率[6]。所有这些都是为了利用数据驱动策略，以改善用户体验和性能。
- en: '**⚖️** **Data** **imbalance issue:**'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**⚖️** **数据不平衡问题：**'
- en: Etsy, [Booking.com](http://Booking.com), and Wayfair each grappled with data
    imbalance issues in their machine learning projects. Etsy faced a power law distribution
    in user sessions, where most users interacted with only a few listings within
    a one-hour window. [Booking.com](http://Booking.com) dealt with imbalanced classes
    in hotel images, as photos of bedrooms and bathrooms vastly outnumbered those
    of other facilities such as saunas or table tennis. Wayfair encountered an imbalance
    in real-world images of furniture, with a majority of images showing the “front”
    angle, leading to poor performance for other angles. All three companies had to
    address these imbalances to improve the performance and fairness of their models.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Etsy、[Booking.com](http://Booking.com)和Wayfair在他们的机器学习项目中都遇到了数据不平衡问题。Etsy面临用户会话的幂律分布，其中大多数用户在一小时窗口内只与几个商品列表互动。[Booking.com](http://Booking.com)处理酒店图像中的不平衡类别，卧室和浴室的照片远远多于其他设施如桑拿或乒乓球的照片。Wayfair遇到了家具真实世界图像的不平衡，大多数图像显示“正面”角度，导致其他角度的性能不佳。这三家公司都必须解决这些不平衡问题，以提高模型性能和公平性。
- en: '**🎨** **Data** **augmentation strategy:**'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**🎨** **数据增强策略：**'
- en: Etsy, [Booking.com](http://Booking.com), and Wayfair each had unique data augmentation
    strategies to address their specific challenges. Etsy used image random rotation,
    translation, zoom, and color contrast transformation to augment their dataset.
    [Booking.com](http://Booking.com) employed a variety of techniques, including
    mirroring, random cropping, affine transformation, aspect ratio distortion, color
    manipulation, and contrast enhancement. They increased their labeled data by 10
    times through these methods, applying distortions on the fly during training.
    Wayfair took a different approach by creating synthetic data with 3D models, generating
    100 views for each 3D model of chairs and sofas, thus providing granular angle
    information for training.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Etsy、[Booking.com](http://Booking.com)和Wayfair各自采用了独特的数据增强策略来解决他们各自的挑战。Etsy使用了图像随机旋转、平移、缩放和颜色对比度变换来增强他们的数据集。[Booking.com](http://Booking.com)采用了包括镜像、随机裁剪、仿射变换、纵横比扭曲、颜色操作和对比度增强在内的各种技术。他们通过这些方法将标记数据增加了10倍，在训练过程中实时应用扭曲。Wayfair通过创建3D模型生成的合成数据采取了不同的方法，为椅子和沙发的每个3D模型生成100个视图，从而为训练提供了细粒度的角度信息。
- en: Next, let’s look at some of the more advanced techniques, such as CutMix, MixUp,
    and AugMix, which are types of **Mixed Sample Data Augmentation** (**MSDA**) techniques.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看一些更高级的技术，例如CutMix、MixUp和AugMix，这些都是**混合样本数据增强**（**MSDA**）技术的类型。
- en: MSDA is a set of techniques that involve mixing data samples to produce an augmented
    dataset, used to train a model (*Figure 7**.9*).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: MSDA是一组涉及混合数据样本以生成增强数据集的技术，用于训练模型（*图7**.9*）。
- en: '![](img/B17259_07_09.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_07_09.jpg)'
- en: Figure 7.9 – Common MSDA techniques
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 – 常见的MSDA技术
- en: CutMix
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CutMix
- en: 'CutMix [7] is an image data augmentation technique where patches are cut and
    pasted among training images. Specifically, a portion of an image is replaced
    by a portion of another image, as shown here:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: CutMix [7]是一种图像数据增强技术，其中在训练图像之间剪切和粘贴补丁。具体来说，图像的一部分被另一图像的一部分所取代，如下所示：
- en: '![](img/B17259_07_10.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_07_10.jpg)'
- en: Figure 7.10 – The result of applying the CutMix function to the images (image
    1 and image 2)
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 – 将CutMix函数应用于图像的结果（图像1和图像2）
- en: It’s designed to encourage a model to make more localized, fine-grained predictions,
    thus improving overall generalization. CutMix also enforces consistent predictions
    outside the mixed regions, further enhancing model robustness.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 它旨在鼓励模型做出更多局部化、细粒度的预测，从而提高整体泛化能力。CutMix还强制在混合区域外进行一致的预测，进一步增强了模型的鲁棒性。
- en: '`torchvision` also offers an in-built API for CutMix, `torchvision.transforms.v2.CutMix`,
    so we don’t have to implement it from scratch.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchvision`还提供了内置的CutMix API，`torchvision.transforms.v2.CutMix`，因此我们不必从头实现它。'
- en: The full notebook with CutMix implementation from scratch can be found in the
    GitHub repo.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 从零开始实现的CutMix完整笔记本可以在GitHub仓库中找到。
- en: CutMix often shows improvements over traditional augmentation techniques on
    benchmark datasets, such as CIFAR-10, CIFAR-100, and ImageNet [7].
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: CutMix在基准数据集（如CIFAR-10、CIFAR-100和ImageNet [7]）上通常比传统的增强技术有改进。
- en: MixUp
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MixUp
- en: MixUp [8] creates virtual training examples by forming combinations of pairs
    of inputs and their corresponding labels.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: MixUp [8]通过形成输入及其对应标签的成对组合来创建虚拟训练示例。
- en: 'If (x i, y i) and (x j, y j) is an arbitrary pair of images in dataset D, where
    x is the image while y is its label, a mixed sample  ~ x ,  ~ y  can be generated
    using the following equations:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如果(x i, y i)和(x j, y j)是数据集D中的任意一对图像，其中x是图像，y是其标签，则可以使用以下方程生成混合样本~x ,  ~y ：
- en: ~ x  = λ x i + (1 − λ) x j
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ~ x  = λ x i + (1 − λ) x j
- en: ~ y  = λ y i + (1 − λ) y j
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ~ y  = λ y i + (1 − λ) y j
- en: where λ is the mixing factor sampled from the beta distribution. The Beta distribution
    is a flexible, continuous probability distribution defined on the interval [0,
    1].
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 其中λ是从Beta分布中采样的混合因子。Beta分布是一种灵活的、在区间[0, 1]上定义的连续概率分布。
- en: 'MixUp acts as a regularizer, preventing overfitting and enhancing the generalization
    capabilities of models. The following implementation shuffles the data and targets,
    and then combines them using a weighted average, based on a value sampled from
    the Beta distribution, creating mixed data and targets for augmentation:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: MixUp充当正则化器，防止过拟合并增强模型的泛化能力。以下实现通过从Beta分布中采样的值进行数据集和目标的洗牌，然后使用加权平均结合它们，从而创建用于增强的混合数据和目标：
- en: '[PRE21]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](img/B17259_07_11.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_07_11.jpg)'
- en: Figure 7.11 – The result of applying MixUp on the images (image 1 and image
    2)
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: On datasets such as CIFAR-100, MixUp has been found to provide significant gains
    in test accuracy compared to models trained without MixUp [8].
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Similar to CutMix, `torchvision` provides a built-in API called `torchvision.transforms.v2.MixUp`,
    eliminating the need for manual implementation.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: AugMix
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The augmentation techniques that we have studied so far have all been fixed
    augmentations, but deep learning models can memorize them [9] and their performance
    can plateau. This is where AugMix [10] can be helpful, as it produces a diverse
    set of augmented images by performing several random augmentations in a sequence.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: AugMix improves model robustness and uncertainty without requiring any changes
    to the model architecture. The full AugMix algorithm also uses a special kind
    of loss function, but we will skip that for simplicity.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code presents a simplified version of AugMix’s core logic:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: At the end of the function, we combine images by using equal weight for each
    of the four transformed pictures. The actual AugMix implementation uses a Dirichlet
    distribution function to combine the images. A Dirichlet distribution is a generalization
    of the beta distribution that we saw in the MixUp technique.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_07_12.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
- en: Figure 7.12 – The result of applying the Augmix function to four different images
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 7**.12*, the top row shows the original image, while the bottom row
    shows the result of applying AugMix. Images 1 and 3 don’t seem to have changed,
    but images 2 and 4 have noticeable changes.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: According to the AugMix paper [10], in experiments with ImageNet and CIFAR,
    AugMix achieved reduced test errors while providing improved robustness against
    corruption.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: We don’t need to create AugMix from scratch, as `torchvision` provides a built-in
    API called `torchvision.transforms.AugMix` for this purpose.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Remix
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Standard data augmentation techniques such as MixUp and CutMix may not be sufficient
    to handle class imbalances, as they do not take the distribution of class labels
    into account. Remix [11] addresses the challenges of training deep learning models
    on imbalanced datasets.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: MixUp and CutMix utilize the same mixing factor to combine samples in both the
    feature space and the label space. In the context of imbalanced data, the authors
    of the Remix paper [11] argued that this approach may not be optimal. Therefore,
    they proposed to separate the mixing factors, allowing for more flexibility in
    their application. By doing so, greater weight can be assigned to the minority
    class, enabling the creation of labels that are more favorable to the underrepresented
    class.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'If (xi, yi; xi, yj) is an arbitrary pair of images in dataset D, a mixed sample
    xRM, y RM can be generated using the following equations:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: x RM = λxxi + (1 − λx)xj
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: y RM = λyyi + (1 − λy)yj
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: λx and λy are the mixing factors sampled from the beta distribution.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a simplified implementation:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Combining previous techniques
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It’s possible to combine these methods to introduce even more diversity to
    the training data, such as the following:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '**CutMix and MixUp**: These can be alternated or used in tandem, creating regions
    in images that are replaced with parts of other images while also blending images
    pixel-wise'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sequential**: You could sequentially apply these techniques (e.g., use MixUp
    first and then CutMix) to further diversify the augmented dataset'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When combining these methods, it’s important to carefully manage the probabilities
    and strengths of each method, thus avoiding introducing too much noise or making
    the training data too divergent from the original distribution.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Also, while combining these methods might improve model robustness and generalization
    in certain scenarios, it can also make training more computationally intensive
    and complex. It’s essential to balance the benefits against the potential trade-offs.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Remember, always validate the effectiveness of combined augmentations on a validation
    set to ensure they are beneficial for the task at hand.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use a long-tailed version of a different dataset called Fashion-MNIST
    for the techniques we just discussed (*Figure 7**.13*). Fashion-MNIST is another
    MNIST variant, consisting of 60,000 training and 10,000 testing images of 10 different
    clothing items, such as shoes, shirts, and dresses, each represented in a grayscale
    image of 28x28 pixels.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_07_13.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
- en: Figure 7.13 – Imbalanced FashionMNIST
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7**.14* shows the overall model accuracy when trained using CutMix,
    MixUp, a combination of both, and Remix on the imbalanced FashionMNIST dataset.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_07_14.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
- en: Figure 7.14 – Overall model accuracy (FashionMNIST)
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: The difference in the performance of these techniques is more apparent when
    looking at the class-wise accuracy numbers (*Figure 7**.15*).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_07_15.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
- en: Figure 7.15 – Class-wise model accuracy (the FashionMNIST dataset)
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the given data, here are some insightful conclusions that can be drawn
    for the various techniques, especially in the context of imbalanced data:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '**Overall performance**: **Cutmix** and **Remix** generally offer the highest
    performance across most classes, followed closely by **Mixup** and **Cutmix+Mixup**.
    **Baseline** seems to be the least effective in general.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance on minority classes**: For the minority class labeled as “6,”
    all techniques show relatively low performance compared to other classes. However,
    **Mixup** and **Cutmix+Mixup** offer a slight improvement over the baseline.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consistency across classes**: **Cutmix** and **Mixup** are more consistent
    across different classes, excluding class “6,” where they are only marginally
    better. **Baseline**, on the other hand, shows significant variability, performing
    extremely well on some classes (such as “0” and “5”) but poorly on others (such
    as “6”).'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Techniques suited for specific classes**: **Cutmix** performs exceptionally
    well for the classes labeled “1” and “8,” where it outperforms all other techniques.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**适用于特定类别的技术**：**Cutmix**在标记为“1”和“8”的类别中表现出色，在这些类别中它优于所有其他技术。'
- en: '**Remix** is particularly strong for the class labeled “2,” where it outshines
    all other techniques.'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**Remix**在标记为“2”的类别中特别强大，它超越了所有其他技术。'
- en: '**Complexity versus benefit**: **Cutmix+Mixup** does not offer a significant
    improvement over **Cutmix** or **Mixup** individually, raising questions about
    whether the additional computational complexity is justified.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂性与收益**：**Cutmix+Mixup**与**Cutmix**或**Mixup**单独使用相比，并没有带来显著的改进，这引发了额外的计算复杂性是否合理的疑问。'
- en: '**Generalizability**: **Cutmix** and **Mixup** appear to be the most robust
    techniques, showing high performance across most classes. These techniques would
    likely perform well on unseen data and are potentially good choices for imbalanced
    datasets.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**泛化性**：**Cutmix**和**Mixup**似乎是最稳健的技术，在大多数类别中表现出高性能。这些技术在未见过的数据上可能表现良好，并且对于不平衡数据集可能是好的选择。'
- en: '**Trade-offs**: **Cutmix** offers high performance but may not be the best
    for minority classes. **Mixup**, although slightly less effective overall, offers
    more balanced performance across classes, including minority ones.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**权衡**：**Cutmix**提供了高性能，但可能不是对少数类别最好的选择。**Mixup**虽然整体上略逊一筹，但在各个类别，包括少数类别中提供了更平衡的性能。'
- en: 'The following are some points to be careful about while performing these image
    augmentations:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行这些图像增强时需要注意以下一些点：
- en: We must ensure that data augmentations preserve the original labels. For instance,
    rotating digits such as 6 and 9 can be problematic in digit recognition tasks.
    Similarly, cropping an image could invalidate its label, such as removing a cat
    from a “cat” image. This is especially crucial in complex tasks, such as image
    segmentation in self-driving cars, where augmentations can alter output labels
    or masks.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们必须确保数据增强保留原始标签。例如，旋转数字如6和9在数字识别任务中可能会出现问题。同样，裁剪图像可能会使其标签无效，例如从“猫”图像中移除猫。这在复杂任务中尤为重要，例如在自动驾驶汽车中的图像分割任务，增强可能会改变输出标签或掩码。
- en: While geometric and color transformations often increase memory usage and training
    time, they are not inherently problematic. Although color alterations can sometimes
    remove important details and affect label integrity, smart manipulation can also
    be beneficial. For instance, tweaking the color space to mimic different lighting
    or camera lenses can improve model performance.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然几何和颜色变换通常会增加内存使用和训练时间，但它们本身并不是问题。尽管颜色变化有时会去除重要细节并影响标签完整性，但智能操作也可能有益。例如，调整颜色空间以模拟不同的光照或相机镜头可以提高模型性能。
- en: To overcome some of the increased memory, time, and cost issues, as mentioned,
    a technique called `AutoAugment` is available in `PyTorch`, which can automatically
    search for the best augmentation policies on the dataset being used.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服一些增加的内存、时间和成本问题，如前所述，PyTorch中有一个名为`AutoAugment`的技术，可以在使用的数据集上自动搜索最佳的增强策略。
- en: 🚀 Deep learning data-level techniques in production at Grab
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 🚀 Grab在生产中使用的深度学习数据级技术
- en: '**🎯** **Problem** **being solved:**'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**🎯** **解决的问题**：'
- en: Grab, a ride-hailing and food delivery company in South-East Asia, faced the
    primary challenge of anonymizing faces and license plates in images collected
    for their geotagged imagery platform, KartaView [12]. This was essential to ensure
    user privacy.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: Grab是一家位于东南亚的打车和食品配送公司，面临着匿名化用于其地理标记图像平台KartaView [12]中收集的图像中的人脸和车牌的主要挑战。这是确保用户隐私所必需的。
- en: '**⚖️** **Data** **imbalance issue**:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '**⚖️** **数据不平衡问题**：'
- en: The dataset used by Grab was imbalanced, particularly in terms of the object
    sizes. Larger regions of interest, such as close-ups of faces or license plates,
    were underrepresented. This skewed distribution led to poor model performance
    in detecting these larger objects.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: Grab使用的数据集不平衡，特别是在物体大小方面。较大的感兴趣区域，如面部特写或车牌，代表性不足。这种分布偏差导致模型在检测这些较大物体时性能不佳。
- en: '**🎨** **Data** **augmentation strategy:**'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '**🎨** **数据增强策略**：'
- en: 'Grab employed a multi-pronged data augmentation approach to address the imbalance:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: Grab采用了多角度的数据增强方法来解决不平衡问题：
- en: '• **Offline augmentation**: One key method they used was the “image view splitting,”
    where each original image is divided into multiple “views” with predefined properties.
    This was crucial to accommodate different types of images such as perspective,
    wide field of view, and 360-degree equirectangular images. Each “view” was treated
    as a separate image with its tags, which helped the model generalize better. They
    also implemented oversampling for images with larger tags, addressing the imbalance
    in their dataset. This was vital for their anchor-based object detection model,
    as the imbalance was affecting the model’s performance in identifying larger objects.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '• **Online augmentation**: They used YOLOv4 for object detection, which allowed
    for a variety of online augmentations, such as saturation, exposure, hue, flip,
    and mosaic.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Modern techniques such as autoencoders and adversarial networks, specifically
    **Generative Adversarial Networks** (**GANs**), have recently gained traction
    in creating synthetic data to enhance image datasets. A GAN comprises two neural
    networks – the generator, which produces synthetic data, and the discriminator,
    which evaluates the authenticity of this data. Together, they work to create realistic
    and high-quality synthetic samples. GANs have also been applied to generate synthetic
    tabular data. For example, they’ve been used to create synthetic medical images
    that significantly improve diagnostic models. We’ll explore these cutting-edge
    techniques in greater detail toward the end of the chapter. In the next section,
    we will learn about applying data-level techniques to NLP problems.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Data-level techniques for text classification
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data imbalance, wherein certain classes in a dataset are underrepresented, is
    not just an issue confined to image or structured data domains. In NLP, imbalanced
    datasets can lead to biased models that might perform well on the majority class
    but are likely to misclassify underrepresented ones. To address this challenge,
    numerous strategies have been devised.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'In NLP, data augmentation can boost model performance, especially with limited
    training data. *Table 7.3* categorizes the various data augmentation techniques
    for text data:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '| **Level** | **Method** | **Description** | **Example techniques** |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
- en: '| Character level | Noise | Introducing randomness at the character level |
    Jumbling characters |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
- en: '| Rule-based | Transformations based on predefined rules | Capitalization |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
- en: '| Word level | Noise | Random word changes | “cat” to “dog” |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
- en: '| Synonyms | Replacing words with their synonyms | “happy” to “joyful” |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
- en: '| Embeddings | Using word embeddings for replacement | “king” to “monarch”
    |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
- en: '| Language models | Leveraging advanced language models for word replacement
    | BERT |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
- en: '| Phrase level | Structure | Altering the structure of phrases | Changing word
    order |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
- en: '| Interpolation | Merging features of two phrases | “The cat sat” + “The dog
    barked” = “The cat barked” |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
- en: '| Document level | Translation | Translating the document to another language
    and back | English to French to English |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
- en: '| Generative | Using models to generate new content | GPT-3 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
- en: Table 7.3 – The categorization of different data augmentation methods (adapted
    from [13])
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation techniques for text can be categorized into character, word,
    phrase, and document levels. Techniques vary from jumbling characters to using
    models such as BERT and GPT-3\. This taxonomy guides us through NLP data augmentation
    methods.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 7.3* shows various data augmentation methods used in NLP. We will break
    down the methods based on the level at which the data is manipulated – character,
    word, phrase, and document. Each level has its unique set of methods, such as
    introducing “noise” at the character level or leveraging “language models” at
    the word level. These methods are not just random transformations; they are often
    carefully designed to preserve the semantic meaning of the text while introducing
    variability.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: What sets this categorization apart is its multilayered approach, which allows
    a more targeted application of data augmentation methods. For instance, if you’re
    dealing with short text snippets, methods at the character or word level may be
    more appropriate. On the other hand, if you’re working with longer documents or
    need to generate entirely new content, then methods at the document level, such
    as “generative” techniques, come into play.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: In the subsequent sections, we will explore a text classification dataset that
    is imbalanced and illustrate various data augmentation techniques using it. These
    methodologies are designed to synthesize additional data, thereby enhancing a
    model’s ability to learn and generalize from the imbalanced information.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Dataset and baseline model
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s take the spam text message classification dataset available on Kaggle
    ([https://www.kaggle.com/datasets/team-ai/spam-text-message-classification](https://www.kaggle.com/datasets/team-ai/spam-text-message-classification)).
    This dataset, primarily used to distinguish spam from legitimate messages, presents
    an imbalance with a majority of “ham” (legitimate) messages and a minority of
    “spam” messages. We are skipping the code here to save space. You can find the
    notebook in the GitHub repo with the name `Data_level_techniques_NLP.ipynb`.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'With a baseline model, we have the following results:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Random oversampling
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One basic technique to handle data imbalance is random oversampling, where
    instances of the minority class are replicated to balance out the class distribution.
    While this technique is easy to implement and often shows improved performance,
    it’s essential to be wary of overfitting:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Random oversampling shows a slight improvement in overall accuracy, rising from
    0.97 to 0.98\. The most notable gain is in the recall for the `spam` class, which
    increased from 0.80 to 0.91, indicating better identification of spam messages.
    However, the precision for `spam` dropped a bit from 0.97 to 0.93.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: The macro average F1-score also improved from 0.93 to 0.95, suggesting that
    the model is now better at handling both classes (`ham` and `spam`) more equally.
    The weighted average metrics remain strong, reinforcing that the model’s overall
    performance has improved without sacrificing its ability to correctly classify
    the majority class (`ham`).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, undersampling can be useful to reduce the size of the majority class,
    particularly by eliminating exact duplicate sentences. For example, you might
    not need 500 copies of “Thanks very much!” However, sentences with similar semantic
    meaning but different wording, such as “Thanks very much!” and “Thanks so much!”,
    should generally be retained. Exact duplicates can be identified, using methods
    such as string matching, while sentences with similar meanings can be detected
    using cosine similarity or the Jaccard similarity of sentence embeddings.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 🚀 Deep learning data-level techniques in production at Cloudflare
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '**🎯** **Problem** **being solved**:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Cloudflare [14] aimed to enhance its **Web Application Firewall** (**WAF**)
    to better identify malicious HTTP requests and protect against common attacks,
    such as SQL injection and **cross-site** **scripting** (**XSS**).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '**⚖️** **Data** **imbalance issue**:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Creating a quality dataset to train the WAF model was difficult, due to strict
    privacy regulations and the absence of labeled data for malicious HTTP requests.
    The heterogeneity of samples also presented challenges as the requests came in
    various formats and encodings. There was a significant lack of samples for specific
    types of attacks, making the dataset imbalanced and leading to the risk of false
    positives or negatives.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '**🎨** **Data** **augmentation strategy**:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: To tackle this, Cloudflare employed a combination of data augmentation and generation
    techniques. These included mutating benign content in various ways, generating
    pseudo-random noise samples, and using language models for synthetic data creation.
    The focus was on increasing the diversity of negative samples while maintaining
    the integrity of the content, thereby forcing the model to consider a broader
    spectrum of structural, semantic, and statistical properties for better classification.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '🚀 **Model deployment**:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: The model that they used significantly improved after employing these data augmentation
    techniques, with a remarkable F1 score of 0.99 after augmentation compared to
    0.61 before. The model has been validated against Cloudflare’s signature-based
    WAF and was found to perform comparably, making it production-ready.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Document-level augmentation
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In document-level augmentation, entire documents are modified to create new
    examples, in order to preserve the broader semantic context or narrative flow
    of the document. One such technique is back translation.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Back translation
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Back translation involves translating a sentence to a different language and
    then reverting it back to the original (*Figure 7**.16*). This produces sentences
    that are syntactically different but semantically similar to the original text,
    providing a form of augmentation.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_07_16.jpg)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
- en: Figure 7.16 – Demonstrating the back translation technique
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: We generate the back-translated text and append it to the original dataset.
    Then, we use the full dataset to train the logistic regression model. Note that
    this can be a time-consuming process, since the translation model binaries are
    resource-intensive. It may also introduce errors, since some words may not be
    exactly translatable across languages. In the GitHub notebook, we used the `BackTranslationAug`
    API from the `nlpaug` library [15].
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: 'The following results show the classification metrics on the test set. The
    precision of the spam class shows improvement over the random oversampling technique,
    while recall is a bit worse:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Back translation maintains an overall accuracy of 0.98, similar to random oversampling.
    It slightly improves `spam` precision to 0.96 but lowers recall to 0.86\. Both
    methods outperform the baseline, with back translation favoring precision over
    recall for the `spam` class.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Character and word-level augmentation
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s briefly go over a few character and word-level augmentation techniques
    that can be applied to NLP problems.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Easy Data Augmentation techniques
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Easy Data Augmentation** (**EDA**) is a suite of data augmentation techniques
    specific to text data. It includes simple operations such as synonym replacement,
    random insertion, random swap, and random deletion. These operations, being simple,
    ensure that the augmented data remains meaningful. The following table shows various
    metrics when using EDA on the dataset:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: After applying EDA, the model retains an overall accuracy of 0.98, consistent
    with both random oversampling and back translation. The precision for `spam` is
    high at 0.96, similar to back translation, while the recall is slightly better
    at 0.88 compared to 0.86 with Back Translation. The macro and weighted averages
    remain robust at 0.95 and 0.98, respectively.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: EDA offers a balanced improvement in both precision and recall for the `spam`
    class, making it a strong contender among the data augmentation techniques we’ve
    tried.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Precision** | **Recall** | **F1-Score** | **Accuracy** |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
- en: '| Baseline model | **0.97** | 0.80 | 0.88 | 0.97 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
- en: '| Random oversampling | 0.93 | **0.91** | **0.92** | **0.98** |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
- en: '| Back translation | 0.96 | 0.86 | 0.91 | **0.98** |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
- en: '| EDA | 0.96 | 0.88 | 0.91 | **0.98** |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
- en: Table 7.4 – Comparing the results of the various NLP data-level techniques for
    the spam class (max per metric in bold)
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: Overall, as we can see in *Table 7.4*, for our dataset, random oversampling
    excels in recall for `spam` but slightly lowers precision. Back translation boosts
    precision at a minor recall trade-off. EDA offers a balanced improvement in both.
    It’s important to note that these results are empirical and specific to the dataset
    used for this analysis. Data augmentation techniques can yield different outcomes,
    depending on the nature of the data, its distribution, and the problem being addressed.
    Therefore, while these techniques show promise in this context, their effectiveness
    may vary when applied to different datasets or NLP tasks.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: We will not be covering phrase-level augmentation techniques in this book due
    to space constraints, but we recommend exploring them on your own.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at some miscellaneous data-level deep learning techniques
    at a high level.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Discussion of other data-level deep learning methods and their key ideas
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to the methods previously discussed, there is a rich array of other
    techniques specifically designed to address imbalanced data challenges. This section
    provides a high-level overview of these alternative approaches, each offering
    unique insights and potential advantages. While we will only touch upon their
    key ideas, we encourage you to delve deeper into the literature and explore them
    further if you find these techniques intriguing.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: Two-phase learning
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Two-phase learning [16][17] is a technique designed to enhance the performance
    of minority classes in multi-class classification problems, without compromising
    the performance of majority classes. The process involves two training phases:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: In the first phase, a deep learning model is first trained on the dataset, which
    is balanced with respect to each class. Balancing can be done using sampling techniques
    such as random oversampling or undersampling.
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the second phase, we freeze all the layers except the last one, and then
    the model is fine-tuned using the entire dataset.
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first phase ensures that all layers are trained on a balanced dataset. The
    second phase calibrates the output probabilities by retraining the last layer
    with the entire dataset, reflecting the original imbalanced class distribution.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: The order of the two phases can be reversed – that is, the first model is trained
    on the full imbalanced data and then fine-tuned on a balanced dataset in the second
    phase. This is called **deferred sampling**, since sampling is done later.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Expansive Over-Sampling
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Introduced in a paper by Damien Dablain et al. [18], **Expansive Over-Sampling**
    (**EOS**) is another data augmentation technique used within a three-phase CNN
    training framework, designed for imbalanced data. It can be considered to incorporate
    both two-phase learning and data augmentation techniques.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: EOS works by creating synthetic training instances as combinations between the
    minority class samples and their nearest “enemies” in the embedded space. The
    term “nearest enemies” refers to instances of other classes that are closest in
    the feature space to a given instance. By creating synthetic instances in this
    way, EOS aims to reduce the generalization gap, which is wider for minority classes.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: The paper’s authors [18] claimed that this method improves accuracy and efficiency
    over common imbalanced learning techniques, requiring fewer parameters and less
    training time.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: Using generative models for oversampling
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative models, including GANs, **Variational AutoEncoders** (**VAEs**),
    diffusion models, and their derivatives, such as StyleGAN, StyleGAN2, and GPT-based
    models, have become prominent tools for producing data points that resemble training
    data.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: VAEs, a specific type of generative model, consist of an encoder and decoder
    that work together to create new instances of data, such as realistic images,
    and can be used to balance imbalanced datasets. On the long-tailed version of
    MNIST, we got a decent performance improvement by using a VAE-augmented model
    when compared to the baseline model on the most imbalanced classes. *Figure 7**.17*
    shows the performance comparison after 50 epochs. You can find the notebook in
    the corresponding chapter of the GitHub repo.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_07_17.jpg)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
- en: Figure 7.17 – VAE-augmented model performance on the long-tailed MNIST dataset
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Diffusion models operate by progressively corrupting an image with noise and
    then reconstructing it, with applications in areas such as medical imaging. Examples
    include DALLE-2 and the open source stable diffusion model.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: Recent studies [19] highlight the utility of synthetic data in enhancing zero-shot
    and few-shot image classification tasks. Specifically, text-to-image generation
    models, when used in conjunction with large-scale pre-trained models such as DALL-E
    and Stable Diffusion, significantly improve performance in scenarios where real-world
    data is sparse or unavailable. These generative models have gained prominence
    for their ability to create high-quality images based on natural language prompts,
    offering a potential solution for imbalanced datasets. For example, if there is
    a scarcity of images featuring a monkey seated in a car, these models can generate
    hundreds or even thousands of such images to augment training datasets. However,
    it’s worth noting that models trained solely on synthetic data may still underperform
    compared to those trained on real data.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: These models often require significant computational resources, making them
    time-consuming and expensive to scale up, especially for vast datasets. Diffusion
    models, in particular, are computationally intensive, and potential overfitting
    can compromise model generalizability. Therefore, it is crucial to balance the
    benefits of data augmentation with the computational cost and potential challenges
    when employing these advanced generative models.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: DeepSMOTE
  id: totrans-328
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **Deep Synthetic Minority Oversampling** (**DeepSMOTE**) technique [20]
    is essentially SMOTE adapted for deep learning models using an encoder-decoder
    architecture, with minor tweaks for image data. DeepSMOTE consists of three major
    components:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '**An encoder/decoder framework to handle complex and high-dimensional data**:
    An encoder/decoder framework is used to learn a compact feature representation
    of the image data. It is trained to reconstruct the original images from this
    compact form, ensuring that essential features are captured.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SMOTE-based oversampling for generating synthetic instances**: Once the feature
    representation is learned, SMOTE is applied in this feature space to generate
    synthetic instances of the minority class. This is particularly useful for image
    data where the raw data is high-dimensional and complex. SMOTE creates these synthetic
    instances by finding the *k*-nearest neighbors in the feature space and generating
    new instances that are interpolations between the instance under consideration
    and its neighbors.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A dedicated loss function**: DeepSMOTE introduces a specialized loss function
    that not only focuses on the reconstruction error (how well the decoder can reconstruct
    the original image from the encoded form) but also includes a penalty term, ensuring
    that the synthetic instances are useful for the classification task.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlike GAN-based oversampling, DeepSMOTE does not require a discriminator. It
    claims to generate high-quality, information-rich synthetic images that can be
    visually inspected.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_07_18.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
- en: Figure 7.18 – Demonstrating the DeepSMOTE technique (adapted from [20])
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: Neural style transfer
  id: totrans-336
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Neural style transfer is a technique in deep learning that artistically blends
    the content of one image with the style of another (*Figure 7**.19*). While its
    primary application is in art and image processing, the concept of generating
    synthetic data samples can be adapted to address data imbalance in machine learning.
    By drawing inspiration from style transfer, one could potentially generate synthetic
    samples for the minority class, blend features of different classes, or adapt
    domain-specific knowledge. However, care must be taken to ensure that synthetic
    data authentically represents real-world scenarios to avoid overfitting and poor
    generalization of real data.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_07_19.jpg)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
- en: Figure 7.19 – Demonstrating the neural style transfer technique
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: We hope that this provides a thorough understanding of data-level deep learning
    methods to address imbalanced data, including oversampling, data augmentation,
    and various other strategies.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-341
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The transition of methods to handle data imbalance from classical machine learning
    models to deep learning models can pose unique challenges, primarily due to the
    distinct types of data that these models have to work with. Classical machine
    learning models typically deal with structured, tabular data, whereas deep learning
    models often grapple with unstructured data, such as images, text, audio, and
    video. This chapter explored how to adapt sampling techniques to work with deep
    learning models. To facilitate this, we used an imbalanced version of the MNIST
    dataset to train a model, which is then employed in conjunction with various oversampling
    methods.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating random oversampling with deep learning models involves duplicating
    samples from minority classes randomly, until each class has an equal number of
    samples. This is usually performed using APIs from libraries such as imbalanced-learn,
    Keras, TensorFlow, or PyTorch, which work together seamlessly for this purpose.
    Once data is oversampled, it can be sent for model training in PyTorch or TensorFlow.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: The chapter also delved into different data augmentation techniques, which can
    be especially beneficial when dealing with limited or imbalanced data. Augmentation
    techniques include rotating, scaling, cropping, blurring, and adding noise, among
    other advanced techniques such as AugMix, CutMix, and MixUp. However, care must
    be taken to ensure these augmentations preserve the original labels and do not
    inadvertently alter vital information in the data. We discussed other methods,
    such as two-phase learning and dynamic sampling, as potential strategies to improve
    model performance on imbalanced data. We also learned about some data-level techniques
    applicable to text, such as back translation and EDA, while running them on a
    spam/ham dataset.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at some algorithm-based methods to deal with
    imbalanced datasets.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apply Mixup interpolation to the Kaggle spam detection NLP dataset used in
    the chapter. See if Mixup helps to improve the model performance. You can refer
    to the paper *Augmenting Data with Mixup for Sentence Classification: An Empirical
    Study* by Guo et al. ([https://arxiv.org/pdf/1905.08941.pdf](https://arxiv.org/pdf/1905.08941.pdf))
    for further reading.'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Refer to the FMix paper [21] and implement the FMix augmentation technique.
    Apply it to the Caltech101 dataset. See whether model performance improves by
    using FMix over the baseline model performance.
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the EOS technique described in the chapter to the CIFAR-10-LT (the long-tailed
    version of CIFAR-10) dataset, and see whether the model performance improves for
    the most imbalanced classes.
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the MDSA techniques we studied in this chapter to the CIFAR-10-LT dataset,
    and see whether the model performance improves for the most imbalanced classes.
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  id: totrans-351
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Samira Pouyanfar, Yudong Tao, Anup Mohan, Haiman Tian, Ahmed S. Kaseb, Kent
    Gauen, Ryan Dailey, Sarah Aghajanzadeh, Yung-Hsiang Lu, Shu-Ching Chen, and Mei-Ling
    Shyu. 2018\. *Dynamic Sampling in Convolutional Neural Networks for Imbalanced
    Data Classification*. In 2018 IEEE Conference on Multimedia Information Processing
    and Retrieval (MIPR), pages 112–117, Miami, FL, April. IEEE.
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'LeNet-5 paper, *Gradient-based learning applied to document* *classification*:
    [http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf).'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'AlexNet paper, *ImageNet Classification with Deep Convolutional Neural* *Networks*:
    [https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html](https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html).'
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Leveraging Real-Time User Actions to Personalize Etsy Ads* (2023): [https://www.etsy.com/codeascraft/leveraging-real-time-user-actions-to-personalize-etsy-ads](https://www.etsy.com/codeascraft/leveraging-real-time-user-actions-to-personalize-etsy-ads).'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Automated image tagging at [Booking.com](http://Booking.com) (2017): [https://booking.ai/automated-image-tagging-at-booking-com-7704f27dcc8b](https://booking.ai/automated-image-tagging-at-booking-com-7704f27dcc8b).'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Shot Angle Prediction: Estimating Pose Angle with Deep Learning for Furniture
    Items Using Images Generated from 3D Models (**2020)*: [https://www.aboutwayfair.com/tech-innovation/shot-angle-prediction-estimating-pose-angle-with-deep-learning-for-furniture-items-using-images-generated-from-3d-models](https://www.aboutwayfair.com/tech-innovation/shot-angle-prediction-estimating-pose-angle-with-deep-learning-for-furniture-items-using-images-generated-from-3d-models).'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'S. Yun, D. Han, S. Chun, S. J. Oh, Y. Yoo, and J. Choe, “*CutMix: Regularization
    Strategy to Train Strong Classifiers With Localizable Features*,” in 2019 IEEE/CVF
    International Conference on Computer Vision (ICCV), Seoul, Korea (South): IEEE,
    Oct. 2019, pp. 6022–6031\. doi: 10.1109/ICCV.2019.00612.'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, “*mixup: Beyond Empirical
    Risk Minimization*.” arXiv, Apr. 27, 2018\. Accessed: Feb. 11, 2023\. [Online].
    Available: [http://arxiv.org/abs/1710.09412](http://arxiv.org/abs/1710.09412).'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: R. Geirhos, C. R. M. Temme, J. Rauber, H. H. Schütt, M. Bethge, and F. A. Wichmann,
    “*Generalisation in humans and deep* *neural networks*.”
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'D. Hendrycks, N. Mu, E. D. Cubuk, B. Zoph, J. Gilmer, and B. Lakshminarayanan,
    “*AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty*.”
    arXiv, Feb. 17, 2020\. Accessed: Aug. 01, 2023\. [Online]. Available: [http://arxiv.org/abs/1912.02781](http://arxiv.org/abs/1912.02781).'
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'H.-P. Chou, S.-C. Chang, J.-Y. Pan, W. Wei, and D.-C. Juan, “*Remix: Rebalanced
    Mixup*.” arXiv, Nov. 19, 2020\. Accessed: Aug. 15, 2023\. [Online]. Available:
    [http://arxiv.org/abs/2007.03943](http://arxiv.org/abs/2007.03943).'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Protecting Personal Data in Grab’s Imagery* (2021): [https://engineering.grab.com/protecting-personal-data-in-grabs-imagery](https://engineering.grab.com/protecting-personal-data-in-grabs-imagery).'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'M. Bayer, M.-A. Kaufhold, and C. Reuter, “*A Survey on Data Augmentation for
    Text Classification*,” ACM Comput. Surv., vol. 55, no. 7, pp. 1–39, Jul. 2023,
    doi: 10.1145/3544558.'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Improving the accuracy of our machine learning WAF using data augmentation
    and sampling* (2022), Vikram Grover: [https://blog.cloudflare.com/data-generation-and-sampling-strategies/](https://blog.cloudflare.com/data-generation-and-sampling-strategies/).'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Data augmentation for* *NLP*: [https://github.com/makcedward/nlpaug](https://github.com/makcedward/nlpaug).'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'B. Kang *et al.*, “*Decoupling Representation and Classifier for Long-Tailed
    Recognition*.” arXiv, Feb. 19, 2020\. Accessed: Dec. 15, 2022\. [Online]. Available:
    [http://arxiv.org/abs/1910.09217](http://arxiv.org/abs/1910.09217).'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'K. Cao, C. Wei, A. Gaidon, N. Arechiga, and T. Ma, “*Learning Imbalanced Datasets
    with Label-Distribution-Aware Margin Loss*”, [Online]. Available: [https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf).'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'D. Dablain, C. Bellinger, B. Krawczyk, and N. Chawla, “*Efficient Augmentation
    for Imbalanced Deep Learning*.” arXiv, Oct. 17, 2022\. Accessed: Jul. 23, 2023\.
    [Online]. Available: [http://arxiv.org/abs/2207.06080](http://arxiv.org/abs/2207.06080).'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'R. He *et al.*, “*Is synthetic data from generative models ready for image
    recognition?*” arXiv, Feb. 15, 2023\. Accessed: Aug. 06, 2023\. [Online]. Available:
    [http://arxiv.org/abs/2210.07574](http://arxiv.org/abs/2210.07574).'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'D. Dablain, B. Krawczyk, and N. V. Chawla, “*DeepSMOTE: Fusing Deep Learning
    and SMOTE for Imbalanced Data*,” IEEE Transactions on Neural Networks and Learning
    Systems, pp. 1–15, 2022, doi: 10.1109/TNNLS.2021.3136503.'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'E. Harris, A. Marcu, M. Painter, M. Niranjan, A. Prügel-Bennett, and J. Hare,
    “*FMix: Enhancing Mixed Sample Data Augmentation*.” arXiv, Feb. 28, 2021\. Accessed:
    Aug. 08, 2023\. [Online]. Available: [http://arxiv.org/abs/2002.12047](http://arxiv.org/abs/2002.12047).'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
