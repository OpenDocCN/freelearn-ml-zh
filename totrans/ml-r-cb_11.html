<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;11.&#xA0;Dimension Reduction"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch11" class="calibre1"/>Chapter 11. Dimension Reduction</h1></div></div></div><p class="calibre7">In this chapter, we will cover the following topics:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Performing feature selection with FSelector</li><li class="listitem">Performing dimension reduction with PCA</li><li class="listitem">Determining the number of principal components using a scree test</li><li class="listitem">Determining the number of principal components using the Kaiser method</li><li class="listitem">Visualizing multivariate data using biplot</li><li class="listitem">Performing dimension reduction with MDS</li><li class="listitem">Reducing dimensions with SVD</li><li class="listitem">Compressing images with SVD</li><li class="listitem">Performing nonlinear dimension reduction with ISOMAP</li><li class="listitem">Performing nonlinear dimension deduction with Local Linear Embedding</li></ul></div></div>

<div class="book" title="Chapter&#xA0;11.&#xA0;Dimension Reduction">
<div class="book" title="Introduction"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch11lvl1sec121" class="calibre1"/>Introduction</h1></div></div></div><p class="calibre7">Most datasets contain features (such as attributes or variables) that are highly redundant. In order to remove irrelevant and redundant data to reduce the computational cost and avoid overfitting, you can reduce the features into a smaller subset without a significant loss of information. The <a id="id862" class="calibre1"/>mathematical procedure of reducing features is known as dimension reduction.</p><p class="calibre7">The reduction of features can increase the efficiency of data processing. Dimension reduction is, therefore, widely used in the fields of pattern recognition, text retrieval, and machine learning. Dimension reduction<a id="id863" class="calibre1"/> can be divided into two parts: feature extraction <a id="id864" class="calibre1"/>and feature selection. Feature <a id="id865" class="calibre1"/>extraction is a technique that uses a lower <a id="id866" class="calibre1"/>dimension space to represent data in a higher dimension space. Feature selection is used to find a subset of the original variables.</p><p class="calibre7">The objective of feature selection is to select a set of relevant features to construct the model. The techniques for feature selection can be categorized into feature ranking and feature selection. Feature ranking ranks features with a certain criteria and then selects features that are above a defined threshold. On the other hand, feature selection searches the optimal subset from a space of feature subsets.</p><p class="calibre7">In feature extraction, the problem can be categorized as linear or nonlinear. The linear method searches an affine space that best explains the variation of data distribution. In contrast, the nonlinear method is a better option for data that is distributed on a highly nonlinear curved surface. Here, we list some common linear and nonlinear methods.</p><p class="calibre7">Here are some common linear methods:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre2">PCA</strong></span>: Principal component analysis <a id="id867" class="calibre1"/>maps data to a lower dimension, so that the variance<a id="id868" class="calibre1"/> of the data in a low dimension representation is maximized.</li><li class="listitem"><span class="strong"><strong class="calibre2">MDS</strong></span>: Multidimensional scaling is a <a id="id869" class="calibre1"/>method that allows you to visualize how near (pattern proximities) objects are to each other and can produce a representation <a id="id870" class="calibre1"/>of your data with lower dimension space. PCA can be regarded as the simplest form of MDS if the distance measurement used in MDS equals the covariance of data.</li><li class="listitem"><span class="strong"><strong class="calibre2">SVD</strong></span>: Singular value decomposition<a id="id871" class="calibre1"/> removes redundant features that are linear correlated <a id="id872" class="calibre1"/>from the perspective of linear algebra. PCA can also be regarded as a specific case of SVD.</li></ul></div><p class="calibre7">Here are some common nonlinear methods:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre2">ISOMAP</strong></span>: ISOMAP<a id="id873" class="calibre1"/> can be viewed as an extension<a id="id874" class="calibre1"/> of MDS, which uses the distance metric of geodesic distances. In this method, geodesic distance is computed by graphing the shortest path distances.</li><li class="listitem"><span class="strong"><strong class="calibre2">LLE</strong></span>: Locally linear embedding<a id="id875" class="calibre1"/> performs local PCA and global eigen-decomposition. LLE<a id="id876" class="calibre1"/> is a local approach, which involves selecting features for each category of the class feature. In contrast, ISOMAP is a global approach, which involves selecting features for all features.</li></ul></div><p class="calibre7">In this chapter, we will first discuss how to perform feature ranking and selection. Next, we will focus on the topic of feature extraction and cover recipes in performing dimension reduction with both linear and nonlinear methods. For linear methods, we will introduce how to perform PCA, determine the number of principal components, and its visualization. We then move on to MDS and SVD. Furthermore, we will introduce the application of SVD to compress images. For nonlinear methods, we will introduce how to perform dimension reduction with ISOMAP and LLE.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Performing feature selection with FSelector"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec122" class="calibre1"/>Performing feature selection with FSelector</h1></div></div></div><p class="calibre7">The <code class="email">FSelector</code> package provides two approaches to select the most influential features from the original feature set. Firstly, rank features by some criteria and select the ones that are<a id="id877" class="calibre1"/> above a defined threshold. Secondly, search for optimum feature subsets from a space of feature subsets. In this<a id="id878" class="calibre1"/> recipe, we will introduce how to perform feature selection with the <code class="email">FSelector</code> package.</p></div>

<div class="book" title="Performing feature selection with FSelector">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch11lvl2sec421" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this recipe, we will continue to use the telecom <code class="email">churn</code> dataset as the input data source to train the support vector machine. For those who have not prepared the dataset, please refer to <a class="calibre1" title="Chapter 5. Classification (I) – Tree, Lazy, and Probabilistic" href="part0060_split_000.html#page">Chapter 5</a>, <span class="strong"><em class="calibre8">Classification (I) – Tree, Lazy, and Probabilistic</em></span>, for detailed information.</p></div></div>

<div class="book" title="Performing feature selection with FSelector">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch11lvl2sec422" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to perform feature selection on a <code class="email">churn</code> dataset:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, install and load the package, <code class="email">FSelector</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; install.packages("FSelector")</strong></span>
<span class="strong"><strong class="calibre2">&gt; library(FSelector)</strong></span>
</pre></div></li><li class="listitem" value="2">Then, we can use <code class="email">random.forest.importance</code> to calculate the weight for each attribute, where we set the importance type to 1:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; weights = random.forest.importance(churn~., trainset, importance.type = 1)</strong></span>
<span class="strong"><strong class="calibre2">&gt; print(weights)</strong></span>
<span class="strong"><strong class="calibre2">                              attr_importance</strong></span>
<span class="strong"><strong class="calibre2">international_plan                 96.3255882</strong></span>
<span class="strong"><strong class="calibre2">voice_mail_plan                    24.8921239</strong></span>
<span class="strong"><strong class="calibre2">number_vmail_messages              31.5420332</strong></span>
<span class="strong"><strong class="calibre2">total_day_minutes                  51.9365357</strong></span>
<span class="strong"><strong class="calibre2">total_day_calls                    -0.1766420</strong></span>
<span class="strong"><strong class="calibre2">total_day_charge                   53.7930096</strong></span>
<span class="strong"><strong class="calibre2">total_eve_minutes                  33.2006078</strong></span>
<span class="strong"><strong class="calibre2">total_eve_calls                    -2.2270323</strong></span>
<span class="strong"><strong class="calibre2">total_eve_charge                   32.4317375</strong></span>
<span class="strong"><strong class="calibre2">total_night_minutes                22.0888120</strong></span>
<span class="strong"><strong class="calibre2">total_night_calls                   0.3407087</strong></span>
<span class="strong"><strong class="calibre2">total_night_charge                 21.6368855</strong></span>
<span class="strong"><strong class="calibre2">total_intl_minutes                 32.4984413</strong></span>
<span class="strong"><strong class="calibre2">total_intl_calls                   51.1154046</strong></span>
<span class="strong"><strong class="calibre2">total_intl_charge                  32.4855194</strong></span>
<span class="strong"><strong class="calibre2">number_customer_service_calls     114.2566676</strong></span>
</pre></div></li><li class="listitem" value="3">Next, we<a id="id879" class="calibre1"/> can use<a id="id880" class="calibre1"/> the <code class="email">cutoff</code> function to obtain the attributes of the top five weights:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; subset = cutoff.k(weights, 5)</strong></span>
<span class="strong"><strong class="calibre2">&gt; f = as.simple.formula(subset, "Class")</strong></span>
<span class="strong"><strong class="calibre2">&gt; print(f)</strong></span>
<span class="strong"><strong class="calibre2">Class ~ number_customer_service_calls + international_plan + </strong></span>
<span class="strong"><strong class="calibre2">    total_day_charge + total_day_minutes + total_intl_calls</strong></span>
<span class="strong"><strong class="calibre2">&lt;environment: 0x00000000269a28e8&gt;</strong></span>
</pre></div></li><li class="listitem" value="4">Next, we can make an evaluator to select the feature subsets:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; evaluator = function(subset) {</strong></span>
<span class="strong"><strong class="calibre2">+   k = 5  </strong></span>
<span class="strong"><strong class="calibre2">+   set.seed(2)</strong></span>
<span class="strong"><strong class="calibre2">+   ind = sample(5, nrow(trainset), replace = TRUE)</strong></span>
<span class="strong"><strong class="calibre2">+   results = sapply(1:k, function(i) {</strong></span>
<span class="strong"><strong class="calibre2">+     train = trainset[ind ==i,]</strong></span>
<span class="strong"><strong class="calibre2">+     test  = trainset[ind !=i,]</strong></span>
<span class="strong"><strong class="calibre2">+     tree  = rpart(as.simple.formula(subset, "churn"), trainset)</strong></span>
<span class="strong"><strong class="calibre2">+     error.rate = sum(test$churn != predict(tree, test, type="class")) / nrow(test)</strong></span>
<span class="strong"><strong class="calibre2">+     return(1 - error.rate)</strong></span>
<span class="strong"><strong class="calibre2">+   })</strong></span>
<span class="strong"><strong class="calibre2">+   return(mean(results))</strong></span>
<span class="strong"><strong class="calibre2">+ }</strong></span>
</pre></div></li><li class="listitem" value="5">Finally, we can find the optimum feature subset using a hill climbing search:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; attr.subset = hill.climbing.search(names(trainset)[!names(trainset) %in% "churn"], evaluator)</strong></span>
<span class="strong"><strong class="calibre2">&gt; f = as.simple.formula(attr.subset, "churn")</strong></span>
<span class="strong"><strong class="calibre2">&gt; print(f)</strong></span>
<span class="strong"><strong class="calibre2">churn ~ international_plan + voice_mail_plan + number_vmail_messages + </strong></span>
<span class="strong"><strong class="calibre2">    total_day_minutes + total_day_calls + total_eve_minutes + </strong></span>
<span class="strong"><strong class="calibre2">    total_eve_charge + total_intl_minutes + total_intl_calls + </strong></span>
<span class="strong"><strong class="calibre2">    total_intl_charge + number_customer_service_calls</strong></span>
<span class="strong"><strong class="calibre2">&lt;environment: 0x000000002224d3d0&gt;</strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Performing feature selection with FSelector">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch11lvl2sec423" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">In this <a id="id881" class="calibre1"/>recipe, we present how to <a id="id882" class="calibre1"/>use the <code class="email">FSelector</code> package to select the most influential features. We first demonstrate how to use the feature ranking approach. In the feature ranking approach, the algorithm first employs a weight function to generate weights for each feature. Here, we use the random forest algorithm with the mean decrease in accuracy (where <code class="email">importance.type = 1</code>) as the importance measurement to gain the weights of each attribute. Besides the random forest algorithm, you can select other feature ranking algorithms (for example, <code class="email">chi.squared</code>, <code class="email">information.gain</code>) from the <code class="email">FSelector</code> package. Then, the process sorts attributes by their weight. At last, we can obtain the top five features from the sorted feature list with the <code class="email">cutoff</code> function. In this case, <code class="email">number_customer_service_calls</code>, <code class="email">international_plan, total_day_charge</code>, <code class="email">total_day_minutes</code>, and <code class="email">total_intl_calls</code> are the five most important features.</p><p class="calibre7">Next, we illustrate how to search for optimum feature subsets. First, we need to make a five-fold cross-validation function to evaluate the importance of feature subsets. Then, we use the hill climbing searching algorithm to find the optimum feature subsets from the original feature sets. Besides the hill-climbing method, one can select other feature selection algorithms (for example, <code class="email">forward.search</code>) from the <code class="email">FSelector</code> package. Lastly, we can find that <code class="email">international_plan + voice_mail_plan + number_vmail_messages + total_day_minutes + total_day_calls + total_eve_minutes + total_eve_charge + total_intl_minutes + total_intl_calls + total_intl_charge + number_customer_service_calls</code> are optimum feature subsets.</p></div></div>

<div class="book" title="Performing feature selection with FSelector">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch11lvl2sec424" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">You can also use the <code class="email">caret</code> package to perform feature selection. As we have discussed related recipes in the model assessment chapter, you can refer to <a class="calibre1" title="Chapter 7. Model Evaluation" href="part0083_split_000.html#page">Chapter 7</a>, <span class="strong"><em class="calibre8">Model Evaluation</em></span>, for more detailed information.</li><li class="listitem">For<a id="id883" class="calibre1"/> both feature<a id="id884" class="calibre1"/> ranking and optimum feature selection, you can explore the package, <code class="email">FSelector</code>, for more related functions:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; help(package="FSelector")</strong></span>
</pre></div></li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Performing dimension reduction with PCA"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec123" class="calibre1"/>Performing dimension reduction with PCA</h1></div></div></div><p class="calibre7">
<span class="strong"><strong class="calibre2">Principal component analysis</strong></span> (<span class="strong"><strong class="calibre2">PCA</strong></span>) is the most widely used linear method in dealing with dimension reduction problems. It is useful when data contains many features, and there is<a id="id885" class="calibre1"/> redundancy (correlation) within<a id="id886" class="calibre1"/> these features. To remove redundant features, PCA maps high dimension data into lower dimensions by reducing features into a smaller number of principal components that account for most of the variance of the original features. In this recipe, we will introduce how to perform dimension reduction with the PCA method.</p></div>

<div class="book" title="Performing dimension reduction with PCA">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch11lvl2sec425" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this recipe, we will use the <code class="email">swiss</code> dataset as our target to perform PCA. The <code class="email">swiss</code> dataset includes standardized fertility measures and socio-economic indicators from around the year 1888 for each of the 47 French-speaking provinces of Switzerland.</p></div></div>

<div class="book" title="Performing dimension reduction with PCA">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch11lvl2sec426" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to perform principal component analysis on the <code class="email">swiss</code> dataset:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, load the <code class="email">swiss</code> dataset:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; data(swiss)</strong></span>
</pre></div></li><li class="listitem" value="2">Exclude the first column of the <code class="email">swiss</code> data:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; swiss = swiss[,-1]</strong></span>
</pre></div></li><li class="listitem" value="3">You can then perform principal component analysis on the <code class="email">swiss</code> data:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; swiss.pca = prcomp(swiss,</strong></span>
<span class="strong"><strong class="calibre2">+ center = TRUE,</strong></span>
<span class="strong"><strong class="calibre2">+ scale  = TRUE)</strong></span>
<span class="strong"><strong class="calibre2">&gt; swiss.pca</strong></span>
<span class="strong"><strong class="calibre2">Standard deviations:</strong></span>
<span class="strong"><strong class="calibre2">[1] 1.6228065 1.0354873 0.9033447 0.5592765 0.4067472</strong></span>

<span class="strong"><strong class="calibre2">Rotation:</strong></span>
<span class="strong"><strong class="calibre2">                         PC1         PC2          PC3        PC4         PC5</strong></span>
<span class="strong"><strong class="calibre2">Agriculture      0.52396452 -0.25834215  0.003003672 -0.8090741  0.06411415</strong></span>
<span class="strong"><strong class="calibre2">Examination  -0.57185792 -0.01145981 -0.039840522 -0.4224580 -0.70198942</strong></span>
<span class="strong"><strong class="calibre2">Education       -0.49150243  0.19028476  0.539337412 -0.3321615  0.56656945</strong></span>
<span class="strong"><strong class="calibre2">Catholic            0.38530580  0.36956307  0.725888143 0.1007965 -0.42176895</strong></span>
<span class="strong"><strong class="calibre2">Infant.Mortality 0.09167606 0.87197641 -0.424976789 -0.2154928 0.06488642</strong></span>
</pre></div></li><li class="listitem" value="4">Obtain a summary from the PCA results:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; summary(swiss.pca)</strong></span>
<span class="strong"><strong class="calibre2">Importance of components:</strong></span>
<span class="strong"><strong class="calibre2">                          PC1    PC2    PC3     PC4     PC5</strong></span>
<span class="strong"><strong class="calibre2">Standard deviation     1.6228 1.0355 0.9033 0.55928 0.40675</strong></span>
<span class="strong"><strong class="calibre2">Proportion of Variance 0.5267 0.2145 0.1632 0.06256 0.03309</strong></span>
<span class="strong"><strong class="calibre2">Cumulative Proportion  0.5267 0.7411 0.9043 0.96691 1.00000</strong></span>
</pre></div></li><li class="listitem" value="5">Lastly, you can use the <code class="email">predict</code> function to output the value of the principal component with the first row of data:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; predict(swiss.pca, newdata=head(swiss, 1))</strong></span>
<span class="strong"><strong class="calibre2">                  PC1       PC2        PC3      PC4       PC5</strong></span>
<span class="strong"><strong class="calibre2">Courtelary -0.9390479 0.8047122 -0.8118681 1.000307 0.4618643</strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Performing dimension reduction with PCA">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch11lvl2sec427" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">Since <a id="id887" class="calibre1"/>the feature selection method may <a id="id888" class="calibre1"/>remove some correlated but informative features, you have to consider combining these correlated features into a single feature with the feature extraction method. PCA is one of the feature extraction methods, which performs orthogonal transformation to convert possibly correlated variables into principal components. Also, you can use these principal components to identify the directions of variance.</p><p class="calibre7">The process of PCA is carried on by the following steps: firstly, find the mean vector, <span class="strong"><img src="../images/00217.jpeg" alt="How it works..." class="calibre24"/></span>, where <span class="strong"><img src="../images/00218.jpeg" alt="How it works..." class="calibre24"/></span> indicates the data point, and <span class="strong"><em class="calibre8">n</em></span> denotes the number of points. Secondly, compute the covariance matrix by the equation, <span class="strong"><img src="../images/00219.jpeg" alt="How it works..." class="calibre24"/></span>. Thirdly, compute the eigenvectors,<span class="strong"><img src="../images/00220.jpeg" alt="How it works..." class="calibre24"/></span>, and the corresponding eigenvalues. In the fourth step, we rank and choose the top <span class="strong"><em class="calibre8">k</em></span> eigenvectors. In the <a id="id889" class="calibre1"/>fifth step, we construct a <span class="strong"><em class="calibre8">d x k</em></span> dimensional<a id="id890" class="calibre1"/> eigenvector matrix, U. Here, <span class="strong"><em class="calibre8">d</em></span> is the number of original dimensions and <span class="strong"><em class="calibre8">k</em></span> is the number of eigenvectors. Finally, we can transform data samples to a new subspace in the equation, <span class="strong"><img src="../images/00221.jpeg" alt="How it works..." class="calibre24"/></span>.</p><p class="calibre7">In the following figure, it is illustrated that we can use two principal components, <span class="strong"><img src="../images/00222.jpeg" alt="How it works..." class="calibre24"/></span>, and <span class="strong"><img src="../images/00223.jpeg" alt="How it works..." class="calibre24"/></span>, to transform the data point from a two-dimensional space to new two-dimensional subspace:</p><div class="mediaobject"><img src="../images/00224.jpeg" alt="How it works..." class="calibre9"/><div class="caption"><p class="calibre12">A sample illustration of PCA</p></div></div><p class="calibre10"> </p><p class="calibre7">In this recipe we use the <code class="email">prcomp</code> function from the <code class="email">stats</code> package to perform PCA on the <code class="email">swiss</code> dataset. First, we remove the standardized fertility measures and use the rest of the predictors as input to the function, <code class="email">prcomp</code>. In addition to this, we set <code class="email">swiss</code> as an input dataset; the variable should be shifted to the zero center by specifying <code class="email">center=TRUE</code>; scale variables into the unit variance with the option, <code class="email">scale=TRUE</code>, and store the <a id="id891" class="calibre1"/>output in the variable, <code class="email">swiss.pca</code>.</p><p class="calibre7">Then, as <a id="id892" class="calibre1"/>we print out the value stored in <code class="email">swiss.pca</code>, we can find the standard deviation and rotation of the principal component. The standard deviation indicates the square root of the eigenvalues of the covariance/correlation matrix. On the other hand, the rotation of the principal components shows the coefficient of the linear combination of the input features. For example, PC1 equals <span class="strong"><em class="calibre8">Agriculture * 0.524 + Examination * -0.572 + Education * -0.492 + Catholic* 0.385 + Infant.Mortality * 0.092</em></span>. Here, we can find that the attribute, <span class="strong"><em class="calibre8">Agriculture</em></span>, contributes the most for PC1, for it has the highest coefficient.</p><p class="calibre7">Additionally, we can use the <code class="email">summary</code> function to obtain the importance of components. The first row shows the standard deviation of each principal component, the second row shows the proportion of variance explained by each component, and the third row shows the cumulative proportion of the explained variance. Finally, you can use the <code class="email">predict</code> function to obtain principal components from the input features. Here, we input the first row of the dataset, and retrieve five principal components.</p></div></div>

<div class="book" title="Performing dimension reduction with PCA">
<div class="book" title="There's more..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch11lvl2sec428" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre7">Another principal component analysis function is <code class="email">princomp</code>. In this function, the calculation is performed by using eigen on a correlation or covariance matrix instead of a single value decomposition used in the <code class="email">prcomp</code> function. In general practice, using <code class="email">prcomp</code> is preferable; however, we cover how to use <code class="email">princomp</code> here:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, use <code class="email">princomp</code> to perform PCA:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; swiss.princomp = princomp(swiss,</strong></span>
<span class="strong"><strong class="calibre2">+ center = TRUE,</strong></span>
<span class="strong"><strong class="calibre2">+ scale  = TRUE)</strong></span>
<span class="strong"><strong class="calibre2">&gt; swiss.princomp</strong></span>
<span class="strong"><strong class="calibre2">Call:</strong></span>
<span class="strong"><strong class="calibre2">princomp(x = swiss, center = TRUE, scale = TRUE)</strong></span>

<span class="strong"><strong class="calibre2">Standard deviations:</strong></span>
<span class="strong"><strong class="calibre2">   Comp.1    Comp.2    Comp.3    Comp.4    Comp.5 </strong></span>
<span class="strong"><strong class="calibre2">42.896335 21.201887  7.587978  3.687888  2.721105 </strong></span>

<span class="strong"><strong class="calibre2"> 5 variables and 47 observations.</strong></span>
</pre></div></li><li class="listitem" value="2">You <a id="id893" class="calibre1"/>can then obtain the <a id="id894" class="calibre1"/>summary information:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; summary(swiss.princomp)</strong></span>
<span class="strong"><strong class="calibre2">Importance of components:</strong></span>
<span class="strong"><strong class="calibre2">                           Comp.1     Comp.2     Comp.3      Comp.4      Comp.5</strong></span>
<span class="strong"><strong class="calibre2">Standard deviation     42.8963346 21.2018868 7.58797830 3.687888330 2.721104713</strong></span>
<span class="strong"><strong class="calibre2">Proportion of Variance  0.7770024  0.1898152 0.02431275 0.005742983 0.003126601</strong></span>
<span class="strong"><strong class="calibre2">Cumulative Proportion   0.7770024  0.9668177 0.99113042 0.996873399 1.000000000</strong></span>
</pre></div></li><li class="listitem" value="3">You can use the <code class="email">predict</code> function to obtain principal components from the input features:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; predict(swiss.princomp, swiss[1,])</strong></span>
<span class="strong"><strong class="calibre2">              Comp.1    Comp.2   Comp.3   Comp.4   Comp.5</strong></span>
<span class="strong"><strong class="calibre2">Courtelary -38.95923 -20.40504 12.45808 4.713234 -1.46634</strong></span>
</pre></div></li></ol><div class="calibre14"/></div><p class="calibre7">In addition to the <code class="email">prcomp</code> and <code class="email">princomp</code> functions from the <code class="email">stats</code> package, you can use the <code class="email">principal</code> function from the <code class="email">psych</code> package:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, install and load the <code class="email">psych</code> package:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; install.packages("psych")</strong></span>
<span class="strong"><strong class="calibre2">&gt; install.packages("GPArotation")</strong></span>
<span class="strong"><strong class="calibre2">&gt; library(psych)</strong></span>
</pre></div></li><li class="listitem" value="2">You can then use the <code class="email">principal</code> function to retrieve the principal components:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; swiss.principal = principal(swiss, nfactors=5, rotate="none")</strong></span>
<span class="strong"><strong class="calibre2">&gt; swiss.principal</strong></span>
<span class="strong"><strong class="calibre2">Principal Components Analysis</strong></span>
<span class="strong"><strong class="calibre2">Call: principal(r = swiss, nfactors = 5, rotate = "none")</strong></span>
<span class="strong"><strong class="calibre2">Standardized loadings (pattern matrix) based upon correlation matrix</strong></span>
<span class="strong"><strong class="calibre2">                   PC1   PC2   PC3   PC4   PC5 h2       u2</strong></span>
<span class="strong"><strong class="calibre2">Agriculture      -0.85 -0.27  0.00  0.45 -0.03  1 -6.7e-16</strong></span>
<span class="strong"><strong class="calibre2">Examination       0.93 -0.01 -0.04  0.24  0.29  1  4.4e-16</strong></span>
<span class="strong"><strong class="calibre2">Education         0.80  0.20  0.49  0.19 -0.23  1  2.2e-16</strong></span>
<span class="strong"><strong class="calibre2">Catholic         -0.63  0.38  0.66 -0.06  0.17  1 -2.2e-16</strong></span>
<span class="strong"><strong class="calibre2">Infant.Mortality -0.15  0.90 -0.38  0.12 -0.03  1 -8.9e-16</strong></span>

<span class="strong"><strong class="calibre2">                       PC1  PC2  PC3  PC4  PC5</strong></span>
<span class="strong"><strong class="calibre2">SS loadings           2.63 1.07 0.82 0.31 0.17</strong></span>
<span class="strong"><strong class="calibre2">Proportion Var        0.53 0.21 0.16 0.06 0.03</strong></span>
<span class="strong"><strong class="calibre2">Cumulative Var        0.53 0.74 0.90 0.97 1.00</strong></span>
<span class="strong"><strong class="calibre2">Proportion Explained  0.53 0.21 0.16 0.06 0.03</strong></span>
<span class="strong"><strong class="calibre2">Cumulative Proportion 0.53 0.74 0.90 0.97 1.00</strong></span>

<span class="strong"><strong class="calibre2">Test of the hypothesis that 5 components are sufficient.</strong></span>

<span class="strong"><strong class="calibre2">The degrees of freedom for the null model are 10 and the objective function was 2.13</strong></span>
<span class="strong"><strong class="calibre2">The degrees of freedom for the model are -5  and the objective function was  0 </strong></span>
<span class="strong"><strong class="calibre2">The total number of observations was  47  with MLE Chi Square =  0  with prob &lt;  NA </strong></span>

<span class="strong"><strong class="calibre2">Fit based upon off diagonal values = 1</strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Determining the number of principal components using the scree test"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec124" class="calibre1"/>Determining the number of principal components using the scree test</h1></div></div></div><p class="calibre7">As we<a id="id895" class="calibre1"/> only need to retain the principal components that account for most of the variance of the original features, we can either use the Kaiser method, scree test, or the percentage of variation explained as the selection criteria. The main purpose of a scree test is to graph the component analysis results as a scree plot and find where the obvious change in the slope (elbow) occurs. In this recipe, we will demonstrate how to determine the number of principal components using a scree plot.</p></div>

<div class="book" title="Determining the number of principal components using the scree test">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch11lvl2sec429" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">Ensure that you have completed the previous recipe by generating a principal component object and save it in the variable, <code class="email">swiss.pca</code>.</p></div></div>

<div class="book" title="Determining the number of principal components using the scree test">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch11lvl2sec430" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform <a id="id896" class="calibre1"/>the following steps to determine the number of principal components with the scree plot:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, you can generate a bar plot by using <code class="email">screeplot</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; screeplot(swiss.pca, type="barplot")</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00225.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">The scree plot in bar plot form</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="2">You can also generate a line plot by using <code class="email">screeplot</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; screeplot(swiss.pca, type="line")</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00226.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">The scree plot in line plot form</p></div></div><p class="calibre13"> </p></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Determining the number of principal components using the scree test">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch11lvl2sec431" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">In this<a id="id897" class="calibre1"/> recipe, we demonstrate how to use a scree plot to determine the number of principal components. In a scree plot, there are two types of plots, namely, bar plots and line plots. As both generated scree plots reveal, the obvious change in slope (the so-called elbow or knee) occurs at component 2. As a result, we should retain component 1, where the component is in a steep curve before component 2, which is where the flat line trend commences. However, as this method can be ambiguous, you can use other methods (such as the Kaiser method) to determine the number of components.</p></div></div>

<div class="book" title="Determining the number of principal components using the scree test">
<div class="book" title="There's more..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch11lvl2sec432" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre7">By default, if you use the <code class="email">plot</code> function on a generated principal component object, you can also retrieve the scree plot. For more details on <code class="email">screeplot</code>, please refer to the following document:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; help(screeplot)</strong></span>
</pre></div><p class="calibre7">You can also use <code class="email">nfactors</code> to perform parallel analysis and nongraphical solutions to the Cattell scree test:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; install.packages("nFactors")</strong></span>
<span class="strong"><strong class="calibre2">&gt; library(nFactors)</strong></span>
<span class="strong"><strong class="calibre2">&gt; ev = eigen(cor(swiss))</strong></span>
<span class="strong"><strong class="calibre2">&gt; ap = parallel(subject=nrow(swiss),var=ncol(swiss),rep=100,cent=.05)</strong></span>
<span class="strong"><strong class="calibre2">&gt; nS = nScree(x=ev$values, aparallel=ap$eigen$qevpea)</strong></span>
<span class="strong"><strong class="calibre2">&gt; plotnScree(nS)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00227.jpeg" alt="There's more..." class="calibre9"/><div class="caption"><p class="calibre12">Non-graphical solution to scree test</p></div></div><p class="calibre10"> </p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Determining the number of principal components using the Kaiser method"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec125" class="calibre1"/>Determining the number of principal components using the Kaiser method</h1></div></div></div><p class="calibre7">In addition to the scree test, you can use the Kaiser method to determine the number of principal<a id="id898" class="calibre1"/> components. In this method, the selection criteria retains eigenvalues greater than <code class="email">1</code>. In this recipe, we will demonstrate how to determine the number of principal components using the Kaiser method.</p></div>

<div class="book" title="Determining the number of principal components using the Kaiser method">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch11lvl2sec433" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">Ensure that you have completed the previous recipe by generating a principal component object and save it in the variable, <code class="email">swiss.pca</code>.</p></div></div>

<div class="book" title="Determining the number of principal components using the Kaiser method">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch11lvl2sec434" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to determine the number of principal components with the Kaiser <a id="id899" class="calibre1"/>method:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, you can obtain the standard deviation from <code class="email">swiss.pca</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; swiss.pca$sdev </strong></span>
<span class="strong"><strong class="calibre2">[1] 1.6228065 1.0354873 0.9033447 0.5592765 0.4067472</strong></span>
</pre></div></li><li class="listitem" value="2">Next, you can obtain the variance from <code class="email">swiss.pca</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; swiss.pca$sdev ^ 2</strong></span>
<span class="strong"><strong class="calibre2">[1] 2.6335008 1.0722340 0.8160316 0.3127902 0.1654433</strong></span>
</pre></div></li><li class="listitem" value="3">Select components with a variance above 1:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; which(swiss.pca$sdev ^ 2&gt; 1)</strong></span>
<span class="strong"><strong class="calibre2">[1] 1 2</strong></span>
</pre></div></li><li class="listitem" value="4">You can also use the scree plot to select components with a variance above 1:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; screeplot(swiss.pca, type="line")</strong></span>
<span class="strong"><strong class="calibre2">&gt; abline(h=1, col="red", lty= 3)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00228.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">Select component with variance above 1 </p></div></div><p class="calibre13"> </p></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Determining the number of principal components using the Kaiser method">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch11lvl2sec435" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">You can also use the Kaiser method to determine the number of components. As the computed principal component object contains the standard deviation of each component, we can<a id="id900" class="calibre1"/> compute the variance as the standard deviation, which is the square root of variance. From the computed variance, we find both component 1 and 2 have a variance above 1. Therefore, we can determine the number of principal components as 2 (both component 1 and 2). Also, we can draw a red line on the scree plot (as shown in the preceding figure) to indicate that we need to retain component 1 and 2 in this case.</p></div></div>

<div class="book" title="Determining the number of principal components using the Kaiser method">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch11lvl2sec436" class="calibre1"/>See also</h2></div></div></div><p class="calibre7">In order to determine which principal components to retain, please refer to:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Ledesma, R. D., and Valero-Mora, P. (2007). <span class="strong"><em class="calibre8">Determining the Number of Factors to Retain in EFA: an easy-to-use computer program for carrying out Parallel Analysis</em></span>. <span class="strong"><em class="calibre8">Practical Assessment, Research &amp; Evaluation</em></span>, 12(2), 1-11.</li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Visualizing multivariate data using biplot"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec126" class="calibre1"/>Visualizing multivariate data using biplot</h1></div></div></div><p class="calibre7">In order<a id="id901" class="calibre1"/> to find out how data and variables are<a id="id902" class="calibre1"/> mapped in regard to the principal component, you can use <code class="email">biplot</code>, which plots data and the projections of original features on to the first two components. In this recipe, we will demonstrate how to use <code class="email">biplot</code> to plot both variables and data on the same figure.</p></div>

<div class="book" title="Visualizing multivariate data using biplot">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch11lvl2sec437" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">Ensure that you have completed the previous recipe by generating a principal component object and save it in the variable, <code class="email">swiss.pca</code>.</p></div></div>

<div class="book" title="Visualizing multivariate data using biplot">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch11lvl2sec438" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to create a biplot:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">You can create a scatter plot using component 1 and 2:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt;  plot(swiss.pca$x[,1], swiss.pca$x[,2], xlim=c(-4,4))</strong></span>
<span class="strong"><strong class="calibre2">&gt; text(swiss.pca$x[,1], swiss.pca$x[,2], rownames(swiss.pca$x), cex=0.7, pos=4, col="red")</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00229.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">The scatter plot of first two components from PCA result</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="2">If <a id="id903" class="calibre1"/>you would like to add features <a id="id904" class="calibre1"/>on the plot, you can create biplot using the generated principal component object:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; biplot(swiss.pca)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00230.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">The biplot using PCA result</p></div></div><p class="calibre13"> </p></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Visualizing multivariate data using biplot">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch11lvl2sec439" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">In this<a id="id905" class="calibre1"/> recipe, we demonstrate how to use <code class="email">biplot</code> to <a id="id906" class="calibre1"/>plot data and projections of original features on to the first two components. In the first step, we demonstrate that we can actually use the first two components to create a scatter plot. Furthermore, if you want to add variables on the same plot, you can use <code class="email">biplot</code>. In <code class="email">biplot</code>, you can see the provinces with higher indicators in the agriculture variable, lower indicators in the education variable, and examination variables scores that are higher in PC1. On the other hand, the provinces with higher infant mortality indicators and lower agriculture indicators score higher in PC2.</p></div></div>

<div class="book" title="Visualizing multivariate data using biplot">
<div class="book" title="There's more..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch11lvl2sec440" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre7">Besides <code class="email">biplot</code> in the <code class="email">stats</code> package, you can also use <code class="email">ggbiplot</code>. However, you may not find this package from CRAN; you have to first install <code class="email">devtools</code> and then install <code class="email">ggbiplot</code> from GitHub:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; install.packages("devtools")</strong></span>

<span class="strong"><strong class="calibre2">&gt; library(ggbiplot)</strong></span>
<span class="strong"><strong class="calibre2">&gt; g = ggbiplot(swiss.pca, obs.scale = 1, var.scale = 1, </strong></span>
<span class="strong"><strong class="calibre2">+ ellipse = TRUE, </strong></span>
<span class="strong"><strong class="calibre2">+ circle = TRUE)</strong></span>
<span class="strong"><strong class="calibre2">&gt; print(g)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00231.jpeg" alt="There's more..." class="calibre9"/><div class="caption"><p class="calibre12">The ggbiplot using PCA result</p></div></div><p class="calibre10"> </p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Performing dimension reduction with MDS"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec127" class="calibre1"/>Performing dimension reduction with MDS</h1></div></div></div><p class="calibre7">
<span class="strong"><strong class="calibre2">Multidimensional scaling</strong></span> (<span class="strong"><strong class="calibre2">MDS</strong></span>) is a technique to create a visual presentation of similarities <a id="id907" class="calibre1"/>or dissimilarities (distance) of a number of objects. The <span class="strong"><em class="calibre8">multi</em></span> prefix indicates that one can create a presentation <a id="id908" class="calibre1"/>map in one, two, or more dimensions. However, we most often use MDS to present the distance between data points in one or two dimensions.</p><p class="calibre7">In MDS, you can either use a metric or a nonmetric solution. The main difference between the two solutions is that metric solutions try to reproduce the original metric, while nonmetric solutions assume that the ranks of the distance are known. In this recipe, we will illustrate how to perform MDS on the <code class="email">swiss</code> dataset.</p></div>

<div class="book" title="Performing dimension reduction with MDS">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch11lvl2sec441" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this recipe, we will continue using the <code class="email">swiss</code> dataset as our input data source.</p></div></div>

<div class="book" title="Performing dimension reduction with MDS">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch11lvl2sec442" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform<a id="id909" class="calibre1"/> the following steps to perform<a id="id910" class="calibre1"/> multidimensional scaling using the metric method:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, you can perform metric MDS with a maximum of two dimensions:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; swiss.dist =dist(swiss)</strong></span>
<span class="strong"><strong class="calibre2">&gt; swiss.mds = cmdscale(swiss.dist, k=2)</strong></span>
</pre></div></li><li class="listitem" value="2">You can then plot the <code class="email">swiss</code> data in a two-dimension scatter plot:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; plot(swiss.mds[,1], swiss.mds[,2], type = "n", main = "cmdscale (stats)")</strong></span>
<span class="strong"><strong class="calibre2">&gt; text(swiss.mds[,1], swiss.mds[,2], rownames(swiss), cex = 0.9, xpd = TRUE)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00232.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">The 2-dimension scatter plot from cmdscale object</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="3">In addition, you can perform nonmetric MDS with <code class="email">isoMDS</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; library(MASS)</strong></span>
<span class="strong"><strong class="calibre2">&gt; swiss.nmmds = isoMDS(swiss.dist, k=2)</strong></span>
<span class="strong"><strong class="calibre2">initial  value 2.979731 </strong></span>
<span class="strong"><strong class="calibre2">iter   5 value 2.431486</strong></span>
<span class="strong"><strong class="calibre2">iter  10 value 2.343353</strong></span>
<span class="strong"><strong class="calibre2">final  value 2.338839 </strong></span>
<span class="strong"><strong class="calibre2">converged</strong></span>
</pre></div></li><li class="listitem" value="4">You <a id="id911" class="calibre1"/>can also plot the <a id="id912" class="calibre1"/>data points in a two-dimension scatter plot:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; plot(swiss.nmmds$points, type = "n", main = "isoMDS (MASS)")</strong></span>
<span class="strong"><strong class="calibre2">&gt; text(swiss.nmmds$points, rownames(swiss), cex = 0.9, xpd = TRUE)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00233.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">The 2-dimension scatter plot from isoMDS object</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="5">You can then plot the data points in a two-dimension scatter plot:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; swiss.sh = Shepard(swiss.dist, swiss.mds)</strong></span>
<span class="strong"><strong class="calibre2">&gt; plot(swiss.sh, pch = ".")</strong></span>
<span class="strong"><strong class="calibre2">&gt; lines(swiss.sh$x, swiss.sh$yf, type = "S")</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00234.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">The Shepard plot from isoMDS object</p></div></div><p class="calibre13"> </p></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Performing dimension reduction with MDS">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch11lvl2sec443" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">MDS reveals the structure of the data by providing a visual presentation of similarities among a<a id="id913" class="calibre1"/> set of objects. In more detail, MDS places an object in an n-dimensional space, where the distances between pairs of <a id="id914" class="calibre1"/>points corresponds to the similarities among the pairs of objects. Usually, the dimensional space is a two-dimensional Euclidean space, but it may be non-Euclidean and have more than two dimensions. In accordance with the meaning of the input matrix, MDS can be mainly categorized into two types: metric MDS, where the input matrix is metric-based, nonmetric MDS, where the input matrix is nonmetric-based.</p><p class="calibre7">Metric MDS is also known as principal coordinate analysis, which first transforms a distance into similarities. In the simplest form, the process linearly projects original data points to a subspace by performing principal components analysis on similarities. On the other hand, the process can also perform a nonlinear projection on similarities by minimizing the stress value, <span class="strong"><img src="../images/00235.jpeg" alt="How it works..." class="calibre24"/></span>, where <span class="strong"><img src="../images/00236.jpeg" alt="How it works..." class="calibre24"/></span> is the distance measurement between the two points, <span class="strong"><img src="../images/00237.jpeg" alt="How it works..." class="calibre24"/></span> and <span class="strong"><img src="../images/00238.jpeg" alt="How it works..." class="calibre24"/></span>, and <span class="strong"><img src="../images/00239.jpeg" alt="How it works..." class="calibre24"/></span> is the similarity measure of two projected points, <span class="strong"><img src="../images/00240.jpeg" alt="How it works..." class="calibre24"/></span> and <span class="strong"><img src="../images/00241.jpeg" alt="How it works..." class="calibre24"/></span>. As a result, we can represent the relationship among objects in the Euclidean space.</p><p class="calibre7">In <a id="id915" class="calibre1"/>contrast to metric MDS, which<a id="id916" class="calibre1"/> use a metric-based input matrix, a nonmetric-based MDS is used when the data is measured at the ordinal level. As only the rank order of the distances between the vectors is meaningful, nonmetric MDS applies a monotonically increasing function, f, on the original distances and projects the distance to new values that preserve the rank order. The normalized equation can be formulated as <span class="strong"><img src="../images/00242.jpeg" alt="How it works..." class="calibre24"/></span>.</p><p class="calibre7">In this recipe, we illustrate how to perform metric and nonmetric MDS on the <code class="email">swiss</code> dataset. To perform metric MDS, we first need to obtain the distance metric from the <code class="email">swiss</code> data. In this step, you can replace the distance measure to any measure as long as it produces a similarity/dissimilarity measure of data points. You can use <code class="email">cmdscale</code> to perform metric multidimensional scaling. Here, we specify <code class="email">k = 2</code>, so the maximum generated dimensions equals <code class="email">2</code>. You can also visually present the distance of the data points on a two-dimensional scatter plot.</p><p class="calibre7">Next, you can perform nonmetric MDS with <code class="email">isoMDS</code>. In nonmetric MDS, we do not match the distances, but only arrange them in order. We also set <code class="email">swiss</code> as an input dataset with maximum dimensions of two. Similar to the metric MDS example, we can plot the distance between data points on a two-dimensional scatter plot. Then, we use a Shepard plot, which shows how well the projected distances match those in the distance matrix. As per the figure in step 4, the projected distance matches well in the distance matrix.</p></div></div>

<div class="book" title="Performing dimension reduction with MDS">
<div class="book" title="There's more..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch11lvl2sec444" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre7">Another visualization method is to present an MDS object as a graph. A sample code is listed here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; library(igraph)</strong></span>
<span class="strong"><strong class="calibre2">&gt; swiss.sample = swiss[1:10,]</strong></span>

<span class="strong"><strong class="calibre2">&gt; g = graph.full(nrow(swiss.sample))</strong></span>
<span class="strong"><strong class="calibre2">&gt; V(g)$label = rownames(swiss.sample)</strong></span>
<span class="strong"><strong class="calibre2">&gt; layout = layout.mds(g, dist = as.matrix(dist(swiss.sample)))</strong></span>
<span class="strong"><strong class="calibre2">&gt; plot(g, layout = layout, vertex.size = 3)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00243.jpeg" alt="There's more..." class="calibre9"/><div class="caption"><p class="calibre12">The graph presentation of MDS object</p></div></div><p class="calibre10"> </p><p class="calibre7">You <a id="id917" class="calibre1"/>can also compare differences<a id="id918" class="calibre1"/> between the generated results from MDS and PCA. You can compare their differences by drawing the projected dimensions on the same scatter plot. If you use a Euclidean distance on MDS, the projected dimensions are exactly the same as the ones projected from PCA:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; swiss.dist = dist(swiss)</strong></span>
<span class="strong"><strong class="calibre2">&gt; swiss.mds = cmdscale(swiss.dist, k=2)</strong></span>
<span class="strong"><strong class="calibre2">&gt; plot(swiss.mds[,1], swiss.mds[,2], type="n")</strong></span>
<span class="strong"><strong class="calibre2">&gt; text(swiss.mds[,1], swiss.mds[,2], rownames(swiss), cex = 0.9, xpd = TRUE)</strong></span>
<span class="strong"><strong class="calibre2">&gt; swiss.pca = prcomp(swiss)</strong></span>
<span class="strong"><strong class="calibre2">&gt; text(-swiss.pca$x[,1],-swiss.pca$x[,2], rownames(swiss), </strong></span>
<span class="strong"><strong class="calibre2">+      ,col="blue", adj = c(0.2,-0.5),cex = 0.9, xpd = TRUE)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00244.jpeg" alt="There's more..." class="calibre9"/><div class="caption"><p class="calibre12">The comparison between MDS and PCA</p></div></div><p class="calibre10"> </p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Reducing dimensions with SVD"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec128" class="calibre1"/>Reducing dimensions with SVD</h1></div></div></div><p class="calibre7">
<span class="strong"><strong class="calibre2">Singular value decomposition</strong></span> (<span class="strong"><strong class="calibre2">SVD</strong></span>) is a type of matrix factorization (decomposition), which can <a id="id919" class="calibre1"/>factorize matrices into two orthogonal matrices and diagonal<a id="id920" class="calibre1"/> matrices. You can multiply the <a id="id921" class="calibre1"/>original matrix back using these three matrices. SVD can reduce redundant data that is linear dependent from the perspective of linear algebra. Therefore, it can be applied to feature selection, image processing, clustering, and many other fields. In this recipe, we will illustrate how to perform dimension reduction with SVD.</p></div>

<div class="book" title="Reducing dimensions with SVD">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch11lvl2sec445" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this recipe, we will continue using the dataset, <code class="email">swiss</code>, as our input data source.</p></div></div>

<div class="book" title="Reducing dimensions with SVD">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch11lvl2sec446" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to perform dimension reduction using SVD:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, you can perform <code class="email">svd</code> on the <code class="email">swiss</code> dataset:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; swiss.svd = svd(swiss)</strong></span>
</pre></div></li><li class="listitem" value="2">You can then plot the percentage of variance explained and the cumulative variance <a id="id922" class="calibre1"/>explained in accordance<a id="id923" class="calibre1"/> with the SVD column:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; plot(swiss.svd$d^2/sum(swiss.svd$d^2), type="l", xlab=" Singular vector", ylab = "Variance explained")</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00245.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">The percent of variance explained</p></div></div><p class="calibre13"> </p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; plot(cumsum(swiss.svd$d^2/sum(swiss.svd$d^2)), type="l", xlab="Singular vector", ylab = "Cumulative percent of variance explained")</strong></span>
</pre></div><p class="calibre26"> </p><div class="mediaobject"><img src="../images/00246.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">Cumulative percent of variance explained</p></div></div><p class="calibre13"> </p><p class="calibre26">
</p></li><li class="listitem" value="3">Next, you<a id="id924" class="calibre1"/> can reconstruct<a id="id925" class="calibre1"/> the data with only one singular vector:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; swiss.recon = swiss.svd$u[,1] %*% diag(swiss.svd$d[1], length(1), length(1)) %*% t(swiss.svd$v[,1])</strong></span>
</pre></div></li><li class="listitem" value="4">Lastly, you can compare the original dataset with the constructed dataset in an image:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; par(mfrow=c(1,2))</strong></span>
<span class="strong"><strong class="calibre2">&gt; image(as.matrix(swiss), main="swiss data Image")</strong></span>
<span class="strong"><strong class="calibre2">&gt; image(swiss.recon,  main="Reconstructed Image")</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00247.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">The comparison between original dataset and re-constructed dataset</p></div></div><p class="calibre13"> </p></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Reducing dimensions with SVD">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch11lvl2sec447" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">SVD is a <a id="id926" class="calibre1"/>factorization of a real or complex <a id="id927" class="calibre1"/>matrix. In detail, the SVD of m x n matrix, A, is the factorization of A into the product of three matrices, <span class="strong"><img src="../images/00248.jpeg" alt="How it works..." class="calibre24"/></span>. Here, U is an m x m orthonormal matrix, D has singular values and is an m x n diagonal matrix, and V<sup class="calibre27">T</sup> is an n x n orthonormal matrix.</p><p class="calibre7">In this recipe, we demonstrate how to perform dimension reduction with SVD. First, you can apply the <code class="email">svd</code> function on the <code class="email">swiss</code> dataset to obtain factorized matrices. You can then generate two plots: one shows the variance explained in accordance to a singular vector, the other shows the cumulative variance explained in accordance to a singular vector.</p><p class="calibre7">The preceding figure shows that the first singular vector can explain 80 percent of variance. We now want to compare the differences from the original dataset and the reconstructed dataset with a single singular vector. We, therefore, reconstruct the data with a<a id="id928" class="calibre1"/> single singular vector and use the <code class="email">image</code> function to present the original and reconstructed datasets side-by-side and see how <a id="id929" class="calibre1"/>they differ from each other. The next figure reveals that these two images are very similar.</p></div></div>

<div class="book" title="Reducing dimensions with SVD">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch11lvl2sec448" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">As we mentioned earlier, PCA can be regarded as a specific case of SVD. Here, we generate the orthogonal vector from the <code class="email">swiss</code> data from SVD and obtained the rotation from <code class="email">prcomp</code>. We can see that the two generated matrices are the same:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; svd.m = svd(scale(swiss))</strong></span>
<span class="strong"><strong class="calibre2">&gt; svd.m$v</strong></span>
<span class="strong"><strong class="calibre2">            [,1]        [,2]         [,3]       [,4]        [,5]</strong></span>
<span class="strong"><strong class="calibre2">[1,]  0.52396452 -0.25834215  0.003003672 -0.8090741  0.06411415</strong></span>
<span class="strong"><strong class="calibre2">[2,] -0.57185792 -0.01145981 -0.039840522 -0.4224580 -0.70198942</strong></span>
<span class="strong"><strong class="calibre2">[3,] -0.49150243  0.19028476  0.539337412 -0.3321615  0.56656945</strong></span>
<span class="strong"><strong class="calibre2">[4,]  0.38530580  0.36956307  0.725888143  0.1007965 -0.42176895</strong></span>
<span class="strong"><strong class="calibre2">[5,]  0.09167606  0.87197641 -0.424976789 -0.2154928  0.06488642</strong></span>
<span class="strong"><strong class="calibre2">&gt; pca.m = prcomp(swiss,scale=TRUE)</strong></span>
<span class="strong"><strong class="calibre2">&gt; pca.m$rotation</strong></span>
<span class="strong"><strong class="calibre2">                         PC1         PC2          PC3        PC4         PC5</strong></span>
<span class="strong"><strong class="calibre2">Agriculture      0.52396452 -0.25834215  0.003003672 -0.8090741  0.06411415</strong></span>
<span class="strong"><strong class="calibre2">Examination  -0.57185792 -0.01145981 -0.039840522 -0.4224580 -0.70198942</strong></span>
<span class="strong"><strong class="calibre2">Education       -0.49150243  0.19028476  0.539337412 -0.3321615  0.56656945</strong></span>
<span class="strong"><strong class="calibre2">Catholic          0.38530580  0.36956307  0.725888143  0.1007965 -0.42176895</strong></span>
<span class="strong"><strong class="calibre2">Infant.Mortality 0.09167606 0.87197641 -0.424976789 -0.2154928 0.06488642</strong></span>
</pre></div></li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Compressing images with SVD"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec129" class="calibre1"/>Compressing images with SVD</h1></div></div></div><p class="calibre7">In the previous recipe, we demonstrated how to factorize a matrix with SVD and then reconstruct the dataset by multiplying the decomposed matrix. Furthermore, the application of matrix <a id="id930" class="calibre1"/>factorization can be applied to image <a id="id931" class="calibre1"/>compression. In this recipe, we will demonstrate how to perform SVD on the classic image processing material, Lenna.</p></div>

<div class="book" title="Compressing images with SVD">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch11lvl2sec449" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this recipe, you should download the image of Lenna beforehand (refer to <a class="calibre1" href="http://www.ece.rice.edu/~wakin/images/lena512.bmp">http://www.ece.rice.edu/~wakin/images/lena512.bmp</a> for this), or you can prepare an image of your own to see how image compression works.</p></div></div>

<div class="book" title="Compressing images with SVD">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch11lvl2sec450" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to compress an image with SVD:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, install and load <code class="email">bmp</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; install.packages("bmp")</strong></span>
<span class="strong"><strong class="calibre2">&gt; library(bmp)</strong></span>
</pre></div></li><li class="listitem" value="2">You can then read the image of Lenna as a numeric matrix with the <code class="email">read.bmp</code> function. When the reader downloads the image, the default name is <code class="email">lena512.bmp</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; lenna = read.bmp("lena512.bmp")</strong></span>
</pre></div></li><li class="listitem" value="3">Rotate and plot the image:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; lenna = t(lenna)[,nrow(lenna):1]</strong></span>
<span class="strong"><strong class="calibre2">&gt; image(lenna) </strong></span>
</pre></div><div class="mediaobject"><img src="../images/00249.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">The picture of Lenna</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="4">Next, you <a id="id932" class="calibre1"/>can perform SVD on the read <a id="id933" class="calibre1"/>numeric matrix and plot the percentage of variance explained:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; lenna.svd = svd(scale(lenna))</strong></span>
<span class="strong"><strong class="calibre2">&gt; plot(lenna.svd$d^2/sum(lenna.svd$d^2), type="l", xlab=" Singular vector", ylab = "Variance explained")</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00250.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">The percentage of variance explained</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="5">Next, you <a id="id934" class="calibre1"/>can obtain the number of <a id="id935" class="calibre1"/>dimensions to reconstruct the image:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; length(lenna.svd$d)</strong></span>
<span class="strong"><strong class="calibre2">[1] 512</strong></span>
</pre></div></li><li class="listitem" value="6">Obtain the point at which the singular vector can explain more than 90 percent of the variance:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; min(which(cumsum(lenna.svd$d^2/sum(lenna.svd$d^2))&gt; 0.9))</strong></span>
<span class="strong"><strong class="calibre2">[1] 18</strong></span>
</pre></div></li><li class="listitem" value="7">You can also wrap the code into a function, <code class="email">lenna_compression</code>, and you can then use this function to plot compressed Lenna:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; lenna_compression = function(dim){</strong></span>
<span class="strong"><strong class="calibre2">+     u=as.matrix(lenna.svd$u[, 1:dim])</strong></span>
<span class="strong"><strong class="calibre2">+     v=as.matrix(lenna.svd$v[, 1:dim])</strong></span>
<span class="strong"><strong class="calibre2">+     d=as.matrix(diag(lenna.svd$d)[1:dim, 1:dim])</strong></span>
<span class="strong"><strong class="calibre2">+     image(u%*%d%*%t(v))</strong></span>
<span class="strong"><strong class="calibre2">+ }</strong></span>
</pre></div></li><li class="listitem" value="8">Also, you can use 18 vectors to reconstruct the image:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; lenna_compression(18)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00251.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">The reconstructed image with 18 components</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="9">You<a id="id936" class="calibre1"/> can obtain the point at which <a id="id937" class="calibre1"/>the singular vector can explain more than 99 percent of the variance;<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; min(which(cumsum(lenna.svd$d^2/sum(lenna.svd$d^2))&gt; 0.99))</strong></span>
<span class="strong"><strong class="calibre2">[1] 92</strong></span>
<span class="strong"><strong class="calibre2">&gt; lenna_compression(92)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00252.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">The reconstructed image with 92 components</p></div></div><p class="calibre13"> </p></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Compressing images with SVD">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch11lvl2sec451" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">In this<a id="id938" class="calibre1"/> recipe, we demonstrate how to compress<a id="id939" class="calibre1"/> an image with SVD. In the first step, we use the package, <code class="email">bmp</code>, to load the image, Lenna, to an R session. Then, as the read image is rotated, we can rotate the image back and use the <code class="email">plot</code> function to plot Lenna in R (as shown in the figure in step 3). Next, we perform SVD on the image matrix to factorize the matrix. We then plot the percentage of variance explained in regard to the number of singular vectors.</p><p class="calibre7">Further, as we discover that we can use 18 components to explain 90 percent of the variance, we then use these 18 components to reconstruct Lenna. Thus, we make a function named <code class="email">lenna_compression</code> with the purpose of reconstructing the image by matrix multiplication. As a result, we enter 18 as the input to the function, which returns a rather blurry Lenna image (as shown in the figure in step 8). However, we can at least see an outline of the image. To obtain a clearer picture, we discover that we can use 92 components to explain 99 percent of the variance. We, therefore, set the input to the function, <code class="email">lenna_compression</code>, as 92. The figure in step 9 shows that this generates a clearer <a id="id940" class="calibre1"/>picture than the one constructed using <a id="id941" class="calibre1"/>merely 18 components.</p></div></div>

<div class="book" title="Compressing images with SVD">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch11lvl2sec452" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">The Lenna picture is one of the most widely used standard test images for compression algorithms. For more details on the Lenna picture, please refer to <a class="calibre1" href="http://www.cs.cmu.edu/~chuck/lennapg/">http://www.cs.cmu.edu/~chuck/lennapg/</a>.</li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Performing nonlinear dimension reduction with ISOMAP"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec130" class="calibre1"/>Performing nonlinear dimension reduction with ISOMAP</h1></div></div></div><p class="calibre7">ISOMAP is one of the approaches for manifold learning, which generalizes linear framework<a id="id942" class="calibre1"/> to nonlinear<a id="id943" class="calibre1"/> data structures. Similar to MDS, ISOMAP creates a visual presentation of similarities or dissimilarities (distance) of a number of objects. However, as the data is structured in a nonlinear format, the Euclidian distance measure of MDS is replaced by the geodesic distance of a data manifold in ISOMAP. In this recipe, we will illustrate how to perform a nonlinear dimension reduction with ISOMAP.</p></div>

<div class="book" title="Performing nonlinear dimension reduction with ISOMAP">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch11lvl2sec453" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this recipe, we will use the <code class="email">digits</code> data from <code class="email">RnavGraphImageData</code> as our input source.</p></div></div>

<div class="book" title="Performing nonlinear dimension reduction with ISOMAP">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch11lvl2sec454" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to perform nonlinear dimension reduction with ISOMAP:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, install and load the <code class="email">RnavGraphImageData</code> and <code class="email">vegan</code> packages:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; install.packages("RnavGraphImageData")</strong></span>
<span class="strong"><strong class="calibre2">&gt; install.packages("vegan")</strong></span>
<span class="strong"><strong class="calibre2">&gt; library(RnavGraphImageData)</strong></span>
<span class="strong"><strong class="calibre2">&gt; library(vegan)</strong></span>
</pre></div></li><li class="listitem" value="2">You can then load the dataset, <code class="email">digits</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; data(digits)</strong></span>
</pre></div></li><li class="listitem" value="3">Rotate and plot the image:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; sample.digit = matrix(digits[,3000],ncol = 16, byrow=FALSE)</strong></span>
<span class="strong"><strong class="calibre2">&gt; image(t(sample.digit)[,nrow(sample.digit):1])</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00253.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">A sample image from the digits dataset</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="4">Next, you<a id="id944" class="calibre1"/> can <a id="id945" class="calibre1"/>randomly sample 300 digits from the population:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; set.seed(2)</strong></span>
<span class="strong"><strong class="calibre2">&gt; digit.idx = sample(1:ncol(digits),size = 600)</strong></span>
<span class="strong"><strong class="calibre2">&gt; digit.select = digits[,digit.idx]</strong></span>
</pre></div></li><li class="listitem" value="5">Transpose the selected digit data and then compute the dissimilarity between objects using <code class="email">vegdist</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; digits.Transpose = t(digit.select)</strong></span>
<span class="strong"><strong class="calibre2">&gt; digit.dist = vegdist(digits.Transpose, method="euclidean")</strong></span>
</pre></div></li><li class="listitem" value="6">Next, you can use <code class="email">isomap</code> to perform dimension reduction:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; digit.isomap = isomap(digit.dist,k = 8, ndim=6, fragmentedOK = TRUE)</strong></span>
<span class="strong"><strong class="calibre2">&gt; plot(digit.isomap)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00254.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">A 2-dimension scatter plot from ISOMAP object</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="7">Finally, you <a id="id946" class="calibre1"/>can <a id="id947" class="calibre1"/>overlay the scatter plot with the minimum spanning tree, marked in red;<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; digit.st = spantree(digit.dist)</strong></span>
<span class="strong"><strong class="calibre2">&gt; digit.plot = plot(digit.isomap, main="isomap k=8")</strong></span>
<span class="strong"><strong class="calibre2">&gt; lines(digit.st, digit.plot, col="red")</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00255.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">A 2-dimension scatter plot overlay with minimum spanning tree</p></div></div><p class="calibre13"> </p></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Performing nonlinear dimension reduction with ISOMAP">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch11lvl2sec455" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">ISOMAP is a nonlinear dimension reduction method and a representative of isometric mapping<a id="id948" class="calibre1"/> methods. ISOMAP <a id="id949" class="calibre1"/>can be regarded as an extension of the metric MDS, where pairwise the Euclidean distance among data points is replaced by geodesic distances induced by a neighborhood graph.</p><p class="calibre7">The description of the ISOMAP algorithm is shown in four steps. First, determine the neighbor of each point. Secondly, construct a neighborhood graph. Thirdly, compute the shortest distance path between two nodes. At last, find a low dimension embedding of the data by performing MDS.</p><p class="calibre7">In this recipe, we demonstrate how to perform a nonlinear dimension reduction using ISOMAP. First, we load the digits data from <code class="email">RnavGraphImageData</code>. Then, after we select one digit and plot its rotated image, we can see an image of the handwritten digit (the numeral 3, in the figure in step 3).</p><p class="calibre7">Next, we randomly sample 300 digits as our input data to ISOMAP. We then transpose the dataset to calculate the distance between each image object. Once the data is ready, we calculate the distance between each object and perform a dimension reduction. Here, we use <code class="email">vegdist</code> to calculate the dissimilarities between each object using a Euclidean measure. We then use ISOMAP to perform a nonlinear dimension reduction on the <code class="email">digits</code> data with the dimension set as <code class="email">6</code>, number of shortest dissimilarities retained for a point as <code class="email">8</code>, and ensure that you analyze the largest connected group by specifying <code class="email">fragmentedOK</code> as <code class="email">TRUE</code>.</p><p class="calibre7">Finally, we <a id="id950" class="calibre1"/>can use the <a id="id951" class="calibre1"/>generated ISOMAP object to make a two-dimension scatter plot (figure in step 6), and also overlay the minimum spanning tree with lines in red on the scatter plot (figure in step 7).</p></div></div>

<div class="book" title="Performing nonlinear dimension reduction with ISOMAP">
<div class="book" title="There's more..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch11lvl2sec456" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre7">You can also use the <code class="email">RnavGraph</code> package to visualize high dimensional data (digits in this case) using <a id="id952" class="calibre1"/>graphs as a navigational infrastructure. For <a id="id953" class="calibre1"/>more information, please refer to <a class="calibre1" href="http://www.icesi.edu.co/CRAN/web/packages/RnavGraph/vignettes/RnavGraph.pdf">http://www.icesi.edu.co/CRAN/web/packages/RnavGraph/vignettes/RnavGraph.pdf</a>.</p><p class="calibre7">Here is a description of how you can use <code class="email">RnavGraph</code> to visualize high dimensional data in a graph:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, install and load the <code class="email">RnavGraph</code> and <code class="email">graph</code> packages:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; install.packages("RnavGraph")</strong></span>
<span class="strong"><strong class="calibre2">&gt; source("http://bioconductor.org/biocLite.R")</strong></span>
<span class="strong"><strong class="calibre2">&gt; biocLite("graph")</strong></span>
<span class="strong"><strong class="calibre2">&gt; library(RnavGraph)</strong></span>
</pre></div></li><li class="listitem" value="2">You can then create an <code class="email">NG_data</code> object from the <code class="email">digit</code> data:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; digit.group = rep(c(1:9,0), each = 1100)</strong></span>
<span class="strong"><strong class="calibre2">&gt; digit.ng_data = ng_data(name = "ISO_digits",</strong></span>
<span class="strong"><strong class="calibre2">+ data = data.frame(digit.isomap$points),</strong></span>
<span class="strong"><strong class="calibre2">+ shortnames = paste('i',1:6, sep = ''),</strong></span>
<span class="strong"><strong class="calibre2">+ group = digit.group[digit.idx],</strong></span>
<span class="strong"><strong class="calibre2">+ labels = as.character(digits.group[digit.idx]))</strong></span>
</pre></div></li><li class="listitem" value="3">Create an <code class="email">NG_graph</code> object from <code class="email">NG_data</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt;  V = shortnames(digit.ng_data)</strong></span>
<span class="strong"><strong class="calibre2">&gt;  G = completegraph(V)</strong></span>
<span class="strong"><strong class="calibre2">&gt;  LG =linegraph(G)</strong></span>
<span class="strong"><strong class="calibre2">&gt;  LGnot = complement(LG)</strong></span>
<span class="strong"><strong class="calibre2">&gt;  ng.LG = ng_graph(name = "3D Transition", graph = LG)</strong></span>
<span class="strong"><strong class="calibre2">&gt; ng.LGnot = ng_graph(name = "4D Transition", graph = LGnot)</strong></span>
</pre></div></li><li class="listitem" value="4">Finally, you<a id="id954" class="calibre1"/> can <a id="id955" class="calibre1"/>visualize the graph in the <code class="email">tk2d</code> plot:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; ng.i.digits = ng_image_array_gray('USPS Handwritten Digits',</strong></span>
<span class="strong"><strong class="calibre2">+ digit.select,16,16,invert = TRUE,</strong></span>
<span class="strong"><strong class="calibre2">+ img_in_row = FALSE)</strong></span>
<span class="strong"><strong class="calibre2">&gt; vizDigits1 = ng_2d(data = digit.ng_data, graph = ng.LG, images = ng.i.digits)</strong></span>
<span class="strong"><strong class="calibre2">&gt; vizDigits2 = ng_2d(data = digit.ng_data, graph = ng.LGnot, images = ng.i.digits)</strong></span>
<span class="strong"><strong class="calibre2">&gt; nav = navGraph(data = digit.ng_data, graph = list(ng.LG, ng.LGnot), viz = list(vizDigits1, vizDigits2))</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00256.jpeg" alt="There's more..." class="calibre9"/><div class="caption"><p class="calibre12">A 3-D Transition graph plot</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="5">One can also view a 4D transition graph plot:<div class="mediaobject"><img src="../images/00257.jpeg" alt="There's more..." class="calibre9"/><div class="caption"><p class="calibre12">A 4D transition graph plot</p></div></div><p class="calibre13"> </p></li></ol><div class="calibre14"/></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Performing nonlinear dimension reduction with Local Linear Embedding"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec131" class="calibre1"/>Performing nonlinear dimension reduction with Local Linear Embedding</h1></div></div></div><p class="calibre7">
<span class="strong"><strong class="calibre2">Locally linear embedding</strong></span> (<span class="strong"><strong class="calibre2">LLE</strong></span>) is an extension of PCA, which reduces data that lies on a <a id="id956" class="calibre1"/>manifold embedded in a high <a id="id957" class="calibre1"/>dimensional space into a low dimensional space. In contrast to ISOMAP, which is a global approach for nonlinear dimension reduction, LLE is a local approach that employs a linear combination of the k-nearest neighbor to preserve local properties of data. In this recipe, we will give a short introduction of how to use LLE on an s-curve data.</p></div>

<div class="book" title="Performing nonlinear dimension reduction with Local Linear Embedding">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch11lvl2sec457" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this recipe, we will use digit data from <code class="email">lle_scurve_data</code> within the <code class="email">lle</code> package as our input source.</p></div></div>

<div class="book" title="Performing nonlinear dimension reduction with Local Linear Embedding">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch11lvl2sec458" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to perform nonlinear dimension reduction with LLE:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, you need to install and load the package, <code class="email">lle</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; install.packages("lle")</strong></span>
<span class="strong"><strong class="calibre2">&gt; library(lle)</strong></span>
</pre></div></li><li class="listitem" value="2">You can then load <code class="email">ll_scurve_data</code> from <code class="email">lle</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; data( lle_scurve_data )</strong></span>
</pre></div></li><li class="listitem" value="3">Next, perform <code class="email">lle</code> on <code class="email">lle_scurve_data</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; X = lle_scurve_data</strong></span>
<span class="strong"><strong class="calibre2">&gt; results = lle( X=X , m=2, k=12,  id=TRUE)</strong></span>
<span class="strong"><strong class="calibre2">finding neighbours</strong></span>
<span class="strong"><strong class="calibre2">calculating weights</strong></span>
<span class="strong"><strong class="calibre2">intrinsic dim: mean=2.47875, mode=2</strong></span>
<span class="strong"><strong class="calibre2">computing coordinates</strong></span>
</pre></div></li><li class="listitem" value="4">Examine <a id="id958" class="calibre1"/>the result <a id="id959" class="calibre1"/>with the <code class="email">str</code> and <code class="email">plot</code> function:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; str( results )</strong></span>
<span class="strong"><strong class="calibre2">List of 4</strong></span>
<span class="strong"><strong class="calibre2"> $ Y     : num [1:800, 1:2] -1.586 -0.415 0.896 0.513 1.477 ...</strong></span>
<span class="strong"><strong class="calibre2"> $ X     : num [1:800, 1:3] 0.955 -0.66 -0.983 0.954 0.958 ...</strong></span>
<span class="strong"><strong class="calibre2"> $ choise: NULL</strong></span>
<span class="strong"><strong class="calibre2"> $ id    : num [1:800] 3 3 2 3 2 2 2 3 3 3 ...</strong></span>
<span class="strong"><strong class="calibre2">&gt;plot( results$Y, main="embedded data", xlab=expression(y[1]), ylab=expression(y[2]) )</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00258.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">A 2-D scatter plot of embedded data</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="5">Lastly, you can use <code class="email">plot_lle</code> to plot the LLE result:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; plot_lle( results$Y, X, FALSE, col="red", inter=TRUE )</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00259.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">A LLE plot of LLE result</p></div></div><p class="calibre13"> </p></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Performing nonlinear dimension reduction with Local Linear Embedding">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch11lvl2sec459" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">LLE is a <a id="id960" class="calibre1"/>nonlinear dimension <a id="id961" class="calibre1"/>reduction method, which computes a low dimensional, neighborhood, preserving embeddings of high dimensional data. The algorithm of LLE can be illustrated in these steps: first, LLE computes the k-neighbors of each data point, <span class="strong"><img src="../images/00218.jpeg" alt="How it works..." class="calibre24"/></span>. Secondly, it computes a set of weights for each point, which minimizes the residual sum of errors, which can best reconstruct each data point from its neighbors. The residual sum of errors can be described as <span class="strong"><img src="../images/00260.jpeg" alt="How it works..." class="calibre24"/></span>, where <span class="strong"><img src="../images/00261.jpeg" alt="How it works..." class="calibre24"/></span> if <span class="strong"><img src="../images/00262.jpeg" alt="How it works..." class="calibre24"/></span> is not one of <span class="strong"><img src="../images/00218.jpeg" alt="How it works..." class="calibre24"/></span>'s k-nearest neighbor, and for each i, <span class="strong"><img src="../images/00263.jpeg" alt="How it works..." class="calibre24"/></span>. Finally, find the vector, Y, which is best reconstructed by the weight, W. The cost function can be illustrated as <span class="strong"><img src="../images/00264.jpeg" alt="How it works..." class="calibre24"/></span>, with the constraint that <span class="strong"><img src="../images/00265.jpeg" alt="How it works..." class="calibre24"/></span>, and <span class="strong"><img src="../images/00266.jpeg" alt="How it works..." class="calibre24"/></span>.</p><p class="calibre7">In this<a id="id962" class="calibre1"/> recipe, we demonstrate <a id="id963" class="calibre1"/>how to perform nonlinear dimension reduction using LLE. First, we load <code class="email">lle_scurve_data</code> from <code class="email">lle</code>. We then perform <code class="email">lle</code> with two dimensions and 12 neighbors, and list the dimensions for every data point by specifying <code class="email">id =TRUE</code>. The LLE has three steps, including: building a neighborhood for each point in the data, finding the weights for linearly approximating the data in that neighborhood, and finding the low dimensional coordinates.</p><p class="calibre7">Next, we can examine the data using the <code class="email">str</code> and <code class="email">plot</code> functions. The <code class="email">str</code> function returns X,Y, choice, and ID. Here, X represents the input data, Y stands for the embedded data, choice indicates the index vector of the kept data, while subset selection and ID show the dimensions of every data input. The <code class="email">plot</code> function returns the scatter plot of the embedded data. Lastly, we use <code class="email">plot_lle</code> to plot the result. Here, we enable the interaction mode by setting the inter equal to <code class="email">TRUE</code>.</p></div></div>

<div class="book" title="Performing nonlinear dimension reduction with Local Linear Embedding">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch11lvl2sec460" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">Another useful package for nonlinear dimension reduction is <code class="email">RDRToolbox</code>, which is a package for nonlinear dimension reduction with ISOMAP and LLE. You can use the following command to install <code class="email">RDRToolbox</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; source("http://bioconductor.org/biocLite.R")</strong></span>
<span class="strong"><strong class="calibre2">&gt; biocLite("RDRToolbox")</strong></span>
<span class="strong"><strong class="calibre2">&gt; library(RDRToolbox)</strong></span>
</pre></div></li></ul></div></div></div></body></html>