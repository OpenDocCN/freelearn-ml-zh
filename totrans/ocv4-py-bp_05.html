<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Using Computational Photography with OpenCV</h1>
                </header>
            
            <article>
                
<p>0The goal of this chapter<span> is to build</span> on what we have covered in the previous chapters about photography and image processing and investigate some algorithms that OpenCV gives you access in a lot more detail. We'll focus on working with digital photo<span>graphy</span> and building tools that will allow you to harness the power of OpenCV, and even think about using it as your go-to tool for editing your photos.</p>
<p class="mce-root">In this chapter, we will cover the following concepts:</p>
<ul>
<li>Planning the app</li>
<li>Understanding the 8-bit problem</li>
<li><span>Using </span><strong>gamma correction</strong></li>
<li>Understanding <strong>high-dynamic-range imaging</strong> (<strong>HDRI</strong>)</li>
<li>Understanding panorama stitching </li>
<li><span>Improving panorama stitching</span></li>
</ul>
<p><span>Learning the basics of digital photography and the concepts of high dynamic imaging will not only allow you to understand computational photography better, but it will make you a better photographer. Since we will explore these topics in detail, you will also understand how much work it takes to write a new algorithm.</span></p>
<p class="mce-root">Through this chapter, you will learn how to work with unprocessed (RAW) images directly from digital cameras, how to use OpenCV's computational photography tools, and how to use low-level OpenCV APIs to build a panorama stitching algorithm.</p>
<p>We have quite a few topics to cover, so let's roll up our sleeves and get started.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting started</h1>
                </header>
            
            <article>
                
<p>You can find the code that we present in this chapter at our GitHub repository at <a href="https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter5">https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter5</a>.</p>
<p><span>We will also use the <kbd>rawpy</kbd></span> <span>and <kbd>exifread</kbd> Python packages for reading RAW images and reading image metadata. For the full list of requirements, you can refer to the <kbd>requirements.txt</kbd> file in the book's Git repository.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Planning the app</h1>
                </header>
            
            <article>
                
<p>We have multiple concepts to familiarize ourselves with. With a view to building your toolbox for image processing, we are going to develop the algorithms that we are going to familiarize ourselves with into Python scripts that use OpenCV to accomplish real-life problems.</p>
<p>We will use OpenCV to implement the following scripts so that you will be able to use them whenever you need to do photo processing:</p>
<ul>
<li><kbd>gamma_correct.py</kbd>: This is a script that applies gamma correction to the input image and shows the resulting image.</li>
<li><kbd>hdr.py</kbd>: This is a script that takes images as input and produces a <strong>high dynamic range</strong> (<strong>HDR</strong>) image as an output.</li>
<li><kbd>panorama.py</kbd>: This is a script that takes multiple images as input and produces a single stitched image that is larger than the individual images.</li>
</ul>
<p>We'll first start with a discussion of how digital photography works and the reason we can't take perfect pictures without needing to do post-processing. Let's start with the 8-bit problem for images.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the 8-bit problem</h1>
                </header>
            
            <article>
                
<p>Typical <strong>Joint Photographic Experts Group</strong> (<strong>JPEG</strong><span>)</span> images that we are used to seeing, work by encoding each pixel into 24 bits—one 8-bit number per <strong>RGB</strong> (<strong>red</strong>, <strong>green</strong>, <strong>blue</strong>) color component, which gives us an integer within the 0-255 range. This is just a number, 255, <em>but is it enough information or not?</em> To understand this, let's try to understand how these numbers are recorded and what these numbers mean.</p>
<p>Most current digital cameras use a <strong>Bayer filter, </strong>or equivalent, that works using the same principles. A Bayer filter is an array of sensors of different colors placed on a grid similar to the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/faac3543-7aab-4455-bcd0-a6cde3768ac0.png" style="width:31.58em;height:20.42em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Image source—https://en.wikipedia.org/wiki/Bayer_filter#/media/File:Bayer_pattern_on_sensor.svg (CC SA 3.0)</div>
<p>In the previous diagram, each of these sensors measures the intensity of the light that gets into it, and a group of four sensors represents a single pixel. The data from these four sensors are combined to provide us with the three values for R, G, and B.</p>
<p>Different cameras might have a slightly different layout of red, green, and blue pixels, but at the end of the day, they are using small sensors that discretize the amount of radiation they get into a single value within the 0-255 range, where 0 means no radiation at all and 255 means the brightest radiation that the sensor can record.</p>
<p>The range of brightness that is detectable is called the <strong>dynamic range</strong> or the <strong>luminance range</strong>. The ratio between the smallest amount of radiation th<span>at could be registere</span><span>d (that is, 1) and the highest (that is, 255) is called the <strong>c</strong></span><strong>ontrast ratio.</strong></p>
<p>As we said, JPEG files have a contrast ratio of <em>255:1</em>. Most current LCD monitors have already surpassed that and have a contrast ratio of up to <em>1,000:1</em>. I bet you are waiting for your eye's ratio. I'm not sure about you, but most humans can see up to <em>15,000:1</em>.</p>
<p>So, we can see quite a lot more than even our best monitor can show, and a lot more than a simple JPEG file stores. Don't despair too much, because the latest digital cameras have been catching up and can now capture intensity ratios of up to <em>28,000:1</em> (the really expensive ones).</p>
<p>The small dynamic range is the reason that, when you are shooting a picture and you have the sun in the background, you either see the sun and <span>the surroundings are all white</span><span> </span><span>without any detail, or everything in the foreground is extremely dark. Here is an example screenshot:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d8e38611-5931-491a-9e4d-01998b5650b2.png" style="width:53.75em;height:17.67em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Image source—https://github.com/mamikonyana/winter-hills (CC SA 4.0)</div>
<p>So, the problem is that we either display things that are too bright, or we display things that are too dark. Before we move forward, let's take a look at how to read files that have more than 8 bits and import the data into OpenCV.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learning about RAW images</h1>
                </header>
            
            <article>
                
<p>Since this chapter is about computational photography, some of you reading it are probably photography enthusiasts and love taking pictures using the RAW formats that your camera supports—be it <strong>Nikon Electronic Format</strong> (<strong>NEF</strong>) or <strong>Canon Raw Version 2</strong> (<strong>CR2</strong>).</p>
<p>Raw files usually capture a lot more information (usually more bits per pixel) than JPEG files, and if you are going to do a lot of post-processing, <span>these files are a lot</span><span> </span><span>more convenient to work with, since they will produce higher-quality final images.</span></p>
<p>So let's take a look at how to open a CR2 file using Python and load it into OpenCV. For that, we will use a Python library called <kbd>rawpy</kbd>. For convenience, we will write a function called <kbd>load_image</kbd> that can handle both RAW images and regular JPEG files so we can abstract this part away and concentrate on more fun things in the rest of the chapter:</p>
<ol>
<li>First, we take care of the imports (as promised, just one small extra library):</li>
</ol>
<pre style="padding-left: 60px">import rawpy<br/>import cv2</pre>
<ol start="2">
<li>We define the function, adding an optional <kbd>bps</kbd> argument, which will let us control how much precision we want the images to have, that is, we want to check if we want the full 16 bits or are just 8 bits good enough:</li>
</ol>
<pre style="padding-left: 60px">def load_image(path, bps=16):</pre>
<ol start="3">
<li>Then, if the file has a <kbd>.CR2</kbd> extension, we open the file with <kbd>rawpy</kbd> and extract the image without trying to do any post-processing, since we want to do that with OpenCV:</li>
</ol>
<pre style="padding-left: 60px">    if path.suffix == '.CR2':<br/>        with rawpy.imread(str(path)) as raw:<br/>            data = raw.postprocess(no_auto_bright=True,<br/>                                   gamma=(1, 1),<br/>                                   output_bps=bps)</pre>
<ol start="4">
<li>As Canon (<span>Canon Inc.—an optical products</span><span> company</span>) and OpenCV use a different ordering of colors, we switch from RGB to <strong>BGR </strong>(<strong>blue</strong>, <strong>green</strong>, and <strong>red</strong>), which is the default ordering in OpenCV and we <kbd>return</kbd> the resulting image:</li>
</ol>
<pre style="padding-left: 60px">        return cv2.cvtColor(data, cv2.COLOR_RGB2BGR)</pre>
<p style="padding-left: 60px">For anything that is not <kbd>.CR2</kbd>, we use OpenCV:</p>
<pre style="padding-left: 60px">    else:<br/>        return cv2.imread(str(path))</pre>
<p>Now we know how to get all our images into OpenCV, it's time to get started with one of the brightest algorithms we have.</p>
<p>Since my camera has a 14-bit dynamic range, we are going to use images captured with my camera:</p>
<pre>def load_14bit_gray(path):<br/>    img = load_image(path, bps=16)<br/>    return (cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) / 4).astype(np.uint16)</pre>
<p>Once we know how to load our pictures, let's try to see how we can best display them on the screen.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using gamma correction</h1>
                </header>
            
            <article>
                
<p><em>Why is everybody still using JPEG files if they can only distinguish between 255 different levels? </em><em>Does it mean it can only capture a dynamic range of 1:255?</em> It turns out there are clever tricks that people use.</p>
<p>As we mentioned before, the camera sensors capture values that are linear, that is, 4<span> </span><span>means that it has 4 times more light than 1, and 80 has 8 times more light than 10. But does the</span> JPEG file format <span>have to use a linear scale? It turns out that it doesn't. </span><span>S</span><span>o, if we are willing to sacrifice the difference between two values, for example, 100 and 101, we can fit another value there.</span></p>
<p>To understand this better, let's look at the histogram of gray pixel values of a RAW image. Here is the code to generate that—just load the image, convert it to grayscale, and show the histogram using <kbd>pyplot</kbd>:</p>
<pre>    images = [load_14bit_gray(p) for p in args.images]<br/>    fig, axes = plt.subplots(2, len(images), sharey=False)<br/>    for i, gray in enumerate(images):<br/>        axes[0, i].imshow(gray, cmap='gray', vmax=2**14)<br/>        axes[1, i].hist(gray.flatten(), bins=256)</pre>
<p>Here is the result of the histogram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3e368499-f598-40c1-bd9d-d1731a0930b5.png" style="width:49.25em;height:30.67em;"/></p>
<p>We have two pictures: the left one is a <em>normal</em> picture, where you can see some clouds, but it's almost impossible to see anything in the foreground, and the right one has tried to capture some detail in the trees, and because of that has burned all the clouds. <em>Is there a way to combine these?</em></p>
<p>If we take a closer look at the histograms, we see that the burned-out part is visible on the right-hand histogram because there are values that are 16,000 that get encoded as 255, that is, white pixels. But on the left-hand picture, there are no white pixels. The way we encode 14-bit values into 8-bit values is very rudimentary: we just divide the values by <em>64 (=2<sup>6</sup>)</em>, so we lose the distinction between 2,500 and 2,501 and 2,502; instead, we only have 39 (out of 255) because the values in the 8-bit format have to be integers.</p>
<p>This is where gamma corrections come in. Instead of simply showing the recorded value as the intensity, we are going to make some corrections, to make the image more visually appealing.</p>
<p>We are going to use a non-linear function to try to emphasize the parts that we think are more important:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ec71864e-36ff-4fea-8bcd-bf7fb0880fbb.png" style="width:8.50em;height:2.42em;"/></p>
<p>Let's try to visualize this formula for two different values—<span><strong>γ = 0.3</strong> and <strong>γ = 3</strong>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f8fd3ea0-b583-418a-91f7-f56ca2b0f7ce.png" style="width:28.00em;height:21.00em;"/></p>
<p>As you can see, small gammas put an emphasis on lower values; the pixel values from <strong>0</strong>-<strong>50</strong> are mapped to pixel values from <strong>0</strong>-<strong>150</strong> (more than half of the available values). The reverse is true of the higher gammas—the values from <strong>200</strong>-<strong>250</strong> are mapped to the values <strong>100</strong>-<strong>250</strong> (more than half of the available values). So, if you want to make your photo brighter, you should pick a gamma value of <strong>γ &lt; 1</strong>, which is often called <strong>gamma compression</strong>. And if you want to make your photos dimmer to show more detail, you should pick a gamma value of <strong>γ &gt; 1</strong>, which is called <strong>gamma expansion.</strong></p>
<p>Instead of using integers for <em>I</em>, we can start with a float number and get to O, then convert that number to an integer to lose even less of information. Let's write some Python code to implement gamma correction:</p>
<ol>
<li>First, let's write a function to apply our formula. Because we are using 14-bit numbers, we will have to change it to the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/beccc155-e722-44a5-a24e-b34c4b5fa6c0.png" style="width:7.67em;height:2.33em;"/></p>
<p style="padding-left: 60px">Thus, the relevant code will be as follows:</p>
<pre style="padding-left: 60px">@functools.lru_cache(maxsize=None)<br/>def gamma_transform(x, gamma, bps=14):<br/>    return np.clip(pow(x / 2**bps, gamma) * 255.0, 0, 255)</pre>
<p style="padding-left: 60px" class="mce-root">Here, we have used the <kbd>@functools.lru_cache</kbd> decorator to make sure we don't compute anything twice.</p>
<ol start="2">
<li>Then, we just iterate over a<span>ll the pixels and apply our transformation function:</span></li>
</ol>
<pre style="padding-left: 60px">def apply_gamma(img, gamma, bps=14):<br/>    corrected = img.copy()<br/>    for i, j in itertools.product(range(corrected.shape[0]),<br/>                                  range(corrected.shape[1])):<br/>        corrected[i, j] = gamma_transform(corrected[i, j], gamma, bps=bps)<br/>    return corrected</pre>
<p>Now let's take a look at how to use this to show the new image alongside the regularly transformed 8-bit image. We will write a script for this:</p>
<ol>
<li>First, let's configure a <kbd>parser</kbd> to load an image and allow setting the <kbd>gamma</kbd> value:</li>
</ol>
<pre style="padding-left: 60px"> if __name__ == '__main__':<br/>    parser = argparse.ArgumentParser()<br/>    parser.add_argument('raw_image', type=Path,<br/>                        help='Location of a .CR2 file.')<br/>    parser.add_argument('--gamma', type=float, default=0.3)<br/>    args = parser.parse_args()</pre>
<ol start="2">
<li>Load the <kbd>gray</kbd> image as a <kbd>14bit</kbd> image:</li>
</ol>
<pre style="padding-left: 60px">    gray = load_14bit_gray(args.raw_image)</pre>
<ol start="3">
<li>Use linear transformation to get output values as an integer in the range [<kbd>0</kbd>-<kbd>255</kbd>]:</li>
</ol>
<pre style="padding-left: 60px">    normal = np.clip(gray / 64, 0, 255).astype(np.uint8)</pre>
<ol start="4">
<li>Use our <kbd>apply_gamma</kbd> function we wrote previously to get a gamma-corrected image:</li>
</ol>
<pre style="padding-left: 60px">    corrected = apply_gamma(gray, args.gamma)</pre>
<ol start="5">
<li>Then, plot both of the images together with their histogram:</li>
</ol>
<pre style="padding-left: 60px">    fig, axes = plt.subplots(2, 2, sharey=False)<br/>    for i, img in enumerate([normal, corrected]):<br/>        axes[0, i].imshow(img, cmap='gray', vmax=255)<br/>        axes[1, i].hist(img.flatten(), bins=256)</pre>
<ol start="6">
<li>Finally, <kbd>show</kbd> the image:</li>
</ol>
<pre style="padding-left: 60px">    plt.show()</pre>
<p>We have now plotted the histogram and will look at the magic that is elaborated in the following two images with their histograms:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/180de854-3e0e-4164-b4b3-963cf8c863b4.png" style="width:51.00em;height:35.42em;"/></p>
<p>Look at the picture at the top right—you can see almost everything! And we are only getting started.</p>
<p>It turns out gamma compensation works great on black and white images, but it can't do everything! It can either correct brightness and we lose most of the color information, or it can correct color information and we lose the brightness information. So, we have to find a new best friend—that is, HDRI.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding high-dynamic-range imaging</h1>
                </header>
            
            <article>
                
<p><strong>High-dynamic-range</strong> imaging (<strong>HDR</strong>) is a technique to produce images that have a greater dynamic range of luminosity (that is, contrast ratio) than could be displayed through the display medium, or captured with the camera using a single shot. There are two main ways to create such images—using special image sensors, such as an oversampled binary image sensor, or the way we will focus on here, by combining multiple <strong>Standard Dynamic Range</strong> (<strong>SDR</strong>) images to produce a combined HDR image.</p>
<p>HDR imaging works with images that use more than 8 bits per channel (usually 32-bit float values), allowing a much wider dynamic range. As we know, t<span>he <em>dynamic range</em> of a scene is th</span><span>e</span> <span>contrast ratio between its brightest and</span> <span>darkes</span><span>t parts.</span></p>
<p class="mce-root">Let's take a closer look at what the luminance values are of certain things that we can see. The following diagram shows values that we can easily see, from the dark sky (around <em>10<sup>-4</sup> cd/m<sup>2</sup></em>) to the sun during sunset (<em>10<sup>5</sup> cd/m<sup>2</sup></em>):</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f2981dc9-a73e-499b-bab2-4393ddf84700.png" style="width:35.67em;height:19.08em;"/></p>
<p>We can see more than these values. Because some people can adjust their eyes to even darker places, we can definitely see the sun when it's not on the horizon but is higher up in the sky, probably up to <em>10<sup>8</sup> cd/m<sup>2</sup></em>, but this range is already quite a big range, so let's stick to it for now. For comparison, a usual 8-bit image has a contrast ratio of <em>256:1</em>, the human eye can see at one time around million to 1, and the 14-bit RAW format shows <em>2<sup>14</sup>:1</em>.</p>
<p>Display media also have limitations; for example, a typical IPS monitor has a contrast ratio of around <em>1,000:1</em>, and a VA monitor could have a contrast ratio of up to <em>6,000:1</em>. So, let's place these values on this spectrum and see how they compare:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/42cafd44-1d8a-4f16-a98f-737a8c9a4532.png" style="width:26.25em;height:14.92em;"/></p>
<p>Now, this doesn't look like we can see much, which is true since it takes time for us to adjust to different lighting conditions. The same is true about a camera. But in just one glance, our naked eye can see quite a lot more than even the best camera can. <em>So how can we remedy this?</em></p>
<p>As we said, the trick is to take multiple pictures in quick succession, which most cameras allow with ease. If we were to take pictures in quick succession that complement each other, we could cover quite a big part of the spectrum with just five JPEGs:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3ddd82b2-79ca-437c-afe4-f86aa18ba231.png" style="width:27.58em;height:11.42em;"/></p>
<p>This seems a little too easy, but remember, taking five pictures is quite easy. But, we are talking about one picture that has all the dynamic range, not five separate pictures. There are two big problems with HDR images:</p>
<ul>
<li><em>How can we combine multiple images into a single image?</em></li>
<li><em>How can we display an image that has a higher dynamic range than our display media?</em></li>
</ul>
<p>However, even before we can combine those images, let's take a closer look at how can we vary the exposure of the camera, that is, its sensitivity to light.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring ways to vary exposure</h1>
                </header>
            
            <article>
                
<p>As we discussed earlier in this chapter, modern <strong>Digital Single Lens Reflector</strong> cameras (<strong>DSLR</strong>s), and other digital cameras as well, have a fixed sensor grid (usually placed as a Bayer filter), which just measures the light intensity of the camera.</p>
<p>I bet you have seen the same camera used to capture beautiful night pictures where the water looks like a silky cloud and stills that sports photographers have taken of a player at full stretch. <em>So how can they use the same camera for such different settings and get results that we see on the screen?</em></p>
<p>When measuring the exposure, it's really hard to measure the luminance that is being captured. It's a lot easier to measure relative speed instead of measuring luminance in the power of 10, which could be quite difficult to adjust. We measure the speed in the power of 2; we call that a <strong>stop</strong>.</p>
<p>The trick is that, even though the camera is restricted, it has to be able to capture a limited luminance range per picture. The range itself could be moved along the luminance spectrum. To overcome this, let's study the shutter speed, aperture, and ISO speed parameters of the camera.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Shutter speed</h1>
                </header>
            
            <article>
                
<p><span class="ILfuVd"><span class="e24Kjd">Shutter speed is not really the speed of the shutter, but it's the length of time for which a camera's shutter is open when taking a photograph. Thus, it's the amount of time for which the</span></span> <span class="ILfuVd"><span class="e24Kjd">digital sensor inside the camera is exposed to light for collecting information. It's the most intuitive control out of all the camera controls because we can feel it happening.</span></span></p>
<p>Shutter speeds are usually measured in fractions of a second. For example, <em>1/60</em> is the fastest speed for which, if we shake the camera while clicking photos while it is held in our hands, it doesn't introduce a blur in the photograph. So if you are going to use your own pictures, make sure to not do this, or get yourself a tripod.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Aperture</h1>
                </header>
            
            <article>
                
<p>Aperture is the diameter of the hole in the optical lens through which the light passes into the camera. The following picture shows examples of the opening set to different aperture values:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/22395445-cfda-4e3e-b0d4-1392f3995960.jpeg" style="width:21.33em;height:14.17em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Image source—https://en.wikipedia.org/wiki/Aperture#/media/File:Lenses_with_different_apertures.jpg (CC SA 4.0)</div>
<p>Aperture is usually measured using an <em>f-number</em>. The f-number is the ratio of the system's focal length to the diameter of the opening (the entrance pupil). We won't concern ourselves with the focal length of the lens; the only thing we need to know is that only zoom <span class="ILfuVd"><span class="e24Kjd">lenses have variable focal lengths, thus if we don't change the magnification on the lens, the focal length will stay the same. So we can measure the <strong>area</strong> of the entrance pupil by squaring the inverse of the <strong>f-number</strong>:</span></span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1ab9769f-3113-47dc-a35b-521a9a2552d2.png" style="width:9.83em;height:3.00em;"/></p>
<p>And, we know that the bigger the area, the more light we will get in our pictures. Thus, if we were to increase the f-number, that would correspond to a decrease in the size of the entrance pupil and our pictures would become darker, enabling us to take pictures during the afternoon.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ISO speed</h1>
                </header>
            
            <article>
                
<p class="mce-root">ISO speed is the sensitivity of the sensors used in cameras. It is measured using numbers that map the sensitivity of the digital sensor to the chemical films that were used when computers were not around yet.</p>
<p>ISO speed is measured in two numbers; for example, <em>100/21°</em>, where the first number is the speed on the arithmetic scale and the second number is the number on the logarithmic scale. Since these numbers have a one-to-one mapping, usually the second one is omitted, and we simply write <em>ISO 100</em>. ISO 100 is two times less sensitive to light than ISO 200 and it is said that the difference is <strong>1 stop</strong>.</p>
<p>It is easier to talk in powers of 2 rather than powers of 10, so photographers came up with the notion of <strong>stops</strong>. One stop is two times different, 2 stops are 4 times different, and so on. Thus, <em>n</em> stops are <em>2<sup>n</sup></em> times different. This analogy has become so widespread that people have started using fractional and real numbers for stops.</p>
<p>Now that we understand how to control the exposure, let's try to look at the algorithms that can combine multiple pictures with different exposures into a single image.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating HDR images using multiple exposure images</h1>
                </header>
            
            <article>
                
<p>Now, once we know how it's possible to get more pictures, we can take multiple photos that have very little or no overlapping dynamic range. Let's have a look at the most popular algorithm for HDR, first published by Paul E Debevec and Jitendra Malik in 2008. </p>
<p>It turns out that if you want to have good results, you have to have pictures that are overlapping, to make sure you have good accuracy and since there is noise in the photos. It's usually common to have 1, 2, or at most 3 stops difference from picture to picture. If we were to shoot five 8-bit photos with a difference of 3 stops, we would cover the human eye's one million to one sensitivity ratio:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3359ab4c-0eaa-4fb3-b51c-c6985079210e.png" style="width:26.83em;height:14.83em;"/></p>
<p>Now let's take a closer look at how the Debevec HDR algorithm works.</p>
<p>First, let's assume that the recorded values the camera sees are some function of the scene's irradiance. We talked about this being linear before, but nothing is truly linear in real life. Let the recorded value matrix be <em><strong>Z</strong></em> and the irradiance matrix be <strong>X</strong>; we have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/cde2add2-a9a6-4caf-b1c1-634856164ae5.png" style="width:5.83em;height:1.25em;"/></p>
<p>Here, we have also used <strong>Δt</strong> as the measure of exposure time, and the function <strong><em>f</em></strong> is called the <strong>response function</strong> of our camera. Also, we assume that if we double the exposure and half the irradiance, we will have the same output and vice versa. This should be true across all images, and the value of <strong><em>E</em></strong> should not change from picture to picture; only the recorded values of <em><strong>Z</strong></em> and the exposure time <strong>Δt</strong> can change. If we apply the <strong>inverse response function</strong>(<strong> <em>f<sup>-1</sup></em></strong>) and take the logarithm of both sides, then we get that for all of the pictures(<em><strong>i</strong></em>) that we have:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/dde52fc9-57b0-487f-8e40-6e06e708d448.png" style="width:12.25em;height:1.42em;"/></p>
<p>Now the trick is to come up with an algorithm that can calculate the <strong><em>f<sup>-1</sup></em></strong>, and that's what Debevec et al. have done.</p>
<p>Of course, our pixel values are not going to follow this rule exactly, and we will have to fit an approximate solution, but, let's take a more detailed look at what these values are.</p>
<p><span>Before we move forward, l</span>et's take a look at how we can recover <strong>Δt<sub>i</sub></strong> values from the picture files in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extracting exposure strength from images</h1>
                </header>
            
            <article>
                
<p><span>Assuming the <strong>principle of reciprocity</strong> for all the camera parameters that we discussed previously, let's try to come up with a function—<kbd>exposure_strength</kbd>—that returns a time equivalent to the exposure:</span></p>
<ol>
<li>First, let's set a reference for ISO speed and f-stop:</li>
</ol>
<pre style="padding-left: 60px">def exposure_strength(path, iso_ref=100, f_stop_ref=6.375):</pre>
<ol start="2">
<li>Then, let's use the <kbd>exifread</kbd> Python package, which makes it easy to read the metadata associated with the images. Most modern cameras record the metadata in this standard format:</li>
</ol>
<pre style="padding-left: 60px">    with open(path, 'rb') as infile:<br/>        tags = exifread.process_file(infile)</pre>
<ol start="3">
<li>Then, let's extract the <kbd>f_stop</kbd> value and see how much bigger the entrance pupil area to the reference was:</li>
</ol>
<pre style="padding-left: 60px">    [f_stop] = tags['EXIF ApertureValue'].values<br/>    rel_aperture_area = 1 / (f_stop.num / f_stop.den / f_stop_ref) ** 2</pre>
<ol start="4">
<li>Then, let's see how much more sensitive the ISO setting was:</li>
</ol>
<pre style="padding-left: 60px">    [iso_speed] = tags['EXIF ISOSpeedRatings'].values<br/>    iso_multiplier = iso_speed / iso_ref</pre>
<ol start="5">
<li>Finally, let's combine all the values with the shutter speed and return <kbd>exposure_time</kbd>:</li>
</ol>
<pre style="padding-left: 60px">    [exposure_time] = tags['EXIF ExposureTime'].values<br/>    exposure_time_float = exposure_time.num / exposure_time.den<br/>    return rel_aperture_area * exposure_time_float * iso_multipli</pre>
<p>Here is an example of the values of the photographs that I am using for this demo, taken from the <strong>Frozen River</strong> photo collection:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr style="height: 32px">
<td style="height: 32px"><strong>Photograph</strong></td>
<td style="height: 32px"><strong>Aperture</strong></td>
<td style="height: 32px"><strong>ISO Speed</strong></td>
<td style="height: 32px"><strong>Shutter Speed</strong></td>
</tr>
<tr style="height: 21.1328px">
<td style="height: 21.1328px">AM5D5669.CR2</td>
<td style="height: 21.1328px">6 3/8</td>
<td style="height: 21.1328px">100</td>
<td style="height: 21.1328px">1/60</td>
</tr>
<tr style="height: 20px">
<td style="height: 20px">AM5D5670.CR2</td>
<td style="height: 20px">6 3/8</td>
<td style="height: 20px">100</td>
<td style="height: 20px">1/250</td>
</tr>
<tr style="height: 10px">
<td style="height: 10px">AM5D5671.CR2</td>
<td style="height: 10px">6 3/8</td>
<td style="height: 10px">100</td>
<td style="height: 10px">1/160</td>
</tr>
<tr style="height: 10px">
<td style="height: 10px">AM5D5672.CR2</td>
<td style="height: 10px">6 3/8</td>
<td style="height: 10px">100</td>
<td style="height: 10px">1/100</td>
</tr>
<tr style="height: 10px">
<td style="height: 10px">AM5D5673.CR2</td>
<td style="height: 10px">6 3/8</td>
<td style="height: 10px">100</td>
<td style="height: 10px">1/40</td>
</tr>
<tr style="height: 10px">
<td style="height: 10px">AM5D5674.CR2</td>
<td style="height: 10px">6 3/8</td>
<td style="height: 10px">160</td>
<td style="height: 10px">1/40</td>
</tr>
<tr style="height: 32px">
<td style="height: 32px">AM5D5676.CR2</td>
<td style="height: 32px">6 3/8</td>
<td style="height: 32px">250</td>
<td style="height: 32px">1/40</td>
</tr>
</tbody>
</table>
<p> </p>
<p>This is the output of the time estimates for these pictures using the <kbd>exposure_strength</kbd> function:</p>
<pre><strong>[0.016666666666666666, 0.004, 0.00625, 0.01, 0.025, 0.04, 0.0625</strong></pre>
<p>Now, once we have the exposure times, let's see how this can be used to get the camera response function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Estimating the camera response function</h1>
                </header>
            
            <article>
                
<p><span>Let's plot <img class="fm-editor-equation" src="assets/cbafee7e-4b86-48bc-9cab-fce681e46113.png" style="width:2.92em;height:1.25em;"/> on the <em>y</em> axis, and <strong>Z<sub>i</sub></strong> on the <em>x</em> axis:<br/></span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/acd17978-8a98-4618-ad89-de957b72155a.png" style="width:58.92em;height:26.08em;"/></p>
<p><span>What we are trying to do is to find an <strong><em>f<sup>-1</sup></em></strong> and, more importantly, the <img class="fm-editor-equation" src="assets/d04631b2-e5aa-4874-b7b3-06192cc9d677.png" style="width:2.33em;height:1.17em;"/> of all the pictures, such that when we add</span> <strong>log(E)</strong> <span>to the log exposure, we will have all the pixels on the same function. You can see the results of the Debevec algorithm in the following screenshot:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/594d6332-a67a-4fc5-b877-afd67b331ccb.png" style="width:58.83em;height:28.92em;"/></p>
<p>The Debevec algorithm estimates both the <strong><em>f<sup>-1</sup></em></strong><span>, </span>which passes approximately through all the pixels, and the <span><img class="fm-editor-equation" src="assets/d04631b2-e5aa-4874-b7b3-06192cc9d677.png" style="width:2.08em;height:1.00em;"/></span>. The <em><strong>E</strong></em> matrix is the resulting HDR image matrix that we recover.</p>
<p>Now let's take a look at how to implement this using OpenCV.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Writing an HDR script using OpenCV</h1>
                </header>
            
            <article>
                
<p>The first step of the script is going to be setting up the script arguments using Python's built-in <kbd>argparse</kbd> module:</p>
<pre>import argparse<br/><br/>if __name__ == '__main__':<br/>    parser = argparse.ArgumentParser()<br/>    img_group = parser.add_mutually_exclusive_group(required=True)<br/>    img_group.add_argument('--image-dir', type=Path)<br/>    img_group.add_argument('--images', type=Path, nargs='+')<br/>    args = parser.parse_args()<br/><br/></pre>
<pre>    if args.image_dir:<br/>        args.images = sorted(args.image_dir.iterdir())</pre>
<p>As you can see, we have set up two mutually exclusive arguments—<kbd>--image-dir</kbd>, a directory that contains the images, and <kbd>--images</kbd>, a list of images that we are going to use. And we make sure that we populate <kbd>args.images</kbd> with the list of all the images, so the rest of the script shouldn't worry about which of the options the user has chosen.</p>
<p>After we have all the command-line arguments, the rest of the procedure is as follows:</p>
<ol>
<li>Read all <kbd>images</kbd> into the memory:</li>
</ol>
<pre style="padding-left: 60px">    images = [load_image(p, bps=8) for p in args.images]</pre>
<ol start="2">
<li>Read the metadata and estimate exposure times using <kbd>exposure_strength</kbd>:</li>
</ol>
<pre style="padding-left: 60px">    times = [exposure_strength(p)[0] for p in args.images]<br/>    times_array = np.array(times, dtype=np.float32)</pre>
<ol start="3">
<li>Calculate the <strong>camera response function</strong>—<kbd>crf_debevec</kbd>:</li>
</ol>
<pre style="padding-left: 60px">    cal_debevec = cv2.createCalibrateDebevec(int samples=200)<br/>    crf_debevec = cal_debevec.process(images, times=times_array)</pre>
<ol start="4">
<li>Use the camera response function to calculate the HDR image:</li>
</ol>
<pre style="padding-left: 60px">    merge_debevec = cv2.createMergeDebevec()<br/>    hdr_debevec = merge_debevec.process(images, times=times_array.copy(),<br/>                                        response=crf_debevec)</pre>
<p>Notice that the HDR image is of type <kbd>float32</kbd> and not <kbd>uint8</kbd>, as it contains the full dynamic range of all exposure images.</p>
<p>Now we have the HDR image and we've come to the next important part. Let's see how we can show the HDR image using our 8-bit image representation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Displaying HDR images</h1>
                </header>
            
            <article>
                
<p>Displaying HDR images is tricky. As we said, HDR has more values than the camera, so we need to figure out a way to display that. Luckily, OpenCV is here to help us again, and, as you've probably guessed by now, we can use gamma correction to map all the different values we have into a smaller spectrum of values in the range <kbd>0</kbd><span> to </span><kbd>255</kbd><span>. This process is called</span> <strong>Tone Mapping</strong><span>.</span></p>
<p><span>OpenCV has a method for it that takes <kbd>gamma</kbd> as an argument:</span></p>
<pre>    tonemap = cv2.createTonemap(gamma=2.2)<br/>    res_debevec = tonemap.process(hdr_debevec)</pre>
<p>Now we have to <kbd>clip</kbd> all the values to become integers:</p>
<pre>    res_8bit = np.clip(res_debevec * 255, 0, 255).astype('uint8')</pre>
<p>After that, we can show our resulting HDR image using <kbd>pyplot</kbd><span>:</span></p>
<pre>    plt.imshow(res_8bit)<br/>    plt.show()</pre>
<p>This results in the following gorgeous image:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3bedf9ea-7f0e-4b41-a0cc-81602427037d.png" style="width:42.17em;height:28.33em;"/></p>
<p>Now, let's see how can we extend the camera's field of view—potentially to 360 degrees!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding panorama stitching</h1>
                </header>
            
            <article>
                
<p>Another very interesting topic in computational photography is <strong>panorama stitching</strong>. I'm sure most of you have a panorama function on your phone. This section will focus on the ideas behind panorama stitching and, instead of just calling a single function, we will go through all the steps involved in creating a panorama from a bunch of separate photos.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Writing script arguments and filtering images</h1>
                </header>
            
            <article>
                
<p>We want to write a script that will take a list of images and will produce a single panorama picture. So, let's set up the <kbd>ArgumentParser</kbd> for our script:</p>
<pre>def parse_args():<br/>    parser = argparse.ArgumentParser()<br/>    img_group = parser.add_mutually_exclusive_group(required=True)<br/>    img_group.add_argument('--image-dir', type=Path)<br/>    img_group.add_argument('--images', type=Path, nargs='+')<br/>    args = parser.parse_args()<br/><br/>    if args.image_dir:<br/>        args.images = sorted(args.image_dir.iterdir())<br/>    return args</pre>
<p>Here, we created an instance of <kbd>ArgumentParser</kbd> and added arguments to pass either an image directory of a list of images. Then, we make sure we get all the images from the image directory if it is passed, instead of passing a list of images.</p>
<p>Now, as you can imagine, the next step is to use a feature extractor and see what the common features that images share are. This is very much like the previous two chapters, that is, <a href="905b17f6-8eea-4d33-9291-17ea93371f2d.xhtml">Chapter 3</a>, <em>Finding Objects via Feature Matching and Perspective Transforms,</em> and <a href="efb28928-4399-4e3d-9ca3-6e773aaaa699.xhtml"/><a href="efb28928-4399-4e3d-9ca3-6e773aaaa699.xhtml">Chapter 4</a>, <em>3D Scene Reconstruction Using Structure from Motion</em>. We will also write a function to filter those images that have common features, so the script is even more versatile. Let's go through the function step by step:</p>
<ol>
<li>Create the <kbd>SURF</kbd> feature extractor and compute all of the features of all images:</li>
</ol>
<pre style="padding-left: 60px">def largest_connected_subset(images):<br/>    finder = cv2.xfeatures2d_SURF.create()<br/>    all_img_features = [cv2.detail.computeImageFeatures2(finder, img)<br/>                        for img in images]</pre>
<ol start="2">
<li>Create a <kbd>matcher</kbd> class that matches an image to its closest neighbors that share the most features:</li>
</ol>
<pre style="padding-left: 60px">    matcher = cv2.detail.BestOf2NearestMatcher_create(False, 0.6)<br/>    pair_matches = matcher.apply2(all_img_features)<br/>    matcher.collectGarbage()</pre>
<ol start="3">
<li>Filter the images and make sure that we have at least two images that share features so we can proceed with the algorithm:</li>
</ol>
<pre style="padding-left: 60px">    _conn_indices = cv2.detail.leaveBiggestComponent(all_img_features, pair_matches, 0.4)<br/>    conn_indices = [i for [i] in _conn_indices]<br/>    if len(conn_indices) &lt; 2:<br/>        raise RuntimeError("Need 2 or more connected images.")<br/><br/>    conn_features = np.array([all_img_features[i] for i in conn_indices])<br/>    conn_images = [images[i] for i in conn_indices]</pre>
<ol start="4">
<li>Run the <kbd>matcher</kbd> again to check whether we have removed any of the images and <kbd>return</kbd> the variables we will need in the future:</li>
</ol>
<pre style="padding-left: 60px">    if len(conn_images) &lt; len(images):<br/>        pair_matches = matcher.apply2(conn_features)<br/>        matcher.collectGarbage()<br/><br/>    return conn_images, conn_features, pair_matches</pre>
<p>After we have filtered the images and have all the features, we move on to the next step, which is setting up a blank canvas for the panorama stitching.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Figuring out relative positions and the final picture size</h1>
                </header>
            
            <article>
                
<p>Once we have separated all the connected pictures and know all the features, it's time to figure out how big the merged panorama is going to be and create the blank canvas to start adding pictures to it. First, we need to find the parameters of the pictures.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Finding camera parameters</h1>
                </header>
            
            <article>
                
<p>In order to be able to merge images, we need to compute homography matrices of all the images and then use those to adjust the images so they can be merged together. We will write a function to do that:</p>
<ol>
<li>First, we are going to create the <kbd>HomographyBasedEstimator()</kbd> function:</li>
</ol>
<pre style="padding-left: 60px">def find_camera_parameters(features, pair_matches):<br/>    estimator = cv2.detail_HomographyBasedEstimator()</pre>
<ol start="2">
<li>Once we have the <kbd>estimator</kbd>, for extracting all the camera parameters, we use the matched <kbd>features</kbd> from different images:</li>
</ol>
<pre style="padding-left: 60px">    success, cameras = estimator.apply(features, pair_matches, None)<br/>    if not success:<br/>        raise RuntimeError("Homography estimation failed.")</pre>
<ol start="3">
<li>We make sure the <kbd>R</kbd> matrices have the correct type:</li>
</ol>
<pre style="padding-left: 60px">    for cam in cameras:<br/>        cam.R = cam.R.astype(np.float32)</pre>
<ol start="4">
<li class="mce-root">Then, we <kbd>return</kbd> all the parameters:</li>
</ol>
<pre style="padding-left: 60px">    return cameras</pre>
<p>It is possible to make these parameters better using a refiner, for example, <kbd>cv2.detail_BundleAdjusterRay</kbd>, but we'll keep things simple for now.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating the canvas for the panorama</h1>
                </header>
            
            <article>
                
<p>Now it's time to create the canvas. For that, we create a <kbd>warper</kbd> object based on our desired rotation schema. For simplicity, let's assume a planar model:</p>
<pre>    warper = cv2.PyRotationWarper('plane', 1)</pre>
<p class="mce-root">Then, we <kbd>enumerate</kbd> over all the connected images and get all the regions of interest in each of the images:</p>
<pre>    stitch_sizes, stitch_corners = [], []<br/>    for i, img in enumerate(conn_images):<br/>        sz = img.shape[1], img.shape[0]<br/>        K = cameras[i].K().astype(np.float32)<br/>        roi = warper.warpRoi(sz, K, cameras[i].R)<br/>        stitch_corners.append(roi[0:2])<br/>        stitch_sizes.append(roi[2:4])</pre>
<p>Finally, we estimate the final <kbd>canvas_size</kbd> based on all regions of interest:</p>
<pre>    canvas_size = cv2.detail.resultRoi(corners=stitch_corners, sizes=stitch_sizes)</pre>
<p>Now, let's see how to use the canvas size to blend all the images together.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Blending the images together</h1>
                </header>
            
            <article>
                
<p>First, we create a <kbd>MultiBandBlender</kbd> object, which will help us merge images together. Instead of just picking values from one or the other image, <kbd>blender</kbd> will do interpolation between the available values:</p>
<pre>    blender = cv2.detail_MultiBandBlender()<br/>    blend_width = np.sqrt(canvas_size[2] * canvas_size[3]) * 5 / 100<br/>    blender.setNumBands((np.log(blend_width) / np.log(2.) - 1.).astype(np.int))<br/>    blender.prepare(canvas_size)</pre>
<p>Then, for each of the connected images, we do the following:</p>
<ol>
<li>We <kbd>warp</kbd> the image and get the <kbd>corner</kbd> locations:</li>
</ol>
<pre style="padding-left: 60px">    for i, img in enumerate(conn_images):<br/>        K = cameras[i].K().astype(np.float32)<br/>        corner, image_wp = warper.warp(img, K, cameras[i].R,<br/>                                       cv2.INTER_LINEAR, cv2.BORDER_REFLECT)</pre>
<ol start="2">
<li>Then, calculate the <kbd>mask</kbd> of the image on the canvas:</li>
</ol>
<pre style="padding-left: 60px">        mask = 255 * np.ones((img.shape[0], img.shape[1]), np.uint8)<br/>        _, mask_wp = warper.warp(mask, K, cameras[i].R,<br/>                                 cv2.INTER_NEAREST, cv2.BORDER_CONSTANT)</pre>
<ol start="3">
<li>After that, convert the values into <kbd>np.int16</kbd> and <kbd>feed</kbd> it into <kbd>blender</kbd>:</li>
</ol>
<pre style="padding-left: 60px">        image_warped_s = image_wp.astype(np.int16)<br/>        blender.feed(cv2.UMat(image_warped_s), mask_wp, stitch_corners[i])</pre>
<ol start="4">
<li>After that, we use the <kbd>blend</kbd> function on <kbd>blender</kbd>, to get the final <kbd>result</kbd>, and save it:</li>
</ol>
<pre style="padding-left: 60px">    result, result_mask = blender.blend(None, None)<br/>    cv2.imwrite('result.jpg', result)</pre>
<p style="padding-left: 60px">We can also scale the image down to 600 pixels wide and display it:</p>
<pre style="padding-left: 60px">    zoomx = 600.0 / result.shape[1]<br/>    dst = cv2.normalize(src=result, dst=None, alpha=255.,<br/>                        norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)<br/>    dst = cv2.resize(dst, dsize=None, fx=zoomx, fy=zoomx)<br/>    cv2.imshow('panorama', dst)<br/>    cv2.waitKey()</pre>
<p>When we use the images from <a href="https://github.com/mamikonyana/yosemite-panorama">https://github.com/mamikonyana/yosemite-panorama</a>, we have this wonderful panorama picture in the end:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="assets/a7a027a5-0cc3-4c62-a350-720fe117d104.png" style="width:58.58em;height:26.58em;"/></p>
<p>You can see that it's not perfect and the white balance requires correcting from picture to picture, but this is a great start. In the next section, we will work on refining the stitching output.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Improving panorama stitching</h1>
                </header>
            
            <article>
                
<p>You can either play with the script that we already have and add or remove certain features (for example, you can add a white balance compensator, to make sure you have a smoother transition from one picture to another), or you can tweak other parameters to learn.</p>
<p>But know this—when you need a quick panorama, OpenCV also has a handy <kbd>Stitcher</kbd> class that does most of what we have discussed already:</p>
<pre>    images = [load_image(p, bps=8) for p in args.images]<br/><br/>    stitcher = cv2.Stitcher_create()<br/>    (status, stitched) = stitcher.stitch(images)</pre>
<p>This code snippet is probably a lot faster than uploading your photos to a panorama service to get a good picture<span>—s</span>o enjoy creating panoramas!</p>
<p>Don't forget to add some code to crop the panorama so it doesn't have black pixels!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned how to take simple images that we can take from our cameras with limited abilities—either with limited dynamic range or limited field of view and use OpenCV to merge multiple images into a single one that is better than the original one.</p>
<p>We left you with three scripts that you could build upon. Most importantly, there are still a lot of features missing from <kbd>panorama.py</kbd>, and there are a lot of other HDR techniques. Best of all, it's possible to do HDR and panorama stitching at the same time. <em>Wouldn't it be splendid to just look around from the mountain top at sunset? Imagine that!</em></p>
<p>This was the last chapter about camera photography. The rest of this book will focus on video monitoring and applying machine learning techniques to image processing tasks. </p>
<p><span>In the next chapter, we will focus on tracking visually salient and moving objects in a scene. This will give you an understanding of how to deal with non-static scenes. We will also explore how we can make an algorithm focus on what's important in a scene quickly, which is a technique known to speed up object detection, object recognition, object tracking, and content-aware image editing.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p>There are a lot of other topics to explore in computational photography:</p>
<ul>
<li>It's especially worth taking a look at the <strong>Exposure Fusion</strong> technique developed by Tom Mertens, et al. The <em>Exposure fusion</em> article by <span>Tom Mertens, Jan Kautz, and Frank Van Reeth, </span>in Computer Graphics and Applications, 2007, Pacific Graphics 2007, proceedings at 15th Pacific Conference on, pages 382–390, IEEE, 2007.</li>
<li><span>The</span> <span><em>Recovering High Dynamic Range Radiance Maps from Photographs</em></span><span> </span><span>article by Paul E Debevec and Jitendra Malik</span><span>, in ACM SIGGRAPH 2008 classes</span><span>, 2008,</span> page 31, ACM, 2008.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Attributions</h1>
                </header>
            
            <article>
                
<p><span>The <strong>Frozen River</strong> photo collection can be found at <a href="https://github.com/mamikonyana/frozen-river">https://github.com/mamikonyana/frozen-river</a> and is verified with a CC-BY-SA-4.0 license.</span></p>


            </article>

            
        </section>
    </body></html>