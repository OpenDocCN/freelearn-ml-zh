<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Creating Art with Style Transfer</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we will explore what was one of the most popular mainstream applications of deep learning in 2017—style transfer. We begin by introducing the concepts of style transfer and then its faster alternative, appropriately named fast neural style transfer. Similar to other chapters, we will provide the intuition behind the models (rather than granular details) and, in doing so, you will gain a deeper understanding and appreciation for the potential of deep learning algorithms. Unlike previous chapters, this chapter will focus more on the steps involved in getting the model working on iOS rather than building up the application, in order to keep it concise. </p>
<p>By the end of this chapter you will have achieved the following:</p>
<ul>
<li>Gained an intuitive understanding of how style transfer works</li>
<li>Gained hands-on experience of working with the Core ML Tools Python package and custom layers to get Keras models working in Core ML</li>
</ul>
<p>Let's get started by introducing style transfer and building our understanding of how it works.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transferring style from one image to another </h1>
                </header>
            
            <article>
                
<p>Imagine being able to have one of the greatest painters in history, such as Vincent van Gogh or Pablo Picasso, recreate a photo of your liking using their own unique style. In a nutshell, this is what style transfer allows us to do. Quite simply, it's the process of generating a photo using the style of one with the content of another, as shown here:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/1c79f7dd-e436-4a7f-8dbe-0a8917fdd108.png" style="width:45.67em;height:14.67em;"/></div>
<p>In this section, we will describe, albeit at a high level, how this works and then move on to an alternative that allows us to perform a similar process in significantly less time.</p>
<div class="packt_infobox">I encourage you to read the original paper, <em>A Neural Algorithm of Artistic Style</em>, by Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge, for a more comprehensive overview. This paper is available at <a href="https://arxiv.org/abs/1508.06576">https://arxiv.org/abs/1508.06576</a>.</div>
<p>At this stage, we have learned that neural networks learn by iteratively reducing a loss, calculated using some specified cost function that is to indicate how well the neural network did with respect to the expected output. The difference between the <strong>predicted output</strong> and <strong>expected output</strong> is then used to adjust the model's weights, through a process known as <strong>backpropagation</strong>, such to minimize this loss.</p>
<div class="packt_infobox">The preceding description (intentionally) skips the details of this process as our goal here is to provide an intuitive understanding, rather than the granular details. I recommend reading Andrew Trask's <em>Grokking Deep Learning</em> for a gentle introduction to the underlying details of neural networks.</div>
<p>Unlike the classification models we have worked with thus far, where the output is a probability distribution across some set of labels, we are instead interested in the model's generative abilities. That is, instead of adjusting the model's weights, we want to adjust the generated image's pixel values so as to reduce some defined cost function.</p>
<p>So if we were to define a cost function that could measure the loss between the generated image and content image, and another to measure the loss between the generated image and style image, we could then simply combine them. Thus we obtain the overall loss and use this to adjust the generated image pixels values, to create something that has the targets content in the style of our targets style as illustrated in the following image:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/17c518be-eba6-4893-9bad-489861742bae.png" style="width:43.50em;height:27.25em;"/></div>
<p>At this point, we have a general idea of the required process; what is left is building some intuition behind these cost functions. That is, how do you determine how well your generated image is, with respect to some content of the content image and with respect to a style of the style image? For this, we will backtrack a little and review what other layers of a CNN learn by inspecting each of their activations. </p>
<div class="packt_infobox">The details and images demonstrating what <strong><span>convolutional neural network</span>s</strong> (<strong>CNNs</strong>) learn have been taken from the paper <em>Visualizing and Understanding Convolutional Networks</em>, by Matthew D. Zeiler and Rob Fergus, which is available at <a href="https://arxiv.org/abs/1311.2901">https://arxiv.org/abs/1311.2901</a>.</div>
<p>A typical architecture of a CNN consists of a series of convolutional and pooling layers, which is then fed into a fully connected network (for case of classification), as illustrated in this image: </p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/68df6574-ed16-4453-9739-0762baa74183.png" style="width:57.00em;height:19.75em;"/></div>
<p>This flat representation misses an important property of a CNN, which is how, after each subsequent pair of convolution and pooling layers, the input's width and height reduce in size. The consequence of this is that the receptive field increases depth into the network; that is, deeper layers have a larger receptive field and thus capture higher level features than shallower layers.</p>
<p>To better illustrate what each layer learns, we will reference the paper <em>Visualizing and Understanding Convolutional Networks</em>, by Matthew D. Zeiler and Rob Fergus. In their paper (previously referenced), they pass through images from their training set to identify the image patches that maximize each layer's activations; by visualizing these patches, we get a sense of what each neuron (hidden unit) at each of the layers learns. Here is an screenshot showing some of these patches across a CNN:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/b02ef6da-b1d9-4d10-8df6-40fdae4fd5b8.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Source: Visualizing and Understanding Convolutional Networks; Matthew D Zeiler, Rob Fergus </div>
<p>What you can see in the preceding figure are nine image patches that maximize an individual hidden unit at each of the layers of this particular network. What has been omitted from the preceding figure is the variance in size; that is, the deeper you go, the larger the image patch will be.</p>
<p>What is hopefully obvious from the preceding image is that the shallower layers extract simple features. For example, we can see that a single hidden unit at <strong>Layer 1</strong> is activated by a diagonal edge and a single hidden <span>unit </span>at <strong>Layer 2</strong> is activated with a vertically striped patch. While the deeper layers extract higher-level <span>features</span>, or more complex features, again, in the preceding figure, we can see that a single hidden unit at <strong>Layer 4</strong> is activated by patches of dog faces. </p>
<p><span>We return to our task of defining a cost function for content and style, starting with the cost function for content. Given a content image and a generated image, we want to measure how close we are so as to minimize this difference, so that we retain the content. We can achieve this by selecting one of the deeper layers from our CNN, which we saw before have a large receptive field, and capture complex features. We pass through both the content images and the generated image and measure the distance between outputted activations (on this layer). This will hopefully seem logical given that the deeper layers learn complex features, such as a dog's face or a car, but decouple them from lower-level features such as edges, color, and textures. The following figure depicts this process:</span></p>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign"><span><img src="assets/6116a977-45e8-465f-9eb2-281c004ab46b.png"/></span></div>
<p>This takes care of our cost function for the content which can be easily tested by running a network that implements this. If implemented correctly, it should result in a generated image that looks similar to that of the input (content image). Let's now turn our attention to measuring style. </p>
<p>We saw in the preceding figure that shallower layers of a network learn simple features such as edges, textures, and color combinations. This gives us a clue as to which layers would be useful when trying to measure style, but we still need a way of extracting and measuring style. However, before we start, what exactly is style? </p>
<p>A quick search on <a href="http://www.dictionary.com/" target="_blank">http://www.dictionary.com/</a> reveals style being defined as <em>a distinctive appearance, typically determined by the principles according to which something is designed</em>. Let's take Katsushika Hokusai's <em>The Great Wave off Kanagawa</em> as an example:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/ed306f8b-c601-42bb-b2a2-f5aa88ea612c.png" style="width:38.83em;height:27.00em;"/></div>
<p><em>The Great Wave off Kanagawa</em> is an output of a process known as <strong>woodblock printing</strong>; this is where an artist's sketch is broken down into layers (carved wooden blocks), with each layer (usually one for each color) used to reproduce the art piece. It's similar to a manual printing press; this process produces a distinctive flat and simplistic style. Another dominate style (and possibly side-effect) that can be seen in the preceding image is that a limited range of colors is being used; for example, the water consists of no more than four colors.</p>
<p>The way we can capture style is as defined in the paper <em>A Neural Algorithm of Artistic Style</em>, by L. Gatys, A. Ecker, and M. Bethge. This way is to use a style matrix (also known as <strong>gram matrix</strong>) to find the correlation between the activations across different channels for a given layer. It is these correlations that define the style and something we can then use to measure the difference between our style image and generated image to influence the style of the generated image.</p>
<p>To make this more concrete, borrowing from an example used by Andrew Ng in his Coursera course on deep learning, let's take <strong>Layer 2</strong> from the earlier example. What the style matrix calculates is the correlation across all channels for a given layer. If we use the following illustration, showing nine activations from two channels, we can see that a correlation exists between vertical textures from the first channel with orange patches from the second channel. That is, when we see a vertical texture in the first channel, we would expect the image patches that maximize the second channel's activations to have an orange tint:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2f0c959f-2afa-49f8-a2eb-e100a3fa9196.png"/></div>
<p>This style matrix is calculated for both the style image and generated image, with our optimization forcing our generated image to adopt these correlations. With both style matrices calculated, we can then calculate the loss by simply finding the sum of the square difference between the two matrices. The following figure illustrates this process, as we have previously done when describing the content loss function:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/d0245269-6f46-4dbd-888c-e71fa9a0f5cc.png"/></div>
<p>With that, we have now concluded our introduction to style transfer, and hopefully given you some intuition of how we can use the network's perceptual understanding of images to extract content and style. This approach works well, but there is one drawback that we will address in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A faster way to transfer style</h1>
                </header>
            
            <article>
                
<p>As you may have inferred from the title of this section, the big drawback of the approach introduced in the previous section is that the process requires iterative optimization, as summarized in the following figure:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/80a70ed5-d413-45d0-bd9c-6fb5e73bdff4.png" style="width:33.75em;height:25.75em;"/></div>
<p>This optimization is akin to training, in terms of performing many iterations to minimize the loss. Therefore, it typically takes a considerable amount of time, even when using a modest computer. As implied at the start of this book, we ideally want to restrict ourselves to performing inference on the edge as it requires significantly less compute power and can be run in near-real time, allowing us to adopt it for interactive applications. Luckily for us, in their paper <em>Perceptual Losses for Real-Time Style Transfer and Super-Resolution</em>, J. Johnson, A. Alahi, and L. Fei-Fei describe a technique that decouples training (optimization) and inference for style transfer.</p>
<p>Previously, we described a network that took as its input a generated image, a style image, and a content image. The network minimized loss by iteratively adjusting the generated image using the loss functions for content and style; this provided the flexibility of allowing us to plug in any style and content image, but came at the cost of being computationally expensive, that is, slow. What if we sacrifice this flexibility for performance by restraining ourselves to a single style and, instead of performing the optimization to generate the image, train a CNN? The CNN would learn the style and, once trained, could generate a stylized image given a content image with a single pass through the network (inference). This is, in essence, what the paper <em>Perceptual Losses for Real-Time Style Transfer and Super-Resolution</em>, describes, and it is the network we will use in this chapter.</p>
<p>To better elucidate the difference between the previous approach and this approach, take a moment to review and compare the preceding figure with the following one:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/6c05bedd-135f-46f8-b726-125d44a0cb3e.png" style="width:43.83em;height:28.75em;"/> </div>
<p>Unlike the previous approach, where we optimized for a given set of content, style, and generated images and adjusted the generated image to minimize loss, we now feed a CNN with a set of content images and have the network generate the image. We then perform the same loss functions as described earlier for a single style. But, instead of adjusting the generated image, we adjust the weights of the networks using the gradients from the loss function. And we repeat until we have sufficiently minimized the mean loss across all of our content images.</p>
<p>Now, with our model trained, we can have our network stylize an image with a single pass, as shown here:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/b348bf5b-bde1-4e9f-8950-9653fa333ae9.png"/></div>
<p>Over the last two sections we have described, at a high-level, how these networks work. Now, it's time to build an application that takes advantage of all this. In the next section, we will quickly walk through converting the trained Keras model to Core ML before moving on to the main topic of this chapter—implementing custom layers for Core ML. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Converting a Keras model to Core ML</h1>
                </header>
            
            <article>
                
<p>Similar to what we did in the previous chapter, in this section we will be converting a trained Keras model into a Core ML model using the <strong>Core ML Tools</strong> package. To avoid any complications of setting up the environment on your local or remote machine, we will leverage the free Jupyter cloud service provided by Microsoft. Head over to <a href="https://notebooks.azure.com">https://notebooks.azure.com</a> and log in (or register if you haven't already).</p>
<p>Once logged in, click on the <span class="packt_screen">Libraries</span> menu link from the navigation bar, which will take you to a page containing a list of all of your libraries, similar to what is shown in the following screenshot: </p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/cdb2fe87-6e5c-4c63-a213-e61ec24d4608.png" style="width:40.67em;height:24.25em;"/></div>
<p>Next, click on the <span class="packt_screen">+ New Library</span> link to bring up the <span class="packt_screen">Create New Library</span> dialog:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/cb802108-7164-4387-8abb-c608e8383555.png" style="width:42.17em;height:21.25em;"/></div>
<p>Then, click on the <span class="packt_screen">From GitHub</span> tab and enter <kbd>https://github.com/packtpublishing/machine-learning-with-core-ml</kbd> in the <span class="packt_screen">GitHub repository</span> field. After that, give your library a meaningful name and click on the <span class="packt_screen">Import</span> button to begin the process of cloning the repository and creating the library. </p>
<p>Once the library has been created, you will be redirected to the root. From there, click on the <kbd>Chapter6/Notebooks</kbd> folder to open up the relevant folder for this chapter, and finally click on the Notebook <kbd>FastNeuralStyleTransfer_Keras2CoreML.ipynb</kbd>. Here is a screenshot of what you should see after clicking on the <kbd>Chapter6</kbd> folder:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/3705777f-bd95-4b87-b458-3baf5a7249d0.png" style="width:49.17em;height:22.75em;"/></div>
<div class="packt_infobox">It's beyond the scope of this book to walk you through the details of the Notebook, including the details of the network and training. For the curious reader, I have included the original Notebooks for each of the models used throughout this book in the accompanying <kbd>chapters</kbd> folder within the <kbd>training</kbd> folder.</div>
<p>With our Notebook now loaded, it's time to walk through each of the cells to create our Core ML model; all of the required code exists and all that remains is executing each of the cells sequentially. To execute a cell, you can either use the shortcut keys <em>Shift</em> + <em>Enter</em> or click on the <span class="packt_screen">Run</span> button in the toolbar (which will run the currently selected cell), as shown in the following screenshot: </p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/228ba8b2-345c-4aca-a980-c5b6eac51b3d.png"/></div>
<p>I will provide a brief explanation of what each cell does. Ensure that you execute each cell as we walk through them so that we all end up with the converted model, which we can then download and import into our iOS project:</p>
<pre>import helpers<br/>reload(helpers)</pre>
<p>We first import a module that includes the a function that will create and return the Keras model we want to convert:</p>
<pre>model = helpers.build_model('images/Van_Gogh-Starry_Night.jpg')</pre>
<p>We then use our <kbd>helpers</kbd> method <kbd>build_model</kbd> to create the model, passing in the style image that the model was trained on. Remember that we are using a feedforward network that has been trained on a single style; while the network can be reused for different styles, the weights are unique per style. </p>
<div class="packt_infobox">Calling <kbd>build_model</kbd> will take some time to return; this is because the model uses a trained model (VGG16) that is downloaded before returning.</div>
<p>Talking of weights (previously trained model), let's now load them by running the following cell:</p>
<pre>model.load_weights('data/van-gogh-starry-night_style.h5')</pre>
<p>Similar to the aforementioned code, we are passing in the weights for the model that was trained on Vincent van Gogh's <em>Starry Night</em> painting for its style.</p>
<p>Next, let's inspect the architecture of the model by calling the <kbd>summary</kbd> method on the model itself:</p>
<pre>model.summary()</pre>
<p>Calling this will return, as the name suggests, a summary of our model. Here is an extract of the summary produced:</p>
<pre>____________________________________________________________________<br/>Layer (type) Output Shape Param # Connected to <br/>====================================================================<br/>input_1 (InputLayer) (None, 320, 320, 3) 0 <br/>____________________________________________________________________<br/>zero_padding2d_1 (ZeroPadding2D) (None, 400, 400, 3) 0 input_1[0][0] <br/>____________________________________________________________________<br/>conv2d_1 (Conv2D) (None, 400, 400, 64) 15616 zero_padding2d_1[0][0] <br/>____________________________________________________________________<br/>batch_normalization_1 (BatchNorm (None, 400, 400, 64) 256 conv2d_1[0][0] <br/>____________________________________________________________________<br/>activation_1 (Activation) (None, 400, 400, 64) 0 batch_normalization_1[0][0] <br/>____________________________________________________________________<br/>...<br/>...<br/>____________________________________________________________________<br/><strong>res_crop_1 (Lambda) (None, 92, 92, 64) 0 add_1[0][0]</strong> <br/>____________________________________________________________________<br/>...<br/>... <br/>____________________________________________________________________<br/><strong>rescale_output (Lambda) (None, 320, 320, 3) 0 conv2d_16[0][0]</strong> <br/>====================================================================<br/>Total params: 552,003<br/>Trainable params: 550,083<br/>Non-trainable params: 1,920</pre>
<p>As previously mentioned, it's out of scope to go into the details of Python, Keras, or the specifics of this model. Instead I present an extract here to highlight the custom layers embedded in the model (the bold lines). In the context of Core ML Tools, custom layers are layers that have not been defined and, therefore, are not handled during the conversion process, so it is our responsibility to handle these. You can think of the conversion process as a process of mapping layers from a machine learning framework, such as Keras, to Core ML. If no mapping exists, then it is left up to us to fill in the details, as illustrated in the following figure:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/1022c0b7-25f4-4e6f-877f-5e7728ac0d4e.png" style="width:41.75em;height:22.75em;"/> </div>
<p>The two custom layers shown previously are both Lambda layers; a Lambda layer is a special Keras class that conveniently allows writing quick-and-dirty layers using just a function or a Lambda expression (similar to a closure in Swift). Lambda is useful for layers that don’t have a state and are commonly seen in Keras models for doing basic computations. Here, we see two being used, <kbd>res_crop</kbd> and <kbd>rescale_output</kbd>.</p>
<p><kbd>res_crop</kbd> is part of the ResNet block that crops (as implied by the name) the output; the function is simple enough, with its definition shown in the following code: </p>
<pre>def res_crop(x):<br/>    return x[:, 2:-2, 2:-2] </pre>
<div class="packt_infobox">I refer you to the paper <em>Deep Residual Learning for Image Recognition</em>, by K. He, X. Zhang, S. Ren, and J. Sun to learn more about ResNet and residual blocks, available here at <a href="https://arxiv.org/pdf/1512.03385.pdf">https://arxiv.org/pdf/1512.03385.pdf</a>.</div>
<p>Essentially, all that this is doing is cropping the outputs with a padding of 2 for the width and height axis. We can further interrogate this by inspecting the input and output shapes of this layer, by running the following cell:</p>
<pre>res_crop_3_layer = [layer for layer in model.layers if layer.name == 'res_crop_3'][0] <br/><br/>print("res_crop_3_layer input shape {}, output shape {}".format(<br/>    res_crop_3_layer.input_shape, res_crop_3_layer.output_shape))</pre>
<p>This cell prints the input and output shape of the layer <kbd>res_crop_3_layer</kbd>; the layer receives a tensor of shape <kbd>(None, 88, 88, 64)</kbd> and outputs a tensor of shape <kbd>(None, 84, 84, 64)</kbd>. Here the tuple is broken down into: (batch size, height, width, channels). The batch size is set to <kbd>None</kbd>, indicating that it is dynamically set during training.</p>
<p>Our next <span>Lambda layer is <kbd>rescale_output</kbd>; this is</span> used at the end of the network to rescale the outputs from the Convolution 2D layer, which passes its data through a tanh activation. This forces our data to be constrained between -1.0 and 1.0, where as we want it in a range of 0 and 255 so that we can convert it into an image. As we did before, let's look at its definition to get a better idea of what this layer does, as shown in the following code:</p>
<pre>def rescale_output(x):<br/>    return (x+1)*127.5 </pre>
<p class="mce-root">This method performs an element-wise operation that maps the values -1.0 and 1.0 to 0 and 255. Similar to the preceding method (<kbd>res_crop</kbd>), we can inspect the input and output shapes of this layer by running the following cell:</p>
<pre>rescale_output_layer = [layer for layer in model.layers if layer.name == 'rescale_output'][0]<br/><br/>print("rescale_output_layer input shape {}, output shape {}".format(<br/>    rescale_output_layer.input_shape, <br/>    rescale_output_layer.output_shape))</pre>
<p>Once run, this cell prints the layer's input shape of <kbd>(None, 320, 320, 3)</kbd> and output shape of <kbd>(None, 320, 320, 3)</kbd>. This tells us that this layer doesn't change the shape of the tensor, as well as shows us the output dimensions of our image as 320 x 320 with three channels (RGB). </p>
<p>We have now reviewed the custom layers and seen what they actually do; the next step is to perform the actual conversion. Run the following cell to ensure that the environment has the Core ML Tools modules installed:</p>
<pre>!pip install coremltools</pre>
<p>Once installed, we can load the required modules by running the following cell:</p>
<pre>import coremltools<br/>from coremltools.proto import NeuralNetwork_pb2, FeatureTypes_pb2</pre>
<p>In this instance, I have prewarned you that our model contains custom layers; in some (if not most) instances, you may discover this <span>only </span>when the conversion process fails. Let's see exactly what this looks like by running the following cell and examining its output:</p>
<pre>coreml_model = coremltools.converters.keras.convert(<br/>    model, <br/>    input_names=['image'], <br/>    image_input_names=['image'], <br/>    output_names="output")</pre>
<p>In the preceding snippet, we are passing our model to the method <kbd>coremltools.converters.keras.convert</kbd>, which is responsible for converting our Keras model to Core ML. Along with the model, we pass in the input and output names for our model, as well as setting <kbd>image_input_names</kbd> to inform the method that we want the input <kbd>image</kbd> to be treated as an image rather than a multidimensional array.</p>
<p>As expected, after running this cell, you will receive an error. If you scroll to the bottom of the output, you will see the line <kbd>ValueError: Keras layer '&lt;class 'keras.layers.core.Lambda'&gt;' not supported</kbd>. At this stage, you will need to review the architecture of your model to identify the layer that caused the error and proceed with what you are about to do.</p>
<p>By enabling the parameter <kbd>add_custom_layers</kbd> in the conversion call, we prevent the method from failing when the converter encounters a layer it doesn't recognize. A placeholder layer named custom will be inserted as part of the conversion process. In addition to recognizing custom layers, we can pass in a <kbd>delegate</kbd> function to the parameter <kbd>custom_conversion_functions</kbd>, which allows us to add metadata to the model's specification stating how the custom layer will be handled. </p>
<p>Let's create this <kbd>delegate</kbd> method now; run the cell with the following code:</p>
<pre>def convert_lambda(layer):<br/>    if layer.function.__name__ == 'rescale_output':<br/>        params = NeuralNetwork_pb2.CustomLayerParams()<br/>        params.className = "RescaleOutputLambda"<br/>        params.description = "Rescale output using ((x+1)*127.5)"<br/>        return params<br/>    elif layer.function.__name__ == 'res_crop':<br/>        params = NeuralNetwork_pb2.CustomLayerParams()<br/>        params.className = "ResCropBlockLambda"<br/>        params.description = "return x[:, 2:-2, 2:-2]"<br/>        return params<br/>    else:<br/>        raise Exception('Unknown layer')<br/>    return None </pre>
<p>This <kbd>delegate</kbd> is passed each custom layer the converter comes across. Because we are dealing with two different layers, we first check which layer we are dealing with and then proceed to create and return an instance of <kbd>CustomLayerParams</kbd>. This class allows us to add some metadata used when creating the model's specification for the Core ML conversion. Here we are setting its <kbd>className</kbd>, which is the name of the Swift (or Objective-C) class in our iOS project that implements this layer, and <kbd>description</kbd>, which is the text shown in Xcode 's ML model viewer.</p>
<p>With our <kbd>delegate</kbd> method now implemented, let's rerun the converter, passing in the appropriate parameters, as shown in the following code: </p>
<pre>coreml_model = coremltools.converters.keras.convert(<br/>    model, <br/>    input_names=['image'], <br/>    image_input_names=['image'], <br/>    output_names="output",<br/>    add_custom_layers=True,<br/>    custom_conversion_functions={ "Lambda": convert_lambda })</pre>
<p><span>If all goes well, you should see the converter output each layer it visits, with no error messages, and finally returning a Core ML model instance. We can now add metadata to our model, which is what is displayed in Xcode 's ML model views:</span></p>
<pre><span>coreml_model.author = 'Joshua Newnham'<br/>coreml_model.license = 'BSD'<br/>coreml_model.short_description = 'Fast Style Transfer based on the style of Van Gogh Starry Night'<br/>coreml_model.input_description['image'] = 'Preprocessed content image'<br/>coreml_model.output_description['output'] = 'Stylized content image' </span></pre>
<p>At this stage, we could save the model and import into Xcode , but there is just one more thing I would like to do to make our life a little easier. At its core (excuse the pun), the Core ML model is a specification of the network (including the model description, model parameters, and metadata) used by Xcode to build the model when imported. We can get a reference to this specification by calling the following statement:</p>
<pre>spec = coreml_model.get_spec() </pre>
<p>With reference to the specification of the models, we next search for the output layer, as shown in the following snippet:</p>
<pre>output = [output for output in spec.description.output if output.name == 'output'][0]</pre>
<p>We can inspect the output simply by printing it out; run the cell with the following code to do just that:</p>
<pre>output</pre>
<p>You should see something similar to this:</p>
<pre>name: "output"<br/>shortDescription: "Stylized content image"<br/>type {<br/>  multiArrayType {<br/>    shape: 3<br/>    shape: 320<br/>    shape: 320<br/>    dataType: DOUBLE<br/>  }<br/>}</pre>
<p>Take note of the type, which is currently <kbd>multiArrayType</kbd> (its iOS equivalent is <kbd>MLMultiArray</kbd>). This is fine but would require us to explicitly convert it to an image; it would be more convenient to just have our model output an image instead of a multidimensional array. We can do this by simply modifying the specification. Specifically, in this instance, this means populating the type's <kbd>imageType</kbd> properties to hint to Xcode that we are expecting an image. Let's do that now by running the cell with this code:</p>
<pre>output.type.imageType.colorSpace = FeatureTypes_pb2.ImageFeatureType.ColorSpace.Value('RGB') <br/>    <br/>output.type.imageType.width = width <br/>output.type.imageType.height = height<br/>    <br/>coreml_model = coremltools.models.MLModel(spec) </pre>
<p>We first set the color space to RGB, then we set the expected width and height of the image. Finally, we create a new model by passing in the updated specification with the statement <kbd>coremltools.models.MLModel(spec)</kbd>. Now, if you interrogate the output, you should see something like the following output:</p>
<pre>name: "output"<br/>shortDescription: "Stylized content image"<br/>type {<br/>  imageType {<br/>    width: 320<br/>    height: 320<br/>    colorSpace: RGB<br/>  }<br/>}</pre>
<p>We have now saved ourselves a whole lot of code to perform this conversion; our final step is to save the model before importing it into Xcode . Run the last cell, which does just that:</p>
<pre>coreml_model.save('output/FastStyleTransferVanGoghStarryNight.mlmodel')</pre>
<p>Before closing the browser, let's download the model. You can do this by returning the <kbd>Chapter6/Notebooks</kbd> directory and drilling down into the <kbd>output</kbd> folder. Here you should see the file <kbd>FastStyleTransferVanGoghStarryNight.mlmodel</kbd>; simply right-click on it and select the <span class="packt_screen">Download</span> menu item (or do it by left-clicking and selecting the <span class="packt_screen">Download</span> toolbar item):</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/c0792c6c-82da-4984-8e22-5463596401dd.png"/></div>
<p>With our model in hand, it's now time to jump into Xcode and implement those custom layers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building custom layers in Swift</h1>
                </header>
            
            <article>
                
<p>In this section, we will be mainly focusing on implementing the custom layers that our model is dependent on, and we'll omit a lot of the application's details by working with an existing template—a structure you have no doubt become quite familiar with.</p>
<p>If you haven't done so already, pull down the latest code from the accompanying repository:<span> <a href="https://github.com/packtpublishing/machine-learning-with-core-ml">https://github.com/packtpublishing/machine-learning-with-core-ml</a></span>. Once downloaded, navigate to the directory <kbd>Chapter6/Start/StyleTransfer/</kbd><span> </span>and open the project <kbd>StyleTransfer.xcodeproj</kbd>. Once loaded, you will see the project for this chapter:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/0de45713-7155-44ad-8787-cd62a5d02fa3.png" style="width:43.83em;height:33.58em;"/></div>
<p>The application consists of two view controllers. The first, <kbd>CameraViewController</kbd>, provides the user with a live stream of the camera and the ability to take a photo. When a photo is taken, the controller presents the other view controller, <kbd>StyleTransferViewController</kbd>, passing along with the captured photo. <kbd>StyleTransferViewController</kbd> then presents the image, along with a horizontal <kbd>CollectionView</kbd> at the bottom containing a set of styles that the user can select by tapping on them. </p>
<p>Each time the user selects a style, the controller updates the <kbd>ImageProcessors</kbd> style property and then calls its method, <kbd>processImage</kbd>, passing in the assigned image. It is here that we will implement the functionality responsible for passing the image to the model and returning the result via the assigned delegates <kbd>onImageProcessorCompleted</kbd> method, which is then presented to the user.</p>
<p>Now, with our project loaded, let's import the model we have just created; locate the downloaded <kbd>.mlmodel</kbd> file and drag it onto Xcode . Once imported, we select it from the left-hand \ panel to inspect the metadata, to remind ourselves what we need to implement:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/a97e6658-5e37-4a88-8712-177aedd2b358.png" style="width:44.00em;height:30.33em;"/></div>
<p>By inspecting the model, we can see that it is expecting an input RGB image of size 320 x 320, and it will output an image with the same dimensions. We can also see that the model is expecting two custom layers named <kbd>ResCropBlockLambda</kbd> and <kbd>RescaleOutputLambda</kbd>. Before implementing these classes, let's hook the model up and, just for fun, see what happens when we try to run it without the custom layers implemented. </p>
<p>Select <kbd>ImageProcessor.swift</kbd> from the left<span>-hand-side</span> panel; in this project, we will get the Vision framework to do all the preprocessing. Start by adding the following properties within the body of the <kbd>ImageProcessor</kbd> class, somewhere such as underneath the <kbd>style</kbd> property:</p>
<pre>lazy var vanCoghModel : VNCoreMLModel = {<br/>    do{<br/>        let model = try VNCoreMLModel(for: FastStyleTransferVanGoghStarryNight().model)<br/>        return model<br/>    } catch{<br/>        fatalError("Failed to obtain VanCoghModel")<br/>    }<br/>}()</pre>
<p>The first property returns an instance of <kbd>VNCoreMLModel</kbd>, wrapping our <kbd>FastStyleTransferVanGoghStarryNight</kbd> model. Wrapping our model is necessary to make it compatible with the Vision framework's requests classes.</p>
<p>Just underneath, add the following snippet, which will be responsible for returning the appropriate <kbd>VNCoreMLModel</kbd>, based on the selected style:</p>
<pre>var model : VNCoreMLModel{<br/>    get{<br/>        if self.style == .VanCogh{<br/>            return self.vanCoghModel<br/>        }<br/>        <br/>        // default<br/>        return self.vanCoghModel<br/>    }<br/>}</pre>
<p>Finally, we create the method that will be responsible for returning an instance of <kbd>VNCoreMLRequest</kbd>, based on the currently selected model (determined by the current <kbd>style</kbd>):</p>
<pre>func getRequest() -&gt; VNCoreMLRequest{<br/>    let request = VNCoreMLRequest(<br/>        model: self.model,<br/>        completionHandler: { [weak self] request, error in<br/>            self?.processRequest(for: request, error: error)<br/>        })<br/>    request.imageCropAndScaleOption = .centerCrop<br/>    return request<br/>}</pre>
<p><kbd>VNCoreMLRequest</kbd> is responsible for performing the necessary preprocessing on the input image before passing it to the assigned Core ML model. <span>We instantiate</span> <kbd>VNCoreMLRequest</kbd><span>, passing in a completion handler that will simply pass its results to the </span><kbd>processRequest</kbd><span> method, of the <kbd>ImageProcessor</kbd> class, when called. We also set the </span><kbd>imageCropAndScaleOption</kbd><span> to </span><kbd>.centerCrop</kbd><span> so that our image is resized to 320 x 320 whilst maintaining its aspect ratio (cropping the centered image on its longest side, if necessary).</span></p>
<p>With our properties now defined, it's time to jump into the <kbd>processImage</kbd> method to initiate the actual work; add the following code (shown in bold, and replacing <span>the </span><kbd>// TODO</kbd><span> comments</span>):</p>
<pre>public func processImage(ciImage:CIImage){        <br/>    DispatchQueue.global(qos: .userInitiated).async {<br/><strong>        let handler = VNImageRequestHandler(ciImage: ciImage)</strong><br/><strong>        do {</strong><br/><strong>            try handler.perform([self.getRequest()])</strong><br/><strong>        } catch {</strong><br/><strong>            print("Failed to perform classification.\n\(error.localizedDescription)")</strong><br/><strong>        }</strong><br/>    }<br/>}</pre>
<p>The preceding method is our entry point to stylizing an image; we start by<span> instantiating an instance of </span><kbd>VNImageRequestHandler</kbd><span>, passing in the image, and initiating the process by calling </span><span>the <kbd>perform</kbd> method. Once the analysis has finished, the request will call the <kbd>delegate</kbd> we assigned to it, <kbd>processRequest</kbd>, passing in a reference of the associated request and the results (or errors if any). Let's flesh out this method now:</span></p>
<pre>func processRequest(for request:VNRequest, error: Error?){<br/>    guard let results = request.results else {<br/>        print("ImageProcess", #function, "ERROR:",<br/>              String(describing: error?.localizedDescription))<br/>        self.delegate?.onImageProcessorCompleted(<br/>            status: -1,<br/>            stylizedImage: nil)<br/>        return<br/>    }<br/>    <br/>    let stylizedPixelBufferObservations =<br/>        results as! [VNPixelBufferObservation]<br/>    <br/>    guard stylizedPixelBufferObservations.count &gt; 0 else {<br/>        print("ImageProcess", #function,"ERROR:",<br/>              "No Results")<br/>        self.delegate?.onImageProcessorCompleted(<br/>            status: -1,<br/>            stylizedImage: nil)<br/>        return<br/>    }<br/>    <br/>    guard let cgImage = stylizedPixelBufferObservations[0]<br/>        .pixelBuffer.toCGImage() else{<br/>        print("ImageProcess", #function, "ERROR:",<br/>              "Failed to convert CVPixelBuffer to CGImage")<br/>        self.delegate?.onImageProcessorCompleted(<br/>            status: -1,<br/>            stylizedImage: nil)<br/>        return<br/>    }<br/>    <br/>    DispatchQueue.main.sync {<br/>        self.delegate?.onImageProcessorCompleted(<br/>            status: 1,<br/>            stylizedImage:cgImage)<br/>    }<br/>}</pre>
<div class="packt_infobox"><span>While </span><kbd>VNCoreMLRequest</kbd><span> is responsible for the image analysis,</span> <kbd>VNImageRequestHandler</kbd> <span>is responsible for executing the request (or requests). </span></div>
<p>If no errors occurred during the analysis, we should be returned the instance of our request with its results property set. As we are only expecting one request and result type, we cast the results to an array of <kbd>VNPixelBufferObservation</kbd>, a type of observation suitable for image analysis with a Core ML model whose role is image-to-image processing, such as our style transfer model.</p>
<p>We can get a reference to our stylized image via the property <kbd>pixelBuffer</kbd>, from the observation obtained from the results. And then we can call the extension method <kbd>toCGImage</kbd> (found in <kbd>CVPixelBuffer+Extension.swift</kbd>) to conveniently obtain the output in a format we can easily use, in this case, updating the image view.</p>
<p>As previously discussed, let's see what happens when we try to run an image through our model without implementing the custom layers. Build and deploy to a device and proceed to take a photo, then select the Van Cogh style from the styles displayed. In doing so, you will observe the build failing and reporting the error: <kbd>Error creating Core ML custom layer implementation from factory for layer "RescaleOutputLambda"</kbd> (as we were expecting). </p>
<p>Let's address this now by implementing each of our custom layers, starting with the <kbd>RescaleOutputLambda</kbd> class. Create a new Swift file named <kbd>RescaleOutputLamdba.class</kbd> and replace the template code with the following:</p>
<pre>import Foundation<br/>import CoreML<br/>import Accelerate<br/><br/>@objc(RescaleOutputLambda) class RescaleOutputLambda: NSObject, MLCustomLayer {    <br/>    required init(parameters: [String : Any]) throws {<br/>        super.init()<br/>    }<br/>    <br/>    func setWeightData(_ weights: [Data]) throws {<br/>        <br/>    }<br/>    <br/>    func outputShapes(forInputShapes inputShapes: [[NSNumber]]) throws<br/>        -&gt; [[NSNumber]] {<br/>            <br/>    }<br/>    <br/>    func evaluate(inputs: [MLMultiArray], outputs: [MLMultiArray]) throws {<br/>        <br/>    }<br/>}</pre>
<p>Here, we have created a concrete class of the protocol <kbd>MLCustomLayer</kbd>, a protocol that defines the behavior of a custom layer in our neural network model. The protocol consists of four required methods and one optional method, which are as follows:</p>
<ul>
<li><kbd>init(parameters)</kbd>: Initializes the custom layer implementation that is passed the dictionary <kbd>parameters</kbd> that includes any additional configuration options for the layer. As you may recall, we created an instance of <kbd>NeuralNetwork_pb2.CustomLayerParams</kbd> for each of our custom layers when converting our Keras model. Here we can add more entries, which will be passed into this dictionary. This provides some flexibility, such as allowing you to adjust your layer based on the set parameters. </li>
<li><kbd>setWeightData()</kbd>: Assigns the weights for the connections within the layer (for layers with trainable weights). </li>
<li><kbd>outputShapes(forInputShapes)</kbd>: This determines how the layer modifies the size of the input data. Our <kbd>RescaleOutputLambda</kbd> layer doesn't change the size of the layer, so we simply need to return the input shape, but we will make use of this when implementing the next custom layer. </li>
<li><kbd>evaluate(inputs, outputs)</kbd>: This performs the actual computation; this method is required and gets called when the model is run on the CPU.</li>
<li><kbd>encode(commandBuffer, inputs, outputs)</kbd>: This method is optional and acts as an alternative to the method <kbd>evaluate</kbd>, which uses the GPU rather than the CPU.</li>
</ul>
<p>Because we are not passing in any custom parameters or setting any trainable weights, we can skip the constructor and <kbd>setWeightData</kbd> methods; let's walk through the remaining methods, starting with <kbd>outputShapes(forInputShapes)</kbd>. </p>
<p>As previously mentioned, this layer doesn't change the shape of the input, therefore, we can simply return the input shape, as shown in the following code: </p>
<pre>func outputShapes(forInputShapes inputShapes: [[NSNumber]]) throws<br/>    -&gt; [[NSNumber]] {<br/>        <strong>return inputShapes</strong><br/>}</pre>
<p>With our <kbd>outputShapes(forInputShapes)</kbd> method now implemented, let's turn our attention to the workhorse of the layer responsible for performing the actual computation, the <kbd>evaluate</kbd> method. The <kbd>evaluate</kbd> method receives an array of <kbd>MLMultiArray</kbd> objects as inputs, along with another array of <kbd>MLMultiArray</kbd> objects, where it is expected to store the results. Having the <kbd>evaluate</kbd> method accept arrays for its input and outputs allows for greater flexibility in supporting different architectures, but, in this example, we are expecting only one input and one output. </p>
<p>As a reminder, this layer is for scaling each element from a range of -1.0 - 1.0 to a range of 0 - 255 (that's what a typical image would be expecting). The simplest approach is to iterate through each element and scale it using the equation we saw in Python: <kbd>((x+1)*127.5</kbd>. This is exactly what we'll do; add the following code (in bold) to the body of your <kbd>evaluate</kbd> method:</p>
<pre>func evaluate(inputs: [MLMultiArray],outputs: [MLMultiArray]) throws {    <br/>    <strong>let rescaleAddition = 1.0</strong><br/><strong>    let rescaleMulitplier = 127.5</strong><br/><strong>    </strong><br/><strong>    for (i, input) in inputs.enumerated(){</strong><br/><strong>        // expecting [1, 1, Channels, Kernel Width, Kernel Height]</strong><br/><strong>        let shape = input.shape </strong><br/><strong>        for c in 0..&lt;shape[2].intValue{</strong><br/><strong>            for w in 0..&lt;shape[3].intValue{</strong><br/><strong>                for h in 0..&lt;shape[4].intValue{</strong><br/><strong>                    let index = [</strong><br/><strong>                        NSNumber(value: 0),</strong><br/><strong>                        NSNumber(value: 0),</strong><br/><strong>                        NSNumber(value: c),</strong><br/><strong>                        NSNumber(value: w),</strong><br/><strong>                        NSNumber(value: h)]</strong><br/><strong>                    let outputValue = NSNumber(</strong><br/><strong>                        value:(input[index].floatValue + rescaleAddition)</strong><br/><strong>                            * rescaleMulitplier)</strong><br/><strong>                    </strong><br/><strong>                    outputs[i][index] = outputValue</strong><br/><strong>                }</strong><br/><strong>            }</strong><br/><strong>        }</strong><br/><strong>    }</strong><br/>} </pre>
<p>The bulk of this method is made up of code used to create the index for obtaining the appropriate value from the input and pointing to its output counterpart. Once an index has been created, the Python formula is ported across to Swift: <kbd>input[index].doubleValue + rescaleAddition) * rescaleMulitplier</kbd>. This concludes our first custom layer; let's now implement our second customer layer, <kbd>ResCropBlockLambda</kbd>. </p>
<p>Create a new file called <kbd>ResCropBlockLambda.swift</kbd> and add the following code, overwriting any existing code:</p>
<pre>import Foundation<br/>import CoreML<br/>import Accelerate<br/><br/>@objc(ResCropBlockLambda) class ResCropBlockLambda: NSObject, MLCustomLayer {<br/>    <br/>    required init(parameters: [String : Any]) throws {<br/>        super.init()<br/>    }<br/>    <br/>    func setWeightData(_ weights: [Data]) throws {<br/>    }<br/>    <br/>    func outputShapes(forInputShapes inputShapes: [[NSNumber]]) throws<br/>        -&gt; [[NSNumber]] {<br/>    }<br/>    <br/>    func evaluate(inputs: [MLMultiArray], outputs: [MLMultiArray]) throws {<br/>    }<br/>}</pre>
<p>As we have done with the previous custom layer, we have stubbed out all the required methods as determined by the <kbd>MLCustomLayer</kbd> protocol. Once again, we can ignore the constructor and <kbd>setWeightData</kbd> method as neither are used in this layer. </p>
<p>If you recall, and as the name suggests, the function of this layer is to crop the width and height of one of the inputs of the residual block. We need to reflect this within the <kbd>outputShapes(forInputShapes)</kbd> method, so that the network knows the input dimensions for subsequent layers. Update the <kbd>outputShapes(forInputShapes)</kbd> method with the following code:</p>
<pre>func outputShapes(forInputShapes inputShapes: [[NSNumber]]) throws<br/>    -&gt; [[NSNumber]] {        <br/><strong>        return [[NSNumber(value:inputShapes[0][0].intValue),</strong><br/><strong>                 NSNumber(value:inputShapes[0][1].intValue),</strong><br/><strong>                 NSNumber(value:inputShapes[0][2].intValue),</strong><br/><strong>                 NSNumber(value:inputShapes[0][3].intValue - 4),</strong><br/><strong>                 NSNumber(value:inputShapes[0][4].intValue - 4)]];</strong><br/>}</pre>
<p>Here, we are removing a constant of <kbd>4</kbd> from the width and height, essentially padding 2 from the width and height. Next, we implement the <kbd>evaluate</kbd> method, which performs this cropping. Replace the <kbd>evaluate</kbd> method with the following code:</p>
<pre>func evaluate(inputs: [MLMultiArray], outputs: [MLMultiArray]) throws {<br/><strong>    for (i, input) in inputs.enumerated(){</strong><br/><strong>        </strong><br/><strong>        // expecting [1, 1, Channels, Kernel Width, Kernel Height]</strong><br/><strong>        let shape = input.shape</strong><br/><strong>        for c in 0..&lt;shape[2].intValue{</strong><br/><strong>            for w in 2...(shape[3].intValue-4){</strong><br/><strong>                for h in 2...(shape[4].intValue-4){</strong><br/><strong>                    let inputIndex = [</strong><br/><strong>                        NSNumber(value: 0),</strong><br/><strong>                        NSNumber(value: 0),</strong><br/><strong>                        NSNumber(value: c),</strong><br/><strong>                        NSNumber(value: w),</strong><br/><strong>                        NSNumber(value: h)]</strong><br/><strong>                    </strong><br/><strong>                    let outputIndex = [</strong><br/><strong>                        NSNumber(value: 0),</strong><br/><strong>                        NSNumber(value: 0),</strong><br/><strong>                        NSNumber(value: c),</strong><br/><strong>                        NSNumber(value: w-2),</strong><br/><strong>                        NSNumber(value: h-2)]</strong><br/><strong>                    </strong><br/><strong>                    outputs[i][outputIndex] = input[inputIndex]</strong><br/><strong>                }</strong><br/><strong>            }</strong><br/><strong>        }</strong><br/><strong>    }</strong><br/>} </pre>
<p>Similar to the <kbd>evaluate</kbd> method of our <kbd>RescaleOutputLambda</kbd> layer, the bulk of this method has to do with creating the indices for the input and output arrays. We simply pad it by restraining the ranges of our loops to the desired width and height. </p>
<p>Now, if you build and run the project, you will be able to run an image through the Van Gogh network getting a stylized version of it back, similar to what is shown in the following image:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/74bc12a2-2c87-4e58-8bd2-b30ead776cf2.png" style="width:44.00em;height:32.67em;"/></div>
<p>When running on the simulator, the whole process took approximately <strong>22.4 seconds</strong>. In the following two sections, we will spend some time looking at how we can reduce this. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Accelerating our layers </h1>
                </header>
            
            <article>
                
<p>Let's return to the layer <kbd>RescaleOutputLambda</kbd> and see where we might be able to shed a second or two off the processing time. As a reminder, the function of this layer is to rescale each element in the output, where our output can be thought of as a large vector. Luckily for us, Apple provides an efficient framework and API for just this. Instead of operating on each element within a loop, we will take advantage of the <kbd>Accelerate</kbd> framework and its vDSPAPI to perform this operation in a single step. This process is called <strong>vectorization</strong> and is made possible by exploiting the CPU's <strong>Single Instruction, Multiple Data</strong> (<span><strong>SIMD</strong>)</span> instruction set. Return to the <kbd>RescaleOutputLambda</kbd> class and update the <kbd>evaluate</kbd> method with the following code:</p>
<pre>func evaluate(inputs: [MLMultiArray], outputs: [MLMultiArray]) throws {<br/>    var rescaleAddition : Float = 1.0<br/>    var rescaleMulitplier : Float = 127.5<br/>    <br/><strong>    for (i, _) in inputs.enumerated(){</strong><br/><strong>        </strong><br/><strong>        let input = inputs[i]</strong><br/><strong>        let output = outputs[i]</strong><br/><strong>        </strong><br/><strong>        let count = input.count</strong><br/><strong>        let inputPointer = UnsafeMutablePointer&lt;Float&gt;(</strong><br/><strong>            OpaquePointer(input.dataPointer)</strong><br/><strong>        )</strong><br/><strong>        let outputPointer = UnsafeMutablePointer&lt;Float&gt;(</strong><br/><strong>            OpaquePointer(output.dataPointer)</strong><br/><strong>        )</strong><br/><strong>        </strong><br/><strong>        vDSP_vsadd(inputPointer, 1,</strong><br/><strong>                   &amp;rescaleAddition,</strong><br/><strong>                   outputPointer, 1,</strong><br/><strong>                   vDSP_Length(count))</strong><br/><strong>        </strong><br/><strong>        vDSP_vsmul(outputPointer, 1,</strong><br/><strong>                   &amp;rescaleMulitplier,</strong><br/><strong>                   outputPointer, 1,</strong><br/><strong>                   vDSP_Length(count))</strong><br/><strong>    }</strong><br/>}</pre>
<p>In the preceding code, we first get a reference to the pointers to each of the input and output buffers, wrapping them in <kbd>UnsafeMutablePointer</kbd>, as required by the vDSP functions. Then, it's simply a matter of applying each of our scaling operations using the equivalent vDSP functions, which we will walk through.</p>
<p>First, we add our constant of <kbd>1</kbd> to the input and save the results in the output buffer, as shown in the following snippet: </p>
<pre>vDSP_vsadd(inputPointer, 1,<br/>           &amp;rescaleAddition,<br/>           outputPointer, 1,<br/>           vDSP_Length(count))</pre>
<p>Where the function <kbd>vDSP_vsadd</kbd> takes in a pointer to our vector (<kbd>inputPointer</kbd>) and adds <kbd>rescaleAddition</kbd> to each of its elements before storing it into the output. </p>
<p>Next, we apply our multiplier to each of the elements of the output (which currently has each of its values set to the input with 1 added to it); the code for this is shown in the following snippet:</p>
<pre>vDSP_vsmul(outputPointer, 1,<br/>           &amp;rescaleMulitplier,<br/>           outputPointer, 1,<br/>           vDSP_Length(count))</pre>
<p>Similar to <kbd>vDSP_vsadd</kbd>, <kbd>vDSP_vsmul</kbd> takes in the input (in this case, our output); the scalar we want to multiply each element by; the output; the stride for persisting the result; and finally, the number of elements we want to operate on.</p>
<p>If you rerun the application, you will see that we have managed to shed a few seconds off the total execution time—not bad considering this layer is run <span>only</span><span> </span><span>once at the end of our network. Can we do better? </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Taking advantage of the GPU </h1>
                </header>
            
            <article>
                
<p>You may recall that when we introduced the <kbd>MLCustomLayer</kbd> protocol, there was an optional method, <kbd>encode(commandBuffer, inputs, outputs)</kbd>, reserved for performing the evaluation on the GPU if the hosting device supported it. This flexibility is one of the advantages Core ML has over other machine learning frameworks; it allows mixing layers, which run on the CPU and GPU, and allows them to work coherently together. </p>
<p>To use the GPU, we will be using Apple's <kbd>Metal</kbd> framework, a graphics framework equivalent to OpenGL and DirectX (and now Vulkan), for those who are familiar with 3D graphics. Unlike our previous solutions, which included all code in a single method, we need to write the code that performs the computation in an external file called a <strong>Metal shader</strong> file. Within this file we will define a kernel, which will be complied and stored on the GPU (when loaded), allowing it to fan out the data in parallel across the GPU. Let's create this kernel now; create a new <kbd>metal</kbd> file called <kbd>rescale.metal</kbd> and add the following code:</p>
<pre>#include &lt;metal_stdlib&gt;<br/>using namespace metal;<br/><br/>kernel void rescale(<br/>    texture2d_array&lt;half, access::read&gt; inTexture [[texture(0)]],<br/>    texture2d_array&lt;half, access::write&gt; outTexture [[texture(1)]],<br/>    ushort3 gid [[thread_position_in_grid]])<br/>{<br/>    if (gid.x &gt;= outTexture.get_width() || gid.y &gt;= outTexture.get_height())<br/>    {<br/>        return;<br/>    }<br/>    <br/>    const float4 x = float4(inTexture.read(gid.xy, gid.z));<br/>    const float4 y = (1.0f + x) * 127.5f;<br/>    <br/>    outTexture.write(half4(y), gid.xy, gid.z);<br/>}  </pre>
<p>It is out of scope to discuss the details of <kbd>metal</kbd>, so instead, we'll just highlight some of the key differences and commonalities between this and the previous approaches. First, it's worth recognizing why GPUs have been a major catalyst for the resurgence of neural networks. The GPU architecture allows a kernel (seen earlier) to be spawned for each element in our array—mass parallelism!</p>
<p>Because GPU frameworks were traditionally built with graphics manipulation in mind, there are some nuances with how we operate on data and what we operate on. The most notable of them is that we have swapped <kbd>MLMultiArray</kbd> for <kbd>texture2d_array</kbd> (textures) and we access them through sampling, using <kbd>thread_position_in_grid</kbd>. Nonetheless, the actual computation should look familiar from the original Python code, <kbd>const float4 y = (1.0f + x) * 127.5f</kbd>. Once calculated, we cast it to float 16 (half) and write it to the output texture. </p>
<p>Our next step is to configure our <kbd>RescaleOutputLambda</kbd> class to use <kbd>Metal</kbd> and the GPU, rather than the CPU. Return to the <kbd>RescaleOutputLambda.swift</kbd> file and make the following amendments. </p>
<p>Start by importing the <kbd>Metal</kbd> framework by adding the following statement at the top of your file:</p>
<pre>import Metal</pre>
<p>Next, we define a class variable of the type <kbd>MTLComputePipelineState</kbd> as a handler to the kernel we have just created, along with setting this up within the constructor of the <kbd>RescaleOutputLambda</kbd> class. Make the following amendments to the class and constructor, as shown in bold in the snippet: </p>
<pre>@objc(RescaleOutputLambda) class RescaleOutputLambda: NSObject, MLCustomLayer {<br/>    <br/><strong>    let computePipeline: MTLComputePipelineState</strong><br/>    <br/>    required init(parameters: [String : Any]) throws {<br/><strong>        let device = MTLCreateSystemDefaultDevice()!</strong><br/><strong>        let library = device.makeDefaultLibrary()!</strong><br/><strong>        let rescaleFunction = library.makeFunction(name: "rescale")!</strong><br/><strong>        self.computePipeline = try! device.makeComputePipelineState(function: rescaleFunction)</strong><br/>        <br/>        super.init()<br/>    }<br/>    ...<br/>}</pre>
<p>If no errors are thrown, we will have reference to a complied version of our rescale kernel; the final step is making use of it. Within the <kbd>RescaleOutputLambda</kbd> class, add the following method:</p>
<pre>func encode(commandBuffer: MTLCommandBuffer,<br/>            inputs: [MTLTexture],<br/>            outputs: [MTLTexture]) throws {<br/>    <br/>    guard let encoder = commandBuffer.makeComputeCommandEncoder() else{<br/>        return<br/>    }<br/>    <br/>    let w = computePipeline.threadExecutionWidth<br/>    let h = computePipeline.maxTotalThreadsPerThreadgroup / w<br/>    let threadGroupSize = MTLSizeMake(w, h, 1)<br/>    <br/>    for i in 0..&lt;inputs.count {<br/>        let threadGroups = MTLSizeMake(<br/>            (inputs[i].width + threadGroupSize.width - 1) /<br/>                threadGroupSize.width,<br/>            (inputs[i].height+ threadGroupSize.height - 1) /<br/>                threadGroupSize.height,<br/>            (inputs[i].arrayLength + threadGroupSize.depth - 1) /<br/>                threadGroupSize.depth)<br/>        <br/>        encoder.setTexture(inputs[i], index: 0)<br/>        encoder.setTexture(outputs[i], index: 1)<br/>        encoder.setComputePipelineState(computePipeline)<br/>        encoder.dispatchThreadgroups(<br/>            threadGroups,<br/>            threadsPerThreadgroup:<br/>            threadGroupSize)<br/>        encoder.endEncoding()<br/>}</pre>
<p>As mentioned before, we will omit the details here and only highlight some key differences and commonalities between this approach and the previous approaches.</p>
<p>In short, the bulk of this method is responsible for passing data through to the compute kernel via the encoder and then dispatching it across the GPU. We first pass the input and output textures, as shown in the following snippet: </p>
<pre>encoder.setTexture(inputs[i], index: 0)<br/>encoder.setTexture(outputs[i], index: 1)</pre>
<p>And then we're setting the handler, which points to the rescale kernel we created in the preceding snippet: </p>
<pre>encoder.setComputePipelineState(computePipeline)</pre>
<p>Finally, dispatch the job to the GPU; in this instance, our compute kernel is invoked for every pixel in every channel of the input texture:</p>
<pre>encoder.dispatchThreadgroups(<br/>    threadGroups,<br/>    threadsPerThreadgroup:<br/>    threadGroupSize)<br/>encoder.endEncoding()</pre>
<p><span>If you build and run again, you will hopefully get the same result but in less time. We have now seen two approaches to optimizing our network; I leave optimizing <kbd>ResCropBlockLambda</kbd> as an exercise for you. For now, let's shift our focus to talking about your model's weight before we wrap up this chapter. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reducing your model's weight</h1>
                </header>
            
            <article>
                
<p>We have spent considerable time discussing layers of a network; we have learned that layers are made up of weights, configured in such a way that they can transform an input into a desirable output. These weights come at a cost, though; each one (by default) is a 32-bit floating-point number with a typical model, especially in computer vision, having millions resulting in networks that are hundreds of megabytes in size. On top of that; it's plausible that your application will have multiple models (with this chapter being a good example, requiring a model for each style). </p>
<p>Fortunately, our model in this chapter has a moderate number of weights and weighs in at a mere 2.2 MB; but this is possibly an exception. So we'll use this chapter as an excuse to explore some ways we can reduce our model's weights. But before doing so, let's quickly discuss why, even though it's probably obvious. The three main reasons why you should be conscious of your model's size include:</p>
<ul>
<li>Download time </li>
<li>Application footprint </li>
<li>Demands on memory </li>
</ul>
<p><span>These could all hinder the user experience and are reasons for a user to either uninstall the application quickly or not even download it in the first place. So how do you reduce your model's size to avoid deterring the user. There are three broad approaches:</span></p>
<ul>
<li>Reduce the number of layers your network uses </li>
<li>Reduce the number of units in each of those layers</li>
<li>Reduce the size of the weights </li>
</ul>
<p>The first two require that you have access to the original network and tools to re-architect and train the model; the last is the most accessible and it's the one we will discuss now. </p>
<p>In iOS 11.2, Apple allowed your networks to use half-precision floating-point numbers (16-bit). Now, with the release of iOS 12, Apple has taken this even further and introduced quantization, which allows us to use eight or less bits to encode our model's weights. In the following figure, we can see how these options compare with one another:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/982b4300-9cc8-4a31-8249-7c6fe6d16583.png" style="width:35.67em;height:15.67em;"/></div>
<p>Let's discuss each in turn, starting with reducing our weights precision by converting it's<span> floating points</span> from 32-bits to 16-bits. </p>
<p>For both of these techniques (half-precision and quantization), we will be using the Core ML Tools Python package; so, begin by opening up your browser and heading over to <a href="https://notebooks.azure.com">https://notebooks.azure.com</a>. Once the page is loaded navigate to the folder <kbd>Chapter6/Notebooks/</kbd> and open the Jupyter Notebook <kbd>FastNeuralStyleTransfer_OptimizeCoreML.ipynb</kbd>. As we did before, we'll walk through each of the Notebook's cells here with the assumption that you will be executing each one as we cover it (if you are working along).</p>
<p>We begin by importing the Core ML Tools package; execute the cell with the following code:</p>
<pre>try:<br/>    import coremltools<br/>except:<br/>    !pip install coremltools    <br/>    import coremltools </pre>
<p>For convenience, we have wrapped the <kbd>import</kbd> in an exception block, so that it automatically installs the package if it doesn't exist. </p>
<div class="packt_infobox">At the time of writing, Core ML 2 was still in beta and only recently publicly announced. If you're using a version of Core ML Tools that is less than 2.0 then replace <kbd>!pip install coremltools</kbd> with <kbd>!pip install coremltools&gt;=2.0b1</kbd> to install the latest beta version to have access to the necessary modules for this section. </div>
<p>Next, we will load our <kbd>mlmodel</kbd> file that we had previously saved, using the following statement:</p>
<pre>coreml_model = coremltools.models.MLModel('output/FastStyleTransferVanGoghStarryNight.mlmodel')</pre>
<p>Next, we perform the conversion by simply calling <kbd>coremltools.utils.convert_neural_network_weights_to_fp16</kbd> and passing in your model. If successful, this method will return an equivalent model (that you passed in), using half precision weights instead of 32-bits for storing its weights. Run the cell with the following code to do just that:</p>
<pre> fp16_coreml_model = coremltools.utils.convert_neural_network_weights_to_fp16(coreml_model)</pre>
<p>Finally, we save it so we can later download it and import into our project; run the next cell with the code:</p>
<pre>fp16_coreml_model.save('output/fp16_FastStyleTransferVanGoghStarryNight.mlmodel')</pre>
<p>And with that executed (essentially three lines of code), we have managed to have our models size, going from 2.2 MB to 1.1 MB - so, what's the catch?</p>
<p>As you might suspect, there is a trade-off here; reducing the precision of your models <span>weights will </span>affect its accuracy, but possibly not enough to be concerned with. The only way you will know is by comparing the optimized model with the original and re-evaluating it on your test data, ensuring that it satisfies your required accuracy/results. For this, Core ML Tools provides a collection of utilities that makes this fairly seamless,<span> </span><span>which you can learn at the official website </span><a href="https://apple.github.io/coremltools/index.html">https://apple.github.io/coremltools/index.html</a><span>. </span></p>
<p>Quantization is no more complicated (with respect to using it via Core ML Tools rather than concept); it's a clever technique, so let's quickly discuss how it achieves 8-bit compression before running through the code.</p>
<p>At a high-level, quantization is a technique that maps a continuous range of values to a discrete set; you can think of it as a process of clustering your values into a discrete set of groups and then creating a lookup table which maps your values to the closest group. The size is then dependent on the number of clusters used (index) rather than value, which allows you to encode your weights using anything from 8-bits to 2-bits. </p>
<p>To make this concept more concrete, the following figure illustrates the results of color quantization; where a 24-bit image is mapped to 16 discrete colors:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/ff9c1e2b-d93e-4767-ba5b-ae5894189d61.png" style="width:41.75em;height:17.83em;"/></div>
<p>Instead of each pixel representing its color (using 24-bits/8-bits per channel), they now are indexes to the 16 color palette, that is, from 24-bits to 4-bits. </p>
<p>Before moving onto how we optimize our models using <span>quantization with the </span>Core ML Tools package, you maybe wondering how this <span>palette (or discrete set of values) is derived. The short answer is that there are many ways, from linearly separating the values into groups, to using an unsupervised learning technique such as k-means, or even using a custom, domain-specific technique. Core ML Tools allows for all variations and the choice will be dependent on your data distribution and that results achieved during testing. Let's jump into it; first, we will start by importing the module:</span></p>
<pre>from coremltools.models.neural_network import quantization_utils as quant_utils</pre>
<p><span>With this statement, we have imported the module and assigned it the alias <kbd>quant_utils</kbd>; the next cell, we optimize our model using a variation of sizes and methods:</span></p>
<pre><span>lq8_coreml_model = quant_utils.quantize_weights(coreml_model, 8, 'linear')<br/>lq4_coreml_model = quant_utils.quantize_weights(coreml_model, 4, 'linear')<br/>km8_coreml_model = quant_utils.quantize_weights(coreml_model, 8, 'kmeans')<br/>km4_coreml_model = quant_utils.quantize_weights(coreml_model, 4, 'kmeans')</span></pre>
<p class="mce-root"><span>Once this is completed, let's save each of our optimized models to the output directory before downloading them to our local disk to import them into Xcode (this may take some time):</span></p>
<pre>coremltools.models.MLModel(lq8_coreml_model) \<br/>    .save('output/lq8_FastStyleTransferVanGoghStarryNight.mlmodel')<br/>coremltools.models.MLModel(lq4_coreml_model) \<br/>    .save('output/lq4_FastStyleTransferVanGoghStarryNight.mlmodel')<br/>coremltools.models.MLModel(km8_coreml_model) \<br/>    .save('output/km8_FastStyleTransferVanGoghStarryNight.mlmodel')<br/>coremltools.models.MLModel(km4_coreml_model) \<br/>    .save('output/km8_FastStyleTransferVanGoghStarryNight.mlmodel')</pre>
<p>I will omit the details of downloading and importing the model into your project as we have already gone through these steps previously in this chapter, but I do encourage that you to inspect the results from each model to get a feel for how each optimization affects the results - of course, these effects are highly dependent on the model, data, and domain. The following figure shows the results of each of the optimizations along with the model's size:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/e59a2efc-e65e-40b9-8700-9ec242b9285c.png"/></div>
<p>Admittedly, it's difficult to see the differences due to the low-resolution of the image (and possibly because you're reading this in black and white) but generally, the quality appears minimal between the original and k-means 8-bit version.</p>
<p>With the release of Core ML 2, Apple offers another powerful feature to optimize you Core ML models; specifically around consolidating multiple models into a single package. This not only reduces the size of your application but also convenient for you, the developer, when interfacing with your model. For example, flexible shapes and sizes allows for variable input and output dimensions, that is, instead of a single fixed input and output dimension, you have the flexibility of having multiple variants or a variable range within a limit. You can learn more about this feature on their official website at <a href="https://developer.apple.com/machine-learning">https://developer.apple.com/machine-learning</a>; but for now, we will wrap up this chapter with a quick summary before moving on to the next chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we introduced the concept of style transfer; a technique that aims to separate the content of an image from its style. We discussed how it achieves this by leveraging a trained CNN, where we saw how deeper layers of a network extract features that distill information about the content of an image, while discarding any extraneous information. </p>
<p>Similarly, we saw that shallower layers extracted the finer details, such as texture and color, which we could use to isolate the style of a given image by looking for the correlations between the feature maps (also known as <strong>convolutional kernels</strong> or <strong>filters</strong>) in each layer. These correlations are what we use to measure style and how we steer our network. Having isolated the content and style, we generated a new image by combining the two.</p>
<p class="mce-root">We then highlighted the limitations of performing style transfer in real time (with current technologies) and introduced a slight variation. Instead of optimizing the style and content each time, we could train a model to learn a particular style. This would allow us to generate a stylized image for a given image with a single pass through the network, as we have done with many other examples we have worked through in this book.</p>
<p class="mce-root">Having introduced the concepts, we then walked through converting the Keras model to Core ML and used this as an opportunity to implement custom layers, a Swift-centric way of implementing layers that have no direct mapping between the machine learning framework and Core ML. Having implemented custom layers, we then spent some time looking at how we can optimize them using the <kbd>Accelerate</kbd> (SIMD) and <kbd>Metal</kbd> frameworks (GPU).</p>
<p>The theme of optimization continued into the next section, where we discussed some of the tools available for reducing a model's size; there, we looked at two approaches and how we could make use of them using the Core ML Tools package along with a cautionary warning of the trade-off between size and accuracy. </p>
<p class="mce-root">In the next chapter, we look at how we can apply what we have learned to recognizing user sketches.</p>


            </article>

            
        </section>
    </body></html>