- en: Advanced Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have made it to the final chapter of this book. However, this doesn't mean
    that we can ignore the topics that we are going to discuss in the upcoming sections.
    These topics are state of the art and will separate you from the rest.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Kernel principal component analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Independent component analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compressed sensing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesian multiple imputations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-organizing maps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we understood what **principal component analysis**
    (**PCA**) is, how it works, and when we should be deploying it. However, as a
    dimensionality reduction technique, do you think that you can put this to use
    in every scenario? Can you recall the roadblock or the underlying assumption behind
    it that we discussed?
  prefs: []
  type: TYPE_NORMAL
- en: Yes, the most important assumption behind PCA is that it works for datasets
    that are linearly separable. However, in the real world, you don't get this kind
    of dataset very often. We need a method to capture non-linear data patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the left-hand side, we have got a dataset in which there are two classes.
    We can see that once we arrive at the projections and establish the components,
    PCA doesn''t have an effect on it and that it is not able to separate it by a
    line in a 2D dimension. That is, PCA can only function well when we have got low-level
    dimensions and linearly separable data. The following plot shows the dataset of
    two classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a232d621-ee4e-41be-8803-acf75c70efd6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is why we bring in the kernel method: so that we can merge it with PCA
    to achieve it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Just to recap what you learned about the kernel method, we will discuss it
    and its importance in brief:'
  prefs: []
  type: TYPE_NORMAL
- en: We have got data in a low dimensional space. However, at times, it's difficult
    to achieve classification (green and red) when we have got non-linear data (as
    shown in the following diagram). This being said, we do have a clear understanding
    that having a tool that can map the data from a lower to a higher dimension will
    result in a proper classification. This tool is called the **kernel method**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same dataset turns out to be linearly separable in the new feature space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows data in low and high dimensional spaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c8e0b260-2472-4be2-a5ab-c9dbd95c0e2b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To classify the green and red points in the preceding diagram, the feature
    mapping function has to take the data and change is from being 2D to 3D, that
    is, *Φ = R² → R³*. The equation for this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a66ab011-d5a2-4bcd-9ac0-a5942e1c9c6d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The goal of the kernel method is to figure out and choose kernel function *K*.
    This is so that we can find the geometry feature in the new dimension and classify
    data patterns. Let''s see how this is done:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce27e77c-769b-44f4-bef5-13c8c45a8758.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/b741c251-2a86-48a1-9ca2-1d36852fe146.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/ab6c39e4-d612-4194-a8fc-d36429518393.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/21979279-0315-447f-b2fc-e8a34ca7c792.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/a28c212f-cb59-4bd0-a194-ec0a992eb0cc.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, Phi is a feature mapping function. But do we always need to know the feature
    mapping function? Not really. Kernel function *K* does the trick. With a given
    kernel function, *K*, we can come up with a feature space, *H*. Two of the popular
    kernel functions are Gaussian and polynomial kernel functions.
  prefs: []
  type: TYPE_NORMAL
- en: Picking an apt kernel function will enable us to figure out the characteristics
    of the data in the new feature space quite well.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have made ourselves familiar with the kernel trick, let's move on
    to the Kernel PCA.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Kernel PCA is an algorithm that not only keeps the main spirit of PCA as
    it is, but goes a step further to make use of the kernel trick so that it is operational
    for non-linear data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define the covariance matrix of the data in the feature space, which
    is the product of the mapping function and the transpose of the mapping function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/390c4461-854c-493d-8ffd-a0e4e8c7a67c.png)'
  prefs: []
  type: TYPE_IMG
- en: It is similar to the one we used for PCA.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to solve the following equation so that we can compute principal
    components:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/5c3b4a63-2c96-4b9e-bec7-23d242aab163.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *C[F]* is the covariance matrix of the data in feature space, *v* is the
    eigenvector, and *λ* (lambda) is the eigenvalues.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s put the value of *step 1* into *step 2 *– that is, the value of *C[F]* in
    the equation of *step 2.* The eigenvector will be as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/703773c2-f899-49f5-b428-f29b89542f81.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/4607835d-fe85-40be-ba82-34c117ed45a4.png) is a scalar number.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s add the kernel function into the equation. Let''s multiply *Φ(x[k]) *on
    both sides of the formula, ![](img/dcda92c5-f517-4edb-838c-8a34df198137.png):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/350e63a8-acbb-4abc-9650-2c1a679c3a25.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s put the value of *v* from the equation in *step 3* into the equation
    of *step 4*, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d1c66f74-0346-4b50-bfa4-e479e9618888.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we call *K *![](img/4f568c8c-7d71-4ba3-98f9-3af69361f697.png). Upon simplifying
    the equation from *step 5* by keying in the value of *K*, we get the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/cf4a358d-3282-40ed-89e8-5d402a80317c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'On doing eigen decomposition, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae3ae0fb-89df-4a34-8075-56d9f82e8481.png)'
  prefs: []
  type: TYPE_IMG
- en: 'On normalizing the feature space for centering, we get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2cf064cb-9d15-463d-9965-6c1ffbd533ad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s execute the Kernel PCA in Python. We will keep this simple and
    work on the Iris dataset. We will also see how we can utilize the new compressed
    dimension in the model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s load the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, load the data and create separate objects for the explanatory and target
    variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s have a look at the explanatory data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/350741b5-b4ee-47cd-b509-36a0b61ff7a0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s split the data into train and test sets, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can standardize the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s have a look at `X_train`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b2e88a9-d310-4d32-a791-ccd5fcec18fa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s apply the kernel PCA on this. Here, we are trying to condense the
    data into just two components. The kernel that''s been chosen here is the radial
    basis function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We have got the new train and test data with the help of the kernel PCA.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what the data looks like:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following as output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f108b9c3-cb99-4b75-9b9e-d87476ddfae8.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, we've got two components here. Earlier, `X_train` showed us four variables.
    Now, the data has been shrunk into two fields.
  prefs: []
  type: TYPE_NORMAL
- en: Independent component analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Independent component analysis** (**ICA**) is similar to PCA in terms of
    dimensionality reduction. However, it originated from the signal processing world
    wherein they had this problem that multiple signals were being transmitted from
    a number of sources, and there were a number of devices set up to capture it.
    However, the problem was that the captured signal by the device was not very clear
    as it happened to be a mix of a number of sources. They needed to have clear and
    independent reception for the different devices that gave birth to ICA. Heralt
    and Jutten came up with this in.'
  prefs: []
  type: TYPE_NORMAL
- en: The difference between PCA and ICA is that PCA focuses upon finding uncorrelated
    factors, whereas ICA is all about deriving independent factors. Confused? Let
    me help you. Uncorrelated factors imply that there is no linear relationship between
    them, whereas independence means that two factors have got no bearing on each
    other. For example, scoring good marks in mathematics is independent of which
    state you live in.
  prefs: []
  type: TYPE_NORMAL
- en: An underlying assumption for this algorithm is that the variables are linear
    mixtures of unknown latent and independent variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data *x[i ](t)* is modeled using hidden variables *s[i](t)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/78e5eed1-8082-4bd9-b049-53969f9be9df.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *i= 1,2,3..........n.*
  prefs: []
  type: TYPE_NORMAL
- en: 'It can also be written in the form of matrix decomposition as **x=As**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/285cf751-6a6b-4b1d-854b-6a4aa1d20554.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A**: Constant mixing matrix'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**s**: Latent factor matrices, which are independent of each other'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have to estimate the values of both **A** and **s** while we have got **X**.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, our goal is to find *W*, which is *W= A*^(*-1*), which is an
    unmixing matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Here, *s[ij] *has to be statistically independent of and non-Gaussian (not following
    normal distribution).
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing for ICA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The preprocessing of ICA can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Centering**: The first step is to center *x*. That is, we need to subtract
    its mean vector from *x* so as to make *x* a zero-mean variable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Whitening**: Before putting the data through ICA, we are supposed to whiten
    the data. This means that the data has to be uncorrelated. Geometrically speaking,
    it tends to restore the initial shape of the data and only the resultant matrix
    needs to be rotated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To find out what unmixing matrices are independent, we have to bank upon non-Gaussianity.
    Let's see how we can do this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will need to maximize the kurtosis, which will turn the distribution
    into a non-Gaussian. This will result in independent components. The following
    diagram shows an image of fast ICA:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/490fddd1-61b4-48b2-92fd-f42080453314.png)'
  prefs: []
  type: TYPE_IMG
- en: For this, we have the `FastICA` library in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at how we can execute this in Python. We will work with the same
    Iris data. This might not be an ideal dataset for executing ICA, but this is being
    done for directional purposes. To execute the code in Python, we will need to
    perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to load the library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to load the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s partition the data into train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s make the data a standard scalar:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to load in the ICA library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We carry out ICA as follows. We will stick to three components here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then plot the results, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f973e18d-de80-4e93-9c42-6f8e91373c7f.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see the three different components here (by color).
  prefs: []
  type: TYPE_NORMAL
- en: Compressed sensing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Compressed sensing is one of the easiest problems to solve in the area of information
    theory and signal processing. It is a signal acquisition and reconstruction technique
    where the signal is compressible. The signal must be sparse. Compressed sensing
    tries to fit samples of a signal to functions, and it has a preference to use
    as few basic functions as possible to match the samples. This is described in
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2564922-b0cf-4d3d-8b76-7214e7f225be.png)'
  prefs: []
  type: TYPE_IMG
- en: This is one of the prime equations that we see in linear algebra, where **y**
    is a **M x 1** matrix, phi is a **M x N** matrix that has got a number of columns
    that is higher than the number of rows, and **x** is a **N x 1** matrix comprising
    **k** non-zero entries. There are so many unknowns, which is expressed as an **N**
    length vector and **M** measurements, wherein **M << N**. In this type of equation,
    we know that many solutions are possible since the null space of this matrix is
    non-trivial. Hence, this equation can accommodate many solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Our goal
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our goal is to find out the solution with a least possible non-zero entry of
    all of the solutions. That is, the solution should give us as few non-zeros as
    possible. Are you wondering where this can be applied? There are plenty of applications
    for it. The areas where it can be applied are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Signal representation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Medical imaging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sparse channel estimation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's say that we have got a time signal. This signal is highly sparse, but
    we know a little bit about it as it has a few frequencies. Can you sense what
    it is from the earlier equation? Yes, it can be deemed as *X*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s call this **unknown** signal *X*. Now, even though we don''t know the
    whole signal, we can still make observations about it, or samples, as shown in
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This will form a random equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to fit the `l1` norm. We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to fit the `l2` norm. We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'By summing up the two sinusoids, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s take the sample out of `n`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create the `idct` matrix operator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba0f4f4d-2003-4acb-b05f-88f6d596e91a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To reconstruct the signal, we must do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: That is how we reconstruct the signal.
  prefs: []
  type: TYPE_NORMAL
- en: Self-organizing maps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Self-organizing maps** (**SOM**) were invented by Teuvo Kohonen in the 1980s.
    Sometimes, they are known as **Kohonen maps**. So, why do they exist? The prime
    motive for these kind of maps is to reduce dimensionality through a neural network.
    The following diagram shows the different 2D patterns from the input layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/45326737-dbab-470b-8d50-9d6a959d22bb.png)'
  prefs: []
  type: TYPE_IMG
- en: They take the number of columns as input. As we can see from the 2D output,
    it transforms and reduces the amount of columns in the dataset into 2D.
  prefs: []
  type: TYPE_NORMAL
- en: The following link leads to the the 2D output: [https://www.cs.hmc.edu/~kpang/nn/som.html](https://www.cs.hmc.edu/~kpang/nn/som.html)
  prefs: []
  type: TYPE_NORMAL
- en: The depiction of the preceding diagram in 2D talks about a health of the country
    based on various factors. That is, it shows whether they are rich or poor. Some
    other factors that are taken into account are education, quality of life, sanitation,
    inflation, and health. Therefore, it forms a huge set of columns or dimensions.
    Countries such as Belgium and Sweden seem to show similar traits, depicting that
    they have got a good score on the health indicator.
  prefs: []
  type: TYPE_NORMAL
- en: Since this is an unsupervised learning technique, the data wasn't labeled. Based
    on patterns alone, the neural network is able to understand which country should
    be placed where.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the situation we just covered, opportunities are aplenty where self-organizing
    maps can be utilized. It can be thought as being similar in nature to K-means
    clustering.
  prefs: []
  type: TYPE_NORMAL
- en: SOM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s go through process of how SOMs learn:'
  prefs: []
  type: TYPE_NORMAL
- en: Each node's weights are initialized by small standardized random values. These
    act like coordinates for different output nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first row's input (taking the first row from all of the variables) is fed
    into the first node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we have got two vectors. If *V* is the current input vector and *W* is
    the node''s weight vector, then we calculate the Euclidean distance, like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/14175655-4cfa-4bfe-bfa3-fb1b6980397d.png)'
  prefs: []
  type: TYPE_IMG
- en: The node that has a weight vector closest to the input vector is tagged as the
    **best-matching unit** (**BMU**).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A similar operation is carried out for all the rows of input and weight vectors.
    BMUs are found for all.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the BMU has been determined for every iteration, the other nodes within
    the BMU''s neighborhood are computed. Nodes within the same radius will have their
    weights updated. A green arrow indicates the radius. Slowly, the neighborhood will
    shrink to the size of just one node, as shown in the following diagram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c16e2611-8dcc-4f93-85bd-f848c0c03343.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The most interesting part of the Kohonen algorithm is that the radius of the
    neighborhood keeps on shrinking. It takes place through the exponential decay
    function. The value of lambda is dependent on sigma. The number of iterations
    that have been chosen for the algorithm to run is given by the following equation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/521300be-39b7-464b-b6cf-54de2bbe33e7.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/19c08a6b-1031-49ce-8524-5cc78e877c20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The weights get updated via the following equation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a0e055c8-766e-4e64-8f0a-63c0da661b15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/97390ac7-2127-4b75-abf3-b28943ae5e8e.png)'
  prefs: []
  type: TYPE_IMG
- en: '*t= 1, 2...* can be explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*L(t)*: Learning rate'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '*D*: Distance of a node from BMU'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*σ*: Width of the function'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let''s carry out one use case of this in Python. We will try to detect
    fraud in a credit card dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s load the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, it''s time to load the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will standardize the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s import the `minisom` library and key in the hyperparameters, that is,
    learning rate, sigma, length, and number of iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s visualize the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output will be generated from the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c7c81ab6-292a-4327-94f0-8311d8093be0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see that the nodes that have a propensity toward fraud have got white
    backgrounds. This means that we can track down those customers with the help of
    those nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This will give you the pattern of frauds.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian multiple imputation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bayesian multiple imputation has got the spirit of the Bayesian framework. It
    is required to specify a parametric model for the complete data and a prior distribution
    over unknown model parameters, *θ*. Subsequently, *m* independent trials are drawn
    from the missing data, as given by the observed data using Bayes' Theorem. Markov
    Chain Monte Carlo can be used to simulate the entire joint posterior distribution
    of the missing data. BMI follows a normal distribution while generating imputations
    for the missing values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say that the data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Y = (Yobs, Ymiss),*'
  prefs: []
  type: TYPE_NORMAL
- en: Here, *Yobs* is the observed *Y* and *Ymiss* is the missing *Y.*
  prefs: []
  type: TYPE_NORMAL
- en: 'If *P(Y|θ)* is the parametric model, the parameter *θ* is the mean and the
    covariance matrix that parameterizes a normal distribution. If this is the case,
    let *P(θ)* be the prior:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/432ba803-d702-4821-9978-219b8a355fa9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s make use of the `Amelia` package in R and execute this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s make the imputation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have studied the Kernel PCA, along with ICA. We also studied
    compressed sensing, the goals of compressed sensing, and self-organizing maps
    and how they work. Finally, we concluded with Bayesian multiple imputations.
  prefs: []
  type: TYPE_NORMAL
