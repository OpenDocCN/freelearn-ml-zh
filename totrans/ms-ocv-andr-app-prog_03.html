<html><head></head><body><div class="chapter" title="Chapter&#xA0;3.&#xA0;Detecting Objects"><div class="titlepage"><div><div><h1 class="title"><a id="ch03"/>Chapter 3. Detecting Objects</h1></div></div></div><p>One of the common applications of computer vision is to detect objects in an image or video. For example, we can use this method to detect a particular book in a heap of many books. One of the methods to detect objects is <a id="id125" class="indexterm"/>
<span class="strong"><strong>feature matching</strong></span>. In this chapter, we will learn the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">What are features?</li><li class="listitem" style="list-style-type: disc">Feature detection, description, and matching in images</li><li class="listitem" style="list-style-type: disc">SIFT detector and descriptor</li><li class="listitem" style="list-style-type: disc">SURF detector and descriptor</li><li class="listitem" style="list-style-type: disc">ORB detector and descriptor</li><li class="listitem" style="list-style-type: disc">BRISK detector and descriptor</li><li class="listitem" style="list-style-type: disc">FREAK descriptor</li></ul></div><div class="section" title="What are features?"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec19"/>What are features?</h1></div></div></div><p>Features <a id="id126" class="indexterm"/>are specific patterns that are unique and can be easily tracked and compared. Good features are those that can be distinctly localized. The following image shows the different kind of features:</p><div class="mediaobject"><img src="graphics/B02052_03_01.jpg" alt="What are features?"/><div class="caption"><p>Explains types of features</p></div></div><p>In the preceding image, patch A is a flat area and is difficult to locate precisely. If we move the rectangle anywhere within the box, the patch contents remain the same. Patch B, being along an edge, is a slightly better feature because if you move it perpendicular to the edge, it changes. However, if you move it parallel to the edge, it is identical to the initial patch. Thus, we can localize these kind of features in at least one dimension. Patch C, being a corner, is a good feature because on moving the rectangle in any direction, the contents of the patch change and can be easily localized. Thus, good features are those which can be easily localized and thus are easy to track.</p><p>In the previous chapters, we have seen some of the edge and corner detection algorithms. In this chapter, we will take a look at some more algorithms by which we can find features. This is called <a id="id127" class="indexterm"/>
<span class="strong"><strong>feature detection</strong></span>. Just detecting features is not enough. We need to be able to differentiate one feature from the other. Hence, we use <a id="id128" class="indexterm"/>
<span class="strong"><strong>feature description</strong></span> to describe the detected features. The descriptions enable us to find similar features in other images, thereby enabling us to identify objects. Features can also be used to align images and to stitch them together. We will take a look at these applications in the later chapters of this book.</p><p>Now we will take a look at some common algorithms available to detect features, such as SIFT, SURF, BRIEF, FAST, and BRISK.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note09"/>Note</h3><p>Note that SIFT and SURF are patented algorithms and hence, their free use is only limited to academic and research purposes. For any commercial use of these algorithms, you need to abide by the patent rules and regulations, or speak to the concerned personal.</p></div></div></div></div>
<div class="section" title="Scale Invariant Feature Transform"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec20"/>Scale Invariant Feature Transform</h1></div></div></div><p>
<span class="strong"><strong>Scale Invariant Feature Transform</strong></span> (<span class="strong"><strong>SIFT</strong></span>) is <a id="id129" class="indexterm"/>one of the most widely recognized feature detection algorithms. It was proposed by David Lowe in 2004.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note10"/>Note</h3><p>Link to the<a id="id130" class="indexterm"/> paper: <a class="ulink" href="http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf">http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf</a>
</p></div></div><p>Some of the <a id="id131" class="indexterm"/>properties of SIFT are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">It is invariant to scaling and rotation changes in objects</li><li class="listitem" style="list-style-type: disc">It is also partially invariant to 3D viewpoint and illumination changes</li><li class="listitem" style="list-style-type: disc">A large number of keypoints (features) can be extracted from a single image</li></ul></div><div class="section" title="Understanding how SIFT works"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec21"/>Understanding how SIFT works</h2></div></div></div><p>SIFT follows a <a id="id132" class="indexterm"/>strategy of matching robust local features. It is divided into four parts:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Scale-space extrema detection</li><li class="listitem" style="list-style-type: disc">Keypoint localization</li><li class="listitem" style="list-style-type: disc">Orientation assignment</li><li class="listitem" style="list-style-type: disc">Keypoint descriptor</li></ul></div><div class="section" title="Scale-space extrema detection"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec03"/>Scale-space extrema detection</h3></div></div></div><p>In this step, an<a id="id133" class="indexterm"/> image is progressively blurred out using Gaussian blur to get rid of some details in the images. It has been mathematically proven (under reasonable assumptions) that performing Gaussian blur is the only way to carry this out effectively.</p><div class="mediaobject"><img src="graphics/B02052_03_02.jpg" alt="Scale-space extrema detection"/><div class="caption"><p>Images of one octave</p></div></div><p>Progressively<a id="id134" class="indexterm"/> blurred images constitute an octave. A new octave is formed by resizing the original image of the previous octave to half and then progressively blurring it. Lowe recommends that you use four octaves of five images each for the best results.</p><p>Thus, we see that the images in the first octave are formed by progressively blurring the original image. The first image of the second octave is obtained by resizing the original image in the first octave. Other images in the second octave are formed by the progressive blurring of the first image in the second octave, and so on.</p><div class="mediaobject"><img src="graphics/B02052_03_03.jpg" alt="Scale-space extrema detection"/><div class="caption"><p>Images of all octaves</p></div></div><p>To precisely <a id="id135" class="indexterm"/>detect edges in an image, we use the Laplacian operator. In this method, second we blur the image a little and then calculate its second derivative. This locates the edges and corners that are good for finding the keypoints. This operation is called the Laplacian of Gaussian.</p><p>The second order derivative is extremely sensitive to noise. The blur helps in smoothing out the noise and in stabilizing the second order derivative. The problem is that calculating all these second order derivatives is computationally expensive. So, we cheat a bit:</p><div class="mediaobject"><img src="graphics/B02052_03_21.jpg" alt="Scale-space extrema detection"/></div><div class="mediaobject"><img src="graphics/B02052_03_22.jpg" alt="Scale-space extrema detection"/></div><p>Here, <span class="emphasis"><em>k</em></span> is a <a id="id136" class="indexterm"/>constant multiplicative factor, which represents the amount of blurring in each image in the scale space. A scale space represents the set of images that have been either scaled-up or scaled-down for the purpose of computing keypoints. For example, as shown in the following figure, there are two sets of images: one set is the original set of five images that have been blurred with different blurring radius and another set of scaled down images. The different parameter values can be seen in the following table:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom"> </th><th style="text-align: left" valign="bottom">
<p>Scale</p>
</th><th style="text-align: left" valign="bottom"> </th><th style="text-align: left" valign="bottom"> </th><th style="text-align: left" valign="bottom"> </th><th style="text-align: left" valign="bottom"> </th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Octave</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>0.707107</p>
</td><td style="text-align: left" valign="top">
<p>1.000000</p>
</td><td style="text-align: left" valign="top">
<p>1.414214</p>
</td><td style="text-align: left" valign="top">
<p>2.000000</p>
</td><td style="text-align: left" valign="top">
<p>2.828427</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>1.414214</p>
</td><td style="text-align: left" valign="top">
<p>2.000000</p>
</td><td style="text-align: left" valign="top">
<p>2.828427</p>
</td><td style="text-align: left" valign="top">
<p>4.000000</p>
</td><td style="text-align: left" valign="top">
<p>5.656854</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>2.828427</p>
</td><td style="text-align: left" valign="top">
<p>4.000000</p>
</td><td style="text-align: left" valign="top">
<p>5.656854</p>
</td><td style="text-align: left" valign="top">
<p>8.000000</p>
</td><td style="text-align: left" valign="top">
<p>11.313708</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>5.656854</p>
</td><td style="text-align: left" valign="top">
<p>8.000000</p>
</td><td style="text-align: left" valign="top">
<p>11.313708</p>
</td><td style="text-align: left" valign="top">
<p>16.000000</p>
</td><td style="text-align: left" valign="top">
<p>22.627417</p>
</td></tr></tbody></table></div><p>To generate the Laplacian of Gaussian images, we calculate the difference between two consecutive images in an octave. This is called the <a id="id137" class="indexterm"/>
<span class="strong"><strong>Difference of Gaussian</strong></span> (<span class="strong"><strong>DoG</strong></span>). These DoG images are approximately equal to those obtained by calculating the Laplacian of Gaussian. Using DoG also has an added benefit. The images obtained are also scale invariant.</p><div class="mediaobject"><img src="graphics/B02052_03_04.jpg" alt="Scale-space extrema detection"/><div class="caption"><p>Difference of Gaussian</p></div></div><p>Using the Laplacian of Gaussian<a id="id138" class="indexterm"/> is not only computationally expensive, but it also depends on the amount of blur applied. This is taken care of in the DoG images as a result of normalization.</p></div><div class="section" title="Keypoint localization"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec04"/>Keypoint localization</h3></div></div></div><p>Now these <a id="id139" class="indexterm"/>images have been sufficiently preprocessed to enable us to find local extremas. To locate keypoints, we need to iterate over each pixel and compare it with all its neighbors. Instead of just comparing the eight neighbors in that image, we compare the value with its neighbors in that image and also with the images above and below it in that octave, which have nine pixels each:</p><div class="mediaobject"><img src="graphics/B02052_03_05.jpg" alt="Keypoint localization"/><div class="caption"><p>Keypoint localization</p></div></div><p>So, we can see that we compare a pixel's value with its <span class="strong"><strong>26 neighbors</strong></span>. A pixel is a keypoint if it is<a id="id140" class="indexterm"/> the minimum or the maximum among all its 26 neighbors. Usually, a non-maxima or a non-minima doesn't have to go through all 26 comparisons as we may have found its result within a few comparisons.</p><p>We do not calculate the keypoints in the uppermost and lowermost images in an octave because we do not have enough neighbors to identify the extremas.</p><p>Most of the time, the extremas are never located at the exact pixels. They may be present in between the pixels, but we have no way to access this information in an image. The keypoints located are just their average positions. We use the Taylor series expansion of the scale space function <span class="inlinemediaobject"><img src="graphics/B02052_03_24.jpg" alt="Keypoint localization"/></span> (up to the quadratic term) shifted till the current point as origin gives us:</p><div class="mediaobject"><img src="graphics/B02052_03_25.jpg" alt="Keypoint localization"/></div><p>Here, <span class="emphasis"><em>D</em></span> and its derivatives are calculated at the point we are currently testing for extrema. Using this formula, by differentiating and equating the result to zero, we can easily find the subpixel keypoint locations:</p><div class="mediaobject"><img src="graphics/B02052_03_06.jpg" alt="Keypoint localization"/><div class="caption"><p>Subpixel extrema localization</p></div></div><p>SIFT recommends<a id="id141" class="indexterm"/> that you generate two such extrema images. Thus, to generate two extremas, we need four DoG images. To generate these four DoG images, we need five Gaussian blurred images. Thus, we need five images in a single octave. It has also been found that the optimal results are obtained when <span class="inlinemediaobject"><img src="graphics/B02052_03_26.jpg" alt="Keypoint localization"/></span> and <span class="inlinemediaobject"><img src="graphics/B02052_03_27.jpg" alt="Keypoint localization"/></span>.</p><p>The number of keypoints located so far is quite high. Some of these keypoints either lie on an edge or don't have enough contrast to be useful to us. So we need to get rid of these keypoints. This approach is similar to that used in the <a id="id142" class="indexterm"/>
<span class="strong"><strong>Harris corner detector</strong></span> to remove edges.</p><p>To remove low contrast keypoints, we simply compare the intensity value of the current pixel to a preselected threshold value. If it is less than the threshold value, it is rejected. Because we have used subpixel keypoints, we again need to use the Taylor series expansion to get the intensity value at subpixel locations.</p><p>For stability, it is not sufficient to reject keypoints with low contrast. The DoG function will have a strong response along edges, even if the location along the edge is poorly determined and therefore, unstable to small amounts of noise.</p><p>To eliminate keypoints along the edges, we calculate two gradients at the keypoint, which are perpendicular to each other. The region around the keypoint can be one of the following three types:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A flat region (both gradients will be small)</li><li class="listitem" style="list-style-type: disc">An edge (here, the gradient parallel to the edge will be small, but the one perpendicular to it will be large)</li><li class="listitem" style="list-style-type: disc">A corner (both gradients will be large)</li></ul></div><p>As we want only <a id="id143" class="indexterm"/>corners as our keypoints, we only accept those keypoints whose both gradient values are high.</p><p>To calculate this, we use the <a id="id144" class="indexterm"/>
<span class="strong"><strong>Hessian matrix</strong></span>. This is similar to the Harris corner detector. In the Harris corner detector, we calculate two different eigenvalues, whereas, in SIFT, we save the computation by just calculating their ratios directly. The Hessian matrix is shown as follows:</p><div class="mediaobject"><img src="graphics/B02052_03_28.jpg" alt="Keypoint localization"/></div></div><div class="section" title="Orientation assignment"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec05"/>Orientation assignment</h3></div></div></div><p>Till now, we<a id="id145" class="indexterm"/> have stable keypoints and we know the scales at which these were detected. So, we have scale invariance. Now we try to assign an orientation to each keypoint. This orientation helps us achieve rotation invariance.</p><p>We try to compute the magnitude and direction of the Gaussian blurred images for each keypoint. The magnitudes and directions are calculated using these formulae:</p><div class="mediaobject"><img src="graphics/B02052_03_07.jpg" alt="Orientation assignment"/></div><p>The magnitude and orientation are calculated for all pixels around the keypoint. We create a 36-bin histogram covering the 360-degree range of orientations. Each sample added to the histogram is<a id="id146" class="indexterm"/> weighted by its gradient magnitude and by a Gaussian-weighted circular window with σ, which is 1.5 times that of the scale of the keypoint. Suppose you get a histogram, as shown in the following figure:</p><div class="mediaobject"><img src="graphics/B02052_03_08.jpg" alt="Orientation assignment"/></div><p>After this has been done for all the neighboring pixels of a particular keypoint, we will get a peak in the histogram. In the preceding figure, we can see that the histogram peaks in the region <span class="strong"><strong>20-29</strong></span>. So, we assign this orientation to the keypoint. Also, any peaks above <span class="strong"><strong>80%</strong></span> value are also converted into keypoints. These new keypoints have the same location and scale as the original keypoint, but its orientation is assigned to the value corresponding to the new peak.</p></div><div class="section" title="Keypoint descriptor"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec06"/>Keypoint descriptor</h3></div></div></div><p>Till now, we <a id="id147" class="indexterm"/>have achieved scale and rotation invariance. We now need to create a descriptor for various keypoints so as to be able to differentiate it from the other keypoints.</p><p>To generate a descriptor, we take a 16x16 window around the keypoint and break it into 16 windows of size 4x4. This can be seen in the following image:</p><div class="mediaobject"><img src="graphics/B02052_03_09.jpg" alt="Keypoint descriptor"/></div><p>We do this<a id="id148" class="indexterm"/> in order to incorporate the fact that objects in two images are rarely never exactly the same. Hence, we try to lose some precision in our calculations. Within each 4x4 window, gradient magnitudes and orientations are calculated. These orientations are put in an 8-bin histogram. Each bin represents an orientation angle of 45 degrees.</p><p>Now that we have a large area to consider, we need to take the distance of the vectors from the keypoint into consideration. To achieve this, we use the Gaussian weighting function:</p><div class="mediaobject"><img src="graphics/B02052_03_10.jpg" alt="Keypoint descriptor"/></div><p>We put the 16 vectors into 8-bin histograms each, and doing this for each of the 4x4 windows we get 4x4x8 = 128 numbers. Once we have all these 128 numbers, we normalize the numbers (by dividing each by the sum of their squares). This set of 128 normalized numbers forms the feature vector.</p><p>By the introduction of the feature vector, some unwanted dependencies arise, which are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Rotation dependence</strong></span>: The <a id="id149" class="indexterm"/>feature vector uses gradient orientations. So, if we rotate the image, our feature vector changes and the gradient orientations are also affected. To achieve rotation independence, we subtract the keypoint's rotation from each orientation. Thus, each gradient orientation is now relative to the keypoint's orientation.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Illumination dependence</strong></span>: Illumination independence<a id="id150" class="indexterm"/> can be achieved by thresholding large values in the feature vector. So any value greater than 0.2 is changed to 0.2 and the resultant feature vector is normalized again. We have now obtained an illumination independent feature vector.</li></ul></div><p>So, now that <a id="id151" class="indexterm"/>we have seen how SIFT works in theory, let's see how it works in OpenCV and its capability to match objects.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note11"/>Note</h3><p>Images and a simplified explanation of SIFT by Utkarsh Sinha can be found at <a class="ulink" href="http://www.aishack.in/">http://www.aishack.in/</a>.</p></div></div></div></div><div class="section" title="SIFT in OpenCV"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec22"/>SIFT in OpenCV</h2></div></div></div><p>We will set up a <a id="id152" class="indexterm"/>new application called <code class="literal">Chapter3</code>, which is similar to the one created in the earlier chapters. We will make changes to <code class="literal">MainActivity.java</code>. Some changes also have to be made to <code class="literal">HomeActivity.java</code>, but they will be self-explanatory.</p><p>First, we open <code class="literal">res</code> | <code class="literal">main_menu.xml</code>. In this file, we will create two items. One to select each image to be matched. As a convention, we will have the first image as the object to detect and the second image as the scene in which we want to detect it:</p><div class="informalexample"><pre class="programlisting">&lt;menu 
    
    tools:context="com.packtpub.masteringopencvandroid.chapter3.MainActivity"&gt;
    &lt;item android:id="@+id/action_load_first_image"
        android:title="@string/action_load_first_image"
        android:orderInCategory="1"
        android:showAsAction="never" /&gt;
    &lt;item android:id="@+id/action_load_second_image"
        android:title="@string/action_load_second_image"
        android:orderInCategory="2"
        android:showAsAction="never" /&gt;
&lt;/menu&gt;</pre></div><p>Now we need to program these items in to our Java code. This is similar to <a class="link" href="ch01.html" title="Chapter 1. Applying Effects to Images">Chapter 1</a>, <span class="emphasis"><em>Applying Effects to Images</em></span>, where we opened the photo picker using intents. We will have two flag variables that will store each image that has been selected. If it is selected, we will perform our computations.</p><p>We will perform<a id="id153" class="indexterm"/> our actual computations in <code class="literal">AsyncTask</code>, as these tasks are computationally expensive; and to avoid blocking the UI thread for a long time, we offload the computation onto an asynchronous background worker—<code class="literal">AsyncTasks</code> that enables us to perform threading:</p><div class="informalexample"><pre class="programlisting">new AsyncTask&lt;Void, Void, Bitmap&gt;() {
    private long startTime, endTime;
    @Override
    protected void onPreExecute() {
        super.onPreExecute();
        startTime = System.currentTimeMillis();
    }

    @Override
    protected Bitmap doInBackground(Void... params) {
        return executeTask();
    }

    @Override
    protected void onPostExecute(Bitmap bitmap) {
        super.onPostExecute(bitmap);
        endTime = System.currentTimeMillis();
        ivImage1.setImageBitmap(bitmap);
        tvKeyPointsObject1.setText("Object 1 : "+keypointsObject1);
        tvKeyPointsObject2.setText("Object 2 : "+keypointsObject2);
        tvKeyPointsMatches.setText("Keypoint Matches : "+keypointMatches);
        tvTime.setText("Time taken : "+(endTime-startTime)+" ms");
    }
}.execute();</pre></div><p>Here, the <code class="literal">executeTask</code> function has been called, which will perform all our computations. First, we need to detect the keypoints, and then we need to use descriptors to describe them.</p><p>We first declare all our variables:</p><div class="informalexample"><pre class="programlisting">FeatureDetector detector;
MatOfKeyPoint keypoints1, keypoints2;
DescriptorExtractor descriptorExtractor;
Mat descriptors1, descriptors2;</pre></div><p>Then, depending <a id="id154" class="indexterm"/>on the algorithm, we initialize these variables. For SIFT, we use the following code snippet:</p><div class="informalexample"><pre class="programlisting">switch (ACTION_MODE){
        case HomeActivity.MODE_SIFT:
                detector = FeatureDetector.create(FeatureDetector.SIFT);
                descriptorExtractor = DescriptorExtractor.create(DescriptorExtractor.SIFT);
                //Add SIFT specific code
                break;
        //Add cases for other algorithms
}</pre></div><p>Now we detect the keypoints:</p><div class="informalexample"><pre class="programlisting">detector.detect(src2, keypoints2);
detector.detect(src1, keypoints1);
keypointsObject1 = keypoints1.toArray().length; //These have been added to display the number of keypoints later.
keypointsObject2 = keypoints2.toArray().length;</pre></div><p>Now that we have the keypoints, we will compute their descriptors:</p><div class="informalexample"><pre class="programlisting">descriptorExtractor.compute(src1, keypoints1, descriptors1);
descriptorExtractor.compute(src2, keypoints2, descriptors2);</pre></div></div></div>
<div class="section" title="Matching features and detecting objects"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec21"/>Matching features and detecting objects</h1></div></div></div><p>Once we <a id="id155" class="indexterm"/>have detected features in two or more objects, and have their descriptors, we can match the features to check whether the images have any similarities. For example, suppose we want to search for a particular book in a heap of many books. OpenCV provides us with two feature matching algorithms:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Brute-force matcher</li><li class="listitem" style="list-style-type: disc">FLANN based matcher</li></ul></div><p>We will see how the two work in the following sections.</p><p>For matching, we first need to declare some variables:</p><div class="informalexample"><pre class="programlisting">DescriptorMatcher descriptorMatcher;
MatOfDMatch matches = new MatOfDMatch();</pre></div><div class="section" title="Brute-force matcher"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec23"/>Brute-force matcher</h2></div></div></div><p>It takes the <a id="id156" class="indexterm"/>descriptor of one feature in the first set and matches it with all other features in the second set, using distance calculations, and the closest one is returned.</p><p>The BF matcher takes two optional parameters. The first one is the distance measurement type, <code class="literal">normType</code>. We should use <code class="literal">NORM_L2</code> for descriptors such as SIFT and SURF. For descriptors that are based on a binary string, such as ORB and BRISK, we use <code class="literal">NORM_HAMMING</code> as the distance measurement. The second one is <code class="literal">crosscheck</code>. If it is set to true, the matcher only returns matches with values (i, j) such that the i<sup>th</sup> descriptor in the first image has the j<sup>th</sup> descriptor in the second set, as the best matches, and vice versa.</p><p>In our case for SIFT, we add the following code:</p><div class="informalexample"><pre class="programlisting">descriptorMatcher = DescriptorMatcher.create(<span class="strong"><strong>DescriptorMatcher.BRUTEFORCE_SL2</strong></span>);</pre></div></div><div class="section" title="FLANN based matcher"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec24"/>FLANN based matcher</h2></div></div></div><p>
<span class="strong"><strong>FLANN</strong></span> stands for <span class="strong"><strong>Fast Library for Approximate Nearest Neighbors</strong></span>. It contains a collection <a id="id157" class="indexterm"/>of algorithms optimized for a fast nearest neighbor search in large datasets and for high-dimensional features. It works faster than the BF matcher for large datasets.</p><p>For<a id="id158" class="indexterm"/> FLANN based matcher, we need to pass two dictionaries, which specifies the algorithm to be used, its related parameters, and so on. The first one is <code class="literal">IndexParams</code>. For various algorithms, the information to be passed is explained in the FLANN docs.</p><p>The second dictionary is <code class="literal">SearchParams</code>. It specifies the number of times the trees in the index should be recursively traversed. Higher values give better precision, but also take more time.</p><p>To use the FLANN based matcher, we need to initialize it as follows:</p><div class="informalexample"><pre class="programlisting">descriptorMatcher = DescriptorMatcher.create(<span class="strong"><strong>DescriptorMatcher.FLANNBASED</strong></span>);</pre></div></div><div class="section" title="Matching the points"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec25"/>Matching the points</h2></div></div></div><p>Once we <a id="id159" class="indexterm"/>have the <code class="literal">DescriptorMatcher</code> object, we use the <code class="literal">match()</code> and <code class="literal">knnMatch()</code> functions. The first one returns all the matches, while the second one returns <span class="emphasis"><em>k</em></span> matches, where <span class="emphasis"><em>k</em></span> is defined by the user.</p><p>After we have computed the descriptors, we can use the following to match the keypoints:</p><div class="informalexample"><pre class="programlisting">descriptorMatcher.match(descriptors1, descriptors2, matches);1</pre></div><p>Now we show the matches obtained using <code class="literal">drawMatches()</code>, which helps us draw the matches. It stacks two images horizontally and draws lines from the first image to the second image, showing the best matches. There is also a <code class="literal">drawMatchesKnn()</code> function, which draws all the <span class="emphasis"><em>k</em></span> best matches. If <span class="emphasis"><em>k = 2</em></span>, it will draw two match lines for each keypoint. So, we have to pass a mask if we want to selectively draw it.</p><p>To draw the matches, we will add a function that will merge the query and train image into one and also display the matches in the same image:</p><div class="informalexample"><pre class="programlisting">static Mat drawMatches(Mat img1, MatOfKeyPoint key1, Mat img2, MatOfKeyPoint key2, MatOfDMatch matches, boolean imageOnly){
        Mat out = new Mat();
        Mat im1 = new Mat();
        Mat im2 = new Mat();
        Imgproc.cvtColor(img1, im1, Imgproc.COLOR_BGR2RGB);
        Imgproc.cvtColor(img2, im2, Imgproc.COLOR_BGR2RGB);
        if (imageOnly){
            MatOfDMatch emptyMatch = new MatOfDMatch();
            MatOfKeyPoint emptyKey1 = new MatOfKeyPoint();
            MatOfKeyPoint emptyKey2 = new MatOfKeyPoint();
            Features2d.drawMatches(im1, emptyKey1, im2, emptyKey2, emptyMatch, out);
        } else {
            Features2d.drawMatches(im1, key1, im2, key2, matches, out);
        }
        Bitmap bmp = Bitmap.createBitmap(out.cols(), out.rows(), Bitmap.Config.ARGB_8888);
        Imgproc.cvtColor(out, out, Imgproc.COLOR_BGR2RGB);
        Core.putText(out, "FRAME", new Point(img1.width() / 2,30), Core.FONT_HERSHEY_PLAIN, 2, new Scalar(0,255,255),3);
        Core.putText(out, "MATCHED", new Point(img1.width() + img2.width() / 2,30), Core.FONT_HERSHEY_PLAIN, 2, new Scalar(255,0,0),3);
        return out;
    }</pre></div><p>Because SIFT <a id="id160" class="indexterm"/>and SURF are patented algorithms, they are not automatically built by OpenCV. We need to manually build the <code class="literal">nonfree</code> module so as to be able to use them in OpenCV. For this, you will need to download Android NDK, which allows us to use the native C++ code along with the Java code. It is available at <a class="ulink" href="https://developer.android.com/tools/sdk/ndk/index.html">https://developer.android.com/tools/sdk/ndk/index.html</a>. Then, extract it to a suitable location.</p><p>First, you need to download some files from OpenCV's source repository, which is located at <a class="ulink" href="https://github.com/Itseez/opencv/tree/master/modules">https://github.com/Itseez/opencv/tree/master/modules</a>. These are <code class="literal">nonfree_init.cpp</code>, <code class="literal">precomp.cpp</code>, <code class="literal">sift.cpp</code>, and <code class="literal">surf.cpp</code>. These will also be available with the code for this chapter, so you can download them directly from there as well. Now, create a folder in your <code class="literal">src</code> directory called <code class="literal">jni</code> and copy these files to there. We need to modify these files a bit.</p><p>Open <code class="literal">precomp.hpp</code> and remove the lines <code class="literal">#include "cvconfig.h"</code> and <code class="literal">#include "opencv2/ocl/private/util.hpp"</code>.</p><p>Open <code class="literal">nonfree_init.cpp</code> and remove the lines of code starting from <code class="literal">#ifdef HAVE_OPENCV_OCL</code> and ending at <code class="literal">#endif</code>.</p><p>Now we will create a file called <code class="literal">Android.mk</code> and copy the following lines of code to it. You need to replace <code class="literal">&lt;OpenCV4Android_SDK_location&gt;</code> accordingly:</p><div class="informalexample"><pre class="programlisting">LOCAL_PATH := $(call my-dir)

include $(CLEAR_VARS)

OPENCV_CAMERA_MODULES:=on
OPENCV_INSTALL_MODULES:=on

include &lt;OpenCV4Android_SDK_location&gt;/sdk/native/jni/OpenCV.mk

LOCAL_MODULE    := nonfree
LOCAL_SRC_FILES := nonfree_init.cpp \
sift.cpp \
surf.cpp
LOCAL_LDLIBS +=  -llog -ldl
include $(BUILD_SHARED_LIBRARY)</pre></div><p>Next, create a file named <code class="literal">Application.mk</code> and copy the following lines of code to it. These define the architecture for which our library would be built:</p><div class="informalexample"><pre class="programlisting">APP_STL := gnustl_static
APP_CPPFLAGS := -frtti -fexceptions
APP_ABI := armeabi-v7a
APP_PLATFORM := android-8</pre></div><p>Open the <code class="literal">build.gradle</code> file in your <code class="literal">app</code> folder. Under the <code class="literal">android</code> section, add the following:</p><div class="informalexample"><pre class="programlisting">sourceSets.main {
    jniLibs.srcDir 'src/main/libs'
    jni.srcDirs = [] //disable automatic ndk-build call
}</pre></div><p>Open a<a id="id161" class="indexterm"/> terminal or a command window if you are on Windows. Then, change the directory to your project using the <code class="literal">cd</code> command. Type the following in the command window:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cd &lt;project_directory&gt;/app/src/main/jni</strong></span>
</pre></div><p>In the terminal window, type the following, replacing <code class="literal">&lt;ndk_dir&gt;</code> with the appropriate directory location:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&lt;ndk_dir&gt;/ndk-build</strong></span>
</pre></div><p>After this, our library should have been successfully built and should be available in the <code class="literal">src</code> | <code class="literal">obj</code> folder, under the correct architecture.</p><p>Now we need to load this library from our Java code. Open <code class="literal">MainActivity.java</code>, and in our OpenCV Manager's callback variable (the <code class="literal">mOpenCVCallback</code> file's <code class="literal">onManagerConnected</code> function) within the case for <code class="literal">LoaderCallbackInterface.SUCCESS</code>, add the following line of code:</p><div class="informalexample"><pre class="programlisting">System.loadLibrary("nonfree");</pre></div><p>The name of the library, <code class="literal">nonfree</code>, is the same as the module name defined in the <code class="literal">Android.mk</code> file.</p><div class="mediaobject"><img src="graphics/B02052_03_11.jpg" alt="Matching the points"/><div class="caption"><p>SIFT feature matching</p></div></div></div><div class="section" title="Detecting objects"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec26"/>Detecting objects</h2></div></div></div><p>In the <a id="id162" class="indexterm"/>previous sections, we detected features in multiple images and matched them to their corresponding features in the other images. The information we obtained is enough to locate objects in a scene.</p><p>We use a function from OpenCV's <code class="literal">calib3d</code> module, <code class="literal">findHomography()</code>.Using this function, we can find a perspective transformation of the object, that is, a rotated and skewed result. Then we use <code class="literal">perspectiveTransform()</code> to locate the object in the scene. We need at least four matching points to calculate the transformation successfully.</p><p>We have seen that there can be some possible errors while matching, which may affect the result. To solve this problem, the algorithm uses either <code class="literal">RANSAC</code> or <code class="literal">LEAST_MEDIAN</code> (which can be specified by the flags). Good matches that provide the correct estimation are called inliers and the remaining are called outliers. <code class="literal">findHomography()</code> returns a mask, which specifies the inlier and outlier points.</p><p>Now we will look at the algorithm to implement it.</p><p>First, we <a id="id163" class="indexterm"/>detect and match keypoints in both the images. This has already been done in the previous sections. Then we set a condition that there has to be a certain number of matches to detect an object.</p><p>If enough matches are found, we extract the locations of matched keypoints in both the images. They are passed to find the perspective transformation. Once we get this 3x3 transformation matrix, we use it to transform the corners of <code class="literal">queryImage</code> to the corresponding points in <code class="literal">trainImage</code>. Then, we draw it.</p><p>Finally, we draw our inliers (if we successfully find the object) or matching keypoints (if it failed).</p></div></div>
<div class="section" title="Speeded Up Robust Features"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec22"/>Speeded Up Robust Features</h1></div></div></div><p>
<span class="strong"><strong>Speeded Up Robust Features</strong></span> (<span class="strong"><strong>SURF</strong></span>) was<a id="id164" class="indexterm"/> proposed by Herbert Bay, Tinne Tuytelaars, and Luc Van Gool in 2006. Some of the drawbacks of SIFT are that it is slow and computationally expensive. To target this problem, SURF was thought of. Apart from the increase in speed, the other motivations behind SURF were as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Fast interest point detection</li><li class="listitem" style="list-style-type: disc">Distinctive interest point description</li><li class="listitem" style="list-style-type: disc">Speeded up descriptor matching</li><li class="listitem" style="list-style-type: disc">Invariant to the following common image transformations:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Image rotation</li><li class="listitem" style="list-style-type: disc">Scale changes</li><li class="listitem" style="list-style-type: disc">Illumination changes</li><li class="listitem" style="list-style-type: disc">Small changes in viewpoint</li></ul></div></li></ul></div><div class="section" title="SURF detector"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec27"/>SURF detector</h2></div></div></div><p>Just as<a id="id165" class="indexterm"/> SIFT approximate Laplacian of Gaussian images to Difference of Gaussian, SURF uses integral images to approximate Laplacian of Gaussian images. An integral image (summed area tables) is an intermediate representation of the image and contains the sum of grayscale pixel values of the image. It is called the <a id="id166" class="indexterm"/>
<span class="strong"><strong>fast Hessian</strong></span> detector. The descriptor, on the other hand, describes a distribution of Haar wavelet responses within the interest point neighborhood.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note12"/>Note</h3><p>You can refer to<a id="id167" class="indexterm"/> the paper at <a class="ulink" href="http://www.vision.ee.ethz.ch/~surf/eccv06.pdf">http://www.vision.ee.ethz.ch/~surf/eccv06.pdf</a>.</p></div></div><p>To select the <a id="id168" class="indexterm"/>location and scale of keypoints, SURF uses the determinant of the Hessian matrix. SURF proves that Gaussian is overrated as the property that no new structures can appear while going down to lower resolutions has only been proved in 1D, but does not apply to the 2D case. Given SIFT's success with the LoG approximation, SURF further approximates LoG using box filters. Box filters approximate Gaussians and can be calculated very quickly. The following image shows an approximation of Gaussians as box filters:</p><div class="mediaobject"><img src="graphics/B02052_03_12.jpg" alt="SURF detector"/></div><p>Due to the use of box filters and integral images, we no longer have to perform repeated Gaussian smoothing. We apply box filters of different sizes directly to the integral image. Instead of iteratively down-scaling images, we up-scale the filter size. Hence, scale analysis is done using only a single image. The output of the preceding 9x9 filter is considered as the initial scale layer. Other layers are obtained by filtering, using gradually bigger filters. Images of the first octave are obtained using filters of size 9x9, 15x15, 21x21, and 27x27. At larger scales, the step size between the filters should also scale accordingly. Hence, for each new octave, the filter size step is doubled (that is, from 6 to 12 to 24). In the next octave, the filter sizes are 39x39, 51x51, and so on.</p><p>In order to localize interest points in the image and over scales, a non-maximum suppression in a 3x3x3 neighborhood is applied. The maxima of the determinant of the Hessian matrix is then interpolated in scale and image space using the method proposed by Brown, and others. Scale space interpolation is especially important in our case, as the difference in scale between the first layers of every octave is relatively large.</p></div><div class="section" title="SURF descriptor"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec28"/>SURF descriptor</h2></div></div></div><p>Now that <a id="id169" class="indexterm"/>we have localized the keypoints, we need to create a descriptor for each, so as to uniquely identify it from other keypoints. SURF works on similar principles of SIFT, but with lesser complexity. Bay and others also proposed a variation of SURF that doesn't take rotation invariance into account, which is called <a id="id170" class="indexterm"/>
<span class="strong"><strong>U-SURF</strong></span> (upright SURF). In many applications, the camera orientation remains more or less constant. Hence, we can save a lot of computation by ignoring rotation invariance.</p><p>First, we need to fix a reproducible orientation based on the information obtained from a circular region centered about the keypoint. Then we construct a square region that is rotated and aligned based on the selected orientation, and then we can extract the SURF descriptor from it.</p><div class="section" title="Orientation assignment"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl4sec01"/>Orientation assignment</h3></div></div></div><p>In order<a id="id171" class="indexterm"/> to add rotation invariance, the orientation of the keypoints must be robust and reproducible. For this, SURF proposes calculating Haar wavelet responses in the <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span> directions. The responses are calculated in a circular neighborhood of radius 6 s around the keypoint, where s is the scale of the image (that is, the value of σ). To calculate the Haar wavelet responses, SURF proposes using a wavelet size of 4 s. After obtaining the wavelet responses and weighing them with a Gaussian kernel <span class="inlinemediaobject"><img src="graphics/B02052_03_29.jpg" alt="Orientation assignment"/></span> centered about the keypoint, the responses are represented as vectors. The vectors are represented as the response strength in the horizontal direction along the abscissa, and the response strength in the vertical direction along the ordinate. All the responses within a sliding orientation window covering an angle of 60 degrees are then summed up. The longest vector calculated is set as the direction of the descriptor:</p><div class="mediaobject"><img src="graphics/B02052_03_13.jpg" alt="Orientation assignment"/><div class="caption"><p>Haar wavelet responses in a 60 degree angle</p></div></div><p>The size of the<a id="id172" class="indexterm"/> sliding window is taken as a parameter, which has to be calculated experimentally. Small window sizes result in single dominating wavelet responses, whereas large window sizes result in maxima in vector lengths that are not descriptive enough. Both result in an unstable orientation of the interest region. This step is skipped for U-SURF, as it is doesn't require rotation invariance.</p></div><div class="section" title="Descriptor based on Haar wavelet responses"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl4sec02"/>Descriptor based on Haar wavelet responses</h3></div></div></div><p>For<a id="id173" class="indexterm"/> the extraction of the descriptor, the first step consists of constructing a square region centered around the interest point and oriented along the orientation selected in the previous section. This is not required for U-SURF. The size of the window is 20 s. The steps to find the descriptor are as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Split the interest region into 4x4 square subregions with 5x5 regularly spaced sample points inside.</li><li class="listitem">Calculate Haar wavelet responses <span class="emphasis"><em>d<sup>x</sup></em></span> and <span class="emphasis"><em>d<sup>y</sup></em></span> [<span class="emphasis"><em>d<sup>x</sup></em></span> = Haar wavelet response in <span class="emphasis"><em>x</em></span> direction; d<sup>y</sup> = Haar wavelet response in <span class="emphasis"><em>y</em></span> direction. The filter size used is 2 s].</li><li class="listitem">Weight the response with a Gaussian kernel centered at the interest point.</li><li class="listitem">Sum the response over each subregion for <span class="emphasis"><em>d<sup>x</sup></em></span> and <span class="emphasis"><em>d<sup>y</sup></em></span> separately, to form a feature vector of length 32.</li><li class="listitem">In order to bring in information about the polarity of the intensity changes, extract the sum of the absolute value of the responses, which is a feature vector of length 64.</li><li class="listitem">Normalize the vector to unit length.</li></ol></div><p>The <a id="id174" class="indexterm"/>wavelet responses are invariant to a bias in illumination (offset). Invariance to contrast (a scale factor) is achieved by turning the descriptor into a unit vector (normalization).</p><p>Experimentally, Bay and others tested a variation of SURF that adds some more features (SURF-128). The sums of <span class="emphasis"><em>d<sup>x</sup></em></span> and <span class="emphasis"><em>|d<sup>x</sup>|</em></span> are computed separately for <span class="emphasis"><em>d<sup>y</sup> &lt; 0</em></span> and <span class="emphasis"><em>d<sup>y</sup> ≥ 0</em></span>. Similarly, the sums of <span class="emphasis"><em>d<sup>y</sup></em></span> and <span class="emphasis"><em>|d<sup>y</sup>|</em></span> are split according to the sign of <span class="emphasis"><em>d<sup>x</sup></em></span>, thereby doubling the number of features. This version of SURF-128 outperforms SURF.</p><p>The following table shows a comparison between the various algorithms in finding features:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom"> </th><th style="text-align: left" valign="bottom">
<p>U-SURF</p>
</th><th style="text-align: left" valign="bottom">
<p>SURF</p>
</th><th style="text-align: left" valign="bottom">
<p>SIFT</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Time (ms)</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>225</p>
</td><td style="text-align: left" valign="top">
<p>354</p>
</td><td style="text-align: left" valign="top">
<p>1036</p>
</td></tr></tbody></table></div><p>While SIFT and SURF work well in finding good features, they are <span class="strong"><strong>patented</strong></span> for commercial use. So, you have to pay some money if you use them for commercial purposes.</p><p>Some of the results we obtain from SURF are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">SURF is faster than SIFT by three times and has a recall precision no worse than SIFT</li><li class="listitem" style="list-style-type: disc">SURF is good at handling images with blurring or rotation</li><li class="listitem" style="list-style-type: disc">SURF is poor at handling images with viewpoint changes</li></ul></div></div></div><div class="section" title="SURF in OpenCV"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec29"/>SURF in OpenCV</h2></div></div></div><p>The code <a id="id175" class="indexterm"/>for SURF needs only a little modification. We just need to add a case in our switch case construct:</p><div class="informalexample"><pre class="programlisting">case HomeActivity.MODE_SURF:
    detector = FeatureDetector.create(<span class="strong"><strong>FeatureDetector.SURF</strong></span>);
    descriptorExtractor = DescriptorExtractor.create(<span class="strong"><strong>DescriptorExtractor.SURF</strong></span>);
    descriptorMatcher = DescriptorMatcher.create(<span class="strong"><strong>DescriptorMatcher.BRUTEFORCE_SL2</strong></span>);
    break;</pre></div><div class="mediaobject"><img src="graphics/B02052_03_14.jpg" alt="SURF in OpenCV"/><div class="caption"><p>SURF feature matching</p></div></div></div></div>
<div class="section" title="Oriented FAST and Rotated BRIEF"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec23"/>Oriented FAST and Rotated BRIEF</h1></div></div></div><a id="id176" class="indexterm"/><p>
<span class="strong"><strong>Oriented FAST and Rotated BRIEF</strong></span> (<span class="strong"><strong>ORB</strong></span>) was developed at OpenCV labs by Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary R. Bradski in 2011, as an efficient and viable alternative to SIFT and SURF. ORB was conceived mainly because SIFT and SURF are patented algorithms. ORB, however, is free to use.</p><p>ORB performs as well as SIFT on these tasks (and better than SURF), while being almost two order of magnitude faster. ORB builds on the well-known FAST keypoint detector and the BRIEF descriptor. Both these techniques are attractive because of their good performance and low cost. ORB's <a id="id177" class="indexterm"/>main contributions are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The addition of a fast and accurate orientation component to FAST</li><li class="listitem" style="list-style-type: disc">The efficient computation of oriented BRIEF features</li><li class="listitem" style="list-style-type: disc">Analysis of variance and correlation of oriented BRIEF features</li><li class="listitem" style="list-style-type: disc">A learning method for decorrelating BRIEF features under rotational invariance, leading to better performance in nearest-neighbor applications</li></ul></div><div class="section" title="oFAST – FAST keypoint orientation"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec30"/>oFAST – FAST keypoint orientation</h2></div></div></div><p>FAST<a id="id178" class="indexterm"/> is a <a id="id179" class="indexterm"/>feature<a id="id180" class="indexterm"/> detection algorithm that is widely recognized due its fast computation properties. It doesn't propose a descriptor to uniquely identify features. Moreover, it does not have any orientation component, so it performs poorly to in-plane rotation and scale changes. We will take a look at how ORB added an orientation component to FAST features.</p><div class="section" title="FAST detector"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec08"/>FAST detector</h3></div></div></div><p>First, we <a id="id181" class="indexterm"/>detect FAST keypoints. FAST takes one parameter from the user, the threshold value between the center pixel, and those in a circular ring around it. We use a ring radius of 9 pixels as it gives good performance. FAST also produces keypoints that are along edges. To overcome this, we use the Harris corner measure to order the keypoints. If we want N keypoints, we first keep the threshold low enough to generate more than N keypoints, and then pick the topmost N based on the Harris corner measure.</p><p>FAST does not produce multiscale features. ORB employs a scale pyramid of the image and produces FAST features (filtered by Harris) at each level in the pyramid.</p></div><div class="section" title="Orientation by intensity centroid"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec09"/>Orientation by intensity centroid</h3></div></div></div><p>To assign <a id="id182" class="indexterm"/>orientation to corners, we use the intensity centroid. We assume that the corner is offset from the intensity centroid and this vector is used to assign orientation to a keypoint.</p><p>To compute the coordinates of the centroid, we use moments. Moments are calculated as follows:</p><div class="mediaobject"><img src="graphics/B02052_03_30.jpg" alt="Orientation by intensity centroid"/></div><p>The coordinates of the centroid can be calculated as follows:</p><div class="mediaobject"><img src="graphics/B02052_03_31.jpg" alt="Orientation by intensity centroid"/></div><p>We <a id="id183" class="indexterm"/>construct a vector <span class="inlinemediaobject"><img src="graphics/B02052_03_32.jpg" alt="Orientation by intensity centroid"/></span> from the keypoint's center, <span class="emphasis"><em>O</em></span>, to the centroid, <span class="emphasis"><em>C</em></span>. The orientation of the patch is obtained by:</p><div class="mediaobject"><img src="graphics/B02052_03_33.jpg" alt="Orientation by intensity centroid"/></div><p>Here,<span class="emphasis"><em> atan2</em></span> is the quadrant-aware version of <span class="emphasis"><em>arctan</em></span>. To improve the rotation invariance of this measure, we make sure that the moments are computed with <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span> remaining within a circular region of radius <span class="emphasis"><em>r</em></span>. We empirically choose <span class="emphasis"><em>r</em></span> to be the patch size so that <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span> run from <span class="emphasis"><em>[−r, r]</em></span>. As <span class="emphasis"><em>|C|</em></span> approaches <span class="emphasis"><em>0</em></span>, the measure becomes unstable; with FAST corners, we have found that this is rarely the case. This method can also work well in images with heavy noise.</p></div></div><div class="section" title="rBRIEF – Rotation-aware BRIEF"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec31"/>rBRIEF – Rotation-aware BRIEF</h2></div></div></div><p>BRIEF<a id="id184" class="indexterm"/> is <a id="id185" class="indexterm"/>a feature<a id="id186" class="indexterm"/> description algorithm that is also known for its fast speed of computation. However, BRIEF also isn't invariant to rotation. ORB tries to add this functionality, without losing out on the speed aspect of BRIEF. The feature vector obtained by <span class="emphasis"><em>n</em></span> binary tests in BRIEF is as follows:</p><div class="mediaobject"><img src="graphics/B02052_03_34.jpg" alt="rBRIEF – Rotation-aware BRIEF"/></div><p>Where <span class="inlinemediaobject"><img src="graphics/B02052_03_35.jpg" alt="rBRIEF – Rotation-aware BRIEF"/></span> is defined as:</p><div class="mediaobject"><img src="graphics/B02052_03_36.jpg" alt="rBRIEF – Rotation-aware BRIEF"/></div><p>
<span class="emphasis"><em>p(x)</em></span> is the intensity value at pixel <span class="emphasis"><em>x</em></span>.</p><div class="section" title="Steered BRIEF"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec10"/>Steered BRIEF</h3></div></div></div><p>The <a id="id187" class="indexterm"/>matching performance of BRIEF falls off sharply for in-plane rotation of more than a few degrees. ORB proposes a method to steer BRIEF according to the orientation of the keypoints. For any feature set of n binary tests at location (x<sup>i</sup>, y<sup>i</sup>), we define the <span class="emphasis"><em>2 x n</em></span> matrix:</p><div class="mediaobject"><img src="graphics/B02052_03_37.jpg" alt="Steered BRIEF"/></div><p>We use the patch orientation <span class="emphasis"><em>θ</em></span> and the corresponding rotation matrix <span class="emphasis"><em>R<sup>θ</sup></em></span>, and construct a <span class="emphasis"><em>steered</em></span> version <span class="emphasis"><em>S<sup>θ</sup></em></span> of <span class="emphasis"><em>S</em></span>:</p><div class="mediaobject"><img src="graphics/B02052_03_38.jpg" alt="Steered BRIEF"/></div><p>Now the steered BRIEF operator becomes:</p><div class="mediaobject"><img src="graphics/B02052_03_39.jpg" alt="Steered BRIEF"/></div><p>We discretize the angle to increments of <span class="emphasis"><em>2π/30</em></span> (12 degrees), and construct a lookup table of precomputed<a id="id188" class="indexterm"/> BRIEF patterns. As long as the keypoint orientation <span class="emphasis"><em>θ</em></span> is consistent across views, the correct set of points <span class="emphasis"><em>S<sup>θ</sup></em></span> will be used to compute its descriptor.</p></div><div class="section" title="Variance and correlation"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec11"/>Variance and correlation</h3></div></div></div><p>One of<a id="id189" class="indexterm"/> the properties of BRIEF is that each bit feature has a large variance and a mean near 0.5. A mean of 0.5 gives a maximum sample variance of 0.25 for a bit feature. Steered BRIEF produces a more uniform appearance to binary tests. High variance causes a feature to respond more differently to inputs.</p><p>Having uncorrelated features is desirable as in that case, each test has a contribution to the results. We search among all the possible binary tests to find ones that have a high variance (and a mean close to 0.5) as well as being uncorrelated.</p><p>ORB specifies the rBRIEF algorithm as follows:</p><p>Set up a <a id="id190" class="indexterm"/>training set of some 300 k keypoints drawn from images in the PASCAL 2006 set. Then, enumerate all the possible binary tests drawn from a 31x31 pixel patch. Each test is a pair of 5x5 subwindows of the patch. If we note the width of our patch as <span class="emphasis"><em>w<sup>p</sup> = 31</em></span> and the width of the test subwindow as <span class="emphasis"><em>w<sup>t</sup> = 5</em></span>, then we have <span class="emphasis"><em>N = (wp − w<sup>t</sup>)<sup>2</sup></em></span> possible subwindows. We would like to select pairs of two from these, so we have <span class="inlinemediaobject"><img src="graphics/B02052_03_40.jpg" alt="Variance and correlation"/></span> 2 binary tests. We eliminate tests that overlap, so we end up with <span class="emphasis"><em>N = 205590</em></span> possible tests. The algorithm is as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Run each test against all training patches.</li><li class="listitem" style="list-style-type: disc">Order the tests by their distance from a mean of 0.5, forming the vector T.</li><li class="listitem" style="list-style-type: disc">Perform a greedy search:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Put the first test into the result vector R and remove it from T.</li><li class="listitem" style="list-style-type: disc">Take the next test from T, and compare it against all tests in R. If its absolute correlation is greater than a threshold, discard it; else add it to R.</li><li class="listitem" style="list-style-type: disc">Repeat the previous step until there are 256 tests in R. If there are fewer than 256, raise the threshold and try again.</li></ul></div></li></ul></div><p>rBRIEF shows significant improvement in the variance and correlation over steered BRIEF. ORB outperforms SIFT and SURF on the outdoor dataset. It is about the same on the indoor set; note that blob detection keypoints, such as SIFT, tend to be better on graffiti type images.</p></div></div><div class="section" title="ORB in OpenCV"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec32"/>ORB in OpenCV</h2></div></div></div><p>The <a id="id191" class="indexterm"/>code for ORB is similar to SIFT and SURF. However, ORB being a binary string-based descriptor, we will use the hamming code in our BF matcher.</p><p>The code for SURF needs only a little modification. We just need to add a case to our switch case construct:</p><div class="informalexample"><pre class="programlisting">case HomeActivity.MODE_ORB:
    detector = FeatureDetector.create(<span class="strong"><strong>FeatureDetector.ORB</strong></span>);
    descriptorExtractor = DescriptorExtractor.create(<span class="strong"><strong>DescriptorExtractor.ORB</strong></span>);
    descriptorMatcher = DescriptorMatcher.create(<span class="strong"><strong>DescriptorMatcher.BRUTEFORCE_HAMMING</strong></span>);
    break;</pre></div><div class="mediaobject"><img src="graphics/B02052_03_15.jpg" alt="ORB in OpenCV"/><div class="caption"><p>ORB feature matching</p></div></div></div></div>
<div class="section" title="Binary Robust Invariant Scalable Keypoints"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec24"/>Binary Robust Invariant Scalable Keypoints</h1></div></div></div><p>
<span class="strong"><strong>Binary Robust Invariant Scalable Keypoints</strong></span> (<span class="strong"><strong>BRISK</strong></span>) was conceived by Leutenegger, Chli, and Siegwart to be an efficient replacement to the state-of-the-art feature detection, description, and <a id="id192" class="indexterm"/>matching algorithms. The motivation behind BRISK was to develop a robust algorithm that can reproduce features in a computationally efficient manner. In some cases, BRISK achieves comparable quality of feature matching as SURF, while requiring much less computation time.</p><div class="section" title="Scale-space keypoint detection"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec33"/>Scale-space keypoint detection</h2></div></div></div><p>The <a id="id193" class="indexterm"/>BRISK detector is based on the AGAST detector, which is an extension of a faster performance version of FAST. To achieve scale invariance, BRISK searches for the maxima in a scale space using the FAST score(s) as the comparison parameter. Despite discretizing the scale axis at coarser intervals than in alternative high-performance detectors (for example, the fast Hessian), the BRISK detector estimates the true scale of each keypoint in the continuous scale space. The BRISK scale space comprises of n octaves, c<sup>i</sup> and <span class="emphasis"><em>n</em></span> intra-octaves, and <span class="emphasis"><em>d<sup>i</sup> [i = {0, 1, 2, …, n-1}]</em></span>. BRISK suggests using <span class="emphasis"><em>n = 4</em></span>.</p><p>The original image is taken as <span class="emphasis"><em>c<sup>0</sup></em></span>, and each successive octave is half-sampled from the previous octave. Each intra-octave d<sup>i</sup> is down-sampled such that it lies between <span class="emphasis"><em>c<sup>i</sup></em></span> and <span class="emphasis"><em>c<sup>i</sup>+1</em></span>. The first intra-octave <span class="emphasis"><em>d<sup>0</sup></em></span> is obtained by down sampling <span class="emphasis"><em>c<sup>0</sup></em></span> by a factor of 1.5. The subsequent intra-octaves are obtained by half sampling the previous intra-octave.</p><div class="mediaobject"><img src="graphics/B02052_03_16.jpg" alt="Scale-space keypoint detection"/><div class="caption"><p>An image showing octaves and intra-octaves</p></div></div><p>The FAST 9-16 detector requires that in a 16 pixel circular radius, at least 9 pixels must be brighter than or darker than the center pixel for the FAST criterion to be fulfilled. BRISK proposes the use of this FAST 9-16 detector.</p><p>The FAST score is<a id="id194" class="indexterm"/> computed for each octave and intra-octave separately. The FAST detector score, <span class="emphasis"><em>s</em></span>, is calculated for each pixel as the maximum threshold for FAST detection, such that an image point is considered as a corner.</p><p>A non-maximum suppression in scale space is carried out on the keypoints obtained after applying the FAST 9-16 detector. The keypoint should be the maximum among its eight neighboring FAST scores in the same octave or intra-octave. This point must also have a higher FAST score than points in the layers above and below it. We then check inside the equally sized square patches having a 2 pixel side length in the layer, where the maximum value is suspected to be present. Interpolation is carried out at the boundaries of the patch, as neighboring layers are represented with different discretizations than that of the current later.</p><p>We try to calculate a subpixel location for each maximum detected in the earlier step. A 2D quadratic function is fitted to the 3x3 patch surrounding the pixel, and the subpixel maximum is determined. This is also done for the layers above and below the current layer. These maximas are then interpolated using a 1D parabola across the scale space, and the local maximum is chosen as the scale for the feature is found.</p></div><div class="section" title="Keypoint description"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec34"/>Keypoint description</h2></div></div></div><p>The<a id="id195" class="indexterm"/> BRISK descriptor is <a id="id196" class="indexterm"/>composed of a binary string by concatenating the results of simple brightness comparison tests. In BRISK, we need to identify the characteristic direction of each keypoint to achieve the rotation invariance.</p><div class="section" title="Sampling pattern and rotation estimation"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec12"/>Sampling pattern and rotation estimation</h3></div></div></div><p>The BRISK <a id="id197" class="indexterm"/>descriptor makes use of a pattern used for sampling the neighborhood of the keypoint. The pattern defines N locations equally spaced on circles concentric with the keypoint, as shown in the following figure:</p><div class="mediaobject"><img src="graphics/B02052_03_17.jpg" alt="Sampling pattern and rotation estimation"/><div class="caption"><p>BRISK sampling pattern</p></div></div><p>In order to avoid aliasing effects when sampling the image intensity of a point p<sup>i</sup> in the pattern, we apply Gaussian smoothing with the standard deviation <span class="inlinemediaobject"><img src="graphics/B02052_03_41.jpg" alt="Sampling pattern and rotation estimation"/></span> proportional to the distance between the points on the respective circles. We then calculate the gradient between two sampling points.</p><p>The formula used is:</p><div class="mediaobject"><img src="graphics/B02052_03_42.jpg" alt="Sampling pattern and rotation estimation"/></div><p>BRISK defines <a id="id198" class="indexterm"/>a subset of short distance pairings, <span class="emphasis"><em>S</em></span>, and another subset of long distance pairings, <span class="emphasis"><em>L</em></span>, as follows:</p><div class="mediaobject"><img src="graphics/B02052_03_43.jpg" alt="Sampling pattern and rotation estimation"/></div><div class="mediaobject"><img src="graphics/B02052_03_44.jpg" alt="Sampling pattern and rotation estimation"/></div><p>Where <span class="emphasis"><em>A</em></span> is the set of all sampling point pairs as follows:</p><div class="mediaobject"><img src="graphics/B02052_03_45.jpg" alt="Sampling pattern and rotation estimation"/></div><p>The threshold distances are set to <span class="inlinemediaobject"><img src="graphics/B02052_03_46.jpg" alt="Sampling pattern and rotation estimation"/></span> and <span class="inlinemediaobject"><img src="graphics/B02052_03_47.jpg" alt="Sampling pattern and rotation estimation"/></span> (<span class="emphasis"><em>t</em></span> is the scale of the keypoint). BRISK estimates the overall characteristic pattern direction of the keypoint <span class="emphasis"><em>k</em></span> to be:</p><div class="mediaobject"><img src="graphics/B02052_03_48.jpg" alt="Sampling pattern and rotation estimation"/></div></div><div class="section" title="Building the descriptor"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec13"/>Building the descriptor</h3></div></div></div><p>In order<a id="id199" class="indexterm"/> to develop a rotation and scale invariant descriptor, BRISK applies the sampling pattern rotated by an angle, <span class="inlinemediaobject"><img src="graphics/B02052_03_49.jpg" alt="Building the descriptor"/></span>, around the keypoint <span class="emphasis"><em>k</em></span>. Short distance intensity comparisons of point pairs, <span class="inlinemediaobject"><img src="graphics/B02052_03_50.jpg" alt="Building the descriptor"/></span> (that is, in the rotated pattern), are calculated to get the bit vector descriptor <span class="emphasis"><em>d<sup>k</sup></em></span>. Each bit <span class="emphasis"><em>b</em></span> corresponds to:</p><div class="mediaobject"><img src="graphics/B02052_03_51.jpg" alt="Building the descriptor"/></div><div class="mediaobject"><img src="graphics/B02052_03_52.jpg" alt="Building the descriptor"/></div><p>BRISK uses a deterministic sampling pattern, resulting in a uniform sampling point density at a given radius around the keypoint. Due to this, the Gaussian smoothing does not modify the information content of a brightness comparison by blurring two close sampling points while comparing them. BRISK uses a lesser number of sampling points than a simple pairwise comparison (because a single point participates in more comparisons), thereby reducing the complexity of looking up the intensity values. As the brightness variations only need to be locally consistent, the comparisons done here are restricted spatially. We obtain a bit string of length 512 using the sampling pattern and the distance thresholds as shown previously.</p></div></div><div class="section" title="BRISK In OpenCV"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec35"/>BRISK In OpenCV</h2></div></div></div><p>Again, the <a id="id200" class="indexterm"/>only change that we will make is to add another case to our switch case construct:</p><div class="informalexample"><pre class="programlisting">case HomeActivity.MODE_BRISK:
    detector = FeatureDetector.create(FeatureDetector.BRISK);
    descriptorExtractor = DescriptorExtractor.create(DescriptorExtractor.BRISK);
    descriptorMatcher = DescriptorMatcher.create(DescriptorMatcher.BRUTEFORCE_HAMMING);
    break;</pre></div><div class="mediaobject"><img src="graphics/B02052_03_18.jpg" alt="BRISK In OpenCV"/><div class="caption"><p>BRISK feature matching</p></div></div></div></div>
<div class="section" title="Fast Retina Keypoint"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec25"/>Fast Retina Keypoint</h1></div></div></div><p>
<span class="strong"><strong>Fast Retina Keypoint</strong></span> (<span class="strong"><strong>FREAK</strong></span>) proposes <a id="id201" class="indexterm"/>a robust descriptor to uniquely identify keypoints and in the process, require less computation time and memory. FREAK has been inspired by the human retina.</p><div class="section" title="A retinal sampling pattern"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec36"/>A retinal sampling pattern</h2></div></div></div><p>FREAK <a id="id202" class="indexterm"/>proposes to use the retinal sampling grid, which is also circular, with the difference of having higher density of points near the center. The density of points drops exponentially as we move away from the center point. This is similar to BRISK, except for the exponential decrease.</p><p>Each keypoint needs to be smoothed to be less sensitive to noise. Unlike BRIEF and ORB, which use the same kernel for all points, FREAK uses a different kernel for each keypoint. The radius of the Gaussian kernel is proportional to the value of σ.</p><div class="mediaobject"><img src="graphics/B02052_03_19.jpg" alt="A retinal sampling pattern"/><div class="caption"><p>Retinal sampling pattern</p></div></div><p>FREAK follows ORB's approach and tries to learn about the pairs by maximizing the variance of the pairs and taking pairs that are not correlated, so as to provide maximum information on each keypoint.</p></div><div class="section" title="A coarse-to-fine descriptor"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec37"/>A coarse-to-fine descriptor</h2></div></div></div><p>We need<a id="id203" class="indexterm"/> to find pairs of sampling points in order to create a bit-vector. We use a method similar to ORB, that is, instead of matching each pair, we try to learn about which pairs would give the best results. We need to find points that are not correlated. The algorithm is as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We create a matrix D of nearly 50,000 extracted keypoints. Each row corresponds to a keypoint that is represented with its large descriptor made of all possible pairs in the retina sampling pattern. We use 43 receptive fields, leading to about 1,000 pairs.</li><li class="listitem" style="list-style-type: disc">We compute the mean of each column. A mean of 0.5 produces the highest variance.</li><li class="listitem" style="list-style-type: disc">Order the columns according to the variance in descending order.</li><li class="listitem" style="list-style-type: disc">Select the best column and iteratively add the remaining columns so that they have low correlation with the chosen columns.</li></ul></div><p>In this approach, we first select pairs that compare sampling points in the outer regions, whereas the last pairs are comparison points in the inner rings of the pattern. This is similar to how our retina works in the sense that we first try to locate an object and then try to verify it by precisely matching points that are densely located near the object.</p></div><div class="section" title="Saccadic search"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec38"/>Saccadic search</h2></div></div></div><p>Humans <a id="id204" class="indexterm"/>do not look at a scene in a fixed manner. Their eyes move around with discontinuous individual movements called saccades. The fovea captures high-resolution information; hence, it is critical in the recognition and matching of objects. The perifoveal region captures low-resolution information, and hence, it used to approximately locate objects.</p><p>FREAK tries to mimic this function of the retina by searching the first 16 bytes of the descriptor, representing the coarse information. If the distance is smaller than a threshold, we continue by searching the next bytes to obtain a more refined result. Due to this, a cascade of comparisons is performed, accelerating the matching step even further as more than 90 percent of the sampling points are discarded with the first 16 byte comparisons.</p></div><div class="section" title="Orientation"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec39"/>Orientation</h2></div></div></div><p>The method FREAK uses to assign orientation<a id="id205" class="indexterm"/> is similar to that of BRISK with the difference being that, instead of using long distance pairs, FREAK uses a predefined set of 45 symmetric sampling pairs.</p></div><div class="section" title="FREAK in OpenCV"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec40"/>FREAK in OpenCV</h2></div></div></div><p>The<a id="id206" class="indexterm"/> code for FREAK is similar to that used for the previous algorithms. However, given that FREAK just provides a descriptor, we will use the FAST detector to detect keypoints:</p><div class="informalexample"><pre class="programlisting">case HomeActivity.MODE_FREAK:
    detector = FeatureDetector.create(<span class="strong"><strong>FeatureDetector.FAST</strong></span>);
    descriptorExtractor = DescriptorExtractor.create(<span class="strong"><strong>DescriptorExtractor.FREAK</strong></span>);
    descriptorMatcher = DescriptorMatcher.create(<span class="strong"><strong>DescriptorMatcher.BRUTEFORCE_HAMMING</strong></span>);
    break;</pre></div><div class="mediaobject"><img src="graphics/B02052_03_20.jpg" alt="FREAK in OpenCV"/><div class="caption"><p>FREAK feature matching</p></div></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec26"/>Summary</h1></div></div></div><p>In this chapter, we have seen how to detect features in an image and match them to features in other images. To perform this task, we looked at various algorithms, such as SIFT, SURF, ORB, BRISK, and FREAK, and their pros and cons. We also saw how we can use these to localize specific objects in a scene. There is one restriction to these methods in that the exact object must be present in the scene image to be detected correctly. In the next chapter, we will take a step forward to detect more general classes of objects, such as human beings, faces, hands, and so on.</p></div></body></html>