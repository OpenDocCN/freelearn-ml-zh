- en: Introduction to Semi-Supervised Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 半监督学习简介
- en: 'Semi-supervised learning is a machine learning branch that tries to solve problems
    with both labeled and unlabeled data with an approach that employs concepts belonging
    to clustering and classification methods. The high availability of unlabeled samples,
    in contrast with the difficulty of labeling huge datasets correctly, drove many
    researchers to investigate the best approaches that allow extending the knowledge
    provided by the labeled samples to a larger unlabeled population without loss
    of accuracy. In this chapter, we''re going to introduce this branch and, in particular,
    we will discuss:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习是机器学习的一个分支，它试图通过采用属于聚类和分类方法的概念，以解决既有标签数据又有未标记数据的问题。与正确标记大型数据集的困难相比，未标记样本的高可用性驱使许多研究人员调查最佳方法，这些方法允许在不损失准确性的情况下，将标签样本提供的知识扩展到更大的未标记群体中。在本章中，我们将介绍这个分支，特别是我们将讨论：
- en: The semi-supervised scenario
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半监督场景
- en: The assumptions needed to efficiently operate in such a scenario
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这种场景下高效操作所需的假设
- en: The different approaches to semi-supervised learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半监督学习的不同方法
- en: Generative Gaussian mixtures algorithm
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成高斯混合模型算法
- en: Contrastive pessimistic likelihood estimation approach
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对比悲观似然估计方法
- en: '**Semi-supervised Support Vector Machines** (**S³VM**)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**半监督支持向量机**（**S³VM**）'
- en: '**Transductive Support Vector Machines** (**TSVM**)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**归纳支持向量机**（**TSVM**）'
- en: Semi-supervised scenario
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 半监督场景
- en: 'A typical semi-supervised scenario is not very different from a supervised
    one. Let''s suppose we have a data generating process, *p[data]*:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的半监督场景与监督场景并没有太大的不同。假设我们有一个数据生成过程，*p[data]*：
- en: '![](img/163ed14d-061e-48f3-89d5-693b530d0525.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/163ed14d-061e-48f3-89d5-693b530d0525.png)'
- en: 'However, contrary to a supervised approach, we have only a limited number *N*
    of samples drawn from *p[data]* and provided with a label, as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与监督方法相反，我们只有有限数量的样本（*N*）是从 *p[data]* 中抽取的，并且提供了标签，如下所示：
- en: '![](img/933c6eb6-6b35-4f83-b713-adaabda6ce98.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/933c6eb6-6b35-4f83-b713-adaabda6ce98.png)'
- en: 'Instead, we have a larger amount (*M*) of unlabeled samples drawn from the
    marginal distribution *p(x)*:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们有一个更大的样本量（*M*），这些样本是从边缘分布 *p(x)* 中抽取的：
- en: '![](img/882d2b8f-70b6-4891-92b6-6cca8e919657.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/882d2b8f-70b6-4891-92b6-6cca8e919657.png)'
- en: 'In general, there are no restrictions on the values of *N* and *M;* however,
    a semi-supervised problem arises when the number of unlabeled samples is much
    higher than the number of complete samples. If we can draw *N* >> *M* labeled
    samples from *p[data]*, it''s probably useless to keep on working with semi-supervised
    approaches and preferring classical supervised methods is likely to be the best
    choice. The extra complexity we need is justified by *M* >> *N*, which is a common
    condition in all those situations where the amount of available unlabeled data
    is large, while the number of correctly labeled samples is quite a lot lower.
    For example, we can easily access millions of free images but detailed labeled
    datasets are expensive and include only a limited subset of possibilities. However,
    is it always possible to apply semi-supervised learning to improve our models?
    The answer to this question is almost obvious: unfortunately no. As a rule of
    thumb, we can say that if the knowledge of *X[u]* increases our knowledge about
    the prior distribution *p(x)*, a semi-supervised algorithm is likely to perform
    better than a purely supervised (and thus limited to *X[l]*) counterpart. On the
    other hand, if the unlabeled samples are drawn from different distributions, the
    final result can be quite a lot worse. In real cases, it''s not so immediately
    necessary to decide whether a semi-supervised algorithm is the best choice; therefore,
    cross-validation and comparisons are the best practices to employ when evaluating
    a scenario.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，对 *N* 和 *M* 的值没有限制；然而，当未标记样本的数量远多于完整样本的数量时，就会出现半监督问题。如果我们能从 *p[data]* 中抽取
    *N* >> *M* 个标记样本，继续使用半监督方法可能就没什么用了，而选择经典监督方法可能是最佳选择。我们需要额外的复杂性是由 *M* >> *N* 来证明的，这在所有那些可用的未标记数据量很大，而正确标记样本数量相当低的情况中是常见的。例如，我们可以轻松地访问数百万张免费图片，但详细标记的数据集很昂贵，并且只包括可能性的一小部分。然而，是否总是可能应用半监督学习来改进我们的模型？对这个问题的答案几乎是显而易见的：不幸的是，不是。作为一个经验法则，我们可以这样说，如果
    *X[u]* 的知识增加了我们对先验分布 *p(x)* 的了解，那么半监督算法可能比纯监督（因此仅限于 *X[l]*) 对手表现得更好。另一方面，如果未标记样本来自不同的分布，最终结果可能会相当糟糕。在实际情况下，并不总是立即有必要决定是否半监督算法是最好的选择；因此，交叉验证和比较是在评估场景时采用的最佳实践。
- en: Transductive learning
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 归纳学习
- en: When a semi-supervised model is aimed at finding the labels for the unlabeled
    samples, the approach is called transductive learning. In this case, we are not
    interested in modeling the whole distribution *p(x|y)*, which implies determining
    the density of both datasets, but rather in finding *p(y|x)* only for the unlabeled
    points. In many cases, this strategy can be time-saving and it's always preferable
    when our goal is more oriented at improving our knowledge about the unlabeled
    dataset.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当半监督模型旨在为未标记样本寻找标签时，这种方法被称为归纳学习。在这种情况下，我们并不感兴趣于建模整个分布 *p(x|y)*，这意味着确定两个数据集的密度，而是仅对未标记点寻找
    *p(y|x)*。在许多情况下，这种策略可以节省时间，并且当我们的目标更倾向于提高我们对未标记数据集的了解时，这始终是首选的。
- en: Inductive learning
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 归纳学习
- en: Contrary to transductive learning, inductive learningconsiders all the *X* samples
    and tries to determine a complete *p(x|y)* or a function *y=f(x)* that can map
    both labeled and unlabeled points to their corresponding labels. In general, this
    method is more complex and requires more computational time; therefore, according
    to *Vapnik's principle*, if not required or necessary, it's always better to pick
    the most pragmatic solution and, possibly, expand it if the problem requires further
    details.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 与归纳学习相反，归纳学习考虑所有 *X* 样本，并试图确定一个完整的 *p(x|y)* 或函数 *y=f(x)*，该函数可以将标记点和未标记点映射到它们相应的标签。通常，这种方法更复杂，需要更多计算时间；因此，根据
    *Vapnik 的原则*，如果不要求或必要，始终选择最实用的解决方案，如果问题需要更多细节，可能还需要扩展它。
- en: Semi-supervised assumptions
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 半监督假设
- en: As explained in the previous section, semi-supervised learning is not guaranteed
    to improve a supervised model. A wrong choice could lead to a dramatic worsening
    in performance; however, it's possible to state some fundamental assumptions which
    are required for semi-supervised learning to work properly. They are not always
    mathematically proven theorems, but rather empirical observations that justify
    the choice of an approach otherwise completely arbitrary.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，半监督学习并不能保证提高监督模型。错误的选择可能导致性能的显著下降；然而，可以提出一些基本假设，这些假设对于半监督学习正常工作至关重要。它们并不总是数学上证明的定理，而是经验观察，这些观察为选择一种否则完全随机的途径提供了理由。
- en: Smoothness assumption
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平滑性假设
- en: 'Let''s consider a real-valued function *f(x)* and the corresponding metric
    spaces *X* and *Y*. Such a function is said to be Lipschitz-continuous if:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个实值函数 *f(x)* 以及相应的度量空间 *X* 和 *Y*。如果一个函数被称为Lipschitz连续的，那么它必须满足以下条件：
- en: '![](img/5a428b7c-fac1-4bac-acc0-b0c195912935.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5a428b7c-fac1-4bac-acc0-b0c195912935.png)'
- en: 'In other words, if two points *x[1]* and *x[2]* are near, the corresponding
    output values *y[1]* and *y[2]* cannot be arbitrarily far from each other. This
    condition is fundamental in regression problems where a generalization is often
    required for points that are between training samples. For example, if we need
    to predict the output for a point *x[t]* : *x*[*1* ]< *x[t]* < *x[2]* and the
    regressor is Lipschitz-continuous, we can be sure that *y[t]* will be correctly
    bounded by *y[1]* and *y[2]*. This condition is often called general smoothness,
    but in semi-supervised it''s useful to add a restriction (correlated with the
    cluster assumption): if two points are in a high density region (cluster) and
    they are close, then the corresponding outputs must be close too. This extra condition
    is very important because, if two samples are in a low density region they can
    belong to different clusters and their labels can be very different. This is not
    always true, but it''s useful to include this constraint to allow some further
    assumptions in many definitions of semi-supervised models.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，如果两个点 *x[1]* 和 *x[2]* 相近，那么相应的输出值 *y[1]* 和 *y[2]* 也不能彼此相隔太远。这个条件在回归问题中是基本的，因为通常需要对位于训练样本之间的点进行泛化。例如，如果我们需要预测点
    *x[t]* 的输出：*x*[*1* ] < *x[t]* < *x[2]*，并且回归器是Lipschitz连续的，我们可以确信 *y[t]* 将被正确地限制在
    *y[1]* 和 *y[2]* 之间。这个条件通常被称为一般平滑性，但在半监督学习中，添加一个限制（与聚类假设相关）是有用的：如果两个点位于高密度区域（聚类）并且它们很近，那么相应的输出也必须很近。这个额外条件非常重要，因为如果两个样本位于低密度区域，它们可能属于不同的聚类，它们的标签可能非常不同。这并不总是正确的，但包含这个约束对于许多半监督模型定义中的进一步假设是有用的。
- en: Cluster assumption
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类假设
- en: 'This assumption is strictly linked to the previous one and it''s probably easier
    to accept. It can be expressed with a chain of interdependent conditions. Clusters
    are high density regions; therefore, if two points are close, they are likely
    to belong to the same cluster and their labels must be the same. Low density regions
    are separation spaces; therefore, samples belonging to a low density region are
    likely to be boundary points and their classes can be different. To better understand
    this concept, it''s useful to think about supervised SVM: only the support vectors
    should be in low density regions. Let''s consider the following bidimensional
    example:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这个假设与前面的假设紧密相连，并且可能更容易接受。它可以表达为一系列相互依赖的条件。聚类是高密度区域；因此，如果两个点很近，它们很可能属于同一个聚类，它们的标签必须相同。低密度区域是分离空间；因此，属于低密度区域的样本很可能属于边界点，它们的类别可能不同。为了更好地理解这个概念，考虑监督SVM是有用的：只有支持向量应该位于低密度区域。让我们考虑以下二维示例：
- en: '![](img/4f8fbd46-2ca9-4bd9-9266-13e3125c1913.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4f8fbd46-2ca9-4bd9-9266-13e3125c1913.png)'
- en: In a semi-supervised scenario, we couldn't know the label of a point belonging
    to a high density region; however, if it is close enough to a labeled point that
    it's possible to build a ball where all the points have the same average density,
    we are allowed to predict the label of our test sample. Instead, if we move to
    a low-density region, the process becomes harder, because two points can be very
    close but with different labels. We are going to discuss the semi-supervised,
    low-density separation problem at the end of this chapter.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在半监督场景中，我们无法知道属于高密度区域的点的标签；然而，如果它足够接近一个已标记的点，以至于可以构建一个所有点都具有相同平均密度的球体，我们允许预测测试样本的标签。相反，如果我们移动到低密度区域，这个过程变得更难，因为两个点可以非常接近但具有不同的标签。我们将在本章末讨论半监督、低密度分离问题。
- en: Manifold assumption
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流形假设
- en: 'This is the less intuitive assumption, but it can be extremely useful to reduce
    the complexity of many problems. First of all, we can provide a non-rigorous definition
    of a manifold. An *n*-manifold is a topological space that is globally curved,
    but locally homeomorphic to an *n*-dimensional Euclidean space. In the following
    diagram, there''s an example of a manifold: the surface of a sphere in *ℜ³*:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个不太直观的假设，但它可以极大地减少许多问题的复杂性。首先，我们可以给出流形的非严格定义。一个*n*-流形是一个全局弯曲但局部与*n*-维欧几里得空间同胚的拓扑空间。在下面的图中，有一个流形的例子：*ℜ³*中的球面表面：
- en: '![](img/78afa71a-9c17-4388-8ce7-dc7ec95c1df4.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/78afa71a-9c17-4388-8ce7-dc7ec95c1df4.png)'
- en: 2D manifold obtained from a spherical surface
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从球面得到的二维流形
- en: The small patch around *P* (for *ε* → *0*) can be mapped to a flat circular
    surface. Therefore, the properties of a manifold are locally based on the Euclidean
    geometry, while, globally, they need a proper mathematical extension which is
    beyond the scope of this book (further information can be found in *Semi-supervised
    learning on Riemannian manifolds*,* Belkin M., Niyogi P.*, *Machine Learning 56*,
    *2004*).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在*P*点周围的小块（对于*ε* → *0*）可以映射到一个平坦的圆形表面。因此，流形的性质在局部基于欧几里得几何，而在全局上，它们需要一个适当的数学扩展，这超出了本书的范围（更多信息可以在*《黎曼流形上的半监督学习》*，Belkin
    M.，Niyogi P.，*《机器学习》56*，*2004*）中找到）。
- en: The manifold assumption states that *p*-dimensional samples (where *p* >> *1*)
    approximately lie on a *q*-dimensional manifold with *p* << *q*. Without excessive
    mathematical rigor, we can say that, for example, if we have *N* *1000*-dimensional
    bounded vectors, they are enclosed into a *1000*-dimensional hypercube with edge-length
    equal to *r*. The corresponding *n*-volume is *r^p = r^(1000)*, therefore, the
    probability of filling the entire space is very small (and decreases with *p*).
    What we observe, instead, is a high density on a lower dimensional manifold. For
    example, if we look at the Earth from space, we might think that its inhabitants
    are uniformly distributed over the whole volume. We know that this is false and,
    in fact, we can create maps and atlases which are represented on two-dimensional
    manifolds. It doesn't make sense to use three-dimensional vectors to map the position
    of a human being. It's easier to use a projection and work with latitude and longitude.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 流形假设指出，*p*-维样本（其中*p* >> *1*）大致位于一个*p* << *q*的*q*-维流形上。不进行过度的数学严谨性，我们可以这样说，例如，如果我们有*N*个*1000*-维有界向量，它们被包含在一个边长等于*r*的*1000*-维超立方体中。相应的*n*-体积是*r^p =
    r^(1000)*，因此，填满整个空间的可能性非常小（并且随着*p*的增加而减小）。我们观察到的是在低维流形上的高密度。例如，如果我们从太空中观察地球，我们可能会认为其居民在整个体积上均匀分布。我们知道这是错误的，实际上，我们可以创建表示在二维流形上的地图和地图集。使用三维向量来映射人的位置是没有意义的。使用投影和纬度、经度工作更容易。
- en: 'This assumption authorizes us to apply dimensionality reduction methods in
    order to avoid the *Curse of Dimensionality*, theorized by Bellman (in *Dynamic
    Programming and Markov Process, Ronald A. Howard*, *The MIT Press*). In the scope
    of machine learning, the main consequence of such an effect is that when the dimensionality
    of the samples increases, in order to achieve a high accuracy, it''s necessary
    to use more and more samples. Moreover, Hughes observed (the phenomenon has been
    named after him and it''s presented in the paper *Hughes G. F., On the mean accuracy
    of statistical pattern recognizers, IEEE Transactions on Information Theory*,
    *1968*, *14/1*) that the accuracy of statistical classifiers is inversely proportional
    to the dimensionality of the samples. This means that whenever it''s possible
    to work on lower dimensional manifolds (in particular in semi-supervised scenarios),
    two advantages are achieved:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个假设授权我们应用降维方法来避免贝尔曼（在《动态规划与马尔可夫过程，罗纳德·A·霍华德*，《麻省理工学院出版社》*）提出的*维度的诅咒*。在机器学习的范围内，这种效应的主要后果是，当样本的维度增加时，为了达到高精度，必须使用越来越多的样本。此外，休斯观察到（这一现象以他的名字命名，并在论文*休斯
    G. F.，关于统计模式识别器的平均精度，IEEE 信息系统传输，1968，14/1*）中提出，统计分类器的精度与样本的维度成反比。这意味着每当有可能在低维流形上工作（特别是在半监督场景中）时，就会实现两个优势：
- en: Less computational time and memory consumption
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更少的计算时间和内存消耗
- en: Higher classification accuracy
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更高的分类精度
- en: Generative Gaussian mixtures
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成高斯混合
- en: 'Generative Gaussian mixtures is an inductive algorithm for semi-supervised
    clustering. Let''s suppose we have a labeled dataset (*X[l]*, *Y[l]*) containing
    *N* samples (drawn from *p[data]*) and an unlabeled dataset *X[u]* containing
    *M* >> *N* samples (drawn from the marginal distribution *p(x)*). It''s not necessary
    that *M* >> *N*, but we want to create a real semi-supervised scenario, with only
    a few labeled samples. Moreover, we are assuming that all unlabeled samples are
    consistent with *p[data]*. This can seem like a vicious cycle, but without this
    assumption, the procedure does not have a strong mathematical foundation. Our
    goal is to determine a complete *p(x|y)* distribution using a generative model.
    In general, it''s possible to use different priors, but we are now employing multivariate
    Gaussians to model our data:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 生成高斯混合是半监督聚类的归纳算法。假设我们有一个包含 *N* 个样本（从 *p[data]* 中抽取）的标记数据集 (*X[l]*, *Y[l]*)
    和一个包含 *M* >> *N* 个样本（从边缘分布 *p(x)* 中抽取）的无标记数据集 *X[u]*。不一定要求 *M* >> *N*，但我们想创建一个只有少量标记样本的真实半监督场景。此外，我们假设所有无标记样本都与
    *p[data]* 一致。这看起来像是一个恶性循环，但没有这个假设，程序就没有坚实的数学基础。我们的目标是使用生成模型确定一个完整的 *p(x|y)* 分布。一般来说，可以使用不同的先验，但我们现在使用多元高斯来模拟我们的数据：
- en: '![](img/4594a146-4eaf-4d21-9205-85bab6323788.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4594a146-4eaf-4d21-9205-85bab6323788.png)'
- en: 'Thus, our model parameters are means and covariance matrices for all Gaussians.
    In other contexts, it''s possible to use binomial or multinomial distributions.
    However, the procedure doesn''t change; therefore, let''s assume that it''s possible
    to approximate *p(x|y)* with a parametrized distribution *p(x|y*, *θ)*. We can
    achieve this goal by minimizing the Kullback-Leibler divergence between the two
    distributions:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的模型参数是所有高斯均值和协方差矩阵。在其他上下文中，可以使用二项式或多项式分布。但是，程序不会改变；因此，让我们假设可以使用参数化分布 *p(x|y*,
    *θ)* 来近似 *p(x|y)*。我们可以通过最小化两个分布之间的Kullback-Leibler散度来实现这一目标：
- en: '![](img/3149ad3a-070e-4f76-9370-e7dcf0c428dc.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3149ad3a-070e-4f76-9370-e7dcf0c428dc.png)'
- en: 'In [Chapter 5](8d541a43-8790-4a91-a79b-e48496f75d90.xhtml), *EM Algorithm and
    Applications* we are going to show that this is equivalent to maximizing the likelihood
    of the dataset. To obtain the likelihood, it''s necessary to define the number
    of expected Gaussians (which is known from the labeled samples) and a weight-vector
    that represents the marginal probability of a specific Gaussian:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](8d541a43-8790-4a91-a79b-e48496f75d90.xhtml)，*EM算法及其应用*中，我们将展示这等价于最大化数据集的似然。为了获得似然，必须定义预期的高斯数量（这可以从标记样本中得知）和一个表示特定高斯边缘概率的权重向量：
- en: '![](img/4da88076-3c81-49ec-aecd-269f48a9c11e.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4da88076-3c81-49ec-aecd-269f48a9c11e.png)'
- en: 'Using the Bayes'' theorem, we get:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 使用贝叶斯定理，我们得到：
- en: '![](img/3712c8bb-c9fc-42c6-b8e0-a73b9373a13f.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3712c8bb-c9fc-42c6-b8e0-a73b9373a13f.png)'
- en: 'As we are working with both labeled and unlabeled samples, the previous expression
    has a double interpretation:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们同时处理标记和未标记的样本，前面的表达式有两个解释：
- en: For unlabeled samples, it is computed by multiplying the *i^(th)* Gaussian weight
    times the probability *p(x[j])* relative to the *i^(th)* Gaussian distribution.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未标记的样本，它是通过将第*i*个高斯权重乘以相对于第*i*个高斯分布的概率*p(x[j])*来计算的。
- en: For labeled samples, it can be represented by a vector p = [0, 0, ... 1, ...
    0, 0] where 1 is the *i^(th)* element. In this way, we force our model to trust
    the labeled samples in order to find the best parameter values that maximize the
    likelihood on the whole dataset.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于标记样本，它可以表示为一个向量p = [0, 0, ... 1, ... 0, 0]，其中1是第*i*个元素。这样，我们迫使我们的模型相信标记样本，以便找到最大化整个数据集似然的最佳参数值。
- en: 'With this distinction, we can consider a single log-likelihood function where
    the term *f[w](y[i]|x[j])* has been substituted by a per sample weight:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种区分，我们可以考虑一个单一的似然函数，其中术语*f[w](y[i]|x[j])*已被每个样本的权重所替代：
- en: '![](img/fc30daf5-a259-4ead-b28f-952f4f76e7de.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fc30daf5-a259-4ead-b28f-952f4f76e7de.png)'
- en: 'It''s possible to maximize the log-likelihood using the EM algorithm (see [Chapter
    5](8d541a43-8790-4a91-a79b-e48496f75d90.xhtml), *EM Algorithm and Applications*).
    In this context, we provide the steps directly:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 使用EM算法（见[第5章](8d541a43-8790-4a91-a79b-e48496f75d90.xhtml)，*EM算法及其应用*）可以最大化对数似然。在这种情况下，我们直接提供步骤：
- en: '*p(y[i]|x[j],θ,w)* is computed according to the previously explained method'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p(y[i]|x[j],θ,w)*是按照之前解释的方法计算的'
- en: 'The parameters of the Gaussians are updated using these rules:'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用以下规则更新高斯参数：
- en: '![](img/3fdcf390-48e9-44b8-b80f-9455d7a941b0.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3fdcf390-48e9-44b8-b80f-9455d7a941b0.png)'
- en: '![](img/ce430896-d1fb-478b-b706-2312a5e536a7.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ce430896-d1fb-478b-b706-2312a5e536a7.png)'
- en: '![](img/0a188d67-68d1-410f-81d0-b6fb4978651a.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0a188d67-68d1-410f-81d0-b6fb4978651a.png)'
- en: '*N* is the total number of samples. The procedure must be iterated until the
    parameters stop modifying or the modifications are lower than a fixed threshold.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*N*是样本总数。必须迭代此过程，直到参数停止修改或修改小于一个固定的阈值。'
- en: Example of a generative Gaussian mixture
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成高斯混合的示例
- en: 'We can now implement this model in Python using a simple bidimensional dataset,
    created using the `make_blobs()` function provided by Scikit-Learn:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用Scikit-Learn提供的`make_blobs()`函数创建一个简单的二维数据集来实现这个模型：
- en: '[PRE0]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We have created 1,000 samples belonging to 2 classes. 750 points have then
    been randomly selected to become our unlabeled dataset (the corresponding class
    has been set to -1). We can now initialize two Gaussian distributions by defining
    their mean, covariance, and weight. One possibility is to use random values:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了属于2个类别的1,000个样本。然后随机选择了750个点作为我们的未标记数据集（相应的类别被设置为-1）。现在我们可以通过定义它们的均值、协方差和权重来初始化两个高斯分布。一种可能性是使用随机值：
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'However, as the covariance matrices must be positive semi definite, it''s useful
    to alter the random values (by multiplying each matrix by the corresponding transpose)
    or to set hard-coded initial parameters. In this case, we could pick the following
    example:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于协方差矩阵必须是正半定的，因此改变随机值（通过将每个矩阵乘以其相应的转置）或设置硬编码的初始参数是有用的。在这种情况下，我们可以选择以下示例：
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The resulting plot is shown in the following graph, where the small diamonds
    represent the unlabeled points and the bigger dots, the samples belonging to the
    known classes:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了结果图，其中小菱形代表未标记的点，大点代表已知类别的样本：
- en: '![](img/233b53fa-0a97-46fd-b3b4-db83cd146b71.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/233b53fa-0a97-46fd-b3b4-db83cd146b71.png)'
- en: Initial configuration of the Gaussian mixture
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯混合的初始配置
- en: 'The two Gaussians are represented by the concentric ellipses. We can now execute
    the training procedure. For simplicity, we repeat the update for a fixed number
    of iterations. The reader can easily modify the code in order to introduce a threshold:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 两个高斯由同心椭圆表示。现在我们可以执行训练过程。为了简单起见，我们重复更新固定次数的迭代。读者可以轻松修改代码以引入一个阈值：
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The first thing at the beginning of each cycle is to initialize the `Pij` matrix
    that will be used to store the *p(y[i]|x[j]**,θ,w)* values. Then, for each sample,
    we can compute *p(y[i]|x[j],θ,w)* considering whether it''s labeled or not. The
    Gaussian probability is computed using the SciPy function `multivariate_normal.pdf()`.
    When the whole *P[ij]* matrix has been populated, we can update the parameters
    (means and covariance matrix) of both Gaussians and the relative weights. The
    algorithm is very fast; after five iterations, we get the stable state represented
    in the following graph:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 每个循环开始时，首先要初始化将用于存储*p(y[i]|x[j]**,θ,w)*值的`Pij`矩阵。然后，对于每个样本，我们可以计算*p(y[i]|x[j],θ,w)*，考虑它是否被标记。高斯概率是通过SciPy函数`multivariate_normal.pdf()`计算的。当整个*P[ij]*矩阵被填充后，我们可以更新两个高斯的参数（均值和协方差矩阵）以及相关权重。这个算法非常快；经过五次迭代后，我们得到以下图中表示的稳定状态：
- en: '![](img/23d5ef13-ea8a-4b9a-9158-ad546f3c2a11.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/23d5ef13-ea8a-4b9a-9158-ad546f3c2a11.png)'
- en: 'The two Gaussians have perfectly mapped the space by setting their parameters
    so as to cover the high-density regions. We can check for some unlabeled points,
    as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 两个高斯通过设置参数以覆盖高密度区域，完美地映射了空间。我们可以检查一些未标记的点，如下所示：
- en: '[PRE4]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'It''s easy to locate them in the previous plot. The corresponding classes can
    be obtained through the last *P[ij]* matrix:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的图中很容易找到它们。相应的类别可以通过最后一个*P[ij]*矩阵获得：
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This immediately verifies that they have been correctly labeled and assigned
    to the right cluster. This algorithm is very fast and produces excellent results
    in terms of density estimation. In [Chapter 5](8d541a43-8790-4a91-a79b-e48496f75d90.xhtml), *EM
    Algorithm and Applications*, we are going to discuss a general version of this
    algorithm, explaining the complete training procedure based on the EM algorithm.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这立即验证了它们已经被正确标记并分配到正确的簇中。这个算法非常快，在密度估计方面产生了优秀的结果。在[第5章](8d541a43-8790-4a91-a79b-e48496f75d90.xhtml)，*EM算法及其应用*中，我们将讨论这个算法的一般版本，解释基于EM算法的完整训练过程。
- en: In all the examples that involve random numbers, the seed is set equal to 1,000
    (`np.random.seed(1000)`). Other values or subsequent experiments without resetting
    it can yield slightly different results.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有涉及随机数的例子中，种子被设置为1,000（`np.random.seed(1000)`）。其他值或未重置的后续实验可能会产生略微不同的结果。
- en: Weighted log-likelihood
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加权对数似然
- en: 'In the previous example, we have considered a single log-likelihood for both
    labeled and unlabeled samples:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的例子中，我们考虑了标记和未标记样本的单个对数似然：
- en: '![](img/cc579607-466f-438a-85ad-3c92800a5816.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cc579607-466f-438a-85ad-3c92800a5816.png)'
- en: 'This is equivalent to saying that we trust the unlabeled points just like the
    labeled ones. However, in some contexts, this assumption can lead to completely
    wrong estimations, as shown in the following graph:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当于说我们像信任标记点一样信任未标记点。然而，在某些情况下，这个假设可能会导致完全错误的估计，如下图中所示：
- en: '![](img/f4a6098c-6199-4920-8626-3cfa01b2869c.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f4a6098c-6199-4920-8626-3cfa01b2869c.png)'
- en: Biased final Gaussian mixture configuration
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 偏斜的最终高斯混合配置
- en: 'In this case, the means and covariance matrices of both Gaussian distributions
    have been biased by the unlabeled points and the resulting density estimation
    is clearly wrong. When this phenomenon happens, the best thing to do is to consider
    a double weighted log-likelihood. If the first *N* samples are labeled and the
    following *M* are unlabeled, the log-likelihood can be expressed as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，两个高斯分布的均值和协方差矩阵都受到了未标记点的影响，导致的结果密度估计明显错误。当这种现象发生时，最好的做法是考虑双重加权对数似然。如果前*N*个样本是标记的，而接下来的*M*个是未标记的，对数似然可以表示如下：
- en: '![](img/d1ed52c3-6911-4b8b-9934-123e08707c50.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d1ed52c3-6911-4b8b-9934-123e08707c50.png)'
- en: In the previous formula, the term *λ*, if less than 1, can underweight the unlabeled
    terms, giving more importance to the labeled dataset. The modifications to the
    algorithm are trivial because each unlabeled weight has to be scaled according
    to *λ*, reducing its estimated probability. In *Semi-Supervised Learning*, *Chapelle
    O.*,* Schölkopf B.*, *Zien A.*, (*edited by*), *The MIT Press*,the reader can
    find a very detailed discussion about the choice of *λ*. There are no golden rules;
    however, a possible strategy could be based on the cross-validation performed
    on the labeled dataset. Another (more complex) approach is to consider different
    increasing values of *λ* and pick the first one where the log-likelihood is maximum.
    I recommend the aforementioned book for further details and strategies.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，如果 *λ* 小于 1，则可以降低未标记项的权重，使有标签数据集的重要性更高。算法的修改是微不足道的，因为每个未标记的权重都必须根据 *λ*
    进行缩放，从而降低其估计概率。在 *Semi-Supervised Learning*，*Chapelle O.*，*Schölkopf B.*，*Zien
    A.*（编者），*MIT Press* 中，读者可以找到关于 *λ* 选择的一个非常详细的讨论。没有金科玉律；然而，一种可能的策略可能是基于对有标签数据集进行的交叉验证。另一种（更复杂）的方法是考虑不同的
    *λ* 增加值，并选择对数似然最大的第一个值。我建议上述书籍以获取更多细节和策略。
- en: Contrastive pessimistic likelihood estimation
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对比悲观似然估计
- en: As explained at the beginning of this chapter, in many real life problems, it's
    cheaper to retrieve unlabeled samples, rather than correctly labeled ones. For
    this reason, many researchers worked to find out the best strategies to carry
    out a semi-supervised classification that could outperform the supervised counterpart.
    The idea is to train a classifier with a few labeled samples and then improve
    its accuracy after adding weighted unlabeled samples. One of the best results
    is the **Contrastive Pessimistic Likelihood Estimation** (**CPLE**) algorithm,
    proposed by M. Loog (in *Loog M.*,* Contrastive Pessimistic Likelihood Estimation
    for Semi-Supervised Classification*, *arXiv:1503.00269*).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章开头所述，在许多现实生活中的问题中，检索未标记样本比正确标记的样本更便宜。因此，许多研究人员致力于找到执行半监督分类的最佳策略，以超越监督方法。想法是使用少量有标签样本训练一个分类器，然后在添加加权未标记样本后提高其准确性。其中最好的结果是
    M. Loog 提出的 **对比悲观似然估计**（**CPLE**）算法，在 *Loog M.*，*Contrastive Pessimistic Likelihood
    Estimation for Semi-Supervised Classification*，*arXiv:1503.00269* 中提出。
- en: 'Before explaining this algorithm, an introduction is necessary. If we have
    a labeled dataset (*X*, *Y*) containing *N* samples, it''s possible to define
    the log-likelihood cost function of a generic estimator, as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在解释此算法之前，有必要进行介绍。如果我们有一个包含 *N* 个样本的有标签数据集 (*X*, *Y*)，我们可以定义一个通用估计器的对数似然成本函数，如下所示：
- en: '![](img/cb4facc0-a6eb-4f12-b726-6237372c39e1.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cb4facc0-a6eb-4f12-b726-6237372c39e1.png)'
- en: 'After training the model, it should be possible to determine *p(y[i]|x[i],
    θ)*, which is the probability of a label given a sample *x[i]*. However, some
    classifiers are not based on this approach (like SVM) and evaluate the right class,
    for example, by checking the sign of a parametrized function *f(x[i], **θ)*. As
    CPLE is a generic framework that can be used with any classification algorithm
    when the probabilities are not available, it''s useful to implement a technique
    called Platt scaling, which allows transforming the decision function into a probability
    through a parametrized sigmoid. For a binary classifier, it can be expressed as
    follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型之后，应该能够确定 *p(y[i]|x[i], θ)*，这是给定样本 *x[i]* 的标签概率。然而，一些分类器不是基于这种方法（如 SVM）的，而是通过检查参数化函数
    *f(x[i]，**θ**) 的符号来评估正确的类别。由于 CPLE 是一个通用的框架，可以在没有概率的情况下与任何分类算法一起使用，因此实现一种称为 Platt
    缩放的技术是有用的，该技术允许通过参数化 sigmoid 将决策函数转换为概率。对于二元分类器，它可以表示如下：
- en: '![](img/e9c6f607-6508-4207-855d-67cff8e13904.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e9c6f607-6508-4207-855d-67cff8e13904.png)'
- en: '*α* and *β* are parameters that must be learned in order to maximize the likelihood.
    Luckily Scikit-Learn provides the method `predict_proba()`, which returns the
    probabilities for all classes. Platt scaling is performed automatically or on
    demand; for example, the SCV classifier needs to have the parameter `probability=True`
    in order to compute the probability mapping. I always recommend checking the documentation
    before implementing a custom solution.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*α* 和 *β* 是必须学习的参数，以便最大化似然。幸运的是，Scikit-Learn 提供了 `predict_proba()` 方法，该方法返回所有类别的概率。Platt
    缩放是自动执行或在需要时执行；例如，SCV 分类器需要将参数 `probability=True` 设置为计算概率映射。我总是建议在实现自定义解决方案之前检查文档。'
- en: 'We can consider a full dataset, made up of labeled and unlabeled samples. For
    simplicity, we can reorganize the original dataset, so that the first *N* samples
    are labeled, while the next *M* are unlabeled:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以考虑一个由标记和无标记样本组成的完整数据集。为了简化，我们可以重新组织原始数据集，使得前*N*个样本是标记的，而接下来的*M*个是无标记的：
- en: '![](img/852b786f-f2d6-4166-b49f-bfc70fdc98c9.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/852b786f-f2d6-4166-b49f-bfc70fdc98c9.png)'
- en: 'As we don''t know the labels for all *x^u* samples, we can decide to use *M*
    *k*-dimensional (k is the number of classes) soft-labels *q[i]* that can be optimized
    during the training process:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们不知道所有*x^u*样本的标签，我们可以决定在训练过程中使用*M*个*k*-维（k是类别数）软标签*q[i]*，这些标签可以被优化：
- en: '![](img/90fb619a-a877-455d-bf7e-77922cc55eb7.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/90fb619a-a877-455d-bf7e-77922cc55eb7.png)'
- en: 'The second condition in the previous formula is necessary to guarantee that
    each *q[i]* represents a discrete probability (all the elements must sum up to
    1.0). The complete log-likelihood cost function can, therefore, be expressed as
    follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 先前公式中的第二个条件是必要的，以保证每个*q[i]*代表一个离散概率（所有元素必须加起来等于1.0）。因此，完整的对数似然成本函数可以表达如下：
- en: '![](img/3fdf450f-1852-4bfa-8336-19bd92532055.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3fdf450f-1852-4bfa-8336-19bd92532055.png)'
- en: 'The first term represents the log-likelihood for the supervised part, while
    the second one is responsible for the unlabeled points. If we train a classifier
    with only the labeled samples, excluding the second addend, we get a parameter
    set *θ[sup]*. CPLE defines a contrastive condition (as a log-likelihood too),
    by defining the improvement in the total cost function given by the semi-supervised
    approach, compared to the supervised solution:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个项代表监督部分的似然对数，而第二个项负责无标记点。如果我们只使用标记样本训练一个分类器，排除第二个加项，我们得到参数集*θ[sup]*。CPLE定义了一个对比条件（也是一个对数似然），通过定义半监督方法给出的总成本函数的改进，与监督解决方案相比：
- en: '![](img/ea514fd2-d071-4331-885f-2ab7b4334e98.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ea514fd2-d071-4331-885f-2ab7b4334e98.png)'
- en: This condition allows imposing that the semi-supervised solution must outperform
    the supervised one, in fact, maximizing it; we both increase the first term and
    reduce the second one, obtaining a proportional increase of CL (the term *contrastive*
    is very common in machine learning and it normally indicates a condition which
    is achieved as the difference between two opposite constraints). If CL doesn't
    increase, it probably means that the unlabeled samples have not been drawn from
    the marginal distribution *p(x)* extracted from *p[data]*.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这个条件允许强制半监督解决方案必须优于监督解决方案，实际上，最大化它；我们同时增加了第一个项并减少了第二个项，从而实现了CL（对比项在机器学习中非常常见，它通常表示一个条件，该条件是在两个相反约束之间的差异中实现的）。如果CL没有增加，这可能意味着无标记样本没有从从*p[data]*中提取的边缘分布*p(x)*中抽取。
- en: 'Moreover, in the previous expression, we have implicitly used soft-labels,
    but as they are initially randomly chosen and there''s no ground truth to support
    their values, it''s a good idea not to trust them by imposing a pessimistic condition
    (as another log-likelihood):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在先前的表达式中，我们隐式地使用了软标签，但由于它们最初是随机选择的，并且没有支持其值的真实标签，因此，通过施加悲观条件（作为另一个对数似然）不信任它们是一个好主意：
- en: '![](img/3fcae4b3-c343-44cf-85b9-2d5bbb3b128c.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3fcae4b3-c343-44cf-85b9-2d5bbb3b128c.png)'
- en: By imposing this constraint, we try to find the soft-labels that minimize the
    contrastive log-likelihood; that's why this is defined as a pessimistic approach.
    It can seem a contradiction; however, trusting soft-labels can be dangerous, because
    the semi-supervised log-likelihood could be increased even with a large percentage
    of misclassification. Our goal is to find the best parameter set that is able
    to guarantee the highest accuracy starting from the supervised baseline (which
    has been obtained using the labeled samples) and improving it, without forgetting
    the structural features provided by the labeled samples.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 通过施加这个约束，我们试图找到最小化对比对数似然的软标签；这就是为什么这被定义为悲观方法。这似乎是一种矛盾；然而，信任软标签可能是危险的，因为半监督对数似然甚至可以通过大量误分类而增加。我们的目标是找到最佳的参数集，它能够从监督基线（使用标记样本获得）开始，保证最高的准确度，并改进它，同时不忘掉标记样本提供的结构特征。
- en: 'Therefore, our final goal can be expressed as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的最终目标可以表达如下：
- en: '![](img/36e7e7a5-d681-4f86-becb-ca5c6cbffac4.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/36e7e7a5-d681-4f86-becb-ca5c6cbffac4.png)'
- en: Example of contrastive pessimistic likelihood estimation
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对比悲观似然估计的示例
- en: 'We are going to implement the CPLE algorithm in Python using a subset extracted
    from the MNIST dataset. For simplicity, we are going to use only the samples representing
    the digits 0 and 1:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用从 MNIST 数据集提取的子集在 Python 中实现 CPLE 算法。为了简单起见，我们将只使用代表数字 0 和 1 的样本：
- en: '[PRE6]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After creating the restricted dataset (*X*, *Y*) which contain 360 samples,
    we randomly select 150 samples (about 42%) to become unlabeled (the corresponding
    *y* is -1). At this point, we can measure the performance of logistic regression
    trained only on the labeled dataset:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建包含 360 个样本的受限数据集（*X*，*Y*）之后，我们随机选择 150 个样本（大约 42%）成为未标记的样本（相应的 *y* 为 -1）。在这个时候，我们可以测量仅使用标记数据集训练的逻辑回归的性能：
- en: '[PRE7]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'So, the logistic regression shows 57% accuracy for the classification of the
    unlabeled samples. We can also evaluate the cross-validation score on the whole
    dataset (before removing some random labels):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，逻辑回归在未标记样本的分类中显示了 57% 的准确率。我们还可以在整个数据集（在移除一些随机标签之前）上评估交叉验证分数：
- en: '[PRE8]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Thus, the classifier achieves an average 48% accuracy when using 10 folds (each
    test set contains 36 samples) if all the labels are known.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当所有标签都已知时，分类器在 10 折（每个测试集包含 36 个样本）的情况下实现了平均 48% 的准确率。
- en: 'We can now implement a CPLE algorithm. The first thing is to initialize a `LogisticRegression`
    instance and the soft-labels:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以实施一个 CPLE 算法。首先，我们需要初始化一个 `LogisticRegression` 实例和软标签：
- en: '[PRE9]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '*q0* is a random array of values bounded in the half-open interval [0, 1];
    therefore, we also need a converter to transform *q[i]* into an actual binary
    label:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*q0* 是一个在半开区间 [0, 1] 内有界值的随机数组；因此，我们还需要一个转换器将 *q[i]* 转换为实际的二进制标签：'
- en: '![](img/0015cb62-47fa-45fd-9646-7d08d9804844.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0015cb62-47fa-45fd-9646-7d08d9804844.png)'
- en: 'We can achieve this using the NumPy function `np.vectorize()`, which allows
    us to apply a transformation to all the elements of a vector:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 NumPy 函数 `np.vectorize()` 来实现这一点，它允许我们将转换应用于向量的所有元素：
- en: '[PRE10]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In order to compute the log-likelihood, we need also a weighted log-loss (similar
    to the Scikit-Learn function `log_loss()`, which, however, computes the negative
    log-likelihood but doesn''t support weights):'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算对数似然，我们还需要一个加权的对数损失（类似于 Scikit-Learn 函数 `log_loss()`，然而，它计算的是负对数似然，但不支持权重）：
- en: '[PRE11]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This function computes the following expression:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数计算以下表达式：
- en: '![](img/8a0191f5-780c-4f33-a0ce-f5b3ecd791f3.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8a0191f5-780c-4f33-a0ce-f5b3ecd791f3.png)'
- en: 'We need also a function to build the dataset with variable soft-labels *q[i]*:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一个函数来构建具有可变软标签 *q[i]* 的数据集：
- en: '[PRE12]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'At this point, we can define our contrastive log-likelihood:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以定义我们的对比对数似然：
- en: '[PRE13]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This method will be called by the optimizer, passing a different *q* vector
    each time. The first step is building the new dataset and computing `Y_soft`,
    which are the labels corresponding to *q*. Then the logistic regression classifier
    is trained with with the dataset (as `Y_n` is a (k, 1) array, it's necessary to
    squeeze it to avoid a warning. The same thing is done when using *Y* as a boolean
    indicator). At this point, it's possible to compute both *p[sup]* and *p[semi]*
    using the method `predict_proba()` and, finally, we can compute the semi-supervised
    and supervised log-loss, which is the term, a function of *q[i]*, that we want
    to minimize, while the maximization of *θ* is done implicitly when training the
    logistic regression.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法将由优化器调用，每次传递不同的 *q* 向量。第一步是构建新的数据集并计算 `Y_soft`，这是与 *q* 对应的标签。然后，使用数据集（因为
    `Y_n` 是一个 (k, 1) 数组，所以需要将其压缩以避免警告。当使用 *Y* 作为布尔指示器时，也执行相同操作）。在这个时候，可以使用 `predict_proba()`
    方法计算 *p[sup]* 和 *p[semi]*，最后，我们可以计算半监督和监督对数损失，这是我们要最小化的关于 *q[i]* 的项，而 *θ* 的最大化是在训练逻辑回归时隐式完成的。
- en: 'The optimization is carried out using the BFGS algorithm implemented in SciPy:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 优化使用 SciPy 中实现的 BFGS 算法进行：
- en: '[PRE14]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This is a very fast algorithm, but the user is encouraged to experiment with
    methods or libraries. The two parameters we need in this case are `f`, which is
    the function to minimize, and `x0`, which is the initial condition for the independent
    variables. `maxiter` is useful for avoiding an excessive number of iterations
    when no improvements are achieved. Once the optimization is complete, `q_end`
    contains the optimal soft-labels. We can, therefore, rebuild our dataset:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常快的算法，但鼓励用户尝试不同的方法或库。在这种情况下，我们需要两个参数：`f`，这是要最小化的函数，以及`x0`，这是独立变量的初始条件。`maxiter`在未取得改进时避免过多迭代是有用的。一旦优化完成，`q_end`包含最优软标签。因此，我们可以重建我们的数据集：
- en: '[PRE15]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'With this final configuration, we can retrain the logistic regression and check
    the cross-validation accuracy:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个最终配置，我们可以重新训练逻辑回归并检查交叉验证的准确率：
- en: '[PRE16]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The semi-supervised solution based on the CPLE algorithms achieves an average
    84% accuracy, outperforming, as expected, the supervised approach. The reader
    can try other examples using different classifiers, such SVM or Decision Trees,
    and verify when CPLE allows obtaining higher accuracy than other supervised algorithms.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 基于CPLE算法的半监督解决方案实现了平均84%的准确率，正如预期的那样，优于监督方法。读者可以尝试使用不同的分类器，如SVM或决策树，并验证当CPLE允许获得比其他监督算法更高的准确率时。
- en: Semi-supervised Support Vector Machines (S3VM)
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 半监督支持向量机（S3VM）
- en: 'When we discussed the cluster assumption, we also defined the low-density regions
    as boundaries and the corresponding problem as low-density separation. A common
    supervised classifier which is based on this concept is a **Support Vector Machine**
    (**SVM**), the objective of which is to maximize the distance between the dense
    regions where the samples must be. For a complete description of linear and kernel-based
    SVMs, please refer to *Bonaccorso G.*, *Machine Learning Algorithms*, *Packt Publishing*;
    however, it''s useful to remind yourself of the basic model for a linear SVM with
    slack variables *ξ[i]*:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们讨论聚类假设时，我们也定义了低密度区域为边界，相应的问题为低密度分离。一个基于这个概念的常见监督分类器是**支持向量机**（**SVM**），其目标是最大化样本必须位于的密集区域之间的距离。有关基于线性核的SVM的完整描述，请参阅*Bonaccorso
    G.*，*机器学习算法*，*Packt Publishing*；然而，提醒自己线性SVM的基本模型对于带有松弛变量*ξ[i]*是有用的：
- en: '![](img/4ce95bed-96a5-44eb-a7d7-8eefada51472.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4ce95bed-96a5-44eb-a7d7-8eefada51472.png)'
- en: 'This model is based on the assumptions that *y[i]* can be either -1 or 1\.
    The slack variables *ξ[i]* or soft-margins are variables, one for each sample,
    introduced to reduce the *strength* imposed by the original condition (*min ||w||*),
    which is based on a hard margin that misclassifies all the samples that are on
    the wrong side. They are defined by the Hinge loss, as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型基于以下假设：*y[i]*可以是-1或1。松弛变量*ξ[i]*或软边界是变量，每个样本一个，引入以减少原始条件（*min ||w||*）施加的*强度*，该条件基于一个硬边界，它会将所有位于错误一侧的样本误分类。它们由Hinge损失定义如下：
- en: '![](img/7732ffee-8269-4bac-971f-7a4a1b4f1448.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7732ffee-8269-4bac-971f-7a4a1b4f1448.png)'
- en: 'With those variables, we allow some points to overcome the limit without being
    misclassified if they remain within a distance controlled by the corresponding
    slack variable (which is also minimized during the training phase, so as to avoid
    uncontrollable growth). In the following diagram, there''s a schematic representation
    of this process:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些变量，我们允许一些点在保持由相应的松弛变量控制的距离内超越限制，而不会发生误分类。在下图中，这个过程有一个示意图：
- en: '![](img/b5093fc7-34e5-4ab3-8186-76a30ecc15b6.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b5093fc7-34e5-4ab3-8186-76a30ecc15b6.png)'
- en: SVM generic scenario
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: SVM通用场景
- en: 'The last elements of each high-density regions are the support vectors. Between
    them, there''s a low-density region (it can also be zero-density in some cases)
    where our separating hyperplane lies. In [Chapter 1](acff0775-f21c-4b6d-8ef2-c78713e21364.xhtml), *Machine
    Learning Model Fundamentals*, we defined the concept of *empirical risk* as a
    proxy for expected risk; therefore, we can turn the SVM problem into the minimization
    of empirical risk under the Hinge cost function (with or without Ridge Regularization
    on w):'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 每个高密度区域的最后元素是支持向量。它们之间有一个低密度区域（在某些情况下也可能是零密度区域），我们的分离超平面就位于其中。在[第1章](acff0775-f21c-4b6d-8ef2-c78713e21364.xhtml)，*机器学习模型基础*中，我们定义了*经验风险*作为预期风险的代理；因此，我们可以将SVM问题转化为在Hinge成本函数（带或不带w上的Ridge正则化）下的经验风险最小化：
- en: '![](img/fe754389-2954-4aeb-be7d-8a4105339c56.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fe754389-2954-4aeb-be7d-8a4105339c56.png)'
- en: Theoretically, every function which is always bounded by two hyperplanes containing
    the support vectors is a good classifier, but we need to minimize the empirical
    risk (and, so, the expected risk); therefore we look for the maximum margin between
    high-density regions. This model is able to separate two dense regions with irregular
    boundaries and, by adopting a kernel function, also in non-linear scenarios. The
    natural question, at this point, is about the best strategy to integrate labeled
    and unlabeled samples when we need to solve this kind of problem in a semi-supervised
    scenario.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，每个总是被包含支持向量的两个超平面所包含的函数都是一个好的分类器，但我们需要最小化经验风险（因此，期望风险）；因此，我们寻找高密度区域之间的最大间隔。这个模型能够分离具有不规则边界的两个密集区域，通过采用核函数，也可以在非线性场景中分离。此时，自然的问题是我们需要解决这种问题的半监督场景中，如何将标记样本和未标记样本的最佳策略整合在一起。
- en: 'The first element to consider is the ratio: if we have a low percentage of
    unlabeled points, the problem is mainly supervised and the generalization ability
    learned using the training set should be enough to correctly classify all the
    unlabeled points. On the other hand, if the number of unlabeled samples is much
    larger, we return to an almost pure clustering scenario (like the one discussed
    in the paragraph about the Generative Gaussian mixtures). In order to exploit
    the strength of semi-supervised methods in low-density separation problems, therefore,
    we should consider situations where the ratio labeled/unlabeled is about 1.0\.
    However, even if we have the predominance of a class (for example, if we have
    a huge unlabeled dataset and only a few labeled samples), it''s always possible
    to use the algorithms we''re going to discuss, even if, sometimes, their performance
    could be equal to or lower than a pure supervised/clustering solution. Transductive
    SMVs, for example, showed better accuracies when the labeled/unlabeled ratio is
    very small, while other methods can behave in a completely different way. However,
    when working with semi-supervised learning (and its assumptions), it is always
    important to bear in mind that each problem is supervised and unsupervised at
    the same time and the best solution must be evaluated in every different context.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 首先需要考虑的是比例：如果我们有很少的未标记点，问题主要是监督性的，使用训练集学习到的泛化能力应该足够正确分类所有未标记点。另一方面，如果未标记样本的数量大得多，我们几乎回到了纯聚类场景（就像在关于生成高斯混合的段落中讨论的那样）。因此，为了利用半监督方法在低密度分离问题中的优势，我们应该考虑标记/未标记比例大约为1.0的情况。然而，即使我们有一个类别的优势（例如，如果我们有一个巨大的未标记数据集和很少的标记样本），我们总是可以使用我们即将讨论的算法，即使有时它们的性能可能等于或低于纯监督/聚类解决方案。例如，当标记/未标记比例非常小的时候，Transductive
    SMVs（传递SMVs）显示了更好的准确率，而其他方法可能会表现出完全不同的行为。然而，当处理半监督学习（及其假设）时，始终要记住每个问题既是监督性的又是无监督性的，并且最佳解决方案必须在每个不同的环境中进行评估。
- en: 'A solution for this problem is offered by the *Semi-Supervised SVM* (also known
    as *S³VM*) algorithm. If we have *N* labeled samples and *M* unlabeled samples,
    the objective function becomes as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个问题，*半监督SVM*（也称为*S³VM*）算法提供了一个解决方案。如果我们有*N*个标记样本和*M*个未标记样本，目标函数如下：
- en: '![](img/8914f477-cc93-413c-a2af-cc0946bbe32c.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8914f477-cc93-413c-a2af-cc0946bbe32c.png)'
- en: 'The first term imposes the standard SVM condition about the maximum separation
    distance, while the second block is divided into two parts:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个项强加了标准SVM关于最大分离距离的条件，而第二个块被分为两部分：
- en: We need to add *N* slack variables *η[i]* to guarantee a soft-margin for the
    labeled samples.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要添加*N*个松弛变量η[i]来保证标记样本的软边界。
- en: At the same time, we have to consider the unlabeled points, which could be classified
    as +1 or -1\. Therefore, we have two corresponding slack-variable sets *ξ[i]* and
    *z[i]*. However, we want to find the smallest variable for each possible pair,
    so as to be sure that the unlabeled sample is placed on the sub-space where the
    maximum accuracy is achieved.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同时，我们必须考虑那些可能被分类为+1或-1的未标记点。因此，我们有两个相应的松弛变量集ξ[i]和z[i]。然而，我们想要找到每个可能对的最小变量，以确保未标记样本被放置在实现最大准确率的子空间中。
- en: 'The constraints necessary to solve the problems become as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 解决问题的必要约束如下：
- en: '![](img/00e27ea5-de91-4860-8d55-45cbd0e09fda.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00e27ea5-de91-4860-8d55-45cbd0e09fda.png)'
- en: The first constraint is limited to the labeled points and it's the same as a
    supervised SVM. The following two, instead, take into account the possibility
    that an unlabeled sample could be classified as +1 or -1\. Let's suppose, for
    example, that the label *y[j]* for the sample *x[j]* should be +1 and the first
    member of the second inequality is a positive number *K* (so the corresponding
    term of the third equation is *-K*). It's easy to verify that the first slack
    variable is *ξ[i] ≥ 1 - K,* while the second one is *z[j] ≥ 1 + K*.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个约束仅限于标记点，它与监督SVM相同。接下来的两个约束，相反，考虑了无标记样本可能被分类为+1或-1的可能性。例如，假设样本*x[j]*的标签*y[j]*应该是+1，第二个不等式的第一个成员是一个正数*K*（因此第三个方程的对应项是*-K*）。很容易验证第一个松弛变量是*ξ[i]
    ≥ 1 - K*，而第二个是*z[j] ≥ 1 + K*。
- en: Therefore, in the objective, *ξ*[*i* ]is chosen to be minimized. This method
    is inductive and yields good (if not excellent) performances; however, it has
    a very high computational cost and should be solved using optimized (native) libraries.
    Unfortunately, it is a non-convex problem and there are no standard methods to
    solve it so it always reaches the optimal configuration.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在目标函数中，*ξ[i]*被选为最小化。这种方法是归纳的，并产生良好的（如果不是极好的）性能；然而，它具有非常高的计算成本，应该使用优化（本地）库来解决。不幸的是，它是一个非凸问题，没有标准方法来解决它，因此它总是达到最优配置。
- en: Example of S3VM
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: S3VM示例
- en: We now implement an S³VM in Python using the SciPy optimization methods, which
    are mainly based on C and FORTRAN implementations. The reader can try it with
    other libraries such as NLOpt and LIBSVM and compare the results. A possibility
    suggested by Bennet and Demiriz is to use the L1-norm for w, so as to linearize
    the objective function; however, this choice seems to produce good results only
    for small datasets. We are going to keep the original formulation based on the
    L2-norm, using an **Sequential Least Squares Programming** (**SLSQP**) algorithm
    to optimize the objective.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用SciPy优化方法在Python中实现S³VM，这些方法主要基于C和FORTRAN实现。读者可以尝试使用其他库，如NLOpt和LIBSVM，并比较结果。Bennet和Demiriz提出的一种可能性是使用w的L1范数，以便线性化目标函数；然而，这种选择似乎只对小型数据集产生良好的结果。我们将保持基于L2范数的原始公式，使用**顺序最小二乘规划**（**SLSQP**）算法来优化目标。
- en: 'Let''s start by creating a bidimensional dataset with both labeled and unlabeled
    samples:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建一个包含标记和无标记样本的双维数据集：
- en: '[PRE17]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'For simplicity (and without any impact, because the samples are shuffled),
    we set last 200 samples as unlabeled (*y = 0*). The corresponding plot is shown
    in the following graph:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见（并且没有任何影响，因为样本是打乱的），我们将最后200个样本设置为无标记（*y = 0*）。相应的图示如下所示：
- en: '![](img/866c542c-e327-4590-a18f-2864b229a397.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/866c542c-e327-4590-a18f-2864b229a397.png)'
- en: Original labeled and unlabeled dataset
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 原始标记和无标记数据集
- en: 'The crosses represent unlabeled points, which are spread throughout the entire
    dataset. At this point we need to initialize all variables required for the optimization
    problem:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 十字代表无标记点，它们遍布整个数据集。在此阶段，我们需要初始化优化问题所需的所有变量：
- en: '[PRE18]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'As the optimization algorithm requires a single array, we have stacked all
    vectors into a horizontal array `theta0` using the `np.hstack()` function. We
    also need to vectorize the `min()` function in order to apply it to arrays:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 由于优化算法需要一个单一的数组，我们使用`np.hstack()`函数将所有向量堆叠成一个水平数组`theta0`。我们还需要将`min()`函数向量化，以便将其应用于数组：
- en: '[PRE19]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, we can define the objective function:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以定义目标函数：
- en: '[PRE20]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The arguments are the current `theta` vector and the complete datasets `Xd`
    and `Yd`. The dot product of *w* has been multiplied by 0.5 to keep the conventional
    notation used for supervised SVMs. The constant can be omitted without any impact.
    At this point, we need to define all the constraints, as they are based on the
    slack variables; each function (which shares the same parameters of the objectives)
    is parametrized with an index, `idx`. The labeled constraint is as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 参数是当前的`theta`向量以及完整的数据集`Xd`和`Yd`。*w*的点积乘以0.5以保持用于监督SVM的传统符号。常数可以省略，没有任何影响。在此阶段，我们需要定义所有基于松弛变量的约束；每个函数（与目标函数共享相同的参数）用一个索引`idx`参数化。标记约束如下：
- en: '[PRE21]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The unlabeled constraints, instead, are as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 无标记约束如下：
- en: '[PRE22]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'They are parametrized with the current `theta` vector, the `Xd` dataset, and
    an `idx` index. We need also to include the constraints for each slack variable
    (*≥ 0*):'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 它们由当前的 `theta` 向量、`Xd` 数据集和 `idx` 索引参数化。我们还需要包括每个松弛变量的约束（*≥ 0*）：
- en: '[PRE23]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can now set up the problem using the SciPy convention:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用 SciPy 习惯设置问题：
- en: '[PRE24]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Each constraint is represented with a dictionary, where `type` is set to `ineq`
    to indicate that it is an inequality, `fun` points to the callable object and
    `args` contains all extra arguments (`theta` is the main x variable and it''s
    automatically added). Using SciPy, it''s possible to minimize the objective using
    the **Sequential Least Squares Programming** (**SLSQP**) or **Constraint Optimization
    by Linear Approximation** (**COBYLA**) algorithms. We preferred the former, because
    it works more rapidly and is more stable:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 每个约束都由一个字典表示，其中 `type` 设置为 `ineq` 以指示它是不等式，`fun` 指向可调用对象，而 `args` 包含所有额外参数（`theta`
    是主要的 x 变量，它被自动添加）。使用 SciPy，可以使用 **Sequential Least Squares Programming**（**SLSQP**）或
    **Constraint Optimization by Linear Approximation**（**COBYLA**）算法最小化目标。我们更喜欢前者，因为它运行得更快且更稳定：
- en: '[PRE25]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'After the training process is complete, we can compute the labels for the unlabeled
    points:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程完成后，我们可以计算未标记点的标签：
- en: '[PRE26]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In the next graph, it''s possible to compare the initial plot (left) with the
    final one where all points have been assigned a label (right):'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个图中，可以比较初始图（左侧）和最终图（右侧），其中所有点都已分配了标签：
- en: '![](img/a4b85e3e-4a9c-4209-98d0-8180b29d82df.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a4b85e3e-4a9c-4209-98d0-8180b29d82df.png)'
- en: As you can see, S³VM succeeded in finding the right label for all unlabeled
    points, confirming the existence of two very dense regions for *x* between *[0,
    2]* (square dots) and *y* between *[0, 2]* (circular dots).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，S³VM 成功地为所有未标记点找到了正确的标签，确认了在 *x* 在 *[0, 2]*（正方形点）和 *y* 在 *[0, 2]*（圆形点）之间存在两个非常密集的区域。
- en: NLOpt is a complete optimization library developed at MIT. It is available for
    different operating systems and programming languages. The website is [https://nlopt.readthedocs.io](https://nlopt.readthedocs.io).
    LIBSVM is an optimized library for solving SVM problems and it is adopted by Scikit-Learn
    together with LIBLINEAR. It's also available for different environments. The homepage
    is [https://www.csie.ntu.edu.tw/~cjlin/libsvm/.](https://www.csie.ntu.edu.tw/~cjlin/libsvm/)
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: NLOpt 是麻省理工学院开发的一个完整的优化库。它适用于不同的操作系统和编程语言。网站是 [https://nlopt.readthedocs.io](https://nlopt.readthedocs.io)。LIBSVM
    是一个用于解决 SVM 问题的优化库，它被 Scikit-Learn 和 LIBLINEAR 一起采用。它也适用于不同的环境。主页是 [https://www.csie.ntu.edu.tw/~cjlin/libsvm/](https://www.csie.ntu.edu.tw/~cjlin/libsvm/)。
- en: Transductive Support Vector Machines (TSVM)
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 归纳支持向量机（TSVM）
- en: 'Another approach to the same problem is offered by the TSVM, proposed by T.
    Joachims (in *Transductive Inference for Text Classification using Support Vector
    Machines*,*Joachims T.*, *ICML Vol. 99/1999*). The idea is to keep the original
    objective with two sets of slack variables: the first for the labeled samples
    and the other for the unlabeled ones:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种解决同一问题的方法是 T. Joachims 提出的 TSVM（在 *Transductive Inference for Text Classification
    using Support Vector Machines*，Joachims T.，*ICML Vol. 99/1999*）。其想法是保持原始目标，并使用两组松弛变量：一组用于标记样本，另一组用于未标记样本：
- en: '![](img/e896ce54-dc1b-4a81-9200-df1c2153320e.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e896ce54-dc1b-4a81-9200-df1c2153320e.png)'
- en: 'As this is a transductive approach, we need to consider the unlabeled samples
    as variable-labeled ones (subject to the learning process), imposing a constraint
    similar to the supervised points. As for the previous algorithm, we assume we
    have *N* labeled samples and *M* unlabeled ones; therefore, the conditions become
    as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个归纳方法，我们需要将未标记的样本视为可变标记样本（受学习过程影响），施加与监督点类似的约束。至于之前的算法，我们假设我们拥有 *N* 个标记样本和
    *M* 个未标记样本；因此，条件如下：
- en: '![](img/e0adc106-9f38-4c21-8851-236dcdd6ef59.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e0adc106-9f38-4c21-8851-236dcdd6ef59.png)'
- en: The first constraint is the classical SVM one and it works only on labeled samples.
    The second one uses the variable *y^((u))[j]* with the corresponding slack variables *ξ[j]*
    to impose a similar condition on the unlabeled samples, while the third one is
    necessary to constrain the labels to being equal to -1 and 1.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个约束是经典的 SVM 约束，它仅适用于标记样本。第二个约束使用变量 *y^((u))[j]* 和相应的松弛变量 *ξ[j]* 对未标记样本施加类似条件，而第三个约束是必要的，以确保标签等于
    -1 和 1。
- en: Just like the semi-supervised SVMs, this algorithm is non-convex and it's useful
    to try different methods to optimize it. Moreover, the author, in the aforementioned
    paper, showed how TSVM works better when the test set (unlabeled) is large and
    the training set (labeled) is relatively small (when a standard supervised SVM
    is outperformed). On the other hand, with large training sets and small test sets,
    a supervised SVM (or other algorithms) are always preferable because they are
    faster and yield better accuracy.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 就像半监督 SVM 一样，此算法是非凸的，尝试不同的方法来优化它是很有用的。此外，作者在上述论文中展示了当测试集（未标记）很大而训练集（标记）相对较小时，TSVM
    如何表现更好（当标准监督 SVM 表现不佳时）。另一方面，当训练集很大而测试集较小时，监督 SVM（或其他算法）总是更可取，因为它们更快，且准确性更高。
- en: Example of TSVM
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TSVM 示例
- en: 'In our Python implementation, we are going to use a bidimensional dataset similar
    to one employed in the previous method; however, in this case, we impose 400 unlabeled
    samples out of a total of 500 points:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 Python 实现中，我们将使用与之前方法中使用的类似的双维数据集；然而，在这种情况下，我们在总共 500 个点中强制 400 个未标记样本：
- en: '[PRE27]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The corresponding plot is shown in the following graph:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 相应的图表如下所示：
- en: '![](img/c9abc37a-205a-4a81-b90e-124f77f97c26.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c9abc37a-205a-4a81-b90e-124f77f97c26.png)'
- en: Original labeled and unlabeled dataset
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 原始标记和未标记数据集
- en: 'The procedure is similar to the one we used before. First of all, we need to
    initialize our variables:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 程序与之前使用的方法类似。首先，我们需要初始化我们的变量：
- en: '[PRE28]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In this case, we also need to define the `y_unlabeled` vector for variable-labels.
    The author also suggests using two C constants (`C_labeled` and `C_unlabeled`)
    in order to be able to weight the misclassification of labeled and unlabeled samples
    differently. We used a value of 1.0 for `C_labeled` and 10.0 for `C_unlabled`,
    because we want to penalize more the misclassification of unlabeled samples.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们还需要定义变量标签的 `y_unlabeled` 向量。作者还建议使用两个 C 常数（`C_labeled` 和 `C_unlabeled`），以便能够不同地加权标记和未标记样本的错误分类。我们为
    `C_labeled` 使用了 1.0 的值，为 `C_unlabled` 使用了 10.0 的值，因为我们想对未标记样本的错误分类进行更多的惩罚。
- en: 'The objective function to optimize is as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 要优化的目标函数如下：
- en: '[PRE29]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'While the labeled and unlabeled constraints are as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 标记和未标记的约束如下：
- en: '[PRE30]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We need also to impose the constraints on the slack variables and on the *y^((u))*:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要对松弛变量和 *y^((u)* 进行约束：
- en: '[PRE31]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'As in the previous example, we can create the constraint dictionary needed
    by SciPy:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一个示例所示，我们可以创建 SciPy 需要的约束字典：
- en: '[PRE32]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In this case, the last constraint is an equality, because we want to force *y^((u))*
    to be equal either to -1 or 1\. At this point, we minimize the objective function:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，最后一个约束是一个等式，因为我们想强制 *y^((u)* 要么等于 -1，要么等于 1。在这个点上，我们最小化目标函数：
- en: '[PRE33]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'When the process is complete, we can compute the labels for the unlabeled samples
    and compare the plots:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 当过程完成后，我们可以计算未标记样本的标签并比较图表：
- en: '[PRE34]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The plot comparison is shown in the following graph:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图表比较如下所示：
- en: '![](img/00c61ef5-5da6-4107-8c9a-36a2ec5a0115.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00c61ef5-5da6-4107-8c9a-36a2ec5a0115.png)'
- en: Original dataset (left). Final labeled dataset (right)
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据集（左）。最终标记数据集（右）
- en: The misclassification (based on the density distribution) is slightly higher
    than S³VM, but it's possible to change the C values and the optimization method
    until the expected result has been reached. A good benchmark is provided by a
    supervised SVM, which can have better performances when the training set is huge
    enough (and when it represents the whole *p[data]* correctly).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '基于密度分布的错误分类略高于 S³VM，但可以通过改变 C 值和优化方法来调整，直到达到预期的结果。当训练集足够大（并且正确代表整个 *p[data]*）时，监督
    SVM 可以提供良好的基准，其性能可能更好。 '
- en: 'It''s interesting to evaluate different combinations of the C parameters, starting
    from a standard supervised SVM. The dataset is smaller, with a high number of
    unlabeled samples:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 评估不同的 C 参数组合很有趣，从标准的监督 SVM 开始。数据集较小，有大量的未标记样本：
- en: '[PRE35]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We use the standard SVM implementation provided by Scikit-Learn (the `SVC()`
    class) with a linear kernel and `C=1.0`:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Scikit-Learn 提供的标准 SVM 实现（`SVC()` 类），使用线性核和 `C=1.0`：
- en: '[PRE36]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The SVM is trained with the labeled samples and the vector `yu_svc` contains
    the prediction for the unlabeled samples. The resulting plot (in comparison with
    the original dataset) is shown in the following graph:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: SVM 使用带标签的样本进行训练，向量 `yu_svc` 包含对未标记样本的预测。与原始数据集相比的结果图如下所示：
- en: '![](img/5d4839c3-9687-4f64-a00f-46f205834f25.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5d4839c3-9687-4f64-a00f-46f205834f25.png)'
- en: Original dataset (left). Final labeled dataset (right) with C = 1.0
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据集（左）。最终标记数据集（右）C = 1.0
- en: 'All the labeled samples are represented with bigger squares and circles. The
    result meets our expectations, but there''s an area *(X [-1, 0] - Y [-2, -1])*,
    where the SVM decided to impose the *circle* class even if the unlabeled points
    are close to a square. This hypothesis can''t be acceptable considering the clustering
    assumption; in fact, in a high-density region there are samples belonging to two
    classes. A similar (or even worse) result is obtained using an S³VM with **CL=10**
    and **CU=5**:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 所有标记样本都用较大的正方形和圆形表示。结果符合我们的预期，但有一个区域 *(X [-1, 0] - Y [-2, -1])*，其中SVM决定将*圆形*类强加，即使未标记的点靠近正方形。考虑到聚类假设，这种假设是不可接受的；事实上，在高密度区域中存在属于两个类别的样本。使用**CL=10**和**CU=5**的S³VM得到的结果（或更差）是类似的：
- en: '![](img/a83b787d-d1ef-4344-8453-fda96b498443.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a83b787d-d1ef-4344-8453-fda96b498443.png)'
- en: Original dataset (left). Final labeled dataset (right) with C[L] = 10 and C[U]
    = 5
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据集（左）。最终标记数据集（右）C[L] = 10 和 C[U] = 5
- en: 'In this case, the classification accuracy is lower because the penalty for
    the unlabeled samples is lower than the one imposed on the labeled points. A supervised
    SVM has obviously better performances. Let''s try with **C****[L]****=10** and
    **C****[U]****=50**:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，分类准确度较低，因为未标记样本的惩罚低于对标记点的惩罚。显然，监督SVM有更好的性能。让我们尝试**C[L]=10**和**C[U]=50**：
- en: '![](img/e36c7b7e-208f-4ca5-af4f-e6b97b134456.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e36c7b7e-208f-4ca5-af4f-e6b97b134456.png)'
- en: Original dataset (left). Final labeled dataset (right) with C[L] = 10 and C[U] =
    50
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据集（左）。最终标记数据集（右）C[L] = 10 和 C[U] = 50
- en: Now, the penalty is quite a lot higher for the unlabeled samples and the result
    appears much more reasonable considering the clustering assumption. All the high-density
    regions are coherent and separated by low-density ones. These examples show how
    the value chosen for the parameters and the optimization method can dramatically
    change the result. My suggestion is to test several configurations (on sub-sampled
    datasets), before picking the final one. In *Semi-Supervised Learning*, *Chapelle
    O.*,* Schölkopf B.*, *Zien A.*, (*edited by*), *The MIT Press,* there are further
    details about possible optimization strategies, with strengths and weaknesses.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于未标记样本的惩罚相当高，考虑到聚类假设，结果看起来更加合理。所有高密度区域都是连贯的，并且由低密度区域分隔。这些例子展示了参数选择和优化方法如何能显著改变结果。我的建议是在选择最终配置（在子样本数据集上）之前测试几种配置。在《*半监督学习*》一书中，作者*Chapelle
    O.*、*Schölkopf B.*、*Zien A.*（编者），*麻省理工学院出版社*，有关于可能的优化策略的更多细节，包括其优缺点。
- en: Summary
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced semi-supervised learning, starting from the scenario
    and the assumptions needed to justify the approaches. We discussed the importance
    of the smoothness assumption when working with both supervised and semi-supervised
    classifiers in order to guarantee a reasonable generalization ability. Then we
    introduced the clustering assumption, which is strictly related to the geometry
    of the datasets and allows coping with density estimation problems with a strong
    structural condition. Finally, we discussed the manifold assumption and its importance
    in order to avoid the curse of dimensionality.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们从场景和所需的假设开始介绍半监督学习，以证明这些方法的有效性。我们讨论了平滑假设在处理监督和半监督分类器时的重要性，以确保合理的泛化能力。然后我们介绍了聚类假设，它与数据集的几何形状密切相关，并允许在具有强烈结构条件的密度估计问题中应对。最后，我们讨论了流形假设及其在避免维度灾难中的重要性。
- en: 'The chapter continued by introducing a generative and inductive model: Generative
    Gaussian mixtures, which allow clustering labeled and unlabeled samples starting
    from the assumption that the prior probabilities are modeled by multivariate Gaussian
    distributions.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 本章继续介绍了一个生成和归纳模型：生成高斯混合模型，它允许从先验概率由多元高斯分布建模的假设出发，对标记和未标记样本进行聚类。
- en: 'The next topic was about a very important algorithm: contrastive pessimistic
    likelihood estimation, which is an inductive, semi-supervised classification framework
    that can be adopted together with any supervised classifier. The main concept
    is to define a contrastive log-likelihood based on soft-labels (representing the
    probabilities for the unlabeled samples) and impose a pessimistic condition in
    order to minimize the trust in the soft-labels. The algorithm can find the best
    configuration that maximizes the log-likelihood, taking into account both labeled
    and unlabeled samples.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的主题是关于一个非常重要的算法：对比悲观似然估计，这是一个归纳、半监督分类框架，可以与任何监督分类器一起采用。主要概念是基于软标签（代表未标记样本的概率）定义对比对数似然，并施加悲观条件以最小化对软标签的信任。该算法可以找到最佳配置，最大化对数似然，同时考虑标记和未标记样本。
- en: Another inductive classification approach is provided by the S³VM algorithm,
    which is an extension of the classical SVM approach, based on two extra optimization
    constraints to address the unlabeled samples. This method is relatively powerful,
    but it's non-convex and, therefore, very sensitive to the algorithms employed
    to minimize the objective function.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种归纳分类方法是S³VM算法，它是经典SVM方法的扩展，基于两个额外的优化约束来解决未标记样本。这种方法相对强大，但它是非凸的，因此对用于最小化目标函数的算法非常敏感。
- en: 'An alternative to S³VM is provided by the TSVM, which tries to minimize the
    objective with a condition based on variable labels. The problem is, hence, divided
    into two parts: the supervised one, which is exactly the same as standard SVM,
    and the semi-supervised one, which has a similar structure but without fixed *y*
    labels. This problem is non-convex too and it''s necessary to evaluate different
    optimization strategies to find the best trade-off between accuracy and computational
    complexity. In the reference section, there are some useful resources so you can
    examine all these problems in depth and find a suitable solution for each particular
    scenario.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: S³VM的替代方案是TSVM，它试图通过基于可变标签的条件最小化目标。因此，问题被分为两部分：监督部分，与标准SVM完全相同；半监督部分，结构相似但没有固定的*y*标签。这个问题也是非凸的，因此有必要评估不同的优化策略，以找到准确性和计算复杂度之间的最佳权衡。在参考文献部分，有一些有用的资源，您可以深入探讨所有这些问题，并为每个特定场景找到合适的解决方案。
- en: In the next chapter, [Chapter 3](c23d1792-167f-416e-a848-fa7a10777697.xhtml), *Graph-Based
    Semi-Supervised Learning* we're continuing this exploration by discussing some
    important algorithms based on the structure underlying the dataset. In particular,
    we're going to employ graph theory to perform the propagation of labels to unlabeled
    samples and to reduce the dimensionality of datasets in non-linear contexts.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，[第3章](c23d1792-167f-416e-a848-fa7a10777697.xhtml)，*基于图的结构半监督学习*中，我们继续这一探索，通过讨论一些基于数据集底层结构的重要算法。特别是，我们将运用图论来执行标签向未标记样本的传播，以及在非线性环境中降低数据集的维度。
