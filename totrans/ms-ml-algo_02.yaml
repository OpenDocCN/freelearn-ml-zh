- en: Introduction to Semi-Supervised Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Semi-supervised learning is a machine learning branch that tries to solve problems
    with both labeled and unlabeled data with an approach that employs concepts belonging
    to clustering and classification methods. The high availability of unlabeled samples,
    in contrast with the difficulty of labeling huge datasets correctly, drove many
    researchers to investigate the best approaches that allow extending the knowledge
    provided by the labeled samples to a larger unlabeled population without loss
    of accuracy. In this chapter, we''re going to introduce this branch and, in particular,
    we will discuss:'
  prefs: []
  type: TYPE_NORMAL
- en: The semi-supervised scenario
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The assumptions needed to efficiently operate in such a scenario
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The different approaches to semi-supervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative Gaussian mixtures algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contrastive pessimistic likelihood estimation approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semi-supervised Support Vector Machines** (**S³VM**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transductive Support Vector Machines** (**TSVM**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semi-supervised scenario
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A typical semi-supervised scenario is not very different from a supervised
    one. Let''s suppose we have a data generating process, *p[data]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/163ed14d-061e-48f3-89d5-693b530d0525.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, contrary to a supervised approach, we have only a limited number *N*
    of samples drawn from *p[data]* and provided with a label, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/933c6eb6-6b35-4f83-b713-adaabda6ce98.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Instead, we have a larger amount (*M*) of unlabeled samples drawn from the
    marginal distribution *p(x)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/882d2b8f-70b6-4891-92b6-6cca8e919657.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In general, there are no restrictions on the values of *N* and *M;* however,
    a semi-supervised problem arises when the number of unlabeled samples is much
    higher than the number of complete samples. If we can draw *N* >> *M* labeled
    samples from *p[data]*, it''s probably useless to keep on working with semi-supervised
    approaches and preferring classical supervised methods is likely to be the best
    choice. The extra complexity we need is justified by *M* >> *N*, which is a common
    condition in all those situations where the amount of available unlabeled data
    is large, while the number of correctly labeled samples is quite a lot lower.
    For example, we can easily access millions of free images but detailed labeled
    datasets are expensive and include only a limited subset of possibilities. However,
    is it always possible to apply semi-supervised learning to improve our models?
    The answer to this question is almost obvious: unfortunately no. As a rule of
    thumb, we can say that if the knowledge of *X[u]* increases our knowledge about
    the prior distribution *p(x)*, a semi-supervised algorithm is likely to perform
    better than a purely supervised (and thus limited to *X[l]*) counterpart. On the
    other hand, if the unlabeled samples are drawn from different distributions, the
    final result can be quite a lot worse. In real cases, it''s not so immediately
    necessary to decide whether a semi-supervised algorithm is the best choice; therefore,
    cross-validation and comparisons are the best practices to employ when evaluating
    a scenario.'
  prefs: []
  type: TYPE_NORMAL
- en: Transductive learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When a semi-supervised model is aimed at finding the labels for the unlabeled
    samples, the approach is called transductive learning. In this case, we are not
    interested in modeling the whole distribution *p(x|y)*, which implies determining
    the density of both datasets, but rather in finding *p(y|x)* only for the unlabeled
    points. In many cases, this strategy can be time-saving and it's always preferable
    when our goal is more oriented at improving our knowledge about the unlabeled
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Inductive learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Contrary to transductive learning, inductive learningconsiders all the *X* samples
    and tries to determine a complete *p(x|y)* or a function *y=f(x)* that can map
    both labeled and unlabeled points to their corresponding labels. In general, this
    method is more complex and requires more computational time; therefore, according
    to *Vapnik's principle*, if not required or necessary, it's always better to pick
    the most pragmatic solution and, possibly, expand it if the problem requires further
    details.
  prefs: []
  type: TYPE_NORMAL
- en: Semi-supervised assumptions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As explained in the previous section, semi-supervised learning is not guaranteed
    to improve a supervised model. A wrong choice could lead to a dramatic worsening
    in performance; however, it's possible to state some fundamental assumptions which
    are required for semi-supervised learning to work properly. They are not always
    mathematically proven theorems, but rather empirical observations that justify
    the choice of an approach otherwise completely arbitrary.
  prefs: []
  type: TYPE_NORMAL
- en: Smoothness assumption
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s consider a real-valued function *f(x)* and the corresponding metric
    spaces *X* and *Y*. Such a function is said to be Lipschitz-continuous if:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5a428b7c-fac1-4bac-acc0-b0c195912935.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In other words, if two points *x[1]* and *x[2]* are near, the corresponding
    output values *y[1]* and *y[2]* cannot be arbitrarily far from each other. This
    condition is fundamental in regression problems where a generalization is often
    required for points that are between training samples. For example, if we need
    to predict the output for a point *x[t]* : *x*[*1* ]< *x[t]* < *x[2]* and the
    regressor is Lipschitz-continuous, we can be sure that *y[t]* will be correctly
    bounded by *y[1]* and *y[2]*. This condition is often called general smoothness,
    but in semi-supervised it''s useful to add a restriction (correlated with the
    cluster assumption): if two points are in a high density region (cluster) and
    they are close, then the corresponding outputs must be close too. This extra condition
    is very important because, if two samples are in a low density region they can
    belong to different clusters and their labels can be very different. This is not
    always true, but it''s useful to include this constraint to allow some further
    assumptions in many definitions of semi-supervised models.'
  prefs: []
  type: TYPE_NORMAL
- en: Cluster assumption
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This assumption is strictly linked to the previous one and it''s probably easier
    to accept. It can be expressed with a chain of interdependent conditions. Clusters
    are high density regions; therefore, if two points are close, they are likely
    to belong to the same cluster and their labels must be the same. Low density regions
    are separation spaces; therefore, samples belonging to a low density region are
    likely to be boundary points and their classes can be different. To better understand
    this concept, it''s useful to think about supervised SVM: only the support vectors
    should be in low density regions. Let''s consider the following bidimensional
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4f8fbd46-2ca9-4bd9-9266-13e3125c1913.png)'
  prefs: []
  type: TYPE_IMG
- en: In a semi-supervised scenario, we couldn't know the label of a point belonging
    to a high density region; however, if it is close enough to a labeled point that
    it's possible to build a ball where all the points have the same average density,
    we are allowed to predict the label of our test sample. Instead, if we move to
    a low-density region, the process becomes harder, because two points can be very
    close but with different labels. We are going to discuss the semi-supervised,
    low-density separation problem at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Manifold assumption
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is the less intuitive assumption, but it can be extremely useful to reduce
    the complexity of many problems. First of all, we can provide a non-rigorous definition
    of a manifold. An *n*-manifold is a topological space that is globally curved,
    but locally homeomorphic to an *n*-dimensional Euclidean space. In the following
    diagram, there''s an example of a manifold: the surface of a sphere in *ℜ³*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/78afa71a-9c17-4388-8ce7-dc7ec95c1df4.png)'
  prefs: []
  type: TYPE_IMG
- en: 2D manifold obtained from a spherical surface
  prefs: []
  type: TYPE_NORMAL
- en: The small patch around *P* (for *ε* → *0*) can be mapped to a flat circular
    surface. Therefore, the properties of a manifold are locally based on the Euclidean
    geometry, while, globally, they need a proper mathematical extension which is
    beyond the scope of this book (further information can be found in *Semi-supervised
    learning on Riemannian manifolds*,* Belkin M., Niyogi P.*, *Machine Learning 56*,
    *2004*).
  prefs: []
  type: TYPE_NORMAL
- en: The manifold assumption states that *p*-dimensional samples (where *p* >> *1*)
    approximately lie on a *q*-dimensional manifold with *p* << *q*. Without excessive
    mathematical rigor, we can say that, for example, if we have *N* *1000*-dimensional
    bounded vectors, they are enclosed into a *1000*-dimensional hypercube with edge-length
    equal to *r*. The corresponding *n*-volume is *r^p = r^(1000)*, therefore, the
    probability of filling the entire space is very small (and decreases with *p*).
    What we observe, instead, is a high density on a lower dimensional manifold. For
    example, if we look at the Earth from space, we might think that its inhabitants
    are uniformly distributed over the whole volume. We know that this is false and,
    in fact, we can create maps and atlases which are represented on two-dimensional
    manifolds. It doesn't make sense to use three-dimensional vectors to map the position
    of a human being. It's easier to use a projection and work with latitude and longitude.
  prefs: []
  type: TYPE_NORMAL
- en: 'This assumption authorizes us to apply dimensionality reduction methods in
    order to avoid the *Curse of Dimensionality*, theorized by Bellman (in *Dynamic
    Programming and Markov Process, Ronald A. Howard*, *The MIT Press*). In the scope
    of machine learning, the main consequence of such an effect is that when the dimensionality
    of the samples increases, in order to achieve a high accuracy, it''s necessary
    to use more and more samples. Moreover, Hughes observed (the phenomenon has been
    named after him and it''s presented in the paper *Hughes G. F., On the mean accuracy
    of statistical pattern recognizers, IEEE Transactions on Information Theory*,
    *1968*, *14/1*) that the accuracy of statistical classifiers is inversely proportional
    to the dimensionality of the samples. This means that whenever it''s possible
    to work on lower dimensional manifolds (in particular in semi-supervised scenarios),
    two advantages are achieved:'
  prefs: []
  type: TYPE_NORMAL
- en: Less computational time and memory consumption
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Higher classification accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative Gaussian mixtures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Generative Gaussian mixtures is an inductive algorithm for semi-supervised
    clustering. Let''s suppose we have a labeled dataset (*X[l]*, *Y[l]*) containing
    *N* samples (drawn from *p[data]*) and an unlabeled dataset *X[u]* containing
    *M* >> *N* samples (drawn from the marginal distribution *p(x)*). It''s not necessary
    that *M* >> *N*, but we want to create a real semi-supervised scenario, with only
    a few labeled samples. Moreover, we are assuming that all unlabeled samples are
    consistent with *p[data]*. This can seem like a vicious cycle, but without this
    assumption, the procedure does not have a strong mathematical foundation. Our
    goal is to determine a complete *p(x|y)* distribution using a generative model.
    In general, it''s possible to use different priors, but we are now employing multivariate
    Gaussians to model our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4594a146-4eaf-4d21-9205-85bab6323788.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, our model parameters are means and covariance matrices for all Gaussians.
    In other contexts, it''s possible to use binomial or multinomial distributions.
    However, the procedure doesn''t change; therefore, let''s assume that it''s possible
    to approximate *p(x|y)* with a parametrized distribution *p(x|y*, *θ)*. We can
    achieve this goal by minimizing the Kullback-Leibler divergence between the two
    distributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3149ad3a-070e-4f76-9370-e7dcf0c428dc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In [Chapter 5](8d541a43-8790-4a91-a79b-e48496f75d90.xhtml), *EM Algorithm and
    Applications* we are going to show that this is equivalent to maximizing the likelihood
    of the dataset. To obtain the likelihood, it''s necessary to define the number
    of expected Gaussians (which is known from the labeled samples) and a weight-vector
    that represents the marginal probability of a specific Gaussian:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4da88076-3c81-49ec-aecd-269f48a9c11e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using the Bayes'' theorem, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3712c8bb-c9fc-42c6-b8e0-a73b9373a13f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we are working with both labeled and unlabeled samples, the previous expression
    has a double interpretation:'
  prefs: []
  type: TYPE_NORMAL
- en: For unlabeled samples, it is computed by multiplying the *i^(th)* Gaussian weight
    times the probability *p(x[j])* relative to the *i^(th)* Gaussian distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For labeled samples, it can be represented by a vector p = [0, 0, ... 1, ...
    0, 0] where 1 is the *i^(th)* element. In this way, we force our model to trust
    the labeled samples in order to find the best parameter values that maximize the
    likelihood on the whole dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With this distinction, we can consider a single log-likelihood function where
    the term *f[w](y[i]|x[j])* has been substituted by a per sample weight:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc30daf5-a259-4ead-b28f-952f4f76e7de.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It''s possible to maximize the log-likelihood using the EM algorithm (see [Chapter
    5](8d541a43-8790-4a91-a79b-e48496f75d90.xhtml), *EM Algorithm and Applications*).
    In this context, we provide the steps directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '*p(y[i]|x[j],θ,w)* is computed according to the previously explained method'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The parameters of the Gaussians are updated using these rules:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/3fdcf390-48e9-44b8-b80f-9455d7a941b0.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/ce430896-d1fb-478b-b706-2312a5e536a7.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/0a188d67-68d1-410f-81d0-b6fb4978651a.png)'
  prefs: []
  type: TYPE_IMG
- en: '*N* is the total number of samples. The procedure must be iterated until the
    parameters stop modifying or the modifications are lower than a fixed threshold.'
  prefs: []
  type: TYPE_NORMAL
- en: Example of a generative Gaussian mixture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can now implement this model in Python using a simple bidimensional dataset,
    created using the `make_blobs()` function provided by Scikit-Learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We have created 1,000 samples belonging to 2 classes. 750 points have then
    been randomly selected to become our unlabeled dataset (the corresponding class
    has been set to -1). We can now initialize two Gaussian distributions by defining
    their mean, covariance, and weight. One possibility is to use random values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'However, as the covariance matrices must be positive semi definite, it''s useful
    to alter the random values (by multiplying each matrix by the corresponding transpose)
    or to set hard-coded initial parameters. In this case, we could pick the following
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting plot is shown in the following graph, where the small diamonds
    represent the unlabeled points and the bigger dots, the samples belonging to the
    known classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/233b53fa-0a97-46fd-b3b4-db83cd146b71.png)'
  prefs: []
  type: TYPE_IMG
- en: Initial configuration of the Gaussian mixture
  prefs: []
  type: TYPE_NORMAL
- en: 'The two Gaussians are represented by the concentric ellipses. We can now execute
    the training procedure. For simplicity, we repeat the update for a fixed number
    of iterations. The reader can easily modify the code in order to introduce a threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The first thing at the beginning of each cycle is to initialize the `Pij` matrix
    that will be used to store the *p(y[i]|x[j]**,θ,w)* values. Then, for each sample,
    we can compute *p(y[i]|x[j],θ,w)* considering whether it''s labeled or not. The
    Gaussian probability is computed using the SciPy function `multivariate_normal.pdf()`.
    When the whole *P[ij]* matrix has been populated, we can update the parameters
    (means and covariance matrix) of both Gaussians and the relative weights. The
    algorithm is very fast; after five iterations, we get the stable state represented
    in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/23d5ef13-ea8a-4b9a-9158-ad546f3c2a11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The two Gaussians have perfectly mapped the space by setting their parameters
    so as to cover the high-density regions. We can check for some unlabeled points,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s easy to locate them in the previous plot. The corresponding classes can
    be obtained through the last *P[ij]* matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This immediately verifies that they have been correctly labeled and assigned
    to the right cluster. This algorithm is very fast and produces excellent results
    in terms of density estimation. In [Chapter 5](8d541a43-8790-4a91-a79b-e48496f75d90.xhtml), *EM
    Algorithm and Applications*, we are going to discuss a general version of this
    algorithm, explaining the complete training procedure based on the EM algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: In all the examples that involve random numbers, the seed is set equal to 1,000
    (`np.random.seed(1000)`). Other values or subsequent experiments without resetting
    it can yield slightly different results.
  prefs: []
  type: TYPE_NORMAL
- en: Weighted log-likelihood
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous example, we have considered a single log-likelihood for both
    labeled and unlabeled samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cc579607-466f-438a-85ad-3c92800a5816.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is equivalent to saying that we trust the unlabeled points just like the
    labeled ones. However, in some contexts, this assumption can lead to completely
    wrong estimations, as shown in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f4a6098c-6199-4920-8626-3cfa01b2869c.png)'
  prefs: []
  type: TYPE_IMG
- en: Biased final Gaussian mixture configuration
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the means and covariance matrices of both Gaussian distributions
    have been biased by the unlabeled points and the resulting density estimation
    is clearly wrong. When this phenomenon happens, the best thing to do is to consider
    a double weighted log-likelihood. If the first *N* samples are labeled and the
    following *M* are unlabeled, the log-likelihood can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d1ed52c3-6911-4b8b-9934-123e08707c50.png)'
  prefs: []
  type: TYPE_IMG
- en: In the previous formula, the term *λ*, if less than 1, can underweight the unlabeled
    terms, giving more importance to the labeled dataset. The modifications to the
    algorithm are trivial because each unlabeled weight has to be scaled according
    to *λ*, reducing its estimated probability. In *Semi-Supervised Learning*, *Chapelle
    O.*,* Schölkopf B.*, *Zien A.*, (*edited by*), *The MIT Press*,the reader can
    find a very detailed discussion about the choice of *λ*. There are no golden rules;
    however, a possible strategy could be based on the cross-validation performed
    on the labeled dataset. Another (more complex) approach is to consider different
    increasing values of *λ* and pick the first one where the log-likelihood is maximum.
    I recommend the aforementioned book for further details and strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Contrastive pessimistic likelihood estimation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As explained at the beginning of this chapter, in many real life problems, it's
    cheaper to retrieve unlabeled samples, rather than correctly labeled ones. For
    this reason, many researchers worked to find out the best strategies to carry
    out a semi-supervised classification that could outperform the supervised counterpart.
    The idea is to train a classifier with a few labeled samples and then improve
    its accuracy after adding weighted unlabeled samples. One of the best results
    is the **Contrastive Pessimistic Likelihood Estimation** (**CPLE**) algorithm,
    proposed by M. Loog (in *Loog M.*,* Contrastive Pessimistic Likelihood Estimation
    for Semi-Supervised Classification*, *arXiv:1503.00269*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Before explaining this algorithm, an introduction is necessary. If we have
    a labeled dataset (*X*, *Y*) containing *N* samples, it''s possible to define
    the log-likelihood cost function of a generic estimator, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cb4facc0-a6eb-4f12-b726-6237372c39e1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After training the model, it should be possible to determine *p(y[i]|x[i],
    θ)*, which is the probability of a label given a sample *x[i]*. However, some
    classifiers are not based on this approach (like SVM) and evaluate the right class,
    for example, by checking the sign of a parametrized function *f(x[i], **θ)*. As
    CPLE is a generic framework that can be used with any classification algorithm
    when the probabilities are not available, it''s useful to implement a technique
    called Platt scaling, which allows transforming the decision function into a probability
    through a parametrized sigmoid. For a binary classifier, it can be expressed as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e9c6f607-6508-4207-855d-67cff8e13904.png)'
  prefs: []
  type: TYPE_IMG
- en: '*α* and *β* are parameters that must be learned in order to maximize the likelihood.
    Luckily Scikit-Learn provides the method `predict_proba()`, which returns the
    probabilities for all classes. Platt scaling is performed automatically or on
    demand; for example, the SCV classifier needs to have the parameter `probability=True`
    in order to compute the probability mapping. I always recommend checking the documentation
    before implementing a custom solution.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can consider a full dataset, made up of labeled and unlabeled samples. For
    simplicity, we can reorganize the original dataset, so that the first *N* samples
    are labeled, while the next *M* are unlabeled:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/852b786f-f2d6-4166-b49f-bfc70fdc98c9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we don''t know the labels for all *x^u* samples, we can decide to use *M*
    *k*-dimensional (k is the number of classes) soft-labels *q[i]* that can be optimized
    during the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/90fb619a-a877-455d-bf7e-77922cc55eb7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The second condition in the previous formula is necessary to guarantee that
    each *q[i]* represents a discrete probability (all the elements must sum up to
    1.0). The complete log-likelihood cost function can, therefore, be expressed as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3fdf450f-1852-4bfa-8336-19bd92532055.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The first term represents the log-likelihood for the supervised part, while
    the second one is responsible for the unlabeled points. If we train a classifier
    with only the labeled samples, excluding the second addend, we get a parameter
    set *θ[sup]*. CPLE defines a contrastive condition (as a log-likelihood too),
    by defining the improvement in the total cost function given by the semi-supervised
    approach, compared to the supervised solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ea514fd2-d071-4331-885f-2ab7b4334e98.png)'
  prefs: []
  type: TYPE_IMG
- en: This condition allows imposing that the semi-supervised solution must outperform
    the supervised one, in fact, maximizing it; we both increase the first term and
    reduce the second one, obtaining a proportional increase of CL (the term *contrastive*
    is very common in machine learning and it normally indicates a condition which
    is achieved as the difference between two opposite constraints). If CL doesn't
    increase, it probably means that the unlabeled samples have not been drawn from
    the marginal distribution *p(x)* extracted from *p[data]*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, in the previous expression, we have implicitly used soft-labels,
    but as they are initially randomly chosen and there''s no ground truth to support
    their values, it''s a good idea not to trust them by imposing a pessimistic condition
    (as another log-likelihood):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3fcae4b3-c343-44cf-85b9-2d5bbb3b128c.png)'
  prefs: []
  type: TYPE_IMG
- en: By imposing this constraint, we try to find the soft-labels that minimize the
    contrastive log-likelihood; that's why this is defined as a pessimistic approach.
    It can seem a contradiction; however, trusting soft-labels can be dangerous, because
    the semi-supervised log-likelihood could be increased even with a large percentage
    of misclassification. Our goal is to find the best parameter set that is able
    to guarantee the highest accuracy starting from the supervised baseline (which
    has been obtained using the labeled samples) and improving it, without forgetting
    the structural features provided by the labeled samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, our final goal can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/36e7e7a5-d681-4f86-becb-ca5c6cbffac4.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of contrastive pessimistic likelihood estimation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are going to implement the CPLE algorithm in Python using a subset extracted
    from the MNIST dataset. For simplicity, we are going to use only the samples representing
    the digits 0 and 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'After creating the restricted dataset (*X*, *Y*) which contain 360 samples,
    we randomly select 150 samples (about 42%) to become unlabeled (the corresponding
    *y* is -1). At this point, we can measure the performance of logistic regression
    trained only on the labeled dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'So, the logistic regression shows 57% accuracy for the classification of the
    unlabeled samples. We can also evaluate the cross-validation score on the whole
    dataset (before removing some random labels):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Thus, the classifier achieves an average 48% accuracy when using 10 folds (each
    test set contains 36 samples) if all the labels are known.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now implement a CPLE algorithm. The first thing is to initialize a `LogisticRegression`
    instance and the soft-labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '*q0* is a random array of values bounded in the half-open interval [0, 1];
    therefore, we also need a converter to transform *q[i]* into an actual binary
    label:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0015cb62-47fa-45fd-9646-7d08d9804844.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can achieve this using the NumPy function `np.vectorize()`, which allows
    us to apply a transformation to all the elements of a vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to compute the log-likelihood, we need also a weighted log-loss (similar
    to the Scikit-Learn function `log_loss()`, which, however, computes the negative
    log-likelihood but doesn''t support weights):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This function computes the following expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a0191f5-780c-4f33-a0ce-f5b3ecd791f3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We need also a function to build the dataset with variable soft-labels *q[i]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we can define our contrastive log-likelihood:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This method will be called by the optimizer, passing a different *q* vector
    each time. The first step is building the new dataset and computing `Y_soft`,
    which are the labels corresponding to *q*. Then the logistic regression classifier
    is trained with with the dataset (as `Y_n` is a (k, 1) array, it's necessary to
    squeeze it to avoid a warning. The same thing is done when using *Y* as a boolean
    indicator). At this point, it's possible to compute both *p[sup]* and *p[semi]*
    using the method `predict_proba()` and, finally, we can compute the semi-supervised
    and supervised log-loss, which is the term, a function of *q[i]*, that we want
    to minimize, while the maximization of *θ* is done implicitly when training the
    logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'The optimization is carried out using the BFGS algorithm implemented in SciPy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a very fast algorithm, but the user is encouraged to experiment with
    methods or libraries. The two parameters we need in this case are `f`, which is
    the function to minimize, and `x0`, which is the initial condition for the independent
    variables. `maxiter` is useful for avoiding an excessive number of iterations
    when no improvements are achieved. Once the optimization is complete, `q_end`
    contains the optimal soft-labels. We can, therefore, rebuild our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'With this final configuration, we can retrain the logistic regression and check
    the cross-validation accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The semi-supervised solution based on the CPLE algorithms achieves an average
    84% accuracy, outperforming, as expected, the supervised approach. The reader
    can try other examples using different classifiers, such SVM or Decision Trees,
    and verify when CPLE allows obtaining higher accuracy than other supervised algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Semi-supervised Support Vector Machines (S3VM)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we discussed the cluster assumption, we also defined the low-density regions
    as boundaries and the corresponding problem as low-density separation. A common
    supervised classifier which is based on this concept is a **Support Vector Machine**
    (**SVM**), the objective of which is to maximize the distance between the dense
    regions where the samples must be. For a complete description of linear and kernel-based
    SVMs, please refer to *Bonaccorso G.*, *Machine Learning Algorithms*, *Packt Publishing*;
    however, it''s useful to remind yourself of the basic model for a linear SVM with
    slack variables *ξ[i]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4ce95bed-96a5-44eb-a7d7-8eefada51472.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This model is based on the assumptions that *y[i]* can be either -1 or 1\.
    The slack variables *ξ[i]* or soft-margins are variables, one for each sample,
    introduced to reduce the *strength* imposed by the original condition (*min ||w||*),
    which is based on a hard margin that misclassifies all the samples that are on
    the wrong side. They are defined by the Hinge loss, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7732ffee-8269-4bac-971f-7a4a1b4f1448.png)'
  prefs: []
  type: TYPE_IMG
- en: 'With those variables, we allow some points to overcome the limit without being
    misclassified if they remain within a distance controlled by the corresponding
    slack variable (which is also minimized during the training phase, so as to avoid
    uncontrollable growth). In the following diagram, there''s a schematic representation
    of this process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5093fc7-34e5-4ab3-8186-76a30ecc15b6.png)'
  prefs: []
  type: TYPE_IMG
- en: SVM generic scenario
  prefs: []
  type: TYPE_NORMAL
- en: 'The last elements of each high-density regions are the support vectors. Between
    them, there''s a low-density region (it can also be zero-density in some cases)
    where our separating hyperplane lies. In [Chapter 1](acff0775-f21c-4b6d-8ef2-c78713e21364.xhtml), *Machine
    Learning Model Fundamentals*, we defined the concept of *empirical risk* as a
    proxy for expected risk; therefore, we can turn the SVM problem into the minimization
    of empirical risk under the Hinge cost function (with or without Ridge Regularization
    on w):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fe754389-2954-4aeb-be7d-8a4105339c56.png)'
  prefs: []
  type: TYPE_IMG
- en: Theoretically, every function which is always bounded by two hyperplanes containing
    the support vectors is a good classifier, but we need to minimize the empirical
    risk (and, so, the expected risk); therefore we look for the maximum margin between
    high-density regions. This model is able to separate two dense regions with irregular
    boundaries and, by adopting a kernel function, also in non-linear scenarios. The
    natural question, at this point, is about the best strategy to integrate labeled
    and unlabeled samples when we need to solve this kind of problem in a semi-supervised
    scenario.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first element to consider is the ratio: if we have a low percentage of
    unlabeled points, the problem is mainly supervised and the generalization ability
    learned using the training set should be enough to correctly classify all the
    unlabeled points. On the other hand, if the number of unlabeled samples is much
    larger, we return to an almost pure clustering scenario (like the one discussed
    in the paragraph about the Generative Gaussian mixtures). In order to exploit
    the strength of semi-supervised methods in low-density separation problems, therefore,
    we should consider situations where the ratio labeled/unlabeled is about 1.0\.
    However, even if we have the predominance of a class (for example, if we have
    a huge unlabeled dataset and only a few labeled samples), it''s always possible
    to use the algorithms we''re going to discuss, even if, sometimes, their performance
    could be equal to or lower than a pure supervised/clustering solution. Transductive
    SMVs, for example, showed better accuracies when the labeled/unlabeled ratio is
    very small, while other methods can behave in a completely different way. However,
    when working with semi-supervised learning (and its assumptions), it is always
    important to bear in mind that each problem is supervised and unsupervised at
    the same time and the best solution must be evaluated in every different context.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A solution for this problem is offered by the *Semi-Supervised SVM* (also known
    as *S³VM*) algorithm. If we have *N* labeled samples and *M* unlabeled samples,
    the objective function becomes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8914f477-cc93-413c-a2af-cc0946bbe32c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The first term imposes the standard SVM condition about the maximum separation
    distance, while the second block is divided into two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: We need to add *N* slack variables *η[i]* to guarantee a soft-margin for the
    labeled samples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the same time, we have to consider the unlabeled points, which could be classified
    as +1 or -1\. Therefore, we have two corresponding slack-variable sets *ξ[i]* and
    *z[i]*. However, we want to find the smallest variable for each possible pair,
    so as to be sure that the unlabeled sample is placed on the sub-space where the
    maximum accuracy is achieved.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The constraints necessary to solve the problems become as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00e27ea5-de91-4860-8d55-45cbd0e09fda.png)'
  prefs: []
  type: TYPE_IMG
- en: The first constraint is limited to the labeled points and it's the same as a
    supervised SVM. The following two, instead, take into account the possibility
    that an unlabeled sample could be classified as +1 or -1\. Let's suppose, for
    example, that the label *y[j]* for the sample *x[j]* should be +1 and the first
    member of the second inequality is a positive number *K* (so the corresponding
    term of the third equation is *-K*). It's easy to verify that the first slack
    variable is *ξ[i] ≥ 1 - K,* while the second one is *z[j] ≥ 1 + K*.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, in the objective, *ξ*[*i* ]is chosen to be minimized. This method
    is inductive and yields good (if not excellent) performances; however, it has
    a very high computational cost and should be solved using optimized (native) libraries.
    Unfortunately, it is a non-convex problem and there are no standard methods to
    solve it so it always reaches the optimal configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Example of S3VM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now implement an S³VM in Python using the SciPy optimization methods, which
    are mainly based on C and FORTRAN implementations. The reader can try it with
    other libraries such as NLOpt and LIBSVM and compare the results. A possibility
    suggested by Bennet and Demiriz is to use the L1-norm for w, so as to linearize
    the objective function; however, this choice seems to produce good results only
    for small datasets. We are going to keep the original formulation based on the
    L2-norm, using an **Sequential Least Squares Programming** (**SLSQP**) algorithm
    to optimize the objective.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by creating a bidimensional dataset with both labeled and unlabeled
    samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'For simplicity (and without any impact, because the samples are shuffled),
    we set last 200 samples as unlabeled (*y = 0*). The corresponding plot is shown
    in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/866c542c-e327-4590-a18f-2864b229a397.png)'
  prefs: []
  type: TYPE_IMG
- en: Original labeled and unlabeled dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'The crosses represent unlabeled points, which are spread throughout the entire
    dataset. At this point we need to initialize all variables required for the optimization
    problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'As the optimization algorithm requires a single array, we have stacked all
    vectors into a horizontal array `theta0` using the `np.hstack()` function. We
    also need to vectorize the `min()` function in order to apply it to arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can define the objective function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The arguments are the current `theta` vector and the complete datasets `Xd`
    and `Yd`. The dot product of *w* has been multiplied by 0.5 to keep the conventional
    notation used for supervised SVMs. The constant can be omitted without any impact.
    At this point, we need to define all the constraints, as they are based on the
    slack variables; each function (which shares the same parameters of the objectives)
    is parametrized with an index, `idx`. The labeled constraint is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The unlabeled constraints, instead, are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'They are parametrized with the current `theta` vector, the `Xd` dataset, and
    an `idx` index. We need also to include the constraints for each slack variable
    (*≥ 0*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now set up the problem using the SciPy convention:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Each constraint is represented with a dictionary, where `type` is set to `ineq`
    to indicate that it is an inequality, `fun` points to the callable object and
    `args` contains all extra arguments (`theta` is the main x variable and it''s
    automatically added). Using SciPy, it''s possible to minimize the objective using
    the **Sequential Least Squares Programming** (**SLSQP**) or **Constraint Optimization
    by Linear Approximation** (**COBYLA**) algorithms. We preferred the former, because
    it works more rapidly and is more stable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'After the training process is complete, we can compute the labels for the unlabeled
    points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In the next graph, it''s possible to compare the initial plot (left) with the
    final one where all points have been assigned a label (right):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a4b85e3e-4a9c-4209-98d0-8180b29d82df.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, S³VM succeeded in finding the right label for all unlabeled
    points, confirming the existence of two very dense regions for *x* between *[0,
    2]* (square dots) and *y* between *[0, 2]* (circular dots).
  prefs: []
  type: TYPE_NORMAL
- en: NLOpt is a complete optimization library developed at MIT. It is available for
    different operating systems and programming languages. The website is [https://nlopt.readthedocs.io](https://nlopt.readthedocs.io).
    LIBSVM is an optimized library for solving SVM problems and it is adopted by Scikit-Learn
    together with LIBLINEAR. It's also available for different environments. The homepage
    is [https://www.csie.ntu.edu.tw/~cjlin/libsvm/.](https://www.csie.ntu.edu.tw/~cjlin/libsvm/)
  prefs: []
  type: TYPE_NORMAL
- en: Transductive Support Vector Machines (TSVM)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another approach to the same problem is offered by the TSVM, proposed by T.
    Joachims (in *Transductive Inference for Text Classification using Support Vector
    Machines*,*Joachims T.*, *ICML Vol. 99/1999*). The idea is to keep the original
    objective with two sets of slack variables: the first for the labeled samples
    and the other for the unlabeled ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e896ce54-dc1b-4a81-9200-df1c2153320e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As this is a transductive approach, we need to consider the unlabeled samples
    as variable-labeled ones (subject to the learning process), imposing a constraint
    similar to the supervised points. As for the previous algorithm, we assume we
    have *N* labeled samples and *M* unlabeled ones; therefore, the conditions become
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e0adc106-9f38-4c21-8851-236dcdd6ef59.png)'
  prefs: []
  type: TYPE_IMG
- en: The first constraint is the classical SVM one and it works only on labeled samples.
    The second one uses the variable *y^((u))[j]* with the corresponding slack variables *ξ[j]*
    to impose a similar condition on the unlabeled samples, while the third one is
    necessary to constrain the labels to being equal to -1 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: Just like the semi-supervised SVMs, this algorithm is non-convex and it's useful
    to try different methods to optimize it. Moreover, the author, in the aforementioned
    paper, showed how TSVM works better when the test set (unlabeled) is large and
    the training set (labeled) is relatively small (when a standard supervised SVM
    is outperformed). On the other hand, with large training sets and small test sets,
    a supervised SVM (or other algorithms) are always preferable because they are
    faster and yield better accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Example of TSVM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In our Python implementation, we are going to use a bidimensional dataset similar
    to one employed in the previous method; however, in this case, we impose 400 unlabeled
    samples out of a total of 500 points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The corresponding plot is shown in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c9abc37a-205a-4a81-b90e-124f77f97c26.png)'
  prefs: []
  type: TYPE_IMG
- en: Original labeled and unlabeled dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'The procedure is similar to the one we used before. First of all, we need to
    initialize our variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we also need to define the `y_unlabeled` vector for variable-labels.
    The author also suggests using two C constants (`C_labeled` and `C_unlabeled`)
    in order to be able to weight the misclassification of labeled and unlabeled samples
    differently. We used a value of 1.0 for `C_labeled` and 10.0 for `C_unlabled`,
    because we want to penalize more the misclassification of unlabeled samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'The objective function to optimize is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'While the labeled and unlabeled constraints are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We need also to impose the constraints on the slack variables and on the *y^((u))*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'As in the previous example, we can create the constraint dictionary needed
    by SciPy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the last constraint is an equality, because we want to force *y^((u))*
    to be equal either to -1 or 1\. At this point, we minimize the objective function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'When the process is complete, we can compute the labels for the unlabeled samples
    and compare the plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot comparison is shown in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00c61ef5-5da6-4107-8c9a-36a2ec5a0115.png)'
  prefs: []
  type: TYPE_IMG
- en: Original dataset (left). Final labeled dataset (right)
  prefs: []
  type: TYPE_NORMAL
- en: The misclassification (based on the density distribution) is slightly higher
    than S³VM, but it's possible to change the C values and the optimization method
    until the expected result has been reached. A good benchmark is provided by a
    supervised SVM, which can have better performances when the training set is huge
    enough (and when it represents the whole *p[data]* correctly).
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s interesting to evaluate different combinations of the C parameters, starting
    from a standard supervised SVM. The dataset is smaller, with a high number of
    unlabeled samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the standard SVM implementation provided by Scikit-Learn (the `SVC()`
    class) with a linear kernel and `C=1.0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The SVM is trained with the labeled samples and the vector `yu_svc` contains
    the prediction for the unlabeled samples. The resulting plot (in comparison with
    the original dataset) is shown in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5d4839c3-9687-4f64-a00f-46f205834f25.png)'
  prefs: []
  type: TYPE_IMG
- en: Original dataset (left). Final labeled dataset (right) with C = 1.0
  prefs: []
  type: TYPE_NORMAL
- en: 'All the labeled samples are represented with bigger squares and circles. The
    result meets our expectations, but there''s an area *(X [-1, 0] - Y [-2, -1])*,
    where the SVM decided to impose the *circle* class even if the unlabeled points
    are close to a square. This hypothesis can''t be acceptable considering the clustering
    assumption; in fact, in a high-density region there are samples belonging to two
    classes. A similar (or even worse) result is obtained using an S³VM with **CL=10**
    and **CU=5**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a83b787d-d1ef-4344-8453-fda96b498443.png)'
  prefs: []
  type: TYPE_IMG
- en: Original dataset (left). Final labeled dataset (right) with C[L] = 10 and C[U]
    = 5
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the classification accuracy is lower because the penalty for
    the unlabeled samples is lower than the one imposed on the labeled points. A supervised
    SVM has obviously better performances. Let''s try with **C****[L]****=10** and
    **C****[U]****=50**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e36c7b7e-208f-4ca5-af4f-e6b97b134456.png)'
  prefs: []
  type: TYPE_IMG
- en: Original dataset (left). Final labeled dataset (right) with C[L] = 10 and C[U] =
    50
  prefs: []
  type: TYPE_NORMAL
- en: Now, the penalty is quite a lot higher for the unlabeled samples and the result
    appears much more reasonable considering the clustering assumption. All the high-density
    regions are coherent and separated by low-density ones. These examples show how
    the value chosen for the parameters and the optimization method can dramatically
    change the result. My suggestion is to test several configurations (on sub-sampled
    datasets), before picking the final one. In *Semi-Supervised Learning*, *Chapelle
    O.*,* Schölkopf B.*, *Zien A.*, (*edited by*), *The MIT Press,* there are further
    details about possible optimization strategies, with strengths and weaknesses.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced semi-supervised learning, starting from the scenario
    and the assumptions needed to justify the approaches. We discussed the importance
    of the smoothness assumption when working with both supervised and semi-supervised
    classifiers in order to guarantee a reasonable generalization ability. Then we
    introduced the clustering assumption, which is strictly related to the geometry
    of the datasets and allows coping with density estimation problems with a strong
    structural condition. Finally, we discussed the manifold assumption and its importance
    in order to avoid the curse of dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: 'The chapter continued by introducing a generative and inductive model: Generative
    Gaussian mixtures, which allow clustering labeled and unlabeled samples starting
    from the assumption that the prior probabilities are modeled by multivariate Gaussian
    distributions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next topic was about a very important algorithm: contrastive pessimistic
    likelihood estimation, which is an inductive, semi-supervised classification framework
    that can be adopted together with any supervised classifier. The main concept
    is to define a contrastive log-likelihood based on soft-labels (representing the
    probabilities for the unlabeled samples) and impose a pessimistic condition in
    order to minimize the trust in the soft-labels. The algorithm can find the best
    configuration that maximizes the log-likelihood, taking into account both labeled
    and unlabeled samples.'
  prefs: []
  type: TYPE_NORMAL
- en: Another inductive classification approach is provided by the S³VM algorithm,
    which is an extension of the classical SVM approach, based on two extra optimization
    constraints to address the unlabeled samples. This method is relatively powerful,
    but it's non-convex and, therefore, very sensitive to the algorithms employed
    to minimize the objective function.
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternative to S³VM is provided by the TSVM, which tries to minimize the
    objective with a condition based on variable labels. The problem is, hence, divided
    into two parts: the supervised one, which is exactly the same as standard SVM,
    and the semi-supervised one, which has a similar structure but without fixed *y*
    labels. This problem is non-convex too and it''s necessary to evaluate different
    optimization strategies to find the best trade-off between accuracy and computational
    complexity. In the reference section, there are some useful resources so you can
    examine all these problems in depth and find a suitable solution for each particular
    scenario.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, [Chapter 3](c23d1792-167f-416e-a848-fa7a10777697.xhtml), *Graph-Based
    Semi-Supervised Learning* we're continuing this exploration by discussing some
    important algorithms based on the structure underlying the dataset. In particular,
    we're going to employ graph theory to perform the propagation of labels to unlabeled
    samples and to reduce the dimensionality of datasets in non-linear contexts.
  prefs: []
  type: TYPE_NORMAL
