<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Clustering Model</h1>
                </header>
            
            <article>
                
<p><span>With classification models behind us, it is now time to dive into clustering models. Currently, in ML.NET there is only one cluster algorithm, k-means. In this chapter, we will dive into k-means clustering as well as the various applications best suited to utilizing a clustering algorithm. In addition, we will build a new ML.NET clustering application that determines the type of a file simply by looking at the content. Finally, we will explore how to evaluate a k-means clustering model with the properties that ML.NET exposes.</span></p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Breaking down the k-means algorithm</li>
<li>Creating the clustering application</li>
<li><span>Evaluating a k-means</span><span> </span><span>model</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Breaking down the k-means algorithm</h1>
                </header>
            
            <article>
                
<p>As mentioned in<span> </span><a href="b8d873e1-9234-4f11-ad94-76df5ffbb228.xhtml">Chapter 1</a>, <em>Getting Started with Machine Learning and ML.NET,</em> k-means clustering, by definition, is an unsupervised learning algorithm. This means that data is grouped into clusters based on the data provided to the model for training. In this section, we will dive into a number of use cases for clustering and the k-means trainer.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Use cases for clustering</h1>
                </header>
            
            <article>
                
<p>Clustering, as you may be beginning to realize, has numerous applications where the output categorizes similar outputs into groups of similar data points.</p>
<p>Some of its potential applications include the following:</p>
<ul>
<li>Natural disaster tracking such as earthquakes or hurricanes and creating clusters of high-danger zones</li>
<li>Book or document grouping based on the authors, subject matter, and sources</li>
<li>Grouping customer data into targeted marketing predictions</li>
<li>Search result grouping of similar results that other users found useful</li>
</ul>
<p>In addition, it has numerous other applications such as predicting malware families or medical purposes for cancer research.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Diving into the k-means trainer</h1>
                </header>
            
            <article>
                
<p>The k-means trainer used in ML.NET is based on the Yinyang method as opposed to a classic k-means implementation. Like some of the trainers we have looked at in previous chapters, all of the input must be of the Float type. In addition, all input must be normalized into a single feature vector. Fortunately, the k-means trainer is included in the main ML.NET NuGet package; therefore, no additional dependencies are required.</p>
<div class="packt_tip packt_infobox">To learn more about the Yinyang implementation, Microsoft Research published a white paper here: <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ding15.pdf">https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ding15.pdf</a>.</div>
<p class="mce-root">Take a look at the following diagram, showing three clusters and a data point:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-628 image-border" src="assets/b7759f97-a307-4176-a449-ad7e854ac847.png" style="width:17.75em;height:17.83em;"/></p>
<p>In clustering, each of these clusters represents a grouping of similar data points. With k-means clustering (and other clustering algorithms), the distances between the data point and each of the clusters are the measures of which cluster the model will return. For k-means clustering specifically, it uses the center point of each of these clusters (also called a centroid) and then calculates the distance to the data point. The smallest of these values is the predicted cluster.</p>
<p>For the k-means trainer, it can be initialized in one of three ways. One way is to utilize a randomized initialization<span>—</span>as you have probably guessed, this can lead to randomized prediction results. Another way is to utilize k-means++, which strives to produce O(log K) predictions. Lastly, <span>k-means||, the default method in ML.NET, uses a parallel method to reduce the number of passes required to initialize.</span></p>
<div class="packt_tip">For more information on <span>k-means||, you can refer to a paper published by Stanford, which explains it in detail: <a href="https://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf">https://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf</a>.<a href="https://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf"><br/></a></span><span><br/></span> <span>For more information on k-means++, you can refer to a paper published by Stanford in 2006, explaining it in detail: <a href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf">http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf</a>.</span></div>
<p><span>We will demonstrate this trainer in the example application in the next section.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating the clustering application</h1>
                </header>
            
            <article>
                
<p>As mentioned earlier, the application we will be creating is a file type classifier. Given a set of attributes statically extracted from a file, the prediction will return if it is a document, an executable, or a script. For those of you who have used the Linux <kbd>file</kbd> command, this is a simplified version but based on machine learning. The attributes included in this example aren't the definitive list of attributes, nor should they be used as-is in a production environment; however, you could use this as a starting point for creating a true ML-based replacement for the Linux <kbd>file</kbd> command.</p>
<p>As with previous chapters, the completed project code, sample dataset, and project files can be downloaded here: <a href="https://github.com/PacktPublishing/Hands-On-Machine-Learning-With-ML.NET/tree/master/chapter05">https://github.com/PacktPublishing/Hands-On-Machine-Learning-With-ML.NET/tree/master/chapter05</a>.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring the project architecture</h1>
                </header>
            
            <article>
                
<p>Building on the project architecture and code we created in previous chapters, the major change architecturally is in the feature extraction being done on both the training and test sets.</p>
<p>Here, you will find the Visual Studio Solution Explorer view of the project. The new additions to the solution are the <kbd>FileTypes</kbd>, <kbd>FileData</kbd>, and <kbd>FilePrediction</kbd> files that we will review later on in this section:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-629 image-border" src="assets/7b2bdf6a-0c57-43f5-bd7b-22b0dba1c1c3.png" style="width:16.92em;height:32.00em;"/></p>
<p>The<span> </span><kbd>sampledata.csv</kbd><span> </span>file contains 80 rows of random files I had on my system, comprising 30 Windows executables, 20 PowerShell scripts, and 20 Word documents. Feel free to adjust the data to fit your own observations or to adjust the trained model. Here is a snippet of the data:</p>
<div>
<pre>0,1,1,0<br/>0,1,1,0<br/>0,1,1,0<br/>2,0,0,0<br/>2,0,0,0<br/>2,0,0,0<br/>2,0,0,0<br/>2,0,0,0<br/>2,0,0,0<br/>2,0,0,0<br/>2,0,0,0<br/>2,0,0,0<br/>2,0,0,0<br/>2,0,0,0<br/>1,1,0,1<br/>1,1,0,1<br/>1,1,0,1<br/>1,1,0,1</pre></div>
<p>Each of these rows contains the value for the properties in the newly created <kbd>FileData</kbd> class that we will review later on in this chapter.</p>
<p>In addition to this, we added the <kbd>testdata.csv</kbd> file, which contains additional data points to test the newly trained model<span> </span>against and evaluate. The breakdown was even with 10 Windows executables, 10 PowerShell scripts, and 10 Word documents. Here is a snippet of the data inside <kbd>testdata.csv</kbd>:</p>
<pre>0,1,1,0<br/>0,1,1,0<br/>2,0,0,0<br/>2,0,0,0<br/>2,0,0,0<br/>2,0,0,0<br/>2,0,0,0<br/>2,0,0,0<br/>1,1,0,1</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Diving into the code</h1>
                </header>
            
            <article>
                
<p>For this application, as noted in the previous section, we are building on top of the work<span> </span>completed in <a href="da0d1d99-ad37-498b-8670-f8cee6ad49bc.xhtml">Chapter 4</a>, <em>Classification Model</em>. For this deep dive, we are going to focus solely on the code that was changed for this application.</p>
<p>Classes that were changed or added are as follows:</p>
<ul>
<li><kbd>Constants</kbd></li>
<li><kbd><kbd>BaseML</kbd></kbd></li>
<li><kbd>FileTypes</kbd></li>
<li><kbd>FileData</kbd></li>
<li><kbd>FileTypePrediction</kbd></li>
<li><kbd>FeatureExtractor</kbd></li>
<li><kbd>Predictor</kbd></li>
<li><kbd>Trainer</kbd></li>
<li><kbd>Program</kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Constants class</h1>
                </header>
            
            <article>
                
<p>The <kbd>Constants</kbd> class has been changed to save the model to <kbd>chapter5.mdl</kbd>, in addition to supporting a feature-extracted <kbd>testdata.csv</kbd> variable. The following code block reflects these changes:</p>
<pre>namespace chapter05.Common<br/>{<br/>    public class Constants<br/>    {<br/>        public const string MODEL_FILENAME = "chapter5.mdl";<br/><br/>        public const string SAMPLE_DATA = "sampledata.csv";<br/><br/>        public const string TEST_DATA = "testdata.csv";<br/>    }<br/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The BaseML class</h1>
                </header>
            
            <article>
                
<p>The sole change in the <kbd>BaseML</kbd> class is the addition of the <kbd>FEATURES</kbd> variable. By using a variable here, we can remove the use of a magic string in our <kbd>Trainer</kbd> class (we will discuss this later in this section):</p>
<pre>protected const string FEATURES = "Features";</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The FileTypes enumeration</h1>
                </header>
            
            <article>
                
<p>The <kbd>FileTypes</kbd> enumeration contains a strongly typed method for mapping our classifications and a numeric value. As we discovered in our previous examples, utilizing an enumeration as opposed to magic or constant values provides better flexibility, as shown here and throughout the remaining classes:</p>
<pre>namespace chapter05.Enums<br/>{<br/>    public enum FileTypes<br/>    {<br/>        Executable = 0,<br/>        Document = 1,<br/>        Script = 2<br/>    }<br/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The FileData class</h1>
                </header>
            
            <article>
                
<p>The <kbd>FileData</kbd><strong> </strong>class is the container class that contains the data to both predict and train our model:</p>
<ol>
<li>First,<span> we add</span><span> constant values for <kbd>True</kbd> and <kbd>False</kbd> since k-means requires floating-point values</span>:</li>
</ol>
<div>
<pre style="padding-left: 60px">public class FileData<br/>{<br/>    private const float TRUE = 1.0f;<br/>    private const float FALSE = 0.0f;</pre></div>
<ol start="2">
<li>Next, we create a constructor that supports both our prediction and training. We optionally pass in the filename for the training to provide a label, in this case, <kbd>ps1</kbd>, <kbd>exe</kbd>, and <kbd>doc</kbd> for scripts, executables, and documents, respectively. We also call helper methods to determine whether the file is binary, or whether it starts with MZ or PK:</li>
</ol>
<pre style="padding-left: 60px">public FileData(Span&lt;byte&gt; data, string fileName = null)<br/>{<br/>    // Used for training purposes only<br/>    if (!string.IsNullOrEmpty(fileName))<br/>    {<br/>        if (fileName.Contains("ps1"))<br/>        {<br/>            Label = (float) FileTypes.Script;<br/>        } else if (fileName.Contains("exe"))<br/>        {<br/>            Label = (float) FileTypes.Executable;<br/>        } else if (fileName.Contains("doc"))<br/>        {<br/>            Label = (float) FileTypes.Document;<br/>        }<br/>    }<br/><br/>    IsBinary = HasBinaryContent(data) ? TRUE : FALSE;<br/><br/>    IsMZHeader = HasHeaderBytes(data.Slice(0, 2), "MZ") ? TRUE : FALSE;<br/><br/>    IsPKHeader = HasHeaderBytes(data.Slice(0, 2), "PK") ? TRUE : FALSE;<br/>}</pre>
<div class="packt_tip">MZ and PK are considered to be magic numbers of Windows executables and modern Microsoft Office files. Magic numbers are unique byte strings that are found at the beginning of every file. In this case, both are simply two bytes. When performing analysis on files, making quick determinations is crucial for performance. For the keen reader, PK is also the magic number for ZIP. Modern Microsoft Office documents are actually ZIP archives. For the sake of simplicity in this example, PK is used as opposed to performing an additional level of detection.</div>
<ol start="3">
<li><span>Next, we also add an additional constructor to support the hard truth setting of values. We will deep dive into the purpose of this addition later on in this section:</span></li>
</ol>
<div>
<pre style="padding-left: 60px">/// &lt;summary&gt;<br/>/// Used for mapping cluster ids to results only<br/>/// &lt;/summary&gt;<br/>/// &lt;param name="fileType"&gt;&lt;/param&gt;<br/>public FileData(FileTypes fileType)<br/>{<br/>    Label = (float)fileType;<br/><br/>    switch (fileType)<br/>    {<br/>        case FileTypes.Document:<br/>            IsBinary = TRUE;<br/>            IsMZHeader = FALSE;<br/>            IsPKHeader = TRUE;<br/>            break;<br/>        case FileTypes.Executable:<br/>            IsBinary = TRUE;<br/>            IsMZHeader = TRUE;<br/>            IsPKHeader = FALSE;<br/>            break;<br/>        case FileTypes.Script:<br/>            IsBinary = FALSE;<br/>            IsMZHeader = FALSE;<br/>            IsPKHeader = FALSE;<br/>            break;<br/>    }<br/>}</pre></div>
<ol start="4">
<li><span>Next, we implement our two helper methods. The first, <kbd>HasBinaryContent</kbd>, as the name implies, takes the raw binary data and searches for non-text characters to ensure it is a binary file. Secondly, we define <kbd>HasHeaderBytes</kbd>; this method takes an array of bytes, converts it into a <kbd>UTF8</kbd> string, and then checks to see whether the string matches the string passed in:</span></li>
</ol>
<div>
<pre style="padding-left: 60px">private static bool HasBinaryContent(Span&lt;byte&gt; fileContent) =&gt;<br/>            System.Text.Encoding.UTF8.GetString(fileContent.ToArray()).Any(a =&gt; char.IsControl(a) &amp;&amp; a != '\r' &amp;&amp; a != '\n');<br/><br/>private static bool HasHeaderBytes(Span&lt;byte&gt; data, string match) =&gt; System.Text.Encoding.UTF8.GetString(data) == match;</pre></div>
<ol start="5">
<li><span>Next, we add the properties used for prediction, training, and testing:</span></li>
</ol>
<div>
<pre style="padding-left: 60px">[ColumnName("Label")]<br/>public float Label { get; set; }<br/><br/>public float IsBinary { get; set; }<br/><br/>public float IsMZHeader { get; set; }<br/><br/>public float IsPKHeader { get; set; }</pre></div>
<ol start="6">
<li><span>Lastly, we override the <kbd>ToString</kbd> method to be used with the feature extraction:</span></li>
</ol>
<div>
<pre style="padding-left: 60px">public override string ToString() =&gt; $"{Label},{IsBinary},{IsMZHeader},{IsPKHeader}";</pre></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The FileTypePrediction class</h1>
                </header>
            
            <article>
                
<p>The <kbd>FileTypePrediction</kbd><strong> </strong>class contains the properties mapped to our prediction output. In k-means clustering, the <kbd>PredictedClusterId</kbd> property stores the closest cluster found. In addition to this, the <kbd>Distances</kbd> array contains the distances from the data point to each of the clusters:</p>
<div>
<pre>using Microsoft.ML.Data;<br/><br/>namespace chapter05.ML.Objects<br/>{<br/>    public class FileTypePrediction<br/>    {<br/>        [ColumnName("PredictedLabel")]<br/>        public uint PredictedClusterId;<br/><br/>        [ColumnName("Score")]<br/>        public float[] Distances;<br/>    }<br/>}</pre></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The FeatureExtractor class</h1>
                </header>
            
            <article>
                
<p>The <kbd>FeatureExtractor</kbd> class that we utilized in the logistic regression example from <a href="8bcfc000-9adc-4eda-a91a-e09f676eac85.xhtml">Chapter 3</a>, <em>Regression Model</em>, has been adapted to support both test and training data extraction:</p>
<ol>
<li>First,<span> we generalize the extraction to take the folder path and the output file. As noted earlier, we also pass in the filename, providing the <kbd>Labeling</kbd> to occur cleanly inside the <kbd>FileData</kbd> class:</span></li>
</ol>
<pre style="padding-left: 60px">private void ExtractFolder(string folderPath, string outputFile)<br/>{<br/>    if (!Directory.Exists(folderPath))<br/>    {<br/>        Console.WriteLine($"{folderPath} does not exist");<br/><br/>        return;<br/>    }<br/><br/>    var files = Directory.GetFiles(folderPath);<br/><br/>    using (var streamWriter =<br/>        new StreamWriter(Path.Combine(AppContext.BaseDirectory, $"../../../Data/{outputFile}")))<br/>    {<br/>        foreach (var file in files)<br/>        {<br/>            var extractedData = new FileData(File.ReadAllBytes(file), file);<br/><br/>            streamWriter.WriteLine(extractedData.ToString());<br/>        }<br/>    }<br/><br/>    Console.WriteLine($"Extracted {files.Length} to {outputFile}");<br/>}</pre>
<ol start="2">
<li>Lastly, we take the two parameters from the command line (called from the <kbd>Program</kbd> class) and simply call the preceding method a second time:</li>
</ol>
<div>
<pre style="padding-left: 60px">public void Extract(string trainingPath, string testPath)<br/>{<br/>    ExtractFolder(trainingPath, Constants.SAMPLE_DATA);<br/>    ExtractFolder(testPath, Constants.TEST_DATA);<br/>}</pre></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Predictor class</h1>
                </header>
            
            <article>
                
<p>There are a couple of changes in this class to handle the file type prediction scenario:</p>
<ol>
<li>First,<span> we add a helper method, <kbd>GetClusterToMap</kbd>, which maps known values to the prediction clusters. Note the use of <kbd>Enum.GetValues</kbd> here; as you add more file types, this method does not need to be modified</span>:</li>
</ol>
<div>
<pre style="padding-left: 60px">private Dictionary&lt;uint, FileTypes&gt; GetClusterToMap(PredictionEngineBase&lt;FileData, FileTypePrediction&gt; predictionEngine)<br/>{<br/>    var map = new Dictionary&lt;uint, FileTypes&gt;();<br/><br/>    var fileTypes = Enum.GetValues(typeof(FileTypes)).Cast&lt;FileTypes&gt;();<br/><br/>    foreach (var fileType in fileTypes)<br/>    {<br/>        var fileData = new FileData(fileType);<br/><br/>        var prediction = predictionEngine.Predict(fileData);<br/><br/>        map.Add(prediction.PredictedClusterId, fileType);<br/>    }<br/><br/>    return map;<br/>}         </pre></div>
<ol start="2">
<li>Next, we pass in the <kbd>FileData</kbd> and <kbd>FileTypePrediction</kbd> types into the <kbd>CreatePredictionEngine</kbd> method to create our prediction engine. Then, we read the file in as a binary file and pass these bytes into the constructor of <kbd>FileData</kbd> prior to running the prediction and mapping initialization:</li>
</ol>
<div>
<pre style="padding-left: 60px">var predictionEngine = MlContext.Model.CreatePredictionEngine&lt;FileData, FileTypePrediction&gt;(mlModel);<br/><br/>var fileData = new FileData(File.ReadAllBytes(inputDataFile));<br/><br/>var prediction = predictionEngine.Predict(fileData);<br/><br/>var mapping = GetClusterToMap(predictionEngine);</pre></div>
<ol start="3">
<li>L<span>astly, we need to adjust the output to match the output that a k-means prediction returns, including the Euclidean distances</span><span>:</span></li>
</ol>
<div>
<pre style="padding-left: 60px">Console.WriteLine(<br/>    $"Based on input file: {inputDataFile}{Environment.NewLine}{Environment.NewLine}" +<br/>    $"Feature Extraction: {fileData}{Environment.NewLine}{Environment.NewLine}" +<br/>    $"The file is predicted to be a {mapping[prediction.PredictedClusterId]}{Environment.NewLine}");<br/><br/>Console.WriteLine("Distances from all clusters:");<br/><br/>for (uint x = 0; x &lt; prediction.Distances.Length; x++) { <br/>    Console.WriteLine($"{mapping[x+1]}: {prediction.Distances[x]}");<br/>}</pre></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Trainer class</h1>
                </header>
            
            <article>
                
<p>Inside the <kbd>Trainer</kbd> class, several modifications need to be made to support k-means classification:</p>
<ol>
<li>The first change is the addition of a <kbd>GetDataView</kbd> helper method, which builds the <kbd>IDataView</kbd> object from the columns previously defined in the <kbd>FileData</kbd> class:</li>
</ol>
<div>
<pre style="padding-left: 60px">private IDataView GetDataView(string fileName)<br/>{<br/>    return MlContext.Data.LoadFromTextFile(path: fileName,<br/>        columns: new[]<br/>        {<br/>            new TextLoader.Column(nameof(FileData.Label), DataKind.Single, 0),<br/>            new TextLoader.Column(nameof(FileData.IsBinary), DataKind.Single, 1),<br/>            new TextLoader.Column(nameof(FileData.IsMZHeader), DataKind.Single, 2),<br/>            new TextLoader.Column(nameof(FileData.IsPKHeader), DataKind.Single, 3)<br/>        },<br/>        hasHeader: false,<br/>        separatorChar: ',');<br/>}</pre></div>
<ol start="2">
<li>We then build the data process pipeline, transforming the columns into a single <kbd>Features</kbd> column:</li>
</ol>
<div>
<pre style="padding-left: 60px">var trainingDataView = GetDataView(trainingFileName);<br/><br/>var dataProcessPipeline = MlContext.Transforms.Concatenate(<br/>    FEATURES,<br/>    nameof(FileData.IsBinary),<br/>    nameof(FileData.IsMZHeader),<br/>    nameof(FileData.IsPKHeader));</pre></div>
<ol start="3">
<li>We can then create the k-means trainer with a cluster size of 3 and create the model:</li>
</ol>
<div>
<pre style="padding-left: 60px">var trainer = MlContext.Clustering.Trainers.KMeans(featureColumnName: FEATURES, numberOfClusters: 3);<br/>var trainingPipeline = dataProcessPipeline.Append(trainer);<br/>var trainedModel = trainingPipeline.Fit(trainingDataView);<br/><br/>MlContext.Model.Save(trainedModel, trainingDataView.Schema, ModelPath);</pre></div>
<div class="packt_tip">The default value for the number of clusters is 5. An interesting experiment to run based either on this dataset or one modified by you is to see how the prediction results change by adjusting this value.</div>
<ol start="4">
<li>Now we evaluate the model we just trained using the testing dataset:</li>
</ol>
<pre style="padding-left: 60px">var testingDataView = GetDataView(testingFileName);<br/><br/>IDataView testDataView = trainedModel.Transform(testingDataView);<br/><br/>ClusteringMetrics modelMetrics = MlContext.Clustering.Evaluate(<br/>    data: testDataView,<br/>    labelColumnName: "Label",<br/>    scoreColumnName: "Score",<br/>    featureColumnName: FEATURES);</pre>
<ol start="5">
<li>Finally, we output all of the classification metrics, each of which we will detail in the next section:</li>
</ol>
<pre style="padding-left: 60px">Console.WriteLine($"Average Distance: {modelMetrics.AverageDistance}");<br/>Console.WriteLine($"Davies Bould Index: {modelMetrics.DaviesBouldinIndex}");<br/>Console.WriteLine($"Normalized Mutual Information: {modelMetrics.NormalizedMutualInformation}");</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Program class</h1>
                </header>
            
            <article>
                
<p>The <kbd>Program</kbd> class, as mentioned in previous chapters, is the main entry point for our application. The only change in the <kbd>Program</kbd> class is the help text to indicate usage for the extract to accept the test folder path for extraction:</p>
<pre>if (args.Length &lt; 2)<br/>{<br/>    Console.WriteLine($"Invalid arguments passed in, exiting.{Environment.NewLine}{Environment.NewLine}Usage:{Environment.NewLine}" +<br/>                      $"predict &lt;path to input file&gt;{Environment.NewLine}" +<br/>                      $"or {Environment.NewLine}" +<br/>                      $"train &lt;path to training data file&gt; &lt;path to test data file&gt;{Environment.NewLine}" +<br/>                      $"or {Environment.NewLine}" + $"extract &lt;path to training folder&gt; &lt;path to test folder&gt;{Environment.NewLine}");<br/><br/>    return;<br/>}</pre>
<p>Finally, we modify the <kbd>switch</kbd>/<kbd>case</kbd> statement to support the additional parameter to the <kbd>extract</kbd> method to support both the training and test datasets:</p>
<pre>switch (args[0])<br/>{<br/>    case "extract":<br/>        new FeatureExtractor().Extract(args[1], args[2]);<br/>        break;<br/>    case "predict":<br/>        new Predictor().Predict(args[1]);<br/>        break;<br/>    case "train":<br/>        new Trainer().Train(args[1], args[2]);<br/>        break;<br/>    default:<br/>        Console.WriteLine($"{args[0]} is an invalid option");<br/>        break;<br/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running the application</h1>
                </header>
            
            <article>
                
<p><span>To run the application, the process is nearly identical to <a href="8bcfc000-9adc-4eda-a91a-e09f676eac85.xhtml">Chapter 3</a>, <em>Regression Model</em>'s example application with the addition of passing in the test dataset when training:</span></p>
<ol>
<li>To run the training on the command line as we did in previous chapters, simply pass in the following command (assuming you have added two sets of files; one each for your training and test sets):</li>
</ol>
<div>
<pre style="padding-left: 60px"><strong>PS chapter05\bin\Debug\netcoreapp3.0&gt; .\chapter05.exe extract ..\..\..\TrainingData\ ..\..\..\TestData\</strong><br/>Extracted 80 to sampledata.csv<br/>Extracted 30 to testdata.csv</pre></div>
<div class="packt_infobox">Included in the code repository are two pre-feature extracted files (<kbd>sampledata.csv</kbd> and t<kbd>estdata.csv</kbd>) to allow you to train a model without performing your own feature extraction.  If you would like to perform your own feature extraction, create a <kbd>TestData</kbd> and <kbd>TrainingData</kbd> folder.  Populate these folders with a sampling of <strong><span>PowerShell </span></strong>(<span><strong>PS1</strong></span>), <span><strong>Windows Executables</strong> </span>(<strong><span>EXE</span></strong>) and <span><strong>Microsoft Word documents</strong> </span>(<strong><span>DOCX</span></strong>).</div>
<ol start="2">
<li>After extracting the data, we must then train the model by passing in the newly created <kbd>sampledata.csv</kbd> and <kbd>testdata.csv</kbd> files:</li>
</ol>
<div>
<pre style="padding-left: 60px"><strong>PS chapter05\bin\Debug\netcoreapp3.0&gt; .\chapter05.exe train ..\..\..\Data\sampledata.csv ..\..\..\Data\testdata.csv</strong> <br/>Average Distance: 0<br/>Davies Bould Index: 0<br/>Normalized Mutual Information: 1</pre></div>
<ol start="3">
<li>To run the model with this file, simply pass in the filename to the built application (in this case, the compiled <kbd>chapter05.exe</kbd> is used) and the predicted output will show:</li>
</ol>
<div>
<pre style="padding-left: 60px"><strong>PS chapter05\bin\Debug\netcoreapp3.0&gt; .\chapter05.exe predict .\chapter05.exe</strong><br/>Based on input file: .\chapter05.exe<br/><br/>Feature Extraction: 0,1,1,0<br/><br/>The file is predicted to be a Executable<br/><br/>Distances from all clusters:<br/>Executable: 0<br/>Script: 2<br/>Document: 2</pre></div>
<p><span>Note the expanded output to include several metric data points—we will go through what each one of these means at the end of this chapter.</span></p>
<p>Feel free to modify the values and see how the prediction changes based on the dataset that the model was trained on. A few areas of experimentation from this point could include the following:</p>
<ul>
<li>Adding some additional features to increase the prediction accuracy</li>
<li>Adding additional file types to the clusters such as video or audio</li>
<li>Adding a new range of files to generate new sample and test data</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating a k-means model</h1>
                </header>
            
            <article>
                
<p>As discussed in previous chapters, evaluating a model is a critical part of the overall model-building process. A poorly trained model will only provide inaccurate predictions. Fortunately, ML.NET provides many popular attributes to calculate model accuracy based on a test set at the time of training to give you an idea of how well your model will perform in a production environment. </p>
<p>In ML.NET, as noted in the example application, there are three properties that comprise the <kbd>ClusteringMetrics</kbd> class object. Let's dive into the properties exposed in the <kbd>ClusteringMetrics</kbd> object:</p>
<ul>
<li>Average distance</li>
<li>The Davies-Bouldin index</li>
<li>Normalized mutual information</li>
</ul>
<p>In the next sections, we will break down how these values are calculated and the ideal values to look for.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Average distance</h1>
                </header>
            
            <article>
                
<p>Also referred to as the <strong>average score</strong> is the distance from the center of a cluster to the test data. The value, of type double, will decrease as the number of clusters increases, effectively creating clusters for the edge cases. In addition to this, a value of 0, such as the one found in our example, is possible when your features create distinct clusters. This means that, if you find yourself seeing poor prediction performance, you should increase the number of clusters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Davies-Bouldin Index</h1>
                </header>
            
            <article>
                
<p>The Davies-Bouldin Index is another measure for the quality of the clustering. Specifically, the Davies-Bouldin Index measures the scatter of cluster separation with values ranging from 0 to 1 (of type double), with a value of 0 being ideal (as was the case of our example).</p>
<div class="packt_tip">For more details on the Davies-Bouldin Index, specifically the math behind the algorithm, a good resource can be found here: <a href="https://en.wikipedia.org/wiki/Davies%E2%80%93Bouldin_index">https://en.wikipedia.org/wiki/Davies%E2%80%93Bouldin_index</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Normalized mutual information</h1>
                </header>
            
            <article>
                
<p>The normalized mutual information metric is used to measure the mutual dependence of the feature variables.</p>
<p>The range of values is from 0 to 1 (the type is of double)—closer to or equal to 1 is ideal, akin to the model we trained earlier in this chapter.</p>
<div class="packt_tip">For more details on normalized mutual information along with the math behind the algorithm, please read <a href="http://en.wikipedia.org/wiki/Mutual_information#Normalized_variants">http://en.wikipedia.org/wiki/Mutual_information#Normalized_variants</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Over the course of this chapter, we dove into ML.NET's clustering support via the k-means clustering algorithm. We have also created and trained our first clustering application using k-means to predict what file type a file is. Lastly, we dove into how to evaluate a k-means clustering model and the various properties that ML.NET exposes to achieve a proper evaluation of a k-means clustering model.</p>
<p>In the next chapter, we will deep dive into anomaly detection algorithms with ML.NET by creating a login anomaly predictor.</p>


            </article>

            
        </section>
    </body></html>