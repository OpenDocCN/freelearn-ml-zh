<html><head></head><body>
		<div id="_idContainer092">
			<h1 id="_idParaDest-91"><em class="italic"><a id="_idTextAnchor092"/>Chapter 5</em>: AWS Services for Data Storing</h1>
			<p>AWS provides a wide range of services to store your data safely and securely. There are various storage options available on AWS such as block storage, file storage, and object storage. It is expensive to manage on-premises data storage due to the higher investment in hardware, admin overheads, and managing system upgrades. With AWS Storage services, you just pay for what you use, and you don't have to manage the hardware. We will also learn about various storage classes offered by Amazon S3 for intelligent access of the data and reducing costs. You can expect questions in the exam on storage classes. As we continue in this chapter, we will master the single-AZ and multi-AZ instances, and <strong class="bold">RTO</strong> (<strong class="bold">Recovery</strong> <strong class="bold">Time</strong> <strong class="bold">Objective</strong>) and <strong class="bold">RPO</strong> (<strong class="bold">Recovery</strong> <strong class="bold">Point</strong> <strong class="bold">Objective</strong>) concepts of Amazon RDS. </p>
			<p>In this chapter, we will learn about storing our data securely for further analytics purposes by means of the following sections:</p>
			<ul>
				<li>Storing data on Amazon S3</li>
				<li>Controlling access on S3 buckets and objects</li>
				<li>Protecting data on Amazon S3</li>
				<li>Securing S3 objects at rest and in transit</li>
				<li>Using other types of data stores </li>
				<li>Relational Database Services (RDSes)</li>
				<li>Managing Failover in Amazon RDS</li>
				<li>Taking automatic backup, RDS snapshots, and restore and read replicas</li>
				<li>Writing to Amazon Aurora with multi-master capabilities</li>
				<li>Storing columnar data on Amazon Redshift</li>
				<li>Amazon DynamoDB for NoSQL databases as a service</li>
			</ul>
			<h1 id="_idParaDest-92"><a id="_idTextAnchor093"/>Technical requirements</h1>
			<p>All you will need for this chapter is an AWS account and the AWS CLI configured. The steps to configure the AWS CLI for your account are explained in detail by Amazon here: <a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html">https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html</a>.</p>
			<p>You can download the code examples from Github, here: <a href="https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-5/">https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-5/</a>.</p>
			<h1 id="_idParaDest-93"><a id="_idTextAnchor094"/>Storing data on Amazon S3</h1>
			<p><strong class="bold">S3</strong> is Amazon's <a id="_idIndexMarker335"/>cloud-based object storage service and it can be <a id="_idIndexMarker336"/>accessed from anywhere via the internet. It is an ideal storage option for large datasets. It is region-based, as your data is stored in a particular region until you move the data to a different region. Your data will never leave that region until it is configured. In a particular region, data is replicated in the availabi<a id="_idTextAnchor095"/>lity zones of that region; this makes S3 regionally resilient. If any of the availability zones fail in a region, then other availability zones will serve your requests. It can be accessed via the AWS Console UI, AWS CLI, or AWS API requests, or via standard HTTP methods. </p>
			<p>S3 has two main components: <strong class="bold">buckets</strong> and <strong class="bold">objects</strong>. </p>
			<ul>
				<li>Buckets <a id="_idIndexMarker337"/>are created in a specific AWS region. Buckets <a id="_idIndexMarker338"/>can contain objects, but cannot contain other buckets. </li>
				<li>Objects have two <a id="_idIndexMarker339"/>main attributes. One is <strong class="bold">Key</strong>, and the <a id="_idIndexMarker340"/>other is <strong class="bold">Value</strong>. Value is <a id="_idIndexMarker341"/>the content being stored, and Key is the name. The maximum size of an object can be 5 TB. As per the Amazon S3 documentation available here, <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingObjects.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingObjects.html</a>, objects also have a version ID, metadata, access control information, and subresources. <p class="callout-heading">Important note</p><p class="callout">As per Amazon's docs, S3 provides read-after-write consistency for PUTS of new objects, which means that if you put a new object or create a new object and you immediately turn around to read the object using its key, then you get the exact data that you just uploaded. However, for overwrites and deletes, it behaves in an <strong class="bold">eventually consistent manner</strong>. This means that if you read an object straight after the delete or overwrite operation, then you may read an old copy or a stale version of the object. It takes some time to replicate the content of the object across three availability zones.</p></li>
			</ul>
			<p>A folder structure can be maintained logically by using a prefix. Let's take an example where an image is <a id="_idIndexMarker342"/>uploaded into a bucket, <strong class="source-inline">bucket-name-example</strong>, with the prefix <strong class="source-inline">folder-name</strong> and the object name as <strong class="source-inline">my-image.jpg</strong>. The entire structure looks like this: <strong class="source-inline">/bucket-name-example/folder-name/my-image.jpg</strong>.</p>
			<p>The content of the object can be read by using the bucket name as <strong class="source-inline">bucket-name-example</strong> and the key as <strong class="source-inline">/folder-name/my-image.jpg</strong>.</p>
			<p>There are several storage classes offered by Amazon for the objects being stored in S3:</p>
			<ul>
				<li><strong class="bold">Standard Storage</strong>: This is the <a id="_idIndexMarker343"/>storage class for frequently <a id="_idIndexMarker344"/>accessed objects and for quick access. S3 Standard has a millisecond first byte latency and objects can be made publicly available. </li>
				<li><strong class="bold">Reduced Redundancy (RR)</strong>: This <a id="_idIndexMarker345"/>option provides less redundancy <a id="_idIndexMarker346"/>than the Standard Storage class. Non-critical and reproducible data can be stored in this class. AWS S3 documentation suggests not to use this class as the Standard Storage class is more cost-effective. </li>
				<li><strong class="bold">Standard Infrequent Access (IA)</strong>: This option is <a id="_idIndexMarker347"/>used when you need data to be returned quickly, but <a id="_idIndexMarker348"/>not for frequent access. The object size has to be a minimum of 128 KB. The minimum storage timeframe is 30 days. If the object is deleted before 30 days, you are still charged for 30 days. <strong class="bold">Standard IA</strong> objects are resilient to the loss of availability zones.</li>
				<li><strong class="bold">One Zone Infrequent Access</strong>: The object <a id="_idIndexMarker349"/>of this storage class is <a id="_idIndexMarker350"/>stored in just one availability zone, which makes it cheaper than <strong class="bold">Standard IA</strong>. The minimum object size and storage timeframe are the same as <strong class="bold">Standard IA</strong>. Objects from this storage class are less available and less resilient. This storage class is used when you have another copy, or if the data can be recreated. A <strong class="bold">One Zone IA</strong> storage class should <a id="_idIndexMarker351"/>be used for long-lived data that is non-critical and replaceable, and where access is infrequent.</li>
				<li><strong class="bold">Glacier</strong>: This option is used for long-term <a id="_idIndexMarker352"/>archiving and backup. It can take anything <a id="_idIndexMarker353"/>from minutes to hours to retrieve objects in this storage class. The minimum storage timeframe is 90 days. You cannot use the Amazon Glacier API to access the objects moved from S3 to Glacier as part of object life cycle management.</li>
				<li><strong class="bold">Glacier Deep Archive</strong>: The minimum <a id="_idIndexMarker354"/>storage duration of this <a id="_idIndexMarker355"/>class is 180 days. This is the least expensive storage class and has a default retrieval time of 12 hours.</li>
				<li><strong class="bold">Intelligent Tiering</strong>: This storage class <a id="_idIndexMarker356"/>is designed to reduce operational overheads. Users pay a <a id="_idIndexMarker357"/>monitoring fee and AWS selects a storage class between Standard (a frequent access tier) and Standard IA (a lower cost infrequent access tier) based on the access pattern of an object. This option is designed for long-lived data with unknown or unpredictable access patterns.</li>
			</ul>
			<p>Through <a id="_idIndexMarker358"/>sets of rules, the transition between storage classes and deletion of the objects can be managed easily, and are <a id="_idIndexMarker359"/>referred to as <strong class="bold">S3 Lifecycle Configurations</strong>. These rules consist of actions. These can be applied to a bucket or a group of objects in that bucket defined by prefixes or tags. Actions can either be <strong class="bold">Transition actions</strong> or <strong class="bold">Expiration actions</strong>. Transition actions <a id="_idIndexMarker360"/>define the storage class transition of the objects following the creation of <em class="italic">a user-defined</em> number of days. Expiration <a id="_idIndexMarker361"/>actions configure the deletion of versioned objects, or the deletion of delete markers or incomplete multipart uploads. This is very useful for managing costs.</p>
			<p>An illustration is given in <em class="italic">Figure 5.1</em>. You can find more details available here: <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html</a>: </p>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="image/B16735_05_001.jpg" alt="Figure 5.1 – A comparison table of S3 Storage classes&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.1 – A comparison table of S3 Storage classes</p>
			<h2 id="_idParaDest-94"><a id="_idTextAnchor096"/>Creating buckets to hold data</h2>
			<p>Now, let's <a id="_idIndexMarker362"/>see how to create a bucket, upload <a id="_idIndexMarker363"/>an object, and read the object using the AWS CLI. </p>
			<ol>
				<li>In the first step, we will check whether we have any buckets created by using the <strong class="source-inline">aws s3 ls</strong> command: <p class="source-code"><strong class="bold">$ pwd</strong></p><p class="source-code"><strong class="bold">/Users/baba/AWS-Certified-Machine-Learning-Specialty-</strong></p><p class="source-code"><strong class="bold">2020-Certification-Guide/Chapter-5/s3demo/demo-files</strong></p><p class="source-code"><strong class="bold">$ aws s3 ls</strong></p></li>
				<li>This command returns nothing here. So, we will create a bucket now by using the <strong class="source-inline">mb</strong> argument. Let's say the bucket name is <strong class="source-inline">demo-bucket-baba</strong> in the <strong class="source-inline">us-east-1</strong> region:<p class="source-code"><strong class="bold">$ aws s3 mb s3://demo-bucket-baba --region us-east-1</strong></p><p class="source-code"><strong class="bold">make_bucket: demo-bucket-baba</strong></p><p class="source-code"><strong class="bold">$ aws s3 ls</strong></p><p class="source-code"><strong class="bold">2020-11-04 14:39:50 demo-bucket-baba</strong></p></li>
				<li>As we have created <a id="_idIndexMarker364"/>a bucket now, our next <a id="_idIndexMarker365"/>step is to copy a file to our bucket using the <strong class="source-inline">cp</strong> argument, as shown in the following code:<p class="source-code"><strong class="bold">$ aws s3 cp sample-file.txt s3://demo-bucket-baba/</strong></p><p class="source-code"><strong class="bold">upload: ./sample-file.txt to s3://demo-bucket-</strong></p><p class="source-code"><strong class="bold">baba/sample-file.txt</strong></p></li>
				<li>To validate the file upload operation via the AWS console, please log in to your AWS account and go to the AWS S3 console to see the same. The AWS S3 console lists the result as shown in <em class="italic">Figure 5.2</em>. The console may have changed by the time you're reading this book!<div id="_idContainer090" class="IMG---Figure"><img src="image/B16735_05_002.jpg" alt="Figure 5.2 – AWS S3 screenshot to list your files&#13;&#10;"/></div><p class="figure-caption">Figure 5.2 – AWS S3 screenshot to list your files</p><p>You <a id="_idIndexMarker366"/>can also <a id="_idIndexMarker367"/>list the files in your S3 bucket from the command line, as shown here:</p><p class="source-code"><strong class="bold">$ aws s3 ls s3://demo-bucket-baba/</strong></p><p class="source-code"><strong class="bold">2020-11-04 14:50:02         99 sample-file.txt</strong></p><ul><li>If there is a scenario for uploading your filesystem directories and files to the S3 bucket, then <strong class="source-inline">--recursive</strong> will do the job for you:<p class="source-code"><strong class="bold">$ aws s3 cp . s3://demo-bucket-baba/ --recursive</strong></p><p class="source-code"><strong class="bold">upload: folder-1/a.txt to s3://demo-bucket-baba/folder-1/a.txt</strong></p><p class="source-code"><strong class="bold">upload: folder-2/sample-image.jpg to s3://demo-bucket-baba/folder-2/sample-image.jpg</strong></p><p class="source-code"><strong class="bold">upload: ./sample-file.txt to s3://demo-bucket-baba/sample-file.txt</strong></p><p class="source-code"><strong class="bold">$ aws s3 ls s3://demo-bucket-baba/</strong></p></li><li>The contents of one bucket can be copied/moved to another bucket via the <strong class="source-inline">cp </strong>command and the <strong class="source-inline">--recursive</strong> parameter. To achieve this, you will have to create <a id="_idIndexMarker368"/>two buckets, <strong class="source-inline">demo-bucket-baba-copied</strong> and <strong class="source-inline">demo-bucket-baba-moved</strong>. The steps are <a id="_idIndexMarker369"/>as follows:<p class="source-code"><strong class="bold">$ aws s3 mb s3://demo-bucket-baba-copied --region us-east-2</strong></p><p class="source-code"><strong class="bold">$ aws s3 mb s3://demo-bucket-baba-moved --region us-east-2</strong></p><p class="source-code"><strong class="bold">$ aws s3 cp s3://demo-bucket-baba s3://demo-bucket-baba-copied/ --recursive</strong></p><p class="source-code"><strong class="bold">$ aws s3 mv s3://demo-bucket-baba s3://demo-bucket-baba-moved/ --recursive</strong></p><p class="source-code"><strong class="bold">$ aws s3 ls </strong></p><p class="source-code"><strong class="bold">2020-11-04 14:39:50 demo-bucket-baba</strong></p><p class="source-code"><strong class="bold">2020-11-04 15:44:28 demo-bucket-baba-copied</strong></p><p class="source-code"><strong class="bold">2020-11-04 15:44:37 demo-bucket-baba-moved</strong></p><p class="source-code"><strong class="bold">$ aws s3 ls s3://demo-bucket-baba/</strong></p><p>If all the commands are run successfully, then the original bucket should be empty at the end (as all the files have now been moved). </p><p class="callout-heading">Note</p><p class="callout">In the certification exam, you will not find many questions on bucket- and object-level operations. However, it is always better to know the basic operations and the required steps. </p></li></ul></li>
				<li>The buckets must be deleted to avoid costs as soon as the hands-on is finished. The bucket has to be empty before you supply the <strong class="source-inline">rb</strong> command:<p class="source-code"><strong class="bold">$ aws s3 rb s3://demo-bucket-baba</strong></p><p class="source-code"><strong class="bold">$ aws s3 rb s3://demo-bucket-baba-moved</strong></p><p class="source-code"><strong class="bold">remove_bucket failed: s3://demo-bucket-baba-moved An error occurred (BucketNotEmpty) when calling the DeleteBucket operation: The bucket you tried to delete is not empty</strong></p></li>
				<li>The <strong class="source-inline">demo-bucket-baba-moved</strong> bucket is not empty, so we couldn't remove the bucket. In such scenarios, use the <strong class="source-inline">--force</strong> parameter to delete the entire bucket and all its contents, as shown here:<p class="source-code"><strong class="bold">$ aws s3 rb s3://demo-bucket-baba-moved --force</strong></p><p class="source-code"><strong class="bold">$ aws s3 rb s3://demo-bucket-baba-copied--force</strong></p><p>If <a id="_idIndexMarker370"/>you want to <a id="_idIndexMarker371"/>delete all the content from a specific prefix inside a bucket using the CLI, then it is easy to use the <strong class="source-inline">rm</strong> command with the <strong class="source-inline">--recursive</strong> parameter. </p></li>
				<li>Let's take an example of a bucket, <strong class="source-inline">test-bucket</strong>, that has a prefix, <strong class="source-inline">images</strong>. This prefix contains four image files named <strong class="source-inline">animal.jpg</strong>, <strong class="source-inline">draw-house.jpg</strong>, <strong class="source-inline">cat.jpg</strong>, and <strong class="source-inline">human.jpg</strong>.</li>
				<li>Now, to delete the contents inside the images, the command will be as follows: <strong class="bold">aws s3 rm s3://test-bucket/images –recursive</strong></li>
				<li>The bucket should now be empty.</li>
			</ol>
			<p>In the next section, we are going to learn about object tags and object metadata. </p>
			<h2 id="_idParaDest-95"><a id="_idTextAnchor097"/>Distinguishing between object tags and object metadata</h2>
			<p>Let's <a id="_idIndexMarker372"/>compare these two <a id="_idIndexMarker373"/>terms:</p>
			<ul>
				<li><strong class="bold">Object Tag</strong>: An object tag is a <strong class="bold">key-value</strong> pair. AWS S3 object tags can help you filter analytics <a id="_idIndexMarker374"/>and metrics, categorize storage, secure objects based on certain categorizations, track costs based on certain categorization of objects, and much more besides. Object tags can be used to create life cycle rules to move objects to cheaper storage tiers. You can have a maximum of 10 tags added to an object and 50 tags to a bucket. A tag key can contain 128 Unicode characters, while a tag value can contain 256 Unicode characters. </li>
				<li><strong class="bold">Object Metadata</strong>: Object metadata <a id="_idIndexMarker375"/>is descriptive data describing an object. It consists of <strong class="bold">name-value</strong> pairs. Object metadata is returned as HTTP headers on objects. They are of two types: one is <strong class="bold">System metadata</strong>, and the other is <strong class="bold">User-defined metadata</strong>. User-defined metadata is a custom name-value pair added to an object by the user. The name must begin with <strong class="bold">x-amz-meta</strong>. You can change all system metadata such as storage class, versioning, and encryption attributes on an object. Further details are available here: <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html</a>.<p class="callout-heading">Important note</p><p class="callout">Metadata names are case-insensitive, whereas tag names are case-sensitive.</p></li>
			</ul>
			<p>In the next section, we are going to learn about controlling access to buckets and objects on Amazon S3 through different policies, including the resource policy and the identity policy.</p>
			<h1 id="_idParaDest-96"><a id="_idTextAnchor098"/>Controlling access to buckets and objects on Amazon S3 </h1>
			<p>Once the <a id="_idIndexMarker376"/>object is stored <a id="_idIndexMarker377"/>in the bucket, the next <a id="_idIndexMarker378"/>major step is to manage <a id="_idIndexMarker379"/>access. S3 is private by default, and access is given to other users or groups or resources via <a id="_idIndexMarker380"/>several methods. This means that access to the objects can be managed via <strong class="bold">Access Control Lists</strong> (<strong class="bold">ACLs</strong>), <strong class="bold">Public Access Settings</strong>, <strong class="bold">Identity Policies</strong>, and <strong class="bold">Bucket Policies</strong>. </p>
			<p>Let's look at some of these in detail.</p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor099"/>S3 bucket policy</h2>
			<p><strong class="bold">S3 bucket policy</strong> is a resource <a id="_idIndexMarker381"/>policy that is attached to a bucket. Resource policies decide who can access that resource. It differs from identity policies in that identity policies can be attached or assigned to the identities inside an account, whereas resource policies can control identities from the same account or different accounts. Resource policies control anonymous principals too, which means an object can be made public through resource policies. The following sample policy allows everyone in the world to read the bucket, because <strong class="source-inline">Principal</strong> is rendered <strong class="source-inline">*</strong>:</p>
			<p class="source-code">{</p>
			<p class="source-code">  "Version":"2012-10-17"</p>
			<p class="source-code">  "Statement":[</p>
			<p class="source-code">    {</p>
			<p class="source-code">      "Sid":"AnyoneCanRead",</p>
			<p class="source-code">      "Effect":"Allow",</p>
			<p class="source-code">      "Principal":"*",</p>
			<p class="source-code">      "Action":["s3:GetObject"],</p>
			<p class="source-code">      "Resource":["arn:aws:s3:::my-bucket/*"]</p>
			<p class="source-code">    }</p>
			<p class="source-code">    ]</p>
			<p class="source-code">}</p>
			<p>By default, everything in S3 is private to the owner. If we want to make a prefix public to the world, then Resource changes to <strong class="source-inline">arn:aws:s3:::my-bucket/some-prefix/*</strong>, and similarly, if it is intended for a specific IAM user or IAM group, then those details go to the principal part in the policy. </p>
			<p>There can be conditions added to the bucket policy too. Let's examine a use case where the organization wants <a id="_idIndexMarker382"/>to keep a bucket public and whitelist particular IP addresses. The policy would look something like this:</p>
			<p class="source-code">{ </p>
			<p class="source-code">  "Version":"2012-10-17" </p>
			<p class="source-code">  "Statement":[ </p>
			<p class="source-code">    { </p>
			<p class="source-code">      "Sid":"ParticularIPRead", </p>
			<p class="source-code">      "Effect":"Allow", </p>
			<p class="source-code">      "Principal":"*", </p>
			<p class="source-code">      "Action":["s3:GetObject"], </p>
			<p class="source-code">      "Resource":["arn:aws:s3:::my-bucket/*"], </p>
			<p class="source-code">      "Condition":{</p>
			<p class="source-code">        "NotIpAddress":{"aws:SourceIp":"2.3.3.6/32"}</p>
			<p class="source-code">      }</p>
			<p class="source-code">    } </p>
			<p class="source-code">    ] </p>
			<p class="source-code">}</p>
			<p>More examples are available in the AWS S3 developer guide, which is available here: <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html</a>.</p>
			<p><strong class="bold">Block public access</strong> is a separate setting <a id="_idIndexMarker383"/>given to the bucket owner to avoid any kind of mistakes in bucket policy. In a real-world scenario, the bucket can be made public through bucket policy by mistake; to avoid such mistakes, or data leaks, AWS has provided this setting. It provides a further level of security, irrespective of the bucket policy. You can choose this while creating a bucket, or it can be set after creating a bucket.</p>
			<p><strong class="bold">Identity policies</strong> are meant for IAM users and IAM roles. <strong class="bold">Identity policies</strong> are validated when an IAM <a id="_idIndexMarker384"/>identity (user or role) requests to access a resource. All requests are denied by default. If an identity intends to access any services, resources, or actions, then access must be provided explicitly through <strong class="bold">identity policies</strong>. The example policy that follows can be attached to an IAM user and the IAM user will be allowed to have full RDS access within a specific region (<strong class="source-inline">us-east-1</strong> in this example):</p>
			<p class="source-code">{</p>
			<p class="source-code">    "Version": "2012-10-17",</p>
			<p class="source-code">    "Statement": [</p>
			<p class="source-code">        {</p>
			<p class="source-code">            "Effect": "Allow",</p>
			<p class="source-code">            "Action": "rds:*",</p>
			<p class="source-code">            "Resource": ["arn:aws:rds:us-east-1:*:*"]</p>
			<p class="source-code">        },</p>
			<p class="source-code">        {</p>
			<p class="source-code">            "Effect": "Allow",</p>
			<p class="source-code">            "Action": ["rds:Describe*"],</p>
			<p class="source-code">            "Resource": ["*"]</p>
			<p class="source-code">        }</p>
			<p class="source-code">    ]</p>
			<p class="source-code">}</p>
			<p><strong class="bold">ACLs</strong> are used to <a id="_idIndexMarker385"/>grant high-level permissions, typically for granting access to other AWS accounts. ACLs are one of the <strong class="bold">Subresources</strong> of a bucket or an object. A bucket or object can be made public quickly via ACLs. AWS doesn't suggest doing this, and you shouldn't expect questions about this on the test. It is good to know about this, but it is not as flexible as the <strong class="bold">S3 bucket policy</strong>.</p>
			<p>Now, let's learn about the methods to protect our data in the next section.</p>
			<h1 id="_idParaDest-98"><a id="_idTextAnchor100"/>Protecting data on Amazon S3</h1>
			<p>In this section, we <a id="_idIndexMarker386"/>will learn how to record every version of an <a id="_idIndexMarker387"/>object. Along with durability, Amazon provides several techniques to secure the data in S3. Some of those techniques involve enabling versioning and encrypting the objects. </p>
			<p>Versioning helps you to roll back to a previous version if there is any problem with the current object during update, delete, or put operations. </p>
			<p>Through encryption, you can control the access of an object. You need the appropriate key to read and write an object. We <a id="_idIndexMarker388"/>will also learn <strong class="bold">Multi Factor Authentication (MFA)</strong> for delete operations. Amazon also allows <strong class="bold">Cross-Region Replication (CRR)</strong> to maintain <a id="_idIndexMarker389"/>a copy of an object in another region, which can be used for data backup during any disaster, for further redundancy, or for the enhancement of data access speed in different regions.</p>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor101"/>Applying bucket versioning</h2>
			<p>Let's now <a id="_idIndexMarker390"/>understand how we can enable <a id="_idIndexMarker391"/>bucket versioning with the help of some hands-on examples. Bucket versioning can be applied while creating a bucket from the AWS S3 console: </p>
			<ol>
				<li value="1">To enable versioning on a bucket from the command line, a bucket must be created first and then versioning can be enabled, as shown in the following example. In this example, I have created a bucket, <strong class="source-inline">version-demo-mlpractice</strong>, and enabled versioning through the <strong class="source-inline">put-bucket-versioning</strong> command:<p class="source-code"><strong class="bold">$ aws s3 mb s3://version-demo-mlpractice/</strong></p><p class="source-code"><strong class="bold">$ aws s3api put-bucket-versioning --bucket version-demo-mlpractice --versioning-configuration Status=Enabled</strong></p><p class="source-code"><strong class="bold">$ aws s3api get-bucket-versioning --bucket version-demo-mlpractice</strong></p><p class="source-code"><strong class="bold">{</strong></p><p class="source-code"><strong class="bold">    "Status": "Enabled"</strong></p><p class="source-code"><strong class="bold">}</strong></p></li>
				<li>We have not created this bucket with any kind of encryption. So, if you run <strong class="source-inline">aws s3api get-bucket-encryption --bucket version-demo-mlpractice</strong>, then it will <a id="_idIndexMarker392"/>output an error that says the <a id="_idIndexMarker393"/>following: <p class="source-code"><strong class="bold">The server side encryption configuration was not found</strong></p></li>
				<li><strong class="bold">Server-Side Encryption</strong> (<strong class="bold">SSE</strong>) can be applied <a id="_idIndexMarker394"/>from the AWS S3 console while creating a bucket. This is called bucket default encryption. You can also apply SSE via the command line using the <strong class="source-inline">put-bucket-encryption</strong> API. The command will look like this:<p class="source-code"><strong class="bold">$ aws s3api put-bucket-encryption --bucket version-demo-mlpractice --server-side-encryption-configuration '{"Rules":[{"ApplyServerSideEncryptionByDefault":</strong></p><p class="source-code"><strong class="bold">{"SSEAlgorithm":"AES256"}}]}'</strong></p></li>
				<li>The same can be verified using the following command: <strong class="source-inline">aws s3api get-bucket-encryption --bucket version-demo-mlpractice</strong>.</li>
			</ol>
			<p>We will learn more about encryption in the next section.</p>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor102"/>Applying encryption to buckets</h2>
			<p>You also <a id="_idIndexMarker395"/>need to understand how <a id="_idIndexMarker396"/>enabling versioning on a bucket would help. There are use cases where a file is updated regularly, and versions will be created for the same file. To simulate this scenario, try the following example:</p>
			<ol>
				<li value="1">In this example, we will create a file with versions written in it. We will overwrite it and retrieve it to check the versions in that file:<p class="source-code"><strong class="bold">$ echo "Version-1"&gt;version-doc.txt</strong></p><p class="source-code"><strong class="bold">$ aws s3 cp version-doc.txt s3://version-demo-mlpractice</strong></p><p class="source-code"><strong class="bold">$ aws s3 cp s3://version-demo-mlpractice/version-doc.txt </strong></p><p class="source-code"><strong class="bold">check.txt</strong></p><p class="source-code"><strong class="bold">$ cat check.txt</strong></p><p class="source-code"><strong class="bold">Version-1</strong></p><p class="source-code"><strong class="bold">$ echo "Version-2"&gt;version-doc.txt</strong></p><p class="source-code"><strong class="bold">$ aws s3 cp version-doc.txt s3://version-demo-mlpractice</strong></p><p class="source-code"><strong class="bold">$ aws s3 cp s3://version-demo-mlpractice/version-doc.txt </strong></p><p class="source-code"><strong class="bold">check.txt</strong></p><p class="source-code"><strong class="bold">$ cat check.txt</strong></p><p class="source-code"><strong class="bold">Version-2</strong></p></li>
				<li>Upon retrieval, we <a id="_idIndexMarker397"/>got the latest version of the file, in other words, <strong class="source-inline">Version-2</strong> in this case. To check each <a id="_idIndexMarker398"/>of the versions and the latest one of them, S3 provides the <strong class="source-inline">list-object-versions</strong> API, as shown here. From the JSON results, you can deduce the latest version:<p class="source-code">$ aws s3api list-object-versions </p><p class="source-code">--bucket version-demo-mlpractice</p><p class="source-code">{</p><p class="source-code">    "Versions": [</p><p class="source-code">        {</p><p class="source-code">            "ETag": </p><p class="source-code">"\"b6690f56ca22c410a2782512d24cdc97\"",</p><p class="source-code">            "Size": 10,</p><p class="source-code">            "StorageClass": "STANDARD",</p><p class="source-code">            "Key": "version-doc.txt",</p><p class="source-code">            "VersionId": </p><p class="source-code">"70wbLG6BMBEQhCXmwsriDgQoXafFmgGi",</p><p class="source-code">            "IsLatest": true,</p><p class="source-code">            "LastModified": "2020-11-07T15:57:05+00:00",</p><p class="source-code">            "Owner": {</p><p class="source-code">                "DisplayName": "baba",</p><p class="source-code">                "ID": "XXXXXXXXXXXX"</p><p class="source-code">            }</p><p class="source-code">        },</p><p class="source-code">        {</p><p class="source-code">            "ETag": "\"5022e6af0dd3d2ea70920438271b21a2\"",</p><p class="source-code">            "Size": 10,</p><p class="source-code">            "StorageClass": "STANDARD",</p><p class="source-code">            "Key": "version-doc.txt",</p><p class="source-code">            "VersionId": "f1iC.9L.MsP00tIb.sUMnfOEae240sIW",</p><p class="source-code">            "IsLatest": false,</p><p class="source-code">            "LastModified": "2020-11-07T15:56:27+00:00",</p><p class="source-code">            "Owner": {</p><p class="source-code">                "DisplayName": "baba",</p><p class="source-code">                "ID": " XXXXXXXXXXXX"</p><p class="source-code">            }</p><p class="source-code">        }</p><p class="source-code">    ]</p><p class="source-code">}</p></li>
				<li>There may be a situation where you have to roll back to the earlier version of the current object. In the preceding example, the latest one is <em class="italic">Version-2</em>. You can make any desired version the latest or current version by parsing the <em class="italic">VersionId</em> sub resource to the <strong class="source-inline">get-object</strong> API call and uploading that object again. The other way is to delete the current or latest version by passing <strong class="source-inline">versionId</strong> to the <strong class="source-inline">–version-id</strong> parameter in the <strong class="source-inline">delete-object</strong> API request. More details about the API are available here: <a href="https://docs.aws.amazon.com/cli/latest/reference/s3api/delete-object.html">https://docs.aws.amazon.com/cli/latest/reference/s3api/delete-object.html</a>.</li>
				<li>When you delete an object in a <a id="_idIndexMarker399"/>versioning-enabled bucket, it does not delete the object from the <a id="_idIndexMarker400"/>bucket. It just creates a marker called <strong class="bold">DeleteMarker</strong>. It looks like this:<p class="source-code"><strong class="bold">$ aws s3api delete-object --bucket version-demo-mlpractice --key version-doc.txt</strong></p><p class="source-code"><strong class="bold">{</strong></p><p class="source-code"><strong class="bold">    "DeleteMarker": true,</strong></p><p class="source-code"><strong class="bold">    "VersionId": "BKv_Cxixtm7V48MWqBO_KUkKbcOaH5JP"</strong></p><p class="source-code"><strong class="bold">}</strong></p></li>
				<li>This means that the object is not deleted. You can list it by using this command: <p class="source-code"><strong class="bold">aws s3api list-object-versions --bucket version-demo-mlpractice</strong></p></li>
				<li>Now the bucket has no objects as <strong class="source-inline">version-doc.txt</strong>, and you can verify this using the <strong class="source-inline">aws s3 ls</strong> command because that marker became the current version of the object with a new ID. If you try to retrieve an object that is deleted, which means a delete marker is serving the current version of the object, then you will get a <strong class="bold">404 error</strong>. Hence, the permanent deletion of an object in a versioning-enabled bucket can only be achieved by deleting the object using their version IDs against each version. If a situation arises to get the object back, then the same object can be retrieved by deleting the delete marker, <strong class="source-inline">VersionId</strong>, as shown in the following example commands. A simple delete request <strong class="bold">(without the version ID)</strong> will not delete the delete marker and create another delete marker with a unique version ID. So, it's possible to have multiple delete markers for the same object. It is important to note at this point that it will consume your storage and you will be billed for it:<p class="source-code"><strong class="bold">$ aws s3 ls s3://version-demo-mlpractice/</strong></p><p class="source-code"><strong class="bold">$ aws s3api delete-object --bucket version-demo-mlpractice --key version-doc.txt --version-id BKv_Cxixtm7V48MWqBO_KUkKbcOaH5JP</strong></p><p class="source-code"><strong class="bold">{</strong></p><p class="source-code"><strong class="bold">    "DeleteMarker": true,</strong></p><p class="source-code"><strong class="bold">    "VersionId": "BKv_Cxixtm7V48MWqBO_KUkKbcOaH5JP"</strong></p><p class="source-code"><strong class="bold">}</strong></p></li>
				<li>Upon <a id="_idIndexMarker401"/>listing the bucket now, the older objects <a id="_idIndexMarker402"/>can be seen:<p class="source-code"><strong class="bold">$ aws s3 ls s3://version-demo-mlpractice/</strong></p><p class="source-code"><strong class="bold">2020-11-07 15:57:05         10 version-doc.txt</strong></p><p>As we have already covered the exam topics and practiced most of the required concepts, we should delete the objects in the bucket and then delete the bucket to save on costs. This step deletes the versions of the object and, in turn, removes the object permanently.</p></li>
				<li>Here, the latest version is deleted by giving the version ID to it, followed by the other version ID:<p class="source-code"><strong class="bold">$ aws s3api delete-object --bucket version-demo-</strong><strong class="bold">mlpractice --key version-doc.txt --version-id 70wbLG6BMBEQhCXmwsriDgQoXafFmgGi</strong></p><p class="source-code"><strong class="bold">$ aws s3api delete-object --bucket version-demo-mlpractice --key version-doc.txt --version-id f1iC.9L.MsP00tIb.sUMnfOEae240sIW</strong></p><p class="source-code"><strong class="bold">$ aws s3api list-object-versions --bucket version-demo-mlpractice</strong></p><p>We can clearly see the empty bucket now. </p><p class="callout-heading">Important note</p><p class="callout">AWS best practices suggest adding another layer of protection through <strong class="bold">MFA delete</strong>. Accidental bucket deletions can be prevented, and the security of the objects in the bucket is ensured. MFA delete can be enabled or disabled via the console and CLI. As documented in AWS docs, MFA delete requires two forms of authentication together: your security credentials, and the concatenation of a valid serial number, a space, and the six-digit code displayed on an approved authentication device.</p></li>
			</ol>
			<p>CRR helps you to separate data between different geographical regions. A typical use case can be business-as-usual activities during a disaster. If a region goes down, then another region can <a id="_idIndexMarker403"/>support the users if CRR is enabled. This improves the availability of the data. Another use case is to <a id="_idIndexMarker404"/>reduce latency if the same data is used by another compute resource, such as EC2 or AWS Lambda being launched in another region. You can also use CRR to copy objects to another AWS account that belongs to a different owner. There are a few important points that are worth noting down for the certification exam:</p>
			<ul>
				<li>In order to use CRR, versioning has to be enabled on both the source and destination bucket. </li>
				<li>Replication is enabled on the source bucket by adding rules. As the source, either an entire bucket, or a prefix, or tags can be replicated. </li>
				<li>Encrypted objects can also be replicated by assigning an appropriate encryption key. </li>
				<li>The destination bucket can be in the same account or in another account. You can change the storage type and ownership of the object in the destination bucket.</li>
				<li>For CRR, an existing role can be chosen or a new IAM role can be created too. </li>
				<li>There can be multiple replication rules on the source bucket, with priority accorded to it. Rules with higher priority override rules with lower priority.</li>
				<li>When you add a replication rule, only new versions an object that are created after the rules are enabled get replicated.</li>
				<li>If versions are deleted from the source bucket, then they are not deleted from the destination bucket.</li>
				<li>When you delete an object <a id="_idIndexMarker405"/>from the source bucket, it creates a delete marker in said source bucket. That delete <a id="_idIndexMarker406"/>marker is not replicated to the destination bucket by S3. </li>
			</ul>
			<p>In the next section, we will cover the concept of securing S3 objects.</p>
			<h1 id="_idParaDest-101"><a id="_idTextAnchor103"/>Securing S3 objects at rest and in transit </h1>
			<p>In the previous <a id="_idIndexMarker407"/>section, we learned about bucket default encryption, which is <a id="_idIndexMarker408"/>completely different from object-level encryption. Buckets are not encrypted, whereas objects are. A question may arise here: <em class="italic">what is bucket default encryption?</em> We will learn these <a id="_idIndexMarker409"/>concepts in this <a id="_idIndexMarker410"/>section. Data during transmission can be protected by using <strong class="bold">Secure Socket Layer (SSL)</strong> or <strong class="bold">Transport Layer Security (TLS)</strong> for the transfer of the HTTPS requests. The next step is to protect the data, where the authorized person can encode and decode the data.</p>
			<p>It is possible to have different <a id="_idIndexMarker411"/>encryption settings on different objects <a id="_idIndexMarker412"/>in the same bucket. S3 supports <strong class="bold">Client-Side Encryption (CSE)</strong> and <strong class="bold">Server-Side Encryption (SSE)</strong> for objects at rest:</p>
			<ul>
				<li><strong class="bold">Client-Side Encryption</strong>: A client uploads the object to S3 via the S3 endpoint. In CSE, the data is <a id="_idIndexMarker413"/>encrypted by the client before uploading to S3. Although the transit between the user and the S3 endpoint happens in an encrypted channel, the data in the channel is already encrypted by the client and can't be seen. In transit, encryption takes place by default through HTTPS. So, AWS S3 stores the encrypted object and cannot read the data in any format at any point in time. In CSE, the client takes care of encrypting the object content. So, control stays with the client in terms of key management and the encryption-decryption process. This leads to a huge amount of CPU usage. S3 is only used for storage.</li>
				<li><strong class="bold">Server-Side Encryption</strong>: A client uploads the object to S3 via the S3 endpoint. Even though the <a id="_idIndexMarker414"/>data in transit is through an encrypted channel that uses HTTPS, the objects themselves are not encrypted inside the channel. Once the data hits S3, then it is encrypted by the S3 service. In SSE, you trust S3 to perform encryption-decryption, object storage, and key management. There are three types of SSE techniques available for S3 objects:<p>a) SSE-C</p><p>b) SSE-S3</p><p>c) SSE-KMS</p></li>
				<li><strong class="bold">Server-Side Encryption with Customer-Provided Keys (SSE-C)</strong>: With <strong class="bold">SSE-C</strong>, the user is <a id="_idIndexMarker415"/>responsible for the key that is used for encryption and decryption. S3 manages the encryption and decryption process. In CSE, the client handles the encryption-decryption process, but in SSE-C, S3 handles the cryptographic operations. This potentially decreases the CPU requirements for these processes. The only overhead here is to manage the keys. Ideally, when a user is doing a <strong class="source-inline">PUT</strong> operation, the user has to provide a key and an object to S3. S3 encrypts the object using the key provided and attaches the hash (a cipher text) to the object. As soon as the object is stored, S3 discards the encryption key. This generated hash is <a id="_idIndexMarker416"/>one-way and cannot be used to generate a new key. When the user provides a <strong class="source-inline">GET</strong> operation request along with the decryption key, the hash identifies whether the specific key was used for encryption. Then, S3 decrypts and discards the key. </li>
				<li><strong class="bold">Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)</strong>: With <strong class="bold">SSE-S3</strong>, AWS handles <a id="_idIndexMarker417"/>both the management of the key and the process of encryption and decryption. When the user uploads an object using a <strong class="source-inline">PUT</strong> operation, the user just provides the unencrypted object. S3 creates a master key to be used for the encryption process. No one can change anything on this master key as this is created, rotated internally, and managed by S3 end to end. This is a unique key for the object. It uses th<a id="_idTextAnchor104"/>e AES-256 algorithm by default.</li>
				<li><strong class="bold">Server-Side Encryption with Customer Master Keys stored in AWS Key Management Service (SSE-KMS)</strong>: <strong class="bold">AWS Key Management Service (KMS)</strong> manages the <strong class="bold">Customer Master Key (CMK)</strong>. AWS <a id="_idIndexMarker418"/>S3 collaborates with AWS <a id="_idIndexMarker419"/>KMS and generates <a id="_idIndexMarker420"/>an AWS-managed CMK. This is the default master key used for <strong class="bold">SSE-KMS</strong>. Every time an object is uploaded, S3 uses a dedicated key to encrypt that object and that <a id="_idIndexMarker421"/>key is a <strong class="bold">Data Encryption Key (DEK)</strong>. The DEK is generated by KMS using the CMK. S3 is provided with both a plain-text version and an encrypted version of the DEK. The plain-text version of DEK is used to encrypt the object and then discarded. The encrypted version of DEK is stored along with the encrypted object. When you're using SSE-KMS, it is not necessary to use the default CMK that is created by S3. You can create and use a customer managed CMK, which means you can control the permission on it as well as the rotation of the key material. So, if you have a regulatory board in your organization that is concerned with rotation of the key or the separation of roles between encryption users and decryption users, then SSE-KMS is the solution. Logging and auditing is also possible on SSE-KMS to track the API calls made against keys. </li>
				<li><strong class="bold">Bucket Default Encryption</strong>: If we set <a id="_idIndexMarker422"/>AES-256 while creating a bucket, or we enable it after creating a bucket, then SSE-S3 would be used when you don't set something at object level while performing a <strong class="source-inline">PUT</strong> operation. </li>
			</ul>
			<p>In the next section, we will learn about some of the data stores used with EC2 instances.</p>
			<h1 id="_idParaDest-102"><a id="_idTextAnchor105"/>Using other types of data stores</h1>
			<p><strong class="bold">Elastic Block Store (EBS)</strong> is used to <a id="_idIndexMarker423"/>create volumes <a id="_idIndexMarker424"/>in an availability zone. The <a id="_idIndexMarker425"/>volume can only be <a id="_idIndexMarker426"/>attached to an EC2 instance in the same availability <a id="_idIndexMarker427"/>zone. Amazon EBS provides both <strong class="bold">SSD (Solid State Drive)</strong> and <strong class="bold">HDD (Hard Disk Drive)</strong> types of volumes. For SSD-based volumes, the dominant <a id="_idIndexMarker428"/>performance attribute is <strong class="bold">IOPS</strong> (<strong class="bold">Input Output Per Second</strong>), and for HDD it is throughput, which is generally measured as MiB/s. The following table shown in <em class="italic">Figure 5.3</em> provides an overview of the different volumes and types:</p>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<img src="image/B16735_05_003.jpg" alt="Figure 5.3 – Different volumes and their use cases&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.3 – Different volumes and their use cases</p>
			<p><strong class="bold">EBS</strong> is resilient to an <a id="_idIndexMarker429"/>availability zone (AZ). If, for some reason, an AZ fails, then the volume cannot be accessed. To avoid such scenarios, <strong class="bold">snapshots</strong> can be created from the EBS volumes and snapshots are stored in S3. Once the snapshot arrives at S3, the data in the snapshot becomes region-resilient. The first snapshot is a full copy of data on the volume and, from then onward, snapshots are incremental. Snapshots can be used to clone a volume. As the snapshot is stored in S3, a volume can be cloned in any AZ in that region. Snapshots can be shared between regions and volumes can be cloned from them during disaster recovery.</p>
			<p>AWS KMS manages <a id="_idIndexMarker430"/>the CMK. AWS KMS uses an AWS-managed CMK for EBS, or AWS KMS can use a customer-managed CMK. CMK is used by EBS when an encrypted volume is created. CMK is used to create an encrypted DEK, which is stored with the volume on the physical disk. This DEK can only be decrypted using KMS, assuming the entity has access to decrypt. When a snapshot is created from the encrypted volume, the snapshot is encrypted with the same DEK. Any volume created from this snapshot also uses that DEK.</p>
			<p><strong class="bold">Instance Store</strong> volumes are the <a id="_idIndexMarker431"/>block storage devices <a id="_idIndexMarker432"/>physically connected to the EC2 instance. They provide the highest <a id="_idIndexMarker433"/>performance, as the <strong class="bold">ephemeral storage</strong> attached to the instance is from the same host where the instance is launched. EBS can be attached to the instance at any time, but the instance store must be attached to the instance at the time of its <a id="_idIndexMarker434"/>launch; it cannot be attached later, once the instance is launched. If there is an issue on the underlying host of an EC2 instance, then the same instance will be launched on another host with a new instance store volume and the earlier instance store (ephemeral storage) and older data is lost. The size and capabilities of the attached volumes depend on the instance types and can be found in more detail here: <a href="https://aws.amazon.com/ec2/instance-types/">https://aws.amazon.com/ec2/instance-types/</a>.</p>
			<p><strong class="bold">Elastic File System (EFS)</strong> provides a <a id="_idIndexMarker435"/>network-based filesystem <a id="_idIndexMarker436"/>that can be mounted within Linux EC2 instances and can be used by multiple instances at once. It is an implementation of <strong class="bold">NFSv4</strong>. It can be used <a id="_idIndexMarker437"/>in general-purpose mode, max I/O performance mode (for scientific analysis or parallel computing), bursting mode, and provisioned throughput mode.</p>
			<p>As we know, in the case of instance stores, the data is volatile. As soon as the instance is lost, the data is lost from the instance store. That is not the case for EFS. EFS is separate from the EC2 instance <a id="_idIndexMarker438"/>storage. EFS is a file store and is accessed by multiple EC2 instances via mount targets inside a VPC. On-premises systems can access EFS storage via hybrid networking to the VPC, such as <strong class="bold">VPN</strong> or <strong class="bold">Direct Connect</strong>. EFS also supports two types of storage classes: Standard and Infrequent Access. Standard is used for frequently accessed data. Infrequent Access is the cost-effective storage class for long-lived, less frequently accessed data. Life cycle policies can be used for the transition of data between storage classes. </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">An instance store is preferred for max I/O requirements and if the data is replaceable and temporary.</p>
			<h1 id="_idParaDest-103"><a id="_idTextAnchor106"/>Relational Database Services (RDSes)</h1>
			<p>This is one of the <a id="_idIndexMarker439"/>most commonly featuring exam topics in AWS exams. You should have sufficient knowledge prior to the exam. In this section, we will learn about Amazon's RDS.</p>
			<p>AWS provides several relational databases as a service to its users. Users can run their desired database on EC2 instances, too. The biggest drawback is that the instance is only available in one availability zone in a region. The EC2 instance has to be administered and monitored to avoid any kind of failure. Custom scripts will be required to maintain a data backup over time. Any database major or minor version update would result in downtime. Database instances running on an EC2 instance cannot be easily scaled if the load increases on the database as replication is not an easy task.</p>
			<p>RDS provides managed database instances that can themselves hold one or more databases. Imagine a database server running on an EC2 instance that you do not have to manage or maintain. You need only access the server and create databases in it. AWS will manage everything else, such as the security of the instance, the operating system running on the instance, the database versions, and high availability of the database server. RDS supports multiple engines such as MySQL, Microsoft SQL Server, MariaDB, Amazon Aurora, Oracle, and PostgreSQL. You can choose any of these based on your requirements.  </p>
			<p>The foundation of <a id="_idIndexMarker440"/>Amazon RDS is a database instance, which can support multiple engines and can have multiple databases created by the user. One database instance can be accessed only by using the database <strong class="bold">CNAME </strong>(CNAME is an <a id="_idIndexMarker441"/>alias for a canonical name in a domain name system database) of the primary instance. RDS uses standard database engines. So, accessing the database using some sort of tool in a self-managed database server is the same as accessing Amazon RDS. </p>
			<p>As we have now understood the requirements of Amazon RDS, let's understand the failover process in Amazon RDS. We will cover what services Amazon offers if something goes wrong with the RDS instance.</p>
			<h1 id="_idParaDest-104"><a id="_idTextAnchor107"/>Managing failover in Amazon RDS</h1>
			<p>RDS instances <a id="_idIndexMarker442"/>can be <strong class="bold">single-AZ</strong> or <strong class="bold">multi-AZ</strong>. In multi-AZ, multiple <a id="_idIndexMarker443"/>instances work together, similar to an active-passive failover design. </p>
			<p>For a single-AZ RDS instance, storage <a id="_idIndexMarker444"/>can be allocated for that instance to use. In a nutshell, a single-AZ RDS instance has one attached block store (EBS storage) available in the same availability zone. This makes the databases and the storage of the RDS instance vulnerable to availability zone failure. The storage allocated to the block storage can be SSD (gp2 or io1) or magnetic. To secure the RDS instance, it is advised to use a security group and provide access based on requirements. </p>
			<p>Multi-AZ is always the best <a id="_idIndexMarker445"/>way to design the architecture to avoid any failure and keep the applications highly available. With multi-AZ features, a standby replica is kept in sync <strong class="bold">synchronously</strong> with the primary instance. The standby instance has its own storage in the assigned availability zone. A standby replica cannot be accessed directly, because all RDS access is via a single database CNAME. You can't access the standby unless a failover happens. The standby provides no performance benefit, but it does constitute an improvement in terms of availability of the RDS instance. It can only happen in the same region, another AZ's subnet in the same region inside the VPC. When a multi-AZ RDS instance is online, you can take a backup from the standby replica without affecting the performance. In a single-AZ instance, availability and performance issues can be significant during backup operation.</p>
			<p>To understand the workings of multi-AZ, let's take an example of a single-AZ instance and expand it to multi-AZ. </p>
			<p>Imagine you have an RDS instance running in availability zone <strong class="source-inline">AZ-A</strong> of the <strong class="source-inline">us-east-1</strong> region inside a VPC named <strong class="source-inline">db-vpc</strong>. This becomes a primary instance in a single-AZ design of an RDS <a id="_idIndexMarker446"/>instance. In this case, there will be storage allocated to the <a id="_idIndexMarker447"/>instance in the <em class="italic">AZ-A</em> availability zone. Once you opt for multi-AZ deployment in another availability zone called <em class="italic">AZ-B</em>, AWS creates a standby instance in availability zone <em class="italic">AZ-B</em> of the <em class="italic">us-east-1 </em>region inside the <em class="italic">db-vpc</em> VPC and allocates storage for the standby instance in <em class="italic">AZ-B</em> of the <em class="italic">us-east-1</em> region. Along with that, RDS will enable <strong class="bold">synchronous replication</strong> from the primary instance to the standby replica. As we have learned earlier, the only way to access our RDS instance is via the database CNAME, hence, the access request goes to the RDS primary instance. As soon as a write request comes to the endpoint, it writes to the primary instance. Then it writes the data to the hardware, which is the block storage attached to the primary instance. At the same time, the primary instance replicates the same data to the standby instance. Finally, the standby instance commits the data to its block storage. </p>
			<p>The primary instance writes the data into the hardware and replicates the data to the standby instance in parallel, so there is a minimal time lag (almost nothing) between the data commit operations in their respective hardware. If an error occurs with the primary instance, then RDS detects this and changes the database endpoint to the standby instance. The clients accessing the database may experience a very short interruption with this. This failover occurs within 60-120 seconds. It does not provide a fault-tolerant system because there will be some impact during the failover operation. </p>
			<p>You should now <a id="_idIndexMarker448"/>understand failover management on Amazon RDS. Let's now learn about taking automatic <a id="_idIndexMarker449"/>RDS backup, using snapshots to restore in the event of any failure, and read replicas in the next section.</p>
			<h1 id="_idParaDest-105"><a id="_idTextAnchor108"/>Taking automatic backup, RDS snapshots, and restore and read replicas</h1>
			<p>In this section, we <a id="_idIndexMarker450"/>will see how RDS <strong class="bold">Automatic Backup</strong> and <strong class="bold">Manual Snapshots</strong> work. These <a id="_idIndexMarker451"/>features come <a id="_idIndexMarker452"/>with Amazon <a id="_idIndexMarker453"/>RDS.</p>
			<p>Let's consider a database that is scheduled to take a backup at 5 A.M. every day. If the application fails at 11 A.M., then it is possible to restart the application from the backup taken at 11 A.M. with the loss of 6 hours' worth of data. This is <a id="_idIndexMarker454"/>called 6 hours <strong class="bold">RPO (Recovery Point Objective)</strong>. So, RPO is defined as the time between the most recent backup and the incident and this defines the amount of data loss. If you want to reduce this, then you have to schedule more incremental backups, which increases the cost and backup frequency. If your business demands a lower RPO value, then the business must spend more on meeting the technical solutions.</p>
			<p>Now, according to our example, an engineer was assigned this task to bring the system online as soon as the disaster occurred. The engineer managed to bring the database online at 2 P.M. on the same day by adding a few extra hardware components to the current system and installed some updated versions of the software. This is called 3 hours <strong class="bold">RTO (Recovery Time Objective)</strong>. So, RTO is <a id="_idIndexMarker455"/>determined as the time between the disaster recovery and full recovery. RTO values can be reduced by having spare hardware and documenting the restoration process. If the business demands a lower RTO value, then your business must spend more money on spare hardware and effective system setup to perform the restoration process. </p>
			<p>In RDS, RPO and RTO play an important role in the selection of <strong class="bold">Automatic Backups</strong> and <strong class="bold">Manual Snapshots</strong>. Both of these backup services use AWS-managed S3 buckets, which means it cannot be visible in the user's AWS S3 console. It is region-resilient because the backup is replicated into multiple availability zones in the AWS region. In case of a single-AZ RDS instance, the backup happens from the single available data store, and for a multi-AZ enabled RDS instance, the backup happens from the standby data store (the primary store remains untouched as regards the backup). </p>
			<p>The snapshots are manual <a id="_idIndexMarker456"/>against RDS instances and these are stored in the AWS-managed S3 bucket. The first snapshot of an RDS instance is a full copy of the data and the onward snapshots are incremental, reflecting the change in the data. In terms of the time taken for the snapshot process, it is higher for the first one and, from then on, the incremental backup is quicker. When any snapshot occurs, it can impact the performance of the single-AZ RDS instance, but not the performance of a multi-AZ RDS instance as this happens on the standby data storage. Manual snapshots do not expire, have to be cleared automatically, and they live past the termination of an RDS instance. When you delete an RDS instance, it suggests making one final snapshot on your behalf and it will contain all the databases inside your RDS instance (there is not just a single database in an RDS instance). When you restore from a manual snapshot, you restore to a single point in time and that affects the RPO. </p>
			<p>To automate this entire process, you can choose a time window when these snapshots can be taken. This is called automatic <a id="_idIndexMarker457"/>backups. These time windows can be managed wisely to essentially lower the RPO value of the business. Automatic backups have a retention period of 0 to 35 days, with 0 being disabled and the maximum is 35 days. To quote AWS documentation, retained automated backups contain system snapshots and transaction logs from a database instance. They also include database instance properties such as allocated storage and a database instance class, which are required to restore it to an active instance. Databases generate transaction logs, which contain the actual change in data in a particular database. These transaction logs are also written to S3 every 5 minutes by RDS. Transaction logs can also be replayed on top of the snapshots to restore to a point in time of 5 minutes' granularity. Theoretically, the RPO can be a 5-minute point in time. </p>
			<p>When you <a id="_idIndexMarker458"/>perform a restore, RDS creates a new RDS instance, which means a new database endpoint to access the instance. The applications using the instances have to point to the new address, which significantly affects the RTO. This means that the restoration process is not very fast, which affects the RTO. To minimize the RTO during a failure, you may consider replicating the data. With replicas, there is a high chance of replicating the corrupted data. The only way to overcome this is to have snapshots and an RDS instance can be restored to a particular point in time prior to the corruption. <strong class="bold">Amazon RDS Read Replicas</strong> are unlike the multi-AZ replicas. In multi-AZ RDS instances, the standby replicas cannot be used directly for anything unless a primary instance fails, whereas <strong class="bold">Read Replicas</strong> can <a id="_idIndexMarker459"/>be used directly, but only for read operations. Read replicas have their own database endpoints and read-heavy applications can directly point to this address. They are kept in sync <strong class="bold">asynchronously</strong> with the primary instance. Read replicas can be created in the same region as the primary instance or in a different region. Read replicas in other regions are called <strong class="bold">Cross-Region Read Replicas</strong> and this <a id="_idIndexMarker460"/>improves the global performance of the application. </p>
			<p>As per AWS documentation, five direct read replicas are allowed per database instance and this helps to scale out the read performances. Read replicas have a very low RPO value due to asynchronous replication. They can be promoted to a read-write database instance in the case of a primary instance failure. This can be done quickly and it offers a fairly low RTO value. </p>
			<p>In the next section, we will learn about Amazon's own database engine, Amazon Aurora.</p>
			<h1 id="_idParaDest-106"><a id="_idTextAnchor109"/>Writing to Amazon Aurora with multi-master capabilities</h1>
			<p>Amazon Aurora <a id="_idIndexMarker461"/>is the most reliable relational <a id="_idIndexMarker462"/>database engine developed by Amazon to deliver speed in a simple and cost-effective manner. Aurora uses a cluster of single primary instances and zero or more replicas. Aurora's replicas can give you the advantage of both read replicas and multi-AZ instances in RDS. Aurora uses a shared cluster volume for storage and is available to all compute instances of the cluster (a maximum of 64 TiB). This allows the Aurora cluster to provision faster and improves availability and performance. Aurora uses SSD-based storage, which provides high IOPS and low latency. Aurora does not ask you to allocate storage, unlike other RDS instances; it is based on the storage that you use. </p>
			<p>Aurora clusters have multiple <a id="_idIndexMarker463"/>endpoints, including <strong class="bold">Cluster Endpoint</strong> and <strong class="bold">Reader Endpoint</strong>. If there are zero replicas, then <a id="_idIndexMarker464"/>the cluster endpoint is the same as the reader endpoint. If there are replicas available, then the reader endpoint is load balanced across the reader endpoints. Cluster endpoints are used for reading/writing, while reader endpoints are intended for reading from the cluster. If you add more replicas, then AWS manages load balancing under the hood for the new replicas.</p>
			<p>When failover occurs, the replicas are promoted to read/write mode and this takes some time. This can be avoided in a <strong class="bold">Multi-Master</strong> mode of an Aurora cluster. This allows multiple instances to perform reads and writes at the same time. </p>
			<h1 id="_idParaDest-107"><a id="_idTextAnchor110"/>Storing columnar data on Amazon Redshift</h1>
			<p>Amazon <a id="_idIndexMarker465"/>Redshift is not used for real-time transaction use, but it is <a id="_idIndexMarker466"/>used for data warehouse purposes. It is designed to support huge volumes of data at a petabyte scale. It is a column-based database used for analytics purpose, long-term processing, tending, and aggregation. <strong class="bold">Redshift Spectrum</strong> can be <a id="_idIndexMarker467"/>used to query data on S3 without loading data to the Redshift cluster (a Redshift cluster is required though). It's not an OLTP, but an OLAP. <strong class="bold">AWS QuickSight</strong> can be integrated with Redshift for visualization, with a SQL-like interface that allows you to connect using JDBC/ODBC connections for querying the data. </p>
			<p>Redshift uses a clustered architecture in one AZ in a VPC with faster network connectivity between the nodes. It is not high availability by design as it is tightly coupled to the AZ. A Redshift cluster <a id="_idIndexMarker468"/>has a <strong class="bold">Leader Node</strong>, and this node is responsible for all the communication between the client and the computing nodes of the cluster, query planning, and aggregation. <strong class="bold">Compute Nodes</strong> are responsible <a id="_idIndexMarker469"/>for running the queries submitted by the leader lode and for storing the data. By default, Redshift uses a public network for communicating with external services or any AWS services. With <strong class="bold">Enhanced VPC Routing</strong>, it can be controlled via customized networking settings.</p>
			<h1 id="_idParaDest-108"><a id="_idTextAnchor111"/>Amazon DynamoDB for NoSQL database as a service</h1>
			<p>Amazon <a id="_idIndexMarker470"/>DynamoDB is a NoSQL <a id="_idIndexMarker471"/>database-as-a-service product within AWS. It's a fully managed key/value and document database. Accessing DynamoDB is easy via its endpoint. The input and output throughputs can be managed or scaled manually or in automatic fashion. It also supports data backup, point-in-time recovery, and data encryption. We will not cover the DynamoDB table structure or key structure in this chapter as this is not required for the certification exam. However, it is good to have a basic knowledge of them. For more details, please refer to the AWS docs available here:<a href=" https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.html"> https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.html</a>.</p>
			<h1 id="_idParaDest-109"><a id="_idTextAnchor112"/>Summary</h1>
			<p>In this chapter, we learned about various data storage services from Amazon, and how to secure data through various policies and use these services. If you are working on machine learning use cases, then you may encounter such scenarios where you have to choose an effective data storage service for your requirements. </p>
			<p>In the next chapter, we will learn about the processing of stored data.</p>
			<h2 id="_idParaDest-110"><a id="_idTextAnchor113"/>Questions</h2>
			<ol>
				<li value="1">To set the region that an S3 bucket is stored in, you must first create the bucket and then set the region separately. <p>A. True</p><p>B. False</p></li>
				<li>Is it mandatory to have both the source bucket and destination bucket in the same region in order to copy the contents of S3 buckets?<p>A. True</p><p>B. False</p></li>
				<li>By default, objects are private in a bucket.<p>A. True</p><p>B. False</p></li>
				<li>By WS, a S3 object is immutable and you can only perform put and delete. Rename is GET and PUT of the same object with a different name.<p>A. True</p><p>B. False</p></li>
				<li>If a user has stored an unversioned object and a versioned object in the same bucket, then the user can only delete the unversioned object. Versioned objects cannot be deleted.<p>A. True</p><p>B. False</p><p class="callout-heading">Answer</p><p class="callout">Versioning applies to a complete bucket and not objects. If versioning is enabled on a bucket, then it can only be suspended; it cannot be disabled. </p></li>
				<li>A simple delete request on a delete marker will:<p>A. Delete the delete marker</p><p>B. Create a copy of the delete marker</p><p>C. Not delete the delete marker, but it will create another delete marker</p><p>D. Delete the original version of the object</p></li>
				<li>Scaling and performance can be improved via RDS multi-AZ instances.<p>A. True</p><p>B. False</p><p class="callout-heading">Answer</p><p class="callout">An RDS multi-AZ has nothing to do with scaling and performance. It is used for failover.</p></li>
				<li>What protocol does EFS use?<p>A. SMB</p><p>B. NFS</p><p>C. EBS</p><p>D. HTTP</p></li>
				<li>Which operating systems does EFS support?<p>A. Linux only</p><p>B. Windows only</p><p>C. Both Windows and Linux</p><p>D. Neither Windows nor Linux</p></li>
				<li>Is EFS a private service?<p>A. Yes.</p><p>B. No, it's a public service.</p><p>C. It's both a private and a public service.</p><p>D. It's neither a private nor a public service.</p></li>
				<li>Which two of the following are correct?<p>A. Multi-AZ:Same Region::Read Replica:Multiple Region</p><p>B. Multi-AZ:Multiple Region::Read Replica:Same Region</p><p>C. Multi-AZ:Synchronous Replication::Read Replica:Asynchronous Replication</p><p>D. Multi-AZ:ASynchronous Replication::Read Replica:Synchronous Replication</p></li>
				<li>Which of the following is correct?<p>A. Read replicas are read-only instances. </p><p>B. Read replicas are read-write instances.</p><p>C. Read replicas are write-only instances.</p><p>D. Read replicas are read-only instances, until promoted to read-write.</p></li>
				<li>Where is EFS accessible from?<p>A. Inside a VPC only </p><p>B. Via AWS endpoints</p><p>C. Anywhere with a public internet connection</p><p>D. Inside a VPC or any on-premises locations connected to that VPC via a hybrid network</p></li>
				<li>Which three of the following are true?<p>A. Instance store volumes are persistent storage. </p><p>B. Instance store volumes are temporary (ephemeral) storage.</p><p>C. Data stored on instance store volumes can be lost if a hardware failure occurs.</p><p>D. Data stored on instance store volumes can be lost when an EC2 instance restarts.</p><p>E. Data stored on instance store volumes can be lost when an EC2 instance stops and starts.</p><p class="callout-heading">Answer</p><p class="callout">Hardware failure can change the underlying host. So, there is no guarantee of the instance store volume. When you stop the instance and start it again, the instance store volume is lost due to the change of host. Instance restarting is different from stop and start; it means an operating system restart.</p></li>
				<li>In order to enable encryption at rest using EC2 and EBS, you need to… <p>A. Configure encryption when creating the EBS volume.</p><p>B. Configure encryption using the appropriate operating system's filesystem.</p><p>C. Configure encryption using X.509 certificates.</p><p>D. Mount the EBS volume in S3 and then encrypt the bucket using a bucket policy.</p></li>
				<li>Which of the following is an action you cannot perform on an EBS snapshot?<p>A. Create an image from a snapshot.</p><p>B. Create an EBS volume from a snapshot.</p><p>C. Share a snapshot with another AWS account.</p><p>D. Make an unencrypted copy of an encrypted snapshot.</p></li>
				<li>With EBS, you need to do the following (choose two).<p>A. Create an encrypted volume from a snapshot of another encrypted volume. </p><p>B. Create an encrypted volume from an encrypted snapshot.</p><p>C. Create an encrypted snapshot from an unencrypted snapshot by creating an encrypted copy of the unencrypted snapshot.</p><p>D. Encrypt an existing volume.</p><p class="callout-heading">Answer</p><p class="callout">For more on EBS, visit the following link:<a href=" https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html#EBSEncryption_c%20onsiderations"> https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html#EBSEncryption_c%20onsiderations</a>.</p></li>
			</ol>
			<h2 id="_idParaDest-111"><a id="_idTextAnchor114"/>Answers</h2>
			<p>1. B</p>
			<p>2. B</p>
			<p>3. A</p>
			<p>4. A</p>
			<p>5. B</p>
			<p>6. C</p>
			<p>7. B</p>
			<p>8. B</p>
			<p>9. A </p>
			<p>10. B </p>
			<p>11. A, C</p>
			<p>12. D</p>
			<p>13. D</p>
			<p>14. B, C, E</p>
			<p>15. A</p>
			<p>16. D</p>
			<p>17. A, C</p>
		</div>
	</body></html>