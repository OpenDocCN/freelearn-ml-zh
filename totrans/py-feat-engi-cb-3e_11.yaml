- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Extracting Features from Text Variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text can be one of the variables in our datasets. For example, in insurance,
    information describing the circumstances of an incident can come from free text
    fields in a form. If a company gathers customer reviews, this information will
    be collected as short pieces of text provided by the users. Text data does not
    show the **tabular** pattern of the datasets that we have worked with throughout
    this book. Instead, information in texts can vary in length and content, as well
    as writing style. We can extract a lot of information from text variables to use
    as predictive features in machine learning models. The techniques we will cover
    in this chapter belong to the realm of **Natural Language Processing** (**NLP**).
    NLP is a subfield of linguistics and computer science. It is concerned with the
    interactions between computer and human language, or, in other words, how to program
    computers to understand human language. NLP includes a multitude of techniques
    to understand the syntax, semantics, and discourse of text. Therefore, to do this
    field justice would require an entire book.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss the methods that will allow us to quickly extract
    features from short pieces of text to complement our predictive models. Specifically,
    we will discuss how to capture a piece of text’s complexity by looking at some
    statistical parameters of the text, such as the word length and count, the number
    of words and unique words used, the number of sentences, and so on. We will use
    the `pandas` and `scikit-learn` libraries, and we will make a shallow dive into
    a very useful Python NLP toolkit called the **Natural Language** **Toolkit** (**NLTK**).
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter includes the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Counting characters, words, and vocabulary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating text complexity by counting sentences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating features with bag-of-words and n-grams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing term frequency-inverse document frequency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cleaning and stemming text variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will use the `pandas`, `matplotlib`, and `scikit-learn`
    Python libraries. We will also use `NLTK`, a comprehensive Python library for
    NLP and text analysis. You can find the instructions to install `NLTK` at [http://www.nltk.org/install.html](http://www.nltk.org/install.html).
  prefs: []
  type: TYPE_NORMAL
- en: If you are using the Python Anaconda distribution, follow the instructions to
    install `NLTK` at [https://anaconda.org/anaconda/nltk](https://anaconda.org/anaconda/nltk).
  prefs: []
  type: TYPE_NORMAL
- en: 'After you have installed `NLTK`, open up a Python console and execute the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: These commands will download the necessary data for you to be able to run the
    recipes in this chapter successfully.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you haven’t downloaded these or the other data sources necessary for `NLTK`
    functionality, `NLTK` will raise an error. Read the error message carefully because
    it will direct you to download the data required to run the command that you are
    trying to execute.
  prefs: []
  type: TYPE_NORMAL
- en: Counting characters, words, and vocabulary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the salient characteristics of text is its complexity. Long descriptions
    are more likely to contain more information than short descriptions. Texts rich
    in different, unique words are more likely to be richer in detail than texts that
    repeat the same words over and over. In the same way, when we speak, we use many
    short words such as articles and prepositions to build the sentence structure,
    yet the main concept is often derived from the nouns and adjectives we use, which
    tend to be longer words. So, as you can see, even without reading the text, we
    can start inferring how much information the text provides by determining the
    number of words, the number of unique words (non-repeated occurrences of a word),
    the lexical diversity, and the length of those words. In this recipe, we will
    learn how to extract these features from a text variable using `pandas`.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are going to use the `scikit-learn`, which comprises around 18,000 news
    posts on 20 different topics. More details about this dataset can be found on
    the following sites:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The scikit-learn dataset website: [https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset](https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The home page for the 20 Newsgroup dataset: [http://qwone.com/~jason/20Newsgroups/](http://qwone.com/~jason/20Newsgroups/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before jumping into the recipe, let’s discuss the features that we are going
    to derive from these text pieces. We mentioned that longer descriptions, more
    words in the article, a greater variety of unique words, and longer words tend
    to correlate with the amount of information that the article provides. Hence,
    we can capture text complexity by extracting the following information about the
    text:'
  prefs: []
  type: TYPE_NORMAL
- en: The total number of characters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The total number of words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The total number of unique words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lexical diversity (total number of words divided by number of unique words)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word average length (number of characters divided by number of words)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this recipe, we will extract these numerical features using `pandas`, which
    has extensive string processing functionalities that can be accessed via the `str`
    vectorized string functions for series.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s begin by loading `pandas` and getting the dataset ready:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load `pandas` and the dataset from `scikit-learn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s load the train set part of the 20 Newsgroup dataset into a `pandas` DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: You can print an example of a text from the DataFrame by executing `print(df['text'][1])`.
    Change the number between `[` and `]` to display different texts. Note how every
    text description is a single string composed of letters, numbers, punctuation,
    and spaces. You can check the datatype by executing `type(df["text"][1])`.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the text variable in a `pandas` DataFrame, we are ready to
    extract the features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s capture the number of characters in each text piece in a new column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: 'You can remove trailing white spaces in a string, including those from new
    lines, before counting the number of characters by adding the `strip()` method
    before the `len()` method, as shown here: `df[''num_char''] =` `df[''text''].str.strip().str.len()`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s capture the number of words in each text in a new column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To count words, we use the `pandas` library’s `split()` method, which splits
    a text at white spaces. Check out the output of `split()` by executing, for instance,
    `df["text"].loc[1].split()` to separate the words of the second text of the DataFrame.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s capture the number of *unique* words in each text in a new column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Python interprets the same word as two different words if one has a capital
    letter. To avoid this behavior, we can apply the `lower()` method before the `split()`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a feature that captures the lexical diversity – that is, the total
    number of words (*step 4*) compared to the number of unique words (*step 5*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s calculate the average word length by dividing the number of characters
    (*step 3*) by the number of words (*step 4*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If we execute `df.head()`, we will see the first five rows of data with the
    text and the newly created features:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.1 – A DataFrame with the text variable and features that summarize
    some of the text’s characteristics](img/B22396_11_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – A DataFrame with the text variable and features that summarize
    some of the text’s characteristics
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have extracted five different features that capture the text complexity,
    which we can use as inputs for our machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we created new features from the raw data straight away without
    doing any data cleaning, removing punctuation, or even stemming words. Note that
    these are steps that are performed ahead of most standard NLP procedures. To learn
    more about this, visit the *Cleaning and stemming text variables* recipe at the
    end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we created five new features that capture text complexity by
    utilizing pandas’ `str` to access the built-in `pandas` functionality to work
    with strings. We worked with the text column of the `train` subset of the 20 Newsgroup
    dataset that comes with `scikit-learn`. Each row in this dataset is composed of
    a string with text.
  prefs: []
  type: TYPE_NORMAL
- en: We used pandas’ `str`, followed by `len()`, to count the number of characters
    in each string – that is, the total number of letters, numbers, symbols, and spaces.
    We also combined `str.len()` with `str.strip()` to remove trailing white spaces
    at the beginning and end of the string and in new lines, before counting the number
    of characters.
  prefs: []
  type: TYPE_NORMAL
- en: To count the number of words, we used pandas’ `str`, followed by `split()`,
    to divide the string into a list of words. The `split()` method creates a list
    of words by breaking the string at the white spaces between words. Next, we counted
    those words with `str.len()`, obtaining the number of words per string.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We can change the behavior of `str.split()` by passing a string or character
    that we would like to use to split the string. For example, `df['text'].str.split(';')`
    divides a string at each occurrence of `;`.
  prefs: []
  type: TYPE_NORMAL
- en: To determine the number of unique words, we used pandas’ `str.split()` function
    to divide the string into a list of words. Next, we applied the built-in Python
    `set()` method within pandas’ `apply()` to return a set of words. Remember that
    a set contains *unique occurrences* of the elements in a list – that is, unique
    words. Next, we counted those words with pandas’ `str.len()` function to return
    the `lower()` function to set all the characters to lowercase before splitting
    the string and counting the number of unique words.
  prefs: []
  type: TYPE_NORMAL
- en: To create the lexical diversity and average word length features, we simply
    performed a vectorized division of two `pandas` series. That’s it; we created
    five new features with information about the complexity of the text.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can check out the distribution of the features extracted from text in each
    of the 20 different news topics present in the dataset by using visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make histogram plots of the newly created features, after you run all of
    the steps in the *How it works...* section of this recipe, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `matplotlib`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the target with the news topics to the 20 Newsgroup DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a function that displays a histogram of a feature of your choice for
    each of the news topics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the function for the number of words feature:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The previous command returns the following plot, where you can see the distribution
    of the number of words in each of the 20 news topics, numbered from 0 to 19 in
    the plot title:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.2 – Histograms showing the distribution of the number of words
    per text, segregated by topic discussed in each text](img/B22396_11_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – Histograms showing the distribution of the number of words per
    text, segregated by topic discussed in each text
  prefs: []
  type: TYPE_NORMAL
- en: The number of words shows a different distribution across the different news
    topics. Therefore, this feature is likely useful in a classification algorithm
    to predict the topic of the text.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To learn more about pandas’ built-in string processing functionality visit [https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html#method-summary](https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html#method-summary).
  prefs: []
  type: TYPE_NORMAL
- en: Estimating text complexity by counting sentences
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One aspect of a piece of text that we can capture in features is its complexity.
    Usually, longer descriptions that contain multiple sentences spread over several
    paragraphs tend to provide more information than descriptions with very few sentences.
    Therefore, capturing the number of sentences may provide some insight into the
    amount of information provided by the text. This process is called `NLTK` Python
    library, which provides this functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will use the `NLTK` Python library. For guidelines on how
    to install `NLTK`, check out the *Technical requirements* section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s begin by importing the required libraries and dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s load `pandas`, the sentence tokenizer from `NLTK`, and the dataset from
    `scikit-learn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To understand the functionality of the sentence tokenizer from `NLTK`, let’s
    create a variable that contains a string with multiple sentences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s separate the string from *step 2* into sentences using `NLTK` library‘s
    sentence tokenizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: If you encounter an error in *step 3*, read the error message carefully and
    download the data source required by `NLTK`, as described in the error message.
    For more details, check out the *Technical* *requirements* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sentence tokenizer returns the list of sentences shown in the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The escape character followed by the letter, `\n`, indicates a new line.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s count the number of sentences in the `text` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The previous command returns `8`, which is the number of sentences in our `text`
    variable. Now, let’s determine the number of sentences in an entire DataFrame.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s load the `train` subset of the 20 Newsgroup dataset into a `pandas` DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To speed up the following steps, we will only work with the first `10` rows
    of the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s also remove the first part of the text, which contains information about
    the email sender, subject, and other details that we are not interested in. Most
    of this information comes before the word `Lines` followed by `:`, so let’s split
    the string at `Lines:` and capture the second part of the string:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let’s create a variable containing the number of sentences per `text`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With the `df` command, you can display the entire DataFrame with the `text`
    variable and the new feature containing the number of sentences per text:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.3 – A DataFrame with the text variable and the number of sentences
    per text](img/B22396_11_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 – A DataFrame with the text variable and the number of sentences
    per text
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can use this new feature as input to machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we separated a string with text into sentences using `sent_tokenizer`
    from the `NLTK` library. `sent_tokenizer` has been pre-trained to recognize capitalization
    and different types of punctuation that signal the beginning and the end of a
    sentence.
  prefs: []
  type: TYPE_NORMAL
- en: First, we applied `sent_tokenizer` to a manually created string to become familiar
    with its functionality. The tokenizer divided the text into a list of eight sentences.
    We combined the tokenizer with the built-in Python `len()` method to count the
    number of sentences in the string.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we loaded a dataset with text and, to speed up the computation, we only
    retained the first 10 rows of the DataFrame using pandas’ `loc[]` function. Next,
    we removed the first part of the text, which contained information about the email
    sender and subject. To do this, we split the string at `Lines:` using pandas’
    `str.split("Lines:")` function, which returned a list with two elements: the strings
    before and after `Lines:`. Utilizing a lambda function within `apply()`, we retained
    the second part of the text – that is, the second string in the list returned
    by `split()`.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we applied `sent_tokenizer` to each row in the DataFrame with the pandas
    `apply()` method to separate the strings into sentences, and then applied the
    built-in Python `len()` method to the list of sentences to return the number of
    sentences per string. This way, we created a new feature that contained the number
    of sentences per text.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`NLTK` has functionalities for word tokenization among other useful features,
    which we can use instead of `pandas` to count and return the number of words.
    You can find out more about `NLTK`’s functionality here:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python 3 Text Processing with NLTK 3 Cookbook*, by Jacob Perkins, Packt Publishing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `NLTK` documentation at [http://www.nltk.org/](http://www.nltk.org/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating features with bag-of-words and n-grams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A **Bag-of-Words** (**BoW**) is a simplified representation of a piece of text
    that captures the words that are present in the text and the number of times each
    word appears in the text. So, for the text string *Dogs like cats, but cats do
    not like dogs*, the derived BoW is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 11.4 – The BoW derived from the sentence Dogs like cats, but cats\
    \ do not\uFEFF like dogs](img/B22396_11_04.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 – The BoW derived from the sentence Dogs like cats, but cats do
    not like dogs
  prefs: []
  type: TYPE_NORMAL
- en: Here, each word becomes a variable, and the value of the variable represents
    the number of times the word appears in the string. As you can see, the BoW captures
    multiplicity but does not retain word order or grammar. That is why it is a simple,
    yet useful way of extracting features and capturing some information about the
    texts we are working with.
  prefs: []
  type: TYPE_NORMAL
- en: 'To capture some syntax, BoW can be used together with **n-grams**. An n-gram
    is a contiguous sequence of *n* items in a given text. Continuing with the sentence
    *Dogs like cats, but cats do not like dogs*, the derived 2-grams are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Dogs like
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: like cats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cats but
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: but do
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: do not
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: like dogs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can create, together with a BoW, a bag of n-grams, where the additional
    variables are given by the 2-grams and the values for each 2-gram are the number
    of times they appear in each string; for this example, the value is 1\. So, our
    final BoW with 2-grams would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.5 – The BoW with 2-grams](img/B22396_11_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 – The BoW with 2-grams
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to create BoWs with or without n-grams using
    `scikit-learn`.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before jumping into this recipe, let’s get familiar with some of the parameters
    of a BoW that we can adjust to make the BoW comprehensive. When creating a BoW
    over several pieces of text, a new feature is created for each unique word that
    appears at least once in *any* of the text pieces we are analyzing. If the word
    appears only in one piece of text, it will show a value of 1 for that particular
    text and 0 for all of the others. Therefore, BoWs tend to be sparse matrices,
    where most of the values are zeros.
  prefs: []
  type: TYPE_NORMAL
- en: The number of columns – that is, the number of words – in a BoW can be quite
    large if we work with huge text corpora, and even larger if we also include n-grams.
    To limit the number of columns and the sparsity of the returned matrix, we can
    retain words that appear across multiple texts; or, in better words, we can retain
    words that appear in, at least, a certain percentage of texts.
  prefs: []
  type: TYPE_NORMAL
- en: To reduce the number of columns and sparsity of the BoW, we should also work
    with words in the same case – for example, lowercase – as Python identifies words
    in a different case as different words. We can also reduce the number of columns
    and sparsity by removing **stop words**. Stop words are very frequently used words
    that make sentences flow, but that do not, per se, carry any useful information.
    Examples of stop words are pronouns such as I, you, and he, as well as prepositions
    and articles.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we will learn how to set words in lowercase, remove stop words,
    retain words with a minimum acceptable frequency, and capture n-grams all together
    with a single transformer from `scikit-learn`: `CountVectorizer()`.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s begin by loading the necessary libraries and getting the dataset ready:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load `pandas`, `CountVectorizer`, and the dataset from `scikit-learn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s load the train set part of the 20 Newsgroup dataset into a pandas DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To make interpreting the results easier, let’s remove punctuation and numbers
    from the text variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn more about regex with Python, follow this link: [https://docs.python.org/3/howto/regex.html](https://docs.python.org/3/howto/regex.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s set up `CountVectorizer()` so that, before creating the BoW, it
    puts the text in lowercase, removes stop words, and retains words that appear
    in, at least, 5% of the text pieces:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To introduce n-grams as part of the returned columns, we can change the value
    of `ngrams_range` to, for example, `(1,2)`. The tuple provides the lower and upper
    boundaries of the range of n-values for different n-grams. In the case of `(1,2)`,
    `CountVectorizer()` will return single words and arrays of two consecutive words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s fit `CountVectorizer()` so that it learns which words should be used
    in the BoW:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s create the BoW:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let’s capture the BoW in a DataFrame with the corresponding feature
    names:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With that, we have created a `pandas` DataFrame that contains words as columns
    and the number of times they appeared in each text as values. You can inspect
    the result by executing `bagofwords.head()`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.6 – A DataFrame with the BoW resulting from the 20 Newsgroup dataset](img/B22396_11_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6 – A DataFrame with the BoW resulting from the 20 Newsgroup dataset
  prefs: []
  type: TYPE_NORMAL
- en: We can use this BoW as input for a machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: scikit-learn’s `CountVectorizer()` converts a collection of text documents into
    a matrix of token counts. These tokens can be individual words or arrays of two
    or more consecutive words – that is, n-grams. In this recipe, we created a BoW
    from a text variable in a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: We loaded the 20 Newsgroup text dataset from `scikit-learn` and removed punctuation
    and numbers from the text rows using pandas’ `replace()` function, which can be
    accessed through pandas’ `str` module, to replace digits, `'\d+'`, or symbols,
    `'[^\w\s]'`, with empty strings, `''`. Then, we used `CountVectorizer()` to create
    the BoW. We set the `lowercase` parameter to `True` to put the words in lowercase
    before extracting the BoW. We set the `stop_words` argument to `english` to ignore
    stop words – that is, to avoid stop words in the BoW. We set `ngram_range` to
    the `(1,1)` tuple to return only single words as columns. Finally, we set `min_df`
    to `0.05` to return words that appeared in at least 5% of the texts, or, in other
    words, in 5% of the rows in the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: After setting up the transformer, we used the `fit()` method to allow the transformer
    to find the words that fulfill the preceding criteria. Finally, using the `transform()`
    method, the transformer returned an object containing the BoW with its feature
    names, which we captured in a `pandas` DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For more details about `CountVectorizer()`, visit the `scikit-learn` library’s
    documentation at [https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).
  prefs: []
  type: TYPE_NORMAL
- en: Implementing term frequency-inverse document frequency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Term Frequency-Inverse Document Frequency** (**TF-IDF**) is a numerical statistic
    that captures how relevant a word is in a document considering the entire collection
    of documents. What does this mean? Some words will appear a lot within a text
    document as well as across documents, such as the English words *the*, *a*, and
    *is*, for example. These words generally convey little information about the actual
    content of the document and don’t make the text stand out from the crowd. TF-IDF
    provides a way to *weigh* the importance of a word by considering how many times
    it appears in a document with regards to how often it appears across documents.
    Hence, commonly occurring words such as *the*, *a*, or *is* will have a low weight,
    and words that are more specific to a topic, such as *leopard*, will have a higher
    weight.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TF-IDF is the product of two statistics: **Term Frequency** (**tf**) and **Inverse
    Document Frequency** (**idf**), represented as follows: **tf-idf = td × idf**.
    tf is, in its simplest form, the count of the word in an individual text. So,
    for term *t*, the tf is calculated as *tf(t) = count(t)* and is determined on
    a text-by-text basis. The idf is a measure of how common the word is across *all*
    documents and is usually calculated on a logarithmic scale. A common implementation
    is given by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>i</mi><mi>d</mi><mi>f</mi><mfenced
    open="(" close=")"><mi>t</mi></mfenced><mo>=</mo><mrow><mrow><mi>log</mi><mo>(</mo></mrow></mrow><mrow><mrow><mfrac><mi>n</mi><mrow><mn>1</mn><mo>+</mo><mi>d</mi><mi>f</mi><mo>(</mo><mi>t</mi><mo>)</mo></mrow></mfrac><mo>)</mo></mrow></mrow></mrow></mrow></math>](img/48.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *n* is the total number of documents, and *df(t)* is the number of documents
    in which the term *t* appears. The bigger the value of *df(t)*, the lower the
    weighting for the term. The importance of a word will be high if it appears a
    lot of times in a text (high *tf*) or few times across texts (high *idf*).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF can be used together with n-grams. Similarly, to weigh an n-gram, we
    compound the n-gram frequency in a certain document with the frequency of the
    n-gram across documents.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to extract features using TF-IDF with or without
    n-grams using `scikit-learn`.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`scikit-learn` uses a slightly different way to calculate the IDF statistic:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:math>](img/49.png)'
  prefs: []
  type: TYPE_IMG
- en: This formulation ensures that a word that appears in all texts receives the
    lowest weight of 1\. In addition, after calculating the TF-IDF for every word,
    `scikit-learn` normalizes the feature vector (that with all the words) to its
    Euclidean norm. For more details on the exact formula, visit the `scikit-learn`
    documentation at [https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting).
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF shares the characteristics of BoW when creating the term matrix – that
    is, high feature space and sparsity. To reduce the number of features and sparsity,
    we can remove stop words, set the characters to lowercase, and retain words that
    appear in a minimum percentage of observations. If you are unfamiliar with these
    terms, visit the *Creating features with bag-of-words and n-grams* recipe in this
    chapter for a recap.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we will learn how to set words into lowercase, remove stop
    words, retain words with a minimum acceptable frequency, capture n-grams, and
    then return the TF-IDF statistic of words, all using a single transformer from
    scikit-learn: `TfidfVectorizer()`.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s begin by loading the necessary libraries and getting the dataset ready:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load `pandas`, `TfidfVectorizer()`, and the dataset from `scikit-learn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s load the train set part of the 20 Newsgroup dataset into a pandas DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To make interpreting the results easier, let’s remove punctuation and numbers
    from the text variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s set up `TfidfVectorizer()` from `scikit-learn` so that, before creating
    the TF-IDF metrics, it puts all text in lowercase, removes stop words, and retains
    words that appear in at least 5% of the text pieces:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To introduce n-grams as part of the returned columns, we can change the value
    of `ngrams_range` to, for example, `(1,2)`. The tuple provides the lower and upper
    boundaries of the range of n-values for different n-grams. In the case of `(1,2)`,
    `TfidfVectorizer()` will return single words and arrays of two consecutive words
    as columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s fit `TfidfVectorizer()` so that it learns which words should be introduced
    as columns of the TF-IDF matrix and determines the words’ `idf`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s create the TF-IDF matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let’s capture the TF-IDF matrix in a DataFrame with the corresponding
    feature names:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With that, we have created a `pandas` DataFrame that contains words as columns
    and the TF-IDF as values. You can inspect the result by executing `tfidf.head()`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.7 – A DataFrame with features resulting from TF-IDF](img/B22396_11_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7 – A DataFrame with features resulting from TF-IDF
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can use this term frequency DataFrame to train machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we extracted the TF-IDF values of words present in at least
    5% of the documents by utilizing `TfidfVectorizer()` from scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: We loaded the 20 Newsgroup text dataset from `scikit-learn` and then removed
    punctuation and numbers from the text rows using pandas’ `replace()`, which can
    be accessed through pandas’ `str`, to replace digits, `'\d+'`, or symbols, `'[^\w\s]'`,
    with empty strings, `''`. Then, we used `TfidfVectorizer()` to create TF-IDF statistics
    for words. We set the `lowercase` parameter to `True` to put words into lowercase
    before making the calculations. We set the `stop_words` argument to `english`
    to avoid stop words in the returned matrix. We set `ngram_range` to the `(1,1)`
    tuple to return single words as features. Finally, we set the `min_df` argument
    to `0.05` to return words that appear at least in 5% of the texts or, in other
    words, in 5% of the rows.
  prefs: []
  type: TYPE_NORMAL
- en: After setting up the transformer, we applied the `fit()` method to let the transformer
    find the words to retain in the final term matrix. With the `transform()` method,
    the transformer returned an object with the words and their TF-IDF values, which
    we then captured in a pandas DataFrame with the appropriate feature names. We
    can now use these features in machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For more details on `TfidfVectorizer()`, visit scikit-learn’s documentation:
    [https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning and stemming text variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some variables in our dataset come from free text fields, which are manually
    completed by users. People have different writing styles, and we use a variety
    of punctuation marks, capitalization patterns, and verb conjugations to convey
    the content, as well as the emotions surrounding it. We can extract (some) information
    from text without taking the trouble to read it by creating statistical parameters
    that summarize the text’s complexity, keywords, and relevance of words in a document.
    We discussed these methods in the previous recipes of this chapter. However, to
    derive these statistics and aggregated features, we should clean the text variables
    first.
  prefs: []
  type: TYPE_NORMAL
- en: Text cleaning or preprocessing involves punctuation removal, stop word elimination,
    character case setting, and word stemming. Punctuation removal consists of deleting
    characters that are not letters, numbers, or spaces; in some cases, we also remove
    numbers. The elimination of stop words refers to removing common words that are
    used in our language to allow for the sentence structure and flow, but that individually
    convey little or no information. Examples of stop words include articles such
    as *the* and *a* for the English language, as well as pronouns such as *I*, *you*
    and *they*, and commonly used verbs in their various conjugations, such as the
    verbs *to be* and *to have*, as well as the auxiliary verbs *would* and *do*.
  prefs: []
  type: TYPE_NORMAL
- en: To allow computers to identify words correctly, it is also necessary to set
    all the words in the same case, since the words *Toy* and *toy* would be identified
    as being different by a computer due to the uppercase *T* in the first one.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, to focus on the *message* of the text, we don’t want computers to consider
    words differently if they show different conjugations. Hence, we would use word
    stemming as part of the preprocessing pipeline. Word stemming refers to reducing
    each word to its root or base so that the words *playing*, *plays*, and *played*
    become *play*, which, in essence, conveys the same or very similar meaning.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to remove punctuation and stop words, set
    words in lowercase, and perform word stemming with pandas and `NLTK`.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are going to use the `NLTK` stem package to perform word stemming, which
    incorporates different algorithms to stem words from English and other languages.
    Each method differs in the algorithm it uses to find the *root* of the word; therefore,
    they may output slightly different results. I recommend reading more about it,
    trying different methods, and choosing the one that serves the project you are
    working on.
  prefs: []
  type: TYPE_NORMAL
- en: More information about NLTK stemmers can be found at [https://www.nltk.org/api/nltk.stem.html](https://www.nltk.org/api/nltk.stem.html).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s begin by loading the necessary libraries and getting the dataset ready:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load `pandas`, `stopwords`, and `SnowballStemmer` from `NLTK` and the dataset
    from `scikit-learn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s load the train set part of the 20 Newsgroup dataset into a pandas DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, let’s begin with the text cleaning.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: After executing each of the commands in this recipe, print some example texts
    by executing, for example, `print(df['text'][10])` so that you can visualize the
    changes introduced to the text. Go ahead and do it now, and then repeat the command
    after each step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin by removing the punctuation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: You can also remove the punctuation using the built-in `string` module from
    Python. First, import the module by executing `import string` and then execute
    `df['text'] =` `df['text'].str.replace('[{}]'.format(string.punctuation), '')`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also remove characters that are numbers, leaving only letters, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s set all words into lowercase:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, let’s start the process of removing stop words.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 6* may fail if you did not download the `NLTK` library’s `stopwords`.
    Visit the *Technical requirements* section in this chapter for more details.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a function that splits a string into a list of words, removes
    the stop words, and finally concatenates the remaining words back into a string:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To be able to process the data with the `scikit-learn` library’s `CountVectorizer()`
    or `TfidfVectorizer()`, we need the text to be in string format. Therefore, after
    removing the stop words, we need to return the words as a single string. We have
    transformed the NLTK library’s stop words list into a set because sets are faster
    to scan than lists. This improves the computation time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s use the function from *step 6* to remove stop words from the `text`
    variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If you want to know which words are stop words, execute `stopwords.words('english')`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Finally, let’s stem the words in our data. We will use `SnowballStemmer` from
    `NLTK` to do so.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s create an instance of `SnowballStemer` for the English language:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Try the stemmer in a single word to see how it works; for example, run `stemmer.stem('running')`.
    You should see `run` as the result of that command. Try different words!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a function that splits a string into a list of words, applies
    `stemmer` to each word, and finally concatenates the stemmed word list back into
    a string:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s use the function from *step 9* to stem the words in our data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, our text is ready to create features based on character and word counts,
    as well as create BoWs or TF-IDF matrices, as described in the previous recipes
    of this chapter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If we execute `print(df[''text''][10])`, we will see a text example after cleaning:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you are counting sentences, you need to do that before removing punctuation,
    as punctuation and capitalization are needed to define the boundaries of each
    sentence.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we removed punctuation, numbers, and stop words from a text
    variable, set the words in lowercase, and finally, stemmed the words to their
    root. We removed punctuation and numbers from the text variable using pandas’
    `replace()`, which can be accessed through pandas’ `str`, to replace digits, `'\d+'`,
    or symbols, `'[^\w\s]'`, with empty strings, `''`. Alternatively, we can use the
    `punctuation` module from the built-in `string` package.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Run `string.punctuation` in your Python console after importing `string` to
    check out the symbols that will be replaced with empty strings.
  prefs: []
  type: TYPE_NORMAL
- en: Next, utilizing pandas’ string processing functionality through `str`, we set
    all of the words to lowercase with the `lower()` method. To remove stop words
    from the text, we used the `stopwords` module from `NLTK`, which contains a list
    of words that are considered frequent – that is, the stop words. We created a
    function that takes a string and splits it into a list of words using pandas’
    `str.split()`, and then, with list comprehension, we looped over the words in
    the list and retained the non-stop words. Finally, with the `join()` method, we
    concatenated the retained words back into a string. We used the built-in Python
    `set()` method over the `NLTK` stop words list to improve computation efficiency
    since it is faster to iterate over sets than over lists. Finally, with pandas’
    `apply()`, we applied the function to each row of our text data.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Run `stopwords.words('english')` in your Python console after importing `stopwords`
    from `NLTK` to visualize the list with the stop words that will be removed.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we stemmed the words using `SnowballStemmer` from `NLTK`. `SnowballStemmer`
    works one word at a time. Therefore, we created a function that takes a string
    and splits it into a list of words using pandas’ `str.split()`. In a list comprehension,
    we applied `SnowballStemmer` word per word and then concatenated the list of stemmed
    words back into a string using the `join()` method. With pandas’ `apply()`, we
    applied the function to stem words to each row of the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: The cleaning steps we performed in this recipe resulted in strings containing
    the original text, without punctuation or numbers, in lowercase, without common
    words, and with the root of the word instead of its conjugated form. The data,
    as it is returned, can be used to derive features, as described in the *Counting
    characters, words, and vocabulary* recipe, or to create BoWs and TI-IDF matrices,
    as described in the *Creating features with bag-of-words and n-grams* and *Implementing
    term frequency-inverse document* *frequency* recipes.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning the texts as we have shown in this recipe can incur data loss, depending
    on the characteristics of the text, and if we seek to interpret the models after
    creating BoW or TF-IDF matrices, understanding the importance of stemmed words
    may not be so straightforward.
  prefs: []
  type: TYPE_NORMAL
