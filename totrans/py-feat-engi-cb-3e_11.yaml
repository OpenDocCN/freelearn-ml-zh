- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Extracting Features from Text Variables
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从文本变量中提取特征
- en: Text can be one of the variables in our datasets. For example, in insurance,
    information describing the circumstances of an incident can come from free text
    fields in a form. If a company gathers customer reviews, this information will
    be collected as short pieces of text provided by the users. Text data does not
    show the **tabular** pattern of the datasets that we have worked with throughout
    this book. Instead, information in texts can vary in length and content, as well
    as writing style. We can extract a lot of information from text variables to use
    as predictive features in machine learning models. The techniques we will cover
    in this chapter belong to the realm of **Natural Language Processing** (**NLP**).
    NLP is a subfield of linguistics and computer science. It is concerned with the
    interactions between computer and human language, or, in other words, how to program
    computers to understand human language. NLP includes a multitude of techniques
    to understand the syntax, semantics, and discourse of text. Therefore, to do this
    field justice would require an entire book.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 文本可以是我们数据集中的一个变量。例如，在保险领域，描述事故情况的详细信息可能来自表单中的自由文本字段。如果一家公司收集客户评价，这些信息将以用户提供的短文本片段的形式收集。文本数据不显示我们在这本书中一直处理的数据集的
    **表格** 模式。相反，文本中的信息在长度、内容和写作风格上可能有所不同。我们可以从文本变量中提取大量信息，用作机器学习模型中的预测特征。本章中我们将涵盖的技术属于
    **自然语言处理** (**NLP**) 的领域。NLP 是语言学和计算机科学的一个子领域，它关注计算机与人类语言之间的交互，换句话说，就是如何编程计算机理解人类语言。NLP
    包括理解文本的句法、语义和语篇的众多技术。因此，要公正地对待这个领域，就需要一本整本书。
- en: In this chapter, we will discuss the methods that will allow us to quickly extract
    features from short pieces of text to complement our predictive models. Specifically,
    we will discuss how to capture a piece of text’s complexity by looking at some
    statistical parameters of the text, such as the word length and count, the number
    of words and unique words used, the number of sentences, and so on. We will use
    the `pandas` and `scikit-learn` libraries, and we will make a shallow dive into
    a very useful Python NLP toolkit called the **Natural Language** **Toolkit** (**NLTK**).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论允许我们快速从短文本片段中提取特征以补充我们的预测模型的方法。具体来说，我们将讨论如何通过查看文本的一些统计参数来捕捉文本的复杂性，例如单词长度和计数、使用的单词和唯一单词的数量、句子的数量等等。我们将使用
    `pandas` 和 `scikit-learn` 库，并且我们将对一个非常有用的 Python NLP 工具包进行浅入浅出的探讨，该工具包称为 **自然语言**
    **工具包** (**NLTK**)。
- en: 'This chapter includes the following recipes:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包括以下食谱：
- en: Counting characters, words, and vocabulary
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算字符、单词和词汇量
- en: Estimating text complexity by counting sentences
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过计算句子来估计文本复杂度
- en: Creating features with bag-of-words and n-grams
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用词袋和 n-gram 创建特征
- en: Implementing term frequency-inverse document frequency
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现词频-逆文档频率
- en: Cleaning and stemming text variables
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清洗和词干提取文本变量
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will use the `pandas`, `matplotlib`, and `scikit-learn`
    Python libraries. We will also use `NLTK`, a comprehensive Python library for
    NLP and text analysis. You can find the instructions to install `NLTK` at [http://www.nltk.org/install.html](http://www.nltk.org/install.html).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用 `pandas`、`matplotlib` 和 `scikit-learn` Python 库。我们还将使用 `NLTK`，这是一个全面的
    Python NLP 和文本分析库。你可以在 [http://www.nltk.org/install.html](http://www.nltk.org/install.html)
    找到安装 `NLTK` 的说明。
- en: If you are using the Python Anaconda distribution, follow the instructions to
    install `NLTK` at [https://anaconda.org/anaconda/nltk](https://anaconda.org/anaconda/nltk).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是 Python Anaconda 发行版，请按照 [https://anaconda.org/anaconda/nltk](https://anaconda.org/anaconda/nltk)
    上的说明安装 `NLTK`。
- en: 'After you have installed `NLTK`, open up a Python console and execute the following:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在你安装了 `NLTK` 之后，打开一个 Python 控制台并执行以下命令：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: These commands will download the necessary data for you to be able to run the
    recipes in this chapter successfully.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这些命令将为你下载运行本章中食谱所需的数据。
- en: Note
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you haven’t downloaded these or the other data sources necessary for `NLTK`
    functionality, `NLTK` will raise an error. Read the error message carefully because
    it will direct you to download the data required to run the command that you are
    trying to execute.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有下载这些或其他必要的 `NLTK` 功能数据源，`NLTK` 将会引发错误。仔细阅读错误信息，因为它会指导你下载运行你试图执行的命令所需的数据。
- en: Counting characters, words, and vocabulary
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算字符、单词和词汇量
- en: One of the salient characteristics of text is its complexity. Long descriptions
    are more likely to contain more information than short descriptions. Texts rich
    in different, unique words are more likely to be richer in detail than texts that
    repeat the same words over and over. In the same way, when we speak, we use many
    short words such as articles and prepositions to build the sentence structure,
    yet the main concept is often derived from the nouns and adjectives we use, which
    tend to be longer words. So, as you can see, even without reading the text, we
    can start inferring how much information the text provides by determining the
    number of words, the number of unique words (non-repeated occurrences of a word),
    the lexical diversity, and the length of those words. In this recipe, we will
    learn how to extract these features from a text variable using `pandas`.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 文本的一个显著特征是其复杂性。长描述比短描述更有可能包含更多信息。包含不同、独特词汇的文本比反复重复相同词汇的文本更可能细节丰富。同样，当我们说话时，我们使用许多短词，如冠词和介词来构建句子结构，而主要概念通常来自我们使用的名词和形容词，这些往往是较长的词。所以，正如你所看到的，即使不阅读文本，我们也可以通过确定单词数量、唯一单词数量（单词的非重复出现）、词汇多样性和这些单词的长度来推断文本提供的信息量。在这个菜谱中，我们将学习如何使用`pandas`从文本变量中提取这些特征。
- en: Getting ready
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We are going to use the `scikit-learn`, which comprises around 18,000 news
    posts on 20 different topics. More details about this dataset can be found on
    the following sites:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用包含大约18,000篇关于20个不同主题的新闻的`scikit-learn`。更多关于这个数据集的详细信息可以在以下网站上找到：
- en: 'The scikit-learn dataset website: [https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset](https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn数据集网站：[https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset](https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset)
- en: 'The home page for the 20 Newsgroup dataset: [http://qwone.com/~jason/20Newsgroups/](http://qwone.com/~jason/20Newsgroups/)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 20个新闻组数据集的主页：[http://qwone.com/~jason/20Newsgroups/](http://qwone.com/~jason/20Newsgroups/)
- en: 'Before jumping into the recipe, let’s discuss the features that we are going
    to derive from these text pieces. We mentioned that longer descriptions, more
    words in the article, a greater variety of unique words, and longer words tend
    to correlate with the amount of information that the article provides. Hence,
    we can capture text complexity by extracting the following information about the
    text:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入菜谱之前，让我们讨论一下我们将从这些文本片段中提取的特征。我们提到，较长的描述、文章中的更多单词、更多唯一词汇和较长的单词往往与文章提供的信息量相关。因此，我们可以通过提取以下关于文本的信息来捕捉文本复杂性：
- en: The total number of characters
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总字符数
- en: The total number of words
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总单词数
- en: The total number of unique words
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 唯一单词的总数
- en: Lexical diversity (total number of words divided by number of unique words)
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词汇多样性（总单词数除以唯一单词数）
- en: Word average length (number of characters divided by number of words)
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词平均长度（字符数除以单词数）
- en: In this recipe, we will extract these numerical features using `pandas`, which
    has extensive string processing functionalities that can be accessed via the `str`
    vectorized string functions for series.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用`pandas`提取这些数值特征，`pandas`具有广泛的字符串处理功能，可以通过`str`向量化字符串函数访问。
- en: How to do it...
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Let’s begin by loading `pandas` and getting the dataset ready:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先加载`pandas`并准备好数据集：
- en: 'Load `pandas` and the dataset from `scikit-learn`:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载`pandas`和来自`scikit-learn`的数据集：
- en: '[PRE1]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s load the train set part of the 20 Newsgroup dataset into a `pandas` DataFrame:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将20个新闻组数据集的火车集部分加载到一个`pandas` DataFrame中：
- en: '[PRE2]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Tip
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: You can print an example of a text from the DataFrame by executing `print(df['text'][1])`.
    Change the number between `[` and `]` to display different texts. Note how every
    text description is a single string composed of letters, numbers, punctuation,
    and spaces. You can check the datatype by executing `type(df["text"][1])`.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过执行`print(df['text'][1])`来打印DataFrame中的文本示例。通过更改`[`和`]`之间的数字来显示不同的文本。注意，每个文本描述都是一个由字母、数字、标点和空格组成的单个字符串。您可以通过执行`type(df["text"][1])`来检查数据类型。
- en: Now that we have the text variable in a `pandas` DataFrame, we are ready to
    extract the features.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将文本变量放入一个`pandas` DataFrame中，我们准备提取特征。
- en: 'Let’s capture the number of characters in each text piece in a new column:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在新列中捕捉每个文本片段中的字符数：
- en: '[PRE3]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Tip
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'You can remove trailing white spaces in a string, including those from new
    lines, before counting the number of characters by adding the `strip()` method
    before the `len()` method, as shown here: `df[''num_char''] =` `df[''text''].str.strip().str.len()`.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在计数字符之前，您可以通过在`len()`方法之前添加`strip()`方法来移除字符串末尾的空白字符，包括新行中的空白字符，如下所示：`df['num_char']
    = df['text'].str.strip().str.len()`。
- en: 'Let’s capture the number of words in each text in a new column:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在新列中捕捉每个文本中的单词数量：
- en: '[PRE4]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: To count words, we use the `pandas` library’s `split()` method, which splits
    a text at white spaces. Check out the output of `split()` by executing, for instance,
    `df["text"].loc[1].split()` to separate the words of the second text of the DataFrame.
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要计算单词数量，我们使用`pandas`库的`split()`方法，该方法在空白处分割文本。例如，通过执行`df["text"].loc[1].split()`来分离DataFrame中第二个文本的单词。
- en: 'Let’s capture the number of *unique* words in each text in a new column:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在新列中捕捉每个文本中的*唯一*单词数量：
- en: '[PRE5]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Python interprets the same word as two different words if one has a capital
    letter. To avoid this behavior, we can apply the `lower()` method before the `split()`
    method.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个单词的首字母大写，Python会将它解释为两个不同的单词。为了避免这种行为，我们可以在`split()`方法之前应用`lower()`方法。
- en: 'Let’s create a feature that captures the lexical diversity – that is, the total
    number of words (*step 4*) compared to the number of unique words (*step 5*):'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个特征来捕捉词汇多样性 – 即总单词数（*步骤4*）与唯一单词数（*步骤5*）的对比：
- en: '[PRE6]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let’s calculate the average word length by dividing the number of characters
    (*step 3*) by the number of words (*step 4*):'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们通过将字符数（*步骤3*）除以单词数（*步骤4*）来计算平均单词长度：
- en: '[PRE7]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'If we execute `df.head()`, we will see the first five rows of data with the
    text and the newly created features:'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果我们执行`df.head()`，我们将看到包含文本和新建特征的前五行数据：
- en: '![Figure 11.1 – A DataFrame with the text variable and features that summarize
    some of the text’s characteristics](img/B22396_11_01.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图11.1 – 包含文本变量和总结文本某些特征的DataFrame](img/B22396_11_01.jpg)'
- en: Figure 11.1 – A DataFrame with the text variable and features that summarize
    some of the text’s characteristics
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1 – 包含文本变量和总结文本某些特征的DataFrame
- en: With that, we have extracted five different features that capture the text complexity,
    which we can use as inputs for our machine learning algorithms.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们已经提取了五个不同的特征来捕捉文本复杂性，我们可以将这些特征用作机器学习算法的输入。
- en: Note
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In this recipe, we created new features from the raw data straight away without
    doing any data cleaning, removing punctuation, or even stemming words. Note that
    these are steps that are performed ahead of most standard NLP procedures. To learn
    more about this, visit the *Cleaning and stemming text variables* recipe at the
    end of this chapter.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们直接从原始数据中创建了新特征，而没有进行任何数据清理、移除标点符号，甚至没有进行词干提取。请注意，这些步骤是在大多数标准NLP过程之前执行的。要了解更多信息，请访问本章末尾的*清洗和词干提取文本变量*菜谱。
- en: How it works...
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we created five new features that capture text complexity by
    utilizing pandas’ `str` to access the built-in `pandas` functionality to work
    with strings. We worked with the text column of the `train` subset of the 20 Newsgroup
    dataset that comes with `scikit-learn`. Each row in this dataset is composed of
    a string with text.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们通过使用`pandas`的`str`来访问内置的`pandas`字符串功能，创建了五个新特征来捕捉文本复杂性。我们处理了`scikit-learn`附带的数据集`20
    Newsgroup`的`train`子集的文本列。这个数据集中的每一行都是一个包含文本的字符串。
- en: We used pandas’ `str`, followed by `len()`, to count the number of characters
    in each string – that is, the total number of letters, numbers, symbols, and spaces.
    We also combined `str.len()` with `str.strip()` to remove trailing white spaces
    at the beginning and end of the string and in new lines, before counting the number
    of characters.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`pandas`的`str`，然后是`len()`，来计算每个字符串中的字符数 – 即字母、数字、符号和空格的总数。我们还结合了`str.len()`和`str.strip()`来移除字符串开头和结尾的空白字符以及在新行中的空白字符，在计数字符之前。
- en: To count the number of words, we used pandas’ `str`, followed by `split()`,
    to divide the string into a list of words. The `split()` method creates a list
    of words by breaking the string at the white spaces between words. Next, we counted
    those words with `str.len()`, obtaining the number of words per string.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算单词数量，我们使用`pandas`的`str`，然后是`split()`，将字符串分割成单词列表。`split()`方法通过在单词之间的空白处断开字符串来创建单词列表。接下来，我们使用`str.len()`来计数这些单词，得到每个字符串的单词数。
- en: Note
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We can change the behavior of `str.split()` by passing a string or character
    that we would like to use to split the string. For example, `df['text'].str.split(';')`
    divides a string at each occurrence of `;`.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过传递一个字符串或字符来改变`str.split()`的行为，我们希望用它来分割字符串。例如，`df['text'].str.split(';')`在`;`的每个出现处分割字符串。
- en: To determine the number of unique words, we used pandas’ `str.split()` function
    to divide the string into a list of words. Next, we applied the built-in Python
    `set()` method within pandas’ `apply()` to return a set of words. Remember that
    a set contains *unique occurrences* of the elements in a list – that is, unique
    words. Next, we counted those words with pandas’ `str.len()` function to return
    the `lower()` function to set all the characters to lowercase before splitting
    the string and counting the number of unique words.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 要确定唯一单词的数量，我们使用了pandas的`str.split()`函数将字符串分割成单词列表。接下来，我们在pandas的`apply()`方法中应用了内置的Python
    `set()`方法，以返回一个单词集合。记住，集合包含列表中元素的*唯一出现* – 那就是唯一单词。接下来，我们使用pandas的`str.len()`函数来计数这些单词，并返回`lower()`函数，在分割字符串和计数唯一单词之前将所有字符设置为小写。
- en: To create the lexical diversity and average word length features, we simply
    performed a vectorized division of two `pandas` series. That’s it; we created
    five new features with information about the complexity of the text.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建词汇多样性和平均词长特征，我们只是简单地执行了两个`pandas`序列的向量除法。就是这样；我们创建了五个关于文本复杂性的新特征。
- en: There’s more...
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: We can check out the distribution of the features extracted from text in each
    of the 20 different news topics present in the dataset by using visualizations.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用可视化来检查数据集中20个不同新闻主题中提取的文本特征的分布。
- en: 'To make histogram plots of the newly created features, after you run all of
    the steps in the *How it works...* section of this recipe, follow these steps:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行本食谱中“如何工作...”部分的全部步骤之后，要制作新创建特征的直方图，请遵循以下步骤：
- en: 'Import `matplotlib`:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`matplotlib`：
- en: '[PRE8]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Add the target with the news topics to the 20 Newsgroup DataFrame:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将新闻主题的目标添加到20个新闻组DataFrame中：
- en: '[PRE9]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Create a function that displays a histogram of a feature of your choice for
    each of the news topics:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数，用于显示每个新闻主题中你选择的特征的直方图：
- en: '[PRE10]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Run the function for the number of words feature:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行单词数量特征的函数：
- en: '[PRE11]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The previous command returns the following plot, where you can see the distribution
    of the number of words in each of the 20 news topics, numbered from 0 to 19 in
    the plot title:'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 之前的命令返回以下图表，其中你可以看到20个新闻主题中每个主题的单词数量分布，图表标题中从0到19编号：
- en: '![Figure 11.2 – Histograms showing the distribution of the number of words
    per text, segregated by topic discussed in each text](img/B22396_11_02.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图11.2 – 每个文本中单词数量的分布直方图，按每个文本中讨论的主题进行细分](img/B22396_11_02.jpg)'
- en: Figure 11.2 – Histograms showing the distribution of the number of words per
    text, segregated by topic discussed in each text
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2 – 每个文本中单词数量的分布直方图，按每个文本中讨论的主题进行细分
- en: The number of words shows a different distribution across the different news
    topics. Therefore, this feature is likely useful in a classification algorithm
    to predict the topic of the text.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 单词数量在不同新闻主题中显示出不同的分布。因此，这个特征可能在分类算法中预测文本主题时很有用。
- en: See also
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: To learn more about pandas’ built-in string processing functionality visit [https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html#method-summary](https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html#method-summary).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于pandas内置字符串处理功能的信息，请访问[https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html#method-summary](https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html#method-summary)。
- en: Estimating text complexity by counting sentences
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过计算句子来估计文本复杂性
- en: One aspect of a piece of text that we can capture in features is its complexity.
    Usually, longer descriptions that contain multiple sentences spread over several
    paragraphs tend to provide more information than descriptions with very few sentences.
    Therefore, capturing the number of sentences may provide some insight into the
    amount of information provided by the text. This process is called `NLTK` Python
    library, which provides this functionality.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在特征中捕捉到的一篇文本的一个方面是其复杂性。通常，包含多个句子并分布在几个段落中的较长的描述，比包含非常少句子的描述提供的信息更多。因此，捕捉句子的数量可能有助于了解文本提供的信息量。这个过程被称为`NLTK`
    Python库，它提供了这个功能。
- en: Getting ready
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will use the `NLTK` Python library. For guidelines on how
    to install `NLTK`, check out the *Technical requirements* section of this chapter.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用`NLTK` Python库。有关如何安装`NLTK`的指南，请参阅本章的*技术要求*部分。
- en: How to do it...
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到...
- en: 'Let’s begin by importing the required libraries and dataset:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先导入所需的库和数据集：
- en: 'Let’s load `pandas`, the sentence tokenizer from `NLTK`, and the dataset from
    `scikit-learn`:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们加载`pandas`、`NLTK`中的句子分词器和`scikit-learn`中的数据集：
- en: '[PRE12]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'To understand the functionality of the sentence tokenizer from `NLTK`, let’s
    create a variable that contains a string with multiple sentences:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了理解`NLTK`中的句子分词器的功能，让我们创建一个包含多个句子的字符串变量：
- en: '[PRE13]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, let’s separate the string from *step 2* into sentences using `NLTK` library‘s
    sentence tokenizer:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用`NLTK`库的句子分词器将步骤2中的字符串拆分为句子：
- en: '[PRE14]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Tip
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: If you encounter an error in *step 3*, read the error message carefully and
    download the data source required by `NLTK`, as described in the error message.
    For more details, check out the *Technical* *requirements* section.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在步骤3中遇到错误，请仔细阅读错误消息并下载`NLTK`所需的数据源，如错误消息中所述。更多详情，请参阅*技术* *要求*部分。
- en: 'The sentence tokenizer returns the list of sentences shown in the following
    output:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 句子分词器返回以下输出中显示的句子列表：
- en: '[PRE15]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The escape character followed by the letter, `\n`, indicates a new line.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 后跟字母的转义字符`\n`表示新的一行。
- en: 'Let’s count the number of sentences in the `text` variable:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们计算`text`变量中的句子数：
- en: '[PRE16]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The previous command returns `8`, which is the number of sentences in our `text`
    variable. Now, let’s determine the number of sentences in an entire DataFrame.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 之前的命令返回`8`，这是我们的`text`变量中的句子数。现在，让我们确定整个DataFrame中的句子数。
- en: 'Let’s load the `train` subset of the 20 Newsgroup dataset into a `pandas` DataFrame:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将20个新闻组数据集的`train`子集加载到`pandas` DataFrame中：
- en: '[PRE17]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'To speed up the following steps, we will only work with the first `10` rows
    of the DataFrame:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了加快以下步骤，我们只处理DataFrame的前`10`行：
- en: '[PRE18]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let’s also remove the first part of the text, which contains information about
    the email sender, subject, and other details that we are not interested in. Most
    of this information comes before the word `Lines` followed by `:`, so let’s split
    the string at `Lines:` and capture the second part of the string:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们也移除文本的第一部分，这部分包含关于电子邮件发送者、主题和其他我们不感兴趣的信息。大部分这些信息都在`Lines`这个词之后，跟着一个冒号`:`，所以让我们在`Lines:`处拆分字符串并捕获字符串的第二部分：
- en: '[PRE19]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Finally, let’s create a variable containing the number of sentences per `text`:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们创建一个包含每`text`中句子数的变量：
- en: '[PRE20]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'With the `df` command, you can display the entire DataFrame with the `text`
    variable and the new feature containing the number of sentences per text:'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用`df`命令，你可以通过`text`变量和新特性（包含每段文本的句子数）显示整个DataFrame：
- en: '![Figure 11.3 – A DataFrame with the text variable and the number of sentences
    per text](img/B22396_11_03.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图11.3 – 包含文本变量和每段文本句子数的DataFrame](img/B22396_11_03.jpg)'
- en: Figure 11.3 – A DataFrame with the text variable and the number of sentences
    per text
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3 – 包含文本变量和每段文本句子数的DataFrame
- en: Now, we can use this new feature as input to machine learning algorithms.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用这个新特性作为机器学习算法的输入。
- en: How it works...
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we separated a string with text into sentences using `sent_tokenizer`
    from the `NLTK` library. `sent_tokenizer` has been pre-trained to recognize capitalization
    and different types of punctuation that signal the beginning and the end of a
    sentence.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们使用`NLTK`库的`sent_tokenizer`将包含文本的字符串拆分为句子。`sent_tokenizer`已经预先训练以识别大写字母和不同类型的标点符号，这些标点符号标志着句子的开始和结束。
- en: First, we applied `sent_tokenizer` to a manually created string to become familiar
    with its functionality. The tokenizer divided the text into a list of eight sentences.
    We combined the tokenizer with the built-in Python `len()` method to count the
    number of sentences in the string.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们手动创建一个字符串并应用`sent_tokenizer`以熟悉其功能。分词器将文本分为一个包含八个句子的列表。我们将分词器与内置的Python
    `len()`方法结合使用，以计算字符串中的句子数。
- en: 'Next, we loaded a dataset with text and, to speed up the computation, we only
    retained the first 10 rows of the DataFrame using pandas’ `loc[]` function. Next,
    we removed the first part of the text, which contained information about the email
    sender and subject. To do this, we split the string at `Lines:` using pandas’
    `str.split("Lines:")` function, which returned a list with two elements: the strings
    before and after `Lines:`. Utilizing a lambda function within `apply()`, we retained
    the second part of the text – that is, the second string in the list returned
    by `split()`.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们加载了一个包含文本的数据集，为了加快计算速度，我们只使用pandas的`loc[]`函数保留了DataFrame的前10行。接下来，我们移除了文本的前一部分，这部分包含了关于电子邮件发送者和主题的信息。为此，我们使用pandas的`str.split("Lines:")`函数在`Lines:`处分割字符串，该函数返回一个包含两个元素的列表：`Lines:`之前的字符串和之后的字符串。利用`apply()`中的lambda函数，我们保留了文本的第二部分，即`split()`返回的列表中的第二个字符串。
- en: Finally, we applied `sent_tokenizer` to each row in the DataFrame with the pandas
    `apply()` method to separate the strings into sentences, and then applied the
    built-in Python `len()` method to the list of sentences to return the number of
    sentences per string. This way, we created a new feature that contained the number
    of sentences per text.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用pandas的`apply()`方法将`sent_tokenizer`应用于DataFrame中的每一行，将字符串分割成句子，然后使用内置的Python
    `len()`方法应用于句子列表，以返回每个字符串的句子数。这样，我们创建了一个包含每个文本的句子数的新特征。
- en: There’s more...
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: '`NLTK` has functionalities for word tokenization among other useful features,
    which we can use instead of `pandas` to count and return the number of words.
    You can find out more about `NLTK`’s functionality here:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`NLTK`除了其他有用功能外，还具有单词分词功能，我们可以用`NLTK`代替`pandas`来计数并返回单词数。你可以在这里了解更多关于`NLTK`功能的信息：'
- en: '*Python 3 Text Processing with NLTK 3 Cookbook*, by Jacob Perkins, Packt Publishing'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《使用NLTK 3进行Python 3文本处理食谱》*，作者：雅各布·珀金斯，Packt出版社'
- en: The `NLTK` documentation at [http://www.nltk.org/](http://www.nltk.org/).
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.nltk.org/](http://www.nltk.org/)上的`NLTK`文档。'
- en: Creating features with bag-of-words and n-grams
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用词袋模型和n-gram创建特征
- en: 'A **Bag-of-Words** (**BoW**) is a simplified representation of a piece of text
    that captures the words that are present in the text and the number of times each
    word appears in the text. So, for the text string *Dogs like cats, but cats do
    not like dogs*, the derived BoW is as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**词袋模型**（**Bag-of-Words**，**BoW**）是对文本的一种简化表示，它捕捉了文本中出现的单词以及每个单词在文本中出现的次数。因此，对于文本字符串
    *Dogs like cats, but cats do not like dogs*，得到的BoW如下：'
- en: "![Figure 11.4 – The BoW derived from the sentence Dogs like cats, but cats\
    \ do not\uFEFF like dogs](img/B22396_11_04.jpg)"
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图11.4 – 从句子 Dogs like cats, but cats do not like dogs 导出的BoW](img/B22396_11_04.jpg)'
- en: Figure 11.4 – The BoW derived from the sentence Dogs like cats, but cats do
    not like dogs
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4 – 从句子 Dogs like cats, but cats do not like dogs 导出的BoW
- en: Here, each word becomes a variable, and the value of the variable represents
    the number of times the word appears in the string. As you can see, the BoW captures
    multiplicity but does not retain word order or grammar. That is why it is a simple,
    yet useful way of extracting features and capturing some information about the
    texts we are working with.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，每个单词成为一个变量，变量的值表示单词在字符串中出现的次数。正如你所见，BoW捕捉了多重性，但没有保留单词顺序或语法。这就是为什么它是一种简单而有用的方式来提取特征并捕捉我们正在处理的文本的一些信息。
- en: 'To capture some syntax, BoW can be used together with **n-grams**. An n-gram
    is a contiguous sequence of *n* items in a given text. Continuing with the sentence
    *Dogs like cats, but cats do not like dogs*, the derived 2-grams are as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '为了捕捉一些语法，BoW可以与**n-gram**一起使用。n-gram是在给定文本中连续的*n*个项的序列。继续使用句子 *Dogs like cats,
    but cats do not like dogs*，得到的2-gram如下： '
- en: Dogs like
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 狗喜欢
- en: like cats
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 喜欢猫
- en: cats but
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 猫和
- en: but do
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 但是
- en: do not
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不
- en: like dogs
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像狗一样
- en: 'We can create, together with a BoW, a bag of n-grams, where the additional
    variables are given by the 2-grams and the values for each 2-gram are the number
    of times they appear in each string; for this example, the value is 1\. So, our
    final BoW with 2-grams would look like this:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建一个与BoW一起的n-gram袋，其中额外的变量由2-gram给出，每个2-gram的值是它们在每个字符串中出现的次数；对于这个例子，值是1。因此，我们的最终包含2-gram的BoW看起来如下：
- en: '![Figure 11.5 – The BoW with 2-grams](img/B22396_11_05.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图11.5 – 包含2-gram的BoW](img/B22396_11_05.jpg)'
- en: Figure 11.5 – The BoW with 2-grams
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5 – 包含2-gram的BoW
- en: In this recipe, we will learn how to create BoWs with or without n-grams using
    `scikit-learn`.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将学习如何使用`scikit-learn`创建带有或不带有n-gram的BoW。
- en: Getting ready
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Before jumping into this recipe, let’s get familiar with some of the parameters
    of a BoW that we can adjust to make the BoW comprehensive. When creating a BoW
    over several pieces of text, a new feature is created for each unique word that
    appears at least once in *any* of the text pieces we are analyzing. If the word
    appears only in one piece of text, it will show a value of 1 for that particular
    text and 0 for all of the others. Therefore, BoWs tend to be sparse matrices,
    where most of the values are zeros.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入这个菜谱之前，让我们熟悉一下BoW的一些参数，我们可以调整这些参数以使BoW更全面。在创建多个文本片段的BoW时，对于我们在分析的文本片段中至少出现一次的每个唯一单词，都会创建一个新特征。如果单词只在一个文本片段中出现，它将为该特定文本显示1的值，而为其他所有文本显示0。因此，BoWs往往是稀疏矩阵，其中大部分值都是零。
- en: The number of columns – that is, the number of words – in a BoW can be quite
    large if we work with huge text corpora, and even larger if we also include n-grams.
    To limit the number of columns and the sparsity of the returned matrix, we can
    retain words that appear across multiple texts; or, in better words, we can retain
    words that appear in, at least, a certain percentage of texts.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们处理大量的文本语料库，BoW中的列数（即单词数）可以相当大，如果我们还包括n-gram，则更大。为了限制列数和返回矩阵的稀疏性，我们可以保留在多个文本中出现的单词；或者换句话说，我们可以保留至少在某个百分比文本中出现的单词。
- en: To reduce the number of columns and sparsity of the BoW, we should also work
    with words in the same case – for example, lowercase – as Python identifies words
    in a different case as different words. We can also reduce the number of columns
    and sparsity by removing **stop words**. Stop words are very frequently used words
    that make sentences flow, but that do not, per se, carry any useful information.
    Examples of stop words are pronouns such as I, you, and he, as well as prepositions
    and articles.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少列数和BoW的稀疏性，我们还应该处理与Python识别单词时相同大小写的单词——例如，小写。我们还可以通过移除**停用词**来减少列数和稀疏性。停用词是非常常用的单词，使句子流畅，但本身并不携带任何有用的信息。停用词的例子包括代词，如我、你和他，以及介词和冠词。
- en: 'In this recipe, we will learn how to set words in lowercase, remove stop words,
    retain words with a minimum acceptable frequency, and capture n-grams all together
    with a single transformer from `scikit-learn`: `CountVectorizer()`.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将学习如何将单词转换为小写，移除停用词，保留具有最低可接受频率的单词，并使用`scikit-learn`的单个转换器`CountVectorizer()`一起捕获n-gram：
- en: How to do it...
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let’s begin by loading the necessary libraries and getting the dataset ready:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先加载必要的库并准备好数据集：
- en: 'Load `pandas`, `CountVectorizer`, and the dataset from `scikit-learn`:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载`pandas`、`CountVectorizer`以及从`scikit-learn`的dataset：
- en: '[PRE21]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Let’s load the train set part of the 20 Newsgroup dataset into a pandas DataFrame:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将20个新闻组数据集的训练集部分加载到pandas DataFrame中：
- en: '[PRE22]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'To make interpreting the results easier, let’s remove punctuation and numbers
    from the text variable:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了使结果更容易解释，让我们从文本变量中移除标点符号和数字：
- en: '[PRE23]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Note
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'To learn more about regex with Python, follow this link: [https://docs.python.org/3/howto/regex.html](https://docs.python.org/3/howto/regex.html)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于Python中的正则表达式，请点击此链接：[https://docs.python.org/3/howto/regex.html](https://docs.python.org/3/howto/regex.html)
- en: 'Now, let’s set up `CountVectorizer()` so that, before creating the BoW, it
    puts the text in lowercase, removes stop words, and retains words that appear
    in, at least, 5% of the text pieces:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们设置`CountVectorizer()`，使其在创建BoW之前将文本转换为小写，移除停用词，并保留至少在5%的文本片段中出现的单词：
- en: '[PRE24]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To introduce n-grams as part of the returned columns, we can change the value
    of `ngrams_range` to, for example, `(1,2)`. The tuple provides the lower and upper
    boundaries of the range of n-values for different n-grams. In the case of `(1,2)`,
    `CountVectorizer()` will return single words and arrays of two consecutive words.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 要将n-gram作为返回列的一部分引入，我们可以将`ngrams_range`的值更改为，例如`(1,2)`。这个元组提供了不同n-gram的n值范围的上下边界。在`(1,2)`的情况下，`CountVectorizer()`将返回单个单词和两个连续单词的数组。
- en: 'Let’s fit `CountVectorizer()` so that it learns which words should be used
    in the BoW:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们调整`CountVectorizer()`，使其学习在BoW中应使用哪些单词：
- en: '[PRE25]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now, let’s create the BoW:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建BoW：
- en: '[PRE26]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Finally, let’s capture the BoW in a DataFrame with the corresponding feature
    names:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们将相应的特征名称与BoW一起捕获到DataFrame中：
- en: '[PRE27]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'With that, we have created a `pandas` DataFrame that contains words as columns
    and the number of times they appeared in each text as values. You can inspect
    the result by executing `bagofwords.head()`:'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过这样，我们创建了一个包含单词作为列和每个文本中它们出现的次数作为值的`pandas` DataFrame。你可以通过执行`bagofwords.head()`来检查结果：
- en: '![Figure 11.6 – A DataFrame with the BoW resulting from the 20 Newsgroup dataset](img/B22396_11_06.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图11.6 – 由20个新闻组数据集生成的BoW DataFrame](img/B22396_11_06.jpg)'
- en: Figure 11.6 – A DataFrame with the BoW resulting from the 20 Newsgroup dataset
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6 – 由20个新闻组数据集生成的BoW DataFrame
- en: We can use this BoW as input for a machine learning model.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个BoW作为机器学习模型的输入。
- en: How it works...
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: scikit-learn’s `CountVectorizer()` converts a collection of text documents into
    a matrix of token counts. These tokens can be individual words or arrays of two
    or more consecutive words – that is, n-grams. In this recipe, we created a BoW
    from a text variable in a DataFrame.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn的`CountVectorizer()`将一组文本文档转换为标记计数的矩阵。这些标记可以是单个单词或两个或更多连续单词的数组——即n-gram。在这个菜谱中，我们从DataFrame中的文本变量创建了一个BoW。
- en: We loaded the 20 Newsgroup text dataset from `scikit-learn` and removed punctuation
    and numbers from the text rows using pandas’ `replace()` function, which can be
    accessed through pandas’ `str` module, to replace digits, `'\d+'`, or symbols,
    `'[^\w\s]'`, with empty strings, `''`. Then, we used `CountVectorizer()` to create
    the BoW. We set the `lowercase` parameter to `True` to put the words in lowercase
    before extracting the BoW. We set the `stop_words` argument to `english` to ignore
    stop words – that is, to avoid stop words in the BoW. We set `ngram_range` to
    the `(1,1)` tuple to return only single words as columns. Finally, we set `min_df`
    to `0.05` to return words that appeared in at least 5% of the texts, or, in other
    words, in 5% of the rows in the DataFrame.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从`scikit-learn`加载了20个新闻组文本数据集，并使用pandas的`replace()`函数从文本行中移除了标点符号和数字，该函数可以通过pandas的`str`模块访问，用空字符串`''`替换数字`'\d+'`或符号`'[^\w\s]'`。然后，我们使用`CountVectorizer()`创建BoW。我们将`lowercase`参数设置为`True`，在提取BoW之前将单词转换为小写。我们将`stop_words`参数设置为`english`以忽略停用词——也就是说，避免BoW中的停用词。我们将`ngram_range`设置为`(1,1)`元组，以仅返回作为列的单个单词。最后，我们将`min_df`设置为`0.05`，以返回至少出现在5%的文本中的单词，换句话说，在DataFrame的5%的行中。
- en: After setting up the transformer, we used the `fit()` method to allow the transformer
    to find the words that fulfill the preceding criteria. Finally, using the `transform()`
    method, the transformer returned an object containing the BoW with its feature
    names, which we captured in a `pandas` DataFrame.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置好转换器之后，我们使用了`fit()`方法让转换器找到满足先前标准的单词。最后，使用`transform()`方法，转换器返回一个包含BoW及其特征名称的对象，我们将它捕获在一个`pandas`
    DataFrame中。
- en: See also
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: For more details about `CountVectorizer()`, visit the `scikit-learn` library’s
    documentation at [https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于`CountVectorizer()`的详细信息，请访问`scikit-learn`库的文档[https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)。
- en: Implementing term frequency-inverse document frequency
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现词频-逆文档频率
- en: '**Term Frequency-Inverse Document Frequency** (**TF-IDF**) is a numerical statistic
    that captures how relevant a word is in a document considering the entire collection
    of documents. What does this mean? Some words will appear a lot within a text
    document as well as across documents, such as the English words *the*, *a*, and
    *is*, for example. These words generally convey little information about the actual
    content of the document and don’t make the text stand out from the crowd. TF-IDF
    provides a way to *weigh* the importance of a word by considering how many times
    it appears in a document with regards to how often it appears across documents.
    Hence, commonly occurring words such as *the*, *a*, or *is* will have a low weight,
    and words that are more specific to a topic, such as *leopard*, will have a higher
    weight.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**词频-逆文档频率**（**TF-IDF**）是一个数值统计量，它捕捉了单词在考虑整个文档集合的情况下在文档中的相关性。这意味着什么？一些单词在文本文档中以及跨文档中都会出现很多次，例如，英语单词
    *the*、*a* 和 *is* 等。这些单词通常对文档的实际内容传达很少信息，并且不会使文本脱颖而出。TF-IDF 通过考虑单词在文档中出现的次数与在文档中出现的频率之间的关系来提供衡量单词重要性的方法。因此，常见的单词如
    *the*、*a* 或 *is* 将具有较低的权重，而更具体于某个主题的单词，如 *leopard*，将具有更高的权重。'
- en: 'TF-IDF is the product of two statistics: **Term Frequency** (**tf**) and **Inverse
    Document Frequency** (**idf**), represented as follows: **tf-idf = td × idf**.
    tf is, in its simplest form, the count of the word in an individual text. So,
    for term *t*, the tf is calculated as *tf(t) = count(t)* and is determined on
    a text-by-text basis. The idf is a measure of how common the word is across *all*
    documents and is usually calculated on a logarithmic scale. A common implementation
    is given by the following:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF 是两个统计量的乘积：**词频**（**tf**）和**逆文档频率**（**idf**），表示如下：**tf-idf = tf × idf**。tf
    在其最简单形式中，是单个文本中单词的计数。因此，对于术语 *t*，tf 的计算为 *tf(t) = count(t)*，并且基于文本进行确定。idf 是衡量单词在
    *所有* 文档中普遍程度的一个指标，通常在对数尺度上计算。一个常见的实现如下：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>i</mi><mi>d</mi><mi>f</mi><mfenced
    open="(" close=")"><mi>t</mi></mfenced><mo>=</mo><mrow><mrow><mi>log</mi><mo>(</mo></mrow></mrow><mrow><mrow><mfrac><mi>n</mi><mrow><mn>1</mn><mo>+</mo><mi>d</mi><mi>f</mi><mo>(</mo><mi>t</mi><mo>)</mo></mrow></mfrac><mo>)</mo></mrow></mrow></mrow></mrow></math>](img/48.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>i</mi><mi>d</mi><mi>f</mi><mfenced
    open="(" close=")"><mi>t</mi></mfenced><mo>=</mo><mrow><mrow><mi>log</mi><mo>(</mo></mrow></mrow><mrow><mrow><mfrac><mi>n</mi><mrow><mn>1</mn><mo>+</mo><mi>d</mi><mi>f</mi><mo>(</mo><mi>t</mi><mo>)</mo></mrow></mfrac><mo>)</mo></mrow></mrow></mrow></mrow></math>](img/48.png)'
- en: Here, *n* is the total number of documents, and *df(t)* is the number of documents
    in which the term *t* appears. The bigger the value of *df(t)*, the lower the
    weighting for the term. The importance of a word will be high if it appears a
    lot of times in a text (high *tf*) or few times across texts (high *idf*).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*n* 是文档的总数，*df(t)* 是包含术语 *t* 的文档数。*df(t)* 的值越大，该术语的权重越低。如果一个单词在文本中出现的次数很多（高
    *tf*）或在文本中出现的次数很少（高 *idf*），则该单词的重要性会很高。
- en: Note
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: TF-IDF can be used together with n-grams. Similarly, to weigh an n-gram, we
    compound the n-gram frequency in a certain document with the frequency of the
    n-gram across documents.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF 可以与 n-gram 结合使用。同样，为了权衡 n-gram，我们将某个文档中 n-gram 的频率与跨文档中 n-gram 的频率相乘。
- en: In this recipe, we will learn how to extract features using TF-IDF with or without
    n-grams using `scikit-learn`.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将学习如何使用 `scikit-learn` 通过 n-gram 或不使用 n-gram 提取 TF-IDF 特征。
- en: Getting ready
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: '`scikit-learn` uses a slightly different way to calculate the IDF statistic:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn` 使用一种稍微不同的方式来计算 IDF 统计量：'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:math>](img/49.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:math>](img/49.png)'
- en: This formulation ensures that a word that appears in all texts receives the
    lowest weight of 1\. In addition, after calculating the TF-IDF for every word,
    `scikit-learn` normalizes the feature vector (that with all the words) to its
    Euclidean norm. For more details on the exact formula, visit the `scikit-learn`
    documentation at [https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这种公式确保了出现在所有文本中的单词获得最低权重1。此外，在计算每个单词的TF-IDF之后，`scikit-learn`将特征向量（包含所有单词的向量）归一化到其欧几里得范数。有关确切公式的更多详细信息，请访问[https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)的`scikit-learn`文档。
- en: TF-IDF shares the characteristics of BoW when creating the term matrix – that
    is, high feature space and sparsity. To reduce the number of features and sparsity,
    we can remove stop words, set the characters to lowercase, and retain words that
    appear in a minimum percentage of observations. If you are unfamiliar with these
    terms, visit the *Creating features with bag-of-words and n-grams* recipe in this
    chapter for a recap.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF在创建术语矩阵时具有与BoW相同的特征——即高特征空间和稀疏性。为了减少特征数量和稀疏性，我们可以移除停用词，将字符转换为小写，并保留在最小百分比观察中出现过的单词。如果您不熟悉这些术语，请访问本章中的*使用词袋和n-gram创建特征*菜谱进行复习。
- en: 'In this recipe, we will learn how to set words into lowercase, remove stop
    words, retain words with a minimum acceptable frequency, capture n-grams, and
    then return the TF-IDF statistic of words, all using a single transformer from
    scikit-learn: `TfidfVectorizer()`.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将学习如何将单词转换为小写，移除停用词，保留具有最低可接受频率的单词，捕获n-gram，然后使用scikit-learn的单个转换器`TfidfVectorizer()`返回单词的TF-IDF统计量。
- en: How to do it...
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Let’s begin by loading the necessary libraries and getting the dataset ready:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先加载必要的库并准备数据集：
- en: 'Load `pandas`, `TfidfVectorizer()`, and the dataset from `scikit-learn`:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载`pandas`、`TfidfVectorizer()`以及从`scikit-learn`中的数据集：
- en: '[PRE28]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Let’s load the train set part of the 20 Newsgroup dataset into a pandas DataFrame:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将20个新闻组数据集的火车集部分加载到一个pandas DataFrame中：
- en: '[PRE29]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'To make interpreting the results easier, let’s remove punctuation and numbers
    from the text variable:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了使结果更容易解释，让我们从文本变量中移除标点符号和数字：
- en: '[PRE30]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, let’s set up `TfidfVectorizer()` from `scikit-learn` so that, before creating
    the TF-IDF metrics, it puts all text in lowercase, removes stop words, and retains
    words that appear in at least 5% of the text pieces:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们设置`scikit-learn`中的`TfidfVectorizer()`，以便在创建TF-IDF度量之前，将所有文本转换为小写，移除停用词，并保留至少在5%的文本片段中出现的单词：
- en: '[PRE31]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Note
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To introduce n-grams as part of the returned columns, we can change the value
    of `ngrams_range` to, for example, `(1,2)`. The tuple provides the lower and upper
    boundaries of the range of n-values for different n-grams. In the case of `(1,2)`,
    `TfidfVectorizer()` will return single words and arrays of two consecutive words
    as columns.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将n-gram作为返回列的一部分引入，我们可以将`ngrams_range`的值更改为，例如`(1,2)`。这个元组提供了不同n-gram的n值范围的上下界。在`(1,2)`的情况下，`TfidfVectorizer()`将返回单个单词和两个连续单词的数组作为列。
- en: 'Let’s fit `TfidfVectorizer()` so that it learns which words should be introduced
    as columns of the TF-IDF matrix and determines the words’ `idf`:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们拟合`TfidfVectorizer()`，以便它学习哪些单词应该作为TF-IDF矩阵的列引入，并确定单词的`idf`：
- en: '[PRE32]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, let’s create the TF-IDF matrix:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建TF-IDF矩阵：
- en: '[PRE33]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Finally, let’s capture the TF-IDF matrix in a DataFrame with the corresponding
    feature names:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们将TF-IDF矩阵捕获到一个具有相应特征名称的DataFrame中：
- en: '[PRE34]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'With that, we have created a `pandas` DataFrame that contains words as columns
    and the TF-IDF as values. You can inspect the result by executing `tfidf.head()`:'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过这样，我们创建了一个包含单词作为列和TF-IDF作为值的`pandas` DataFrame。您可以通过执行`tfidf.head()`来检查结果：
- en: '![Figure 11.7 – A DataFrame with features resulting from TF-IDF](img/B22396_11_07.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图11.7 – 由TF-IDF产生的特征DataFrame](img/B22396_11_07.jpg)'
- en: Figure 11.7 – A DataFrame with features resulting from TF-IDF
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.7 – 由TF-IDF产生的特征DataFrame
- en: Now, we can use this term frequency DataFrame to train machine learning models.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用这个词频DataFrame来训练机器学习模型。
- en: How it works...
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we extracted the TF-IDF values of words present in at least
    5% of the documents by utilizing `TfidfVectorizer()` from scikit-learn.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们通过使用scikit-learn的`TfidfVectorizer()`提取了至少存在于5%的文档中的单词的TF-IDF值。
- en: We loaded the 20 Newsgroup text dataset from `scikit-learn` and then removed
    punctuation and numbers from the text rows using pandas’ `replace()`, which can
    be accessed through pandas’ `str`, to replace digits, `'\d+'`, or symbols, `'[^\w\s]'`,
    with empty strings, `''`. Then, we used `TfidfVectorizer()` to create TF-IDF statistics
    for words. We set the `lowercase` parameter to `True` to put words into lowercase
    before making the calculations. We set the `stop_words` argument to `english`
    to avoid stop words in the returned matrix. We set `ngram_range` to the `(1,1)`
    tuple to return single words as features. Finally, we set the `min_df` argument
    to `0.05` to return words that appear at least in 5% of the texts or, in other
    words, in 5% of the rows.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从`scikit-learn`加载了20个新闻组文本数据集，然后使用pandas的`replace()`从文本行中删除了标点符号和数字，这可以通过pandas的`str`访问，用空字符串`''`替换数字`'\d+'`或符号`'[^\w\s]'`。然后，我们使用`TfidfVectorizer()`为单词创建TF-IDF统计信息。我们将`lowercase`参数设置为`True`，在计算之前将单词转换为小写。我们将`stop_words`参数设置为`english`，以避免返回矩阵中的停用词。我们将`ngram_range`设置为`(1,1)`元组，以返回单个单词作为特征。最后，我们将`min_df`参数设置为`0.05`，以返回至少出现在5%的文本或换句话说，在5%的行中的单词。
- en: After setting up the transformer, we applied the `fit()` method to let the transformer
    find the words to retain in the final term matrix. With the `transform()` method,
    the transformer returned an object with the words and their TF-IDF values, which
    we then captured in a pandas DataFrame with the appropriate feature names. We
    can now use these features in machine learning algorithms.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置完转换器后，我们应用了`fit()`方法，让转换器找到最终项矩阵中需要保留的单词。使用`transform()`方法，转换器返回了一个包含单词及其TF-IDF值的对象，然后我们将其捕获到一个具有适当特征名称的pandas
    DataFrame中。现在，我们可以将这些特征用于机器学习算法。
- en: See also
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'For more details on `TfidfVectorizer()`, visit scikit-learn’s documentation:
    [https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 有关`TfidfVectorizer()`的更多详细信息，请访问scikit-learn的文档：[https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)
- en: Cleaning and stemming text variables
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 清洗和词干化文本变量
- en: Some variables in our dataset come from free text fields, which are manually
    completed by users. People have different writing styles, and we use a variety
    of punctuation marks, capitalization patterns, and verb conjugations to convey
    the content, as well as the emotions surrounding it. We can extract (some) information
    from text without taking the trouble to read it by creating statistical parameters
    that summarize the text’s complexity, keywords, and relevance of words in a document.
    We discussed these methods in the previous recipes of this chapter. However, to
    derive these statistics and aggregated features, we should clean the text variables
    first.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据集中的某些变量来自自由文本字段，这些字段由用户手动填写。人们有不同的写作风格，我们使用各种标点符号、大写模式、动词变位来传达内容以及与之相关的情感。我们可以通过创建总结文本复杂度、关键词和文档中单词相关性的统计参数来从文本中提取（一些）信息，而无需费心阅读它。我们已在本章前面的菜谱中讨论了这些方法。然而，为了推导这些统计信息和聚合特征，我们首先应该清理文本变量。
- en: Text cleaning or preprocessing involves punctuation removal, stop word elimination,
    character case setting, and word stemming. Punctuation removal consists of deleting
    characters that are not letters, numbers, or spaces; in some cases, we also remove
    numbers. The elimination of stop words refers to removing common words that are
    used in our language to allow for the sentence structure and flow, but that individually
    convey little or no information. Examples of stop words include articles such
    as *the* and *a* for the English language, as well as pronouns such as *I*, *you*
    and *they*, and commonly used verbs in their various conjugations, such as the
    verbs *to be* and *to have*, as well as the auxiliary verbs *would* and *do*.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 文本清理或预处理包括标点符号删除、停用词消除、字符大小写设置和词干提取。标点符号删除包括删除不是字母、数字或空格的字符；在某些情况下，我们也会删除数字。停用词消除是指删除在我们语言中用于允许句子结构和流畅性的常见单词，但它们各自传达的信息很少或没有。例如，英语中的停用词包括诸如*the*和*a*之类的冠词，以及诸如*I*、*you*和*they*之类的代词，以及各种变形中常用的动词，如*to
    be*和*to have*，以及诸如*would*和*do*之类的助动词。
- en: To allow computers to identify words correctly, it is also necessary to set
    all the words in the same case, since the words *Toy* and *toy* would be identified
    as being different by a computer due to the uppercase *T* in the first one.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让计算机正确识别单词，还需要将所有单词设置为相同的格式，因为由于第一个单词中的大写*T*，计算机会将*Toy*和*toy*识别为不同的单词。
- en: Finally, to focus on the *message* of the text, we don’t want computers to consider
    words differently if they show different conjugations. Hence, we would use word
    stemming as part of the preprocessing pipeline. Word stemming refers to reducing
    each word to its root or base so that the words *playing*, *plays*, and *played*
    become *play*, which, in essence, conveys the same or very similar meaning.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了专注于文本的*信息*，我们不希望计算机在单词不同变形的情况下将单词视为不同。因此，我们将使用词干提取作为预处理流程的一部分。词干提取是指将每个单词还原为其词根或基础，使得单词*playing*、*plays*和*played*变为*play*，本质上传达的是相同或非常相似的意义。
- en: In this recipe, we will learn how to remove punctuation and stop words, set
    words in lowercase, and perform word stemming with pandas and `NLTK`.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将学习如何使用pandas和`NLTK`删除标点符号、停用词，将单词转换为小写，并执行词干提取。
- en: Getting ready
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We are going to use the `NLTK` stem package to perform word stemming, which
    incorporates different algorithms to stem words from English and other languages.
    Each method differs in the algorithm it uses to find the *root* of the word; therefore,
    they may output slightly different results. I recommend reading more about it,
    trying different methods, and choosing the one that serves the project you are
    working on.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`NLTK`的词干提取包来执行词干提取，该包包含不同的算法，可以从英语和其他语言中提取单词。每种方法在寻找单词的*根*时使用的算法不同，因此它们可能会输出略微不同的结果。我建议您了解更多相关信息，尝试不同的方法，并选择适合您正在工作的项目的那个。
- en: More information about NLTK stemmers can be found at [https://www.nltk.org/api/nltk.stem.html](https://www.nltk.org/api/nltk.stem.html).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于NLTK词干提取器的信息可以在[https://www.nltk.org/api/nltk.stem.html](https://www.nltk.org/api/nltk.stem.html)找到。
- en: How to do it...
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let’s begin by loading the necessary libraries and getting the dataset ready:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先加载必要的库并准备数据集：
- en: 'Load `pandas`, `stopwords`, and `SnowballStemmer` from `NLTK` and the dataset
    from `scikit-learn`:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`NLTK`加载`pandas`、`stopwords`和`SnowballStemmer`，以及从`scikit-learn`加载数据集：
- en: '[PRE35]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Let’s load the train set part of the 20 Newsgroup dataset into a pandas DataFrame:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先将20个新闻组数据集的训练集部分加载到pandas DataFrame中：
- en: '[PRE36]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Now, let’s begin with the text cleaning.
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，让我们开始文本清理。
- en: Note
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: After executing each of the commands in this recipe, print some example texts
    by executing, for example, `print(df['text'][10])` so that you can visualize the
    changes introduced to the text. Go ahead and do it now, and then repeat the command
    after each step.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行本食谱中的每个命令后，通过执行例如`print(df['text'][10])`之类的命令来打印一些示例文本，以便您可以可视化对文本引入的变化。现在就做吧，然后在每个步骤之后重复该命令。
- en: 'Let’s begin by removing the punctuation:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先删除标点符号：
- en: '[PRE37]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Tip
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: You can also remove the punctuation using the built-in `string` module from
    Python. First, import the module by executing `import string` and then execute
    `df['text'] =` `df['text'].str.replace('[{}]'.format(string.punctuation), '')`.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用Python内置的`string`模块来删除标点符号。首先，通过执行`import string`来导入模块，然后执行`df['text']
    = df['text'].str.replace('[{}]'.format(string.punctuation), '')`。
- en: 'We can also remove characters that are numbers, leaving only letters, as follows:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们也可以移除数字字符，只留下字母，如下所示：
- en: '[PRE38]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now, let’s set all words into lowercase:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们将所有单词转换为小写：
- en: '[PRE39]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Now, let’s start the process of removing stop words.
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，让我们开始移除停用词的过程。
- en: Note
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '*Step 6* may fail if you did not download the `NLTK` library’s `stopwords`.
    Visit the *Technical requirements* section in this chapter for more details.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有下载`NLTK`库的`stopwords`，*步骤6*可能会失败。请访问本章的*技术要求*部分以获取更多详细信息。
- en: 'Let’s create a function that splits a string into a list of words, removes
    the stop words, and finally concatenates the remaining words back into a string:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个函数，该函数可以将字符串分割成单词列表，移除停用词，并将剩余的单词重新连接成字符串：
- en: '[PRE40]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Note
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To be able to process the data with the `scikit-learn` library’s `CountVectorizer()`
    or `TfidfVectorizer()`, we need the text to be in string format. Therefore, after
    removing the stop words, we need to return the words as a single string. We have
    transformed the NLTK library’s stop words list into a set because sets are faster
    to scan than lists. This improves the computation time.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够使用`scikit-learn`库的`CountVectorizer()`或`TfidfVectorizer()`处理数据，我们需要文本以字符串格式存在。因此，在移除停用词后，我们需要将单词作为单个字符串返回。我们已经将NLTK库的停用词列表转换为一个集合，因为集合比列表扫描更快，这提高了计算时间。
- en: 'Now, let’s use the function from *step 6* to remove stop words from the `text`
    variable:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用第*6步*中的函数从`text`变量中移除停用词：
- en: '[PRE41]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: If you want to know which words are stop words, execute `stopwords.words('english')`.
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你想知道哪些单词是停用词，请执行`stopwords.words('english')`。
- en: Finally, let’s stem the words in our data. We will use `SnowballStemmer` from
    `NLTK` to do so.
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，让我们使用`NLTK`的`SnowballStemer`来提取我们数据中的词干。
- en: 'Let’s create an instance of `SnowballStemer` for the English language:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们为英语语言创建一个`SnowballStemer`实例：
- en: '[PRE42]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Tip
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: Try the stemmer in a single word to see how it works; for example, run `stemmer.stem('running')`.
    You should see `run` as the result of that command. Try different words!
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试在单个单词上使用词干提取器以查看其工作方式；例如，运行`stemmer.stem('running')`。你应该看到`run`作为该命令的结果。尝试不同的单词！
- en: 'Let’s create a function that splits a string into a list of words, applies
    `stemmer` to each word, and finally concatenates the stemmed word list back into
    a string:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个函数，该函数可以将字符串分割成单词列表，对每个单词应用`stemmer`，并将词干列表重新连接成字符串：
- en: '[PRE43]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Let’s use the function from *step 9* to stem the words in our data:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用第*9步*中的函数来对数据中的单词进行词干提取：
- en: '[PRE44]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Now, our text is ready to create features based on character and word counts,
    as well as create BoWs or TF-IDF matrices, as described in the previous recipes
    of this chapter.
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们的文本已经准备好根据字符和单词计数创建特征，以及创建BoWs或TF-IDF矩阵，正如本章前面的食谱所描述的。
- en: 'If we execute `print(df[''text''][10])`, we will see a text example after cleaning:'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果我们执行`print(df['text'][10])`，我们将在清理后看到一个文本示例：
- en: '[PRE45]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Note
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you are counting sentences, you need to do that before removing punctuation,
    as punctuation and capitalization are needed to define the boundaries of each
    sentence.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在计算句子数，你需要在移除标点符号之前这样做，因为标点和大小写是定义每个句子边界的必要条件。
- en: How it works...
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we removed punctuation, numbers, and stop words from a text
    variable, set the words in lowercase, and finally, stemmed the words to their
    root. We removed punctuation and numbers from the text variable using pandas’
    `replace()`, which can be accessed through pandas’ `str`, to replace digits, `'\d+'`,
    or symbols, `'[^\w\s]'`, with empty strings, `''`. Alternatively, we can use the
    `punctuation` module from the built-in `string` package.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们从文本变量中移除了标点符号、数字和停用词，将单词转换为小写，并最终将单词还原到词根。我们使用pandas的`replace()`函数从文本变量中移除标点符号和数字，该函数可以通过pandas的`str`访问，用空字符串`''`替换数字`'\d+'`或符号`'[^\w\s]'`。或者，我们可以使用内置`string`包中的`punctuation`模块。
- en: Tip
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: Run `string.punctuation` in your Python console after importing `string` to
    check out the symbols that will be replaced with empty strings.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入`string`后，在Python控制台中运行`string.punctuation`以查看将被替换为空字符串的符号。
- en: Next, utilizing pandas’ string processing functionality through `str`, we set
    all of the words to lowercase with the `lower()` method. To remove stop words
    from the text, we used the `stopwords` module from `NLTK`, which contains a list
    of words that are considered frequent – that is, the stop words. We created a
    function that takes a string and splits it into a list of words using pandas’
    `str.split()`, and then, with list comprehension, we looped over the words in
    the list and retained the non-stop words. Finally, with the `join()` method, we
    concatenated the retained words back into a string. We used the built-in Python
    `set()` method over the `NLTK` stop words list to improve computation efficiency
    since it is faster to iterate over sets than over lists. Finally, with pandas’
    `apply()`, we applied the function to each row of our text data.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，通过pandas的字符串处理功能`str`，我们使用`lower()`方法将所有单词转换为小写。为了从文本中移除停用词，我们使用了`NLTK`中的`stopwords`模块，该模块包含了一组频繁出现的单词列表——即停用词。我们创建了一个函数，该函数接受一个字符串并使用pandas的`str.split()`将其拆分为单词列表，然后使用列表推导，我们遍历列表中的单词并保留非停用词。最后，使用`join()`方法，我们将保留的单词重新连接成字符串。我们使用Python内置的`set()`方法在`NLTK`停用词列表上提高计算效率，因为遍历集合比遍历列表要快。最后，通过pandas的`apply()`，我们将该函数应用于文本数据的每一行。
- en: Tip
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: Run `stopwords.words('english')` in your Python console after importing `stopwords`
    from `NLTK` to visualize the list with the stop words that will be removed.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在从`NLTK`导入`stopwords`后，在Python控制台中运行`stopwords.words('english')`以可视化将要被移除的停用词列表。
- en: Finally, we stemmed the words using `SnowballStemmer` from `NLTK`. `SnowballStemmer`
    works one word at a time. Therefore, we created a function that takes a string
    and splits it into a list of words using pandas’ `str.split()`. In a list comprehension,
    we applied `SnowballStemmer` word per word and then concatenated the list of stemmed
    words back into a string using the `join()` method. With pandas’ `apply()`, we
    applied the function to stem words to each row of the DataFrame.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用`NLTK`中的`SnowballStemmer`对单词进行了词干提取。`SnowballStemmer`一次处理一个单词。因此，我们创建了一个函数，该函数接受一个字符串并使用pandas的`str.split()`将其拆分为单词列表。在列表推导中，我们逐个单词应用`SnowballStemmer`，然后使用`join()`方法将提取的词干单词列表重新连接成字符串。通过pandas的`apply()`，我们将该函数应用于DataFrame的每一行以提取词干。
- en: The cleaning steps we performed in this recipe resulted in strings containing
    the original text, without punctuation or numbers, in lowercase, without common
    words, and with the root of the word instead of its conjugated form. The data,
    as it is returned, can be used to derive features, as described in the *Counting
    characters, words, and vocabulary* recipe, or to create BoWs and TI-IDF matrices,
    as described in the *Creating features with bag-of-words and n-grams* and *Implementing
    term frequency-inverse document* *frequency* recipes.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中我们执行的清洗步骤产生了包含原始文本的字符串，没有标点符号或数字，全部小写，没有常用词汇，并且使用词根而不是其变形。返回的数据可以用来推导特征，如*计数字符、单词和词汇*食谱中所述，或者创建BoWs和TI-IDF矩阵，如*使用词袋和n-gram创建特征*和*实现词频-逆文档频率*食谱中所述。
- en: Cleaning the texts as we have shown in this recipe can incur data loss, depending
    on the characteristics of the text, and if we seek to interpret the models after
    creating BoW or TF-IDF matrices, understanding the importance of stemmed words
    may not be so straightforward.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 如本食谱所示，清洗文本可能会根据文本的特征导致数据丢失，如果我们希望在创建BoW或TF-IDF矩阵后解释模型，理解词干的重要性可能并不那么直接。
