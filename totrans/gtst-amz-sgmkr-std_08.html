<html><head></head><body>
		<div id="_idContainer083">
			<h1 id="_idParaDest-91"><em class="italic"><a id="_idTextAnchor090"/>Chapter 6</em>: Detecting ML Bias and Explaining Models with SageMaker Clarify</h1>
			<p><strong class="bold">Machine learning</strong> (<strong class="bold">ML</strong>) models are increasingly being used to help make business decisions across industries, such as in financial services, healthcare, education, and human resources (HR), thanks to the automation ML provides, with improved accuracy over humans. However, ML models are never perfect. They can make poor decisions—even unfair ones if not trained and evaluated carefully. An ML model can be biased in a way that hurts disadvantaged groups. Having an ability to understand bias in data and ML models during the ML life cycle is critical for creating a socially fair ML model. <strong class="bold">SageMaker Clarify</strong> computes ML biases in datasets and in ML models to help you gain an understanding of the limitation of ML models so that you can take appropriate action to mitigate these biases. </p>
			<p>ML models have long been considered as black box operations because it is rather difficult to see how a prediction is made. SageMaker Clarify computes feature attribution to help you explain how an ML model makes a decision so that it is no longer a black box to us. SageMaker Clarify integrates with SageMaker Studio so that you can easily review the results while building ML models. With SageMaker Clarify, you will be able to know more about your ML models, promote fairness and explainability in your ML use cases, and meet regulatory requirements if required.</p>
			<p>In this chapter, we will be learning about the following topics:</p>
			<ul>
				<li>Understanding bias, fairness in ML, and ML explainability</li>
				<li>Detecting bias in ML</li>
				<li>Explaining ML models using <strong class="bold">SHapley Additive exPlanations</strong> (<strong class="bold">SHAP</strong>) values</li>
			</ul>
			<h1 id="_idParaDest-92"><a id="_idTextAnchor091"/>Technical requirements</h1>
			<p>For this chapter, you need to access the code provided at <a href="https://github.com/PacktPublishing/Getting-Started-with-Amazon-SageMaker-Studio/tree/main/chapter06">https://github.com/PacktPublishing/Getting-Started-with-Amazon-SageMaker-Studio/tree/main/chapter06</a>.</p>
			<h1 id="_idParaDest-93"><a id="_idTextAnchor092"/>Understanding bias, fairness in ML, and ML explainability</h1>
			<p>There are two <a id="_idIndexMarker380"/>types of bias in ML that we can analyze and <a id="_idIndexMarker381"/>mitigate to ensure fairness—<strong class="bold">data bias</strong> and <strong class="bold">model bias</strong>. <strong class="bold">Data bias</strong> is an imbalance <a id="_idIndexMarker382"/>in the training data across different groups and categories that can be introduced into an ML solution simply due to <a id="_idIndexMarker383"/>a sampling error, or intricately due to inherent <a id="_idIndexMarker384"/>reasons that are unfortunately ingrained in society. Data bias, if neglected, can translate into poor accuracy in general and unfair prediction against a certain group in a trained model. It is more critical than ever to be able <a id="_idIndexMarker385"/>to discover inherent biases in the data early and take action to address them. <strong class="bold">Model bias</strong>, on the other hand, refers to bias introduced by model prediction, such as the distribution of classification and errors among <a id="_idIndexMarker386"/>advantaged and disadvantaged groups. Should the model favor an advantaged group for a particular outcome or disproportionally predict incorrectly <a id="_idIndexMarker387"/>for a disadvantaged group, causing undesirable consequences in real-world ML applications such as loan-approval prediction systems, we as data scientists need to take action to understand why this has happened and mitigate the behavior. </p>
			<p>Ensuring fairness in ML <a id="_idIndexMarker388"/>starts with understanding the data and detecting biases within it. Data bias may lead to model bias, as it is well understood that the model will learn what is presented in the data, including any bias, and will replicate that bias in its inferences. Quantifying biases using metrics that are developed and accepted by the ML community is key to detection and choosing mitigation approaches. </p>
			<p>Being able to explain how the model makes a decision is another key factor to ensure fairness in ML models. People had long thought that ML is a magical black box—it predicts things better than humans can, but nobody knows why or how. But ML researchers have developed frameworks to help unbox the black box, the most notable one being SHAP. SHAP computes and assigns an importance score for each feature for a particular prediction. This importance <a id="_idIndexMarker389"/>score is called a <strong class="bold">Shapley value</strong> and is an implementation of cooperative game theory to allocate credit for a model's output among its input features. With the Shapley values for each feature for a prediction, we can describe how and why the model makes such a prediction and which feature contributes the most to this. Should there be a sensitive feature that contributes significantly to model prediction, we need to take action to address this effect. </p>
			<p><strong class="bold">Amazon SageMaker Clarify</strong> helps developers discover underlying bias in the training data and <a id="_idIndexMarker390"/>model prediction and explain feature importance for an ML model. SageMaker Clarify computes various metrics to measure bias in the data so that you do not have to be an expert in the science of ML bias. You can use <a id="_idIndexMarker391"/>SageMaker Clarify with the SageMaker <strong class="bold">software development kit</strong> (<strong class="bold">SDK</strong>) to analyze data and models from a notebook, which we will focus on in this chapter. SageMaker Clarify also integrates with Amazon SageMaker Data Wrangler so that you can detect bias using a simple graphical interface. SageMaker <a id="_idIndexMarker392"/>Clarify further integrates with <strong class="bold">Amazon SageMaker Experiments</strong> to provide graphical results for each experiment and <strong class="bold">Amazon SageMaker Model Monitor</strong> so that you can identify bias and feature importance <a id="_idIndexMarker393"/>in a trained model and in inference data in production.</p>
			<p>Let's get started with an ML example to see how we can detect bias using SageMaker Clarify.</p>
			<h1 id="_idParaDest-94"><a id="_idTextAnchor093"/>Detecting bias in ML</h1>
			<p>For this <a id="_idIndexMarker394"/>chapter, I'd like to use an ML adult census <a id="_idIndexMarker395"/>income dataset from the <strong class="bold">University of California Irvine</strong> (<strong class="bold">UCI</strong>) ML repository (<a href="https://archive.ics.uci.edu/ml/datasets/adult">https://archive.ics.uci.edu/ml/datasets/adult</a>). This dataset contains demographic information <a id="_idIndexMarker396"/>from census data and income level as a prediction target. The goal of the dataset is to predict whether a person earns over or below <strong class="bold">United States dollars</strong> (<strong class="bold">USD</strong>) <strong class="bold">$50,000</strong> (<strong class="bold">$50K</strong>) per year based on the census information. This is a great example and is the type of ML use case that includes socially sensitive <a id="_idIndexMarker397"/>categories such as gender and race, and is under the most scrutiny and regulation to ensure fairness when producing an ML model. </p>
			<p>In this section, we will analyze the dataset to detect data bias in the training data, mitigate if there is any bias, train an ML model, and analyze whether there is any model bias against a particular group.</p>
			<h2 id="_idParaDest-95"><a id="_idTextAnchor094"/>Detecting pretraining bias</h2>
			<p>Please open <a id="_idIndexMarker398"/>the notebook in <strong class="source-inline">Getting-Started-with-Amazon-SageMaker-Studio</strong><strong class="source-inline">/chapter06/01-ml_fairness_clarify.ipynb</strong> and follow the next steps:</p>
			<ol>
				<li>We will use SageMaker Experiments to organize the analysis and training job. Therefore, we install <strong class="source-inline">sagemaker-experiments</strong> in the first cell, and we set up the SageMaker session and import the required libraries in the following two cells.</li>
				<li>In the fourth cell, we load the train and test datasets from the UCI ML repository. The <strong class="source-inline">orig_columns</strong> values are parsed from <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names">https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names</a>. The original dataset has both string representation and ordinal representation for education level in the <strong class="source-inline">education</strong> and <strong class="source-inline">education-num</strong> features. Let's just keep the ordinal representation and drop the <strong class="source-inline">education</strong> column. We also move the <strong class="source-inline">target</strong> column to the first column because we will use SageMaker's built-in <strong class="source-inline">XGBoost</strong> algorithm to train an ML model to predict the target. The <strong class="source-inline">target</strong> column contains the label for income greater than $50K (<strong class="source-inline">&gt;50K</strong>) and less than and equal to $50K (<strong class="source-inline">&lt;=50K</strong>). You can see an illustration of this in the following screenshot:</li>
			</ol>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="image/B17447_04_01.jpg" alt="Figure 6.1 – Screenshot of the DataFrame after step 2&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.1 – Screenshot of the DataFrame after step 2</p>
			<ol>
				<li value="3">We encode <a id="_idIndexMarker399"/>the categorical features in the train data (<strong class="source-inline">df</strong>) and test data (<strong class="source-inline">df_valtest</strong>) with <strong class="source-inline">OrdinalEncoder</strong> from <strong class="source-inline">sklearn</strong> to make the dataset compatible with the XGBoost algorithm. After the encoding, the <strong class="source-inline">target</strong> variable of values <strong class="source-inline">&gt;50K</strong> and <strong class="source-inline">&lt;=50K</strong> are encoded as <strong class="source-inline">1</strong> and <strong class="source-inline">0</strong>, respectively; there is a potentially sensitive <strong class="source-inline">sex</strong> category with <strong class="source-inline">Male</strong> and <strong class="source-inline">Female</strong> values encoded as <strong class="source-inline">1</strong> and <strong class="source-inline">0</strong>, respectively. We further take 10% of the test dataset as the validation dataset for model training.</li>
				<li>With this dataset, there are many angles from which we can analyze the data for bias and fairness. Intuitively, gender equality in income would be one angle we could start with. Let's make some visualizations to understand it qualitatively, as follows: <p class="source-code">df['sex'].value_counts(sort=False).plot(kind='bar', title='Total count by sex', rot=0)</p><p class="source-code">plt.xlabel('Sex (0: Female, 1: Male)')</p><p class="source-code">df['target'].value_counts(sort=False).plot(kind='bar', title='Target distribution', rot=0)</p><p class="source-code">plt.xlabel('target (0: &lt;=50K, 1: &gt;50K)')</p><p class="source-code">df[df['target']==1]['sex'].value_counts(sort=False).plot(kind='bar', title='Earning &gt;$50K by sex', rot=0)</p><p class="source-code">plt.xlabel('Sex (0: Female, 1: Male)')</p></li>
			</ol>
			<p>In the <a id="_idIndexMarker400"/>next screenshot, we can observe the following: </p>
			<ul>
				<li>The total number of females is about half that of males. </li>
				<li>There are many more people whose earnings are below $50K. </li>
				<li>There are more males than females who earn more than $50K.</li>
			</ul>
			<p>You can see the output here:</p>
			<div>
				<div id="_idContainer076" class="IMG---Figure">
					<img src="image/B17447_04_02.jpg" alt="Figure 6.2 – Output of the plotting, showing the distribution of sex and income level; an imbalanced distribution in sex and income level can be observed&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.2 – Output of the plotting, showing the distribution of sex and income level; an imbalanced distribution in sex and income level can be observed</p>
			<p>This distribution <a id="_idIndexMarker401"/>may be reflective of social inequality, but how do we quantify these skewed distributions so that we can be more aware of the bias in the dataset automatically and programmatically? This is where SageMaker Clarify comes into play.</p>
			<p>SageMaker Clarify from the SageMaker SDK (<strong class="source-inline">sagemaker.clarify</strong>) uses a dedicated container and SageMaker Processing to compute ML bias and explain ML predictions. We can start by instantiating <strong class="source-inline">sagemaker.clarify.SageMakerClarifyProcessor</strong> with the type of compute resource that fits the dataset, as follows:</p>
			<p class="source-code">from sagemaker import clarify</p>
			<p class="source-code">clarify_processor = clarify.SageMakerClarifyProcessor(</p>
			<p class="source-code">         role=role, </p>
			<p class="source-code">         instance_count=1, </p>
			<p class="source-code">         instance_type='ml.m5.xlarge', </p>
			<p class="source-code">         sagemaker_session=sess)</p>
			<ol>
				<li value="5">We will use <strong class="source-inline">SageMakerClarifyProcessor.run_pre_training_bias()</strong> specifically to compute the data bias prior to training an ML model. The metrics it returns allow us <a id="_idIndexMarker402"/>to quantify data bias based on the target and facet we choose and allow us to take action to mitigate the bias. But first, <strong class="source-inline">run_pre_training_bias()</strong> requires <a id="_idIndexMarker403"/>two configurations: a <strong class="bold">data configuration</strong> and a <strong class="bold">bias configuration</strong>. In a data configuration, we <a id="_idIndexMarker404"/>specify the input training data and the feature-heading information in <strong class="source-inline">clarify.DataConfig()</strong>, as shown in the following code block:<p class="source-code">pretraining_bias_report_output_path = f's3://{bucket}/{prefix}/{experiment_name}-{exp_trial_1.trial_name}/clarify-pretraining-bias'</p><p class="source-code">bias_data_config = clarify.DataConfig(</p><p class="source-code">    s3_data_input_path=train_s3_uri,</p><p class="source-code">    s3_output_path=pretraining_bias_report_output_path,</p><p class="source-code">    label='target',</p><p class="source-code">    headers=df.columns.tolist(),</p><p class="source-code">    dataset_type='text/csv')</p></li>
			</ol>
			<p>Because the training data in <strong class="source-inline">train_s3_uri</strong> does not contain column headers, the feature columns are provided in the <strong class="source-inline">headers</strong> argument. In the <strong class="source-inline">label</strong> argument, we specify the target variable from the dataset, which has to be one of the column names in what's input in the <strong class="source-inline">headers</strong> argument. </p>
			<p>In a bias configuration, we specify the facets—that is, the sensitive categories that we would like to analyze using <strong class="source-inline">clarify.BiasConfig()</strong>, as follows:</p>
			<p class="source-code">bias_config = clarify.BiasConfig(</p>
			<p class="source-code">    label_values_or_threshold=[1], </p>
			<p class="source-code">    facet_name=['sex', 'race'], </p>
			<p class="source-code">    facet_values_or_threshold=[[0], None])</p>
			<p>We would like to analyze how much gender bias (the <strong class="source-inline">sex</strong> column) there is in the dataset and, in particular, how the outcome (the <strong class="source-inline">target</strong> column) is impacted by gender. To do so, we specify a positive class (<strong class="source-inline">&gt;50K</strong> or <strong class="source-inline">1</strong>) from the target in a list to the <strong class="source-inline">label_values_or_threshold</strong> argument. We specify the facet(s) to be <strong class="source-inline">sex</strong> and <strong class="source-inline">race</strong>. Although in this example we are mostly focused on gender bias, we are adding a <strong class="source-inline">race</strong> feature to showcase that you can use multiple features as facets and <a id="_idIndexMarker405"/>that SageMaker Clarify would analyze bias in all the facets at once. The last required argument, <strong class="source-inline">facet_values_or_threshold</strong>, is there to specify the sensitive category in the facets for SageMaker Clarify to focus on when quantifying the bias. <strong class="source-inline">facet_values_or_threshold=[[0], None]</strong> corresponds to <strong class="source-inline">facet_name=['sex', 'race']</strong>. This means that we are asking Clarify to only calculate the bias metrics for class <strong class="source-inline">0</strong> in <strong class="source-inline">sex</strong>, which is female, while not specifying a class (<strong class="source-inline">None</strong>) for <strong class="source-inline">race</strong>, which will force Clarify to calculate bias metrics for all classes in <strong class="source-inline">race</strong>. </p>
			<ol>
				<li value="6">Once the setup is complete, we can run the processing job with the configurations, as follows:<p class="source-code">clarify_processor.run_pre_training_bias(</p><p class="source-code">    data_config=bias_data_config,</p><p class="source-code">    data_bias_config=bias_config,</p><p class="source-code">    methods='all',</p><p class="source-code">    job_name=jobname,</p><p class="source-code">    experiment_config=experiment_config)</p></li>
			</ol>
			<p>We ask Clarify to compute all possible pretraining bias with <strong class="source-inline">methods='all'</strong>. SageMaker Clarify integrates with SageMaker Experiments, so we also provide an experiment and trial configuration for this job. In the notebook, we name the experiment <strong class="source-inline">experiment_name = 'adult-income-clarify'</strong>.</p>
			<ol>
				<li value="7">We can visualize the Clarify results in the <strong class="bold">Experiments and trials</strong> dropdown in the left sidebar, where we can find the experiment we created for this example. Double-click the <strong class="source-inline">adult-income-clarify</strong> entry, and right-click the new trial entry <a id="_idIndexMarker406"/>whose name is a timestamp to select <strong class="bold">Open in trial component list</strong>, as shown in the following screenshot: </li>
			</ol>
			<div>
				<div id="_idContainer077" class="IMG---Figure">
					<img src="image/B17447_04_03.jpg" alt="Figure 6.3 – Selecting a trial to view the SageMaker Clarify result&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.3 – Selecting a trial to view the SageMaker Clarify result</p>
			<p>A new page with a <strong class="bold">TRIAL COMPONENTS</strong> list will show up in the main working area. We can open the <strong class="bold">Trial details</strong> page to see the result by right-clicking on the entry and selecting <strong class="bold">Open in trial details</strong>, as shown in the following <a id="_idIndexMarker407"/>screenshot: </p>
			<div>
				<div id="_idContainer078" class="IMG---Figure">
					<img src="image/B17447_04_04.jpg" alt="Figure 6.4 – Selecting a trial component to view the SageMaker Clarify result&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.4 – Selecting a trial component to view the SageMaker Clarify result</p>
			<ol>
				<li value="8">On the <strong class="bold">Trial</strong> <strong class="bold">components</strong> page, move to the <strong class="bold">Bias report</strong> tab to find the analysis results, as shown in the following screenshot. Here, you can find metrics calculated by SageMaker Clarify: </li>
			</ol>
			<div>
				<div id="_idContainer079" class="IMG---Figure">
					<img src="image/B17447_04_05.jpg" alt="Figure 6.5 – Reviewing the pretraining bias report on the trial details page in SageMaker Studio&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.5 – Reviewing the pretraining bias report on the trial details page in SageMaker Studio</p>
			<p>With each <a id="_idIndexMarker408"/>metric, you can see a description to understand what it means. For further information, you can expand a line item to see how the metric is calculated, with an example and interpretation. Furthermore, you can find additional papers in the details' descriptions to read more about the mathematical definition for all metrics. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">For <a id="_idIndexMarker409"/>convenience, this <strong class="bold">Uniform Resource Locator</strong> (<strong class="bold">URL</strong>) takes <a id="_idIndexMarker410"/>you to the technical whitepaper: <a href="https://pages.awscloud.com/rs/112-TZM-766/images/Amazon.AI.Fairness.and.Explainability.Whitepaper.pdf">https://pages.awscloud.com/rs/112-TZM-766/images/Amazon.AI.Fairness.and.Explainability.Whitepaper.pdf</a>. This is a good read if you are interested in the math behind the metrics.</p>
			<p>Let's review the bias in the data. Most notably, it is reported that there is <strong class="bold">Class Imbalance (CI)</strong>, which was measured as 0.34, and <strong class="bold">Difference in Positive Proportions in Labels (DPL)</strong>, which was measured as 0.2. <strong class="bold">Class Imbalance (CI)</strong> reveals the imbalance between males and females (<strong class="source-inline">sex</strong> feature) as there are 0.34 or 34% fewer females compared to males. <strong class="bold">Difference in Positive Proportions in Labels (DPL)</strong> quantifies how much more of a positive outcome (<strong class="source-inline">&gt;50K</strong>) there is for males compared to females. There are 0.2 or 20% more males who earn &gt;50K in the dataset than females. These two <a id="_idIndexMarker411"/>metrics alone not only confirm the imbalance we saw in the chart we plotted in the notebook but also quantify the imbalance. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">There is no valid result for <strong class="bold">Conditional Demographic Disparity in Labels (CDDL)</strong> because we did not specify a conditional group in <strong class="source-inline">clarify.BiasConfig(group_name=None)</strong>.</p>
			<p class="callout">You can view the analysis for other facets and categories we have specified in <strong class="source-inline">clarify.BiasConfig()</strong>—<strong class="source-inline">race</strong>, for example—by toggling the <strong class="bold">Column analyzed for bias</strong> and <strong class="bold">Column value or threshold analyzed for bias</strong> drop-down lists.</p>
			<p class="callout">SageMaker <a id="_idIndexMarker412"/>Clarify also saves a copy of the analysis in <strong class="bold">Portable Document Format</strong> (<strong class="bold">PDF</strong>), <strong class="bold">HyperText Markup Language</strong> (<strong class="bold">HTML</strong>), and IPYNB formats in <strong class="bold">Simple Storage Service </strong>(<strong class="bold">S3</strong>). You can find the S3 path stored in <strong class="source-inline">pretraining_bias_report_output_path</strong> <strong class="source-inline">variable</strong>. </p>
			<p>This imbalance <a id="_idIndexMarker413"/>in the data, if left unmitigated, could very well be ingrained into an ML model after training and it could start repeating what it learned from the biased data. Let's see how to mitigate it.</p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor095"/>Mitigating bias and training a model</h2>
			<p>There are a <a id="_idIndexMarker414"/>couple of <a id="_idIndexMarker415"/>data science approaches to mitigate the data imbalance, such as matching, oversampling, and undersampling. In this example, let's try a simple matching in terms of gender and target outcome to balance the male and female samples and the proportion in a positive outcome (<strong class="source-inline">&gt;50K</strong>). We'll proceed as follows:</p>
			<ol>
				<li value="1">Coming back to the notebook, we continue to work with the data to address the bias, as follows:<p class="source-code">max_female_sample=df.groupby(['sex', 'target'], </p><p class="source-code">           group_keys=False).count().loc[(0, 1)]['age']</p><p class="source-code">df_sampled=df.groupby(['sex', 'target'], </p><p class="source-code">group_keys=False).apply(lambda x: x.sample(max_female_sample))</p></li>
			</ol>
			<p>This generates a sampled and matched dataset that has an equal amount of both genders and an equal proportion in the target outcome. </p>
			<ol>
				<li value="2">We can <a id="_idIndexMarker416"/>verify the effectiveness of this approach by plotting the <a id="_idIndexMarker417"/>same charts and creating another pretraining bias analysis using SageMaker Clarify for this sampled and matched dataset with an identical bias configuration. Note that we are creating another trial in SageMaker Experiments to track this run and direct the output to a different output S3 location. The code is illustrated in the following snippet:<p class="source-code">pretraining_bias_report_output_path = f's3://{bucket}/{prefix}/{experiment_name}-{exp_trial_2.trial_name}/clarify-pretraining-bias'</p><p class="source-code">bias_data_config = clarify.DataConfig(</p><p class="source-code">    s3_data_input_path=train_sampled_s3_uri,</p><p class="source-code">    s3_output_path=pretraining_bias_report_output_path,</p><p class="source-code">    label='target',</p><p class="source-code">    headers=df_sampled.columns.tolist(),</p><p class="source-code">    dataset_type='text/csv')</p></li>
			</ol>
			<p>We then use the same <strong class="source-inline">bias_config</strong> and call the <strong class="source-inline">clarify_processor.run_pre_training_bias()</strong> method as we did before to run a pre-training bias analysis job after bias mitigation.</p>
			<ol>
				<li value="3">After the SageMaker Clarify job is done, we can open the <strong class="bold">Bias report</strong> feature on the trial details page for the new pretraining bias analysis job. You can see that <strong class="bold">Class Imbalance (CI)</strong> and <strong class="bold">Difference in Positive Proportions in Labels (DPL)</strong> are now both zeros. In fact, there are zeros across all the bias metrics.</li>
				<li>We have successfully zeroed out the data bias we observed previously. Let's get the model training started with SageMaker's built-in <strong class="source-inline">XGBoost</strong> algorithm, which is a great tool for structured data such as we have. We run this training job as a new trial <a id="_idIndexMarker418"/>component in the second trial, <strong class="source-inline">exp_trial_2</strong>. For the hyperparameter, we choose a <strong class="source-inline">binary:logistic</strong> objective for binary classification, <strong class="source-inline">error</strong> as an <a id="_idIndexMarker419"/>evaluation metric, and <strong class="source-inline">50</strong> rounds of optimization. The code is illustrated in the following snippet:<p class="source-code">experiment_config={'ExperimentName': experiment_name,</p><p class="source-code">                   'TrialName': exp_trial_2.trial_name,</p><p class="source-code">                   'TrialComponentDisplayName': 'Training'}</p><p class="source-code">...</p><p class="source-code">xgb = sagemaker.estimator.Estimator(</p><p class="source-code">        image,</p><p class="source-code">        role,</p><p class="source-code">        instance_type='ml.m5.xlarge',</p><p class="source-code">        instance_count=1,</p><p class="source-code">        output_path=train_s3_output,</p><p class="source-code">        enable_sagemaker_metrics=True,</p><p class="source-code">        sagemaker_session=sess)</p><p class="source-code">xgb.set_hyperparameters(objective='binary:logistic',</p><p class="source-code">                        eval_metric='error',</p><p class="source-code">                        num_round=50)</p><p class="source-code">...</p><p class="source-code">data_channels={'train': train_input, 'validation': val_input}</p><p class="source-code">xgb.fit(inputs=data_channels, </p><p class="source-code">        job_name=jobname, </p><p class="source-code">        experiment_config=experiment_config, </p><p class="source-code">        wait=True)</p></li>
			</ol>
			<p>The training job completes in about 5 minutes, including the infrastructure provisioning. </p>
			<ol>
				<li value="5">We create <a id="_idIndexMarker420"/>a SageMaker model from the training job so that later, we <a id="_idIndexMarker421"/>can use it in SageMaker Clarify jobs to analyze model bias. Here's the code to do this:<p class="source-code">model = xgb.create_model(name=model_name)</p><p class="source-code">container_def = model.prepare_container_def()</p><p class="source-code">sess.create_model(model_name, role, container_def)</p></li>
			</ol>
			<p>After the model is trained, we can use SageMaker Clarify to detect and measure biases that occur in the prediction.</p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor096"/>Detecting post-training bias</h2>
			<p>The following steps analyze biases in prediction and data after the model is trained. To run a post-training <a id="_idIndexMarker422"/>bias analysis with SageMaker Clarify, we need <a id="_idIndexMarker423"/>to prepare <a id="_idIndexMarker424"/>three configurations: a <strong class="bold">data configuration</strong>, a <strong class="bold">bias configuration</strong>, and a <strong class="bold">model configuration</strong>. Proceed <a id="_idIndexMarker425"/>as follows: </p>
			<ol>
				<li value="1">Create a new <strong class="source-inline">clarify.DataConfig()</strong> instance to analyze the matched training data and direct the output to a different output S3 location, as follows:<p class="source-code">posttraining_bias_report_output_path = f's3://{bucket}/{prefix}/{experiment_name}-{exp_trial_2.trial_name}/clarify-posttraining-bias' </p><p class="source-code">bias_data_config = clarify.DataConfig(</p><p class="source-code">    s3_data_input_path=train_sampled_s3_uri,</p><p class="source-code">    s3_output_path=posttraining_bias_report_output_path,</p><p class="source-code">    label='target',</p><p class="source-code">    headers=df_sampled.columns.tolist(),</p><p class="source-code">    dataset_type='text/csv')</p></li>
				<li>The bias configuration remains the same as what we used in the pretraining bias analysis. We continue to analyze how model prediction is impacted by <strong class="source-inline">sex</strong>, <strong class="source-inline">race</strong>, and <strong class="source-inline">target</strong> distribution.</li>
				<li>When a post-training analysis job is started, a SageMaker real-time endpoint with the ML model is created to make a prediction on the input data for a short duration <a id="_idIndexMarker426"/>of time to avoid additional traffic to your production endpoint, if any. This endpoint is also called a shadow endpoint and will be deprovisioned once the analysis job finishes. For the model configuration, we specify a model and configure the endpoint. <strong class="source-inline">accept_type</strong> denotes the endpoint response payload format, and <strong class="source-inline">content_type</strong> indicates the payload format of the request to the endpoint. </li>
			</ol>
			<p>We also specify a probability threshold of 0.5 to convert the probability output from the XGBoost model to binary hard labels, as follows: </p>
			<p class="source-code">model_config = clarify.ModelConfig(</p>
			<p class="source-code">    model_name=model_name,</p>
			<p class="source-code">    instance_type='ml.m5.xlarge',</p>
			<p class="source-code">    instance_count=1,</p>
			<p class="source-code">    accept_type='text/csv',</p>
			<p class="source-code">    content_type='text/csv')</p>
			<p class="source-code">predictions_config = clarify.ModelPredictedLabelConfig(probability_threshold=0.5)</p>
			<p>A prediction above 0.5 is predicted as 1 (<strong class="source-inline">&gt;50K</strong>); otherwise, it is 0 (<strong class="source-inline">&lt;=50K</strong>).</p>
			<ol>
				<li value="4">Finally, we run the job with the configurations. We request to compute all valid post-training bias metrics, as follows:<p class="source-code">clarify_processor.run_post_training_bias(</p><p class="source-code">    data_config=bias_data_config,</p><p class="source-code">    data_bias_config=bias_config,</p><p class="source-code">    model_config=model_config,</p><p class="source-code">    model_predicted_label_config=predictions_config,</p><p class="source-code">    methods='all',    </p><p class="source-code">    job_name=jobname,</p><p class="source-code">    experiment_config=experiment_config)</p></li>
				<li>We can <a id="_idIndexMarker427"/>also review the results on the trial details page of the second trial (<strong class="source-inline">exp_trial_2.trial_name</strong>), as shown in the following screenshot. We see a different set of metrics are shown compared to a pretraining bias analysis. A post-training bias job focuses on analyzing predicted labels or comparing the predictions with the observed target values in the data with respect to groups with different attributes: </li>
			</ol>
			<div>
				<div id="_idContainer080" class="IMG---Figure">
					<img src="image/B17447_04_06.jpg" alt="Figure 6.6 – Reviewing the post-training bias report on the trial details page in SageMaker Studio&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.6 – Reviewing the post-training bias report on the trial details page in SageMaker Studio</p>
			<p>There is <a id="_idIndexMarker428"/>very low bias in most of the measures such as <strong class="bold">Accuracy Difference (AD)</strong>, meaning that the model is equally accurate in predicting the income level for the two sexes. However, there is one metric that has rather high biases: <strong class="bold">Treatment Equality (TE)</strong>. This measures whether a <em class="italic">Type 1</em> error (false positive) and a <em class="italic">Type 2</em> error (false negative) are affecting the two genders in the same way. This is the difference in the ratio of false negatives to false positives between the male and female groups. A positive value translates to females having a lower ratio of false negatives to false positives. That means that the model is more often incorrectly predicting a female to be a high-income earner when in fact they are not; rather, it is the other way around. Having a higher false-positive rate for females compared with males is somewhat concerning and could lead to unfair consequences with such models.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The technical whitepaper I shared in the <em class="italic">Detecting pretraining bias</em> section also has many more details on the post-training metrics. You can find the paper here: <a href="https://pages.awscloud.com/rs/112-TZM-766/images/Amazon.AI.Fairness.and.Explainability.Whitepaper.pdf">https://pages.awscloud.com/rs/112-TZM-766/images/Amazon.AI.Fairness.and.Explainability.Whitepaper.pdf</a>. </p>
			<p>After understanding <a id="_idIndexMarker429"/>how to measure the bias, both pretraining and post-training, we should also explore how the ML model makes decisions in the way it does with SageMaker Clarify.</p>
			<h1 id="_idParaDest-98"><a id="_idTextAnchor097"/>Explaining ML models using SHAP values</h1>
			<p>SageMaker Clarify also computes model-agnostic feature attribution based on the concept of Shapley values. Shapley values can be used to determine the contribution each feature <a id="_idIndexMarker430"/>makes to model predictions. Feature attribution helps explain how a model makes decisions. Having a quantifiable <a id="_idIndexMarker431"/>approach to describe how a model makes decisions enables us to have trust in an ML model that meets regulatory requirements and supports the human decision-making process. </p>
			<p>Similar to setting up configurations to run bias analysis jobs using SageMaker Clarify, it takes three configurations to set up a model explainability job: a <strong class="bold">data configuration</strong>, a <strong class="bold">model configuration</strong>, and an <strong class="bold">explainability configuration</strong>. Let's follow the next steps from the same notebook:</p>
			<ol>
				<li value="1">Create a data configuration with the training dataset (matched). This is similar to the data configurations we created before. The code is illustrated in the following snippet:<p class="source-code">explainability_data_config = clarify.DataConfig(</p><p class="source-code">    s3_data_input_path=train_sampled_s3_uri,</p><p class="source-code">    s3_output_path=explainability_output_path,</p><p class="source-code">    label='target',</p><p class="source-code">    headers=df_sampled.columns.tolist(),</p><p class="source-code">    dataset_type='text/csv')</p></li>
				<li>Create or reuse the <strong class="source-inline">model_config</strong> argument that was created before for the post-training bias analysis job.</li>
				<li>Create a <strong class="source-inline">clarify.SHAPConfig()</strong> instance with a baseline. A baseline is an instance of a data point that would be used to compute the Shapley values with the input data. For the same model, you can expect to get different explanations <a id="_idIndexMarker432"/>with respect to different <a id="_idIndexMarker433"/>baselines, so the choice of a baseline is crucial. It is desirable to select a general baseline with very low information content, such as an average or median feature vector. In this case, in our example, we would interpret the model attribution as to why a particular person is predicted as a high-income earner compared to an average person. Alternatively, you can choose to explain the model with respect to a particular type of data. For example, we can choose a baseline from a similar demographic that represents the people in the inference. The code is illustrated in the following snippet:<p class="source-code">baseline = df_sampled.query('target == 1').mode().iloc[0, 1:].astype(int).tolist()</p><p class="source-code">shap_config = clarify.SHAPConfig(</p><p class="source-code">    baseline=[baseline],</p><p class="source-code">    num_samples=15,</p><p class="source-code">    agg_method='mean_abs')</p></li>
			</ol>
			<p>In our example, let's simulate an "average" high-income (<strong class="source-inline">&gt;50K</strong>) person from the training data using <strong class="source-inline">mode</strong> for the baseline. The <strong class="source-inline">num_samples</strong> argument is used to determine the size of the generated synthetic dataset to compute the SHAP values. You can also leave it empty to make Clarify choose a number automatically. <strong class="source-inline">agg_method='mean_abs'</strong> denotes how to aggregate for global SHAP values. </p>
			<ol>
				<li value="4">Afterward, we <a id="_idIndexMarker434"/>start the analysis job with <a id="_idIndexMarker435"/>the configurations, as follows:<p class="source-code">clarify_processor.run_explainability(</p><p class="source-code">    data_config=explainability_data_config,</p><p class="source-code">    model_config=model_config,</p><p class="source-code">    explainability_config=shap_config,</p><p class="source-code">    job_name=jobname,</p><p class="source-code">    experiment_config=experiment_config,</p><p class="source-code">    wait=False,</p><p class="source-code">    logs=False)</p></li>
				<li>Once the processing job completes, we can view the results on the trial details page in SageMaker Experiments under the <strong class="bold">Explainability</strong> tab, as shown in the following screenshot. Here, we see the global feature attribution in SHAP values for the top 10 features in the dataset. The <strong class="source-inline">education-num</strong> feature, which represents the highest education level, contributes the most to predicting the income level (<strong class="source-inline">&gt;50K</strong> or <strong class="source-inline">&lt;=50K</strong>): </li>
			</ol>
			<div>
				<div id="_idContainer081" class="IMG---Figure">
					<img src="image/B17447_04_07.jpg" alt="Figure 6.7 – Reviewing the model explainability results in SHAP values in SageMaker Studio&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.7 – Reviewing the model explainability results in SHAP values in SageMaker Studio</p>
			<ol>
				<li value="6">Besides <a id="_idIndexMarker436"/>global SHAP values, we can also <a id="_idIndexMarker437"/>review local SHAP explanations for any given data point to explain how a model makes predictions on this particular data point. SageMaker Clarify computes and saves local explanations <a id="_idIndexMarker438"/>for the entire dataset that is provided in <strong class="source-inline">clarify.DataConfig()</strong> in a <strong class="bold">comma-separated values</strong> (<strong class="bold">CSV</strong>) file in the output S3 location <strong class="source-inline">explainability_output_path</strong>. We can plot the local SHAP values for each feature for a particular data point (the 500th row) with the following code: <p class="source-code">S3Downloader.download(f'{explainability_output_path}/explanations_shap/out.csv', './', sagemaker_session=sess)</p><p class="source-code">local_explanations_out = pd.read_csv('out.csv')</p><p class="source-code">feature_names = [str.replace(c, '_label0', '') for c in local_explanations_out.columns.to_series()]</p><p class="source-code">local_explanations_out.columns = feature_names</p><p class="source-code">selected_example = 500</p><p class="source-code">print(f'Example number: {selected_example}')</p><p class="source-code">print(f'with model prediction: {sum(local_explanations_out.iloc[selected_example]) &gt; 0}')</p><p class="source-code">print()</p><p class="source-code">print(f'Feature values: \n{df_sampled.iloc[selected_example].to_frame().T}')</p><p class="source-code">local_explanations_out.iloc[selected_example].plot(</p><p class="source-code">    kind='barh', </p><p class="source-code">    title=f'Local explanation for the {selected_example}th example.', </p><p class="source-code">    rot=0)</p></li>
			</ol>
			<p>As shown in <em class="italic">Figure 6.8</em>, we can see how the XGBoost model predicts this data point as &lt;=50K. The <strong class="source-inline">marital-status</strong>, <strong class="source-inline">education-num</strong>, and <strong class="source-inline">capital-gain</strong> factors are the top three factors that the model thinks of this person as a low-income earner. Thanks to SHAP values computed by SageMaker Clarify, we can understand and explain how the model makes the prediction on an individual basis too.</p>
			<div>
				<div id="_idContainer082" class="IMG---Figure">
					<img src="image/B17447_06_008.jpg" alt="Figure 6.8 – Explaining individual prediction&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.8 – Explaining individual prediction</p>
			<p>Let's <a id="_idIndexMarker439"/>summarize the chapter after you've completed <a id="_idIndexMarker440"/>the example</p>
			<h1 id="_idParaDest-99"><a id="_idTextAnchor098"/>Summary</h1>
			<p>In this chapter, we explored biases in ML and ML explainability with an adult income example. We learned that the data could contain unfair biases against a certain group or category in the dataset, which could translate into an ML model making unfair predictions. We worked through an adult income-level prediction example in SageMaker Studio to analyze and compute any bias prior to model training using <strong class="bold">SageMaker Clarify</strong>. Clarify produces metrics to quantify imbalance in the dataset that could potentially lead to unfair biases. We mitigated the imbalances using sampling and matching techniques and proceeded to train an ML model. We further analyzed the resulting ML model for potential bias in predictions using SageMaker Clarify. Finally, we reviewed how the ML model makes decisions using SageMaker Clarify and SHAP values. </p>
			<p>In the next chapter, we will learn where to go after training an ML model in SageMaker. Hosting an ML model in the cloud is critical for most ML use cases, and being able to use the right tool for model hosting from SageMaker is key to successful ML adoption for your organization. We will learn about various options for hosting an ML model and how to optimize compute resources and cost using SageMaker's hosting features.</p>
		</div>
	</body></html>