<html><head></head><body><div id="sbo-rt-content"><div class="chapter" title="Chapter 1. Exploratory Data Analysis"><div class="titlepage"><div><div><h1 class="title"><a id="ch01"/>Chapter 1. Exploratory Data Analysis</h1></div></div></div><p>Before I dive into more complex methods to analyze your data later in the book, I would like to stop at basic data exploratory tasks on which almost all data scientists spend at least 80-90% of their productive time. The data preparation, cleansing, transforming, and joining the data alone is estimated to be a $44 billion/year industry alone (<span class="emphasis"><em>Data Preparation in the Big Data Era</em></span> by <span class="emphasis"><em>Federico Castanedo</em></span> and <span class="emphasis"><em>Best Practices for Data Integration</em></span>, <span class="emphasis"><em>O'Reilly Media</em></span>, <span class="emphasis"><em>2015</em></span>). Given this fact, it is surprising that people only recently started spending more time on the science of developing best practices and establishing good habits, documentation, and teaching materials for the whole process of data preparation (<span class="emphasis"><em>Beautiful Data: The Stories Behind Elegant Data Solutions</em></span>, edited by <span class="emphasis"><em>Toby Segaran</em></span> and <span class="emphasis"><em>Jeff Hammerbacher</em></span>, <span class="emphasis"><em>O'Reilly Media</em></span>, <span class="emphasis"><em>2009</em></span> and <span class="emphasis"><em>Advanced Analytics with Spark: Patterns for Learning from Data at Scale</em></span> by <span class="emphasis"><em>Sandy Ryza et al.</em></span>, <span class="emphasis"><em>O'Reilly Media</em></span>, <span class="emphasis"><em>2015</em></span>).</p><p>Few data scientists would agree on specific tools and techniques—and there are multiple ways to perform the exploratory data analysis, ranging from Unix command line to using very popular open source and commercial ETL and visualization tools. The focus of this chapter is how to use Scala and a laptop-based environment to benefit from techniques that are commonly referred as a functional paradigm of programming. As I will discuss, these techniques can be transferred to exploratory analysis over distributed system of machines using Hadoop/Spark.</p><p>What has functional programming to do with it? Spark was developed in Scala for a good reason. Many basic principles that lie at the foundation of functional programming, such as lazy evaluation, immutability, absence of side effects, list comprehensions, and monads go really well with processing data in distributed environments, specifically, when performing the data preparation and transformation tasks on big data. Thanks to abstractions, these techniques work well on a local workstation or a laptop. As mentioned earlier, this does not preclude us from processing very large datasets up to dozens of TBs on modern laptops connected to distributed clusters of storage/processing nodes. We can do it one topic or focus area at the time, but often we even do not have to sample or filter the dataset with proper partitioning. We will use Scala as our primary tool, but will resort to other tools if required.</p><p>While Scala is complete in the sense that everything that can be implemented in other languages can be implemented in Scala, Scala is fundamentally a high-level, or even a scripting, language. One does not have to deal with low-level details of data structures and algorithm implementations that in their majority have already been tested by a plethora of applications and time, in, say, Java or C++—even though Scala has its own collections and even some basic algorithm implementations today. Specifically, in this chapter, I'll be focusing on using Scala/Spark only for high-level tasks.</p><p>In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Installing Scala</li><li class="listitem" style="list-style-type: disc">Learning simple techniques for initial data exploration</li><li class="listitem" style="list-style-type: disc">Learning how to downsample the original dataset for faster turnover</li><li class="listitem" style="list-style-type: disc">Discussing the implementation of basic data transformation and aggregations in Scala</li><li class="listitem" style="list-style-type: disc">Getting familiar with big data processing tools such as Spark and Spark Notebook</li><li class="listitem" style="list-style-type: disc">Getting code for some basic visualization of datasets</li></ul></div><div class="section" title="Getting started with Scala"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec08"/>Getting started with Scala</h1></div></div></div><p>If you have already installed Scala, you can skip this paragraph. One can get the latest Scala download from <a class="ulink" href="http://www.scala-lang.org/download/">http://www.scala-lang.org/download/</a>. I used Scala version 2.11.7 on<a id="id0" class="indexterm"/> Mac OS X El Capitan 10.11.5. You can use any other <a id="id1" class="indexterm"/>version you like, but you might face some compatibility problems with other packages such as Spark, a common problem in open source software as the technology adoption usually lags by a few released versions.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip02"/>Tip</h3><p>In most cases, you should try to maintain precise match between the recommended versions as difference in versions can lead to obscure errors and a lengthy debugging process.</p></div></div><p>If you installed Scala correctly, after typing <code class="literal">scala</code>, you should see something similar to the following:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro ~]$ scala</strong></span>
<span class="strong"><strong>Welcome to Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40).</strong></span>
<span class="strong"><strong>Type in expressions to have them evaluated.</strong></span>
<span class="strong"><strong>Type :help for more information.</strong></span>

<span class="strong"><strong>scala&gt;</strong></span>
</pre></div><p>This is a Scala <span class="strong"><strong>read-evaluate-print-loop</strong></span> (<span class="strong"><strong>REPL</strong></span>) prompt. Although Scala programs can be compiled, the<a id="id2" class="indexterm"/> content of this chapter will be in REPL, as <a id="id3" class="indexterm"/>we are focusing on interactivity with, maybe, a few exceptions. The <code class="literal">:help</code> command provides a some utility commands available in REPL (note the colon at the start):</p><div class="mediaobject"><img src="Images/B04935_01_09.jpg" alt="Getting started with Scala" width="800" height="484"/></div></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Distinct values of a categorical field"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec09"/>Distinct values of a categorical field</h1></div></div></div><p>Now, you have a <a id="id4" class="indexterm"/>dataset and a computer. For convenience, I have provided you a small anonymized and obfuscated sample of clickstream data with the book repository that you can get at <a class="ulink" href="https://github.com/alexvk/ml-in-scala.git">https://github.com/alexvk/ml-in-scala.git</a>. The file in the <code class="literal">chapter01/data/clickstream</code> directory contains lines with timestamp, session ID, and some additional event information such as URL, category information, and so on at the time of the call. The first thing one would do is apply transformations to find out the distribution of values for different columns in the dataset.</p><p>
<span class="emphasis"><em>Figure 01-1 shows</em></span> screenshot shows the output of the dataset in the terminal window of the <code class="literal">gzcat chapter01/data/clickstream/clickstream_sample.tsv.gz | less –U</code> command. The columns are tab (<code class="literal">^I</code>) separated. One can notice that, as in many real-world big data datasets, many values are missing. The first column of the dataset is recognizable as the timestamp. The file contains complex data such as arrays, structs, and maps, another feature of big data datasets.</p><p>Unix provides a few tools to dissect the datasets. Probably, <span class="strong"><strong>less</strong></span>, <span class="strong"><strong>cut</strong></span>, <span class="strong"><strong>sort</strong></span>, and <span class="strong"><strong>uniq</strong></span> are the most frequently used tools for text file manipulations. <span class="strong"><strong>Awk</strong></span>, <span class="strong"><strong>sed</strong></span>, <span class="strong"><strong>perl</strong></span>, and <span class="strong"><strong>tr</strong></span> can do more complex transformations and substitutions. Fortunately, Scala allows you to transparently use command-line tools from within Scala REPL, as shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B04935_01_01.jpg" alt="Distinct values of a categorical field" width="900" height="932"/><div class="caption"><p>Figure 01-1. The clickstream file as an output of the less -U Unix command</p></div></div><p>Fortunately, Scala allows you to transparently use command-line tools from within Scala REPL:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro]$ scala</strong></span>
<span class="strong"><strong>…</strong></span>
<span class="strong"><strong>scala&gt; import scala.sys.process._</strong></span>
<span class="strong"><strong>import scala.sys.process._</strong></span>
<span class="strong"><strong>scala&gt; val histogram = ( "gzcat chapter01/data/clickstream/clickstream_sample.tsv.gz"  #|  "cut -f 10" #| "sort" #|  "uniq -c" #| "sort -k1nr" ).lineStream</strong></span>
<span class="strong"><strong>histogram: Stream[String] = Stream(7731 http://www.mycompany.com/us/en_us/, ?)</strong></span>
<span class="strong"><strong>scala&gt; histogram take(10) foreach println </strong></span>
<span class="strong"><strong>7731 http://www.mycompany.com/us/en_us/</strong></span>
<span class="strong"><strong>3843 http://mycompanyplus.mycompany.com/plus/</strong></span>
<span class="strong"><strong>2734 http://store.mycompany.com/us/en_us/?l=shop,men_shoes</strong></span>
<span class="strong"><strong>2400 http://m.mycompany.com/us/en_us/</strong></span>
<span class="strong"><strong>1750 http://store.mycompany.com/us/en_us/?l=shop,men_mycompanyid</strong></span>
<span class="strong"><strong>1556 http://www.mycompany.com/us/en_us/c/mycompanyid?sitesrc=id_redir</strong></span>
<span class="strong"><strong>1530 http://store.mycompany.com/us/en_us/</strong></span>
<span class="strong"><strong>1393 http://www.mycompany.com/us/en_us/?cp=USNS_KW_0611081618</strong></span>
<span class="strong"><strong>1379 http://m.mycompany.com/us/en_us/?ref=http%3A%2F%2Fwww.mycompany.com%2F</strong></span>
<span class="strong"><strong>1230 http://www.mycompany.com/us/en_us/c/running</strong></span>
</pre></div><p>I used the <code class="literal">scala.sys.process</code> package to call familiar Unix commands from Scala REPL. From the<a id="id5" class="indexterm"/> output, we can immediately see the customers of our Webshop are mostly interested in men's shoes and running, and that most visitors are using the referral code, <span class="strong"><strong>KW_0611081618</strong></span>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip03"/>Tip</h3><p>One may wonder when we start using complex Scala types and algorithms. Just wait, a lot of highly optimized tools were created before Scala and are much more efficient for explorative data analysis. In the initial stage, the biggest bottleneck is usually just the disk I/O and slow interactivity. Later, we will discuss more iterative algorithms, which are usually more memory intensive. Also note that the UNIX pipeline operations can be implicitly parallelized on modern multi-core computer architectures, as they are in Spark (we will show it in the later chapters).</p><p>It has been shown that using compression, implicit or explicit, on input data files can actually save you the I/O time. This is particularly true for (most) modern semi-structured datasets with repetitive values and sparse content. Decompression can also be implicitly parallelized on modern fast multi-core computer architectures, removing the computational bottleneck, except, maybe in cases where compression is implemented implicitly in hardware (SSD, where we don't need to compress the files explicitly). We also recommend using directories<a id="id6" class="indexterm"/> rather than files as a paradigm for the dataset, where the insert operation is reduced to dropping the data file into a directory. This is how the datasets are presented in big data Hadoop tools such as Hive and Impala.</p></div></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Summarization of a numeric field"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec10"/>Summarization of a numeric field</h1></div></div></div><p>Let's look at <a id="id7" class="indexterm"/>the numeric data, even though most of the columns in the dataset are either categorical or complex. The traditional way to summarize the numeric data is a five-number-summary, which is a representation of the median or mean, interquartile range, and minimum and maximum. I'll leave the computations of the median and interquartile ranges till the Spark DataFrame is introduced, as it makes these computations extremely easy; but we can compute mean, min, and max in Scala by just applying the corresponding operators:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; import scala.sys.process._</strong></span>
<span class="strong"><strong>import scala.sys.process._</strong></span>
<span class="strong"><strong>scala&gt; val nums = ( "gzcat chapter01/data/clickstream/clickstream_sample.tsv.gz"  #|  "cut -f 6" ).lineStream</strong></span>
<span class="strong"><strong>nums: Stream[String] = Stream(0, ?) </strong></span>
<span class="strong"><strong>scala&gt; val m = nums.map(_.toDouble).min</strong></span>
<span class="strong"><strong>m: Double = 0.0</strong></span>
<span class="strong"><strong>scala&gt; val m = nums.map(_.toDouble).sum/nums.size</strong></span>
<span class="strong"><strong>m: Double = 3.6883642764024662</strong></span>
<span class="strong"><strong>scala&gt; val m = nums.map(_.toDouble).max</strong></span>
<span class="strong"><strong>m: Double = 33.0</strong></span>
</pre></div><div class="section" title="Grepping across multiple fields"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec07"/>Grepping across multiple fields</h2></div></div></div><p>Sometimes one needs to get an idea of how a certain value looks across multiple fields—most common<a id="id8" class="indexterm"/> are IP/MAC addresses, dates, and formatted messages. For examples, if I want to see all IP addresses mentioned throughout a file or a document, I need to replace the <code class="literal">cut</code> command in the previous example by <code class="literal">grep -o -E [1-9][0-9]{0,2}(?:\\.[1-9][0-9]{0,2}){3}</code>, where the <code class="literal">–o</code> option instructs <code class="literal">grep</code> to print only the matching parts—a more precise regex for the IP address should be <code class="literal">grep –o –E (?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)</code>, but is about 50% slower on my laptop and the original one works in most practical cases. I'll leave it as an excursive to run this command on the sample file provided with the book.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Basic, stratified, and consistent sampling"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec11"/>Basic, stratified, and consistent sampling</h1></div></div></div><p>I've met quite a few data practitioners who scorn sampling. Ideally, if one can process the whole dataset, the model can only improve. In practice, the tradeoff is much more complex. First, one can build more complex models on a sampled set, particularly if the time complexity<a id="id9" class="indexterm"/> of the model building is non-linear—and in most<a id="id10" class="indexterm"/> situations, if it is at least <span class="emphasis"><em>N* log(N)</em></span>. A faster model building <a id="id11" class="indexterm"/>cycle allows you to iterate over models and converge on the best approach faster. In many situations, <span class="emphasis"><em>time to action</em></span> is beating the potential improvements in the prediction accuracy due to a model built on complete dataset.</p><p>Sampling may be combined with appropriate filtering—in many practical situation, focusing on a subproblem at a time leads to better understanding of the whole problem domain. In many cases, this partitioning is at the foundation of the algorithm, like in decision trees, which are considered later. Often the nature of the problem requires you to focus on the subset of original data. For example, a cyber security analysis is often focused around a specific set of IPs rather than the whole network, as it allows to iterate over hypothesis faster. Including the set of all IPs in the network may complicate things initially if not throw the modeling off the right track.</p><p>When dealing with rare events, such as clickthroughs in ADTECH, sampling the positive and negative cases with different probabilities, which is also sometimes called oversampling, often leads to better predictions in short amount of time.</p><p>Fundamentally, sampling is equivalent to just throwing a coin—or calling a random number generator—for each data row. Thus it is very much like a stream filter operation, where the filtering is on an augmented column of random numbers. Let's consider the following example:</p><div class="informalexample"><pre class="programlisting">import scala.util.Random
import util.Properties

val threshold = 0.05

val lines = scala.io.Source.fromFile("chapter01/data/iris/in.txt").getLines
val newLines = lines.filter(_ =&gt;
    Random.nextDouble() &lt;= threshold
)

val w = new java.io.FileWriter(new java.io.File("out.txt"))
newLines.foreach { s =&gt;
    w.write(s + Properties.lineSeparator)
}
w.close</pre></div><p>This is all good, but<a id="id12" class="indexterm"/> it has the following disadvantages:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The number<a id="id13" class="indexterm"/> of lines in the resulting file is not known<a id="id14" class="indexterm"/> beforehand—even though on average it should be 5% of the original file</li><li class="listitem" style="list-style-type: disc">The results of the sampling is non-deterministic—it is hard to rerun this process for either testing or verification</li></ul></div><p>To fix the first point, we'll need to pass a more complex object to the function, as we need to maintain the state during the original list traversal, which makes the original algorithm less functional and parallelizable (this will be discussed later):</p><div class="informalexample"><pre class="programlisting">import scala.reflect.ClassTag
import scala.util.Random
import util.Properties

def reservoirSample[T: ClassTag](input: Iterator[T],k: Int): Array[T] = {
  val reservoir = new Array[T](k)
  // Put the first k elements in the reservoir.
  var i = 0
  while (i &lt; k &amp;&amp; input.hasNext) {
    val item = input.next()
    reservoir(i) = item
    i += 1
  }

  if (i &lt; k) {
    // If input size &lt; k, trim the array size
    reservoir.take(i)
  } else {
    // If input size &gt; k, continue the sampling process.
    while (input.hasNext) {
      val item = input.next
      val replacementIndex = Random.nextInt(i)
      if (replacementIndex &lt; k) {
        reservoir(replacementIndex) = item
      }
      i += 1
    }
    reservoir
  }
}

val numLines=15
val w = new java.io.FileWriter(new java.io.File("out.txt"))
val lines = io.Source.fromFile("chapter01/data/iris/in.txt").getLines
reservoirSample(lines, numLines).foreach { s =&gt;
    w.write(s + scala.util.Properties.lineSeparator)
}
w.close</pre></div><p>This will <a id="id15" class="indexterm"/>output <code class="literal">numLines</code> lines. Similarly to reservoir <a id="id16" class="indexterm"/>sampling, stratified sampling is guaranteed to provide the same ratios <a id="id17" class="indexterm"/>of input/output rows for all strata defined by levels of another attribute. We can achieve this by splitting the original dataset into <span class="emphasis"><em>N</em></span> subsets corresponding to the levels, performing the reservoir sampling, and merging the results afterwards. However, MLlib library, which will be covered in <a class="link" href="ch03.xhtml" title="Chapter 3. Working with Spark and MLlib">Chapter 3</a>,<span class="emphasis"><em> Working with Spark and MLlib</em></span>, already has stratified sampling implementation:</p><div class="informalexample"><pre class="programlisting">val origLinesRdd = sc.textFile("file://...")
val keyedRdd = origLines.keyBy(r =&gt; r.split(",")(0))
val fractions = keyedRdd.countByKey.keys.map(r =&gt; (r, 0.1)).toMap
val sampledWithKey = keyedRdd.sampleByKeyExact(fractions)
val sampled = sampledWithKey.map(_._2).collect</pre></div><p>The other bullet point is more subtle; sometimes we want a consistent subset of values across multiple datasets, either for reproducibility or to join with another sampled dataset. In general, if we sample two datasets, the results will contain random subsets of IDs which might have very little or no intersection. The cryptographic hashing functions come to the help here. The result of applying a hash function such as MD5 or SHA1 is a sequence of bits that is statistically uncorrelated, at least in theory. We will use the <code class="literal">MurmurHash</code> <a id="id18" class="indexterm"/>function, which is part of the <code class="literal">scala.util.hashing</code> package:</p><div class="informalexample"><pre class="programlisting">import scala.util.hashing.MurmurHash3._

val markLow = 0
val markHigh = 4096
val seed = 12345

def consistentFilter(s: String): Boolean = {
  val hash = stringHash(s.split(" ")(0), seed) &gt;&gt;&gt; 16
  hash &gt;= markLow &amp;&amp; hash &lt; markHigh
}

val w = new java.io.FileWriter(new java.io.File("out.txt"))
val lines = io.Source.fromFile("chapter01/data/iris/in.txt").getLines
lines.filter(consistentFilter).foreach { s =&gt;
     w.write(s + Properties.lineSeparator)
}
w.close</pre></div><p>This function is guaranteed to return exactly the same subset of records based on the value of the first field—it is either all records where the first field equals a certain value or none—and will come up with approximately one-sixteenth of the original sample; the range of <code class="literal">hash</code> is <code class="literal">0</code> to <code class="literal">65,535</code>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note02"/>Note</h3><p>MurmurHash? It is not a cryptographic hash!</p><p>Unlike<a id="id19" class="indexterm"/> cryptographic hash functions, such as MD5 and <a id="id20" class="indexterm"/>SHA1, MurmurHash is not specifically <a id="id21" class="indexterm"/>designed to be hard to find an inverse of a hash. It is, however, really fast and efficient. This is what really matters in our use case.</p></div></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Working with Scala and Spark Notebooks"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec12"/>Working with Scala and Spark Notebooks</h1></div></div></div><p>Often the most frequent values or five-number summary are not sufficient to get the first understanding of the data. The<a id="id22" class="indexterm"/> term <span class="strong"><strong>descriptive statistics</strong></span> is very generic and may refer to very complex ways to describe the data. Quantiles, a <a id="id23" class="indexterm"/>
<span class="strong"><strong>Paretto</strong></span> chart or, when more than one attribute is analyzed, correlations are also examples of descriptive statistics. When sharing all these ways to look at the<a id="id24" class="indexterm"/> data aggregates, in many cases, it is also <a id="id25" class="indexterm"/>important to share the specific computations to get to them.</p><p>Scala or <a id="id26" class="indexterm"/>Spark Notebook <a class="ulink" href="https://github.com/Bridgewater/scala-notebook">https://github.com/Bridgewater/scala-notebook</a>, <a class="ulink" href="https://github.com/andypetrella/spark-notebook">https://github.com/andypetrella/spark-notebook</a> record the whole transformation path and the results can be shared as a JSON-based <code class="literal">*.snb</code> file. The Spark Notebook project can be downloaded from <a class="ulink" href="http://spark-notebook.io">http://spark-notebook.io</a>, and I will provide a sample <code class="literal">Chapter01.snb</code> file with the book. I will use Spark, which I will cover in more detail in <a class="link" href="ch03.xhtml" title="Chapter 3. Working with Spark and MLlib">Chapter 3</a>, <span class="emphasis"><em>Working with Spark and MLlib</em></span>.</p><p>For this particular example, Spark will run in the local mode. Even in the local mode Spark can utilize parallelism on your workstation, but it is limited to the number of cores and hyperthreads that can run on your laptop or workstation. With a simple configuration change, however, Spark can be pointed to a distributed set of machines and use resources across a distributed <a id="id27" class="indexterm"/>set of nodes.</p><p>Here is the set of <a id="id28" class="indexterm"/>commands to download the Spark Notebook and copy the necessary files from the code repository:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro]$ wget http://s3.eu-central-1.amazonaws.com/spark-notebook/zip/spark-notebook-0.6.3-scala-2.11.7-spark-1.6.1-hadoop-2.6.4-with-hive-with-parquet.zip</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro]$ unzip -d ~/ spark-notebook-0.6.3-scala-2.11.7-spark-1.6.1-hadoop-2.6.4-with-hive-with-parquet.zip</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro]$ ln -sf ~/ spark-notebook-0.6.3-scala-2.11.7-spark-1.6.1-hadoop-2.6.4-with-hive-with-parquet ~/spark-notebook</strong></span>
<span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro]$ cp chapter01/notebook/Chapter01.snb ~/spark-notebook/notebooks</strong></span>
<span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro]$ cp chapter01/ data/kddcup/kddcup.parquet ~/spark-notebook</strong></span>
<span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro]$ cd ~/spark-notebook</strong></span>
<span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro]$ bin/spark-notebook </strong></span>
<span class="strong"><strong>Play server process ID is 2703</strong></span>
<span class="strong"><strong>16/04/14 10:43:35 INFO play: Application started (Prod)</strong></span>
<span class="strong"><strong>16/04/14 10:43:35 INFO play: Listening for HTTP on /0:0:0:0:0:0:0:0:9000</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>Now you can open the notebook at <code class="literal">http://localhost:9000</code> in your browser, as shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B04935_01_02.jpg" alt="Working with Scala and Spark Notebooks" width="1000" height="600"/><div class="caption"><p>Figure 01-2. The first page of the Spark Notebook with the list of notebooks</p></div></div><p>Open <a id="id29" class="indexterm"/>the <code class="literal">Chapter01</code> notebook by clicking on it. The statements are<a id="id30" class="indexterm"/> organized into cells and can be executed by clicking on the small right arrow at the top, as shown in the following screenshot, or run all cells at once by navigating to <span class="strong"><strong>Cell</strong></span> | <span class="strong"><strong>Run All</strong></span>:</p><div class="mediaobject"><img src="Images/B04935_01_03.jpg" alt="Working with Scala and Spark Notebooks" width="900" height="548"/><div class="caption"><p>Figure 01-3. Executing the first few cells in the notebook</p></div></div><p>First, we will look at the discrete variables. For example, to get the other observable attributes. This task would be totally impossible if distribution of the labels, issue the following code:</p><div class="informalexample"><pre class="programlisting">val labelCount = df.groupBy("lbl").count().collect
labelCount.toList.map(row =&gt; (row.getString(0), row.getLong(1)))</pre></div><p>The first time<a id="id31" class="indexterm"/> I read the dataset, it took about a minute on <a id="id32" class="indexterm"/>MacBook Pro, but Spark caches the data in memory and the subsequent aggregation runs take only about a second. Spark Notebook provides you the distribution of the values, as shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B04935_01_04.jpg" alt="Working with Scala and Spark Notebooks" width="900" height="764"/><div class="caption"><p>Figure 01-4. Computing the distribution of values for a categorical field</p></div></div><p>I can also look at crosstab counts for pairs of discrete variables, which gives me an idea of interdependencies <a id="id33" class="indexterm"/>between the variables using <a class="ulink" href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameStatFunctions">http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameStatFunctions</a>—the object does not support computing correlation <a id="id34" class="indexterm"/>measures<a id="id35" class="indexterm"/> such as chi-square yet:</p><div class="mediaobject"><img src="Images/B04935_01_05.jpg" alt="Working with Scala and Spark Notebooks" width="800" height="511"/><div class="caption"><p>Figure 01-5. Contingency table or crosstab</p></div></div><p>However, we can see that the most popular service is private and it correlates well with the <code class="literal">SF</code> flag. Another way to analyze dependencies is to look at <code class="literal">0</code> entries. For example, the <code class="literal">S2</code> and <code class="literal">S3</code> flags are clearly related to the SMTP and FTP traffic since all other entries are <code class="literal">0</code>.</p><p>Of course, the <a id="id36" class="indexterm"/>most interesting correlations are with the target<a id="id37" class="indexterm"/> variable, but these are better discovered by supervised learning algorithms that I will cover in <a class="link" href="ch03.xhtml" title="Chapter 3. Working with Spark and MLlib">Chapter 3</a>, <span class="emphasis"><em>Working with Spark and MLlib,</em></span> and <a class="link" href="ch05.xhtml" title="Chapter 5. Regression and Classification">Chapter 5</a>, <span class="emphasis"><em>Regression and Classification</em></span>.</p><div class="mediaobject"><img src="Images/B04935_01_06.jpg" alt="Working with Scala and Spark Notebooks" width="600" height="561"/><div class="caption"><p>Figure 01-6. Computing simple aggregations using org.apache.spark.sql.DataFrameStatFunctions.</p></div></div><p>Analogously, we can compute correlations for numerical variables with the <code class="literal">dataFrame.stat.corr()</code> and <code class="literal">dataFrame.stat.cov()</code> functions (refer to <span class="emphasis"><em>Figure 01-6)</em></span>. In this case, the class supports the <span class="strong"><strong>Pearson correlation coefficient</strong></span>. Alternatively, we can use the standard <a id="id38" class="indexterm"/>SQL syntax on the parquet file directly:</p><div class="informalexample"><pre class="programlisting">sqlContext.sql("SELECT lbl, protocol_type, min(duration), avg(duration), stddev(duration), max(duration) FROM parquet.`kddcup.parquet` group by lbl, protocol_type")</pre></div><p>Finally, I promised you to compute percentiles. Computing percentiles usually involves sorting the whole dataset, which is expensive; however, if the tile is one of the first or the last ones, usually it is possible to optimize the computation:</p><div class="informalexample"><pre class="programlisting">val pct = sqlContext.sql("SELECT duration FROM parquet.`kddcup.parquet` where protocol_type = 'udp'").rdd.map(_.getLong(0)).cache
pct.top((0.05*pct.count).toInt).last</pre></div><p>Computing<a id="id39" class="indexterm"/> the exact percentiles for a more generic case is more <a id="id40" class="indexterm"/>computationally expensive and is provided as a part of the Spark Notebook example code.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Basic correlations"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec13"/>Basic correlations</h1></div></div></div><p>You probably <a id="id41" class="indexterm"/>noticed that detecting correlations from contingency tables is hard. Detecting patterns takes practice, but many people are much better at recognizing the patterns visually. Detecting actionable patterns is one of the primary goals of machine learning. While advanced supervised machine learning techniques that will be covered in <a class="link" href="ch04.xhtml" title="Chapter 4. Supervised and Unsupervised Learning">Chapter 4</a>, <span class="emphasis"><em>Supervised and Unsupervised Learning</em></span> and <a class="link" href="ch05.xhtml" title="Chapter 5. Regression and Classification">Chapter 5</a>, <span class="emphasis"><em>Regression and Classification</em></span> exist, initial analysis of interdependencies between variables can help with the right transformation of variables or selection of the best inference technique.</p><p>Multiple well-established visualization tools exist and there are multiple sites, such as <a class="ulink" href="http://www.kdnuggets.com">http://www.kdnuggets.com</a>, which specialize on ranking and providing recommendations on data analysis, data explorations, and visualization software. I am not going to question the validity and accuracy of such rankings in this book, and very few sites actually mention Scala as a specific way to visualize the data, even if this is possible with, say, a <code class="literal">D3.js</code> package. A good visualization is a great way to deliver your findings to a larger audience. One look is worth a thousand words.</p><p>For the purposes <a id="id42" class="indexterm"/>of this chapter, I will use <span class="strong"><strong>Grapher</strong></span> that is present on every Mac OS notebook. To open <span class="strong"><strong>Grapher</strong></span>, go to Utilities (<span class="emphasis"><em>shift</em></span> + <span class="emphasis"><em>command</em></span> + <span class="emphasis"><em>U</em></span> in Finder) and click on the <span class="strong"><strong>Grapher</strong></span> icon (or search by name by pressing <span class="emphasis"><em>command</em></span> + <span class="emphasis"><em>space</em></span>). Grapher presents many options, including the following <span class="strong"><strong>Log-Log</strong></span> and <span class="strong"><strong>Polar</strong></span> coordinates:</p><div class="mediaobject"><img src="Images/B04935_01_07.jpg" alt="Basic correlations" width="600" height="431"/><div class="caption"><p>Figure 01-7. The Grapher window</p></div></div><p>Fundamentally, the amount of information that can be delivered through visualization is limited by the number of pixels on the screen, which, for most modern computers, is in millions and color variations, which arguably can also be in millions (<span class="emphasis"><em>Judd</em></span>, <span class="emphasis"><em>Deane B.</em></span>; <span class="emphasis"><em>Wyszecki</em></span>, <span class="emphasis"><em>Günter</em></span> (<span class="emphasis"><em>1975</em></span>). <span class="emphasis"><em>Color in Business, Science and Industry</em></span>. <span class="emphasis"><em>Wiley Series in Pure and Applied Optics (3rd ed.)</em></span>. New York). If I am working on a multidimensional TB dataset, the dataset first needs to be summarized, processed, and reduced to a size that can be viewed on a computer screen.</p><p>For the purpose of illustration, I will use the Iris UCI dataset that can be found at <a class="ulink" href="https://archive.ics.uci.edu/ml/datasets/Iris">https://archive.ics.uci.edu/ml/datasets/Iris</a>. To bring the dataset into the tool, type the following code (on Mac OS):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro]$ pbcopy &lt; chapter01/data/iris/in.txt</strong></span>
</pre></div><p>Open the new <a id="id43" class="indexterm"/>
<span class="strong"><strong>Point Set</strong></span> in the <span class="strong"><strong>Grapher</strong></span> (<span class="emphasis"><em>command</em></span> + <span class="emphasis"><em>alt</em></span> + <span class="emphasis"><em>P</em></span>), press <span class="strong"><strong>Edit Points…</strong></span> and paste the data by pressing <span class="emphasis"><em>command</em></span> + <span class="emphasis"><em>V</em></span>. The tools has line-fitting capabilities with basic linear, polynomial, and exponential families and provides the popular chi-squared metric to estimate the goodness of the fit with respect to the number of free parameters:</p><div class="mediaobject"><img src="Images/B04935_01_08.jpg" alt="Basic correlations" width="900" height="558"/><div class="caption"><p>Figure 01-8. Fitting the Iris dataset using Grapher on Mac OS X</p></div></div><p>We will cover how<a id="id44" class="indexterm"/> to estimate the goodness of model fit in the following chapters.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec14"/>Summary</h1></div></div></div><p>I've tried to establish a common ground to perform a more complex data science later in the book. Don't expect these to be a complete set of exploratory techniques, as the exploratory techniques can extend to running very complex modes. However, we covered simple aggregations, sampling, file operations such as read and write, working with tools such as notebooks and Spark DataFrames, which brings familiar SQL constructs into the arsenal of an analyst working with Spark/Scala.</p><p>The next chapter will take a completely different turn by looking at the data pipelines as a part of a data-driven enterprise and cover the data discovery process from the business perspective: what are the ultimate goals we are trying to accomplish by doing the data analysis. I will cover a few traditional topics of ML, such as supervised and unsupervised learning, after this before delving into more complex representations of the data, where Scala really shows it's advantage over SQL.</p></div></div>



  </body></html>