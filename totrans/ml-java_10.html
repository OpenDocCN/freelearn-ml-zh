<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Text Mining with Mallet - Topic Modeling and Spam Detection</h1>
                </header>
            
            <article>
                
<p>In this chapter, we'll first discuss what <strong>text mining</strong> is, what kind of analysis it is able to offer, and why you might want to use it in your application. We'll then discuss how to work with <strong>Mallet</strong>, a Java library for natural-language processing, covering data import and text pre-processing. Afterward, we will look into two text-mining applications: <strong>topic modeling</strong>, where we will discuss how text mining can be used to identify topics found in  text documents without reading them individually, and <strong>spam detection</strong>, where we will discuss how to automatically classify text documents into categories.</p>
<p>This chapter will cover the following topics:</p>
<ul>
<li>Introducing text mining</li>
<li>Installing and working with Mallet</li>
<li>Topic modeling</li>
<li>Spam detection</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introducing text mining</h1>
                </header>
            
            <article>
                
<p>Text mining, or text analytics, refers to the process of automatically extracting high-quality information from text documents, most often written in natural language, where high-quality information is considered to be relevant, novel, and interesting.</p>
<p>While a typical text analytics application is used to scan a set of documents to generate a search index, text mining can be used in many other applications, including text categorization into specific domains; text clustering to automatically organize a set of documents; sentiment analysis to identify and extract subjective information in documents; concept or entity extraction that is capable of identifying people, places, organizations, and other entities from documents; document summarization to automatically provide the most important points in the original document; and learning relations between named entities.</p>
<p>The process based on statistical pattern mining usually involves the following steps:</p>
<ol>
<li>Information retrieval and extraction</li>
<li>Transforming unstructured text data into structured data; for example, parsing, removing noisy words, lexical analysis, calculating word frequencies, and deriving linguistic features</li>
<li>Discovery of patterns from structured data and tagging or annotation</li>
<li>Evaluation and interpretation of the results</li>
</ol>
<p>Later in this chapter, we will look at two application areas: topic modeling and <strong>text categorization</strong>. Let's examine what they bring to the table.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Topic modeling</h1>
                </header>
            
            <article>
                
<p>Topic modeling is an unsupervised technique and might be useful if you need to analyze a large archive of text documents and wish to understand what the archive contains, without necessarily reading every single document by yourself. A text document can be a blog post, an email, a tweet, a document, a book chapter, a diary entry, and so on. Topic modeling looks for patterns in a corpus of text; more precisely, it identifies topics as lists of words that appear in a statistically meaningful way. The most well-known algorithm is <strong>Latent Dirichlet Allocation</strong> (<strong>LDA</strong>), which assumes that the author composed a piece of text by selecting words from possible baskets of words, where each basket corresponds to a topic. Using this assumption, it becomes possible to mathematically decompose text into the most likely baskets from where the words first came. The algorithm then iterates over this process until it converges to the most likely distribution of words into baskets, which we call <em>topics</em>.</p>
<p>For example, if we use topic modeling on a series of news articles, the algorithm would return a list of topics and keywords that most likely comprise of these topics. Using the example of news articles, the list might look similar to the following:</p>
<ul>
<li>Winner, goal, football, score, first place</li>
<li>Company, stocks, bank, credit, business</li>
<li>Election, opponent, president, debate, upcoming</li>
</ul>
<p>By looking at the keywords, we can recognize that the news articles were concerned with sports, business, upcoming election, and so on. Later in this chapter, we will learn how to implement topic modeling using the news article example.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Text classification</h1>
                </header>
            
            <article>
                
<p>In text classification, or text categorization, the goal is to assign a text document according to its content to one or more classes or categories, which tend to be a more general subject area, such as vehicles or pets. Such general classes are referred to as topics, and the classification task is then called <strong>text classification</strong>, <strong>text categorization</strong>, <strong>topic classification</strong>, or <strong>topic spotting</strong>. While documents can be categorized according to other attributes such as document type, author, and publication year, the focus in this chapter will be on the document content only. Examples of text classification include the following components:</p>
<ul>
<li>Spam detection in email messages, user comments, web pages, and so on</li>
<li>Detection of sexually explicit content</li>
<li>Sentiment detection, which automatically classifies a product or service review as positive or negative</li>
<li>Email sorting according to content</li>
<li>Topic-specific search, where search engines restrict searches to a particular topic or genre, hence providing more accurate results</li>
</ul>
<p>These examples show how important text classification is in information retrieval systems; hence, most modern information retrieval systems use some kind of text classifier. The classification task that we will use as an example in this book is text classification for detecting email spam.</p>
<p>We will continue this chapter with an introduction to Mallet, a Java-based package for statistical natural-language processing, document classification, clustering, topic modeling, information extraction, and other machine-learning applications to text. We will then cover two text analytics applications, namely, topics modeling and spam detection as text classification.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Installing Mallet</h1>
                </header>
            
            <article>
                
<p>Mallet is available for download at the UMass Amherst University website at <span class="URLPACKT"><a href="http://mallet.cs.umass.edu/download.php">http://mallet.cs.umass.edu/download.php</a></span>. Navigate to the <span class="packt_screen">Download</span> section as shown in the following screenshot and select the latest stable release (<strong>2.0.8</strong>, at the time of writing this book):</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-504 image-border" src="Images/8d8e3fb3-83a6-48cc-b297-3650b7e8140d.png" style="width:110.42em;height:83.50em;" width="1325" height="1002"/></p>
<p>Download the ZIP file and extract the content. In the extracted directory, you should find a folder named <kbd>dist</kbd> with two JAR files: <kbd>mallet.jar</kbd> and <kbd>mallet-deps.jar</kbd>. The first one contains all of the packaged Mallet classes, while the second one packs all of the dependencies. We will include both JARs files in your project as referenced libraries, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-505 image-border" src="Images/f4ea3672-2a16-4e49-a313-bdb4ae51066c.png" style="width:38.08em;height:27.58em;" width="457" height="331"/></p>
<p>If you are using Eclipse, right-click on <span class="packt_screen">Project</span>, select <span class="packt_screen">Properties</span>, and pick <span class="packt_screen">Java Build Path</span>. Select the <span class="packt_screen">Libraries</span> tab and click <span class="packt_screen">Add External JARs</span>. Now, select the two JARs files and confirm, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-506 image-border" src="Images/5d0141f0-1ce7-4a70-a288-9857e639ec1f.png" style="width:61.75em;height:44.33em;" width="741" height="532"/></p>
<p>Now we are ready to start using Mallet.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Working with text data</h1>
                </header>
            
            <article>
                
<p>One of the main challenges in text mining is transforming unstructured written natural language into structured attribute-based instances. The process involves many steps, as shown here:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-507 image-border" src="Images/76c1e7b0-4d6a-4ed6-bff4-981855149687.png" style="width:44.58em;height:15.08em;" width="597" height="202"/></p>
<p>First, we extract some text from the internet, existing documents, or databases. At the end of the first step, the text could still be present in the XML format or some other proprietary format. The next step is to extract the actual text and segment it into parts of the document, for example, title, headline, abstract, and body. The third step is involved with normalizing text encoding to ensure the characters are presented in the same way; for example, documents encoded in formats such as ASCII, ISO 8859-1 and Windows-1250 are transformed into Unicode encoding. Next, tokenization splits the document into particular words, while the next step removes frequent words that usually have low predictive power, for example, the, a, I, and we.</p>
<p>The <strong>Part-Of-Speech</strong> (<strong>POS</strong>) tagging and lemmatization step could be included to transform each token to its basic form, which is known as <strong>lemma</strong>, by removing word endings and modifiers. For example, running becomes run, and better becomes good. A simplified approach is stemming, which operates on a single word without any context of how the particular word is used, and therefore cannot distinguish between words having different meaning, depending on the part of speech, for example, axes as a plural of axe as well as axis.</p>
<p>The last step transforms tokens into a feature space. Most often, feature space is a <strong>Bag-Of-Words</strong> (<strong>BoW</strong>) presentation. In this presentation, a set of all words appearing in the dataset is created. Each document is then presented as a vector that counts how many times a particular word appears in the document.</p>
<p>Consider the following example with two sentences:</p>
<ul>
<li>Jacob likes table tennis. Emma likes table tennis too</li>
<li>Jacob also likes basketball</li>
</ul>
<p>The BoW in this case consists of {Jacob, likes, table, tennis, Emma, too, also, basketball}, which has eight distinct words. The two sentences could be now presented as vectors using the indexes of the list, indicating how many times a word at a particular index appears in the document, as follows:</p>
<ul>
<li>[1, 2, 2, 2, 1, 0, 0, 0]</li>
<li>[1, 1, 0, 0, 0, 0, 1, 1]</li>
</ul>
<p>Such vectors finally become instances for further learning.</p>
<div class="packt_infobox">Another very powerful presentation based on the BoW model is <strong>word2vec</strong>. Word2vec was introduced in 2013 by a team of researchers led by Tomas Mikolov at Google. Word2vec is a neural network that learns distributed representations for words. An interesting property of this presentation is that words appear in clusters, so that some word relationships, such as analogies, can be reproduced using vector math. A famous example shows that king−man+woman returns queen. Further details and implementation are available at the following link: <span class="URLPACKT"><a href="https://code.google.com/archive/p/word2vec/">https://code.google.com/archive/p/word2vec/</a>.<br/></span></div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Importing data</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will not look into how to scrap a set of documents from a website or extract them from database. Instead, we will assume that we have already collected them as set of documents and stored them in the <kbd>.txt</kbd> file format. Now let's look at two options for loading them. The first option addresses the situation where each document is stored in its own <kbd>.txt</kbd> file. The second option addresses the situation where all of the documents are stored in a single file by taking one per line.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Importing from directory</h1>
                </header>
            
            <article>
                
<p>Mallet supports reading from directory with the <kbd>cc.mallet.pipe.iterator.FileIterator</kbd> class. A file iterator is constructed with the following three parameters:</p>
<ul>
<li>A list of <kbd>File[]</kbd> directories with text files</li>
<li>A file filter that specifies which files to select within a directory</li>
<li>A pattern that is applied to a filename to produce a class label</li>
</ul>
<p>Consider the data structured into folders as shown in the following screenshot. We have documents organized in five topics by folders (<kbd>tech</kbd>, <kbd>entertainment</kbd>, <kbd>politics</kbd>, <kbd>sport</kbd>, and <kbd>business</kbd>). Each folder contains documents on particular topics, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-508 image-border" src="Images/8c8de3e3-7eb8-4a67-adef-fc6b9320aecf.png" style="width:14.42em;height:19.67em;" width="173" height="236"/></p>
<p>In this case, we initialize <kbd>iterator</kbd> as follows:</p>
<pre>FileIterator iterator = 
  new FileIterator(new File[]{new File("path-to-my-dataset")}, 
  new TxtFilter(), 
  FileIterator.LAST_DIRECTORY); </pre>
<p>The first parameter specifies the path to our root folder, the second parameter limits the iterator to the <kbd>.txt</kbd> files only, while the last parameter asks the method to use the last directory name in the path as class label.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Importing from file</h1>
                </header>
            
            <article>
                
<p>Another option to load the documents is through <kbd>cc.mallet.pipe.iterator.CsvIterator.CsvIterator(Reader, Pattern, int, int, int)</kbd>, which assumes all of the documents are in a single file and returns one instance per line extracted by a regular expression. The class is initialized by the following components:</p>
<ul>
<li><kbd>Reader</kbd>: This is the object that specifies how to read from a file</li>
<li><kbd>Pattern</kbd>: This is a regular expression, extracting three groups: data, target label, and document name</li>
<li><kbd>int, int, int</kbd>: These are the indexes of data, target, and name groups as they appear in a regular expression</li>
</ul>
<p>Consider a text document in the following format, specifying the document name, category, and content:</p>
<pre>AP881218 local-news A 16-year-old student at a private <br/>   Baptist...  
AP880224 business The Bechtel Group Inc. offered in 1985 to...  
AP881017 local-news A gunman took a 74-year-old woman hostage...  
AP900117 entertainment Cupid has a new message for lovers <br/>   this...  
AP880405 politics The Reagan administration is weighing w...  </pre>
<p>To parse a line into three groups, we can use the following regular expression:</p>
<pre>^(\\S*)[\\s,]*(\\S*)[\\s,]*(.*)$</pre>
<p>There are three groups that appear in parenthesies <kbd>()</kbd>, where the third group contains the data, the second group contains the target class, and the first group contains the document ID. <kbd>iterator</kbd> is initialized as follows:</p>
<pre>CsvIterator iterator = new CsvIterator ( 
fileReader, 
Pattern.compile("^(\\S*)[\\s,]*(\\S*)[\\s,]*(.*)$"), 
  3, 2, 1)); </pre>
<p>Here, the regular expression extracts the three groups separated by an empty space and their order is <kbd>3, 2, 1</kbd>.</p>
<p>Now let's move to the data-preprocessing pipeline.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Pre-processing text data</h1>
                </header>
            
            <article>
                
<p>Once we initialize an iterator that will go through the data, we need to pass the data through a sequence of transformations as described at the beginning of this section. Mallet supports this process through a pipeline and a wide variety of steps that could be included in a pipeline, which are collected in the <kbd>cc.mallet.pipe</kbd> package. Some examples are as follows:</p>
<ul>
<li><kbd>Input2CharSequence</kbd>: This is a pipe that can read from various kinds of text sources (either URL, file, or reader) into <kbd>CharSequence</kbd></li>
<li><kbd>CharSequenceRemoveHTML</kbd>: This pipe removes HTML from <kbd>CharSequence</kbd></li>
<li><kbd>MakeAmpersandXMLFriendly</kbd>: This converts <kbd>&amp;</kbd> into <kbd>&amp;amp</kbd> in tokens of a token sequence</li>
<li><kbd>TokenSequenceLowercase</kbd>: This converts the text in each token in the token sequence in the data field into lowercase</li>
<li><kbd>TokenSequence2FeatureSequence</kbd>: This converts the token sequence in the data field of each instance into a feature sequence</li>
<li><kbd>TokenSequenceNGrams</kbd>: This converts the token sequence in the data field into a token sequence of ngrams, that is, a combination of two or more words</li>
</ul>
<div class="packt_infobox">The full list of processing steps is available in the following Mallet documentation: <a href="http://mallet.cs.umass.edu/api/index.html?cc/mallet/pipe/iterator/package-tree.html">http://mallet.cs.umass.edu/api/index.html?cc/mallet/pipe/iterator/package-tree.html</a>.</div>
<p>Now we are ready to build a class that will import our data. We will do that using the following steps:</p>
<ol>
<li>Let's build a pipeline, where each processing step is denoted as a pipeline in Mallet. Pipelines can be wired together in a serial fashion with a list of <kbd>ArrayList&lt;Pipe&gt;</kbd> objects:</li>
</ol>
<pre style="padding-left: 60px">ArrayList&lt;Pipe&gt; pipeList = new ArrayList&lt;Pipe&gt;(); </pre>
<ol start="2">
<li>Let's begin by reading data from a file object and converting all of the characters into lowercase:</li>
</ol>
<pre style="padding-left: 60px">pipeList.add(new Input2CharSequence("UTF-8")); 
pipeList.add( new CharSequenceLowercase() );</pre>
<ol start="3">
<li>We will tokenize raw strings with a regular expression. The following pattern includes unicode letters and numbers and the underscore character:</li>
</ol>
<pre style="padding-left: 60px">Pattern tokenPattern = 
Pattern.compile("[\\p{L}\\p{N}_]+"); 
 
pipeList.add(new CharSequence2TokenSequence(tokenPattern)); </pre>
<ol start="4">
<li>We will now remove stop words, that is, frequent words with no predictive power, using a standard English stop list. Two additional parameters indicate whether stop-word removal should be case-sensitive and mark deletions instead of just deleting the words. We'll set both of them to <kbd>false</kbd>:</li>
</ol>
<pre style="padding-left: 60px">pipeList.add(new TokenSequenceRemoveStopwords(new File(stopListFilePath), "utf-8", false, false, false));</pre>
<ol start="5">
<li>Instead of storing the actual words, we can convert them into integers, indicating a word index in the BoW:</li>
</ol>
<pre style="padding-left: 60px">pipeList.add(new TokenSequence2FeatureSequence()); </pre>
<ol start="6">
<li>We'll do the same for the class label; instead of the label string, we'll use an integer, indicating a position of the label in our bag of words:</li>
</ol>
<pre style="padding-left: 60px">pipeList.add(new Target2Label()); </pre>
<ol start="7">
<li>We could also print the features and the labels by invoking the <kbd>PrintInputAndTarget</kbd> pipe:</li>
</ol>
<pre style="padding-left: 60px">pipeList.add(new PrintInputAndTarget()); </pre>
<ol start="8">
<li>We store the list of pipelines in a <kbd>SerialPipes</kbd> class that will covert an instance through a sequence of pipes:</li>
</ol>
<pre style="padding-left: 60px">SerialPipes pipeline = new SerialPipes(pipeList); </pre>
<p>Now let's take a look at how apply this in a text-mining application!</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Topic modeling for BBC News</h1>
                </header>
            
            <article>
                
<p>As discussed earlier, the goal of topic modeling is to identify patterns in a text corpus that correspond to document topics. In this example, we will use a dataset originating from BBC News. This dataset is one of the standard benchmarks in machine-learning research, and is available for non-commercial and research purposes.</p>
<p>The goal is to build a classifier that is able to assign a topic to an uncategorized document.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">BBC dataset</h1>
                </header>
            
            <article>
                
<p>In 2006, Greene and Cunningham collected the BBC dataset to study a particular document—<em>Clustering challenge using support vector machines</em>. The dataset consists of 2,225 documents from the BBC News website from 2004 to 2005, corresponding to the stories collected from five topical areas: business, entertainment, politics, sport, and technology. The dataset can be seen at the following website: <span class="URLPACKT"><a href="http://mlg.ucd.ie/datasets/bbc.html">http://mlg.ucd.ie/datasets/bbc.html</a>.<br/></span></p>
<p>We can download the raw text files under the <span class="packt_screen">Dataset: BBC</span> section. You will also notice that the website contains an already processed dataset, but, for this example, we want to process the dataset by ourselves. The ZIP contains five folders, one per topic. The actual documents are placed in the corresponding topic folder, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-509 image-border" src="Images/c7869097-c8f5-4e07-a09c-1c7995546906.png" style="width:92.00em;height:61.83em;" width="1104" height="742"/></p>
<p>Now, let's build a topic classifier.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Modeling</h1>
                </header>
            
            <article>
                
<p>We will begin the modeling phase using the following steps:</p>
<ol>
<li>We will start by importing the dataset and processing the text using the following lines of code:</li>
</ol>
<pre style="padding-left: 60px">import cc.mallet.types.*; 
import cc.mallet.pipe.*; 
import cc.mallet.pipe.iterator.*; 
import cc.mallet.topics.*; 
 
import java.util.*; 
import java.util.regex.*; 
import java.io.*; 
 
public class TopicModeling { 
 
  public static void main(String[] args) throws Exception { 
 
String dataFolderPath = "data/bbc"; 
String stopListFilePath = "data/stoplists/en.txt"; </pre>
<ol start="2">
<li>We will then create a default <kbd>pipeline</kbd>  object as previously described:</li>
</ol>
<pre style="padding-left: 60px">ArrayList&lt;Pipe&gt; pipeList = new ArrayList&lt;Pipe&gt;(); 
pipeList.add(new Input2CharSequence("UTF-8")); 
Pattern tokenPattern = Pattern.compile("[\\p{L}\\p{N}_]+"); 
pipeList.add(new CharSequence2TokenSequence(tokenPattern)); 
pipeList.add(new TokenSequenceLowercase()); 
pipeList.add(new TokenSequenceRemoveStopwords(new File(stopListFilePath), "utf-8", false, false, false)); 
pipeList.add(new TokenSequence2FeatureSequence()); 
pipeList.add(new Target2Label()); 
SerialPipes pipeline = new SerialPipes(pipeList); </pre>
<ol start="3">
<li>Next, we will initialize the <kbd>folderIterator</kbd> object:</li>
</ol>
<pre style="padding-left: 60px">FileIterator folderIterator = new FileIterator( 
    new File[] {new File(dataFolderPath)}, 
    new TxtFilter(), 
    FileIterator.LAST_DIRECTORY);</pre>
<ol start="4">
<li>We will now construct a new instance list with the <kbd>pipeline</kbd> that we want to use to process the text:</li>
</ol>
<pre style="padding-left: 60px">InstanceList instances = new InstanceList(pipeline);</pre>
<ol start="5">
<li>We process each instance provided by  <kbd>iterator</kbd>:</li>
</ol>
<pre style="padding-left: 60px">instances.addThruPipe(folderIterator); </pre>
<ol start="6">
<li>Now let's create a model with five topics using the <kbd>cc.mallet.topics.ParallelTopicModel.ParallelTopicModel</kbd> class that implements a simple threaded LDA model. LDA is a common method for topic modeling that uses Dirichlet distribution to estimate the probability that a selected topic generates a particular document. We will not dive deep into the details in this chapter; the reader is referred to the original paper by D. Blei et al. (2003).</li>
</ol>
<div class="packt_infobox">Note: There is another classification algorithm in machine learning with the same initialism that refers to <strong>Linear Discriminant Analysis</strong> (<strong>LDA</strong>). Beside the common acronym, it has nothing in common with the LDA model.</div>
<p style="padding-left: 60px">The class is instantiated with parameters alpha and beta, which can be broadly interpreted as follows:</p>
<ul>
<li>High alpha value means that each document is likely to contain a mixture of most of the topics, and not any single topic specifically. A low alpha value puts less of such constraints on documents, and this means that it is more likely that a document may contain mixture of just a few, or even only one, of the topics.</li>
<li>A high beta value means that each topic is likely to contain a mixture of most of the words, and not any word specifically; while a low value means that a topic may contain a mixture of just a few of the words.</li>
</ul>
<p style="padding-left: 60px">In our case, we initially keep both parameters low (alpha_t = <kbd>0.01</kbd>, beta_w = <kbd>0.01</kbd>) as we assume topics in our dataset are not mixed much and there are many words for each of the topics:</p>
<pre style="padding-left: 60px">int numTopics = 5; 
ParallelTopicModel model =  
new ParallelTopicModel(numTopics, 0.01, 0.01); </pre>
<ol start="7">
<li>We will add <kbd>instances</kbd> to the model, and since we are using parallel implementation, we will specify the number of threads that will run in parallel, as follows:</li>
</ol>
<pre style="padding-left: 60px">model.addInstances(instances); 
model.setNumThreads(4);</pre>
<ol start="8">
<li>We will now run the model for a selected number of iterations. Each iteration is used for better estimation of internal LDA parameters. For testing, we can use a small number of iterations, for example, 50; while in real applications, use <kbd>1000</kbd> or <kbd>2000</kbd> iterations. Finally, we will call the <kbd>void estimate() </kbd>method that will actually build an LDA model:</li>
</ol>
<pre style="padding-left: 60px">model.setNumIterations(1000); 
model.estimate(); </pre>
<p>The model outputs the following result:</p>
<pre>    0 0,06654  game england year time win world 6 
    1 0,0863  year 1 company market growth economy firm 
    2 0,05981  people technology mobile mr games users music 
    3 0,05744  film year music show awards award won 
    4 0,11395  mr government people labour election party blair 
    
    [beta: 0,11328] 
    &lt;1000&gt; LL/token: -8,63377
    
    Total time: 45 seconds
  </pre>
<p><kbd>LL/token</kbd> indicates the model's log-likelihood, divided by the total number of tokens, indicating how likely the data is given the model. Increasing values mean the model is improving.</p>
<p>The output also shows the top words describing each topic. The words correspond to initial topics really well:</p>
<ul>
<li><strong>Topic 0</strong>: <kbd>game</kbd>, <kbd>england</kbd>, <kbd>year</kbd>, <kbd>time</kbd>, <kbd>win</kbd>, <kbd>world</kbd>, <kbd>6</kbd> ⇒ sport</li>
<li><strong>Topic 1</strong>: <kbd>year</kbd>, <kbd>1</kbd>, <kbd>company</kbd>, <kbd>market</kbd>, <kbd>growth</kbd>, <kbd>economy</kbd>, <kbd>firm</kbd> ⇒ finance</li>
<li><strong>Topic 2</strong>: <kbd>people</kbd>, <kbd>technology</kbd>, <kbd>mobile</kbd>, <kbd>mr</kbd>, <kbd>games</kbd>, <kbd>users</kbd>, <kbd>music</kbd> ⇒ tech</li>
<li><strong>Topic 3</strong>: <kbd>film</kbd>, <kbd>year</kbd>, <kbd>music</kbd>, <kbd>show</kbd>, <kbd>awards</kbd>, <kbd>award</kbd>, <kbd>won</kbd> ⇒ entertainment</li>
<li><strong>Topic 4</strong>: <kbd>mr</kbd>, <kbd>government</kbd>, <kbd>people</kbd>, <kbd>labor</kbd>, <kbd>election</kbd>, <kbd>party</kbd>, <kbd>blair</kbd> ⇒ politics</li>
</ul>
<p>There are still some words that don't make much sense, for instance, <kbd>mr</kbd>, <kbd>1</kbd>, and <kbd>6</kbd>. We could include them in the stop word list. Also, some words appear twice, for example, <kbd>award</kbd> and <kbd>awards</kbd>. This happened because we didn't apply any stemmer or lemmatization pipe.</p>
<p>In the next section, we'll take a look to check whether the model is any good.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Evaluating a model</h1>
                </header>
            
            <article>
                
<p>As statistical topic modeling has an unsupervised nature, it makes model selection difficult. For some applications, there may be some extrinsic tasks at hand, such as information retrieval or document classification, for which performance can be evaluated. However, in general, we want to estimate the model's ability to generalize topics regardless of the task.</p>
<p>In 2009, Wallach et al. introduced an approach that measures the quality of a model by computing the log probability of held-out documents under the model. The likelihood of unseen documents can be used to compare models—higher likelihood implies a better model.</p>
<p>We will evaluate the model using the following steps:</p>
<ol start="1">
<li>Let's split the documents into training and test sets (that is, held-out documents), where we use 90% for training and 10% for testing:</li>
</ol>
<pre style="padding-left: 60px">// Split dataset 
InstanceList[] instanceSplit= instances.split(new Randoms(), new <br/>   double[] {0.9, 0.1, 0.0}); </pre>
<ol start="2">
<li>Now let's rebuild our model using only <kbd>90%</kbd> of our documents:</li>
</ol>
<pre style="padding-left: 60px">// Use the first 90% for training 
model.addInstances(instanceSplit[0]); 
model.setNumThreads(4); 
model.setNumIterations(50); 
model.estimate(); </pre>
<ol start="3">
<li>We will initialize an <kbd>estimator</kbd> object that implements Wallach's log probability of held-out documents, <kbd>MarginalProbEstimator</kbd>:</li>
</ol>
<pre style="padding-left: 60px">// Get estimator 
MarginalProbEstimator estimator = model.getProbEstimator(); </pre>
<div class="packt_infobox">An intuitive description of LDA is summarized by Annalyn Ng in her blog: <span class="URLPACKT"><a href="https://annalyzin.wordpress.com/2015/06/21/laymans-explanation-of-topic-modeling-with-lda-2/">https://annalyzin.wordpress.com/2015/06/21/laymans-explanation-of-topic-modeling-with-lda-2/</a>. </span>To get deeper insight into the LDA algorithm, its components, and its working, take a look at the original paper LDA by David Blei et al. (2003) at <span class="URLPACKT"><a href="http://jmlr.csail.mit.edu/papers/v3/blei03a.html">http://jmlr.csail.mit.edu/papers/v3/blei03a.html</a>,</span> or take a look at the summarized presentation by D. Santhanam of Brown University at <span class="URLPACKT"><a href="http://www.cs.brown.edu/courses/csci2950-p/spring2010/lectures/2010-03-03_santhanam.pdf">http://www.cs.brown.edu/courses/csci2950-p/spring2010/lectures/2010-03-03_santhanam.pdf</a></span>.</div>
<p style="padding-left: 60px">The class implements many estimators that require quite deep theoretical knowledge of how the LDA method works. We'll pick the left-to-right evaluator, which is appropriate for a wide range of applications, including text mining, and speech recognition. The left-to-right evaluator is implemented as the <kbd>double evaluateLeftToRight</kbd> method, accepting the following components:</p>
<ul>
<li style="padding-left: 60px"><kbd>Instances heldOutDocuments</kbd>: This tests the instances.</li>
<li style="padding-left: 60px"><kbd>int numParticles</kbd>: This algorithm parameter indicates the number of left-to-right tokens, where the default value is 10.</li>
<li style="padding-left: 60px"><kbd>boolean useResampling</kbd>: This states whether to resample topics in left-to-right evaluation; resampling is more accurate, but leads to quadratic scaling in the length of documents.</li>
<li style="padding-left: 60px"><kbd>PrintStream docProbabilityStream</kbd>: This is the file or <kbd>stdout</kbd> in which we write the inferred log probabilities per document.</li>
</ul>
<ol start="4">
<li>Let's run  <kbd>estimator</kbd>, as follows:</li>
</ol>
<pre style="padding-left: 60px">double loglike = estimator.evaluateLeftToRight( 
  instanceSplit[1], 10, false, null);); 
System.out.println("Total log likelihood: "+loglike); </pre>
<p>In our particular case, the <kbd>estimator</kbd> outputs the following <kbd>log likelihood</kbd>, which makes sense when it is compared to other models that are either constructed with different parameters, pipelines, or data—the higher the log likelihood, the better the model is:</p>
<pre>    Total time: 3 seconds
    Topic Evaluator: 5 topics, 3 topic bits, 111 topic mask
    Total log likelihood: -360849.4240795393</pre>
<p>Now let's take a look at how to make use of this model.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Reusing a model</h1>
                </header>
            
            <article>
                
<p>As we are usually not building models on the fly, it often makes sense to train a model once and use it repeatedly to classify new data.</p>
<p>Note that, if you'd like to classify new documents, they need go through the same pipeline as other documents—the pipe needs to be the same for both training and classification. During training, the pipe's data alphabet is updated with each training instance. If you create a new pipe with the same steps, you don't produce the same pipeline as its data alphabet is empty. Therefore, to use the model on new data, we have to save or load the pipe along with the model and use this pipe to add new instances.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Saving a model</h1>
                </header>
            
            <article>
                
<p>Mallet supports a standard method for saving and restoring objects based on serialization.</p>
<p>We simply create a new instance of the <kbd>ObjectOutputStream</kbd> class and write the object into a file, as follows:</p>
<pre>String modelPath = "myTopicModel"; 
 
//Save model 
ObjectOutputStream oos = new ObjectOutputStream( 
new FileOutputStream (new File(modelPath+".model"))); 
oos.writeObject(model); 
oos.close();    
   
//Save pipeline 
oos = new ObjectOutputStream( 
new FileOutputStream (new File(modelPath+".pipeline"))); 
oos.writeObject(pipeline); 
oos.close(); </pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Restoring a model</h1>
                </header>
            
            <article>
                
<p>Restoring a model saved through serialization is simply an inverse operation using the <kbd>ObjectInputStream</kbd> class:</p>
<pre>String modelPath = "myTopicModel"; 
 
//Load model 
ObjectInputStream ois = new ObjectInputStream( 
  new FileInputStream (new File(modelPath+".model"))); 
ParallelTopicModel model = (ParallelTopicModel) ois.readObject(); 
ois.close();    
 
// Load pipeline 
ois = new ObjectInputStream( 
  new FileInputStream (new File(modelPath+".pipeline"))); 
SerialPipes pipeline = (SerialPipes) ois.readObject(); 
ois.close();    </pre>
<p>We discussed how to build an LDA model to automatically classify documents into topics. In the next example, we'll look into another text mining problem—text classification.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Detecting email spam </h1>
                </header>
            
            <article>
                
<p>Spam or electronic spam refers to unsolicited messages, typically carrying advertising content, infected attachments, links to phishing or malware sites, and so on. While the most widely recognized form of spam is email spam, spam abuses appear in other media as well: website comments, instant messaging, internet forums, blogs, online ads, and so on.</p>
<p>In this chapter, we will discuss how to build Naive Bayesian spam filtering, using BoW representation to identify spam emails. Naive Bayes spam filtering is one of the basic techniques that was implemented in the first commercial spam filters; for instance, Mozilla Thunderbird mail client uses native implementation of such filtering. While the example in this chapter will use email spam, the underlying methodology can be applied to other type of text-based spam as well.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Email spam dataset</h1>
                </header>
            
            <article>
                
<p>In 2000, Androutsopoulos et al. collected one of the first email spam datasets to benchmark spam-filtering algorithms. They studied how the Naive Bayes classifier can be used to detect spam, if additional pipes such as stop list, stemmer, and lemmatization contribute to better performance. The dataset was reorganized by Andrew Ng in OpenClassroom's machine-learning class, available for download at <span class="URLPACKT"><a href="http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&amp;doc=exercises/ex6/ex6.html">http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&amp;doc=exercises/ex6/ex6.html</a></span>.</p>
<p>Select and download the second option, <kbd>ex6DataEmails.zip</kbd>, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-510 image-border" src="Images/089d7c7f-00e3-4f7c-beff-9c635d90401c.png" style="width:80.58em;height:35.33em;" width="967" height="424"/></p>
<p>The ZIP contains the following folders:</p>
<ul>
<li>The <kbd>nonspam-train</kbd> and <kbd>spam-train</kbd> folders contain the pre-processed emails that you will use for training. They have 350 emails each.</li>
<li>The <kbd>nonspam-test</kbd> and <kbd>spam-test</kbd> folders constitute the test set, containing 130 spam and 130 nonspam emails. These are the documents that you will make predictions on. Notice that, even though separate folders tell you the correct labeling, you should make your predictions on all of the test documents without this knowledge. After you make your predictions, you can use the correct labeling to check whether your classifications were correct.</li>
</ul>
<p>To leverage Mallet's folder iterator, let's reorganize the folder structure as follows. We will create two folders, <kbd>train</kbd> and <kbd>test</kbd>, and put the <kbd>spam/nospam</kbd> folders under the corresponding folders. The initial folder structure is as shown in the following screenshot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-511 image-border" src="Images/d70d02fe-2552-4cdf-a0d7-5a65f00d0f72.png" style="width:20.25em;height:11.58em;" width="243" height="139"/></div>
<p>The final folder structure will be as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-519 image-border" src="Images/8fc63051-3265-49f9-915c-0efac24c48a2.png" style="width:20.33em;height:13.58em;" width="244" height="163"/></p>
<p>The next step is to transform email messages to feature vectors.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Feature generation</h1>
                </header>
            
            <article>
                
<p>We will perform feature generation using the following steps:</p>
<ol>
<li>We will create a default pipeline, as described previously:</li>
</ol>
<pre style="padding-left: 60px">ArrayList&lt;Pipe&gt; pipeList = new ArrayList&lt;Pipe&gt;(); 
pipeList.add(new Input2CharSequence("UTF-8")); 
Pattern tokenPattern = Pattern.compile("[\\p{L}\\p{N}_]+"); 
pipeList.add(new CharSequence2TokenSequence(tokenPattern)); 
pipeList.add(new TokenSequenceLowercase()); 
pipeList.add(new TokenSequenceRemoveStopwords(new <br/>   File(stopListFilePath), "utf-8", false, false, false)); 
pipeList.add(new TokenSequence2FeatureSequence()); 
pipeList.add(new FeatureSequence2FeatureVector()); 
pipeList.add(new Target2Label()); 
SerialPipes pipeline = new SerialPipes(pipeList); </pre>
<p style="padding-left: 60px">Note that we added an additional <kbd>FeatureSequence2FeatureVector</kbd> pipe that transforms a feature sequence into a feature vector. When we have data in a feature vector, we can use any classification algorithm, as we saw in the previous chapters. We'll continue our example in Mallet to demonstrate how to build a classification model.</p>
<ol start="2">
<li>We initialize a folder iterator to load our examples in the <kbd>train</kbd> folder comprising email examples in the <kbd>spam</kbd> and <kbd>nonspam</kbd> subfolders, which will be used as example labels:</li>
</ol>
<pre style="padding-left: 60px">FileIterator folderIterator = new FileIterator( 
    new File[] {new File(dataFolderPath)}, 
    new TxtFilter(), 
    FileIterator.LAST_DIRECTORY); </pre>
<ol start="3">
<li>We will construct a new instance list with the <kbd>pipeline</kbd> object that we want to use to process the text:</li>
</ol>
<pre style="padding-left: 60px">InstanceList instances = new InstanceList(pipeline); </pre>
<ol start="4">
<li>We will process each instance provided by the iterator:</li>
</ol>
<pre style="padding-left: 60px">instances.addThruPipe(folderIterator); </pre>
<p>We have now loaded the data and transformed it into feature vectors. Let's train our model on the training set and predict the <kbd>spam/nonspam</kbd> classification on the <kbd>test</kbd> set.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Training and testing</h1>
                </header>
            
            <article>
                
<p>Mallet implements a set of classifiers in the <kbd>cc.mallet.classify</kbd> package, including decision trees, Naive Bayes, AdaBoost, bagging, boosting, and many others. We'll start with a basic classifier, that is, a Naive Bayes classifier. A classifier is initialized by the <kbd>ClassifierTrainer</kbd> class, which returns a classifier when we invoke its <kbd>train(Instances)</kbd> method:</p>
<pre>ClassifierTrainer classifierTrainer = new NaiveBayesTrainer(); 
Classifier classifier = classifierTrainer.train(instances); </pre>
<p>Now let's see how this classier works and evaluate its performance on a separate dataset.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Model performance</h1>
                </header>
            
            <article>
                
<p>To evaluate the classifier on a separate dataset, we will use the following steps:</p>
<ol>
<li>Let's start by importing the emails located in our <kbd>test</kbd> folder:</li>
</ol>
<pre style="padding-left: 60px">InstanceList testInstances = new <br/>   InstanceList(classifier.getInstancePipe()); 
folderIterator = new FileIterator( 
    new File[] {new File(testFolderPath)}, 
    new TxtFilter(), 
    FileIterator.LAST_DIRECTORY); </pre>
<ol start="2">
<li>We will pass the data through the same pipeline that we initialized during training:</li>
</ol>
<pre style="padding-left: 60px">testInstances.addThruPipe(folderIterator); </pre>
<ol start="3">
<li>To evaluate classifier performance, we'll use the <kbd>cc.mallet.classify.Trial</kbd> class, which is initialized with a classifier and set of test instances:</li>
</ol>
<pre style="padding-left: 60px">Trial trial = new Trial(classifier, testInstances); </pre>
<ol start="4">
<li>The evaluation is performed immediately at initialization. We can then simply take out the measures that we care about. In our example, we'd like to check the precision and recall on classifying spam email messages, or F-measure, which returns a harmonic mean of both values, as follows:</li>
</ol>
<pre style="padding-left: 60px">System.out.println( 
  "F1 for class 'spam': " + trial.getF1("spam")); 
System.out.println( 
  "Precision:" + trial.getPrecision(1)); 
System.out.println( 
  "Recall:" + trial.getRecall(1)); </pre>
<p>The evaluation object outputs the following results:</p>
<pre>    F1 for class 'spam': 0.9731800766283524
    Precision: 0.9694656488549618
    Recall: 0.9769230769230769
  </pre>
<p>The results show that the model correctly discovers 97.69% of spam messages (recall), and when it marks an email as spam, it is correct in 96.94% cases. In other words, it misses approximately 2 per 100 spam messages and marks 3 per 100 valid messages as spam. So, it's not really perfect, but it's more than a good start!</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we discussed how text mining is different than traditional attribute-based learning, requiring a lot of pre-processing steps to transform written natural language into feature vectors. Further, we discussed how to leverage Mallet, a Java-based library for NLP by applying it to two real-life problems. First, we modeled topics in a news corpus using the LDA model to build a model that is able to assign a topic to new document. We also discussed how to build a Naive Bayesian spam-filtering classifier using the BoW representation.</p>
<p>This chapter concludes the technical demonstrations of how to apply various libraries to solve machine-learning tasks. As we weren't able to cover more interesting applications and give further details at many points, the next chapter gives some further pointers on how to continue learning and dive deeper into particular topics.</p>


            </article>

            
        </section>
    </div>



  </body></html>