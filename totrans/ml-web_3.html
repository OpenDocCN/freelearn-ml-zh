<html><head></head><body><div class="chapter" title="Chapter&#xA0;3.&#xA0;Supervised Machine Learning">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h1 class="title"><a id="ch03"/>&#13;
 Chapter 3. Supervised Machine Learning</h1>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>In this chapter, the most relevant regression and classification techniques are discussed. All of these algorithms share the same background procedure, and usually the name of the algorithm refers to both a classification and a regression method. The linear regression algorithms, Naive Bayes, decision tree, and support vector machine are going to be discussed in the following sections. To understand how to employ the techniques, a classification and a regression problem will be solved using the mentioned methods. Essentially, a labeled train dataset will be used to <span class="emphasis">&#13;
<em>train the models</em>&#13;
</span>&#13;
 , which means to find the values of the parameters, as we discussed in the introduction. As usual, the code is available in my GitHub folder at <a class="ulink" href="https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/">https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/</a>&#13;
 .</p>&#13;
<p>We will conclude the chapter with an extra algorithm that may be used for classification, although it is not specifically designed for this purpose (hidden Markov model). We will now begin to explain the general causes of error in the methods when predicting the true labels associated with a dataset.</p>&#13;
<div class="section" title="Model error estimation">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h1 class="title"><a id="ch03lvl1sec17"/>&#13;
 Model error estimation</h1>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>We said that the<a id="id231" class="indexterm"/>&#13;
 trained model is used to predict the labels of new data, and the quality of the prediction depends on the ability of the model to <span class="emphasis">&#13;
<em>generalize</em>&#13;
</span>&#13;
 , that is, the correct prediction of cases not present in the trained data. This is a well-known problem in literature and related to two concepts: bias and variance of the outputs.</p>&#13;
<p>The bias is the error due to a wrong assumption in the algorithm. Given a point <span class="emphasis">&#13;
<em>x<sup>(t)</sup>&#13;
</em>&#13;
</span>&#13;
 with label <span class="emphasis">&#13;
<em>y<sub>t</sub>&#13;
</em>&#13;
</span>&#13;
 , the model is biased if it is trained with different training sets, and the predicted label <span class="emphasis">&#13;
<em>y<sub>t</sub>&#13;
 <sup>pred</sup>&#13;
</em>&#13;
</span>&#13;
 will always be different from <span class="emphasis">&#13;
<em>y<sub>t</sub>&#13;
</em>&#13;
</span>&#13;
 . The variance error instead refers to the different, wrongly predicted labels of the given point <span class="emphasis">&#13;
<em>x<sup>(t)</sup>&#13;
</em>&#13;
</span>&#13;
 . A classic example to explain the concepts is to consider a circle with the true value at the center (true label), as shown in the following figure. The closer the predicted labels are to the center, the more unbiased the model and the lower the variance (top left in the following figure). The other three cases are also shown here:</p>&#13;
<div class="mediaobject"><img src="Image00141.jpg" alt="Model error estimation"/>&#13;
<div class="caption">&#13;
<p>Variance and bias example.</p>&#13;
</div>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>A model with<a id="id232" class="indexterm"/>&#13;
 low variance and low bias errors will have the predicted labels that is blue dots (as show in the preceding figure) concentrated on the red center (true label). The high bias error occurs when the predictions are far away from the true label, while high variance appears when the predictions are in a wide range of values.</p>&#13;
<p>We have already seen that labels can be continuous or discrete, corresponding to regression classification problems respectively. Most of the models are suitable for solving both problems, and we are going to use word regression and classification referring to the same model. More formally, given a set of <span class="emphasis">&#13;
<em>N</em>&#13;
</span>&#13;
 data points and corresponding labels <span class="inlinemediaobject"><img src="Image00142.jpg" alt="Model error estimation"/>&#13;
</span>&#13;
 , a model with a set of parameters <span class="inlinemediaobject"><img src="Image00143.jpg" alt="Model error estimation"/>&#13;
</span>&#13;
 with the true parameter values <span class="inlinemediaobject"><img src="Image00144.jpg" alt="Model error estimation"/>&#13;
</span>&#13;
 will<a id="id233" class="indexterm"/>&#13;
 have the <span class="strong">&#13;
<strong>mean square error</strong>&#13;
</span>&#13;
 (<span class="strong">&#13;
<strong>MSE</strong>&#13;
</span>&#13;
 ), equal to:</p>&#13;
<div class="mediaobject"><img src="Image00145.jpg" alt="Model error estimation"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>We will use<a id="id234" class="indexterm"/>&#13;
 the MSE as a measure to evaluate the methods discussed in this chapter. Now we will start describing the generalized linear methods.</p>&#13;
</div>&#13;
</div>&#13;

<div class="section" title="Generalized linear models">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h1 class="title"><a id="ch03lvl1sec18"/>&#13;
 Generalized linear models</h1>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>The generalized linear model<a id="id235" class="indexterm"/>&#13;
 is a group of models that try to find the <span class="emphasis">&#13;
<em>M</em>&#13;
</span>&#13;
 parameters <span class="inlinemediaobject"><img src="Image00146.jpg" alt="Generalized linear models"/>&#13;
</span>&#13;
 that form a linear relationship between the labels <span class="emphasis">&#13;
<em>y<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 and the feature vector <span class="emphasis">&#13;
<em>x<sup>(i)</sup>&#13;
</em>&#13;
</span>&#13;
 that is as follows:</p>&#13;
<div class="mediaobject"><img src="Image00147.jpg" alt="Generalized linear models"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Here, <span class="inlinemediaobject"><img src="Image00148.jpg" alt="Generalized linear models"/>&#13;
</span>&#13;
 are the errors of the model. The algorithm for finding the parameters tries to minimize the total error of the model defined by the cost function <span class="emphasis">&#13;
<em>J</em>&#13;
</span>&#13;
 :</p>&#13;
<div class="mediaobject"><img src="Image00149.jpg" alt="Generalized linear models"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>The minimization <a id="id236" class="indexterm"/>&#13;
 of <span class="emphasis">&#13;
<em>J</em>&#13;
</span>&#13;
 is achieved using an iterative algorithm called <span class="strong">&#13;
<strong>batch gradient descent</strong>&#13;
</span>&#13;
 :</p>&#13;
<div class="mediaobject"><img src="Image00150.jpg" alt="Generalized linear models"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Here, α is called learning rate, and it is a trade-off between convergence speed and convergence precision. An alternative algorithm that is called <span class="strong">&#13;
<strong>stochastic gradient descent</strong>&#13;
</span>&#13;
 , that<a id="id237" class="indexterm"/>&#13;
 is loop for <span class="inlinemediaobject"><img src="Image00151.jpg" alt="Generalized linear models"/>&#13;
</span>&#13;
 :</p>&#13;
<div class="mediaobject"><img src="Image00152.jpg" alt="Generalized linear models"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>The θ<sub>j</sub>&#13;
 is updated <a id="id238" class="indexterm"/>&#13;
 for each training example <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 instead of waiting to sum over the entire training set. The last algorithm converges near the minimum of <span class="emphasis">&#13;
<em>J</em>&#13;
</span>&#13;
 , typically faster than batch gradient descent, but the final solution may oscillate around the real values of the parameters. The following paragraphs describe the most common model <span class="inlinemediaobject"><img src="Image00153.jpg" alt="Generalized linear models"/>&#13;
</span>&#13;
 and the corresponding cost function, <span class="emphasis">&#13;
<em>J</em>&#13;
</span>&#13;
 .</p>&#13;
<div class="section" title="Linear regression">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h2 class="title"><a id="ch03lvl3sec15"/>&#13;
 Linear regression</h2>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>Linear regression<a id="id239" class="indexterm"/>&#13;
 is the simplest algorithm and is based on the <a id="id240" class="indexterm"/>&#13;
 model:</p>&#13;
<div class="mediaobject"><img src="Image00154.jpg" alt="Linear regression"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>The cost function and update rule are:</p>&#13;
<div class="mediaobject"><img src="Image00155.jpg" alt="Linear regression"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
</div>&#13;
<div class="section" title="Ridge regression">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h2 class="title"><a id="ch03lvl3sec16"/>&#13;
 Ridge regression</h2>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>Ridge regression, also<a id="id241" class="indexterm"/>&#13;
 known as <span class="strong">&#13;
<strong>Tikhonov regularization</strong>&#13;
</span>&#13;
 , adds <a id="id242" class="indexterm"/>&#13;
 a term to the cost<a id="id243" class="indexterm"/>&#13;
 function <span class="emphasis">&#13;
<em>J</em>&#13;
</span>&#13;
 such that:</p>&#13;
<div class="mediaobject"><img src="Image00156.jpg" alt="Ridge regression"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>&#13;
<span class="inlinemediaobject"><img src="Image00157.jpg" alt="Ridge regression"/>&#13;
</span>&#13;
 , where λ is the regularization parameter. The additional term has the function needed to<a id="id244" class="indexterm"/>&#13;
 prefer a certain set of parameters over all the possible solutions <a id="id245" class="indexterm"/>&#13;
 penalizing all the parameters θ<sub>j</sub>&#13;
 different from <span class="emphasis">&#13;
<em>0</em>&#13;
</span>&#13;
 . The final set of θ<sub>j</sub>&#13;
 <span class="emphasis">&#13;
<em>shrank</em>&#13;
</span>&#13;
 around <span class="emphasis">&#13;
<em>0</em>&#13;
</span>&#13;
 , lowering the variance of the parameters but introducing a bias error. Indicating with the superscript <span class="emphasis">&#13;
<em>l</em>&#13;
</span>&#13;
 the parameters from the linear regression, the ridge regression parameters are related by the following formula:</p>&#13;
<div class="mediaobject"><img src="Image00158.jpg" alt="Ridge regression"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>This clearly shows that the larger the λ value, the more the ridge parameters are shrunk around <span class="emphasis">&#13;
<em>0</em>&#13;
</span>&#13;
 .</p>&#13;
</div>&#13;
<div class="section" title="Lasso regression">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h2 class="title"><a id="ch03lvl3sec17"/>&#13;
 Lasso regression</h2>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>Lasso regression is<a id="id246" class="indexterm"/>&#13;
 an algorithm similar to ridge regression, the <a id="id247" class="indexterm"/>&#13;
 only difference being that the regularization term is the sum of the absolute values of the parameters:</p>&#13;
<div class="mediaobject"><img src="Image00159.jpg" alt="Lasso regression"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<div class="mediaobject"><img src="Image00157.jpg" alt="Lasso regression"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
</div>&#13;
<div class="section" title="Logistic regression">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h2 class="title"><a id="ch03lvl3sec18"/>&#13;
 Logistic regression</h2>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>Despite the<a id="id248" class="indexterm"/>&#13;
 name, this algorithm is used for (binary) classification<a id="id249" class="indexterm"/>&#13;
 problems, so we define the labels<span class="inlinemediaobject"><img src="Image00160.jpg" alt="Logistic regression"/>&#13;
</span>&#13;
 . The model is given the so-called logistic function expressed by:</p>&#13;
<div class="mediaobject"><img src="Image00161.jpg" alt="Logistic regression"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>In this case, the cost function is defined as follows:</p>&#13;
<div class="mediaobject"><img src="Image00162.jpg" alt="Logistic regression"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>From this, the update rule is formally the same as linear regression (but the model definition, <span class="inlinemediaobject"><img src="Image00163.jpg" alt="Logistic regression"/>&#13;
</span>&#13;
 , is different):</p>&#13;
<div class="mediaobject"><img src="Image00164.jpg" alt="Logistic regression"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Note that the prediction for a point <span class="emphasis">&#13;
<em>p</em>&#13;
</span>&#13;
 , <span class="inlinemediaobject"><img src="Image00165.jpg" alt="Logistic regression"/>&#13;
</span>&#13;
 , is a continuous value between <span class="emphasis">&#13;
<em>0</em>&#13;
</span>&#13;
 and <span class="emphasis">&#13;
<em>1</em>&#13;
</span>&#13;
 . So usually, to estimate the class label, we have a threshold at <span class="inlinemediaobject"><img src="Image00165.jpg" alt="Logistic regression"/>&#13;
</span>&#13;
 =0.5 such that:</p>&#13;
<div class="mediaobject"><img src="Image00166.jpg" alt="Logistic regression"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>The logistic regression algorithm is applicable to multiple label problems using the techniques one <a id="id250" class="indexterm"/>&#13;
 versus all or one versus one. Using the first method, a<a id="id251" class="indexterm"/>&#13;
 problem with <span class="emphasis">&#13;
<em>K</em>&#13;
</span>&#13;
 classes is solved by training <span class="emphasis">&#13;
<em>K</em>&#13;
</span>&#13;
 logistic regression models, each one assuming the labels of the considered class <span class="emphasis">&#13;
<em>j</em>&#13;
</span>&#13;
 as <span class="emphasis">&#13;
<em>+1</em>&#13;
</span>&#13;
 and all the rest as <span class="emphasis">&#13;
<em>0</em>&#13;
</span>&#13;
 . The second approach consists of training a model for each pair of labels (<span class="inlinemediaobject"><img src="Image00167.jpg" alt="Logistic regression"/>&#13;
</span>&#13;
 trained models).</p>&#13;
</div>&#13;
<div class="section" title="Probabilistic interpretation of generalized linear models">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h2 class="title"><a id="ch03lvl2sec17"/>&#13;
 Probabilistic interpretation of generalized linear models</h2>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>Now that <a id="id252" class="indexterm"/>&#13;
 we have seen the generalized <a id="id253" class="indexterm"/>&#13;
 linear model, let's find the parameters θ<sub>j</sub>&#13;
 that satisfy the relationship:</p>&#13;
<div class="mediaobject"><img src="Image00168.jpg" alt="Probabilistic interpretation of generalized linear models"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>In the case of linear regression, we can assume <span class="inlinemediaobject"><img src="Image00148.jpg" alt="Probabilistic interpretation of generalized linear models"/>&#13;
</span>&#13;
 as normally distributed with mean <span class="emphasis">&#13;
<em>0</em>&#13;
</span>&#13;
 and variance σ<sup>2</sup>&#13;
 such that the probability is <span class="inlinemediaobject"><img src="Image00169.jpg" alt="Probabilistic interpretation of generalized linear models"/>&#13;
</span>&#13;
 equivalent to:</p>&#13;
<div class="mediaobject"><img src="Image00170.jpg" alt="Probabilistic interpretation of generalized linear models"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Therefore, the total likelihood of the system can be expressed as follows:</p>&#13;
<div class="mediaobject"><img src="Image00171.jpg" alt="Probabilistic interpretation of generalized linear models"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>In the case <a id="id254" class="indexterm"/>&#13;
 of the logistic regression <a id="id255" class="indexterm"/>&#13;
 algorithm, we are assuming that the logistic function itself is the probability:</p>&#13;
<div class="mediaobject"><img src="Image00172.jpg" alt="Probabilistic interpretation of generalized linear models"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<div class="mediaobject"><img src="Image00173.jpg" alt="Probabilistic interpretation of generalized linear models"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Then the likelihood can be expressed by:</p>&#13;
<div class="mediaobject"><img src="Image00174.jpg" alt="Probabilistic interpretation of generalized linear models"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>In both cases, it can be shown that maximizing the likelihood is equivalent to minimizing the cost function, so the gradient descent will be the same.</p>&#13;
</div>&#13;
<div class="section" title="k-nearest neighbours (KNN)">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h2 class="title"><a id="ch03lvl2sec18"/>&#13;
 k-nearest neighbours (KNN)</h2>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>This is a <a id="id256" class="indexterm"/>&#13;
 very simple classification (or regression) method in which given a set of feature vectors <span class="inlinemediaobject"><img src="Image00175.jpg" alt="k-nearest neighbours (KNN)"/>&#13;
</span>&#13;
 with corresponding labels <span class="emphasis">&#13;
<em>y<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 , a test point <span class="emphasis">&#13;
<em>x<sup>(t)</sup>&#13;
</em>&#13;
</span>&#13;
 is assigned to the label value with <a id="id257" class="indexterm"/>&#13;
 the majority of the label occurrences in the <span class="emphasis">&#13;
<em>K</em>&#13;
</span>&#13;
 nearest neighbors <span class="inlinemediaobject"><img src="Image00176.jpg" alt="k-nearest neighbours (KNN)"/>&#13;
</span>&#13;
 found, using a distance measure such as the following:</p>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">&#13;
<span class="strong">&#13;
<strong>Euclidean</strong>&#13;
</span>&#13;
 : <span class="inlinemediaobject"><img src="Image00177.jpg" alt="k-nearest neighbours (KNN)"/>&#13;
</span>&#13;
</li>&#13;
<li class="listitem">&#13;
<span class="strong">&#13;
<strong>Manhattan</strong>&#13;
</span>&#13;
 : <span class="inlinemediaobject"><img src="Image00178.jpg" alt="k-nearest neighbours (KNN)"/>&#13;
</span>&#13;
</li>&#13;
<li class="listitem">&#13;
<span class="strong">&#13;
<strong>Minkowski</strong>&#13;
</span>&#13;
 : <span class="inlinemediaobject"><img src="Image00179.jpg" alt="k-nearest neighbours (KNN)"/>&#13;
</span>&#13;
 (if <span class="emphasis">&#13;
<em>q=2</em>&#13;
</span>&#13;
 , this reduces to the Euclidean distance)</li>&#13;
</ul>&#13;
</div>&#13;
<p>In the case of regression, the value <span class="emphasis">&#13;
<em>y<sub>t</sub>&#13;
</em>&#13;
</span>&#13;
 is calculated by replacing the majority of occurrences by the average of the labels <span class="inlinemediaobject"><img src="Image00180.jpg" alt="k-nearest neighbours (KNN)"/>&#13;
</span>&#13;
 <span class="inlinemediaobject"><img src="Image00181.jpg" alt="k-nearest neighbours (KNN)"/>&#13;
</span>&#13;
 . The simplest average (or the majority of occurrences) has uniform weights, so each point has the same importance regardless of their actual distance from x<span class="emphasis">&#13;
<em>&#13;
<sup>(t)</sup>&#13;
</em>&#13;
</span>&#13;
 . However, a weighted average with weights equal to the inverse distance from <span class="emphasis">&#13;
<em>x<sup>(t)</sup>&#13;
</em>&#13;
</span>&#13;
 may be used.</p>&#13;
</div>&#13;
</div>&#13;

<div class="section" title="Naive Bayes">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h1 class="title"><a id="ch03lvl1sec19"/>&#13;
 Naive Bayes</h1>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>&#13;
<span class="strong">&#13;
<strong>Naive Bayes</strong>&#13;
</span>&#13;
 is <a id="id258" class="indexterm"/>&#13;
 a classification algorithm based on Bayes' probability theorem and conditional independence hypothesis on the features. Given a set of <span class="emphasis">&#13;
<em>m</em>&#13;
</span>&#13;
 features, <span class="inlinemediaobject"><img src="Image00182.jpg" alt="Naive Bayes"/>&#13;
</span>&#13;
 , and a set of labels (classes) <span class="inlinemediaobject"><img src="Image00183.jpg" alt="Naive Bayes"/>&#13;
</span>&#13;
 , the probability of having<a id="id259" class="indexterm"/>&#13;
 label <span class="emphasis">&#13;
<em>c</em>&#13;
</span>&#13;
 (also given the feature set <span class="emphasis">&#13;
<em>x<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 ) is expressed by Bayes' theorem:</p>&#13;
<div class="mediaobject"><img src="Image00184.jpg" alt="Naive Bayes"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Here:</p>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">&#13;
<span class="inlinemediaobject"><img src="Image00185.jpg" alt="Naive Bayes"/>&#13;
</span>&#13;
 is called the likelihood distribution</li>&#13;
<li class="listitem">&#13;
<span class="inlinemediaobject"><img src="Image00186.jpg" alt="Naive Bayes"/>&#13;
</span>&#13;
 is the posteriori distribution</li>&#13;
<li class="listitem">&#13;
<span class="inlinemediaobject"><img src="Image00187.jpg" alt="Naive Bayes"/>&#13;
</span>&#13;
 is the prior distribution</li>&#13;
<li class="listitem">&#13;
<span class="inlinemediaobject"><img src="Image00188.jpg" alt="Naive Bayes"/>&#13;
</span>&#13;
 is called the evidence</li>&#13;
</ul>&#13;
</div>&#13;
<p>The predicted class associated with the set of features <span class="inlinemediaobject"><img src="Image00182.jpg" alt="Naive Bayes"/>&#13;
</span>&#13;
 will be the value <span class="emphasis">&#13;
<em>p</em>&#13;
</span>&#13;
 such that the probability is maximized:</p>&#13;
<div class="mediaobject"><img src="Image00189.jpg" alt="Naive Bayes"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>However, the equation cannot be computed. So, an assumption is needed.</p>&#13;
<p>Using the rule on conditional probability <span class="inlinemediaobject"><img src="Image00190.jpg" alt="Naive Bayes"/>&#13;
</span>&#13;
 , we can write the numerator of the<a id="id260" class="indexterm"/>&#13;
 previous formula as follows:</p>&#13;
<div class="mediaobject"><img src="Image00191.jpg" alt="Naive Bayes"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<div class="mediaobject"><img src="Image00192.jpg" alt="Naive Bayes"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<div class="mediaobject"><img src="Image00193.jpg" alt="Naive Bayes"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>We now use the assumption that each feature <span class="emphasis">&#13;
<em>x<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 is conditionally independent given <span class="emphasis">&#13;
<em>c</em>&#13;
</span>&#13;
 (for example, to calculate the probability of <span class="emphasis">&#13;
<em>x<sub>1</sub>&#13;
</em>&#13;
</span>&#13;
 given <span class="emphasis">&#13;
<em>c</em>&#13;
</span>&#13;
 , the knowledge of the label <span class="emphasis">&#13;
<em>c</em>&#13;
</span>&#13;
 makes the knowledge of the other feature <span class="emphasis">&#13;
<em>x<sub>0</sub>&#13;
</em>&#13;
</span>&#13;
 redundant, <span class="inlinemediaobject"><img src="Image00194.jpg" alt="Naive Bayes"/>&#13;
</span>&#13;
 ):</p>&#13;
<div class="mediaobject"><img src="Image00195.jpg" alt="Naive Bayes"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Under this assumption, the probability of having label <span class="emphasis">&#13;
<em>c</em>&#13;
</span>&#13;
 is then equal to:</p>&#13;
<p>&#13;
<span class="inlinemediaobject"><img src="Image00196.jpg" alt="Naive Bayes"/>&#13;
</span>&#13;
 ––––––––(1)</p>&#13;
<p>Here, the <span class="emphasis">&#13;
<em>+1</em>&#13;
</span>&#13;
 in the numerator and the <span class="emphasis">&#13;
<em>M</em>&#13;
</span>&#13;
 in the denominator are constants, useful for avoiding<a id="id261" class="indexterm"/>&#13;
 the <span class="emphasis">&#13;
<em>0/0</em>&#13;
</span>&#13;
 situation (<span class="strong">&#13;
<strong>Laplace smoothing</strong>&#13;
</span>&#13;
 ).</p>&#13;
<p>Due to the fact<a id="id262" class="indexterm"/>&#13;
 that the denominator of (<span class="strong">&#13;
<strong>1</strong>&#13;
</span>&#13;
 ) does not depend on the labels (it is summed over all possible labels), the final predicted label <span class="emphasis">&#13;
<em>p</em>&#13;
</span>&#13;
 is obtained by finding the maximum of the numerator of (<span class="strong">&#13;
<strong>1</strong>&#13;
</span>&#13;
 ):</p>&#13;
<p>&#13;
<span class="inlinemediaobject"><img src="Image00197.jpg" alt="Naive Bayes"/>&#13;
</span>&#13;
 ––––––––(2)</p>&#13;
<p>Given the usual training set <span class="inlinemediaobject"><img src="Image00198.jpg" alt="Naive Bayes"/>&#13;
</span>&#13;
 , where <span class="inlinemediaobject"><img src="Image00199.jpg" alt="Naive Bayes"/>&#13;
</span>&#13;
 (<span class="emphasis">&#13;
<em>M</em>&#13;
</span>&#13;
 features) corresponding to the labels set <span class="inlinemediaobject"><img src="Image00200.jpg" alt="Naive Bayes"/>&#13;
</span>&#13;
 , the probability <span class="emphasis">&#13;
<em>P(y=c)</em>&#13;
</span>&#13;
 is simply calculated in frequency terms as the number of training examples associated with the class <span class="emphasis">&#13;
<em>c</em>&#13;
</span>&#13;
 over the total number of examples, <span class="inlinemediaobject"><img src="Image00201.jpg" alt="Naive Bayes"/>&#13;
</span>&#13;
 . The conditional probabilities <span class="inlinemediaobject"><img src="Image00202.jpg" alt="Naive Bayes"/>&#13;
</span>&#13;
 instead are evaluated by following a distribution. We are going to discuss<a id="id263" class="indexterm"/>&#13;
 two models, <span class="strong">&#13;
<strong>Multinomial Naive Bayes</strong>&#13;
</span>&#13;
 and <a id="id264" class="indexterm"/>&#13;
 <span class="strong">&#13;
<strong>Gaussian Naive Bayes</strong>&#13;
</span>&#13;
 .</p>&#13;
<div class="section" title="Multinomial Naive Bayes">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h2 class="title"><a id="ch03lvl2sec19"/>&#13;
 Multinomial Naive Bayes</h2>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>Let's assume we <a id="id265" class="indexterm"/>&#13;
 want to determine whether an e-mail <span class="emphasis">&#13;
<em>s</em>&#13;
</span>&#13;
 given by a set of word occurrences <span class="inlinemediaobject"><img src="Image00203.jpg" alt="Multinomial Naive Bayes"/>&#13;
</span>&#13;
 is spam <span class="emphasis">&#13;
<em>(1)</em>&#13;
</span>&#13;
 or not <span class="emphasis">&#13;
<em>(0)</em>&#13;
</span>&#13;
 so that <span class="inlinemediaobject"><img src="Image00204.jpg" alt="Multinomial Naive Bayes"/>&#13;
</span>&#13;
 . <span class="emphasis">&#13;
<em>M</em>&#13;
</span>&#13;
 is the size of the vocabulary (number of features). There are <span class="inlinemediaobject"><img src="Image00205.jpg" alt="Multinomial Naive Bayes"/>&#13;
</span>&#13;
 words and <span class="emphasis">&#13;
<em>N</em>&#13;
</span>&#13;
 training examples (e-mails).</p>&#13;
<p>Each email <span class="emphasis">&#13;
<em>x<sup>(i)</sup>&#13;
</em>&#13;
</span>&#13;
 with label <span class="emphasis">&#13;
<em>y<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 such that <span class="inlinemediaobject"><img src="Image00206.jpg" alt="Multinomial Naive Bayes"/>&#13;
</span>&#13;
 is the number of times the word <span class="emphasis">&#13;
<em>j</em>&#13;
</span>&#13;
 in the <a id="id266" class="indexterm"/>&#13;
 vocabulary occurs in the training example <span class="emphasis">&#13;
<em>l</em>&#13;
</span>&#13;
 . For<a id="id267" class="indexterm"/>&#13;
 example, <span class="inlinemediaobject"><img src="Image00207.jpg" alt="Multinomial Naive Bayes"/>&#13;
</span>&#13;
 represents the number of times the word <span class="emphasis">&#13;
<em>1</em>&#13;
</span>&#13;
 , or <span class="emphasis">&#13;
<em>w<sub>1</sub>&#13;
</em>&#13;
</span>&#13;
 , occurs in the third e-mail. In this case, multinomial distribution on the likelihood is applied:</p>&#13;
<div class="mediaobject"><img src="Image00208.jpg" alt="Multinomial Naive Bayes"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Here, the normalization constants in the front can be discarded because they do not depend on the label <span class="emphasis">&#13;
<em>y</em>&#13;
</span>&#13;
 , and so the <span class="emphasis">&#13;
<em>arg max</em>&#13;
</span>&#13;
 operator will not be affected. The important part is the evaluation of the single word <span class="emphasis">&#13;
<em>w<sub>j</sub>&#13;
</em>&#13;
</span>&#13;
 : probability over the training set:</p>&#13;
<div class="mediaobject"><img src="Image00209.jpg" alt="Multinomial Naive Bayes"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Here <span class="emphasis">&#13;
<em>N<sub>iy</sub>&#13;
</em>&#13;
</span>&#13;
 is the number of times the word <span class="emphasis">&#13;
<em>j</em>&#13;
</span>&#13;
 occurs, that is associated with label <span class="emphasis">&#13;
<em>y</em>&#13;
</span>&#13;
 , and <span class="emphasis">&#13;
<em>N<sub>y</sub>&#13;
</em>&#13;
</span>&#13;
 is the portion of the training set with label <span class="emphasis">&#13;
<em>y</em>&#13;
</span>&#13;
 .</p>&#13;
<p>This is the analogue of <span class="inlinemediaobject"><img src="Image00202.jpg" alt="Multinomial Naive Bayes"/>&#13;
</span>&#13;
 ,<span class="inlinemediaobject"><img src="Image00210.jpg" alt="Multinomial Naive Bayes"/>&#13;
</span>&#13;
 on equation (<span class="strong">&#13;
<strong>1</strong>&#13;
</span>&#13;
 ) and the multinomial distribution likelihood. Due to the exponent on the probability, usually the logarithm is applied to <a id="id268" class="indexterm"/>&#13;
 compute<a id="id269" class="indexterm"/>&#13;
 the final algorithm <span class="emphasis">&#13;
<em>(2)</em>&#13;
</span>&#13;
 :</p>&#13;
<div class="mediaobject"><img src="Image00211.jpg" alt="Multinomial Naive Bayes"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
</div>&#13;
<div class="section" title="Gaussian Naive Bayes">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h2 class="title"><a id="ch03lvl2sec20"/>&#13;
 Gaussian Naive Bayes</h2>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>If the features <a id="id270" class="indexterm"/>&#13;
 vectors <span class="emphasis">&#13;
<em>x<sup>(i)</sup>&#13;
</em>&#13;
</span>&#13;
 have continuous values, this<a id="id271" class="indexterm"/>&#13;
 method can be applied. For example, we want to classify images in <span class="emphasis">&#13;
<em>K</em>&#13;
</span>&#13;
 classes, each feature <span class="emphasis">&#13;
<em>j</em>&#13;
</span>&#13;
 is a pixel, and <span class="emphasis">&#13;
<em>x<sub>j</sub>&#13;
 <sup>(i)</sup>&#13;
</em>&#13;
</span>&#13;
 is the <span class="emphasis">&#13;
<em>j-th</em>&#13;
</span>&#13;
 pixel of the <span class="emphasis">&#13;
<em>i-th</em>&#13;
</span>&#13;
 image in the training set with <span class="emphasis">&#13;
<em>N</em>&#13;
</span>&#13;
 images and labels <span class="inlinemediaobject"><img src="Image00200.jpg" alt="Gaussian Naive Bayes"/>&#13;
</span>&#13;
 . Given an unlabeled image represented by the pixels <span class="inlinemediaobject"><img src="Image00212.jpg" alt="Gaussian Naive Bayes"/>&#13;
</span>&#13;
 , in this case, <span class="inlinemediaobject"><img src="Image00202.jpg" alt="Gaussian Naive Bayes"/>&#13;
</span>&#13;
 in equation (<span class="strong">&#13;
<strong>1</strong>&#13;
</span>&#13;
 ) becomes:</p>&#13;
<div class="mediaobject"><img src="Image00213.jpg" alt="Gaussian Naive Bayes"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Here:</p>&#13;
<div class="mediaobject"><img src="Image00214.jpg" alt="Gaussian Naive Bayes"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>And:</p>&#13;
<div class="mediaobject"><img src="Image00215.jpg" alt="Gaussian Naive Bayes"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
</div>&#13;
</div>&#13;

<div class="section" title="Decision trees">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h1 class="title"><a id="ch03lvl1sec20"/>&#13;
 Decision trees</h1>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>This class of<a id="id272" class="indexterm"/>&#13;
 algorithms aims to predict the unknown labels splitting the dataset, by generating a set of simple rules that are learnt from the features values. For example, consider a case of deciding whether to take an umbrella today or not based on the values of humidity, wind, temperature, and pressure. This is a classification problem, and an example of the decision tree can be like what is shown in the following figure based on data of 100 days. Here is a sample table:</p>&#13;
<div class="informaltable">&#13;
<table border="1">&#13;
<colgroup><col/>&#13;
<col/>&#13;
<col/>&#13;
<col/>&#13;
<col/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr>&#13;
<th valign="bottom">&#13;
<p>Humidity (%)</p>&#13;
</th>&#13;
<th valign="bottom">&#13;
<p>Pressure (mbar)</p>&#13;
</th>&#13;
<th valign="bottom">&#13;
<p>Wind (Km/h)</p>&#13;
</th>&#13;
<th valign="bottom">&#13;
<p>Temperature (C)</p>&#13;
</th>&#13;
<th valign="bottom">&#13;
<p>Umbrella</p>&#13;
</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td valign="top">&#13;
<p>56</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>1,021</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>5</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>21</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>Yes</p>&#13;
</td>&#13;
</tr>&#13;
<tr>&#13;
<td valign="top">&#13;
<p>65</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>1,018</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>3</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>18</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>No</p>&#13;
</td>&#13;
</tr>&#13;
<tr>&#13;
<td valign="top">&#13;
<p>80</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>1,020</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>10</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>17</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>No</p>&#13;
</td>&#13;
</tr>&#13;
<tr>&#13;
<td valign="top">&#13;
<p>81</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>1,015</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>11</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>20</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>Yes</p>&#13;
</td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
</div>&#13;
<div class="mediaobject"><img src="Image00216.jpg" alt="Decision trees"/>&#13;
<div class="caption">&#13;
<p>Decision tree for predicting whether to bring an umbrella or not based on a record of 100 days.</p>&#13;
</div>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>In the preceding<a id="id273" class="indexterm"/>&#13;
 figure the numbers in squares represent the days on which an umbrella has been brought, while the circled numbers indicate days in which an umbrella was not necessary.</p>&#13;
<p>The decision tree presents two types of nodes: decision nodes, which have two (or more) branches when a decision split is applied; and leaf nodes, when data is classified. The stopping criterion is usually a maximum number of decision nodes (depth of the tree) or a minimum of data points to continue to split (typically around 2 to 5). The problem of decision trees learning is to build the <span class="emphasis">&#13;
<em>best</em>&#13;
</span>&#13;
 tree out of all the possible node combinations, that is, estimate the hierarchy of the rules to be applied (in other words, whether the first decision node should be on humidity or on temperature, and so on). More formally, given a training set of <span class="inlinemediaobject"><img src="Image00217.jpg" alt="Decision trees"/>&#13;
</span>&#13;
 with <span class="emphasis">&#13;
<em>x<sup>(i)</sup>&#13;
</em>&#13;
</span>&#13;
 in <span class="emphasis">&#13;
<em>R<sup>m</sup>&#13;
</em>&#13;
</span>&#13;
 and corresponding labels <span class="emphasis">&#13;
<em>y<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 , we need to find the best rule to partition the data <span class="emphasis">&#13;
<em>S</em>&#13;
</span>&#13;
 at node <span class="emphasis">&#13;
<em>k</em>&#13;
</span>&#13;
 . If the chosen feature, <span class="emphasis">&#13;
<em>j</em>&#13;
</span>&#13;
 , is continuous, each split rule is given by a feature <sub>&#13;
<span class="emphasis">&#13;
<em>j</em>&#13;
</span>&#13;
</sub>&#13;
 and a threshold <span class="emphasis">&#13;
<em>t<sup>j</sup>&#13;
 <sub>k</sub>&#13;
</em>&#13;
</span>&#13;
 that splits <span class="emphasis">&#13;
<em>S</em>&#13;
</span>&#13;
 in <span class="inlinemediaobject"><img src="Image00218.jpg" alt="Decision trees"/>&#13;
</span>&#13;
 for <span class="inlinemediaobject"><img src="Image00219.jpg" alt="Decision trees"/>&#13;
</span>&#13;
 and <span class="inlinemediaobject"><img src="Image00220.jpg" alt="Decision trees"/>&#13;
</span>&#13;
 for <span class="inlinemediaobject"><img src="Image00221.jpg" alt="Decision trees"/>&#13;
</span>&#13;
 , <span class="inlinemediaobject"><img src="Image00222.jpg" alt="Decision trees"/>&#13;
</span>&#13;
 . The best split rule <span class="inlinemediaobject"><img src="Image00223.jpg" alt="Decision trees"/>&#13;
</span>&#13;
 for the node <span class="emphasis">&#13;
<em>k</em>&#13;
</span>&#13;
 is associated with the minimum of the impurity <span class="emphasis">&#13;
<em>I</em>&#13;
</span>&#13;
 function that measures how much the rule is able to separate the data into partitions with different labels (that is, each branch will contain the minimum amount of label mixing):</p>&#13;
<div class="mediaobject"><img src="Image00224.jpg" alt="Decision trees"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<div class="mediaobject"><img src="Image00225.jpg" alt="Decision trees"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Here, <span class="inlinemediaobject"><img src="Image00226.jpg" alt="Decision trees"/>&#13;
</span>&#13;
 are<a id="id274" class="indexterm"/>&#13;
 the numbers of data points on the left and right branches, respectively. <span class="emphasis">&#13;
<em>N<sub>k</sub>&#13;
</em>&#13;
</span>&#13;
 is the number of data points on node <span class="emphasis">&#13;
<em>k</em>&#13;
</span>&#13;
 , and <span class="emphasis">&#13;
<em>H</em>&#13;
</span>&#13;
 is a measure that can assume different expressions using the probability of each target value <span class="emphasis">&#13;
<em>l</em>&#13;
</span>&#13;
 at branch <span class="emphasis">&#13;
<em>b</em>&#13;
</span>&#13;
 (<span class="emphasis">&#13;
<em>b</em>&#13;
</span>&#13;
 can be left or right), <span class="inlinemediaobject"><img src="Image00227.jpg" alt="Decision trees"/>&#13;
</span>&#13;
 :</p>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">Entropy of the branch: <span class="inlinemediaobject"><img src="Image00228.jpg" alt="Decision trees"/>&#13;
</span>&#13;
</li>&#13;
<li class="listitem">Gini impurity of the branch: <span class="inlinemediaobject"><img src="Image00229.jpg" alt="Decision trees"/>&#13;
</span>&#13;
</li>&#13;
<li class="listitem">Misclassification: <span class="inlinemediaobject"><img src="Image00230.jpg" alt="Decision trees"/>&#13;
</span>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">Mean squared error (variance): <span class="inlinemediaobject"><img src="Image00231.jpg" alt="Decision trees"/>&#13;
</span>&#13;
 (where <span class="inlinemediaobject"><img src="Image00232.jpg" alt="Decision trees"/>&#13;
</span>&#13;
 )</li>&#13;
</ul>&#13;
</div>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
<p>Note that the <a id="id275" class="indexterm"/>&#13;
 latter is typically used in regression problems while the others are employed in classification. Note also that usually in literature, the <span class="emphasis">&#13;
<em>information gain</em>&#13;
</span>&#13;
 definition is introduced as the difference between <span class="emphasis">&#13;
<em>H</em>&#13;
</span>&#13;
 at node <span class="emphasis">&#13;
<em>k</em>&#13;
</span>&#13;
 and <span class="inlinemediaobject"><img src="Image00233.jpg" alt="Decision trees"/>&#13;
</span>&#13;
</p>&#13;
<p>&#13;
<span class="inlinemediaobject"><img src="Image00234.jpg" alt="Decision trees"/>&#13;
</span>&#13;
 where <span class="inlinemediaobject"><img src="Image00235.jpg" alt="Decision trees"/>&#13;
</span>&#13;
</p>&#13;
<p>If the feature <span class="emphasis">&#13;
<em>j</em>&#13;
</span>&#13;
 is discrete with <span class="emphasis">&#13;
<em>d</em>&#13;
</span>&#13;
 number of possible values, there is no binary threshold <span class="emphasis">&#13;
<em>t<sup>j</sup>&#13;
 <sub>k</sub>&#13;
</em>&#13;
</span>&#13;
 to calculate and the data is split into <span class="emphasis">&#13;
<em>d</em>&#13;
</span>&#13;
 partitions. The measure <span class="emphasis">&#13;
<em>H</em>&#13;
</span>&#13;
 is calculated over <span class="emphasis">&#13;
<em>d</em>&#13;
</span>&#13;
 subsets.</p>&#13;
<p>For example, we can determine the rule for the first node (<span class="emphasis">&#13;
<em>k=0</em>&#13;
</span>&#13;
 ) for the preceding example using the entropy as the impurity measure <span class="emphasis">&#13;
<em>H</em>&#13;
</span>&#13;
 .</p>&#13;
<p>All the features are continuous, so the values of <span class="emphasis">&#13;
<em>t<sup>j</sup>&#13;
 <sub>0</sub>&#13;
</em>&#13;
</span>&#13;
 are needed. Assuming that <span class="emphasis">&#13;
<em>j=0</em>&#13;
</span>&#13;
 is the humidity and sorting in increasing order, the possible humidity values in the dataset we have are as follows:</p>&#13;
<div class="informaltable">&#13;
<table border="1">&#13;
<colgroup><col/>&#13;
<col/>&#13;
<col/>&#13;
<col/>&#13;
<col/>&#13;
<col/>&#13;
<col/>&#13;
<col/>&#13;
<col/>&#13;
<col/>&#13;
<col/>&#13;
</colgroup>&#13;
<tbody>&#13;
<tr>&#13;
<td valign="top">&#13;
<p>h</p>&#13;
</td>&#13;
<td colspan="2" valign="top" style="text-align: center">&#13;
<p>0</p>&#13;
</td>&#13;
<td colspan="2" valign="top" style="text-align: center">&#13;
<p>1</p>&#13;
</td>&#13;
<td colspan="2" valign="top" style="text-align: center">&#13;
<p>….</p>&#13;
</td>&#13;
<td colspan="2" valign="top" style="text-align: center">&#13;
<p>98</p>&#13;
</td>&#13;
<td colspan="2" valign="top" style="text-align: center">&#13;
<p>99</p>&#13;
</td>&#13;
</tr>&#13;
<tr>&#13;
<td valign="top">&#13;
<p>umbrella</p>&#13;
</td>&#13;
<td colspan="2" valign="top" style="text-align: center">&#13;
<p>yes</p>&#13;
</td>&#13;
<td colspan="2" valign="top" style="text-align: center">&#13;
<p>no</p>&#13;
</td>&#13;
<td colspan="2" valign="top" style="text-align: center">&#13;
<p>….</p>&#13;
</td>&#13;
<td colspan="2" valign="top" style="text-align: center">&#13;
<p>no</p>&#13;
</td>&#13;
<td colspan="2" valign="top" style="text-align: center">&#13;
<p>no</p>&#13;
</td>&#13;
</tr>&#13;
<tr>&#13;
<td valign="top">&#13;
<p>humidity</p>&#13;
</td>&#13;
<td colspan="2" valign="top" style="text-align: center">&#13;
<p>&#13;
<span class="strong">&#13;
<strong>58</strong>&#13;
</span>&#13;
</p>&#13;
</td>&#13;
<td colspan="2" valign="top" style="text-align: center">&#13;
<p>62</p>&#13;
</td>&#13;
<td colspan="2" valign="top" style="text-align: center">&#13;
<p>….</p>&#13;
</td>&#13;
<td colspan="2" valign="top" style="text-align: center">&#13;
<p>88</p>&#13;
</td>&#13;
<td colspan="2" valign="top" style="text-align: center">&#13;
<p>89</p>&#13;
</td>&#13;
</tr>&#13;
<tr>&#13;
<td valign="top"> </td>&#13;
<td valign="top">&#13;
<p>&lt;</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>&gt;=</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>&lt;</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>&gt;=</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>&lt;</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>&gt;=</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>&lt;</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>&gt;=</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>&lt;</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>&gt;=</p>&#13;
</td>&#13;
</tr>&#13;
<tr>&#13;
<td valign="top">&#13;
<p>yes</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>0</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>11</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>14</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>32</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>7</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>20</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>29</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>12</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>78</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>0</p>&#13;
</td>&#13;
</tr>&#13;
<tr>&#13;
<td valign="top">&#13;
<p>no</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>0</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>89</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>21</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>33</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>13</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>60</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>10</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>49</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>22</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>0</p>&#13;
</td>&#13;
</tr>&#13;
<tr>&#13;
<td valign="top">&#13;
<div class="mediaobject"><img src="Image00236.jpg" alt="Decision trees"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
</td>&#13;
<td colspan="2" valign="top" style="text-align: center">&#13;
<p>&#13;
<span class="strong">&#13;
<strong>0.5</strong>&#13;
</span>&#13;
</p>&#13;
</td>&#13;
<td colspan="2" valign="top" style="text-align: center">&#13;
<p>0.99</p>&#13;
</td>&#13;
<td colspan="2" valign="top" style="text-align: center">&#13;
<p>0.85</p>&#13;
</td>&#13;
<td colspan="2" valign="top" style="text-align: center">&#13;
<p>0.76</p>&#13;
</td>&#13;
<td colspan="2" valign="top" style="text-align: center">&#13;
<p>0.76</p>&#13;
</td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
</div>&#13;
<p>So, the threshold <a id="id276" class="indexterm"/>&#13;
 value for the humidity feature is <span class="inlinemediaobject"><img src="Image00237.jpg" alt="Decision trees"/>&#13;
</span>&#13;
 = 58; and in the same way, we can calculate the threshold values for temperature <span class="emphasis">&#13;
<em>t<sup>1</sup>&#13;
 <sub>0</sub>&#13;
</em>&#13;
</span>&#13;
 , wind <span class="emphasis">&#13;
<em>t<sup>2</sup>&#13;
 <sub>0</sub>&#13;
</em>&#13;
</span>&#13;
 , and pressure <span class="emphasis">&#13;
<em>t<sup>3</sup>&#13;
 <sub>0</sub>&#13;
</em>&#13;
</span>&#13;
 . Now we can record to determine the best rule for the first node, computing the impurity for each of the four features:</p>&#13;
<div class="informaltable">&#13;
<table border="1">&#13;
<colgroup><col/>&#13;
<col/>&#13;
<col/>&#13;
<col/>&#13;
<col/>&#13;
<col/>&#13;
<col/>&#13;
<col/>&#13;
</colgroup>&#13;
<tbody>&#13;
<tr>&#13;
<td rowspan="2" colspan="2" valign="top" style="text-align: center">&#13;
<p>yes</p>&#13;
</td>&#13;
<td colspan="2" valign="top" style="text-align: center">&#13;
<p>umbrella</p>&#13;
</td>&#13;
<td rowspan="2" colspan="2" valign="top" style="text-align: center">&#13;
<p>yes</p>&#13;
<p>no</p>&#13;
</td>&#13;
<td colspan="2" valign="top" style="text-align: center">&#13;
<p>umbrella</p>&#13;
</td>&#13;
</tr>&#13;
<tr>&#13;
<td valign="top">&#13;
<p>no</p>&#13;
</td>&#13;
<td valign="top"> </td>&#13;
<td valign="top"> </td>&#13;
<td valign="top"> </td>&#13;
</tr>&#13;
<tr>&#13;
<td rowspan="2" valign="top">&#13;
<p>Humidity j=0</p>&#13;
</td>&#13;
<td valign="top">&#13;
<div class="mediaobject"><img src="Image00238.jpg" alt="Decision trees"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
</td>&#13;
<td valign="top">&#13;
<p>0</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>0</p>&#13;
</td>&#13;
<td rowspan="2" valign="top">&#13;
<p>Temperature j=1</p>&#13;
</td>&#13;
<td valign="top">&#13;
<div class="mediaobject"><img src="Image00239.jpg" alt="Decision trees"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
</td>&#13;
<td valign="top">&#13;
<p>21</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>32</p>&#13;
</td>&#13;
</tr>&#13;
<tr>&#13;
<td valign="top">&#13;
<div class="mediaobject"><img src="Image00240.jpg" alt="Decision trees"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
</td>&#13;
<td valign="top">&#13;
<p>11</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>89</p>&#13;
</td>&#13;
<td valign="top">&#13;
<div class="mediaobject"><img src="Image00241.jpg" alt="Decision trees"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
</td>&#13;
<td valign="top">&#13;
<p>11</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>36</p>&#13;
</td>&#13;
</tr>&#13;
<tr>&#13;
<td colspan="4" valign="top" style="text-align: center">&#13;
<p>Impurity:</p>&#13;
<div class="mediaobject"><img src="Image00242.jpg" alt="Decision trees"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
</td>&#13;
<td colspan="4" valign="top" style="text-align: center">&#13;
<p>Impurity:</p>&#13;
<div class="mediaobject"><img src="Image00243.jpg" alt="Decision trees"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
</td>&#13;
</tr>&#13;
<tr>&#13;
<td rowspan="2" colspan="2" valign="top" style="text-align: center">&#13;
<p>yes</p>&#13;
</td>&#13;
<td colspan="2" valign="top" style="text-align: center">&#13;
<p>umbrella</p>&#13;
</td>&#13;
<td rowspan="2" colspan="2" valign="top" style="text-align: center">&#13;
<p>yes</p>&#13;
<p>no</p>&#13;
</td>&#13;
<td colspan="2" valign="top" style="text-align: center">&#13;
<p>umbrella</p>&#13;
</td>&#13;
</tr>&#13;
<tr>&#13;
<td valign="top">&#13;
<p>no</p>&#13;
</td>&#13;
<td valign="top"> </td>&#13;
<td valign="top"> </td>&#13;
<td valign="top"> </td>&#13;
</tr>&#13;
<tr>&#13;
<td rowspan="2" valign="top">&#13;
<p>Wind j=2</p>&#13;
</td>&#13;
<td valign="top">&#13;
<div class="mediaobject"><img src="Image00244.jpg" alt="Decision trees"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
</td>&#13;
<td valign="top">&#13;
<p>48</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>5</p>&#13;
</td>&#13;
<td rowspan="2" valign="top">&#13;
<p>Pressure j=3</p>&#13;
</td>&#13;
<td valign="top">&#13;
<div class="mediaobject"><img src="Image00245.jpg" alt="Decision trees"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
</td>&#13;
<td valign="top">&#13;
<p>39</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>3</p>&#13;
</td>&#13;
</tr>&#13;
<tr>&#13;
<td valign="top">&#13;
<div class="mediaobject"><img src="Image00246.jpg" alt="Decision trees"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
</td>&#13;
<td valign="top">&#13;
<p>1</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>46</p>&#13;
</td>&#13;
<td valign="top">&#13;
<div class="mediaobject"><img src="Image00247.jpg" alt="Decision trees"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
</td>&#13;
<td valign="top">&#13;
<p>45</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>13</p>&#13;
</td>&#13;
</tr>&#13;
<tr>&#13;
<td colspan="4" valign="top" style="text-align: center">&#13;
<p>&#13;
<span class="strong">&#13;
<strong>Impurity</strong>&#13;
</span>&#13;
 :</p>&#13;
<div class="mediaobject"><img src="Image00248.jpg" alt="Decision trees"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
</td>&#13;
<td colspan="4" valign="top" style="text-align: center">&#13;
<p>Impurity:</p>&#13;
<div class="mediaobject"><img src="Image00249.jpg" alt="Decision trees"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
</td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
</div>&#13;
<p>Therefore, for<a id="id277" class="indexterm"/>&#13;
 node <span class="emphasis">&#13;
<em>0</em>&#13;
</span>&#13;
 , the best rule is given by:</p>&#13;
<div class="mediaobject"><img src="Image00250.jpg" alt="Decision trees"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>That is, the wind feature with threshold <span class="emphasis">&#13;
<em>t<sup>2</sup>&#13;
 <sub>0</sub>&#13;
</em>&#13;
</span>&#13;
 . We can repeat the same procedure to find the best rule for the following decision nodes until the end of the tree.</p>&#13;
<p>Decision trees learning is able to handle large datasets, though it tends not to generalize well, especially with a large set of features (<span class="emphasis">&#13;
<em>N</em>&#13;
</span>&#13;
 ≈<span class="emphasis">&#13;
<em>M</em>&#13;
</span>&#13;
 ). In such cases, it is advisable to set a small depth of the tree or use some dimensionality reduction techniques. Setting the minimum number of data points to split or the minimum number of data points in a leaf node will also help prevent overfitting. This algorithm may lead to over-complex trees; they can be <span class="emphasis">&#13;
<em>pruned</em>&#13;
</span>&#13;
 to reduce the branches that do not affect the quality of the prediction. Various pruning techniques are available, but they are beyond the scope of this book. Note also that a series of decision trees can be trained at the same time, composing of a so-called <span class="strong">&#13;
<strong>random forest</strong>&#13;
</span>&#13;
 . A<a id="id278" class="indexterm"/>&#13;
 random forest trains each tree with a random sample of the original data points, and a random subset of features is available for each decision node learning. The result is an average of the predictions in a regression problem or the majority in a classification problem.</p>&#13;
</div>&#13;

<h1>读累了记得休息一会哦~</h1>
<p> </p>
<p><strong>公众号：古德猫宁李</strong></p>
<ul>
<li>电子书搜索下载</li>
<li>书单分享</li>
<li>书友学习交流</li>

<p> </p>
</ul>
<p><strong>网站：</strong><a href="https://www.chenjin5.com">沉金书屋 https://www.chenjin5.com</a></p>
<ul>
<li>电子书搜索下载</li>
<li>电子书打包资源分享</li>
<li>学习资源分享</li>

</ul>

<div class="section" title="Support vector machine">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h1 class="title"><a id="ch03lvl1sec21"/>&#13;
 Support vector machine</h1>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>This algorithm, <span class="strong">&#13;
<strong>Support Vector Machine</strong>&#13;
</span>&#13;
 (<span class="strong">&#13;
<strong>SVM</strong>&#13;
</span>&#13;
 ), tries to geometrically separate the dataset <span class="inlinemediaobject"><img src="Image00251.jpg" alt="Support vector machine"/>&#13;
</span>&#13;
 into two subsets labeled with <span class="emphasis">&#13;
<em>y<sub>i</sub>&#13;
 =+1</em>&#13;
</span>&#13;
 and <span class="emphasis">&#13;
<em>y<sub>i</sub>&#13;
 =-1</em>&#13;
</span>&#13;
 . The next figure shows the data <a id="id279" class="indexterm"/>&#13;
 perfectly separated into two classes (empty circles and black circles), that is, the case the data in which the decision boundary (or hyperplane) given by the black line fully separates the two classes (in other words, there are no misclassified data points):</p>&#13;
<div class="mediaobject"><img src="Image00252.jpg" alt="Support vector machine"/>&#13;
<div class="caption">&#13;
<p>Sketch of the dataset separated into two classes (empty and filled circles) by the black line (decision boundary)</p>&#13;
</div>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>The hyperplane is mathematically described by the equation <span class="inlinemediaobject"><img src="Image00253.jpg" alt="Support vector machine"/>&#13;
</span>&#13;
 , where <span class="inlinemediaobject"><img src="Image00254.jpg" alt="Support vector machine"/>&#13;
</span>&#13;
 is the distance of the hyperplane from the origin and <span class="emphasis">&#13;
<em>w</em>&#13;
</span>&#13;
 is the normal to the hyperplane. The goal of the algorithm is to maximize the distance of the decision boundary from the data points. In practice, we consider the closest points <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 to the hyperplane, called support vectors, that lie in two planes <span class="emphasis">&#13;
<em>H<sub>1</sub>&#13;
</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>H<sub>2</sub>&#13;
</em>&#13;
</span>&#13;
 at distances <span class="emphasis">&#13;
<em>d<sub>1</sub>&#13;
</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>d<sub>2</sub>&#13;
</em>&#13;
</span>&#13;
 from the decision boundary such that:</p>&#13;
<p>&#13;
<span class="inlinemediaobject"><img src="Image00255.jpg" alt="Support vector machine"/>&#13;
</span>&#13;
 for <span class="emphasis">&#13;
<em>H<sub>1</sub>&#13;
</em>&#13;
</span>&#13;
 such that <span class="emphasis">&#13;
<em>y<sub>i</sub>&#13;
 =+1</em>&#13;
</span>&#13;
 ––––––––(1)</p>&#13;
<p>&#13;
<span class="inlinemediaobject"><img src="Image00256.jpg" alt="Support vector machine"/>&#13;
</span>&#13;
 for <span class="emphasis">&#13;
<em>H<sub>2</sub>&#13;
</em>&#13;
</span>&#13;
 such that <span class="emphasis">&#13;
<em>y<sub>i</sub>&#13;
 =-1</em>&#13;
</span>&#13;
 ––––––––(2)</p>&#13;
<p>Assuming <span class="emphasis">&#13;
<em>d<sub>1</sub>&#13;
 =d<sub>2</sub>&#13;
</em>&#13;
</span>&#13;
 , the common distance is called margin so that the support vector machine<a id="id280" class="indexterm"/>&#13;
 method finds the values of <span class="emphasis">&#13;
<em>w</em>&#13;
</span>&#13;
 and <span class="emphasis">&#13;
<em>b</em>&#13;
</span>&#13;
 that maximize the margin.</p>&#13;
<p>Since the distance between <span class="emphasis">&#13;
<em>H<sub>1</sub>&#13;
</em>&#13;
</span>&#13;
 and <span class="emphasis">&#13;
<em>H<sub>2</sub>&#13;
</em>&#13;
</span>&#13;
 is given by <span class="inlinemediaobject"><img src="Image00257.jpg" alt="Support vector machine"/>&#13;
</span>&#13;
 , the margin is equal to <span class="inlinemediaobject"><img src="Image00258.jpg" alt="Support vector machine"/>&#13;
</span>&#13;
 and the support vector machine algorithm is equivalent to:</p>&#13;
<p>&#13;
<span class="inlinemediaobject"><img src="Image00259.jpg" alt="Support vector machine"/>&#13;
</span>&#13;
 such that <span class="inlinemediaobject"><img src="Image00260.jpg" alt="Support vector machine"/>&#13;
</span>&#13;
 ,</p>&#13;
<p>Here, the square operation and the factor <span class="inlinemediaobject"><img src="Image00261.jpg" alt="Support vector machine"/>&#13;
</span>&#13;
 have been added to allow the use of a quadratic programming method to solve the mathematical problem. Now, the problem can be rewritten in a Lagrangian form using the Lagrange multipliers a<span class="emphasis">&#13;
<em>&#13;
<sub>i</sub>&#13;
 &gt;0</em>&#13;
</span>&#13;
 :</p>&#13;
<div class="mediaobject"><img src="Image00262.jpg" alt="Support vector machine"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Setting the derivatives with respect to <span class="inlinemediaobject"><img src="Image00263.jpg" alt="Support vector machine"/>&#13;
</span>&#13;
 and <span class="emphasis">&#13;
<em>b</em>&#13;
</span>&#13;
 to <span class="emphasis">&#13;
<em>0</em>&#13;
</span>&#13;
 , we obtain:</p>&#13;
<p>&#13;
<span class="inlinemediaobject"><img src="Image00264.jpg" alt="Support vector machine"/>&#13;
</span>&#13;
 ––––––––(3)</p>&#13;
<p>&#13;
<span class="inlinemediaobject"><img src="Image00265.jpg" alt="Support vector machine"/>&#13;
</span>&#13;
 ––––––––(4)</p>&#13;
<p>So the <a id="id281" class="indexterm"/>&#13;
 optimized Lagrangian becomes:</p>&#13;
<div class="mediaobject"><img src="Image00266.jpg" alt="Support vector machine"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Here, <span class="inlinemediaobject"><img src="Image00267.jpg" alt="Support vector machine"/>&#13;
</span>&#13;
 .</p>&#13;
<p>This is known as a dual form of the original problem, which depends only on the maximization of a<span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 :</p>&#13;
<div class="mediaobject"><img src="Image00268.jpg" alt="Support vector machine"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>The solutions <span class="inlinemediaobject"><img src="Image00269.jpg" alt="Support vector machine"/>&#13;
</span>&#13;
 (the cases a<span class="emphasis">&#13;
<em>&#13;
<sub>i</sub>&#13;
 =0</em>&#13;
</span>&#13;
 return null vectors) are found using a technique called quadratic programming and represent the support vectors <span class="emphasis">&#13;
<em>w</em>&#13;
</span>&#13;
 through formula <span class="strong">&#13;
<strong>(3)</strong>&#13;
</span>&#13;
 :</p>&#13;
<p>&#13;
<span class="inlinemediaobject"><img src="Image00270.jpg" alt="Support vector machine"/>&#13;
</span>&#13;
 ––––––––(5).</p>&#13;
<p>a<span class="emphasis">&#13;
<em>s</em>&#13;
</span>&#13;
 satisfy the equation (combination of equation <span class="strong">&#13;
<strong>(1)</strong>&#13;
</span>&#13;
 and <span class="strong">&#13;
<strong>(2)</strong>&#13;
</span>&#13;
 ):</p>&#13;
<div class="mediaobject"><img src="Image00271.jpg" alt="Support vector machine"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Substituting equation <span class="strong">&#13;
<strong>(3)</strong>&#13;
</span>&#13;
 and multiplying both sides by <span class="emphasis">&#13;
<em>y<sub>s</sub>&#13;
</em>&#13;
</span>&#13;
 (which is <span class="emphasis">&#13;
<em>+1</em>&#13;
</span>&#13;
 or <span class="emphasis">&#13;
<em>-1</em>&#13;
</span>&#13;
 ), we obtain:</p>&#13;
<div class="mediaobject"><img src="Image00272.jpg" alt="Support vector machine"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Averaging <a id="id282" class="indexterm"/>&#13;
 over all the support vectors <span class="emphasis">&#13;
<em>N<sub>s</sub>&#13;
</em>&#13;
</span>&#13;
 we can have a better estimate of the parameter <span class="emphasis">&#13;
<em>b</em>&#13;
</span>&#13;
 :</p>&#13;
<p>&#13;
<span class="inlinemediaobject"><img src="Image00273.jpg" alt="Support vector machine"/>&#13;
</span>&#13;
 ––––––––(6)</p>&#13;
<p>The equations <span class="strong">&#13;
<strong>(5)</strong>&#13;
</span>&#13;
 and <span class="strong">&#13;
<strong>(6)</strong>&#13;
</span>&#13;
 return the values of the parameters that define the support vector machines algorithm, from which it is possible to predict the class of all test points <span class="emphasis">&#13;
<em>t</em>&#13;
</span>&#13;
 :</p>&#13;
<div class="mediaobject"><img src="Image00274.jpg" alt="Support vector machine"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<div class="mediaobject"><img src="Image00275.jpg" alt="Support vector machine"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>If a line is not able to completely separate the data points into two classes, we need to allow the data points to be misclassified by an error <span class="inlinemediaobject"><img src="Image00276.jpg" alt="Support vector machine"/>&#13;
</span>&#13;
 such that:</p>&#13;
<div class="mediaobject"><img src="Image00277.jpg" alt="Support vector machine"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<div class="mediaobject"><img src="Image00278.jpg" alt="Support vector machine"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>And we need to maximize the margin, trying to minimize the misclassification errors. This condition is translated into this equation:</p>&#13;
<p>&#13;
<span class="inlinemediaobject"><img src="Image00279.jpg" alt="Support vector machine"/>&#13;
</span>&#13;
 such that <span class="inlinemediaobject"><img src="Image00280.jpg" alt="Support vector machine"/>&#13;
</span>&#13;
</p>&#13;
<p>Here, the <a id="id283" class="indexterm"/>&#13;
 parameter <span class="emphasis">&#13;
<em>C</em>&#13;
</span>&#13;
 is set to balance the size of the margin with the misclassification errors (<span class="emphasis">&#13;
<em>C=0</em>&#13;
</span>&#13;
 trivially no misclassification and maximum margin, <span class="emphasis">&#13;
<em>C&gt;&gt;1</em>&#13;
</span>&#13;
 many misclassified points and a narrow margin). Applying the same method as before, the dual problem is subjected to Lagrange multipliers' conditions with an upper bound <span class="emphasis">&#13;
<em>C</em>&#13;
</span>&#13;
 :</p>&#13;
<div class="mediaobject"><img src="Image00281.jpg" alt="Support vector machine"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Until now, we have treated problems in which only two classes are considered. Real problems may have multiple classes, and two procedures are commonly used to employ this method (as seen for logistic regression): one versus all or one versus one. Given a problem with <span class="emphasis">&#13;
<em>M</em>&#13;
</span>&#13;
 classes, the first method trains <span class="emphasis">&#13;
<em>M</em>&#13;
</span>&#13;
 SVM models, each one assuming the labels of the considered class <span class="emphasis">&#13;
<em>j +1</em>&#13;
</span>&#13;
 and all the rest <span class="emphasis">&#13;
<em>-1</em>&#13;
</span>&#13;
 . The second method instead trains a model for each pair of classes <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>j</em>&#13;
</span>&#13;
 , leading to <span class="inlinemediaobject"><img src="Image00282.jpg" alt="Support vector machine"/>&#13;
</span>&#13;
 trained models. Clearly, the second method is computationally more expensive but the results are generally more precise.</p>&#13;
<p>In a similar way, SVM can be used in regression problems, that is, whenever <span class="emphasis">&#13;
<em>y<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 is continuous between <span class="emphasis">&#13;
<em>-1</em>&#13;
</span>&#13;
 and <span class="emphasis">&#13;
<em>1</em>&#13;
</span>&#13;
 . In this case, the goal is to find the parameters <span class="emphasis">&#13;
<em>w</em>&#13;
</span>&#13;
 and <span class="emphasis">&#13;
<em>b</em>&#13;
</span>&#13;
 such that:</p>&#13;
<div class="mediaobject"><img src="Image00283.jpg" alt="Support vector machine"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>We assume that the true values <span class="emphasis">&#13;
<em>t<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 can differ from the predicted value <span class="emphasis">&#13;
<em>y<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 of a maximum <span class="inlinemediaobject"><img src="Image00284.jpg" alt="Support vector machine"/>&#13;
</span>&#13;
 and the <a id="id284" class="indexterm"/>&#13;
 predictions can further be misclassified of about <span class="inlinemediaobject"><img src="Image00285.jpg" alt="Support vector machine"/>&#13;
</span>&#13;
 depending on whether <span class="emphasis">&#13;
<em>y<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 is larger or smaller than <span class="emphasis">&#13;
<em>t<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 . The following figure shows for an example point <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 the various predictions <span class="emphasis">&#13;
<em>y<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 lying around the true value <span class="emphasis">&#13;
<em>t<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 , and the associated errors:</p>&#13;
<div class="mediaobject"><img src="Image00286.jpg" alt="Support vector machine"/>&#13;
<div class="caption">&#13;
<p>The predictions <span class="emphasis">&#13;
<em>y<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 lie around the true value <span class="emphasis">&#13;
<em>&#13;
<sub>ti</sub>&#13;
</em>&#13;
</span>&#13;
</p>&#13;
</div>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>The minimization problem becomes:</p>&#13;
<div class="mediaobject"><img src="Image00287.jpg" alt="Support vector machine"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Such that:</p>&#13;
<div class="mediaobject"><img src="Image00288.jpg" alt="Support vector machine"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>It is possible<a id="id285" class="indexterm"/>&#13;
 to show that the associated dual problem is now equal to:</p>&#13;
<p>&#13;
<span class="inlinemediaobject"><img src="Image00289.jpg" alt="Support vector machine"/>&#13;
</span>&#13;
 subject to <span class="inlinemediaobject"><img src="Image00290.jpg" alt="Support vector machine"/>&#13;
</span>&#13;
 .</p>&#13;
<p>Here, <span class="inlinemediaobject"><img src="Image00291.jpg" alt="Support vector machine"/>&#13;
</span>&#13;
 are the Lagrangian multipliers.</p>&#13;
<p>The new prediction, <span class="emphasis">&#13;
<em>y<sub>p</sub>&#13;
</em>&#13;
</span>&#13;
 , can be found by applying the formula <span class="inlinemediaobject"><img src="Image00292.jpg" alt="Support vector machine"/>&#13;
</span>&#13;
 , where the parameter <span class="emphasis">&#13;
<em>b</em>&#13;
</span>&#13;
 can be obtained as before—averaging on the subset <span class="emphasis">&#13;
<em>S</em>&#13;
</span>&#13;
 given by the support vectors associated with the subset <span class="inlinemediaobject"><img src="Image00293.jpg" alt="Support vector machine"/>&#13;
</span>&#13;
 and <span class="inlinemediaobject"><img src="Image00294.jpg" alt="Support vector machine"/>&#13;
</span>&#13;
 :</p>&#13;
<div class="mediaobject"><img src="Image00295.jpg" alt="Support vector machine"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<div class="section" title="Kernel trick">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h2 class="title"><a id="ch03lvl2sec21"/>&#13;
 Kernel trick</h2>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>There are datasets that are not linearly separable in a certain space, but if it is transformed in the right space, then a hyperplane can separate the data into the desired two or more classes. Consider <a id="id286" class="indexterm"/>&#13;
 the example shown in the<a id="id287" class="indexterm"/>&#13;
 following figure:</p>&#13;
<div class="mediaobject"><img src="Image00296.jpg" alt="Kernel trick"/>&#13;
<div class="caption">&#13;
<p>In a two-dimensional space, the dataset shown on the left is not separable. Mapping the dataset in a three-dimensional space, the two classes are separable.</p>&#13;
</div>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>We can clearly see that the two classes are not linearly separable in two-dimensional space (the left figure). Suppose we then apply a kernel function <span class="emphasis">&#13;
<em>K</em>&#13;
</span>&#13;
 on the data such that:</p>&#13;
<div class="mediaobject"><img src="Image00297.jpg" alt="Kernel trick"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>The data is now separable by a two-dimensional plane (the right figure). The kernel function on the SVM algorithm is applied to the matrix <span class="emphasis">&#13;
<em>H<sub>ij</sub>&#13;
</em>&#13;
</span>&#13;
 , replacing the dot product on the variable <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>j</em>&#13;
</span>&#13;
 :</p>&#13;
<div class="mediaobject"><img src="Image00298.jpg" alt="Kernel trick"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Popular<a id="id288" class="indexterm"/>&#13;
 kernel functions used on the <a id="id289" class="indexterm"/>&#13;
 SVM algorithm are:</p>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">Linear kernel: <span class="inlinemediaobject"><img src="Image00299.jpg" alt="Kernel trick"/>&#13;
</span>&#13;
</li>&#13;
<li class="listitem">Radial basis kernel (RBF): <span class="inlinemediaobject"><img src="Image00300.jpg" alt="Kernel trick"/>&#13;
</span>&#13;
</li>&#13;
<li class="listitem">Polynomial kernel: <span class="inlinemediaobject"><img src="Image00301.jpg" alt="Kernel trick"/>&#13;
</span>&#13;
</li>&#13;
<li class="listitem">Sigmoid kernel: <span class="inlinemediaobject"><img src="Image00302.jpg" alt="Kernel trick"/>&#13;
</span>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
</div>&#13;
</div>&#13;

<div class="section" title="A comparison of methods">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h1 class="title"><a id="ch03lvl1sec22"/>&#13;
 A comparison of methods</h1>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>We can now test the methods discussed in this chapter to solve a regression problem and a classification <a id="id290" class="indexterm"/>&#13;
 problem. To avoid overfitting, the dataset is<a id="id291" class="indexterm"/>&#13;
 typically split into two sets: the training set, in which the model parameters are fitted, and a test set, where the accuracy of the model is evaluated. However, it may be necessary to use a third set, the validation set, in which the hyperparameters (for example, <span class="emphasis">&#13;
<em>C</em>&#13;
</span>&#13;
 and <span class="inlinemediaobject"><img src="Image00303.jpg" alt="A comparison of methods"/>&#13;
</span>&#13;
 for SVM, or α in ridge regression) can be optimized. The original dataset may be too small to allow splitting into three sets, and also the results may be affected by the particular choice of data points on the training, validation, and test sets. A common way to solve this issue is by evaluating the model following the so-called cross-validation procedure—the dataset is split into <span class="emphasis">&#13;
<em>k</em>&#13;
</span>&#13;
 subsets (called folds) and the model is trained as follows:</p>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">A model is trained using <span class="emphasis">&#13;
<em>k-1</em>&#13;
</span>&#13;
 of the folds as the training data.</li>&#13;
<li class="listitem">The resulting <a id="id292" class="indexterm"/>&#13;
 model is tested on the remaining <a id="id293" class="indexterm"/>&#13;
 part of the data.</li>&#13;
<li class="listitem">This procedure is repeated as many times as the number of folds decided at the beginning, each time with different <span class="emphasis">&#13;
<em>k-1</em>&#13;
</span>&#13;
 folds to train (and consequently different test fold). The final accuracy is obtained by the average of the accuracies obtained on the different <span class="emphasis">&#13;
<em>k</em>&#13;
</span>&#13;
 iterations.</li>&#13;
</ul>&#13;
</div>&#13;
<div class="section" title="Regression problem">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h2 class="title"><a id="ch03lvl2sec22"/>&#13;
 Regression problem</h2>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>We are using<a id="id294" class="indexterm"/>&#13;
 the housing dataset of Boston's suburbs stored at <a class="ulink" href="http://archive.ics.uci.edu/ml/datasets/Housing">http://archive.ics.uci.edu/ml/datasets/Housing</a>&#13;
 and in the author's repository (<a class="ulink" href="https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/">https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/</a>&#13;
 ), in which <a id="id295" class="indexterm"/>&#13;
 the code used in this paragraph is also available. The dataset has 13 features:</p>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">&#13;
<span class="strong">&#13;
<strong>CRIM</strong>&#13;
</span>&#13;
 : Per capita crime rate by town</li>&#13;
<li class="listitem">&#13;
<span class="strong">&#13;
<strong>ZN</strong>&#13;
</span>&#13;
 : Proportion of residential land zoned for lots over 25,000 sqft</li>&#13;
<li class="listitem">&#13;
<span class="strong">&#13;
<strong>INDUS</strong>&#13;
</span>&#13;
 : Proportion of non-retail business acres per town</li>&#13;
<li class="listitem">&#13;
<span class="strong">&#13;
<strong>CHAS</strong>&#13;
</span>&#13;
 : Charles River dummy variable (<span class="emphasis">&#13;
<em>= 1</em>&#13;
</span>&#13;
 if tract bounds river; <span class="emphasis">&#13;
<em>0</em>&#13;
</span>&#13;
 otherwise)</li>&#13;
<li class="listitem">&#13;
<span class="strong">&#13;
<strong>NOX</strong>&#13;
</span>&#13;
 : Nitric oxides concentration (parts per 10 million)</li>&#13;
<li class="listitem">&#13;
<span class="strong">&#13;
<strong>RM</strong>&#13;
</span>&#13;
 : Average number of rooms per dwelling</li>&#13;
<li class="listitem">&#13;
<span class="strong">&#13;
<strong>AGE</strong>&#13;
</span>&#13;
 : Proportion of owner-occupied units built prior to 1940</li>&#13;
<li class="listitem">&#13;
<span class="strong">&#13;
<strong>DIS</strong>&#13;
</span>&#13;
 : Weighted distances from five Boston employment centers</li>&#13;
<li class="listitem">&#13;
<span class="strong">&#13;
<strong>RAD</strong>&#13;
</span>&#13;
 : Index of accessibility to radial highways</li>&#13;
<li class="listitem">&#13;
<span class="strong">&#13;
<strong>TAX</strong>&#13;
</span>&#13;
 : Full-value property tax rate per $10,000</li>&#13;
<li class="listitem">&#13;
<span class="strong">&#13;
<strong>PTRATIO</strong>&#13;
</span>&#13;
 : Pupil-teacher ratio by town</li>&#13;
<li class="listitem">&#13;
<span class="strong">&#13;
<strong>B</strong>&#13;
</span>&#13;
 : <span class="emphasis">&#13;
<em>1000(Bk - 0.63)^2</em>&#13;
</span>&#13;
 , where <span class="emphasis">&#13;
<em>Bk</em>&#13;
</span>&#13;
 is the proportion of blacks by town</li>&#13;
<li class="listitem">&#13;
<span class="strong">&#13;
<strong>LSTAT</strong>&#13;
</span>&#13;
 : The percentage of lower status of the population and the labels that we want to predict are MEDV, which represent the house value values (in $1000)</li>&#13;
</ul>&#13;
</div>&#13;
<p>To evaluate the quality of the models, the mean squared error defined in the introduction and the coefficient of determination, <span class="emphasis">&#13;
<em>R<sup>2</sup>&#13;
</em>&#13;
</span>&#13;
 , are calculated. <span class="emphasis">&#13;
<em>R<sup>2</sup>&#13;
</em>&#13;
</span>&#13;
 is given by:</p>&#13;
<div class="mediaobject"><img src="Image00304.jpg" alt="Regression problem"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Here, <span class="emphasis">&#13;
<em>y<sub>i</sub>&#13;
 <sup>pred</sup>&#13;
</em>&#13;
</span>&#13;
 indicates<a id="id296" class="indexterm"/>&#13;
 the predicted label from the model.</p>&#13;
<p>The best result is <span class="emphasis">&#13;
<em>R<sup>2</sup>&#13;
 =1</em>&#13;
</span>&#13;
 , which means the model perfectly fits the data, while <span class="emphasis">&#13;
<em>R<sup>2</sup>&#13;
 =0</em>&#13;
</span>&#13;
 is associated with a model with a constant line (negative values indicate an increasingly worse fit). The code to compute to train the linear regression, ridge regression, lasso regression, and SVM regression using the <code class="literal">sklearn</code>&#13;
 library is as follows (IPython notebook at <a class="ulink" href="https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/">https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/</a>&#13;
 ):</p>&#13;
<div class="mediaobject"><img src="Image00305.jpg" alt="Regression problem"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<div class="mediaobject"><img src="Image00306.jpg" alt="Regression problem"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>The housing data is loaded using the pandas library and reshuffled to randomize the cross-validation folds subset data (10 folds have been used) by applying the function <code class="literal">df.iloc[np.random.permutation(len(df))]</code>&#13;
 . The output of this script is as follows:</p>&#13;
<div class="mediaobject"><img src="Image00307.jpg" alt="Regression problem"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>The best <a id="id297" class="indexterm"/>&#13;
 model fit is obtained using a random forest (with 50 trees); it returns an average coefficient of determination of <span class="emphasis">&#13;
<em>0.86</em>&#13;
</span>&#13;
 and <span class="emphasis">&#13;
<em>MSE=11.5</em>&#13;
</span>&#13;
 . As expected, the decision tree regressor has a lower <span class="emphasis">&#13;
<em>R<sup>2</sup>&#13;
</em>&#13;
</span>&#13;
 and higher MSE than the random forest (<span class="emphasis">&#13;
<em>0.67</em>&#13;
</span>&#13;
 and <span class="emphasis">&#13;
<em>25</em>&#13;
</span>&#13;
 respectively). The support vector machine <a id="id298" class="indexterm"/>&#13;
 with the <span class="strong">&#13;
<strong>rbf kernel</strong>&#13;
</span>&#13;
 (<span class="emphasis">&#13;
<em>C=1,</em>&#13;
</span>&#13;
 <span class="inlinemediaobject"><img src="Image00308.jpg" alt="Regression problem"/>&#13;
</span>&#13;
 ) is the worst model, with a huge MSE error <span class="emphasis">&#13;
<em>83.9</em>&#13;
</span>&#13;
 and <span class="emphasis">&#13;
<em>0.0</em>&#13;
</span>&#13;
 at <span class="emphasis">&#13;
<em>R<sup>2</sup>&#13;
</em>&#13;
</span>&#13;
 , while SVM with the linear kernel (<span class="emphasis">&#13;
<em>C=1</em>&#13;
</span>&#13;
 , <span class="inlinemediaobject"><img src="Image00308.jpg" alt="Regression problem"/>&#13;
</span>&#13;
 ) returns a decent model (<span class="emphasis">&#13;
<em>0.69 R<sup>2</sup>&#13;
</em>&#13;
</span>&#13;
 and <span class="emphasis">&#13;
<em>25.8</em>&#13;
</span>&#13;
 MSE). The lasso and ridge regressors have comparable results, around <span class="emphasis">&#13;
<em>0.7 R<sup>2</sup>&#13;
</em>&#13;
</span>&#13;
 and <span class="emphasis">&#13;
<em>24</em>&#13;
</span>&#13;
 MSE. An important procedure to improve the model results is feature selection. It often happens that only a subset of the total features is relevant to perform the model training while the other features may not contribute at all to the model <span class="emphasis">&#13;
<em>R<sup>2</sup>&#13;
</em>&#13;
</span>&#13;
 . Feature selection may improve <span class="emphasis">&#13;
<em>R<sup>2</sup>&#13;
</em>&#13;
</span>&#13;
 because misleading data is disregarded and training time is reduced (fewer features to consider).</p>&#13;
<p>There are many techniques for extracting the best features for a certain model, but in this context, we explore the so-called recursive feature elimination method (RSE), which essentially considers the attributes associated with the largest absolute weights until the desired number of features are selected. In the case of the SVM algorithm, the weights are just the values of <span class="emphasis">&#13;
<em>w</em>&#13;
</span>&#13;
 , while for regression, they are the model parameters θ. Using the <code class="literal">sklearn</code>&#13;
 built-in function <code class="literal">RFE</code>&#13;
 specifying only the best <a id="id299" class="indexterm"/>&#13;
 four attributes (<code class="literal">best_features</code>&#13;
 ):</p>&#13;
<div class="mediaobject"><img src="Image00309.jpg" alt="Regression problem"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>The output is:</p>&#13;
<div class="mediaobject"><img src="Image00310.jpg" alt="Regression problem"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>The <code class="literal">RFE</code>&#13;
 function <a id="id300" class="indexterm"/>&#13;
 returns a list of Booleans (the <code class="literal">support_</code>&#13;
 attribute) to indicate which features are selected (true values) and which are not (false values). The selected features are then used to evaluate the model as we have done before.</p>&#13;
<p>Even by using only four features, the best model remains the random forest with 50 trees, and the <span class="emphasis">&#13;
<em>R<sup>2</sup>&#13;
</em>&#13;
</span>&#13;
 is just marginally lower than that for the model trained with the full set of features (<span class="emphasis">&#13;
<em>0.82</em>&#13;
</span>&#13;
 against <span class="emphasis">&#13;
<em>0.86</em>&#13;
</span>&#13;
 ). The other models—lasso, ridge, decision tree, and linear SVM regressors—have a more significant <span class="emphasis">&#13;
<em>R<sup>2</sup>&#13;
</em>&#13;
</span>&#13;
 drop, but the results are still comparable with their corresponding full-trained models. Note that the KNN algorithm does not provide weights on the features, so the <code class="literal">RFE</code>&#13;
 method cannot be applied.</p>&#13;
</div>&#13;
<div class="section" title="Classification problem">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h2 class="title"><a id="ch03lvl2sec23"/>&#13;
 Classification problem</h2>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>To test the <a id="id301" class="indexterm"/>&#13;
 classifiers learned in this chapter, the dataset about car evaluation quality (inaccurate, accurate, good, and very good) based on six features that describe the main characteristics of a car (buying price, maintenance cost, number of doors, number of persons to carry, size of luggage boot, and safety). The dataset can be found at <a class="ulink" href="http://archive.ics.uci.edu/ml/datasets/Car+Evaluation">http://archive.ics.uci.edu/ml/datasets/Car+Evaluation</a>&#13;
 or on my GitHub account, together with the code discussed here (<a class="ulink" href="https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/">https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/</a>&#13;
 ). To evaluate the accuracy of the classification, we will use the precision, recall, and f-measure. Given a dataset with only two classes (positive and negative), we define the number of true positive points (<span class="emphasis">&#13;
<em>tp</em>&#13;
</span>&#13;
 ) the points correctly labeled as positive, the number of false positive (<span class="emphasis">&#13;
<em>fp</em>&#13;
</span>&#13;
 ) the points wrongly labeled as positive (negative points) and the number of false negative (<span class="emphasis">&#13;
<em>fn</em>&#13;
</span>&#13;
 ) the number of points erroneously assigned to the negative class. Using these definitions, the precision, recall and f-measure can be calculated as:</p>&#13;
<div class="mediaobject"><img src="Image00311.jpg" alt="Classification problem"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<div class="mediaobject"><img src="Image00312.jpg" alt="Classification problem"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<div class="mediaobject"><img src="Image00313.jpg" alt="Classification problem"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>In a classification <a id="id302" class="indexterm"/>&#13;
 problem, a perfect precision (<span class="emphasis">&#13;
<em>1.0</em>&#13;
</span>&#13;
 ) for a given class <span class="emphasis">&#13;
<em>C</em>&#13;
</span>&#13;
 means that each point assigned to class <span class="emphasis">&#13;
<em>C</em>&#13;
</span>&#13;
 belongs to class <span class="emphasis">&#13;
<em>C</em>&#13;
</span>&#13;
 (there is no information about the number of points from class <span class="emphasis">&#13;
<em>C</em>&#13;
</span>&#13;
 erroneously labeled), whereas a recall equal to <span class="emphasis">&#13;
<em>1.0</em>&#13;
</span>&#13;
 means that each point from class <span class="emphasis">&#13;
<em>C</em>&#13;
</span>&#13;
 was labeled as belonging to class <span class="emphasis">&#13;
<em>C</em>&#13;
</span>&#13;
 (but there is no information about the other points wrongly assigned to class <span class="emphasis">&#13;
<em>C</em>&#13;
</span>&#13;
 ).</p>&#13;
<p>Note that in the case of multiple classes, these metrics are usually calculated as many times the number of labels, each time considering a class as the positive and all others as the negative. Different averages over the multiple classes' metrics are then used to estimate the total precision, recall, and f-measure.</p>&#13;
<p>The code to classify the cars dataset is as follows. First, we load all the libraries and the data into a pandas data frame.</p>&#13;
<div class="mediaobject"><img src="Image00314.jpg" alt="Classification problem"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>The following are the<a id="id303" class="indexterm"/>&#13;
 feature values that are categorical:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>buying 0      v-high, high, med, low</strong>&#13;

</span>&#13;


<span class="strong">
<strong>maintenance 1  v-high, high, med, low</strong>&#13;

</span>&#13;


<span class="strong">
<strong>doors 2       2, 3, 4, 5-more</strong>&#13;

</span>&#13;


<span class="strong">
<strong>persons 3     2, 4, more</strong>&#13;

</span>&#13;


<span class="strong">
<strong>lug_boot 4    small, med, big</strong>&#13;

</span>&#13;


<span class="strong">
<strong>safety 5      low, med, high</strong>&#13;

</span>&#13;


<span class="strong">
<strong>car evaluation 6 unacc,acc,good,vgood</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>These are mapped into numbers to be used in the classification algorithms:</p>&#13;
<div class="mediaobject"><img src="Image00315.jpg" alt="Classification problem"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Since we need to calculate and save the measures for all the methods, we write a standard function, <code class="literal">CalcMeasures</code>&#13;
 , and divide the labels' vector <code class="literal">Y</code>&#13;
 from the features <code class="literal">X</code>&#13;
 :</p>&#13;
<div class="mediaobject"><img src="Image00316.jpg" alt="Classification problem"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>A <code class="literal">10</code>&#13;
 crossing<a id="id304" class="indexterm"/>&#13;
 validation folds has been used and the code is:</p>&#13;
<div class="mediaobject"><img src="Image00317.jpg" alt="Classification problem"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>The measures' <a id="id305" class="indexterm"/>&#13;
 values are stored in the data frames:</p>&#13;
<div class="mediaobject"><img src="Image00318.jpg" alt="Classification problem"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Each measure has been evaluated four times—the number of car evaluation classes that fill the arrays according to the index mapping:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">'acc': 0, 'unacc': 2, 'good': 1, 'vgood': 3</pre>&#13;
</div>&#13;
<p>The best model <a id="id306" class="indexterm"/>&#13;
 is SVM with rbf kernel (<span class="emphasis">&#13;
<em>C=50</em>&#13;
</span>&#13;
 ), but random forest (50 trees) and decision trees also return excellent results (measures over <span class="emphasis">&#13;
<em>0.9</em>&#13;
</span>&#13;
 for all the four classes). Naive Bayes, logistic regression, and SVM with linear kernel (<span class="emphasis">&#13;
<em>C=50</em>&#13;
</span>&#13;
 ) return poor models, especially for the accurate, good, and very good classes, because there are few points with those labels:</p>&#13;
<div class="mediaobject"><img src="Image00319.jpg" alt="Classification problem"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>In percentage, the very good (v-good) and good are 3.993% and 3.762% respectively, compared to 70.0223% of inaccurate and 22.222% of accurate. So, we can conclude that these algorithms are not suitable for predicting classes that are scarcely represented in a dataset.</p>&#13;
</div>&#13;
</div>&#13;

<div class="section" title="Hidden Markov model">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h1 class="title"><a id="ch03lvl1sec23"/>&#13;
 Hidden Markov model</h1>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>Although this method <a id="id307" class="indexterm"/>&#13;
 cannot be strictly considered a supervised learning algorithm, it can be also used to perform something that is really similar to classification, so we decided to include it here. To introduce the subject, we are going to present an example. Consider the simplistic case of predicting whether a salesman in front of you is lying or not (two states <span class="inlinemediaobject"><img src="Image00320.jpg" alt="Hidden Markov model"/>&#13;
</span>&#13;
 ) by observing his glance: eye contact, looking down, or looking aside (each observation <span class="emphasis">&#13;
<em>O<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 has the values <span class="emphasis">&#13;
<em>0</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>1</em>&#13;
</span>&#13;
 , and <span class="emphasis">&#13;
<em>2</em>&#13;
</span>&#13;
 respectively). Imagine a sequence of observations of the salesman's glances O=O<span class="emphasis">&#13;
<em>&#13;
<sub>0</sub>&#13;
</em>&#13;
</span>&#13;
 , O<span class="emphasis">&#13;
<em>&#13;
<sub>1</sub>&#13;
</em>&#13;
</span>&#13;
 , O<span class="emphasis">&#13;
<em>&#13;
<sub>2</sub>&#13;
</em>&#13;
</span>&#13;
 , O<span class="emphasis">&#13;
<em>&#13;
<sub>3</sub>&#13;
</em>&#13;
</span>&#13;
 , O<span class="emphasis">&#13;
<em>&#13;
<sub>4</sub>&#13;
</em>&#13;
</span>&#13;
 ,… are <span class="emphasis">&#13;
<em>0, 1, 0, 2,…</em>&#13;
</span>&#13;
 We want to infer the transition matrix <span class="emphasis">&#13;
<em>A</em>&#13;
</span>&#13;
 between states <span class="emphasis">&#13;
<em>S<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 at consecutive times <span class="emphasis">&#13;
<em>t</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>t+1</em>&#13;
</span>&#13;
 (or, in this example, two consecutive sentences):</p>&#13;
<div class="mediaobject"><img src="Image00321.jpg" alt="Hidden Markov model"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Any entry of <span class="emphasis">&#13;
<em>A</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>a<sub>ij</sub>&#13;
</em>&#13;
</span>&#13;
 represents the probability to stay at state <span class="emphasis">&#13;
<em>S<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 at time <span class="emphasis">&#13;
<em>t+1</em>&#13;
</span>&#13;
 given the state <span class="emphasis">&#13;
<em>S<sub>j</sub>&#13;
</em>&#13;
</span>&#13;
 at time <span class="emphasis">&#13;
<em>t</em>&#13;
</span>&#13;
 . Therefore, <span class="emphasis">&#13;
<em>0.3</em>&#13;
</span>&#13;
 (<span class="emphasis">&#13;
<em>a<sub>01</sub>&#13;
</em>&#13;
</span>&#13;
 ) is the probability that the salesman is not lying on the sentence at time <span class="emphasis">&#13;
<em>t+1</em>&#13;
</span>&#13;
 given that he is lying on the sentence at time <span class="emphasis">&#13;
<em>t</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>0.6 (a<sub>10</sub>&#13;
 )</em>&#13;
</span>&#13;
 is vice versa, <span class="emphasis">&#13;
<em>0.7(a<sub>00</sub>&#13;
 )</em>&#13;
</span>&#13;
 represents the probability that the salesman is lying on the sentence at time <span class="emphasis">&#13;
<em>t</em>&#13;
</span>&#13;
 and at time <span class="emphasis">&#13;
<em>t+10.4(a<sub>11</sub>&#13;
 )</em>&#13;
</span>&#13;
 is the probability that he is not lying on the sentence at time <span class="emphasis">&#13;
<em>t+1</em>&#13;
</span>&#13;
 after he was sincere at time <span class="emphasis">&#13;
<em>t</em>&#13;
</span>&#13;
 . In a similar way, it is possible to define the matrix <span class="emphasis">&#13;
<em>B</em>&#13;
</span>&#13;
 that correlates the <a id="id308" class="indexterm"/>&#13;
 salesman's intention with his three possible behaviors:</p>&#13;
<div class="mediaobject"><img src="Image00322.jpg" alt="Hidden Markov model"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Any entry <span class="emphasis">&#13;
<em>b<sub>j(k)</sub>&#13;
</em>&#13;
</span>&#13;
 is the probability to have observation <span class="emphasis">&#13;
<em>k</em>&#13;
</span>&#13;
 at time <span class="emphasis">&#13;
<em>t</em>&#13;
</span>&#13;
 given the state <span class="emphasis">&#13;
<em>S<sub>j</sub>&#13;
</em>&#13;
</span>&#13;
 at time <span class="emphasis">&#13;
<em>t</em>&#13;
</span>&#13;
 . For example, <span class="emphasis">&#13;
<em>0.7</em>&#13;
</span>&#13;
 (<span class="emphasis">&#13;
<em>b<sub>00</sub>&#13;
</em>&#13;
</span>&#13;
 ), <span class="emphasis">&#13;
<em>0.1</em>&#13;
</span>&#13;
 (<span class="emphasis">&#13;
<em>b<sub>01</sub>&#13;
</em>&#13;
</span>&#13;
 ), and <span class="emphasis">&#13;
<em>0.2</em>&#13;
</span>&#13;
 (<span class="emphasis">&#13;
<em>b<sub>02</sub>&#13;
</em>&#13;
</span>&#13;
 ) are the probabilities that the salesman is lying given the behavioral observations—eye contact, looking down, and looking aside—respectively. These relationships are described in the following figure:</p>&#13;
<div class="mediaobject"><img src="Image00323.jpg" alt="Hidden Markov model"/>&#13;
<div class="caption">&#13;
<p>Salesman behavior – two states hidden Markov model</p>&#13;
</div>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>The initial state distribution of the salesman can be also defined: <span class="inlinemediaobject"><img src="Image00324.jpg" alt="Hidden Markov model"/>&#13;
</span>&#13;
 (he is slightly more inclined to lie than to tell the truth in the first sentence at time <span class="emphasis">&#13;
<em>0</em>&#13;
</span>&#13;
 ). Note that all of these matrices <span class="inlinemediaobject"><img src="Image00325.jpg" alt="Hidden Markov model"/>&#13;
</span>&#13;
 are row stochastic; that is, the rows sum to <span class="emphasis">&#13;
<em>1</em>&#13;
</span>&#13;
 and there is no direct dependency on time. A <span class="strong">&#13;
<strong>hidden Markov model</strong>&#13;
</span>&#13;
 (<span class="strong">&#13;
<strong>HMM</strong>&#13;
</span>&#13;
 ) is given by the composition of the three matrices (<span class="inlinemediaobject"><img src="Image00326.jpg" alt="Hidden Markov model"/>&#13;
</span>&#13;
 ) that describe the relationship between the known <a id="id309" class="indexterm"/>&#13;
 sequence of observations <span class="emphasis">&#13;
<em>O=O<sub>0</sub>&#13;
 , O<sub>1</sub>&#13;
 ,…O<sub>T-1</sub>&#13;
</em>&#13;
</span>&#13;
 and the corresponding hidden states sequence <span class="emphasis">&#13;
<em>S=S<sub>0</sub>&#13;
 , S<sub>1</sub>&#13;
 ,… S<sub>T-1</sub>&#13;
</em>&#13;
</span>&#13;
 . In general, the standard notation symbols employed by this algorithm are summarized as follows:</p>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">&#13;
<span class="emphasis">&#13;
<em>T</em>&#13;
</span>&#13;
 is the length of the observation sequence <span class="emphasis">&#13;
<em>O=O<sub>0</sub>&#13;
</em>&#13;
</span>&#13;
 , O<sub>1</sub>&#13;
 ,… O<sub>T-1</sub>&#13;
 and the hidden states sequence <span class="emphasis">&#13;
<em>S=S<sub>0</sub>&#13;
 , S<sub>1</sub>&#13;
 ,… S<sub>T-1</sub>&#13;
</em>&#13;
</span>&#13;
</li>&#13;
<li class="listitem">&#13;
<span class="emphasis">&#13;
<em>N</em>&#13;
</span>&#13;
 is the number of possible (hidden) states in the model</li>&#13;
<li class="listitem">&#13;
<span class="emphasis">&#13;
<em>M</em>&#13;
</span>&#13;
 is the number of the possible observation values: <span class="inlinemediaobject"><img src="Image00327.jpg" alt="Hidden Markov model"/>&#13;
</span>&#13;
</li>&#13;
<li class="listitem">&#13;
<span class="emphasis">&#13;
<em>A</em>&#13;
</span>&#13;
 is the state transition matrix</li>&#13;
<li class="listitem">&#13;
<span class="emphasis">&#13;
<em>B</em>&#13;
</span>&#13;
 is the observation probability matrix</li>&#13;
<li class="listitem">π is the initial state distribution</li>&#13;
</ul>&#13;
</div>&#13;
<p>In the preceding example, <span class="emphasis">&#13;
<em>M=3</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>N=2</em>&#13;
</span>&#13;
 , and we imagine to predict the sequence of the salesman's intentions over the course of his speech (which are the hidden states) <span class="emphasis">&#13;
<em>S=S<sub>0</sub>&#13;
 , S<sub>1</sub>&#13;
 ,… S<sub>T-1</sub>&#13;
</em>&#13;
</span>&#13;
 , observing the values of his behavior <span class="emphasis">&#13;
<em>O=O<sub>0</sub>&#13;
 , O<sub>1</sub>&#13;
 ,… O<sub>T-1</sub>&#13;
</em>&#13;
</span>&#13;
 . This is achieved by calculating the probability of each state sequence <span class="emphasis">&#13;
<em>S</em>&#13;
</span>&#13;
 as:</p>&#13;
<div class="mediaobject"><img src="Image00328.jpg" alt="Hidden Markov model"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>For instance, fixing <span class="emphasis">&#13;
<em>T=4</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>S=0101</em>&#13;
</span>&#13;
 , and <span class="emphasis">&#13;
<em>O=1012</em>&#13;
</span>&#13;
 :</p>&#13;
<div class="mediaobject"><img src="Image00329.jpg" alt="Hidden Markov model"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>In the same way, we can calculate the probability of all other combinations of hidden states and find the most probable sequence <span class="emphasis">&#13;
<em>S</em>&#13;
</span>&#13;
 . An efficient algorithm for finding the most probable sequence <span class="emphasis">&#13;
<em>S</em>&#13;
</span>&#13;
 is<a id="id310" class="indexterm"/>&#13;
 the <span class="strong">&#13;
<strong>Viterbi algorithm</strong>&#13;
</span>&#13;
 , which consists of computing the maximum probability of the set of partial sequences from <span class="emphasis">&#13;
<em>0</em>&#13;
</span>&#13;
 to <span class="emphasis">&#13;
<em>t</em>&#13;
</span>&#13;
 until <span class="emphasis">&#13;
<em>T-1</em>&#13;
</span>&#13;
 . In practice, we calculate the following quantities:</p>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">&#13;
<span class="inlinemediaobject"><img src="Image00330.jpg" alt="Hidden Markov model"/>&#13;
</span>&#13;
</li>&#13;
<li class="listitem">For <span class="emphasis">&#13;
<em>t=1,…,T-1</em>&#13;
</span>&#13;
 and <span class="emphasis">&#13;
<em>i=0,…,N-1</em>&#13;
</span>&#13;
 , the maximum probability of being at state <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 at time <span class="emphasis">&#13;
<em>t</em>&#13;
</span>&#13;
 among the possible paths coming from different states <span class="emphasis">&#13;
<em>j</em>&#13;
</span>&#13;
 is <span class="inlinemediaobject"><img src="Image00331.jpg" alt="Hidden Markov model"/>&#13;
</span>&#13;
 . The partial sequence associated with the maximum of <span class="inlinemediaobject"><img src="Image00332.jpg" alt="Hidden Markov model"/>&#13;
</span>&#13;
 is the most probable partial sequence until time <span class="emphasis">&#13;
<em>t</em>&#13;
</span>&#13;
 .</li>&#13;
<li class="listitem">The final<a id="id311" class="indexterm"/>&#13;
 most probable sequence is associated with the maximum of the probability at time <span class="emphasis">&#13;
<em>T-1</em>&#13;
</span>&#13;
 : <span class="inlinemediaobject"><img src="Image00333.jpg" alt="Hidden Markov model"/>&#13;
</span>&#13;
 .</li>&#13;
</ul>&#13;
</div>&#13;
<p>For example, given the preceding model, the most likely sequence of length <span class="emphasis">&#13;
<em>T=2</em>&#13;
</span>&#13;
 can be calculated as:</p>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">&#13;
<span class="emphasis">&#13;
<em>P(10)=0.0024</em>&#13;
</span>&#13;
</li>&#13;
<li class="listitem">&#13;
<span class="emphasis">&#13;
<em>P(00)=0.0294</em>&#13;
</span>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">So d<span class="emphasis">&#13;
<em>1</em>&#13;
</span>&#13;
 (0)=P(00)=0.0294</li>&#13;
</ul>&#13;
</div>&#13;
</li>&#13;
<li class="listitem">&#13;
<span class="emphasis">&#13;
<em>P(01)=0.076</em>&#13;
</span>&#13;
</li>&#13;
<li class="listitem">&#13;
<span class="emphasis">&#13;
<em>P(11)=0.01</em>&#13;
</span>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">So d<span class="emphasis">&#13;
<em>&#13;
<sub>1</sub>&#13;
 (1)=P(01)=0.076</em>&#13;
</span>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
<p>And the final most probable sequence is <span class="emphasis">&#13;
<em>00</em>&#13;
</span>&#13;
 (two consecutive false sentences).</p>&#13;
<p>Another way to think about the most likely sequence is by maximizing the number of correct states; that is, consider for each time <span class="emphasis">&#13;
<em>t</em>&#13;
</span>&#13;
 the state <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 with the maximum probability <span class="inlinemediaobject"><img src="Image00334.jpg" alt="Hidden Markov model"/>&#13;
</span>&#13;
 . Using an algorithm called backward algorithm, it is possible to show that the probability of a given state <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 , <span class="inlinemediaobject"><img src="Image00335.jpg" alt="Hidden Markov model"/>&#13;
</span>&#13;
 , is:</p>&#13;
<div class="mediaobject"><img src="Image00336.jpg" alt="Hidden Markov model"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Here:</p>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">&#13;
<span class="inlinemediaobject"><img src="Image00337.jpg" alt="Hidden Markov model"/>&#13;
</span>&#13;
</li>&#13;
<li class="listitem">&#13;
<span class="inlinemediaobject"><img src="Image00338.jpg" alt="Hidden Markov model"/>&#13;
</span>&#13;
 and <span class="inlinemediaobject"><img src="Image00339.jpg" alt="Hidden Markov model"/>&#13;
</span>&#13;
 Probabilities <a id="id312" class="indexterm"/>&#13;
 of the partial observation sequence until time <span class="emphasis">&#13;
<em>t</em>&#13;
</span>&#13;
 , where the HMM is on state <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 : <span class="inlinemediaobject"><img src="Image00340.jpg" alt="Hidden Markov model"/>&#13;
</span>&#13;
</li>&#13;
<li class="listitem">&#13;
<span class="inlinemediaobject"><img src="Image00341.jpg" alt="Hidden Markov model"/>&#13;
</span>&#13;
 and <span class="inlinemediaobject"><img src="Image00342.jpg" alt="Hidden Markov model"/>&#13;
</span>&#13;
 Probability of the partial sequence after time <span class="emphasis">&#13;
<em>t</em>&#13;
</span>&#13;
 until <span class="emphasis">&#13;
<em>T-1</em>&#13;
</span>&#13;
 given the state at time <span class="emphasis">&#13;
<em>t</em>&#13;
</span>&#13;
 equal to <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 : <span class="inlinemediaobject"><img src="Image00343.jpg" alt="Hidden Markov model"/>&#13;
</span>&#13;
</li>&#13;
<li class="listitem">The combination of the probabilities to stay on state <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 before and after time <span class="emphasis">&#13;
<em>t</em>&#13;
</span>&#13;
 result in the value of <span class="inlinemediaobject"><img src="Image00344.jpg" alt="Hidden Markov model"/>&#13;
</span>&#13;
 .</li>&#13;
</ul>&#13;
</div>&#13;
<p>Note that the two methods of calculating the most likely sequence do not necessarily return the same result.</p>&#13;
<p>The reverse problem—find the optimal HMM <span class="inlinemediaobject"><img src="Image00345.jpg" alt="Hidden Markov model"/>&#13;
</span>&#13;
 given a sequence <span class="emphasis">&#13;
<em>O=O<sub>0</sub>&#13;
 ,O<sub>1</sub>&#13;
 ,…O<sub>T-1</sub>&#13;
</em>&#13;
</span>&#13;
 and the values of the parameters <span class="emphasis">&#13;
<em>N</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>M</em>&#13;
</span>&#13;
 —is also solvable iteratively using the <a id="id313" class="indexterm"/>&#13;
 <span class="strong">&#13;
<strong>Baum-Welch algorithm</strong>&#13;
</span>&#13;
 . Defining the probability of occurring at state <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 at time <span class="emphasis">&#13;
<em>t</em>&#13;
</span>&#13;
 and to go at state <span class="emphasis">&#13;
<em>j</em>&#13;
</span>&#13;
 at time <span class="emphasis">&#13;
<em>t+1</em>&#13;
</span>&#13;
 as:</p>&#13;
<p>&#13;
<span class="inlinemediaobject"><img src="Image00346.jpg" alt="Hidden Markov model"/>&#13;
</span>&#13;
 where <span class="inlinemediaobject"><img src="Image00347.jpg" alt="Hidden Markov model"/>&#13;
</span>&#13;
 for <span class="inlinemediaobject"><img src="Image00348.jpg" alt="Hidden Markov model"/>&#13;
</span>&#13;
 and <span class="inlinemediaobject"><img src="Image00349.jpg" alt="Hidden Markov model"/>&#13;
</span>&#13;
 .</p>&#13;
<p>Then the <a id="id314" class="indexterm"/>&#13;
 Baum-Welch algorithm is as follows:</p>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">Initialize <span class="inlinemediaobject"><img src="Image00350.jpg" alt="Hidden Markov model"/>&#13;
</span>&#13;
</li>&#13;
<li class="listitem">Calculate <span class="inlinemediaobject"><img src="Image00351.jpg" alt="Hidden Markov model"/>&#13;
</span>&#13;
 and <span class="inlinemediaobject"><img src="Image00352.jpg" alt="Hidden Markov model"/>&#13;
</span>&#13;
</li>&#13;
<li class="listitem">Recompute the model matrices as:<span class="inlinemediaobject"><img src="Image00353.jpg" alt="Hidden Markov model"/>&#13;
</span>&#13;
 where <span class="inlinemediaobject"><img src="Image00354.jpg" alt="Hidden Markov model"/>&#13;
</span>&#13;
 and <span class="inlinemediaobject"><img src="Image00355.jpg" alt="Hidden Markov model"/>&#13;
</span>&#13;
 is Kronacker symbol, which is equal to <span class="emphasis">&#13;
<em>1</em>&#13;
</span>&#13;
 if <span class="inlinemediaobject"><img src="Image00356.jpg" alt="Hidden Markov model"/>&#13;
</span>&#13;
 and <span class="emphasis">&#13;
<em>0</em>&#13;
</span>&#13;
 otherwise</li>&#13;
<li class="listitem">Iterate until the convergence of: <span class="inlinemediaobject"><img src="Image00357.jpg" alt="Hidden Markov model"/>&#13;
</span>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
<p>In the next<a id="id315" class="indexterm"/>&#13;
 section, we are going to show a piece of Python code that implements these equations to test the HMM algorithm.</p>&#13;
<div class="section" title="A Python example">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h2 class="title"><a id="ch03lvl2sec24"/>&#13;
 A Python example</h2>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>As usual, the <code class="literal">hmm_example.py</code>&#13;
 file discussed hereafter is available at <a class="ulink" href="https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/">https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/</a>&#13;
 .</p>&#13;
<p>We start defining <a id="id316" class="indexterm"/>&#13;
 a class in which we pass the model <a id="id317" class="indexterm"/>&#13;
 matrices:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>class HMM:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    def __init__(self):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        </strong>&#13;

</span>&#13;


<span class="strong">
<strong>self.pi = pi</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        self.A = A</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        self.B = B</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>The Viterbi algorithm and the maximization of the number of correct states are implemented in the following two functions:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>    def ViterbiSequence(self,observations):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        deltas = [{}]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        seq = {}</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        N = self.A.shape[0]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        states = [i for i in range(N)]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        T = len(observations)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        #initialization</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        for s in states:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            deltas[0][s] = self.pi[s]*self.B[s,observations[0]]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            seq[s] = [s]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        #compute Viterbi</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        for t in range(1,T):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            deltas.append({})</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            newseq = {}</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            for s in states:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                (delta,state) = max((deltas[t-1][s0]*self.A[s0,s]*self.B[s,observations[t]],s0) for s0 in states)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                deltas[t][s] = delta</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                newseq[s] = seq[state] + [s]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            seq = newseq</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            </strong>&#13;

</span>&#13;


<span class="strong">
<strong>        (delta,state) = max((deltas[T-1][s],s) for s in states)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        return  delta,' sequence: ', seq[state]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        </strong>&#13;

</span>&#13;


<span class="strong">
<strong>    def maxProbSequence(self,observations):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        N = self.A.shape[0]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        states = [i for i in range(N)]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        T = len(observations)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        M = self.B.shape[1]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        # alpha_t(i) = P(O_1 O_2 ... O_t, q_t = S_i | hmm)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        # Initialize alpha</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        alpha = np.zeros((N,T))</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        c = np.zeros(T) #scale factors</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        alpha[:,0] = pi.T * self.B[:,observations[0]]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        c[0] = 1.0/np.sum(alpha[:,0])</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        alpha[:,0] = c[0] * alpha[:,0]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        # Update alpha for each observation step</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        for t in range(1,T):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            alpha[:,t] = np.dot(alpha[:,t-1].T, self.A).T * self.B[:,observations[t]]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            c[t] = 1.0/np.sum(alpha[:,t])</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            alpha[:,t] = c[t] * alpha[:,t]</strong>&#13;

</span>&#13;



<span class="strong">
<strong>        # beta_t(i) = P(O_t+1 O_t+2 ... O_T | q_t = S_i , hmm)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        # Initialize beta</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        beta = np.zeros((N,T))</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        beta[:,T-1] = 1</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        beta[:,T-1] = c[T-1] * beta[:,T-1]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        # Update beta backwards froT end of sequence</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        for t in range(len(observations)-1,0,-1):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            beta[:,t-1] = np.dot(self.A, (self.B[:,observations[t]] * beta[:,t]))</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            beta[:,t-1] = c[t-1] * beta[:,t-1]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            </strong>&#13;

</span>&#13;


<span class="strong">
<strong>        norm = np.sum(alpha[:,T-1])</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        seq = ''</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        for t in range(T):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            g,state = max(((beta[i,t]*alpha[i,t])/norm,i) for i in states)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            seq +=str(state)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            </strong>&#13;

</span>&#13;


<span class="strong">
<strong>        return seq</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>Since the<a id="id318" class="indexterm"/>&#13;
 multiplication of probabilities will result in an <a id="id319" class="indexterm"/>&#13;
 underflow problem, all the Α<span class="emphasis">&#13;
<em>&#13;
<sub>t</sub>&#13;
 (i)</em>&#13;
</span>&#13;
 and Β<span class="emphasis">&#13;
<em>&#13;
<sub>t</sub>&#13;
 (i)</em>&#13;
</span>&#13;
 have been multiplied by a constant such that for <span class="inlinemediaobject"><img src="Image00358.jpg" alt="A Python example"/>&#13;
</span>&#13;
 :</p>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">&#13;
<span class="inlinemediaobject"><img src="Image00359.jpg" alt="A Python example"/>&#13;
</span>&#13;
</li>&#13;
<li class="listitem">&#13;
<span class="inlinemediaobject"><img src="Image00360.jpg" alt="A Python example"/>&#13;
</span>&#13;
 where <span class="inlinemediaobject"><img src="Image00361.jpg" alt="A Python example"/>&#13;
</span>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
<p>Now we can initialize the HMM model with the matrices in the salesman's intentions example and use the two preceding functions:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>pi = np.array([0.6, 0.4])</strong>&#13;

</span>&#13;


<span class="strong">
<strong>A = np.array([[0.7, 0.3],</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                           [0.6, 0.4]])</strong>&#13;

</span>&#13;


<span class="strong">
<strong>B = np.array([[0.7, 0.1, 0.2],</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                           [0.1, 0.6, 0.3]]) </strong>&#13;

</span>&#13;


<span class="strong">
<strong>hmmguess = HMM(pi,A,B)   </strong>&#13;

</span>&#13;


<span class="strong">
<strong>print 'Viterbi sequence:',hmmguess.ViterbiSequence(np.array([0,1,0,2]))</strong>&#13;

</span>&#13;


<span class="strong">
<strong>print 'max prob sequence:',hmmguess.maxProbSequence(np.array([0,1,0,2])) </strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>The result is:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>Viterbi: (0.0044, 'sequence: ', [0, 1, 0, 0])</strong>&#13;

</span>&#13;


<span class="strong">
<strong>Max prob sequence: 0100</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>In this particular case, the two methods return the same sequence, and you can easily verify that by changing the initial matrices, the algorithms may lead to different results. We obtain that the sequence of behaviors; eye contact, looking down, eye contact, looking aside is likely associated with the salesman states' sequence; lie, not lie, lie, lie with a probability of <span class="emphasis">&#13;
<em>0.0044</em>&#13;
</span>&#13;
 .</p>&#13;
<p>It is also <a id="id320" class="indexterm"/>&#13;
 possible to implement the Baum-Welch algorithm<a id="id321" class="indexterm"/>&#13;
 to find the optimal HMM given the sequence of observations and the parameters <span class="emphasis">&#13;
<em>N</em>&#13;
</span>&#13;
 and <span class="emphasis">&#13;
<em>M</em>&#13;
</span>&#13;
 . Here is the code:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>    def train(self,observations,criterion):</strong>&#13;

</span>&#13;



<span class="strong">
<strong>        N = self.A.shape[0]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        T = len(observations)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        M = self.B.shape[1]</strong>&#13;

</span>&#13;



<span class="strong">
<strong>        A = self.A</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        B = self.B</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        pi = copy(self.pi)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        </strong>&#13;

</span>&#13;


<span class="strong">
<strong>        convergence = False</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        while not convergence:</strong>&#13;

</span>&#13;



<span class="strong">
<strong>            # alpha_t(i) = P(O_1 O_2 ... O_t, q_t = S_i | hmm)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            # Initialize alpha</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            alpha = np.zeros((N,T))</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            c = np.zeros(T) #scale factors</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            alpha[:,0] = pi.T * self.B[:,observations[0]]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            c[0] = 1.0/np.sum(alpha[:,0])</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            alpha[:,0] = c[0] * alpha[:,0]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            # Update alpha for each observation step</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            for t in range(1,T):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                alpha[:,t] = np.dot(alpha[:,t-1].T, self.A).T * self.B[:,observations[t]]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                c[t] = 1.0/np.sum(alpha[:,t])</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                alpha[:,t] = c[t] * alpha[:,t]</strong>&#13;

</span>&#13;



<span class="strong">
<strong>            #P(O=O_0,O_1,...,O_T-1 | hmm)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            P_O = np.sum(alpha[:,T-1])</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            # beta_t(i) = P(O_t+1 O_t+2 ... O_T | q_t = S_i , hmm)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            # Initialize beta</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            beta = np.zeros((N,T))</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            beta[:,T-1] = 1</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            beta[:,T-1] = c[T-1] * beta[:,T-1]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            # Update beta backwards froT end of sequence</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            for t in range(len(observations)-1,0,-1):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                beta[:,t-1] = np.dot(self.A, (self.B[:,observations[t]] * beta[:,t]))</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                beta[:,t-1] = c[t-1] * beta[:,t-1]</strong>&#13;

</span>&#13;



<span class="strong">
<strong>            gi = np.zeros((N,N,T-1));</strong>&#13;

</span>&#13;



<span class="strong">
<strong>            for t in range(T-1):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                for i in range(N):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                    </strong>&#13;

</span>&#13;


<span class="strong">
<strong>                    gamma_num = alpha[i,t] * self.A[i,:] * self.B[:,observations[t+1]].T * \</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                            beta[:,t+1].T</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                    gi[i,:,t] = gamma_num / P_O</strong>&#13;

</span>&#13;


<span class="strong">
<strong>  </strong>&#13;

</span>&#13;


<span class="strong">
<strong>            # gamma_t(i) = P(q_t = S_i | O, hmm)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            gamma = np.squeeze(np.sum(gi,axis=1))</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            # Need final gamma element for new B</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            prod =  (alpha[:,T-1] * beta[:,T-1]).reshape((-1,1))</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            gamma_T = prod/P_O</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            gamma = np.hstack((gamma,  gamma_T)) #append one Tore to gamma!!!</strong>&#13;

</span>&#13;



<span class="strong">
<strong>            newpi = gamma[:,0]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            newA = np.sum(gi,2) / np.sum(gamma[:,:-1],axis=1).reshape((-1,1))</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            newB = copy(B)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            </strong>&#13;

</span>&#13;


<span class="strong">
<strong>            sumgamma = np.sum(gamma,axis=1)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            for ob_k in range(M):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                list_k = observations == ob_k</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                newB[:,ob_k] = np.sum(gamma[:,list_k],axis=1) / sumgamma</strong>&#13;

</span>&#13;



<span class="strong">
<strong>            if np.max(abs(pi - newpi)) &lt; criterion and \</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                   np.max(abs(A - newA)) &lt; criterion and \</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                   np.max(abs(B - newB)) &lt; criterion:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                convergence = True;</strong>&#13;

</span>&#13;


<span class="strong">
<strong>  </strong>&#13;

</span>&#13;


<span class="strong">
<strong>            A[:],B[:],pi[:] = newA,newB,newpi</strong>&#13;

</span>&#13;



<span class="strong">
<strong>        self.A[:] = newA</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        self.B[:] = newB</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        self.pi[:] = newpi</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        self.gamma = gamma</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>Note that the code uses the shallow copy from the module <code class="literal">copy</code>&#13;
 , which creates a new container populated with references to the contents of the original object (in this case, <code class="literal">pi</code>&#13;
 , <code class="literal">B</code>&#13;
 ). That is, <code class="literal">newpi</code>&#13;
 is a different object from <code class="literal">pi</code>&#13;
 but <code class="literal">newpi[0]</code>&#13;
 is a reference of <code class="literal">pi[0]</code>&#13;
 . The NumPy squeeze function instead is needed to drop the redundant dimension from a matrix.</p>&#13;
<p>Using the same<a id="id322" class="indexterm"/>&#13;
 behaviors sequence <span class="emphasis">&#13;
<em>O=0, 1, 0, 2</em>&#13;
</span>&#13;
 , we obtain<a id="id323" class="indexterm"/>&#13;
 that the optimal model is given by:</p>&#13;
<p>&#13;
<span class="inlinemediaobject"><img src="Image00362.jpg" alt="A Python example"/>&#13;
</span>&#13;
 ,<span class="inlinemediaobject"><img src="Image00363.jpg" alt="A Python example"/>&#13;
</span>&#13;
 ,<span class="inlinemediaobject"><img src="Image00364.jpg" alt="A Python example"/>&#13;
</span>&#13;
</p>&#13;
<p>This means that the state sequence must start from a true salesman's sentence and continuously oscillate between the two states <span class="emphasis">&#13;
<em>lie</em>&#13;
</span>&#13;
 and <span class="emphasis">&#13;
<em>not lie</em>&#13;
</span>&#13;
 . A true salesman's sentence (not lie) is certainly related to the eye contact value, while a lie is related to the looking down and looking aside behaviors.</p>&#13;
<p>In this simple introduction on HMM, we have assumed that each observation is a scalar value, but in real applications, each <span class="emphasis">&#13;
<em>O<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 is usually a vector of features. And usually, this method is used as a classification training as many HMM l<span class="emphasis">&#13;
<em>&#13;
<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 , as classes to predict and then a test time chooses the class with the highest <span class="inlinemediaobject"><img src="Image00365.jpg" alt="A Python example"/>&#13;
</span>&#13;
 . Continuing with this example, we can imagine building a <span class="emphasis">&#13;
<em>true machine</em>&#13;
</span>&#13;
 to test each salesman we talk to. Imagine that for each sentence (observation) <span class="emphasis">&#13;
<em>O<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 of our speaker, we can extract three features glances with three possible values <span class="emphasis">&#13;
<em>e<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 (eye contact, looking down, and looking aside), voice sound <span class="emphasis">&#13;
<em>v<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 with three possible values (too loud, too low, and flat), and hand movement <span class="emphasis">&#13;
<em>h<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 with two possible values (shaking and calm) O<span class="emphasis">&#13;
<em>i=(e<sub>i</sub>&#13;
 , v<sub>i</sub>&#13;
 , h<sub>i</sub>&#13;
 )</em>&#13;
</span>&#13;
 . At training time, we ask our friend to tell lies and we use these observations to train an HMM l<span class="emphasis">&#13;
<em>0</em>&#13;
</span>&#13;
 using Baum-Welch. We repeat the training process but with true sentences and train l<span class="emphasis">&#13;
<em>1</em>&#13;
</span>&#13;
 . At test time, we record the sentence of the salesman <span class="emphasis">&#13;
<em>O</em>&#13;
</span>&#13;
 and calculate both: <span class="inlinemediaobject"><img src="Image00366.jpg" alt="A Python example"/>&#13;
</span>&#13;
 ,<span class="inlinemediaobject"><img src="Image00367.jpg" alt="A Python example"/>&#13;
</span>&#13;
 . The class prediction will be the one with the highest probability.</p>&#13;
<p>Note that HMM has been applied in various fields, but the applications in which it performs quite well <a id="id324" class="indexterm"/>&#13;
 are speech recognition <a id="id325" class="indexterm"/>&#13;
 tasks, handwritten character recognition, and action recognition.</p>&#13;
</div>&#13;
</div>&#13;

<div class="section" title="Summary">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h1 class="title"><a id="ch03lvl1sec24"/>&#13;
 Summary</h1>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>In this chapter, the major classification and regression algorithms, together with the techniques to implement them, were discussed. You should now be able to understand in which situation each method can be used and how to implement it using Python and its libraries (sklearn and pandas).</p>&#13;
<p>In the next chapter, we will cover the most relevant techniques used to learn from web data (web data mining).</p>&#13;
</div>&#13;
</body></html>