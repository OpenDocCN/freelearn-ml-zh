- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Undersampling Methods
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 欠采样方法
- en: Sometimes, you have so much data that adding more data by oversampling only
    makes things worse. Don’t worry, as we have a strategy for those situations as
    well. It’s called undersampling, or downsampling. In this chapter, you will learn
    about the concept of undersampling, including when to use it and the various techniques
    to perform it. You will also see how to use these techniques via the `imbalanced-learn`
    library APIs and compare their performance with some classical machine learning
    models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，你拥有如此多的数据，通过过采样添加更多数据只会使事情变得更糟。别担心，我们也有针对这些情况的策略。这被称为欠采样或降采样。在本章中，你将了解欠采样的概念，包括何时使用它以及执行它的各种技术。你还将看到如何通过`imbalanced-learn`库的API使用这些技术，并将它们的性能与一些经典的机器学习模型进行比较。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Introducing undersampling
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍欠采样
- en: When to avoid undersampling in the majority class
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多数类中何时避免欠采样
- en: Removing examples uniformly
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均匀地移除示例
- en: Strategies for removing noisy observations
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除噪声观察的策略
- en: Strategies for removing easy observations
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除简单观察的策略
- en: By the end of this chapter, you’ll have mastered various undersampling techniques
    for imbalanced datasets and will be able to confidently apply them with the `imbalanced-learn`
    library to build better machine learning models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将掌握各种用于不平衡数据集的欠采样技术，并能够自信地使用`imbalanced-learn`库来构建更好的机器学习模型。
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter will make use of common libraries such as `matplotlib`, `seaborn`,
    `pandas`, `numpy`, `scikit-learn`, and `imbalanced-learn`. The code and notebooks
    for this chapter can be found on GitHub at [https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter03](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter03).
    To run the notebook, there are two options: you can click the **Open in Colab**
    icon at the top of the chapter’s notebook, or you can launch it directly from
    [https://colab.research.google.com](https://colab.research.google.com) using the
    GitHub URL of the notebook.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将使用常见的库，如`matplotlib`、`seaborn`、`pandas`、`numpy`、`scikit-learn`和`imbalanced-learn`。本章的代码和笔记本可以在GitHub上找到，网址为[https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter03](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter03)。要运行笔记本，有两种选择：你可以点击章节笔记本顶部的**在Colab中打开**图标，或者你可以直接从[https://colab.research.google.com](https://colab.research.google.com)使用笔记本的GitHub
    URL启动它。
- en: Introducing undersampling
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍欠采样
- en: Two households, both alike in dignity,
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 两个家庭，同样有尊严，
- en: In fair Verona, where we lay our scene,
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在公平的维罗纳，我们设景之处，
- en: From ancient grudge break to new mutiny,
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 从古老的仇恨到新的叛乱，
- en: Where civil blood makes civil hands unclean.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，市民的血使市民的手变得不洁。
- en: – Opening lines of *Romeo and Juliet*, by Shakespeare
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ——莎士比亚《罗密欧与朱丽叶》的开篇诗句
- en: Let’s look at a scenario inspired by Shakespeare’s play *Romeo and Juliet*.
    Imagine a town with two warring communities (viz., the Montagues and Capulets).
    They have been enemies for generations. The Montagues are in the minority and
    the Capulets are in the majority in the town. The Montagues are super rich and
    powerful. The Capulets are not that well off. This creates a complex situation
    in the town. There are regular riots in the town because of this rivalry. One
    day, the Montagues win the king’s favor and conspire to eliminate some Capulets
    to bring their numbers down. The idea is that if fewer Capulets are in the town,
    the Montagues will no longer be in the minority. The king agrees to the plan as
    he hopes for peace after its execution. We will use this story in this chapter
    to illustrate various undersampling algorithms.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个受莎士比亚戏剧《罗密欧与朱丽叶》启发的场景。想象一个有两个敌对社区（即，蒙塔古家族和凯普莱特家族）的城镇。他们世世代代都是敌人。蒙塔古家族在城镇中属于少数派，而凯普莱特家族是多数派。蒙塔古家族非常富有和强大。凯普莱特家族并不那么富裕。这给城镇带来了复杂的情况。由于这种竞争，城镇里经常发生暴乱。有一天，蒙塔古家族赢得了国王的青睐，并密谋消灭一些凯普莱特家族成员以减少他们的数量。想法是，如果城镇中的凯普莱特家族成员减少，蒙塔古家族将不再是少数派。国王同意了这个计划，因为他希望在执行后实现和平。我们将在这个章节中使用这个故事来说明各种欠采样算法。
- en: Sometimes, it is not sufficient to oversample the minority class. With oversampling,
    you can run into problems such as overfitting and longer training time. To solve
    these problems and to approach the issue of class imbalance differently, people
    have thought of the opposite of oversampling—that is, **undersampling**. This
    is also often referred to in the literature as **downsampling** or **negative
    downsampling** to denote that the negative class (that is, the majority class)
    is being undersampled.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，仅仅对少数类别进行过采样是不够的。过采样可能会导致过拟合和更长的训练时间。为了解决这些问题，并从不同的角度处理类别不平衡问题，人们想到了过采样的对立面——即**欠采样**。在文献中，这也常被称为**降采样**或**负降采样**，以表示负类别（即多数类别）正在被欠采样。
- en: 'Undersampling techniques reduce the number of samples in the majority class(es).
    This method has two obvious advantages over oversampling:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 欠采样技术减少了多数类别中的样本数量。这种方法与过采样相比有两个明显的优势：
- en: '**The data size remains in check**: Even if data imbalance is not a concern,
    dealing with massive datasets—ranging from terabytes to petabytes—often necessitates
    data reduction for practical training. The sheer volume can make training impractical
    both in terms of time and computational costs. Cloud providers such as Amazon
    Web Services, Microsoft Azure, and Google Cloud charge for compute units in addition
    to storage, making large-scale training expensive. Given that you’re likely to
    use only a fraction of the available training data anyway, it’s crucial to be
    strategic about which data to retain and which to discard. Undersampling becomes
    not just a method for balancing classes but also a cost-effective strategy for
    efficient training, potentially reducing training time from days to hours.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据规模得到控制**：即使数据不平衡不是问题，处理从太字节到拍字节不等的大量数据集通常需要数据降维以进行实际训练。数据量本身就可以使训练在时间和计算成本上变得不切实际。云服务提供商如亚马逊网络服务、微软Azure和谷歌云除了存储费用外，还会对计算单元收费，使得大规模训练变得昂贵。鉴于你很可能只使用可用训练数据的一小部分，因此关于保留哪些数据以及丢弃哪些数据要有战略性地考虑。欠采样不仅是一种平衡类别的手段，而且是一种有效的成本效益策略，可以将训练时间从几天减少到几小时。'
- en: '**There is a smaller chance of overfitting**: By using undersampling techniques,
    the number of majority class instances can be reduced, allowing the model to focus
    more on the minority class instances. This, in turn, improves the model’s ability
    to generalize across both classes. As a result, the model becomes less likely
    to overfit to the majority class and is better equipped to handle new, unseen
    data, thus reducing the likelihood of overfitting. We are going to discuss the
    various undersampling methods in this chapter.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过拟合的可能性更小**：通过使用欠采样技术，可以减少多数类别的实例数量，使模型更多地关注少数类别的实例。这反过来又提高了模型在两个类别之间泛化的能力。因此，模型不太可能对多数类别过拟合，并且更好地准备处理新的、未见过的数据，从而降低过拟合的可能性。我们将在本章中讨论各种欠采样方法。'
- en: '*Figure 3**.1* shows the general idea behind undersampling graphically.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3.1*以图形方式展示了欠采样的基本思想。'
- en: '![](img/B17259_03_01.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_03_01.jpg)'
- en: Figure 3.1 – General idea behind undersampling showing (a) imbalanced data with
    two classes and (b) data after undersampling
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 – 欠采样的基本思想展示：(a)具有两个类别的失衡数据，(b)欠采样后的数据
- en: In *Figure 3**.1(a)*, we show the original data containing many data points
    from the circle class. In *Figure 3**.1(b)*, we show the resampled data after
    removing some data points from the circle class.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图3.1(a)*中，我们展示了包含来自圆形类别的许多数据点的原始数据。在*图3.1(b)*中，我们展示了从圆形类别中移除一些数据点后的重采样数据。
- en: When to avoid undersampling the majority class
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 当何时避免对多数类别进行欠采样
- en: 'Undersampling is not a panacea and may not always work. It depends on the dataset
    and model under consideration:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 欠采样并非万能良药，并不总是有效。它取决于所考虑的数据集和模型：
- en: '**Too little training data for all the classes**: If the dataset is already
    small, undersampling the majority class can lead to a significant loss of information.
    In such cases, it is advisable to try gathering more data or exploring other techniques,
    such as oversampling the minority class to balance the class distribution.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**所有类别的训练数据都太少**：如果数据集本身已经很小，对多数类别进行欠采样可能会导致信息损失很大。在这种情况下，建议尝试收集更多数据或探索其他技术，例如对少数类别进行过采样以平衡类别分布。'
- en: '**Majority class equally important or more important than minority class**:
    In specific scenarios, such as the spam filtering example mentioned in [*Chapter
    1*](B17259_01.xhtml#_idTextAnchor015), *Introduction to Data Imbalance in Machine
    Learning*, it is crucial to maintain high accuracy in identifying the majority
    class instances. In such situations, undersampling the majority class might reduce
    the model’s ability to accurately classify majority class instances, leading to
    a higher false positive rate. Instead, alternative methods, such as cost-sensitive
    learning or adjusting the decision threshold (both of these are discussed in [*Chapter
    5*](B17259_05.xhtml#_idTextAnchor151), *Cost-Sensitive Learning*), can be considered.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多数类同等重要或比少数类更重要**：在特定场景下，例如在[*第一章*](B17259_01.xhtml#_idTextAnchor015)“机器学习中的数据不平衡介绍”中提到的垃圾邮件过滤示例，保持识别多数类实例的高准确性至关重要。在这种情况下，对多数类进行下采样可能会降低模型准确分类多数类实例的能力，导致更高的误报率。相反，可以考虑其他方法，如成本敏感学习或调整决策阈值（这两种方法都在[*第五章*](B17259_05.xhtml#_idTextAnchor151)“成本敏感学习”中讨论过）。'
- en: '**When undersampling harms model performance or causes overfitting of the model**:
    Undersampling the majority class might decrease overall model performance, as
    it discards potentially valuable information. Some of the undersampling methods
    discard the examples near the decision boundary, which can also alter the decision
    boundary. Also, by reducing the size of the majority class, undersampling can
    cause underfitting, where the model becomes too simple to capture the underlying
    trends in the limited training data and performs poorly on new, unseen data. There
    is some risk of overfitting as well when using undersampling techniques if the
    model memorizes the reduced dataset. In such cases, exploring other techniques
    such as ensemble methods ([*Chapter 4*](B17259_04.xhtml#_idTextAnchor120), *Ensemble
    Methods*), hybrid methods that combine oversampling and undersampling (discussed
    toward the end of this chapter), or using different algorithms less prone to overfitting
    might be better.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**当下采样损害模型性能或导致模型过拟合时**：对多数类进行下采样可能会降低整体模型性能，因为它丢弃了可能有价值的信息。一些下采样方法会丢弃决策边界附近的示例，这也会改变决策边界。此外，通过减少多数类的大小，下采样可能导致欠拟合，即模型变得过于简单，无法捕捉有限训练数据中的潜在趋势，在新未见过的数据上表现不佳。在使用下采样技术时，如果模型记住了减少后的数据集，也存在过拟合的风险。在这种情况下，探索其他技术，如集成方法（[*第四章*](B17259_04.xhtml#_idTextAnchor120)“集成方法”中讨论过）、结合过采样和下采样的混合方法（在本章末尾讨论），或使用不太容易过拟合的不同算法可能更好。'
- en: 🚀 Undersampling techniques in production at Meta, Microsoft, and Uber
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 🚀 Meta、Microsoft和Uber在生产中的下采样技术
- en: The main challenge in tasks such as ad click prediction is handling massive
    and imbalanced datasets. For instance, a single day of Facebook ads can contain
    hundreds of millions of instances, with an average **ClickThrough Rate** (**CTR**)
    of just 0.1%. To address this, Meta employs two specialized techniques, as detailed
    in the paper *Practical Lessons from Predicting Clicks on Ads at Facebook* [1].
    The first is uniform subsampling, which uniformly reduces the training data volume
    and has shown that using just 10% of the data results in only a 1% reduction in
    model performance. The second is negative downsampling, which specifically targets
    negative (“no-click”) examples and uses an optimal downsampling rate of 0.025.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在广告点击预测等任务中，主要挑战是处理大规模且不平衡的数据集。例如，Facebook单日广告可能包含数亿个实例，平均点击率（**CTR**）仅为0.1%。为了解决这个问题，Meta在论文《从预测Facebook广告点击中汲取的实用经验》[1]中详细介绍了两种专门的技巧。第一种是均匀下采样，它均匀地减少了训练数据量，并表明使用仅10%的数据只会导致模型性能降低1%。第二种是负样本下采样，它专门针对负样本（“无点击”）示例，并使用最优的下采样率为0.025。
- en: Similarly, Microsoft and Uber have very similar approaches to tackling these
    challenges. To estimate the CTR of sponsored ads on Bing search [2], Microsoft
    uses a 50% negative downsampling rate for non-click cases, effectively halving
    the training time while maintaining similar performance metrics. Uber Eats also
    employs negative downsampling to reduce the training data in order to train models
    that predict whether to send push notifications to customers about new restaurants
    [3]. In addition, they remove the least important features when building the final
    version of the model.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，微软和优步在应对这些挑战方面有非常相似的方法。为了估计必应搜索中赞助广告的点击率[2]，微软对非点击案例使用50%的负样本下采样率，有效地将训练时间减半，同时保持相似的性能指标。优步外卖也采用负样本下采样来减少训练数据，以便训练预测是否向客户发送有关新餐厅推送通知的模型[3]。此外，他们在构建模型最终版本时移除了最不重要的特征。
- en: Let’s look at one of the ways of classifying undersampling methods next.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看分类下采样方法的一种方式。
- en: Fixed versus cleaning undersampling
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 固定方法与清洗方法
- en: Undersampling methods can be divided into two categories based on how data points
    get removed from the majority class. These categories are fixed methods and cleaning
    methods.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 根据从多数类中移除数据点的方式，下采样方法可以分为两类：固定方法和清洗方法。
- en: In **fixed methods**, the number of examples in the majority class is reduced
    to a fixed number. Usually, we reduce the number of majority class samples to
    the size of the minority class. For example, if there are 100 million samples
    in the majority class and 10 million samples in the minority class, you will be
    left with only 10 million samples of both classes after applying the fixed method.
    Some such methods are random undersampling and instance hardness-based undersampling.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在**固定方法**中，多数类的示例数量减少到固定的数量。通常，我们会将多数类的样本数量减少到少数类的规模。例如，如果多数类有1亿个样本，少数类有1000万个样本，应用固定方法后，你将只剩下1000万个两个类的样本。这类方法包括随机下采样和基于实例硬度的下采样。
- en: In **cleaning methods**, the number of samples of the majority class is reduced
    based on some pre-determined criteria, independent of the absolute number of examples.
    Once this criterion is met, the algorithm doesn’t care about the size of the majority
    or minority class.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在**清洗方法**中，基于某些预先确定的准则减少多数类的样本数量，与示例的绝对数量无关。一旦满足这个准则，算法就不关心多数类或少数类的规模。
- en: '*Table 3.1* summarizes the key differences between the two methods in a tabular
    format:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*表3.1* 以表格形式总结了两种方法之间的关键差异：'
- en: '|  | **Fixed** **undersampling methods** | **Cleaning** **undersampling methods**
    |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | **固定** **下采样方法** | **清洗** **下采样方法** |'
- en: '| Key idea | Selects a specific number of majority class instances to remove
    | Identifies and removes noisy, redundant, or misclassified majority class instances
    aiming to improve decision boundaries between classes |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 关键思想 | 选择特定数量的多数类实例进行移除 | 识别并移除噪声、冗余或误分类的多数类实例，旨在改善类之间的决策边界 |'
- en: '| Relationship between instances | Doesn’t consider relationships between instances
    | Evaluates relationships between instances |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 实例之间的关系 | 不考虑实例之间的关系 | 评估实例之间的关系 |'
- en: '| Performance and ease of implementation | Faster and easier to implement |
    Sometimes, may have a better model performance and generalization than fixed undersampling
    methods |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 性能和实现难度 | 实现更快且更容易 | 有时，可能比固定下采样方法有更好的模型性能和泛化能力 |'
- en: '| Examples | Random undersamplingInstance hardness-based undersampling | Tomek
    linksNeighborhood cleaning rule |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 示例 | 随机下采样 | 基于实例硬度的下采样 | 托梅克链接 | 邻域清洗规则 |'
- en: Table 3.1 – Fixed versus cleaning undersampling methods
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.1 – 固定方法与清洗方法下采样
- en: 'Let’s create an imbalanced dataset using the `make_classification` API from
    `sklearn`. We will apply various undersampling techniques throughout this chapter
    to balance this dataset:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`sklearn`的`make_classification` API创建一个不平衡的数据集。我们将在本章中应用各种下采样技术来平衡这个数据集：
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Figure 3**.2* shows what the dataset looks like on a 2D plot. For the complete
    notebook code, please refer to the GitHub repository of this chapter.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3.2* 展示了数据集在二维图上的样子。完整的笔记本代码请参考本章的GitHub仓库。'
- en: '![](img/B17259_03_02.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_03_02.jpg)'
- en: Figure 3.2 – Plotting a dataset with an imbalance ratio of 1:99
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 – 绘制不平衡率为1:99的数据集
- en: Model calibration and threshold adjustment
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 模型校准和阈值调整
- en: After applying undersampling techniques, you may want to recalibrate the model’s
    probability scores. Why? As undersampling alters the original distribution of
    the classes, the model’s confidence estimates are biased [4] and may no longer
    accurately reflect the true likelihood of each class in the real-world scenario.
    Failing to recalibrate can lead to misleading or suboptimal decision-making when
    the model is deployed. Therefore, recalibrating the model’s probability scores
    ensures that the model not only classifies instances correctly but also estimates
    the probabilities in a manner that is consistent with the actual class distribution,
    enhancing its reliability. For a deeper understanding of this process, especially
    how to recalibrate model scores to account for the effects of downsampling, please
    refer to [*Chapter 10*](B17259_10.xhtml#_idTextAnchor279), *Model Calibration*.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用欠采样技术后，你可能需要重新校准模型的概率分数。为什么？因为欠采样改变了原始类别的分布，模型的置信度估计存在偏差[4]，可能不再准确反映现实场景中每个类别的真实可能性。未能重新校准可能导致模型部署时做出误导性或次优的决策。因此，重新校准模型的概率分数确保模型不仅能够正确分类实例，而且以与实际类别分布一致的方式估计概率，从而提高其可靠性。对于更深入理解此过程，特别是如何校准模型分数以考虑下采样的影响，请参阅[*第10章*](B17259_10.xhtml#_idTextAnchor279)，*模型校准*。
- en: In the context of imbalanced datasets, threshold adjustment techniques can be
    a critical complement to undersampling methods. Whether or not we end up applying
    any sampling techniques, adjusting the threshold to determine the correct class
    label can be crucial for correctly interpreting the model’s performance. For a
    more in-depth understanding of various threshold adjustment techniques, you can
    refer to [*Chapter 5*](B17259_05.xhtml#_idTextAnchor151), *Cost-Sensitive Learning*.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在不平衡数据集的背景下，阈值调整技术可以是欠采样方法的临界补充。无论我们最终是否应用任何采样技术，调整阈值以确定正确的类别标签对于正确解释模型性能至关重要。对于更深入理解各种阈值调整技术，你可以参阅[*第5章*](B17259_05.xhtml#_idTextAnchor151)，*成本敏感学习*。
- en: Undersampling approaches
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 欠采样方法
- en: 'Let’s look at a second way to categorize undersampling algorithms. There are
    a few ways the king can eliminate some Capulets:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看第二种对欠采样算法进行分类的方法。国王可以通过几种方式消除一些凯普莱特家族成员：
- en: He can eliminate the Capulets uniformly from the whole town, thereby removing
    a few Capulets from all areas of the town
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他可以从整个城镇均匀地消除凯普莱特家族，从而从城镇的所有地区移除一些凯普莱特家族成员
- en: Alternatively, the king can remove the Capulets who live near the houses of
    the Montagues
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者，国王可以移除住在蒙太古家族房屋附近的凯普莱特家族
- en: Lastly, he can remove the Capulets who live far away from the houses of the
    Montagues
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，他可以移除住在远离蒙太古家族房屋的凯普莱特家族成员
- en: 'These are the three major approaches used in undersampling techniques. We either
    remove the majority samples uniformly, remove the majority samples near the minority
    samples, or remove the majority samples far from the minority samples. We can
    also combine the last two approaches by removing some nearby and some far away
    samples. The following diagram gives the classification of these methods:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是在欠采样技术中使用的三种主要方法。我们要么均匀地移除多数样本，要么移除靠近少数样本的多数样本，要么移除远离少数样本的多数样本。我们也可以通过移除一些靠近的和一些远离的样本来结合后两种方法。以下图表给出了这些方法的分类：
- en: '![](img/B17259_03_03.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_03_03.jpg)'
- en: Figure 3.3 – Categorization of undersampling techniques
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 – 欠采样技术分类
- en: The following figure illustrates the difference between the two criteria. In
    *Figure 3**.4(a)*, we show the original dataset. In *Figure 3**.4(b)*, we show
    the same dataset after removing the examples close to the decision boundary. Notice
    how examples closer to the class boundary are removed.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表说明了两种标准之间的差异。在*图3.4(a)*中，我们展示了原始数据集。在*图3.4(b)*中，我们展示了移除靠近决策边界例子后的相同数据集。注意靠近类别边界的例子是如何被移除的。
- en: Majority class examples far from the minority class may not effectively help
    models establish a decision boundary. Hence, such majority class examples away
    from the decision boundary can be removed. In *Figure 3**.4(c)*, we show the dataset
    after removing examples far away from the boundary. The examples far from the
    decision boundary can be considered easy-to-classify examples.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数类别的例子远离少数类别可能无法有效地帮助模型建立决策边界。因此，这样的远离决策边界的多数类别例子可以被移除。在*图3.4(c)*中，我们展示了移除远离边界例子后的数据集。远离决策边界的例子可以被认为是易于分类的例子。
- en: '![](img/B17259_03_04.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – Difference between two general approaches to undersampling
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Having discussed various ways to classify the various undersampling techniques,
    let’s now look at them in more detail.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Removing examples uniformly
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are two major ways of removing the majority class examples uniformly from
    the data. The first way is to remove the examples randomly, and the other way
    involves using clustering techniques. Let’s discuss both of these methods in detail.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Random UnderSampling
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first technique the king might think of is to pick Capulets randomly and
    remove them from the town. This is a naïve approach. It might work, and the king
    might be able to bring peace to the town. But the king might cause unforeseen
    damage by picking up some influential Capulets. However, it is an excellent place
    to start our discussion. This technique can be considered a close cousin of random
    oversampling. In **Random UnderSampling** (**RUS**), as the name suggests, we
    randomly extract observations from the majority class until the classes are balanced.
    This technique inevitably leads to data loss, might harm the underlying structure
    of the data, and thus performs poorly sometimes.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_03_05.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – Comic explaining the main idea behind the RUS method
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code sample for using RUS:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `sampling_strategy` value can be used to specify the desired ratio of minority
    and majority classes, the default being that they will be made equal in number.
    *Figure 3**.6* shows the application of the `RandomUnderSampler` technique, where
    the right plot shows that most of the negative class samples got dropped:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_03_06.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – Plotting datasets before and after undersampling using RandomUnderSampler
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Next, we transition to a smarter technique that forms groups among majority
    class examples.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: ClusterCentroids
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The second technique the king might follow to carry out uniform undersampling
    is to divide the Capulet population into groups based on location. Then, keep
    one Capulet from each group and remove other Capulets from the group. This method
    of undersampling is called the **ClusterCentroids** method. If there are *N* items
    in the minority class, we create *N* clusters from the points of the majority
    class. For example, this can be done using the K-means algorithm. K-means is a
    clustering algorithm that groups nearby points into different clusters and assigns
    centroids to each group.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_03_07.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – Comic illustrating the main idea behind the ClusterCentroids method
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: In the ClusterCentroids technique, we first apply the K-means algorithm to all
    of the majority class data. Then, for each cluster, we keep the centroid and remove
    all other examples within that cluster. It’s worth noting that the centroid might
    not even be a part of the original data, which is an important aspect of this
    method.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 3**.8*, we show the working of ClusterCentroids. In *Figure 3**.8(a)*,
    we start with an imbalanced dataset. In *Figure 3**.8(b)*, we calculate the centroids
    for the three clusters. These centroids are shown as stars in the diagram. Finally,
    we remove all majority class samples except the centroids from the dataset in
    *Figure 3**.8(c)*.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图3.8*中，我们展示了ClusterCentroids的工作原理。在*图3.8(a)*中，我们从一个不平衡的数据集开始。在*图3.8(b)*中，我们计算了三个聚类的质心。这些质心在图中以星号表示。最后，我们在*图3.8(c)*中从数据集中移除了所有大多数类别的样本，除了质心。
- en: '![](img/B17259_03_08.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_03_08.jpg)'
- en: Figure 3.8 – Illustrating how the ClusterCentroids method works
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8 – 展示ClusterCentroids方法的工作原理
- en: 'Here is the code for using ClusterCentroids:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是使用ClusterCentroids的代码：
- en: '[PRE2]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following is the output:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '[PRE3]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*Figure 3**.9* shows the application of the ClusterCentroids technique, where
    the right plot shows that most of the negative class samples got dropped.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3.9*展示了ClusterCentroids技术的应用，其中右侧的图表显示大多数负类样本被剔除。'
- en: '![](img/B17259_03_09.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_03_09.jpg)'
- en: Figure 3.9 – Plotting datasets before and after undersampling using ClusterCentroids
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9 – 使用ClusterCentroids在降采样前后绘制数据集
- en: One thing to note is that ClusterCentroids can be computationally expensive
    because it uses the K-means algorithm by default, which can be slow. We recommend
    exploring various parameters in the ClusterCentroids method, such as the estimator,
    which specifies the clustering method to be used. For example, K-means can be
    replaced with MiniBatchKMeans, a faster variant of the K-means clustering algorithm.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一点是，ClusterCentroids可能计算成本较高，因为它默认使用K-means算法，这可能会很慢。我们建议探索ClusterCentroids方法中的各种参数，例如estimator，它指定了要使用的聚类方法。例如，K-means可以被MiniBatchKMeans替换，这是K-means聚类算法的一个更快变体。
- en: In the following section, we will attempt to eliminate the majority class examples
    in a more strategic manner.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将尝试以更战略性的方式消除大多数类别的示例。
- en: Strategies for removing noisy observations
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 移除噪声观测值的策略
- en: The king might decide to look at the friendships and locations of the citizens
    before removing anyone. The king might decide to remove the Capulets who are rich
    and live near the Montagues. This could bring peace to the city by separating
    the feuding clans. Let’s look at some strategies to do that with our data.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 国王可能会在剔除任何人之前查看公民的友谊和位置。国王可能会决定剔除那些富有且住在蒙太古家族附近的凯普莱特家族。这可以通过分离争斗的家族来为城市带来和平。让我们看看一些使用我们的数据来实现这一点的策略。
- en: ENN, RENN, and AllKNN
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ENN、RENN和AllKNN
- en: The king can remove the Capulets based on their neighbors. For example, if one
    or more of the three closest neighbors of a Capulet is a Montague, the king can
    remove the Capulet. This technique is called `imbalanced-learn` library gives
    us options to decide which classes we would like to resample and what kind of
    class arrangement the neighbors of the sample should have.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 国王可以根据邻居来剔除凯普莱特家族。例如，如果一个或多个凯普莱特家族最近三个邻居中的一个是蒙太古家族，国王就可以剔除凯普莱特家族。这种技术称为`imbalanced-learn`库为我们提供了选择我们想要重采样的类别以及样本邻居应该具有的类别排列方式。
- en: 'There are two different criteria that we can follow for excluding the samples:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以遵循两种不同的标准来排除样本：
- en: We can choose to exclude samples whose one or more neighbors are not from the
    same class as themselves
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以选择排除一个或多个邻居不属于自身相同类别的样本
- en: We can decide to exclude samples whose majority of neighbors are not from the
    same class as themselves
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以决定排除大多数邻居不属于自身相同类别的样本
- en: In *Figure 3**.10*, we show the working of the ENN algorithm. Here, we remove
    the majority samples that have one or more minority neighbors. In *Figure 3**.10(a)*,
    we show the original dataset. In *Figure 3**.10(b)*, we highlight the majority
    class samples that have one or more minority class nearest neighbors. The highlighted
    majority class samples are shown as solid boxes, and their neighbors are shown
    by creating curves around them.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图3.10*中，我们展示了ENN算法的工作原理。在这里，我们移除了具有一个或多个少数邻居的大多数样本。在*图3.10(a)*中，我们展示了原始数据集。在*图3.10(b)*中，我们突出显示了具有一个或多个少数类最近邻的大多数类样本。突出显示的大多数类样本以实心框表示，它们的邻居通过围绕它们创建曲线来表示。
- en: '![](img/B17259_03_10.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_03_10.jpg)'
- en: Figure 3.10 – Illustrating how the ENN method works
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10 – 展示ENN方法的工作原理
- en: 'Here is the code for using ENN:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是使用ENN的代码：
- en: '[PRE4]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The following is the output:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '[PRE5]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](img/B17259_03_11.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_03_11.jpg)'
- en: Figure 3.11 – Plotting datasets before and after undersampling using ENN
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11 – 使用ENN进行下采样前后的数据集绘图
- en: Here, `n_neighbors` is the size of the neighborhood to consider to compute the
    nearest neighbors.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`n_neighbors`是要考虑的邻域大小，用于计算最近邻。
- en: 'There are two variants of ENN that we won’t dive into, but you can explore
    them if you are interested: `imblearn.under_sampling.RepeatedEditedNearestNeighbours`)
    and `imblearn.under_sampling.AllKNN`). In RENN [6], we repeat the process followed
    in ENN until there are no more examples that can be removed or the maximum number
    of cycle counts has been reached. This algorithm also removes the noisy data.
    It is stronger in removing the boundary examples as the algorithm is repeated
    several times (*Figure 3**.12*).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种ENN变体我们不会深入探讨，但如果你感兴趣，可以探索它们：`imblearn.under_sampling.RepeatedEditedNearestNeighbours`)和`imblearn.under_sampling.AllKNN`)。在RENN
    [6]中，我们重复在ENN中遵循的过程，直到没有更多可以删除的示例或达到最大循环计数。此算法也移除噪声数据。由于算法重复多次，它在移除边界示例方面更强（*图3**.12*）。
- en: '![](img/B17259_03_12.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_03_12.jpg)'
- en: Figure 3.12 – Plotting datasets before and after undersampling using RENN
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12 – 使用RENN进行下采样前后的数据集绘图
- en: In the **AllKNN** method [6], we repeat **ENN** but with the number of neighbors
    going from 1 to K.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在**AllKNN**方法[6]中，我们重复**ENN**，但邻居的数量从1增加到K。
- en: Tomek links
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Tomek链接
- en: In 1976, Ivan Tomek proposed the idea of **Tomek links** [7]. Two examples are
    said to form Tomek links if they belong to two different classes, and there is
    no third point with a shorter distance to them than the distance between the two
    points. The intuition behind Tomek links is that “*if two points are from different
    classes, they should not be nearest to each other.*” These points are part of
    the noise, and we can eliminate the majority member or both points to reduce noise.
    This is as if the king decides to remove the Capulets whose best friends are Montagues.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 1976年，伊万·托梅克提出了**Tomek链接**的概念[7]。如果两个例子属于两个不同的类别，并且没有第三个点与它们的距离比两个点之间的距离短，则称这两个例子形成Tomek链接。Tomek链接背后的直觉是“*如果两个点来自不同的类别，它们不应该彼此最近。*”这些点是噪声的一部分，我们可以消除多数成员或两个点以减少噪声。这就像国王决定移除那些最好的朋友是蒙太古家族的凯普莱特家族成员。
- en: 'We can use the `TomekLinks` API as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以这样使用`TomekLinks` API：
- en: '[PRE6]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following is the output:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的输出是：
- en: '[PRE7]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](img/B17259_03_13.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_03_13.jpg)'
- en: Figure 3.13 – Plotting datasets before and after undersampling using TomekLinks
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.13 – 使用TomekLinks进行下采样前后的数据集绘图
- en: '*Figure 3**.14* shows the working of the Tomek links algorithm. In *Figure
    3**.14(a)*, we have the original dataset. In *Figure 3**.14(b),* we find and highlight
    the Tomek links. Notice that the points in these links are close to each other.
    In *Figure 3**.14(c)*, we show the dataset after removing the majority class samples
    (depicted as circles) that belong to the Tomek links. Notice the two circles present
    in part *(b)* but missing in part *(c)*. Similarly, we show the dataset after
    removing all the points in Tomek links in part *(d)* of the diagram.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3**.14*展示了Tomek链接算法的工作原理。在*图3**.14(a)*中，我们有原始数据集。在*图3**.14(b)*中，我们找到并突出显示Tomek链接。注意这些链接中的点彼此很近。在*图3**.14(c)*中，我们展示了移除属于Tomek链接的多数类样本（以圆圈表示）后的数据集。注意在部分*(b)*中出现的两个圆圈但在部分*(c)*中缺失。同样，我们在图例的部分*(d)*中展示了移除Tomek链接中所有点后的数据集。'
- en: '![](img/B17259_03_14.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_03_14.jpg)'
- en: Figure 3.14 – Illustrating how the TomekLinks algorithm works
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.14 – 展示TomekLinks算法的工作原理
- en: Tomek links is a resource-intensive method due to its requirement of calculating
    pairwise distances between all examples. As stated in *A Study of the Behavior
    of Several Methods for Balancing Machine Learning Training Data* [8], performing
    this process on a reduced dataset would be more computationally efficient when
    dealing with large amounts of data.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其要求计算所有示例之间的成对距离，Tomek链接是一种资源密集型方法。正如在*《几种平衡机器学习训练数据行为的研究》*[8]中所述，在处理大量数据时，在减少的数据集上执行此过程将更具有计算效率。
- en: In the next method, we will try to remove majority class examples from the perspective
    of minority class examples. Can we remove the nearest neighbors of the minority
    class that belong to the majority class?
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个方法中，我们将尝试从少数类示例的角度尝试移除多数类示例。我们能否移除属于多数类的少数类示例的最近邻？
- en: Neighborhood Cleaning Rule
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 社区清洁规则
- en: Apart from removing the Capulets whose one or more nearest neighbors are Montagues,
    the king might decide to look at the nearest neighbors of Montagues and remove
    the Capulets who might come up as one of the nearest neighbors for a Montague.
    In the **Neighborhood Cleaning Rule** (**NCR**) [9], we apply an ENN algorithm,
    train a KNN on the remaining data, and then remove all the majority class samples
    that are the nearest neighbors of a minority sample.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code for using `NeighourhoodCleaningRule`:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following is the output:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](img/B17259_03_15.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
- en: Figure 3.15 – Plotting datasets before and after undersampling using NCR
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Instance hardness threshold
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The king might ask a minister, “*Which Capulets have mixed well with Montagues?*”
    The minister, based on their knowledge of the town, will give a list of those
    Capulets. Then, the king will remove the Capulets whose names are on the list.
    This method of using another model to identify noisy samples is known as the **instance
    hardness threshold**. In this method, we train a classification model on the data,
    such as a decision tree, random forest, or linear SVM.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: In addition to predicting the class of an instance, these classifiers can return
    their class probabilities. Class probabilities show the confidence the model has
    in classifying the instances. With the instance hardness threshold method [10],
    we remove the majority class samples that received low probability estimates (referred
    to as the “hard instances”). These instances are considered “hard to classify”
    due to class overlap, a principal contributor to instance hardness.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'The `imbalanced-learn` library provides an API for utilizing `InstanceHardnessThreshold`,
    where we can specify the estimator used to estimate the hardness of the examples.
    In this case, we use `LogisticRegression` as the estimator:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following is the output:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](img/B17259_03_16.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
- en: Figure 3.16 – Plotting datasets before and after undersampling using InstanceHardnessThreshold
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Since classification models need to draw a decision boundary between the majority
    and minority classes, the majority class examples that are too far away from the
    minority class examples may not help the model decide this decision boundary.
    Considering this, we will look at methods in the next section that will remove
    such easy majority class examples.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Strategies for removing easy observations
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The reverse of the strategy to remove the rich and famous Capulets is to remove
    the poor and weak Capulets. This section will discuss the techniques for **removing
    the majority samples far away from the minority samples**. Instead of removing
    the samples from the boundary between the two classes, we use them for training
    a model. This way, we can train a model to better discriminate between the classes.
    However, one downside is that these algorithms risk retaining noisy data points,
    which could then be used to train the model, potentially introducing noise into
    the predictive system.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Condensed Nearest Neighbors
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Condensed Nearest Neighbors** (**CNNeighbors**) [11] is an algorithm that
    works as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: We add all minority samples to a set and one randomly selected majority sample.
    Let’s call this set `C`.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We train a KNN model with *k = 1* on set `C`.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we repeat the following four steps for each of the remaining majority
    samples:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We consider one majority sample; let’s call it `e`.
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: We try to predict the class of `e` using KNN.
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If the predicted class matches the original class, we remove the sample. The
    intuition is that there is little to learn from `e` as even a *1-NN* classifier
    can learn it.
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Otherwise, we add the sample to our set `C` and train the *1-NN* on `C` again.
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: This method removes the easy-to-classify samples from the majority class.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'The code to use `CondensedNearestNeighbour` is as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The following is the output:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](img/B17259_03_17.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
- en: Figure 3.17 – Plotting datasets before and after undersampling using CNNeighbors
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: However, the CNNeighbors method can be computationally expensive, as it evaluates
    each majority class example using the KNN algorithm. This makes the CNNeighbors
    method unsuitable for big data applications.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: One-sided selection
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The king might decide to remove some rich and many poor Capulets. This way,
    only the middle-class Capulets will stay in the town. In one-sided selection [12],
    we do just that. This method is a combination of CNNeighbors and Tomek links.
    We first resample using a CNNeighbors. Then, we remove the Tomek links from the
    resampled data. It reduces both noisy and easy-to-identify samples.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code for `OneSidedSelection`. When we don’t provide the `n_neighbors`
    parameter, the default value of `1` is taken:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following is the output:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](img/B17259_03_18.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
- en: Figure 3.18 – Plotting datasets before and after undersampling using OneSidedSelection
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Here, `n_seeds_S` is the number of minority class samples used as seeds in the
    method, and it can significantly impact the method’s performance. It is advisable
    to tune this parameter.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Combining undersampling and oversampling
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You might wonder whether we can combine undersampling techniques with oversampling
    techniques to produce even better results. The answer is yes. Oversampling methods
    increase the number of samples of the minority class but also usually increase
    the noise in the data. Some undersampling techniques can help us remove the noise,
    for example, ENN, Tomek links, NCR, and instance hardness. We can combine these
    methods with SMOTE to produce good results. The combination of SMOTE with ENN
    [13] and Tomek links [14] has been well researched. Also, the `imbalanced-learn`
    library supports both of them: `SMOTEENN` and `SMOTETomek`.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Model performance comparison
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s explore how some popular models perform using the various undersampling
    techniques we’ve discussed. We use two datasets for this comparison: one synthetic
    dataset and one real-world dataset called `thyroid_sick` from the `imbalanced-learn`
    library. We’ll evaluate the performance of 11 different undersampling techniques
    against a baseline of no sampling, using both logistic regression and random forest
    models. *Figures 3.19* to *3.22* show the average precision values for models
    trained using these various methods.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: You can find the notebook in the GitHub repository of the chapter.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_03_19.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
- en: Figure 3.19 – Average precision when using various methods on the thyroid_sick
    dataset using random forest
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_03_20.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: Figure 3.20 – Average precision when using various methods on synthetic data
    using random forest
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_03_21.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
- en: Figure 3.21 – Average precision when using various methods on the thyroid_sick
    dataset using logistic regression
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_03_22.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: Figure 3.22 – Average precision when using various methods on synthetic data
    using logistic regression
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some more observations:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: The effectiveness of undersampling techniques can vary significantly depending
    on the dataset and its characteristics
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No single technique dominates across all datasets, emphasizing the need for
    empirical testing to choose the best method for your specific problem
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, which method will work best for your data? There is no easy answer to this
    question. The key here is to develop an intuition about the inner workings of
    these methods and have a pipeline that can help you test different techniques.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: However, certain techniques can be time-consuming. In our testing on a dataset
    with a million examples, methods such as `CondensedNearestNeighbor`, `ClusterCentroids`,
    and `ALLKNN` took longer than others. If you’re dealing with large amounts of
    data, planning to scale in the future, or are pressed for time, you may want to
    avoid these methods or tune their parameters. Techniques such as `RandomUnderSampler`
    and `InstanceHardnessThreshold` are more suitable for rapid iterative development.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: That brings us to the end of this chapter.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed undersampling, an approach to address the class
    imbalance in datasets by reducing the number of samples in the majority class.
    We reviewed the advantages of undersampling, such as keeping the data size in
    check and reducing the chances of overfitting. Undersampling methods can be categorized
    into fixed methods, which reduce the number of majority class samples to a fixed
    size, and cleaning methods, which reduce majority class samples based on predetermined
    criteria.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: We went over various undersampling techniques, including random undersampling,
    instance hardness-based undersampling, ClusterCentroids, ENN, Tomek links, NCR,
    instance hardness, CNNeighbors, one-sided selection, and combinations of undersampling
    and oversampling techniques, such as `SMOTEENN` and `SMOTETomek`.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: We concluded with a performance comparison of various undersampling techniques
    from the `imbalanced-learn` library on logistic regression and random forest models,
    using a few datasets, and benchmarked their performance and effectiveness.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Once you identify that your dataset is imbalanced and could potentially benefit
    from applying undersampling techniques, go ahead and experiment with the various
    methods discussed in this chapter. Evaluate their effectiveness using the appropriate
    metrics, such as PR-AUC, to find the most suitable approach for improving your
    model’s performance.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will go over various ensemble-based techniques.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Explore the various undersampling APIs available from the `imbalanced-learn`
    library at [https://imbalanced-learn.org/stable/references/under_sampling.html](https://imbalanced-learn.org/stable/references/under_sampling.html).
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explore the `NearMiss` undersampling technique, available through the `imblearn.under_sampling.NearMiss`
    API. Which class of methods does it belong to? Apply the `NearMiss` method to
    the dataset that we used in the chapter.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try all the undersampling methods discussed in this chapter on the `us_crime`
    dataset from UCI. You can find this dataset in the `fetch_datasets` API of the
    `imbalanced-learn` library. Find the undersampling method with the highest `f1-score`
    metric for `LogisticRegression` and `XGBoost` models.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Can you identify an undersampling method of your own? (Hint: think about combining
    the various approaches to undersampling in new ways.)'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'X. He et al., “*Practical Lessons from Predicting Clicks on Ads at Facebook*,”
    in Proceedings of the Eighth International Workshop on Data Mining for Online
    Advertising, New York NY USA: ACM, Aug. 2014, pp. 1–9\. doi: 10.1145/2648584.2648589.'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'X. Ling, W. Deng, C. Gu, H. Zhou, C. Li, and F. Sun, “*Model Ensemble for Click
    Prediction in Bing Search Ads*,” in Proceedings of the 26th International Conference
    on World Wide Web Companion - WWW ’17 Companion, Perth, Australia: ACM Press,
    2017, pp. 689–698\. doi: 10.1145/3041021.3054192.'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*How Uber Optimizes the Timing of Push Notifications using ML and Linear* *Programming*:
    [https://www.uber.com/blog/how-uber-optimizes-push-notifications-using-ml/](https://www.uber.com/blog/how-uber-optimizes-push-notifications-using-ml/).'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A. D. Pozzolo, O. Caelen, R. A. Johnson, and G. Bontempi, “*Calibrating Probability
    with Undersampling for Unbalanced Classification*,” in 2015 IEEE Symposium Series
    on Computational Intelligence, Cape Town, South Africa: IEEE, Dec. 2015, pp. 159–166\.
    doi: 10.1109/SSCI.2015.33.'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '(Introducing the ENN method) D. L. Wilson, “*Asymptotic Properties of Nearest
    Neighbor Rules Using Edited Data*,” IEEE Trans. Syst., Man, Cybern., vol. SMC-2,
    no. 3, pp. 408–421, Jul. 1972, doi: 10.1109/TSMC.1972.4309137.'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '(Introducing the RENN and AllKNN methods) “*An Experiment with the Edited Nearest-Neighbor
    Rule*,” IEEE Trans. Syst., Man, Cybern., vol. SMC-6, no. 6, pp. 448–452, Jun.
    1976, doi: 10.1109/TSMC.1976.4309523.'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'I. Tomek, “*Two Modifications of CNN*,” IEEE Trans. Syst., Man, Cybern., vol.
    SMC-6, no. 11, pp. 769–772, Nov. 1976, doi: 10.1109/TSMC.1976.4309452.'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'G. E. A. P. A. Batista, R. C. Prati, and M. C. Monard, “*A study of the behavior
    of several methods for balancing machine learning training data*,” SIGKDD Explor.
    Newsl., vol. 6, no. 1, pp. 20–29, Jun. 2004, doi: 10.1145/1007730.1007735.'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '(Introducing the neighborhood cleaning rule method) J. Laurikkala, “*Improving
    Identification of Difficult Small Classes by Balancing Class Distribution*,” in
    Artificial Intelligence in Medicine, S. Quaglini, P. Barahona, and S. Andreassen,
    Eds., in Lecture Notes in Computer Science, vol. 2101\. Berlin, Heidelberg: Springer
    Berlin Heidelberg, 2001, pp. 63–66\. doi: 10.1007/3-540-48229-6_9.'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '(Introducing the instance hardness threshold technique) M. R. Smith, T. Martinez,
    and C. Giraud-Carrier, “*An instance level analysis of data complexity*,” Mach
    Learn, vol. 95, no. 2, pp. 225–256, May 2014, doi: 10.1007/s10994-013-5422-z.'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'P. Hart, “*The condensed nearest neighbor rule (corresp.)*,” IEEE transactions
    on information theory, vol. 14, no. 3, pp. 515–516, 1968, https://citeseerx.ist.psu.edu/document?     repid=rep1&type=pdf&doi=7c3771fd6829630cf450af853 df728ecd8da4ab2.'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (Introducing the one-sided selection method) M. Kubat and S. Matwin, “*Addressing
    The Curse Of Imbalanced Training Sets:* *One-sided Selection*”.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (Application of `SMOTEENN` and `SMOTETomek` methods) Gustavo EAPA Batista, Ronaldo
    C Prati, and Maria Carolina Monard. *A study of the behavior of several methods
    for balancing machine learning training data*. ACM SIGKDD explorations newsletter,
    6(1):20–29, 2004.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '(Application of the `SMOTETomek` method) Gustavo EAPA Batista, Ana LC Bazzan,
    and Maria Carolina Monard. *Balancing training data for automated annotation of
    keywords: a case study*. In WOB, 10–18\. 2003.'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
