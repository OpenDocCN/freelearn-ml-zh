- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Undersampling Methods
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¬ é‡‡æ ·æ–¹æ³•
- en: Sometimes, you have so much data that adding more data by oversampling only
    makes things worse. Donâ€™t worry, as we have a strategy for those situations as
    well. Itâ€™s called undersampling, or downsampling. In this chapter, you will learn
    about the concept of undersampling, including when to use it and the various techniques
    to perform it. You will also see how to use these techniques via the `imbalanced-learn`
    library APIs and compare their performance with some classical machine learning
    models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰æ—¶å€™ï¼Œä½ æ‹¥æœ‰å¦‚æ­¤å¤šçš„æ•°æ®ï¼Œé€šè¿‡è¿‡é‡‡æ ·æ·»åŠ æ›´å¤šæ•°æ®åªä¼šä½¿äº‹æƒ…å˜å¾—æ›´ç³Ÿã€‚åˆ«æ‹…å¿ƒï¼Œæˆ‘ä»¬ä¹Ÿæœ‰é’ˆå¯¹è¿™äº›æƒ…å†µçš„ç­–ç•¥ã€‚è¿™è¢«ç§°ä¸ºæ¬ é‡‡æ ·æˆ–é™é‡‡æ ·ã€‚åœ¨æœ¬ç« ä¸­ï¼Œä½ å°†äº†è§£æ¬ é‡‡æ ·çš„æ¦‚å¿µï¼ŒåŒ…æ‹¬ä½•æ—¶ä½¿ç”¨å®ƒä»¥åŠæ‰§è¡Œå®ƒçš„å„ç§æŠ€æœ¯ã€‚ä½ è¿˜å°†çœ‹åˆ°å¦‚ä½•é€šè¿‡`imbalanced-learn`åº“çš„APIä½¿ç”¨è¿™äº›æŠ€æœ¯ï¼Œå¹¶å°†å®ƒä»¬çš„æ€§èƒ½ä¸ä¸€äº›ç»å…¸çš„æœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« å°†æ¶µç›–ä»¥ä¸‹ä¸»é¢˜ï¼š
- en: Introducing undersampling
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»‹ç»æ¬ é‡‡æ ·
- en: When to avoid undersampling in the majority class
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨å¤šæ•°ç±»ä¸­ä½•æ—¶é¿å…æ¬ é‡‡æ ·
- en: Removing examples uniformly
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‡åŒ€åœ°ç§»é™¤ç¤ºä¾‹
- en: Strategies for removing noisy observations
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç§»é™¤å™ªå£°è§‚å¯Ÿçš„ç­–ç•¥
- en: Strategies for removing easy observations
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç§»é™¤ç®€å•è§‚å¯Ÿçš„ç­–ç•¥
- en: By the end of this chapter, youâ€™ll have mastered various undersampling techniques
    for imbalanced datasets and will be able to confidently apply them with the `imbalanced-learn`
    library to build better machine learning models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°æœ¬ç« ç»“æŸæ—¶ï¼Œä½ å°†æŒæ¡å„ç§ç”¨äºä¸å¹³è¡¡æ•°æ®é›†çš„æ¬ é‡‡æ ·æŠ€æœ¯ï¼Œå¹¶èƒ½å¤Ÿè‡ªä¿¡åœ°ä½¿ç”¨`imbalanced-learn`åº“æ¥æ„å»ºæ›´å¥½çš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æŠ€æœ¯è¦æ±‚
- en: 'This chapter will make use of common libraries such as `matplotlib`, `seaborn`,
    `pandas`, `numpy`, `scikit-learn`, and `imbalanced-learn`. The code and notebooks
    for this chapter can be found on GitHub at [https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter03](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter03).
    To run the notebook, there are two options: you can click the **Open in Colab**
    icon at the top of the chapterâ€™s notebook, or you can launch it directly from
    [https://colab.research.google.com](https://colab.research.google.com) using the
    GitHub URL of the notebook.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« å°†ä½¿ç”¨å¸¸è§çš„åº“ï¼Œå¦‚`matplotlib`ã€`seaborn`ã€`pandas`ã€`numpy`ã€`scikit-learn`å’Œ`imbalanced-learn`ã€‚æœ¬ç« çš„ä»£ç å’Œç¬”è®°æœ¬å¯ä»¥åœ¨GitHubä¸Šæ‰¾åˆ°ï¼Œç½‘å€ä¸º[https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter03](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter03)ã€‚è¦è¿è¡Œç¬”è®°æœ¬ï¼Œæœ‰ä¸¤ç§é€‰æ‹©ï¼šä½ å¯ä»¥ç‚¹å‡»ç« èŠ‚ç¬”è®°æœ¬é¡¶éƒ¨çš„**åœ¨Colabä¸­æ‰“å¼€**å›¾æ ‡ï¼Œæˆ–è€…ä½ å¯ä»¥ç›´æ¥ä»[https://colab.research.google.com](https://colab.research.google.com)ä½¿ç”¨ç¬”è®°æœ¬çš„GitHub
    URLå¯åŠ¨å®ƒã€‚
- en: Introducing undersampling
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»‹ç»æ¬ é‡‡æ ·
- en: Two households, both alike in dignity,
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¤ä¸ªå®¶åº­ï¼ŒåŒæ ·æœ‰å°Šä¸¥ï¼Œ
- en: In fair Verona, where we lay our scene,
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å…¬å¹³çš„ç»´ç½—çº³ï¼Œæˆ‘ä»¬è®¾æ™¯ä¹‹å¤„ï¼Œ
- en: From ancient grudge break to new mutiny,
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å¤è€çš„ä»‡æ¨åˆ°æ–°çš„å›ä¹±ï¼Œ
- en: Where civil blood makes civil hands unclean.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œå¸‚æ°‘çš„è¡€ä½¿å¸‚æ°‘çš„æ‰‹å˜å¾—ä¸æ´ã€‚
- en: â€“ Opening lines of *Romeo and Juliet*, by Shakespeare
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: â€”â€”èå£«æ¯”äºšã€Šç½—å¯†æ¬§ä¸æœ±ä¸½å¶ã€‹çš„å¼€ç¯‡è¯—å¥
- en: Letâ€™s look at a scenario inspired by Shakespeareâ€™s play *Romeo and Juliet*.
    Imagine a town with two warring communities (viz., the Montagues and Capulets).
    They have been enemies for generations. The Montagues are in the minority and
    the Capulets are in the majority in the town. The Montagues are super rich and
    powerful. The Capulets are not that well off. This creates a complex situation
    in the town. There are regular riots in the town because of this rivalry. One
    day, the Montagues win the kingâ€™s favor and conspire to eliminate some Capulets
    to bring their numbers down. The idea is that if fewer Capulets are in the town,
    the Montagues will no longer be in the minority. The king agrees to the plan as
    he hopes for peace after its execution. We will use this story in this chapter
    to illustrate various undersampling algorithms.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹ä¸€ä¸ªå—èå£«æ¯”äºšæˆå‰§ã€Šç½—å¯†æ¬§ä¸æœ±ä¸½å¶ã€‹å¯å‘çš„åœºæ™¯ã€‚æƒ³è±¡ä¸€ä¸ªæœ‰ä¸¤ä¸ªæ•Œå¯¹ç¤¾åŒºï¼ˆå³ï¼Œè’™å¡”å¤å®¶æ—å’Œå‡¯æ™®è±ç‰¹å®¶æ—ï¼‰çš„åŸé•‡ã€‚ä»–ä»¬ä¸–ä¸–ä»£ä»£éƒ½æ˜¯æ•Œäººã€‚è’™å¡”å¤å®¶æ—åœ¨åŸé•‡ä¸­å±äºå°‘æ•°æ´¾ï¼Œè€Œå‡¯æ™®è±ç‰¹å®¶æ—æ˜¯å¤šæ•°æ´¾ã€‚è’™å¡”å¤å®¶æ—éå¸¸å¯Œæœ‰å’Œå¼ºå¤§ã€‚å‡¯æ™®è±ç‰¹å®¶æ—å¹¶ä¸é‚£ä¹ˆå¯Œè£•ã€‚è¿™ç»™åŸé•‡å¸¦æ¥äº†å¤æ‚çš„æƒ…å†µã€‚ç”±äºè¿™ç§ç«äº‰ï¼ŒåŸé•‡é‡Œç»å¸¸å‘ç”Ÿæš´ä¹±ã€‚æœ‰ä¸€å¤©ï¼Œè’™å¡”å¤å®¶æ—èµ¢å¾—äº†å›½ç‹çš„é’çï¼Œå¹¶å¯†è°‹æ¶ˆç­ä¸€äº›å‡¯æ™®è±ç‰¹å®¶æ—æˆå‘˜ä»¥å‡å°‘ä»–ä»¬çš„æ•°é‡ã€‚æƒ³æ³•æ˜¯ï¼Œå¦‚æœåŸé•‡ä¸­çš„å‡¯æ™®è±ç‰¹å®¶æ—æˆå‘˜å‡å°‘ï¼Œè’™å¡”å¤å®¶æ—å°†ä¸å†æ˜¯å°‘æ•°æ´¾ã€‚å›½ç‹åŒæ„äº†è¿™ä¸ªè®¡åˆ’ï¼Œå› ä¸ºä»–å¸Œæœ›åœ¨æ‰§è¡Œåå®ç°å’Œå¹³ã€‚æˆ‘ä»¬å°†åœ¨è¿™ä¸ªç« èŠ‚ä¸­ä½¿ç”¨è¿™ä¸ªæ•…äº‹æ¥è¯´æ˜å„ç§æ¬ é‡‡æ ·ç®—æ³•ã€‚
- en: Sometimes, it is not sufficient to oversample the minority class. With oversampling,
    you can run into problems such as overfitting and longer training time. To solve
    these problems and to approach the issue of class imbalance differently, people
    have thought of the opposite of oversamplingâ€”that is, **undersampling**. This
    is also often referred to in the literature as **downsampling** or **negative
    downsampling** to denote that the negative class (that is, the majority class)
    is being undersampled.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰æ—¶å€™ï¼Œä»…ä»…å¯¹å°‘æ•°ç±»åˆ«è¿›è¡Œè¿‡é‡‡æ ·æ˜¯ä¸å¤Ÿçš„ã€‚è¿‡é‡‡æ ·å¯èƒ½ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆå’Œæ›´é•¿çš„è®­ç»ƒæ—¶é—´ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œå¹¶ä»ä¸åŒçš„è§’åº¦å¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œäººä»¬æƒ³åˆ°äº†è¿‡é‡‡æ ·çš„å¯¹ç«‹é¢â€”â€”å³**æ¬ é‡‡æ ·**ã€‚åœ¨æ–‡çŒ®ä¸­ï¼Œè¿™ä¹Ÿå¸¸è¢«ç§°ä¸º**é™é‡‡æ ·**æˆ–**è´Ÿé™é‡‡æ ·**ï¼Œä»¥è¡¨ç¤ºè´Ÿç±»åˆ«ï¼ˆå³å¤šæ•°ç±»åˆ«ï¼‰æ­£åœ¨è¢«æ¬ é‡‡æ ·ã€‚
- en: 'Undersampling techniques reduce the number of samples in the majority class(es).
    This method has two obvious advantages over oversampling:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ¬ é‡‡æ ·æŠ€æœ¯å‡å°‘äº†å¤šæ•°ç±»åˆ«ä¸­çš„æ ·æœ¬æ•°é‡ã€‚è¿™ç§æ–¹æ³•ä¸è¿‡é‡‡æ ·ç›¸æ¯”æœ‰ä¸¤ä¸ªæ˜æ˜¾çš„ä¼˜åŠ¿ï¼š
- en: '**The data size remains in check**: Even if data imbalance is not a concern,
    dealing with massive datasetsâ€”ranging from terabytes to petabytesâ€”often necessitates
    data reduction for practical training. The sheer volume can make training impractical
    both in terms of time and computational costs. Cloud providers such as Amazon
    Web Services, Microsoft Azure, and Google Cloud charge for compute units in addition
    to storage, making large-scale training expensive. Given that youâ€™re likely to
    use only a fraction of the available training data anyway, itâ€™s crucial to be
    strategic about which data to retain and which to discard. Undersampling becomes
    not just a method for balancing classes but also a cost-effective strategy for
    efficient training, potentially reducing training time from days to hours.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ•°æ®è§„æ¨¡å¾—åˆ°æ§åˆ¶**ï¼šå³ä½¿æ•°æ®ä¸å¹³è¡¡ä¸æ˜¯é—®é¢˜ï¼Œå¤„ç†ä»å¤ªå­—èŠ‚åˆ°æ‹å­—èŠ‚ä¸ç­‰çš„å¤§é‡æ•°æ®é›†é€šå¸¸éœ€è¦æ•°æ®é™ç»´ä»¥è¿›è¡Œå®é™…è®­ç»ƒã€‚æ•°æ®é‡æœ¬èº«å°±å¯ä»¥ä½¿è®­ç»ƒåœ¨æ—¶é—´å’Œè®¡ç®—æˆæœ¬ä¸Šå˜å¾—ä¸åˆ‡å®é™…ã€‚äº‘æœåŠ¡æä¾›å•†å¦‚äºšé©¬é€Šç½‘ç»œæœåŠ¡ã€å¾®è½¯Azureå’Œè°·æ­Œäº‘é™¤äº†å­˜å‚¨è´¹ç”¨å¤–ï¼Œè¿˜ä¼šå¯¹è®¡ç®—å•å…ƒæ”¶è´¹ï¼Œä½¿å¾—å¤§è§„æ¨¡è®­ç»ƒå˜å¾—æ˜‚è´µã€‚é‰´äºä½ å¾ˆå¯èƒ½åªä½¿ç”¨å¯ç”¨è®­ç»ƒæ•°æ®çš„ä¸€å°éƒ¨åˆ†ï¼Œå› æ­¤å…³äºä¿ç•™å“ªäº›æ•°æ®ä»¥åŠä¸¢å¼ƒå“ªäº›æ•°æ®è¦æœ‰æˆ˜ç•¥æ€§åœ°è€ƒè™‘ã€‚æ¬ é‡‡æ ·ä¸ä»…æ˜¯ä¸€ç§å¹³è¡¡ç±»åˆ«çš„æ‰‹æ®µï¼Œè€Œä¸”æ˜¯ä¸€ç§æœ‰æ•ˆçš„æˆæœ¬æ•ˆç›Šç­–ç•¥ï¼Œå¯ä»¥å°†è®­ç»ƒæ—¶é—´ä»å‡ å¤©å‡å°‘åˆ°å‡ å°æ—¶ã€‚'
- en: '**There is a smaller chance of overfitting**: By using undersampling techniques,
    the number of majority class instances can be reduced, allowing the model to focus
    more on the minority class instances. This, in turn, improves the modelâ€™s ability
    to generalize across both classes. As a result, the model becomes less likely
    to overfit to the majority class and is better equipped to handle new, unseen
    data, thus reducing the likelihood of overfitting. We are going to discuss the
    various undersampling methods in this chapter.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è¿‡æ‹Ÿåˆçš„å¯èƒ½æ€§æ›´å°**ï¼šé€šè¿‡ä½¿ç”¨æ¬ é‡‡æ ·æŠ€æœ¯ï¼Œå¯ä»¥å‡å°‘å¤šæ•°ç±»åˆ«çš„å®ä¾‹æ•°é‡ï¼Œä½¿æ¨¡å‹æ›´å¤šåœ°å…³æ³¨å°‘æ•°ç±»åˆ«çš„å®ä¾‹ã€‚è¿™åè¿‡æ¥åˆæé«˜äº†æ¨¡å‹åœ¨ä¸¤ä¸ªç±»åˆ«ä¹‹é—´æ³›åŒ–çš„èƒ½åŠ›ã€‚å› æ­¤ï¼Œæ¨¡å‹ä¸å¤ªå¯èƒ½å¯¹å¤šæ•°ç±»åˆ«è¿‡æ‹Ÿåˆï¼Œå¹¶ä¸”æ›´å¥½åœ°å‡†å¤‡å¤„ç†æ–°çš„ã€æœªè§è¿‡çš„æ•°æ®ï¼Œä»è€Œé™ä½è¿‡æ‹Ÿåˆçš„å¯èƒ½æ€§ã€‚æˆ‘ä»¬å°†åœ¨æœ¬ç« ä¸­è®¨è®ºå„ç§æ¬ é‡‡æ ·æ–¹æ³•ã€‚'
- en: '*Figure 3**.1* shows the general idea behind undersampling graphically.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾3.1*ä»¥å›¾å½¢æ–¹å¼å±•ç¤ºäº†æ¬ é‡‡æ ·çš„åŸºæœ¬æ€æƒ³ã€‚'
- en: '![](img/B17259_03_01.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_03_01.jpg)'
- en: Figure 3.1 â€“ General idea behind undersampling showing (a) imbalanced data with
    two classes and (b) data after undersampling
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾3.1 â€“ æ¬ é‡‡æ ·çš„åŸºæœ¬æ€æƒ³å±•ç¤ºï¼š(a)å…·æœ‰ä¸¤ä¸ªç±»åˆ«çš„å¤±è¡¡æ•°æ®ï¼Œ(b)æ¬ é‡‡æ ·åçš„æ•°æ®
- en: In *Figure 3**.1(a)*, we show the original data containing many data points
    from the circle class. In *Figure 3**.1(b)*, we show the resampled data after
    removing some data points from the circle class.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨*å›¾3.1(a)*ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åŒ…å«æ¥è‡ªåœ†å½¢ç±»åˆ«çš„è®¸å¤šæ•°æ®ç‚¹çš„åŸå§‹æ•°æ®ã€‚åœ¨*å›¾3.1(b)*ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä»åœ†å½¢ç±»åˆ«ä¸­ç§»é™¤ä¸€äº›æ•°æ®ç‚¹åçš„é‡é‡‡æ ·æ•°æ®ã€‚
- en: When to avoid undersampling the majority class
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å½“ä½•æ—¶é¿å…å¯¹å¤šæ•°ç±»åˆ«è¿›è¡Œæ¬ é‡‡æ ·
- en: 'Undersampling is not a panacea and may not always work. It depends on the dataset
    and model under consideration:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ¬ é‡‡æ ·å¹¶éä¸‡èƒ½è‰¯è¯ï¼Œå¹¶ä¸æ€»æ˜¯æœ‰æ•ˆã€‚å®ƒå–å†³äºæ‰€è€ƒè™‘çš„æ•°æ®é›†å’Œæ¨¡å‹ï¼š
- en: '**Too little training data for all the classes**: If the dataset is already
    small, undersampling the majority class can lead to a significant loss of information.
    In such cases, it is advisable to try gathering more data or exploring other techniques,
    such as oversampling the minority class to balance the class distribution.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ‰€æœ‰ç±»åˆ«çš„è®­ç»ƒæ•°æ®éƒ½å¤ªå°‘**ï¼šå¦‚æœæ•°æ®é›†æœ¬èº«å·²ç»å¾ˆå°ï¼Œå¯¹å¤šæ•°ç±»åˆ«è¿›è¡Œæ¬ é‡‡æ ·å¯èƒ½ä¼šå¯¼è‡´ä¿¡æ¯æŸå¤±å¾ˆå¤§ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå»ºè®®å°è¯•æ”¶é›†æ›´å¤šæ•°æ®æˆ–æ¢ç´¢å…¶ä»–æŠ€æœ¯ï¼Œä¾‹å¦‚å¯¹å°‘æ•°ç±»åˆ«è¿›è¡Œè¿‡é‡‡æ ·ä»¥å¹³è¡¡ç±»åˆ«åˆ†å¸ƒã€‚'
- en: '**Majority class equally important or more important than minority class**:
    In specific scenarios, such as the spam filtering example mentioned in [*Chapter
    1*](B17259_01.xhtml#_idTextAnchor015), *Introduction to Data Imbalance in Machine
    Learning*, it is crucial to maintain high accuracy in identifying the majority
    class instances. In such situations, undersampling the majority class might reduce
    the modelâ€™s ability to accurately classify majority class instances, leading to
    a higher false positive rate. Instead, alternative methods, such as cost-sensitive
    learning or adjusting the decision threshold (both of these are discussed in [*Chapter
    5*](B17259_05.xhtml#_idTextAnchor151), *Cost-Sensitive Learning*), can be considered.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¤šæ•°ç±»åŒç­‰é‡è¦æˆ–æ¯”å°‘æ•°ç±»æ›´é‡è¦**ï¼šåœ¨ç‰¹å®šåœºæ™¯ä¸‹ï¼Œä¾‹å¦‚åœ¨[*ç¬¬ä¸€ç« *](B17259_01.xhtml#_idTextAnchor015)â€œæœºå™¨å­¦ä¹ ä¸­çš„æ•°æ®ä¸å¹³è¡¡ä»‹ç»â€ä¸­æåˆ°çš„åƒåœ¾é‚®ä»¶è¿‡æ»¤ç¤ºä¾‹ï¼Œä¿æŒè¯†åˆ«å¤šæ•°ç±»å®ä¾‹çš„é«˜å‡†ç¡®æ€§è‡³å…³é‡è¦ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¯¹å¤šæ•°ç±»è¿›è¡Œä¸‹é‡‡æ ·å¯èƒ½ä¼šé™ä½æ¨¡å‹å‡†ç¡®åˆ†ç±»å¤šæ•°ç±»å®ä¾‹çš„èƒ½åŠ›ï¼Œå¯¼è‡´æ›´é«˜çš„è¯¯æŠ¥ç‡ã€‚ç›¸åï¼Œå¯ä»¥è€ƒè™‘å…¶ä»–æ–¹æ³•ï¼Œå¦‚æˆæœ¬æ•æ„Ÿå­¦ä¹ æˆ–è°ƒæ•´å†³ç­–é˜ˆå€¼ï¼ˆè¿™ä¸¤ç§æ–¹æ³•éƒ½åœ¨[*ç¬¬äº”ç« *](B17259_05.xhtml#_idTextAnchor151)â€œæˆæœ¬æ•æ„Ÿå­¦ä¹ â€ä¸­è®¨è®ºè¿‡ï¼‰ã€‚'
- en: '**When undersampling harms model performance or causes overfitting of the model**:
    Undersampling the majority class might decrease overall model performance, as
    it discards potentially valuable information. Some of the undersampling methods
    discard the examples near the decision boundary, which can also alter the decision
    boundary. Also, by reducing the size of the majority class, undersampling can
    cause underfitting, where the model becomes too simple to capture the underlying
    trends in the limited training data and performs poorly on new, unseen data. There
    is some risk of overfitting as well when using undersampling techniques if the
    model memorizes the reduced dataset. In such cases, exploring other techniques
    such as ensemble methods ([*Chapter 4*](B17259_04.xhtml#_idTextAnchor120), *Ensemble
    Methods*), hybrid methods that combine oversampling and undersampling (discussed
    toward the end of this chapter), or using different algorithms less prone to overfitting
    might be better.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å½“ä¸‹é‡‡æ ·æŸå®³æ¨¡å‹æ€§èƒ½æˆ–å¯¼è‡´æ¨¡å‹è¿‡æ‹Ÿåˆæ—¶**ï¼šå¯¹å¤šæ•°ç±»è¿›è¡Œä¸‹é‡‡æ ·å¯èƒ½ä¼šé™ä½æ•´ä½“æ¨¡å‹æ€§èƒ½ï¼Œå› ä¸ºå®ƒä¸¢å¼ƒäº†å¯èƒ½æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚ä¸€äº›ä¸‹é‡‡æ ·æ–¹æ³•ä¼šä¸¢å¼ƒå†³ç­–è¾¹ç•Œé™„è¿‘çš„ç¤ºä¾‹ï¼Œè¿™ä¹Ÿä¼šæ”¹å˜å†³ç­–è¾¹ç•Œã€‚æ­¤å¤–ï¼Œé€šè¿‡å‡å°‘å¤šæ•°ç±»çš„å¤§å°ï¼Œä¸‹é‡‡æ ·å¯èƒ½å¯¼è‡´æ¬ æ‹Ÿåˆï¼Œå³æ¨¡å‹å˜å¾—è¿‡äºç®€å•ï¼Œæ— æ³•æ•æ‰æœ‰é™è®­ç»ƒæ•°æ®ä¸­çš„æ½œåœ¨è¶‹åŠ¿ï¼Œåœ¨æ–°æœªè§è¿‡çš„æ•°æ®ä¸Šè¡¨ç°ä¸ä½³ã€‚åœ¨ä½¿ç”¨ä¸‹é‡‡æ ·æŠ€æœ¯æ—¶ï¼Œå¦‚æœæ¨¡å‹è®°ä½äº†å‡å°‘åçš„æ•°æ®é›†ï¼Œä¹Ÿå­˜åœ¨è¿‡æ‹Ÿåˆçš„é£é™©ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¢ç´¢å…¶ä»–æŠ€æœ¯ï¼Œå¦‚é›†æˆæ–¹æ³•ï¼ˆ[*ç¬¬å››ç« *](B17259_04.xhtml#_idTextAnchor120)â€œé›†æˆæ–¹æ³•â€ä¸­è®¨è®ºè¿‡ï¼‰ã€ç»“åˆè¿‡é‡‡æ ·å’Œä¸‹é‡‡æ ·çš„æ··åˆæ–¹æ³•ï¼ˆåœ¨æœ¬ç« æœ«å°¾è®¨è®ºï¼‰ï¼Œæˆ–ä½¿ç”¨ä¸å¤ªå®¹æ˜“è¿‡æ‹Ÿåˆçš„ä¸åŒç®—æ³•å¯èƒ½æ›´å¥½ã€‚'
- en: ğŸš€ Undersampling techniques in production at Meta, Microsoft, and Uber
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸš€ Metaã€Microsoftå’ŒUberåœ¨ç”Ÿäº§ä¸­çš„ä¸‹é‡‡æ ·æŠ€æœ¯
- en: The main challenge in tasks such as ad click prediction is handling massive
    and imbalanced datasets. For instance, a single day of Facebook ads can contain
    hundreds of millions of instances, with an average **ClickThrough Rate** (**CTR**)
    of just 0.1%. To address this, Meta employs two specialized techniques, as detailed
    in the paper *Practical Lessons from Predicting Clicks on Ads at Facebook* [1].
    The first is uniform subsampling, which uniformly reduces the training data volume
    and has shown that using just 10% of the data results in only a 1% reduction in
    model performance. The second is negative downsampling, which specifically targets
    negative (â€œno-clickâ€) examples and uses an optimal downsampling rate of 0.025.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¹¿å‘Šç‚¹å‡»é¢„æµ‹ç­‰ä»»åŠ¡ä¸­ï¼Œä¸»è¦æŒ‘æˆ˜æ˜¯å¤„ç†å¤§è§„æ¨¡ä¸”ä¸å¹³è¡¡çš„æ•°æ®é›†ã€‚ä¾‹å¦‚ï¼ŒFacebookå•æ—¥å¹¿å‘Šå¯èƒ½åŒ…å«æ•°äº¿ä¸ªå®ä¾‹ï¼Œå¹³å‡ç‚¹å‡»ç‡ï¼ˆ**CTR**ï¼‰ä»…ä¸º0.1%ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒMetaåœ¨è®ºæ–‡ã€Šä»é¢„æµ‹Facebookå¹¿å‘Šç‚¹å‡»ä¸­æ±²å–çš„å®ç”¨ç»éªŒã€‹[1]ä¸­è¯¦ç»†ä»‹ç»äº†ä¸¤ç§ä¸“é—¨çš„æŠ€å·§ã€‚ç¬¬ä¸€ç§æ˜¯å‡åŒ€ä¸‹é‡‡æ ·ï¼Œå®ƒå‡åŒ€åœ°å‡å°‘äº†è®­ç»ƒæ•°æ®é‡ï¼Œå¹¶è¡¨æ˜ä½¿ç”¨ä»…10%çš„æ•°æ®åªä¼šå¯¼è‡´æ¨¡å‹æ€§èƒ½é™ä½1%ã€‚ç¬¬äºŒç§æ˜¯è´Ÿæ ·æœ¬ä¸‹é‡‡æ ·ï¼Œå®ƒä¸“é—¨é’ˆå¯¹è´Ÿæ ·æœ¬ï¼ˆâ€œæ— ç‚¹å‡»â€ï¼‰ç¤ºä¾‹ï¼Œå¹¶ä½¿ç”¨æœ€ä¼˜çš„ä¸‹é‡‡æ ·ç‡ä¸º0.025ã€‚
- en: Similarly, Microsoft and Uber have very similar approaches to tackling these
    challenges. To estimate the CTR of sponsored ads on Bing search [2], Microsoft
    uses a 50% negative downsampling rate for non-click cases, effectively halving
    the training time while maintaining similar performance metrics. Uber Eats also
    employs negative downsampling to reduce the training data in order to train models
    that predict whether to send push notifications to customers about new restaurants
    [3]. In addition, they remove the least important features when building the final
    version of the model.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼åœ°ï¼Œå¾®è½¯å’Œä¼˜æ­¥åœ¨åº”å¯¹è¿™äº›æŒ‘æˆ˜æ–¹é¢æœ‰éå¸¸ç›¸ä¼¼çš„æ–¹æ³•ã€‚ä¸ºäº†ä¼°è®¡å¿…åº”æœç´¢ä¸­èµåŠ©å¹¿å‘Šçš„ç‚¹å‡»ç‡[2]ï¼Œå¾®è½¯å¯¹éç‚¹å‡»æ¡ˆä¾‹ä½¿ç”¨50%çš„è´Ÿæ ·æœ¬ä¸‹é‡‡æ ·ç‡ï¼Œæœ‰æ•ˆåœ°å°†è®­ç»ƒæ—¶é—´å‡åŠï¼ŒåŒæ—¶ä¿æŒç›¸ä¼¼çš„æ€§èƒ½æŒ‡æ ‡ã€‚ä¼˜æ­¥å¤–å–ä¹Ÿé‡‡ç”¨è´Ÿæ ·æœ¬ä¸‹é‡‡æ ·æ¥å‡å°‘è®­ç»ƒæ•°æ®ï¼Œä»¥ä¾¿è®­ç»ƒé¢„æµ‹æ˜¯å¦å‘å®¢æˆ·å‘é€æœ‰å…³æ–°é¤å…æ¨é€é€šçŸ¥çš„æ¨¡å‹[3]ã€‚æ­¤å¤–ï¼Œä»–ä»¬åœ¨æ„å»ºæ¨¡å‹æœ€ç»ˆç‰ˆæœ¬æ—¶ç§»é™¤äº†æœ€ä¸é‡è¦çš„ç‰¹å¾ã€‚
- en: Letâ€™s look at one of the ways of classifying undersampling methods next.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹åˆ†ç±»ä¸‹é‡‡æ ·æ–¹æ³•çš„ä¸€ç§æ–¹å¼ã€‚
- en: Fixed versus cleaning undersampling
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å›ºå®šæ–¹æ³•ä¸æ¸…æ´—æ–¹æ³•
- en: Undersampling methods can be divided into two categories based on how data points
    get removed from the majority class. These categories are fixed methods and cleaning
    methods.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®ä»å¤šæ•°ç±»ä¸­ç§»é™¤æ•°æ®ç‚¹çš„æ–¹å¼ï¼Œä¸‹é‡‡æ ·æ–¹æ³•å¯ä»¥åˆ†ä¸ºä¸¤ç±»ï¼šå›ºå®šæ–¹æ³•å’Œæ¸…æ´—æ–¹æ³•ã€‚
- en: In **fixed methods**, the number of examples in the majority class is reduced
    to a fixed number. Usually, we reduce the number of majority class samples to
    the size of the minority class. For example, if there are 100 million samples
    in the majority class and 10 million samples in the minority class, you will be
    left with only 10 million samples of both classes after applying the fixed method.
    Some such methods are random undersampling and instance hardness-based undersampling.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨**å›ºå®šæ–¹æ³•**ä¸­ï¼Œå¤šæ•°ç±»çš„ç¤ºä¾‹æ•°é‡å‡å°‘åˆ°å›ºå®šçš„æ•°é‡ã€‚é€šå¸¸ï¼Œæˆ‘ä»¬ä¼šå°†å¤šæ•°ç±»çš„æ ·æœ¬æ•°é‡å‡å°‘åˆ°å°‘æ•°ç±»çš„è§„æ¨¡ã€‚ä¾‹å¦‚ï¼Œå¦‚æœå¤šæ•°ç±»æœ‰1äº¿ä¸ªæ ·æœ¬ï¼Œå°‘æ•°ç±»æœ‰1000ä¸‡ä¸ªæ ·æœ¬ï¼Œåº”ç”¨å›ºå®šæ–¹æ³•åï¼Œä½ å°†åªå‰©ä¸‹1000ä¸‡ä¸ªä¸¤ä¸ªç±»çš„æ ·æœ¬ã€‚è¿™ç±»æ–¹æ³•åŒ…æ‹¬éšæœºä¸‹é‡‡æ ·å’ŒåŸºäºå®ä¾‹ç¡¬åº¦çš„ä¸‹é‡‡æ ·ã€‚
- en: In **cleaning methods**, the number of samples of the majority class is reduced
    based on some pre-determined criteria, independent of the absolute number of examples.
    Once this criterion is met, the algorithm doesnâ€™t care about the size of the majority
    or minority class.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨**æ¸…æ´—æ–¹æ³•**ä¸­ï¼ŒåŸºäºæŸäº›é¢„å…ˆç¡®å®šçš„å‡†åˆ™å‡å°‘å¤šæ•°ç±»çš„æ ·æœ¬æ•°é‡ï¼Œä¸ç¤ºä¾‹çš„ç»å¯¹æ•°é‡æ— å…³ã€‚ä¸€æ—¦æ»¡è¶³è¿™ä¸ªå‡†åˆ™ï¼Œç®—æ³•å°±ä¸å…³å¿ƒå¤šæ•°ç±»æˆ–å°‘æ•°ç±»çš„è§„æ¨¡ã€‚
- en: '*Table 3.1* summarizes the key differences between the two methods in a tabular
    format:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¡¨3.1* ä»¥è¡¨æ ¼å½¢å¼æ€»ç»“äº†ä¸¤ç§æ–¹æ³•ä¹‹é—´çš„å…³é”®å·®å¼‚ï¼š'
- en: '|  | **Fixed** **undersampling methods** | **Cleaning** **undersampling methods**
    |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | **å›ºå®š** **ä¸‹é‡‡æ ·æ–¹æ³•** | **æ¸…æ´—** **ä¸‹é‡‡æ ·æ–¹æ³•** |'
- en: '| Key idea | Selects a specific number of majority class instances to remove
    | Identifies and removes noisy, redundant, or misclassified majority class instances
    aiming to improve decision boundaries between classes |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| å…³é”®æ€æƒ³ | é€‰æ‹©ç‰¹å®šæ•°é‡çš„å¤šæ•°ç±»å®ä¾‹è¿›è¡Œç§»é™¤ | è¯†åˆ«å¹¶ç§»é™¤å™ªå£°ã€å†—ä½™æˆ–è¯¯åˆ†ç±»çš„å¤šæ•°ç±»å®ä¾‹ï¼Œæ—¨åœ¨æ”¹å–„ç±»ä¹‹é—´çš„å†³ç­–è¾¹ç•Œ |'
- en: '| Relationship between instances | Doesnâ€™t consider relationships between instances
    | Evaluates relationships between instances |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| å®ä¾‹ä¹‹é—´çš„å…³ç³» | ä¸è€ƒè™‘å®ä¾‹ä¹‹é—´çš„å…³ç³» | è¯„ä¼°å®ä¾‹ä¹‹é—´çš„å…³ç³» |'
- en: '| Performance and ease of implementation | Faster and easier to implement |
    Sometimes, may have a better model performance and generalization than fixed undersampling
    methods |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| æ€§èƒ½å’Œå®ç°éš¾åº¦ | å®ç°æ›´å¿«ä¸”æ›´å®¹æ˜“ | æœ‰æ—¶ï¼Œå¯èƒ½æ¯”å›ºå®šä¸‹é‡‡æ ·æ–¹æ³•æœ‰æ›´å¥½çš„æ¨¡å‹æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ› |'
- en: '| Examples | Random undersamplingInstance hardness-based undersampling | Tomek
    linksNeighborhood cleaning rule |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| ç¤ºä¾‹ | éšæœºä¸‹é‡‡æ · | åŸºäºå®ä¾‹ç¡¬åº¦çš„ä¸‹é‡‡æ · | æ‰˜æ¢…å…‹é“¾æ¥ | é‚»åŸŸæ¸…æ´—è§„åˆ™ |'
- en: Table 3.1 â€“ Fixed versus cleaning undersampling methods
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨3.1 â€“ å›ºå®šæ–¹æ³•ä¸æ¸…æ´—æ–¹æ³•ä¸‹é‡‡æ ·
- en: 'Letâ€™s create an imbalanced dataset using the `make_classification` API from
    `sklearn`. We will apply various undersampling techniques throughout this chapter
    to balance this dataset:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä½¿ç”¨`sklearn`çš„`make_classification` APIåˆ›å»ºä¸€ä¸ªä¸å¹³è¡¡çš„æ•°æ®é›†ã€‚æˆ‘ä»¬å°†åœ¨æœ¬ç« ä¸­åº”ç”¨å„ç§ä¸‹é‡‡æ ·æŠ€æœ¯æ¥å¹³è¡¡è¿™ä¸ªæ•°æ®é›†ï¼š
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Figure 3**.2* shows what the dataset looks like on a 2D plot. For the complete
    notebook code, please refer to the GitHub repository of this chapter.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾3.2* å±•ç¤ºäº†æ•°æ®é›†åœ¨äºŒç»´å›¾ä¸Šçš„æ ·å­ã€‚å®Œæ•´çš„ç¬”è®°æœ¬ä»£ç è¯·å‚è€ƒæœ¬ç« çš„GitHubä»“åº“ã€‚'
- en: '![](img/B17259_03_02.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_03_02.jpg)'
- en: Figure 3.2 â€“ Plotting a dataset with an imbalance ratio of 1:99
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾3.2 â€“ ç»˜åˆ¶ä¸å¹³è¡¡ç‡ä¸º1:99çš„æ•°æ®é›†
- en: Model calibration and threshold adjustment
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ ¡å‡†å’Œé˜ˆå€¼è°ƒæ•´
- en: After applying undersampling techniques, you may want to recalibrate the modelâ€™s
    probability scores. Why? As undersampling alters the original distribution of
    the classes, the modelâ€™s confidence estimates are biased [4] and may no longer
    accurately reflect the true likelihood of each class in the real-world scenario.
    Failing to recalibrate can lead to misleading or suboptimal decision-making when
    the model is deployed. Therefore, recalibrating the modelâ€™s probability scores
    ensures that the model not only classifies instances correctly but also estimates
    the probabilities in a manner that is consistent with the actual class distribution,
    enhancing its reliability. For a deeper understanding of this process, especially
    how to recalibrate model scores to account for the effects of downsampling, please
    refer to [*Chapter 10*](B17259_10.xhtml#_idTextAnchor279), *Model Calibration*.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åº”ç”¨æ¬ é‡‡æ ·æŠ€æœ¯åï¼Œä½ å¯èƒ½éœ€è¦é‡æ–°æ ¡å‡†æ¨¡å‹çš„æ¦‚ç‡åˆ†æ•°ã€‚ä¸ºä»€ä¹ˆï¼Ÿå› ä¸ºæ¬ é‡‡æ ·æ”¹å˜äº†åŸå§‹ç±»åˆ«çš„åˆ†å¸ƒï¼Œæ¨¡å‹çš„ç½®ä¿¡åº¦ä¼°è®¡å­˜åœ¨åå·®[4]ï¼Œå¯èƒ½ä¸å†å‡†ç¡®åæ˜ ç°å®åœºæ™¯ä¸­æ¯ä¸ªç±»åˆ«çš„çœŸå®å¯èƒ½æ€§ã€‚æœªèƒ½é‡æ–°æ ¡å‡†å¯èƒ½å¯¼è‡´æ¨¡å‹éƒ¨ç½²æ—¶åšå‡ºè¯¯å¯¼æ€§æˆ–æ¬¡ä¼˜çš„å†³ç­–ã€‚å› æ­¤ï¼Œé‡æ–°æ ¡å‡†æ¨¡å‹çš„æ¦‚ç‡åˆ†æ•°ç¡®ä¿æ¨¡å‹ä¸ä»…èƒ½å¤Ÿæ­£ç¡®åˆ†ç±»å®ä¾‹ï¼Œè€Œä¸”ä»¥ä¸å®é™…ç±»åˆ«åˆ†å¸ƒä¸€è‡´çš„æ–¹å¼ä¼°è®¡æ¦‚ç‡ï¼Œä»è€Œæé«˜å…¶å¯é æ€§ã€‚å¯¹äºæ›´æ·±å…¥ç†è§£æ­¤è¿‡ç¨‹ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•æ ¡å‡†æ¨¡å‹åˆ†æ•°ä»¥è€ƒè™‘ä¸‹é‡‡æ ·çš„å½±å“ï¼Œè¯·å‚é˜…[*ç¬¬10ç« *](B17259_10.xhtml#_idTextAnchor279)ï¼Œ*æ¨¡å‹æ ¡å‡†*ã€‚
- en: In the context of imbalanced datasets, threshold adjustment techniques can be
    a critical complement to undersampling methods. Whether or not we end up applying
    any sampling techniques, adjusting the threshold to determine the correct class
    label can be crucial for correctly interpreting the modelâ€™s performance. For a
    more in-depth understanding of various threshold adjustment techniques, you can
    refer to [*Chapter 5*](B17259_05.xhtml#_idTextAnchor151), *Cost-Sensitive Learning*.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸å¹³è¡¡æ•°æ®é›†çš„èƒŒæ™¯ä¸‹ï¼Œé˜ˆå€¼è°ƒæ•´æŠ€æœ¯å¯ä»¥æ˜¯æ¬ é‡‡æ ·æ–¹æ³•çš„ä¸´ç•Œè¡¥å……ã€‚æ— è®ºæˆ‘ä»¬æœ€ç»ˆæ˜¯å¦åº”ç”¨ä»»ä½•é‡‡æ ·æŠ€æœ¯ï¼Œè°ƒæ•´é˜ˆå€¼ä»¥ç¡®å®šæ­£ç¡®çš„ç±»åˆ«æ ‡ç­¾å¯¹äºæ­£ç¡®è§£é‡Šæ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ã€‚å¯¹äºæ›´æ·±å…¥ç†è§£å„ç§é˜ˆå€¼è°ƒæ•´æŠ€æœ¯ï¼Œä½ å¯ä»¥å‚é˜…[*ç¬¬5ç« *](B17259_05.xhtml#_idTextAnchor151)ï¼Œ*æˆæœ¬æ•æ„Ÿå­¦ä¹ *ã€‚
- en: Undersampling approaches
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¬ é‡‡æ ·æ–¹æ³•
- en: 'Letâ€™s look at a second way to categorize undersampling algorithms. There are
    a few ways the king can eliminate some Capulets:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹ç¬¬äºŒç§å¯¹æ¬ é‡‡æ ·ç®—æ³•è¿›è¡Œåˆ†ç±»çš„æ–¹æ³•ã€‚å›½ç‹å¯ä»¥é€šè¿‡å‡ ç§æ–¹å¼æ¶ˆé™¤ä¸€äº›å‡¯æ™®è±ç‰¹å®¶æ—æˆå‘˜ï¼š
- en: He can eliminate the Capulets uniformly from the whole town, thereby removing
    a few Capulets from all areas of the town
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»–å¯ä»¥ä»æ•´ä¸ªåŸé•‡å‡åŒ€åœ°æ¶ˆé™¤å‡¯æ™®è±ç‰¹å®¶æ—ï¼Œä»è€Œä»åŸé•‡çš„æ‰€æœ‰åœ°åŒºç§»é™¤ä¸€äº›å‡¯æ™®è±ç‰¹å®¶æ—æˆå‘˜
- en: Alternatively, the king can remove the Capulets who live near the houses of
    the Montagues
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ–è€…ï¼Œå›½ç‹å¯ä»¥ç§»é™¤ä½åœ¨è’™å¤ªå¤å®¶æ—æˆ¿å±‹é™„è¿‘çš„å‡¯æ™®è±ç‰¹å®¶æ—
- en: Lastly, he can remove the Capulets who live far away from the houses of the
    Montagues
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ€åï¼Œä»–å¯ä»¥ç§»é™¤ä½åœ¨è¿œç¦»è’™å¤ªå¤å®¶æ—æˆ¿å±‹çš„å‡¯æ™®è±ç‰¹å®¶æ—æˆå‘˜
- en: 'These are the three major approaches used in undersampling techniques. We either
    remove the majority samples uniformly, remove the majority samples near the minority
    samples, or remove the majority samples far from the minority samples. We can
    also combine the last two approaches by removing some nearby and some far away
    samples. The following diagram gives the classification of these methods:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ˜¯åœ¨æ¬ é‡‡æ ·æŠ€æœ¯ä¸­ä½¿ç”¨çš„ä¸‰ç§ä¸»è¦æ–¹æ³•ã€‚æˆ‘ä»¬è¦ä¹ˆå‡åŒ€åœ°ç§»é™¤å¤šæ•°æ ·æœ¬ï¼Œè¦ä¹ˆç§»é™¤é è¿‘å°‘æ•°æ ·æœ¬çš„å¤šæ•°æ ·æœ¬ï¼Œè¦ä¹ˆç§»é™¤è¿œç¦»å°‘æ•°æ ·æœ¬çš„å¤šæ•°æ ·æœ¬ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡ç§»é™¤ä¸€äº›é è¿‘çš„å’Œä¸€äº›è¿œç¦»çš„æ ·æœ¬æ¥ç»“åˆåä¸¤ç§æ–¹æ³•ã€‚ä»¥ä¸‹å›¾è¡¨ç»™å‡ºäº†è¿™äº›æ–¹æ³•çš„åˆ†ç±»ï¼š
- en: '![](img/B17259_03_03.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_03_03.jpg)'
- en: Figure 3.3 â€“ Categorization of undersampling techniques
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾3.3 â€“ æ¬ é‡‡æ ·æŠ€æœ¯åˆ†ç±»
- en: The following figure illustrates the difference between the two criteria. In
    *Figure 3**.4(a)*, we show the original dataset. In *Figure 3**.4(b)*, we show
    the same dataset after removing the examples close to the decision boundary. Notice
    how examples closer to the class boundary are removed.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹å›¾è¡¨è¯´æ˜äº†ä¸¤ç§æ ‡å‡†ä¹‹é—´çš„å·®å¼‚ã€‚åœ¨*å›¾3.4(a)*ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åŸå§‹æ•°æ®é›†ã€‚åœ¨*å›¾3.4(b)*ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç§»é™¤é è¿‘å†³ç­–è¾¹ç•Œä¾‹å­åçš„ç›¸åŒæ•°æ®é›†ã€‚æ³¨æ„é è¿‘ç±»åˆ«è¾¹ç•Œçš„ä¾‹å­æ˜¯å¦‚ä½•è¢«ç§»é™¤çš„ã€‚
- en: Majority class examples far from the minority class may not effectively help
    models establish a decision boundary. Hence, such majority class examples away
    from the decision boundary can be removed. In *Figure 3**.4(c)*, we show the dataset
    after removing examples far away from the boundary. The examples far from the
    decision boundary can be considered easy-to-classify examples.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•°ç±»åˆ«çš„ä¾‹å­è¿œç¦»å°‘æ•°ç±»åˆ«å¯èƒ½æ— æ³•æœ‰æ•ˆåœ°å¸®åŠ©æ¨¡å‹å»ºç«‹å†³ç­–è¾¹ç•Œã€‚å› æ­¤ï¼Œè¿™æ ·çš„è¿œç¦»å†³ç­–è¾¹ç•Œçš„å¤šæ•°ç±»åˆ«ä¾‹å­å¯ä»¥è¢«ç§»é™¤ã€‚åœ¨*å›¾3.4(c)*ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç§»é™¤è¿œç¦»è¾¹ç•Œä¾‹å­åçš„æ•°æ®é›†ã€‚è¿œç¦»å†³ç­–è¾¹ç•Œçš„ä¾‹å­å¯ä»¥è¢«è®¤ä¸ºæ˜¯æ˜“äºåˆ†ç±»çš„ä¾‹å­ã€‚
- en: '![](img/B17259_03_04.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 â€“ Difference between two general approaches to undersampling
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Having discussed various ways to classify the various undersampling techniques,
    letâ€™s now look at them in more detail.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Removing examples uniformly
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are two major ways of removing the majority class examples uniformly from
    the data. The first way is to remove the examples randomly, and the other way
    involves using clustering techniques. Letâ€™s discuss both of these methods in detail.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Random UnderSampling
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first technique the king might think of is to pick Capulets randomly and
    remove them from the town. This is a naÃ¯ve approach. It might work, and the king
    might be able to bring peace to the town. But the king might cause unforeseen
    damage by picking up some influential Capulets. However, it is an excellent place
    to start our discussion. This technique can be considered a close cousin of random
    oversampling. In **Random UnderSampling** (**RUS**), as the name suggests, we
    randomly extract observations from the majority class until the classes are balanced.
    This technique inevitably leads to data loss, might harm the underlying structure
    of the data, and thus performs poorly sometimes.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_03_05.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 â€“ Comic explaining the main idea behind the RUS method
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code sample for using RUS:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `sampling_strategy` value can be used to specify the desired ratio of minority
    and majority classes, the default being that they will be made equal in number.
    *Figure 3**.6* shows the application of the `RandomUnderSampler` technique, where
    the right plot shows that most of the negative class samples got dropped:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_03_06.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 â€“ Plotting datasets before and after undersampling using RandomUnderSampler
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Next, we transition to a smarter technique that forms groups among majority
    class examples.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: ClusterCentroids
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The second technique the king might follow to carry out uniform undersampling
    is to divide the Capulet population into groups based on location. Then, keep
    one Capulet from each group and remove other Capulets from the group. This method
    of undersampling is called the **ClusterCentroids** method. If there are *N* items
    in the minority class, we create *N* clusters from the points of the majority
    class. For example, this can be done using the K-means algorithm. K-means is a
    clustering algorithm that groups nearby points into different clusters and assigns
    centroids to each group.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_03_07.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 â€“ Comic illustrating the main idea behind the ClusterCentroids method
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: In the ClusterCentroids technique, we first apply the K-means algorithm to all
    of the majority class data. Then, for each cluster, we keep the centroid and remove
    all other examples within that cluster. Itâ€™s worth noting that the centroid might
    not even be a part of the original data, which is an important aspect of this
    method.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 3**.8*, we show the working of ClusterCentroids. In *Figure 3**.8(a)*,
    we start with an imbalanced dataset. In *Figure 3**.8(b)*, we calculate the centroids
    for the three clusters. These centroids are shown as stars in the diagram. Finally,
    we remove all majority class samples except the centroids from the dataset in
    *Figure 3**.8(c)*.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨*å›¾3.8*ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ClusterCentroidsçš„å·¥ä½œåŸç†ã€‚åœ¨*å›¾3.8(a)*ä¸­ï¼Œæˆ‘ä»¬ä»ä¸€ä¸ªä¸å¹³è¡¡çš„æ•°æ®é›†å¼€å§‹ã€‚åœ¨*å›¾3.8(b)*ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—äº†ä¸‰ä¸ªèšç±»çš„è´¨å¿ƒã€‚è¿™äº›è´¨å¿ƒåœ¨å›¾ä¸­ä»¥æ˜Ÿå·è¡¨ç¤ºã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨*å›¾3.8(c)*ä¸­ä»æ•°æ®é›†ä¸­ç§»é™¤äº†æ‰€æœ‰å¤§å¤šæ•°ç±»åˆ«çš„æ ·æœ¬ï¼Œé™¤äº†è´¨å¿ƒã€‚
- en: '![](img/B17259_03_08.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_03_08.jpg)'
- en: Figure 3.8 â€“ Illustrating how the ClusterCentroids method works
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾3.8 â€“ å±•ç¤ºClusterCentroidsæ–¹æ³•çš„å·¥ä½œåŸç†
- en: 'Here is the code for using ClusterCentroids:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä½¿ç”¨ClusterCentroidsçš„ä»£ç ï¼š
- en: '[PRE2]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following is the output:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯è¾“å‡ºï¼š
- en: '[PRE3]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*Figure 3**.9* shows the application of the ClusterCentroids technique, where
    the right plot shows that most of the negative class samples got dropped.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾3.9*å±•ç¤ºäº†ClusterCentroidsæŠ€æœ¯çš„åº”ç”¨ï¼Œå…¶ä¸­å³ä¾§çš„å›¾è¡¨æ˜¾ç¤ºå¤§å¤šæ•°è´Ÿç±»æ ·æœ¬è¢«å‰”é™¤ã€‚'
- en: '![](img/B17259_03_09.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_03_09.jpg)'
- en: Figure 3.9 â€“ Plotting datasets before and after undersampling using ClusterCentroids
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾3.9 â€“ ä½¿ç”¨ClusterCentroidsåœ¨é™é‡‡æ ·å‰åç»˜åˆ¶æ•°æ®é›†
- en: One thing to note is that ClusterCentroids can be computationally expensive
    because it uses the K-means algorithm by default, which can be slow. We recommend
    exploring various parameters in the ClusterCentroids method, such as the estimator,
    which specifies the clustering method to be used. For example, K-means can be
    replaced with MiniBatchKMeans, a faster variant of the K-means clustering algorithm.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: éœ€è¦æ³¨æ„çš„ä¸€ç‚¹æ˜¯ï¼ŒClusterCentroidså¯èƒ½è®¡ç®—æˆæœ¬è¾ƒé«˜ï¼Œå› ä¸ºå®ƒé»˜è®¤ä½¿ç”¨K-meansç®—æ³•ï¼Œè¿™å¯èƒ½ä¼šå¾ˆæ…¢ã€‚æˆ‘ä»¬å»ºè®®æ¢ç´¢ClusterCentroidsæ–¹æ³•ä¸­çš„å„ç§å‚æ•°ï¼Œä¾‹å¦‚estimatorï¼Œå®ƒæŒ‡å®šäº†è¦ä½¿ç”¨çš„èšç±»æ–¹æ³•ã€‚ä¾‹å¦‚ï¼ŒK-meanså¯ä»¥è¢«MiniBatchKMeansæ›¿æ¢ï¼Œè¿™æ˜¯K-meansèšç±»ç®—æ³•çš„ä¸€ä¸ªæ›´å¿«å˜ä½“ã€‚
- en: In the following section, we will attempt to eliminate the majority class examples
    in a more strategic manner.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å°è¯•ä»¥æ›´æˆ˜ç•¥æ€§çš„æ–¹å¼æ¶ˆé™¤å¤§å¤šæ•°ç±»åˆ«çš„ç¤ºä¾‹ã€‚
- en: Strategies for removing noisy observations
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç§»é™¤å™ªå£°è§‚æµ‹å€¼çš„ç­–ç•¥
- en: The king might decide to look at the friendships and locations of the citizens
    before removing anyone. The king might decide to remove the Capulets who are rich
    and live near the Montagues. This could bring peace to the city by separating
    the feuding clans. Letâ€™s look at some strategies to do that with our data.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: å›½ç‹å¯èƒ½ä¼šåœ¨å‰”é™¤ä»»ä½•äººä¹‹å‰æŸ¥çœ‹å…¬æ°‘çš„å‹è°Šå’Œä½ç½®ã€‚å›½ç‹å¯èƒ½ä¼šå†³å®šå‰”é™¤é‚£äº›å¯Œæœ‰ä¸”ä½åœ¨è’™å¤ªå¤å®¶æ—é™„è¿‘çš„å‡¯æ™®è±ç‰¹å®¶æ—ã€‚è¿™å¯ä»¥é€šè¿‡åˆ†ç¦»äº‰æ–—çš„å®¶æ—æ¥ä¸ºåŸå¸‚å¸¦æ¥å’Œå¹³ã€‚è®©æˆ‘ä»¬çœ‹çœ‹ä¸€äº›ä½¿ç”¨æˆ‘ä»¬çš„æ•°æ®æ¥å®ç°è¿™ä¸€ç‚¹çš„ç­–ç•¥ã€‚
- en: ENN, RENN, and AllKNN
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ENNã€RENNå’ŒAllKNN
- en: The king can remove the Capulets based on their neighbors. For example, if one
    or more of the three closest neighbors of a Capulet is a Montague, the king can
    remove the Capulet. This technique is called `imbalanced-learn` library gives
    us options to decide which classes we would like to resample and what kind of
    class arrangement the neighbors of the sample should have.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: å›½ç‹å¯ä»¥æ ¹æ®é‚»å±…æ¥å‰”é™¤å‡¯æ™®è±ç‰¹å®¶æ—ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä¸€ä¸ªæˆ–å¤šä¸ªå‡¯æ™®è±ç‰¹å®¶æ—æœ€è¿‘ä¸‰ä¸ªé‚»å±…ä¸­çš„ä¸€ä¸ªæ˜¯è’™å¤ªå¤å®¶æ—ï¼Œå›½ç‹å°±å¯ä»¥å‰”é™¤å‡¯æ™®è±ç‰¹å®¶æ—ã€‚è¿™ç§æŠ€æœ¯ç§°ä¸º`imbalanced-learn`åº“ä¸ºæˆ‘ä»¬æä¾›äº†é€‰æ‹©æˆ‘ä»¬æƒ³è¦é‡é‡‡æ ·çš„ç±»åˆ«ä»¥åŠæ ·æœ¬é‚»å±…åº”è¯¥å…·æœ‰çš„ç±»åˆ«æ’åˆ—æ–¹å¼ã€‚
- en: 'There are two different criteria that we can follow for excluding the samples:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥éµå¾ªä¸¤ç§ä¸åŒçš„æ ‡å‡†æ¥æ’é™¤æ ·æœ¬ï¼š
- en: We can choose to exclude samples whose one or more neighbors are not from the
    same class as themselves
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€‰æ‹©æ’é™¤ä¸€ä¸ªæˆ–å¤šä¸ªé‚»å±…ä¸å±äºè‡ªèº«ç›¸åŒç±»åˆ«çš„æ ·æœ¬
- en: We can decide to exclude samples whose majority of neighbors are not from the
    same class as themselves
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å†³å®šæ’é™¤å¤§å¤šæ•°é‚»å±…ä¸å±äºè‡ªèº«ç›¸åŒç±»åˆ«çš„æ ·æœ¬
- en: In *Figure 3**.10*, we show the working of the ENN algorithm. Here, we remove
    the majority samples that have one or more minority neighbors. In *Figure 3**.10(a)*,
    we show the original dataset. In *Figure 3**.10(b)*, we highlight the majority
    class samples that have one or more minority class nearest neighbors. The highlighted
    majority class samples are shown as solid boxes, and their neighbors are shown
    by creating curves around them.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨*å›¾3.10*ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ENNç®—æ³•çš„å·¥ä½œåŸç†ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç§»é™¤äº†å…·æœ‰ä¸€ä¸ªæˆ–å¤šä¸ªå°‘æ•°é‚»å±…çš„å¤§å¤šæ•°æ ·æœ¬ã€‚åœ¨*å›¾3.10(a)*ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åŸå§‹æ•°æ®é›†ã€‚åœ¨*å›¾3.10(b)*ä¸­ï¼Œæˆ‘ä»¬çªå‡ºæ˜¾ç¤ºäº†å…·æœ‰ä¸€ä¸ªæˆ–å¤šä¸ªå°‘æ•°ç±»æœ€è¿‘é‚»çš„å¤§å¤šæ•°ç±»æ ·æœ¬ã€‚çªå‡ºæ˜¾ç¤ºçš„å¤§å¤šæ•°ç±»æ ·æœ¬ä»¥å®å¿ƒæ¡†è¡¨ç¤ºï¼Œå®ƒä»¬çš„é‚»å±…é€šè¿‡å›´ç»•å®ƒä»¬åˆ›å»ºæ›²çº¿æ¥è¡¨ç¤ºã€‚
- en: '![](img/B17259_03_10.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_03_10.jpg)'
- en: Figure 3.10 â€“ Illustrating how the ENN method works
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾3.10 â€“ å±•ç¤ºENNæ–¹æ³•çš„å·¥ä½œåŸç†
- en: 'Here is the code for using ENN:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä½¿ç”¨ENNçš„ä»£ç ï¼š
- en: '[PRE4]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The following is the output:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯è¾“å‡ºï¼š
- en: '[PRE5]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](img/B17259_03_11.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_03_11.jpg)'
- en: Figure 3.11 â€“ Plotting datasets before and after undersampling using ENN
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾3.11 â€“ ä½¿ç”¨ENNè¿›è¡Œä¸‹é‡‡æ ·å‰åçš„æ•°æ®é›†ç»˜å›¾
- en: Here, `n_neighbors` is the size of the neighborhood to consider to compute the
    nearest neighbors.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œ`n_neighbors`æ˜¯è¦è€ƒè™‘çš„é‚»åŸŸå¤§å°ï¼Œç”¨äºè®¡ç®—æœ€è¿‘é‚»ã€‚
- en: 'There are two variants of ENN that we wonâ€™t dive into, but you can explore
    them if you are interested: `imblearn.under_sampling.RepeatedEditedNearestNeighbours`)
    and `imblearn.under_sampling.AllKNN`). In RENN [6], we repeat the process followed
    in ENN until there are no more examples that can be removed or the maximum number
    of cycle counts has been reached. This algorithm also removes the noisy data.
    It is stronger in removing the boundary examples as the algorithm is repeated
    several times (*Figure 3**.12*).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸¤ç§ENNå˜ä½“æˆ‘ä»¬ä¸ä¼šæ·±å…¥æ¢è®¨ï¼Œä½†å¦‚æœä½ æ„Ÿå…´è¶£ï¼Œå¯ä»¥æ¢ç´¢å®ƒä»¬ï¼š`imblearn.under_sampling.RepeatedEditedNearestNeighbours`)å’Œ`imblearn.under_sampling.AllKNN`)ã€‚åœ¨RENN
    [6]ä¸­ï¼Œæˆ‘ä»¬é‡å¤åœ¨ENNä¸­éµå¾ªçš„è¿‡ç¨‹ï¼Œç›´åˆ°æ²¡æœ‰æ›´å¤šå¯ä»¥åˆ é™¤çš„ç¤ºä¾‹æˆ–è¾¾åˆ°æœ€å¤§å¾ªç¯è®¡æ•°ã€‚æ­¤ç®—æ³•ä¹Ÿç§»é™¤å™ªå£°æ•°æ®ã€‚ç”±äºç®—æ³•é‡å¤å¤šæ¬¡ï¼Œå®ƒåœ¨ç§»é™¤è¾¹ç•Œç¤ºä¾‹æ–¹é¢æ›´å¼ºï¼ˆ*å›¾3**.12*ï¼‰ã€‚
- en: '![](img/B17259_03_12.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_03_12.jpg)'
- en: Figure 3.12 â€“ Plotting datasets before and after undersampling using RENN
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾3.12 â€“ ä½¿ç”¨RENNè¿›è¡Œä¸‹é‡‡æ ·å‰åçš„æ•°æ®é›†ç»˜å›¾
- en: In the **AllKNN** method [6], we repeat **ENN** but with the number of neighbors
    going from 1 to K.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨**AllKNN**æ–¹æ³•[6]ä¸­ï¼Œæˆ‘ä»¬é‡å¤**ENN**ï¼Œä½†é‚»å±…çš„æ•°é‡ä»1å¢åŠ åˆ°Kã€‚
- en: Tomek links
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Tomeké“¾æ¥
- en: In 1976, Ivan Tomek proposed the idea of **Tomek links** [7]. Two examples are
    said to form Tomek links if they belong to two different classes, and there is
    no third point with a shorter distance to them than the distance between the two
    points. The intuition behind Tomek links is that â€œ*if two points are from different
    classes, they should not be nearest to each other.*â€ These points are part of
    the noise, and we can eliminate the majority member or both points to reduce noise.
    This is as if the king decides to remove the Capulets whose best friends are Montagues.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 1976å¹´ï¼Œä¼Šä¸‡Â·æ‰˜æ¢…å…‹æå‡ºäº†**Tomeké“¾æ¥**çš„æ¦‚å¿µ[7]ã€‚å¦‚æœä¸¤ä¸ªä¾‹å­å±äºä¸¤ä¸ªä¸åŒçš„ç±»åˆ«ï¼Œå¹¶ä¸”æ²¡æœ‰ç¬¬ä¸‰ä¸ªç‚¹ä¸å®ƒä»¬çš„è·ç¦»æ¯”ä¸¤ä¸ªç‚¹ä¹‹é—´çš„è·ç¦»çŸ­ï¼Œåˆ™ç§°è¿™ä¸¤ä¸ªä¾‹å­å½¢æˆTomeké“¾æ¥ã€‚Tomeké“¾æ¥èƒŒåçš„ç›´è§‰æ˜¯â€œ*å¦‚æœä¸¤ä¸ªç‚¹æ¥è‡ªä¸åŒçš„ç±»åˆ«ï¼Œå®ƒä»¬ä¸åº”è¯¥å½¼æ­¤æœ€è¿‘ã€‚*â€è¿™äº›ç‚¹æ˜¯å™ªå£°çš„ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å¯ä»¥æ¶ˆé™¤å¤šæ•°æˆå‘˜æˆ–ä¸¤ä¸ªç‚¹ä»¥å‡å°‘å™ªå£°ã€‚è¿™å°±åƒå›½ç‹å†³å®šç§»é™¤é‚£äº›æœ€å¥½çš„æœ‹å‹æ˜¯è’™å¤ªå¤å®¶æ—çš„å‡¯æ™®è±ç‰¹å®¶æ—æˆå‘˜ã€‚
- en: 'We can use the `TomekLinks` API as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è¿™æ ·ä½¿ç”¨`TomekLinks` APIï¼š
- en: '[PRE6]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following is the output:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢çš„è¾“å‡ºæ˜¯ï¼š
- en: '[PRE7]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](img/B17259_03_13.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_03_13.jpg)'
- en: Figure 3.13 â€“ Plotting datasets before and after undersampling using TomekLinks
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾3.13 â€“ ä½¿ç”¨TomekLinksè¿›è¡Œä¸‹é‡‡æ ·å‰åçš„æ•°æ®é›†ç»˜å›¾
- en: '*Figure 3**.14* shows the working of the Tomek links algorithm. In *Figure
    3**.14(a)*, we have the original dataset. In *Figure 3**.14(b),* we find and highlight
    the Tomek links. Notice that the points in these links are close to each other.
    In *Figure 3**.14(c)*, we show the dataset after removing the majority class samples
    (depicted as circles) that belong to the Tomek links. Notice the two circles present
    in part *(b)* but missing in part *(c)*. Similarly, we show the dataset after
    removing all the points in Tomek links in part *(d)* of the diagram.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾3**.14*å±•ç¤ºäº†Tomeké“¾æ¥ç®—æ³•çš„å·¥ä½œåŸç†ã€‚åœ¨*å›¾3**.14(a)*ä¸­ï¼Œæˆ‘ä»¬æœ‰åŸå§‹æ•°æ®é›†ã€‚åœ¨*å›¾3**.14(b)*ä¸­ï¼Œæˆ‘ä»¬æ‰¾åˆ°å¹¶çªå‡ºæ˜¾ç¤ºTomeké“¾æ¥ã€‚æ³¨æ„è¿™äº›é“¾æ¥ä¸­çš„ç‚¹å½¼æ­¤å¾ˆè¿‘ã€‚åœ¨*å›¾3**.14(c)*ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç§»é™¤å±äºTomeké“¾æ¥çš„å¤šæ•°ç±»æ ·æœ¬ï¼ˆä»¥åœ†åœˆè¡¨ç¤ºï¼‰åçš„æ•°æ®é›†ã€‚æ³¨æ„åœ¨éƒ¨åˆ†*(b)*ä¸­å‡ºç°çš„ä¸¤ä¸ªåœ†åœˆä½†åœ¨éƒ¨åˆ†*(c)*ä¸­ç¼ºå¤±ã€‚åŒæ ·ï¼Œæˆ‘ä»¬åœ¨å›¾ä¾‹çš„éƒ¨åˆ†*(d)*ä¸­å±•ç¤ºäº†ç§»é™¤Tomeké“¾æ¥ä¸­æ‰€æœ‰ç‚¹åçš„æ•°æ®é›†ã€‚'
- en: '![](img/B17259_03_14.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_03_14.jpg)'
- en: Figure 3.14 â€“ Illustrating how the TomekLinks algorithm works
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾3.14 â€“ å±•ç¤ºTomekLinksç®—æ³•çš„å·¥ä½œåŸç†
- en: Tomek links is a resource-intensive method due to its requirement of calculating
    pairwise distances between all examples. As stated in *A Study of the Behavior
    of Several Methods for Balancing Machine Learning Training Data* [8], performing
    this process on a reduced dataset would be more computationally efficient when
    dealing with large amounts of data.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå…¶è¦æ±‚è®¡ç®—æ‰€æœ‰ç¤ºä¾‹ä¹‹é—´çš„æˆå¯¹è·ç¦»ï¼ŒTomeké“¾æ¥æ˜¯ä¸€ç§èµ„æºå¯†é›†å‹æ–¹æ³•ã€‚æ­£å¦‚åœ¨*ã€Šå‡ ç§å¹³è¡¡æœºå™¨å­¦ä¹ è®­ç»ƒæ•°æ®è¡Œä¸ºçš„ç ”ç©¶ã€‹*[8]ä¸­æ‰€è¿°ï¼Œåœ¨å¤„ç†å¤§é‡æ•°æ®æ—¶ï¼Œåœ¨å‡å°‘çš„æ•°æ®é›†ä¸Šæ‰§è¡Œæ­¤è¿‡ç¨‹å°†æ›´å…·æœ‰è®¡ç®—æ•ˆç‡ã€‚
- en: In the next method, we will try to remove majority class examples from the perspective
    of minority class examples. Can we remove the nearest neighbors of the minority
    class that belong to the majority class?
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ä¸ªæ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬å°†å°è¯•ä»å°‘æ•°ç±»ç¤ºä¾‹çš„è§’åº¦å°è¯•ç§»é™¤å¤šæ•°ç±»ç¤ºä¾‹ã€‚æˆ‘ä»¬èƒ½å¦ç§»é™¤å±äºå¤šæ•°ç±»çš„å°‘æ•°ç±»ç¤ºä¾‹çš„æœ€è¿‘é‚»ï¼Ÿ
- en: Neighborhood Cleaning Rule
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¤¾åŒºæ¸…æ´è§„åˆ™
- en: Apart from removing the Capulets whose one or more nearest neighbors are Montagues,
    the king might decide to look at the nearest neighbors of Montagues and remove
    the Capulets who might come up as one of the nearest neighbors for a Montague.
    In the **Neighborhood Cleaning Rule** (**NCR**) [9], we apply an ENN algorithm,
    train a KNN on the remaining data, and then remove all the majority class samples
    that are the nearest neighbors of a minority sample.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code for using `NeighourhoodCleaningRule`:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following is the output:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](img/B17259_03_15.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
- en: Figure 3.15 â€“ Plotting datasets before and after undersampling using NCR
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Instance hardness threshold
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The king might ask a minister, â€œ*Which Capulets have mixed well with Montagues?*â€
    The minister, based on their knowledge of the town, will give a list of those
    Capulets. Then, the king will remove the Capulets whose names are on the list.
    This method of using another model to identify noisy samples is known as the **instance
    hardness threshold**. In this method, we train a classification model on the data,
    such as a decision tree, random forest, or linear SVM.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: In addition to predicting the class of an instance, these classifiers can return
    their class probabilities. Class probabilities show the confidence the model has
    in classifying the instances. With the instance hardness threshold method [10],
    we remove the majority class samples that received low probability estimates (referred
    to as the â€œhard instancesâ€). These instances are considered â€œhard to classifyâ€
    due to class overlap, a principal contributor to instance hardness.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'The `imbalanced-learn` library provides an API for utilizing `InstanceHardnessThreshold`,
    where we can specify the estimator used to estimate the hardness of the examples.
    In this case, we use `LogisticRegression` as the estimator:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following is the output:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](img/B17259_03_16.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
- en: Figure 3.16 â€“ Plotting datasets before and after undersampling using InstanceHardnessThreshold
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Since classification models need to draw a decision boundary between the majority
    and minority classes, the majority class examples that are too far away from the
    minority class examples may not help the model decide this decision boundary.
    Considering this, we will look at methods in the next section that will remove
    such easy majority class examples.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Strategies for removing easy observations
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The reverse of the strategy to remove the rich and famous Capulets is to remove
    the poor and weak Capulets. This section will discuss the techniques for **removing
    the majority samples far away from the minority samples**. Instead of removing
    the samples from the boundary between the two classes, we use them for training
    a model. This way, we can train a model to better discriminate between the classes.
    However, one downside is that these algorithms risk retaining noisy data points,
    which could then be used to train the model, potentially introducing noise into
    the predictive system.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Condensed Nearest Neighbors
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Condensed Nearest Neighbors** (**CNNeighbors**) [11] is an algorithm that
    works as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: We add all minority samples to a set and one randomly selected majority sample.
    Letâ€™s call this set `C`.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We train a KNN model with *k = 1* on set `C`.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we repeat the following four steps for each of the remaining majority
    samples:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We consider one majority sample; letâ€™s call it `e`.
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: We try to predict the class of `e` using KNN.
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If the predicted class matches the original class, we remove the sample. The
    intuition is that there is little to learn from `e` as even a *1-NN* classifier
    can learn it.
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Otherwise, we add the sample to our set `C` and train the *1-NN* on `C` again.
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: This method removes the easy-to-classify samples from the majority class.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'The code to use `CondensedNearestNeighbour` is as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The following is the output:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](img/B17259_03_17.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
- en: Figure 3.17 â€“ Plotting datasets before and after undersampling using CNNeighbors
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: However, the CNNeighbors method can be computationally expensive, as it evaluates
    each majority class example using the KNN algorithm. This makes the CNNeighbors
    method unsuitable for big data applications.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: One-sided selection
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The king might decide to remove some rich and many poor Capulets. This way,
    only the middle-class Capulets will stay in the town. In one-sided selection [12],
    we do just that. This method is a combination of CNNeighbors and Tomek links.
    We first resample using a CNNeighbors. Then, we remove the Tomek links from the
    resampled data. It reduces both noisy and easy-to-identify samples.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code for `OneSidedSelection`. When we donâ€™t provide the `n_neighbors`
    parameter, the default value of `1` is taken:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following is the output:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](img/B17259_03_18.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
- en: Figure 3.18 â€“ Plotting datasets before and after undersampling using OneSidedSelection
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Here, `n_seeds_S` is the number of minority class samples used as seeds in the
    method, and it can significantly impact the methodâ€™s performance. It is advisable
    to tune this parameter.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Combining undersampling and oversampling
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You might wonder whether we can combine undersampling techniques with oversampling
    techniques to produce even better results. The answer is yes. Oversampling methods
    increase the number of samples of the minority class but also usually increase
    the noise in the data. Some undersampling techniques can help us remove the noise,
    for example, ENN, Tomek links, NCR, and instance hardness. We can combine these
    methods with SMOTE to produce good results. The combination of SMOTE with ENN
    [13] and Tomek links [14] has been well researched. Also, the `imbalanced-learn`
    library supports both of them: `SMOTEENN` and `SMOTETomek`.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Model performance comparison
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Letâ€™s explore how some popular models perform using the various undersampling
    techniques weâ€™ve discussed. We use two datasets for this comparison: one synthetic
    dataset and one real-world dataset called `thyroid_sick` from the `imbalanced-learn`
    library. Weâ€™ll evaluate the performance of 11 different undersampling techniques
    against a baseline of no sampling, using both logistic regression and random forest
    models. *Figures 3.19* to *3.22* show the average precision values for models
    trained using these various methods.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: You can find the notebook in the GitHub repository of the chapter.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_03_19.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
- en: Figure 3.19 â€“ Average precision when using various methods on the thyroid_sick
    dataset using random forest
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_03_20.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: Figure 3.20 â€“ Average precision when using various methods on synthetic data
    using random forest
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_03_21.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
- en: Figure 3.21 â€“ Average precision when using various methods on the thyroid_sick
    dataset using logistic regression
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_03_22.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: Figure 3.22 â€“ Average precision when using various methods on synthetic data
    using logistic regression
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some more observations:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: The effectiveness of undersampling techniques can vary significantly depending
    on the dataset and its characteristics
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No single technique dominates across all datasets, emphasizing the need for
    empirical testing to choose the best method for your specific problem
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, which method will work best for your data? There is no easy answer to this
    question. The key here is to develop an intuition about the inner workings of
    these methods and have a pipeline that can help you test different techniques.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: However, certain techniques can be time-consuming. In our testing on a dataset
    with a million examples, methods such as `CondensedNearestNeighbor`, `ClusterCentroids`,
    and `ALLKNN` took longer than others. If youâ€™re dealing with large amounts of
    data, planning to scale in the future, or are pressed for time, you may want to
    avoid these methods or tune their parameters. Techniques such as `RandomUnderSampler`
    and `InstanceHardnessThreshold` are more suitable for rapid iterative development.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: That brings us to the end of this chapter.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed undersampling, an approach to address the class
    imbalance in datasets by reducing the number of samples in the majority class.
    We reviewed the advantages of undersampling, such as keeping the data size in
    check and reducing the chances of overfitting. Undersampling methods can be categorized
    into fixed methods, which reduce the number of majority class samples to a fixed
    size, and cleaning methods, which reduce majority class samples based on predetermined
    criteria.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: We went over various undersampling techniques, including random undersampling,
    instance hardness-based undersampling, ClusterCentroids, ENN, Tomek links, NCR,
    instance hardness, CNNeighbors, one-sided selection, and combinations of undersampling
    and oversampling techniques, such as `SMOTEENN` and `SMOTETomek`.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: We concluded with a performance comparison of various undersampling techniques
    from the `imbalanced-learn` library on logistic regression and random forest models,
    using a few datasets, and benchmarked their performance and effectiveness.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Once you identify that your dataset is imbalanced and could potentially benefit
    from applying undersampling techniques, go ahead and experiment with the various
    methods discussed in this chapter. Evaluate their effectiveness using the appropriate
    metrics, such as PR-AUC, to find the most suitable approach for improving your
    modelâ€™s performance.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will go over various ensemble-based techniques.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Explore the various undersampling APIs available from the `imbalanced-learn`
    library at [https://imbalanced-learn.org/stable/references/under_sampling.html](https://imbalanced-learn.org/stable/references/under_sampling.html).
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explore the `NearMiss` undersampling technique, available through the `imblearn.under_sampling.NearMiss`
    API. Which class of methods does it belong to? Apply the `NearMiss` method to
    the dataset that we used in the chapter.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try all the undersampling methods discussed in this chapter on the `us_crime`
    dataset from UCI. You can find this dataset in the `fetch_datasets` API of the
    `imbalanced-learn` library. Find the undersampling method with the highest `f1-score`
    metric for `LogisticRegression` and `XGBoost` models.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Can you identify an undersampling method of your own? (Hint: think about combining
    the various approaches to undersampling in new ways.)'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'X. He et al., â€œ*Practical Lessons from Predicting Clicks on Ads at Facebook*,â€
    in Proceedings of the Eighth International Workshop on Data Mining for Online
    Advertising, New York NY USA: ACM, Aug. 2014, pp. 1â€“9\. doi: 10.1145/2648584.2648589.'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'X. Ling, W. Deng, C. Gu, H. Zhou, C. Li, and F. Sun, â€œ*Model Ensemble for Click
    Prediction in Bing Search Ads*,â€ in Proceedings of the 26th International Conference
    on World Wide Web Companion - WWW â€™17 Companion, Perth, Australia: ACM Press,
    2017, pp. 689â€“698\. doi: 10.1145/3041021.3054192.'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*How Uber Optimizes the Timing of Push Notifications using ML and Linear* *Programming*:
    [https://www.uber.com/blog/how-uber-optimizes-push-notifications-using-ml/](https://www.uber.com/blog/how-uber-optimizes-push-notifications-using-ml/).'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A. D. Pozzolo, O. Caelen, R. A. Johnson, and G. Bontempi, â€œ*Calibrating Probability
    with Undersampling for Unbalanced Classification*,â€ in 2015 IEEE Symposium Series
    on Computational Intelligence, Cape Town, South Africa: IEEE, Dec. 2015, pp. 159â€“166\.
    doi: 10.1109/SSCI.2015.33.'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '(Introducing the ENN method) D. L. Wilson, â€œ*Asymptotic Properties of Nearest
    Neighbor Rules Using Edited Data*,â€ IEEE Trans. Syst., Man, Cybern., vol. SMC-2,
    no. 3, pp. 408â€“421, Jul. 1972, doi: 10.1109/TSMC.1972.4309137.'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '(Introducing the RENN and AllKNN methods) â€œ*An Experiment with the Edited Nearest-Neighbor
    Rule*,â€ IEEE Trans. Syst., Man, Cybern., vol. SMC-6, no. 6, pp. 448â€“452, Jun.
    1976, doi: 10.1109/TSMC.1976.4309523.'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'I. Tomek, â€œ*Two Modifications of CNN*,â€ IEEE Trans. Syst., Man, Cybern., vol.
    SMC-6, no. 11, pp. 769â€“772, Nov. 1976, doi: 10.1109/TSMC.1976.4309452.'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'G. E. A. P. A. Batista, R. C. Prati, and M. C. Monard, â€œ*A study of the behavior
    of several methods for balancing machine learning training data*,â€ SIGKDD Explor.
    Newsl., vol. 6, no. 1, pp. 20â€“29, Jun. 2004, doi: 10.1145/1007730.1007735.'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '(Introducing the neighborhood cleaning rule method) J. Laurikkala, â€œ*Improving
    Identification of Difficult Small Classes by Balancing Class Distribution*,â€ in
    Artificial Intelligence in Medicine, S. Quaglini, P. Barahona, and S. Andreassen,
    Eds., in Lecture Notes in Computer Science, vol. 2101\. Berlin, Heidelberg: Springer
    Berlin Heidelberg, 2001, pp. 63â€“66\. doi: 10.1007/3-540-48229-6_9.'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '(Introducing the instance hardness threshold technique) M. R. Smith, T. Martinez,
    and C. Giraud-Carrier, â€œ*An instance level analysis of data complexity*,â€ Mach
    Learn, vol. 95, no. 2, pp. 225â€“256, May 2014, doi: 10.1007/s10994-013-5422-z.'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'P. Hart, â€œ*The condensed nearest neighbor rule (corresp.)*,â€ IEEE transactions
    on information theory, vol. 14, no. 3, pp. 515â€“516, 1968, https://citeseerx.ist.psu.edu/document?â€¨    repid=rep1&type=pdf&doi=7c3771fd6829630cf450af853 df728ecd8da4ab2.'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (Introducing the one-sided selection method) M. Kubat and S. Matwin, â€œ*Addressing
    The Curse Of Imbalanced Training Sets:* *One-sided Selection*â€.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (Application of `SMOTEENN` and `SMOTETomek` methods) Gustavo EAPA Batista, Ronaldo
    C Prati, and Maria Carolina Monard. *A study of the behavior of several methods
    for balancing machine learning training data*. ACM SIGKDD explorations newsletter,
    6(1):20â€“29, 2004.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '(Application of the `SMOTETomek` method) Gustavo EAPA Batista, Ana LC Bazzan,
    and Maria Carolina Monard. *Balancing training data for automated annotation of
    keywords: a case study*. In WOB, 10â€“18\. 2003.'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
