["```py\nimport numpy as np\n\n>>> nb_users = 1000\n>>> users = np.zeros(shape=(nb_users, 4))\n\n>>> for i in range(nb_users):\n>>>    users[i, 0] = np.random.randint(0, 4)\n>>>    users[i, 1] = np.random.randint(0, 2)\n>>>    users[i, 2] = np.random.randint(0, 5)\n>>>    users[i, 3] = np.random.randint(0, 5)\n```", "```py\n>>> nb_product = 20\n>>> user_products = np.random.randint(0, nb_product, size=(nb_users, 5))\n```", "```py\nfrom sklearn.neighbors import NearestNeighbors\n\n>>> nn = NearestNeighbors(n_neighbors=20, radius=2.0)\n>>> nn.fit(users)\nNearestNeighbors(algorithm='auto', leaf_size=30, metric='minkowski',\n metric_params=None, n_jobs=1, n_neighbors=20, p=2, radius=2.0)\n```", "```py\n>>> test_user = np.array([2, 0, 3, 2])\n>>> d, neighbors = nn.kneighbors(test_user.reshape(1, -1))\n\n>>> print(neighbors)\narray([[933,  67, 901, 208,  23, 720, 121, 156, 167,  60, 337, 549,  93,\n 563, 326, 944, 163, 436, 174,  22]], dtype=int64)\n```", "```py\n>>> suggested_products = []\n\n>>> for n in neighbors:\n>>>    for products in user_products[n]:\n>>>       for product in products:\n>>>          if product != 0 and product not in suggested_products:\n>>>             suggested_products.append(product)\n\n>>> print(suggested_products)\n[14, 5, 13, 4, 8, 9, 16, 18, 10, 7, 1, 19, 12, 11, 6, 17, 15, 3, 2]\n```", "```py\n>>> nb_items = 1000\n>>> items = np.zeros(shape=(nb_items, 4))\n\n>>> for i in range(nb_items):\n>>>    items[i, 0] = np.random.randint(0, 100)\n>>>    items[i, 1] = np.random.randint(0, 100)\n>>>    items[i, 2] = np.random.randint(0, 100)\n>>>    items[i, 3] = np.random.randint(0, 100)\n```", "```py\n>>> nn = NearestNeighbors(n_neighbors=10, radius=5.0)\n>>> nn.fit(items)\n```", "```py\n>>> test_product = np.array([15, 60, 28, 73])\n>>> d, suggestions = nn.radius_neighbors(test_product.reshape(1, -1), radius=20)\n\n>>> print(suggestions)\n[array([657, 784, 839, 342, 446, 196], dtype=int64)]\n\n>>> d, suggestions = nn.radius_neighbors(test_product.reshape(1, -1), radius=30)\n\n>>> print(suggestions)\n[ array([844, 340, 657, 943, 461, 799, 715, 863, 979, 784, 54, 148, 806,\n 465, 585, 710, 839, 695, 342, 881, 864, 446, 196, 73, 663, 580, 216], dtype=int64)]\n```", "```py\nfrom scipy.spatial.distance import hamming\n\n>>> a = np.array([0, 1, 0, 0, 1, 0, 1, 1, 0, 0])\n>>> b = np.array([1, 1, 0, 0, 0, 1, 1, 1, 1, 0])\n>>> d = hamming(a, b)\n\n>>> print(d)\n0.40000000000000002\n```", "```py\nfrom scipy.spatial.distance import jaccard\n\n>>> d = jaccard(a, b)\n>>> print(d)\n0.5714285714285714\n```", "```py\n>>> nn = NearestNeighbors(n_neighbors=10, radius=5.0, metric='hamming')\n>>> nn.fit(items) >>> nn = NearestNeighbors(n_neighbors=10, radius=5.0, metric='jaccard')\n>>> nn.fit(items)\n```", "```py\n{ user_1: { item1: rating, item2: rating, ... }, ..., user_n: ... }\n```", "```py\nfrom scikits.crab.models import MatrixPreferenceDataModel\n\n>>> user_item_matrix = {\n 1: {1: 2, 2: 5, 3: 3},\n 2: {1: 5, 4: 2},\n 3: {2: 3, 4: 5, 3: 2},\n 4: {3: 5, 5: 1},\n 5: {1: 3, 2: 3, 4: 1, 5: 3}\n }\n\n>>> model = MatrixPreferenceDataModel(user_item_matrix)\n```", "```py\nfrom scikits.crab.similarities import UserSimilarity\nfrom scikits.crab.metrics import euclidean_distances\n\n>>> similarity_matrix = UserSimilarity(model, euclidean_distances)\n```", "```py\nfrom scikits.crab.recommenders.knn import UserBasedRecommender\n\n>>> recommender = UserBasedRecommender(model, similarity_matrix, with_preference=True)\n\n>>> print(recommender.recommend(2))\n[(2, 3.6180339887498949), (5, 3.0), (3, 2.5527864045000417)]\n```", "```py\nimport warnings\n\n>>> with warnings.catch_warnings():\n>>>    warnings.simplefilter(\"ignore\")\n>>>    print(recommender.recommend(2))\n```", "```py\n>>> M = np.random.randint(0, 6, size=(20, 10))\n\n>>> print(M)\narray([[0, 4, 5, 0, 1, 4, 3, 3, 1, 3],\n [1, 4, 2, 5, 3, 3, 3, 4, 3, 1],\n [1, 1, 2, 2, 1, 5, 1, 4, 2, 5],\n [0, 4, 1, 2, 2, 5, 1, 1, 5, 5],\n [2, 5, 3, 1, 1, 2, 2, 4, 1, 1],\n [1, 4, 3, 3, 0, 0, 2, 3, 3, 5],\n [3, 5, 2, 1, 5, 3, 4, 1, 0, 2],\n [5, 2, 2, 0, 1, 0, 4, 4, 1, 0],\n [0, 2, 4, 1, 3, 1, 3, 0, 5, 4],\n [2, 5, 1, 5, 3, 0, 1, 4, 5, 2],\n [1, 0, 0, 5, 1, 3, 2, 0, 3, 5],\n [5, 3, 1, 5, 0, 0, 4, 2, 2, 2],\n [5, 3, 2, 4, 2, 0, 4, 4, 0, 3],\n [3, 2, 5, 1, 1, 2, 1, 1, 3, 0],\n [1, 5, 5, 2, 5, 2, 4, 5, 1, 4],\n [4, 0, 2, 2, 1, 0, 4, 4, 3, 3],\n [4, 2, 2, 3, 3, 4, 5, 3, 5, 1],\n [5, 0, 5, 3, 0, 0, 3, 5, 2, 2],\n [1, 3, 2, 2, 3, 0, 5, 4, 1, 0],\n [1, 3, 1, 4, 1, 5, 4, 4, 2, 1]])\n```", "```py\nfrom scipy.linalg import svd\n\nimport numpy as np\n\n>>> U, s, V = svd(M, full_matrices=True)\n>>> S = np.diag(s)\n\n>>> print(U.shape)\n(20L, 20L)\n\n>>> print(S.shape)\n(10L, 10L)\n\n>>> print(V.shape)\n(10L, 10L)\n```", "```py\n>>> Uk = U[:, 0:8]\n>>> Sk = S[0:8, 0:8]\n>>> Vk = V[0:8, :]\n```", "```py\n>>> Su = Uk.dot(np.sqrt(Sk).T)\n>>> Si = np.sqrt(Sk).dot(Vk).T\n>>> Er = np.mean(M, axis=1)\n\n>>> r5_2 = Er[5] + Su[5].dot(Si[2])\n>>> print(r5_2)\n2.38848720112\n```", "```py\n# Linux\n>>> ./pyspark\n\n# Mac OS X\n>>> pyspark\n\n# Windows\n>>> pyspark\n\nPython 2.7.12 |Anaconda 4.0.0 (64-bit)| (default, Jun 29 2016, 11:07:13) [MSC v.1500 64 bit (AMD64)] on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\nAnaconda is brought to you by Continuum Analytics.\nPlease check out: http://continuum.io/thanks and https://anaconda.org\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevewl(newLevel).\nWelcome to\n ____ __\n / __/__ ___ _____/ /__\n _\\ \\/ _ \\/ _ `/ __/ '_/\n /__ / .__/\\_,_/_/ /_/\\_\\ version 2.0.2\n /_/\n\nUsing Python version 2.7.12 (default, Jun 29 2016 11:07:13)\nSparkSession available as 'spark'.\n>>>\n```", "```py\nfrom pyspark.mllib.recommendation import Rating\n\nimport numpy as np\n\n>>> nb_users = 200\n>>> nb_products = 100\n\n>>> ratings = []\n\n>>> for _ in range(10):\n>>>    for i in range(nb_users):\n>>>        rating = Rating(user=i, \n>>>                        product=np.random.randint(1, nb_products), \n>>>                        rating=np.random.randint(0, 5))\n>>>        ratings.append(rating)\n\n>>> ratings = sc.parallelize(ratings)\n```", "```py\nfrom pyspark.mllib.recommendation import ALS \n>>> model = ALS.train(ratings, rank=5, iterations=10)\n```", "```py\n>>> test = ratings.map(lambda rating: (rating.user, rating.product))\n```", "```py\n>>> predictions = model.predictAll(test)\n```", "```py\n>>> full_predictions = predictions.map(lambda pred: ((pred.user, pred.product), pred.rating))\n```", "```py\n>>> split_ratings = ratings.map(lambda rating: ((rating.user, rating.product), rating.rating))\n>>> joined_predictions = split_ratings.join(full_predictions)\n```", "```py\n>>> mse = joined_predictions.map(lambda x: (x[1][0] - x[1][1]) ** 2).mean()\n```", "```py\n>>> print('MSE: %.3f' % mse)\nMSE: 0.580\n\n>>> prediction = model.predict(10, 20)\n>>> print('Prediction: %3.f' % prediction)\nPrediction: 2.810\n```", "```py\nfrom pyspark import SparkContext, SparkConf\n\n>>> conf = SparkConf().setAppName('ALS').setMaster('local[*]')\n>>> sc = SparkContext(conf=conf)\n```", "```py\n# Linux, Mac OSx\n./spark-submit als_spark.py\n\n# Windows\nspark-submit als_spark.py\n```"]