- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ensemble Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Think of a top executive at a major company. They don’t make decisions on their
    own. Throughout the day, they need to make numerous critical decisions. How do
    they make those choices? Not alone, but by consulting their advisors.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say that an executive consults five different advisors from different
    departments, each proposing a slightly different solution based on their expertise,
    skills, and domain knowledge. To make the most effective decision, the executive
    combines the insights and opinions of all five advisors to create a hybrid solution
    that incorporates the best parts of each proposal. This scenario illustrates the
    concept of **ensemble methods**, where multiple weak classifiers are combined
    to create a stronger and more accurate classifier. By combining different approaches,
    ensemble methods can often achieve better performance than relying on a single
    classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a strong model through ensemble methods by combining the results
    from multiple weak classifiers. These weak classifiers, such as simplified decision
    trees, neural networks, or support vector machines, perform slightly better than
    random guessing. In contrast, a strong model, created by ensembling these weak
    classifiers, performs significantly better than random guessing. The weak classifiers
    can be fed different sources of information. There are two general approaches
    for building ensembles of models: bagging and boosting.'
  prefs: []
  type: TYPE_NORMAL
- en: The problem with traditional ensemble methods is that they use classifiers that
    assume balanced data. Thus, they may not work very well with imbalanced datasets.
    So, we combine the popular machine learning ensembling methods with the techniques
    for dealing with imbalanced data that we studied in previous chapters. We are
    going to discuss those combinations in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the topics that will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Bagging techniques for imbalanced data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting techniques for imbalanced data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble of ensembles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model performance comparison
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In *Figure 4**.1*, we have categorized the various ensembling techniques that
    we will cover in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – Overview of ensembling techniques
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will understand how to adapt ensemble models
    such as bagging and boosting to account for class imbalances in datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Python notebooks for this chapter are available on GitHub at [https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter04](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter04).
    As usual, you can open the GitHub notebook using Google Colab by clicking on the
    **Open in Colab** icon at the top of this chapter’s notebook or by launching it
    from [https://colab.research.google.com](https://colab.research.google.com) using
    the GitHub URL of the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will continue to use a synthetic dataset generated using
    the `make_classification` API, just as we did in the previous chapters. Toward
    the end of this chapter, we will test the methods we learned in this chapter on
    some real datasets. Our full dataset contains 90,000 examples with a 1:99 imbalance
    ratio. Here is what the training dataset looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – Plot of a dataset with a 1:99 imbalance ratio
  prefs: []
  type: TYPE_NORMAL
- en: With our imbalanced dataset ready to use, let’s look at the first ensembling
    method, called bagging.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging techniques for imbalanced data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine a business executive with thousands of confidential files regarding
    an important merger or acquisition. The analysts assigned to the case don’t have
    enough time to review all the files. Each can randomly select some files from
    the set and start reviewing them. Later, they can combine their insights in a
    meeting to draw conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: This scenario is a metaphor for a process in machine learning called bagging
    [1], which is short for **bootstrap aggregating**. In bagging, much like the analysts
    in the previous scenario, we create several subsets of the original dataset, train
    a weak learner on each subset, and then aggregate their predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Why use weak learners instead of strong learners? The rationale applies to
    both bagging and boosting methods (discussed later in this chapter). There are
    several reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Speed**: Weak learners are computationally efficient and inexpensive to execute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Diversity**: Weak learners are more likely to make different types of errors,
    which is advantageous when combining their predictions. Using strong learners
    could result in them all making the same type of error, leading to less effective
    ensembles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overfitting**: As a corollary to the previous point, the diversity in errors
    helps reduce the risk of overfitting in the ensemble.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interpretability**: While the ensemble as a whole may not be easily interpretable,
    its individual components – often simpler models – are easier to understand and
    interpret.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, back to bagging. The first step of the algorithm is called **bootstrapping**.
    In this step, we make several subsets or smaller groups of data by randomly picking
    items from the main data. The data is picked with the possibility of picking the
    same item more than once (this process is called “random sampling with replacement”),
    so these smaller groups may have some items in common. Then, we train our classifiers
    on each of these smaller groups.
  prefs: []
  type: TYPE_NORMAL
- en: The second step is called **aggregating**. The test sample is passed to each
    classifier at the time of prediction. After this, we take the average or majority
    prediction as the real answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in *Figure 4**.3*, the dataset is first sampled with replacement into
    three subsets. Then, separate classifiers are trained on each of the subsets.
    Finally, the results of the classifiers are combined at the time of prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Demonstrating how bagging works
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4**.4* summarizes the bagging algorithm in a pseudocode format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Bagging pseudocode
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll train a bagging classifier model from `sklearn` on the dataset we created
    previously. Since it’s possible to provide a base estimator to `BaggingClassifier`,
    we’ll use `DecisionTreeClassifier` with the maximum depth of the trees being `6`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s plot the decision boundary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You may refer to the definition of `plot_decision_boundary()` in the corresponding
    notebook on GitHub. We use the `DecisionBoundaryDisplay` API from the `sklearn.inspection`
    module to plot the decision boundary.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4**.5* shows the learned decision boundary on the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – The decision boundary of BaggingClassifier on the training data
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s also note the baseline metric of average precision when using this model
    on our test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 4**.6* shows the resulting PR curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – Precision-recall curve of BaggingClassifier on the test data
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some other metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In this chapter, we will also consider the **F2 score** (Fbeta-score with beta=2.0),
    which proportionally combines precision and recall, giving more weight to recall
    and less weight to precision.
  prefs: []
  type: TYPE_NORMAL
- en: So, what problems may we face when using `BaggingClassifier` on an imbalanced
    dataset? An obvious thing could be that when bootstrapping, some subsets on which
    base classifiers get trained may have very few minority class examples or none
    at all. This would mean that each of the individual base classifiers is going
    to perform poorly on the minority class, and combining their performance would
    still be poor.
  prefs: []
  type: TYPE_NORMAL
- en: We can combine undersampling techniques with bagging (one such method is UnderBagging)
    or oversampling techniques with bagging (one such method is OverBagging) to get
    better results. We will discuss such techniques next.
  prefs: []
  type: TYPE_NORMAL
- en: UnderBagging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The UnderBagging [2] technique uses random undersampling at the time of bootstrapping
    (or selection of subsets). We choose the whole set of the minority class examples
    for each classifier and bootstrap with replacement as many examples from the majority
    class as there are minority class examples. The aggregation step remains the same
    as in bagging. We can choose any classifier, say a decision tree, for training.
  prefs: []
  type: TYPE_NORMAL
- en: There are variants of UnderBagging where resampling with replacement of the
    minority class can also be applied to obtain more diverse ensembles.
  prefs: []
  type: TYPE_NORMAL
- en: 'The flowchart in *Figure 4**.7* represents the main steps in the UnderBagging
    algorithm with three subsets of data. It involves creating multiple subsets of
    data, performing random undersampling for the majority class in each subset, training
    classifiers on each subset, and finally combining the predictions of the classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – Demonstrating how the UnderBagging algorithm works
  prefs: []
  type: TYPE_NORMAL
- en: 'The `imbalanced-learn` library provides an implementation for `BalancedBaggingClassifier`.
    By default, this classifier uses a decision tree as the base classifier and `RandomUnderSampler`
    as the sampler via the `sampler` parameter. *Figure 4**.8* shows the decision
    boundary of the trained UnderBagging model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – The decision boundary of the UnderBagging classifier on the training
    data
  prefs: []
  type: TYPE_NORMAL
- en: 🚀 Bagging classifier in production at Microsoft
  prefs: []
  type: TYPE_NORMAL
- en: In a real-world application at Microsoft [3], the team faced a significant challenge
    in forecasting Live Site Incident escalations (previously mentioned in [*Chapter
    2*](B17259_02.xhtml#_idTextAnchor042), *Oversampling Methods*). The dataset was
    highly imbalanced, making it difficult for standard classifiers to perform well.
    To tackle this issue, Microsoft employed ensemble methods, specifically `BalancedBaggingClassifier`
    from the `imbalanced-learn` library. They used UnderBagging, where each bootstrap
    sample is randomly undersampled to get a balanced class distribution. As we have
    just discussed, UnderBagging uses all minority class samples and a random selection
    of majority class samples to train the model.
  prefs: []
  type: TYPE_NORMAL
- en: Bagged classification delivered the best results during their evaluation and
    also proved to be more consistent after they tracked it over a few months. They
    were able to significantly improve their forecasting accuracy for incident escalations.
  prefs: []
  type: TYPE_NORMAL
- en: OverBagging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Instead of random undersampling of the majority class samples, the minority
    class is oversampled (with replacement) at the time of bootstrapping. This method
    is called OverBagging [2]. As a variant, both minority and majority class examples
    can be resampled with replacements to achieve an equal number of majority and
    minority class examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'The flowchart in *Figure 4**.9* represents the main steps in the OverBagging
    algorithm with three subsets of data. It involves creating multiple subsets of
    data, performing random oversampling for the minority class in each subset, training
    classifiers on each subset, and finally combining the predictions of the classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – Demonstrating how the OverBagging algorithm works
  prefs: []
  type: TYPE_NORMAL
- en: For OverBagging, we can use the same `BalancedBaggingClassifier` with `RandomOverSampler`
    in the `sampler` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will see the following decision boundary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – The decision boundary of the OverBagging classifier on the training
    data
  prefs: []
  type: TYPE_NORMAL
- en: We will compare the performance metrics of these techniques after discussing
    the various bagging techniques.
  prefs: []
  type: TYPE_NORMAL
- en: SMOTEBagging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Can we use SMOTE at the time of bootstrapping instead of random oversampling
    of minority class examples? The answer is yes. The majority class will be bootstrapped
    with replacement, and the minority class will be sampled using SMOTE until a balancing
    ratio is reached.
  prefs: []
  type: TYPE_NORMAL
- en: The pseudocode for SMOTEBagging [2] is very similar to that for OverBagging,
    with the key difference being the use of the SMOTE algorithm instead of random
    oversampling to augment the minority class data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to OverBagging, we can implement `SMOTEBagging` using the `BalancedBagging``     Classifier` API with SMOTE as the `sampler` parameter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The decision boundary is not very different from OverBagging:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – The decision boundary of the SMOTEBagging classifier on the training
    data
  prefs: []
  type: TYPE_NORMAL
- en: A note about random forest and how it is related to bagging
  prefs: []
  type: TYPE_NORMAL
- en: Random forest [4] is another model that is based on the concept of bagging.
    The way the `RandomForestClassifier` and `BaggingClassifier` models from `sklearn`
    differ from each other is the fact that `RandomForestClassifier` considers a random
    subset of features while trying to decide the feature on which to split the nodes
    in the decision tree, while `BaggingClassifier` takes all the features.
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 4.1* highlights the difference between random forest and bagging classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **RandomForestClassifier** | **BaggingClassifier** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Base classifier | Decision trees | Any classifier. |'
  prefs: []
  type: TYPE_TB
- en: '| Bootstrap sampling | Yes | Yes. |'
  prefs: []
  type: TYPE_TB
- en: '| Take a subset of features | Yes (at each node) | No, by default. We can use
    the `max_features` hyperparameter to take subsets of features. |'
  prefs: []
  type: TYPE_TB
- en: '| Works best with? | Any tabular data, but it shines with large feature sets
    | Any tabular data, but it’s best when the base classifier is carefully chosen.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Handles missing values and outliers | Yes, inherently | Depends on the base
    classifier. |'
  prefs: []
  type: TYPE_TB
- en: Table 4.1 – RandomForestClassifier versus BaggingClassifier
  prefs: []
  type: TYPE_NORMAL
- en: The `imbalanced-learn` library provides the `BalancedRandomForestClassifier`
    class to tackle the imbalanced datasets where each of the bootstraps is undersampled
    before the individual decision trees are trained. As an exercise, we encourage
    you to learn about `BalancedRandomForestClassifier`. See how it relates to the
    other techniques we just discussed. Also, try out the various sampling strategies
    and explore the parameters this class offers.
  prefs: []
  type: TYPE_NORMAL
- en: Comparative performance of bagging methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s compare the performance of various bagging methods using the same dataset
    we’ve employed so far. We’ll use the decision tree as a baseline and evaluate
    different techniques across several performance metrics. The highest values for
    each metric across all techniques are highlighted in bold:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **TECHNIQUE** | **F2** | **PRECISION** | **RECALL** | **AVERAGE PRECISION**
    | **AUC-ROC** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SMOTEBagging | **0.928** | 0.754 | 0.985 | 0.977 | **1.000** |'
  prefs: []
  type: TYPE_TB
- en: '| OverBagging | 0.888 | 0.612 | **1.000** | 0.976 | **1.000** |'
  prefs: []
  type: TYPE_TB
- en: '| UnderBagging | 0.875 | 0.609 | 0.981 | 0.885 | 0.999 |'
  prefs: []
  type: TYPE_TB
- en: '| Bagging | 0.891 | 0.967 | 0.874 | 0.969 | **1.000** |'
  prefs: []
  type: TYPE_TB
- en: '| Balanced random forest | 0.756 | 0.387 | 0.993 | 0.909 | 0.999 |'
  prefs: []
  type: TYPE_TB
- en: '| Random forest | 0.889 | **0.975** | 0.870 | **0.979** | **1.000** |'
  prefs: []
  type: TYPE_TB
- en: '| Decision tree | 0.893 | 0.960 | 0.878 | 0.930 | 0.981 |'
  prefs: []
  type: TYPE_TB
- en: Table 4.2 – Performance comparison of various bagging techniques
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some conclusions we can draw from *Table 4.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: For maximizing the F2 score, **SMOTEBagging** did the best
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For high precision, **bagging** and **random forest** performed exceptionally
    well
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For high recall, **OverBagging** and **balanced random forest** are strong choices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For general performance across all metrics, **SMOTEBagging** and **bagging**
    proved to be solid options
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In conclusion, although ensemble approaches such as bagging and random forest
    establish robust benchmarks that are challenging to outperform, incorporating
    imbalanced learning strategies such as SMOTEBagging can lead to notable gains.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our discussion of bagging techniques. If bagging is the wisdom
    of the crowd, boosting is the master sculptor, refining the previous art with
    each stroke. We’ll try to understand how boosting works in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting techniques for imbalanced data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Imagine two friends doing group study to solve their mathematics assignment.
    The first student is strong in most topics but weak in two topics: complex numbers
    and triangles. So, the first student asks the second student to spend more time
    on these two topics. Then, while solving the assignments, they combine their answers.
    Since the first student knows most of the topics well, they decided to give more
    weight to his answers to the assignment questions. What these two students are
    doing is the key idea behind boosting.'
  prefs: []
  type: TYPE_NORMAL
- en: In bagging, we noticed that we could train all the classifiers in parallel.
    These classifiers are trained on a subset of the data, and all of them have an
    equal say at the time of prediction.
  prefs: []
  type: TYPE_NORMAL
- en: In boosting, the classifiers are trained one after the other. While every classifier
    learns from the whole data, points in the dataset are assigned different weights
    based on their difficulty of classification. Classifiers are also assigned weights
    that tell us about their predictive power. While predicting new data, the weighted
    sum of the classifiers is used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Boosting begins by training the first classifier on the whole dataset, with
    each data point assigned the same weight. In the second iteration, the data points
    that were misclassified in the first iteration are given more weight, and a second
    classifier is trained with these new weights. A weight is also assigned to the
    classifiers themselves based on their overall performance. This process continues
    through multiple iterations with different classifiers. *Figure 4**.12* illustrates
    this concept for a two-class dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_04_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.12 – Boosting idea: (left) the decision boundary from the first classifier;
    (middle) the weights of misclassified data points are bumped up for the second
    classifier; (right) the decision boundary from the second classifier'
  prefs: []
  type: TYPE_NORMAL
- en: The kind of boosting we just described is called **AdaBoost**. There is another
    category of boosting algorithms called gradient boosting, where the main focus
    is on minimizing the residuals (the difference between the actual value and predicted
    output value) of the previous model, trying to correct the previous model’s mistakes.
    There are several popular gradient boosting implementations, such as **XGBoost**,
    **LightGBM**, and **CatBoost**.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will mostly focus on AdaBoost and modify it to account for
    data imbalance. However, swapping AdaBoost with XGBoost, for example, shouldn’t
    be too difficult.
  prefs: []
  type: TYPE_NORMAL
- en: AdaBoost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AdaBoost, short for adaptive boosting, is one of the earliest boosting methods
    based on decision trees. Decision trees are classifiers that are easy to ensemble
    together:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_04_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.13 – AdaBoost pseudocode
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how to import the classifier from the `sklearn` library
    and train it on the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s plot what the decision boundary looks like after the model gets trained
    on the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 4**.14* shows the decision boundary of the model on the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_04_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.14 – The decision boundary of AdaBoostClassifier on the training data
  prefs: []
  type: TYPE_NORMAL
- en: We can make oversampling and undersampling an integral part of the boosting
    algorithm, similar to how we did for the bagging algorithm. We will discuss that
    next.
  prefs: []
  type: TYPE_NORMAL
- en: RUSBoost, SMOTEBoost, and RAMOBoost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As you might have guessed, we can combine AdaBoost with resampling techniques.
    Here is the main idea: at each boosting iteration, before training a classifier
    on the incorrect examples from the previous iteration, we sample the data (via
    some undersampling or oversampling variant). Here’s the general pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input**: The training data and some decision tree classifiers.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Output**: An aggregated classifier.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the equal weights for all the samples of the training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Repeat this for each decision tree classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Resample the data using a data sampling method:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If the sampling method used is **Random UnderSampling** (**RUS**), the method
    is called **RUSBoost** [5].
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If the sampling method used is SMOTE, the method is called **SMOTEBoost** [6].
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: In **RAMOBoost** (short for **Ranked Minority Oversampling in Boosting** [7]),
    oversampling of the minority class is done based on the weight of the minority
    class examples. If the weight of an example is more (because the model didn’t
    do well on that example in the previous iteration), then it’s oversampled more,
    and vice versa.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a classifier on the resampled data, giving more importance to samples
    with higher weights based on previous iterations.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the error for the classifier on the given data by comparing its predictions
    with the actual outputs.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Consider all the wrongly classified examples for the next iteration. Increase
    the weights of such wrongly classified examples.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Combine all the decision tree classifiers into a final classifier, where the
    classifiers with smaller error values on the training data have a larger say in
    the final prediction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In this pseudocode, *Step 4 (I)* is the only extra step we have added compared
    to the AdaBoost algorithm. Let’s discuss the pros and cons of these techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: In RUSBoost, as the data is reduced, we tend to have a faster training time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SMOTEBoost produces synthetic samples from the minority class. Thus, it adds
    diversity to the data and may improve the classifier’s accuracy. However, it would
    increase the time to train and may not be scalable to very large datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAMOBoost gives preference to the samples near the class boundaries. This can
    improve performance in some cases. However, like SMOTEBoost, this method may increase
    the training time and cost and may cause overfitting of the final model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `imbalanced-learn` library provides the implementation for `RUSBoostClassifier`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s examine the decision boundary of the trained model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting plot is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_04_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.15 – The decision boundary of RUSBoostClassifier on training data
  prefs: []
  type: TYPE_NORMAL
- en: The `imbalanced-learn` library doesn’t have the implementations of RAMOBoost
    and SMOTEBoost yet (as of version 0.11.0). You can check the open source repository
    at [https://github.com/dialnd/imbalanced-algorithms](https://github.com/dialnd/imbalanced-algorithms)
    for reference implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Can we create multiple subsets of the majority class, train an ensemble from
    each of these subsets, and combine all weak classifiers in these ensembles into
    a final output? This approach will be explored in the next section, where we will
    utilize the ensemble of ensembles technique.
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble of ensembles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Can we combine boosting and bagging? As we saw earlier, in bagging, we create
    multiple subsets of data and then train classifiers on those datasets. We can
    treat AdaBoost as a classifier while doing bagging. The process is simple: first,
    we create the bags and then train different AdaBoost classifiers on each bag.
    Here, AdaBoost is an ensemble in itself. Thus, these models are called an **ensemble**
    **of ensembles**.'
  prefs: []
  type: TYPE_NORMAL
- en: On top of having an ensemble of ensembles, we can also do undersampling (or
    oversampling) at the time of bagging. This gives us the **benefits of bagging**,
    **boosting**, and **random undersampling** (or oversampling) in a single model.
    We will discuss one such algorithm in this section, called **EasyEnsemble**. Since
    random undersampling doesn’t have significant overhead, both algorithms have training
    times similar to any other algorithm with the same number of weak classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: EasyEnsemble
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The EasyEnsemble algorithm [8] generates balanced datasets from the original
    dataset and trains a different AdaBoost classifier on each of the balanced datasets.
    Later, it creates an aggregate classifier that makes predictions based on the
    majority votes of the AdaBoost classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_04_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.16 – EasyEnsemble pseudocode
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4**.17* summarizes the EasyEnsemble algorithm using three subsets of
    the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_04_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.17 – EasyEnsemble algorithm explained
  prefs: []
  type: TYPE_NORMAL
- en: Instead of randomly undersampling the majority class examples, we can randomly
    oversample the minority class examples too.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `imbalanced-learn` library provides the API for EasyEnsemble using `EasyEnsembleClassifier`.
    The `EasyEnsembleClassifier` API provides a `base_estimator` argument that can
    be used to set any classifier, with the default being `AdaBoostClassifier`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s plot the decision boundary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B17259_04_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.18 – The decision boundary of EasyEnsembleClassifier on the training
    data
  prefs: []
  type: TYPE_NORMAL
- en: By default, EasyEnsemble uses `AdaBoostClassifier` as the base estimator. However,
    we can use any other estimator as well, such as `XGBoostClassifier`, or tune it
    in other ways, say by passing another `sampling_strategy`.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our discussion of EasyEnsemble. Next, we will compare the various
    boosting methods that we’ve studied.
  prefs: []
  type: TYPE_NORMAL
- en: Comparative performance of boosting methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s compare the performance of the various boosting methods we’ve discussed.
    We use a decision tree as a baseline and RUSBoost, AdaBoost, XGBoost, and EasyEnsemble,
    along with two variants. By default, `EasyEnsembleClassifier` uses `AdaBoostClassifier`
    as a baseline estimator. We use XGBoost instead as the estimator in the second
    variant of `EasyEnsembleClassifier`; in the third variant, we use `not majority`
    for our `sampling_strategy`, along with the XGBoost estimator:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Technique** | **F2 Score** | **Precision** | **Recall** | **Average Precision**
    | **AUC-ROC** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| EasyEnsemble (`estimator=XGBoost` and `sampling_strategy =` `not_majority`)
    | 0.885 | 0.933 | 0.874 | **0.978** | **1.000** |'
  prefs: []
  type: TYPE_TB
- en: '| EasyEnsemble (`estimator=XGBoost`) | 0.844 | 0.520 | **1.000** | **0.978**
    | 0.999 |'
  prefs: []
  type: TYPE_TB
- en: '| EasyEnsemble | 0.844 | 0.519 | **1.000** | 0.940 | 0.999 |'
  prefs: []
  type: TYPE_TB
- en: '| RUSBoost | 0.836 | 0.517 | 0.989 | 0.948 | **1.000** |'
  prefs: []
  type: TYPE_TB
- en: '| AdaBoost | **0.907** | 0.938 | 0.900 | **0.978** | **1.000** |'
  prefs: []
  type: TYPE_TB
- en: '| XGBoost | 0.885 | 0.933 | 0.874 | 0.968 | **1.000** |'
  prefs: []
  type: TYPE_TB
- en: '| Decision Tree | 0.893 | **0.960** | 0.878 | 0.930 | 0.981 |'
  prefs: []
  type: TYPE_TB
- en: Table 4.3 – Performance comparison of various boosting techniques
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some conclusions from *Table 4.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: For the highest F2 score, AdaBoost is the best choice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For high precision, the plain decision tree beats all other techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For perfect recall, EasyEnsemble (`estimator=XGBoost`) and EasyEnsemble perform
    perfectly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For overall balanced performance, AdaBoost and EasyEnsemble (`estimator=XGBoost`
    and `sampling_strategy=not_majority`) are strong contenders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ensembling techniques such as RUSBoost and EasyEnsemble are specifically
    designed for handling data imbalance and improving recall compared to a baseline
    model such as the decision tree or even AdaBoost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, the results indicate that while ensemble methods such as AdaBoost and
    XGBoost provide robust baselines that are hard to beat, leveraging imbalanced
    learning techniques can indeed modify the decision boundaries of the resulting
    classifiers, which can potentially help with improving the recall. The efficacy
    of these techniques, however, largely depends on the dataset and performance metric
    under consideration.
  prefs: []
  type: TYPE_NORMAL
- en: By wrapping up our journey through the ensemble of ensembles, we’ve added yet
    another powerful and dynamic tool to our machine learning arsenal.
  prefs: []
  type: TYPE_NORMAL
- en: Model performance comparison
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The effectiveness of the techniques we’ve discussed so far can be highly dependent
    on the dataset they are applied to. In this section, we will conduct a comprehensive
    comparative analysis that compares the various techniques we have discussed so
    far while using the logistic regression model as a baseline. For a comprehensive
    review of the complete implementation, please consult the accompanying notebook
    available on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: 'The analysis spans four distinct datasets, each with its own characteristics
    and challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Synthetic data with Sep: 0.5**: A simulated dataset with moderate separation
    between classes, serving as a baseline to understand algorithm performance in
    simplified conditions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Synthetic data with Sep: 0.9**: Another synthetic dataset, but with a higher
    degree of separation, allowing us to examine how algorithms perform as class distinguishability
    improves.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`imblearn`) related to healthcare, chosen for its practical importance and
    the natural class imbalance often seen in medical datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`imblearn` as well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our primary metric for evaluation is average precision, a summary measure that
    combines both precision and recall, thereby providing a balanced view of algorithm
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: We’d like to emphasize that we are using the vanilla versions of the various
    ensemble models for comparison. With some additional effort in tuning the hyperparameters
    of these models, we could certainly enhance the performance of these implementations.
    We leave that as an exercise for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'By comparing these diverse algorithms across a variety of datasets, this analysis
    aims to provide some valuable insights into the following aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: How conventional and specialized techniques stack up against each other
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dependency of algorithm effectiveness on dataset characteristics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The practical implications of choosing one algorithm over another in different
    scenarios
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 4**.19* compares the performance of various bagging and boosting techniques
    using the average precision score, while using the logistic regression model as
    a baseline, over two synthetic datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_04_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.19 – Average precision scores on synthetic datasets
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4**.20* shows similar plots across two real-world datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_04_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.20 – Average precision scores on the thyroid_sick and abalone_19 datasets
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s analyze these results for each of the datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Synthetic data with Sep 0.5** (*Figure 4**.19*, left): XGBoost and logistic
    regression performed the best in terms of average precision, scoring 0.30 and
    0.27, respectively. Interestingly, ensemble methods designed specifically for
    imbalanced data, such as SMOTEBagging and OverBagging, perform comparably or even
    worse than conventional methods such as bagging. This suggests that specialized
    methods do not always guarantee an advantage in simpler synthetic settings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Synthetic data with Sep 0.9** (*Figure 4**.19*, right): EasyEnsemble takes
    the lead on this dataset with an average precision score of 0.64, closely followed
    by logistic regression and XGBoost. This higher separation seems to allow EasyEnsemble
    to capitalize on its focus on balancing, leading to better performance. Other
    ensemble methods such as UnderBagging and OverBagging perform reasonably but do
    not surpass the leaders.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Thyroid sick dataset** (*Figure 4**.20*, left): In a real-world dataset focusing
    on thyroid sickness, XGBoost far outperforms all other methods with an average
    precision of 0.97\. Other ensemble methods such as bagging, OverBagging, and SMOTEBagging
    also score high, suggesting that ensembles are particularly effective for this
    dataset. Interestingly, boosting and RUSBoost do not keep pace, indicating that
    not all boosting variants are universally effective.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Abalone 19 dataset** (*Figure 4**.20*, right): For the Abalone 19 dataset,
    all methods perform relatively poorly, with XGBoost standing out with an average
    precision of 0.13\. EasyEnsemble comes in second with a score of 0.09, while traditional
    methods such as logistic regression and bagging lag behind. This could indicate
    that the dataset is particularly challenging for most methods, and specialized
    imbalanced techniques can only make marginal improvements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are some overall insights:'
  prefs: []
  type: TYPE_NORMAL
- en: Conventional methods such as XGBoost and logistic regression often provide strong
    baselines that are difficult to beat
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The efficacy of specialized imbalanced learning techniques can vary significantly,
    depending on the dataset and its inherent complexities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble methods generally perform well across various datasets, but their effectiveness
    can be context-dependent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of performance metric – in this case, average precision – can significantly
    influence the evaluation, making it crucial to consider multiple metrics for a
    comprehensive understanding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We hope that this chapter has shown how you can incorporate sampling techniques
    with ensemble methods to achieve improved results, especially when dealing with
    imbalanced data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ensemble methods in machine learning create strong classifiers by combining
    results from multiple weak classifiers using approaches such as bagging and boosting.
    However, these methods assume balanced data and may struggle with imbalanced datasets.
    Combining ensemble methods with sampling methods such as oversampling and undersampling
    leads to techniques such as UnderBagging, OverBagging, and SMOTEBagging, all of
    which can help address imbalanced data issues.
  prefs: []
  type: TYPE_NORMAL
- en: Ensembles of ensembles, such as EasyEnsemble, combine boosting and bagging techniques
    to create powerful classifiers for imbalanced datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble-based imbalance learning techniques can be an excellent addition to
    your toolkit. The ones based on KNN, viz., SMOTEBoost, and RAMOBoost can be slow.
    However, the ensembles based on random undersampling and random oversampling are
    less costly. Also, boosting methods are found to sometimes work better than bagging
    methods in the case of imbalanced data. We can combine random sampling techniques
    with boosting to get better overall performance. As we emphasized previously,
    it’s empirical, and we have to try to know what would work best for our data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to change the model to account for the
    imbalance in data and the various costs incurred by the model because of misclassifying
    the minority class examples.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Try using `RUSBoostClassifier` on the `abalone_19` dataset and compare the performance
    with other techniques from the previous chapters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between the `BalancedRandomForestClassifier` and `BalancedBaggingClassifier`
    classes in the `imbalanced-learn` library?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'L. Breiman, *Bagging predictors*, Mach Learn, vol. 24, no. 2, pp. 123–140,
    Aug. 1996, doi: 10.1007/BF00058655, [https://link.springer.com/content/pdf/10.1007/BF00058655.pdf](https://link.springer.com/content/pdf/10.1007/BF00058655.pdf).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '(The paper that introduced OverBagging, UnderBagging, and SMOTEBagging) S.
    Wang and X. Yao, *Diversity analysis on imbalanced data sets by using ensemble
    models*, in 2009 IEEE Symposium on Computational Intelligence and Data Mining,
    Nashville, TN, USA: IEEE, Mar. 2009, pp. 324–331\. doi: 10.1109/CIDM.2009.4938667,
    [https://www.cs.bham.ac.uk/~wangsu/documents/papers/CIDMShuo.pdf](https://www.cs.bham.ac.uk/~wangsu/documents/papers/CIDMShuo.pdf).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Live Site Incident escalation forecast* (2023), [https://medium.com/data-science-at-microsoft/live-site-incident-escalation-forecast-566763a2178](https://medium.com/data-science-at-microsoft/live-site-incident-escalation-forecast-566763a2178)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'L. Breiman, *Random Forests*, Machine Learning, vol. 45, no. 1, pp. 5–32, 2001,
    doi: 10.1023/A:1010933404324, [https://link.springer.com/content/pdf/10.1023/A:1010933404324.pdf](https://link.springer.com/content/pdf/10.1023/A:1010933404324.pdf).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '(The paper that introduced the RUSBoost algorithm) C. Seiffert, T. M. Khoshgoftaar,
    J. Van Hulse, and A. Napolitano, *RUSBoost: A Hybrid Approach to Alleviating Class
    Imbalance*, IEEE Trans. Syst., Man, Cybern. A, vol. 40, no. 1, pp. 185–197, Jan.
    2010, doi: 10.1109/TSMCA.2009.2029559, [https://www.researchgate.net/profile/Jason-Van-Hulse/publication/224608502_RUSBoost_A_Hybrid_Approach_to_Alleviating_Class_Imbalance/links/0912f50f4bec299a8c000000/RUSBoost-A-Hybrid-Approach-to-Alleviating-Class-Imbalance.pdf](https://www.researchgate.net/profile/Jason-Van-Hulse/publication/224608502_RUSBoost_A_Hybrid_Approach_to_Alleviating_Class_Imbalance/links/0912f50f4bec299a8c000000/RUSBoost-A-Hybrid-Approach-to-Alleviating-Class-Imbalance.pdf).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '(The paper that introduced the SMOTEBoost algorithm) N. V. Chawla, A. Lazarevic,
    L. O. Hall, and K. W. Bowyer, *SMOTEBoost: Improving Prediction of the Minority
    Class in Boosting*, in Knowledge Discovery in Databases: PKDD 2003, N. Lavrač,
    D. Gamberger, L. Todorovski, and H. Blockeel, Eds., in Lecture Notes in Computer
    Science, vol. 2838\. Berlin, Heidelberg: Springer Berlin Heidelberg, 2003, pp.
    107–119\. doi: 10.1007/978-3-540-39804-2_12, [https://www3.nd.edu/~dial/publications/chawla2003smoteboost.pdf](https://www3.nd.edu/~dial/publications/chawla2003smoteboost.pdf).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '(The paper that introduced RAMOBoost algorithm) Sheng Chen, Haibo He, and E.
    A. Garcia, *RAMOBoost: Ranked Minority Oversampling in Boosting*, IEEE Trans.
    Neural Netw., vol. 21, no. 10, pp. 1624–1642, Oct. 2010, doi: 10.1109/TNN.2010.2066988,
    [https://ieeexplore.ieee.org/abstract/document/5559472](https://ieeexplore.ieee.org/abstract/document/5559472).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '(The paper that introduced EasyEnsemble) Xu-Ying Liu, Jianxin Wu, and Zhi-Hua
    Zhou, *Exploratory Undersampling for Class-Imbalance Learning*, IEEE Trans. Syst.,
    Man, Cybern. B, vol. 39, no. 2, pp. 539–550, Apr. 2009, doi: 10.1109/TSMCB.2008.2007853,
    [http://129.211.169.156/publication/tsmcb09.pdf](http://129.211.169.156/publication/tsmcb09.pdf).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
