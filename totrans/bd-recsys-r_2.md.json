["```py\nx1 <- rnorm(30)\nx2 <- rnorm(30)\nEuc_dist = dist(rbind(x1,x2) ,method=\"euclidean\")\n```", "```py\nvec1 = c( 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0 )\nvec2 = c( 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0 )\nlibrary(lsa)\ncosine(vec1,vec2)\n```", "```py\nCoef = cor(mtcars, method=\"pearson\")\nwhere mtcars is the dataset\n```", "```py\n#PCA\ndata(USArrests)\nhead(states)\n[1] \"Alabama\"    \"Alaska\"     \"Arizona\"    \"Arkansas\"   \"California\" \"Colorado\"\n\nnames(USArrests)\n[1] \"Murder\"   \"Assault\"  \"UrbanPop\" \"Rape\"\n\n#let us use apply() to the USArrests dataset row wise to calculate the variance to see how each variable is varying\napply(USArrests , 2, var)\n\nMurder    Assault   UrbanPop       Rape\n  18.97047 6945.16571  209.51878   87.72916\n#We observe that Assault has the most variance. It is important to note at this point that\n\n#Scaling the features is a very step while applying PCA.\n\n#Applying PCA after scaling the feature as below\npca =prcomp(USArrests , scale =TRUE)\n\npca\n```", "```py\n[1] 1.5748783 0.9948694 0.5971291 0.4164494\n```", "```py\n                PC1        PC2        PC3         PC4\nMurder   -0.5358995  0.4181809 -0.3412327  0.64922780\nAssault  -0.5831836  0.1879856 -0.2681484 -0.74340748\nUrbanPop -0.2781909 -0.8728062 -0.3780158  0.13387773\nRape     -0.5434321 -0.1673186  0.8177779  0.08902432\n\n#Now lets us understand the components of pca output.\n\nnames(pca)\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"\n\n#Pca$rotation  contains the principal component loadings matrix which explains\n\n#proportion of each variable along each principal component.\n\n#now let us learn interpreting the results of pca using biplot graph. Biplot is used to how the proportions of each variable along the two principal components.\n\n#below code changes the directions of the biplot, if we donot include the below two lines the plot will be mirror image to the below one.\npca$rotation=-pca$rotation\npca$x=-pca$x\nbiplot (pca , scale =0)\n```", "```py\n#k-means clustering\nlibrary(cluster)\ndata(iris)\niris$Species = as.numeric(iris$Species)\nkmeans<- kmeans(x=iris, centers=5)\nclusplot(iris,kmeans$cluster, color=TRUE, shade=TRUE,labels=13, lines=0)\n```", "```py\nlibrary(cluster)\nlibrary(ggplot2)\ndata(iris)\niris$Species = as.numeric(iris$Species)\ncost_df <- data.frame()\nfor(i in 1:100){\nkmeans<- kmeans(x=iris, centers=i, iter.max=50)\ncost_df<- rbind(cost_df, cbind(i, kmeans$tot.withinss))\n}\nnames(cost_df) <- c(\"cluster\", \"cost\")\n#Elbow method to identify the idle number of Cluster\n#Cost plot\nggplot(data=cost_df, aes(x=cluster, y=cost, group=1)) +\ntheme_bw(base_family=\"Garamond\") +\ngeom_line(colour = \"darkgreen\") +\ntheme(text = element_text(size=20)) +\nggtitle(\"Reduction In Cost For Values of 'k'\\n\") +\nxlab(\"\\nClusters\") +\nylab(\"Within-Cluster Sum of Squares\\n\")\n```", "```py\n  #SVM\nlibrary(e1071)\ndata(iris)\nsample = iris[sample(nrow(iris)),]\ntrain = sample[1:105,]\ntest = sample[106:150,]\ntune =tune(svm,Species~.,data=train,kernel =\"radial\",scale=FALSE,ranges =list(cost=c(0.001,0.01,0.1,1,5,10,100)))\ntune$best.model\n```", "```py\nbest.tune(method = svm, train.x = Species ~ ., data = train, ranges = list(cost = c(0.001,\n    0.01, 0.1, 1, 5, 10, 100)), kernel = \"radial\", scale = FALSE)\n```", "```py\n   SVM-Type:  C-classification\n SVM-Kernel:  radial\n       cost:  10\n      gamma:  0.25\n\nNumber of Support Vectors:  25\n\nsummary(tune)\n\nParameter tuning of 'svm':\n- sampling method: 10-fold cross validation\n- best parameters:\n cost\n   10\n- best performance: 0.02909091\n- Detailed performance results:\n   cost      error dispersion\n1 1e-03 0.72909091 0.20358585\n2 1e-02 0.72909091 0.20358585\n3 1e-01 0.04636364 0.08891242\n4 1e+00 0.04818182 0.06653568\n5 5e+00 0.03818182 0.06538717\n6 1e+01 0.02909091 0.04690612\n7 1e+02 0.07636364 0.08679584\n\nmodel =svm(Species~.,data=train,kernel =\"radial\",cost=10,scale=FALSE)\n// cost =10 is chosen from summary result of tune variable\n```", "```py\npred = predict(model,test)\n```", "```py\nlibrary(tree)\ndata(iris)\nsample = iris[sample(nrow(iris)),]\ntrain = sample[1:105,]\ntest = sample[106:150,]\nmodel = tree(Species~.,train)\nsummary(model)\n```", "```py\ntree(formula = Species ~ ., data = train, x = TRUE, y = TRUE)\nVariables actually used in tree construction:\n[1] \"Petal.Length\" \"Sepal.Length\" \"Petal.Width\"\nNumber of terminal nodes:  5\nResidual mean deviance:  0.1332 = 13.32 / 100\nMisclassification error rate: 0.0381 = 4 / 105 '\n//plotting the decision tree\nplot(model)text(model)\npred = predict(model,test[,-5],type=\"class\")\n> pred\n [1] setosa     setosa     virginica  setosa     setosa     setosa     versicolor\n [8] virginica  virginica  setosa     versicolor versicolor virginica  versicolor\n[15] virginica  virginica  setosa     virginica  virginica  versicolor virginica\n[22] versicolor setosa     virginica  setosa     versicolor virginica  setosa    \n[29] versicolor versicolor versicolor virginica  setosa     virginica  virginica\n[36] versicolor setosa     versicolor setosa     versicolor versicolor setosa    \n[43] versicolor setosa     setosa    \nLevels: setosa versicolor virginica\n```", "```py\n#randomForest\nlibrary(randomForest)\ndata(iris)\nsample = iris[sample(nrow(iris)),]\ntrain = sample[1:105,]\ntest = sample[106:150,]\nmodel =randomForest(Species~.,data=train,mtry=2,importance =TRUE,proximity=TRUE)\nmodel\n```", "```py\n randomForest(formula = Species ~ ., data = train, mtry = 2, importance = TRUE,      proximity = TRUE)\n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 5.71%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa         40          0         0  0.00000000\nversicolor      0         28         3  0.09677419\nvirginica       0          3        31  0.08823529\n\npred = predict(model,newdata=test[,-5])\npred\npred\n       119         77         88         90         51         20         96\n virginica versicolor versicolor versicolor versicolor     setosa versicolor\n         1          3        118        127          6        102          5\n    setosa     setosa  virginica  virginica     setosa  virginica     setosa\n        91          8         23        133         17         78         52\nversicolor     setosa     setosa  virginica     setosa  virginica versicolor\n        63         82         84        116         70         50        129\nversicolor versicolor  virginica  virginica versicolor     setosa  virginica\n       150         34          9        120         41         26        121\n virginica     setosa     setosa  virginica     setosa     setosa  virginica\n       145        138         94          4        104         81        122\n virginica  virginica versicolor     setosa  virginica versicolor  virginica\n        18        105        100\n    setosa  virginica versicolor\nLevels: setosa versicolor virginica\n```", "```py\n#Boosting in R\nlibrary(gbm)\ndata(iris)\nsample = iris[sample(nrow(iris)),]\ntrain = sample[1:105,]\ntest = sample[106:150,]\nmodel = gbm(Species~.,data=train,distribution=\"multinomial\",n.trees=5000,interaction.depth=4)\nsummary(model)\n```", "```py\n//the preceding summary states the relative importance of the variables of the model.\n\npred = predict(model,newdata=test[,-5],n.trees=5000)\n\npred[1:5,,]\n        setosa versicolor virginica\n[1,]  5.630363  -2.947531 -5.172975\n[2,]  5.640313  -3.533578 -5.103582\n[3,] -5.249303   3.742753 -3.374590\n[4,] -5.271020   4.047366 -3.770332\n[5,] -5.249324   3.819050 -3.439450\n\n//pick the response with the highest probability from the resulting pred matrix, by doing apply(.., 1, which.max) on the vector output from prediction.\np.pred <- apply(pred,1,which.max)\np.pred\n[1] 1 1 3 3 2 2 3 1 3 1 3 2 2 1 2 3 2 2 3 3 1 1 3 1 3 3 3 1 1 2 2 2 2 2 2 2 1 1 3 1 2\n[42] 1 3 2 3\n```"]