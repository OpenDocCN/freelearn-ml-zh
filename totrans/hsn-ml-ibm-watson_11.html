<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Creating a Facial Expression Platform on IBM Cloud</h1>
                </header>
            
            <article>
                
<p><span>In this chapter, we will</span> <span>cover a complete IBM Cloud-based expression classification solution that will use deep lear</span>ning machine learning<span> techniques on the IBM Cloud platform. The solution will implement a simple, yet efficient, ML model using TensorFlow and the ML services available with IBM Watson Studio.<span class="Apple-converted-space"> </span></span><span>The goal is to illustrate an end-to-end solution for a complex ML task.</span></p>
<p><span>We will divide this chapter into the following areas:</span></p>
<ul>
<li><span>Understanding facial expression classification</span></li>
<li><span>Exploring expression databases</span></li>
<li><span>Preprocessing faces</span></li>
<li><span>Learning the expression classifier</span></li>
<li><span>Evaluating the expression classifier</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding facial expression classification</h1>
                </header>
            
            <article>
                
<p><span>Let's start with a brief discussion leading up to just what we mean when we say <em>facial expression classification</em>.<span class="Apple-converted-space"> </span></span></p>
<p><span>In <a href="99762d15-664d-4987-82cf-74440dedabb3.xhtml">Chapter 7</a>, <em>Deep Learning Using TensorFlow on the IBM Cloud</em>, we used IBM Watson Studio to perform <em>object detection</em> within random images. In that use case, we asked our project's model to find or detect common or known objects that might be pictured within an image. For example, we submitted an image of an animal and the solution correctly detected and identified a pinto horse, although no further information about the detected object was produced, such as whether the horse was angry or frightened.</span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Face detection</h1>
                </header>
            
            <article>
                
<p><span>Perhaps the next step on this journey (after object detection within an image)</span> is face de<span>tection.</span> <span>Face detection is a computer technology being applied in a variety of applications that strive to identify human faces within digital images.<span class="Apple-converted-space"> </span></span></p>
<p><span>Facial recognition is a way of detecting and identifying a human face through the use of technology. A facial recognition solution utilizes th</span>e logic of biometrics to m<span>ap facial features from a photograph (or even video) and then compares the information with a database of known faces, looking for a match.</span></p>
<div class="packt_infobox"><span>Biometrics is the measurement and statistical examination of people's unique physical and behavioral characteristics, based upon the principle that each and every individual can be accurately identified by his or her intrinsic physical or behavioral traits.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Facial expression analysis</h1>
                </header>
            
            <article>
                
<p><span>Now we can get to the point: <em>facial expression analysis</em>.<span class="Apple-converted-space"> </span></span></p>
<p><span>This concept commonly classifies all</span> <span>facial expressions as relating to one of the six universal emotions—</span>j<span>oy (happiness)</span><span>,</span> s<span>urprise</span><span>,</span> d<span>isgust</span><span>,</span> s<span>adness</span><span>,</span> a<span>nger</span><span>,</span> and f<span>ear</span> <span>as well as</span> n<span>eutral</span><span>.<span class="Apple-converted-space"> </span></span></p>
<p><span>Emotions are one element that makes us human and (believe it or not) they are difficult to hide, since all emotions, suppressed or not, are likely to have a noticeable physical effect that can be of value if we can automate the process of detecting and then interpreting the physical effects.</span></p>
<p><span>Once detected, the interpretation (of facial expressions) process is (perhaps) just another classification exercise. In practice, you'll find that facial expression classification will be based upon what is known as <strong>TBM</strong> or the <strong>transferable belief model</strong> framework. <span class="Apple-converted-space"> </span></span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TBM</h1>
                </header>
            
            <article>
                
<p><span>TBM offers an interesting premise. Without intending to provide a comprehensive explanation of the TBM framework, a key point is th</span>at it introduces degrees of belief and transfer (gi<span>ving rise to the name of the met</span>hod: the transferable belief model), which <span>allows the model to make the necessary assumptions required to perform adequate classification (of the expressions). Basically, this means it scores its assumptions, that is, the assumption that the expression is a happy expression is determined to have a <em>n</em> percentage chance of being correct (we'll see this in action later in this chapter when we review the results of our project).</span></p>
<p><span>Further (and I'm oversimplifying), TBM looks to us</span>e quantified beliefs to make it<span>s classification decisions. Something perhaps more easily understood is that facial expression analysis</span> <span>extracts an expression skeleton of facial features (the mouth, eyes and eyebrows) and then derives simple distance coefficients from facial images.</span> These characteristic distances are then fed to a rule-based decision system that relies on TBM in order to assign a facial expression to the face image. </p>
<p><span>Again, the goal is not to define the theory behind TBM, or even the intimate details of a facial expression analysis solution, but more to show a working example of such; therefore, we will go on to the next section and our use case example and leave to the reader the further work of researching this topic.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring expression databases</h1>
                </header>
            
            <article>
                
<p><span>At the core of</span> <span>all</span> <span>facial expression analysis solutions is</span> an expression database. </p>
<p><span>A (<strong>facial</strong>) <strong>expression database</strong> is a collection of images showing the specific</span> <span>facial expressions</span><span> of a range of </span><span>emotions</span><span>. These images must be well annotated or</span> <span>emotion</span><span>-tagged if they are to be useful to</span> <span>expression recognition systems and their related algorithms</span><span>. </span></p>
<p><span>A major hindrance to new developments in the area of <strong>automatic human behavior analysis</strong> is the lack of suitable databases with displays of behavior and affect. There have been directed advances in this area, as in the <strong>MMI Facial Expression Database</strong> project, which aims </span>to deliver large volumes of <span><strong>visual data of facial expressions</strong> to the facial expression analysis community.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<div class="packt_infobox"><span>The MMI Facial Expression Database was initially created in 2002 as a resource for building and evaluating facial expression recognition algorithms. One significance of this database is that others databases focus on the expressions of the six basic emotions (which we mentioned earlier), whereas this database contains both these prototypical expressions as well as expressions with a single <strong>Facial Action Coding System</strong> (<strong>FACS</strong>) or  <strong>Action Unit</strong> (<strong>AU</strong>) activated, for all existing AUs and many other <strong>Action Descriptors</strong> (<strong>AD</strong>). Recently recordings of naturalistic expressions have been added too.</span> <span><br/>
The database is freely available to the scientific community. Find out more about the database online at <a href="https://mmifacedb.eu">https://mmifacedb.eu</a>.</span></div>
<p><span>In other example projects, we've been able to create our own test data or alter existing datasets to use within a project. However, with <strong>expression analysis</strong> projects, it is really not realistic to create a reasonably sized database (stating with nothing), which would require the collection and processing of literally thousands of images, all appropriately documented.<span class="Apple-converted-space"> </span></span></p>
<p><span>After collection, each (facial) image needs to be reviewed and</span> <span>categorized based on the emotion shown into one of seven categories (angry, disgust, fear, happy, sad, surprise, and neutral). To further complicate this work, images may not be aligned and properly proportioned.<span class="Apple-converted-space"> </span></span></p>
<p><span>The bottom line is that, even if you have a large number of images, if the images are not correctly labeled or simply do not contain detectable facial images, the performance of the expression analysis and detection process will be compromised (it will perform poorly).</span></p>
<p><span>These types of challenges make the classification process more difficult because the model is forced to generalize.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training with the Watson Visual Recognition service </h1>
                </header>
            
            <article>
                
<p><span>Considering the above mentioned challenges, IBM Watson Studio helps us to get started anyway by offering (right out of the box) the</span> <span><strong>Watson Visual Recognition</strong> service.<span class="Apple-converted-space"> </span></span></p>
<p><span>This valuable service helps with the process of accurately analyzing, classifying, and training images using machine learning logic (although, to be sure, it still requires reasonable amounts of relevant training data to begin with, but more on that in a bit).<span class="Apple-converted-space"> </span></span></p>
<p class="mce-root"/>
<p><span>Thankfully, there is a set of built-in models that is available to us to provide highly accurate re</span>sults without endle<span>ss training.<span class="Apple-converted-space"> </span></span><span>The models are as follows:</span></p>
<ul>
<li><span><strong>General model</strong></span>: <span>General classifier categories</span></li>
<li><span><strong>Face model:</strong> Locate faces within an image, gender, and age</span></li>
<li><span><strong>Explicit model</strong></span>: <span>Whether an image is inappropriate for general use</span></li>
<li><span><strong>b model</strong></span>:<span> Specifically for images of food items</span></li>
</ul>
<p><span>In this chapter's project, we will show how to use the Visual Recognition Service and the Face model to build an end-to-end working solution that can look at an image of a human face, perform expression analysis and simple classification, and ultimately, determine whether the individual is feeling happy or sad.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preprocessing faces</h1>
                </header>
            
            <article>
                
<p><span>We have just mentioned that building a suitable expression database is a lot of work. To be able to build an end-to-end working expression analysis solution (and fit it all into a single chapter of this book), we will take some liberties with our project:</span></p>
<ul>
<li><span>We will limit our model's ability to detect and classify only two emotions—happiness and sadness</span></li>
<li><span>We will supply only a limited amount of expression data to train our model</span></li>
</ul>
<p><span>Obviously, in the real world, our second assumption is a risky one; as in any ML model, less training data typically produces a less valuable result.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing the training data</h1>
                </header>
            
            <article>
                
<p><span>Again, if we have decided to satisfy only t</span>he minimal requirements for <span>using the face model and the Visual Recognition service, we can get away with collecting only 10 images for each class which we intend our model to train on (10 happy faces, 10 sad faces, and 10 negative faces).</span></p>
<p><span>These individual training files will need to be the following:<span class="Apple-converted-space"> </span></span></p>
<ul>
<li><span>Either JPEG (<kbd>.jpg</kbd>) and PNG (<kbd>.png</kbd>) formatted</span></li>
<li><span>At least 32*32 pixels in size</span></li>
<li><span>Compressed as a group into</span> class Z<span>IP files, that is, 10 happy faces in a <kbd>happy.zip</kbd> file and 10 sad faces in a <kbd>sad.zip</kbd> file</span></li>
</ul>
<p><span>A sampling of our initial happy model training data is shown in the following screenshot:</span></p>
<p class="CDPAlignCenter CDPAlign"><span><img src="assets/e66f972a-ec25-4ac9-a6aa-3c4865f06efb.png" style="width:42.75em;height:16.42em;"/></span></p>
<p><span>The preceding images are of 10 faces showing what we think can be labeled as being representative of happy facial expressions. Notice that the individual files have all been added to a compressed (ZIP) file named <kbd>happy.zip</kbd>.</span></p>
<p><span>A sampling of our initial sad<em> </em>model training data is shown in the following screenshot:</span></p>
<p class="CDPAlignCenter CDPAlign"><span><img src="assets/f50f2a2d-d146-4d66-849b-9f761a3f7b29.png" style="width:43.50em;height:16.25em;"/></span></p>
<p><span>And clearly, as the former group displayed happiness, the later images are of faces showing what we think can be labeled as being representative of sad expressions (individual files and the zipped file, <kbd>sad.zip</kbd>).</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Negative or non-positive classing </h1>
                </header>
            
            <article>
                
<p><span>For the face model to work correctly, negative images are also required, not to be used to create a class (we will cover this in an upcoming section) within the created classifier, but to define what the updated classifie</span>r is not. Neg<span>ative example files should not contain images that have the subject of any of the positive classes (happy and sad). In essence, the face images in this group should be perhaps considered to b</span>e neutral. You <span>only need to specify one negative example file.<span class="Apple-converted-space"> </span></span></p>
<div class="mce-root packt_tip"><span>Because you want to give the model examples of w</span>hat not to l<span>ook for, you must provide the negative class. Providing a ML model with all positive images would mean that it would just assume that everything is positive and produce a risky result.</span></div>
<p><span>So</span> <span>finally, our initial negative model training data is shown in the following screenshot:</span></p>
<p class="CDPAlignCenter CDPAlign"><span><img src="assets/37ef7b79-dd09-413b-a9a1-72864e84c0a9.png" style="width:43.50em;height:16.25em;"/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing the environment</h1>
                </header>
            
            <article>
                
<p><span>Let's now get moving along with the project model development.<span class="Apple-converted-space"> </span></span></p>
<p><span>The next step (assuming you have already created a new IBM Watson Studio project) is to</span> <span>associate the Watson Visual Recognition Service to the project. We covered how to do this in</span> <a href="99762d15-664d-4987-82cf-74440dedabb3.xhtml">Chapter 7</a><em><span>,</span> Deep Learning Using TensorFlow on the IBM Cloud</em>, <span>so we will assume you have already added the service to this new project. If not, review <a href="99762d15-664d-4987-82cf-74440dedabb3.xhtml">Chapter 7</a><em>, Deep Learning Using TensorFlow on the IBM Cloud</em>, or the online Watson Studio documentation.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Project assets</h1>
                </header>
            
            <article>
                
<p><span>In this chapter's project, our assets will primarily be the training images we have collected and, indirectly perhaps, classified. These image assets are added in a way similar to the process we used to add data assets to Watson Studio projects in earlier chapters, but there are a few differences, which we will soon see.<span class="Apple-converted-space"> </span></span></p>
<p><span>For now, we will perform the following steps:</span></p>
<ol>
<li><span>Go to the <span class="packt_screen">Assets</span> tab and, under <span class="packt_screen">Models</span>, click on</span> <span><span class="packt_screen">New Visual Recognition mode</span>l:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><span><img src="assets/d5b40a59-b895-4128-a930-6e5d8f5b74ad.png" style="width:45.25em;height:9.50em;"/></span></p>
<ol start="2">
<li class="mce-root">Once the model is created (it should take only a few moments), you can browse to or drag and drop the training (<kbd>.zip</kbd>) files we collected in the earlier section of this chapter to add them to our new project. <span>This will upload the image files to</span> <strong>Cloud Object Storage</strong> <span>(</span><strong>COO</strong><span>), making them available to be used in our project:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><span><img src="assets/1f296ff0-2fc6-4adc-ae50-92703f30f9ee.png" style="width:19.83em;height:32.42em;"/></span></p>
<p><span>You do not have to load each image file independently, just the three zipped files (<kbd>happy.zip</kbd>, <kbd>sad.zip</kbd>, and <kbd>negative.zip</kbd>). The ZIP files should then be listed as shown in the preceding screenshot.</span></p>
<div class="packt_infobox"><span>Although you can upload the ZIP files, Watson will not allow those ZIP files to use the Preview feature on the <span class="packt_screen">Data Assets</span> page. This is not a problem though, as you can still preview the images from the <span class="packt_screen">Model</span> page, as we will see shortly.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating classes for our model</h1>
                </header>
            
            <article>
                
<p><span>Now, from the <span class="packt_screen">Default Custom Model</span> page, we need to create two classes. Perform the following steps to create the class for the model:</span></p>
<ol>
<li><span>Click on <span class="packt_screen">Create a class.</span></span></li>
<li><span>Enter a class name for it.</span></li>
</ol>
<ol start="3">
<li><span>Click the blue <span class="packt_screen">Create</span> button:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><span><img src="assets/f960a915-1d9e-477a-8647-387adc7af744.png" style="width:26.25em;height:15.75em;"/></span></p>
<p><span>We </span><span>actually</span><span> </span><span>only</span><span> </span><span>need to create two classes for this proje</span>ct: happy and sad, si<span>nce Watson has already created the negative class for us. There is only one negative class per model, but you can have as many other classes as required for your project's objectives.</span></p>
<p><span>Once the classes are created, you then need to simply d</span><span>rag and drop the <kbd>.zip</kbd> files into the corresponding classes as shown in the following screenshot:</span></p>
<p class="CDPAlignCenter CDPAlign"><span><img src="assets/4fea40c0-94fa-4332-9571-87ac1e30bf53.png" style="width:43.25em;height:26.17em;"/></span></p>
<p><span>As you can see, we dropped each of the three ZIP files onto their corresponding class panes.</span></p>
<p><span>As we stated earlier in this chapter, Watson preview doesn't work with zipped image file assets; however, from the <span class="packt_screen">Default Custom Model</span> page (shown in the following screenshot) we can click on <span class="packt_screen">All Images</span> and scroll through what was loaded to see the filename, label, and content:</span></p>
<p class="CDPAlignCenter CDPAlign"><span><img src="assets/39206f6e-f778-4a52-973a-a95f1fc87364.png" style="width:48.33em;height:31.83em;"/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Automatic labeling</h1>
                </header>
            
            <article>
                
<p><span>Earlier in this chapter, we pointed out that, after collecting images to be used for expression analysis and recognition, each individual image must be annotated or labelled as to which emotion group it belongs to. This can be a daunting task. Fortunately, when using IBM Watson Studio, you can simply include the appropriate images in a ZIP file and drop the ZIP file onto a class and Watson</span> will automatically l<span>abel the image file. For example, in the following screenshot, you can see that we can correctly identify those images to include within our <strong>happy</strong> class (shown outlined in green) while a single image, <kbd>sad05</kbd> (shown outlined in red), does not belong to and should be excluded from our ZIP file:</span></p>
<p class="CDPAlignCenter CDPAlign"><span><img src="assets/eba475d9-828e-47e1-956b-5db9bc63cc7c.png" style="width:49.58em;height:17.67em;"/></span></p>
<p><span>This is a pretty easy process, but it could invite errors. Since it is easy and quick, you may mistakenly include images that will dilute the training sample. Keep in mind that, even if the image files are named intuitively, such as</span> happy or sad, W<span>atson doesn't care about the names, it simply labels all of the images in the file as <em>positive</em> or <em>matching</em> to the class. <span class="Apple-converted-space"> </span></span></p>
<p><span>Finally, there's one more note about the training data. Once you go to the trouble of collecting and uploading data as an IBM Watson Studio asset, that data is available to any of your projects and, if you want to, you can share it with any other Watson Studio user! This promotes the development of assets across projects and users and increases the return on your investment.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learning the expression classifier</h1>
                </header>
            
            <article>
                
<p><span>Once you notice that the model status (indicated at the upper-right of the <span class="packt_screen">Default Custom Model</span> page) has changed to</span> <span class="packt_screen"><span>Model</span> <span>is ready to train</span></span><span>,</span> <span>you can then click on</span> <span>the</span> <span><span class="packt_screen">Train Model</span></span> <span>button</span> <span>to start training the Face model on the training images we provided:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3b5eb50d-742a-4330-96e3-285cd285fddc.png"/></div>
<p><span>Since we provided only roughly 30 training images, the training process should take less than 5 or 10 minutes.</span> <span>During the training, you will not be able to make any changes to the model or classes.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating the expression classifier</h1>
                </header>
            
            <article>
                
<p><span>Once the model training is complete, you should see the following </span><span><span class="packt_screen">Training successful </span></span><span>message:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/65975056-898f-47e1-9b9d-47653ea42fc0.png"/></div>
<p><span>From this point, you can click on the</span> <span class="packt_screen">here</span><span> hyperlink in the popup to view and test the model.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Viewing the model training results</h1>
                </header>
            
            <article>
                
<p><span>After successfully training the model, you will be redirected to a page where you can see an overview or</span> <span><span class="packt_screen">Summary</span></span> <span>(<span class="packt_screen">Model ID</span>, <span class="packt_screen">Status</span>, and other metadata) of the model build (take a note of the <span class="packt_screen">Model ID</span> as that will be required during the implementation stage):</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8b28522a-9f8c-461e-be40-3e5b4a1eede1.png"/></div>
<p><span>We can also see our model's <span class="packt_screen">Classes</span> information:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d150d8ec-dac7-4f83-b0ec-0876f0f455cb.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing the model</h1>
                </header>
            
            <article>
                
<p><span>To test and understand how our model performs (or whether it even works!), you can upload images in the <span class="packt_screen">Test</span> tab of the previous page view.<span class="Apple-converted-space"> </span>Let's try it!</span></p>
<p><span>To test with images, click on <span class="packt_screen">Test</span>:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/937da7de-c674-410d-99d8-7869e96140e8.png"/></div>
<p class="mce-root"/>
<p><span>Like we did to collect our happy and sad images, we can identify several random images (without regard to the expression shown in the image) to test our models ability to detect expression:</span></p>
<p class="CDPAlignCenter CDPAlign"><span><img src="assets/48473fd5-e379-4919-a66a-fe5496c5d244.png" style="width:33.75em;height:10.17em;"/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Test scores</h1>
                </header>
            
            <article>
                
<p><span>The idea is for our model to interpret the preceding images, perform some expression analysis, and then classify each image as either happy or sad facial expressions. In addition, the model should generate and display a score for each of the defined classes (except for the negative class).</span></p>
<p><span>As you have seen in our model, we defined just two classes to be classified—</span>happy and sad. The model shou<span>ld, for each test image, display a percentage score showing the percentage of whether the detected expression is happy or sad. For example, the following score indicates that there is approximately 90 percent chance that the expression identified is</span> happy<span>:</span></p>
<p class="CDPAlignCenter CDPAlign"><span><img src="assets/c86b5570-4744-41c9-ad04-b9fbb17b7f75.png" style="width:22.42em;height:4.83em;"/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Test the model</h1>
                </header>
            
            <article>
                
<p><span>To test the model with the following images, we can simply drag and drop the image files onto the preceding page to let the classifier analyze them:</span></p>
<p class="CDPAlignCenter CDPAlign"><span><img src="assets/6d1caeb0-f9a4-499b-bfa1-b248955c13e9.png" style="width:39.50em;height:21.17em;"/></span></p>
<p>Take a look at the following screenshot as well:</p>
<p class="CDPAlignCenter CDPAlign"><span><img src="assets/e944204a-4135-4146-8942-516dfbbe06c0.png" style="width:38.83em;height:19.33em;"/></span></p>
<p><span>Success! </span><span>It appears that all four of our random faces have been evaluated and scored by our Face model correctly. We can see that, in the first two images, the model indicates 11 and 90 percent that the individuals are happy and sad, respectively.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Improving the model</h1>
                </header>
            
            <article>
                
<p><span>Even though it appears that our little solution is working correctly, we still have to keep in mind that the model has been trained on a very small amount of data.<span class="Apple-converted-space"> </span></span></p>
<p><span>To improve the model, from the <span class="packt_screen">Default Custom Model</span> page, you can click on the blue button labeled <span class="packt_screen">Edit and Retrain</span>:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9c4fd313-60c5-456b-a5a7-51edef621695.png"/></div>
<p><span>This will make our project editable.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">More training data</h1>
                </header>
            
            <article>
                
<p><span>Some improvements to our solution would include adding additional images to the happy and sad groups. To do this, you can create a new ZIP file with new and additional images and upload it to IBM Watson Studio (in the same fashion as we did earlier in this chapter), upload the file, and drop the new ZIP file into the respective class. Watson will add the new images (and not overwrite what's already been defined):</span></p>
<p class="CDPAlignCenter CDPAlign"><span><img src="assets/d9978cb0-9da4-40f7-85cd-e7cf8b0f8f04.png" style="width:52.75em;height:18.83em;"/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adding more classes</h1>
                </header>
            
            <article>
                
<p><span>Another great improvement for our solution would be to add additional classes. This</span><span> is to allow our model to support the detection of, perhaps, a third emotion or expression other than happy and sad. Let's try to ad</span>d anger as <span>our third (not counting negative) class:</span></p>
<ol>
<li><span>The first step, of course, is to collect and compress (zip up) our angry images training data:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><span><img src="assets/b9d06d43-eb94-4e57-a5a5-e24e389a3b18.png" style="width:38.42em;height:13.08em;"/></span></p>
<div class="packt_infobox"><span>Remember that it is not important what the individual image files are named, but it is important that they all represent the same emotion, <em>anger</em>.</span></div>
<ol start="2">
<li>After we upload the <kbd>angry.zip</kbd> file (as another data asset available to our project), we can then go ahead and click on <span class="packt_screen">Create a class</span>, enter <kbd>angry</kbd> for the class name, and then click on <span class="packt_screen">Create</span>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><span><img src="assets/300d414b-72c5-4892-939d-f02cc5d135d9.png" style="width:39.25em;height:16.67em;"/></span></p>
<p style="padding-left: 90px"><span>After a moment or two, our new</span> <kbd>angry</kbd><span> class is ready:</span></p>
<p class="CDPAlignCenter CDPAlign"><span><img src="assets/8afbac0c-9176-4595-b1b6-2424ad008523.png"/></span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="3">
<li>Now we can once more click on the <span class="packt_screen">Train Model</span> button to start retraining the Face model on the training images we provided, along with our new angry class. After a few moments, we should see the <span class="packt_screen">Model Trained</span> message again:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/cf224e13-b4d5-47ef-a5fa-d053b03d6814.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Results</h1>
                </header>
            
            <article>
                
<p><span>Once again, we can go to the <span class="packt_screen">Default Custom Model</span> page, click on the <span class="packt_screen">Test</span> tab, and drop some new test images for the model to evaluate and classify:</span></p>
<p class="CDPAlignCenter CDPAlign"><span><img src="assets/8a4801df-ec7f-4ac1-a3eb-aaaddd42ac7b.png" style="width:43.00em;height:23.00em;"/></span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span>You can see that the model has correctly classified the first image as</span> 0.77 <span>angry. We also retested a previous image as a bit of a regression test and the model again correctly classified it (as</span> 0.66 <span>happy).<span class="Apple-converted-space"> </span></span></p>
<p><span>Notice that now our model provides three scores for each test image: <span class="packt_screen">angry</span>, <span class="packt_screen">happy</span>, and <span class="packt_screen">sad</span>, corresponding to each of our model's defined classes:</span></p>
<p class="CDPAlignCenter CDPAlign"><span><img src="assets/780287b9-88da-4316-9564-9ee67aa16bfd.png"/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p><span>In this chapter, we explored the concepts behind expression analysis and detection and used IBM Watson Studio, the Watson Visual</span> <span>Recognition</span> s<span>ervice, and the default face model to build, train, and test an end-to-end, working visual expression classification solution with almost zero programming!</span></p>
<p><span>In the next chapter, we will discover a</span><span>utomated classification of lithofacies formation using ML on the IBM Cloud platform.</span></p>


            </article>

            
        </section>
    </body></html>