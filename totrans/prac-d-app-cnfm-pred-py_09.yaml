- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Conformal Prediction for Computer Vision
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算机视觉中的一致性预测
- en: In today’s fast-paced world, computer vision has grown beyond mere image recognition
    to be a fundamental cornerstone in numerous real-world applications. From self-driving
    cars navigating bustling streets to medical imaging systems that detect early
    signs of diseases, the demand for reliable and accurate computer vision models
    has never been higher. However, with the increasing complexity of these systems
    and their applications, a critical need arises for the ability to quantify the
    uncertainty associated with their predictions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今快节奏的世界里，计算机视觉已经超越了仅仅图像识别的范畴，成为众多现实应用中的基本基石。从在繁忙街道上导航的自动驾驶汽车到能够检测疾病早期迹象的医疗成像系统，对可靠且准确的计算机视觉模型的需求从未如此之高。然而，随着这些系统和它们应用的日益复杂，迫切需要一种能力来量化与它们预测相关的不确定性。
- en: Enter **conformal prediction**, a ground-breaking framework that offers a robust
    means to encapsulate the uncertainty inherent in machine learning models. While
    traditional computer vision models often produce a singular prediction, the true
    power of conformal prediction lies in its ability to provide a set of possible
    outcomes, each backed by a confidence level. This offers practitioners a more
    informed, nuanced view of the model’s predictions, enabling safer and more reliable
    deployments in critical applications.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍**一致性预测**，这是一个开创性的框架，提供了一种稳健的方法来封装机器学习模型中固有的不确定性。虽然传统的计算机视觉模型通常只产生一个预测，但一致性预测的真正力量在于其提供一系列可能的输出，每个输出都附带一个置信水平。这为从业者提供了一个更明智、更细致的模型预测视角，使得在关键应用中部署更安全、更可靠的模型成为可能。
- en: 'This chapter dives deep into the marriage of conformal prediction and computer
    vision. We begin by illuminating the necessity of uncertainty quantification in
    computer vision, highlighting its significance in real-world scenarios including
    **autonomous driving** and **healthcare diagnostics**. As we navigate deeper,
    we’ll explore the Achilles’ heel of modern deep learning models: *their tendency
    to produce* *miscalibrated predictions*.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章深入探讨了一致性预测与计算机视觉的结合。我们首先阐明在计算机视觉中量化不确定性的必要性，强调其在包括**自动驾驶**和**医疗诊断**在内的现实场景中的重要性。随着我们进一步探索，我们将揭示现代深度学习模型的阿喀琉斯之踵：*它们产生*
    *错误的预测*的倾向。
- en: By the journey’s end, you’ll gain hands-on experience building state-of-the-art
    computer vision classifiers imbued with the power of conformal prediction. We’ll
    introduce and guide you through the best open source conformal prediction libraries
    in computer vision applications to ensure you have all the tools necessary to
    embark on this journey.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 到旅程结束时，你将获得实际操作经验，构建融入一致性预测能力的最先进的计算机视觉分类器。我们将介绍并引导你通过计算机视觉应用中最佳的开放源代码一致性预测库，确保你拥有开始这段旅程所需的所有工具。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Uncertainty quantification for computer vision
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机视觉中的不确定性量化
- en: Why deep learning produces miscalibrated predictions
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么深度学习会产生错误的预测
- en: Various approaches to quantify uncertainty in computer vision problems
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机视觉问题中量化不确定性的各种方法
- en: Conformal prediction for computer vision
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机视觉中的一致性预测
- en: Building computer vision classifiers using Conformal prediction
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一致性预测构建计算机视觉分类器
- en: Uncertainty quantification for computer vision
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算机视觉中的不确定性量化
- en: As a domain, computer vision has transformed many sectors by automating complex
    tasks that were once reserved for human eyes and cognition. Computer vision models
    have become an integral part of modern technology, whether it’s detecting pedestrians
    on the road, identifying potential tumours in medical scans, or even analyzing
    satellite images for environmental studies. However, as the reliance on these
    models grows, so does the need to understand and quantify the uncertainty associated
    with their predictions.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个领域，计算机视觉通过自动化曾经仅限于人类视觉和认知的复杂任务，已经改变了众多行业。计算机视觉模型已成为现代技术不可或缺的一部分，无论是检测道路上的行人，识别医学扫描中的潜在肿瘤，还是分析卫星图像进行环境研究。然而，随着对这些模型的依赖性增加，理解和量化与它们预测相关的不确定性的需求也在增长。
- en: Why does uncertainty matter?
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么不确定性很重要？
- en: 'Before deep-diving into the mechanics, it’s essential to understand why we
    need **uncertainty quantification** (**UQ**) in the first place. Let’s go through
    some of the reasons as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨机制之前，了解为什么我们需要**不确定性量化（UQ**）本身是至关重要的。以下是一些原因：
- en: '**Safety and reliability**: A wrong prediction can have severe consequences
    in critical applications, such as medical imaging or autonomous driving. Knowing
    the confidence level in a prediction can aid in decision-making, such as whether
    to trust the model’s prediction or seek human intervention.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全和可靠性**：在关键应用中，如医学成像或自动驾驶，错误的预测可能带来严重的后果。了解预测的置信水平可以帮助决策，例如是否信任模型的预测或寻求人工干预。'
- en: '**Model improvements**: Uncertainty measurements can provide insights into
    areas where the model might be lacking, helping to guide data collection and training
    enhancements.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型改进**：不确定性测量可以提供关于模型可能不足的领域的见解，有助于指导数据收集和训练改进。'
- en: '**Trustworthiness**: Knowing that a system acknowledges its limitations and
    can provide confidence intervals or uncertainty metrics for end users and stakeholders
    makes it more trustworthy.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可信度**：知道一个系统承认其局限性，并能向最终用户和利益相关者提供置信区间或不确定性度量，使其更加可信。'
- en: Navigating the world of computer vision, one inevitably encounters uncertainties
    that can influence the accuracy of model predictions. But what are the sources
    of these uncertainties, and can they be managed? Let’s delve into the two primary
    types of uncertainty in computer vision.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉的世界中导航，不可避免地会遇到可能影响模型预测准确性的不确定性。但不确定性的来源是什么，它们可以被管理吗？让我们深入了解计算机视觉中的两种主要类型的不确定性。
- en: Types of uncertainty in computer vision
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算机视觉中的不确定性类型
- en: 'Uncertainty in computer vision can be broadly classified into two categories:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉中的不确定性可以大致分为两类：
- en: '**Aleatoric uncertainty**: This type of uncertainty arises from the inherent
    noise in the data. For instance, low-light images, blurry images, or images taken
    from varying angles introduce variability that the model might find challenging
    to handle. Aleatoric uncertainty is often irreducible, meaning no matter how good
    the model becomes, this uncertainty will always exist due to the inherent noise
    in the observations.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机不确定性**：这种不确定性源于数据中的固有噪声。例如，低光照图像、模糊图像或从不同角度拍摄的图像引入了模型可能难以处理的变异性。随机不确定性通常是不可减少的，这意味着无论模型变得多好，由于观察中的固有噪声，这种不确定性始终存在。'
- en: '**Epistemic uncertainty**: This type of uncertainty stems from the model itself.
    It could be due to incomplete training data, model architecture choices, or the
    optimization process. Given enough data or improvements in model design, epistemic
    uncertainty can be reduced.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**认知不确定性**：这种不确定性源于模型本身。它可能是由于训练数据不完整、模型架构选择或优化过程造成的。只要有足够的数据或模型设计的改进，认知不确定性是可以降低的。'
- en: In the realm of computer vision, it’s not enough to simply get a prediction.
    As advanced as our models are, they can sometimes be overly confident, potentially
    leading to misinformed decisions. How do we gauge the reliability of these predictions?
    Enter the world of uncertainty quantification.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉领域，仅仅得到一个预测是不够的。尽管我们的模型很先进，但它们有时可能会过于自信，这可能导致错误的决策。我们如何衡量这些预测的可靠性？这就引入了不确定性量化（Uncertainty
    Quantification，UQ）的世界。
- en: Quantifying uncertainty
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不确定性量化
- en: Modern computer vision models, especially deep learning architectures, produce
    predictions that are often overconfident. This miscalibration can be misleading,
    especially in critical applications. The need, therefore, is not just to produce
    a prediction but also to accompany it with a measure of confidence or uncertainty.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现代计算机视觉模型，尤其是深度学习架构，产生的预测往往过于自信。这种误校准可能是误导性的，尤其是在关键应用中。因此，需要的不仅仅是产生预测，还要伴随一个置信度或不确定性的度量。
- en: Various methods have been proposed to quantify uncertainty, ranging from Bayesian
    neural networks, which provide a distribution over model parameters, to ensemble
    methods, which rely on the variability of predictions across different models.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 已经提出了各种方法来量化不确定性，从提供模型参数分布的贝叶斯神经网络，到依赖于不同模型预测变异性集成的集成方法。
- en: However, as we’ll see in the subsequent sections, Conformal prediction offers
    a fresh and rigorous perspective on uncertainty quantification tailored to the
    needs of computer vision applications.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如我们将在后续章节中看到的，符合预测为计算机视觉应用量身定制的不确定性量化提供了一种新颖而严谨的视角。
- en: Uncertainty quantification for computer vision is not a theoretical exercise
    but a crucial aspect of building reliable, safe, and trustworthy models. Understanding
    and accounting for their inherent uncertainties will be paramount as computer
    vision systems continue to permeate every sector.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉的不确定性量化并非理论练习，而是构建可靠、安全和值得信赖模型的关键方面。随着计算机视觉系统继续渗透到各个领域，理解和考虑其固有的不确定性将至关重要。
- en: Why does deep learning produce miscalibrated predictions?
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么深度学习会产生错误的预测？
- en: The **ImageNet Large Scale Visual Recognition Challenge** (**ILSVRC**) is an
    annual competition where research teams evaluate their algorithms on a given dataset,
    aiming to push the boundaries of computer vision. 2012 was a watershed moment
    for the field, marking a significant shift towards the dominance of deep learning
    in computer vision ([https://www.image-net.org/challenges/LSVRC/2012/](https://www.image-net.org/challenges/LSVRC/2012/)).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**ImageNet大规模视觉识别挑战赛**（**ILSVRC**）是一个年度竞赛，研究团队在给定的数据集上评估他们的算法，旨在推动计算机视觉的边界。2012年是该领域的分水岭，标志着深度学习在计算机视觉领域的主导地位发生了重大转变（[https://www.image-net.org/challenges/LSVRC/2012/](https://www.image-net.org/challenges/LSVRC/2012/)）。'
- en: Before the advent of deep learning, computer vision primarily relied on hand-engineered
    features and traditional machine learning techniques. Algorithms such as **Scale-Invariant
    Feature Transform** (**SIFT**), **Histogram of Oriented Gradients** (**HOG**),
    and **Speeded-Up Robust Features** (**SURF**) were commonly used to extract features
    from images. These features would then be fed into machine learning classifiers
    such as **Support Vector Machines** (**SVM**) to make predictions. While these
    methods had their successes, they had significant limitations regarding scalability
    and performance on more complex datasets.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习出现之前，计算机视觉主要依赖于手工设计的特征和传统的机器学习技术。例如**尺度不变特征变换**（**SIFT**）、**方向梯度直方图**（**HOG**）和**加速鲁棒特征**（**SURF**）等算法常用于从图像中提取特征。然后，这些特征会被输入到机器学习分类器，如**支持向量机**（**SVM**）进行预测。虽然这些方法取得了一定的成功，但在可扩展性和在更复杂的数据集上的性能方面存在重大局限性。
- en: In 2012, a deep convolutional neural network named AlexNet ([https://en.wikipedia.org/wiki/AlexNet](https://en.wikipedia.org/wiki/AlexNet)),
    developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, was entered
    into the ILSVRC. It achieved a top-5 error rate of 15.3%, a staggering 10.8 percentage
    points lower than the second-place finisher. This dramatic improvement was an
    incremental step and a quantum leap in performance.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 2012年，由Alex Krizhevsky、Ilya Sutskever和Geoffrey Hinton开发的一个名为AlexNet的深度卷积神经网络（[https://en.wikipedia.org/wiki/AlexNet](https://en.wikipedia.org/wiki/AlexNet)），被提交到ILSVRC竞赛中。它实现了15.3%的顶级错误率，比第二名低10.8个百分点，这是一个显著的进步，是性能的逐步提升和质的飞跃。
- en: Why was AlexNet revolutionary?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么AlexNet具有革命性？
- en: '**Deep architecture**: AlexNet was considerably deeper than other networks
    of its time. It had five convolutional layers followed by three fully connected
    layers. This depth allowed it to learn more complex and hierarchical features
    from the ImageNet dataset.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度架构**：与当时其他网络相比，AlexNet的深度要大得多。它有五个卷积层，后面跟着三个全连接层。这种深度使得它能够从ImageNet数据集中学习更复杂和层次化的特征。'
- en: '**GPU training**: The team utilized **graphics processing units** (**GPUs**)
    to train the network, which made it feasible to process the massive amount of
    data in the ImageNet dataset and efficiently train the deep architecture.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPU训练**：该团队利用**图形处理单元**（**GPU**）来训练网络，这使得处理ImageNet数据集中的大量数据并高效训练深度架构成为可能。'
- en: '**ReLU activation**: Instead of traditional tanh or sigmoid activation functions,
    AlexNet employed the **Rectified Linear Unit** (**ReLU**) activation. This choice
    helped combat the vanishing gradient problem, enabling the training of deeper
    networks.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ReLU激活**：与传统的tanh或sigmoid激活函数不同，AlexNet采用了**修正线性单元**（**ReLU**）激活。这一选择有助于解决梯度消失问题，使得训练更深层的网络成为可能。'
- en: '**Dropout**: To prevent overfitting, AlexNet introduced the dropout technique,
    where random subsets of neurons were “dropped out” during training, forcing the
    network to learn redundant representations.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Dropout**：为了防止过拟合，AlexNet引入了dropout技术，在训练过程中随机丢弃神经元子集，迫使网络学习冗余表示。'
- en: The dawn of 2012 marked a transformative moment in the realm of computer vision.
    Propelled by the unprecedented achievements of AlexNet in the ImageNet competition,
    the entire industry pivoted towards deep learning, especially **convolutional**
    **neural networks** (**CNNs**). As we journey through the aftermath of this revolution,
    we’ll witness the exponential growth in research, widespread industry adoption,
    and the relentless quest for more data and computational power.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 2012年的黎明标志着计算机视觉领域的一个变革时刻。在ImageNet竞赛中AlexNet前所未有的成就的推动下，整个行业转向了深度学习，特别是**卷积**
    **神经网络**（**CNNs**）。随着我们穿越这场革命的余波，我们将见证研究的指数级增长、广泛的行业采用以及对更多数据和计算能力的不懈追求。
- en: Post-2012 – the deep learning surge
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2012年后 – 深度学习的兴起
- en: 'The 2012 ImageNet competition, marked by the triumph of AlexNet, became a watershed
    moment in the field of computer vision. This victory underscored the profound
    potential of deep learning, especially **convolutional neural networks** (**CNNs**).
    As a result, the following happened:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 2012年的ImageNet竞赛，以AlexNet的胜利为标志，成为了计算机视觉领域的一个转折点。这次胜利凸显了深度学习的巨大潜力，特别是**卷积神经网络**（**CNNs**）。因此，以下情况发生了：
- en: '**Research boom**: After 2012, there was an explosion of research into deep
    learning for computer vision. Variants and improvements upon AlexNet, such as
    VGG, GoogLeNet, and ResNet, were rapidly developed, pushing the envelope further.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**研究热潮**：2012年后，对计算机视觉深度学习的研究爆炸式增长。AlexNet的变体和改进，如VGG、GoogLeNet和ResNet，迅速开发出来，进一步推动了边界。'
- en: '**Industry adoption**: Tech giants and start-ups began investing heavily in
    deep learning research and applications, from facial recognition systems to augmented
    reality.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**行业采用**：科技巨头和初创公司开始大量投资深度学习研究和应用，从面部识别系统到增强现实。'
- en: '**Datasets and compute**: The success of deep learning fueled the creation
    of even larger datasets and a race for more powerful computation infrastructure,
    further accelerating the innovation cycle.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集和计算资源**：深度学习成功推动了更大数据集的创建和更强大计算基础设施的竞赛，进一步加速了创新周期。'
- en: The 2012 ImageNet competition was a turning point, heralding the era of deep
    learning in computer vision. The principles and breakthroughs of AlexNet laid
    the foundation for the subsequent advancements we see today, from self-driving
    cars to real-time video analytics.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 2012年的ImageNet竞赛是一个转折点，预示着计算机视觉深度学习时代的到来。AlexNet的原则和突破为今天我们所看到的从自动驾驶汽车到实时视频分析等后续进步奠定了基础。
- en: The "calibration crisis" in deep learning – a turning point in 2017
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习的“校准危机”——2017年的转折点
- en: 'Since its triumphant ascendance following the 2012 ImageNet competition, deep
    learning experienced rapid advancements and widespread adoption across many domains.
    The community was engrossed in developing architectures, optimization techniques,
    and applications for five consecutive years. Yet, amidst this whirlwind of innovation,
    a significant concern remained largely overlooked: *the miscalibration of predictions
    produced by deep* *learning systems*.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 自2012年ImageNet竞赛以来的辉煌上升之后，深度学习在许多领域经历了快速发展和广泛应用。社区连续五年专注于开发架构、优化技术和应用。然而，在这场创新的风暴中，一个重大的担忧却被很大程度上忽视了：**深度学习系统产生的预测结果的不准确校准**。
- en: In real-world applications where automated systems drive decisions, it’s not
    enough for classification networks to merely provide accurate results. These systems
    play an integral role in various critical sectors, from healthcare to finance,
    and a misjudgment can have significant consequences. Therefore, it’s crucial that
    these classification networks not only deliver precise outcomes but also possess
    the self-awareness to flag potential uncertainties or errors in their predictions.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，自动化系统驱动决策时，分类网络仅仅提供准确结果是不够的。这些系统在医疗保健到金融等各个关键领域发挥着至关重要的作用，任何误判都可能产生重大后果。因此，这些分类网络不仅需要提供精确的结果，还需要具备自我意识，以标记预测中潜在的不确定性或错误。
- en: For instance, in a medical diagnostic tool, beyond correctly identifying a disease,
    the system should also indicate its confidence level in that diagnosis. Medical
    professionals can take appropriate precautions if uncertain, perhaps seeking additional
    tests or expert opinions.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在医疗诊断工具中，除了正确识别疾病外，系统还应指出其对诊断的置信水平。如果医疗专业人员不确定，可以采取适当的预防措施，例如寻求额外的测试或专家意见。
- en: 'Take another example: a self-driving car equipped with a neural network designed
    to identify pedestrians and various obstacles on the road. In such a scenario,
    the car’s system doesn’t just need to recognize people or obstructions; it must
    do so accurately and in real time. Any delay or misidentification could lead to
    potentially dangerous situations.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 再举一个例子：一辆配备神经网络以识别道路上行人和各种障碍物的自动驾驶汽车。在这种情况下，汽车的系统不仅需要识别人或障碍物，而且必须准确且实时地做到这一点。任何延迟或误识别都可能导致潜在的危险情况。
- en: Furthermore, it’s not only about detecting obstacles but also understanding
    the level of certainty in that detection. Imagine a scenario where a self-driving
    car’s detection network struggles to determine whether there’s an obstruction
    ahead confidently. If the car’s system is uncertain about an object—perhaps due
    to poor lighting conditions or an obscured view—it should be programmed to lean
    more heavily on data from its other sensors, such as lidar or radar, to decide
    whether braking is necessary and to proceed cautiously, slow down, or even halt.
    This dual requirement of precise detection and self-awareness of its own certainty
    levels ensures safer navigation and decision-making, especially in dynamic and
    unpredictable road environments. See the article *Risk-Sensitive Decision-Making
    for Autonomous-Driving* ([https://uu.diva-portal.org/smash/get/diva2:1698692/FULLTEXT01.pdf](https://uu.diva-portal.org/smash/get/diva2:1698692/FULLTEXT01.pdf))
    if you are interested in more details on the subject.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这不仅仅是关于检测障碍，还包括理解检测的确定性水平。想象一下这样的场景：自动驾驶汽车的检测网络难以自信地确定前方是否有障碍。如果汽车的系统对物体不确定——可能是由于照明条件差或视线受阻——它应该被编程为更多地依赖来自其其他传感器的数据，例如激光雷达或雷达，以决定是否需要制动，并谨慎行驶，减速或甚至停车。这种精确检测和自我意识其确定性水平的双重要求确保了更安全的导航和决策，尤其是在动态和不可预测的道路环境中。如果您想了解更多关于这个主题的细节，请参阅文章《自动驾驶的风险敏感决策》（[https://uu.diva-portal.org/smash/get/diva2:1698692/FULLTEXT01.pdf](https://uu.diva-portal.org/smash/get/diva2:1698692/FULLTEXT01.pdf)）。
- en: Accurate confidence estimates play a pivotal role in enhancing model interpretability.
    Humans inherently understand and relate to probabilities, making it an intuitive
    measure to gauge predictions.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 准确的置信度估计在增强模型可解释性方面发挥着关键作用。人类天生理解并能够与概率建立联系，这使得它成为衡量预测的一个直观指标。
- en: When a model provides well-calibrated confidence levels, it offers an additional
    layer of information that bolsters its credibility to the user. This is especially
    crucial for neural networks, as their decision-making processes can be complex
    and challenging to decipher. Moreover, reliable probability assessments can be
    integrated into broader probabilistic models, further expanding their utility
    and application.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型提供良好的校准置信水平时，它为用户提供了额外的信息层，增强了其可信度。这对于神经网络尤为重要，因为它们的决策过程可能复杂且难以解读。此外，可靠的概率评估可以集成到更广泛的概率模型中，进一步扩大其效用和应用范围。
- en: This combination of accuracy and introspection ensures that automated decision-making
    systems are trustworthy and reliable, fostering confidence in their integration
    into critical applications.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这种准确性和内省的结合确保了自动化决策系统值得信赖且可靠，有助于增强其在关键应用中的集成信心。
- en: Miscalibration refers to the disparity between a model’s stated confidence in
    its predictions and the actual accuracy of those predictions. For instance, if
    a model claims 90% confidence for a set of predictions, one expects approximately
    90% of those predictions to be correct. However, despite their high accuracy,
    deep learning models often needed to catch up on their expressed confidence and
    actual correctness.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 校准误差指的是模型对其预测的置信度与实际预测准确度之间的差异。例如，如果一个模型声称对一组预测有90%的置信度，人们预期大约90%的预测是正确的。然而，尽管深度学习模型具有较高的准确率，但它们往往需要赶上其表达的置信度和实际正确性。
- en: Fast forward to the present, and while contemporary neural networks have seen
    significant advancements in accuracy compared to those from a decade ago, it’s
    intriguing to note that they no longer maintain calibration.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 快进到今天，尽管与十年前相比，当代神经网络在准确性方面取得了显著进步，但值得注意的是，它们不再保持校准。
- en: It wasn’t until 2017 that the magnitude of this issue was brought to the forefront
    of the AI community’s attention. A pivotal paper, *On Calibration of Modern Neural
    Networks (Guo, 2017)*, ([https://proceedings.mlr.press/v70/guo17a.html](https://proceedings.mlr.press/v70/guo17a.html))
    discovered that deep neural networks are poorly calibrated, spotlighting the calibration
    conundrum inherent in deep learning systems.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 直到2017年，这个问题的重要性才被带到人工智能社区的关注焦点。一篇关键论文，*《现代神经网络的校准》（Guo, 2017）*，([https://proceedings.mlr.press/v70/guo17a.html](https://proceedings.mlr.press/v70/guo17a.html))
    发现深度神经网络校准不佳，突显了深度学习系统中固有的校准难题。
- en: 'This research not only underscored the severe miscalibration of these systems
    but also brought to light a startling revelation: several of the *state-of-the-art*
    techniques that had been hailed as breakthroughs, such as dropout, weight decay,
    and batch normalization, were paradoxically exacerbating the miscalibration issue.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这项研究不仅强调了这些系统的严重误校准，而且还揭示了惊人的发现：一些被誉为突破性的*最先进*技术，如dropout、权重衰减和批量归一化，反而加剧了误校准问题。
- en: This seminal paper served as a wake-up call. It prompted introspection within
    the community, urging researchers to question and revisit the techniques they
    had championed. The paper was a critique and an invitation to explore and rectify
    the issue. Its lucid exposition and profound insights made it a must-read for
    anyone in the field.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇开创性的论文起到了警钟的作用。它促使社区进行反思，敦促研究人员质疑并重新审视他们所倡导的技术。这篇论文是对问题的批判和探索及纠正的邀请。它清晰的表达和深刻的洞察力使它成为该领域任何人的必读之作。
- en: While the years following the 2012 ImageNet competition were characterized by
    rapid progress and unbridled optimism, the 2017 paper served as a moment of reckoning.
    It underscored the importance of introspection in science and the continuous need
    to refine, recalibrate, and, if necessary, rethink our approaches, ensuring that
    the AI systems we build are accurate and reliably calibrated.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然2012年ImageNet竞赛之后的几年以快速进步和无拘无束的乐观为特征，但2017年的论文却是一个清算的时刻。它强调了科学中反思的重要性以及不断改进、重新校准和必要时重新思考我们方法的持续需求，以确保我们构建的人工智能系统是准确且可靠校准的。
- en: Confidence calibration is the problem of predicting probability estimates that
    represent the actual outcome. It is crucial for classification models in many
    applications because good confidence estimates provide valuable information to
    establish trustworthiness with the user. Good probability estimates can be used
    for model interpretability, as humans have a natural cognitive intuition for probabilities.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 置信度校准是预测表示实际结果的概率估计的问题。在许多应用中，这对于分类模型至关重要，因为良好的置信度估计可以为建立与用户的信任提供有价值的信息。良好的概率估计可用于模型可解释性，因为人类对概率有自然的认知直觉。
- en: The authors of the paper *On Calibration of Modern Neural Networks* found that
    increased model capacity and lack of regularization are closely related to the
    miscalibration phenomenon observed in deep neural networks. Model capacity has
    increased dramatically over the past few years, with networks having hundreds
    or thousands of layers and hundreds of convolutional filters per layer. Recent
    work shows that very deep or wide models can generalize better than smaller ones
    while exhibiting the capacity to fit the training set easily. However, this increased
    capacity can lead to overfitting and miscalibration.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 论文《现代神经网络的校准》的作者发现，模型容量增加和缺乏正则化与深度神经网络中观察到的误校准现象密切相关。在过去几年中，模型容量大幅增加，网络拥有数百或数千层，每层有数百个卷积滤波器。最近的研究表明，非常深或宽的模型可以比较小的模型更好地泛化，同时展现出轻松拟合训练集的能力。然而，这种增加的容量可能导致过拟合和误校准。
- en: Regarding shallow classical neural networks, the paper *On Calibration of Modern
    Neural Networks* posited that traditional (or shallow) neural networks were well
    calibrated. This belief stemmed from a highly cited 2005 paper by Niculescu-Mizil
    and R. Caruana, titled *Predicting good probabilities with supervised learning*
    ([https://www.cs.cornell.edu/~alexn/papers/calibration.icml05.crc.rev3.pdf](https://www.cs.cornell.edu/~alexn/papers/calibration.icml05.crc.rev3.pdf)).
    Presented at the prestigious ICML conference, this paper has amassed over 1,570
    citations since its publication. One of the conclusions reached was that shallow
    (classical) neural networks were “well calibrated.”
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 关于浅层经典神经网络，论文《现代神经网络的校准》提出，传统的（或浅层的）神经网络校准良好。这种信念源于Niculescu-Mizil和R. Caruana于2005年发表的一篇高度引用的论文，题为《使用监督学习预测良好的概率》([https://www.cs.cornell.edu/~alexn/papers/calibration.icml05.crc.rev3.pdf](https://www.cs.cornell.edu/~alexn/papers/calibration.icml05.crc.rev3.pdf))。这篇论文在著名的ICML会议上发表，自发表以来已有超过1,570次引用。其中得出的一个结论是，浅层（经典）神经网络“校准良好”。
- en: However, this conclusion about the calibration of shallow neural networks was
    upended later. In a 2020 study titled *Are Traditional Neural Networks Well-Calibrated?*
    ([https://ieeexplore.ieee.org/document/8851962](https://ieeexplore.ieee.org/document/8851962)),
    the authors debunked the widely held belief that shallow neural networks are well-calibrated.
    Their findings revealed that traditional shallow networks are poorly calibrated,
    and their ensembles exhibit the same issue. Fortunately, the researchers also
    highlighted that the calibration of these networks can be significantly enhanced
    using the Venn-ABERS conformal prediction method we learned about in previous
    chapters.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，关于浅层神经网络校准的结论后来被颠覆了。在一篇2020年的研究中，题为《传统神经网络是否校准良好？》([https://ieeexplore.ieee.org/document/8851962](https://ieeexplore.ieee.org/document/8851962))，作者们驳斥了浅层神经网络校准良好的广泛观点。他们的发现显示，传统的浅层网络校准不良，其集成也表现出同样的问题。幸运的是，研究人员还强调，使用我们在前几章中了解到的Venn-ABERS一致性预测方法可以显著提高这些网络的校准。
- en: Overconfidence in modern deep learning computer vision models
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 现代深度学习计算机视觉模型的过度自信
- en: 'Many deep learning models designed for computer vision predominantly utilize
    convolution-based architectures. These architectures have propelled the field
    forward, achieving unprecedented predictive accuracy. However, there’s an unintended
    side effect: *these models often produce* *overconfident predictions*:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为计算机视觉设计的许多深度学习模型主要利用基于卷积的架构。这些架构推动了该领域的发展，实现了前所未有的预测准确性。然而，存在一个意想不到的副作用：*这些模型经常产生*
    *过度自信的预测*：
- en: '**Accuracy versus quality**: The relentless pursuit of accuracy in deep learning
    has led to models that can correctly classify images with remarkable precision.
    However, accuracy is just one facet of a model’s performance. Predictive quality,
    which encompasses aspects such as the reliability and calibration of predictions,
    is equally vital.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确性 versus 质量**：深度学习对准确性的不懈追求导致了能够以非凡的精确度正确分类图像的模型。然而，准确性只是模型性能的一个方面。预测质量，包括预测的可靠性和校准等方面，同样至关重要。'
- en: '**The overconfidence issue**: Even as these models achieve higher accuracy
    rates, they tend to be excessively confident in their predictions. This means
    they do so with high confidence when they make an error, indicating they believe
    strongly in the incorrect prediction.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过度自信问题**：尽管这些模型实现了更高的准确率，但它们在预测中往往过于自信。这意味着当它们犯错时，会以很高的信心做出预测，表明它们坚信错误的预测。'
- en: '**Implications in critical applications**: This overconfidence poses considerable
    risks, especially in sectors where the stakes are high. Consider healthcare: a
    misdiagnosis by a computer vision system analyzing medical scans might lead medical
    professionals to pursue incorrect treatments if made with high confidence. Similarly,
    an overconfident misinterpretation of a road scene in autonomous vehicles could
    result in dangerous manoeuvres.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在关键应用中的影响**：这种过度自信带来了相当大的风险，尤其是在风险较高的领域。以医疗保健为例：一个分析医学扫描的计算机视觉系统如果做出误诊，并且有很高的信心，可能会导致医疗专业人员采取错误的治疗措施。同样，在自动驾驶汽车中，对道路场景过度自信的误解可能导致危险的操作。'
- en: In essence, as the deep learning community pushes the boundaries of accuracy,
    it’s imperative also to address the calibration of these models. Ensuring that
    they not only make accurate predictions but also gauge the confidence of those
    predictions appropriately is crucial, especially when these models are employed
    in life-critical applications.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，随着深度学习社区推动准确性的边界，也必须解决这些模型的校准问题。确保它们不仅做出准确的预测，而且适当地衡量这些预测的信心是至关重要的，尤其是在这些模型被用于生命攸关的应用时。
- en: Various approaches to quantify uncertainty in computer vision problems
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算机视觉问题中量化不确定性的各种方法
- en: 'Uncertainty quantification in computer vision is crucial for ensuring vision-based
    systems’ reliability and safety, especially when deployed in critical applications.
    Over the years, various approaches have been developed to address and quantify
    this uncertainty. Here’s a look at some of the most prominent methods:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉中量化不确定性对于确保基于视觉的系统可靠性和安全性至关重要，尤其是在部署在关键应用中时。多年来，已经开发出各种方法来处理和量化这种不确定性。以下是一些最显著的方法：
- en: '**Bayesian Neural Networks** (**BNNs**): These neural networks treat weights
    as probability distributions rather than fixed values. By doing so, they can provide
    a measure of uncertainty for their predictions. During inference, multiple forward
    passes are made with different weight samples, producing a distribution of outputs
    that capture the model’s uncertainty.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**贝叶斯神经网络**（**BNNs**）：这些神经网络将权重视为概率分布而不是固定值。通过这样做，它们可以为它们的预测提供不确定性度量。在推理期间，通过不同的权重样本进行多次正向传递，产生一个捕获模型不确定性的输出分布。'
- en: '**Monte Carlo dropout**: Monte Carlo dropout involves performing dropout during
    inference. By running the network multiple times with dropout and averaging the
    results, a distribution over the outputs is obtained, which can be used to gauge
    uncertainty.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**蒙特卡洛dropout**：蒙特卡洛dropout涉及在推理期间执行dropout。通过多次运行具有dropout的网络并平均结果，可以得到一个输出分布，可以用来衡量不确定性。'
- en: '**Ensemble methods**: Ensemble methods involve training multiple models and
    aggregating their predictions. The variance in predictions across models can be
    used as a proxy for uncertainty. This approach is computationally expensive but
    often leads to more robust uncertainty estimates.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成方法**：集成方法涉及训练多个模型并汇总它们的预测。模型之间预测的方差可以用作不确定性的代理。这种方法在计算上很昂贵，但通常会导致更稳健的不确定性估计。'
- en: '**Deep Gaussian processes**: Deep Gaussian processes combine deep learning
    with Gaussian processes to provide a non-parametric way to estimate uncertainty.
    They offer a rich way to capture complex uncertainties but can be computationally
    challenging for large datasets.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度高斯过程**：深度高斯过程将深度学习与高斯过程相结合，提供了一种非参数估计不确定性的方法。它们提供了一种丰富的方式来捕捉复杂的不确定性，但对于大数据集来说可能在计算上具有挑战性。'
- en: '**Conformal prediction**: Conformal prediction provides a set of possible outcomes
    for a prediction, each with a confidence level. This set-based prediction approach
    is designed to guarantee coverage, meaning that the actual outcome will fall within
    the predicted set with a probability equal to the confidence level.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一致性预测**：一致性预测为预测提供了一组可能的输出，每个输出都有一个置信水平。这种基于集合的预测方法旨在保证覆盖范围，这意味着实际结果将以等于置信水平的概率落在预测集合中。'
- en: '**Calibration techniques**: While not directly measuring uncertainty, calibration
    techniques such as Platt scaling or temperature scaling ensure that the predicted
    confidence scores reflect the true likelihood of correctness. A well-calibrated
    model’s predicted probabilities are more interpretable and can be used as a measure
    of uncertainty.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**校准技术**：虽然不是直接测量不确定性，但校准技术，如 Platt 缩放或温度缩放，确保预测的置信度得分反映了正确性的真实可能性。一个校准良好的模型的预测概率更具可解释性，可以用作不确定性的度量。'
- en: The superiority of conformal prediction in uncertainty quantification
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一致性预测在不确定性量化中的优越性
- en: 'Quantifying uncertainty is fundamental to building robust and reliable machine
    learning models. Several methodologies have emerged over the years, each with
    its own merits. However, conformal prediction stands out as a particularly compelling
    framework. Let’s explain why:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 量化不确定性是构建稳健和可靠的机器学习模型的基础。多年来，已经出现了几种方法，每种方法都有其优点。然而，一致性预测脱颖而出，成为一个特别有吸引力的框架。让我们来解释一下原因：
- en: '**Distribution-free framework**: One of the most notable features of conformal
    prediction is that it doesn’t make any assumptions about the distribution of the
    data. Many uncertainty quantification methods are based on certain probabilistic
    assumptions or rely on specific data distributions to function effectively. In
    contrast, conformal prediction remains agnostic to these considerations, making
    it versatile and widely applicable across diverse datasets.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无分布框架**：一致性预测最显著的特点之一是它不对数据的分布做出任何假设。许多不确定性量化方法基于某些概率假设或依赖于特定的数据分布才能有效运作。相比之下，一致性预测对这些考虑因素保持无偏见，使其具有通用性和广泛适用于各种数据集。'
- en: '**Theoretical guarantees**: conformal prediction offers robust theoretical
    guarantees for its predictions. Specifically, it provides a set of potential outcomes
    for a prediction, and each outcome is associated with a confidence level. The
    framework ensures that the actual outcome will fall within the predicted set with
    a probability corresponding to the confidence level. This is a powerful assurance,
    especially in critical applications where understanding the bounds of a prediction
    is essential.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**理论保证**：一致性预测为它的预测提供了稳健的理论保证。具体来说，它为预测提供了一组潜在的结果，并且每个结果都与一个置信水平相关联。该框架确保实际结果将以与置信水平相对应的概率落在预测集合内。这是一种强大的保证，尤其是在理解预测界限至关重要的关键应用中。'
- en: '**Model independence**: Another significant advantage of conformal prediction
    is its independence from the underlying model. Whether you’re working with a simple
    linear regression, a complex deep learning architecture, or any other model, conformal
    prediction can be applied seamlessly. This flexibility ensures that practitioners
    are open in their choice of model when seeking to quantify uncertainty.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型独立性**：一致性预测的另一个显著优势是其与底层模型的独立性。无论您是在使用简单的线性回归、复杂的深度学习架构还是任何其他模型，一致性预测都可以无缝应用。这种灵活性确保了实践者在寻求量化不确定性时对模型的选择是开放的。'
- en: '**Scalability with dataset size**: conformal prediction is not sensitive to
    the size of the dataset. Whether dealing with a small dataset with limited entries
    or a massive one with millions of data points, the framework remains effective
    and reliable. This scalability is especially beneficial in modern applications
    where data can range from scarce to overwhelmingly abundant.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与数据集大小可扩展性**：一致性预测对数据集的大小不敏感。无论是处理具有有限条目的小型数据集还是处理包含数百万数据点的庞大数据集，该框架都保持有效和可靠。这种可扩展性在数据可以从稀缺到极其丰富现代应用中特别有益。'
- en: While numerous approaches exist for uncertainty quantification, conformal prediction
    emerges as a frontrunner due to its distribution-free nature, robust theoretical
    underpinnings, model independence, and scalability. For practitioners seeking
    a robust and reliable method to gauge the uncertainty of their machine learning
    models, conformal prediction presents a compelling choice.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然存在许多不确定性量化方法，但由于其无分布的特性、稳健的理论基础、模型独立性和可扩展性，一致性预测成为了一种领先的方法。对于寻求一种稳健且可靠的方法来评估其机器学习模型的不确定性的人来说，一致性预测提供了一个有吸引力的选择。
- en: Conformal prediction for computer vision
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一致性预测在计算机视觉中的应用
- en: In this section, we will dive deeper into the diverse applications of conformal
    prediction in computer vision. With its broad range of problems, from image classification
    to object detection, computer vision presents challenges that require precise
    and reliable machine learning models. As we navigate these applications, we will
    demonstrate how conformal prediction is a robust tool to quantify the uncertainty
    associated with these models.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入探讨一致性预测在计算机视觉中的多样化应用。由于计算机视觉涵盖了从图像分类到目标检测的广泛问题，它提出了需要精确和可靠机器学习模型的挑战。在我们导航这些应用的过程中，我们将展示一致性预测是如何作为一种稳健的工具来量化与这些模型相关的不确定性的。
- en: By exploring these practical examples, we aim to underscore the importance of
    understanding the model’s confidence in its predictions. Understanding is crucial,
    especially when decisions based on these predictions could have significant consequences.
    Conformal prediction, with its ability to provide a measure of uncertainty, can
    greatly aid researchers and practitioners in making informed decisions based on
    the outputs of their models. This improves the system’s reliability and paves
    the way for more transparent and trustworthy AI implementations in computer vision.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 通过探索这些实际例子，我们旨在强调理解模型对其预测的置信度的重要性。理解至关重要，尤其是在基于这些预测的决策可能产生重大后果的情况下。具有提供不确定性度量能力的一致预测可以极大地帮助研究人员和实践者根据其模型输出做出明智的决策。这提高了系统的可靠性，并为计算机视觉中更透明和值得信赖的AI实现铺平了道路。
- en: Uncertainty sets for image classifiers using conformal prediction
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用一致预测的图像分类器的不确定性集
- en: In 2020, researchers from the University of California, Berkeley, published
    a paper titled *Uncertainty sets for image classifiers using Conformal* *Prediction*
    ([https://arxiv.org/abs/2009.14193](https://arxiv.org/abs/2009.14193)).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在2020年，加州大学伯克利分校的研究人员发表了一篇题为《使用一致预测的图像分类器的不确定性集》(*Uncertainty sets for image
    classifiers using Conformal* *Prediction*)的论文([https://arxiv.org/abs/2009.14193](https://arxiv.org/abs/2009.14193))。
- en: This was the first time that computer vision researchers applied conformal prediction
    to the computer vision problem. The paper described the first conformal prediction
    method explicitly developed for computer vision, RAPS, which is the current state
    of the art for image classification.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这是计算机视觉研究人员首次将一致预测应用于计算机视觉问题。论文描述了第一个专门为计算机视觉开发的一致预测方法RAPS，它是当前图像分类的最新水平。
- en: 'Here are the key points from the paper:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是论文中的关键点：
- en: The paper proposes a new method called **regularized adaptive predictive sets**
    (**RAPS**) for generating stable prediction sets with neural network classifiers
    guaranteed to achieve a desired coverage level.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 论文提出了一种名为**正则化自适应预测集**(**RAPS**)的新方法，用于生成具有神经网络分类器保证达到所需覆盖水平的稳定预测集。
- en: RAPS modifies an existing conformal prediction algorithm to produce smaller,
    more stable prediction sets by regularizing the influence of noisy probability
    estimates for unlikely classes.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAPS通过正则化对不可能类别的噪声概率估计的影响，修改了现有的一致预测算法，以产生更小、更稳定的预测集。
- en: RAPS is evaluated on ImageNet classification using ResNet and other CNN models.
    It achieves the desired coverage levels while producing prediction sets that are
    substantially smaller (5 to 10 times smaller) than a standalone Platt scaling
    baseline.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAPS在ImageNet分类中使用ResNet和其他CNN模型进行评估。它在产生预测集的同时，实现了所需的覆盖水平，并且这些预测集的规模显著减小（比独立
    Platt 缩放基线小5到10倍）。
- en: The method satisfies theoretical guarantees on coverage and is proven to provide
    the best performance for selecting fixed-size sets.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该方法在覆盖范围上满足理论保证，并被证明在选择固定大小集合时提供最佳性能。
- en: RAPS provides a practical way to obtain prediction sets from any image classifier
    that can reliably quantify uncertainty and identify complex test examples. The
    authors suggest applications in areas such as medical imaging and active learning.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAPS提供了一种从任何能够可靠量化不确定性和识别复杂测试示例的图像分类器中获取预测集的实用方法。作者建议在医学成像和主动学习等领域应用。
- en: 'Here is a summary of how the RAPS algorithm works:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是RAPS算法工作原理的总结：
- en: It uses a pre-trained image classifier to compute class probability estimates
    for images in the calibration set and class probability estimates for a new test
    image.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它使用预训练的图像分类器来计算校准集中图像的类别概率估计以及新测试图像的类别概率估计。
- en: 'For every image within the calibration set, RAPS calculates conformity scores,
    denoted as E j as follows: E j = ∑ i=1 k ′    ( ˆ π  (i)(x j) + λ1[i > k reg]).
    This is achieved by arranging the probability estimates in a descending sequence.
    The scores are then computed by accumulating these probability estimates, starting
    from the highest and continuing down to (and including) the probability estimate
    of the image’s actual class. The calculation is illustrated in the following *Figure
    9**.1*.'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于校准集中的每张图像，RAPS计算一致性分数，记为E j，如下所示：E j = ∑ i=1 k ′    ( ˆ π  (i)(x j) + λ1[i
    > k reg]). 这是通过将概率估计按降序排列来实现的。然后通过累积这些概率估计来计算分数，从最高开始，一直持续到（包括）图像实际类别的概率估计。计算过程如图9**.1**所示。
- en: A high value of λ acts as a deterrent against creating sets that are larger
    than k reg.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 高λ值起到阻止创建大于k reg的集合的威慑作用。
- en: As is standard in inductive conformal prediction, the model then computes the
    1-alpha quantile of the conformity scores computed on the calibration set.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在归纳一致预测中，模型随后计算在校准集上计算的一致性分数的1-α分位数。
- en: Outputs the k* highest-score classes where the conformity score E test for the
    test point is greater or equal the 1-alpha quantile.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出k*最高分数的类别，其中测试点的符合性分数E test大于或等于1-α分位数。
- en: 'The following figure illustrates the RAPS method. The figure is from Anastasios
    N. Angelopoulos’ blog *Uncertainty Sets for Image Classifiers using Conformal*
    *Prediction*: ([https://people.eecs.berkeley.edu/~angelopoulos/blog/posts/conformal-classification/](https://people.eecs.berkeley.edu/~angelopoulos/blog/posts/conformal-classification/)).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了RAPS方法。该图来自Anastasios N. Angelopoulos的博客 *使用一致预测的图像分类的不确定性集合* (*Uncertainty
    Sets for Image Classifiers using Conformal* *Prediction*)：([https://people.eecs.berkeley.edu/~angelopoulos/blog/posts/conformal-classification/](https://people.eecs.berkeley.edu/~angelopoulos/blog/posts/conformal-classification/))。
- en: '![Figure 9.1 – An illustration of the RAPS method (the red line is drawn to
    achieve exact coverage)](img/B19925_09_01.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图9.1 – RAPS方法的示意图（红线是为了达到精确覆盖率）](img/B19925_09_01.jpg)'
- en: Figure 9.1 – An illustration of the RAPS method (the red line is drawn to achieve
    exact coverage)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 – RAPS方法的示意图（红线是为了达到精确覆盖率）
- en: The parameters λ and k reg are estimated by the RAPS model on the calibration
    set. The intuition behind parameters is that a high λ discourages sets larger
    than k reg.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 参数λ和k reg由RAPS模型在校准集上估计。参数背后的直觉是，高λ值会阻止大于k reg的集合。
- en: By construction, this prediction set provably contains the true class with probability
    of at least 1-α, where α is the desired error level. The regularization penalty
    allows RAPS to produce smaller, more stable sets than previous methods such as
    Platt scaling or the unregularized adaptive method.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 通过构造，这个预测集以至少1-α的概率包含真实类别，其中α是期望的错误水平。正则化惩罚允许RAPS产生比以前的方法（如Platt缩放或未正则化的自适应方法）更小、更稳定的集合。
- en: This approach allows researchers to use any underlying classifier and produce
    predictive sets that are assured to meet a designated error rate, such as 90%,
    all while maintaining a minimal average size. Its ease of deployment makes it
    a compelling, automated method to gauge the uncertainty of image classifiers,
    which is crucial in areas including medical diagnostics, autonomous vehicles,
    and screening hazardous online content.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法允许研究人员使用任何底层分类器，并产生确保满足指定错误率（如90%）的预测集，同时保持最小的平均大小。其部署的简便性使其成为衡量图像分类器不确定性的有吸引力的自动化方法，这在医疗诊断、自动驾驶和筛选危险在线内容等领域至关重要。
- en: In summary, RAPS leverages conformal prediction ideas to guarantee coverage,
    modifies the conformal score to enable smaller sets, and calibrates the procedure
    correctly using held-out data.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，RAPS利用一致预测的思想来保证覆盖率，修改一致分数以实现更小的集合，并使用保留数据正确校准程序。
- en: Building computer vision classifiers using conformal prediction
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用一致预测构建计算机视觉分类器
- en: Let’s illustrate the application of conformal prediction to computer vision
    in practice. We will use a notebook from the book repository available at `https://github.com/PacktPublishing/Practical-Guide-to-Applied-Conformal-Prediction/blob/main/Chapter_09.ipynb`.
    This notebook extensively uses notebooks from Anastasios Angelopolous’ *Conformal
    Prediction* repo at [https://github.com/aangelopoulos/conformal-prediction](https://github.com/aangelopoulos/conformal-prediction).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过实际应用来说明一致预测在计算机视觉中的应用。我们将使用来自书籍存储库的笔记本，该存储库可在`https://github.com/PacktPublishing/Practical-Guide-to-Applied-Conformal-Prediction/blob/main/Chapter_09.ipynb`找到。这个笔记本广泛使用了Anastasios
    Angelopolous的 *Conformal Prediction* 仓库中的笔记本，[https://github.com/aangelopoulos/conformal-prediction](https://github.com/aangelopoulos/conformal-prediction)。
- en: 'After loading the data, set up the problem and define the desired coverage
    and the number of points in the calibration set:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据后，设置问题和定义所需的覆盖率和校准集中的点数：
- en: '[PRE0]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The softmax scores were split into the calibration and test datasets, obtaining
    calibration and test labels:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 将softmax分数分为校准集和测试数据集，获得校准和测试标签：
- en: '[PRE2]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The test dataset contains 49,000 points, and the calibration dataset contains
    1,000 points. Both datasets include images and human-readable labels from the
    ImageNet dataset.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 测试数据集包含49,000个点，校准数据集包含1,000个点。这两个数据集都包括来自ImageNet数据集的图像和可读标签。
- en: Naïve Conformal prediction
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 朴素符合预测
- en: 'We''ll first look at a naïve way to produce prediction sets using conformal
    prediction:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将探讨使用符合预测的朴素方法来生成预测集：
- en: Compute a non-conformity score for each calibration point
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每个校准点计算一个非一致性分数
- en: Then, an empirical quantile of the calibration scores will be evaluated
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，将评估校准分数的经验分位数。
- en: 'This closely resembles what we observed with inductive conformal prediction
    in earlier chapters. We determine non-conformity scores through hinge loss, and
    then use the distribution of these scores to calculate the quantile based on the
    desired coverage. This process, including the final sample correction formula,
    parallels our approach for inductive conformal prediction:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们之前章节中观察到的归纳符合预测非常相似。我们通过 hinge 损失确定非一致性分数，然后使用这些分数的分布来根据所需的覆盖范围计算分位数。这个过程，包括最终的样本校正公式，与我们的归纳符合预测方法类似：
- en: '[PRE7]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can form the prediction sets for test set objects using the computed get
    adjusted quantile on nonconformity scores:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用计算得到的非一致性分数的调整分位数来为测试集对象形成预测集：
- en: '[PRE10]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The result is an array showcasing sets of predictions. This Boolean array signifies
    ImageNet classes according to the Boolean values it holds. The Boolean values
    indicate the classes chosen by the model, with `True` signifying a class is selected
    and `False` meaning the class is not included in the prediction set.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个显示预测集合的数组。这个布尔数组根据它持有的布尔值表示ImageNet类别。布尔值表示模型选择的类别，其中`True`表示选择了该类别，而`False`表示该类别不包括在预测集中。
- en: '![Figure 9.2 – An illustration of the prediction sets for the test set](img/B19925_09_02.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图9.2 – 测试集预测集的示意图](img/B19925_09_02.jpg)'
- en: Figure 9.2 – An illustration of the prediction sets for the test set
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 – 测试集预测集的示意图
- en: 'We can calculate the empirical coverage, which comes very close to the specified
    confidence level of 90%:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以计算经验覆盖度，它非常接近指定的90%置信水平：
- en: '[PRE11]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We can look at some of the objects and prediction sets.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看一些对象和预测集。
- en: '![Figure 9.3 – An object from the test set, the prediction set produced by
    the naïve variant of conformal prediction was the label "palace"](img/B19925_09_03.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图9.3 – 测试集中的对象，由符合预测的朴素变体产生的预测集标签为“宫殿”](img/B19925_09_03.jpg)'
- en: Figure 9.3 – An object from the test set, the prediction set produced by the
    naïve variant of conformal prediction was the label "palace"
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 – 测试集中的对象，由符合预测的朴素变体产生的预测集标签为“宫殿”
- en: For objects with higher levels of uncertainty, prediction sets contain more
    than one element.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不确定性较高的对象，预测集中包含多个元素。
- en: '![Figure 9.4 – An object from the test set, the prediction set produced by
    the naïve variant of conformal prediction was [‘Crock Pot’, ‘digital clock’]](img/B19925_09_04.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图9.4 – 测试集中的对象，由符合预测的朴素变体产生的预测集为[‘慢炖锅’，‘数字时钟’]](img/B19925_09_04.jpg)'
- en: Figure 9.4 – An object from the test set, the prediction set produced by the
    naïve variant of conformal prediction was [‘Crock Pot’, ‘digital clock’]
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 – 测试集中的对象，由符合预测的朴素变体产生的预测集为[‘慢炖锅’，‘数字时钟’]
- en: 'The naïve method presents two significant issues:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素方法存在两个显著问题：
- en: Firstly, the probabilities produced by CNNs often need to be more accurate,
    resulting in sets that don’t achieve the intended coverage
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，CNN生成的概率通常需要更准确，导致无法达到预期覆盖率的集合
- en: Secondly, for instances where the model lacks confidence, the naive method must
    include numerous classes to attain the desired confidence threshold, leading to
    an excessively large set
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，对于模型缺乏信心的实例，朴素方法必须包含多个类别以达到所需的置信度阈值，从而导致集合过大
- en: Temperature scaling isn’t a remedy, as it only adjusts the score of the primary
    class, and calibrating the remaining scores is an overwhelming task. Interestingly,
    even with the perfect calibration of all scores, the naive approach would still
    fall short of achieving coverage.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 温度缩放并不是解决办法，因为它只调整主要类的分数，而校准剩余分数是一项艰巨的任务。有趣的是，即使所有分数都完美校准，朴素方法仍然无法达到覆盖范围。
- en: Alternative ways of constructing prediction sets were developed to address these
    issues, namely **Adaptive Prediction Sets** (**APS**) and **Regularized** **Adaptive
    Prediction** **Sets** (**RAPS**).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 开发了构建预测集的替代方法来解决这些问题，即**自适应预测集**（**APS**）和**正则化自适应预测集**（**RAPS**）。
- en: Adaptive Prediction Sets (APS)
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自适应预测集（APS）
- en: Next, we'll look at APS, described in the NeurIPS spotlight paper, *Classification
    with Valid and Adaptive Coverage* (2000) (https://proceedings.neurips.cc/paper/2020/file/244edd7e85dc81602b7615cd705545f5-Paper.pdf).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将查看 NeurIPS 焦点论文中描述的 APS，即 *具有有效和自适应覆盖的分类*（2000）(https://proceedings.neurips.cc/paper/2020/file/244edd7e85dc81602b7615cd705545f5-Paper.pdf)。
- en: In essence, APS presents a simple approach. Instead of directly using the softmax
    scores, a new threshold is determined based on a calibration dataset. For example,
    if sets with a projected probability of 93% yield a 90% coverage on the calibration
    set, then a 93% threshold would be adopted. APS is a particular implementation
    of RAPS, and unlike the naïve approach, it aims to achieve precise coverage.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，APS 提出了一种简单的方法。它不是直接使用软化分数，而是根据校准数据集确定一个新的阈值。例如，如果具有 93% 预测概率的集合在校准集上产生
    90% 的覆盖率，则采用 93% 的阈值。APS 是 RAPS 的特定实现，与朴素方法不同，它旨在实现精确的覆盖率。
- en: 'However, APS does face a practical hurdle: the average size of its sets is
    significantly large. Deep learning classifiers grapple with a permutation dilemma:
    their scores for less certain classes, such as those ranked from 10 to 1,000,
    don’t reflect accurate probability estimates. The arrangement of these classes
    is largely swayed by noise, prompting APS to opt for vast sets, especially for
    complex images.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，APS 面临一个实际障碍：其集合的平均大小显著较大。深度学习分类器在与排列困境作斗争：对于不太确定的类别，如排名第 10 到 1,000 的类别，它们的分数并不反映准确的概率估计。这些类别的排列很大程度上受到噪声的影响，促使
    APS 选择庞大的集合，特别是对于复杂图像。
- en: 'The code describing APS is as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 描述 APS 的代码如下：
- en: '[PRE13]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let’s look at the code in more detail. It uses APS to generate prediction sets
    based on a specified quantile threshold:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地查看代码。它使用 APS 根据指定的分位数阈值生成预测集：
- en: '`cal_pi = cal_smx.argsort(1)[:, ::-1]`: This sorts the softmax scores ``cal_smx`
    for each instance score from `cal_smx` in descending order and returns the indices
    of the sorted values.'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`cal_pi = cal_smx.argsort(1)[:, ::-1]`: 这对每个实例的 `cal_smx` 软化分数进行降序排序，并返回排序值的索引。'
- en: '`cal_srt = np.take_along_axis(cal_smx, cal_pi, axis=1).cumsum(axis=1)`: For
    each row, it rearranges the scores based on the indices from `cal_pi`, then computes
    the cumulative sum along the columns.'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`cal_srt = np.take_along_axis(cal_smx, cal_pi, axis=1).cumsum(axis=1)`: 对于每一行，它根据
    `cal_pi` 的索引重新排列分数，然后计算沿列的累积和。'
- en: '`cal_scores = np.take_along_axis(cal_srt, cal_pi.argsort(axis=1), axis=1)[range(n_cal),
    cal_labels]`: This step retrieves the specific scores corresponding to the true
    labels `(cal_labels)`. It first reverts the sorted order of `cal_pi` to get the
    original ordering and then picks the scores associated with the true labels for
    each instance.'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`cal_scores = np.take_along_axis(cal_srt, cal_pi.argsort(axis=1), axis=1)[range(n_cal),
    cal_labels]`: 此步骤检索与真实标签 `(cal_labels)` 对应的特定分数。它首先将 `cal_pi` 的排序顺序反转以获取原始顺序，然后为每个实例选择与真实标签相关的分数。'
- en: '`qhat = np.quantile(cal_scores, np.ceil((n_cal + 1) * (1 - alpha)) / n_cal,
    method="higher")`: Calculates the quantile value based on the provided `alpha`.
    This value will serve as the threshold for the prediction phase.*   `test_pi =
    test_smx.argsort(1)[:, ::-1]`: Similarly, for the test set, it sorts the scores
    from `test_smx` in descending order and returns the indices of the sorted values.*   `test_srt=
    np.take_along_axis(test_smx, test_pi, axis=1).cumsum(axis=1)`: Rearranges the
    test set scores based on the `test_pi` sorted indices and computes the cumulative
    sum.*   `prediction_sets= np.take_along_axis(test_srt <= qhat, test_pi.argsort(axis=1),
    axis=1)`: For each instance in the test set, it determines which scores are below
    the quantile threshold `qhat`. This Boolean array (`test_srt <= qhat`) is then
    rearranged into its original order using `test_pi.argsort(axis=1)`, resulting
    in the final prediction sets where `True` entries indicate inclusion in the set.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qhat = np.quantile(cal_scores, np.ceil((n_cal + 1) * (1 - alpha)) / n_cal,
    method="higher")`: 基于提供的 `alpha` 计算分位数值。此值将作为预测阶段的阈值。*   `test_pi = test_smx.argsort(1)[:,
    ::-1]`: 类似地，对于测试集，它按降序对 `test_smx` 中的分数进行排序，并返回排序值的索引。*   `test_srt= np.take_along_axis(test_smx,
    test_pi, axis=1).cumsum(axis=1)`: 根据排序索引 `test_pi` 重新排列测试集分数，并计算累积和。*   `prediction_sets=
    np.take_along_axis(test_srt <= qhat, test_pi.argsort(axis=1), axis=1)`: 对于测试集中的每个实例，它确定哪些分数低于分位数阈值
    `qhat`。然后，布尔数组 (`test_srt <= qhat`) 使用 `test_pi.argsort(axis=1)` 重新排列到其原始顺序，从而得到最终预测集，其中
    `True` 条目表示包含在集合中。'
- en: In essence, this code is used to calibrate model scores to define a threshold
    and then uses this threshold to generate prediction sets for a new (test) dataset.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，此代码用于校准模型得分以定义一个阈值，然后使用此阈值为新（测试）数据集生成预测集。
- en: We can look at some objects and prediction sets generated by APS.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看一些由APS生成的对象和预测集。
- en: '![Figure 9.5 – An object from the test set](img/B19925_09_05.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图9.5 – 测试集中的对象](img/B19925_09_05.jpg)'
- en: Figure 9.5 – An object from the test set
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 – 测试集中的对象
- en: 'Unfortunately, as already mentioned and demonstrated in this example, the prediction
    sets produced by APS can be vast. The preceding example produced a prediction
    set of:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，正如前面提到的并在本例中演示的那样，APS生成的预测集可能非常庞大。前一个例子生成了一个预测集：
- en: '[''King Charles Spaniel'', ''Rhodesian Ridgeback'', ''Afghan Hound'', ''Basset
    Hound'', ''Bloodhound'', ''Redbone Coonhound'', ''Otterhound'', ''Weimaraner'',
    ''Irish Terrier'', ''Norfolk Terrier'', ''Norwich Terrier'', ''Australian Terrier'',
    ''Dandie Dinmont Terrier'', ''Tibetan Terrier'', ''Soft-coated Wheaten Terrier'',
    ''Flat-Coated Retriever'', ''Golden Retriever'', ''Labrador Retriever'', ''Vizsla'',
    ''English Setter'', ''Irish Setter'', ''Gordon Setter'', ''Clumber Spaniel'',
    ''English Springer Spaniel'', ''Welsh Springer Spaniel'', ''Cocker Spaniels'',
    ''Sussex Spaniel'', ''Irish Water Spaniel'', ''Briard'', ''Bullmastiff'', ''Leonberger'',
    ''Newfoundland'', ''Chow Chow'', ''Miniature Poodle'', ''Standard Poodle'', ''lion'',
    ''brown bear'', ''grasshopper'', ''leafhopper'', ''doormat'', ''handkerchief'',
    ''maze'', ''prayer rug'', ''tennis ball'', ''acorn''].'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '[''King Charles Spaniel'', ''Rhodesian Ridgeback'', ''Afghan Hound'', ''Basset
    Hound'', ''Bloodhound'', ''Redbone Coonhound'', ''Otterhound'', ''Weimaraner'',
    ''Irish Terrier'', ''Norfolk Terrier'', ''Norwich Terrier'', ''Australian Terrier'',
    ''Dandie Dinmont Terrier'', ''Tibetan Terrier'', ''Soft-coated Wheaten Terrier'',
    ''Flat-Coated Retriever'', ''Golden Retriever'', ''Labrador Retriever'', ''Vizsla'',
    ''English Setter'', ''Irish Setter'', ''Gordon Setter'', ''Clumber Spaniel'',
    ''English Springer Spaniel'', ''Welsh Springer Spaniel'', ''Cocker Spaniels'',
    ''Sussex Spaniel'', ''Irish Water Spaniel'', ''Briard'', ''Bullmastiff'', ''Leonberger'',
    ''Newfoundland'', ''Chow Chow'', ''Miniature Poodle'', ''Standard Poodle'', ''lion'',
    ''brown bear'', ''grasshopper'', ''leafhopper'', ''doormat'', ''handkerchief'',
    ''maze'', ''prayer rug'', ''tennis ball'', ''acorn''].'
- en: Regularized Adaptive Prediction Sets (RAPS)
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化自适应预测集（RAPS）
- en: We now get hands-on with RAPS, which was briefly introduced in the *Uncertainty
    sets for image classifiers using conformal prediction* section earlier in this
    chapter.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来实际操作RAPS，它在本章前面的*使用一致性预测的图像分类器的置信集*部分中简要介绍过。
- en: 'We set the RAPS regularization parameters (a larger `lam_reg` value and smaller
    `k_reg` value leads to smaller sets) and regularization vector in the following
    code block:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在以下代码块中设置了RAPS正则化参数（更大的`lam_reg`值和更小的`k_reg`值会导致集合更小）和正则化向量：
- en: '[PRE26]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'As previously, we compute non-conformity scores and obtain score quantiles:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们计算非一致性得分并获取得分分位数：
- en: '[PRE31]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We can deploy predictions on the test set using the following code:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码在测试集上部署预测：
- en: '[PRE37]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Let’s look at some objects and prediction sets generated by RAPS.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看一些由RAPS生成的对象和预测集。
- en: "![Figure 9.6 – An object from the test set\uFEFF; the prediction set produced\
    \ by RAPS was [‘electric ray’]](img/B19925_09_06.jpg)"
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图9.6 – 测试集中的对象；RAPS生成的预测集为[‘electric ray’]](img/B19925_09_06.jpg)'
- en: Figure 9.6 – An object from the test set; the prediction set produced by RAPS
    was [‘electric ray’]
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 – 测试集中的对象；RAPS生成的预测集为[‘electric ray’]
- en: We can see that for objects with little uncertainty, RAPS produces one-element
    prediction sets. Unlike APS, RAPS still produces rather parsimonious prediction
    sets for objects involving more uncertainty.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，对于不确定性较小的对象，RAPS生成单元素预测集。与APS不同，RAPS对于涉及更多不确定性的对象仍然生成相当节俭的预测集。
- en: "![Figure 9\uFEFF.7 – An object from the test set\uFEFF; the prediction set\
    \ produced by RAPS was [‘red wolf’, ‘coyote’, ‘dhole’, ‘gray fox’]](img/B19925_09_07.jpg)"
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图9.7 – 测试集中的对象；RAPS生成的预测集为[‘red wolf’，‘coyote’，‘dhole’，‘gray fox’]](img/B19925_09_07.jpg)'
- en: Figure 9.7 – An object from the test set; the prediction set produced by RAPS
    was [‘red wolf’, ‘coyote’, ‘dhole’, ‘gray fox’]
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 – 测试集中的对象；RAPS生成的预测集为[‘red wolf’，‘coyote’，‘dhole’，‘gray fox’]
- en: Let’s summarize the chapter next.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结本章内容。
- en: Summary
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In the rapidly evolving realm of technology, computer vision has transformed
    from mere image recognition into an integral component of countless real-world
    applications. As these applications span diverse fields such as autonomous vehicles
    and medical diagnostics, the pressure on computer vision models to deliver accurate
    and reliable predictions intensifies. With the growing sophistication of these
    models comes a dire need: quantifying prediction uncertainties.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在技术快速发展的领域，计算机视觉已经从单纯的图像识别转变为无数现实应用的重要组成部分。随着这些应用跨越多个领域，如自动驾驶汽车和医疗诊断，对计算机视觉模型提供准确和可靠预测的压力不断加大。随着这些模型日益复杂，一个迫切的需求随之而来：量化预测不确定性。
- en: This is where conformal prediction shines. Unlike traditional models that typically
    output a singular prediction, conformal prediction offers a range of potential
    outcomes, each coupled with a confidence measure. This novel approach grants users
    a detailed perspective on model predictions, which is invaluable for applications
    where precision is paramount.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是一致性预测发光的地方。与通常只输出单一预测的传统模型不同，一致性预测提供了一系列可能的预测结果，每个结果都附带一个置信度度量。这种新颖的方法使用户能够获得对模型预测的详细视角，这对于精度至关重要的应用来说是无价的。
- en: 'This chapter delved into the symbiotic relationship between conformal prediction
    and computer vision. We started by emphasizing the importance of uncertainty quantification
    in computer vision, citing its pivotal role in areas including autonomous transportation
    and medical imaging. Further, we shed light on a major area for improvement in
    contemporary deep learning models: their tendency to deliver miscalibrated predictions.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 本章深入探讨了一致性预测与计算机视觉之间的共生关系。我们首先强调了不确定性量化在计算机视觉中的重要性，并引用了其在自动驾驶交通和医学成像等领域的关键作用。进一步地，我们揭示了当代深度学习模型的一个主要改进领域：它们倾向于提供失准的预测。
- en: By working through this chapter, you have acquired the expertise to craft cutting-edge
    computer vision classifiers infused with the capabilities of conformal prediction.
    Additionally, you got experience of the top-tier open source conformal prediction
    tools tailored for computer vision, ensuring you’re well equipped for future endeavors.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 通过学习本章，您已经获得了制作融合一致性预测能力的尖端计算机视觉分类器的专业知识。此外，您还获得了使用针对计算机视觉定制的顶级开源一致性预测工具的经验，确保您为未来的努力做好了充分准备。
- en: The key achievements in this chapter are to grasp the role of uncertainty quantification
    in computer vision, unravel the reasons behind deep learning’s miscalibrated predictions,
    explore diverse strategies to measure uncertainty in computer vision tasks, comprehend
    the fundamentals and applications of conformal prediction in computer vision,
    and attain mastery of constructing computer vision classifiers powered by conformal
    prediction.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的关键成就包括掌握不确定性量化在计算机视觉中的作用，揭示深度学习预测失准的原因，探索测量计算机视觉任务中不确定性的多种策略，理解计算机视觉中一致性预测的基本原理和应用，以及掌握构建由一致性预测驱动的计算机视觉分类器。
- en: In the next chapter, we will navigate the world of conformal prediction in NLP,
    understand its significance, and learn how to harness its power for more reliable
    and confident predictions.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探索自然语言处理中的一致性预测世界，了解其重要性，并学习如何利用其力量进行更可靠和自信的预测。
