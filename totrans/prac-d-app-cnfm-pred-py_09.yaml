- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Conformal Prediction for Computer Vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In today’s fast-paced world, computer vision has grown beyond mere image recognition
    to be a fundamental cornerstone in numerous real-world applications. From self-driving
    cars navigating bustling streets to medical imaging systems that detect early
    signs of diseases, the demand for reliable and accurate computer vision models
    has never been higher. However, with the increasing complexity of these systems
    and their applications, a critical need arises for the ability to quantify the
    uncertainty associated with their predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Enter **conformal prediction**, a ground-breaking framework that offers a robust
    means to encapsulate the uncertainty inherent in machine learning models. While
    traditional computer vision models often produce a singular prediction, the true
    power of conformal prediction lies in its ability to provide a set of possible
    outcomes, each backed by a confidence level. This offers practitioners a more
    informed, nuanced view of the model’s predictions, enabling safer and more reliable
    deployments in critical applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter dives deep into the marriage of conformal prediction and computer
    vision. We begin by illuminating the necessity of uncertainty quantification in
    computer vision, highlighting its significance in real-world scenarios including
    **autonomous driving** and **healthcare diagnostics**. As we navigate deeper,
    we’ll explore the Achilles’ heel of modern deep learning models: *their tendency
    to produce* *miscalibrated predictions*.'
  prefs: []
  type: TYPE_NORMAL
- en: By the journey’s end, you’ll gain hands-on experience building state-of-the-art
    computer vision classifiers imbued with the power of conformal prediction. We’ll
    introduce and guide you through the best open source conformal prediction libraries
    in computer vision applications to ensure you have all the tools necessary to
    embark on this journey.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Uncertainty quantification for computer vision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why deep learning produces miscalibrated predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various approaches to quantify uncertainty in computer vision problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conformal prediction for computer vision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building computer vision classifiers using Conformal prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uncertainty quantification for computer vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a domain, computer vision has transformed many sectors by automating complex
    tasks that were once reserved for human eyes and cognition. Computer vision models
    have become an integral part of modern technology, whether it’s detecting pedestrians
    on the road, identifying potential tumours in medical scans, or even analyzing
    satellite images for environmental studies. However, as the reliance on these
    models grows, so does the need to understand and quantify the uncertainty associated
    with their predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Why does uncertainty matter?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before deep-diving into the mechanics, it’s essential to understand why we
    need **uncertainty quantification** (**UQ**) in the first place. Let’s go through
    some of the reasons as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Safety and reliability**: A wrong prediction can have severe consequences
    in critical applications, such as medical imaging or autonomous driving. Knowing
    the confidence level in a prediction can aid in decision-making, such as whether
    to trust the model’s prediction or seek human intervention.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model improvements**: Uncertainty measurements can provide insights into
    areas where the model might be lacking, helping to guide data collection and training
    enhancements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trustworthiness**: Knowing that a system acknowledges its limitations and
    can provide confidence intervals or uncertainty metrics for end users and stakeholders
    makes it more trustworthy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Navigating the world of computer vision, one inevitably encounters uncertainties
    that can influence the accuracy of model predictions. But what are the sources
    of these uncertainties, and can they be managed? Let’s delve into the two primary
    types of uncertainty in computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: Types of uncertainty in computer vision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Uncertainty in computer vision can be broadly classified into two categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Aleatoric uncertainty**: This type of uncertainty arises from the inherent
    noise in the data. For instance, low-light images, blurry images, or images taken
    from varying angles introduce variability that the model might find challenging
    to handle. Aleatoric uncertainty is often irreducible, meaning no matter how good
    the model becomes, this uncertainty will always exist due to the inherent noise
    in the observations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Epistemic uncertainty**: This type of uncertainty stems from the model itself.
    It could be due to incomplete training data, model architecture choices, or the
    optimization process. Given enough data or improvements in model design, epistemic
    uncertainty can be reduced.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the realm of computer vision, it’s not enough to simply get a prediction.
    As advanced as our models are, they can sometimes be overly confident, potentially
    leading to misinformed decisions. How do we gauge the reliability of these predictions?
    Enter the world of uncertainty quantification.
  prefs: []
  type: TYPE_NORMAL
- en: Quantifying uncertainty
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Modern computer vision models, especially deep learning architectures, produce
    predictions that are often overconfident. This miscalibration can be misleading,
    especially in critical applications. The need, therefore, is not just to produce
    a prediction but also to accompany it with a measure of confidence or uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: Various methods have been proposed to quantify uncertainty, ranging from Bayesian
    neural networks, which provide a distribution over model parameters, to ensemble
    methods, which rely on the variability of predictions across different models.
  prefs: []
  type: TYPE_NORMAL
- en: However, as we’ll see in the subsequent sections, Conformal prediction offers
    a fresh and rigorous perspective on uncertainty quantification tailored to the
    needs of computer vision applications.
  prefs: []
  type: TYPE_NORMAL
- en: Uncertainty quantification for computer vision is not a theoretical exercise
    but a crucial aspect of building reliable, safe, and trustworthy models. Understanding
    and accounting for their inherent uncertainties will be paramount as computer
    vision systems continue to permeate every sector.
  prefs: []
  type: TYPE_NORMAL
- en: Why does deep learning produce miscalibrated predictions?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **ImageNet Large Scale Visual Recognition Challenge** (**ILSVRC**) is an
    annual competition where research teams evaluate their algorithms on a given dataset,
    aiming to push the boundaries of computer vision. 2012 was a watershed moment
    for the field, marking a significant shift towards the dominance of deep learning
    in computer vision ([https://www.image-net.org/challenges/LSVRC/2012/](https://www.image-net.org/challenges/LSVRC/2012/)).
  prefs: []
  type: TYPE_NORMAL
- en: Before the advent of deep learning, computer vision primarily relied on hand-engineered
    features and traditional machine learning techniques. Algorithms such as **Scale-Invariant
    Feature Transform** (**SIFT**), **Histogram of Oriented Gradients** (**HOG**),
    and **Speeded-Up Robust Features** (**SURF**) were commonly used to extract features
    from images. These features would then be fed into machine learning classifiers
    such as **Support Vector Machines** (**SVM**) to make predictions. While these
    methods had their successes, they had significant limitations regarding scalability
    and performance on more complex datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In 2012, a deep convolutional neural network named AlexNet ([https://en.wikipedia.org/wiki/AlexNet](https://en.wikipedia.org/wiki/AlexNet)),
    developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, was entered
    into the ILSVRC. It achieved a top-5 error rate of 15.3%, a staggering 10.8 percentage
    points lower than the second-place finisher. This dramatic improvement was an
    incremental step and a quantum leap in performance.
  prefs: []
  type: TYPE_NORMAL
- en: Why was AlexNet revolutionary?
  prefs: []
  type: TYPE_NORMAL
- en: '**Deep architecture**: AlexNet was considerably deeper than other networks
    of its time. It had five convolutional layers followed by three fully connected
    layers. This depth allowed it to learn more complex and hierarchical features
    from the ImageNet dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GPU training**: The team utilized **graphics processing units** (**GPUs**)
    to train the network, which made it feasible to process the massive amount of
    data in the ImageNet dataset and efficiently train the deep architecture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ReLU activation**: Instead of traditional tanh or sigmoid activation functions,
    AlexNet employed the **Rectified Linear Unit** (**ReLU**) activation. This choice
    helped combat the vanishing gradient problem, enabling the training of deeper
    networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dropout**: To prevent overfitting, AlexNet introduced the dropout technique,
    where random subsets of neurons were “dropped out” during training, forcing the
    network to learn redundant representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dawn of 2012 marked a transformative moment in the realm of computer vision.
    Propelled by the unprecedented achievements of AlexNet in the ImageNet competition,
    the entire industry pivoted towards deep learning, especially **convolutional**
    **neural networks** (**CNNs**). As we journey through the aftermath of this revolution,
    we’ll witness the exponential growth in research, widespread industry adoption,
    and the relentless quest for more data and computational power.
  prefs: []
  type: TYPE_NORMAL
- en: Post-2012 – the deep learning surge
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The 2012 ImageNet competition, marked by the triumph of AlexNet, became a watershed
    moment in the field of computer vision. This victory underscored the profound
    potential of deep learning, especially **convolutional neural networks** (**CNNs**).
    As a result, the following happened:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Research boom**: After 2012, there was an explosion of research into deep
    learning for computer vision. Variants and improvements upon AlexNet, such as
    VGG, GoogLeNet, and ResNet, were rapidly developed, pushing the envelope further.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Industry adoption**: Tech giants and start-ups began investing heavily in
    deep learning research and applications, from facial recognition systems to augmented
    reality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Datasets and compute**: The success of deep learning fueled the creation
    of even larger datasets and a race for more powerful computation infrastructure,
    further accelerating the innovation cycle.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The 2012 ImageNet competition was a turning point, heralding the era of deep
    learning in computer vision. The principles and breakthroughs of AlexNet laid
    the foundation for the subsequent advancements we see today, from self-driving
    cars to real-time video analytics.
  prefs: []
  type: TYPE_NORMAL
- en: The "calibration crisis" in deep learning – a turning point in 2017
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since its triumphant ascendance following the 2012 ImageNet competition, deep
    learning experienced rapid advancements and widespread adoption across many domains.
    The community was engrossed in developing architectures, optimization techniques,
    and applications for five consecutive years. Yet, amidst this whirlwind of innovation,
    a significant concern remained largely overlooked: *the miscalibration of predictions
    produced by deep* *learning systems*.'
  prefs: []
  type: TYPE_NORMAL
- en: In real-world applications where automated systems drive decisions, it’s not
    enough for classification networks to merely provide accurate results. These systems
    play an integral role in various critical sectors, from healthcare to finance,
    and a misjudgment can have significant consequences. Therefore, it’s crucial that
    these classification networks not only deliver precise outcomes but also possess
    the self-awareness to flag potential uncertainties or errors in their predictions.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, in a medical diagnostic tool, beyond correctly identifying a disease,
    the system should also indicate its confidence level in that diagnosis. Medical
    professionals can take appropriate precautions if uncertain, perhaps seeking additional
    tests or expert opinions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take another example: a self-driving car equipped with a neural network designed
    to identify pedestrians and various obstacles on the road. In such a scenario,
    the car’s system doesn’t just need to recognize people or obstructions; it must
    do so accurately and in real time. Any delay or misidentification could lead to
    potentially dangerous situations.'
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, it’s not only about detecting obstacles but also understanding
    the level of certainty in that detection. Imagine a scenario where a self-driving
    car’s detection network struggles to determine whether there’s an obstruction
    ahead confidently. If the car’s system is uncertain about an object—perhaps due
    to poor lighting conditions or an obscured view—it should be programmed to lean
    more heavily on data from its other sensors, such as lidar or radar, to decide
    whether braking is necessary and to proceed cautiously, slow down, or even halt.
    This dual requirement of precise detection and self-awareness of its own certainty
    levels ensures safer navigation and decision-making, especially in dynamic and
    unpredictable road environments. See the article *Risk-Sensitive Decision-Making
    for Autonomous-Driving* ([https://uu.diva-portal.org/smash/get/diva2:1698692/FULLTEXT01.pdf](https://uu.diva-portal.org/smash/get/diva2:1698692/FULLTEXT01.pdf))
    if you are interested in more details on the subject.
  prefs: []
  type: TYPE_NORMAL
- en: Accurate confidence estimates play a pivotal role in enhancing model interpretability.
    Humans inherently understand and relate to probabilities, making it an intuitive
    measure to gauge predictions.
  prefs: []
  type: TYPE_NORMAL
- en: When a model provides well-calibrated confidence levels, it offers an additional
    layer of information that bolsters its credibility to the user. This is especially
    crucial for neural networks, as their decision-making processes can be complex
    and challenging to decipher. Moreover, reliable probability assessments can be
    integrated into broader probabilistic models, further expanding their utility
    and application.
  prefs: []
  type: TYPE_NORMAL
- en: This combination of accuracy and introspection ensures that automated decision-making
    systems are trustworthy and reliable, fostering confidence in their integration
    into critical applications.
  prefs: []
  type: TYPE_NORMAL
- en: Miscalibration refers to the disparity between a model’s stated confidence in
    its predictions and the actual accuracy of those predictions. For instance, if
    a model claims 90% confidence for a set of predictions, one expects approximately
    90% of those predictions to be correct. However, despite their high accuracy,
    deep learning models often needed to catch up on their expressed confidence and
    actual correctness.
  prefs: []
  type: TYPE_NORMAL
- en: Fast forward to the present, and while contemporary neural networks have seen
    significant advancements in accuracy compared to those from a decade ago, it’s
    intriguing to note that they no longer maintain calibration.
  prefs: []
  type: TYPE_NORMAL
- en: It wasn’t until 2017 that the magnitude of this issue was brought to the forefront
    of the AI community’s attention. A pivotal paper, *On Calibration of Modern Neural
    Networks (Guo, 2017)*, ([https://proceedings.mlr.press/v70/guo17a.html](https://proceedings.mlr.press/v70/guo17a.html))
    discovered that deep neural networks are poorly calibrated, spotlighting the calibration
    conundrum inherent in deep learning systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'This research not only underscored the severe miscalibration of these systems
    but also brought to light a startling revelation: several of the *state-of-the-art*
    techniques that had been hailed as breakthroughs, such as dropout, weight decay,
    and batch normalization, were paradoxically exacerbating the miscalibration issue.'
  prefs: []
  type: TYPE_NORMAL
- en: This seminal paper served as a wake-up call. It prompted introspection within
    the community, urging researchers to question and revisit the techniques they
    had championed. The paper was a critique and an invitation to explore and rectify
    the issue. Its lucid exposition and profound insights made it a must-read for
    anyone in the field.
  prefs: []
  type: TYPE_NORMAL
- en: While the years following the 2012 ImageNet competition were characterized by
    rapid progress and unbridled optimism, the 2017 paper served as a moment of reckoning.
    It underscored the importance of introspection in science and the continuous need
    to refine, recalibrate, and, if necessary, rethink our approaches, ensuring that
    the AI systems we build are accurate and reliably calibrated.
  prefs: []
  type: TYPE_NORMAL
- en: Confidence calibration is the problem of predicting probability estimates that
    represent the actual outcome. It is crucial for classification models in many
    applications because good confidence estimates provide valuable information to
    establish trustworthiness with the user. Good probability estimates can be used
    for model interpretability, as humans have a natural cognitive intuition for probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: The authors of the paper *On Calibration of Modern Neural Networks* found that
    increased model capacity and lack of regularization are closely related to the
    miscalibration phenomenon observed in deep neural networks. Model capacity has
    increased dramatically over the past few years, with networks having hundreds
    or thousands of layers and hundreds of convolutional filters per layer. Recent
    work shows that very deep or wide models can generalize better than smaller ones
    while exhibiting the capacity to fit the training set easily. However, this increased
    capacity can lead to overfitting and miscalibration.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding shallow classical neural networks, the paper *On Calibration of Modern
    Neural Networks* posited that traditional (or shallow) neural networks were well
    calibrated. This belief stemmed from a highly cited 2005 paper by Niculescu-Mizil
    and R. Caruana, titled *Predicting good probabilities with supervised learning*
    ([https://www.cs.cornell.edu/~alexn/papers/calibration.icml05.crc.rev3.pdf](https://www.cs.cornell.edu/~alexn/papers/calibration.icml05.crc.rev3.pdf)).
    Presented at the prestigious ICML conference, this paper has amassed over 1,570
    citations since its publication. One of the conclusions reached was that shallow
    (classical) neural networks were “well calibrated.”
  prefs: []
  type: TYPE_NORMAL
- en: However, this conclusion about the calibration of shallow neural networks was
    upended later. In a 2020 study titled *Are Traditional Neural Networks Well-Calibrated?*
    ([https://ieeexplore.ieee.org/document/8851962](https://ieeexplore.ieee.org/document/8851962)),
    the authors debunked the widely held belief that shallow neural networks are well-calibrated.
    Their findings revealed that traditional shallow networks are poorly calibrated,
    and their ensembles exhibit the same issue. Fortunately, the researchers also
    highlighted that the calibration of these networks can be significantly enhanced
    using the Venn-ABERS conformal prediction method we learned about in previous
    chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Overconfidence in modern deep learning computer vision models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many deep learning models designed for computer vision predominantly utilize
    convolution-based architectures. These architectures have propelled the field
    forward, achieving unprecedented predictive accuracy. However, there’s an unintended
    side effect: *these models often produce* *overconfident predictions*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy versus quality**: The relentless pursuit of accuracy in deep learning
    has led to models that can correctly classify images with remarkable precision.
    However, accuracy is just one facet of a model’s performance. Predictive quality,
    which encompasses aspects such as the reliability and calibration of predictions,
    is equally vital.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The overconfidence issue**: Even as these models achieve higher accuracy
    rates, they tend to be excessively confident in their predictions. This means
    they do so with high confidence when they make an error, indicating they believe
    strongly in the incorrect prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implications in critical applications**: This overconfidence poses considerable
    risks, especially in sectors where the stakes are high. Consider healthcare: a
    misdiagnosis by a computer vision system analyzing medical scans might lead medical
    professionals to pursue incorrect treatments if made with high confidence. Similarly,
    an overconfident misinterpretation of a road scene in autonomous vehicles could
    result in dangerous manoeuvres.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In essence, as the deep learning community pushes the boundaries of accuracy,
    it’s imperative also to address the calibration of these models. Ensuring that
    they not only make accurate predictions but also gauge the confidence of those
    predictions appropriately is crucial, especially when these models are employed
    in life-critical applications.
  prefs: []
  type: TYPE_NORMAL
- en: Various approaches to quantify uncertainty in computer vision problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Uncertainty quantification in computer vision is crucial for ensuring vision-based
    systems’ reliability and safety, especially when deployed in critical applications.
    Over the years, various approaches have been developed to address and quantify
    this uncertainty. Here’s a look at some of the most prominent methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bayesian Neural Networks** (**BNNs**): These neural networks treat weights
    as probability distributions rather than fixed values. By doing so, they can provide
    a measure of uncertainty for their predictions. During inference, multiple forward
    passes are made with different weight samples, producing a distribution of outputs
    that capture the model’s uncertainty.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monte Carlo dropout**: Monte Carlo dropout involves performing dropout during
    inference. By running the network multiple times with dropout and averaging the
    results, a distribution over the outputs is obtained, which can be used to gauge
    uncertainty.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensemble methods**: Ensemble methods involve training multiple models and
    aggregating their predictions. The variance in predictions across models can be
    used as a proxy for uncertainty. This approach is computationally expensive but
    often leads to more robust uncertainty estimates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep Gaussian processes**: Deep Gaussian processes combine deep learning
    with Gaussian processes to provide a non-parametric way to estimate uncertainty.
    They offer a rich way to capture complex uncertainties but can be computationally
    challenging for large datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conformal prediction**: Conformal prediction provides a set of possible outcomes
    for a prediction, each with a confidence level. This set-based prediction approach
    is designed to guarantee coverage, meaning that the actual outcome will fall within
    the predicted set with a probability equal to the confidence level.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Calibration techniques**: While not directly measuring uncertainty, calibration
    techniques such as Platt scaling or temperature scaling ensure that the predicted
    confidence scores reflect the true likelihood of correctness. A well-calibrated
    model’s predicted probabilities are more interpretable and can be used as a measure
    of uncertainty.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The superiority of conformal prediction in uncertainty quantification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Quantifying uncertainty is fundamental to building robust and reliable machine
    learning models. Several methodologies have emerged over the years, each with
    its own merits. However, conformal prediction stands out as a particularly compelling
    framework. Let’s explain why:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Distribution-free framework**: One of the most notable features of conformal
    prediction is that it doesn’t make any assumptions about the distribution of the
    data. Many uncertainty quantification methods are based on certain probabilistic
    assumptions or rely on specific data distributions to function effectively. In
    contrast, conformal prediction remains agnostic to these considerations, making
    it versatile and widely applicable across diverse datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Theoretical guarantees**: conformal prediction offers robust theoretical
    guarantees for its predictions. Specifically, it provides a set of potential outcomes
    for a prediction, and each outcome is associated with a confidence level. The
    framework ensures that the actual outcome will fall within the predicted set with
    a probability corresponding to the confidence level. This is a powerful assurance,
    especially in critical applications where understanding the bounds of a prediction
    is essential.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model independence**: Another significant advantage of conformal prediction
    is its independence from the underlying model. Whether you’re working with a simple
    linear regression, a complex deep learning architecture, or any other model, conformal
    prediction can be applied seamlessly. This flexibility ensures that practitioners
    are open in their choice of model when seeking to quantify uncertainty.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability with dataset size**: conformal prediction is not sensitive to
    the size of the dataset. Whether dealing with a small dataset with limited entries
    or a massive one with millions of data points, the framework remains effective
    and reliable. This scalability is especially beneficial in modern applications
    where data can range from scarce to overwhelmingly abundant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While numerous approaches exist for uncertainty quantification, conformal prediction
    emerges as a frontrunner due to its distribution-free nature, robust theoretical
    underpinnings, model independence, and scalability. For practitioners seeking
    a robust and reliable method to gauge the uncertainty of their machine learning
    models, conformal prediction presents a compelling choice.
  prefs: []
  type: TYPE_NORMAL
- en: Conformal prediction for computer vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will dive deeper into the diverse applications of conformal
    prediction in computer vision. With its broad range of problems, from image classification
    to object detection, computer vision presents challenges that require precise
    and reliable machine learning models. As we navigate these applications, we will
    demonstrate how conformal prediction is a robust tool to quantify the uncertainty
    associated with these models.
  prefs: []
  type: TYPE_NORMAL
- en: By exploring these practical examples, we aim to underscore the importance of
    understanding the model’s confidence in its predictions. Understanding is crucial,
    especially when decisions based on these predictions could have significant consequences.
    Conformal prediction, with its ability to provide a measure of uncertainty, can
    greatly aid researchers and practitioners in making informed decisions based on
    the outputs of their models. This improves the system’s reliability and paves
    the way for more transparent and trustworthy AI implementations in computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: Uncertainty sets for image classifiers using conformal prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In 2020, researchers from the University of California, Berkeley, published
    a paper titled *Uncertainty sets for image classifiers using Conformal* *Prediction*
    ([https://arxiv.org/abs/2009.14193](https://arxiv.org/abs/2009.14193)).
  prefs: []
  type: TYPE_NORMAL
- en: This was the first time that computer vision researchers applied conformal prediction
    to the computer vision problem. The paper described the first conformal prediction
    method explicitly developed for computer vision, RAPS, which is the current state
    of the art for image classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the key points from the paper:'
  prefs: []
  type: TYPE_NORMAL
- en: The paper proposes a new method called **regularized adaptive predictive sets**
    (**RAPS**) for generating stable prediction sets with neural network classifiers
    guaranteed to achieve a desired coverage level.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAPS modifies an existing conformal prediction algorithm to produce smaller,
    more stable prediction sets by regularizing the influence of noisy probability
    estimates for unlikely classes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAPS is evaluated on ImageNet classification using ResNet and other CNN models.
    It achieves the desired coverage levels while producing prediction sets that are
    substantially smaller (5 to 10 times smaller) than a standalone Platt scaling
    baseline.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The method satisfies theoretical guarantees on coverage and is proven to provide
    the best performance for selecting fixed-size sets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAPS provides a practical way to obtain prediction sets from any image classifier
    that can reliably quantify uncertainty and identify complex test examples. The
    authors suggest applications in areas such as medical imaging and active learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is a summary of how the RAPS algorithm works:'
  prefs: []
  type: TYPE_NORMAL
- en: It uses a pre-trained image classifier to compute class probability estimates
    for images in the calibration set and class probability estimates for a new test
    image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For every image within the calibration set, RAPS calculates conformity scores,
    denoted as E j as follows: E j = ∑ i=1 k ′    ( ˆ π  (i)(x j) + λ1[i > k reg]).
    This is achieved by arranging the probability estimates in a descending sequence.
    The scores are then computed by accumulating these probability estimates, starting
    from the highest and continuing down to (and including) the probability estimate
    of the image’s actual class. The calculation is illustrated in the following *Figure
    9**.1*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A high value of λ acts as a deterrent against creating sets that are larger
    than k reg.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As is standard in inductive conformal prediction, the model then computes the
    1-alpha quantile of the conformity scores computed on the calibration set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Outputs the k* highest-score classes where the conformity score E test for the
    test point is greater or equal the 1-alpha quantile.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following figure illustrates the RAPS method. The figure is from Anastasios
    N. Angelopoulos’ blog *Uncertainty Sets for Image Classifiers using Conformal*
    *Prediction*: ([https://people.eecs.berkeley.edu/~angelopoulos/blog/posts/conformal-classification/](https://people.eecs.berkeley.edu/~angelopoulos/blog/posts/conformal-classification/)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – An illustration of the RAPS method (the red line is drawn to
    achieve exact coverage)](img/B19925_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – An illustration of the RAPS method (the red line is drawn to achieve
    exact coverage)
  prefs: []
  type: TYPE_NORMAL
- en: The parameters λ and k reg are estimated by the RAPS model on the calibration
    set. The intuition behind parameters is that a high λ discourages sets larger
    than k reg.
  prefs: []
  type: TYPE_NORMAL
- en: By construction, this prediction set provably contains the true class with probability
    of at least 1-α, where α is the desired error level. The regularization penalty
    allows RAPS to produce smaller, more stable sets than previous methods such as
    Platt scaling or the unregularized adaptive method.
  prefs: []
  type: TYPE_NORMAL
- en: This approach allows researchers to use any underlying classifier and produce
    predictive sets that are assured to meet a designated error rate, such as 90%,
    all while maintaining a minimal average size. Its ease of deployment makes it
    a compelling, automated method to gauge the uncertainty of image classifiers,
    which is crucial in areas including medical diagnostics, autonomous vehicles,
    and screening hazardous online content.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, RAPS leverages conformal prediction ideas to guarantee coverage,
    modifies the conformal score to enable smaller sets, and calibrates the procedure
    correctly using held-out data.
  prefs: []
  type: TYPE_NORMAL
- en: Building computer vision classifiers using conformal prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s illustrate the application of conformal prediction to computer vision
    in practice. We will use a notebook from the book repository available at `https://github.com/PacktPublishing/Practical-Guide-to-Applied-Conformal-Prediction/blob/main/Chapter_09.ipynb`.
    This notebook extensively uses notebooks from Anastasios Angelopolous’ *Conformal
    Prediction* repo at [https://github.com/aangelopoulos/conformal-prediction](https://github.com/aangelopoulos/conformal-prediction).
  prefs: []
  type: TYPE_NORMAL
- en: 'After loading the data, set up the problem and define the desired coverage
    and the number of points in the calibration set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The softmax scores were split into the calibration and test datasets, obtaining
    calibration and test labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The test dataset contains 49,000 points, and the calibration dataset contains
    1,000 points. Both datasets include images and human-readable labels from the
    ImageNet dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Naïve Conformal prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ll first look at a naïve way to produce prediction sets using conformal
    prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute a non-conformity score for each calibration point
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, an empirical quantile of the calibration scores will be evaluated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This closely resembles what we observed with inductive conformal prediction
    in earlier chapters. We determine non-conformity scores through hinge loss, and
    then use the distribution of these scores to calculate the quantile based on the
    desired coverage. This process, including the final sample correction formula,
    parallels our approach for inductive conformal prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can form the prediction sets for test set objects using the computed get
    adjusted quantile on nonconformity scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The result is an array showcasing sets of predictions. This Boolean array signifies
    ImageNet classes according to the Boolean values it holds. The Boolean values
    indicate the classes chosen by the model, with `True` signifying a class is selected
    and `False` meaning the class is not included in the prediction set.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – An illustration of the prediction sets for the test set](img/B19925_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – An illustration of the prediction sets for the test set
  prefs: []
  type: TYPE_NORMAL
- en: 'We can calculate the empirical coverage, which comes very close to the specified
    confidence level of 90%:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We can look at some of the objects and prediction sets.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – An object from the test set, the prediction set produced by
    the naïve variant of conformal prediction was the label "palace"](img/B19925_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – An object from the test set, the prediction set produced by the
    naïve variant of conformal prediction was the label "palace"
  prefs: []
  type: TYPE_NORMAL
- en: For objects with higher levels of uncertainty, prediction sets contain more
    than one element.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – An object from the test set, the prediction set produced by
    the naïve variant of conformal prediction was [‘Crock Pot’, ‘digital clock’]](img/B19925_09_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – An object from the test set, the prediction set produced by the
    naïve variant of conformal prediction was [‘Crock Pot’, ‘digital clock’]
  prefs: []
  type: TYPE_NORMAL
- en: 'The naïve method presents two significant issues:'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, the probabilities produced by CNNs often need to be more accurate,
    resulting in sets that don’t achieve the intended coverage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secondly, for instances where the model lacks confidence, the naive method must
    include numerous classes to attain the desired confidence threshold, leading to
    an excessively large set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Temperature scaling isn’t a remedy, as it only adjusts the score of the primary
    class, and calibrating the remaining scores is an overwhelming task. Interestingly,
    even with the perfect calibration of all scores, the naive approach would still
    fall short of achieving coverage.
  prefs: []
  type: TYPE_NORMAL
- en: Alternative ways of constructing prediction sets were developed to address these
    issues, namely **Adaptive Prediction Sets** (**APS**) and **Regularized** **Adaptive
    Prediction** **Sets** (**RAPS**).
  prefs: []
  type: TYPE_NORMAL
- en: Adaptive Prediction Sets (APS)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we'll look at APS, described in the NeurIPS spotlight paper, *Classification
    with Valid and Adaptive Coverage* (2000) (https://proceedings.neurips.cc/paper/2020/file/244edd7e85dc81602b7615cd705545f5-Paper.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: In essence, APS presents a simple approach. Instead of directly using the softmax
    scores, a new threshold is determined based on a calibration dataset. For example,
    if sets with a projected probability of 93% yield a 90% coverage on the calibration
    set, then a 93% threshold would be adopted. APS is a particular implementation
    of RAPS, and unlike the naïve approach, it aims to achieve precise coverage.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, APS does face a practical hurdle: the average size of its sets is
    significantly large. Deep learning classifiers grapple with a permutation dilemma:
    their scores for less certain classes, such as those ranked from 10 to 1,000,
    don’t reflect accurate probability estimates. The arrangement of these classes
    is largely swayed by noise, prompting APS to opt for vast sets, especially for
    complex images.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code describing APS is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at the code in more detail. It uses APS to generate prediction sets
    based on a specified quantile threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cal_pi = cal_smx.argsort(1)[:, ::-1]`: This sorts the softmax scores ``cal_smx`
    for each instance score from `cal_smx` in descending order and returns the indices
    of the sorted values.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`cal_srt = np.take_along_axis(cal_smx, cal_pi, axis=1).cumsum(axis=1)`: For
    each row, it rearranges the scores based on the indices from `cal_pi`, then computes
    the cumulative sum along the columns.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`cal_scores = np.take_along_axis(cal_srt, cal_pi.argsort(axis=1), axis=1)[range(n_cal),
    cal_labels]`: This step retrieves the specific scores corresponding to the true
    labels `(cal_labels)`. It first reverts the sorted order of `cal_pi` to get the
    original ordering and then picks the scores associated with the true labels for
    each instance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`qhat = np.quantile(cal_scores, np.ceil((n_cal + 1) * (1 - alpha)) / n_cal,
    method="higher")`: Calculates the quantile value based on the provided `alpha`.
    This value will serve as the threshold for the prediction phase.*   `test_pi =
    test_smx.argsort(1)[:, ::-1]`: Similarly, for the test set, it sorts the scores
    from `test_smx` in descending order and returns the indices of the sorted values.*   `test_srt=
    np.take_along_axis(test_smx, test_pi, axis=1).cumsum(axis=1)`: Rearranges the
    test set scores based on the `test_pi` sorted indices and computes the cumulative
    sum.*   `prediction_sets= np.take_along_axis(test_srt <= qhat, test_pi.argsort(axis=1),
    axis=1)`: For each instance in the test set, it determines which scores are below
    the quantile threshold `qhat`. This Boolean array (`test_srt <= qhat`) is then
    rearranged into its original order using `test_pi.argsort(axis=1)`, resulting
    in the final prediction sets where `True` entries indicate inclusion in the set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In essence, this code is used to calibrate model scores to define a threshold
    and then uses this threshold to generate prediction sets for a new (test) dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We can look at some objects and prediction sets generated by APS.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – An object from the test set](img/B19925_09_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – An object from the test set
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, as already mentioned and demonstrated in this example, the prediction
    sets produced by APS can be vast. The preceding example produced a prediction
    set of:'
  prefs: []
  type: TYPE_NORMAL
- en: '[''King Charles Spaniel'', ''Rhodesian Ridgeback'', ''Afghan Hound'', ''Basset
    Hound'', ''Bloodhound'', ''Redbone Coonhound'', ''Otterhound'', ''Weimaraner'',
    ''Irish Terrier'', ''Norfolk Terrier'', ''Norwich Terrier'', ''Australian Terrier'',
    ''Dandie Dinmont Terrier'', ''Tibetan Terrier'', ''Soft-coated Wheaten Terrier'',
    ''Flat-Coated Retriever'', ''Golden Retriever'', ''Labrador Retriever'', ''Vizsla'',
    ''English Setter'', ''Irish Setter'', ''Gordon Setter'', ''Clumber Spaniel'',
    ''English Springer Spaniel'', ''Welsh Springer Spaniel'', ''Cocker Spaniels'',
    ''Sussex Spaniel'', ''Irish Water Spaniel'', ''Briard'', ''Bullmastiff'', ''Leonberger'',
    ''Newfoundland'', ''Chow Chow'', ''Miniature Poodle'', ''Standard Poodle'', ''lion'',
    ''brown bear'', ''grasshopper'', ''leafhopper'', ''doormat'', ''handkerchief'',
    ''maze'', ''prayer rug'', ''tennis ball'', ''acorn''].'
  prefs: []
  type: TYPE_NORMAL
- en: Regularized Adaptive Prediction Sets (RAPS)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now get hands-on with RAPS, which was briefly introduced in the *Uncertainty
    sets for image classifiers using conformal prediction* section earlier in this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We set the RAPS regularization parameters (a larger `lam_reg` value and smaller
    `k_reg` value leads to smaller sets) and regularization vector in the following
    code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'As previously, we compute non-conformity scores and obtain score quantiles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We can deploy predictions on the test set using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Let’s look at some objects and prediction sets generated by RAPS.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 9.6 – An object from the test set\uFEFF; the prediction set produced\
    \ by RAPS was [‘electric ray’]](img/B19925_09_06.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – An object from the test set; the prediction set produced by RAPS
    was [‘electric ray’]
  prefs: []
  type: TYPE_NORMAL
- en: We can see that for objects with little uncertainty, RAPS produces one-element
    prediction sets. Unlike APS, RAPS still produces rather parsimonious prediction
    sets for objects involving more uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 9\uFEFF.7 – An object from the test set\uFEFF; the prediction set\
    \ produced by RAPS was [‘red wolf’, ‘coyote’, ‘dhole’, ‘gray fox’]](img/B19925_09_07.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – An object from the test set; the prediction set produced by RAPS
    was [‘red wolf’, ‘coyote’, ‘dhole’, ‘gray fox’]
  prefs: []
  type: TYPE_NORMAL
- en: Let’s summarize the chapter next.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the rapidly evolving realm of technology, computer vision has transformed
    from mere image recognition into an integral component of countless real-world
    applications. As these applications span diverse fields such as autonomous vehicles
    and medical diagnostics, the pressure on computer vision models to deliver accurate
    and reliable predictions intensifies. With the growing sophistication of these
    models comes a dire need: quantifying prediction uncertainties.'
  prefs: []
  type: TYPE_NORMAL
- en: This is where conformal prediction shines. Unlike traditional models that typically
    output a singular prediction, conformal prediction offers a range of potential
    outcomes, each coupled with a confidence measure. This novel approach grants users
    a detailed perspective on model predictions, which is invaluable for applications
    where precision is paramount.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter delved into the symbiotic relationship between conformal prediction
    and computer vision. We started by emphasizing the importance of uncertainty quantification
    in computer vision, citing its pivotal role in areas including autonomous transportation
    and medical imaging. Further, we shed light on a major area for improvement in
    contemporary deep learning models: their tendency to deliver miscalibrated predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: By working through this chapter, you have acquired the expertise to craft cutting-edge
    computer vision classifiers infused with the capabilities of conformal prediction.
    Additionally, you got experience of the top-tier open source conformal prediction
    tools tailored for computer vision, ensuring you’re well equipped for future endeavors.
  prefs: []
  type: TYPE_NORMAL
- en: The key achievements in this chapter are to grasp the role of uncertainty quantification
    in computer vision, unravel the reasons behind deep learning’s miscalibrated predictions,
    explore diverse strategies to measure uncertainty in computer vision tasks, comprehend
    the fundamentals and applications of conformal prediction in computer vision,
    and attain mastery of constructing computer vision classifiers powered by conformal
    prediction.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will navigate the world of conformal prediction in NLP,
    understand its significance, and learn how to harness its power for more reliable
    and confident predictions.
  prefs: []
  type: TYPE_NORMAL
