<html><head></head><body>
		<div class="Content" id="_idContainer190">
			<h1 id="_idParaDest-172"><em class="italics"><a id="_idTextAnchor222"/>Appendix</em></h1>
		</div>
		<div>
			<div class="Content" id="_idContainer191">
			</div>
		</div>
		<div class="Content" id="_idContainer192">
			<h2>About</h2>
			<p>This section is included to assist the students to perform the activities in the book. It includes detailed steps that are to be performed by the students to achieve the objectives of the activities.</p>
		</div>
		<div class="Content" id="_idContainer234">
			<h2 id="_idParaDest-173"><a id="_idTextAnchor223"/>Chapter 1: Introduction to Clustering Methods</h2>
			<h3 id="_idParaDest-174"><a id="_idTextAnchor224"/>Activity 1: k-means Clustering with Three Clusters</h3>
			<p>Solution:<a id="_idTextAnchor225"/></p>
			<ol>
				<li><a id="_idTextAnchor226"/>Load the Iris dataset in the <strong class="inline">iris_data</strong> variable:<p class="snippet"><a id="_idTextAnchor227"/>iris_data&lt;-iris<a id="_idTextAnchor228"/></p></li>
				<li><a id="_idTextAnchor229"/>Create a <strong class="inline">t_color</strong> column and make its default value <strong class="inline">red</strong>. Change the value of the two species to <strong class="inline">green</strong> and <strong class="inline">blue</strong> so the third one remains <strong class="inline">red</strong>:<p class="snippet"><a id="_idTextAnchor230"/>iris_data$t_color='red'</p><p class="snippet"><a id="_idTextAnchor231"/>iris_data$t_color[which(iris_data$Species=='setosa')]&lt;-'green'</p><p class="snippet"><a id="_idTextAnchor232"/><a id="_idTextAnchor233"/>iris_data$t_color[which(iris_data$Species=='virginica')]&lt;-'blue'<a id="_idTextAnchor234"/></p><h4>Note</h4><p class="callout">Here, we change the <strong class="inline">color</strong> column of only those values whose species is <strong class="inline">setosa</strong> or <strong class="inline">virginica</strong>)</p></li>
				<li><a id="_idTextAnchor235"/>Choose any three random cluster centers:<p class="snippet"><a id="_idTextAnchor236"/>k1&lt;-c(7,3)</p><p class="snippet"><a id="_idTextAnchor237"/>k2&lt;-c(5,3)</p><p class="snippet"><a id="_idTextAnchor238"/>k3&lt;-c(6,2.5)<a id="_idTextAnchor239"/></p></li>
				<li><a id="_idTextAnchor240"/>Plot the <strong class="inline">x</strong>, <strong class="inline">y</strong> plot by entering the sepal length and sepal width in the <strong class="inline">plot()</strong> function, along with color:<p class="snippet"><a id="_idTextAnchor241"/>plot(iris_data$Sepal.Length,iris_data$Sepal.Width,col=iris_data$t_color)</p><p class="snippet"><a id="_idTextAnchor242"/>points(k1[1],k1[2],pch=4)</p><p class="snippet"><a id="_idTextAnchor243"/>points(k2[1],k2[2],pch=5)</p><p class="snippet"><a id="_idTextAnchor244"/>points(k3[1],k3[2],pch=6)<a id="_idTextAnchor245"/></p><p>Here is the output:</p><div class="IMG---Figure" id="_idContainer193"><img alt="Figure 1.36: Scatter plot for the given cluster centers" src="image/C12628_01_36.jpg"/></div><h6><a id="_idTextAnchor246"/></h6><h6><a id="_idTextAnchor247"/>Figure 1.36: Scatter plot for the given cluster centers</h6></li>
				<li>C<a id="_idTextAnchor248"/>hoose a number of iterations:<p class="snippet">n<a id="_idTextAnchor249"/>umber_of_steps&lt;-10</p></li>
				<li><a id="_idTextAnchor250"/>C<a id="_idTextAnchor251"/>hoose an the initial value of <strong class="inline">n</strong>:<p class="snippet">n<a id="_idTextAnchor252"/>&lt;-1</p></li>
				<li><a id="_idTextAnchor253"/>S<a id="_idTextAnchor254"/>tart the <strong class="inline">while</strong> loop for finding the cluster centers:<p class="snippet">w<a id="_idTextAnchor255"/>hile(n&lt;number_of_steps){</p></li>
				<li><a id="_idTextAnchor256"/>C<a id="_idTextAnchor257"/>alculate the distance of each point from the current cluster centers. We're calculating the Euclidean distance here using the <strong class="inline">sqrt</strong> function:<p class="snippet">i<a id="_idTextAnchor258"/>ris_data$distance_to_clust1 &lt;- sqrt((iris_data$Sepal.Length-k1[1])^2+(iris_data$Sepal.Width-k1[2])^2)</p><p class="snippet">i<a id="_idTextAnchor259"/>ris_data$distance_to_clust2 &lt;- sqrt((iris_data$Sepal.Length-k2[1])^2+(iris_data$Sepal.Width-k2[2])^2)</p><p class="snippet">i<a id="_idTextAnchor260"/>ris_data$distance_to_clust3 &lt;- sqrt((iris_data$Sepal.Length-k3[1])^2+(iris_data$Sepal.Width-k3[2])^2)</p></li>
				<li><a id="_idTextAnchor261"/>A<a id="_idTextAnchor262"/>ssign each point to a cluster to whose center it is closest:<a id="_idTextAnchor263"/><p class="snippet"> <a id="_idTextAnchor264"/> iris_data$clust_1 &lt;- 1*(iris_data$distance_to_clust1&lt;=iris_data$distance_to_clust2 &amp; iris_data$distance_to_clust1&lt;=iris_data$distance_to_clust3)</p><p class="snippet"> <a id="_idTextAnchor265"/> iris_data$clust_2 &lt;- 1*(iris_data$distance_to_clust1&gt;iris_data$distance_to_clust2 &amp; iris_data$distance_to_clust3&gt;iris_data$distance_to_clust2)</p><p class="snippet"> <a id="_idTextAnchor266"/> iris_data$clust_3 &lt;- 1*(iris_data$distance_to_clust3&lt;iris_data$distance_to_clust1 &amp; iris_data$distance_to_clust3&lt;iris_data$distance_to_clust2)</p></li>
				<li><a id="_idTextAnchor267"/>C<a id="_idTextAnchor268"/>alculate new cluster centers by calculating the mean <strong class="inline">x</strong> and <strong class="inline">y</strong> coordinates of each center with the <strong class="inline">mean()</strong> function in R:<p class="snippet"> <a id="_idTextAnchor269"/> k1[1]&lt;-mean(iris_data$Sepal.Length[which(iris_data$clust_1==1)])</p><p class="snippet"> <a id="_idTextAnchor270"/> k1[2]&lt;-mean(iris_data$Sepal.Width[which(iris_data$clust_1==1)])</p><p class="snippet"> <a id="_idTextAnchor271"/> k2[1]&lt;-mean(iris_data$Sepal.Length[which(iris_data$clust_2==1)])</p><p class="snippet"> <a id="_idTextAnchor272"/> k2[2]&lt;-mean(iris_data$Sepal.Width[which(iris_data$clust_2==1)])</p><p class="snippet"> <a id="_idTextAnchor273"/> k3[1]&lt;-mean(iris_data$Sepal.Length[which(iris_data$clust_3==1)])</p><p class="snippet"> <a id="_idTextAnchor274"/> k3[2]&lt;-mean(iris_data$Sepal.Width[which(iris_data$clust_3==1)])</p><p class="snippet"> <a id="_idTextAnchor275"/><a id="_idTextAnchor276"/> n=n+1</p><p class="snippet">}<a id="_idTextAnchor277"/><a id="_idTextAnchor278"/></p></li>
				<li><a id="_idTextAnchor279"/>C<a id="_idTextAnchor280"/>hoose the color for each center to plot a scatterplot:<a id="_idTextAnchor281"/><p class="snippet">i<a id="_idTextAnchor282"/>ris_data$color='red'</p><p class="snippet">i<a id="_idTextAnchor283"/>ris_data$color[which(iris_data$clust_2==1)]&lt;-'blue'</p><p class="snippet">i<a id="_idTextAnchor284"/>ris_data$color[which(iris_data$clust_3==1)]&lt;-'green'</p></li>
				<li><a id="_idTextAnchor285"/>P<a id="_idTextAnchor286"/>lot the final plot:<p class="snippet">p<a id="_idTextAnchor287"/>lot(iris_data$Sepal.Length,iris_data$Sepal.Width,col=iris_data$color)</p><p class="snippet">p<a id="_idTextAnchor288"/>oints(k1[1],k1[2],pch=4)</p><p class="snippet">p<a id="_idTextAnchor289"/>oints(k2[1],k2[2],pch=5)</p><p class="snippet">points(k3[1],k3[2],pch=6)</p><p>T<a id="_idTextAnchor290"/>he output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer194">
					<img alt="Figure 1.37: Scatter plot representing different species in different colors" src="image/C12628_01_37.jpg"/>
				</div>
			</div>
			<h6><a id="_idTextAnchor291"/><a id="_idTextAnchor292"/>Figure 1.37: Scatter plot representing different species in different colors</h6>
			<h3 id="_idParaDest-175"><a id="_idTextAnchor293"/>Activity 2: Customer Segmentation with k-means </h3>
			<p>Solution:</p>
			<ol>
				<li value="1">Download the data from <a href="https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Activity02/wholesale_customers_data.csv">https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Activity02/wholesale_customers_data.csv</a>.</li>
				<li>Read the data into the <strong class="inline">ws</strong> variable:<p class="snippet">ws&lt;-read.csv('wholesale_customers_data.csv')</p></li>
				<li>Store only column 5 and 6 in the <strong class="inline">ws</strong> variable by discarding the rest of the columns:<p class="snippet">ws&lt;-ws[5:6]</p></li>
				<li>Import the <strong class="inline">factoextra</strong> library:<p class="snippet">library(factoextra)</p></li>
				<li>Calculate the cluster centers for two centers:<p class="snippet">clus&lt;-kmeans(ws,2)</p></li>
				<li>Plot the chart for two clusters:<p class="snippet">fviz_cluster(clus,data=ws)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer195"><img alt="Figure 1.38: Chart for two clusters" src="image/C12628_01_38.jpg"/></div><h6>Figure 1.38: Chart for two clusters</h6><p>Notice how outliers are also part of the two clusters.</p></li>
				<li>Calculate the cluster centers for three clusters:<p class="snippet">clus&lt;-kmeans(ws,3)</p></li>
				<li>Plot the chart for three clusters:<p class="snippet">fviz_cluster(clus,data=ws)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer196"><img alt="Figure 1.39: Chart for three clusters" src="image/C12628_01_39.jpg"/></div><h6>Figure 1.39: Chart for three clusters</h6><p>Notice some outliers are now a part of a separate cluster.</p></li>
				<li>Calculate the cluster centers for four centers:<p class="snippet">clus&lt;-kmeans(ws,4)</p></li>
				<li>Plot the chart for four clusters:<p class="snippet">fviz_cluster(clus,data=ws)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer197"><img alt="Figure 1.40: Chart for four clusters" src="image/C12628_01_40.jpg"/></div><h6>Figure 1.40: Chart for four clusters</h6><p>Notice how outliers have started separating in two different clusters.</p></li>
				<li>Calculate the cluster centers for five clusters:<p class="snippet">clus&lt;-kmeans(ws,5)</p></li>
				<li>Plot the chart for five clusters:<p class="snippet">fviz_cluster(clus,data=ws)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer198"><img alt="Figure 1.41: Chart for five clusters" src="image/C12628_01_41.jpg"/></div><h6>Figure 1.41: Chart for five clusters</h6><p>Notice how outliers have clearly formed two separate clusters in red and blue, while the rest of the data is classified in three different clusters.</p></li>
				<li>Calculate the cluster centers for six clusters:<p class="snippet">clus&lt;-kmeans(ws,6)</p></li>
				<li>Plot the chart for six clusters:<p class="snippet">fviz_cluster(clus,data=ws)</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer199">
					<img alt="Figure 1.42: Chart for six clusters" src="image/C12628_01_42.jpg"/>
				</div>
			</div>
			<h6>Figure 1.42: Chart for six clusters</h6>
			<h3 id="_idParaDest-176"><a id="_idTextAnchor294"/>Activity 3: Performing Customer Segmentation with k-medoids Clustering </h3>
			<p>Solution:</p>
			<ol>
				<li value="1">Read the CSV file into the <strong class="inline">ws</strong> variable:<p class="snippet">ws&lt;-read.csv('wholesale_customers_data.csv')</p></li>
				<li>Store only columns 5 and 6 in the <strong class="inline">ws</strong> variable:<p class="snippet">ws&lt;-ws[5:6]</p></li>
				<li>Import the <strong class="inline">factoextra</strong> library for visualization:<p class="snippet">library(factoextra)</p></li>
				<li>Import the <strong class="inline">cluster</strong> library for clustering by PAM:<p class="snippet">library(cluster)</p></li>
				<li>Calculate clusters by entering data and the number of clusters in the <strong class="inline">pam</strong> function:<p class="snippet">clus&lt;-pam(ws,4)</p></li>
				<li>Plot a visualization of the clusters:<p class="snippet">fviz_cluster(clus,data=ws)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer200"><img alt="Figure 1.43: K-medoid plot of the clusters" src="image/C12628_01_43.jpg"/></div><h6>Figure 1.43: K-medoid plot of the clusters</h6></li>
				<li>Again, calculate the clusters with k-means and plot the output to compare with the output of the <strong class="inline">pam</strong> clustering:<p class="snippet">clus&lt;-kmeans(ws,4)</p><p class="snippet">fviz_cluster(clus,data=ws)</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer201">
					<img alt="Figure 1.44: K-means plot of the clusters" src="image/C12628_01_44.jpg"/>
				</div>
			</div>
			<h6>Figure 1.44: K-means plot of the clusters</h6>
			<h3 id="_idParaDest-177"><a id="_idTextAnchor295"/>Activity 4: Finding the Ideal Number of Market Segments</h3>
			<p>Solution:</p>
			<ol>
				<li value="1">Read the downloaded dataset into the <strong class="inline">ws</strong> variable:<p class="snippet">ws&lt;-read.csv('wholesale_customers_data.csv')</p></li>
				<li>Store only columns 5 and 6 in the variable by discarding other columns:<p class="snippet">ws&lt;-ws[5:6]</p></li>
				<li>Calculate the optimal number of clusters with the silhouette score:<p class="snippet">fviz_nbclust(ws, kmeans, method = "silhouette",k.max=20)</p><p>Here is the output:</p><div class="IMG---Figure" id="_idContainer202"><img alt="Figure 1.45: Graph representing optimal number of clusters with silhouette score" src="image/C12628_01_45.jpg"/></div><h6>Figure 1.45: Graph representing optimal number of clusters with the silhouette score</h6><p>The optimal number of clusters, according to the silhouette score, is two.</p></li>
				<li>Calculate the optimal number of clusters with the WSS score:<p class="snippet">fviz_nbclust(ws, kmeans, method = "wss", k.max=20)</p><p>Here is the output:</p><div class="IMG---Figure" id="_idContainer203"><img alt="Figure 1.46: Optimal number of clusters with the WSS score" src="image/C12628_01_46.jpg"/></div><h6>Figure 1.46: Optimal number of clusters with the WSS score</h6><p>The optimum number of clusters according to the WSS elbow method is around six.</p></li>
				<li>Calculate the optimal number of clusters with the Gap statistic:<p class="snippet">fviz_nbclust(ws, kmeans, method = "gap_stat",k.max=20)</p><p>Here is the output:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer204">
					<img alt="Figure 1.47: Optimal number of clusters with the Gap statistic" src="image/C12628_01_47.jpg"/>
				</div>
			</div>
			<h6>Figure 1.47: Optimal number of clusters with the Gap statistic</h6>
			<p>The optimal number of clusters according to the Gap statistic is one.</p>
			<h2 id="_idParaDest-178">Chapter 2: Ad<a id="_idTextAnchor296"/>vanced Clustering Methods</h2>
			<h3 id="_idParaDest-179"><a id="_idTextAnchor297"/>Activity 5: Implementing k-modes Clustering on the Mushroom Dataset</h3>
			<p>Solution:</p>
			<ol>
				<li value="1">Download <strong class="inline">mushrooms.csv</strong> from https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/blob/master/Lesson02/Activity05/mushrooms.csv.</li>
				<li>After downloading, load the <strong class="inline">mushrooms.csv</strong> file in R:<p class="snippet">ms&lt;-read.csv('mushrooms.csv')</p></li>
				<li>Check the dimensions of the dataset:<p class="snippet">dim(ms)</p><p>The output is as follows:</p><p class="snippet">[1] 8124   23</p></li>
				<li>Check the distribution of all columns:<p class="snippet">summary.data.frame(ms)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer205"><img alt="Figure 2.29: Screenshot of summary of distribution of all columns" src="image/C12628_02_29.jpg"/></div><h6>Figure 2.29: Screenshot of the summary of distribution of all columns</h6><p>Each column contains all the unique labels and their count.</p></li>
				<li>Store all the columns of the dataset, except for the final label, in a new variable, <strong class="inline">ms_k</strong>:<p class="snippet">ms_k&lt;-ms[,2:23]</p></li>
				<li>Import the <strong class="inline">klaR</strong> library, which has the <strong class="inline">kmodes</strong> function:<p class="snippet">install.packages('klaR')</p><p class="snippet">library(klaR)</p></li>
				<li>Calculate <strong class="inline">kmodes</strong> clusters and store them in a <strong class="inline">kmodes_ms</strong> variable. Enter the dataset without <strong class="inline">true</strong> labels as the first parameter and enter the number of clusters as the second parameter:<p class="snippet">kmodes_ms&lt;-kmodes(ms_k,2)</p></li>
				<li>Check the results by creating a table of <strong class="inline">true</strong> labels and <strong class="inline">cluster</strong> labels:<p class="snippet">result = table(ms$class, kmodes_ms$cluster)</p><p class="snippet">result</p><p>The output is as follows:</p><p class="snippet">       1    2</p><p class="snippet">  e   80 4128</p><p class="snippet">  p 3052  864</p></li>
			</ol>
			<p>As you can see, most of the edible mushrooms are in cluster 2 and most of the poisonous mushrooms are in cluster 1. So, using k-modes clustering has done a reasonable job of identifying whether each mushroom is edible or poisonous.</p>
			<h3 id="_idParaDest-180"><a id="_idTextAnchor298"/>Activity 6: Implementing DBSCAN and Visualizing the Results</h3>
			<p>Solution:</p>
			<ol>
				<li value="1">Import the <strong class="inline">dbscan</strong> and <strong class="inline">factoextra</strong> library:<p class="snippet">library(dbscan)</p><p class="snippet">library(factoextra)</p></li>
				<li>Import the <strong class="inline">multishapes</strong> dataset:<p class="snippet">data(multishapes)</p></li>
				<li>Put the columns of the <strong class="inline">multishapes</strong> dataset in the <strong class="inline">ms</strong> variable:<p class="snippet">ms&lt;-multishapes[,1:2]</p></li>
				<li>Plot the dataset as follows:<p class="snippet">plot(ms)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer206"><img alt="Figure 2.30: Plot of the multishapes dataset" src="image/C12628_02_30.jpg"/></div><h6>Figure 2.30: Plot of the multishapes dataset</h6></li>
				<li>Perform k-means clustering on the dataset and plot the results:<p class="snippet">km.res&lt;-kmeans(ms,4)</p><p class="snippet">fviz_cluster(km.res, ms,ellipse = FALSE)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer207"><img alt="" src="image/C12628_02_31.jpg"/></div><h6>Figure 2.31: Plot of k-means on the multishapes dataset</h6></li>
				<li>Perform DBSCAN on the <strong class="inline">ms</strong> variable and plot the results:<p class="snippet">db.res&lt;-dbscan(ms,eps = .15)</p><p class="snippet">fviz_cluster(db.res, ms,ellipse = FALSE,geom = 'point')</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer208">
					<img alt="Figure 2.32: Plot of DBCAN on the multishapes dataset" src="image/C12628_02_32.jpg"/>
				</div>
			</div>
			<h6>Figure 2.32: Plot of DBCAN on the multishapes dataset</h6>
			<p>Here, you can see all the points in black are anomalies and are not present in any cluster, and the clusters formed in DBSCAN are not possible with any other type of clustering method. These clusters have taken all types of shapes and sizes, whereas in k-means, all clusters are of a spherical shape.</p>
			<h3 id="_idParaDest-181"><a id="_idTextAnchor299"/>Activity 7: Performing a Hierarchical Cluster Analysis on the Seeds Dataset</h3>
			<p>Solution:</p>
			<ol>
				<li value="1">Read the downloaded file into the <strong class="inline">sd</strong> variable:<p class="snippet">sd&lt;-read.delim('seeds_dataset.txt')</p><h4>Note</h4><p class="callout">Make changes to the path as per the location of the file on your system.</p></li>
				<li>First, put all the columns of the dataset other than final labels into the <strong class="inline">sd_c</strong> variable:<p class="snippet">sd_c&lt;-sd[,1:7]</p></li>
				<li>Import the <strong class="inline">cluster</strong> library:<p class="snippet">library(cluster)</p></li>
				<li>Calculate the hierarchical clusters and plot the dendrogram:<p class="snippet">h.res&lt;-hclust(dist(sd_c),"ave")</p><p class="snippet">plot(h.res)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer209"><img alt="Figure 2.33: Cluster dendrogram" src="image/C12628_02_33.jpg"/></div><h6>Figure 2.33: Cluster dendrogram</h6></li>
				<li>Cut the tree at <strong class="inline">k=3</strong> and plot a table to see how the results of the clustering have performed at classifying the three types of seeds:<p class="snippet">memb &lt;- cutree(h.res, k = 3)</p><p class="snippet">results&lt;-table(sd$X1,memb)</p><p class="snippet">results</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer210"><img alt="Figure 2.34: Table classifying the three types of seeds" src="image/C12628_02_34.jpg"/></div><h6>Figure 2.34: Table classifying the three types of seeds</h6></li>
				<li>Perform divisive clustering on the <strong class="inline">sd_c</strong> dataset and plot the dendrogram:<p class="snippet">d.res&lt;-diana(sd_c,metric ="euclidean",)</p><p class="snippet">plot(d.res)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer211"><img alt="Figure 2.35: Dendrogram of divisive clustering" src="image/C12628_02_35.jpg"/></div><h6>Figure 2.35: Dendrogram of divisive clustering</h6></li>
				<li>Cut the tree at <strong class="inline">k=3</strong> and plot a table to see how the results of the clustering have performed at classifying the three types of seeds:<p class="snippet">memb &lt;- cutree(h.res, k = 3)</p><p class="snippet">results&lt;-table(sd$X1,memb)</p><p class="snippet">results</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer212">
					<img alt="Figure 2.36: Table classifying the three types of seeds" src="image/C12628_02_36.jpg"/>
				</div>
			</div>
			<h6>Figure 2.36: Table classifying the three types of seeds</h6>
			<p>You can see that both types of clustering methods have produced identical results. These results also demonstrate that divisive clustering is the reverse of hierarchical clustering.</p>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor300"/>Chapter 3: Probability Distributions</h2>
			<h3 id="_idParaDest-183"><a id="_idTextAnchor301"/>Activity 8: Finding the Standard Distribution Closest to the Distribution of Variables of the Iris Dataset</h3>
			<p>Solution:</p>
			<ol>
				<li value="1">Load the Iris dataset into the <strong class="inline">df</strong> variable:<p class="snippet">df&lt;-iris</p></li>
				<li>Select rows corresponding to the setosa species only:<p class="snippet">df=df[df$Species=='setosa',]</p></li>
				<li>Import the <strong class="inline">kdensity</strong> library:<p class="snippet">library(kdensity)</p></li>
				<li>Calculate and plot the KDE from the <strong class="inline">kdensity</strong> function for sepal length:<p class="snippet">dist &lt;- kdensity(df$Sepal.Length)</p><p class="snippet">plot(dist)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer213"><img alt="Figure 3.36 Plot of the KDE for sepal length" src="image/C12628_03_36.jpg"/></div><h6>Figure 3.36 Plot of the KDE for sepal length</h6><p>This distribution is closest to the normal distribution, which we studied in the previous section. Here, the mean and median are both around 5.</p></li>
				<li>Calculate and plot the KDE from the <strong class="inline">kdensity</strong> function for sepal width:<p class="snippet">dist &lt;- kdensity(df$Sepal.Width)</p><p class="snippet">plot(dist)</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer214">
					<img alt="Figure 3.37 Plot of the KDE for sepal width" src="image/C12628_03_37.jpg"/>
				</div>
			</div>
			<h6>Figure 3.37 Plot of th<a id="_idTextAnchor302"/>e KDE for sepal width</h6>
			<p>This distribution is also closest to normal distribution. We can formalize this similarity with a Kolmogorov-Smirnov test.</p>
			<h3 id="_idParaDest-184"><a id="_idTextAnchor303"/>Activity 9: Calculating the CDF and Performing the Kolmogorov-Simonov Test with the Normal Distribution</h3>
			<p>Solution:</p>
			<ol>
				<li value="1">Load the Iris dataset into the <strong class="inline">df</strong> variable:<p class="snippet">df&lt;-iris</p></li>
				<li>Keep rows with the setosa species only:<p class="snippet">df=df[df$Species=='setosa',]</p></li>
				<li>Calculate the mean and standard deviation of the sepal length column of <strong class="inline">df</strong>:<p class="snippet">sdev&lt;-sd(df$Sepal.Length)</p><p class="snippet">mn&lt;-mean(df$Sepal.Length)</p></li>
				<li>Generate a new distribution with the standard deviation and mean of the sepal length column:<p class="snippet">xnorm&lt;-rnorm(100,mean=mn,sd=sdev)</p></li>
				<li>Plot the CDF of both <strong class="inline">xnorm</strong> and the sepal length column:<p class="snippet">plot(ecdf(xnorm),col='blue')</p><p class="snippet">plot(ecdf(df$Sepal.Length),add=TRUE,pch = 4,col='red')</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer215"><img alt="Figure 3.38: The CDF of xnorm and sepal length" src="image/C12628_03_38.jpg"/></div><h6>Figure 3.38: The CDF of xnorm and sepal length</h6><p>The samples look very close to each other in the distribution. Let's see, in the next test, whether the sepal length sample belongs to the normal distribution or not.</p></li>
				<li>Perform the Kolmogorov-Smirnov test on the two samples, as follows:<p class="snippet">ks.test(xnorm,df$Sepal.Length)</p><p>The output is as follows:</p><p class="snippet">    Two-sample Kolmogorov-Smirnov test</p><p class="snippet">data: xnorm and df$Sepal.Length</p><p class="snippet">D = 0.14, p-value = 0.5307</p><p class="snippet">alternative hypothesis: two-sided</p><p>Here, <strong class="inline">p-value</strong> is very high and the <strong class="inline">D</strong> value is low, so we can assume that the distribution of sepal length is closely approximated by the normal distribution.</p></li>
				<li>Repeat the same steps for the sepal width column of <strong class="inline">df</strong>:<p class="snippet">sdev&lt;-sd(df$Sepal.Width)</p><p class="snippet">mn&lt;-mean(df$Sepal.Width)</p><p class="snippet">xnorm&lt;-rnorm(100,mean=mn,sd=sdev)</p><p class="snippet">plot(ecdf(xnorm),col='blue')</p><p class="snippet">plot(ecdf(df$Sepal.Width),add=TRUE,pch = 4,col='red')</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer216"><img alt="Figure 3.39: CDF of xnorm and sepal width" src="image/C12628_03_39.jpg"/></div><h6>Figure 3.39: CDF of xnorm and sepal width</h6></li>
				<li>Perform the Kolmogorov-Smirnov test as follows:<p class="snippet">ks.test(xnorm,df$Sepal.Length)</p><p>The output is as follows:</p><p class="snippet">    Two-sample Kolmogorov-Smirnov test</p><p class="snippet">data: xnorm and df$Sepal.Width</p><p class="snippet">D = 0.12, p-value = 0.7232</p><p class="snippet">alternative hypothesis: two-sided</p></li>
			</ol>
			<p>Here, also, the sample distribution of sepal width is closely approximated by the normal distribution.</p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor304"/>Chapter 4: Dimension Reduction</h2>
			<h3 id="_idParaDest-186"><a id="_idTextAnchor305"/>Activity 10: Performing PCA and Market Basket Analysis on a New Dataset</h3>
			<p>Solution:</p>
			<ol>
				<li value="1">Before starting our main analysis, we will remove one variable that will not be relevant to us:<p class="snippet">Boston&lt;-Boston[,-12]</p></li>
				<li>We will create dummy variables. We will end up with one original dataset, and one dummy variable dataset. We do that as follows:<p class="snippet">Boston_original&lt;-Boston</p><p>Next, we will create dummy variables for each of the measurements in the original dataset. You can find out the meaning of each of the variables in the dataset in the documentation of the MASS package, available at <a href="https://cran.r-project.org/web/packages/MASS/MASS.pdf">https://cran.r-project.org/web/packages/MASS/MASS.pdf</a>. </p></li>
				<li>Create dummy variables for whether a town has high or low crime per capita:<p class="snippet">Boston$highcrim&lt;-1*(Boston$indus&gt;median(Boston$crim))</p><p class="snippet">Boston$lowcrim&lt;-1*(Boston$indus&lt;=median(Boston$crim))</p><p>Create dummy variables for whether a town has a high or low proportion of land zoned for lots over 25,000 feet: </p><p class="snippet">Boston$highzn&lt;-1*(Boston$zn&gt;median(Boston$zn))</p><p class="snippet">Boston$lowzn&lt;-1*(Boston$zn&lt;=median(Boston$zn))</p><p>Create dummy variables for whether a town has a high or low proportion of non-retail business acres per town:</p><p class="snippet">Boston$highindus&lt;-1*(Boston$indus&gt;median(Boston$indus))</p><p class="snippet">Boston$lowindus&lt;-1*(Boston$indus&lt;=median(Boston$indus))</p><p>Create dummy variables for whether a town borders the Charles River:</p><p class="snippet">Boston$highchas&lt;-(Boston$chas)</p><p class="snippet">Boston$lowchas&lt;-(1-Boston$chas)</p><p>Create dummy variables for whether a town has a high or low nitrogen oxide concentration:</p><p class="snippet">Boston$highnox&lt;-1*(Boston$nox&gt;median(Boston$nox))</p><p class="snippet">Boston$lownox&lt;-1*(Boston$nox&lt;=median(Boston$nox))</p><p>Create dummy variables for whether a town has a high or low average number of rooms per dwelling:</p><p class="snippet">Boston$highrm&lt;-1*(Boston$rm&gt;median(Boston$rm))</p><p class="snippet">Boston$lowrm&lt;-1*(Boston$rm&lt;=median(Boston$rm))</p><p>Create dummy variables for whether a town has a high or low proportion of owner-occupied units built prior to 1940:</p><p class="snippet">Boston$highage&lt;-1*(Boston$age&gt;median(Boston$age))</p><p class="snippet">Boston$lowage&lt;-1*(Boston$age&lt;=median(Boston$age))</p><p>Create dummy variables for whether a town has a high or low average distance to five of Boston's employment centers:</p><p class="snippet">Boston$highdis&lt;-1*(Boston$dis&gt;median(Boston$dis))</p><p class="snippet">Boston$lowdis&lt;-1*(Boston$dis&lt;=median(Boston$dis))</p><p>Create dummy variables for whether a town has a high or low index of accessibility to radial highways:</p><p class="snippet">Boston$highrad&lt;-1*(Boston$rad&gt;median(Boston$rad))</p><p class="snippet">Boston$lowrad&lt;-1*(Boston$rad&lt;=median(Boston$rad))</p><p>Create dummy variables for whether a town has a high or low full-value property tax rate:</p><p class="snippet">Boston$hightax&lt;-1*(Boston$tax&gt;median(Boston$tax))</p><p class="snippet">Boston$lowtax&lt;-1*(Boston$tax&lt;=median(Boston$tax))</p><p>Create dummy variables for whether a town has a high or low pupil-teacher ratio:</p><p class="snippet">Boston$highptratio&lt;-1*(Boston$ptratio&gt;median(Boston$ptratio))</p><p class="snippet">Boston$lowptratio&lt;-1*(Boston$ptratio&lt;=median(Boston$ptratio))</p><p>Create dummy variables for whether a town has a high or low proportion of lower-status population:</p><p class="snippet">Boston$highlstat&lt;-1*(Boston$lstat&gt;median(Boston$lstat))</p><p class="snippet">Boston$lowlstat&lt;-1*(Boston$lstat&lt;=median(Boston$lstat))</p><p>Create dummy variables for whether a town has a high or low median home value:</p><p class="snippet">Boston$highmedv&lt;-1*(Boston$medv&gt;median(Boston$medv))</p><p class="snippet">Boston$lowmedv&lt;-1*(Boston$medv&lt;=median(Boston$medv))</p></li>
				<li>Create a dataset that consists entirely of the dummy variables we have just created:<p class="snippet">Bostondummy&lt;-Boston[,14:ncol(Boston)]</p></li>
				<li>Finally, we will restore our <strong class="inline">Boston_2</strong> dataset to its original form before all of the dummy variables were added:<p class="snippet">Boston&lt;-Boston_original</p></li>
				<li>Calculate the eigenvalues and eigenvectors of the covariance matrix of the dataset, as follows:<p class="snippet">Boston_cov&lt;-cov(Boston)</p><p class="snippet">Boston_eigen&lt;-eigen(Boston_cov)</p><p class="snippet">print(Boston_eigen$vectors)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer217"><img alt="Figure 4.17: Eigenvectors of the covariance matrix" src="image/C12628_04_17.jpg"/></div><h6>Figure 4.17: Eigenvectors of the covariance matrix</h6></li>
				<li>Print eigen values as follows:<p class="snippet">print(Boston_eigen$values)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer218"><img alt="Figure 4.18: Eigenvalues of the covariance matrix" src="image/C12628_04_18.jpg"/></div><h6>Figure 4.18: Eigenvalues of the covariance matrix</h6></li>
				<li>For the third part, we create a simple scree plot based on the eigenvalues:<p class="snippet">plot(Boston_eigen$values,type='o')</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer219"><img alt="Figure 4.19: Plot of the eigenvalues" src="image/C12628_04_19.jpg"/></div><h6>Figure 4.19: Plot of the eigenvalues</h6></li>
				<li>Next, we choose the number of eigenvectors we will use (I chose 10), and we transform the dataset to be 10-dimensional, as follows:<p class="snippet">neigen&lt;-10</p><p class="snippet">transformed&lt;-t(t(as.matrix(Boston_eigen$vectors[,1:neigen])) %*% t(as.matrix(Boston)))</p></li>
				<li>Then, we restore the dataset as much as possible:<p class="snippet">restored&lt;- t(as.matrix(Boston_eigen$vectors[,1:neigen]) %*% t(as.matrix(transformed)))</p></li>
				<li>Finally, we can check how close our restoration is to the original dataset, as follows:<p class="snippet">print(head(restored-Boston))</p></li>
				<li>Here, we need to specify a <strong class="inline">support</strong> threshold (for example, 20%), and complete the first pass through the data:<p class="snippet">support_thresh&lt;-0.2</p><p class="snippet">firstpass&lt;-unname(which(colMeans(Bostondummy,na.rm=TRUE)&gt;support_thresh))</p></li>
				<li>Here, we complete the second pass through the data:<p class="snippet">secondcand&lt;-t(combn(firstpass,2))</p><p class="snippet">secondpass&lt;-NULL</p><p class="snippet">k&lt;-1</p><p class="snippet">while(k&lt;=nrow(secondcand)){</p><p class="snippet">support&lt;-mean(Bostondummy[,secondcand[k,1]]*Bostondummy[,secondcand[k,2]],na.rm=TRUE)</p><p class="snippet">if(support&gt;support_thresh){</p><p class="snippet">secondpass&lt;-rbind(secondpass,secondcand[k,])</p><p class="snippet">}</p><p class="snippet">k&lt;-k+1</p><p class="snippet">}</p></li>
				<li>Here, we complete the third pass, and then do filtering based on the <strong class="inline">confidence</strong> and <strong class="inline">lift</strong> thresholds:<p class="snippet">thirdpass&lt;-NULL</p><p class="snippet">k&lt;-1</p><p class="snippet">while(k&lt;=nrow(secondpass)){</p><p class="snippet">j&lt;-1</p><p class="snippet">while(j&lt;=length(firstpass)){</p><p class="snippet">n&lt;-1</p><p class="snippet">product&lt;-1</p><p class="snippet">while(n&lt;=ncol(secondpass)){</p><p class="snippet">product&lt;-product*Bostondummy[,secondpass[k,n]]</p><p class="snippet">n&lt;-n+1</p><p class="snippet">}</p><p class="snippet">if(!(firstpass[j] %in% secondpass[k,])){</p><p class="snippet">product&lt;-product*Bostondummy[,firstpass[j]]</p><p class="snippet">support&lt;-mean(product,na.rm=TRUE)</p><p class="snippet">if(support&gt;support_thresh){</p><p class="snippet">thirdpass&lt;-rbind(thirdpass,c(secondpass[k,],firstpass[j]))</p><p class="snippet">}</p><p class="snippet">}</p><p class="snippet">j&lt;-j+1</p><p class="snippet">}</p><p class="snippet">k&lt;-k+1</p><p class="snippet">}</p><p class="snippet">thirdpass_conf&lt;-NULL</p><p class="snippet">k&lt;-1</p><p class="snippet">while(k&lt;=nrow(thirdpass)){</p><p class="snippet">support&lt;-mean(Bostondummy[,thirdpass[k,1]]*Bostondummy[,thirdpass[k,2]]*Bostondummy[,thirdpass[k,3]],na.rm=TRUE)</p><p class="snippet">confidence&lt;-mean(Bostondummy[,thirdpass[k,1]]*Bostondummy[,thirdpass[k,2]]*Bostondummy[,thirdpass[k,3]],na.rm=TRUE)/mean(Bostondummy[,thirdpass[k,1]]*Bostondummy[,thirdpass[k,2]],na.rm=TRUE)</p><p class="snippet">lift&lt;-confidence/mean(Bostondummy[,thirdpass[k,3]],na.rm=TRUE)</p><p class="snippet">thirdpass_conf&lt;-rbind(thirdpass_conf,unname(c(thirdpass[k,],support,confidence,lift)))</p><p class="snippet">k&lt;-k+1</p><p class="snippet">}</p></li>
				<li>Our final output is the list of three-item baskets that have passed the <strong class="inline">support</strong>, <strong class="inline">confidence</strong>, and <strong class="inline">lift</strong> thresholds:<p class="snippet">print(head(thirdpass_conf))</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer220">
					<img alt="Figure 4.20: Output of the three-item basket" src="image/C12628_04_20.jpg"/>
				</div>
			</div>
			<h6>Figure 4.20: Output of the three-item basket</h6>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor306"/>Chapter 5: Data Comparison Methods</h2>
			<h3 id="_idParaDest-188"><a id="_idTextAnchor307"/>Activity 11: Create an Image Signature for a Photograph of a Person</h3>
			<p>Solution:</p>
			<ol>
				<li value="1">Download the Borges photo to your computer and save it as <strong class="inline">borges.jpg</strong>. Make sure that it is saved in R's working directory. If it is not in R's working directory, then change R's working directory using the <strong class="inline">setwd()</strong> function. Then, you can load this image into a variable called <strong class="inline">im</strong> (short for image), as follows:<p class="snippet">install.packages('imager')</p><p class="snippet">library('imager')</p><p class="snippet">filepath&lt;-'borges.jpg'</p><p class="snippet">im &lt;- imager::load.image(file =filepath) </p><p>The rest of the code we will explore will use this image, called <strong class="inline">im</strong>. Here, we have loaded a picture of the Alamo into <strong class="inline">im</strong>. However, you can run the rest of the code on any image, simply by saving the image to your working directory and specifying its path in the f<strong class="inline">ilepath</strong> variable.</p></li>
				<li>The signature we are developing is meant to be used for grayscale images. So, we will convert this image to grayscale, using functions in the <strong class="inline">imager</strong> package:<p class="snippet">im&lt;-imager::rm.alpha(im)</p><p class="snippet">im&lt;-imager::grayscale(im)</p><p class="snippet">im&lt;-imager::imsplit(im,axis = "x", nb = 10)   </p><p>The second line of this code is the conversion to grayscale. The last line performs a split of the image into 10 equal sections.</p></li>
				<li>The following code creates an empty matrix that we will fill with information about each section of our 10x10 grid:<p class="snippet">matrix &lt;- matrix(nrow = 10, ncol = 10)</p><p>Next, we will run the following loop. The first line of this loop uses the <strong class="inline">imsplit</strong> command. This command was also used previously to split the x axis into 10 equal parts. This time, for each of the 10 x-axis splits, we will do a split along the y-axis, also splitting it into 10 equal parts:</p><p class="snippet">for (i in 1:10) {</p><p class="snippet">  is &lt;- imager::imsplit(im = im[[i]], axis = "y", nb = 10)</p><p class="snippet">  for (j in 1:10) {</p><p class="snippet">    matrix[j,i] &lt;- mean(is[[j]])</p><p class="snippet">  }</p><p class="snippet">}</p><p>The output so far is the <strong class="inline">matrix</strong> variable. We will use this in <em class="italics">step 4</em>.</p></li>
				<li>Get the signature of the Borges photograph by running the following code:<p class="snippet">borges_signature&lt;-get_signature(matrix)</p><p class="snippet">borges_signature</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer221"><img alt="Figure 5.12: Matrix of borges_signature" src="image/C12628_05_13.jpg"/></div><h6>Figure 5.12: Matrix of borges_signature</h6></li>
				<li>Next, we will start calculating a signature using a 9x9 matrix, instead of a 10x10 matrix. We start with the same process we used before. The following lines of code load our Borges image like we did previously. The final line of this code splits the image into equal parts, but instead of 10 equal parts, we set <strong class="inline">nb=9</strong> so that we split the image into 9 equal parts:<p class="snippet">filepath&lt;-'borges.jpg'</p><p class="snippet">im &lt;- imager::load.image(file =filepath) </p><p class="snippet">im&lt;-imager::rm.alpha(im)</p><p class="snippet">im&lt;-imager::grayscale(im)</p><p class="snippet">im&lt;-imager::imsplit(im,axis = "x", nb = 9)</p></li>
				<li>The following code creates an empty matrix that we will fill with information about each section of our 9x9 grid:<p class="snippet">matrix &lt;- matrix(nrow = 9, ncol = 9)</p><p>Note that we use <strong class="inline">nrow=9</strong> and <strong class="inline">ncol=9</strong> so that we have a 9x9 matrix to fill with our brightness measurements. </p></li>
				<li>Next, we will run the following loop. The first line of this loop uses the <strong class="inline">imsplit</strong> command. This command was also used earlier to split the x axis into 9 equal parts. This time, for each of the 9 x axis splits, we will do a split along the y axis, also splitting it into 9 equal parts:<p class="snippet">for (i in 1:9) {</p><p class="snippet">  is &lt;- imager::imsplit(im = im[[i]], axis = "y", nb = 9)</p><p class="snippet">  for (j in 1:9) {</p><p class="snippet">    matrix[j,i] &lt;- mean(is[[j]])</p><p class="snippet">  }</p><p class="snippet">}</p><p>The output so far is the <strong class="inline">matrix</strong> variable. We will repeat <em class="italics">Step 4</em>.</p></li>
				<li>Get a 9x9 signature of the Borges photograph by running the following code:<p class="snippet">borges_signature_ninebynine&lt;-get_signature(matrix)</p><p class="snippet">borges_signature_ninebynine</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer222">
					<img alt="Figure 5.13: Matrix of borges_signature_ninebynine" src="image/C12628_05_14.jpg"/>
				</div>
			</div>
			<h6>Figure 5.13: Matrix of borges_signature_ninebynine</h6>
			<h3 id="_idParaDest-189"><a id="_idTextAnchor308"/>Activity 12: Create an Image Signature for the Watermarked Image</h3>
			<p>Solution:</p>
			<ol>
				<li value="1">Download the watermarked photo to your computer and save it as <strong class="inline">alamo_marked.jpg</strong>. Make sure that it is saved in R's working directory. If it is not in R's working directory, then change R's working directory using the <strong class="inline">setwd()</strong> function. Then, you can load this image into a variable called <strong class="inline">im</strong> (short for image), as follows:<p class="snippet">install.packages('imager')</p><p class="snippet">library('imager')</p><p class="snippet">filepath&lt;-'alamo_marked.jpg'</p><p class="snippet">im &lt;- imager::load.image(file =filepath) </p><p>The rest of the code we will explore will use this image called <strong class="inline">im</strong>. Here, we have loaded a watermarked picture of the Alamo into <strong class="inline">im</strong>. However, you can run the rest of the code on any image, simply by saving the image to your working directory, and specifying its path in the <strong class="inline">filepath</strong> variable.</p></li>
				<li>The signature we are developing is meant to be used for grayscale images. So, we will convert this image to grayscale by using functions in the <strong class="inline">imager</strong> package:<p class="snippet">im&lt;-imager::rm.alpha(im)</p><p class="snippet">im&lt;-imager::grayscale(im)</p><p class="snippet">im&lt;-imager::imsplit(im,axis = "x", nb = 10)   </p><p>The second line of this code is the conversion to grayscale. The last line performs a split of the image into 10 equal sections.</p></li>
				<li>The following code creates an empty matrix that we will fill with information about each section of our 10x10 grid:<p class="snippet">matrix &lt;- matrix(nrow = 10, ncol = 10)</p><p>Next, we will run the following loop. The first line of this loop uses the <strong class="inline">imsplit</strong> command. This command was also used earlier to split the x axis into 10 equal parts. This time, for each of the 10 x-axis splits, we will do a split along the y axis, also splitting it into 10 equal parts:</p><p class="snippet">for (i in 1:10) {</p><p class="snippet">  is &lt;- imager::imsplit(im = im[[i]], axis = "y", nb = 10)</p><p class="snippet">  for (j in 1:10) {</p><p class="snippet">    matrix[j,i] &lt;- mean(is[[j]])</p><p class="snippet">  }</p><p class="snippet">}</p><p>The output so far is the <strong class="inline">matrix</strong> variable. We will use this in <em class="italics">Step 4</em>.</p></li>
				<li>We can get the signature of the watermarked photograph by running the following code:<p class="snippet">watermarked_signature&lt;-get_signature(matrix)</p><p class="snippet">watermarked_signature</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer223"><img alt="Figure 5.14: Signature of watermarked image" src="image/C12628_05_06.jpg"/></div><h6>Figure 5.14: Signature of watermarked image</h6><p>The final output of this activity is the <strong class="inline">watermarked_signature</strong> variable, which is the analytic signature of the watermarked Alamo photo. If you have completed all of the exercises and activities so far, then you should have three analytic signatures: one called <strong class="inline">building_signature</strong>, one called <strong class="inline">borges_signature</strong>, and one called <strong class="inline">watermarked_signature</strong>.</p></li>
				<li>After completing this activity, we have stored this signature in a variable called <strong class="inline">watermarked_signature</strong>. Now, we can compare it to our original Alamo signature, as follows:<p class="snippet">comparison&lt;-mean(abs(watermarked_signature-building_signature))</p><p class="snippet">comparison</p><p>In this case, the result we get is 0.015, indicating a very close match between the original image signature and this new image's signature.</p></li>
			</ol>
			<p>What we have seen is that our analytic signature method returns similar signatures for similar images, and different signatures for different images. This is exactly what we want a signature to do, and so we can judge this method a success.</p>
			<h3 id="_idParaDest-190"><a id="_idTextAnchor309"/>Activity 13: Performing Factor Analysis</h3>
			<p>Solution:</p>
			<ol>
				<li value="1">The data file can be downloaded from <a href="https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson05/Data/factor.csv">https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson05/Data/factor.csv</a>. Save it to your computer and make sure that it is in R's working directory. If you save it as <strong class="inline">factor.csv</strong>, then you can load it in R by executing the following command:<p class="snippet">factor&lt;-read.csv('factor.csv')</p></li>
				<li>Load the <strong class="inline">psych</strong> package as follows:<p class="snippet">library(psych)</p></li>
				<li>We will be performing factor analysis on the user ratings, which are recorded in columns 2 through 11 of the data. We can select these columns as follows:<p class="snippet">ratings&lt;-factor[,2:11]</p></li>
				<li>Create a correlation matrix of the ratings data as follows:<p class="snippet">ratings_cor&lt;-cor(ratings)</p></li>
				<li>Determine the number of factors we should use by creating a scree plot. A scree plot is produced as one of the outputs of the following command:<p class="snippet">parallel &lt;- fa.parallel(ratings_cor, fm = 'minres', fa = 'fa')</p></li>
				<li>The scree plot looks like the following:<div class="IMG---Figure" id="_idContainer224"><img alt="Figure 5.15: Parallel Analysis Scree Plots" src="image/C12628_05_16.jpg"/></div><h6>Figure 5.15: Parallel Analysis Scree Plots</h6><p>The scree plot shows one factor whose eigenvalue is much higher than the others. While we are free to choose any number of factors in our analysis, the single factor that is much larger than the others provides good reason to use one factor in our analysis.</p></li>
				<li>We can perform factor analysis as follows, specifying the number of factors in the <strong class="inline">nfactors</strong> parameter:<p class="snippet">factor_analysis&lt;-fa(ratings_cor, nfactors=1)</p><p>This stores the results of our factor analysis in a variable called <strong class="inline">factor_analysis</strong>:</p></li>
				<li>We can examine the results of our factor analysis as follows:<p class="snippet">print(factor_analysis)</p><p>The output looks as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer225">
					<img alt="Figure 5.16: Result of factor analysis" src="image/C12628_05_17.jpg"/>
				</div>
			</div>
			<h6>Figure 5.16: Result of factor analysis</h6>
			<p>The numbers under <strong class="inline">MR1</strong> show us the factor loadings for each category for our single factor. Since we have only one explanatory factor, all of the categories that have positive loadings on this factor are positively correlated with each other. We could interpret this factor as general positivity, since it would indicate that if people rate one category highly, they will also rate other categories highly, and if they rate one category poorly, they are likely to rate other categories poorly.</p>
			<p>The only major exception to this rule is <strong class="inline">Category 10</strong>, which records users' average ratings of religious institutions. In this case, the factor loading is large and negative. This indicates that people who rate most other categories highly tend to rate religious institutions poorly, and vice versa. So, maybe we can interpret the positivity factor we have found as positivity about recreational activities, instead since religious institutions are arguably not places for recreation but rather for worship. It seems that, in this dataset, those who are positive about recreational activities are negative about worship, and vice versa. For the factor loadings that are close to 0, we can also conclude that the rule about positivity about recreation holds less strongly. You can see that factor analysis has enabled us to find relationships between the observations in our data that we had not previously suspected.</p>
			<h2 id="_idParaDest-191"><a id="_idTextAnchor310"/>Chapter 6: Anomaly Detection</h2>
			<h3 id="_idParaDest-192"><a id="_idTextAnchor311"/>Activity 14: Finding Univariate Anomalies Using a Parametric Method and a Non-parametric Method</h3>
			<p>Solution:</p>
			<ol>
				<li value="1">Load the data as follows:<p class="snippet">data(islands)</p></li>
				<li>Draw a boxplot as follows:<p class="snippet">boxplot(islands)</p><div class="IMG---Figure" id="_idContainer226"><img alt="Figure 6.21: Boxplot of the islands dataset" src="image/C12628_06_21.jpg"/></div><h6>Figure 6.21: Boxplot of the islands dataset</h6><p>You should notice that the data is extremely fat-tailed, meaning that the median and interquartile range take up a relatively tiny portion of the plot compared to the many observations that R has classified as outliers.</p></li>
				<li>Create a new log-transformed dataset as follows:<p class="snippet">log_islands&lt;-log(islands)</p></li>
				<li>Create a boxplot of the log-transformed data as follows:<p class="snippet">boxplot(log_islands)</p><div class="IMG---Figure" id="_idContainer227"><img alt="Figure 6.22: Boxplot of log-transformed dataset" src="image/C12628_06_22.jpg"/></div><h6>Figure 6.22: Boxplot of log-transformed dataset</h6><p>You should notice that there are only five outliers after the log transformation.</p></li>
				<li>Calculate the interquartile range:<p class="snippet">interquartile_range&lt;-quantile(islands,.75)-quantile(islands,.25)</p></li>
				<li>Add 1.5 times the interquartile range to the third quartile to get the upper limit of the non-outlier data:<p class="snippet">upper_limit&lt;-quantile(islands,.75)+1.5*interquartile_range</p></li>
				<li>Classify outliers as any observations above this upper limit:<p class="snippet">outliers&lt;-islands[which(islands&gt;upper_limit)]</p></li>
				<li>Calculate the interquartile range for the log-transformed data:<p class="snippet">interquartile_range_log&lt;-quantile(log_islands,.75)-quantile(log_islands,.25)</p></li>
				<li>Add 1.5 times the interquartile range to the third quartile to get the upper limit of the non-outlier data:<p class="snippet">upper_limit_log&lt;-quantile(log_islands,.75)+1.5*interquartile_range_log</p></li>
				<li>Classify outliers as any observations above this upper limit:<p class="snippet">outliers_log&lt;-islands[which(log_islands&gt;upper_limit_log)]</p></li>
				<li>Print the non-transformed outliers as follows:<p class="snippet">print(outliers)</p><p>For the non-transformed outliers, we obtain the following:</p><div class="IMG---Figure" id="_idContainer228"><img alt="Figure 6.23: Non-transformed outliers" src="image/C12628_06_23.jpg"/></div><h6>Figure 6.23: Non-transformed outliers</h6><p>Print the log-transformed outliers as follows:</p><p class="snippet">print(outliers_log)</p><p>For the log-transformed outliers, we obtain the following:</p><div class="IMG---Figure" id="_idContainer229"><img alt="Figure 6.24: Log-transformed outliers" src="image/C12628_06_24.jpg"/></div><h6>Figure 6.24: Log-transformed outliers</h6></li>
				<li>Calculate the mean and standard deviation of the data:<p class="snippet">island_mean&lt;-mean(islands)</p><p class="snippet">island_sd&lt;-sd(islands)</p></li>
				<li>Select observations that are more than two standard deviations away from the mean:<p class="snippet">outliers&lt;-islands[which(islands&gt;(island_mean+2*island_sd))]</p><p class="snippet">outliers</p><p>We obtain the following outliers:</p><div class="IMG---Figure" id="_idContainer230"><img alt="Figure 6.25: Screenshot of the outliers" src="image/C12628_06_25.jpg"/></div><h6>Figure 6.25: Screenshot of the outliers</h6></li>
				<li>First, we calculate the mean and standard deviation of the log-transformed data:<p class="snippet">island_mean_log&lt;-mean(log_islands)</p><p class="snippet">island_sd_log&lt;-sd(log_islands)</p></li>
				<li>Select observations that are more than two standard deviations away from the mean:<p class="snippet">outliers_log&lt;-log_islands[which(log_islands&gt;(island_mean_log+2*island_sd_log))]</p></li>
				<li>We print the log-transformed outliers as follows:<p class="snippet">print(outliers_log)</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer231">
					<img alt="Figure 6.26: Log-transformed outliers" src="image/C12628_06_26.jpg"/>
				</div>
			</div>
			<h6>Figure 6.26: Log-transformed outliers</h6>
			<h3 id="_idParaDest-193"><a id="_idTextAnchor312"/>Activity 15: Using Mahalanobis Distance to Find Anomalies</h3>
			<p>Solution: </p>
			<ol>
				<li value="1">You can load and plot the data as follows:<p class="snippet">data(cars)</p><p class="snippet">plot(cars)</p><p>The output plot is the following:</p><div class="IMG---Figure" id="_idContainer232"><img alt="Figure 6.27: Plot of the cars dataset" src="image/C12628_06_27.jpg"/></div><h6>Figure 6.27: Plot of the cars dataset</h6></li>
				<li>Calculate the centroid:<p class="snippet">centroid&lt;-c(mean(cars$speed),mean(cars$dist))</p></li>
				<li>Calculate the covariance matrix:<p class="snippet">cov_mat&lt;-cov(cars)</p></li>
				<li>Calculate the inverse of the covariance matrix:<p class="snippet">inv_cov_mat&lt;-solve(cov_mat)</p></li>
				<li>Create a <strong class="inline">NULL</strong> variable, which will hold each of our calculated distances:<p class="snippet">all_distances&lt;-NULL</p></li>
				<li>We can loop through each observation and find the Mahalanobis distance between them and the centroid of the data:<p class="snippet">k&lt;-1</p><p class="snippet">while(k&lt;=nrow(cars)){</p><p class="snippet">the_distance&lt;-cars[k,]-centroid</p><p class="snippet">mahalanobis_dist&lt;-t(matrix(as.numeric(the_distance)))%*% matrix(inv_cov_mat,nrow=2) %*% matrix(as.numeric(the_distance))</p><p class="snippet">all_distances&lt;-c(all_distances,mahalanobis_dist)</p><p class="snippet">k&lt;-k+1</p><p class="snippet">}</p></li>
				<li>Plot all observations that have particularly high Mahalanobis distances to see our outliers:<p class="snippet">plot(cars)</p><p class="snippet">points(cars$speed[which(all_distances&gt;quantile(all_distances,.9))], cars$dist[which(all_distances&gt;quantile(all_distances,.9))],col='red',pch=19)</p><p>We can see the output plot as follows, with the outlier points shown in red:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer233">
					<img alt="Figure 6.28: Plot with outliers marked" src="image/C12628_06_28.jpg"/>
				</div>
			</div>
			<h6>Figure 6.28: Plot with outliers marked</h6>
		</div>
	</body></html>