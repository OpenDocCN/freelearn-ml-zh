<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Beyond Feedforward Networks – CNN and RNN</h1>
                </header>
            
            <article>
                
<p><strong>Artificial Neural Networks</strong> (<strong>ANNs</strong>) are now extremely widespread tools in various technologies. In the simplest application, ANNs provide a feedforward architecture for connections between neurons. The feedforward neural network is the first and simplest type of ANN devised. In the presence of basic hypotheses that interact with some problems, the intrinsic unidirectional structure of feedforward networks is strongly limiting. However, it is possible to start from it and create networks in which the results of computing one unit affect the computational process of another. It is evident that algorithms that manage the dynamics of these networks must meet new convergence criteria.</p>
<p>In this chapter, we'll go over the main ANN architectures, such as convolutional NNs, recurrent NNs, and <span><strong>long short-term memory</strong> (<strong>LSTM</strong>)</span>. We'll explain the concepts behind each type of NN and tell you which problem they should be applied <span>to</span>. Each type of NN is implemented with TensorFlow on a realistic dataset.</p>
<p>The topics covered are:</p>
<ul>
<li>Convolutional networks and their applications</li>
<li>Recurrent networks</li>
<li>LSTM architectures</li>
</ul>
<p>At the end of the chapter, we will understand training, testing, and evaluating a <strong>convolutional neural network</strong><span> (</span><strong>CNN</strong>). We will learn how to train and test the CNN model in Google Cloud Platform. We will cover the concepts as CNN and RNN architecture. We will also be able to train an LSTM model. The reader will learn which type of neural network to apply to different problems and how to define and implement them on G<span>oogle Cloud Platform</span>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Convolutional neural networks</h1>
                </header>
            
            <article>
                
<p>ANN is a family of models <span>inspired </span>from biological neural networks (the human brain) that, starting from the mechanisms regulating natural neural networks, plan to simulate human thinking. They are used to estimate or approximate functions that may depend on a large number of inputs, many of which are often unknown. ANNs are generally presented as interconnected neuron systems among which an exchange of messages takes place. Each connection has a related weight; the value of the weight is adjustable based on experience, and this makes neural networks an instrument adaptable to the various types of input and having the ability to learn.</p>
<p>ANNs define the neuron as a central processing unit, which performs a mathematical operation to generate one output from a set of inputs. The output of a neuron is a function of the weighted sum of the inputs plus the bias. Each neuron performs a very simple operation that involves activation if the total amount of signal received exceeds an activation threshold. In the following figure, a simple ANN architecture <span>is shown</span>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c87c20a5-de1e-4326-a726-6876f608cc91.png" style=""/></div>
<p class="NormalPACKT"><span>Essentially, </span>CNN are ANNs. In fact, just like the latter, CNNs are made up of neurons connected to one another by weighted branches (weight); the training parameters of the nets are once again the weight and the bias.</p>
<p class="NormalPACKT"><span>In CNN, the connection pattern between neurons is inspired by the structure of the visual cortex in the animal world. The individual neurons present in this part of the brain (visual cortex) respond to certain stimuli in a narrow region of the observation, called the <strong>receptive field</strong></span>. <span>The receptive fields of different neurons are partially overlapped in order to cover the entire field of vision. The response of a single neuron to stimuli taking place in its receptive field can be mathematically approximated by a convolution operation.</span></p>
<p class="NormalPACKT"><span>Everything related to the training of a neural network, that is, forward/backward propagation and updating of the weight, also applies in this context; moreover, a whole CNN always uses a single function of differentiable cost. However, CNNs make a specific assumption that their input has a precise data structure, such as an image, and this allows them to take specific properties in their architectureto better process such data.</span></p>
<p class="NormalPACKT"><span>The normal neural networks stratified with an FC architecture—where every neuron of each layer is connected to all the neurons of the previous layer (excluding bias neurons)—in general do not scale well with an increase in the size of input data.</span></p>
<p class="NormalPACKT"><span>Let's take a practical example: suppose we want to analyze an image to detect objects. To start, let's see how the image is processed. As we know, in the coding of an image, it is divided into a grid of small squares, each of which represents a pixel. At this point, to encode the color images, it will be enough to identify for each square a certain number of shades and different color gradations. And then we code each one by means of an appropriate sequence of bits. Here is a simple image encoding:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-819 image-border" src="assets/3d7e6060-4f75-44d2-b22a-3cb150720eeb.png" style=""/></div>
<p class="NormalPACKT"><span>The number of squares in the grid defines the resolution of the image. For example, an image that is 1,600 pixels wide and 800 pixels high (1,600 x 800) contains (multiplied)</span> 1,280,000 <span>pixels, or 1.2 megapixels. To this, we must multiply the three color channels, finally obtaining 1,600 x 800 x 3 = 3,840,000. So, each neuron completely connected in the first hidden layer would have</span> 3,840,000 <span>weights. This is only for a single neuron; considering the whole network, the thing would certainly become unmanageable!</span></p>
<p class="NormalPACKT"><span>CNNs are designed to recognize visual patterns directly in images represented by pixels and require zero or very limited preprocessing. They are able to recognize extremely variable patterns, such as freehand writing and images representing the real world.</span></p>
<p class="NormalPACKT"><span>Typically, a CNN consists of several alternate convolution and subsampling levels (pooling) followed by one or more FC final levels in the case of classification. The following figure shows a classic image-processing pipeline:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/cfbfe186-3a40-4f4a-a38c-fdecbf683450.png" style=""/></div>
<p class="NormalPACKT"><span>To solve problems in the real world, these steps can be combined and stacked as often as necessary. For example, you can have two, three, or even more layers of</span> <strong>Convolution</strong><span>. You can enter all the <strong>Pooling</strong> you want to reduce the size of the data.</span></p>
<p class="NormalPACKT"><span>As already mentioned,</span><span> </span><span>different types of levels</span><span> are typically used</span><span> </span><span>in a CNN</span><span>. In the following sections, the main ones will be covered.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Convolution layer</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT"><span>This is the main type of layer; the use of one or more of these layers in a CNN is essential. The parameters of a convolutional layer, in practice, relate to a set of workable filters. Each filter is spatially small, along the width and height dimensions, but it extends over the entire depth of the input volume to which it is applied.</span></p>
<p class="NormalPACKT"><span>Unlike normal neural networks, convolutional layers have neurons organized in three dimensions: <strong>width</strong>, <strong>height</strong>, and <strong>depth</strong>. They are shown in the following figure:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-826 image-border" src="assets/98a4cb93-dbff-4d8e-867d-ba939b15ed8e.png" style=""/></div>
<p>During forward propagation, each filter is translated<span>—</span>or more precisely, convolved—along the width and height of the input volume, producing a two-dimensional activation map (or feature map) for that filter. As the filter is moved along the input area, a scalar product operation is performed between the values ​​of the filter and those of the input portion to which it is applied.</p>
<p>Intuitively, the network will have as its objective the learning of filters that are activated in the presence of some specific type of feature in a given spatial region of the input. The queuing of all these feature maps (for all filters) along the depth dimension forms the output volume of a convolutional layer. Each element of this volume can be interpreted as the output of a neuron that observes only a small region of the input and which shares its parameters with the other neurons in the same feature map. This is because these values ​​all come from the application of the same filter.</p>
<p>In summary, let's focus our attention on the following points:</p>
<ul>
<li><strong>Local receptive field</strong>: Each neuron of a layer is (completely) connected to a small region of the input (called a <strong>local receptive field</strong>); each connection learns a weight.</li>
<li><strong>Shared weights</strong>: Since the interesting features (edge, blob, and so on) can be found anywhere in the image, the neurons of the same layer share the weights. This means that all the neurons of the same layer will recognize the same feature, placed at different points of the input.</li>
<li><strong>Convolution</strong>: The same weight map is applied to different positions. The convolution output is called a <strong>feature map</strong>.</li>
</ul>
<p>Each filter captures a feature present in the previous layer. So to extract different features, we need to train multiple convolutional filters. Each filter returns a feature map that highlights different characteristics.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Rectified Linear Units</h1>
                </header>
            
            <article>
                
<p><strong>Rectified Linear Units</strong> (<strong>ReLU</strong>) play the role of neuronal activation function in neural networks. A ReLU level is composed of neurons that apply the function f<em>(x) = max (0, x)</em>. These levels increase the non-linearity of the network and at the same time do not modify the receiving fields of convolution levels. The function of the ReLUs is preferred over others, such as the hyperbolic tangent or the sigmoid, since, in comparison to these, it leads to a much faster training process without significantly affecting the generalization accuracy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pooling layers</h1>
                </header>
            
            <article>
                
<p>These layers are periodically inserted into a network to reduce the spatial size (width and height) of current representations, as well as volumes in a specific network stage; this serves to reduce the number of parameters and the computational time of the network. It also monitors overfitting. A pooling layer operates on each depth slice of the input volume independently to resize it spatially.</p>
<p>For example, this technique partitions an input image into a set of squares, and for each of the resulting regions, it returns the maximum value as output.</p>
<p>CNNs also use pooling layers located immediately after the convolutional layers. A pooling layer divides input into regions and selects a single representative value (max-pooling and average pooling). Using a pooling layer:</p>
<ul>
<li>Reduces the calculations of subsequent layers</li>
<li>Increases the robustness of the features with respect to spatial position</li>
</ul>
<p>It is based on the concept that, once a certain feature has been identified, its precise position in the input is not as important as its approximate position in relation to the other features. In the typical CNN architecture, convolution levels and pooling levels are repeatedly alternated.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fully connected layer</h1>
                </header>
            
            <article>
                
<p>This type of layer is exactly the same as any of the layers of a classical ANN with <strong>fully connected</strong> (<strong>FC</strong>) architecture. Simply in an FC layer, each neuron is connected to all the neurons of the previous layer, specifically to their activations.</p>
<p>This type of layer, unlike what has been seen so far in CNNs, does not use the property of local connectivity. An FC layer is connected to the entire input volume, and, therefore, as you can imagine, there will be many connections. The only settable parameter of this type of layer is the number of K neurons that make it up. What basically defines an FC layer is as follows: connecting its K neurons with all the input volume and calculating the activation of each of its K neurons.</p>
<p>In fact, its output will be a single 1 x 1 x K vector, containing the calculated activations. The fact that after using a single FC layer you switch from an input volume (organized in three dimensions) to a single output vector (in a single dimension) suggests that after applying an FC layer, no more  convoluted layers <span>can be used.</span> The main function of FC layers in the context of CNNs is to carry out a sort of grouping of the information obtained up to that moment, expressing it with a single number (the activation of one of its neurons), which will be used in subsequent calculations for the final classification.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Structure of a CNN</h1>
                </header>
            
            <article>
                
<p>After analyzing every component of a CNN<span> </span><span>in detail</span><span>, it is time to see the general structure of a CNN as a whole. For example, starting from the images as input layers, there will be a certain series of convolutional layers interspersed with a ReLU layer and, when necessary, the standardization and pooling layers. Finally, there will be a series of FC layers before the output layer. Here is an example of a CNN architecture:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b4fd6b63-f3f3-4956-8b4b-1b5dbc86088d.png" style=""/></div>
<div>
<p>The basic idea is to start with a large image and continuously reduce the data step by step until you get a single result. The more the convolution passages you have, the more the neural network will be able to understand and process complex functions.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow overview</h1>
                </header>
            
            <article>
                
<p>TensorFlow is an open source numerical computing library provided by Google for machine intelligence. It hides all of the programming required to build deep learning models and gives developers a black box interface to program.</p>
<p>In TensorFlow, nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. TensorFlow was originally developed by the Google brain team within Google's machine intelligence research for machine learning and deep neural networks research, but it is now available in the public domain. TensorFlow exploits GPU processing when configured appropriately.</p>
<p>The generic use cases for TensorFlow are as follows:</p>
<ul>
<li>Image recognition</li>
<li>Computer vision</li>
<li>Voice/sound recognition</li>
<li>Time series analysis</li>
<li>Language detection</li>
<li>Language translation</li>
<li>Text-based processing</li>
<li>Handwriting Recognition</li>
<li>Many others</li>
</ul>
<p>To use TensorFlow, we must first install Python. If you don't have a Python installation on your machine, it's time to get it. Python is a dynamic <strong>object-oriented programming</strong> (<strong>OOP</strong>) language that can be used for many types of software development. It offers strong support for integration with other languages and programs, is provided with a large standard library, and can be learned within a few days. Many Python programmers can confirm a substantial increase in productivity and feel that it encourages the development of higher quality code and maintainability.</p>
<p>Python runs on Windows, Linux/Unix, macOS X, OS/2, Amiga, Palm handhelds, and Nokia phones. It also works on Java and .NET virtual machines. Python is licensed under the OSI-approved open source license; its use is free, including for commercial products.</p>
<p>Python was created in the early 1990s by Guido van Rossum at Stichting Mathematisch Centrum in the Netherlands as a successor of a language called <strong>ABC</strong>. Guido remains Python's principal author, although it includes many contributions from others.</p>
<div>
<div class="packt_tip">If you do not know which version to use, there is an English document that can help you choose. In principle, if you have to start from scratch, we recommend choosing Python 3.6. All information about the available versions and how to install Python is given at <a href="https://www.python.org/" target="_blank">https://www.python.org/</a>.</div>
</div>
<p>After properly installing the Python version of our machine, we have to worry about installing TensorFlow. We can retrieve all library information and available versions of the operating system from the following link: <a href="https://www.tensorflow.org/" target="_blank">https://www.tensorflow.org/</a>.</p>
<p>Also, in the install section, we can find a series of guides that explain how to install a version of TensorFlow that allows us to write applications in Python. Guides are available for the following operating systems:</p>
<ul>
<li>Ubuntu</li>
<li>macOS X</li>
<li>Windows</li>
</ul>
<p>For example, to install TensorFlow on Windows, we must choose one of the following types:</p>
<ul>
<li>TensorFlow with CPU support only</li>
<li>TensorFlow with GPU support</li>
</ul>
<p>To install TensorFlow, start a terminal with privileges as administrator. Then issue the appropriate <kbd>pip3 install</kbd> command in that terminal. To install the CPU-only version, enter the following command:</p>
<pre><strong>C:\&gt; pip3 install --upgrade tensorflow</strong></pre>
<p>A series of code lines will be displayed on the video to keep us informed of the execution of the installation procedure, as shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-818 image-border" src="assets/0bfbadcf-f411-4353-a285-bb857b7d5c74.png" style=""/></div>
<div>
<p>At the end of the process, the following code is displayed:</p>
<pre><strong>Successfully installed absl-py-0.1.10 markdown-2.6.11 numpy-1.14.0 protobuf-3.5.1 setuptools-38.5.1 tensorflow-1.5.0 tensorflow-tensorboard-1.5.1 werkzeug-0.14.1</strong></pre>
<p>To validate the installation, invoke <kbd>python</kbd> from a shell as follows:</p>
<pre><strong>python</strong></pre>
<p>Enter the following short program inside the Python interactive shell:</p>
<pre><strong>&gt;&gt;&gt; import tensorflow as tf</strong><br/><strong>&gt;&gt;&gt; hello = tf.constant('Hello, TensorFlow!')</strong><br/><strong>&gt;&gt;&gt; sess = tf.Session()</strong><br/><strong>&gt;&gt;&gt; print(sess.run(hello))</strong></pre>
<p>If the system outputs the following, then you are ready to begin writing TensorFlow programs:</p>
<pre><strong>Hello, TensorFlow!</strong></pre>
<p>In this case, you will have a confirmation of correct installation of the library on your computer. Now you just need to use it.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Handwriting Recognition using CNN and TensorFlow</h1>
                </header>
            
            <article>
                
<p><strong>Handwriting Recognition</strong> (<strong>HWR</strong>)<span> is a very commonly used procedure in modern technology. An image of written text can be detected offline from a piece of paper by optical scanning (<strong>optical character recognition</strong> or <strong>OCR</strong>) or intelligent word recognition. Alternatively, pen tip movements can be detected online (for example, from a pen computer surface, a task that is generally easier since there are more clues available).</span></p>
<p>Technically, recognition of handwriting is the ability of a computer to receive and interpret a handwritten intelligible input from sources such as paper documents, photos, touchscreens, and other devices. HWR is performed through various techniques that generally require OCR. However, a complete script recognition system also manages formatting, carries out correct character segmentation, and finds the most plausible words.</p>
<p><strong>Modified National Institute of Standards and Technology</strong> (<strong>MNIST</strong>) is a large database of handwritten digits. It has a set of 70,000 examples of data. It is a subset of NIST's larger dataset. The digits are of 28 x 28 pixel resolution and are stored in a matrix of 70,000 rows and 785 columns; 784 columns form each pixel value from the 28 x 28 matrix and one value is the actual digit. The digits have been size-normalized and centered in a fixed-size image.</p>
<div class="packt_tip">
<p>The digit images in the MNIST set were originally selected and experimented with by Chris Burges and Corinna Cortes using bounding box normalization and centering. Yann LeCun's version uses centering by center of mass within in a larger window. The data is available on Yann LeCun's website at<br/>
<a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>.<a href="http://yann.lecun.com/exdb/mnist/"/></p>
</div>
<p>Each image is created as 28 x 28. The following figure shows a sample of images of 0-8 from the MNIST dataset:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-825 image-border" src="assets/4c607089-743f-4f34-930b-5d205e1a866d.png" style=""/></div>
<div>
<p>MNIST has a sample of several handwritten digits. This dataset can be fed for our training to an Python program and our code can recognize any new handwritten digit that is presented as data for prediction. This is a case where the neural network architecture functions as a computer vision system for an AI application. The following table shows the distribution of the MNIST dataset available on LeCun's website:</p>
<table style="width: 404px;height: 771px">
<tbody>
<tr>
<td>
<p><strong>Digit</strong></p>
</td>
<td>
<p><strong>Count</strong></p>
</td>
</tr>
<tr>
<td>
<p>0</p>
</td>
<td>
<p>5923</p>
</td>
</tr>
<tr>
<td>
<p>1</p>
</td>
<td>
<p>6742</p>
</td>
</tr>
<tr>
<td>
<p>2</p>
</td>
<td>
<p>5958</p>
</td>
</tr>
<tr>
<td>
<p>3</p>
</td>
<td>
<p>6131</p>
</td>
</tr>
<tr>
<td>
<p>4</p>
</td>
<td>
<p>5842</p>
</td>
</tr>
<tr>
<td>
<p>5</p>
</td>
<td>
<p>5421</p>
</td>
</tr>
<tr>
<td>
<p>6</p>
</td>
<td>
<p>5918</p>
</td>
</tr>
<tr>
<td>
<p>7</p>
</td>
<td>
<p>6265</p>
</td>
</tr>
<tr>
<td>
<p>8</p>
</td>
<td>
<p>5851</p>
</td>
</tr>
<tr>
<td>
<p>9</p>
</td>
<td>
<p>5949</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>We will use the TensorFlow library to train and test the MNIST dataset. We will split the dataset of 70,000 rows into 60,000 training rows and 10,000 test rows. Next, we'll find the accuracy of the model. The model can then be used to predict any incoming dataset of 28 x 28 pixel handwritten digits containing numbers between zero and nine. For our sample Python code, we use a 100-row training dataset and a 10-row test dataset. In this example, we will learn to use the TensorFlow layers module that provides a high-level API that makes it easy to construct a neural network. It provides methods that facilitate creating dense (FC) layers and convolutional layers, adding activation functions, and applying dropout regularization.</p>
<p>To start we will analyze the code line by line, then we will see how to process it with the tools made available by Google Cloud Platform. Now, let's go through the code to learn how to apply a CNN to solve a HWR problem. Let's start from the beginning of the code:</p>
<pre>from __future__ import absolute_import<br/>from __future__ import division<br/>from __future__ import print_function</pre>
<p>These three lines are added to write a Python 2/3 compatible code base. So let's move on to importing modules:</p>
<pre>import numpy as np<br/>import tensorflow as tf</pre>
<p>In this way, we have imported the <kbd>numpy</kbd> and <kbd>tensorflow</kbd> module. Let's analyze the next line of code:</p>
<pre>tf.logging.set_verbosity(tf.logging.INFO)</pre>
<p>This code sets the threshold for what messages will be logged. After an initial phase, we pass to define the function that will allow us to build a CNN model:</p>
<pre>def cnn_model_fn(features, labels, mode):</pre>
<p>We have thus defined the function. Now let's move on:</p>
<pre>input_layer = tf.reshape(features["x"], [-1, 28, 28, 1])</pre>
<p>In this code line, we have passed the input tensors in the form (<kbd>batch_size</kbd>, <kbd>image_width</kbd>, <kbd>image_height</kbd>, <kbd>channels</kbd>) as expected from the methods in the layers module, for creating convolutional and pooling layers for two-dimensional image data. Let's move on to the first convolutional layer:</p>
<pre>conv1 = tf.layers.conv2d(<br/>      inputs=input_layer,<br/>      filters=32,<br/>      kernel_size=[5, 5],<br/>      padding="same",<br/>      activation=tf.nn.relu)</pre>
<p>This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. The number of filters in the convolution is 32, the height and width of the 2D convolution window are <kbd>[5,5]</kbd>, and the activation function is a ReLU function. To do this, we used the <kbd>conv2d()</kbd> method in the layers module. Next, we connect our first pooling layer to the convolutional layer we just created:</p>
<pre>pool1 = tf.layers.max_pooling2d(inputs=conv1,<br/>                       pool_size=[2, 2], strides=2)</pre>
<p>We used the <kbd>max_pooling2d()</kbd> method in layers to construct a layer that performs max pooling with a 2 x 2 filter and stride of <kbd>2</kbd>. Now we will connect a second convolutional layer to our CNN:</p>
<pre>conv2 = tf.layers.conv2d(<br/>    inputs=pool1,<br/>    filters=64,<br/>    kernel_size=[5, 5],<br/>    padding="same",<br/>    activation=tf.nn.relu)</pre>
<p>Now we will connect a second pooling layer to our CNN:</p>
<pre>pool2 = tf.layers.max_pooling2d(inputs=conv2,<br/>                   pool_size=[2, 2], strides=2)<br/>pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])</pre>
<p>Next, we will add a dense layer:</p>
<pre>dense = tf.layers.dense(inputs=pool2_flat,<br/>                units=1024, activation=tf.nn.relu)</pre>
<p>With this code, we added a dense layer with 1,024 neurons and ReLU activation to our CNN to perform classification on the features extracted by the convolution/pooling layers.</p>
<div class="packt_tip">
<p>Remember, a ReLU level is composed of neurons that apply the function <em>f(x) = max (0, x)</em>. These levels increase the non-linearity of the network, and at the same time, they do not modify the receiving fields of convolution levels.</p>
</div>
<p>To improve the results, we will apply dropout regularization to our dense layer:</p>
<pre>dropout = tf.layers.dropout(inputs=dense,<br/>            rate=0.4, training=mode ==<br/>                      tf.estimator.ModeKeys.TRAIN)</pre>
<p>To do this we used the dropout method in layers. Next, we will add the final layer to our neural network:</p>
<pre>logits = tf.layers.dense(inputs=dropout, units=10)</pre>
<p>This is the <kbd>logits</kbd> layer, which will return the raw values for our predictions. With the previous code, we created a dense layer with <kbd>10</kbd> neurons (one for each target class 0–9), with linear activation. We just have to generate the predictions:</p>
<pre>predictions = {<br/>      "classes": tf.argmax(input=logits, axis=1),<br/>       "probabilities": tf.nn.softmax(logits, name="softmax_tensor")<br/>  }<br/>  if mode == tf.estimator.ModeKeys.PREDICT:<br/>        return tf.estimator.EstimatorSpec(mode=mode,<br/>                           predictions=predictions)</pre>
<p>We converted the raw values generated from our predictions into two different formats that our model function can return: a digit from 0–9 and the probability that the example is a zero, is a one, is a two, and so on. We compile our predictions in a dict and return an <kbd>EstimatorSpec</kbd> object. Now, we will pass to define a <kbd>loss</kbd> function:</p>
<pre>loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)</pre>
<p>A <kbd>loss</kbd> function measures how closely the model's predictions match the target classes. This function is used for both training and evaluation. We will configure our model to optimize this loss value during training:</p>
<pre>if mode == tf.estimator.ModeKeys.TRAIN:<br/>    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)<br/>    train_op = optimizer.minimize(<br/>        loss=loss,<br/>        global_step=tf.train.get_global_step())<br/>    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)</pre>
<p>We used a learning rate of <kbd>0.001</kbd> and stochastic gradient descent as the optimization algorithm. Now, we will add an accuracy metric in our model:</p>
<pre>  eval_metric_ops = {<br/>      "accuracy": tf.metrics.accuracy(<br/>          labels=labels, predictions=predictions["classes"])}<br/>  return tf.estimator.EstimatorSpec(<br/>      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)</pre>
<p>To do this, we defined the <kbd>eval_metric_ops</kbd> dict in the <kbd>EVAL</kbd> mode. We have thus defined the architecture of our network; now it is necessary to define the code to train and test our network. To do this, we will add a <kbd>main()</kbd> function to our Python code:</p>
<pre>def main(unused_argv):</pre>
<p>Then we will load training and eval data:</p>
<pre>  mnist = tf.contrib.learn.datasets.load_dataset("mnist")<br/>  train_data = mnist.train.images <br/>  train_labels = np.asarray(mnist.train.labels, dtype=np.int32)<br/>  eval_data = mnist.test.images <br/>  eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)</pre>
<p>In this piece of code, we stored the training feature data and training labels as <kbd>numpy</kbd> arrays in <kbd>train_data</kbd> and <kbd>train_labels</kbd>, respectively. Similarly, we stored the evaluation feature data and evaluation labels in <kbd>eval_data</kbd> and <kbd>eval_labels</kbd>, respectively. Next, we will create an <kbd>Estimator</kbd> for our model:</p>
<pre>  mnist_classifier = tf.estimator.Estimator(<br/>      model_fn=cnn_model_fn, model_dir="/tmp/mnist_convnet_model")</pre>
<p>An <kbd>Estimator</kbd> is a TensorFlow class for performing high-level model training, evaluation, and inference. The following code sets up logging for predictions:</p>
<pre>  tensors_to_log = {"probabilities": "softmax_tensor"}<br/>  logging_hook = tf.train.LoggingTensorHook(<br/>      tensors=tensors_to_log, every_n_iter=50)</pre>
<p>Now we're ready to train our model:</p>
<pre>    train_input_fn = tf.estimator.inputs.numpy_input_fn(<br/>      x={"x": train_data},<br/>      y=train_labels,<br/>      batch_size=100,<br/>      num_epochs=None,<br/>      shuffle=True)<br/>  mnist_classifier.train(<br/>      input_fn=train_input_fn,<br/>      steps=15000,<br/>      hooks=[logging_hook])</pre>
<p>To do this, we have created <kbd>train_input_fn</kbd> and called <kbd>train()</kbd>on <kbd>mnist_classifier</kbd>. In the previous code, we fixed <kbd>steps=15000</kbd>, which means the model will train for 15,000 steps in all.</p>
<div class="packt_tip">
<p>The time required to perform this training varies depending on the processor installed on our machine, but in any case, it will probably be more than 1 hour. To perform such training in less time, you can reduce the number of steps passed to the <kbd>train()</kbd> function; it is clear that this change will have a <span>negative effect </span>on the accuracy of the algorithm.</p>
</div>
<p>Finally, we will evaluate the model and print the results:</p>
<pre>    eval_input_fn = tf.estimator.inputs.numpy_input_fn(<br/>      x={"x": eval_data},<br/>      y=eval_labels,<br/>      num_epochs=1,<br/>      shuffle=False)<br/>  eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)<br/>  print(eval_results)</pre>
<p>We called the <kbd>evaluate</kbd> method, which evaluates the metrics we specified in the <kbd>eval_metriced_ops</kbd> argument in the <kbd>model_fn</kbd>. Our Python code ends with the following lines:</p>
<pre>if __name__ == "__main__":<br/>  tf.app.run()</pre>
<p>These lines are just a very quick wrapper that handles flag parsing and then dispatches to your own main function. At this point, we just have to copy the entire code into a file with a <kbd>.py</kbd> extension and run it on a machine where Python and TensorFlow are installed.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Run Python code on Google Cloud Shell</h1>
                </header>
            
            <article>
                
<p>Google Cloud Shell provides command-line access to cloud resources directly from your browser. You can easily manage projects and resources without having to install the Google Cloud SDK or other tools in your system. With Cloud Shell, the <kbd>gcloud</kbd> command-line tool from Cloud SDK and other necessary utilities are always available, updated and fully authenticated when you need them.</p>
<p>The following are some of the features of the Google Cloud Shell:</p>
<ul>
<li>It's a shell environment for managing resources hosted on Google Cloud Platform.</li>
<li>We can manage our GCP resources with the flexibility of a Linux shell. Cloud Shell provides command-line access to an instance of the virtual machine in a terminal window that opens in the web console.</li>
<li>It offers integrated authorization for access to projects and resources hosted on Google Cloud Platform.</li>
<li>Many of your favorite command-line tools, from bash and sh to emacs and vim, are already preinstalled and updated. Administration tools such as the MySQL client, Kubernetes, and Docker are configured and ready. You no longer need to worry about installing the latest version and all of its dependencies. Simply connect to Cloud Shell.</li>
</ul>
<p>Developers will have access to all favorite <span>preconfigured</span><span> </span><span>development tools. You will find development and implementation tools for Java, Go, Python, Node.js, PHP, and Ruby. Run your web applications within the Cloud Shell instance and preview them in the browser. Then commit to the repository again with the preconfigured Git and Mercurial clients.</span></p>
<p>Cloud Shell provisions 5 GB of permanent disk storage space, mounted as the <kbd>$ HOME</kbd> directory on the Cloud Shell instance. All files stored in the <kbd>$ HOME</kbd> directory, including user configuration scripts and files such as <kbd>bashrc</kbd> and <kbd>vimrc</kbd>, persist from one session to another.</p>
<p>To start Cloud Shell, just click on the <span class="packt_screen">Activate Google Cloud Shell</span> button at the top of the console window, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ff23e585-6877-4cbd-9c15-825aa579545f.png"/></div>
<p>A Cloud Shell session opens inside a new frame at the bottom of the console and displays a command-line prompt. It can take a few seconds for the shell session to be initialized. Now, our Cloud Shell session is ready to use, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-827 image-border" src="assets/e3fc2475-c7d1-4feb-bc98-9e05eade34ad.png" style=""/></div>
<p>At this point, we need to transfer the <kbd>cnn_hwr.py</kbd> file containing the Python code in the Google Cloud Platform. We have seen that to do so, we can use the resources made available by Google Cloud Storage. Then we open the Google Cloud Storage browser and create a new bucket.</p>
<div class="packt_tip">
<p>Remember that buckets are the basic containers that hold your data. Everything you store in Cloud Storage must be contained in a bucket. You can use buckets to organize your data and control access to your data, but unlike directories and folders, you cannot nest buckets.</p>
</div>
<p>To transfer the <kbd>cnn_hwr.py</kbd> file to Google Storage, perform the following steps:</p>
<ol>
<li>Just click on the <span class="packt_screen">CREATE BUCKET</span> icon</li>
<li>Type the name of the new bucket (<kbd>cnn-hwr</kbd>) in the create a bucket window</li>
<li>After this, a new bucket is available in the buckets list</li>
<li>Click on the <kbd>cnn-hwr</kbd> bucket</li>
<li>Click on uploads files icon in the window opened</li>
<li>Select the file <span>in the dialog window opened </span></li>
<li>Click <span class="packt_screen">Open</span></li>
</ol>
<p>At this point, our file will be available in the new bucket, as shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e2410819-05e9-449f-b5f4-236d1b4d423b.png"/></div>
<p>Now we can access the file from Cloud Shell. To do this, we create a new folder in the shell. Type the following command in the shell prompt:</p>
<pre><strong>mkdir CNN-HWR</strong></pre>
<p>Now, to copy the file from the Google Storage bucket to the <kbd>CNN-HWR</kbd> folder, simply type this command in the shell prompt:</p>
<pre><strong>gsutil cp gs://cnn-hwr-mlengine/cnn_hwr.py CNN-HWR</strong></pre>
<p>The following code is displayed:</p>
<pre><strong>giuseppe_ciaburro@progetto-1-191608:~$ gsutil cp gs://cnn-hwr/cnn_hwr.py CNN-HWR</strong><br/><strong>Copying gs://cnn-hwr/cnn_hwr.py...</strong><br/><strong>/ [1 files][ 5.7 KiB/ 5.7 KiB]</strong><br/><strong>Operation completed over 1 objects/5.7 KiB.</strong></pre>
<p>Now let's move into the folder and verify the presence of the file:</p>
<pre><strong>$cd CNN-HWR</strong><br/><strong>$ls</strong><br/><strong>cnn_hwr.py</strong></pre>
<p>We just have to run the file:</p>
<pre><strong>$ python cnn_hwr.py</strong></pre>
<p>A series of preliminary instructions is displayed:</p>
<pre><strong>Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.</strong><br/><strong>Extracting MNIST-data/train-images-idx3-ubyte.gz</strong><br/><strong>Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.</strong><br/><strong>Extracting MNIST-data/train-labels-idx1-ubyte.gz</strong><br/><strong>Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.</strong><br/><strong>Extracting MNIST-data/t10k-images-idx3-ubyte.gz</strong><br/><strong>Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.</strong><br/><strong>Extracting MNIST-data/t10k-labels-idx1-ubyte.gz</strong><br/><strong>INFO:tensorflow:Using default config.</strong></pre>
<p>They indicate that the data download was successful, as was the invocation of the TensorFlow library. From this point on, the training of the network begins, which, as we have anticipated, may be quite long. At the end of the algorithm execution, the following information will be returned:</p>
<pre><strong>INFO:tensorflow:Saving checkpoints for 15000 into /tmp/mnist_convnet_model/model.ckpt.</strong><br/><strong>INFO:tensorflow:Loss for final step: 2.2751274.INFO:tensorflow:Starting evaluation at 2018-02-19-08:47:04</strong><br/><strong>INFO:tensorflow:Restoring parameters from /tmp/mnist_convnet_model/model.ckpt-15000</strong><br/><strong>INFO:tensorflow:Finished evaluation at 2018-02-19-08:47:56</strong><br/><strong>INFO:tensorflow:Saving dict for global step 15000: accuracy = 0.9723, global_step = 15000, loss = 0.098432</strong><br/><strong>{'loss': 0.098432, 'global_step': 15000, 'accuracy': 0.9723}</strong></pre>
<p>In this case, we've achieved an accuracy of <kbd>97.2</kbd> percent on our test dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recurrent neural network</h1>
                </header>
            
            <article>
                
<p>Feedforward neural networks are based on input data that is powered to the network and converted into output. If it is a supervised learning algorithm, the output is a label that can recognize the input. Basically, these algorithms connect raw data to specific categories by recognizing patterns. Recurrent networks, on the other hand, take as input not only the current input data that is powered to the network, but also what they have experienced over time.</p>
<p>An <strong>recurrent neural network</strong> (<strong>RNN</strong>) is a neural model in which a bidirectional flow of information is present. In other words, while the propagation of signals <span>in feedforward networks </span>takes place only in a continuous manner in a direction from inputs to outputs, recurrent networks are different. In them, this propagation can also occur from a neural layer following a previous one, or between neurons belonging to the same layer, and even between a neuron and itself.</p>
<p>The decision made by a recurrent network at a specific instant affects the decision it will reach immediately afterwards. So, recurrent networks have two input sources—the present and the recent past<span>—</span>that combine to determine how to respond to new data, just as people do in life everyday.</p>
<p>Recurrent networks are distinguished from feedforward networks thanks to the feedback loop linked to their past decisions, thus accepting their output momentarily as inputs. This feature can be emphasized by saying that recurrent networks have memory. Adding memory to neural networks has a purpose: there is information in the sequence itself and recurrent networks use it to perform the tasks that feedforward networks cannot.</p>
<p>Access to memory occurs through the content rather than by address or location. One approach to this is that the memory content is the pattern of activations on the nodes of an RNN. The idea is to start the network with an activation scheme that is a partial or noisy representation of the requested memory content and that the network stabilizes on the required content.</p>
<p>RNN is a class of neural network where there is at least one feedback connection between neurons that form a directed cycle. A typical RNN with connections between output layer and hidden layer is represented in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/cf599c2a-4e1e-4a61-923d-fa7a950ceda9.png" style=""/></div>
<p>In the recurring network shown in the figure, both the input level and the output level are used to define the weights of the hidden level.</p>
<p>Ultimately, we can think of RNNs as a variant of ANNs: these variants can be characterized on a different number of hidden levels and a different trend of the data flow. The RNN are characterized by a different trend of the data flow, in fact the connections between the neurons form a cycle. Unlike feedforward networks, RNNs can use internal memory for their processing. RNNs are a class of ANNs that feature connections between hidden layers that are propagated through time in order to learn sequences.</p>
<p>The way the data is kept in memory and flows at different time periods makes RNNs powerful and successful. RNN use cases include the following fields:</p>
<ul>
<li>Stock market predictions</li>
<li>Image captioning</li>
<li>Weather forecast</li>
<li>Time-series-based forecasts</li>
<li>Language translation</li>
<li>Speech recognition</li>
<li>HWR</li>
<li>Audio or video processing</li>
<li>Robotics action sequencing</li>
</ul>
<p>Recurrent networks are designed to recognize patterns as a sequence of data and are helpful in prediction and forecasting. They can work on text, images, speech, and time series data. RNNs are among the powerful ANNs and represent the biological brain, including memory with processing power.</p>
<p>Recurrent networks take inputs from the current input (like a feedforward network) and the output that was calculated previously. In the following figure, we compare a single neuron operating scheme for both a feedforward neural network and an RNN:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-817 image-border" src="assets/95ac9383-d0c0-4058-b8d6-2684554946cb.png" style=""/></div>
<p>As we can see in the simple, just-proposed single neuron scheme, the feedback signal is added to the input signal in the RNN. Feedback is a considerable and significant feature. A feedback network is more likely to update and has more computing capacity than a simple network limited to one-way signals from input to output. Feedback networks show phenomena and processes not revealed by one-way networks.</p>
<p>To understand the differences between ANN and RNN, we consider the RNN as a network of neural networks, and the cyclic nature is unfolded in the following manner: the state of a neuron is considered at different time periods (<em>t-1</em>, <em>t</em>, <em>t+1</em>, and so on) until convergence or until the total number of epochs is reached.</p>
<p>The network learning phase can be performed using gradient descent procedures similar to those leading to the backpropagation algorithm for feedforward networks. At least this is valid in the case of simple architectures and deterministic activation functions. When activations are stochastic, simulated annealing approaches may be more appropriate.</p>
<p>RNN architectures can have many different forms. There are more variants in the way the data flows backwards:</p>
<ul>
<li>Fully recurrent</li>
<li>Recursive</li>
<li>Hopfield</li>
<li>Elman networks</li>
<li>LSTM</li>
<li>Gated recurrent unit</li>
<li>Bidirectional</li>
<li>Recurrent MLP</li>
</ul>
<p>In the following pages, we will analyze the architecture of some of these networks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fully recurrent neural networks</h1>
                </header>
            
            <article>
                
<p>A fully RNN is a network of neurons, each with a directed (one-way) connection to every other neuron. Each neuron has a time-varying, real-valued activation. Each connection has a modifiable real-valued weight. Input neurons, output neurons, and hidden neurons are expected. This type of network is a multilayer perceptron with the previous set of hidden unit activations feeding back into the network along with the inputs, as shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-820 image-border" src="assets/7a3453ca-581a-4f84-be6f-df21024492a6.png" style=""/></div>
<div>
<p>At each step, each non-input unit calculates its current activation as a nonlinear function of the weighted sum of activations of all units that connect to it.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recursive neural networks</h1>
                </header>
            
            <article>
                
<p><span>A recursive network is just a generalization of a recurrent network. In a recurrent network, the weights are shared and dimensionality remains constant along the length of the sequence. In a recursive network, the weights are shared and dimensionality remains constant but at every node. The following figure shows what a recursive neural network looks like:</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-821 image-border" src="assets/dab54544-cb52-4a9c-a2ea-30990052aa8f.png" style=""/></div>
<div>
<p>Recursive neural networks can be used for learning tree-like structures. They are highly useful for parsing natural scenes and language.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hopfield recurrent neural networks</h1>
                </header>
            
            <article>
                
<div>
<p><span>In 1982, physicist John J. Hopfield published a fundamental article in which a mathematical model commonly known as the <strong>Hopfield network</strong> was introduced. This network highlighted new computational capabilities deriving from the collective behavior of a large number of simple processing elements. A Hopfield Network is a form of recurrent ANN.</span></p>
</div>
<p>According to Hopfield every physical system can be considered as a potential memory device if it has a certain number of stable states, which act as an attractor for the system itself. On the basis of this consideration, he formulated the thesis that the stability and placement of such attractors represented spontaneous properties of systems consisting of considerable quantities of mutually interacting neurons.</p>
<p>Structurally, the Hopfield network constitutes a recurrent symmetrical neural network (therefore with a synaptic weights matrix that is symmetric), one that is completely connected and in which each neuron is connected to all the others, as shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-824 image-border" src="assets/0e4650f6-14ac-4d42-b5c5-009679651721.png" style=""/></div>
<p>As already mentioned before, a recurrent network is a neural model in which a flow of bidirectional information is present; in other words, while in feedforward networks the propagation of the signals takes place only in a continuous manner in the direction that leads from the inputs to the outputs in the recurrent networks this propagation can also occur from a neural layer following a previous one or between neurons belonging to at the same layer (Hopfield network) and even between a neuron and itself.</p>
<p>The dynamics of a Hopfield network is described by a nonlinear system of differential equations and the neuron update mechanism can be:</p>
<ul>
<li><strong>Asynchronous</strong>: One neuron is updated at a time</li>
<li><strong>Synchronous</strong>: All neurons are updated at the same time</li>
<li><strong>Continuous</strong>: All the neurons are continually updated</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Elman neural networks</h1>
                </header>
            
            <article>
                
<p>The Elman neural network is a feedforward network in which the hidden layer, besides being connected to the output layer, forks into another identical layer, called the <strong>context layer</strong>, to which it is connected with weights equal to one. At each moment of time (each time the data is passed to the neurons of the input layer), the neurons of the context layer maintain the previous values and pass them to the respective neurons of the hidden layer. The following figure shows an Elman network scheme:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9ef7e2f5-d6cb-4ba1-a4b8-26a326797a9e.png" style=""/></div>
<div>
<p>Like feedforward networks, Elman's RNNs can be trained with an algorithm called <strong>Backpropagation Through Time</strong> (<strong>BPTT</strong>), a variant of the backpropagation created specifically for the RNNs. Substantially, this algorithm unrolls the neural network transforming it into a feedforward network, with a number of layers equal to the length of the sequence to be learned; subsequently, the classic backpropagation algorithm is applied. Alternatively, it is possible to use global optimization methods, such as genetic algorithms, especially with RNN topologies on which it is not possible to apply BPTT.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Long short-term memory networks</h1>
                </header>
            
            <article>
                
<p>LSTM is a particular architecture of RNN, originally conceived by Hochreiter and Schmidhuber in 1997. This type of neural network has been recently rediscovered in the context of deep learning because it is free from the problem of vanishing gradient, and in practice it offers excellent results and performance.</p>
<div class="packt_tip">
<p>The vanishing gradient problem affects the training of ANNs with gradient-based learning methods. In gradient-based methods such as backpropagation, weights are adjusted proportionally to the gradient of the error. Because of the way in which the aforementioned gradients are calculated, we obtain the effect that their module decreases exponentially, proceeding towards the deepest layers. The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value. In the worst case, this may completely stop the neural network from further training.</p>
</div>
<p>LSTM-based networks are ideal for prediction and classification of time sequences, and they are supplanting many classic machine learning approaches. In fact, in 2012, Google replaced its voice recognition models, passing from the Hidden Markov Models (which represented the standard for over 30 years) to deep learning neural networks. In 2015, it switched to the RNNs LSTM combined with <strong>connectionist temporal classification</strong> (<strong>CTC</strong>).</p>
<div class="packt_tip">
<p>CTC is a type of neural network output and associated scoring function for training RNNs.</p>
</div>
<p>This is due to the fact that LSTM networks are able to consider long-term dependencies between data, and in the case of speech recognition, this means managing the context within a sentence to improve recognition capacity.</p>
<p>An LSTM network consists of cells (LSTM blocks) linked together. Each cell is in turn composed of three types of ports: <strong>input gate</strong>, <strong>output gate</strong>, and <strong>forget gate</strong>. They respectively implement the write, read, and reset functions on the cell memory. The ports are not binary but analogical (generally managed by a sigmoid activation function mapped in a range (0, 1), where zero indicates total inhibition and 1 indicates total activation), and they are multiplicative. The presence of these ports allows the LSTM cells to remember information for an indefinite amount of time. In fact, if the input gate is below the activation threshold, the cell will maintain the previous state, while if it is enabled, the current state will be combined with the input value. As the name suggests, the forget gate resets the current state of the cell (when its value is brought to zero), and the output gate decides whether the value inside the cell must be taken out or not.</p>
<p>The following figure shows an LSTM unit:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-823 image-border" src="assets/84226791-6851-4aa4-975d-682e1f237ec4.png" style=""/></div>
<div>
<p>The approaches based on neural networks are very powerful, as they allow capture of the characteristics and relationships between the data. In particular, it has also been seen that LSTM networks, <span>in practice, </span>offer high performance and excellent recognition rates. One disadvantage is that the neural networks are black box models, so their behavior is not predictable, and it is not possible to trace the logic with which they process the data.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Handwriting Recognition using RNN and TensorFlow</h1>
                </header>
            
            <article>
                
<p><span>To practice RNNs, we will use the dataset previously used to construct the CNN. I refer to the MNIST dataset, a large database of handwritten digits. It has a set of 70,000 examples of data. It is a subset of NIST's larger dataset. Images of 28 x 28 pixel resolution are stored in a matrix of 70,000 rows and 785 columns; each pixel value from the 28 x 28 matrix and one value is the actual digit. In a fixed-size image, the digits have been size-normalized.</span></p>
<div>
<p>In this case, we will implement an RNN (LSTM) using the TensorFlow library to classify images. We will consider every image row as a sequence of pixels. Because the MNIST image shape is 28 x 28, we will handle 28 sequences of 28 time steps for every sample.</p>
<p>To start, we will analyze the code line by line; then we will see how to process it with the tools made available by Google Cloud Platform. Now, let's go through the code to learn how to apply an RNN (LSTM) to solve an HWR problem. Let's start from the beginning of the code:</p>
<pre>from __future__ import absolute_import<br/>from __future__ import division<br/>from __future__ import print_function</pre>
<p>These three lines are added to write a Python 2/3 compatible code base. So let's move on to importing modules:</p>
<pre>import tensorflow as tf<br/>from tensorflow.contrib import rnn</pre>
<p>In this way, we have imported the <kbd>tensorflow</kbd> module and, from <kbd>tensorflow.contrib</kbd>, the <kbd>rnn</kbd> module. The <kbd>tensorflow.contrib</kbd> contains volatile or experimental code. The <kbd>rnn</kbd> <span>module </span>is a module for constructing RNN Cells and additional RNN operations. Let's analyze the next lines of code:</p>
<pre>from tensorflow.examples.tutorials.mnist import input_data<br/>mnist = input_data.read_data_sets("/tmp/data/", one_hot=True)</pre>
<p>The first line is used to import the <kbd>mnist</kbd> dataset from the TensorFlow library; in fact, the <kbd>minist</kbd> dataset is already present in the library as an example. The second line reads the data from a local directory. Let's move on to set the training parameters:</p>
<pre>learning_rate = 0.001<br/>training_steps = 20000<br/>batch_size = 128<br/>display_step = 1000</pre>
<p>The <kbd>learning_rate</kbd> is a value used by the learning algorithm to determine how quickly the weights are adjusted. It determines the acquisition time for neurons with weights that are trained using the algorithm. The <kbd>training_steps</kbd> sets the number of times the training process is performed. The <kbd>batch_size</kbd> is the number of samples you feed in your network. The <kbd>display_step</kbd> decides how many steps are shown the partial results of the training. Now let's set the network parameters:</p>
<pre>num_input = 28<br/>timesteps = 28<br/>num_hidden = 128<br/>num_classes = 10</pre>
<p>The first parameter (<kbd>num_input</kbd>) sets the MNIST data input (image shape: 28 x 28). The <kbd>timesteps</kbd> parameter is equivalent to the number of time steps you run your RNN. The <kbd>num_hidden</kbd> parameter sets the number of hidden layers of the neural network. Finally the <kbd>num_classes</kbd> parameter sets the MNIST total classes (0-9 digits). Let's analyze the following lines of code:</p>
<pre>X = tf.placeholder("float", [None, timesteps, num_input])<br/>Y = tf.placeholder("float", [None, num_classes])</pre>
<p>In these lines of code, we used a <kbd>tf.placeholder()</kbd> function. A placeholder is simply a variable that we will assign data to at a later date. It allows us to create our operations and build our computation graph without needing the data. In this way, we have set up the <kbd>tf.Graph</kbd> input. A <kbd>tf.Graph</kbd> contains two relevant kinds of information: graph structure and graph collections. TensorFlow uses a dataflow graph to represent your computation in terms of the dependencies between individual operations. This leads to a low-level programming model in which you first define the dataflow graph and then create a TensorFlow session to run parts of the graph across a set of local and remote devices. Let's move on to define <kbd>weights</kbd>:</p>
<pre>weights = {<br/>    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))<br/>}<br/>biases = {<br/>    'out': tf.Variable(tf.random_normal([num_classes]))<br/>}</pre>
<p>Weights in a network are the most important factor for converting an input to impact the output. This is similar to slope in linear regression, where a weight is multiplied to the input to add up to form the output. Weights are numerical parameters that determine how strongly each of the neurons affects the other. Bias is like the intercept added in a linear equation. It is an additional parameter used to adjust the output along with the weighted sum of the inputs to the neuron. Now we have to define the <kbd>RNN</kbd> by creating a new function:</p>
<pre>def RNN(x, weights, biases):<br/>    x = tf.unstack(x, timesteps, 1)<br/>    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)<br/>    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)<br/>    return tf.matmul(outputs[-1], weights['out']) + biases['out']</pre>
<p>The <kbd>unstack()</kbd> function is used to get a list of <kbd>timesteps</kbd> tensors of shape (<kbd>batch_size</kbd>, <kbd>n_input</kbd>). Then we have defined an <kbd>lstm</kbd> cell with TensorFlow, and we've got an <kbd>lstm</kbd> cell output. Finally, we have placed a linear activation, using the <kbd>RNN</kbd> in the inner loop and last output. Let's move on:</p>
<pre>logits = RNN(X, weights, biases)<br/>prediction = tf.nn.softmax(logits)</pre>
<p>The first line of code uses the newly defined <kbd>RNN</kbd> function to build the network, while the second line of code predicts using the function <kbd>tf.nn.softmax()</kbd>, which computes <kbd>softmax</kbd> activations. Next, we will define <kbd>loss</kbd> and <kbd>optimizer</kbd>:</p>
<pre>loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(<br/>    logits=logits, labels=Y))<br/>optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)<br/>train_op = optimizer.minimize(loss_op)</pre>
<p>The <kbd>loss</kbd> function maps an event or values of one or more variables onto a real number, intuitively representing some <kbd>cost</kbd> associated with the event. We have used the <kbd>tf.reduce_mean()</kbd> function, which computes the mean of elements across the dimensions of a tensor. The <kbd>optimizer</kbd> base class provides methods to compute gradients for a loss and apply gradients to variables. A collection of subclasses implement classic optimization algorithms such as gradient descent and AdaGrad. Let's go ahead to evaluate model:</p>
<pre>correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))<br/>accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))</pre>
<p>Then we will initialize the variables by assigning their default value:</p>
<pre>init = tf.global_variables_initializer()</pre>
<p>Now we can start training the network:</p>
<pre>with tf.Session() as sess:<br/>    sess.run(init)<br/>    for step in range(1, training_steps+1):<br/>        batch_x, batch_y = mnist.train.next_batch(batch_size)<br/>        batch_x = batch_x.reshape((batch_size, timesteps, num_input))<br/>        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})<br/>        if step % display_step == 0 or step == 1:<br/>            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,<br/>                                                                 Y: batch_y})<br/>            print("Step " + str(step) + ", Minibatch Loss= " + \<br/>                  "{:.4f}".format(loss) + ", Training Accuracy= " + \<br/>                  "{:.3f}".format(acc))<br/>    print("End of the optimization process ")</pre>
<p>Finally we will calculate the accuracy for <kbd>128</kbd> mnist test images:</p>
<pre>    test_len = 128<br/>    test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))<br/>    test_label = mnist.test.labels[:test_len]<br/>    print("Testing Accuracy:", \<br/>        sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))</pre>
<p>At this point, we just have to copy the entire code into a file with a <kbd>.py</kbd> extension and run it on a machine where Python and TensorFlow are installed.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">LSTM on Google Cloud Shell</h1>
                </header>
            
            <article>
                
<p><span>After having thoroughly analyzed the Python code, it is time to run it around to classify the images contained in the dataset. To do this, we work in a similar way to what was done in the case of the example on CNN. So we will use the Google Cloud Shell. Google Cloud Shell provides command-line access to Cloud resources directly from your browser. You can easily manage projects and resources without having to install the Google Cloud SDK or other tools in your system. With Cloud Shell, the <kbd>gcloud</kbd> command-line tool from Cloud SDK and other necessary utilities are always available, updated and fully authenticated when you need them.</span></p>
<p>To start Cloud Shell, just click the <span class="packt_screen">Activate Google Cloud Shell</span> button at the top of the console window, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ebe7c69a-a3d7-4867-aeeb-3f223daf727a.png"/></div>
<p>A Cloud Shell session opens inside a new frame at the bottom of the console and displays a command-line prompt. It can take a few seconds for the shell session to be initialized. Now, our Cloud Shell session is ready to use, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-822 image-border" src="assets/443a8156-ece7-4c3d-a847-f7686d3b2d3a.png" style=""/></div>
<p>At this point, we need to transfer the <kbd>rnn_hwr.py</kbd> file containing the Python code in the Google Cloud Platform. We have seen that to do so, we can use the resources made available by Google Cloud Storage. Then we open the Google Cloud Storage browser and create a new bucket.</p>
<p>To transfer the <kbd>cnn_hwr.py</kbd> file on Google Storage, follow these steps:</p>
<ol>
<li>Just click on <span class="packt_screen">CREATE BUCKET</span> icon</li>
<li>Type the name of the new bucket (<kbd>rnn-hwr</kbd>) in the create a bucket window</li>
<li>After this, a new bucket is available in the buckets list</li>
<li>Click on the <kbd>rnn-hwr</kbd> bucket</li>
<li>Click on <span class="packt_screen">UPLOAD FILES</span> icon in the window opened</li>
<li>Select the file in the dialog window opened</li>
<li>Click <span class="packt_screen">Open</span></li>
</ol>
<p>At this point, our file will be available in the new bucket, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ec3a8d23-345c-4434-8d2d-f3d2d63d6b62.png"/></div>
<p>Now we can access the file from the Cloud Shell. To do this, we create a new folder in the shell. Type this command in the shell prompt:</p>
<pre><strong>mkdir RNN-HWR</strong></pre>
<p>Now, to copy the file from the Google Storage bucket to the <kbd>CNN-HWR</kbd> folder, simply type the following command in the shell prompt:</p>
<pre><strong>gsutil cp gs://rnn-hwr-mlengine/rnn_hwr.py RNN-HWR</strong></pre>
<p>The following code is displayed:</p>
<pre><strong>giuseppe_ciaburro@progetto-1-191608:~$ gsutil cp gs://rnn-hwr/rnn_hwr.py RNN-HWR</strong><br/><strong>Copying gs://rnn-hwr/rnn_hwr.py...</strong><br/><strong>/ [1 files][ 4.0 KiB/ 4.0 KiB]</strong><br/><strong>Operation completed over 1 objects/4.0 KiB.</strong></pre>
<p>Now let's move into the folder and verify the presence of the file:</p>
<pre><strong>$cd RNN-HWR</strong><br/><strong>$ls</strong><br/><strong>rnn_hwr.py</strong></pre>
<p>We just have to run the file:</p>
<pre><strong>$ python rnn_hwr.py</strong></pre>
<p>A series of preliminary instructions is displayed:</p>
<pre><strong>Extracting /tmp/data/train-images-idx3-ubyte.gz</strong><br/><strong>Extracting /tmp/data/train-labels-idx1-ubyte.gz</strong><br/><strong>Extracting /tmp/data/t10k-images-idx3-ubyte.gz</strong><br/><strong>Extracting /tmp/data/t10k-labels-idx1-ubyte.gz</strong></pre>
<p>They indicate that the data download was successful, as was the invocation of the TensorFlow library. From this point on, the training of the network begins, which, as we have anticipated, may be quite long. At the end of the algorithm execution, the following information will be returned:</p>
<pre><strong>Step 1, Minibatch Loss= 2.9727, Training Accuracy= 0.117</strong><br/><strong>Step 1000, Minibatch Loss= 1.8381, Training Accuracy= 0.430</strong><br/><strong>Step 2000, Minibatch Loss= 1.4021, Training Accuracy= 0.602</strong><br/><strong>Step 3000, Minibatch Loss= 1.1560, Training Accuracy= 0.672</strong><br/><strong>Step 4000, Minibatch Loss= 0.9748, Training Accuracy= 0.727</strong><br/><strong>Step 5000, Minibatch Loss= 0.8156, Training Accuracy= 0.750</strong><br/><strong>Step 6000, Minibatch Loss= 0.7572, Training Accuracy= 0.758</strong><br/><strong>Step 7000, Minibatch Loss= 0.5930, Training Accuracy= 0.812</strong><br/><strong>Step 8000, Minibatch Loss= 0.5583, Training Accuracy= 0.805</strong><br/><strong>Step 9000, Minibatch Loss= 0.4324, Training Accuracy= 0.914</strong><br/><strong>Step 10000, Minibatch Loss= 0.4227, Training Accuracy= 0.844</strong><br/><strong>Step 11000, Minibatch Loss= 0.2818, Training Accuracy= 0.906</strong><br/><strong>Step 12000, Minibatch Loss= 0.3205, Training Accuracy= 0.922</strong><br/><strong>Step 13000, Minibatch Loss= 0.4042, Training Accuracy= 0.891</strong><br/><strong>Step 14000, Minibatch Loss= 0.2918, Training Accuracy= 0.914</strong><br/><strong>Step 15000, Minibatch Loss= 0.1991, Training Accuracy= 0.938</strong><br/><strong>Step 16000, Minibatch Loss= 0.2815, Training Accuracy= 0.930</strong><br/><strong>Step 17000, Minibatch Loss= 0.1790, Training Accuracy= 0.953</strong><br/><strong>Step 18000, Minibatch Loss= 0.2627, Training Accuracy= 0.906</strong><br/><strong>Step 19000, Minibatch Loss= 0.1616, Training Accuracy= 0.945</strong><br/><strong>Step 20000, Minibatch Loss= 0.1017, Training Accuracy= 0.992</strong><br/><strong>Optimization Finished!</strong><br/><strong>Testing Accuracy: 0.9765625</strong></pre>
<p>In this case, we've achieved an accuracy of <kbd>97.6</kbd> percent on our test dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we tried to broaden the concepts underlying standard neural networks by adding features to solve more complex problems. To begin with, we discovered the architecture of CNNs. CNNs are ANNs in which the hidden layers are usually constituted by convolutional layers, pooling layers, FC layers, and normalization layers. The concepts underlying CNN were covered.</p>
<p>We understood training, testing, and evaluating a CNN through the analysis of a real case. For this purpose, an HWR problem was addressed in Google Cloud Platform.</p>
<p>Then, we explored RNN. Recurrent networks take, as their input, not only current input data that is powered to the network but also what they have experienced over time. Several RNN architectures were analyzed. In particular, we focused on LSTM networks.</p>


            </article>

            
        </section>
    </body></html>