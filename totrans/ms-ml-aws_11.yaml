- en: Implementing Deep Learning with TensorFlow on AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow is a very popular deep learning framework that can be used to train
    deep neural networks, such as those described in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: About TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow as a general machine learning library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and serving the TensorFlow model through SageMaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a custom neural net with TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: About TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow is a library for deep learning, first released by Google in 2015\.
    Initially, it included a core library that allowed users to work with tensors
    (multidimensional arrays) in symbolic form, thus enabling low-level neural network design
    and training at high performance. Nowadays, it's a fully fledged deep learning
    library that allows data scientists to build models for complex problems, such
    as image recognition, using high-level primitives. You can also use TensorFlow
    for solving standard machine learning problems, such as the ones we've been considering
    in the past chapters. TensorFlow has similar abstractions to the ones we have
    been using in `scikit-learn`, Apache Spark, and SageMaker. For example, it allows
    the user to create classification models or regression models using high-level
    abstractions, such as estimators, predictors, and evaluators.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow as a general machine learning library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section we will  show how we use TensorFlow to create a regression
    model for the house estimation problem of [Chapter 3](eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml), *Predicting
    House Value with Regression Algorithms*. To get started, we will first launch
    a SageMaker notebook and choose the TensorFlow kernel (`conda_tensorflow_p36`),
    which has all the necessary TensorFlow dependencies needed for this section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3d9cd933-f996-44b9-972a-bf56df7f2ace.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s consider the estimation problem from [Chapter 3](eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml), *Predicting
    House Value with Regression Algorithms*. Recall that we had a set of indicators
    (age of the house, distance to nearest center, and so on) to estimate the median
    value of the house (expressed in the `medv` column, which is our target feature),
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d2519263-cc3e-49ff-90f5-505b78a5769b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In [Chapter 3](eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml), *Predicting House
    Value with Regression Algorithms*, we identified 11 learning features to use for
    predicting the target feature (`medv`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'With this information, we define a TensorFlow linear regressor capable of solving
    our regression problem with a pre-built neural net:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: For the regressor, we decided to create a single-feature input, which assembles
    the rest of the features into a vector of numbers that will represent the input
    layer. It is also possible to create one named feature per training feature (as
    we did in [Chapter 3](eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml), *Predicting
    House Value with Regression Algorithms*), but we'll just have a single vector
    feature to simplify the prediction service discussed at the end of the section.
  prefs: []
  type: TYPE_NORMAL
- en: To construct a regressor, we need to pass in the TensorFlow feature columns,
    which can be of several different kinds. The `tf.feature_column` package provides
    functions to construct different kinds of columns, depending on the encoding being
    used by the model (for example, categorical, bucketized, and so on.). The feature
    columns inform the model on the expected format of the data being submitted as
    input. In our case, we will just tell the model to expect vector rows of length
    11.
  prefs: []
  type: TYPE_NORMAL
- en: 'To construct the actual data to be passed into the model, we need to create
    a matrix. The `pandas` library has a convenient method, `as_matrix()`, so we''ll
    slice the training features and build a matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we''ll create the vector of features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Once we have these two things, we can start plugging the data into the model.
    TensorFlow expects the data to be fed by defining a function that knows how to
    source the data into tensors (the building blocks of TensorFlow that represents
    a multidimensional array).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code block for plugging in the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `tf.estimator.inputs.numpy_input_fn` utility is able to construct such a
    function by providing the training matrix and target feature vectors. It will
    also create partitions of the data for running through the network a number of
    epochs. It also allows the user to pick the size of the batch (recall the mini-batch
    method mentioned in [Chapter 3](eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml), *Predicting
    House Value with Regression Algorithms*, for stochastic gradient descent) and
    other data feeding parameters. In essence, the underlying regressor's neural network
    relies on the `training_input_fn` function for creating the input tensors at each
    stage of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Likewise, we create a similar function for feeding the testing data, in preparation
    for model evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To train the model, we call the usual `fit()` method, providing the function
    we created for sourcing the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `steps` argument is a limit we can impose on the number of total steps.
    A step, here, is one gradient descent update for one batch. Hence, each epoch
    runs a number of steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once it completes the training, TensorFlow will output the loss metric in the
    `final` epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then evaluate the accuracy of our model by running the test dataset
    (by providing the test dataset sourcing function):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The average loss depends on the units of the target feature, so let's look at
    building a scatter plot like the one we created in [Chapter 3](eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml), *Predicting
    House Value with Regression Algorithms*, to compare actual versus predicted house
    values. To do that, we first need to obtain `predictions`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We simply call the `predict()` function to get `predictions`, again providing
    the test dataset sourcing function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The `predictions` returned a value that is actually a Python generator of single-value
    vectors, so we can obtain a list of `predictions` by constructing the list-through-list
    comprehension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We can thus examine `predicted_values`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We can plug in the predicted values as a column to our original `pandas` test
    dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This allows us to use the pandas plotting method to create the chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the result in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18617ff1-e19b-487e-be7b-4b62070de8c1.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that there is a clear correlation. To improve the performance, we would
    have to tune our regression model, the size of the batches, steps, epochs, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: Training and serving the TensorFlow model through SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Instead of training the model in a notebook instance, we train the model using
    the SageMaker infrastructure. In previous chapters, we used built-in estimators,
    such as BlazingText, XGBoost, and **Factorization Machines** (**FMs**). In this
    section, we will show how we can build our own TensorFlow models and train them
    through SageMaker, much like we did with these pre-built models. To do this, we
    just have to teach SageMaker how our TensorFlow model should be constructed and
    comply with some conventions regarding the format, location, and structure of
    the data. Through a Python script, we specify all of this.
  prefs: []
  type: TYPE_NORMAL
- en: 'SageMaker will rely on this Python script to perform the training within SageMaker
    training instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The first few lines in the preceding code block are the usual imports and session
    creation necessary for getting started with SageMaker. The next important thing
    is the creation of a TensorFlow estimator. Note how we provide the constructor
    with a Python script, TensorFlow version, and Python version, as well as the usual
    parameters for instance number and type.
  prefs: []
  type: TYPE_NORMAL
- en: 'Upon calling the `tf_estimator.fit(training_data_s3_path)` function, SageMaker
    will do the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Launch an EC2 instance (server).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the S3 data to a local directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Call the `tf_train.py` Python script to train the model. The Python script is
    expected to store the model on a certain local directory of the EC2 instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Package the stored model in a `.tar.gz` file and upload it to S3\. Additionally,
    it will create an Amazon container and SageMaker model identifier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hence, the training happens on a SageMaker managed server, but the model it
    produces is a SageMaker compatible model, which can be used to serve predictions
    or run batch transform jobs, like the ones we worked with in previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at the `tf_train.py` Python script, which is responsible for
    the model training and saving the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'This Python script must receive some information from the SageMaker container.
    In particular, it must receive the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The local directory where SageMaker has downloaded the data (from S3)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The location where the Python script needs to store the trained model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other hyperparameters needed by the model (we will not dive into this yet and
    work with just fixed values, but we will show in [Chapter 14](7de65295-dd1f-4eb3-af00-3868ed7e2df9.xhtml), *Optimizing*
    *Models in Spark and SageMaker*, how these can be used for hyperparameter tuning)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The first part of the script is just setting up an argument parser. Since SageMaker
    calls this script as a black box, it needs to be able to inject such arguments
    to the script. With these arguments, it can train the TensorFlow model. You might
    notice that the training is exactly the same as what we did in the previous section.
    The only new part is saving the model and the definition of a new kind of function
    (`serving_input_fn`). This function has a similar purpose to the ones we used
    for training and testing, but instead, it will be used at the serving time (that
    is, each time a prediction request is made to the service). It is responsible
    for defining the necessary transformation from an input tensor placeholder to
    the features expected by the model. The `tf.estimator.export.build_parsing_serving_input_receiver_fn` utility
    can conveniently build a function for such purposes. It builds a function that
    expects `tf.Example` (a `protobuf`-serialized dictionary of features) fed into
    a string placeholder, so that it can parse such examples into feature tensors.
    In our case, we just have a single vector as input, so the transformation is straightforward.
    The last line in our script saves the model into the location requested by SageMaker
    through the `local_model_dir` argument. In order for the deserialization and unpacking
    to work, the convention is to save the model in a `/export/Servo` subdirectory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we run the `fit()` command, we can deploy the model as usual:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: For this example, we used a non-GPU instance type, but these are largely recommended
    for serious serving and training. We will dive into this in [Chapter 15](691fc3d8-e4b8-4e3f-a8d9-e13f53f058c4.xhtml), *Tuning
    Clusters for Machine Learning**.*
  prefs: []
  type: TYPE_NORMAL
- en: The `deploy()` command will launch a container capable of serving the model
    we constructed. However, constructing the payload to send to such service is not
    as trivial as the examples in the previous chapter, as we need to construct `tf.Example`.
  prefs: []
  type: TYPE_NORMAL
- en: 'At prediction time we want to obtain the price given a specific feature vector.
    Suppose we want to find the price for these features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The first step is to construct a `tf.train.Example` instance, which in our
    case consists of a single feature called `inputs` with the floating point values
    of `features_vector`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to serialize the `model_input protobuf` message using `SerializeToString`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Since this is really a string of bytes, we need to further encode `model_input`
    so that it can be sent in the payload as a string without special characters.
    We use `base64` encoding to do such a thing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we call our `predictor` service by assembling a JSON request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Note there is a special convention used for sending base64, encoded `protobuf`
    examples by creating a dictionary keyed with `b64`. The output decoded from JSON
    is a dictionary with the following prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The `inputs` and `outputs` payload JSON keys are part of the contract for SageMaker
    and should not be confused with the name of our single feature, `inputs`, which
    can be an arbitrary string.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a custom neural net with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, *Training and serving the TensorFlow model through
    SageMaker*, we used the high-level library of TensorFlow to construct a regression
    model using a `LinearRegressor`. In this section, we will show how we can construct
    an actual neural network using the Keras library from TensorFlow. Keras facilitates
    the design of neural networks by hiding some of the complexity behind the core
    (low-level) TensorFlow library.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will use the ubiquitous MNIST dataset, which consists of
    a series of images of handwritten digits along with the real label (values between
    0 and 1). The MNIST dataset can be downloaded from [https://www.kaggle.com/c/digit-recognizer/data](https://www.kaggle.com/c/digit-recognizer/data).
  prefs: []
  type: TYPE_NORMAL
- en: The dataset comes as a CSV with 784 columns corresponding to each of the pixels
    in the 28 x 28 image. The values for each column represent the strength of the
    pixel in a gray scale from 0 to 255\. It also has an additional column for the
    label with a value between 0 and 9, corresponding to the actual digit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s download the dataset and do our usual splitting into testing and training
    using `pandas` and `scikit-learn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We can inspect the dataset through `train.head()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/144dc974-87ff-464d-97f6-8521f09e3286.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we can see, the columns are labeled `pixelX`, where `x` is a number between
    `0` and `783`. Let''s define the names of these columns in distinct variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Each row in this dataset becomes a training example and thus represent the input
    layer of our network. On the other end of the network, we will have 10 nodes,
    each representing the probability of each digit given each input vector. For our
    example, we will just use one middle layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram depicts our network structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd1bc0d1-990d-4431-9d49-ca56f9657017.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To define such a network in Keras is very simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Note how easy it is to define such a model. It consists of three layers:'
  prefs: []
  type: TYPE_NORMAL
- en: An input layer, where each vector is of size 784, and each gradient descent
    update will feed a mini-batch of five examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A middle dense layer (meaning each node will connect to every other node in
    the next layer) with a **Rectified Linear Unit** (**ReLU**) activation function
    on each node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An output layer of size 10 using a softmax activation function (as we want a
    probability distribution over the digits)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition to defining the network through a sequence of layers, TensorFlow
    will need to compile the model. This basically entails providing the kind of optimization
    method to use, the `loss` function, and the required metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The next stage will be to fit the model with our data. In order to feed the
    dataset into TensorFlow, we need to create `numpy` matrices, where each row is
    a training instance and each column represents a node in the input layer. Conveniently,
    the `pandas` method `dataframe.as_matrix()` does exactly that, so we will slice
    the dataset to include the training columns and construct such a matrix. Additionally,
    we will normalize the matrix to have each grayscale value between 0 and 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Likewise, we obtain the `labels` vector by transforming the `pandas` series
    into a vector of digits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have our training matrix and labels, we are ready to fit our model. We
    do this by simply calling `fit()` and providing the labeled training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The training will end with the loss and accuracy metrics on the training dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: In order to determine whether our model is overfitting (that is, it just learns
    how to classify the images in our training dataset but fails to generalize over
    new images), we need to test our model in the testing dataset. For this, we will
    perform the same transformations we made on our training dataset, but for the
    test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `evaluate()` function of our model will provide accuracy evaluation metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that our simple model is, in fact, fairly accurate. Let''s examine a few
    images in the testing dataset to see how the prediction matches the actual digit.
    For doing this, we will plot the images and compare them to the predicted digit
    by performing the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will define a function that obtains the predicted label for a particular
    row (`index`) on our testing dataset matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '`model.predict()` will obtain the predictions given a matrix of features. In
    this case, we just need one single row, so we slice our matrix into a single row
    to obtain the prediction for just the index in question. The predictions will
    be a vector of 10 components, each representing the strength of each digit. We
    use the `argmax` function to find the digit that maximizes the strength (that
    is, finding the most probable digit).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we define a function, `show_image()`, which, given an index, will plot
    the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We rely on the `PIL` library to perform the plotting. In order to plot the image,
    we need to denormalize our values back to the 0-255 range and reshape the 784
    pixels into a 28x28 image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s examine a few instances in the following screenshots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/39b4fed9-a53d-4633-8af1-6d0fd46d561b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And the second instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/757408e6-a4fa-48d0-9342-1d07f478acb2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following images were not able to be recognized correctly by the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bf05441b-f53d-491c-b1dc-093e51b57567.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And the second instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/490e171f-14d4-4147-ac00-87a704f37b3c.png)'
  prefs: []
  type: TYPE_IMG
- en: You may probably agree that even a human could make similar mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: So, how can we build a service on top of our `model`?
  prefs: []
  type: TYPE_NORMAL
- en: 'One simple way to do this is to create an `estimator` instance from our `model`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Recall that `LinearRegressor` we used in the previous section was also an `estimator` instance,
    so the same process for training, serializing, and serving the model would apply
    starting from this `estimator` instance.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we went through the process of creating two different TensorFlow
    models: one using the high-level library of estimators, and the other using Keras
    to build a custom neural network. In the process, we also showed how SageMaker
    can seamlessly handle the training and serving of TensorFlow models.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, *Image Classification and Detection with SageMaker*,  we
    will show how to use deep learning out-of-the-box on AWS to detect and recognize
    images.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the questions for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the difference between an epoch, batch, and step?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How would you design a network that would be able to provide recommendations
    for the theme park dataset considered in [Chapter 6](c940bfe6-b849-4179-b8f8-65e5d44652d6.xhtml),
    *Analyzing Visitor Patterns to Make Recommendations*?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How would you build a network that is capable of classifying the ads in [Chapter
    5](ccd8e969-f651-4fb9-8ef2-026286577e70.xhtml), *Customer Segmentation Using Clustering
    Algorithms*, as clicks/not-clicks?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
