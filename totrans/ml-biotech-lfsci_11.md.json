["```py\nimport nltk\nimport scipy\n```", "```py\nparagraph = \"\"\"Biotechnology is a broad area of biology, involving the use of living systems and organisms to develop or make products. Depending on the tools and applications, it often overlaps with related scientific fields. In the late 20th and early 21st centuries, biotechnology has expanded to include new and diverse sciences, such as genomics, recombinant gene techniques, applied immunology, and development of pharmaceutical therapies and diagnostic tests. The term biotechnology was first used by Karl Ereky in 1919, meaning the production of products from raw materials with the aid of living organisms.\"\"\"\n```", "```py\nfrom nltk.tokenize import sent_tokenize\nnltk.download('popular')\nsentences = sent_tokenize(paragraph)\nprint(sentences)\n```", "```py\nfrom nltk.tokenize import word_tokenize\nwords = word_tokenize(sentences[0])\nprint(words)\n```", "```py\ntokens = word_tokenize(sentences[0])\ntags = nltk.pos_tag(tokens)\nprint(tags)\n```", "```py\nfreqdist = nltk.FreqDist(word_tokenize(paragraph))\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(10,3))\nplt.xlabel(\"Samples\", fontsize=20)\nplt.xticks(fontsize=14)\nplt.ylabel(\"Counts\", fontsize=20)\nplt.yticks(fontsize=14)\nsns.set_style(\"darkgrid\")\nfreqdist.plot(30,cumulative=False) \n```", "```py\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nnltk.download('punkt')\nnltk.download('stopwords')\nimport re\nSTOP_WORDS = stopwords.words()\ndef cleaner(text):\n    text = text.lower() #Convert to lower case\n    text = re.sub(\"[^a-zA-Z]+\", ' ', text) # Only keep text, remove punctuation and numbers\n    text_tokens = word_tokenize(text) #Tokenize the words\n    tokens_without_sw = [word for word in text_tokens if not word in STOP_WORDS] #Remove the stop words\n    filtered_sentence = (\" \").join(tokens_without_sw) # Join all the words or tokens back to a single string\n    return filtered_sentence\n```", "```py\nclean_paragraph = cleaner(paragraph)\nclean_paragraph\n```", "```py\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(10,3))\nplt.xlabel(\"Samples\", fontsize=20)\nplt.xticks(fontsize=14)\n\nplt.ylabel(\"Counts\", fontsize=20)\nplt.yticks(fontsize=14)\n\nsns.set_style(\"darkgrid\")\nfreqdist.plot(30,cumulative=False)\n```", "```py\nimport spacy\nspacy_paragraph = nlp(paragraph)\nspacy_paragraph = nlp(paragraph)\nprint([(X.text, X.label_) for X in spacy_paragraph.ents])\n```", "```py\nfrom spacy import displacy\ndisplacy.render(nlp(str(sentences)), jupyter=True, style='ent')\n```", "```py\ndisplacy.render(nlp(str(sentences[0])), style='dep', jupyter = True, options = {'distance': 120})\n```", "```py\n    from pymed import PubMed\n    pubmed = PubMed()\n    ```", "```py\n    query = \"monoclonal antibody\"\n    results = pubmed.query(query, max_results=100)\n    ```", "```py\n    articleList = []\n    for article in results:\n        articleDict = article.toDict()\n        articleList.append(articleDict)\n    ```", "```py\n    df = pd.DataFrame(articleList)\n    df.head()\n    ```", "```py\nsns.displot(df.abstract.str.len(), bins=25)\nsns.displot(df.title.str.len(), bins=25)\n```", "```py\ndf[\"text\"] = df[\"title\"] + \" \" + df[\"abstract\"]\ndf[[\"title\", \"abstract\", \"text\"]]\n```", "```py\ndf.text.str.split(expand=True).stack().value_counts()\n```", "```py\nfrom nltk.corpus import stopwords\nSTOP_WORDS = stopwords.words()\ndef cleaner(text):\n    if type(text) == str:\n        text = text.lower()\n        text = re.sub(\"[^a-zA-Z]+\", ' ', text)\n        text_tokens = word_tokenize(text)\n        tokens_without_sw = [word for word in text_tokens if not word in STOP_WORDS]\n        filtered_sentence = (\" \").join(tokens_without_sw)\n        return filtered_sentence\n```", "```py\ncleaner(\"Biotech in 2021 is a wonderful field to work and study in!\")\n```", "```py\ndf[\"clean_text\"] = df[\"text\"].apply(lambda x: cleaner(x))\n```", "```py\ndf[[\"text\", \"clean_text\"]].head()\n```", "```py\n    from wordcloud import WordCloud, STOPWORDS\n    plt.figure(figsize=(20,10))\n    # Drop nans\n    df2 = df[[\"clean_text\"]].dropna()\n    # Create word cloud\n    wordcloud = WordCloud(width = 5000, \n                          height = 3000, \n                          random_state=1, \n                          background_color='white', \n                          colormap='Blues', \n                          collocations=False, \n                          stopwords = STOPWORDS).generate(' '.join(df2['clean_text']))\n    ```", "```py\n    plt.figure( figsize=(15,10) )\n    plt.imshow(wordcloud)\n    ```", "```py\ndef dataset_generator(query, num_results, ):\n    results = pubmed.query(query, max_results=num_results)\n    articleList = []\n    for article in results:\n        articleDict = article.toDict()\n        articleList.append(articleDict)\n    print(f\"Found {len(articleList)} results for the query '{query}'.\")\n    return pd.DataFrame(articleList)\n```", "```py\ndf1 = dataset_generator(\"monoclonal antibodies\", 600)\ndf2 = dataset_generator(\"machine learning\", 600)\ndf3 = dataset_generator(\"covid-19\", 600)\ndf4 = dataset_generator(\"particle physics\", 600)\ndf = pd.concat([df1, df2, df3, df4])\n```", "```py\ndf = df[[\"title\", \"abstract\"]]\n```", "```py\ndf[\"text\"] = df[\"title\"] + \" \" + df[\"abstract\"]\ndf = df.dropna()\nprint(df.shape)\n```", "```py\ndef cleaner(text):\n    if type(text) == str:\n        text = text.lower()\n        text = re.sub(\"[^a-zA-Z]+\", ' ', text)\n        text_tokens = word_tokenize(text)\n        tokens_without_sw = [word for word in text_tokens if not word in STOP_WORDS]\n        filtered_sentence = (\" \").join(tokens_without_sw)\n        return filtered_sentence\ndf[\"text\"] = df[\"text\"].apply(lambda x: cleaner(x))\n```", "```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n```", "```py\nvectors = TfidfVectorizer(stop_words=\"english\", max_features=5500)\nvectors.fit(df.text.values)\nfeatures = vectors.transform(df.text.values)\n```", "```py\nfrom sklearn.cluster import MiniBatchKMeans\ncls = MiniBatchKMeans(n_clusters=4)\ncls.fit(features)\n```", "```py\ndf[\"cluster\"] = cls.predict(features)\ndf[[\"text\", \"cluster\"]].head()\n```", "```py\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca_features_2d = pca.fit_transform(features.toarray())\npca_features_2d_centers = pca.transform(cls.cluster_centers_)\n```", "```py\ndf[\"pc1\"], df[\"pc2\"] = pca_features_2d[:,0], pca_features_2d[:,1]\ndf[[\"text\", \"cluster\", \"pc1\", \"pc2\"]].head()\n```", "```py\nplt.figure(figsize=(15,8))\nnew_cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"mycmap\", colors)\nplt.scatter(df[\"pc1\"], df[\"pc2\"], c=df[\"cluster\"], cmap=new_cmap)\nplt.scatter(pca_features_2d_centers[:, 0], pca_features_2d_centers[:,1], marker='*', s=500, c='r')\nplt.xlabel(\"PC1\", fontsize=20)\nplt.ylabel(\"PC2\", fontsize=20)\n```", "```py\n    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n    vectors = TfidfVectorizer(max_features=5500, stop_words=\"english\")\n    nmf_features = vectors.fit_transform(df.text)\n    ```", "```py\n    from sklearn.decomposition import NMF\n    n_topics = 10\n    cls = NMF(n_components=n_topics)\n    cls.fit(features)\n    ```", "```py\n    num_topic_words = 3\n    feature_names = vectors.get_feature_names()\n    for i, j in enumerate(cls.components_):\n        print(i, end=' ')\n        for k in j.argsort()[-1:-num_topic_words-1:-1]:\n            print(feature_names[k], end=' ')\n    ```", "```py\n    AWS_ACCESS_KEY_ID = \"add-access-key-here\"\n    AWS_SECRET_ACCESS_KEY = \"add-secret-access-key-here\"\n    AWS_REGION = \"us-east-2\"\n    s3_client = boto3.client('s3', aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY, region_name=AWS_REGION)\n    ```", "```py\n    with open(\"Monoclonal Production Article.pdf\", \"rb\") as f:\n        s3_client.upload_fileobj(f, \"biotech-machine-learning\", \"pdfs/Monoclonal Production Article.pdf\")\n    ```", "```py\n    textract_client = boto3.client('textract', aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY, region_name=AWS_REGION)\n    ```", "```py\n    response = textract_client.start_document_text_detection(\n                       DocumentLocation={'S3Object': {'Bucket': \"biotech-machine-learning\", 'Name': \"pdfs/Monoclonal Production Article.pdf\"} })\n    ```", "```py\n    results = textract_client.get_document_text_detection(JobId=response[\"JobId\"])\n    ```", "```py\n    documentText = \"\"\n    for item in results[\"Blocks\"]:\n        if item[\"BlockType\"] == \"LINE\":\n            documentText = documentText + item[\"Text\"]\n    ```", "```py\ncomprehend_client = boto3.client('comprehend', aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY, region_name=AWS_REGION)\n```", "```py\nresponse = comprehend_client.detect_entities(\n    Text=documentText[:5000],\n    LanguageCode='en',\n)\nprint(response[\"Entities\"])\n```", "```py\npd.DataFrame(response[\"Entities\"]).sort_values(by='Score', ascending=False).head()\n```", "```py\nresponse = comprehend_client.detect_key_phrases(\n    Text=documentText[:5000],\n    LanguageCode='en',\n)\nresponse[\"KeyPhrases\"][0]\n```", "```py\nresponse = comprehend_client.detect_sentiment(\n    Text=documentText[:5000],\n    LanguageCode='en',\n)\nprint(response)\n```", "```py\nresponse = comprehend_client.detect_dominant_language(\n    Text=documentText[:5000],\n)\nresponse\n```", "```py\nimport scipy\nimport torch\nimport pandas as pd\nfrom sentence_transformers import SentenceTransformer, util\n```", "```py\nmodel = SentenceTransformer('msmarco-distilbert-base-v4')\n```", "```py\ndatabase = df[\"abstract\"].values \n```", "```py\ndatabase_embeddings = model.encode(database)\n```", "```py\nquery = \"One of the best discoveries were monoclonal antibodies\"\nquery_embedding = model.encode(query)\n```", "```py\nimport scipy\ncos_scores = util.pytorch_cos_sim(query_embedding, \n                             database_embeddings)[0]\n```", "```py\ndef askQuestion(query, top_k):\n    print(f\"#########################################\")\n    print(f\"#### {query} ####\")\n    print(f\"#########################################\")\n    query_embedding = model.encode(query, convert_to_tensor=True)\n    cos_scores = util.pytorch_cos_sim(query_embedding, \n                 database_embeddings)[0]\n    top_results = torch.topk(cos_scores, k=top_k)\n\n    for score, idx in zip(top_results[0], top_results[1]):\n        print(\"#### Score: {:.4f}\".format(score))\n        print(\"#### Title: \", df.loc[float(idx)].title)\n        print(\"#### Abstract: \", df.loc[float(idx)].abstract)\n        print(\"#################################\")\n\n```", "```py\nquery = ' What is known about the removal of harmful cyanobacteria?\naskQuestion(query, 5)        \n```"]