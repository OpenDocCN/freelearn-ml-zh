- en: Chapter 5. Data Preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After providing solid foundations for an understanding of the two basic linear
    models for regression and classification, we devote this chapter to a discussion
    about the data feeding the model. In the next pages, we will describe what can
    routinely be done to prepare the data in the best way and how to deal with more
    challenging situations, such as when data is missing or outliers are present.
  prefs: []
  type: TYPE_NORMAL
- en: Real-world experiments produce real data, which, in contrast to synthetic or
    simulated data, is often very varied. Real data is also quite messy, and frequently
    it proves wrong in ways that are obvious and some that are, initially, quite subtle.
    As a data practitioner, you will almost never find your data already prepared
    in the right form to be immediately analyzed for your purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Writing a compendium of bad data and its remedies is outside the scope of this
    book, but our intention is to provide you with the basics to help you manage the
    majority of common data problems and correctly feed your algorithm. After all,
    the commonly known acronym **garbage in, garbage out** (**GIGO**) is a truth that
    we have to face and accept.
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout this chapter, we will therefore discover a variety of topics, Python
    classes, and functions that will allow you to:'
  prefs: []
  type: TYPE_NORMAL
- en: Properly scale numeric features and have an easier time not just comparing and
    interpreting coefficients but also when dealing with unusual or missing values
    or with very sparse matrices (very common in textual data processing)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Turn qualitative features into numeric values that can be accepted by a regression
    model and correctly transformed into predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transform numeric features in the smartest possible way to convert non-linear
    relationships in your data into linear ones
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determine what to do when important data is missing to estimate a replacement
    or even just let the regression manage the best solution by itself
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Repair any unusual or strange value in your data and make your regression model
    always work properly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Numeric feature scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 3](part0023_split_000.html#LTSU2-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 3. Multiple Regression in Action"), *Multiple Regression in Action*,
    inside the feature scaling section, we discussed how changing your original variables
    to a similar scale could help better interpret the resulting regression coefficients.
    Moreover, scaling is essential when using gradient descent-based algorithms because
    it facilitates quicker converging to a solution. For gradient descent, we will
    introduce other techniques that can only work using scaled features. However,
    apart for the technical requirements of certain algorithms, now our intention
    is to draw your attention to how feature scaling can be helpful when working with
    data that can sometimes be missing or faulty.
  prefs: []
  type: TYPE_NORMAL
- en: 'Missing or wrong data can happen not just during training but also during the
    production phase. Now, if a missing value is encountered, you have two design
    options to create a model sufficiently robust to cope with such a problem:'
  prefs: []
  type: TYPE_NORMAL
- en: Actively deal with the missing values (there is a paragraph in this chapter
    devoted to this)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Passively deal with it and:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your system throws an error and everything goes down (and remains down till
    the problem is solved)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Your system ignores the missing data and computes the values that are not missing
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is certainly worrying that your prediction system can get struck and could
    stop, but ignoring it and summing the present values could produce highly biased
    results. If your regression equation has been designed to work with all its variables,
    then it cannot work properly when some data is missing. Anyway, let''s again recall
    the linear regression formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Numeric feature scaling](img/00089.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you may guess, the bias coefficient is actually there to stay; it will always
    appear, whatever the situation with your predictors is. Consequently, even in
    an extreme case, such as when all the *X* are missing, if you standardize your
    variables thus they have zero mean.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see this in practice and discover how properly rescaling your predictors
    can help to fix missing values, allow advanced optimization techniques such as
    gradient descent, regularization, and stochastic learning (more about the latter
    two in later chapters), and easily detect outlying and anomalous values.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s upload our basic packages and functions for the analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Please notice that the Boston dataset is also reloaded. We refer to *y* as the
    target variable and to *X* as the predictors' array.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Since we would also like to have a test on the logistic regression, we now transform
    the target variable into a binary response by putting all values above the score
    of 25 at level "1".
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: After this operation, our qualitative response variable is named `yq`.
  prefs: []
  type: TYPE_NORMAL
- en: Mean centering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For all rescaling operations, we suggest the functions to be found in the `preprocessing`
    module of the Scikit-learn package. In particular, we will be using `StandardScaler`
    and `MinMaxScaler`. Like all classes in Scikit-learn, they both have a `fit` method
    that will record and store the parameters that allow correct scaling. They also
    feature a `transform` method that could be immediately applied on the same data
    (the `fit_transform` method will also do the trick) or on any other data, for
    example data used for validation, testing, or even, later on, production.
  prefs: []
  type: TYPE_NORMAL
- en: The `StandardScaler` class will rescale your variables by removing the mean,
    an action also called centering. In fact, in your training set the rescaled variables
    will all have zero mean and the features will be forced to the unit variance.
    After fitting, the class will contain the `mean_` and `std_` vectors, granting
    you access to the means and standard deviations that made the scaling possible.
    Therefore, in any following set for testing purpose or predictions in production,
    you will be able to apply the exact same transformations, thus maintaining the
    data consistency necessary for the algorithm to work exactly.
  prefs: []
  type: TYPE_NORMAL
- en: The `MinMaxScaler` class will rescale your variables setting a new minimum and
    maximum value in the range pointed out by you. After fitting, `min_` and `scale_`
    will report the minimum values (subtracted from the original variables) and the
    scale used for dividing your variables to have the intended maximum values, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you reuse one of the two classes, after being trained, on other new data,
    the new variables might have different maximum and minimum values, causing the
    resulting transformed variables to be off-scale (above maximum or below minimum,
    or with an anomalous value). When this happens, it is important to check if the
    new data has anomalous values and question whether we used the correct data for
    the training phase when we defined the transformations and the coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now upload both the scaling classes and get a remainder of the coefficients
    and intercept value when fitting a linear regression on the Boston dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![Mean centering](img/00090.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you get your results from your Jupyter Notebook in scientific notation, it
    could be helpful to first use `import numpy as np` and then `np.set_printoptions(precision=5,
    suppress=True)`.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, let's notice the intercept. Given the linear regression formula,
    we can expect that to be the regression output when all predictors are zero. Let's
    also have a look at the minimum values to check if they are negative, zero, or
    positive.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![Mean centering](img/00091.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Given the range in our variables, there could never be a situation when all
    the predictors are zero, implying that the intercept, though still functional
    and essential for the proper working of our model, is not representing any really
    expected value.
  prefs: []
  type: TYPE_NORMAL
- en: Now, as a first scaling operation, let's just center the variables, that is,
    remove the mean, and see if this action changes something in our linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![Mean centering](img/00092.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Though the coefficients have remained the same, now the intercept is **22.533**,
    a value that has a particular meaning in our Boston Housing prices problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Having the intercept valued as the average target value means that when one
    or more values are missing we expect them to automatically get a zero value if
    we centered the variable, and our regression will naturally tend to output the
    average value of the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: Standardization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At this point, we can also try scaling everything to unit variance and check
    the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![Standardization](img/00093.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As expected, now the coefficients are different, and each one now represents
    the unit change in the target after a modification in the predictors' equivalent
    to a standard deviation. However, the scales are not fully comparable if the distributions
    of our predictors are not normal (standardization implies a normal bell-shaped
    distribution), yet we can now compare the impact of each predictor and allow both
    the automatic handling of missing values and the correct functioning of advanced
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Normalization rescales as standardization, by acting on ranges of the predictors,
    but it has different properties. In fact, when using normalization the zero value
    is the minimum value in the range of values of each predictor. That means that
    zero doesn't represent the mean anymore. Moreover, rescaling between the maximum
    and the minimum can become misleading if there are anomalous values at either
    one of the extremities (most of your values will get squeezed around a certain
    region in `[0,1]`, usually in the center of the range).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![Normalization](img/00094.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Applying the `MinMaxScaler` in a range of 0 and 1 drastically changes both the
    coefficients and the intercept, but this could be acceptable under certain circumstances.
    In fact, when working with big data derived from textual data or logs, we sometimes
    realize that the matrices we are working on are not especially populated with
    values, zero often being the most frequent value to be encountered. To speed up
    the calculations and allow huge matrices to be kept in memory, matrices are stored
    in a sparse format.
  prefs: []
  type: TYPE_NORMAL
- en: Sparse matrices do not occupy all the memory necessary for their size, they
    just store coordinates and non-zero values. In such situations, standardizing
    the variables would change zero to the mean and a large number of previously zero
    cells would have to be defined, causing the matrix to occupy much more memory.
    Scaling between zero and one allows taking values in a comparable order and keeping
    all previously zero entries, thus not modifying the matrix dimensions in memory.
  prefs: []
  type: TYPE_NORMAL
- en: The logistic regression case
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A special discussion has to be devoted to logistic regression. As we illustrated
    in the previous chapter, in logistic regression we model the odds ratio of the
    probability of response. We can use the standardized coefficients as a trick to
    face missing data, as seen with linear regression, but things then turn out to
    be a bit different from when we try to guess a target numeric value in linear
    regression analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s explore an example that could clarify the matter. We will be using the
    Boston dataset to demonstrate the logistic regression case and we will use the
    `yq` vector, previously defined, as a response variable. For the logistic regression,
    we won''t be using the Scikit-learn implementation this time but rather the Statsmodels
    package, so we can easily show some insights about the coefficients in the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![The logistic regression case](img/00095.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Using the standardized predictors, as in a linear regression, we can interpret
    the coefficients in terms of the same scale and consider the intercept as the
    response when all predictors have an average value. Contrary to linear regression,
    in logistic regression a unit change in predictors changes the odds ratio of the
    response of a quantity equivalent to the exponentiation of the coefficients themselves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We recall how odds ratios are calculated: given a certain probability *p* of
    an event, an odds ratio is the ratio between *p* and its complement to 1, odds
    *ratio = p / (1−p)*. When the odds ratio is equivalent to 1, our probability is
    exactly 0.5\. When the probability is above 0.5 the odds ratio is above 1; when,
    on the contrary, our probability is less than 0.5 then the odds ratio is below
    1\. By applying the natural logarithm (as logistic regression does), the values
    are distributed around the zero value (50% probability). Clearly working with
    probabilities is more intuitive therefore, a simple transformation, the sigmoid
    transformation, will convert the coefficients to more understandable probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Transforming the intercept into a probability using the sigmoid function, we
    obtain `0.045`, which is the probability of a house value above `25` when all
    predictors bear the mean value. Please notice that such a probability is different
    from the average probability in the sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In fact, that''s the baseline probability of having a house value above `25`,
    taking account of any possible value of the predictors. What we extracted from
    logistic regression is instead a particular probability, not a general one. You
    can actually get a comparable likelihood when you model a logistic regression
    with only an intercept (the so-called null model), thus allowing the predictors
    to vary freely:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![The logistic regression case](img/00096.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Qualitative feature encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Beyond numeric features, which have been the main topic of this section so far,
    a great part of your data will also comprise qualitative variables. Databases
    especially tend to record data readable and understandable by human beings; consequently,
    they are quite crowded by qualitative data, which can appear in data fields in
    the form of text or just single labels explicating information, such as telling
    you the class of an observation or some of its characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a better understanding of qualitative variables, a working example could
    be a weather dataset. Such a dataset describes conditions under which you would
    want to play tennis because of weather information such as outlook, temperature,
    humidity, and wind, which are all kinds of information that can be rendered by
    numeric measurements. However, you will easily find such data online and recorded
    in datasets with their qualitative translations such as `sunny` or `overcast`,
    rather than numeric satellite/weather-station measurements. We will work on this
    kind of data to demonstrate how it is still possible to transform it in such a
    way that it can be included effectively into a linear model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'A linear regressor can analyze qualitative data only after having been properly
    transformed into a numeric value. A common type of qualitative data comprises
    nominal variables, which are expressed by a limited set of textual labels. For
    instance, a nominal variable could be the color of a product or the outlook of
    the weather (as in our weather example). The textual values that a variable can
    assume are called levels; in our example, outlook has three levels: `sunny`, `overcast`
    and `rainy`, all of them represented as strings.'
  prefs: []
  type: TYPE_NORMAL
- en: If we think that any of these can be present or not (each label excludes the
    other), we can easily transform each nominal variable with n levels into n distinct
    variables, each one telling us if a certain characteristic is present or not.
    If we use the value of 1 for denoting the presence of the level and 0 for its
    absence (like binary coding, such as in computers), we will have a working transformation
    of qualitative information into a numeric one (technically it is a Boolean, but
    for practical purposes we model it as a 0 – 1 numeric integer). Such transformed
    variables are called indicator or binary variables in machine learning terminology,
    whereas in statistics they are described as **dichotomies** (a more technical
    term) or dummies variables. They act in a regression formula as modifiers of the
    intercept when the level is present.
  prefs: []
  type: TYPE_NORMAL
- en: When the levels of a variable are ordered, there's another possible transformation.
    For instance, they can be qualitative labels such as good, average, acceptable,
    and bad. In such an occurrence, the labels can also be converted into growing
    or decreasing numbers following the ordering of the meaning of the labels. Therefore,
    in our example, good could be 3, average 2, acceptable 1, and bad 0\. Such encoding
    directly translates a qualitative variable into a numeric one, but it works only
    with labels that can be ordered (that is, where you can define *greater than*
    and *lower than* relations). The transformation implies, since a single coefficient
    will be calculated in the regression model for all the levels, that the difference
    in the outcome passing from good to average is the same as passing from acceptable
    to bad. In reality, this is often not true due to non-linearity. In such a case,
    binary encoding is still the best solution.
  prefs: []
  type: TYPE_NORMAL
- en: Dummy coding with Pandas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The fastest way to transform a set of qualitative variables into binary ones
    is using a Pandas function, `get_dummies`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'After transforming all your data into a Pandas DataFrame, it is quite easy
    to call single variables and single cases to be transformed into binary variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Pandas can really transform all your variables in a breeze; all you need is
    to point out the DataFrame you want to entirely transform or specify just the
    variables to be converted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'After the transformation, a regression model can immediately analyze the resulting
    new DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![Dummy coding with Pandas](img/00097.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Some regression methods do not really like you to have all the binary variables
    expressing a qualitative variable (but this is not so in our case). Certain optimization
    methods do not love perfect collinearity, such as in the case of a complete binarization
    (in fact, if you know all the other dichotomies, then the remaining ones can be
    perfectly guessed by summing the others—it has value 1 when the sum of the others
    is zero). In such a case, you just need to drop a level of your choice from each
    set of binary variables. By doing so, the omitted coefficient will be incorporated
    into the intercept and the regression model will just work as before, though with
    a different set of variables and coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![Dummy coding with Pandas](img/00098.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '`get_dummies` presents only one drawback: it constructs the binary variables
    directly, reading the levels from the dataset you are converting. Consequently,
    if you first build a set of binary variables from a sample and then another one
    from another sample of your same data, it can produce different transformed datasets
    because of rare levels not appearing in one of the samples.'
  prefs: []
  type: TYPE_NORMAL
- en: DictVectorizer and one-hot encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Scikit-learn package offers a way, though a bit less direct, to consistently
    transform your qualitative variables into numeric ones.
  prefs: []
  type: TYPE_NORMAL
- en: The `DictVectorizer` class can read datasets made of a list of dictionaries,
    properly transform the string label data into a set of binaries, and leave the
    numeric data untouched. If instead you already have qualitative variables coded
    as numeric types in your dataset, all you need is to transform them to string
    values before having them processed by `DictVectorizer`.
  prefs: []
  type: TYPE_NORMAL
- en: 'All you have to do is first create a dictionary representation of your dataset,
    as in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: A dictionary representation is in the form of a list of dictionaries whose keys
    are the variables' names and whose values are their numeric or label value. To
    obtain such a representation, you will need to duplicate your dataset, and that
    could represent a big limitation if you are working with little memory available.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the class keeps memory of the transformations and thus everything
    can be exactly replicated on any other data sample using the `transform` method,
    overcoming the limitation we've seen with the Pandas `get_dummies` method.
  prefs: []
  type: TYPE_NORMAL
- en: You can also easily visualize the transformations by calling the `features_names_`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: If the limit number of binarizations does not justify the entire conversion
    of the dataset into a dictionary representation, by using the `LabelEncoder` and
    `LabelBinarizer` class, available in the `preprocessing` package in Scikit-learn,
    you can encode and transform a single variable at a time.
  prefs: []
  type: TYPE_NORMAL
- en: '`LabelEncoder` turns the labels into numbers and `LabelBinarizer` transforms
    the numbers into dichotomies. The consistency of all such operations across different
    samples is guaranteed by the `fit` and `transforms` methods that are characteristic
    of all the classes present in Scikit-learn, where `fit` picks and records the
    parameters from data and the `transform` method applies it to new data afterwards.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s test a transformation on the outlook variable. We first convert the
    text labels into numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The assigned numbers are given by the position of the label in the list that
    you get using an `inverse_transform` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Or by just requiring the recorded classes, glancing at the `classes_` internal
    variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Once numerically encoded, `LabelBinarizer` can transform everything into indicator
    variables, allowing you to decide what values should be placed in the dichotomy.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, if you worry about missing values, you can encode the negative value
    as `−1`, leaving the missing case at `0` (in that case, the missing value will
    be passively taken in charge by the intercept as seen before).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Another great advantage of this method is that it allows sparse representations,
    thus saving memory when working with large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Feature hasher
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One-hot encoding is a powerful transformation that allows any kind of data to
    be represented just using a binary variable. Using the same approach, you can
    even transform text into variables that can be analyzed by a linear regression
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea is to transform any occurrence of a certain word in the text into
    a specific binary variable, so the model will assign a binary value connected
    to the presence of a word in the text. For instance, if you want to analyze the
    Latin motto *Nomina sunt consequentia rerum* (that means "names follow things"),
    you can force all the text to lowercase and enumerate all the distinct words present
    in the text by tokenizing them. By doing so, you intend to separate them (in our
    case, the tokenization is quite simple, we just split by spaces) in a way that
    it is often referred to as a **bag of words** (**BoW**) representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code just transforms all your textual data into a dictionary containing
    the lowercase words and their positional index in a vector of binary variables.
  prefs: []
  type: TYPE_NORMAL
- en: The length of this vector is the length of the dictionary, and each binary flag
    has unit value when the corresponding word is present in the analyzed text. Therefore,
    the vector for all our phrase is `[1,1,1,1]` and the vector for a phrase just
    containing the word `'rerum'` should be `[1,0,0,0]` because the positional index
    of the word is `0`.
  prefs: []
  type: TYPE_NORMAL
- en: Figuratively, you can imagine our vector as a line of lamps; each time, you
    turn on only those whose corresponding word is present in the text you are analyzing.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Transforming a word into an indicator is just a starting point. You can also
    count how many times a word appears in a text and normalize that count by considering
    the length of the text you are transforming. In fact, in longer text, certain
    words have a higher chance of appearing multiple times than in shorter ones. By
    normalizing the word count, for instance, in such a way that the sum of word counts
    cannot be over a certain number, it will appear like reducing all texts to the
    same length. These are just some of the possible transformations that are part
    of **natural language processing** (**NLP**), and they are viable for a linear
    regression model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Scikit-learn package offers a specialized class for automatically transforming
    text into vectors of binary variables; this is the `CountVectorizer class`. It
    allows the transformation of a list or array of textual data into a sparse matrix.
    Setting the `binary` parameter to `True`, when transforming the data with just
    binary encoding, represents the sparse matrix as a set of unit values in correspondence
    to the texts where a word is present. As a simple example, we can encode this
    series of texts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The only common word in the corpus (the term for a collection of documents
    subject to a linguistic analysis, so it is common to have a bilingual corpus or
    even a more heterogeneous one) is `''dog''`. This should be reflected in our matrix;
    in fact, just a single column always has the unit value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: To visualize the resulting matrix as an output, which would otherwise just be
    made up of the coordinates where the unit values are in the matrix, we need to
    make the resulting sparse matrix dense using the `.todense()` method.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Being a toy dataset, such transformation won't imply much in our example. Beware
    of doing the same when your corpus is large because that could cause an out-of-memory
    error on your system.
  prefs: []
  type: TYPE_NORMAL
- en: 'We notice that the third column has three units, so we imagine that it could
    represent the word `''dog''`. We can verify that by requiring a list representing
    the dictionary and the positional arrangement of words using the `.get_feature_names()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Leveraging the ability to quickly build dictionaries of words, you can transform
    and use it even for the prediction of text.
  prefs: []
  type: TYPE_NORMAL
- en: 'The only trouble you can incur using such a representation is when you encounter
    a word never seen before. Let''s see what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Such behavior should actually have been expected. Since no word of the phrase
    has been encountered before, it doesn't have any space to fit in the vectorized
    text.
  prefs: []
  type: TYPE_NORMAL
- en: A quick and working solution would be to define a very large sparse vector (which
    until filled with data won't occupy much space, no matter the dimensions) and
    to use the specific characteristics of hash functions to deterministically assign
    a position to every word in the vector, without having the need to observe the
    word itself before the assignment. This is also called the **hashing trick** and
    can be applied using the Scikit-learn `HashingVectorizers`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The `HashingVectorizer` class has quite a few options for you to explore, especially
    for text treatment, such as allowing more sophisticated tokenizing (even custom
    ones), the removal of common words, stripping accents, and the parsing of different
    encodings.
  prefs: []
  type: TYPE_NORMAL
- en: In a replica of what we have done before with `CountVectorizer`, we fixed an
    output vector of 11 elements. By doing so, we can notice and discuss two relevant
    characteristics of the preceding output.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, clearly the position of the words is different (it depends on the hash
    function), and we cannot get back a dictionary of what word is in what position
    (but we can be confident that the hashing function has done its job properly).
    Now, we have no fear of vectorizing any previously unseen new text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The second thing is that, by observing well the distributions of unit values
    in the matrix, you have values concentrating on certain positions, whereas others
    are left empty. This is due to *the collision problem* with hash functions when
    bounded in a limited number of positions (actually we set the `n_features` parameter
    to 11 for ease of understanding, though in a real analysis it is good practice
    to set it to higher figures).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To avoid any unwanted collision, a good `n_features` value is between *2**21*
    and *2**24*, depending on the expected variety of text (the more variety, the
    larger the vector should be).
  prefs: []
  type: TYPE_NORMAL
- en: Numeric feature transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Numeric features can be transformed, regardless of the target variable. This
    is often a prerequisite for better performance of certain classifiers, particularly
    distance-based. We usually avoid ( besides specific cases such as when modeling
    a percentage or distributions with long queues) transforming the target, since
    we will make any pre-existent linear relationship between the target and other
    features non-linear.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will keep on working on the Boston Housing dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'As before, we fit the model using `LinearRegression` from Scikit-learn, this
    time measuring its R-squared value using the `r2_score` function from the `metrics`
    module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Observing residuals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Residuals are what's left from the original response when the predicted value
    is removed. It is numeric information telling us what the linear model wasn't
    able to grasp and predict by its set of coefficients and intercepts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Obtaining residuals when working with Scikit-learn requires just one operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The residuals of a linear regression always have mean zero and their standard
    deviation depends on the size of the error produced. Residuals can provide insight
    on an unusual observation and non-linearity because, after telling us about what's
    left, they can direct us to specific troublesome data points or puzzling patterns
    in data.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the specific problem of detecting non-linearity, we are going to use a
    plot based on residuals called the **partial residual plot**. In this plot, we
    compare the regression residuals summed with the values derived from the modeled
    coefficient of a variable against the original values of the variable itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '![Observing residuals](img/00099.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: After having calculated the residual of the regression, we decide to inspect
    one variable at a time. After picking up our selected variable, we create a partial
    residual by summing the residuals of the regression with the multiplication of
    the variable values multiplied by its coefficient. In such a way, we *extract*
    the variable from the regression line and we put it in the residuals. Now, as
    partial residuals, we have both the errors and the coefficient-weighted variable.
    If we plot it against the variable itself, we can notice whether there is any
    non-linear pattern. If there is one, we know that we should try some modification.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, there is some sign that the points bend after the value 2 of our
    variable, a clear non-linearity sign such as any bend or pattern different from
    an elongated, straight cloud of points. Square, inverse, logarithmic transformations
    can often solve such problems without adding new terms, such as when using the
    polynomial expansion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '![Observing residuals](img/00100.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Just notice how an inverse square transformation rendered the partial residual
    plot straighter, something that is reflected in a higher R-squared value, indicating
    an increased capacity of the model to capture the data distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a rule, the following transformations should always be tried (singularly
    or in combination) to find a fix for a non-linearity:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Function names | Functions |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Logarithmic | `np.log(x)` |'
  prefs: []
  type: TYPE_TB
- en: '| Exponential | `np.exp(x)` |'
  prefs: []
  type: TYPE_TB
- en: '| Squared | `x**2` |'
  prefs: []
  type: TYPE_TB
- en: '| Cubed | `x**3` |'
  prefs: []
  type: TYPE_TB
- en: '| Square root | `np.sqrt(x)` |'
  prefs: []
  type: TYPE_TB
- en: '| Cube root | `x**(1./3.)` |'
  prefs: []
  type: TYPE_TB
- en: '| Inverse | `1\. / x` |'
  prefs: []
  type: TYPE_TB
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Some of the transformations suggested in the preceding table won''t work properly
    after normalization or otherwise in the presence of zero and negative values:
    logarithmic transformation needs positive values above zero, square root won''t
    work with negative values, and inverse transformation won''t operate with zero
    values. Sometimes adding a constant may help (like in the case of `np.log(x+1)`).
    Generally, just try the possible transformations, according to your data values.'
  prefs: []
  type: TYPE_NORMAL
- en: Summarizations by binning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When it is not easy to figure out the exact transformation, a quick solution
    could be to transform the continuous numeric variable into a series of binary
    variables, thus allowing the estimation of a coefficient for each single part
    of the numeric range of the variable.
  prefs: []
  type: TYPE_NORMAL
- en: Though fast and convenient, this solution will increase the size of your dataset
    (unless you use a sparse representation of the matrix) and it will risk too much
    overfitting on your data.
  prefs: []
  type: TYPE_NORMAL
- en: First, you divide your values into equally spaced bins and you notice the edges
    of the bins using the `histogram` function from Numpy. After that, using the `digitize`
    function, you convert the value in their bin number, based on the bin boundaries
    provided before. Finally, you can transform all the bin numbers into binary variables
    using the previously present `LabelBinarizer` from Scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, all you have to do is replace the previous variable with this
    new set of binary indicators and refit the model for checking the improvement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Missing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Missing data appears often in real-life data, sometimes randomly in random occurrences,
    more often because of some bias in its recording and treatment. All linear models
    work on complete numeric matrices and cannot deal directly with such problems;
    consequently, it is up to you to take care of feeding suitable data for the algorithm
    to process.
  prefs: []
  type: TYPE_NORMAL
- en: Even if your initial dataset does not present any missing data, it is still
    possible to encounter missing values in the production phase. In such a case,
    the best strategy is surely that of dealing with them passively, as presented
    at the beginning of the chapter, by standardizing all the numeric variables.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As for as indicator variables, in order to passively intercept missing values,
    a possible strategy is instead to encode the presence of the label as `1` and
    its absence as `-1`, leaving the zero value for missing values.
  prefs: []
  type: TYPE_NORMAL
- en: When missing values are present from the beginning of the project, it is certainly
    better to deal with them explicitly, trying to figure out if there is any systematic
    pattern behind missing values. In Python arrays, upon which both the Pandas and
    Scikit-learn packages are built, missing values are marked by a special value,
    **Not a Number** (**NaN**), which is repeatable using the value available from
    the NumPy constant `nan`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating a toy array with a missing value is easy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Also discovering where missing values are in a vector (the result is a vector
    of Booleans):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Replacing all missing elements can be done easily by slicing or by the function
    `nan_to_num`, which turns every `nan` to zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Using slicing, you could decide to use something more sophisticated than a
    constant, such as the mean of valid elements in the vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Missing data imputation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Consistency of treatment between samples of data is essential when working with
    predictive models. If you replace the missing values with a certain constant or
    a particular mean, that should be consistent during both the training and the
    production phase. The Scikit-learn package offers the `Imputer` class in the `preprocessing`
    module which can learn a solution by the `fit` method and then consistently apply
    it by the `transform` one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start demonstrating it after putting some missing values in the Boston
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is quite unlikely that you will get the same result due to the random nature
    of the sampling process. Please notice that the exercise sets a seed so you can
    count on the same results on your PC.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now about a quarter of observations in the variable should be missing. Let''s
    use `Imputer` to replace them using a mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Imputer allows you to define any value as missing (sometimes in a re-elaborated
    dataset missing values could be encoded with negative values or other extreme
    values) and to choose alternative strategies rather than the mean. Other alternatives
    are **median** and **mode**. The median is useful if you suspect that outlying
    values are influencing and biasing the average (in house prices, some very expensive
    and exclusive houses or areas could be the reason). Mode, the most frequent value,
    is instead the optimal choice if you are working with discrete values (for instance
    a sequence of integer values with a limited range).
  prefs: []
  type: TYPE_NORMAL
- en: Keeping track of missing values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you suspect that there is some bias in the missing value pattern, by imputing
    them you will lose any trace of it. Before imputing, a good practice is to create
    a binary variable recording where all missing values were and to add it as a feature
    to the dataset. As seen before, it is quite easy using NumPy to create such a
    new feature, transforming the Boolean vector created by `isnan` into a vector
    of integers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The linear regression model will create a coefficient for this indicator of
    missing values and, if any pattern exists, its informative value will be captured
    by a coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: Outliers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After properly transforming all the quantitative and qualitative variables and
    fixing any missing data, what's left is just to detect any possible outlier and
    to deal with it by removing it from the data or by imputing it as if it were a
    missing case.
  prefs: []
  type: TYPE_NORMAL
- en: An outlier, sometimes also referred to as an anomaly, is an observation that
    is very different from all the others you have observed so far. It can be viewed
    as an unusual case that stands out, and it could pop up due to a mistake (an erroneous
    value completely out of scale) or simply a value that occurred (rarely, but it
    occurred). Though understanding the origin of an outlier could help to fix the
    problem in the most appropriate way (an error could be legitimately removed; a
    rare case could be kept or capped or even imputed as a missing case), what is
    of utmost concern is the effect of one or more outliers on your regression analysis
    results. Any anomalous data in a regression analysis means a distortion of the
    regression's coefficients and a limit on the ability of the model to correctly
    predict usual cases.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Despite the importance of controlling outliers, unfortunately practitioners
    often overlook this activity because, in contrast to the other preparations illustrated
    throughout the chapter, omitting to detect outliers won't stop the analysis you
    are working on and you will get your regression coefficients and results (both
    probably quite inexact). However, having an analysis run smoothly to the end doesn't
    mean that everything is fine with the analysis itself. An outlier can distort
    an analysis in two ways depending on whether the anomalous value is on the target
    variable or on the predictors.
  prefs: []
  type: TYPE_NORMAL
- en: In order to detect outliers, there are a few approaches, some based on the observation
    of variables taken singularly (the single-variable, or univariate, approach),
    and some based on reworking all the variables together into a synthetic measure
    (the multivariate approach).
  prefs: []
  type: TYPE_NORMAL
- en: 'The best single variable approach is based on the observation of standardized
    variables and on the plotting of box plots:'
  prefs: []
  type: TYPE_NORMAL
- en: Using standardized variables, everything scoring further than the absolute value
    of three standard deviations from the mean is suspect, though such a rule of thumb
    doesn't generalize well if the distribution is not normal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using boxplots, the **interquartile range** (shortened to **IQR**; it is the
    difference between the values at the 75^(th) and the 25^(th) percentile) is used
    to detect suspect outliers beyond the 75^(th) and 25^(th) percentiles. If there
    are examples whose values are outside the IQR, they can be considered suspicious,
    especially if their value is beyond 1.5 times the IQR's boundary value. If they
    exceed 3 times the IQR's limit, they are almost certainly outliers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Scikit-learn package offers a couple of classes for automatically detecting
    outliers using sophisticated approaches: `EllipticEnvelope` and `OneClassSVM`.
    Though a treatise of both these complex algorithms is out of scope here, if outliers
    or unusual data is the main problem with your data, we suggest having a look at
    this web page for some quick recipes you can adopt in your scripts: [http://scikit-learn.org/stable/modules/outlier_detection.html](http://scikit-learn.org/stable/modules/outlier_detection.html).
    Otherwise, you could always read our previous book *Python Data Science Essentials*,
    *Alberto Boschetti* and *Luca Massaron*, *Packt Publishing*.'
  prefs: []
  type: TYPE_NORMAL
- en: Outliers on the response
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step in looking for outliers is to check the response variable. In
    observing plots of the variable distribution and of the residuals of the regression,
    it is important to check if there are values that, because of a too high or too
    low value, are out of the main distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, unless accompanied by outlying predictors, outliers in the response
    have little impact on the estimated coefficients; however, from a statistical
    point of view, since they affect the amount of the root-squared error, they reduce
    the explained variance (the squared r) and inflate the standard errors of the
    estimate. Both such effects represent a problem when your approach is a statistical
    one, whereas they are of little concern for data science purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'To figure out which responses are outliers, we should first monitor the target
    distribution. We start by recalling the Boston dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'A `boxplot` function can hint at any outlying values in the target variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '![Outliers on the response](img/00101.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The box display and its whiskers tell us that quite a few values are out of
    the IQR, so they are suspect ones. We also notice a certain concentration at the
    value 50; in fact the values are capped at 50.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we can try to build our regression model and inspect the resulting
    residuals. We will standardize them using the Root Mean Squared error. An easy
    approach to implement though it is not the most precise, it is still enough good
    to reveal any significant problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '![Outliers on the response](img/00102.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Making a scatterplot of the values fitted by the regression against the standardized
    residuals, we notice there are a few outlying cases over three standard deviations
    from the zero mean. The capped values especially, clearly visible in the graph
    as a line of points, seem problematic.
  prefs: []
  type: TYPE_NORMAL
- en: Outliers among the predictors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we inspected the target variable, it is now time to have a look also at the
    predictors. If unusual observations were outliers in the target variable, similar
    cases in the predictors are instead named influential or high leverage observations
    because they can really make an impact on more than the **sum of squared errors**
    (**SSE**), this time influencing coefficients and the intercept—in a word, the
    entire regression solution (that's why they are so important to catch).
  prefs: []
  type: TYPE_NORMAL
- en: 'After standardizing, we start having a look at the distributions using boxplots:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '![Outliers among the predictors](img/00103.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '![Outliers among the predictors](img/00104.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: After observing all the boxplots, we can conclude that there are variables with
    restricted variance, such as **B**, **ZN**, and **CRIM**, which are characterized
    by a long tail of values. There are also some suspect cases from **DIS** and **LSTAT**.
    We can delimit all these cases by looking for the values above the represented
    thresholds, variable after variable, but it would be helpful to catch all of them
    at once.
  prefs: []
  type: TYPE_NORMAL
- en: '**Principal Component Analysis** (**PCA**) is a technique that can reduce complex
    datasets into fewer dimensions, the summation of the original variables of the
    dataset. Without delving too much into the technicalities of the algorithm, you
    just need to know that the new dimensions produced by the algorithm have decreasing
    explicatory power; consequently, plotting the top ones against each other is just
    like plotting all the dataset''s information. By glancing at such synthetic representations,
    you can spot groups and isolated points that, if very far from the center of the
    graph, are also quite influential on the regression model.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '![Outliers among the predictors](img/00105.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The first dimension created by PCA can explain 47% of the dataset''s information,
    the second and the third 11% and 9.5%, respectively (the `explained_variance_ratio_`
    method can provide you with such information). Now all we have to do is to plot
    the first dimension against the second and the third and look for lonely points
    away from the center because those are our high leverage cases to be investigated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '![Outliers among the predictors](img/00106.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '![Outliers among the predictors](img/00107.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Removing or replacing outliers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After being able to detect outliers and influential observations, we just need
    to discuss what we can do with them. You might believe it's OK just to delete
    them but, on the contrary, removing or replacing an outlier is something to consider
    carefully.
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, outlying observations may be justified by three reasons (their remedies
    change accordingly):'
  prefs: []
  type: TYPE_NORMAL
- en: They are outliers because they are rare occurrences, so they appear unusual
    with regard to other observations. If this is the case, removing the data points
    could not be the correct solution because the points are part of the distribution
    you want to model and they stand out just because of chance. The best solution
    would be to increase the sample number. If augmenting your sample size is not
    possible, then remove them or try to resample in order to avoid having them drawn.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some errors have happened in the data processing and the outlying observations
    are from another distribution (some data has been mixed, maybe from different
    times or another geographical context). In this case, prompt removal is called
    for.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The value is a mistake due to faulty input or processing. In such an occurrence,
    the value has to be considered as missing and you should perform an imputation
    of the now missing value to get a reasonable value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As a rule, just keep in mind that removing data points is necessary only when
    the points are different from the data you want to use for prediction and when,
    by removing them, you get direct confirmation that they had a lot of influence
    on the coefficients or on the intercept of the regression model. In all other
    cases, avoid any kind of selection in order to improve the model since it is a
    form of data snooping (more on the topic of how data snooping can negatively affect
    your models in the next chapter).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have dealt with many different problems that you may encounter
    when preparing your data to be analyzed by a linear model.
  prefs: []
  type: TYPE_NORMAL
- en: We started by discussing rescaling variables and understanding how new variables'
    scales not only permit a better insight into the data, but also help us deal with
    unexpectedly missing data.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we learned how to encode qualitative variables and deal with the extreme
    variety of possible levels with unpredictable variables and textual information
    just by using the hashing trick. We then returned to quantitative variables and
    learned how to transform in a linear shape and obtain better regression models.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we dealt with some possible data pathologies, missing and outlying
    values, showing a few quick fixes that, in spite of their simplicity, are extremely
    effective and performant.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, before proceeding to more sophisticated linear models, we just
    need to illustrate the data science principles that can help you obtain really
    good working predictive engines and not just mere mathematical curve fitting exercises.
    And that's precisely the topic of the next chapter.
  prefs: []
  type: TYPE_NORMAL
