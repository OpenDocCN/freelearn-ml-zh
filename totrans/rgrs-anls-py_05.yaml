- en: Chapter 5. Data Preparation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章。数据准备
- en: After providing solid foundations for an understanding of the two basic linear
    models for regression and classification, we devote this chapter to a discussion
    about the data feeding the model. In the next pages, we will describe what can
    routinely be done to prepare the data in the best way and how to deal with more
    challenging situations, such as when data is missing or outliers are present.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在为理解回归和分类的两个基本线性模型打下坚实基础之后，我们将本章用于讨论为模型提供数据。在接下来的几页中，我们将描述通常可以如何以最佳方式准备数据，以及如何处理更具挑战性的情况，例如数据缺失或存在异常值。
- en: Real-world experiments produce real data, which, in contrast to synthetic or
    simulated data, is often very varied. Real data is also quite messy, and frequently
    it proves wrong in ways that are obvious and some that are, initially, quite subtle.
    As a data practitioner, you will almost never find your data already prepared
    in the right form to be immediately analyzed for your purposes.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界的实验产生真实数据，与合成或模拟数据相比，真实数据通常变化很大。真实数据也相当混乱，并且经常以明显和某些初始时相当微妙的方式证明是错误的。作为数据从业者，你几乎永远不会找到已经以适合你分析目的的正确形式准备好的数据。
- en: Writing a compendium of bad data and its remedies is outside the scope of this
    book, but our intention is to provide you with the basics to help you manage the
    majority of common data problems and correctly feed your algorithm. After all,
    the commonly known acronym **garbage in, garbage out** (**GIGO**) is a truth that
    we have to face and accept.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 编写关于不良数据和其补救措施的汇编超出了本书的范围，但我们的目的是为你提供基础知识，帮助你管理大多数常见的数据问题，并正确地为你的算法提供数据。毕竟，众所周知的缩写词**垃圾输入，垃圾输出**（**GIGO**）是一个我们必须面对和接受的事实。
- en: 'Throughout this chapter, we will therefore discover a variety of topics, Python
    classes, and functions that will allow you to:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本章中，我们将发现各种主题、Python类和函数，这些将允许你：
- en: Properly scale numeric features and have an easier time not just comparing and
    interpreting coefficients but also when dealing with unusual or missing values
    or with very sparse matrices (very common in textual data processing)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确缩放数值特征，不仅更容易比较和解释系数，而且在处理异常或缺失值，或者处理非常稀疏的矩阵（在文本数据处理中非常常见）时也会更加容易
- en: Turn qualitative features into numeric values that can be accepted by a regression
    model and correctly transformed into predictions
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将定性特征转换为回归模型可以接受的数值，并正确地转换为预测
- en: Transform numeric features in the smartest possible way to convert non-linear
    relationships in your data into linear ones
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以最智能的方式转换数值特征，将数据中的非线性关系转换为线性关系
- en: Determine what to do when important data is missing to estimate a replacement
    or even just let the regression manage the best solution by itself
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定当重要数据缺失时应该做什么，以估计一个替代方案，甚至让回归自己管理最佳解决方案
- en: Repair any unusual or strange value in your data and make your regression model
    always work properly
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修复数据中的任何异常或奇怪值，并确保你的回归模型始终正常工作
- en: Numeric feature scaling
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数值特征缩放
- en: In [Chapter 3](part0023_split_000.html#LTSU2-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 3. Multiple Regression in Action"), *Multiple Regression in Action*,
    inside the feature scaling section, we discussed how changing your original variables
    to a similar scale could help better interpret the resulting regression coefficients.
    Moreover, scaling is essential when using gradient descent-based algorithms because
    it facilitates quicker converging to a solution. For gradient descent, we will
    introduce other techniques that can only work using scaled features. However,
    apart for the technical requirements of certain algorithms, now our intention
    is to draw your attention to how feature scaling can be helpful when working with
    data that can sometimes be missing or faulty.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](part0023_split_000.html#LTSU2-a2faae6898414df7b4ff4c9a487a20c6 "第3章。实际操作中的多元回归")，*实际操作中的多元回归*，特征缩放部分，我们讨论了如何将原始变量转换为相似尺度，这有助于更好地解释结果回归系数。此外，当使用基于梯度的算法时，缩放是必不可少的，因为它有助于更快地收敛到解决方案。对于梯度下降，我们将介绍其他只能通过缩放特征才能工作的技术。然而，除了某些算法的技术要求之外，现在我们的目的是引起你注意特征缩放在处理有时可能缺失或错误的数据时的帮助。
- en: 'Missing or wrong data can happen not just during training but also during the
    production phase. Now, if a missing value is encountered, you have two design
    options to create a model sufficiently robust to cope with such a problem:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失或错误的数据不仅可能在训练过程中发生，也可能在生产阶段发生。现在，如果遇到缺失值，您有两个设计选项来创建一个足够鲁棒以应对此类问题的模型：
- en: Actively deal with the missing values (there is a paragraph in this chapter
    devoted to this)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主动处理缺失值（本章中有一段专门介绍这一点）。
- en: 'Passively deal with it and:'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 被动地处理它并：
- en: Your system throws an error and everything goes down (and remains down till
    the problem is solved)
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的系统抛出错误，一切都会崩溃（并且会一直处于崩溃状态，直到问题得到解决）。
- en: Your system ignores the missing data and computes the values that are not missing
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的系统忽略缺失数据并计算非缺失值。
- en: 'It is certainly worrying that your prediction system can get struck and could
    stop, but ignoring it and summing the present values could produce highly biased
    results. If your regression equation has been designed to work with all its variables,
    then it cannot work properly when some data is missing. Anyway, let''s again recall
    the linear regression formula:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一定令人担忧的是，您的预测系统可能会陷入困境并停止，但忽略它并汇总现值可能会产生高度偏差的结果。如果您的回归方程被设计为使用所有变量，那么当某些数据缺失时，它将无法正常工作。无论如何，让我们再次回顾线性回归公式：
- en: '![Numeric feature scaling](img/00089.jpeg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![数值特征缩放](img/00089.jpeg)'
- en: As you may guess, the bias coefficient is actually there to stay; it will always
    appear, whatever the situation with your predictors is. Consequently, even in
    an extreme case, such as when all the *X* are missing, if you standardize your
    variables thus they have zero mean.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所猜想的，偏差系数实际上始终存在；无论您的预测变量情况如何，它都会出现。因此，即使在极端情况下，例如当所有 *X* 都缺失时，如果您标准化变量使它们具有零均值。
- en: Let's see this in practice and discover how properly rescaling your predictors
    can help to fix missing values, allow advanced optimization techniques such as
    gradient descent, regularization, and stochastic learning (more about the latter
    two in later chapters), and easily detect outlying and anomalous values.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看实际操作，并了解如何正确缩放预测变量可以帮助修复缺失值，允许使用高级优化技术，如梯度下降、正则化和随机学习（关于后两种技术将在后续章节中详细介绍），以及轻松检测异常值。
- en: 'First, let''s upload our basic packages and functions for the analysis:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们上传用于分析的基本包和函数：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Please notice that the Boston dataset is also reloaded. We refer to *y* as the
    target variable and to *X* as the predictors' array.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，波士顿数据集也被重新加载了。我们将 *y* 作为目标变量，将 *X* 作为预测变量的数组。
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Since we would also like to have a test on the logistic regression, we now transform
    the target variable into a binary response by putting all values above the score
    of 25 at level "1".
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们还想对逻辑回归进行测试，我们现在将目标变量转换为二元响应，将所有高于25分值的分数设置为“1”级别。
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: After this operation, our qualitative response variable is named `yq`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个操作之后，我们的定性响应变量被命名为 `yq`。
- en: Mean centering
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 均值中心化
- en: For all rescaling operations, we suggest the functions to be found in the `preprocessing`
    module of the Scikit-learn package. In particular, we will be using `StandardScaler`
    and `MinMaxScaler`. Like all classes in Scikit-learn, they both have a `fit` method
    that will record and store the parameters that allow correct scaling. They also
    feature a `transform` method that could be immediately applied on the same data
    (the `fit_transform` method will also do the trick) or on any other data, for
    example data used for validation, testing, or even, later on, production.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有缩放操作，我们建议使用 Scikit-learn 包的 `preprocessing` 模块中的函数。特别是，我们将使用 `StandardScaler`
    和 `MinMaxScaler`。像 Scikit-learn 中的所有类一样，它们都有 `fit` 方法，可以记录并存储允许正确缩放的参数。它们还提供了一个
    `transform` 方法，可以立即应用于相同的数据（`fit_transform` 方法也可以做到这一点）或任何其他数据，例如用于验证、测试的数据，甚至以后的生产数据。
- en: The `StandardScaler` class will rescale your variables by removing the mean,
    an action also called centering. In fact, in your training set the rescaled variables
    will all have zero mean and the features will be forced to the unit variance.
    After fitting, the class will contain the `mean_` and `std_` vectors, granting
    you access to the means and standard deviations that made the scaling possible.
    Therefore, in any following set for testing purpose or predictions in production,
    you will be able to apply the exact same transformations, thus maintaining the
    data consistency necessary for the algorithm to work exactly.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '`StandardScaler` 类将通过移除均值来重新缩放你的变量，这一行为也称为中心化。实际上，在你的训练集中，重新缩放的变量将具有零均值，并且特征将被强制为单位方差。拟合后，该类将包含
    `mean_` 和 `std_` 向量，使你能够访问使缩放成为可能的均值和标准差。因此，在随后的任何用于测试目的或生产中的预测的集合中，你将能够应用完全相同的转换，从而保持算法精确工作所需的数据一致性。'
- en: The `MinMaxScaler` class will rescale your variables setting a new minimum and
    maximum value in the range pointed out by you. After fitting, `min_` and `scale_`
    will report the minimum values (subtracted from the original variables) and the
    scale used for dividing your variables to have the intended maximum values, respectively.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`MinMaxScaler` 类将根据你指定的范围重新缩放你的变量，设置新的最小值和最大值。拟合后，`min_` 和 `scale_` 将分别报告从原始变量中减去的最小值和用于将变量除以以获得预期最大值的缩放比例。'
- en: Tip
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: If you reuse one of the two classes, after being trained, on other new data,
    the new variables might have different maximum and minimum values, causing the
    resulting transformed variables to be off-scale (above maximum or below minimum,
    or with an anomalous value). When this happens, it is important to check if the
    new data has anomalous values and question whether we used the correct data for
    the training phase when we defined the transformations and the coefficients.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将这两个类别之一在训练后用于其他新的数据，新的变量可能会有不同的最大值和最小值，导致生成的转换变量超出范围（超过最大值或低于最小值，或具有异常值）。当这种情况发生时，重要的是要检查新数据是否有异常值，并质疑我们在定义转换和系数时是否使用了正确的训练数据。
- en: 'Let''s now upload both the scaling classes and get a remainder of the coefficients
    and intercept value when fitting a linear regression on the Boston dataset:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们上传这两个缩放类，并在对波士顿数据集拟合线性回归时获取系数和截距值的余数：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![Mean centering](img/00090.jpeg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![均值中心化](img/00090.jpeg)'
- en: Tip
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: If you get your results from your Jupyter Notebook in scientific notation, it
    could be helpful to first use `import numpy as np` and then `np.set_printoptions(precision=5,
    suppress=True)`.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你从你的 Jupyter Notebook 获得的结果是科学记数法，那么首先使用 `import numpy as np` 然后使用 `np.set_printoptions(precision=5,
    suppress=True)` 可能会有所帮助。
- en: In particular, let's notice the intercept. Given the linear regression formula,
    we can expect that to be the regression output when all predictors are zero. Let's
    also have a look at the minimum values to check if they are negative, zero, or
    positive.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，让我们注意到截距。根据线性回归公式，我们可以预期当所有预测变量为零时，这将是对应的回归输出。让我们也看看最小值，以检查它们是负数、零还是正数。
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![Mean centering](img/00091.jpeg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![均值中心化](img/00091.jpeg)'
- en: Given the range in our variables, there could never be a situation when all
    the predictors are zero, implying that the intercept, though still functional
    and essential for the proper working of our model, is not representing any really
    expected value.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们的变量范围，不可能出现所有预测变量都为零的情况，这意味着尽管截距仍然功能性和对模型正确工作至关重要，但它并不代表任何真正期望的值。
- en: Now, as a first scaling operation, let's just center the variables, that is,
    remove the mean, and see if this action changes something in our linear regression.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，作为一个初步的缩放操作，让我们仅仅将变量中心化，即移除均值，看看这个动作是否会在我们的线性回归中引起变化。
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Mean centering](img/00092.jpeg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![均值中心化](img/00092.jpeg)'
- en: 'Though the coefficients have remained the same, now the intercept is **22.533**,
    a value that has a particular meaning in our Boston Housing prices problem:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管系数保持不变，但现在截距为 **22.533**，这个值在我们的波士顿房价问题中具有特殊意义：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Having the intercept valued as the average target value means that when one
    or more values are missing we expect them to automatically get a zero value if
    we centered the variable, and our regression will naturally tend to output the
    average value of the target variable.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 将截距值设为目标值的平均值意味着，当一个或多个值缺失时，我们期望如果中心化了变量，它们将自动获得零值，并且我们的回归将自然地倾向于输出目标变量的平均值。
- en: Standardization
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标准化
- en: 'At this point, we can also try scaling everything to unit variance and check
    the results:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们还可以尝试将所有内容缩放到单位方差并检查结果：
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Standardization](img/00093.jpeg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![标准化](img/00093.jpeg)'
- en: As expected, now the coefficients are different, and each one now represents
    the unit change in the target after a modification in the predictors' equivalent
    to a standard deviation. However, the scales are not fully comparable if the distributions
    of our predictors are not normal (standardization implies a normal bell-shaped
    distribution), yet we can now compare the impact of each predictor and allow both
    the automatic handling of missing values and the correct functioning of advanced
    algorithms.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，现在系数不同了，每个系数现在代表在预测变量等效于标准差的变化后，目标变量的单位变化。然而，如果我们的预测变量分布不是正态分布（标准化意味着正态钟形分布），则这些尺度并不完全可比；尽管如此，我们现在可以比较每个预测变量的影响，并允许自动处理缺失值以及高级算法的正确运行。
- en: Normalization
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 归一化
- en: Normalization rescales as standardization, by acting on ranges of the predictors,
    but it has different properties. In fact, when using normalization the zero value
    is the minimum value in the range of values of each predictor. That means that
    zero doesn't represent the mean anymore. Moreover, rescaling between the maximum
    and the minimum can become misleading if there are anomalous values at either
    one of the extremities (most of your values will get squeezed around a certain
    region in `[0,1]`, usually in the center of the range).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化与标准化类似，通过作用于预测变量的范围进行缩放，但它具有不同的特性。实际上，在使用归一化时，零值是每个预测变量值域中的最小值。这意味着零不再代表平均值。此外，如果两端存在异常值（大多数值将挤压在
    `[0,1]` 的某个区域，通常在值域的中心），则最大值和最小值之间的缩放可能会产生误导。
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![Normalization](img/00094.jpeg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![归一化](img/00094.jpeg)'
- en: Applying the `MinMaxScaler` in a range of 0 and 1 drastically changes both the
    coefficients and the intercept, but this could be acceptable under certain circumstances.
    In fact, when working with big data derived from textual data or logs, we sometimes
    realize that the matrices we are working on are not especially populated with
    values, zero often being the most frequent value to be encountered. To speed up
    the calculations and allow huge matrices to be kept in memory, matrices are stored
    in a sparse format.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在0到1的范围内应用 `MinMaxScaler` 会极大地改变系数和截距，但在某些情况下这可能是可以接受的。事实上，当处理来自文本数据或日志的大数据时，我们有时会发现我们正在工作的矩阵并不是特别密集，零经常是遇到的最频繁的值。为了加快计算速度并允许巨大的矩阵保持在内存中，矩阵以稀疏格式存储。
- en: Sparse matrices do not occupy all the memory necessary for their size, they
    just store coordinates and non-zero values. In such situations, standardizing
    the variables would change zero to the mean and a large number of previously zero
    cells would have to be defined, causing the matrix to occupy much more memory.
    Scaling between zero and one allows taking values in a comparable order and keeping
    all previously zero entries, thus not modifying the matrix dimensions in memory.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏矩阵并不占用它们大小所需的全部内存，它们只存储坐标和非零值。在这种情况下，标准化变量会将零值变为平均值，并且必须定义大量之前为零的单元格，导致矩阵占用更多的内存。在0到1之间进行缩放允许以可比较的顺序取值，并保留所有之前为零的条目，从而不修改内存中的矩阵维度。
- en: The logistic regression case
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归案例
- en: A special discussion has to be devoted to logistic regression. As we illustrated
    in the previous chapter, in logistic regression we model the odds ratio of the
    probability of response. We can use the standardized coefficients as a trick to
    face missing data, as seen with linear regression, but things then turn out to
    be a bit different from when we try to guess a target numeric value in linear
    regression analysis.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 需要专门讨论逻辑回归。正如我们在上一章中所述，在逻辑回归中，我们建模响应概率的几率比。我们可以使用标准化系数作为处理缺失数据的技巧，就像在线性回归中看到的那样，但事情在尝试猜测线性回归分析中的目标数值时会有所不同。
- en: 'Let''s explore an example that could clarify the matter. We will be using the
    Boston dataset to demonstrate the logistic regression case and we will use the
    `yq` vector, previously defined, as a response variable. For the logistic regression,
    we won''t be using the Scikit-learn implementation this time but rather the Statsmodels
    package, so we can easily show some insights about the coefficients in the model:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来探讨这个问题，以澄清情况。我们将使用波士顿数据集来演示逻辑回归案例，并将之前定义的 `yq` 向量用作响应变量。对于逻辑回归，这次我们不会使用
    Scikit-learn 实现，而是使用 Statsmodels 包，这样我们可以轻松地展示模型中系数的一些见解：
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![The logistic regression case](img/00095.jpeg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归案例](img/00095.jpeg)'
- en: 'Using the standardized predictors, as in a linear regression, we can interpret
    the coefficients in terms of the same scale and consider the intercept as the
    response when all predictors have an average value. Contrary to linear regression,
    in logistic regression a unit change in predictors changes the odds ratio of the
    response of a quantity equivalent to the exponentiation of the coefficients themselves:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用标准化的预测值，就像线性回归一样，我们可以用相同的尺度来解释系数，并将截距视为当所有预测值都取平均值时的响应。与线性回归相反，在逻辑回归中，预测值的一个单位变化会改变响应的优势比，其量级相当于系数本身的指数化：
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We recall how odds ratios are calculated: given a certain probability *p* of
    an event, an odds ratio is the ratio between *p* and its complement to 1, odds
    *ratio = p / (1−p)*. When the odds ratio is equivalent to 1, our probability is
    exactly 0.5\. When the probability is above 0.5 the odds ratio is above 1; when,
    on the contrary, our probability is less than 0.5 then the odds ratio is below
    1\. By applying the natural logarithm (as logistic regression does), the values
    are distributed around the zero value (50% probability). Clearly working with
    probabilities is more intuitive therefore, a simple transformation, the sigmoid
    transformation, will convert the coefficients to more understandable probabilities:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们回顾一下如何计算优势比：给定一个事件发生的概率 *p*，优势比是 *p* 与其补数到 1 的比率，优势比 = p / (1−p)。当优势比等于 1
    时，我们的概率正好是 0.5。当概率高于 0.5 时，优势比高于 1；相反，当我们的概率小于 0.5 时，优势比低于 1。通过应用自然对数（正如逻辑回归所做的那样），值将分布在零值（50%
    概率）周围。显然，处理概率更直观，因此，一个简单的转换，即 Sigmoid 转换，将系数转换为更可理解的概率：
- en: '[PRE11]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Transforming the intercept into a probability using the sigmoid function, we
    obtain `0.045`, which is the probability of a house value above `25` when all
    predictors bear the mean value. Please notice that such a probability is different
    from the average probability in the sample:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Sigmoid 函数将截距转换为概率，我们得到 `0.045`，这是当所有预测值都取平均值时，房屋价值超过 `25` 的概率。请注意，这样的概率与样本中的平均概率不同：
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In fact, that''s the baseline probability of having a house value above `25`,
    taking account of any possible value of the predictors. What we extracted from
    logistic regression is instead a particular probability, not a general one. You
    can actually get a comparable likelihood when you model a logistic regression
    with only an intercept (the so-called null model), thus allowing the predictors
    to vary freely:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这是在考虑任何可能的预测值时，房屋价值超过 `25` 的基线概率。我们从逻辑回归中提取的实际上是一个特定的概率，而不是一个普遍的概率。实际上，当你用只有一个截距（所谓的空模型）来建模逻辑回归，允许预测值自由变化时，你可以得到一个可比的似然性：
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![The logistic regression case](img/00096.jpeg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归案例](img/00096.jpeg)'
- en: Qualitative feature encoding
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定性特征编码
- en: Beyond numeric features, which have been the main topic of this section so far,
    a great part of your data will also comprise qualitative variables. Databases
    especially tend to record data readable and understandable by human beings; consequently,
    they are quite crowded by qualitative data, which can appear in data fields in
    the form of text or just single labels explicating information, such as telling
    you the class of an observation or some of its characteristics.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 除了本节迄今为止主要讨论的数值特征之外，你的大部分数据也将包含定性变量。数据库尤其倾向于记录人类可读和可理解的数据；因此，它们充满了定性数据，这些数据可以以文本或仅以单个标签的形式出现在数据字段中，解释信息，例如告诉你观察值的类别或其某些特征。
- en: 'For a better understanding of qualitative variables, a working example could
    be a weather dataset. Such a dataset describes conditions under which you would
    want to play tennis because of weather information such as outlook, temperature,
    humidity, and wind, which are all kinds of information that can be rendered by
    numeric measurements. However, you will easily find such data online and recorded
    in datasets with their qualitative translations such as `sunny` or `overcast`,
    rather than numeric satellite/weather-station measurements. We will work on this
    kind of data to demonstrate how it is still possible to transform it in such a
    way that it can be included effectively into a linear model:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解定性变量，一个工作示例可以是天气数据集。这样的数据集描述了由于天气信息（如展望、温度、湿度和风速）而想要打网球的条件，这些信息都是可以通过数值测量来表示的。然而，你很容易在网上找到这样的数据，并且它们被记录在数据集中，这些数据集包含了它们的定性翻译，如“晴朗”或“阴天”，而不是数值卫星或气象站的测量。我们将处理这类数据，以展示即使这样，它仍然可以被转换成可以有效地包含到线性模型中的形式：
- en: '[PRE14]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'A linear regressor can analyze qualitative data only after having been properly
    transformed into a numeric value. A common type of qualitative data comprises
    nominal variables, which are expressed by a limited set of textual labels. For
    instance, a nominal variable could be the color of a product or the outlook of
    the weather (as in our weather example). The textual values that a variable can
    assume are called levels; in our example, outlook has three levels: `sunny`, `overcast`
    and `rainy`, all of them represented as strings.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归器只有在被正确转换为数值后才能分析定性数据。一种常见的定性数据类型是名义变量，它们通过一组有限的文本标签来表示。例如，一个名义变量可以是产品的颜色或天气的展望（如我们的天气示例）。变量可以假设的文本值被称为级别；在我们的例子中，展望有三个级别：“晴朗”、“阴天”和“雨天”，所有这些都以字符串的形式表示。
- en: If we think that any of these can be present or not (each label excludes the
    other), we can easily transform each nominal variable with n levels into n distinct
    variables, each one telling us if a certain characteristic is present or not.
    If we use the value of 1 for denoting the presence of the level and 0 for its
    absence (like binary coding, such as in computers), we will have a working transformation
    of qualitative information into a numeric one (technically it is a Boolean, but
    for practical purposes we model it as a 0 – 1 numeric integer). Such transformed
    variables are called indicator or binary variables in machine learning terminology,
    whereas in statistics they are described as **dichotomies** (a more technical
    term) or dummies variables. They act in a regression formula as modifiers of the
    intercept when the level is present.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们认为这些中的任何一个都可能存在或不存在（每个标签排除另一个），我们可以轻松地将每个具有n个级别的名义变量转换成n个不同的变量，每个变量告诉我们某个特征是否存在。如果我们用1表示级别的存在，用0表示其不存在（类似于二进制编码，如计算机中的编码），我们将得到将定性信息转换为数值信息的工作转换（技术上它是布尔值，但为了实际目的，我们将其建模为0-1的数值整数）。这种转换后的变量在机器学习术语中被称为指标或二元变量，而在统计学中它们被描述为**二分法**（一个更技术性的术语）或虚拟变量。当级别存在时，它们在回归公式中作为截距的修饰符。
- en: When the levels of a variable are ordered, there's another possible transformation.
    For instance, they can be qualitative labels such as good, average, acceptable,
    and bad. In such an occurrence, the labels can also be converted into growing
    or decreasing numbers following the ordering of the meaning of the labels. Therefore,
    in our example, good could be 3, average 2, acceptable 1, and bad 0\. Such encoding
    directly translates a qualitative variable into a numeric one, but it works only
    with labels that can be ordered (that is, where you can define *greater than*
    and *lower than* relations). The transformation implies, since a single coefficient
    will be calculated in the regression model for all the levels, that the difference
    in the outcome passing from good to average is the same as passing from acceptable
    to bad. In reality, this is often not true due to non-linearity. In such a case,
    binary encoding is still the best solution.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当变量的级别是有序的时候，还有另一种可能的转换。例如，它们可以是定性标签，如好、平均、可接受和差。在这种情况下，标签也可以转换为按照标签意义的顺序递增或递减的数字。因此，在我们的例子中，好可以是
    3，平均 2，可接受 1，差 0。这种编码直接将定性变量转换为数值变量，但它只适用于可以排序的标签（即可以定义“大于”和“小于”关系的标签）。这种转换意味着，由于回归模型将为所有级别计算单个系数，因此从好到平均的产出差异与从可接受到差的产出差异相同。在现实中，这通常不成立，因为存在非线性。在这种情况下，二元编码仍然是最佳解决方案。
- en: Dummy coding with Pandas
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Pandas 进行虚拟编码
- en: 'The fastest way to transform a set of qualitative variables into binary ones
    is using a Pandas function, `get_dummies`:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 将一组定性变量转换为二元变量的最快方式是使用 Pandas 函数 `get_dummies`：
- en: '[PRE15]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'After transforming all your data into a Pandas DataFrame, it is quite easy
    to call single variables and single cases to be transformed into binary variables:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在将所有数据转换成 Pandas DataFrame 之后，调用单个变量和单个案例转换为二元变量的操作相当简单：
- en: '[PRE16]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Pandas can really transform all your variables in a breeze; all you need is
    to point out the DataFrame you want to entirely transform or specify just the
    variables to be converted:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas 可以轻松地将所有变量转换；你所需要做的只是指出你想要完全转换的 DataFrame 或指定要转换的变量：
- en: '[PRE17]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'After the transformation, a regression model can immediately analyze the resulting
    new DataFrame:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 转换后，回归模型可以立即分析得到的新 DataFrame：
- en: '[PRE18]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![Dummy coding with Pandas](img/00097.jpeg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Pandas 进行虚拟编码](img/00097.jpeg)'
- en: 'Some regression methods do not really like you to have all the binary variables
    expressing a qualitative variable (but this is not so in our case). Certain optimization
    methods do not love perfect collinearity, such as in the case of a complete binarization
    (in fact, if you know all the other dichotomies, then the remaining ones can be
    perfectly guessed by summing the others—it has value 1 when the sum of the others
    is zero). In such a case, you just need to drop a level of your choice from each
    set of binary variables. By doing so, the omitted coefficient will be incorporated
    into the intercept and the regression model will just work as before, though with
    a different set of variables and coefficients:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一些回归方法并不喜欢所有表示定性变量的二元变量（但我们的情况并非如此）。某些优化方法不喜欢完美的共线性，例如在完全二值化的情况下（实际上，如果你知道所有其他二分法，那么剩余的可以通过求和来完美猜测——当其他变量的和为零时，它具有值
    1）。在这种情况下，你只需从每个二元变量集中删除一个你选择的级别。这样做，被省略的系数将包含在截距中，回归模型将像以前一样工作，尽管变量和系数有所不同：
- en: '[PRE19]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![Dummy coding with Pandas](img/00098.jpeg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Pandas 进行虚拟编码](img/00098.jpeg)'
- en: '`get_dummies` presents only one drawback: it constructs the binary variables
    directly, reading the levels from the dataset you are converting. Consequently,
    if you first build a set of binary variables from a sample and then another one
    from another sample of your same data, it can produce different transformed datasets
    because of rare levels not appearing in one of the samples.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_dummies` 只有一个缺点：它直接构建二元变量，从你要转换的数据集中读取级别。因此，如果你首先从一个样本中构建一组二元变量，然后从另一个样本中构建另一组，由于样本中未出现稀有级别，可能会产生不同的转换数据集。'
- en: DictVectorizer and one-hot encoding
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DictVectorizer 和独热编码
- en: The Scikit-learn package offers a way, though a bit less direct, to consistently
    transform your qualitative variables into numeric ones.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 包提供了一种方法，虽然不是那么直接，但可以将你的定性变量一致地转换为数值变量。
- en: The `DictVectorizer` class can read datasets made of a list of dictionaries,
    properly transform the string label data into a set of binaries, and leave the
    numeric data untouched. If instead you already have qualitative variables coded
    as numeric types in your dataset, all you need is to transform them to string
    values before having them processed by `DictVectorizer`.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`DictVectorizer`类可以读取由字典列表组成的数据集，将字符串标签数据适当地转换为一系列二进制值，并保持数值数据不变。如果你已经在你的数据集中将定性变量编码为数值类型，你只需要在它们被`DictVectorizer`处理之前将它们转换为字符串值。'
- en: 'All you have to do is first create a dictionary representation of your dataset,
    as in the following example:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要做的第一件事是创建你数据集的字典表示，如下例所示：
- en: '[PRE20]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: A dictionary representation is in the form of a list of dictionaries whose keys
    are the variables' names and whose values are their numeric or label value. To
    obtain such a representation, you will need to duplicate your dataset, and that
    could represent a big limitation if you are working with little memory available.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 字典表示形式为字典列表，其键是变量的名称，值是它们的数值或标签值。为了获得这种表示，你需要复制你的数据集，如果你正在处理可用内存较少的情况，这可能会代表一个很大的限制。
- en: On the other hand, the class keeps memory of the transformations and thus everything
    can be exactly replicated on any other data sample using the `transform` method,
    overcoming the limitation we've seen with the Pandas `get_dummies` method.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，类别保留了转换的记忆，因此可以使用`transform`方法在任何其他数据样本上精确复制所有内容，克服了我们之前看到的Pandas `get_dummies`方法的限制。
- en: You can also easily visualize the transformations by calling the `features_names_`
    method.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以通过调用`features_names_`方法轻松可视化转换。
- en: '[PRE21]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: If the limit number of binarizations does not justify the entire conversion
    of the dataset into a dictionary representation, by using the `LabelEncoder` and
    `LabelBinarizer` class, available in the `preprocessing` package in Scikit-learn,
    you can encode and transform a single variable at a time.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果限制的二元化数量不足以证明将整个数据集转换为字典表示的合理性，你可以使用Scikit-learn中`preprocessing`包中的`LabelEncoder`和`LabelBinarizer`类，一次编码和转换一个变量。
- en: '`LabelEncoder` turns the labels into numbers and `LabelBinarizer` transforms
    the numbers into dichotomies. The consistency of all such operations across different
    samples is guaranteed by the `fit` and `transforms` methods that are characteristic
    of all the classes present in Scikit-learn, where `fit` picks and records the
    parameters from data and the `transform` method applies it to new data afterwards.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`LabelEncoder`将标签转换为数字，`LabelBinarizer`将数字转换为二分法。所有这些操作在不同样本之间的一致性由Scikit-learn中所有类特有的`fit`和`transforms`方法保证，其中`fit`从数据中挑选并记录参数，而`transform`方法随后应用于新数据。'
- en: 'Let''s test a transformation on the outlook variable. We first convert the
    text labels into numbers:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在`outlook`变量上测试一个转换。我们首先将文本标签转换为数字：
- en: '[PRE22]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The assigned numbers are given by the position of the label in the list that
    you get using an `inverse_transform` method:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 分配的数字由使用`inverse_transform`方法获得的标签列表中的位置给出：
- en: '[PRE23]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Or by just requiring the recorded classes, glancing at the `classes_` internal
    variable:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 或者只需要求记录的课程，浏览`classes_`内部变量：
- en: '[PRE24]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Once numerically encoded, `LabelBinarizer` can transform everything into indicator
    variables, allowing you to decide what values should be placed in the dichotomy.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦进行数值编码，`LabelBinarizer`可以将所有内容转换为指示变量，允许你决定应该放置哪些值在二分法中。
- en: In fact, if you worry about missing values, you can encode the negative value
    as `−1`, leaving the missing case at `0` (in that case, the missing value will
    be passively taken in charge by the intercept as seen before).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，如果你担心丢失的值，你可以将负值编码为`−1`，将缺失情况保留为`0`（在这种情况下，缺失值将被动地由截距项负责，就像之前看到的那样）。
- en: '[PRE25]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Another great advantage of this method is that it allows sparse representations,
    thus saving memory when working with large datasets.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的另一个优点是它允许稀疏表示，因此在处理大型数据集时可以节省内存。
- en: Feature hasher
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征哈希器
- en: One-hot encoding is a powerful transformation that allows any kind of data to
    be represented just using a binary variable. Using the same approach, you can
    even transform text into variables that can be analyzed by a linear regression
    model.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 单热编码是一种强大的转换，它允许仅使用二进制变量来表示任何类型的数据。使用相同的方法，你甚至可以将文本转换为可以被线性回归模型分析的变量。
- en: 'The idea is to transform any occurrence of a certain word in the text into
    a specific binary variable, so the model will assign a binary value connected
    to the presence of a word in the text. For instance, if you want to analyze the
    Latin motto *Nomina sunt consequentia rerum* (that means "names follow things"),
    you can force all the text to lowercase and enumerate all the distinct words present
    in the text by tokenizing them. By doing so, you intend to separate them (in our
    case, the tokenization is quite simple, we just split by spaces) in a way that
    it is often referred to as a **bag of words** (**BoW**) representation:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 理念是将文本中任何特定单词的出现转换为特定的二元变量，这样模型就会分配一个与文本中单词出现相关的二元值。例如，如果你想分析拉丁格言 *Nomina sunt
    consequentia rerum*（这意味着“名称跟随事物”），你可以将所有文本转换为小写，并通过分词来列举文本中出现的所有不同单词。这样做是为了将它们（在我们的情况下，分词非常简单，我们只是通过空格分割）以通常被称为**词袋模型**（**BoW**）的表示方式分开：
- en: '[PRE26]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The preceding code just transforms all your textual data into a dictionary containing
    the lowercase words and their positional index in a vector of binary variables.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码只是将你的所有文本数据转换为一个字典，其中包含小写单词及其在二元变量向量中的位置索引。
- en: The length of this vector is the length of the dictionary, and each binary flag
    has unit value when the corresponding word is present in the analyzed text. Therefore,
    the vector for all our phrase is `[1,1,1,1]` and the vector for a phrase just
    containing the word `'rerum'` should be `[1,0,0,0]` because the positional index
    of the word is `0`.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这个向量的长度是字典的长度，并且当相应的单词出现在分析文本中时，每个二元标志具有单位值。因此，我们所有短语的向量是 `[1,1,1,1]`，而只包含单词
    `'rerum'` 的短语的向量应该是 `[1,0,0,0]`，因为该单词的位置索引是 `0`。
- en: Figuratively, you can imagine our vector as a line of lamps; each time, you
    turn on only those whose corresponding word is present in the text you are analyzing.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 比喻地说，你可以想象我们的向量就像一排灯；每次，你只打开那些对应于你正在分析文本中出现的单词的灯。
- en: Tip
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Transforming a word into an indicator is just a starting point. You can also
    count how many times a word appears in a text and normalize that count by considering
    the length of the text you are transforming. In fact, in longer text, certain
    words have a higher chance of appearing multiple times than in shorter ones. By
    normalizing the word count, for instance, in such a way that the sum of word counts
    cannot be over a certain number, it will appear like reducing all texts to the
    same length. These are just some of the possible transformations that are part
    of **natural language processing** (**NLP**), and they are viable for a linear
    regression model.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 将单词转换为指示符只是一个起点。你还可以计算一个单词在文本中出现的次数，并通过考虑你正在转换的文本长度来归一化这个计数。实际上，在较长的文本中，某些单词出现多次的可能性比在较短的文本中要高。通过归一化单词计数，例如，以这种方式，单词计数的总和不能超过某个数值，这样就会使所有文本看起来具有相同的长度。这些只是自然语言处理（**NLP**）中可能的一些转换，它们适用于线性回归模型。
- en: 'The Scikit-learn package offers a specialized class for automatically transforming
    text into vectors of binary variables; this is the `CountVectorizer class`. It
    allows the transformation of a list or array of textual data into a sparse matrix.
    Setting the `binary` parameter to `True`, when transforming the data with just
    binary encoding, represents the sparse matrix as a set of unit values in correspondence
    to the texts where a word is present. As a simple example, we can encode this
    series of texts:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 包提供了一个专门用于自动将文本转换为二元变量向量的类；这就是 `CountVectorizer` 类。它允许将文本数据的列表或数组转换为稀疏矩阵。将
    `binary` 参数设置为 `True`，在仅使用二元编码转换数据时，将稀疏矩阵表示为对应于单词出现在文本中的单位值集合。作为一个简单的例子，我们可以对以下文本系列进行编码：
- en: '[PRE27]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The only common word in the corpus (the term for a collection of documents
    subject to a linguistic analysis, so it is common to have a bilingual corpus or
    even a more heterogeneous one) is `''dog''`. This should be reflected in our matrix;
    in fact, just a single column always has the unit value:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库（对受语言学分析的文档集合的术语，因此常见的是双语语料库或甚至更异质的语料库）中唯一的共同单词是 `'dog'`。这应该反映在我们的矩阵中；事实上，只有一个列始终具有单位值：
- en: '[PRE28]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: To visualize the resulting matrix as an output, which would otherwise just be
    made up of the coordinates where the unit values are in the matrix, we need to
    make the resulting sparse matrix dense using the `.todense()` method.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将生成的矩阵可视化为一个输出，否则它将仅由矩阵中单位值所在的坐标组成，我们需要使用`.todense()`方法将生成的稀疏矩阵转换为密集矩阵。
- en: Tip
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Being a toy dataset, such transformation won't imply much in our example. Beware
    of doing the same when your corpus is large because that could cause an out-of-memory
    error on your system.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个玩具数据集，这种转换在我们的例子中不会产生太多影响。当你的语料库很大时，请注意不要做同样的事情，因为这可能会在你的系统上引起内存溢出错误。
- en: 'We notice that the third column has three units, so we imagine that it could
    represent the word `''dog''`. We can verify that by requiring a list representing
    the dictionary and the positional arrangement of words using the `.get_feature_names()`
    method:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到第三列有三个单位，所以我们想象它可能代表单词“dog”。我们可以通过要求一个表示字典和单词位置排列的列表来验证这一点，使用`.get_feature_names()`方法：
- en: '[PRE29]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Leveraging the ability to quickly build dictionaries of words, you can transform
    and use it even for the prediction of text.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 利用快速构建单词字典的能力，你可以将其转换并用于文本预测。
- en: 'The only trouble you can incur using such a representation is when you encounter
    a word never seen before. Let''s see what happens:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种表示可能遇到的唯一问题是当你遇到一个之前从未见过的单词时。让我们看看会发生什么：
- en: '[PRE30]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Such behavior should actually have been expected. Since no word of the phrase
    has been encountered before, it doesn't have any space to fit in the vectorized
    text.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这种行为实际上是可以预料的。由于短语中的单词之前都没有遇到过，所以它们在向量化的文本中没有空间容纳。
- en: A quick and working solution would be to define a very large sparse vector (which
    until filled with data won't occupy much space, no matter the dimensions) and
    to use the specific characteristics of hash functions to deterministically assign
    a position to every word in the vector, without having the need to observe the
    word itself before the assignment. This is also called the **hashing trick** and
    can be applied using the Scikit-learn `HashingVectorizers`.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 一个快速有效的解决方案是定义一个非常大的稀疏向量（在填充数据之前不会占用太多空间，无论维度如何），并使用哈希函数的特定特性来确定性地为向量中的每个单词分配一个位置，而无需在分配之前观察单词本身。这也被称为**哈希技巧**，可以使用Scikit-learn的`HashingVectorizers`应用。
- en: '[PRE31]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The `HashingVectorizer` class has quite a few options for you to explore, especially
    for text treatment, such as allowing more sophisticated tokenizing (even custom
    ones), the removal of common words, stripping accents, and the parsing of different
    encodings.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '`HashingVectorizer`类为你提供了许多选项来探索，特别是对于文本处理，例如允许更复杂的分词（甚至自定义分词），去除常见单词，去除重音符号，以及解析不同的编码。'
- en: In a replica of what we have done before with `CountVectorizer`, we fixed an
    output vector of 11 elements. By doing so, we can notice and discuss two relevant
    characteristics of the preceding output.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前用`CountVectorizer`所做的事情的一个复制品中，我们固定了一个11个元素的输出向量。通过这样做，我们可以注意到并讨论先前输出中的两个相关特征。
- en: 'First, clearly the position of the words is different (it depends on the hash
    function), and we cannot get back a dictionary of what word is in what position
    (but we can be confident that the hashing function has done its job properly).
    Now, we have no fear of vectorizing any previously unseen new text:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，很明显单词的位置是不同的（它取决于哈希函数），我们无法获取一个字典，以了解哪个单词在哪个位置（但我们可以确信哈希函数已经正确地完成了其工作）。现在，我们不再担心向量化任何之前未见过的新的文本：
- en: '[PRE32]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The second thing is that, by observing well the distributions of unit values
    in the matrix, you have values concentrating on certain positions, whereas others
    are left empty. This is due to *the collision problem* with hash functions when
    bounded in a limited number of positions (actually we set the `n_features` parameter
    to 11 for ease of understanding, though in a real analysis it is good practice
    to set it to higher figures).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 第二件事是，通过仔细观察矩阵中单元值的分布，你会发现某些位置的值集中在特定位置，而其他位置则留空。这是由于在有限数量的位置中，哈希函数的*碰撞问题*导致的（实际上我们为了便于理解，将`n_features`参数设置为11，但在实际分析中，将参数设置为更高的数值是好的实践）。
- en: Tip
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: To avoid any unwanted collision, a good `n_features` value is between *2**21*
    and *2**24*, depending on the expected variety of text (the more variety, the
    larger the vector should be).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免任何不想要的冲突，一个好的`n_features`值应该在*2**21*和*2**24*之间，这取决于预期的文本多样性（多样性越大，向量应该越大）。
- en: Numeric feature transformation
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数字特征转换
- en: Numeric features can be transformed, regardless of the target variable. This
    is often a prerequisite for better performance of certain classifiers, particularly
    distance-based. We usually avoid ( besides specific cases such as when modeling
    a percentage or distributions with long queues) transforming the target, since
    we will make any pre-existent linear relationship between the target and other
    features non-linear.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 数值特征可以进行转换，无论目标变量如何。这通常是某些分类器（尤其是基于距离的分类器）性能更好的先决条件。我们通常避免（除了特定情况，例如在建模百分比或具有长尾的分布时）转换目标变量，因为我们将会使目标变量与其他特征之间的任何现有线性关系变得非线性。
- en: 'We will keep on working on the Boston Housing dataset:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续在波士顿房价数据集上工作：
- en: '[PRE33]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'As before, we fit the model using `LinearRegression` from Scikit-learn, this
    time measuring its R-squared value using the `r2_score` function from the `metrics`
    module:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们使用Scikit-learn的`LinearRegression`来拟合模型，这次我们使用`metrics`模块中的`r2_score`函数来测量其R-squared值：
- en: '[PRE34]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Observing residuals
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 观察残差
- en: Residuals are what's left from the original response when the predicted value
    is removed. It is numeric information telling us what the linear model wasn't
    able to grasp and predict by its set of coefficients and intercepts.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 残差是在预测值被移除后从原始响应中留下的部分。它是数字信息，告诉我们线性模型无法通过其系数和截距集来理解和预测。
- en: 'Obtaining residuals when working with Scikit-learn requires just one operation:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Scikit-learn处理时，获取残差只需要一个操作：
- en: '[PRE35]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The residuals of a linear regression always have mean zero and their standard
    deviation depends on the size of the error produced. Residuals can provide insight
    on an unusual observation and non-linearity because, after telling us about what's
    left, they can direct us to specific troublesome data points or puzzling patterns
    in data.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归的残差总是具有零均值，其标准差取决于产生的误差大小。残差可以提供对异常观察和非线性的洞察，因为，在告诉我们剩余信息之后，它们可以引导我们到具体的问题数据点或数据中的令人困惑的模式。
- en: 'For the specific problem of detecting non-linearity, we are going to use a
    plot based on residuals called the **partial residual plot**. In this plot, we
    compare the regression residuals summed with the values derived from the modeled
    coefficient of a variable against the original values of the variable itself:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 对于检测非线性的特定问题，我们将使用基于残差的图表，称为**部分残差图**。在这个图表中，我们将回归残差与从变量的模型系数中得出的值相加，与变量的原始值本身进行比较：
- en: '[PRE36]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '![Observing residuals](img/00099.jpeg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![观察残差](img/00099.jpeg)'
- en: After having calculated the residual of the regression, we decide to inspect
    one variable at a time. After picking up our selected variable, we create a partial
    residual by summing the residuals of the regression with the multiplication of
    the variable values multiplied by its coefficient. In such a way, we *extract*
    the variable from the regression line and we put it in the residuals. Now, as
    partial residuals, we have both the errors and the coefficient-weighted variable.
    If we plot it against the variable itself, we can notice whether there is any
    non-linear pattern. If there is one, we know that we should try some modification.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算了回归的残差之后，我们决定一次检查一个变量。在选择了我们的变量之后，我们通过将回归的残差与变量值的乘积乘以其系数相加来创建一个部分残差。通过这种方式，我们*提取*变量从回归线上，并将其放入残差中。现在，作为部分残差，我们既有误差，也有系数加权的变量。如果我们将其与变量本身绘制在一起，我们可以注意到是否存在任何非线性模式。如果有，我们知道我们应该尝试一些修改。
- en: 'In our case, there is some sign that the points bend after the value 2 of our
    variable, a clear non-linearity sign such as any bend or pattern different from
    an elongated, straight cloud of points. Square, inverse, logarithmic transformations
    can often solve such problems without adding new terms, such as when using the
    polynomial expansion:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，有一些迹象表明，当变量的值达到2之后，点开始弯曲，这是一个明显的非线性迹象，比如任何弯曲或与拉长的、直线型的点云不同的模式。平方、倒数、对数变换通常可以解决这类问题，而无需添加新的项，例如在使用多项式展开时：
- en: '[PRE37]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![Observing residuals](img/00100.jpeg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![观察残差](img/00100.jpeg)'
- en: Just notice how an inverse square transformation rendered the partial residual
    plot straighter, something that is reflected in a higher R-squared value, indicating
    an increased capacity of the model to capture the data distribution.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 注意一下逆平方变换如何使部分残差图变得更直，这一点反映在更高的R-squared值上，表明模型捕捉数据分布的能力有所提高。
- en: 'As a rule, the following transformations should always be tried (singularly
    or in combination) to find a fix for a non-linearity:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 作为规则，以下转换应该始终尝试（单独或组合）以找到解决非线性问题的方法：
- en: '| Function names | Functions |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 函数名称 | 函数 |'
- en: '| --- | --- |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Logarithmic | `np.log(x)` |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 对数 | `np.log(x)` |'
- en: '| Exponential | `np.exp(x)` |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 指数 | `np.exp(x)` |'
- en: '| Squared | `x**2` |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 平方 | `x**2` |'
- en: '| Cubed | `x**3` |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 立方 | `x**3` |'
- en: '| Square root | `np.sqrt(x)` |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 平方根 | `np.sqrt(x)` |'
- en: '| Cube root | `x**(1./3.)` |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 立方根 | `x**(1./3.)` |'
- en: '| Inverse | `1\. / x` |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 反函数 | `1./x` |'
- en: Tip
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'Some of the transformations suggested in the preceding table won''t work properly
    after normalization or otherwise in the presence of zero and negative values:
    logarithmic transformation needs positive values above zero, square root won''t
    work with negative values, and inverse transformation won''t operate with zero
    values. Sometimes adding a constant may help (like in the case of `np.log(x+1)`).
    Generally, just try the possible transformations, according to your data values.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 表中建议的一些转换在归一化后或存在零和负值的情况下可能无法正常工作：对数转换需要大于零的正值，平方根不能与负值一起使用，反转换不能与零值一起操作。有时添加一个常数可能有所帮助（例如`np.log(x+1)`）。通常，只需根据你的数据值尝试可能的转换。
- en: Summarizations by binning
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过分箱进行总结
- en: When it is not easy to figure out the exact transformation, a quick solution
    could be to transform the continuous numeric variable into a series of binary
    variables, thus allowing the estimation of a coefficient for each single part
    of the numeric range of the variable.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 当难以确定确切的转换时，一个快速的解决方案是将连续的数值变量转换为一系列二进制变量，从而允许估计变量数值范围内的每个单独部分的系数。
- en: Though fast and convenient, this solution will increase the size of your dataset
    (unless you use a sparse representation of the matrix) and it will risk too much
    overfitting on your data.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然快速方便，但这种解决方案会增加你的数据集大小（除非你使用矩阵的稀疏表示）并且可能会对你的数据过度拟合。
- en: First, you divide your values into equally spaced bins and you notice the edges
    of the bins using the `histogram` function from Numpy. After that, using the `digitize`
    function, you convert the value in their bin number, based on the bin boundaries
    provided before. Finally, you can transform all the bin numbers into binary variables
    using the previously present `LabelBinarizer` from Scikit-learn.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你将你的值分成等间隔的箱，并使用Numpy中的`histogram`函数来观察箱的边缘。之后，使用`digitize`函数，根据之前提供的箱边界将它们的值转换为箱号。最后，你可以使用之前存在的Scikit-learn中的`LabelBinarizer`将所有箱号转换为二进制变量。
- en: 'At this point, all you have to do is replace the previous variable with this
    new set of binary indicators and refit the model for checking the improvement:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，你所要做的就是用这组新的二进制指标替换之前的变量，并重新调整模型以检查改进：
- en: '[PRE38]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Missing data
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缺失数据
- en: Missing data appears often in real-life data, sometimes randomly in random occurrences,
    more often because of some bias in its recording and treatment. All linear models
    work on complete numeric matrices and cannot deal directly with such problems;
    consequently, it is up to you to take care of feeding suitable data for the algorithm
    to process.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失数据在现实生活中的数据中很常见，有时在随机发生中随机出现，更常见的是由于记录和处理中的某些偏差。所有线性模型都基于完整的数值矩阵工作，并且不能直接处理这类问题；因此，照顾好为算法处理提供合适的数据就取决于你了。
- en: Even if your initial dataset does not present any missing data, it is still
    possible to encounter missing values in the production phase. In such a case,
    the best strategy is surely that of dealing with them passively, as presented
    at the beginning of the chapter, by standardizing all the numeric variables.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你的初始数据集没有缺失数据，在生成阶段仍然可能遇到缺失值。在这种情况下，最好的策略无疑是被动处理它们，就像本章开头所展示的那样，通过标准化所有数值变量。
- en: Tip
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: As for as indicator variables, in order to passively intercept missing values,
    a possible strategy is instead to encode the presence of the label as `1` and
    its absence as `-1`, leaving the zero value for missing values.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 对于指示变量，为了被动地拦截缺失值，一种可能的策略是将标签的存在编码为`1`，其不存在编码为`-1`，将零值留给缺失值。
- en: When missing values are present from the beginning of the project, it is certainly
    better to deal with them explicitly, trying to figure out if there is any systematic
    pattern behind missing values. In Python arrays, upon which both the Pandas and
    Scikit-learn packages are built, missing values are marked by a special value,
    **Not a Number** (**NaN**), which is repeatable using the value available from
    the NumPy constant `nan`.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 当缺失值从项目开始时就存在时，明确处理它们当然更好，试图找出是否存在任何系统性的缺失值模式。在 Python 数组中，Pandas 和 Scikit-learn
    包都是基于它构建的，缺失值由一个特殊值标记，**非数字**（**NaN**），可以使用来自 NumPy 常数的值重复使用。
- en: 'Creating a toy array with a missing value is easy:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个包含缺失值的玩具数组很容易：
- en: '[PRE39]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Also discovering where missing values are in a vector (the result is a vector
    of Booleans):'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，也可以发现向量中缺失值的位置（结果是布尔向量）：
- en: '[PRE40]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Replacing all missing elements can be done easily by slicing or by the function
    `nan_to_num`, which turns every `nan` to zero:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 通过切片或使用函数 `nan_to_num`，可以轻松地替换所有缺失元素，该函数将每个 `nan` 转换为零：
- en: '[PRE41]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Using slicing, you could decide to use something more sophisticated than a
    constant, such as the mean of valid elements in the vector:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 使用切片，你可以选择比常数更复杂的东西，比如向量中有效元素的均值：
- en: '[PRE42]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Missing data imputation
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺失数据插补
- en: Consistency of treatment between samples of data is essential when working with
    predictive models. If you replace the missing values with a certain constant or
    a particular mean, that should be consistent during both the training and the
    production phase. The Scikit-learn package offers the `Imputer` class in the `preprocessing`
    module which can learn a solution by the `fit` method and then consistently apply
    it by the `transform` one.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用预测模型时，数据样本之间处理的一致性至关重要。如果你用某个常数或特定的均值来替换缺失值，那么在训练和生成阶段都应该保持一致。Scikit-learn
    包在 `preprocessing` 模块中提供了 `Imputer` 类，它可以通过 `fit` 方法学习解决方案，然后通过 `transform` 方法一致地应用它。
- en: 'Let''s start demonstrating it after putting some missing values in the Boston
    dataset:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在向波士顿数据集中添加一些缺失值后演示这一点：
- en: '[PRE43]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Tip
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: It is quite unlikely that you will get the same result due to the random nature
    of the sampling process. Please notice that the exercise sets a seed so you can
    count on the same results on your PC.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 由于采样过程的随机性，你得到相同的结果的可能性相当低。请注意，练习集设置了一个种子，这样你可以在你的电脑上得到相同的结果。
- en: 'Now about a quarter of observations in the variable should be missing. Let''s
    use `Imputer` to replace them using a mean:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在变量中应有大约四分之一的观测值缺失。让我们使用 `Imputer` 通过均值来替换它们：
- en: '[PRE44]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Imputer allows you to define any value as missing (sometimes in a re-elaborated
    dataset missing values could be encoded with negative values or other extreme
    values) and to choose alternative strategies rather than the mean. Other alternatives
    are **median** and **mode**. The median is useful if you suspect that outlying
    values are influencing and biasing the average (in house prices, some very expensive
    and exclusive houses or areas could be the reason). Mode, the most frequent value,
    is instead the optimal choice if you are working with discrete values (for instance
    a sequence of integer values with a limited range).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '`Imputer` 允许你定义任何值作为缺失值（有时在重新加工的数据集中，缺失值可以用负值或其他极端值编码）并选择替代策略，而不是均值。其他替代方案是
    **中位数** 和 **众数**。如果你怀疑异常值正在影响并偏置平均值（在房价中，一些非常昂贵和专属的房屋或地区可能是原因），中位数是有用的。众数，即最频繁的值，如果你正在处理离散值（例如有限范围的整数序列），则是最佳选择。'
- en: Keeping track of missing values
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跟踪缺失值
- en: 'If you suspect that there is some bias in the missing value pattern, by imputing
    them you will lose any trace of it. Before imputing, a good practice is to create
    a binary variable recording where all missing values were and to add it as a feature
    to the dataset. As seen before, it is quite easy using NumPy to create such a
    new feature, transforming the Boolean vector created by `isnan` into a vector
    of integers:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你怀疑缺失值模式中存在一些偏差，通过插补它们，你将失去任何痕迹。在插补之前，一个好的做法是创建一个二进制变量，记录所有缺失值的位置，并将其作为特征添加到数据集中。如前所述，使用
    NumPy 创建这样一个新特征非常容易，将 `isnan` 创建的布尔向量转换为整数向量：
- en: '[PRE45]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The linear regression model will create a coefficient for this indicator of
    missing values and, if any pattern exists, its informative value will be captured
    by a coefficient.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归模型将为这个缺失值指标创建一个系数，如果存在任何模式，其信息值将通过系数捕捉。
- en: Outliers
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异常值
- en: After properly transforming all the quantitative and qualitative variables and
    fixing any missing data, what's left is just to detect any possible outlier and
    to deal with it by removing it from the data or by imputing it as if it were a
    missing case.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在适当转换所有定量和定性变量并修复任何缺失数据后，剩下的只是检测任何可能的异常值，并通过从数据中删除它或将其作为缺失值处理来处理它。
- en: An outlier, sometimes also referred to as an anomaly, is an observation that
    is very different from all the others you have observed so far. It can be viewed
    as an unusual case that stands out, and it could pop up due to a mistake (an erroneous
    value completely out of scale) or simply a value that occurred (rarely, but it
    occurred). Though understanding the origin of an outlier could help to fix the
    problem in the most appropriate way (an error could be legitimately removed; a
    rare case could be kept or capped or even imputed as a missing case), what is
    of utmost concern is the effect of one or more outliers on your regression analysis
    results. Any anomalous data in a regression analysis means a distortion of the
    regression's coefficients and a limit on the ability of the model to correctly
    predict usual cases.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值，有时也称为异常，是与其他所有观察到的值非常不同的观察值。它可以被视为一个不寻常的案例，脱颖而出，它可能是由错误（一个完全超出尺度的错误值）或仅仅是偶尔发生（虽然很少，但确实发生了）的值引起的。尽管理解异常值的来源可能有助于以最合适的方式解决问题（错误可以合法地删除；罕见的情况可以保留或限制，甚至可以将其作为缺失值处理），但最关心的是一个或多个异常值对回归分析结果的影响。回归分析中的任何异常数据都意味着回归系数的扭曲以及模型正确预测常规案例的能力受到限制。
- en: Tip
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Despite the importance of controlling outliers, unfortunately practitioners
    often overlook this activity because, in contrast to the other preparations illustrated
    throughout the chapter, omitting to detect outliers won't stop the analysis you
    are working on and you will get your regression coefficients and results (both
    probably quite inexact). However, having an analysis run smoothly to the end doesn't
    mean that everything is fine with the analysis itself. An outlier can distort
    an analysis in two ways depending on whether the anomalous value is on the target
    variable or on the predictors.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管控制异常值的重要性不言而喻，但遗憾的是，实践者往往忽略了这一活动，因为与其他章节中展示的其他准备工作相比，未能检测到异常值并不会阻止你正在进行的分析，你仍然会得到回归系数和结果（两者可能都相当不准确）。然而，分析过程顺利并不意味着分析本身就没有问题。异常值可能会根据异常值是在目标变量还是预测变量上出现而以两种方式扭曲分析。
- en: In order to detect outliers, there are a few approaches, some based on the observation
    of variables taken singularly (the single-variable, or univariate, approach),
    and some based on reworking all the variables together into a synthetic measure
    (the multivariate approach).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检测异常值，有几种方法，一些基于对变量单独的观察（单变量方法，或称单变量方法），而另一些则基于将所有变量一起重新组合成一个综合度量（多变量方法）。
- en: 'The best single variable approach is based on the observation of standardized
    variables and on the plotting of box plots:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳的单变量方法基于标准化变量的观察和箱线图的绘制：
- en: Using standardized variables, everything scoring further than the absolute value
    of three standard deviations from the mean is suspect, though such a rule of thumb
    doesn't generalize well if the distribution is not normal
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用标准化变量，任何得分超过平均值的绝对值三个标准差的值都是可疑的，尽管如果分布不是正态分布，这样的经验法则并不很好地推广。
- en: Using boxplots, the **interquartile range** (shortened to **IQR**; it is the
    difference between the values at the 75^(th) and the 25^(th) percentile) is used
    to detect suspect outliers beyond the 75^(th) and 25^(th) percentiles. If there
    are examples whose values are outside the IQR, they can be considered suspicious,
    especially if their value is beyond 1.5 times the IQR's boundary value. If they
    exceed 3 times the IQR's limit, they are almost certainly outliers.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用箱线图，**四分位数范围**（简称**IQR**；它是第75百分位数和第25百分位数之间的差值）用于检测超出第75百分位数和第25百分位数的可疑异常值。如果存在值超出IQR的例子，它们可以被认为是可疑的，尤其是如果它们的值超过IQR边界值的1.5倍。如果它们超过IQR限制的3倍，它们几乎可以肯定是异常值。
- en: Tip
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'The Scikit-learn package offers a couple of classes for automatically detecting
    outliers using sophisticated approaches: `EllipticEnvelope` and `OneClassSVM`.
    Though a treatise of both these complex algorithms is out of scope here, if outliers
    or unusual data is the main problem with your data, we suggest having a look at
    this web page for some quick recipes you can adopt in your scripts: [http://scikit-learn.org/stable/modules/outlier_detection.html](http://scikit-learn.org/stable/modules/outlier_detection.html).
    Otherwise, you could always read our previous book *Python Data Science Essentials*,
    *Alberto Boschetti* and *Luca Massaron*, *Packt Publishing*.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn包提供了一些类，用于使用复杂的方法自动检测异常值：`EllipticEnvelope`和`OneClassSVM`。尽管这两个复杂算法的详细论述超出了本文的范围，但如果异常值或异常数据是您数据中的主要问题，我们建议您查看这个网页，以获取一些您可以在脚本中采用的快速解决方案：[http://scikit-learn.org/stable/modules/outlier_detection.html](http://scikit-learn.org/stable/modules/outlier_detection.html)。否则，您也可以阅读我们之前出版的书籍
    *Python数据科学基础*，作者 *Alberto Boschetti* 和 *Luca Massaron*，由 *Packt Publishing* 出版。
- en: Outliers on the response
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 响应中的异常值
- en: The first step in looking for outliers is to check the response variable. In
    observing plots of the variable distribution and of the residuals of the regression,
    it is important to check if there are values that, because of a too high or too
    low value, are out of the main distribution.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找异常值的第一步是检查响应变量。在观察变量分布和回归残差的图表时，重要的是检查是否存在由于过高或过低而超出主要分布的值。
- en: Usually, unless accompanied by outlying predictors, outliers in the response
    have little impact on the estimated coefficients; however, from a statistical
    point of view, since they affect the amount of the root-squared error, they reduce
    the explained variance (the squared r) and inflate the standard errors of the
    estimate. Both such effects represent a problem when your approach is a statistical
    one, whereas they are of little concern for data science purposes.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，除非伴随有异常的自变量，否则响应中的异常值对估计系数的影响很小；然而，从统计学的角度来看，由于它们影响了均方根误差的量，它们减少了解释方差（平方r）并增加了估计的标准误差。这两种影响在统计方法中都是一个问题，而对于数据科学目的来说，这些问题则微不足道。
- en: 'To figure out which responses are outliers, we should first monitor the target
    distribution. We start by recalling the Boston dataset:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定哪些响应是异常值，我们首先应该监控目标分布。我们首先回忆一下波士顿数据集：
- en: '[PRE46]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'A `boxplot` function can hint at any outlying values in the target variable:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '`箱线图`函数可以提示目标变量中的任何异常值：'
- en: '[PRE47]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '![Outliers on the response](img/00101.jpeg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![响应中的异常值](img/00101.jpeg)'
- en: The box display and its whiskers tell us that quite a few values are out of
    the IQR, so they are suspect ones. We also notice a certain concentration at the
    value 50; in fact the values are capped at 50.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 箱线图及其须须告诉我们，相当多的值超出了四分位数范围，因此它们是可疑的。我们还注意到在值50处有一定的集中；实际上，值被限制在50。
- en: 'At this point, we can try to build our regression model and inspect the resulting
    residuals. We will standardize them using the Root Mean Squared error. An easy
    approach to implement though it is not the most precise, it is still enough good
    to reveal any significant problem:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以尝试构建我们的回归模型并检查产生的残差。我们将使用均方根误差对其进行标准化。虽然这不是最精确的方法，但它仍然足够好，可以揭示任何显著的问题：
- en: '[PRE48]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '![Outliers on the response](img/00102.jpeg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![响应中的异常值](img/00102.jpeg)'
- en: Making a scatterplot of the values fitted by the regression against the standardized
    residuals, we notice there are a few outlying cases over three standard deviations
    from the zero mean. The capped values especially, clearly visible in the graph
    as a line of points, seem problematic.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 通过绘制回归拟合值与标准化残差的散点图，我们发现有几个异常值超过三个标准差，从零均值来看。特别是截顶值，在图中作为点线清晰可见，似乎存在问题。
- en: Outliers among the predictors
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自变量中的异常值
- en: As we inspected the target variable, it is now time to have a look also at the
    predictors. If unusual observations were outliers in the target variable, similar
    cases in the predictors are instead named influential or high leverage observations
    because they can really make an impact on more than the **sum of squared errors**
    (**SSE**), this time influencing coefficients and the intercept—in a word, the
    entire regression solution (that's why they are so important to catch).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查目标变量后，现在我们也应该看看预测变量。如果异常观测值在目标变量中是异常值，那么在预测变量中的类似情况则被称为有影响力的或高杠杆观测值，因为它们真的可以影响不仅仅是**平方和误差**（**SSE**），这次影响系数和截距——简而言之，整个回归解（这就是为什么它们如此重要，需要捕捉）。
- en: 'After standardizing, we start having a look at the distributions using boxplots:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化后，我们开始使用箱线图来观察分布：
- en: '[PRE49]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '![Outliers among the predictors](img/00103.jpeg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![预测变量中的异常值](img/00103.jpeg)'
- en: '[PRE50]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '![Outliers among the predictors](img/00104.jpeg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![预测变量中的异常值](img/00104.jpeg)'
- en: After observing all the boxplots, we can conclude that there are variables with
    restricted variance, such as **B**, **ZN**, and **CRIM**, which are characterized
    by a long tail of values. There are also some suspect cases from **DIS** and **LSTAT**.
    We can delimit all these cases by looking for the values above the represented
    thresholds, variable after variable, but it would be helpful to catch all of them
    at once.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 观察完所有箱线图后，我们可以得出结论，存在一些方差受限的变量，如**B**、**ZN**和**CRIM**，它们的特点是值的长尾。还有一些来自**DIS**和**LSTAT**的疑似案例。我们可以通过寻找超过表示阈值的值来界定所有这些案例，一个变量一个变量地，但一次性捕捉所有这些案例将是有帮助的。
- en: '**Principal Component Analysis** (**PCA**) is a technique that can reduce complex
    datasets into fewer dimensions, the summation of the original variables of the
    dataset. Without delving too much into the technicalities of the algorithm, you
    just need to know that the new dimensions produced by the algorithm have decreasing
    explicatory power; consequently, plotting the top ones against each other is just
    like plotting all the dataset''s information. By glancing at such synthetic representations,
    you can spot groups and isolated points that, if very far from the center of the
    graph, are also quite influential on the regression model.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）是一种可以将复杂数据集简化为较少维度的技术，即数据集原始变量的总和。不深入探讨算法的技术细节，你只需知道算法产生的新维度具有递减的解释力；因此，将这些维度相互对比就像绘制整个数据集的信息一样。通过观察这些合成表示，你可以发现群组和孤立点，如果它们离图表中心非常远，那么它们对回归模型的影响也相当大。'
- en: '[PRE51]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '![Outliers among the predictors](img/00105.jpeg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![预测变量中的异常值](img/00105.jpeg)'
- en: 'The first dimension created by PCA can explain 47% of the dataset''s information,
    the second and the third 11% and 9.5%, respectively (the `explained_variance_ratio_`
    method can provide you with such information). Now all we have to do is to plot
    the first dimension against the second and the third and look for lonely points
    away from the center because those are our high leverage cases to be investigated:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: PCA创建的第一个维度可以解释数据集信息的47%，第二个和第三个分别是11%和9.5%（`explained_variance_ratio_`方法可以提供此类信息）。现在我们只需将第一个维度与第二个和第三个维度进行对比，寻找远离中心的孤立点，因为这些就是我们需要调查的高杠杆案例：
- en: '[PRE52]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '![Outliers among the predictors](img/00106.jpeg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![预测变量中的异常值](img/00106.jpeg)'
- en: '[PRE53]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '![Outliers among the predictors](img/00107.jpeg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![预测变量中的异常值](img/00107.jpeg)'
- en: Removing or replacing outliers
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 移除或替换异常值
- en: After being able to detect outliers and influential observations, we just need
    to discuss what we can do with them. You might believe it's OK just to delete
    them but, on the contrary, removing or replacing an outlier is something to consider
    carefully.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在能够检测到异常值和有影响力的观测值之后，我们只需讨论我们可以如何处理它们。你可能认为只需删除它们就可以了，但相反，移除或替换一个异常值是需要仔细考虑的事情。
- en: 'In fact, outlying observations may be justified by three reasons (their remedies
    change accordingly):'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，异常观测值可能由三个原因（相应的补救措施也随之改变）得到解释：
- en: They are outliers because they are rare occurrences, so they appear unusual
    with regard to other observations. If this is the case, removing the data points
    could not be the correct solution because the points are part of the distribution
    you want to model and they stand out just because of chance. The best solution
    would be to increase the sample number. If augmenting your sample size is not
    possible, then remove them or try to resample in order to avoid having them drawn.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们是异常值，因为它们是罕见事件，所以它们与其他观察结果相比显得不寻常。如果情况如此，删除数据点可能不是正确的解决方案，因为这些点是您想要建模的分布的一部分，它们之所以突出，仅仅是因为偶然。最好的解决方案是增加样本数量。如果增加样本量不可行，那么删除它们或尝试重新采样以避免它们被选中。
- en: Some errors have happened in the data processing and the outlying observations
    are from another distribution (some data has been mixed, maybe from different
    times or another geographical context). In this case, prompt removal is called
    for.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些错误发生在数据处理中，异常观察值来自另一个分布（可能有一些数据被混合，可能是来自不同时间或另一个地理环境）。在这种情况下，需要立即删除。
- en: The value is a mistake due to faulty input or processing. In such an occurrence,
    the value has to be considered as missing and you should perform an imputation
    of the now missing value to get a reasonable value.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该值是由于输入或处理错误而造成的错误。在这种情况下，该值必须被视为缺失值，并且您应该执行缺失值的插补，以获得合理的值。
- en: Tip
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: As a rule, just keep in mind that removing data points is necessary only when
    the points are different from the data you want to use for prediction and when,
    by removing them, you get direct confirmation that they had a lot of influence
    on the coefficients or on the intercept of the regression model. In all other
    cases, avoid any kind of selection in order to improve the model since it is a
    form of data snooping (more on the topic of how data snooping can negatively affect
    your models in the next chapter).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，只有当数据点与您用于预测的数据不同，并且通过删除它们，您可以直接确认它们对回归模型的系数或截距有很大影响时，删除数据点才是必要的。在其他所有情况下，为了避免数据挖掘（关于数据挖掘如何负面地影响您的模型的内容将在下一章中详细介绍），应避免任何形式的筛选来改进模型。
- en: Summary
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have dealt with many different problems that you may encounter
    when preparing your data to be analyzed by a linear model.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们处理了您在准备数据以便由线性模型分析时可能遇到的各种问题。
- en: We started by discussing rescaling variables and understanding how new variables'
    scales not only permit a better insight into the data, but also help us deal with
    unexpectedly missing data.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先讨论了变量的缩放和了解新变量的尺度不仅允许我们更好地洞察数据，而且帮助我们处理意外缺失的数据。
- en: Then, we learned how to encode qualitative variables and deal with the extreme
    variety of possible levels with unpredictable variables and textual information
    just by using the hashing trick. We then returned to quantitative variables and
    learned how to transform in a linear shape and obtain better regression models.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们学习了如何通过使用散列技巧来编码定性变量，并处理具有不可预测变量和文本信息的极端多样的可能级别。然后，我们回到定量变量，学习了如何将其转换为线性形状并获得更好的回归模型。
- en: Finally, we dealt with some possible data pathologies, missing and outlying
    values, showing a few quick fixes that, in spite of their simplicity, are extremely
    effective and performant.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们处理了一些可能的数据异常，如缺失值和异常值，展示了一些简单而非常有效和高效的快速修复方法。
- en: At this point, before proceeding to more sophisticated linear models, we just
    need to illustrate the data science principles that can help you obtain really
    good working predictive engines and not just mere mathematical curve fitting exercises.
    And that's precisely the topic of the next chapter.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在此阶段，在继续到更复杂的线性模型之前，我们只需要说明可以帮助您获得真正优秀的预测引擎的数据科学原则，而不仅仅是数学曲线拟合练习。这正是下一章的主题。
