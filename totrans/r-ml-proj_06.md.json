["```py\n# setting the working directory where the files need to be downloaded\nsetwd('/home/sunil/Desktop/book/chapter 6/MNIST')\n# download the training and testing dataset from source\ndownload.file(\"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\"train-images-idx3-ubyte.gz\")\ndownload.file(\"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\"train-labels-idx1-ubyte.gz\")\ndownload.file(\"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\"t10k-images-idx3-ubyte.gz\")\ndownload.file(\"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",\"t10k-labels-idx1-ubyte.gz\")\n# unzip the training and test zip files that are downloaded\nR.utils::gunzip(\"train-images-idx3-ubyte.gz\")\nR.utils::gunzip(\"train-labels-idx1-ubyte.gz\")\nR.utils::gunzip(\"t10k-images-idx3-ubyte.gz\")\nR.utils::gunzip(\"t10k-labels-idx1-ubyte.gz\")\n```", "```py\n# function to load the image files\nload_image_file = function(filename) {\n  ret = list()\n  # opening the binary file in read mode \n  f = file(filename, 'rb')\n  # reading the binary file into a matrix called x\n readBin(f, 'integer', n = 1, size = 4, endian = 'big')\n n = readBin(f, 'integer', n = 1, size = 4, endian = 'big')\n nrow = readBin(f, 'integer', n = 1, size = 4, endian = 'big')\n ncol = readBin(f, 'integer', n = 1, size = 4, endian = 'big')\n x = readBin(f, 'integer', n = n * nrow * ncol, size = 1, signed = FALSE)\n  # closing the file\n  close(f)\n  # converting the matrix and returning the dataframe\n  data.frame(matrix(x, ncol = nrow * ncol, byrow = TRUE))\n}\n# function to load label files\nload_label_file = function(filename) {\n  # reading the binary file in read mode\n  f = file(filename, 'rb')\n  # reading the labels binary file into y vector \n  readBin(f, 'integer', n = 1, size = 4, endian = 'big')\n  n = readBin(f, 'integer', n = 1, size = 4, endian = 'big')\n  y = readBin(f, 'integer', n = n, size = 1, signed = FALSE)\n  # closing the file\n  close(f)\n  # returning the y vector\n  y\n}\n```", "```py\n# load training images data through the load_image_file custom function\ntrain = load_image_file(\"train-images-idx3-ubyte\")\n# load  test data through the load_image_file custom function\ntest  = load_image_file(\"t10k-images-idx3-ubyte\")\n# load the train dataset labels\ntrain.y = load_label_file(\"train-labels-idx1-ubyte\")\n# load the test dataset labels\ntest.y  = load_label_file(\"t10k-labels-idx1-ubyte\")\n```", "```py\n# helper function to visualize image given a record of pixels\nshow_digit = function(arr784, col = gray(12:1 / 12), ...) {\n  image(matrix(as.matrix(arr784), nrow = 28)[, 28:1], col = col, ...)\n}\n```", "```py\n# viewing image corresponding to record 3 in the train dataset\nshow_digit(train[3, ])\n```", "```py\n# setting the working directory\nsetwd('/home/sunil/Desktop/book/chapter 6/MNIST')\n# function to load image files\nload_image_file = function(filename) {\n  ret = list()\n  f = file(filename, 'rb')\n  readBin(f, 'integer', n = 1, size = 4, endian = 'big')\n  n    = readBin(f, 'integer', n = 1, size = 4, endian = 'big')\n  nrow = readBin(f, 'integer', n = 1, size = 4, endian = 'big')\n  ncol = readBin(f, 'integer', n = 1, size = 4, endian = 'big')\n  x = readBin(f, 'integer', n = n * nrow * ncol, size = 1, signed\n= FALSE)\n  close(f)\n  data.frame(matrix(x, ncol = nrow * ncol, byrow = TRUE))\n}\n# function to load the label files\nload_label_file = function(filename) {\n  f = file(filename, 'rb')\n  readBin(f, 'integer', n = 1, size = 4, endian = 'big')\n  n = readBin(f, 'integer', n = 1, size = 4, endian = 'big')\n  y = readBin(f, 'integer', n = n, size = 1, signed = FALSE)\n  close(f)\n  y }\n# loading the image files\ntrain = load_image_file(\"train-images-idx3-ubyte\")\ntest  = load_image_file(\"t10k-images-idx3-ubyte\")\n# loading the labels\ntrain.y = load_label_file(\"train-labels-idx1-ubyte\")\ntest.y  = load_label_file(\"t10k-labels-idx1-ubyte\")\n# lineaerly transforming the grey scale image i.e. between 0 and 255 to # 0 and 1\ntrain.x <- data.matrix(train/255)\ntest <- data.matrix(test/255)\n# verifying the distribution of the digit labels in train dataset\nprint(table(train.y))\n# verifying the distribution of the digit labels in test dataset\nprint(table(test.y))\n```", "```py\ntrain.y\n   0    1    2   3    4    5    6    7    8    9 \n5923 6742 5958 6131 5842 5421 5918 6265 5851 5949 \n\ntest.y\n   0    1    2    3    4    5    6    7    8    9 \n 980 1135 1032 1010  982  892  958 1028  974 1009 \n```", "```py\n# including the required mxnet library \nlibrary(mxnet)\n# defining the input layer in the network architecture\ndata <- mx.symbol.Variable(\"data\")\n# defining the first hidden layer with 128 neurons and also naming the # layer as fc1\n# passing the input data layer as input to the fc1 layer\nfc1 <- mx.symbol.FullyConnected(data, name=\"fc1\", num_hidden=128)\n# defining the ReLU activation function on the fc1 output and also # naming the layer as ReLU1\nact1 <- mx.symbol.Activation(fc1, name=\"ReLU1\", act_type=\"relu\")\n# defining the second hidden layer with 64 neurons and also naming the # layer as fc2\n# passing the previous activation layer output as input to the\nfc2 layer\nfc2 <- mx.symbol.FullyConnected(act1, name=\"fc2\", num_hidden=64)\n# defining the ReLU activation function on the fc2 output and also \n# naming the layer as ReLU2\nact2 <- mx.symbol.Activation(fc2, name=\"ReLU2\", act_type=\"relu\")\n# defining the third and final hidden layer in our network with 10 \n# neurons and also naming the layer as fc3\n# passing the previous activation layer output as input to the\nfc3 layer\nfc3 <- mx.symbol.FullyConnected(act2, name=\"fc3\", num_hidden=10)\n# defining the output layer with softmax activation function to obtain # class probabilities \nsoftmax <- mx.symbol.SoftmaxOutput(fc3, name=\"sm\")\n# defining that the experiment should run on cpu\ndevices <- mx.cpu()\n# setting the seed for the experiment so as to ensure that the results # are reproducible\nmx.set.seed(0)\n# building the model with the network architecture defined above\nmodel <- mx.model.FeedForward.create(softmax, X=train.x, y=train.y,\nctx=devices, num.round=10, array.batch.size=100,array.layout =\"rowmajor\",\nlearning.rate=0.07, momentum=0.9,  eval.metric=mx.metric.accuracy,\ninitializer=mx.init.uniform(0.07), \nepoch.end.callback=mx.callback.log.train.metric(100))\n```", "```py\nStart training with 1 devices\n[1] Train-accuracy=0.885783334343384\n[2] Train-accuracy=0.963616671562195\n[3] Train-accuracy=0.97510000983874\n[4] Train-accuracy=0.980016676982244\n[5] Train-accuracy=0.984233343303204\n[6] Train-accuracy=0.986883342464765\n[7] Train-accuracy=0.98848334223032\n[8] Train-accuracy=0.990800007780393\n[9] Train-accuracy=0.991300007204215\n[10] Train-accuracy=0.991516673564911\n```", "```py\n# making predictions on the test dataset\npreds <- predict(model, test)\n# verifying the predicted output\nprint(dim(preds))\n# getting the label for each observation in test dataset; the\n# predicted class is the one with highest probability\npred.label <- max.col(t(preds)) - 1\n# observing the distribution of predicted labels in the test dataset\nprint(table(pred.label))\n```", "```py\n[1]    10 10000\npred.label\n   0    1    2    3    4    5    6    7    8    9 \n 980 1149 1030 1021 1001  869  960 1001  964 1025 \n```", "```py\n# obtaining the performance of the model\nprint(accuracy(pred.label,test.y))\n```", "```py\nAccuracy (PCC): 97.73% \nCohen's Kappa: 0.9748 \nUsers accuracy: \n   0    1    2    3    4    5    6    7    8    9 \n98.8 99.6 98.0 97.7 98.3 96.1 97.9 96.3 96.6 97.7 \nProducers accuracy: \n   0    1    2    3    4    5    6    7    8    9 \n98.8 98.3 98.2 96.7 96.4 98.6 97.7 98.9 97.6 96.2 \nConfusion matrix \n   y\nx      0    1    2    3    4    5    6    7    8    9\n  0  968    0    1    1    1    2    3    1    2    1\n  1    1 1130    3    0    0    1    3    8    1    2\n  2    0    1 1011    2    2    0    0   11    3    0\n  3    1    2    6  987    0   14    2    2    4    3\n  4    1    0    2    1  965    2   10    3    6   11\n  5    1    0    0    4    0  857    2    0    3    2\n  6    5    2    3    0    4    5  938    0    3    0\n  7    0    0    2    2    1    1    0  990    3    2\n  8    1    0    4    8    0    5    0    3  941    2\n  9    2    0    0    5    9    5    0   10    8  986\n```", "```py\n# Visualizing the network architecture\ngraph.viz(model$symbol)\n```", "```py\nmodel <- mx.model.FeedForward.create(softmax, X=train.x, y=train.y,\nctx=devices, num.round=50, array.batch.size=100,array.layout =\"rowmajor\",\nlearning.rate=0.07, momentum=0.9,  eval.metric=mx.metric.accuracy,\ninitializer=mx.init.uniform(0.07), \nepoch.end.callback=mx.callback.log.train.metric(100))\n```", "```py\n[35] Train-accuracy=0.999933333396912\n[36] Train-accuracy=1\n[37] Train-accuracy=1\n[38] Train-accuracy=1\n[39] Train-accuracy=1\n[40] Train-accuracy=1\n[41] Train-accuracy=1\n[42] Train-accuracy=1\n[43] Train-accuracy=1\n[44] Train-accuracy=1\n[45] Train-accuracy=1\n[46] Train-accuracy=1\n[47] Train-accuracy=1\n[48] Train-accuracy=1\n[49] Train-accuracy=1\n[50] Train-accuracy=1\n[1]    10 10000\npred.label\n   0    1    2    3    4    5    6    7    8    9 \n 992 1139 1029 1017  983  877  953 1021  972 1017 \nAccuracy (PCC): 98.21% \nCohen's Kappa: 0.9801 \nUsers accuracy: \n   0    1    2    3    4    5    6    7    8    9 \n99.3 99.5 98.2 98.2 98.1 97.1 98.0 97.7 98.0 97.8 \nProducers accuracy: \n   0    1    2    3    4    5    6    7    8    9 \n98.1 99.1 98.4 97.5 98.0 98.7 98.5 98.3 98.3 97.1 \nConfusion matrix \n   y\nx      0    1    2    3    4    5    6    7    8    9\n  0  973    0    2    2    1    3    5    1    3    2\n  1    1 1129    0    0    1    1    3    2    0    2\n  2    1    0 1013    1    3    0    0    9    2    0\n  3    0    1    5  992    0   10    1    1    3    4\n  4    0    0    2    0  963    2    7    1    1    7\n  5    0    0    0    4    1  866    2    0    2    2\n  6    2    2    1    0    3    5  939    0    1    0\n  7    0    1    6    3    1    1    0 1004    2    3\n  8    1    1    3    4    0    2    1    3  955    2\n  9    2    1    0    4    9    2    0    7    5  987\n```", "```py\ndropout1 <- mx.symbol.Dropout(data = act1, p = 0.5)\ndropout2 <- mx.symbol.Dropout(data = act2, p = 0.3)\n```", "```py\n# code to read the dataset and transform it to train.x and train.y remains # same as earlier project, therefore that code is not shown here\n# including the required mxnet library \nlibrary(mxnet)\n# defining the input layer in the network architecture\ndata <- mx.symbol.Variable(\"data\")\n# defining the first hidden layer with 128 neurons and also naming the # layer as fc1\n# passing the input data layer as input to the fc1 layer\nfc1 <- mx.symbol.FullyConnected(data, name=\"fc1\", num_hidden=128)\n# defining the ReLU activation function on the fc1 output and also naming the layer as ReLU1\nact1 <- mx.symbol.Activation(fc1, name=\"ReLU1\", act_type=\"relu\")\n# defining a 50% dropout of weights learnt\ndropout1 <- mx.symbol.Dropout(data = act1, p = 0.5)\n# defining the second hidden layer with 64 neurons and also naming the layer as fc2\n# passing the previous dropout output as input to the fc2 layer\nfc2 <- mx.symbol.FullyConnected(dropout1, name=\"fc2\", num_hidden=64)\n# defining the ReLU activation function on the fc2 output and also naming the layer as ReLU2\nact2 <- mx.symbol.Activation(fc2, name=\"ReLU2\", act_type=\"relu\")\n# defining a dropout with 30% weight drop\ndropout2 <- mx.symbol.Dropout(data = act2, p = 0.3)\n# defining the third and final hidden layer in our network with 10 neurons and also naming the layer as fc3\n# passing the previous dropout output as input to the fc3 layer\nfc3 <- mx.symbol.FullyConnected(dropout2, name=\"fc3\", num_hidden=10)\n# defining the output layer with softmax activation function to\nobtain class probabilities \nsoftmax <- mx.symbol.SoftmaxOutput(fc3, name=\"sm\")\n# defining that the experiment should run on cpu\ndevices <- mx.cpu()\n# setting the seed for the experiment so as to ensure that the results are reproducible\nmx.set.seed(0)\n# building the model with the network architecture defined above\nmodel <- mx.model.FeedForward.create(softmax, X=train.x, y=train.y, ctx=devices, num.round=50, array.batch.size=100,array.layout = \"rowmajor\", learning.rate=0.07, momentum=0.9,  eval.metric=mx.metric.accuracy, initializer=mx.init.uniform(0.07), epoch.end.callback=mx.callback.log.train.metric(100))\n# making predictions on the test dataset\npreds <- predict(model, test)\n# verifying the predicted output\nprint(dim(preds))\n# getting the label for each observation in test dataset; the predicted class is the one with highest probability\npred.label <- max.col(t(preds)) - 1\n# observing the distribution of predicted labels in the test\ndataset\nprint(table(pred.label))\n# including the rfUtilities library so as to use accuracy function\nlibrary(rfUtilities)\n# obtaining the performance of the model\nprint(accuracy(pred.label,test.y))\n# printing the network architecture\ngraph.viz(model$symbol) \n```", "```py\n[35] Train-accuracy=0.958950003186862\n[36] Train-accuracy=0.958983335793018\n[37] Train-accuracy=0.958083337446054\n[38] Train-accuracy=0.959683336317539\n[39] Train-accuracy=0.95990000406901\n[40] Train-accuracy=0.959433337251345\n[41] Train-accuracy=0.959066670437654\n[42] Train-accuracy=0.960250004529953\n[43] Train-accuracy=0.959983337720235\n[44] Train-accuracy=0.960450003842513\n[45] Train-accuracy=0.960150004227956\n[46] Train-accuracy=0.960533337096373\n[47] Train-accuracy=0.962033336758614\n[48] Train-accuracy=0.96005000303189\n[49] Train-accuracy=0.961366670827071\n[50] Train-accuracy=0.961350003282229\n[1]    10 10000\npred.label\n   0    1    2    3    4    5    6    7    8    9 \n 984 1143 1042 1022  996  902  954 1042  936  979 \nAccuracy (PCC): 97.3% \nCohen's Kappa: 0.97 \nUsers accuracy: \n   0    1    2    3    4    5    6    7    8    9 \n98.7 98.9 98.1 97.6 98.2 97.3 97.6 97.4 94.3 94.7 \nProducers accuracy: \n   0    1    2    3    4    5    6    7    8    9 \n98.3 98.3 97.1 96.5 96.8 96.2 98.0 96.1 98.1 97.7 \nConfusion matrix \n   y\nx      0    1    2    3    4    5    6    7    8    9\n  0  967    0    0    0    0    2    5    1    6    3\n  1    0 1123    3    0    1    1    3    5    2    5\n  2    1    2 1012    4    3    0    0   14    4    2\n  3    2    1    4  986    0    6    1    3   12    7\n  4    0    0    3    0  964    2    5    0    5   17\n  5    2    3    0    9    0  868    7    0    9    4\n  6    3    2    0    0    5    3  935    0    6    0\n  7    4    1    9    4    3    3    0 1001    6   11\n  8    1    3    1    2    1    3    2    1  918    4\n  9    0    0    0    5    5    4    0    3    6  956\n```", "```py\n## setting the working directory\nsetwd('/home/sunil/Desktop/book/chapter 6/MNIST')\n# function to load image files\nload_image_file = function(filename) {\n  ret = list()\n  f = file(filename, 'rb')\n  readBin(f, 'integer', n = 1, size = 4, endian = 'big')\n  n    = readBin(f, 'integer', n = 1, size = 4, endian = 'big')\n  nrow = readBin(f, 'integer', n = 1, size = 4, endian = 'big')\n  ncol = readBin(f, 'integer', n = 1, size = 4, endian = 'big')\n  x = readBin(f, 'integer', n = n * nrow * ncol, size = 1, signed\n= FALSE)\n  close(f)\n  data.frame(matrix(x, ncol = nrow * ncol, byrow = TRUE))\n}\n# function to load label files\nload_label_file = function(filename) {\n  f = file(filename, 'rb')\n  readBin(f, 'integer', n = 1, size = 4, endian = 'big')\n  n = readBin(f, 'integer', n = 1, size = 4, endian = 'big')\n  y = readBin(f, 'integer', n = n, size = 1, signed = FALSE)\n  close(f)\n  y\n}\n# load images\ntrain = load_image_file(\"train-images-idx3-ubyte\")\ntest  = load_image_file(\"t10k-images-idx3-ubyte\")\n# converting the train and test data into a format as required by LeNet\ntrain.x <- t(data.matrix(train))\ntest <- t(data.matrix(test))\n# loading the labels\ntrain.y = load_label_file(\"train-labels-idx1-ubyte\")\ntest.y  = load_label_file(\"t10k-labels-idx1-ubyte\")\n# linearly transforming the grey scale image i.e. between 0 and 255 to # 0 and 1\ntrain.x <- train.x/255\ntest <- test/255\n# including the required mxnet library \nlibrary(mxnet)\n# input\ndata <- mx.symbol.Variable('data')\n# first convolution layer\nconv1 <- mx.symbol.Convolution(data=data, kernel=c(5,5), num_filter=20)\n# applying the tanh activation function\ntanh1 <- mx.symbol.Activation(data=conv1, act_type=\"tanh\")\n# applying max pooling \npool1 <- mx.symbol.Pooling(data=tanh1, pool_type=\"max\", kernel=c(2,2), stride=c(2,2))\n# second conv\nconv2 <- mx.symbol.Convolution(data=pool1, kernel=c(5,5), num_filter=50)\n# applying the tanh activation function again\ntanh2 <- mx.symbol.Activation(data=conv2, act_type=\"tanh\")\n#performing max pooling again\npool2 <- mx.symbol.Pooling(data=tanh2, pool_type=\"max\",\n                           kernel=c(2,2), stride=c(2,2))\n# flattening the data\nflatten <- mx.symbol.Flatten(data=pool2)\n# first fullconnected later\nfc1 <- mx.symbol.FullyConnected(data=flatten, num_hidden=500)\n# applying the tanh activation function\ntanh3 <- mx.symbol.Activation(data=fc1, act_type=\"tanh\")\n# second fullconnected layer\nfc2 <- mx.symbol.FullyConnected(data=tanh3, num_hidden=10)\n# defining the output layer with softmax activation function to obtain # class probabilities \nlenet <- mx.symbol.SoftmaxOutput(data=fc2)\n# transforming the train and test dataset into a format required by \n# MxNet functions\ntrain.array <- train.x\ndim(train.array) <- c(28, 28, 1, ncol(train.x))\ntest.array <- test\ndim(test.array) <- c(28, 28, 1, ncol(test))\n# setting the seed for the experiment so as to ensure that the\n# results are reproducible\nmx.set.seed(0)\n# defining that the experiment should run on cpu\ndevices <- mx.cpu()\n# building the model with the network architecture defined above\nmodel <- mx.model.FeedForward.create(lenet, X=train.array, y=train.y,\nctx=devices, num.round=3, array.batch.size=100, learning.rate=0.05, \nmomentum=0.9, wd=0.00001, eval.metric=mx.metric.accuracy, \n           epoch.end.callback=mx.callback.log.train.metric(100))\n# making predictions on the test dataset\npreds <- predict(model, test.array)\n# getting the label for each observation in test dataset; the\n# predicted class is the one with highest probability\npred.label <- max.col(t(preds)) - 1\n# including the rfUtilities library so as to use accuracy\nfunction\nlibrary(rfUtilities)\n# obtaining the performance of the model\nprint(accuracy(pred.label,test.y))\n# printing the network architecture\ngraph.viz(model$symbol,direction=\"LR\")\n```", "```py\nStart training with 1 devices\n[1] Train-accuracy=0.678916669438283\n[2] Train-accuracy=0.978666676680247\n[3] Train-accuracy=0.98676667680343\nAccuracy (PCC): 98.54% \nCohen's Kappa: 0.9838 \nUsers accuracy: \n    0     1     2     3     4     5     6     7     8     9 \n 99.8 100.0  97.0  98.4  98.9  98.2  98.2  98.7  98.2  97.8 \nProducers accuracy: \n   0    1    2    3    4    5    6    7    8    9 \n98.0 96.9 99.1 99.3 99.0 99.3 99.6 97.7 98.7 98.3  \nConfusion matrix \n   y\nx      0    1    2    3    4    5    6    7    8    9\n  0  978    0    2    2    1    3    7    0    4    1\n  1    0 1135   15    2    1    0    5    7    1    5\n  2    0    0 1001    2    1    1    0    3    2    0\n  3    0    0    0  994    0    5    0    1    1    0\n  4    0    0    1    0  971    0    1    0    0    8\n  5    0    0    0    3    0  876    2    0    1    0\n  6    0    0    0    0    2    1  941    0    1    0\n  7    1    0    7    1    3    1    0 1015    3    8\n  8    1    0    6    1    1    1    2    1  956    0\n  9    0    0    0    5    2    4    0    1    5  987\n```", "```py\nReLU1 <- mx.symbol.Activation(data=conv1, act_type=\"relu\")\npool1 <- mx.symbol.Pooling(data=ReLU1, pool_type=\"max\",\n                           kernel=c(2,2), stride=c(2,2))\nReLU2 <- mx.symbol.Activation(data=conv1, act_type=\"relu\")\npool2 <- mx.symbol.Pooling(data=ReLU2, pool_type=\"max\",\n                           kernel=c(2,2), stride=c(2,2))\nReLU3 <- mx.symbol.Activation(data=conv1, act_type=\"relu\")\nfc2 <- mx.symbol.FullyConnected(data=ReLU3, num_hidden=10)\n```", "```py\nStart training with 1 devices\n[1] Train-accuracy=0.627283334874858\n[2] Train-accuracy=0.979916676084201\n[3] Train-accuracy=0.987366676231225\nAccuracy (PCC): 98.36% \nCohen's Kappa: 0.9818 \nUsers accuracy: \n   0    1    2    3    4    5    6    7    8    9 \n99.8 99.7 97.9 99.4 98.6 96.5 97.7 98.2 97.4 97.9 \nProducers accuracy: \n   0    1    2    3    4    5    6    7    8    9 \n97.5 97.2 99.6 95.6 99.7 99.2 99.7 98.0 99.6 98.2 \nConfusion matrix \n   y\nx      0    1    2    3    4    5    6    7    8    9\n  0  978    0    3    1    1    2   12    0    5    1\n  1    1 1132    6    0    2    1    5   11    1    6\n  2    0    0 1010    1    0    0    0    1    2    0\n  3    0    2    4 1004    0   23    1    3    9    4\n  4    0    0    1    0  968    0    1    0    0    1\n  5    0    1    0    1    0  861    2    0    3    0\n  6    0    0    0    0    0    3  936    0    0    0\n  7    1    0    6    3    0    1    0 1010    1    9\n  8    0    0    2    0    1    0    1    0  949    0\n  9    0    0    0    0   10    1    0    3    4  988\n```", "```py\n# loading the required libraries\nlibrary(mxnet)\nlibrary(imager)\n# loading the inception_bn model to memory\nmodel = mx.model.load(\"/home/sunil/Desktop/book/chapter 6/Inception/Inception_BN\", iteration=39)\n# loading the mean image\nmean.img = as.array(mx.nd.load(\"/home/sunil/Desktop/book/chapter 6/Inception/mean_224.nd\")[[\"mean_img\"]])\n# loading the image that need to be classified\nim <- load.image(\"/home/sunil/Desktop/book/chapter 6/image1.jpeg\")\n# displaying the image\nplot(im)\n```", "```py\n# function to pre-process the image so as to be consumed by predict function that is using inception_bn model\npreproc.image <- function(im, mean.image) {\n  # crop the image\n  shape <- dim(im)\n  short.edge <- min(shape[1:2])\n  xx <- floor((shape[1] - short.edge) / 2)\n  yy <- floor((shape[2] - short.edge) / 2)\n  cropped <- crop.borders(im, xx, yy)\n  # resize to 224 x 224, needed by input of the model.\n  resized <- resize(cropped, 224, 224)\n  # convert to array (x, y, channel)\n  arr <- as.array(resized) * 255\n  dim(arr) <- c(224, 224, 3)\n  # subtract the mean\n  normed <- arr - mean.img\n  # Reshape to format needed by mxnet (width, height, channel,\nnum)\n  dim(normed) <- c(224, 224, 3, 1)\n  return(normed)\n}\n# calling the image pre-processing function on the image to be classified\nnormed <- preproc.image(im, mean.img)\n# predicting the probabilties of labels for the image using the pre-trained model\nprob <- predict(model, X=normed)\n# sorting and filtering the top three labels with highest\nprobabilities\nmax.idx <- order(prob[,1], decreasing = TRUE)[1:3]\n# printing the ids with highest probabilities\nprint(max.idx)\n```", "```py\n[1] 471 627 863\n```", "```py\n# loading the pre-trained labels from inception_bn model \nsynsets <- readLines(\"/home/sunil/Desktop/book/chapter\n6/Inception/synset.txt\")\n# printing the english labels corresponding to the top 3 ids with highest probabilities\nprint(paste0(\"Predicted Top-classes: \", synsets[max.idx]))\n```", "```py\n[1] \"Predicted Top-classes: n02948072 candle, taper, wax light\"        \n[2] \"Predicted Top-classes: n03666591 lighter, light, igniter, ignitor\"\n[3] \"Predicted Top-classes: n04456115 torch\"      \n```", "```py\nim2 <- load.image(\"/home/sunil/Desktop/book/chapter 6/image2.jpeg\")\nplot(im2)\nnormed <- preproc.image(im2, mean.img)\nprob <- predict(model, X=normed)\nmax.idx <- order(prob[,1], decreasing = TRUE)[1:3]\nprint(paste0(\"Predicted Top-classes: \", synsets[max.idx]))\n```", "```py\n[1] \"Predicted Top-classes: n03529860 home theater, home theatre\"   \n[2] \"Predicted Top-classes: n03290653 entertainment center\"         [3] \"Predicted Top-classes: n04404412 television, television system\"\n```", "```py\n# getting the labels for third image\nim3 <- load.image(\"/home/sunil/Desktop/book/chapter\n6/image3.jpeg\")\nplot(im3)\nnormed <- preproc.image(im3, mean.img)\nprob <- predict(model, X=normed)\nmax.idx <- order(prob[,1], decreasing = TRUE)[1:3]\nprint(paste0(\"Predicted Top-classes: \", synsets[max.idx]))\n```", "```py\n[1] \"Predicted Top-classes: n04326547 stone wall\" \n[2] \"Predicted Top-classes: n03891251 park bench\" \n[3] \"Predicted Top-classes: n04604644 worm fence, snake fence, snake-rail fence, Virginia fence\"\n```"]