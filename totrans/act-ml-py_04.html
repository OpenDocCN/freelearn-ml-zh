<html><head></head><body>
<div id="book-content" class="calibre2">
<div id="sbo-rt-content" class="calibre3"><div id="_idContainer054" class="calibre4">
			<h1 id="_idParaDest-39" class="calibre8"><a id="_idTextAnchor040" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>3</h1>
			<h1 id="_idParaDest-40" class="calibre8"><a id="_idTextAnchor041" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Managing the Human in the Loop</h1>
			<p class="calibre6">Active ML promises more efficient ML by intelligently selecting the most informative samples for labeling by human oracles. However, the success of these human-in-the-loop systems depends on effective interface design and workflow management. In this chapter, we will cover best practices for optimizing the human role in active ML. First, we will explore interactive system design, discussing how to create labeling interfaces that enable efficient and accurate annotations. Next, we will provide an extensive overview of the leading human-in-the-loop frameworks for managing the labeling pipeline. We will then turn to handling model-label disagreements through adjudication and quality control. After that, we will discuss strategies for recruiting qualified labelers and managing them effectively. Finally, we will examine techniques for evaluating and ensuring high-quality annotations and properly balanced datasets. By the end of this chapter, you will have the skills to build optimized human-in-the-loop systems that fully leverage the symbiosis between humans <span>and AI.</span></p>
			<p class="calibre6">In this chapter, we will discuss the <span>following topics:</span></p>
			<ul class="calibre16">
				<li class="calibre20">Designing interactive learning systems <span>and workflows</span></li>
				<li class="calibre20">Handling <span>model-label disagreements</span></li>
				<li class="calibre20">Effectively managing <span>human-in-the-loop systems</span></li>
				<li class="calibre20">Ensuring annotation quality and <span>dataset balance</span></li>
			</ul>
			<h1 id="_idParaDest-41" class="calibre8"><a id="_idTextAnchor042" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Technical requirements</h1>
			<p class="calibre6">In this chapter, we will be using the <strong class="source-inline">huggingface</strong> package, so you’ll need to install it, <span>as follows:</span></p>
			<pre class="source-code">
pip install datasets transformers huggingface_hub &amp;&amp; apt-get install git-lfs</pre>			<p class="calibre6">Plus, you will need the <span>following imports:</span></p>
			<pre class="source-code">
from transformers import pipeline
import torch
from datasets import load_dataset
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score</pre>			<h1 id="_idParaDest-42" class="calibre8"><a id="_idTextAnchor043" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Designing interactive learning systems and workflows</h1>
			<p class="calibre6">The effectiveness of a human-in-the-loop system depends heavily on how well the labeling interface and workflow <a id="_idIndexMarker125" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>are designed. Even with advanced active ML algorithms selecting the most useful data points, poor interface design can cripple the labeling process. Without intuitive controls, informative queries, and efficient workflows adapted to humans, annotation quality and speed <span>will suffer.</span></p>
			<p class="calibre6">In this section, we will cover best practices for optimizing the human experience when interacting with active ML systems. Following these guidelines will enable you to create intuitive labeling pipelines, minimize ambiguity, and streamline the labeling process as much as possible. We will also discuss strategies for integrating active ML queries, collecting labeler feedback, and combining expert and crowd labelers. By focusing on human-centered design, you can develop interactive systems that maximize the utility of human input for <span>your models.</span></p>
			<p class="calibre6">To begin, we’ll provide definitions for the terms mentioned previously as they will be the main focus of <span>this section.</span></p>
			<p class="calibre6">A <strong class="bold">labeling interface</strong> is a user interface <a id="_idIndexMarker126" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>through which human annotators provide labels for data samples, such as <em class="italic">Roboflow</em>, <em class="italic">Encord</em>, and <em class="italic">LabelBox</em>, to name a few. It includes the visual presentation of each sample, as well as the controls and mechanisms for entering or selecting the desired labels. For example, an object detection labeling interface may display an image and provide tools to draw bounding boxes around objects and select class labels for each box. The annotator can then label cars, pedestrians, animals, and other objects appearing in the image using the interface controls. This can be seen in <span><em class="italic">Figure 3</em></span><em class="italic">.1</em>, where the bounding boxes have been drawn around the dogs using the <span><em class="italic">dog</em></span><span> class:</span></p>
			<div class="calibre18">
				<div id="_idContainer048" class="img---figure">
					<img src="image/B21789_03_01.jpg" alt="Figure 3.1 – Example of a labeling interface where the annotator is drawing bounding boxes around the dogs" class="calibre60"/>
				</div>
			</div>
			<p class="calibre6" lang="en-US" xml:lang="en-US">Figure 3.1 – Example of a labeling interface where the annotator is drawing bounding boxes around the dogs</p>
			<p class="calibre6">Now, let’s talk <span>about workflows.</span></p>
			<p class="calibre6">A <strong class="bold">workflow</strong> is the end-to-end sequence <a id="_idIndexMarker127" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>of steps that’s followed by the annotator to complete the labeling task. It encompasses the full life <a id="_idIndexMarker128" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>cycle of a labeling job, from receiving the samples to be labeled, interacting with the interface to apply labels, submitting the completed annotations, and potentially handling exceptions or errors, as depicted in <span><em class="italic">Figure 3</em></span><em class="italic">.2</em>. Optimizing the workflow involves streamlining these steps so that annotators can complete labeling efficiently. For an image classification task, the workflow may proceed <span>as follows:</span></p>
			<ol class="calibre16">
				<li class="calibre17">The annotator logs in to the labeling system and receives a batch of images <span>to label.</span></li>
				<li class="calibre17">The annotator is shown the first image and uses the interface to <span>apply labels.</span></li>
				<li class="calibre17">The annotator submits the labeled image and moves on to the <span>next image.</span></li>
				<li class="calibre17">After labeling the batch, the annotator submits the job, which triggers a reviewing stage for a quality check to <span>be performed.</span></li>
				<li class="calibre17">A reviewer checks the labeled images for accuracy <span>and consistency.</span></li>
				<li class="calibre17">If any errors are found, the images with incorrect annotations are returned to the annotator, at which point they need to re-label <span>those images.</span></li>
				<li class="calibre17">Once all the images pass<a id="_idIndexMarker129" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> the quality checks, the job is marked <span>as complete:</span></li>
			</ol>
			<div class="calibre18">
				<div id="_idContainer049" class="img---figure">
					<img src="image/B21789_03_02.jpg" alt="Figure 3.2 – Illustration of the labeling workflow" class="calibre61"/>
				</div>
			</div>
			<p class="calibre6" lang="en-US" xml:lang="en-US">Figure 3.2 – Illustration of the labeling workflow</p>
			<p class="calibre6">In short, the labeling interface focuses on the specific moment of interacting with an individual sample and applying labels. The workflow looks more holistically at the overall process and how to smoothly guide annotators through their labeling work. An effective human-in-the-loop system needs to design both aspects carefully around the <span>human user.</span></p>
			<p class="calibre6">Now that we know what a labeling interface and a workflow are, we understand that they can greatly impact the efficiency, accuracy, and overall quality of annotations in an active ML system. When designing interactive systems, there are several <span>key considerations:</span></p>
			<ul class="calibre16">
				<li class="calibre20"><strong class="bold">Intuitive and efficient interfaces</strong>: The labeling interface should be intuitive and easy to use. When choosing the labeling interface that you want to use for a project, keep in mind that the UI the labelers will use has to be simple and efficient. For example, is it easy to draw a bounding box or a polygon around the objects of interest in a computer vision annotation project? Are there features to speed up the labeling process, such as using a pre-trained model such as Meta’s <strong class="bold">Segment Anything Model</strong> (<strong class="bold">SAM</strong>) (<a href="https://segment-anything.com/" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3">https://segment-anything.com/</a>), which can segment any object in images and pre-label the objects on the image? A good example of this is the Smart Polygon feature provided by the<a id="_idIndexMarker130" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> labeling platform Roboflow, which allows users to automatically label objects with polygons or bounding boxes, as shown in <span><em class="italic">Figure 3</em></span><em class="italic">.3</em>. We will discuss Roboflow later in <span>this chapter:</span></li>
			</ul>
			<p class="calibre6"> </p>
			<div class="calibre18">
				<div id="_idContainer050" class="img---figure">
					<img src="image/B21789_03_03.jpg" alt="Figure 3.3 - Roboflow’s Smart Polygon feature using SAM to automatically label objects on images, as demonstrated at https://blog.roboflow.com/automated-polygon-labeling-computer-vision/" class="calibre62"/>
				</div>
			</div>
			<p class="calibre6" lang="en-US" xml:lang="en-US">Figure 3.3 - Roboflow’s Smart Polygon feature using SAM to automatically label objects on images, as demonstrated at <a href="https://blog.roboflow.com/automated-polygon-labeling-computer-vision/" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3">https://blog.roboflow.com/automated-polygon-labeling-computer-vision/</a></p>
			<ul class="calibre16">
				<li class="calibre20"><strong class="bold">Onboarding resources</strong>: Minimal training should be required for labelers to use the labeling platform. To minimize the amount of training needed, an effective approach is to provide an onboarding document initially written by someone knowledgeable about using the labeling interface. This document can then be edited and updated by the labelers themselves as they learn how to overcome obstacles. This way, the training for labelers becomes less exhaustive as they can utilize the knowledge gained by each labeler and pass on what they've learned to new members joining <span>the team.</span></li>
				<li class="calibre20"><strong class="bold">Ontologies – naming convention</strong>: An important consideration when designing a workflow for an annotation project is to carefully choose the names of the classes used for annotation. Having multiple spellings or words for the same class is a common issue. For example, in a project that aims to classify different types of pets, if a labeler decides to use the breed of the dog (for example, Australian Shepherd) instead of the class <em class="italic">dog</em>, it can cause issues later on. Fixing these issues is not always easy and can be time-consuming and expensive. Therefore, it is essential to select a labeling platform that allows the use of a labeling ontology. A <strong class="bold">labeling ontology</strong> is a <a id="_idIndexMarker131" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>structured vocabulary that provides a common understanding of how to label data, defines the classes, and outlines the relationships between different classes, especially in cases where nested structures<a id="_idIndexMarker132" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> exist. The ontology should also support the correction of typos or incorrect naming. In such cases, the names of the classes should be modifiable and update all objects labeled using that class. Additionally, the ontologies should be shareable across annotation projects to ensure consistency throughout. The ontology should support annotations of different styles, such as bounding boxes, polygons, classifications, and more. For example, the labeling platform Encord has a flexible labeling ontology feature, as depicted in <span><em class="italic">Figure 3</em></span><em class="italic">.4</em>. We will discuss Encord later in <span>this chapter:</span></li>
			</ul>
			<div class="calibre18">
				<div id="_idContainer051" class="img---figure">
					<img src="image/B21789_03_04.jpg" alt="Figure 3.4 – Encord’s labeling ontology" class="calibre63"/>
				</div>
			</div>
			<p class="calibre6" lang="en-US" xml:lang="en-US">Figure 3.4 – Encord’s labeling ontology</p>
			<ul class="calibre16">
				<li class="calibre20"><strong class="bold">Informative queries</strong>: The samples that are presented to labelers should provide sufficient context and information to ensure clarity in the labeling task. This may involve providing complete documents or images instead of just extracts. Queries should be designed to minimize any potential ambiguity. For example, when using<a id="_idIndexMarker133" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> active ML to select the most informative frames for labeling from unlabeled videos, it is crucial to carefully organize the workflow to ensure that labelers are aware of the specific video they are labeling. In this case, the solution would be to ensure that the selected frames are separated by video and sorted before being sent to the labeling platform. The frames should then be presented to the labelers as a video with <em class="italic">jumps</em> due to the absence of non-selected frames. However, it is important to note that these missing frames do not remove the overall context entirely. The labeling platform Encord offers a solution for cases like this one with<a id="_idIndexMarker134" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> their feature called <em class="italic">image sequences</em>. Their image sequences format presents groups of images as a video to <span>the labelers.</span></li>
				<li class="calibre20"><strong class="bold">Automated workflows</strong>: Labeling tasks should be automated as much as possible. The process of creating a task should involve minimal human intervention. One effective method for achieving this automation is by implementing a script that can run model inference on the unlabeled data pool, then use these predictions in the active ML sampling, particularly when utilizing uncertainty sampling, and, finally, send the selected data samples to the labeling platform and assign them to labelers and reviewers based on <span>their availability.</span></li>
				<li class="calibre20"><strong class="bold">Labeler feedback</strong>: Allowing labelers to provide feedback or ask questions on difficult or ambiguous samples enables the annotation quality to improve over time. Therefore, the labeling platform should include a commenting and chatting system, which would allow labelers to help each other or seek guidance from field experts <span>and reviewers.</span></li>
			</ul>
			<p class="calibre6">By focusing on these aspects, you <a id="_idIndexMarker135" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>can create labeling systems that are adapted to human strengths and limitations. Well-designed interfaces and workflows result in more accurate, consistent, and efficient annotations, which are essential for the success of an ML project. Now, let’s explore the current labeling platforms that you <span>can consider.</span></p>
			<h1 id="_idParaDest-43" class="calibre8"><a id="_idTextAnchor044" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Exploring human-in-the-loop labeling tools</h1>
			<p class="calibre6"><strong class="bold">Human-in-the-loop labeling frameworks</strong> are critical for enabling effective collaboration between humans and ML systems. In this section, we will explore some of the leading human-in-the-loop labeling tools for <span>active ML.</span></p>
			<p class="calibre6">We will look at how these<a id="_idIndexMarker136" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> frameworks allow humans to provide annotations, verify predictions, adjust model confidence thresholds, and guide model training through interfaces and workflows optimized for human-AI collaboration. Key capabilities provided by human-in-the-loop frameworks include annotation-assisted active ML, human verification of predictions, confidence calibration, and <span>model interpretability.</span></p>
			<p class="calibre6">The labeling tools we will examine include Snorkel AI, Prodigy, Encord, Roboflow, and others. We will walk through examples of how these tools can be leveraged to build applied active learning systems with effective human guidance. The strengths and weaknesses of different approaches will be discussed. By the end of this section, you will have a solid understanding of how to determine the right human-in-the-loop framework for your ML project<a id="_idIndexMarker137" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> based on your use case needs <span>and constraints.</span></p>
			<h2 id="_idParaDest-44" class="calibre9"><a id="_idTextAnchor045" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Common labeling platforms</h2>
			<p class="calibre6">Many labeling platforms offer a range <a id="_idIndexMarker138" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>of features for data labeling, including AI-assisted labeling, active ML, collaboration tools, and quality control tools. However, they vary in terms of pricing, model training, deployment capabilities, data management and curation tools, and model explainability features. For instance, when we examine six of the most frequently used labeling platforms, as depicted in <span><em class="italic">Figure 3</em></span><em class="italic">.5</em>, we can observe <span>these distinctions:</span></p>
			<div class="calibre18">
				<div id="_idContainer052" class="img---figure">
					<img src="image/B21789_03_05.jpg" alt="Figure 3.5 – Comparison table of six of the most common labeling platforms" class="calibre64"/>
				</div>
			</div>
			<p class="calibre6" lang="en-US" xml:lang="en-US">Figure 3.5 – Comparison table of six of the most common labeling platforms</p>
			<p class="calibre6">Overall, all of the labeling platforms mentioned previously offer a variety of advantages and disadvantages. It is important to choose a platform that is well suited to the specific needs and requirements of the <span>ML projects.</span></p>
			<p class="calibre6">If you are seeking a platform with a wide array of features, including AI-assisted labeling, active learning, collaboration<a id="_idIndexMarker139" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> tools, and <a id="_idIndexMarker140" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>quality control, then <a id="_idIndexMarker141" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Snorkel AI (<a href="https://snorkel.ai/" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3">https://snorkel.ai/</a>), Encord (<a href="https://encord.com/" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3">https://encord.com/</a>), LabelBox (<a href="https://labelbox.com/" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3">https://labelbox.com/</a>), and Dataloop (<a href="https://dataloop.ai/" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3">https://dataloop.ai/</a>) may <a id="_idIndexMarker142" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>be suitable options for you. On the<a id="_idIndexMarker143" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> other hand, if you require a platform specifically designed for <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) tasks, then<a id="_idIndexMarker144" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> Prodigy (<a href="https://prodi.gy/" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3">https://prodi.gy/</a>) might be a <span>good choice.</span></p>
			<p class="calibre6">Next, we’ll examine how to<a id="_idIndexMarker145" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> handle situations where the model and the <span>human disagree.</span></p>
			<h1 id="_idParaDest-45" class="calibre8"><a id="_idTextAnchor046" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Handling model-label disagreements</h1>
			<p class="calibre6">Disagreements between model<a id="_idIndexMarker146" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> predictions and human labels are inevitable. In this section, we will study how to identify and <span>resolve conflicts.</span></p>
			<h2 id="_idParaDest-46" class="calibre9"><a id="_idTextAnchor047" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Programmatically identifying mismatches</h2>
			<p class="calibre6">To identify discrepancies between the <a id="_idIndexMarker147" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>model’s predictions and the human-annotated labels, we can write some simple Python code that highlights the mismatches <span>for review.</span></p>
			<p class="calibre6">Let’s consider the example of an NLP sentiment classifier. This type of classifier is designed to analyze and understand the sentiment or emotions expressed in text. By examining the words, phrases, and context used in a given piece of text, an NLP sentiment classifier can determine whether the sentiment is positive, negative, or neutral. First, we will use the <strong class="source-inline">sentiment-analysis</strong> model <span>from Huggingface:</span></p>
			<pre class="source-code">
sentiment_pipeline = pipeline("sentiment-analysis")
data = ["I love you", "I hate you"]
sentiment_pipeline(data)</pre>			<p class="calibre6">The returns the <span>following output:</span></p>
			<pre class="source-code">
[{'label': 'POSITIVE', 'score': 0.9998656511306763},
{'label': 'NEGATIVE', 'score': 0.9991129040718079}]</pre>			<p class="calibre6">The model correctly classifies these two sentences. Now, we want to study what flags a mismatch between a labeled dataset and the model’s predictions for further review. So, we will download a labeled dataset from <strong class="source-inline">huggingface</strong> called <em class="italic">imdb</em>. This dataset is a large movie review dataset that’s used for binary sentiment classification. We can load this dataset with the following line <span>of code:</span></p>
			<pre class="source-code">
imdb = load_dataset("imdb")</pre>			<p class="calibre6">For testing purposes, we’ll only use a <span>few samples:</span></p>
			<pre class="source-code">
small_dataset = imdb["train"].shuffle(seed=120).
    select([i for i in list(range(5))])
print(small_dataset)</pre>			<p class="calibre6">This returns the <span>following output:</span></p>
			<pre class="source-code">
Dataset({
    features: ['text', 'label'],
    num_rows: 5
})</pre>			<p class="calibre6">We can take a better look at this <a id="_idIndexMarker148" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>dataset by looking at the last item in <span>the dataset:</span></p>
			<pre class="source-code">
small_dataset[-1]</pre>			<p class="calibre6">This gives us the <span>following output:</span></p>
			<pre class="source-code">
{'text': "Shot into car from through the windscreen, someone is playing someone else their latest song, someone else didn't react, according to the voice-over. I just wonder how that came to be made. There were too many scenes in this movie that I wondered about how come a camera was there. If the scenes shot where the Warhols descended on a BJM post-party are true then that was inexcusable exploitation to the max, if not, then it was a total fabrication, either way it made me uncomfortable, if that was the purpose? All the way thru this movie I kept wondering how the footage came about. Taken at face value, a nice portrait of the (tortured) genius we all believe ourselves to be.",
'label': 1}</pre>			<p class="calibre6">Here, we have a field called <strong class="source-inline">text</strong>, which provides a review of the movie, and a field called <strong class="source-inline">label</strong>, which classifies whether the sentiment is positive or negative. In this case, it <span>is positive.</span></p>
			<p class="calibre6">Let’s gather the model’s predictions on these <span>five samples:</span></p>
			<pre class="source-code">
classes = ['NEGATIVE', 'POSITIVE']
results = []
for review in small_dataset['text']:
results.append(classes.index(sentiment_pipeline(review)[0]['label']))
print(results)</pre>			<p class="calibre6">This returns the <span>following output:</span></p>
			<pre class="source-code">
[1, 0, 0, 1, 0]</pre>			<p class="calibre6">Now, to find out if we have <a id="_idIndexMarker149" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>mismatches with the original annotations from the dataset, we must define <strong class="source-inline">x_true</strong> and <strong class="source-inline">y_true</strong>, <span>as follows:</span></p>
			<pre class="source-code">
y_true = np.array(small_dataset['label'])
x_true = np.array(small_dataset['text'])</pre>			<p class="calibre6">Here, <strong class="source-inline">x_true</strong> is an array of the reviews and <strong class="source-inline">y_true</strong> is an array of labels. We can compare these to the <span>model’s predictions:</span></p>
			<pre class="source-code">
# Compare to true labels
mismatches = np.where(results != y_true)[0]
# Return mismatched samples
X_mismatched = x_true[mismatches]
y_mismatched = y_true[mismatches]
print(f"There are {len(X_mismatched)} mismatches: {X_mismatched}")</pre>			<p class="calibre6">This returns the <span>following output:</span></p>
			<pre class="source-code">
There are 2 mismatches: ['"Meatball Machine" has got to be one of the most complex ridiculous, awful and over-exaggerated sci-fi horror films that I have ever came across. It is about good against evil and a coming-of-age tale, with the aim of to entertain with bloody, sleazy and humorous context. Because of that the violence isn\'t particularly gruesome and it doesn\'t make you squirm, but the gratuitous bloodletting and nudity does run freely. The performances by Issei Takahashi and Toru Tezuka is the worst i have seen, if that was not enough it is also directed by an unheard of director called Yudai Yamaguchi. This movie just have it all, it is bad to the bone!, A must see for every b-movie freak!!!... Simply: an enjoying and rare gem.'
"Shot into car from through the windscreen, someone is playing someone else their latest song, someone else didn't react, according to the voice-over. I just wonder how that came to be made. There were too many scenes in this movie that I wondered about how come a camera was there. If the scenes shot where the Warhols descended on a BJM post-party are true then that was inexcusable exploitation to the max, if not, then it was a total fabrication, either way it made me uncomfortable, if that was the purpose? All the way thru this movie I kept wondering how the footage came about. Taken at face value, a nice portrait of the (tortured) genius we all believe ourselves to be."]</pre>			<p class="calibre6">Here, data points where<a id="_idIndexMarker150" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> the model and human disagree have been returned. These would be selected for additional review in <span>this case.</span></p>
			<h2 id="_idParaDest-47" class="calibre9"><a id="_idTextAnchor048" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Manual review of conflicts</h2>
			<p class="calibre6">After sampling the mismatched cases, we <a id="_idIndexMarker151" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>can do a manual review. Here are some <span>example scenarios:</span></p>
			<ul class="calibre16">
				<li class="calibre20">The model predicts <em class="italic">dog</em> but the human labeled it as <em class="italic">cat</em>. On review, the photo quality was poor and it was a dog. This is a <span>human error.</span></li>
				<li class="calibre20">The model predicts <em class="italic">negative</em> sentiment but the text was confidently <em class="italic">positive</em> according to the reviewer. This indicates a weakness in the model and needs to <span>be fixed.</span></li>
			</ul>
			<p class="calibre6">A manual review of model predictions provides valuable insights into the errors made by both the model and human labelers. One key strength is its ability to identify <strong class="bold">systematic biases</strong> in the training <a id="_idIndexMarker152" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>data, as well as cases where the model fails in ways that humans would not. However, a manual review is time-consuming and limited by human subjectivity and oversight. Typically, only a small subset of cases is reviewed, which may not uncover all weaknesses in the model. While a manual review serves as a useful debugging tool during model <a id="_idIndexMarker153" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>development, conducting large-scale reviews is often impractical. In such cases, alternative techniques such as active learning cycles may be necessary to further improve the <span>model’s robustness.</span></p>
			<p class="calibre6">Another way to utilize mismatched sampling is by including the mismatched samples in the active ML pool for re-labeling. This allows for a better understanding of confusing cases and enables the model to be trained to handle such cases more effectively. This iterative process of continuously adding and re-labeling data helps fine-tune the model without the need for <span>manual reviewing.</span></p>
			<p class="calibre6">By systematically identifying, understanding, and resolving model-label disagreements, the system improves over time. The key is to maintain human oversight in the process. In the next section, we will talk about how to manage <span>human-in-the-loop systems.</span></p>
			<h1 id="_idParaDest-48" class="calibre8"><a id="_idTextAnchor049" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Effectively managing human-in-the-loop systems</h1>
			<p class="calibre6">Getting high-quality annotations requires finding, vetting, supporting, and retaining effective labelers. It is crucial to <a id="_idIndexMarker154" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>build an appropriate labeling team that meets the requirements of the <span>ML project.</span></p>
			<p class="calibre6">The first option is to establish an internal labeling team. This involves hiring full-time employees to label data, which enables close management and training. Cultivating domain expertise is easier when done internally. However, there are drawbacks to this, such as higher costs and turnover. This option is only suitable for large, ongoing <span>labeling requirements.</span></p>
			<p class="calibre6">Another option is to crowdsource labeling tasks using platforms such as ScaleAI, which allow labeling tasks to be distributed to a large, on-demand workforce. This option provides flexibility and lower costs, but it can lack domain expertise. Quality control becomes challenging when working with anonymous <span>crowd workers.</span></p>
			<p class="calibre6">You could use third-party labeling services, such as Innovatiana, which specializes in providing trained annotators for ML projects. This option leverages existing labeling teams and workflows. However, it can be more costly than crowdsourcing and challenging <span>to manage.</span></p>
			<p class="calibre6">Lastly, hybrid options are also <a id="_idIndexMarker155" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>doable. For example, you could use a mixed strategy that combines third-party labelers with internal reviewers. The optimal approach depends on budget, timelines, data sensitivity, field expertise, and project scope. A combination of sources provides flexibility. The key is instituting strong training, validation, and monitoring to get the quality needed from any <span>labeling resource.</span></p>
			<p class="calibre6"><em class="italic">The next question here is how to manage the labeling </em><span><em class="italic">team efficiently.</em></span></p>
			<p class="calibre6">Managing a labeling team for maximum efficiency requires setting clear guidelines so that labelers understand expectations and the big-picture goals. As mentioned earlier in this chapter, workflows should be structured to optimize labelers’ time, automating where possible and minimizing redundant tasks. Providing good tools is <span>also key.</span></p>
			<p class="calibre6">When feasible, give labelers access to dashboards and metrics so that they can see the growing dataset. This keeps them engaged in the process. For example, in <span><em class="italic">Figure 3</em></span><em class="italic">.6</em>, we can see that in the Encord platform, the labelers can see how much they have labeled and how much they <span>have left:</span></p>
			<div class="calibre18">
				<div id="_idContainer053" class="img---figure">
					<img src="image/B21789_03_06.jpg" alt="Figure 3.6 – Encord’s Instance label tasks status for an annotation project" class="calibre65"/>
				</div>
			</div>
			<p class="calibre6" lang="en-US" xml:lang="en-US">Figure 3.6 – Encord’s Instance label tasks status for an annotation project</p>
			<p class="calibre6">Open communication channels allow labelers to easily discuss ambiguities, ask questions, and provide feedback. Make yourself accessible as a resource. Review labeling speed, accuracy, and costs to identify opportunities to improve productivity through enhancements to<a id="_idIndexMarker156" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> tools, training, <span>or workflows.</span></p>
			<p class="calibre6">Effective management also requires holding regular meetings to discuss progress, issues, and feedback. Designate senior labelers to help train and support newer members. Break large projects into stages with milestones to maintain focus. Highlight top performers and celebrate wins to motivate the team. Address underperformance through coaching <span>and training.</span></p>
			<p class="calibre6">With strong training, optimized workflows, communication, and performance management, a labeling team can <span>work efficiently.</span></p>
			<p class="calibre6">Another issue to be aware of and to address is that annotators can introduce biases based on their personal experiences, cultural backgrounds, or misunderstandings of the task at hand. This can lead to biased datasets where certain perspectives or characteristics are overrepresented or underrepresented, thus affecting the fairness and accuracy of ML models trained on these datasets. Biased annotations can potentially lead to AI models that perpetuate or even amplify these biases. This is particularly concerning in sensitive applications such as facial recognition, sentiment analysis, and predictive policing, where biased data can lead to unfair or discriminatory outcomes. As ML evolves, there’s a growing emphasis on developing general-purpose foundational models that significantly reduce<a id="_idIndexMarker157" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> the reliance on extensive <span>human labeling.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">A general-purpose foundational model is a versatile AI system that’s been trained on vast amounts of data that can be adapted or fine-tuned to perform a wide range of tasks across different domains without the need for task-specific training <span>from scratch.</span></p>
			<p class="calibre6">An example of such innovation is SAM when it is used as a feature on labeling platforms to help accelerate labeling, which embodies the capability to understand and segment various objects in images or videos without the need for explicit, detailed human annotation for every new object type. This not only streamlines the development process by requiring less manual labeling but also aims to mitigate bias by relying on generalized learning capabilities that can adapt to diverse scenarios without inheriting the specific biases of a small group of human annotators. However, the design and training of these foundational models still necessitate careful consideration of the data they’re trained on and the potential biases inherent in those datasets, highlighting the ongoing importance of fairness and ethical considerations in the field <span>of AI.</span></p>
			<p class="calibre6">Even with an efficient labeling team, can we ensure consistent quality of annotations over time? In the following section, we will explore this issue and discuss methods for ensuring the ongoing quality <span>of annotations.</span></p>
			<h1 id="_idParaDest-49" class="calibre8"><a id="_idTextAnchor050" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Ensuring annotation quality and dataset balance</h1>
			<p class="calibre6">Maintaining high annotation quality and target class balance requires diligent management. In this section, we’ll look at some techniques that can help assure <span>labeling quality.</span></p>
			<h2 id="_idParaDest-50" class="calibre9"><a id="_idTextAnchor051" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Assess annotator skills</h2>
			<p class="calibre6">It is highly recommended that<a id="_idIndexMarker158" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> annotators undergo thorough training sessions and complete qualification tests before they can work independently. This ensures that they have a solid foundation of knowledge and understanding in their respective tasks. These performance metrics can be visualized in the labeling platform when the reviewers accept or reject annotations. If a labeler has many rejected annotations, it is necessary to ensure that they understand the task and assess what help can be provided <span>to them.</span></p>
			<p class="calibre6">It is advisable to periodically assess the labeler’s skills by providing control samples for evaluation purposes. This ongoing evaluation helps maintain the quality and consistency of their work <span>over time.</span></p>
			<p class="calibre6">For example, designing datasets <a id="_idIndexMarker159" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>with known labels and asking the labelers to label these evaluation sets can be a good way to check if the task is well understood. Then, we can assess the accuracy of the annotations using a simple <span>Python script.</span></p>
			<p class="calibre6">First, we must define some dummy annotations that have been made by a labeler and some <span>real annotations:</span></p>
			<pre class="source-code">
dummy_annotator_labels = ['positive', 'negative', 'positive', 
    'positive', 'positive']
dummy_known_labels = ['negative', 'negative', 'positive', 'positive', 
    'negative']</pre>			<p class="calibre6">Then, we must calculate the accuracy and kappa score using the <span><strong class="source-inline">sklearn</strong></span><span> function:</span></p>
			<pre class="source-code">
accuracy = accuracy_score(dummy_annotator_labels, dummy_known_labels)
print(f"Annotator accuracy: {accuracy*100:.2f}%")
kappa = cohen_kappa_score(dummy_annotator_labels, dummy_known_labels)
print(f"Cohen's Kappa: {kappa:.3f}")</pre>			<p class="calibre6">This returns the <span>following output:</span></p>
			<pre class="source-code">
Annotator accuracy: 60.00%
Cohen's Kappa: 0.286</pre>			<p class="calibre6">This technique is a simple and easy way to implement a basic assessment of the annotator’s skills. Aim for an accuracy of above 90% and a kappa score of above 0.80 and then we can investigate <span>poor agreements.</span></p>
			<h2 id="_idParaDest-51" class="calibre9"><a id="_idTextAnchor052" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Use multiple annotators</h2>
			<p class="calibre6">If your budget allows, you can assign each data point to multiple annotators to identify conflicts. These conflicts can then<a id="_idIndexMarker160" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> be resolved through consensus or by an <span>expert reviewer.</span></p>
			<p class="calibre6">For example, with sentiment analytics labeling, we have our dummy annotations for <span>three labelers:</span></p>
			<pre class="source-code">
dummy_annotator_labels_1 = ['positive', 'negative', 'positive', 
    'positive', 'positive']
dummy_annotator_labels_2 = ['positive', 'negative', 'positive', 
    'negative', 'positive']
dummy_annotator_labels_3 = ['negative', 'negative', 'positive', 
    'positive', 'negative']</pre>			<p class="calibre6">We can create a pandas DataFrame with the labels from the <span>three labelers:</span></p>
			<pre class="source-code">
df = pd.DataFrame({
    "Annotator1": dummy_annotator_labels_1,
    "Annotator2": dummy_annotator_labels_2,
    "Annotator3": dummy_annotator_labels_3
})</pre>			<p class="calibre6">Then, we can take the majority vote as a <span>real label:</span></p>
			<pre class="source-code">
df["MajorityVote"] = df.mode(axis=1)[0]
print(df["MajorityVote"])</pre>			<p class="calibre6">This returns the <span>following output:</span></p>
			<pre class="source-code">
0    positive
1    negative
2    positive
3    positive
4    positive</pre>			<p class="calibre6">This method can be expensive because the labelers work on the same data, but it can ultimately result in more accurate annotations. Its feasibility depends on the priorities of the ML project, as well as <a id="_idIndexMarker161" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>the budget and organization of the labeling team. For instance, if the labeling team consists of junior labelers who are new to the field, this method may be a <span>suitable choice.</span></p>
			<h2 id="_idParaDest-52" class="calibre9"><a id="_idTextAnchor053" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Balanced sampling</h2>
			<p class="calibre6">To prevent imbalanced datasets, we can actively sample minority classes at higher rates during <span>data collection.</span></p>
			<p class="calibre6">When collecting a dataset, it is important to <a id="_idIndexMarker162" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>monitor the distribution of labels across classes and adjust sampling rates accordingly. Without intervention, datasets often end up skewed toward majority classes due to their natural <span>higher frequencies.</span></p>
			<p class="calibre6">Let’s look at some ways to actively sample minority classes at <span>higher rates:</span></p>
			<ul class="calibre16">
				<li class="calibre20">Employing active ML approaches <a id="_idIndexMarker163" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>such as <strong class="bold">uncertainty sampling</strong> can bias selection toward rare cases. Indeed, uncertainty sampling actively selects the data points that the current model is least certain about for labeling. These tend to be edge cases and rare examples, rather than the common cases the model has already seen many examples of. Since, by definition, minority classes occur less frequently, the model is naturally more uncertain about these classes. So, uncertainty sampling will tend to pick more examples from the under-represented classes for labeling to improve the <span>model’s understanding.</span></li>
				<li class="calibre20">Checking label distributions periodically during data collection is important. If minority classes are underrepresented, it is recommended to selectively sample more data points with those labels. This can be achieved by sampling the data from the unlabeled data pool using a pre-trained model that can identify the unrepresented classes. To ensure higher representation, the sampling strategy should be set to select specific classes with a higher ratio. For example, let’s reuse the <em class="italic">imdb</em> dataset from <span>Hugging Face:</span><pre class="source-code">
dataset = load_dataset('imdb')</pre><p class="list-inset">For testing purposes, we assume that the dataset is unlabeled and that the labels attached to it are from the model’s predictions. So, our goal is to sample the under-represented class. Let’s assume class <strong class="source-inline">0</strong> is under-represented and we want to over-sample it. First, we must take the training dataset as our dummy unlabeled data pool and convert it into a <span>pandas </span><span><strong class="source-inline">DataFrame</strong></span><span>:</span></p><pre class="source-code">dummy_unlabeled_dataset_with_predictions_from_a_model \
    dataset['train']
df = pd.DataFrame(
    dummy_unlabeled_dataset_with_predictions_from_a_model)</pre></li>				<li class="calibre20">Next, we must get the number of data points for <span>each label:</span><pre class="source-code">
n_label_0 = df[df['label'] == 0].shape[0]
n_label_1 = df[df['label'] == 1].shape[0]</pre></li>				<li class="calibre20">Now, we must calculate<a id="_idIndexMarker164" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> the number of samples to sample for each label, assuming we want to sample 1,000 samples and we want 80% of these samples to belong to class <strong class="source-inline">0</strong> and 20% to <span>class </span><span><strong class="source-inline">1</strong></span><span>:</span><pre class="source-code">
nb_samples = 1000
n_sample_0 = int(0.8 * nb_samples)
n_sample_1 = int(0.2 * nb_samples)
sample_0 = df[df['label'] == 0].sample(n_sample_0, 
    replace=False)
sample_1 = df[df['label'] == 1].sample(n_sample_1, 
    replace=False)
# Concatenate the two samples into a single dataframe
sample_df = pd.concat([sample_0, sample_1], ignore_index=True)
# Print the sample dataframe
print(f"We have {len(sample_df['label'][sample_df['label']==0])} class 0 samples and {len(sample_df['label'][sample_df['label']==1])} class 1 samples")</pre><p class="list-inset">This gives us the <span>following output:</span></p><pre class="source-code">We have 800 class 0 samples and 200 class 1 samples</pre><p class="list-inset">So, we sampled with the correct ratio and can, in theory, add these samples to our labeling queue next. By setting a higher sampling ratio for class <strong class="source-inline">0</strong> from the unlabeled data, we selectively oversample the minority class when getting new <span>labeled data.</span></p></li>			</ul>
			<p class="calibre6">The key is closely tracking the<a id="_idIndexMarker165" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> evolving label distribution and steering sampling toward under-represented classes. This prevents highly imbalanced datasets that fail to provide sufficient examples for minority classes. The result is higher-quality, more balanced <span>training data.</span></p>
			<h1 id="_idParaDest-53" class="calibre8"><a id="_idTextAnchor054" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Summary</h1>
			<p class="calibre6">This chapter explored strategies for effectively incorporating human input into active ML systems. We discussed how to design workflows that enable efficient collaboration between humans and AI models. Leading open source frameworks for human-in-the-loop learning were reviewed, including their capabilities for annotation, verification, and <span>active learning.</span></p>
			<p class="calibre6">Handling model-label disagreements is a key challenge in human-AI systems. Techniques such as manually reviewing conflicts and active learning cycles help identify and resolve mismatches. Carefully managing the human annotation workforce is also critical as it covers recruiters, training, quality control, <span>and tooling.</span></p>
			<p class="calibre6">A major focus was ensuring high-quality balanced datasets while using methods such as qualification exams, inter-annotator metrics such as the accuracy or the Kappa score, consensus evaluations, and targeted sampling. By implementing robust processes around collaboration, conflict resolution, annotator management, and data labeling quality, the usefulness of human input in the loop can <span>be maximized.</span></p>
			<p class="calibre6">In the next chapter, we will shift our focus to applying active ML approaches specifically for computer vision tasks such as image classification, semantic segmentation, and <span>object detection.</span></p>
		</div>
	</div>
</div>
</body></html>