["```py\nimport pandas as pd\npath = \"/content/UCI-News-Aggregator-Classifier/data/uci-news-aggregator.csv\"\ndf = pd.read_csv(path)\ndf.head()\n```", "```py\ndf[\"CATEGORY\"].value_counts().plot(kind = 'bar')\n```", "```py\ndf[\"HOSTNAME\"].value_counts()[:20].plot(kind = 'bar')\n```", "```py\nimport os\nroot = \"./articles\"\nfake = os.path.join(root, \"fake\")\nreal = os.path.join(root, \"real\")\nfor dir in [root, real, fake]:\n  if not os.path.exists(dir):\n    os.mkdir(dir)\n```", "```py\ndf2 = df.groupby('CATEGORY').apply(lambda x: x.sample(250))\n```", "```py\ndf2[\"CATEGORY\"].value_counts().plot(kind='bar')\n```", "```py\nfrom newspaper import Article\nURL_LIST = df2[\"URL\"].tolist()\nTITLE_LIST = df2[\"TITLE\"].tolist()\nfor id_url, article_url in enumerate(URL_LIST):\n  article = Article(article_url)\n  try:\n    # Download and parse article\n    article.download()\n    article.parse()\n    text = article.text\n    # Save to file\n    filename = os.path.join(real, \"Article_{}.txt\".format(id_url))\n    article_title = TITLE_LIST[id_url]\n    with open(filename, \"w\") as text_file:\n      text_file.write(\" %s \\n %s\" % (article_title, text))\n  except:\n    print(\"Could not download the article at: {}\".format(article_url))\n```", "```py\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='EleutherAI/gpt-neo-2.7B')\n```", "```py\nchat_prompt = 'Generate a five-line poem about flowers'\nmodel_output = generator(prompt,\n                 max_length=100)\nresponse = model_output[0]['generated_text']\nprint(response)\n```", "```py\nFlowers bloom in gardens bright,\nTheir petals open to the light,\nTheir fragrance sweet and pure,\nA colorful feast for eyes to lure,\nNature's art, forever to endure.\n```", "```py\nfor id_title, title in enumerate(TITLE_LIST):\n  # Generate the article\n  article = generator(title, max_length = 500)[0][\"generated_text\"]\n  # Save to file\n  filename = os.path.join(fake, \"Article_{}.txt\".format(id_url))\n  with open(filename, \"w\") as text_file:\n      text_file.write(\" %s \\n %s\" % (title, text))\n```", "```py\nX = []\nY = []\nfor file in os.listdir(real):\n  try:\n    with open(file, \"r\") as article_file:\n      article = file.read()\n      X.append(article)\n      Y.append(0)\n  except:\n    print(\"Error reading: {}\".format(file))\n    continue\nfor file in os.listdir(fake):\n  try:\n    with open(file, \"r\") as article_file:\n      article = file.read()\n      X.append(article)\n      Y.append(1)\n  except:\n    print(\"Error reading: {}\".format(file))\n    continue\n```", "```py\nFUNCTION_WORD_FILE = '../static/function_words.txt'\nwith open(FUNCTION_WORD_FILE,'r') as fwf:\n  k = fwf.readlines()\n  func_words = [w.rstrip() for w in k]\n  #There might be duplicates!\n  func_words = list(set(func_words))\ndef calculate_function_words(text):\n  function_word_counter = 0\n  text_length = len(text.split(' '))\n  for word in func_words:\n    function_word_counter = function_word_counter + text.count(word)\n  if text_length == 0:\n    feature = 0\n  else:\n    feature = function_word_counter / total_length\n  return feature\n```", "```py\ndef calculate_punctuation(text):\n  punctuations = =[ k for k in string.punctuation]\n  punctuation_counter = 0\n  total_length = len(text.split())\n  for punc in punctuations:\n    punctuation_counter = punctuation_counter + text.count(punc)\n  if text_length == 0:\n    feature = 0\n  else:\n    feature = punctuation_counter / total_length\n  return feature\n```", "```py\ndef calculate_ari(text):\n  chars = len(text.split())\n  words = len(text.split(' '))\n  sentences = len(text.split('.'))\n  if words == 0 or sentences == 0:\n    feature = 0\n  else:\n    feature = 4.71* (chars / words) + 0.5* (words / sentences) - 21.43\n  return feature\n```", "```py\nX_Features = []\nfor x in X:\n  feature_vector = []\n  feature_vector.append(calculate_function_words(x))\n  feature_vector.append(calculate_punctuation(x))\n  feature_vector.append(calculate_ari(x))\n  X_Features.append(feature_vector)\n```", "```py\nfrom sklearn.metrics import confusion_matrix\ndef evaluate_model(actual, predicted):\n  confusion = confusion_matrix(actual, predicted)\n  tn, fp, fn, tp = confusion.ravel()\n  total = tp + fp + tn + fn\n  accuracy = 100 * (tp + tn) / total\n  if tp + fp != 0:\n    precision = tp / (tp + fp)\n  else:\n    precision = 0\n  if tp + fn != 0:\n    recall = tp / (tp + fn)\n  else:\n    recall = 0\n  if precision == 0 or recall == 0:\n    f1 = 0\n  else:\n    f1 = 2 * precision * recall / (precision + recall)\n  evaluation = { 'accuracy': accuracy,'precision': precision,'recall': recall,'f1': f1}\n  return evaluation\n```", "```py\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X_Features, Y)\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train, Y_train)\nY_predicted = model.predict(X_test)\nprint(evaluate_model(Y_test, Y_pred))\n```", "```py\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(n_estimators = 100)\nmodel.fit(X_train, Y_train)\nY_predicted = model.predict(X_test)\nprint(evaluate_model(Y_test, Y_pred))\n```", "```py\nfrom sklearn.neural_network import MLPClassifier\nmodel = MLPClassifier(hidden_layer_sizes = (50, 25, 10),max_iter = 100,activation = 'relu',solver = 'adam',random_state = 123)\nmodel.fit(X_train, Y_train)\nY_predicted = model.predict(X_test)\nprint(evaluate_model(Y_test, Y_pred))\n```", "```py\nfrom sklearn import svm\nmodel = svm.SVC(kernel='linear')\nmodel.fit(X_train, Y_train)\nY_predicted = model.predict(X_test)\nprint(evaluate_model(Y_test, Y_pred))\n```", "```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntf_idf = TfidfVectorizer()\nX_train_TFIDF = tf_idf.fit_transform(X_train)\nX_test_TFIDF = tf_idf.transform(X_test)\n```", "```py\nfrom sklearn.neural_network import MLPClassifier\nmodel = MLPClassifier(hidden_layer_sizes = (300, 200, 100),max_iter = 100,activation = 'relu',solver = 'adam',random_state = 123)\nmodel.fit(X_train_TFIDF, Y_train)\nY_predicted = model.predict(X_test_TFIDF)\nprint(evaluate_model(Y_test, Y_pred))\n```", "```py\nimport nltk\nnltk.download('punkt')\ncorpus = []\nfor x in X_train:\n  # Split into sentences\n  sentences_tokens = nltk.sent_tokenize(x)\n  # Split each sentence into words\n  word_tokens = [nltk.word_tokenize(sent) for sent in sentences_tokens]\n  # Add to corpus\n  corpus = corpus + word_tokens\n```", "```py\nfrom gensim.models import Word2Vec\nmodel = Word2Vec(corpus, min_count=1, vector_size = 30)\n```", "```py\nX_train_vector_mean = []\nfor x in X_train:\n  # Create a 30-element vector with all zeroes\n  vector = [0 for _ in range(30)]\n  # Create a vector for out-of-vocab words\n  oov = [0 for _ in range(30)]\n  words = x.split(' ')\n  for word in words:\n    if word in model.wv.vocab:\n      # Word is present in the vocab\n      vector = np.sum([vector, model[word]], axis = 0)\n    else:\n      # Out of Vocabulary\n      vector = np.sum([vector, oov], axis = 0)\n  # Calculate the mean\n  mean_vector = vector / len(words)\n  X_train_vector_mean.append(mean_vector)\n```", "```py\nX_train_vector_appended = []\nmax_words = 40\nfor x in X_train:\n  words = x.split(' ')\n  num_words = max(max_words, len(words))\n  feature_vector = []\n  for word in words[:num_words]:\n    if word in model.wv.vocab:\n      # Word is present in the vocab\n      vector = np.sum([vector, model[word]], axis = 0)\n    else:\n      # Out of Vocabulary\n      vector = np.sum([vector, oov], axis = 0)\n    feature_vector = feature_vector + vector\n  if num_words < max_words:\n    pads = [0 for _ in range(30*(max_words-num_words))]\n    feature_vector = feature_vector + pads\n  X_train_vector_appended.append(feature_vector)\n```", "```py\nfrom sklearn.neural_network import MLPClassifier\nmodel = MLPClassifier(hidden_layer_sizes = (1000, 700, 500, 200),max_iter = 100,activation = 'relu',solver = 'adam',random_state = 123)\nmodel.fit(X_train_vector_appended, Y_train)\nY_predicted = model.predict(X_test_vector_appended)\nprint(evaluate_model(Y_test, Y_pred))\n```", "```py\nimport torch\nfrom pytorch_transformers import BertTokenizer\nfrom pytorch_transformers import BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base uncased',\noutput_hidden_states=True)\nmodel.eval()\n```", "```py\n    X_train_BERT = []\n    ```", "```py\n    for x in X_train:\n    ```", "```py\n      # Add CLS and SEP\n    ```", "```py\n      marked_text = \"[CLS] \" + x + \" [SEP]\"\n    ```", "```py\n      # Split the sentence into tokens.\n    ```", "```py\n      tokenized_text = tokenizer.tokenize(marked_text)\n    ```", "```py\n      # Map the token strings to their vocabulary indices.\n    ```", "```py\n      indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n    ```", "```py\n      tokens_tensor = torch.tensor([indexed_tokens])\n    ```", "```py\n      with torch.no_grad():\n    ```", "```py\n        outputs = model(tokens_tensor)\n    ```", "```py\n        feature_vector = outputs[0]\n    ```", "```py\n      X_train_BERT.append(feature_vector)\n    ```", "```py\n    X_train_BERT = []\n    ```", "```py\n    for x in X_train:\n    ```", "```py\n      # Add CLS and SEP\n    ```", "```py\n      marked_text = \"[CLS] \" + x + \" [SEP]\"\n    ```", "```py\n      # Split the sentence into tokens.\n    ```", "```py\n      tokenized_text = tokenizer.tokenize(marked_text)\n    ```", "```py\n      # Map the token strings to their vocabulary indeces.\n    ```", "```py\n      indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n    ```", "```py\n      tokens_tensor = torch.tensor([indexed_tokens])\n    ```", "```py\n      with torch.no_grad():\n    ```", "```py\n        outputs = model(tokens_tensor)\n    ```", "```py\n        hidden_states = outputs[2]\n    ```", "```py\n        feature_vector = torch.stack(hidden_states).sum(0)\n    ```", "```py\n      X_train_BERT.append(feature_vector)\n    ```", "```py\n    X_train_BERT = []\n    ```", "```py\n    for x in X_train:\n    ```", "```py\n      # Add CLS and SEP\n    ```", "```py\n      marked_text = \"[CLS] \" + x + \" [SEP]\"\n    ```", "```py\n      # Split the sentence into tokens.\n    ```", "```py\n      tokenized_text = tokenizer.tokenize(marked_text)\n    ```", "```py\n      # Map the token strings to their vocabulary indeces.\n    ```", "```py\n      indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n    ```", "```py\n      tokens_tensor = torch.tensor([indexed_tokens])\n    ```", "```py\n      with torch.no_grad():\n    ```", "```py\n        outputs = model(tokens_tensor)\n    ```", "```py\n        hidden_states = outputs[2]\n    ```", "```py\n        feature_vector = torch.stack(hidden_states[-4:]).sum(0)\n    ```", "```py\n      X_train_BERT.append(feature_vector)\n    ```", "```py\n    X_train_BERT = []\n    ```", "```py\n    for x in X_train:\n    ```", "```py\n      # Add CLS and SEP\n    ```", "```py\n      marked_text = \"[CLS] \" + x + \" [SEP]\"\n    ```", "```py\n      # Split the sentence into tokens.\n    ```", "```py\n      tokenized_text = tokenizer.tokenize(marked_text)\n    ```", "```py\n      # Map the token strings to their vocabulary indeces.\n    ```", "```py\n      indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n    ```", "```py\n      tokens_tensor = torch.tensor([indexed_tokens])\n    ```", "```py\n      with torch.no_grad():\n    ```", "```py\n        outputs = model(tokens_tensor)\n    ```", "```py\n        hidden_states = outputs[2]\n    ```", "```py\n        feature_vector = torch.cat([hidden_states[i] for i in [-1,-2,-3,-4]], dim=-1)\n    ```", "```py\n      X_train_BERT.append(feature_vector)\n    ```", "```py\nfrom sklearn.neural_network import MLPClassifier\nmodel = MLPClassifier(hidden_layer_sizes = (1000, 700, 500, 200),max_iter = 100,activation = 'relu',solver = 'adam',random_state = 123)\nmodel.fit(X_train_BERT, Y_train)\nY_predicted = model.predict(X_test_BERT)\nprint(evaluate_model(Y_test, Y_pred))\n```"]