<html><head></head><body><div class="calibre1" title="Chapter&#xA0;3.&#xA0;Processing the Colors of an Image"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h1 class="title"><a id="ch03" class="calibre6"/>Chapter 3. Processing the Colors of an Image</h1></div></div></div><p class="calibre8">In this chapter, we will cover the following recipes:</p><div class="calibre1"><ul class="itemizedlist"><li class="listitem">Comparing colors using the Strategy design pattern</li><li class="listitem">Segmenting an image with the GrabCut algorithm</li><li class="listitem">Converting color representations</li><li class="listitem">Representing colors with hue, saturation, and brightness</li></ul></div><div class="calibre1" title="Introduction"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h1 class="title1"><a id="ch03lvl1sec21" class="calibre6"/>Introduction</h1></div></div></div><p class="calibre8">The ability to see the world in colors is one of the important characteristics of the human visual system. The retina of the human eye includes specialized photoreceptors, called cones, which are responsible for the perception of colors. There are three types of cones that differ in the wavelength range of light they absorb; using the stimuli from these different cells, the human brain is able to create color perception. Most other animals only have rod cells, which are photoreceptors with better light sensitivity but that cover the full spectrum of visible light without color discrimination. In the human eye, rods are mainly located at the periphery of the retina, while the cones are concentrated in the central part.</p><p class="calibre8">In digital imaging, colors are generally reproduced by using the red, green, and blue additive primary colors. These have been selected because when they are combined together, they can produce a wide gamut of different colors. In fact, this choice of primaries mimics well the trichromatic color perception of the human visual system as the different cone cells have sensitivity located around the red, green, and blue spectrum. In this chapter, you will play with the pixel color and see how an image can be segmented based on the color information. In addition, you will learn that it can sometimes be useful to use a different color representation when performing color image processing.</p></div></div>
<div class="calibre1" title="Comparing colors using the Strategy design pattern"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h1 class="title1"><a id="ch03lvl1sec22" class="calibre6"/>Comparing colors using the Strategy design pattern</h1></div></div></div><p class="calibre8">Let's say we want to build a simple algorithm that will identify all of the pixels in an image that have a given color. For this, the algorithm has to accept an image and a color as input and will return a binary image showing the pixels that have the specified color. The tolerance with which we want to accept a color will be another parameter to be specified before running the algorithm.</p><p class="calibre8">In order to accomplish this objective, this recipe will use the <span><strong class="calibre15">Strategy design pattern</strong></span>. This object-oriented design pattern constitutes an excellent way of encapsulating an algorithm in a class. It becomes then easier to replace a given algorithm with another one, or to chain several algorithms together in order to build a more complex process. In addition, this pattern facilitates the deployment of an algorithm by hiding as much of its complexity as possible behind an intuitive programming interface.</p><div class="calibre1" title="How to do it…"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch03lvl2sec63" class="calibre6"/>How to do it…</h2></div></div></div><p class="calibre8">Once an algorithm has been encapsulated in a class using the Strategy design pattern, it can be deployed by creating an instance of this class. Typically, the instance will be created when the program is initialized. At the time of construction, the class instance will initialize the different parameters of the algorithm with their default values so that it will immediately be ready to be used. The algorithm's parameter values can also be read and set using appropriate methods. In the case of an application with a GUI, these parameters can be displayed and modified using different widgets (text fields, sliders, and so on) so that a user can easily play with them.</p><p class="calibre8">We will show you the structure of a <code class="literal">Strategy</code> class in the next section; let's start with an example of how it can be deployed and used. Let's write a simple <code class="literal">main</code> function that will run our proposed color detection algorithm:</p><pre class="programlisting">    int main() 
    { 
      //1. Create image processor object 
      ColorDetector cdetect; 
 
      //2. Read input image 
      cv::Mat image= cv::imread("boldt.jpg"); 
      if (image.empty()) return 0;  
 
      //3. Set input parameters 
      cdetect.setTargetColor(230,190,130);  // here blue sky 
 
      //4. Process the image and display the result 
      cv::namedWindow("result"); 
      cv::Mat result = cdetect.process(image); 
      cv::imshow("result",result); 
 
      cv::waitKey(); 
      return 0; 
    } 
</pre><p class="calibre8">Running this program to detect a blue sky in the colored version of the <span><em class="calibre16">Castle</em></span> image presented in the previous chapter produces the following output:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How to do it…" src="graphics/image_03_002.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Here, a white pixel indicates a positive detection of the sought color, and black indicates negative.</p><p class="calibre8">Obviously, the algorithm we encapsulated in this class is relatively simple (as we will see next, it is composed of just one scanning loop and one tolerance parameter). The Strategy design pattern becomes really powerful when the algorithm to be implemented is more complex, has many steps, and includes several parameters.</p></div><div class="calibre1" title="How it works…"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch03lvl2sec64" class="calibre6"/>How it works…</h2></div></div></div><p class="calibre8">The core process of this algorithm is easy to build. It is a simple scanning loop that goes over each pixel, comparing its color with the target color. Using what we learned in the <span><em class="calibre16">Scanning an image with iterators</em></span> recipe of the previous chapter, this loop can be written as follows:</p><pre class="programlisting">    // get the iterators 
    cv::Mat_&lt;cv::Vec3b&gt;::const_iterator it= image.begin&lt;cv::Vec3b&gt;(); 
    cv::Mat_&lt;cv::Vec3b&gt;::const_iterator itend= image.end&lt;cv::Vec3b&gt;(); 
    cv::Mat_&lt;uchar&gt;::iterator itout= result.begin&lt;uchar&gt;(); 
 
    //for each pixel 
    for ( ; it!= itend; ++it, ++itout) { 
 
      // compute distance from target color 
      if (getDistanceToTargetColor(*it)&lt;=maxDist) { 
        *itout= 255; 
      } else { 
       *itout= 0; 
      } 
    } 
</pre><p class="calibre8">The <code class="literal">cv::Mat</code> variable image refers to the input image, while <code class="literal">result</code> refers to the binary output image. Therefore, the first step consists of setting up the required iterators. The scanning loop then becomes easy to implement. Note that the input image iterators are declared <code class="literal">const</code> as the values of their elements are not modified. The distance between the current pixel color and the target color is evaluated for each pixel in order to check whether it is within the tolerance parameter defined by <code class="literal">maxDist</code>. If that is the case, the value <code class="literal">255</code> (white) is then assigned to the output image; if not, <code class="literal">0</code> (black) is assigned. To compute the distance to the target color, the <code class="literal">getDistanceToTargetColor</code> method is used. There are different ways to compute this distance.</p><p class="calibre8">One could, for example, calculate the Euclidean distance between the three vectors that contain the RGB color values. To keep this computation simple, we sum the absolute differences of the RGB values (this is also known as the <span><strong class="calibre15">city-block distance</strong></span>). Note that in modern architecture, a floating-point Euclidean distance can be faster to compute than a simple city-block distance (in addition, you can also use squared Euclidean distances to avoid the costly square-root); this is also something to take into consideration in your design. Also, for more flexibility, we write the <code class="literal">getDistanceToTargetColor</code> method in terms of a <code class="literal">getColorDistance</code> method, as follows:</p><pre class="programlisting">    // Computes the distance from target color. 
    int getDistanceToTargetColor(const cv::Vec3b&amp; color) const { 
      return getColorDistance(color, target); 
    } 
    // Computes the city-block distance between two colors. 
    int getColorDistance(const cv::Vec3b&amp; color1,  
    const cv::Vec3b&amp; color2) const { 
      return abs(color1[0]-color2[0])+
             abs(color1[1]-color2[1])+ 
             abs(color1[2]-color2[2]); 
    } 
</pre><p class="calibre8">Note how we used <code class="literal">cv::Vec3d</code> to hold the three unsigned chars that represent the RGB values of a color. The <code class="literal">target</code> variable obviously refers to the specified target color, and as we will see, it is defined as a member variable in the class algorithm that we will define. Now, let's complete the definition of the processing method. Users will provide an input image, and the result will be returned once the image scanning is completed:</p><pre class="programlisting">    cv::Mat ColorDetector::process(const cv::Mat &amp;image) { 
 
      // re-allocate binary map if necessary 
      // same size as input image, but 1-channel 
      result.create(image.size(),CV_8U); 
 
      // processing loop above goes here 
      return result; 
    }
</pre><p class="calibre8">Each time this method is called, it is important to check if the output image that contains the resulting binary map needs to be reallocated to fit the size of the input image. This is why we use the <code class="literal">create</code> method of <code class="literal">cv::Mat</code>. Remember that this method will only proceed to reallocation if the specified size and/or depth do not correspond to the current image structure.</p><p class="calibre8">Now that we have the core processing method defined, let's see what additional methods should be added in order to deploy this algorithm. We have previously determined what input and output data our algorithm requires. Therefore, we define the class attributes that will hold this data:</p><pre class="programlisting">    class ColorDetector {
      private: 
 
      // minimum acceptable distance 
      int maxDist;  
      // target color 
      cv::Vec3b target; 
 
      // image containing resulting binary map 
      cv::Mat result;
</pre><p class="calibre8">In order to create an instance of the class that encapsulates our algorithm (which we have named <code class="literal">ColorDetector</code>), we need to define a constructor. Remember that one of the objectives of the Strategy design pattern is to make algorithm deployment as easy as possible. The simplest constructor that can be defined is an empty one. It will create an instance of the class algorithm in a valid state. We then want the constructor to initialize all the input parameters to their default values (or the values that are known to generally give a good result). In our case, we decided that a distance of <code class="literal">100</code> is generally an acceptable tolerance parameter. We also set the default target color. We chose black for no particular reason. The idea is to make sure we always start with predictable and valid input values:</p><pre class="programlisting">    // empty constructor 
    // default parameter initialization here 
    ColorDetector() : maxDist(100), target(0,0,0) {} 
</pre><p class="calibre8">Another option would have been not create an empty constructor and rather force the user to input a target color and a color distance in a more elaborated constructor:</p><pre class="programlisting">    // another constructor with target and distance 
    ColorDetector(uchar blue, uchar green, uchar red, int mxDist); 
</pre><p class="calibre8">At this point, a user who creates an instance of our class algorithm can immediately call the process method with a valid image and obtain a valid output. This is another objective of the Strategy pattern, that is, to make sure that the algorithm always runs with valid parameters. Obviously, the users of this class will want to use their own settings. This is done by providing the user with the appropriate getters and setters. Let's start with the <code class="literal">color</code> tolerance parameter:</p><pre class="programlisting">    // Sets the color distance threshold 
    // Threshold must be positive, 
    // otherwise distance threshold is set to 0. 
    void setColorDistanceThreshold(int distance) { 
 
      if (distance&lt;0) 
        distance=0; 
        maxDist= distance; 
      } 
 
      // Gets the color distance threshold 
      int getColorDistanceThreshold() const { 
        return maxDist; 
      }
</pre><p class="calibre8">Note how we first check the validity of the input. Again, this is to make sure that our algorithm will never be run in an invalid state. The target color can be set in a similar manner, as follows:</p><pre class="programlisting">    // Sets the color to be detected 
    void setTargetColor(uchar blue,
                        uchar green,
                        uchar red) { 
      // BGR order 
      target = cv::Vec3b(blue, green, red); 
    } 
    // Sets the color to be detected 
    void setTargetColor(cv::Vec3b color) { 
      target= color; 
    } 
 
    // Gets the color to be detected 
    cv::Vec3b getTargetColor() const { 
      return target; 
    } 
</pre><p class="calibre8">This time, it is interesting to note that we have provided the user with two definitions of the <code class="literal">setTargetColor</code> method. In the first version of the definition, the three color components are specified as three arguments, while in the second version, <code class="literal">cv::Vec3b</code> is used to hold the color values. Again, the objective is to facilitate the use of our class algorithm. The users can simply select the setter that best fits their needs.</p></div><div class="calibre1" title="There's more…"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch03lvl2sec65" class="calibre6"/>There's more…</h2></div></div></div><p class="calibre8">The example algorithm used in this recipe consisted of identifying the pixels of an image that has a color sufficiently close to a specified target color. This computation could have been done otherwise. Interestingly, an OpenCV function performs a similar task in order to extract a connected component of a given color. Also, the implementation of a Strategy design pattern could be complemented using function objects. Finally, OpenCV has defined a base class, <code class="literal">cv::Algorithm</code>, that implements the Strategy design pattern concepts.</p><div class="calibre1" title="Computing the distance between two color vectors"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h3 class="title3"><a id="ch03lvl3sec14" class="calibre6"/>Computing the distance between two color vectors</h3></div></div></div><p class="calibre8">To compute the distance between two color vectors, we used the following simple formula:</p><pre class="programlisting">    return abs(color[0]-target[0])+
           abs(color[1]-target[1])+
           abs(color[2]-target[2]); 
</pre><p class="calibre8">However, OpenCV includes a function to compute the Euclidean norm of a vector. Consequently, we could have computed our distance as follows:</p><pre class="programlisting">    return static_cast&lt;int&gt;(
           cv::norm&lt;int,3&gt;(cv::Vec3i(color[0]-target[0],
                                     color[1]-target[1],
                                     color[2]-target[2]))); 
</pre><p class="calibre8">A very similar result would then be obtained using this definition of the <code class="literal">getDistance</code> method. Here, we use <code class="literal">cv::Vec3i</code> (a 3-vector array of integers) because the result of the subtraction is an integer value.</p><p class="calibre8">It is also interesting to recall from 
<a href="ch02.html" title="Chapter 2. Manipulating Pixels">Chapter 2</a>
, <span><em class="calibre16">Manipulating Pixels</em></span>, that the OpenCV matrix and vector data structures include a definition of the basic arithmetic operators. Consequently, one could have proposed the following definition for the distance computation:</p><pre class="programlisting">    return static_cast&lt;int&gt;( cv::norm&lt;uchar,3&gt;(color-target));// wrong! 
</pre><p class="calibre8">This definition may look right at the first glance; however, it is wrong. This is because all these operators always include a call to <code class="literal">saturate_cast</code> (see the <span><em class="calibre16">Scanning an image with neighbor access</em></span> recipe in the previous chapter) in order to ensure that the results stay within the domain of the input type (here, it is <code class="literal">uchar</code>). Therefore, in the cases where the target value is greater than the corresponding color value, the value <code class="literal">0</code> will be assigned instead of the negative value that one would have expected. A correct formulation would then be as follows:</p><pre class="programlisting">    cv::Vec3b dist; 
    cv::absdiff(color,target,dist); 
    return cv::sum(dist)[0];
</pre><p class="calibre8">However, using two function calls to compute the distance between two 3-vector arrays is inefficient.</p></div><div class="calibre1" title="Using OpenCV functions"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h3 class="title3"><a id="ch03lvl3sec15" class="calibre6"/>Using OpenCV functions</h3></div></div></div><p class="calibre8">In this recipe, we used a loop with iterators in order to perform our computation. Alternatively, we could have achieved the same result by calling a sequence of OpenCV functions. The color detection method will then be written as follows:</p><pre class="programlisting">    cv::Mat ColorDetector::process(const cv::Mat &amp;image) { 
      cv::Mat output; 
      // compute absolute difference with target color 
      cv::absdiff(image,cv::Scalar(target),output); 
 
      // split the channels into 3 images 
      std::vector&lt;cv::Mat&gt; images; 
      cv::split(output,images); 
 
      // add the 3 channels (saturation might occurs here) 
      output= images[0]+images[1]+images[2]; 
      // apply threshold 
      cv::threshold(output,                  // same input/output image 
                    output,   
                    maxDist,                // threshold (must be &lt; 256) 
                    255,                    // max value 
                    cv::THRESH_BINARY_INV); // thresholding mode 
 
      return output; 
    }
</pre><p class="calibre8">This method uses the <code class="literal">absdiff</code> function, which computes the absolute difference between the pixels of an image and, in this case, a scalar value. Instead of a scalar value, another image can be provided as the second argument to this function. In the latter case, a pixel-by-pixel difference will be applied; consequently, the two images must be of the same size. The individual channels of the difference image are then extracted using the <code class="literal">split</code> function (discussed in the <span><em class="calibre16">There's more...</em></span> section of the <span><em class="calibre16">Performing simple image arithmetic</em></span> recipe of 
<a href="ch02.html" title="Chapter 2. Manipulating Pixels">Chapter 2</a>
, <span><em class="calibre16">Manipulating Pixels</em></span>) in order to be able to add them together. It is important to note that the result of this sum may sometimes be greater than <code class="literal">255</code>, but because saturation is always applied, the result will be stopped at <code class="literal">255</code>. The consequence is that with this version, the <code class="literal">maxDist</code> parameter must also be less than <code class="literal">256</code>; this should be corrected if you consider this behavior unacceptable.</p><p class="calibre8">The last step is to create a binary image by using the <code class="literal">cv::threshold</code> function. This function is commonly used to compare all the pixels with a threshold value (the third parameter), and in the regular thresholding mode (<code class="literal">cv::THRESH_BINARY</code>), it assigns the defined maximum value (the fourth parameter) to all the pixels greater than the specified threshold and <code class="literal">0</code> to the other pixels. Here, we used the inverse mode (<code class="literal">cv::THRESH_BINARY_INV</code>) in which the defined maximum value is assigned to the pixels that have a value lower than or equal to the threshold. Of interest are also the <code class="literal">cv::THRESH_TOZERO</code> and <code class="literal">cv::THRESH_TOZERO_INV</code> modes, which leave the pixels greater than or lower than the threshold unchanged.</p><p class="calibre8">Using the OpenCV functions is generally a good idea. You can then quickly build complex applications and potentially reduce the number of bugs. The result is often more efficient (thanks to the optimization efforts invested by the OpenCV contributors). However, when many intermediate steps are performed, you may find that the resulting method consumes more memory.</p></div><div class="calibre1" title="The floodFill function"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h3 class="title3"><a id="ch03lvl3sec16" class="calibre6"/>The floodFill function</h3></div></div></div><p class="calibre8">Our <code class="literal">ColorDetector</code> class identifies the pixels in an image that have a color similar to a given target color. The decision to accept or not a pixel is simply made on a per-pixel basis. The <code class="literal">cv::floodFill</code> function proceeds in a very similar way with one important difference: in this case, the decision to accept a pixel also depends on the state of its neighbors. The idea is to identify a connected area of a certain color. The user specifies a starting pixel location and tolerance parameters that determine color similarity.</p><p class="calibre8">The seed pixel defines the color that is seek and from this seed location, the neighbors are considered in order to identify pixels of similar color; then the neighbors of the accepted neighbors are also considered and so on. This way, one area of constant color will be extracted from the image. For example, to detect the blue sky area in our example image, you could proceed as follows:</p><pre class="programlisting">    cv::floodFill(image,              // input/ouput image 
           cv::Point(100, 50),        // seed point 
           cv::Scalar(255, 255, 255), // repainted color 
           (cv::Rect*)0,              // bounding rect of the repainted set 
           cv::Scalar(35, 35, 35),    // low/high difference threshold 
           cv::Scalar(35, 35, 35),    // identical most of the time  
           cv::FLOODFILL_FIXED_RANGE);// pixels compared to seed 
</pre><p class="calibre8">The seed pixel (<code class="literal">100</code>, <code class="literal">50</code>) is located in the sky. All connected pixels will be tested and the ones having a similar color will be repainted in a new color specified by the third parameter. To determine if a color is similar or not, different thresholds are defined independently for values that are higher or lower than the reference color. Here, we used fixed range mode, which implies that the tested pixels will all be compared to the seed pixel's color. The default mode is the one where each tested pixel is compared to the color of its neighbors. The result obtained is as follows:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="The floodFill function" src="graphics/image_03_003.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">A single connected area is repainted by the algorithm (here, we painted the sky in white). Therefore, even if there are some pixels somewhere else with a similar color (in the water, for instance), these ones would not be identified unless they were connected to the sky area.</p></div><div class="calibre1" title="Functor or function object"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h3 class="title3"><a id="ch03lvl3sec17" class="calibre6"/>Functor or function object</h3></div></div></div><p class="calibre8">Using the C++ operator overloading, it is possible to create a class for which its instances behave as functions. The idea is to overload the <code class="literal">operator()</code> method so that a call to the processing method of a class looks exactly like a simple function call. The resulting class instance is called a function object, or a <span><strong class="calibre15">functor</strong></span>. Often, a functor includes a full constructor such that it can be used immediately after being created. For example, you can define the full constructor of your <code class="literal">ColorDetector</code> class as follows:</p><pre class="programlisting">    // full constructor 
    ColorDetector(uchar blue, uchar green, uchar red, int  maxDist=100): 
                  maxDist(maxDist) {  
 
      // target color 
      setTargetColor(blue, green, red); 
    } 
</pre><p class="calibre8">Obviously, you can still use the setters and getters that have been defined previously. The functor method can be defined as follows:</p><pre class="programlisting">    cv::Mat operator()(const cv::Mat &amp;image) { 
      // color detection code here  
    } 
</pre><p class="calibre8">To detect a given color with this functor method, simply write the following code snippet:</p><pre class="programlisting">    ColorDetector colordetector(230,190,130,  // color 
                                100);         // threshold 
    cv::Mat result= colordetector(image);     // functor call 
</pre><p class="calibre8">As you can see, the call to the color detection method now looks like a function call.</p></div><div class="calibre1" title="The OpenCV base class for algorithms"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h3 class="title3"><a id="ch03lvl3sec18" class="calibre6"/>The OpenCV base class for algorithms</h3></div></div></div><p class="calibre8">OpenCV offers many algorithms that perform various computer vision tasks. To facilitate their use, most of these algorithms have been made subclass of a generic base class called <code class="literal">cv::Algorithm</code>. This one implements some of the concepts dictated by the Strategy design pattern. First, all these algorithms are created dynamically using a specialized static method that makes sure that the algorithm is always created in a valid state (that is, with valid default values for the unspecified parameters). Let's consider, for example, one of these subclasses, <code class="literal">cv::ORB</code>; this one is an interest point operator that will be discussed in the <span><em class="calibre16">Detecting FAST features at Multiple Scales</em></span> recipe in <a href="ch08.html" title="Chapter 8. Detecting Interest Points">
Chapter 8
</a>, <span><em class="calibre16">Detecting Interest Points</em></span>. Here, we simply use it as an illustrative example of an algorithm.</p><p class="calibre8">An instance of this algorithm is therefore created as follows:</p><pre class="programlisting">    cv::Ptr&lt;cv::ORB&gt; ptrORB = cv::ORB::create(); // default state 
</pre><p class="calibre8"> 
Once created, the algorithm can then be used. For example, the generic methods <code class="literal">read</code> and <code class="literal">write</code> can be used to load or store the state of the algorithm. The algorithms also have specialized methods (in the case of ORB, for example, the methods <code class="literal">detect</code> and <code class="literal">compute</code> can be used to trigger its main computational units). Algorithms also have specialized setter methods that allows specifying their internal parameters. Note that we could have declared the pointer as <code class="literal">cv::Ptr&lt;cv::Algorithm&gt;</code> but, in this case, we would not be able to use its specialized methods.</p></div></div><div class="calibre1" title="See also"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch03lvl2sec66" class="calibre6"/>See also</h2></div></div></div><div class="calibre1"><ul class="itemizedlist"><li class="listitem">The policy-based class design, introduced by A. Alexandrescu, is an interesting variant of the Strategy design pattern in which algorithms are selected at compile time</li><li class="listitem">The <span><em class="calibre16">Converting color representation</em></span> recipe introduces the concept of perceptually uniform color spaces to achieve more intuitive color comparison</li></ul></div></div></div>
<div class="calibre1" title="Segmenting an image with the GrabCut algorithm"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h1 class="title1"><a id="ch03lvl1sec23" class="calibre6"/>Segmenting an image with the GrabCut algorithm</h1></div></div></div><p class="calibre8">The previous recipe showed how color information can be useful to segment an image into area corresponding to specific elements of a scene. Objects often have distinctive colors, and these ones can often be extracted by identifying areas of similar colors. OpenCV proposes an implementation of a popular algorithm for image segmentation: the <span><strong class="calibre15">GrabCut</strong></span> algorithm. GrabCut is a complex and computationally expensive algorithm, but it generally produces very accurate results. It is the best algorithm to use when you want to extract a foreground object in a still image (for example, to cut and paste an object from one picture to another).</p><div class="calibre1" title="How to do it…"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch03lvl2sec67" class="calibre6"/>How to do it…</h2></div></div></div><p class="calibre8">The <code class="literal">cv::grabCut</code> function is easy to use. You just need to input an image and label some of its pixels as belonging to the background or to the foreground. Based on this partial labeling, the algorithm will then determine a foreground/ background segmentation for the complete image.</p><p class="calibre8">One way to specify a partial foreground/background labeling for an input image is by defining a rectangle inside which the foreground object is included:</p><pre class="programlisting">    // define bounding rectangle 
    // the pixels outside this rectangle 
    // will be labeled as background 
    cv::Rect rectangle(5,70,260,120); 
</pre><p class="calibre8">This defines the following area in the image:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How to do it…" src="graphics/image_03_004.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">All the pixels outside this rectangle will then be marked as background. In addition to the input image and its segmentation image, calling the <code class="literal">cv::grabCut</code> function requires the definition of two matrices, which will contain the models built by the algorithm as follows:</p><pre class="programlisting">    cv::Mat result;                     // segmentation (4 possible values) 
    cv::Mat bgModel,fgModel;            // the models (internally used) 
    // GrabCut segmentation    
    cv::grabCut(image,                  // input image 
                result,                 // segmentation result 
                rectangle,              // rectangle containing foreground 
                bgModel,fgModel,        // models 
                5,                      // number of iterations 
                cv::GC_INIT_WITH_RECT); // use rectangle 
</pre><p class="calibre8"> 
Note how we specified that we are using the bounding rectangle mode with the <code class="literal">cv::GC_INIT_WITH_RECT</code> flag as the last argument of the function (the next section, <span><em class="calibre16">How it works...</em></span>, will discuss the other available mode). The input/output segmentation image can have one of the following four values:</p><div class="calibre1"><ul class="itemizedlist"><li class="listitem"><code class="literal">cv::GC_BGD</code>: This is the value of the pixels that certainly belong to the background (for example, pixels outside the rectangle in our example)</li><li class="listitem"><code class="literal">cv::GC_FGD</code>: This is the value of the pixels that certainly belong to the foreground (there are none in our example)</li><li class="listitem"><code class="literal">cv::GC_PR_BGD</code>: This is the value of the pixels that probably belong to the background</li><li class="listitem"><code class="literal">cv::GC_PR_FGD</code>: This is the value of the pixels that probably belong to the foreground (that is, the initial value of the pixels inside the rectangle in our example)</li></ul></div><p class="calibre8">We get a binary image of the segmentation by extracting the pixels that have a value equal to <code class="literal">cv::GC_PR_FGD</code>. This is accomplished with the following code:</p><pre class="programlisting">    // Get the pixels marked as likely foreground 
    cv::compare(result,cv::GC_PR_FGD,result,cv::CMP_EQ); 
    // Generate output image 
    cv::Mat foreground(image.size(),CV_8UC3,cv::Scalar(255,255,255)); 
    image.copyTo(foreground,// bg pixels are not copied result);
</pre><p class="calibre8">To extract all the foreground pixels, that is, with values equal to <code class="literal">cv::GC_PR_FGD</code> or <code class="literal">cv::GC_FGD</code>, it is possible to check the value of the first bit, as follows:</p><pre class="programlisting">    // checking first bit with bitwise-and 
    result= result&amp;1; // will be 1 if FG 
</pre><p class="calibre8">This is possible because these constants are defined as values <code class="literal">1</code> and <code class="literal">3</code>, while the other two (<code class="literal">cv::GC_BGD</code> and <code class="literal">cv::GC_PR_BGD</code>) are defined as <code class="literal">0</code> and <code class="literal">2</code>. In our example, the same result is obtained because the segmentation image does not contain the <code class="literal">cv::GC_FGD</code> pixels (only the <code class="literal">cv::GC_BGD</code> pixels have been inputted).</p><p class="calibre8">The following image is then obtained:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How to do it…" src="graphics/image_03_005.jpg" class="calibre17"/></div><p class="calibre8">
</p></div><div class="calibre1" title="How it works…"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch03lvl2sec68" class="calibre6"/>How it works…</h2></div></div></div><p class="calibre8">In the preceding example, the GrabCut algorithm was able to extract the foreground object by simply specifying a rectangle inside which this objects (the castle) was contained. Alternatively, one could also assign the values <code class="literal">cv::GC_BGD</code> and <code class="literal">cv::GC_FGD</code> to some specific pixels of the input image, which are provided by using a mask image as the second argument of the <code class="literal">cv::grabCut</code> function. You would then specify <code class="literal">GC_INIT_WITH_MASK</code> as the input mode flag. These input labels could be obtained, for example, by asking a user to interactively mark a few elements of the image. It is also possible to combine these two input modes.</p><p class="calibre8">Using this input information, the GrabCut algorithm creates the background/foreground segmentation by proceeding as follows. Initially, a foreground label (<code class="literal">cv::GC_PR_FGD</code>) is tentatively assigned to all the unmarked pixels. Based on the current classification, the algorithm groups the pixels into clusters of similar colors (that is, <code class="literal">K</code> clusters for the background and <code class="literal">K</code> clusters for the foreground). The next step is to determine a background/ foreground segmentation by introducing boundaries between the foreground and background pixels.</p><p class="calibre8">This is done through an optimization process that tries to connect pixels with similar labels, and that imposes a penalty for placing a boundary in the regions of relatively uniform intensity. This optimization problem can be efficiently solved using the Graph Cuts algorithm, a method that can find the optimal solution of a problem by representing it as a connected graph on which cuts are applied in order to compose an optimal configuration. The obtained segmentation produces new labels for the pixels.</p><p class="calibre8">The clustering process can then be repeated, and a new optimal segmentation is found again, and so on. Therefore, the GrabCut algorithm is an iterative procedure that gradually improves the segmentation result. Depending on the complexity of the scene, a good solution can be found in more or less number of iterations (in easy cases, one iteration would be enough).</p><p class="calibre8">This explains the argument of the function where the user can specify the number of iterations to be applied. The two internal models maintained by the algorithm are passed as an argument of the function (and returned). Therefore, it is possible to call the function with the models of the last run again if one wishes to improve the segmentation result by performing additional iterations.</p></div><div class="calibre1" title="See also"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch03lvl2sec69" class="calibre6"/>See also</h2></div></div></div><div class="calibre1"><ul class="itemizedlist"><li class="listitem">The article <span><em class="calibre16">GrabCut: Interactive Foreground Extraction using Iterated Graph Cuts</em></span> in<span><em class="calibre16"> ACM Transactions on Graphics (SIGGRAPH) volume 23, issue 3, August 2004, C. Rother, V. Kolmogorov, and A. Blake</em></span> describes the GrabCut algorithm in detail</li><li class="listitem">The <span><em class="calibre16">Segmenting images using watersheds</em></span> recipe in <a href="ch05.html" title="Chapter 5. Transforming Images with Morphological Operations">Chapter 5</a>, <span><em class="calibre16">Transforming Images with Morphological Operations</em></span>, presents another image segmentation algorithm</li></ul></div></div></div>
<div class="calibre1" title="Converting color representations"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h1 class="title1"><a id="ch03lvl1sec24" class="calibre6"/>Converting color representations</h1></div></div></div><p class="calibre8">The RGB color space is based on the use of the red, green, and blue additive primary colors. We saw in the first recipe of this chapter that these primaries have been chosen because they can produce a good range of colors well aligned with the human visual system. It is often the default color space in digital imagery because this is the way color images are acquired, that is, through the use of red, green, and blue filters. Additionally, the red, green, and blue channels are normalized such that when combined in equal amounts, a gray-level intensity is obtained, that is, from black <code class="literal">(0,0,0)</code> to white <code class="literal">(255,255,255)</code>.</p><p class="calibre8">Unfortunately, computing the distance between the colors using the RGB color space is not the best way to measure the similarity between two given colors. Indeed, RGB is not a <span><strong class="calibre15">perceptually uniform color space</strong></span>. This means that two colors at a given distance might look very similar, while two other colors separated by the same distance might look very different.</p><p class="calibre8">To solve this problem, other color representations that have the property of being perceptually uniform have been introduced. In particular, the <span><strong class="calibre15">CIE L*a*b*</strong></span> is one such color model. By converting our images to this representation, the Euclidean distance between an image pixel and the target color will then be a meaningful measure of the visual similarity between the two colors. In this recipe, we will show you how to convert colors from one representation to another in order to work with other color spaces.</p><div class="calibre1" title="How to do it…"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch03lvl2sec70" class="calibre6"/>How to do it…</h2></div></div></div><p class="calibre8">Conversion of images between different color spaces is easily done through the use of the <code class="literal">cv::cvtColor</code> OpenCV function. Let's revisit the <code class="literal">ColorDetector</code> class of the first recipe of this chapter, <span><em class="calibre16">Comparing colors using the Strategy design pattern</em></span>. We now convert the input image to the CIE L*a*b* color space at the beginning of the process method:</p><pre class="programlisting">    cv::Mat ColorDetector::process(const cv::Mat &amp;image) { 
 
      // re-allocate binary map if necessary 
      // same size as input image, but 1-channel 
      result.create(image.rows,image.cols,CV_8U); 
 
      // Converting to Lab color space  
      cv::cvtColor(image, converted, CV_BGR2Lab); 
 
      // get the iterators of the converted image  
      cv::Mat_&lt;cv::Vec3b&gt;::iterator it=  converted.begin&lt;cv::Vec3b&gt;(); 
      cv::Mat_&lt;cv::Vec3b&gt;::iterator itend= converted.end&lt;cv::Vec3b&gt;(); 
      // get the iterator of the output image  
      cv::Mat_&lt;uchar&gt;::iterator itout= result.begin&lt;uchar&gt;(); 
 
      // for each pixel 
      for ( ; it!= itend; ++it, ++itout) { 
</pre><p class="calibre8">The <code class="literal">converted</code> variable contains the image after color conversion. In the <code class="literal">ColorDetector</code> class, it is defined as a class attribute:</p><pre class="programlisting">    class ColorDetector { 
      private: 
      // image containing color converted image 
      cv::Mat converted; 
</pre><p class="calibre8">You also need to convert the input target color. You can do this by creating a temporary image that contains only one pixel. Note that you need to keep the same signature as in the earlier recipes, that is, the user continues to supply the target color in RGB:</p><pre class="programlisting">    // Sets the color to be detected 
    void setTargetColor(unsigned char red, unsigned char green,
                        unsigned char blue) { 
 
      // Temporary 1-pixel image 
      cv::Mat tmp(1,1,CV_8UC3); 
      tmp.at&lt;cv::Vec3b&gt;(0,0)= cv::Vec3b(blue, green, red); 
 
      // Converting the target to Lab color space  
      cv::cvtColor(tmp, tmp, CV_BGR2Lab); 
 
      target= tmp.at&lt;cv::Vec3b&gt;(0,0); 
    } 
</pre><p class="calibre8">If the application of the preceding recipe is compiled with this modified class, it will now detect the pixels of the target color using the CIE L*a*b* color model.</p></div><div class="calibre1" title="How it works…"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch03lvl2sec71" class="calibre6"/>How it works…</h2></div></div></div><p class="calibre8">When an image is converted from one color space to another, a linear or nonlinear transformation is applied on each input pixel to produce the output pixels. The pixel type of the output image will match the one of the input image. Even if you work with 8-bit pixels most of the time, you can also use a color conversion with floating-point images (in which case, the pixel values are generally assumed to vary between <code class="literal">0</code> and <code class="literal">1.0</code>) or with integer images (with pixels generally varying between <code class="literal">0</code> and <code class="literal">65535</code>). However, the exact domain of the pixel values depends on the specific color space and destination image type. For example, with the <code class="literal">CIE L*a*b*</code> color space, the <code class="literal">L</code> channel, which represents the brightness of each pixel, varies between <code class="literal">0</code> and <code class="literal">100</code>, and it is rescaled between <code class="literal">0</code> and <code class="literal">255</code> in the case of the 8-bit images.</p><p class="calibre8">The <code class="literal">a</code> and <code class="literal">b</code> channels correspond to the chromaticity components. These channels contain information about the color of a pixel, independent of its brightness. Their values vary between <code class="literal">-127</code> and <code class="literal">127</code>; for 8-bit images, <code class="literal">128</code> is added to each value in order to make it fit within the <code class="literal">0</code> to <code class="literal">255</code> interval. However, note that the 8-bit color conversion will introduce rounding errors that will make the transformation imperfectly reversible.</p><p class="calibre8">Most commonly used color spaces are available. It is just a question of providing the right color space conversion code to the OpenCV function (for CIE L*a*b*, this code is <code class="literal">CV_BGR2Lab</code>). Among these is YCrCb, which is the color space used in JPEG compression. To convert a color space from BGR to YCrCb, the code will be <code class="literal">CV_BGR2YCrCb</code>. Note that all the conversions that involve the three regular primary colors, red, green, and blue, are available in the RGB and BGR order.</p><p class="calibre8">The <span><strong class="calibre15">CIE L*u*v*</strong></span> color space is another perceptually uniform color space. You can convert from BGR to CIE L*u*v by using the <code class="literal">CV_BGR2Luv</code> code. Both L*a*b* and L*u*v* use the same conversion formula for the brightness channel but use a different representation for the chromaticity channels. Also, note that since these two color spaces distort the RGB color domain in order to make it perceptually uniform, these transformations are nonlinear (therefore, they are costly to compute).</p><p class="calibre8">There is also the CIE XYZ color space (with the <code class="literal">CV_BGR2XYZ</code> code). It is a standard color space used to represent any perceptible color in a device-independent way. In the computation of the L*u*v and L*a*b color spaces, the XYZ color space is used as an intermediate representation. The transformation between RGB and XYZ is linear. It is also interesting to note that the <code class="literal">Y</code> channel corresponds to a gray-level version of the image.</p><p class="calibre8">HSV and HLS are interesting color spaces because they decompose the colors into their hue and saturation components plus the value or luminance component, which is a more natural way for humans to describe colors. The next recipe will present this color space.</p><p class="calibre8">You can also convert color images to gray-level intensities. The output will be a one-channel image:</p><pre class="programlisting">    cv::cvtColor(color, gray, CV_BGR2Gray); 
</pre><p class="calibre8">It is also possible to do the conversion in the other direction, but the three channels of the resulting color image will then be identically filled with the corresponding values in the gray-level image.</p></div><div class="calibre1" title="See also"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch03lvl2sec72" class="calibre6"/>See also</h2></div></div></div><div class="calibre1"><ul class="itemizedlist"><li class="listitem">The <span><em class="calibre16">Using the mean shift algorithm to find an object</em></span> recipe in <a href="ch04.html" title="Chapter 4. Counting the Pixels with Histograms">Chapter 4</a>, <span><em class="calibre16">Counting the Pixels with Histograms</em></span>, uses the HSV color space in order to find an object in an image.</li><li class="listitem">Many good references are available on the color space theory. Among them, the following is a complete reference: <span><em class="calibre16">The Structure and Properties of Color Spaces and the Representation of Color Images, E. Dubois, Morgan and Claypool Publishers, 2009</em></span>.</li></ul></div></div></div>
<div class="calibre1" title="Representing colors with hue, saturation, and brightness"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h1 class="title1"><a id="ch03lvl1sec25" class="calibre6"/>Representing colors with hue, saturation, and brightness</h1></div></div></div><p class="calibre8">In this chapter, we played with image colors. We used different color spaces and tried to identify image areas of uniform color. The RGB color space was initially considered, and although it is an effective representation for the capture and display of colors in electronic imaging systems, this representation is not very intuitive. Indeed, this is not the way humans think about colors; they most often describe colors in terms of their tint, brightness, or colorfulness (that is, whether it is a vivid or pastel color). A color space based on the concept of hue, saturation, and brightness has then been introduced to help users to specify the colors using properties that are more intuitive to them. In this recipe, we will explore the concepts of hue, saturation, and brightness as a means to describe colors.</p><div class="calibre1" title="How to do it..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch03lvl2sec73" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">The conversion of a BGR image into another color space is done using the <code class="literal">cv::cvtColor</code> function that was explored in the previous recipe. Here, we will use the <code class="literal">CV_BGR2HSV</code> conversion code:</p><pre class="programlisting">    // convert into HSV space 
    cv::Mat hsv; 
    cv::cvtColor(image, hsv, CV_BGR2HSV); 
</pre><p class="calibre8">We can go back to the BGR space using the <code class="literal">CV_HSV2BGR</code> code. We can visualize each of the HSV components by splitting the converted image channels into three independent images, as follows:</p><pre class="programlisting">    // split the 3 channels into 3 images 
    std::vector&lt;cv::Mat&gt; channels; 
    cv::split(hsv,channels); 
    // channels[0] is the Hue 
    // channels[1] is the Saturation 
    // channels[2] is the Value 
</pre><p class="calibre8">Note that the third channel is the value of the color, that is, an approximate measure of the brightness of the color. Since we are working on 8-bit images, OpenCV rescales the channel values to cover the <code class="literal">0</code> to <code class="literal">255</code> range (except for the hue, which is rescaled between <code class="literal">0</code> and <code class="literal">0180</code> as it will be explained in the next section). This is very convenient as we are able to display these channels as gray-level images.</p><p class="calibre8">The value channel of the castle image will then look as follows:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_03_008.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The same image in the saturation channel will look as follows:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_03_010.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Finally, the image with the hue channel is as follows:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/B05388_03_07.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">These images are interpreted in the next section.</p></div><div class="calibre1" title="How it works…"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch03lvl2sec74" class="calibre6"/>How it works…</h2></div></div></div><p class="calibre8">The hue/saturation/value color space has been introduced because this representation corresponds to the way humans tend to naturally organize colors. Indeed, humans prefer to describe colors with intuitive attributes such as tint, colorfulness, and brightness. These three attributes are the basis of most phenomenal color spaces. <span><strong class="calibre15">Hue</strong></span> designates the dominant color; the names that we give to colors (such as green, yellow, blue, and red) correspond to the different hue values. <span><strong class="calibre15">Saturation</strong></span> tells us how vivid the color is; pastel colors have low saturation, while the colors of the rainbow are highly saturated. Finally, brightness is a subjective attribute that refers to the luminosity of a color. Other phenomenal color spaces use the concept of color <span><strong class="calibre15">value</strong></span> or color <span><strong class="calibre15">lightness</strong></span> as a way to characterize the relative color intensity.</p><p class="calibre8">These color components try to mimic the intuitive human perception of colors. In consequence, there is no standard definition for them. In the literature, you will find several different definitions and formulae of the hue, saturation, and brightness. OpenCV proposes two implementations of phenomenal color spaces: the HSV and the HLS color spaces. The conversion formulas are slightly different, but they give very similar results.</p><p class="calibre8">The value component is probably the easiest to interpret. In the OpenCV implementation of the HSV space, it is defined as the maximum value of the three BGR components. It is a very simplistic implementation of the brightness concept. For a definition of brightness that matches the human visual system better, you should use the L channel of the perceptually uniform L*a*b* and L*u*v* color spaces. For example, the L channel takes into account the fact that a green color appears to human brighter than, for instance, a blue color of same intensity.</p><p class="calibre8">To compute the saturation, OpenCV uses a formula based on the minimum and maximum values of the BGR components:</p><p class="calibre8">

</p><div class="mediaobject"><img alt="How it works…" src="graphics/B05388_03_13.jpg" class="calibre17"/></div><p class="calibre8">

</p><p class="calibre8">The idea is that a grayscale color in which the three R, G, and B components are all equal will correspond to a perfectly desaturated color; therefore, it will have a saturation value of <code class="literal">0</code>. Saturation is a value between <code class="literal">0</code> and <code class="literal">1.0</code>. For 8-bit images, saturation is rescaled to a value between <code class="literal">0</code> and <code class="literal">255</code>, and when displayed as a gray-level image, brighter areas correspond to the colors that have a higher saturation color.</p><p class="calibre8">For example, from the saturation image in the previous section, it can be seen that the blue of the water is more saturated than the light blue pastel color of the sky, as expected. The different shades of gray have, by definition, a saturation value equal to zero (because, in this case, all three BGR components are equal). This can be observed on the different roofs of the castle, which are made of a dark gray stone. Finally, in the saturation image, you may have noticed some white spots located in areas that correspond to very dark regions of the original image. These are a consequence of the used definition for saturation. Indeed, because saturation measures only the relative difference between the maximum and minimum BGR values, a triplet such as <code class="literal">(1,0,0)</code> gives a perfect saturation of <code class="literal">1.0</code>, even if this color would be seen as black. Consequently, the saturation values measured in dark regions are unreliable and should not be considered.</p><p class="calibre8">The hue of a color is generally represented by an angle value between <code class="literal">0</code> and <code class="literal">360</code>, with the red color at <code class="literal">0</code> degrees. In the case of an 8-bit image, OpenCV divides this angle by two to fit within the 1-byte range. Therefore, each hue value corresponds to a given color tint independent of its brightness and saturation. For example, both the sky and the water have the same hue value, approximately <code class="literal">200</code> degrees (intensity, <code class="literal">100</code>), which corresponds to the blue shade; the green color of the trees in the background has a hue of around <code class="literal">90</code> degrees. It is important to note that hue is less reliable when evaluated for colors that have a very low saturation.</p><p class="calibre8">The HSB color space is often represented by a cone, where each point inside corresponds to a particular color. The angular position corresponds to the hue of the color, the saturation is the distance from the central axis, and the brightness is given by the height. The tip of the cone corresponds to the black color for which the hue and saturation are undefined:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How it works…" src="graphics/image_03_014.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">We can also generate an artificial image that will illustrate the different hue/saturation combinations.</p><pre class="programlisting">    cv::Mat hs(128, 360, CV_8UC3);   
    for (int h = 0; h &lt; 360; h++) { 
      for (int s = 0; s &lt; 128; s++) { 
        hs.at&lt;cv::Vec3b&gt;(s, h)[0] = h/2;    // all hue angles 
        // from high saturation to low 
        hs.at&lt;cv::Vec3b&gt;(s, h)[1] = 255-s*2; 
        hs.at&lt;cv::Vec3b&gt;(s, h)[2] = 255;    // constant value 
      }       
    }
</pre><p class="calibre8">The columns of the following screenshot show the different possible hues (from 0 to 180), while the different lines illustrate the effect of saturation; the top part of the image shows fully saturated colors while the bottom part corresponds to unsaturated colors. A brightness value of <code class="literal">255</code> has been attributed to all the displayed colors:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How it works…" src="graphics/image_03_015.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Interesting effects can be created by playing with the HSV values. Several color effects that can be created using photo editing software are accomplished from this color space. For example, you may decide to modify an image by assigning a constant brightness to all the pixels of an image without changing the hue and saturation. This can be done as follows:</p><pre class="programlisting">    // convert into HSV space 
    cv::Mat hsv; 
    cv::cvtColor(image, hsv, CV_BGR2HSV); 
    // split the 3 channels into 3 images 
    std::vector&lt;cv::Mat&gt; channels; 
    cv::split(hsv,channels); 
    // Value channel will be 255 for all pixels 
    channels[2]= 255; 
    // merge back the channels 
    cv::merge(channels,hsv); 
    // reconvert to BGR 
    cv::Mat newImage; 
    cv::cvtColor(hsv,newImage,CV_HSV2BGR); 
</pre><p class="calibre8">This gives the following image, which now looks like a drawing.</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How it works…" src="graphics/image_03_017.jpg" class="calibre17"/></div><p class="calibre8">
</p></div><div class="calibre1" title="There's more…"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch03lvl2sec75" class="calibre6"/>There's more…</h2></div></div></div><p class="calibre8">The HSV color space can also be very convenient to use when you want to look for objects of specific colors.</p><div class="calibre1" title="Using colors for detection - skin tone detection"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h3 class="title3"><a id="ch03lvl3sec19" class="calibre6"/>Using colors for detection - skin tone detection</h3></div></div></div><p class="calibre8">Color information can be very useful for the initial detection of specific objects. For example, the detection of road signs in a driver-assistance application could rely on the colors of standard signs in order to quickly identify potential road sign candidates. The detection of skin color is another example in which the detected skin regions could be used as an indicator of the presence of a human in an image; this approach is very often used in gesture recognition where skin tone detection is used to detect hand positions.</p><p class="calibre8">In general, to detect an object using color, you first need to collect a large database of image samples that contain the object captured from different viewing conditions. These will be used to define the parameters of your classifier. You also need to select the color representation that you will use for classification. For skin tone detection, many studies have shown that skin color from the diverse ethnical groups clusters well in the hue/saturation space. For this reason, we will simply use the hue and saturation values to identify the skin tones in the following image:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="Using colors for detection - skin tone detection" src="graphics/image_03_018.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">We have defined a function that classifies the pixels of an image as skin or non-skin simply based on an interval of values (the minimum and maximum hue, and the minimum and maximum saturation):</p><pre class="programlisting">    void detectHScolor(const cv::Mat&amp; image,  // input image 
               double minHue, double maxHue,  // Hue interval 
               double minSat, double maxSat,  // saturation interval 
               cv::Mat&amp; mask) {               // output mask 
 
      // convert into HSV space 
      cv::Mat hsv; 
      cv::cvtColor(image, hsv, CV_BGR2HSV); 
 
      // split the 3 channels into 3 images 
      std::vector&lt;cv::Mat&gt; channels; 
      cv::split(hsv, channels); 
      // channels[0] is the Hue 
      // channels[1] is the Saturation 
      // channels[2] is the Value 
 
      // Hue masking 
      cv::Mat mask1; // below maxHue 
      cv::threshold(channels[0], mask1, maxHue, 255,
                    cv::THRESH_BINARY_INV); 
      cv::Mat mask2; // over minHue 
      cv::threshold(channels[0], mask2, minHue, 255, cv::THRESH_BINARY); 
 
      cv::Mat hueMask; // hue mask 
      if (minHue &lt; maxHue) 
        hueMask = mask1 &amp; mask2; 
      else // if interval crosses the zero-degree axis 
        hueMask = mask1 | mask2; 
 
      // Saturation masking 
      // between minSat and maxSat 
      cv::Mat satMask; // saturation mask 
      cv::inRange(channels[1], minSat, maxSat, satMask); 
 
      // combined mask 
      mask = hueMask &amp; satMask; 
    }
</pre><p class="calibre8">Having a large set of skin (and non-skin) samples at our disposal, we could have used a probabilistic approach in which the likelihood of observing a given color in the skin class versus that of observing the same color in the non-skin class would have been estimated. Here, we empirically define an acceptable hue/saturation interval for our test image (remember that the 8-bit version of the hue goes from <code class="literal">0</code> to <code class="literal">180</code> and saturation goes from <code class="literal">0</code> to <code class="literal">255</code>):</p><pre class="programlisting">    // detect skin tone 
    cv::Mat mask; 
    detectHScolor(image, 160, 10,  // hue from 320 degrees to 20 degrees  
                  25, 166,         // saturation from ~0.1 to 0.65 
                  mask); 
 
    // show masked image 
    cv::Mat detected(image.size(), CV_8UC3, cv::Scalar(0, 0, 0)); 
    image.copyTo(detected, mask); 
</pre><p class="calibre8">The following detection image is obtained as the result:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="Using colors for detection - skin tone detection" src="graphics/image_03_020.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Note that, for simplicity, we have not considered color brightness in the detection. In practice, excluding brighter colors would have reduced the possibility of wrongly detecting a bright reddish colors as skin. Obviously, a reliable and accurate detection of skin color would require a much more elaborate analysis. It is also very difficult to guarantee good detection across different images because many factors influence color rendering in photography, such as white balancing and lighting conditions. Nevertheless, as shown here, using hue/saturation information as an initial detector gives us acceptable results.</p></div></div><div class="calibre1" title="See also"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch03lvl2sec76" class="calibre6"/>See also</h2></div></div></div><div class="calibre1"><ul class="itemizedlist"><li class="listitem"><a href="ch05.html" title="Chapter 5. Transforming Images with Morphological Operations">Chapter 5</a>, <span><em class="calibre16">Transforming Images with Morphological Operations</em></span>, shows you how to post-process binary images obtained from detection</li><li class="listitem">The article, <span><em class="calibre16">A survey of skin-color modeling and detection methods, Pattern Recognition, vol. 40, 2007, P. Kakumanu, S. Makrogiannis, N. Bourbakis</em></span>, reviews different methods of skin detection</li></ul></div></div></div></body></html>