["```py\nimport numpy as np\n\n>>> nb_points = 100\n>>> X = np.linspace(-nb_points, nb_points, 200, dtype=np.float32)\n```", "```py\nimport tensorflow as tf\n\n>>> graph = tf.Graph()\n```", "```py\n>>> with graph.as_default():\n>>>    Xt = tf.placeholder(tf.float32, shape=(None, 1), name='x')\n>>>    Y = tf.pow(Xt, 3.0, name='x_3')\n>>>    Yd = tf.gradients(Y, Xt, name='dx')\n>>>    Yd2 = tf.gradients(Yd, Xt, name='d2x')\n```", "```py\n>>> session = tf.InteractiveSession(graph=graph)\n```", "```py\n>>> X2, dX, d2X = session.run([Y, Yd, Yd2], feed_dict={Xt: X.reshape((nb_points*2, 1))})\n```", "```py\nfrom sklearn.datasets import make_classification\n\n>>> nb_samples = 500\n>>> X, Y = make_classification(n_samples=nb_samples, n_features=2, n_redundant=0, n_classes=2)\n```", "```py\nimport tensorflow as tf\n\n>>> graph = tf.Graph()\n\n>>> with graph.as_default():\n>>>    Xt = tf.placeholder(tf.float32, shape=(None, 2), name='points')\n>>>    Yt = tf.placeholder(tf.float32, shape=(None, 1), name='classes')\n>>> \n>>>    W = tf.Variable(tf.zeros((2, 1)), name='weights')\n>>>    bias = tf.Variable(tf.zeros((1, 1)), name='bias')\n>>> \n>>>    Ye = tf.matmul(Xt, W) + bias\n>>>    Yc = tf.round(tf.sigmoid(Ye))\n>>> \n>>>    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Ye, labels=Yt))\n>>>    training_step = tf.train.GradientDescentOptimizer(0.025).minimize(loss)\n```", "```py\n>>> session = tf.InteractiveSession(graph=graph)\n>>> tf.global_variables_initializer().run()\n```", "```py\n>>> feed_dict = {\n>>>    Xt: X,\n>>>    Yt: Y.reshape((nb_samples, 1))\n>>> }\n\n>>> for i in range(5000):\n>>>    loss_value, _ = session.run([loss, training_step], feed_dict=feed_dict)\n>>>    if i % 100 == 0:\n>>>    print('Step %d, Loss: %.3f' % (i, loss_value))\nStep 0, Loss: 0.269\nStep 100, Loss: 0.267\nStep 200, Loss: 0.265\nStep 300, Loss: 0.264\nStep 400, Loss: 0.263\nStep 500, Loss: 0.262\nStep 600, Loss: 0.261\nStep 700, Loss: 0.260\nStep 800, Loss: 0.260\nStep 900, Loss: 0.259\n...\n```", "```py\n>>> Wc, Wb = W.eval(), bias.eval()\n\n>>> print(Wc)\n[[-1.16501403]\n [ 3.10014033]]\n\n>>> print(Wb)\n[[-0.12583369]]\n```", "```py\nfrom sklearn.datasets import make_classification\n\n>>> nb_samples = 1000\n>>> nb_features = 3\n\n>>> X, Y = make_classification(n_samples=nb_samples, n_features=nb_features, \n>>> n_informative=3, n_redundant=0, n_classes=2, n_clusters_per_class=3)\n```", "```py\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n>>> X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n\n>>> lr = LogisticRegression()\n>>> lr.fit(X_train, Y_train)\n>>> print('Score: %.3f' % lr.score(X_test, Y_test))\nScore: 0.715\n```", "```py\nimport tensorflow as tf\nimport tensorflow.contrib.layers as tfl\n\n>>> graph = tf.Graph()\n\n>>> with graph.as_default():\n>>>    Xt = tf.placeholder(tf.float32, shape=(None, nb_features), name='X')\n>>>    Yt = tf.placeholder(tf.float32, shape=(None, 1), name='Y')\n>>> \n>>>    layer_1 = tfl.fully_connected(Xt, num_outputs=50, activation_fn=tf.tanh)\n>>>    layer_2 = tfl.fully_connected(layer_1, num_outputs=1,\n>>>                                  activation_fn=tf.sigmoid)\n>>> \n>>>    Yo = tf.round(layer_2)\n>>> \n>>>    loss = tf.nn.l2_loss(layer_2 - Yt)\n>>>    training_step = tf.train.GradientDescentOptimizer(0.025).minimize(loss)\n```", "```py\n>>> session = tf.InteractiveSession(graph=graph)\n>>> tf.global_variables_initializer().run()\n\n>>> nb_epochs = 200\n>>> batch_size = 50\n\n>>> for e in range(nb_epochs):\n>>>    total_loss = 0.0\n>>>    Xb = np.ndarray(shape=(batch_size, nb_features), dtype=np.float32)\n>>>    Yb = np.ndarray(shape=(batch_size, 1), dtype=np.float32)\n>>> \n>>>    for i in range(0, X_train.shape[0]-batch_size, batch_size):\n>>>       Xb[:, :] = X_train[i:i+batch_size, :]\n>>>       Yb[:, 0] = Y_train[i:i+batch_size]\n>>> \n>>>       loss_value, _ = session.run([loss, training_step], \n>>>                                   feed_dict={Xt: Xb, Yt: Yb})\n>>>       total_loss += loss_value\n>>> \n>>>        Y_predicted = session.run([Yo], \n>>>               feed_dict={Xt: X_test.reshape((X_test.shape[0], nb_features))})\n>>>        accuracy = 1.0 -\n>>>            (np.sum(np.abs(np.array(Y_predicted[0]).squeeze(axis=1) -Y_test)) /\n>>>            float(Y_test.shape[0]))\n>>> \n>>>        print('Epoch %d) Total loss: %.2f - Accuracy: %.2f' % \n>>>              (e, total_loss, accuracy))\n\nEpoch 0) Total loss: 78.19 - Accuracy: 0.66\nEpoch 1) Total loss: 75.02 - Accuracy: 0.67\nEpoch 2) Total loss: 72.28 - Accuracy: 0.68\nEpoch 3) Total loss: 68.52 - Accuracy: 0.71\nEpoch 4) Total loss: 63.50 - Accuracy: 0.79\nEpoch 5) Total loss: 57.51 - Accuracy: 0.84\n...\nEpoch 195) Total loss: 15.34 - Accuracy: 0.94\nEpoch 196) Total loss: 15.32 - Accuracy: 0.94\nEpoch 197) Total loss: 15.31 - Accuracy: 0.94\nEpoch 198) Total loss: 15.29 - Accuracy: 0.94\nEpoch 199) Total loss: 15.28 - Accuracy: 0.94\n```", "```py\nfrom scipy.misc import face\n\n>>> img = face(gray=True)\n```", "```py\nimport numpy as np\n\n>>> kernel = np.array(\n>>>    [[0, 1, 0],\n>>>     [1, -4, 0],\n>>>     [0, 1, 0]], \n>>>    dtype=np.float32)\n\n>>> cfilter = np.zeros((3, 3, 1, 1), dtype=np.float32)\n>>> cfilter[:, :, 0, 0] = kernel \n```", "```py\nimport tensorflow as tf\n\n>>> graph = tf.Graph()\n\n>>> with graph.as_default():\n>>>    x = tf.placeholder(tf.float32, shape=(None, 768, 1024, 1), name='image')\n>>>    f = tf.constant(cfilter)\n\n>>>    y = tf.nn.conv2d(x, f, strides=[1, 1, 1, 1], padding='SAME')\n\n>>> session = tf.InteractiveSession(graph=graph)\n\n>>> c_img = session.run([y], feed_dict={x: img.reshape((1, 768, 1024, 1))})\n>>> n_img = np.array(c_img).reshape((768, 1024))\n```", "```py\nfrom keras.models import Sequential\n\n>>> model = Sequential()\n\n>>> model.add(...)\n>>> model.add(...)\n...\n>>> model.add(...)\n```", "```py\nfrom keras.datasets import cifar10\n\n>>> (X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n```", "```py\nfrom keras.models import Sequential\nfrom keras.layers.convolutional import Conv2D, ZeroPadding2D\nfrom keras.layers.pooling import MaxPooling2D\n\n>>> model = Sequential()\n\n>>> model.add(Conv2D(32, kernel_size=(5, 5), activation='relu', input_shape=(32 ,32, 3)))\n>>> model.add(MaxPooling2D(pool_size=(2, 2)))\n\n>>> model.add(Conv2D(64, kernel_size=(4, 4), activation='relu'))\n>>> model.add(ZeroPadding2D((1, 1)))\n\n>>> model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n>>> model.add(MaxPooling2D(pool_size=(2, 2)))\n>>> model.add(ZeroPadding2D((1, 1)))\n```", "```py\nfrom keras.layers.core import Dense, Dropout, Flatten\n\n>>> model.add(Dropout(0.2))\n>>> model.add(Flatten())\n>>> model.add(Dense(128, activation='relu'))\n>>> model.add(Dropout(0.2))\n>>> model.add(Dense(10, activation='softmax')) \n```", "```py\n>>> model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n```", "```py\nfrom keras.utils import to_categorical\n\n>>> model.fit(X_train / 255.0, to_categorical(Y_train), batch_size=32, epochs=15)\n```", "```py\nEpoch 1/15\n50000/50000 [==============================] - 25s - loss: 1.5845 - acc: 0.4199 \nEpoch 2/15\n50000/50000 [==============================] - 24s - loss: 1.2368 - acc: 0.5602 \nEpoch 3/15\n50000/50000 [==============================] - 26s - loss: 1.0678 - acc: 0.6247 \nEpoch 4/15\n50000/50000 [==============================] - 25s - loss: 0.9495 - acc: 0.6658 \nEpoch 5/15\n50000/50000 [==============================] - 26s - loss: 0.8598 - acc: 0.6963 \nEpoch 6/15\n50000/50000 [==============================] - 26s - loss: 0.7829 - acc: 0.7220 \nEpoch 7/15\n50000/50000 [==============================] - 26s - loss: 0.7204 - acc: 0.7452 \nEpoch 8/15\n50000/50000 [==============================] - 26s - loss: 0.6712 - acc: 0.7629 \nEpoch 9/15\n50000/50000 [==============================] - 27s - loss: 0.6286 - acc: 0.7779 \nEpoch 10/15\n50000/50000 [==============================] - 27s - loss: 0.5753 - acc: 0.7952 \nEpoch 11/15\n50000/50000 [==============================] - 27s - loss: 0.5433 - acc: 0.8049 \nEpoch 12/15\n50000/50000 [==============================] - 27s - loss: 0.5112 - acc: 0.8170 \nEpoch 13/15\n50000/50000 [==============================] - 27s - loss: 0.4806 - acc: 0.8293 \nEpoch 14/15\n50000/50000 [==============================] - 28s - loss: 0.4551 - acc: 0.8365 \nEpoch 15/15\n50000/50000 [==============================] - 28s - loss: 0.4342 - acc: 0.8444\n```", "```py\n>>> scores = model.evaluate(X_test / 255.0, to_categorical(Y_test))\n>>> print('Loss: %.3f' % scores[0])\n>>> print('Accuracy: %.3f' % scores[1])\nLoss: 0.972\nAccuracy: 0.719\n```"]