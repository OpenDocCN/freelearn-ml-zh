- en: Machine Learning with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers the use of Spark on the Microsoft platform and will also
    provide a walk-through on how to train ML models using Spark, along with the options
    available in Azure to perform Spark-based ML training.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: ML with Azure Databricks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure HDInsight with Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Walkthroughs of some labs so that you can see exciting technologies in action
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning with Azure Databricks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By adopting ML, enterprises are looking to improve their business, or even radically
    transform it by using data as the lifeblood for digital transformation. Databricks
    empowers companies to develop their data science competency quickly, and turn
    that into a competitive advantage by providing a fully integrated unified analytics
    platform in Azure.
  prefs: []
  type: TYPE_NORMAL
- en: Enterprises want to leverage the treasure trove of data they have collected
    historically. Organizations have begun to collect more data recently. This includes
    data in a variety of forms, including new customer data in the form of clickstreams,
    web logs, and sensor data from Internet of Things devices and machines, as well
    as audio, images, and videos.
  prefs: []
  type: TYPE_NORMAL
- en: Using insights from this data, enterprises across various verticals can improve
    business outcomes in many different ways that impact our daily lives. These include
    medical diagnosis, fraud detection, detecting cyber attacks, optimizing manufacturing
    pipelines, customer engagement, and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: Databricks offers a unified analytics platform that brings data engineers, data
    scientists, and businesses together to collaborate across the data life cycle,
    starting from ETL programs, through to building analytic applications for production
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: Data engineers can use Databricks' ETL capability to create new datasets from
    various sources, including structured, semi-structured, and unstructured data.
    Data scientists can choose from a variety of programming languages, such as SQL,
    R, Python, Scala, and Java, and **Machine Learning** (**ML**) frameworks and libraries
    including Scikit-learn, Apache Spark ML, TensorFlow, and Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Databricks allows enterprises to explore data, and create and test their models
    in a collaborative way, using Databricks' notebook and visualization capacities.
    Time-to-delivery is quick and the process of sending ML pipelines to production
    is also quick.
  prefs: []
  type: TYPE_NORMAL
- en: What challenges is Databricks trying to solve?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Integrating data is always difficult. However, integration challenges are even
    more difficult in ML because of the various frameworks and libraries that need
    to be integrated.
  prefs: []
  type: TYPE_NORMAL
- en: Databricks has a focus enterprise readiness of data science platforms in terms
    of security and manageability.
  prefs: []
  type: TYPE_NORMAL
- en: How do we get started with Apache Spark and Azure Databricks? The first step
    is to get the Azure Databricks software set up in Azure.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Apache Spark and Azure Databricks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this walk-through, we will start to explore Azure Databricks. A key step
    in the process is to set up an instance of Azure Databricks, and this is covered
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in to the Azure portal ([https://portal.azure.com/](https://portal.azure.com/)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select + Create a resource | Analytics | Azure Databricks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the Azure Databricks Service dialog, provide the workspace configuration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Workspace name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter a name for your Azure Databricks workspace
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Subscription:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select your Azure subscription
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Resource group:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new resource group ([https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-overview](https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-overview))
    or use an existing one
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Location:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select a geographical region ([https://docs.azuredatabricks.net/administration-guide/cloud-configurations/regions.html](https://docs.azuredatabricks.net/administration-guide/cloud-configurations/regions.html))
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pricing Tier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select a pricing tier ([https://azure.microsoft.com/en-us/pricing/details/databricks/](https://azure.microsoft.com/en-us/pricing/details/databricks/)).
    If you select **Trial (Premium - 14-Days Free DBUs)**, the workspace has access
    to free Premium Azure Databricks DBUs for 14 days.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Select **Pin to dashboard** and then click **Create**. The portal will display Deployment
    in progress. After a few minutes, the Azure Databricks service page displays,
    as can be seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/0d80d523-e551-4514-a52e-238742d48224.png)'
  prefs: []
  type: TYPE_IMG
- en: 'On the left-hand side, you can access fundamental Azure Databricks entities:
    workspace, clusters, tables, notebooks, jobs, and libraries.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The workspace is the special root folder that stores your Azure Databricks
    assets, such as notebooks and libraries, and the data that you import:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bfb2c9d5-24f8-4f71-b84a-2462a473b7cf.png)'
  prefs: []
  type: TYPE_IMG
- en: Creating a cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A cluster is a collection of Azure Databricks computation resources:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a cluster, click the **Clusters** button in the sidebar and click **Create
    Clus****ter**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/edeef29f-8e46-49b1-91ac-e7b8d0c296eb.png)'
  prefs: []
  type: TYPE_IMG
- en: On the **New Cluster** page, specify the cluster name.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select 4.2 (includes Apache Spark 2.3.1, Scala 11) in the Databricks Runtime
    Version dropdown.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click Create Cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a Databricks Notebook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A Notebook is a collection of cells that run computations on a Spark cluster.
    To create a Notebook in the Workspace, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the sidebar, click the Workspace button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the `Workspace` folder, select Create Notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/bf8c4a5f-e3e5-48c3-9331-55a2e727e2b4.png)'
  prefs: []
  type: TYPE_IMG
- en: On the Create Notebook dialog, enter a name and select SQL in the **Language**
    dropdown.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click Create. The Notebook opens with an empty cell at the top.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using SQL in Azure Databricks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, you can run a SQL statement to create a table and work with
    data using SQL Statements:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Copy and paste this code snippet into the notebook cell to see a list of the
    Azure Databricks datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The code appears as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Press *Shift* + *Enter*. The notebook automatically attaches to the cluster
    you created in *Step 2*, creates the table, loads the data, and returns `OK`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e484bd3c-d107-42d8-aeaa-246d91fc84a9.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, you can run a SQL statement ...
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Displaying data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Display a chart of the average diamond price by color:'
  prefs: []
  type: TYPE_NORMAL
- en: Click the Bar chart icon
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click Plot options
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Drag color into the Keys box
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Drag price into the Values box
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the Aggregation dropdown, select AVG:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/497d3a75-697e-4db4-805a-e4bb75a1aad4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click Apply to display the bar chart:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b2c3fccf-5845-49a3-98fb-cb1a87f91fdd.png)'
  prefs: []
  type: TYPE_IMG
- en: Machine Learning with HDInsight
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark is the largest open source process in data processing. Since its
    release, Apache Spark has seen rapid adoption by enterprises across a wide range
    of industries. Apache Spark is a fast, in-memory data processing engine, with
    elegant and expressive development APIs to allow data workers to efficiently execute
    streaming. In addition, Apache Spark facilitates ML and SQL workloads that require
    fast iterative access to datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The focus of the current chapter is Apache Spark, which is an open source system
    for fast, large-scale data processing and ML.
  prefs: []
  type: TYPE_NORMAL
- en: The Data Science virtual machine provides you with a standalone (single node
    in-process) instance of the Apache Spark platform.
  prefs: []
  type: TYPE_NORMAL
- en: What is Spark?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark is designed as a high-performance, general-purpose computation engine
    for fast, large-scale big data processes. Spark works by distributing its workload
    across different nodes in a cluster. Spark scales out to process large volumes
    of data. Spark is aimed at big data batch processing, and is excellent for producing
    analytics using low-latency, high-performance data as a basis for operations.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark consists of Spark Core and a set of libraries. Core is the distributed
    execution engine and the Java, Scala, and Python APIs offer a platform for distributed
    ETL application development. This allows developers to quickly achieve success
    by writing applications in Java, Scala, or Python.
  prefs: []
  type: TYPE_NORMAL
- en: Spark is built on the concept of a **Resilient Distributed Dataset** (**RDD**),
    which has been a core Spark concept for working with data since the inception
    of Spark. RDDs are similar to dataframes in R. RDDs are high-level abstractions
    of the data that provide data scientists with a schema to retrieve and work with
    data. RDDs are immutable collections representing datasets and have the inbuilt
    capability of reliability and failure recovery. RDDs create new RDDs upon any
    operation, such as transformation or action. They also store the lineage, which
    is used to recover from failures. For example, it's possible to separate the data
    out into the appropriate field and columns in that dataset, which means that data
    scientists can work with them more intuitively.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Spark process involves a number of steps, which can involve more than one
    RDD. So, it is possible to have more than one RDD during processing. Here is an
    example, which shows how RDDs can be the source and the output of different processing
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5adcc927-b958-45d5-acb2-b8356d7b5f0b.png)'
  prefs: []
  type: TYPE_IMG
- en: Apache Spark allows for complex data engineering, and it comes with a built-in
    set of over 80 high-level operators. As well as longer processing, it is possible
    to interactively query data within the shell. In addition to the Map and Reduce
    operations, it supports SQL queries, Streaming Data, ML, and Graph Data Processing.
  prefs: []
  type: TYPE_NORMAL
- en: Developers can use these capabilities standalone, or combine them to run in
    a single data pipeline use case.
  prefs: []
  type: TYPE_NORMAL
- en: Spark powers a stack of libraries, including SQL and DataFrames ([https://spark.apache.org/sql/](https://spark.apache.org/sql/)),
    MLlib ([https://spark.apache.org/mllib/](https://spark.apache.org/mllib/)) for
    ML, GraphX ([https://spark.apache.org/graphx/](https://spark.apache.org/graphx/)),
    and Spark Streaming ([https://spark.apache.org/streaming/](https://spark.apache.org/streaming/)).
    You can combine these libraries seamlessly in the same application.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, RDDs will be the focus of the ML exercises. We will be focusing
    on hands-on exercises using Jupyter Notebooks. Jupyter Notebooks are available
    on the Data Science Virtual Machine, and they are installed by default as a service
    with the Azure HDInsight deployment of Spark.
  prefs: []
  type: TYPE_NORMAL
- en: HDInsight and Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark is an open source parallel processing framework that supports in-memory
    processing to boost the performance of big data analytic applications. The Apache
    Spark cluster on HDInsight is compatible with Azure Storage (WASB), as well as
    Azure Data Lake Store.
  prefs: []
  type: TYPE_NORMAL
- en: When the developer creates a Spark cluster on HDInsight, the Azure compute resources
    are already created with Spark installed and configured. It only takes about 10
    minutes to create a Spark cluster in HDInsight. The data to be processed is stored
    in Azure Storage or Azure Data Lake Storage.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark provides primitives for in-memory cluster computing, which means
    that it is the perfect partner for HDInsight. An Apache Spark job can load and
    cache data ...
  prefs: []
  type: TYPE_NORMAL
- en: The YARN operation system in Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: YARN is one of the key features in the second-generation Hadoop 2 version of
    the Apache Software Foundation's open source distributed processing framework,
    and it is retained and progressed in Hadoop Version 3\. YARN is implemented on
    Azure HDInsight to facilitate large-scale, distributed operating systems for big
    data applications and predictive analytics.
  prefs: []
  type: TYPE_NORMAL
- en: YARN is efficient because it decouples MapReduce's resource management and scheduling
    capabilities from the data processing component. Since Apache Spark uses this
    methodology, it empowers Hadoop to support more varied processing approaches and
    a broader array of applications.
  prefs: []
  type: TYPE_NORMAL
- en: How can we use Spark to conduct predictive analytics? ML focuses on taking data
    and applying a process to that data to produce a predicted output. There are many
    different types of ML algorithms that we can create and use with Spark. One of
    the most common methods is supervised ML, which works by taking in some data that
    consists of a vector of what we call features and a label. What do we mean by
    this?
  prefs: []
  type: TYPE_NORMAL
- en: A vector is a set of information that we use in order to make a prediction.
    A label is a characteristic that is used to make the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will take an example. Let''s say that we have a set of information about
    people, and we would like to predict something about this group of people: whether
    they are likely to become homeless or not. The characteristics of these people
    might include their age, education level, earnings, military service, and so on.
    The characteristics of the people would be called the features, and the thing
    that we would like to predict is known as the **label**.'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the data scientist would take some data where they know that they
    have already become homeless, and therefore the label value would be known to
    the researchers at that point.
  prefs: []
  type: TYPE_NORMAL
- en: Using Spark, we would then process the data and fit the data to the model to
    see how successful it is. The model would tell us what we need to see in the characteristics
    in order to see the likelihood of homelessness occurring for these individuals.
  prefs: []
  type: TYPE_NORMAL
- en: The model is essentially a function that specifies what we expect to see in
    the vector features to see the outcome, or prediction.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to take previously unseen, new data that doesn't contain a
    known label to see how it fits the model. This dataset just has the features because
    the actual label, or outcome, isn't known. In this supervised learning example,
    data with a known label is used to train the model to predict data with the known
    label, and then the model is faced with data that does not have the known label.
  prefs: []
  type: TYPE_NORMAL
- en: In unsupervised learning, the label is not known. Unsupervised learning takes
    a similar approach, where the data scientist will ingest data and feed it, which
    simply has the vector of features, and no label is present. With this type of
    data science methodology, I may simply be looking at the similarities that are
    found in the vector features in order to see whether there are any clusters or
    commonalities in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Working with data in a Spark environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When data scientists work with data in an Apache Spark environment, they typically
    work with either RDDs or DataFrames. In our examples so far, the data may be stored
    in the RDD format, and it is fed into the model by building a predictive feed
    into the model.
  prefs: []
  type: TYPE_NORMAL
- en: In these exercises, the Spark library is called `spark.mllib`. The MLlib library
    is the original ML library that comes with Spark. The newer library is called
    **Spark ML**.
  prefs: []
  type: TYPE_NORMAL
- en: Using Jupyter Notebooks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Jupyter Notebook is an incredibly powerful tool for collaboratively developing
    and producing data science projects. It integrates code, comments, and code output
    into a single document that combines code, data visualizations, narrative text,
    mathematical equations, and other data science artefacts. Notebooks are increasingly
    popular in today's data science workflows because they encourage iterative and
    rapid development for data science teams. Jupyter project is the successor to
    the earlier IPython Notebook. It is possible to use many different programming
    languages within Jupyter Notebooks, but this chapter will focus on Apache Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Jupyter is free, open source, and browser-based. It can be used to create notebooks
    for working with your code in normal ways, such as writing and commenting code.
    One key feature of Jupyter Notebooks is that they are very useful for collaboration
    with other team members, thereby enabling productivity. The Jupyter Notebook supports
    a number of different engines, also known as kernels. The Jupyter Notebook can
    be used to run code on Python or Scala.
  prefs: []
  type: TYPE_NORMAL
- en: In this walk-through, the Spark ML tutorial will be used in order to introduce
    the concepts of Spark and ML.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the data science virtual machine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you are using the Ubuntu Linux DSVM edition, there is a requirement to do
    a one-time setup step to enable a local single node Hadoop HDFS and YARN instance.
    By default, Hadoop services are installed but disabled on the DSVM. In order to
    enable it, it is necessary to run the following commands as root the first time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You can stop the Hadoop-related ...
  prefs: []
  type: TYPE_NORMAL
- en: Running Spark MLib commands in Jupyter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The default Jupyter lab will demonstrate features and capabilities of Spark's
    MLlib toolkit for ML problems. The walk-through uses a sample dataset, which holds
    data from real trips of NYC taxis. The data holds the NYC taxi trip and fare dataset
    to show MLlib's modeling features for binary classification and regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: In this lab, many different Spark MLib functions will be used, including data
    ingestion, data exploration, data preparation (featurizing and transformation),
    modeling, prediction, model persistence, and model evaluation on an independent
    validation dataset. Data visualization will also be used in order to demonstrate
    the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The lab will focus on two types of learning: classification offers the opportunity
    to try out supervised and unsupervised Learning. The first sample will use binary
    classification to predict whether a tip will be given. In the second sample, regression
    will be used to predict the level of tip given.'
  prefs: []
  type: TYPE_NORMAL
- en: In Jupyter, the code is executed in the cell. The cell structure is a simple
    way of enabling the data scientist to query the RDD dataframe, and interactively
    display information in the Jupyter Notebook, including data visualization. It's
    a very flexible, intuitive, and powerful way to work with our data to use ML using
    Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Data ingestion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first activity is to set the appropriate directory paths.
  prefs: []
  type: TYPE_NORMAL
- en: 'Set the location of training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the model storage directory path. This is where models will be saved:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In the Jupyter menu, put the cursor in cell and select the Run option from the
    menu. This will assign the training and test sets to the `taxi_train_file_loc`
    and `taxi_valid_file_loc `variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, the data will be set into a new dataframe, and it will be cleaned. Data
    ingestion was completed using the `spark.read.csv` function, which assigns the
    data to a new ...
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the next step, it's important to explore the data. It's easy to visualize
    the data right in the Jupyter interface by plotting the target variables and features.
    The data is summarized using SQL. Then, the data is plotted using `matplotlib`.
    To plot the data, the dataframe will first have to be converted to a pandas dataframe.
    At that point, matplotlib can use it to generate plots.
  prefs: []
  type: TYPE_NORMAL
- en: Since Spark is designed to work with large big data datasets, if the Spark dataframe
    is large, a sample of the data can be used for data visualization purposes.
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, 50% of the data was sampled prior to converting the
    data into the dataframe format, and then it was incorporated into a pandas dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code is provided in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This will produce a histogram of the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The relationship between the fare amount and the tip amount is shown in the
    following chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/74038367-c36c-42f6-b2e0-2a0e7bc1f714.png)'
  prefs: []
  type: TYPE_IMG
- en: Feature engineering in Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In feature engineering, we can take care of numerous data engineering tasks,
    such as creating new features and grouping, transforming, and cleaning up data.
    The data can undergo further indexing and be enriched by additional classification,
    grouping, and the encoding of categorical features.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example code, we create a new feature by binning hours into
    traffic time buckets using Spark SQL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Using Spark for prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this part of the chapter, the exercise is to use the Spark sample code to
    create a logistic regression model, save the model, and evaluate the performance
    of  the model on a test dataset.  For modeling, the features and class labels
    are specified using the `RFormula` function. In this example, we will train the
    model using the pipeline formula and a logistic regression estimator. This can
    be seen from the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code block sets up the training formula and assigns it to the
    `classFormula` variable, which can be seen from the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code block trains the pipeline model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code block saves the model that we have created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code block uses the model to predict results, using test data.
    The following code block helps to evaluate the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we plot the ROC curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The regression output is shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a4bf5967-5356-47d0-ba1f-5abf0099000f.png)'
  prefs: []
  type: TYPE_IMG
- en: The ROC Curve is the blue curve, which sits very high at the top-left corner.
    The ROC result shows that the model is performing extremely well. As a heuristic,
    the closer the blue line is to the top of the upper-left side of the chart, the
    better the result.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, the random forest model regression method will be used to predict
    how much of a tip will be given. In a standard classification tree, the data is
    split based on the homogeneity of the data. A decision tree is built top-down
    from a root node. The process involves partitioning data into subsets that contain
    instances that are homogeneous.
  prefs: []
  type: TYPE_NORMAL
- en: In a regression tree, the target variable is a real-value number. In this case,
    the data is fitted against a regression model to the target variable using each
    of the independent variables. For each independent variable, the data is split
    at several split points. We calculate the **Sum of Squared Error **(**SSE**) at
    each split boundary between the predicted value and the actual values. The variable
    resulting in the lowest SSE is selected for the node. Then, this process is recursively
    continued until all of the data is covered.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we define the indexer for the categorical variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we set up the random forest estimator. The value is set to the `randForest`variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to fit the model using the defined formula and the relevant
    transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The next crucial step is to save the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to use the model to predict against the test data so that we
    can evaluate its success:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'There''s no substitute for visualizing the data. In the next code block, the
    data is visualized using a scattergram format. The end result is shown after the
    code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting chart can be found next:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/de0a0f54-8e02-4f8b-9bb1-0f00cf143aab.png)'
  prefs: []
  type: TYPE_IMG
- en: The lower the **Root Mean Square Error** (**RMSE**) value, the better the absolute
    fit. The RMSE is calculated as the square root of the variance of the residuals.
    It specifies the absolute fit of the model to the data. In other words, it denotes
    how close the observed actual data points are to the model's predicted values.
    As the square root of a variance, RMSE can be conceived as the standard deviation
    of the unexplained variance. The RMSE has the useful property of being in the
    same units as the response variable, so it intuitively makes sense. Lower RMSE
    values indicate a better fit. RMSE is a good measure of how accurately the model
    predicts the response. RMSE is the most important criterion for fit in this case,
    since the main purpose of the model is prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '**R-sqr** is intuitive. Its value ranges from zero to one, with zero indicating
    that the proposed model does not improve prediction over the mean model, and one
    indicating perfect prediction. Improvement in the regression model results in
    proportional increases in R-sqr.'
  prefs: []
  type: TYPE_NORMAL
- en: Whereas R-sqr is a relative measure of fit, RMSE is an absolute measure of fit,
    and that's why it's shown here.
  prefs: []
  type: TYPE_NORMAL
- en: Loading a pipeline model and evaluating the test data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we will load a pipeline model and then evaluate the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'In the next step, we define random forest models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will define a modeling pipeline that includes formulas, feature transformations,
    and an estimator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Setting up an HDInsight cluster with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's crucial to provision an HDInsight Spark cluster in order to start the work.
  prefs: []
  type: TYPE_NORMAL
- en: Provisioning an HDInsight cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you already have a Spark HDInsight cluster running, you can skip this procedure.
  prefs: []
  type: TYPE_NORMAL
- en: In a web browser, navigate to [http://portal.azure.com](http://portal.azure.com) and,
    if prompted, sign in using the Microsoft account that is associated with your
    Azure subscription.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the Microsoft Azure portal, in the Hub Menu, click New. Then, in the Data
    + Analytics section, select HDInsight and create a new HDInsight cluster with
    the following settings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Cluster Name**: Enter a unique name (and make a note of it!)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Subscription**: Select your Azure subscription'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cluster Type**: Spark'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cluster Operating System**: Linux'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**HDInsight Version**: Choose the latest version of Spark'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cluster Tier**: Standard'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cluster Login Username**: Enter a username of your choice (and ...'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you were introduced to some of the latest big data analytics
    technologies in Microsoft Azure. The chapter has focused on two main technologies:
    Azure HDInsight with Spark, and Azure Databricks.'
  prefs: []
  type: TYPE_NORMAL
- en: In the chapter, we have looked at the different ways of modelling data, and
    we have covered useful tips to help you to understand what the models actually
    mean. Often, this is not the end of the data science process, because this may
    throw up new questions as you get new insights. So, it is a process rather than
    a race—but that's what makes it interesting!
  prefs: []
  type: TYPE_NORMAL
- en: Further references
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Review the Spark Machine Learning Programming Guide at [https://spark.apache.org/docs/latest/ml-guide.html](https://spark.apache.org/docs/latest/ml-guide.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for Microsoft Azure HDInsight, including Spark Clusters is at [https://azure.microsoft.com/en-us/documentation/services/hdinsight](https://azure.microsoft.com/en-us/documentation/services/hdinsight)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation and getting started guidance for Programming with Scala is at [http://www.scala-lang.org/documentation/](http://www.scala-lang.org/documentation/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation and getting started guidance for Programming with Python is at [https://www.python.org/doc/](https://www.python.org/doc/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can view the Spark SQL and DataFrames Programming Guide at [https://spark.apache.org/docs/latest/sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification and Regression: [https://spark.apache.org/docs/latest/ml-classification-regression.html](https://spark.apache.org/docs/latest/ml-classification-regression.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipelines: [https://spark.apache.org/docs/latest/ml-pipeline.html ...](https://spark.apache.org/docs/latest/ml-pipeline.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
