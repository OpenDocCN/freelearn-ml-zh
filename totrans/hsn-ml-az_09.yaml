- en: Machine Learning with Spark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Spark 进行机器学习
- en: This chapter covers the use of Spark on the Microsoft platform and will also
    provide a walk-through on how to train ML models using Spark, along with the options
    available in Azure to perform Spark-based ML training.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了 Microsoft 平台上 Spark 的使用，并将提供如何使用 Spark 训练 ML 模型的演练，以及 Azure 中可用于执行基于 Spark
    的 ML 训练的选项。
- en: 'We will be covering the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主题：
- en: ML with Azure Databricks
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Azure Databricks 的机器学习
- en: Azure HDInsight with Spark
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure HDInsight 与 Spark
- en: Walkthroughs of some labs so that you can see exciting technologies in action
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 某些实验室的演练，以便您可以看到令人兴奋的技术在实际应用中的表现
- en: Machine learning with Azure Databricks
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Azure Databricks 进行机器学习
- en: By adopting ML, enterprises are looking to improve their business, or even radically
    transform it by using data as the lifeblood for digital transformation. Databricks
    empowers companies to develop their data science competency quickly, and turn
    that into a competitive advantage by providing a fully integrated unified analytics
    platform in Azure.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 通过采用机器学习，企业希望改善他们的业务，甚至通过使用数据作为数字化转型生命线的方式彻底改变它。Databricks 使公司能够快速开发他们的数据科学能力，并通过在
    Azure 中提供完全集成的统一分析平台将其转化为竞争优势。
- en: Enterprises want to leverage the treasure trove of data they have collected
    historically. Organizations have begun to collect more data recently. This includes
    data in a variety of forms, including new customer data in the form of clickstreams,
    web logs, and sensor data from Internet of Things devices and machines, as well
    as audio, images, and videos.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 企业希望利用他们历史上收集的数据宝库。组织最近开始收集更多数据。这包括各种形式的数据，包括以点击流、网页日志和物联网设备和机器的传感器数据形式的新客户数据，以及音频、图像和视频。
- en: Using insights from this data, enterprises across various verticals can improve
    business outcomes in many different ways that impact our daily lives. These include
    medical diagnosis, fraud detection, detecting cyber attacks, optimizing manufacturing
    pipelines, customer engagement, and so forth.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析这些数据，各个垂直领域的企业可以通过多种方式改善业务成果，这些方式影响我们的日常生活。这包括医疗诊断、欺诈检测、检测网络攻击、优化制造流程、客户参与等等。
- en: Databricks offers a unified analytics platform that brings data engineers, data
    scientists, and businesses together to collaborate across the data life cycle,
    starting from ETL programs, through to building analytic applications for production
    environments.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 提供了一个统一的分析平台，将数据工程师、数据科学家和业务人员聚集在一起，在整个数据生命周期中进行协作，从 ETL 程序开始，到为生产环境构建分析应用程序。
- en: Data engineers can use Databricks' ETL capability to create new datasets from
    various sources, including structured, semi-structured, and unstructured data.
    Data scientists can choose from a variety of programming languages, such as SQL,
    R, Python, Scala, and Java, and **Machine Learning** (**ML**) frameworks and libraries
    including Scikit-learn, Apache Spark ML, TensorFlow, and Keras.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程师可以使用 Databricks 的 ETL 功能从各种来源创建新的数据集，包括结构化、半结构化和非结构化数据。数据科学家可以选择多种编程语言，如
    SQL、R、Python、Scala 和 Java，以及 **机器学习** (**ML**) 框架和库，包括 Scikit-learn、Apache Spark
    ML、TensorFlow 和 Keras。
- en: Databricks allows enterprises to explore data, and create and test their models
    in a collaborative way, using Databricks' notebook and visualization capacities.
    Time-to-delivery is quick and the process of sending ML pipelines to production
    is also quick.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 允许企业以协作的方式探索数据，并使用 Databricks 的笔记本和可视化能力创建和测试他们的模型。交付时间快，将机器学习管道发送到生产的过程也很快。
- en: What challenges is Databricks trying to solve?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Databricks 正在尝试解决哪些挑战？
- en: Integrating data is always difficult. However, integration challenges are even
    more difficult in ML because of the various frameworks and libraries that need
    to be integrated.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 集成数据始终是困难的。然而，由于需要集成的各种框架和库，集成挑战在机器学习中更为困难。
- en: Databricks has a focus enterprise readiness of data science platforms in terms
    of security and manageability.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 在数据科学平台的安全性和可管理性方面注重企业就绪性。
- en: How do we get started with Apache Spark and Azure Databricks? The first step
    is to get the Azure Databricks software set up in Azure.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何开始使用 Apache Spark 和 Azure Databricks？ 第一步是在 Azure 中设置 Azure Databricks 软件。
- en: Getting started with Apache Spark and Azure Databricks
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用 Apache Spark 和 Azure Databricks
- en: 'In this walk-through, we will start to explore Azure Databricks. A key step
    in the process is to set up an instance of Azure Databricks, and this is covered
    here:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将开始探索Azure Databricks。过程中的一个关键步骤是设置Azure Databricks的一个实例，这在本部分中介绍：
- en: Log in to the Azure portal ([https://portal.azure.com/](https://portal.azure.com/)).
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录到Azure门户([https://portal.azure.com/](https://portal.azure.com/))。
- en: Select + Create a resource | Analytics | Azure Databricks.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择+创建资源 | 分析 | Azure Databricks。
- en: In the Azure Databricks Service dialog, provide the workspace configuration.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Azure Databricks服务对话框中，提供工作区配置。
- en: 'Workspace name:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 工作区名称：
- en: Enter a name for your Azure Databricks workspace
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为您的Azure Databricks工作区输入一个名称
- en: 'Subscription:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 订阅：
- en: Select your Azure subscription
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择您的Azure订阅
- en: 'Resource group:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 资源组：
- en: Create a new resource group ([https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-overview](https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-overview))
    or use an existing one
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个新的资源组([https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-overview](https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-overview))或使用现有的一个
- en: 'Location:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 位置：
- en: Select a geographical region ([https://docs.azuredatabricks.net/administration-guide/cloud-configurations/regions.html](https://docs.azuredatabricks.net/administration-guide/cloud-configurations/regions.html))
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个地理区域([https://docs.azuredatabricks.net/administration-guide/cloud-configurations/regions.html](https://docs.azuredatabricks.net/administration-guide/cloud-configurations/regions.html))
- en: 'Pricing Tier:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定价层：
- en: Select a pricing tier ([https://azure.microsoft.com/en-us/pricing/details/databricks/](https://azure.microsoft.com/en-us/pricing/details/databricks/)).
    If you select **Trial (Premium - 14-Days Free DBUs)**, the workspace has access
    to free Premium Azure Databricks DBUs for 14 days.
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择定价层([https://azure.microsoft.com/en-us/pricing/details/databricks/](https://azure.microsoft.com/en-us/pricing/details/databricks/))。如果您选择**试用（高级
    - 14天免费DBUs）**，则工作区将免费获得14天的高级Azure Databricks DBUs。
- en: 'Select **Pin to dashboard** and then click **Create**. The portal will display Deployment
    in progress. After a few minutes, the Azure Databricks service page displays,
    as can be seen in the following screenshot:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**固定到仪表板**，然后点击**创建**。门户将显示“部署进行中”。几分钟后，Azure Databricks服务页面将显示，如下面的截图所示：
- en: '![](img/0d80d523-e551-4514-a52e-238742d48224.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/0d80d523-e551-4514-a52e-238742d48224.png)'
- en: 'On the left-hand side, you can access fundamental Azure Databricks entities:
    workspace, clusters, tables, notebooks, jobs, and libraries.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧，您可以访问基本的Azure Databricks实体：工作区、集群、表、笔记本、作业和库。
- en: 'The workspace is the special root folder that stores your Azure Databricks
    assets, such as notebooks and libraries, and the data that you import:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 工作区是存储您的Azure Databricks资产（如笔记本和库）以及您导入的数据的特殊根文件夹：
- en: '![](img/bfb2c9d5-24f8-4f71-b84a-2462a473b7cf.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/bfb2c9d5-24f8-4f71-b84a-2462a473b7cf.png)'
- en: Creating a cluster
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建集群
- en: 'A cluster is a collection of Azure Databricks computation resources:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 集群是一组Azure Databricks计算资源：
- en: 'To create a cluster, click the **Clusters** button in the sidebar and click **Create
    Clus****ter**:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要创建集群，请在侧边栏中点击**集群**按钮，然后点击**创建集群**：
- en: '![](img/edeef29f-8e46-49b1-91ac-e7b8d0c296eb.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/edeef29f-8e46-49b1-91ac-e7b8d0c296eb.png)'
- en: On the **New Cluster** page, specify the cluster name.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**新建集群**页面，指定集群名称。
- en: Select 4.2 (includes Apache Spark 2.3.1, Scala 11) in the Databricks Runtime
    Version dropdown.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Databricks运行时版本下拉列表中选择4.2（包括Apache Spark 2.3.1，Scala 11）。
- en: Click Create Cluster.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击创建集群。
- en: Create a Databricks Notebook
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建Databricks笔记本
- en: 'A Notebook is a collection of cells that run computations on a Spark cluster.
    To create a Notebook in the Workspace, follow these steps:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本是一组在Spark集群上运行计算的单元格。要在工作区中创建笔记本，请按照以下步骤操作：
- en: In the sidebar, click the Workspace button.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在侧边栏中，点击工作区按钮。
- en: 'In the `Workspace` folder, select Create Notebook:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`工作区`文件夹中，选择创建笔记本：
- en: '![](img/bf8c4a5f-e3e5-48c3-9331-55a2e727e2b4.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/bf8c4a5f-e3e5-48c3-9331-55a2e727e2b4.png)'
- en: On the Create Notebook dialog, enter a name and select SQL in the **Language**
    dropdown.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在创建笔记本对话框中，输入一个名称并在**语言**下拉列表中选择SQL。
- en: Click Create. The Notebook opens with an empty cell at the top.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击创建。笔记本以顶部一个空单元格打开。
- en: Using SQL in Azure Databricks
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Azure Databricks中使用SQL
- en: 'In this section, you can run a SQL statement to create a table and work with
    data using SQL Statements:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您可以使用SQL语句创建表并使用SQL语句处理数据：
- en: 'Copy and paste this code snippet into the notebook cell to see a list of the
    Azure Databricks datasets:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将此代码片段复制并粘贴到笔记本单元格中，以查看Azure Databricks数据集的列表：
- en: '[PRE0]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The code appears as follows:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码如下所示：
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Press *Shift* + *Enter*. The notebook automatically attaches to the cluster
    you created in *Step 2*, creates the table, loads the data, and returns `OK`:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按 *Shift* + *Enter*。笔记本会自动连接到你在 *步骤 2* 中创建的集群，创建表格，加载数据，并返回 `OK`：
- en: '![](img/e484bd3c-d107-42d8-aeaa-246d91fc84a9.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e484bd3c-d107-42d8-aeaa-246d91fc84a9.png)'
- en: Next, you can run a SQL statement ...
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，你可以运行一个 SQL 语句 ...
- en: Displaying data
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 显示数据
- en: 'Display a chart of the average diamond price by color:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 显示按颜色划分的平均钻石价格图表：
- en: Click the Bar chart icon
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击柱状图图标
- en: Click Plot options
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击绘图选项
- en: Drag color into the Keys box
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将颜色拖入键值框
- en: Drag price into the Values box
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将价格拖入值值框
- en: 'In the Aggregation dropdown, select AVG:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在聚合下拉菜单中，选择 AVG：
- en: '![](img/497d3a75-697e-4db4-805a-e4bb75a1aad4.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/497d3a75-697e-4db4-805a-e4bb75a1aad4.png)'
- en: 'Click Apply to display the bar chart:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击应用以显示柱状图：
- en: '![](img/b2c3fccf-5845-49a3-98fb-cb1a87f91fdd.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b2c3fccf-5845-49a3-98fb-cb1a87f91fdd.png)'
- en: Machine Learning with HDInsight
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 HDInsight 进行机器学习
- en: Apache Spark is the largest open source process in data processing. Since its
    release, Apache Spark has seen rapid adoption by enterprises across a wide range
    of industries. Apache Spark is a fast, in-memory data processing engine, with
    elegant and expressive development APIs to allow data workers to efficiently execute
    streaming. In addition, Apache Spark facilitates ML and SQL workloads that require
    fast iterative access to datasets.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 是数据处理领域最大的开源项目。自其发布以来，Apache Spark 已被各行各业的企业迅速采用。Apache Spark 是一个快速、内存数据处理的引擎，具有优雅且表达性强的开发
    API，允许数据工作者高效地执行流处理。此外，Apache Spark 还简化了需要快速迭代访问数据集的 ML 和 SQL 工作负载。
- en: The focus of the current chapter is Apache Spark, which is an open source system
    for fast, large-scale data processing and ML.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 当前章节的重点是 Apache Spark，它是一个用于快速、大规模数据处理和机器学习的开源系统。
- en: The Data Science virtual machine provides you with a standalone (single node
    in-process) instance of the Apache Spark platform.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学虚拟机为您提供了 Apache Spark 平台的独立（单节点进程）实例。
- en: What is Spark?
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 Spark？
- en: Spark is designed as a high-performance, general-purpose computation engine
    for fast, large-scale big data processes. Spark works by distributing its workload
    across different nodes in a cluster. Spark scales out to process large volumes
    of data. Spark is aimed at big data batch processing, and is excellent for producing
    analytics using low-latency, high-performance data as a basis for operations.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 被设计为一个高性能、通用计算引擎，用于快速、大规模的大数据处理。Spark 通过在集群的不同节点间分配其工作负载来工作。Spark 可以扩展以处理大量数据。Spark
    面向大数据批处理，非常适合使用低延迟、高性能数据作为操作基础进行数据分析。
- en: Apache Spark consists of Spark Core and a set of libraries. Core is the distributed
    execution engine and the Java, Scala, and Python APIs offer a platform for distributed
    ETL application development. This allows developers to quickly achieve success
    by writing applications in Java, Scala, or Python.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 由 Spark Core 和一系列库组成。Core 是分布式执行引擎，Java、Scala 和 Python API 为分布式
    ETL 应用程序开发提供了一个平台。这使得开发者可以通过在 Java、Scala 或 Python 中编写应用程序来快速取得成功。
- en: Spark is built on the concept of a **Resilient Distributed Dataset** (**RDD**),
    which has been a core Spark concept for working with data since the inception
    of Spark. RDDs are similar to dataframes in R. RDDs are high-level abstractions
    of the data that provide data scientists with a schema to retrieve and work with
    data. RDDs are immutable collections representing datasets and have the inbuilt
    capability of reliability and failure recovery. RDDs create new RDDs upon any
    operation, such as transformation or action. They also store the lineage, which
    is used to recover from failures. For example, it's possible to separate the data
    out into the appropriate field and columns in that dataset, which means that data
    scientists can work with them more intuitively.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 是基于 **弹性分布式数据集**（**RDD**）的概念构建的，自从 Spark 诞生以来，RDD 就一直是 Spark 处理数据的核心概念。RDD
    与 R 中的数据框类似。RDD 是对数据进行高级抽象，为数据科学家提供了一种模式来检索和使用数据。RDD 是不可变的数据集集合，代表数据集，并具有内置的可靠性和故障恢复能力。RDD
    在任何操作（如转换或操作）上都会创建新的 RDD。它们还存储了 lineage，用于从故障中恢复。例如，可以将数据分离到数据集中的适当字段和列中，这意味着数据科学家可以更直观地与之交互。
- en: 'The Spark process involves a number of steps, which can involve more than one
    RDD. So, it is possible to have more than one RDD during processing. Here is an
    example, which shows how RDDs can be the source and the output of different processing
    steps:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 处理过程涉及多个步骤，可能涉及多个 RDD。因此，在处理过程中可能存在多个 RDD。以下是一个示例，展示了 RDD 如何成为不同处理步骤的源和输出：
- en: '![](img/5adcc927-b958-45d5-acb2-b8356d7b5f0b.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5adcc927-b958-45d5-acb2-b8356d7b5f0b.png)'
- en: Apache Spark allows for complex data engineering, and it comes with a built-in
    set of over 80 high-level operators. As well as longer processing, it is possible
    to interactively query data within the shell. In addition to the Map and Reduce
    operations, it supports SQL queries, Streaming Data, ML, and Graph Data Processing.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 允许进行复杂的数据工程，并自带一套超过 80 个高级操作符。除了更长的处理时间外，还可在 shell 中交互式查询数据。除了
    Map 和 Reduce 操作外，它还支持 SQL 查询、流数据、机器学习（ML）和图数据处理。
- en: Developers can use these capabilities standalone, or combine them to run in
    a single data pipeline use case.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者可以使用这些功能独立使用，或者将它们组合起来在单个数据管道用例中运行。
- en: Spark powers a stack of libraries, including SQL and DataFrames ([https://spark.apache.org/sql/](https://spark.apache.org/sql/)),
    MLlib ([https://spark.apache.org/mllib/](https://spark.apache.org/mllib/)) for
    ML, GraphX ([https://spark.apache.org/graphx/](https://spark.apache.org/graphx/)),
    and Spark Streaming ([https://spark.apache.org/streaming/](https://spark.apache.org/streaming/)).
    You can combine these libraries seamlessly in the same application.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 驱动一系列库，包括 SQL 和 DataFrame（[https://spark.apache.org/sql/](https://spark.apache.org/sql/)）、MLlib（[https://spark.apache.org/mllib/](https://spark.apache.org/mllib/)）用于机器学习、GraphX（[https://spark.apache.org/graphx/](https://spark.apache.org/graphx/)）和
    Spark Streaming（[https://spark.apache.org/streaming/](https://spark.apache.org/streaming/)）。您可以在同一应用程序中无缝组合这些库。
- en: In this chapter, RDDs will be the focus of the ML exercises. We will be focusing
    on hands-on exercises using Jupyter Notebooks. Jupyter Notebooks are available
    on the Data Science Virtual Machine, and they are installed by default as a service
    with the Azure HDInsight deployment of Spark.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重点关注机器学习练习中的 RDD。我们将专注于使用 Jupyter Notebooks 进行动手练习。Jupyter Notebooks
    可在数据科学虚拟机上使用，并且默认作为服务安装在 Azure HDInsight 部署的 Spark 上。
- en: HDInsight and Spark
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HDInsight 和 Spark
- en: Apache Spark is an open source parallel processing framework that supports in-memory
    processing to boost the performance of big data analytic applications. The Apache
    Spark cluster on HDInsight is compatible with Azure Storage (WASB), as well as
    Azure Data Lake Store.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 是一个开源的并行处理框架，支持内存处理以提升大数据分析应用的性能。HDInsight 上的 Apache Spark 集群与
    Azure 存储（WASB）以及 Azure 数据湖存储兼容。
- en: When the developer creates a Spark cluster on HDInsight, the Azure compute resources
    are already created with Spark installed and configured. It only takes about 10
    minutes to create a Spark cluster in HDInsight. The data to be processed is stored
    in Azure Storage or Azure Data Lake Storage.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当开发者在 HDInsight 上创建 Spark 集群时，Azure 计算资源已经创建，并安装了 Spark，并进行了配置。在 HDInsight 中创建
    Spark 集群只需大约 10 分钟。要处理的数据存储在 Azure 存储或 Azure 数据湖存储中。
- en: Apache Spark provides primitives for in-memory cluster computing, which means
    that it is the perfect partner for HDInsight. An Apache Spark job can load and
    cache data ...
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 提供了内存集群计算的原始功能，这意味着它是 HDInsight 的完美伴侣。Apache Spark 作业可以加载和缓存数据
    ...
- en: The YARN operation system in Apache Spark
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark 中的 YARN 操作系统
- en: YARN is one of the key features in the second-generation Hadoop 2 version of
    the Apache Software Foundation's open source distributed processing framework,
    and it is retained and progressed in Hadoop Version 3\. YARN is implemented on
    Azure HDInsight to facilitate large-scale, distributed operating systems for big
    data applications and predictive analytics.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: YARN 是 Apache 软件基金会开源分布式处理框架 Hadoop 第二代 2.0 版本中的关键特性之一，它在 Hadoop 版本 3 中得到保留并进一步发展。YARN
    在 Azure HDInsight 上实现，以促进大数据应用和预测分析的大规模分布式操作系统。
- en: YARN is efficient because it decouples MapReduce's resource management and scheduling
    capabilities from the data processing component. Since Apache Spark uses this
    methodology, it empowers Hadoop to support more varied processing approaches and
    a broader array of applications.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: YARN 的效率在于它将 MapReduce 的资源管理和调度能力与数据处理组件解耦。由于 Apache Spark 使用这种方法，它使 Hadoop
    能够支持更多样化的处理方法和更广泛的应用程序。
- en: How can we use Spark to conduct predictive analytics? ML focuses on taking data
    and applying a process to that data to produce a predicted output. There are many
    different types of ML algorithms that we can create and use with Spark. One of
    the most common methods is supervised ML, which works by taking in some data that
    consists of a vector of what we call features and a label. What do we mean by
    this?
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何使用 Spark 进行预测分析？机器学习（ML）侧重于获取数据并应用一个过程到这些数据上，以产生预测输出。我们可以创建和使用许多不同类型的 ML
    算法与 Spark 一起。最常见的方法之一是监督式 ML，它通过接收一些数据来实现，这些数据由我们称为特征的向量和一个标签组成。我们这是什么意思？
- en: A vector is a set of information that we use in order to make a prediction.
    A label is a characteristic that is used to make the prediction.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 向量是一组我们用来进行预测的信息。标签是用于预测的特征。
- en: 'We will take an example. Let''s say that we have a set of information about
    people, and we would like to predict something about this group of people: whether
    they are likely to become homeless or not. The characteristics of these people
    might include their age, education level, earnings, military service, and so on.
    The characteristics of the people would be called the features, and the thing
    that we would like to predict is known as the **label**.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将举一个例子。假设我们有一组关于人们的信息，我们想要预测这个群体的一些情况：他们是否有可能无家可归。这些人的特征可能包括他们的年龄、教育水平、收入、军事服务等等。这些人的特征被称为特征，而我们想要预测的东西被称为
    **标签**。
- en: In this case, the data scientist would take some data where they know that they
    have already become homeless, and therefore the label value would be known to
    the researchers at that point.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，数据科学家会取一些已知他们已经无家可归的数据，因此标签值在那个时刻对研究人员来说是已知的。
- en: Using Spark, we would then process the data and fit the data to the model to
    see how successful it is. The model would tell us what we need to see in the characteristics
    in order to see the likelihood of homelessness occurring for these individuals.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Spark，我们会对数据进行处理，并将数据拟合到模型中，以查看其成功程度。模型会告诉我们，为了看到这些个人发生无家可归的可能性，我们需要在特征中看到什么。
- en: The model is essentially a function that specifies what we expect to see in
    the vector features to see the outcome, or prediction.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 模型本质上是一个函数，它指定了我们期望在向量特征中看到什么，以看到结果或预测。
- en: The next step is to take previously unseen, new data that doesn't contain a
    known label to see how it fits the model. This dataset just has the features because
    the actual label, or outcome, isn't known. In this supervised learning example,
    data with a known label is used to train the model to predict data with the known
    label, and then the model is faced with data that does not have the known label.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是取之前未见过的、不包含已知标签的新数据，以查看其与模型拟合的情况。这个数据集只包含特征，因为实际的标签或结果并不为人所知。在这个监督学习示例中，使用已知标签的数据来训练模型预测具有已知标签的数据，然后模型面对的是没有已知标签的数据。
- en: In unsupervised learning, the label is not known. Unsupervised learning takes
    a similar approach, where the data scientist will ingest data and feed it, which
    simply has the vector of features, and no label is present. With this type of
    data science methodology, I may simply be looking at the similarities that are
    found in the vector features in order to see whether there are any clusters or
    commonalities in the data.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习中，标签是未知的。无监督学习采取类似的方法，数据科学家将摄取数据并输入它，这仅仅包含特征的向量，没有标签存在。使用这种类型的数据科学方法，我可能只是在查看向量特征中发现的相似性，以查看数据中是否存在任何聚类或共性。
- en: Working with data in a Spark environment
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Spark 环境中处理数据
- en: When data scientists work with data in an Apache Spark environment, they typically
    work with either RDDs or DataFrames. In our examples so far, the data may be stored
    in the RDD format, and it is fed into the model by building a predictive feed
    into the model.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据科学家在 Apache Spark 环境中处理数据时，他们通常使用 RDD 或 DataFrame。在我们之前的例子中，数据可能以 RDD 格式存储，并通过构建一个预测性输入到模型中来输入到模型中。
- en: In these exercises, the Spark library is called `spark.mllib`. The MLlib library
    is the original ML library that comes with Spark. The newer library is called
    **Spark ML**.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些练习中，Spark 库被称作 `spark.mllib`。MLlib 库是 Spark 最初附带的原生 ML 库。较新的库被称作 **Spark
    ML**。
- en: Using Jupyter Notebooks
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Jupyter Notebooks
- en: The Jupyter Notebook is an incredibly powerful tool for collaboratively developing
    and producing data science projects. It integrates code, comments, and code output
    into a single document that combines code, data visualizations, narrative text,
    mathematical equations, and other data science artefacts. Notebooks are increasingly
    popular in today's data science workflows because they encourage iterative and
    rapid development for data science teams. Jupyter project is the successor to
    the earlier IPython Notebook. It is possible to use many different programming
    languages within Jupyter Notebooks, but this chapter will focus on Apache Spark.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter Notebook 是一个极其强大的工具，用于协作开发和生产数据科学项目。它将代码、注释和代码输出整合到一个单一文档中，该文档结合了代码、数据可视化、叙述性文本、数学方程式和其他数据科学元素。由于它们鼓励数据科学团队进行迭代和快速开发，笔记本在当今的数据科学工作流程中越来越受欢迎。Jupyter
    项目是早期 IPython Notebook 的继承者。在 Jupyter Notebook 中，可以使用许多不同的编程语言，但本章将专注于 Apache
    Spark。
- en: Jupyter is free, open source, and browser-based. It can be used to create notebooks
    for working with your code in normal ways, such as writing and commenting code.
    One key feature of Jupyter Notebooks is that they are very useful for collaboration
    with other team members, thereby enabling productivity. The Jupyter Notebook supports
    a number of different engines, also known as kernels. The Jupyter Notebook can
    be used to run code on Python or Scala.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter 是免费的、开源的、基于浏览器的。它可以用来创建笔记本，以正常方式处理代码，例如编写和注释代码。Jupyter Notebook 的一个关键特性是它们非常适合与其他团队成员协作，从而提高生产力。Jupyter
    Notebook 支持多种不同的引擎，也称为内核。Jupyter Notebook 可以用来在 Python 或 Scala 上运行代码。
- en: In this walk-through, the Spark ML tutorial will be used in order to introduce
    the concepts of Spark and ML.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，将使用 Spark ML 教程来介绍 Spark 和 ML 的概念。
- en: Configuring the data science virtual machine
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置数据科学虚拟机
- en: 'If you are using the Ubuntu Linux DSVM edition, there is a requirement to do
    a one-time setup step to enable a local single node Hadoop HDFS and YARN instance.
    By default, Hadoop services are installed but disabled on the DSVM. In order to
    enable it, it is necessary to run the following commands as root the first time:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是 Ubuntu Linux DSVM 版本，需要执行一次设置步骤以启用本地单节点 Hadoop HDFS 和 YARN 实例。默认情况下，Hadoop
    服务已安装但未启用。为了启用它，第一次需要以 root 用户运行以下命令：
- en: '[PRE2]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can stop the Hadoop-related ...
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以停止与 Hadoop 相关的 ...
- en: Running Spark MLib commands in Jupyter
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Jupyter 中运行 Spark MLib 命令
- en: The default Jupyter lab will demonstrate features and capabilities of Spark's
    MLlib toolkit for ML problems. The walk-through uses a sample dataset, which holds
    data from real trips of NYC taxis. The data holds the NYC taxi trip and fare dataset
    to show MLlib's modeling features for binary classification and regression problems.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的 Jupyter lab 将展示 Spark MLlib 工具包在 ML 问题上的功能和能力。本教程使用一个示例数据集，该数据集包含来自纽约市出租车真实行程的数据。这些数据包含纽约市出租车行程和费用数据集，以展示
    MLlib 在二进制分类和回归问题上的建模功能。
- en: In this lab, many different Spark MLib functions will be used, including data
    ingestion, data exploration, data preparation (featurizing and transformation),
    modeling, prediction, model persistence, and model evaluation on an independent
    validation dataset. Data visualization will also be used in order to demonstrate
    the results.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在本实验中，将使用许多不同的 Spark MLib 函数，包括数据摄取、数据探索、数据准备（特征化和转换）、建模、预测、模型持久化和在独立验证数据集上的模型评估。还将使用数据可视化来展示结果。
- en: 'The lab will focus on two types of learning: classification offers the opportunity
    to try out supervised and unsupervised Learning. The first sample will use binary
    classification to predict whether a tip will be given. In the second sample, regression
    will be used to predict the level of tip given.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 本实验将重点关注两种学习类型：分类提供了尝试监督学习和无监督学习的机会。第一个示例将使用二进制分类来预测是否会被给小费。在第二个示例中，将使用回归来预测小费的金额。
- en: In Jupyter, the code is executed in the cell. The cell structure is a simple
    way of enabling the data scientist to query the RDD dataframe, and interactively
    display information in the Jupyter Notebook, including data visualization. It's
    a very flexible, intuitive, and powerful way to work with our data to use ML using
    Spark.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Jupyter 中，代码在单元格中执行。单元格结构是一种简单的方式，使数据科学家能够查询 RDD 数据框，并在 Jupyter Notebook 中交互式地显示信息，包括数据可视化。使用
    Spark 进行 ML 的工作方式非常灵活、直观且强大。
- en: Data ingestion
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据摄取
- en: The first activity is to set the appropriate directory paths.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 第一项活动是设置适当的目录路径。
- en: 'Set the location of training data:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置训练数据的位置：
- en: '[PRE3]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Set the model storage directory path. This is where models will be saved:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置模型存储目录路径。这是模型将被保存的位置：
- en: '[PRE4]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the Jupyter menu, put the cursor in cell and select the Run option from the
    menu. This will assign the training and test sets to the `taxi_train_file_loc`
    and `taxi_valid_file_loc `variables.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Jupyter菜单中，将光标放在单元格中，然后从菜单中选择“运行”选项。这将把训练集和测试集分配给`taxi_train_file_loc`和`taxi_valid_file_loc`变量。
- en: Next, the data will be set into a new dataframe, and it will be cleaned. Data
    ingestion was completed using the `spark.read.csv` function, which assigns the
    data to a new ...
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，数据将被设置到一个新的数据框中，并进行清理。数据摄入是通过`spark.read.csv`函数完成的，该函数将数据分配到一个新的...
- en: Data exploration
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据探索
- en: In the next step, it's important to explore the data. It's easy to visualize
    the data right in the Jupyter interface by plotting the target variables and features.
    The data is summarized using SQL. Then, the data is plotted using `matplotlib`.
    To plot the data, the dataframe will first have to be converted to a pandas dataframe.
    At that point, matplotlib can use it to generate plots.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，探索数据非常重要。在Jupyter界面中通过绘制目标变量和特征很容易可视化数据。数据使用SQL进行总结。然后，使用`matplotlib`绘制数据。为了绘制数据，数据框首先需要转换为pandas数据框。此时，matplotlib可以使用它来生成图表。
- en: Since Spark is designed to work with large big data datasets, if the Spark dataframe
    is large, a sample of the data can be used for data visualization purposes.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Spark旨在处理大型大数据集，如果Spark数据框很大，可以使用数据样本进行数据可视化。
- en: In the following example, 50% of the data was sampled prior to converting the
    data into the dataframe format, and then it was incorporated into a pandas dataframe.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，在将数据转换为数据框格式之前，对50%的数据进行了采样，然后将其合并到一个pandas数据框中。
- en: 'The code is provided in the following snippet:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 代码如下所示：
- en: '[PRE5]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This will produce a histogram of the results.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成结果的直方图。
- en: 'The relationship between the fare amount and the tip amount is shown in the
    following chart:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 车费金额和小费金额之间的关系在以下图表中显示：
- en: '![](img/74038367-c36c-42f6-b2e0-2a0e7bc1f714.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/74038367-c36c-42f6-b2e0-2a0e7bc1f714.png)'
- en: Feature engineering in Spark
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark中的特征工程
- en: In feature engineering, we can take care of numerous data engineering tasks,
    such as creating new features and grouping, transforming, and cleaning up data.
    The data can undergo further indexing and be enriched by additional classification,
    grouping, and the encoding of categorical features.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征工程中，我们可以处理许多数据工程任务，例如创建新特征和分组、转换和清理数据。数据可以进一步进行索引，并通过额外的分类、分组和分类特征的编码来丰富。
- en: 'In the following example code, we create a new feature by binning hours into
    traffic time buckets using Spark SQL:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例代码中，我们使用Spark SQL将小时分组到交通时间桶中创建一个新特征：
- en: '[PRE6]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Using Spark for prediction
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark进行预测
- en: 'In this part of the chapter, the exercise is to use the Spark sample code to
    create a logistic regression model, save the model, and evaluate the performance
    of  the model on a test dataset.  For modeling, the features and class labels
    are specified using the `RFormula` function. In this example, we will train the
    model using the pipeline formula and a logistic regression estimator. This can
    be seen from the following code snippet:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的这一部分，练习是使用Spark示例代码创建逻辑回归模型，保存模型，并在测试数据集上评估模型的性能。在建模时，使用`RFormula`函数指定特征和类标签。在这个例子中，我们将使用管道公式和逻辑回归估计器来训练模型。这可以从以下代码片段中看出：
- en: '[PRE7]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following code block sets up the training formula and assigns it to the
    `classFormula` variable, which can be seen from the following code:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码块设置训练公式并将其分配给`classFormula`变量，这可以从以下代码中看出：
- en: '[PRE8]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following code block trains the pipeline model:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码块训练了管道模型：
- en: '[PRE9]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following code block saves the model that we have created:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码块保存了我们创建的模型：
- en: '[PRE10]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following code block uses the model to predict results, using test data.
    The following code block helps to evaluate the model:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码块使用模型对测试数据进行预测，并使用以下代码块帮助评估模型：
- en: '[PRE11]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, we plot the ROC curve:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们绘制ROC曲线：
- en: '[PRE12]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The regression output is shown next:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个示例显示了回归输出：
- en: '![](img/a4bf5967-5356-47d0-ba1f-5abf0099000f.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a4bf5967-5356-47d0-ba1f-5abf0099000f.png)'
- en: The ROC Curve is the blue curve, which sits very high at the top-left corner.
    The ROC result shows that the model is performing extremely well. As a heuristic,
    the closer the blue line is to the top of the upper-left side of the chart, the
    better the result.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ROC曲线是蓝色曲线，位于图表的左上角非常高的位置。ROC结果表明模型表现极好。作为一个启发式方法，蓝色线越接近图表左上角顶部，结果就越好。
- en: In this section, the random forest model regression method will be used to predict
    how much of a tip will be given. In a standard classification tree, the data is
    split based on the homogeneity of the data. A decision tree is built top-down
    from a root node. The process involves partitioning data into subsets that contain
    instances that are homogeneous.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，将使用随机森林模型回归方法来预测将给出多少小费。在标准分类树中，数据基于数据的同质性进行分割。决策树从根节点自上而下构建。这个过程涉及将数据分割成包含同质实例的子集。
- en: In a regression tree, the target variable is a real-value number. In this case,
    the data is fitted against a regression model to the target variable using each
    of the independent variables. For each independent variable, the data is split
    at several split points. We calculate the **Sum of Squared Error **(**SSE**) at
    each split boundary between the predicted value and the actual values. The variable
    resulting in the lowest SSE is selected for the node. Then, this process is recursively
    continued until all of the data is covered.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归树中，目标变量是一个实数值。在这种情况下，数据使用每个独立变量拟合到目标变量。对于每个独立变量，数据在几个分割点处被分割。我们在预测值和实际值之间的每个分割边界计算**均方误差（SSE**）。导致最低SSE的变量被选为节点。然后，这个过程递归地继续，直到所有数据都被覆盖。
- en: 'The code is as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 代码如下：
- en: '[PRE13]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, we define the indexer for the categorical variables:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义分类变量的索引器：
- en: '[PRE14]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then, we set up the random forest estimator. The value is set to the `randForest`variable:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们设置了随机森林估计器。其值设置为`randForest`变量：
- en: '[PRE15]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The next step is to fit the model using the defined formula and the relevant
    transformations:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是使用定义的公式和相关转换来拟合模型：
- en: '[PRE16]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The next crucial step is to save the model:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个关键步骤是保存模型：
- en: '[PRE17]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then, we need to use the model to predict against the test data so that we
    can evaluate its success:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要使用模型对测试数据进行预测，以便我们可以评估其成功：
- en: '[PRE18]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'There''s no substitute for visualizing the data. In the next code block, the
    data is visualized using a scattergram format. The end result is shown after the
    code block:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可视化没有替代品。在下一个代码块中，数据使用散点图格式进行可视化。代码块后的结果是：
- en: '[PRE19]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The resulting chart can be found next:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图表可以在下面找到：
- en: '![](img/de0a0f54-8e02-4f8b-9bb1-0f00cf143aab.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/de0a0f54-8e02-4f8b-9bb1-0f00cf143aab.png)'
- en: The lower the **Root Mean Square Error** (**RMSE**) value, the better the absolute
    fit. The RMSE is calculated as the square root of the variance of the residuals.
    It specifies the absolute fit of the model to the data. In other words, it denotes
    how close the observed actual data points are to the model's predicted values.
    As the square root of a variance, RMSE can be conceived as the standard deviation
    of the unexplained variance. The RMSE has the useful property of being in the
    same units as the response variable, so it intuitively makes sense. Lower RMSE
    values indicate a better fit. RMSE is a good measure of how accurately the model
    predicts the response. RMSE is the most important criterion for fit in this case,
    since the main purpose of the model is prediction.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**均方根误差（RMSE**）值越低，绝对拟合越好。RMSE是残差方差的平方根。它指定了模型对数据的绝对拟合。换句话说，它表示观察到的实际数据点与模型预测值有多接近。作为方差的平方根，RMSE可以被视为未解释方差的均方差。RMSE具有有用的属性，即它与响应变量具有相同的单位，因此它直观地有意义。较低的RMSE值表示更好的拟合。RMSE是衡量模型预测响应准确性的良好指标。在本例中，RMSE是拟合最重要的标准，因为模型的主要目的是预测。'
- en: '**R-sqr** is intuitive. Its value ranges from zero to one, with zero indicating
    that the proposed model does not improve prediction over the mean model, and one
    indicating perfect prediction. Improvement in the regression model results in
    proportional increases in R-sqr.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**R-sqr**是直观的。其值范围从零到一，零表示所提出的模型没有比平均模型改善预测，一表示完美预测。回归模型的改进导致R-sqr成比例增加。'
- en: Whereas R-sqr is a relative measure of fit, RMSE is an absolute measure of fit,
    and that's why it's shown here.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 而R-sqr是拟合的相对度量，RMSE是拟合的绝对度量，这就是为什么它在这里被展示。
- en: Loading a pipeline model and evaluating the test data
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载管道模型并评估测试数据
- en: 'In this example, we will load a pipeline model and then evaluate the test data:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将加载一个管道模型，然后评估测试数据：
- en: '[PRE20]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In the next step, we define random forest models:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们定义随机森林模型：
- en: '[PRE21]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, we will define a modeling pipeline that includes formulas, feature transformations,
    and an estimator:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将定义一个包含公式、特征转换和估计器的建模管道：
- en: '[PRE22]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Setting up an HDInsight cluster with Spark
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Spark 配置 HDInsight 集群
- en: It's crucial to provision an HDInsight Spark cluster in order to start the work.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 配置 HDInsight Spark 集群以开始工作至关重要。
- en: Provisioning an HDInsight cluster
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置 HDInsight 集群
- en: If you already have a Spark HDInsight cluster running, you can skip this procedure.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经有一个正在运行的 Spark HDInsight 集群，您可以跳过此步骤。
- en: In a web browser, navigate to [http://portal.azure.com](http://portal.azure.com) and,
    if prompted, sign in using the Microsoft account that is associated with your
    Azure subscription.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在网页浏览器中，导航到 [http://portal.azure.com](http://portal.azure.com) 并，如果需要，使用与您的
    Azure 订阅关联的 Microsoft 账户登录。
- en: 'In the Microsoft Azure portal, in the Hub Menu, click New. Then, in the Data
    + Analytics section, select HDInsight and create a new HDInsight cluster with
    the following settings:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在微软 Azure 门户中，在中心菜单中点击新建。然后，在数据 + 分析部分，选择 HDInsight 并创建一个新的 HDInsight 集群，设置如下：
- en: '**Cluster Name**: Enter a unique name (and make a note of it!)'
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集群名称**：输入一个唯一的名称（并记下它！）'
- en: '**Subscription**: Select your Azure subscription'
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**订阅**：选择您的 Azure 订阅'
- en: '**Cluster Type**: Spark'
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集群类型**：Spark'
- en: '**Cluster Operating System**: Linux'
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集群操作系统**：Linux'
- en: '**HDInsight Version**: Choose the latest version of Spark'
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HDInsight 版本**：选择 Spark 的最新版本'
- en: '**Cluster Tier**: Standard'
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集群等级**：标准'
- en: '**Cluster Login Username**: Enter a username of your choice (and ...'
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集群登录用户名**：输入您选择的用户名（并 ...'
- en: Summary
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, you were introduced to some of the latest big data analytics
    technologies in Microsoft Azure. The chapter has focused on two main technologies:
    Azure HDInsight with Spark, and Azure Databricks.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您被介绍到了微软 Azure 中的一些最新大数据分析技术。本章主要关注两种主要技术：带有 Spark 的 Azure HDInsight 和
    Azure Databricks。
- en: In the chapter, we have looked at the different ways of modelling data, and
    we have covered useful tips to help you to understand what the models actually
    mean. Often, this is not the end of the data science process, because this may
    throw up new questions as you get new insights. So, it is a process rather than
    a race—but that's what makes it interesting!
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了不同的数据建模方式，并提供了有用的技巧来帮助您理解模型的实际含义。通常，这并不是数据科学过程的终点，因为随着您获得新的见解，这可能会提出新的问题。因此，这是一个过程而不是一场竞赛——这正是它的有趣之处！
- en: Further references
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步参考
- en: Review the Spark Machine Learning Programming Guide at [https://spark.apache.org/docs/latest/ml-guide.html](https://spark.apache.org/docs/latest/ml-guide.html)
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 [https://spark.apache.org/docs/latest/ml-guide.html](https://spark.apache.org/docs/latest/ml-guide.html)
    查阅 Spark 机器学习编程指南
- en: Documentation for Microsoft Azure HDInsight, including Spark Clusters is at [https://azure.microsoft.com/en-us/documentation/services/hdinsight](https://azure.microsoft.com/en-us/documentation/services/hdinsight)
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包括 Spark 集群在内的 Microsoft Azure HDInsight 文档在 [https://azure.microsoft.com/en-us/documentation/services/hdinsight](https://azure.microsoft.com/en-us/documentation/services/hdinsight)
- en: Documentation and getting started guidance for Programming with Scala is at [http://www.scala-lang.org/documentation/](http://www.scala-lang.org/documentation/)
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scala 编程的文档和入门指南在 [http://www.scala-lang.org/documentation/](http://www.scala-lang.org/documentation/)
- en: Documentation and getting started guidance for Programming with Python is at [https://www.python.org/doc/](https://www.python.org/doc/)
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 编程的文档和入门指南在 [https://www.python.org/doc/](https://www.python.org/doc/)
- en: You can view the Spark SQL and DataFrames Programming Guide at [https://spark.apache.org/docs/latest/sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html)
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以在 [https://spark.apache.org/docs/latest/sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html)
    查看 Spark SQL 和 DataFrame 编程指南
- en: Classification and Regression: [https://spark.apache.org/docs/latest/ml-classification-regression.html](https://spark.apache.org/docs/latest/ml-classification-regression.html)
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类和回归：[https://spark.apache.org/docs/latest/ml-classification-regression.html](https://spark.apache.org/docs/latest/ml-classification-regression.html)
- en: Pipelines: [https://spark.apache.org/docs/latest/ml-pipeline.html ...](https://spark.apache.org/docs/latest/ml-pipeline.html)
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道：[https://spark.apache.org/docs/latest/ml-pipeline.html ...](https://spark.apache.org/docs/latest/ml-pipeline.html)
