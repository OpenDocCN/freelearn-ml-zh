- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sentiment Analysis with BERT and Transfer Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Transformer architecture** is a **neural network model** that has gained
    significant popularity in **natural language processing** (**NLP**). It was first
    introduced in a paper by Vaswani et al. in 2017\. The main advantage of the Transformer
    is its ability to handle parallel processing, which makes it faster than RNNs.
    Another important advantage of the Transformer is its ability to handle long-range
    dependencies in sequences. This is achieved through the use of attention mechanisms,
    which allow the model to focus on specific parts of the input when generating
    the output.
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, the Transformer has been applied to a wide range of NLP tasks,
    including machine translation, question-answering, and summarization. Its success
    can be attributed to its simplicity, scalability, and effectiveness in capturing
    long-term dependencies. However, like any model, the Transformer also has some
    limitations, such as its high computational cost and reliance on large amounts
    of data for training. Despite these limitations, the Transformer remains a powerful
    tool for NLP researchers and practitioners. One of the factors that makes it possible
    is its ability to use and adapt already pre-trained networks for particular tasks
    with a much lower amount of computational resources and training data.
  prefs: []
  type: TYPE_NORMAL
- en: There are two main approaches to transfer learning, known as **fine-tuning**
    and **transfer learning**. Fine-tuning is a process that further adjusts a pre-trained
    model to better suit a specific task or dataset. It involves unfreezing some or
    all of the layers in the pre-trained model and training them on the new data.
    The process of transfer learning typically involves taking a pre-trained model,
    removing the final layers that are specific to the original task, and adding new
    layers or modifying the existing ones to fit the new task. The parameters of the
    model’s hidden layers are usually frozen during the training phase. This is one
    of the main differences from fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: A general overview of the Transformer architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A brief discussion of Transformer’s main components and how they work together
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An example of how to apply the transfer learning technique to build a new model
    for sentiment analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the technical requirements for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The PyTorch library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A modern C++ compiler with C++20 support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The CMake build system version >= 3.22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code files for this chapter can be found in the following GitHub repo:
    [https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-C-second-edition/tree/master/Chapter11/pytorch](https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-C-second-edition/tree/master/Chapter11/pytorch).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To configure the development environment, please follow the instructions described
    in the following document: [https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/blob/main/env_scripts/README.md](https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/blob/main/env_scripts/README.md).'
  prefs: []
  type: TYPE_NORMAL
- en: Also, you can explore the scripts in that folder to see configuration details.
  prefs: []
  type: TYPE_NORMAL
- en: 'To build the example project for this chapter, you can use the following script:
    [https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/blob/main/build_scripts/build_ch11.sh](https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/blob/main/build_scripts/build_ch11.sh).'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of the Transformer architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Transformer is a type of neural network architecture that was first introduced
    in the paper *Attention Is All You Need* by Google researchers. It has become
    widely used in NLP and other domains due to its ability to process long-range
    dependencies and attention mechanisms. The following scheme shows the general
    Transformer architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – The Transformer architecture](img/B19849_11_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – The Transformer architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main components of the Transformer architecture: the encoder
    and the decoder. The encoder processes the input sequence, while the decoder generates
    the output sequence. Common elements of these Transformer components are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Self-attention**: The model uses self-attention mechanisms to learn the relationships
    between different parts of an input sequence. This allows it to capture long-distance
    dependencies, which are important for understanding context in natural languages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cross-attention**: This is a type of attention mechanism that is used when
    working with two or more sequences. In this case, the elements from one sequence
    attend to elements from another sequence, allowing the model to learn the relationships
    between the two inputs. It’s used for communication between the encoder and decoder
    parts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-head attention**: Instead of using a single attention mechanism, the
    Transformer uses multiple attention heads, each with its own weights. This helps
    to model different aspects of the input sequence and improve the model’s representational
    power.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Positional encoding**: Since the Transformer does not use recurrent or convolutional
    layers, it needs a way to preserve positional information about the input sequences.
    Positional encoding is used to add this information to the inputs. This is done
    by adding sine and cosine functions of different frequencies to the embedding
    vectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feedforward neural networks**: Between the attention layers, there are fully
    connected feedforward neural networks. These networks help to transform the output
    of the attention layer into a more meaningful representation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Residual connections and normalization**: Like many deep learning models,
    the Transformer includes residual connections and batch normalization to improve
    training stability and convergence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s see in detail the main differences and tasks that are solved by the encoder
    and decoder parts.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **encoder** part of a Transformer is responsible for encoding the input
    data into a fixed-length vector representation, known as an embedding. This embedding
    captures the important features and information from the input and represents
    it in a more abstract form. The encoder typically consists of multiple layers
    of self-attention mechanisms and feedforward networks. Each layer of the encoder
    helps to refine and improve the representation of the input, capturing more complex
    patterns and dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of internal representations, the encoder produces embeddings that
    can be either contextual or positional. Contextual embeddings focus on capturing
    semantic and syntactic information from the text, while positional embeddings
    encode information about the order and position of words in the sequence. Both
    types of embeddings play a role in capturing the context and relationships between
    words in a sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: Contextual embeddings allow the model to understand the meaning of words based
    on their context, taking into account surrounding words and their relationships
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Positional embeddings, on the other hand, provide information about the position
    of each word in the sequence, helping the model understand the order and structure
    of the text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Together, contextual and positional embeddings in the encoder help the Transformer
    to better understand and represent the input data, enabling it to generate more
    accurate and meaningful outputs in downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Decoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **decoder** part of a Transformer takes the encoded representation as input
    and generates the final output. It consists of layers of attention and feedforward
    networks, similar to the encoder. However, it also includes additional layers
    that allow it to predict and generate outputs. The decoder predicts the next token
    in a sequence based on the encoded representation of the input sequence and its
    own previous predictions.
  prefs: []
  type: TYPE_NORMAL
- en: The output probabilities of the Transformer decoder represent the likelihood
    of each possible token being the next word in the sequence. These probabilities
    are calculated using a softmax function, which assigns a probability value to
    each token based on its relevance to the context.
  prefs: []
  type: TYPE_NORMAL
- en: During training, the decoder uses the encoded embedding to generate predictions
    and compare them with the true output. The difference between the predicted and
    true outputs is used to update the parameters of the decoder and improve its performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Output sampling is an essential part of the decoder process. It involves selecting
    the next token based on output probabilities. There are many methods for sampling
    the output. The following list shows some of the popular ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Greedy search**: This is the simplest method of sampling, where the most
    probable token at each step is selected based on the softmax probability distribution
    of the token values. While it is fast and easy to implement, it may not always
    find the optimal solution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Top-k sampling**: Top-k sampling selects the top *k* tokens with the highest
    probabilities from the softmax distribution at each step, instead of selecting
    the most probable token. This method can help to diversify the samples and prevent
    the model from getting stuck in a local optimum.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nucleus sampling**: Nucleus sampling, also known as top-p sampling, is a
    variant of top-k sampling that selects a subset of tokens from the top of the
    softmax distribution based on their probabilities. By selecting multiple tokens
    within a range of probabilities, nucleus sampling can improve the diversity and
    coverage of the output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You now have an understanding of how the main Transformer components work and
    what elements are used within it. But this leaves the topic of how inputs are
    preprocessed before the decoder takes them to process uncovered. Let’s see how
    input text can be converted into Transformer input.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tokenization is the process of breaking down a sequence of text into smaller
    units, called tokens. These tokens can be individual words, subwords, or even
    characters, depending on the specific task and model architecture. For example,
    in the sentence *I love eating pizza*, the tokens would be *I*, *love*, *eating*,
    and *pizza*.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many tokenization methods used in Transformer models. The most used
    are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Word tokenization**: This method splits the text into individual words. It
    is the most common approach and is suitable for tasks such as translation and
    text classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Subword tokenization**: In this method, the text is split into smaller units,
    called **subwords**. This can improve performance on tasks where words are often
    misspelled or truncated, such as machine translation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Character tokenization**: This method breaks down the text into individual
    characters. It can be useful for tasks that require fine-grained analysis, such
    as sentiment analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of tokenization method depends on the characteristics of the dataset
    and the requirements of the task.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following subsection, we will use the BERT model, which uses `[CLS]`
    (classify) and `[SEP]` (end). These tokens serve specific purposes in the model’s
    architecture. Here’s a step-by-step explanation of the WordPiece tokenization
    algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Initial vocabulary**: Start with a small vocabulary that includes special
    tokens used by the model and the initial alphabet. The initial alphabet contains
    all the characters present at the beginning of a word and the characters inside
    a word preceded by the WordPiece prefix.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`##` for BERT) to each character within a word, resulting in a split such as
    `w ##o ##r ##d`. This creates subwords from each character in the original word.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`score = (freq_of_pair)/(freq_of_first_element×freq_of_second_element)`. This
    prioritizes the merging of pairs where the individual parts are less frequent
    in the vocabulary.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Merging pairs**: The algorithm merges pairs with high scores, which means
    that the algorithm merges pairs that occur less frequently into individual elements.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Iterative merging**: The process repeats until a desired number of merge
    operations has been performed or a predefined threshold is reached. At this point,
    the final vocabulary is created.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With the vocabulary, we can tokenize any word from the input in a similar way
    as we did previously, during the vocabulary construction. So, at first, we search
    for the whole word in the vocabulary. If we don’t find it, we remove a character
    from the beginning preceding a subword with the `##` prefix and search again.
    This process is continued until we find a subword. If we don’t find any tokens
    for a word in a vocabulary, we usually skip this word.
  prefs: []
  type: TYPE_NORMAL
- en: The vocabulary is typically represented as a dictionary or lookup table that
    maps each word to a unique integer index. The vocabulary size is an important
    factor in the performance of a Transformer, as it determines the complexity of
    the model and its ability to handle different types of text.
  prefs: []
  type: TYPE_NORMAL
- en: Word embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even though we use tokenization to convert words into numbers, it doesn't provide
    any semantic meaning to the neural network. To be able to represent semantic proximity,
    we can use embedding. Embedding is where we map an arbitrary entity to a specific
    vector, for example, a node in a graph, an object in a picture, or the definition
    of a word. A set of embedding vectors can be treated as vector meaning space.
  prefs: []
  type: TYPE_NORMAL
- en: There are many approaches to creating embeddings for words. For example, the
    ones most known in classical NLP are Word2Vec and GloVe, which are based on statistical
    analysis and are actually standalone models.
  prefs: []
  type: TYPE_NORMAL
- en: For the Transformer, a different approach was proposed. With tokens as input,
    we pass them into the MLP layer, which gives embedding vectors for each of the
    tokens as output. This layer is trained along with the rest of the model and is
    an internal model component. In this way, we can have embeddings that are more
    fine-tuned for specific training data and particular tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Using the encoder and decoder parts separately
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The original Transformer architecture has both parts, the encoder and the decoder.
    But recently, it has been found that these parts can be used separately to solve
    different tasks. The two most well-known base architectures based on them are
    BERT and GPT.
  prefs: []
  type: TYPE_NORMAL
- en: '**BERT** stands for **bidirectional encoder representations from transformers**.
    It’s a state-of-the-art NLP model that uses a bidirectional approach to understand
    the context of words in a sentence. Unlike traditional models that only consider
    words in one direction, BERT can look at words both before and after a given word
    to better understand their meaning. It is based only on the encoder Transformer
    part. This makes it particularly useful for tasks that require understanding the
    context, such as semantic similarity and text classification. Also, it is designed
    to understand context in both directions, which means it can consider both the
    previous and following words in a sentence.'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, **GPT**, which stands for **generative pre-trained transformer**,
    is a generative model based only on the decoder Transformer part. It is designed
    to generate human-like text by predicting the next word in a sequence.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will develop a sentiment analysis model with the PyTorch
    library using BERT as the base model.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis example with BERT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to build a machine learning model that can detect
    review sentiment (detect whether a review is positive or negative) using PyTorch.
    As a training set, we are going to use the Large Movie Review Dataset, which contains
    a set of 25,000 movie reviews for training and 25,000 for testing, both of which
    are highly polarized.
  prefs: []
  type: TYPE_NORMAL
- en: As we said before, we will use an already pre-trained BERT model. BERT was chosen
    due to its ability to understand context and relationships between words, making
    it particularly effective for tasks such as question-answering, sentiment analysis,
    and text classification. Let’s remember that transfer learning is a machine learning
    approach that involves transferring knowledge from a pre-trained model to a new
    or different problem domain. It is used when there is a lack of labeled data for
    the specific task at hand, or when training a model from scratch would be too
    computationally expensive.
  prefs: []
  type: TYPE_NORMAL
- en: 'Applying the transfer learning algorithm involves the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Selection of a pre-trained model**: The model should be chosen based on relevance
    to the task.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Adding a new task-specific head**: This could be, for example, a combination
    of fully connected linear layers with a final softmax for classification.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Freezing the pre-trained parameters**: Freezing the parameters allows the
    model to retain its pre-learned knowledge.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Training on the new dataset**: The model is trained using a combination of
    the pre-trained weights and the new data, allowing it to learn specific features
    and patterns relevant to the new domain in new layers.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We know that BERT-like models are used to extract some semantic knowledge from
    input data; in our case, it will be text. Also, BERT-like models usually represent
    extracted knowledge in the form of embedding vectors. These vectors can be used
    to train a new model head, for example, for a classification task. We will follow
    the previously described steps.
  prefs: []
  type: TYPE_NORMAL
- en: Exporting the model and vocabulary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The traditional method of using some pre-trained model in C++ using the PyTorch
    library is to load this model as a TorchScript. A common approach to getting this
    script is to trace a model available in Python and save it. There are a lot of
    pre-trained models on the [https://huggingface.co/](https://huggingface.co/) site.
    Also, these models are available with the Python API. So, let’s write a simple
    Python program to export the base BERT model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows how to import the required Python modules
    and load a pre-trained model for tracing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We imported the `BertModel` and `BertTokenizer` classes from the `transformers`
    module, which is the library from Hugging Face that allows us to work with different
    Transformer-based models. We used the `bert-base-cased` model, which is the raw
    BERT model that was trained to understand general language semantics on large
    corpus of texts, and we also loaded the tokenizer module specialized for BERT
    models—BertTokenizer. Notice also that we used the `torchscript=True` parameter
    to be able to trace and save the model. This parameter tells the library to use
    operators and modules suitable for Torch JIT tracing.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we have the loaded tokenizer object, we can tokenize some sample text
    for tracing as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we defined the maximum number of tokens that can be generated, which is
    `128`. The BERT model we loaded can process a maximum of 512 tokens at once, so
    you should configure this number for your task, for example, you can use a smaller
    number of tokens to satisfy performance restrictions on embedded devices. Also,
    we told the tokenizer to truncate longer sequences and pad shorter sequences to
    `max_length`. We also made the tokenizer return PyTorch tensors by specifying
    `return_tensors="pt"`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We used two values returned from the tokenizer: `input_ids`, which is the token
    values, and `attention_mask`, which is a binary mask filled with `1` for real
    tokens and `0` for padded tokens that shouldn’t be processed.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we have tokens and masks, we can export the model as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We switched the model into evaluation mode because it will be used for tracing,
    not for training. Then, we used the `torch.jit.trace` function to trace the model
    on sample input that is the tuple of our generated tokens and attention mask.
    We used the `save` method of the traced module to save the model script into a
    file.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As well as the model, we also have to export the tokenizer vocabulary, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we just went through all the available tokens in the tokenizer object
    and saved them as `[value - id]` pairs listed in the text file.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the tokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can load the saved scripted model directly with the PyTorch C++ API, but
    we can’t do the same with the tokenizer. Also, there is no tokenizer implementation
    in the PyTorch C++ API. So, we have to implement the tokenizer by ourselves:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest tokenizer can actually be implemented easily. It can have the
    following definition for the header file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We defined the `Tokenizer` class with a constructor that takes the name of the
    vocabulary file and the maximum length of the token sequence to produce. We also
    defined a single method, `tokenize`, that takes input text as an argument.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The constructor can be implemented as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We simply opened the given text file and read it line by line. We split each
    line into two components, which are the token string value and the corresponding
    ID. These components are delimited with the space character. Also, we converted
    `id` from a `string` to an `integer` value. All parsed token ID pairs were saved
    into the `std::unordered_map` container to be able to search for the token ID
    effectively.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The `tokenize` method implementation is slightly complex. We define it as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we got the special token ID values from the loaded vocabulary. These token
    IDs are needed by the BERT model to correctly process inputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The PAD token is used to mark empty tokens in the case when our input text
    is too short. It can be done as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As in a Python program, we created two vectors, one for token IDs and one for
    the attention mask. We used `pad_token_id` as the default value for token IDs
    and filled the attention mask with zeros. Then, we put `start_token_id` as the
    first element and put the corresponding value in the attention mask.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Having defined output containers, we now define the intermediate objects for
    input text processing, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We moved our input text string into a `stringstream` object to be able to read
    it word by word. We also defined the corresponding word string object.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The top-level processing cycle can be defined as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we used the `getline` function to split an input string stream into words
    using the space character as a delimiter. This is a simple approach to splitting
    input text. Usually, tokenizers use more complex strategies for splitting. But
    it’s enough for our task and our dataset. Also, we added the check that if we
    reach the maximum sequence length, we stop text processing, so we truncate it.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then, we have to identify the longest possible prefix in the first word that
    exists in the vocabulary; it can be a whole word. The implementation starts as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we defined the `start` variable to track the beginning of the prefix of
    a word, it's initialized with 0 which is the current word's beginning position.
    We defined the `end` variable to track the end of the prefix, it's initialized
    with the current word length as the word's last position. Initially, they pointed
    to the beginning and the end of the word. Then, in the internal loop, we continuously
    reduced the size of the prefix by decrementing the `end` variable. After each
    decrement, if we didn’t find the prefix in the vocabulary, we repeated the process
    until the end of the word. Also, we made it so that after a successful token search,
    we swapped the `start` and `end` variables to split the word and continue prefix
    searches for the remaining part of the word. This was done because a word can
    consist of several tokens. Also, in this code, we made a check for the maximum
    token sequence length to stop the whole token search process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following step is where we carry out a prefix search in the vocabulary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we extracted the prefix from the original word by using the `substr` function.
    If the `start` variable is not `0`, we are working with the internal part of the
    word, so the addition of the `##` special prefix is carried out. We used the `find`
    method of the unordered map container to find a token (prefix). If the search
    was successful, we placed the token ID in the next position of the `input_ids`
    container, made a corresponding mark in `attention_mask`, increased the `input_id`
    index to move the current sequence position, and broke the loop to start working
    with the following word part.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After implementing the code for filling in the input IDs and attention mask,
    we put them into PyTorch tensor objects and return the output as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Before converting input IDs and masks into tensors, we finalized the token ID
    sequence with the `end_token_id` value. Then, we used the `torch.tensor` function
    to create tensor objects. This function can take different inputs, and one of
    them is just `std::vector` with numeric values. Also, we used the `unsqueeze`
    function to add the batch dimension to tensors. We also returned the final tensors
    as a standard pair.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the following subsection, we will implement a dataset loader class using
    the implemented tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the dataset loader
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have to develop parser and data loader classes to move the dataset to memory
    in a format suitable for use with PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the parser. The dataset we have is organized as follows: there
    are two folders for the training and testing sets, and each of these folders contains
    two child folders, named `pos` and `neg`, which is where the positive and negative
    review files are placed, respectively. Each file in the dataset contains exactly
    one review, and its sentiment is determined by the folder it’s placed in. In the
    following code sample, we will define the interface for the reader class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We defined two vectors, `pos_samples_` and `neg_samples_`, which contain the
    reviews that were read from the corresponding folders.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We will assume that the object of this class should be initialized with the
    path to the root folder where one of the datasets is placed (the training set
    or testing set). We can initialize this in the following way:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The most important parts of this class are the `constructor` and the `read_directory`
    methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The constructor is the main point wherein we fill the containers, `pos_samples_`
    and `neg_samples_`, with actual reviews from the `pos` and `neg` folders:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `read_directory` method implements the logic for iterating files in the
    given directory and reads them as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We used the standard library directory iterator class, `fs::directory_iterator`,
    to get every file in the folder. The object of this class returns the object of
    the `fs::directory_entry` class, and this object can be used to determine whether
    this is a regular file with the `is_regular_file` method. We got the file path
    of this entry with the `path` method. We read the whole file to one string object
    using the `rdbuf` method of the `std::ifstream` type object.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that the `ImdbReader` class has been implemented, we can go further and
    start the dataset implementation. Our dataset class should return a pair of items:
    one representing the tokenized text and another the sentiment value. Also, we
    need to develop a custom function to convert the vector of tensors in a batch
    into one single tensor. This function is required if we want to make PyTorch compatible
    with our custom training data.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s define the `ImdbSample` type for a custom training data sample. We will
    use this with the `torch::data::Dataset` type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`ImdbData` represents the training data and has two tensors for a token sequence
    and an attention mask. `ImdbSample` represents the whole sample with a target
    value. A tensor contains `1` or `0` for positive or negative sentiment, respectively.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following code snippet shows the `ImdbDataset` class’ declaration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We inherited our dataset class from the `torch::data::Dataset` class so that
    we can use it for data loader initialization. The PyTorch data loader object is
    responsible for sampling random training objects and making batches from them.
    The objects of our `ImdbDataset` class should be initialized with the root dataset
    path for the `ImdbReader` and `Tokenizer` objects. The constructor implementation
    is trivial; we just initialize the reader and store a pointer to the tokenizer.
    Notice that we used the pointer to the tokenizer to share it among the train and
    test datasets later. We overrode two methods from the `torch::data::Dataset` class:
    the `get` and `size` methods.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following code shows how we implement the `size` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `size` method returns the number of reviews in the `ImdbReader` object.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The `get` method has a more complicated implementation than the previous method,
    as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: First, we got the review text and sentiment value from the given index (the
    function argument value). In the `size` method, we returned the total number of
    positive and negative reviews, so if the input index is greater than the number
    of positive reviews, then this index points to a negative one. Then, we subtracted
    the number of positive reviews from it.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After we got the correct index, we also got the corresponding text review, assigned
    its address to the `review` pointer, and initialized the `target` tensor. The
    `torch::tensor` function was used to initialize the `target` tensor. This function
    takes an arbitrary numeric value and tensor options such as the required type.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: With the review text, we just used the tokenizer object to create two tensors
    with token IDs and a sentiment mask. They are packed in the `tokenizer_out` pair
    object. We returned the pair of training tensors and one target tensor.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To be able to effectively use batched training, we create a special class so
    that PyTorch is able to convert our non-standard sample type into a batch tensor.
    In the simplest case, we will get the `std::vector` object of training samples
    instead of a single batch tensor. It was done as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We inherited our class from the special PyTorch `torch::data::transforms::Collation`
    type and specialized it with the template parameter of our `ImdbExample` class.
    Having such a class, we overrode the virtual `apply_batch` function with an implementation
    that takes as input the `std::vector` object containing `ImdbExample` objects
    and returns the single `ImdbExample` object. It means that we merged all input
    IDs, attention masks, and target tensors in three separate tensors. This was done
    by creating three separate containers for input IDs, attention masks, and target
    tensors. They were filled in a simple loop over the input samples. Then, we just
    used the `torch::stack` function to merge (stack) these containers in single tensors.
    This class will be used in the DataLoader type object construction later.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The next step is to create a `model` class. We already have the exported model
    that we are going to use as a pre-trained part. We create a simple classification
    head with two linear fully connected layers and one dropout layer for regularization:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The header file for this class will look as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We included the `torch\script.h` header file to use the `torch::jit::script::Module`
    class. The instance of this class will be used as a representation of the previously
    exported BERT model. See the `bert_` member variable. Also, we defined member
    variables for the linear and dropout layers as instances of the `torch::jit::script::Module`
    class. We inherited our `ModelImpl` class from the `torch::nn::Module` class to
    integrate it into the PyTorch auto-gradient system.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The constructor implementation looks as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We used `torch::jit::load` to load the model that we exported from Python. This
    function takes a single argument—the model filename. Also, we initialized the
    dropout and linear layers and registered them in the parent `torch::nn::Module`
    object. The `fc1_` linear layer is the input one; it takes the 768-dimensional
    output of the BERT model. The `fc2_` linear layer is the output layer. It processes
    the internal 512-dimensional state into two classes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The main model functionality is implemented in the `forward` function as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This function takes the input token IDs plus the corresponding attention mask
    and returns a two-dimensional tensor with classification results. We interpret
    the sentiment analysis as a classification task. The `forward` implementation
    has two parts. One part is where we preprocess inputs with the loaded BERT model;
    this preprocessing is just an inference with pretrained BERT model backbone. The
    second part is where we pass the BERT output through our classification head.
    This is the trainable part. To use the BERT model, that is, the `torch::jit:script::Module`
    object, we packed inputs into the `std::vector` container of the `torch::jit::Ivalue`
    objects. The conversion from `torch::Tensor` was done automatically.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we used standard for PyTorch `forward` function for inference. This function
    returns the `torch::jit` tuple object; the return type actually depends on the
    initial model that was traced. So, to get the PyTorch tensor from the `torch::jit`
    value object, we explicitly used the `toTuple` method to say how to interpret
    the output result. Then, we accessed the second tuple element by using the `elements`
    method, which provides the indexing operator for tuple elements. Finally, to get
    the tensor, we used the `toTensor` method of the `jit::Ivalue` object, which is
    a tuple element.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The BERT model we used returns two tensors. The first one represents the embedding
    values for input tokens, and the second one is the pooled output. Pooled output
    is the embeddings of the `[CLS]` token, for the input text. The linear layer weights
    that produced this output were trained from the next sentence prediction (classification)
    objective during BERT pre-training. So, these are the ideal values to use in the
    following text classification tasks. That is why we took the second element of
    the tuple returned by the BERT model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The second part of the `forward` function is also simple. We passed the BERT
    output to the `fc1_` linear layer followed by the `relu` activation function.
    After this operation, we got the `512` internal hidden state. Then, this state
    was processed by the `dropout_` module to introduce some regularization into the
    model. The final stage was the use of the `fc2_` output linear module, which returns
    a two-dimensional vector followed by the `softmax` function. The `softmax` function
    converts logits into probability values with the range `[0,1]` because raw logits
    from a linear layer can have arbitrary values and need to convert them into target
    values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, we have described all the components required for the training process.
    Let’s see how the model training can be implemented.
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first step in training is creating the dataset object, which can be done
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We created the `tokenizer` and `train_dataset` objects just by passing the
    paths to the corresponding files. The test dataset can be created in the same
    way with the same `tokenizer` object. Now that we have the dataset, we create
    a data loader as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We specified the size of the batch and used the `make_data_loader` function
    to create the data loader object. This object uses the dataset to effectively
    load and organize training samples in batches. We used the transformation `map`
    function of the `train_dataset` object with an instance of our collation `Stack`
    class to allow PyTorch to merge our training samples into tensor batches. Also,
    we specified some data loader options, that is, the batch size and the number
    of worker threads used to load and preprocess data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We create the model object as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: We used the `torch::cuda::is_available` function to determine whether the CUDA
    device is available in a system and initialized the `device` variable correspondingly.
    Using a CUDA device can significantly improve the training and inference of a
    model. The model was created with the constructor that takes the path to the exported
    BERT model. After model object initialization, we moved this object to the particular
    device.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last component required for training is an optimizer, which we create as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We used the `AdamW` optimizer, which is an improved version of the popular Adam
    optimizer. To construct the optimizer object, we passed the model parameters and
    the learning rate option as constructor arguments.
  prefs: []
  type: TYPE_NORMAL
- en: 'The training cycle can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: There are two nested loops. One is over the epochs and there is another, an
    internal one, that is over the batches. At the beginning of each internal loop
    is when a new epoch starts. We switched the model into training mode. This switch
    can be done once, but usually, you have some testing code that switches the model
    into evaluation mode, so this switch returns the model to the required state.
    Here, we omitted the test code for simplicity. It looks pretty similar to the
    training one, the only difference being disabling gradient calculations. In the
    internal loop, we used simple range-based `for` loop C++ syntax to iterate over
    batches. For every batch, at first, we cleared the gradient values by calling
    the `zero_grad` function for the optimizer object. Then, we decoupled the batch
    into separate tensor objects. Also, we moved these tensors to a GPU device if
    one is available. This was done with the `.to(device)` calls. We removed an additional
    dimension from the model input tensors with the `squeeze` method. This dimension
    appeared during an automatic batch creation.
  prefs: []
  type: TYPE_NORMAL
- en: Once all the tensors were prepared, we made a prediction with our model that
    gave us the `output` tensor. This output was used in the `torch::cross_entropy_loss`
    loss function, which is usually used for multi-class classification. It takes
    a tensor with probabilities for every class and the one-hot-encoded labels tensor.
    Then, we used the `backward` method of the `loss` tensor to calculate gradients.
    Also, we clipped gradients with the `clip_grad_norm_` function by setting a top
    value limit, to prevent them from exploding. Once the gradients were ready, we
    used the optimizer `step` function to update model weights according to the optimizer
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: This architecture, with the settings we used, can result in more than 80% accuracy
    in the sentiment analysis of movie reviews in 500 training epochs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced the Transformer architecture, a powerful model used
    in NLP and other fields of machine learning. We discussed the key components of
    the Transformer architecture, which include tokenization, embeddings, positional
    encoding, encoder, decoder, attention mechanisms, multi-head attention, cross-attention,
    residual connections, normalization layers, feedforward layers, and sampling techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in the last part of this chapter, we developed an application so that
    we could perform a sentiment analysis of movie reviews. We applied the transfer
    learning technique to use the features learned by the pre-trained model in a new
    model designed for our specific task. We used the BERT model to produce a embedding
    representation of input texts and attached a linear layer classification head
    to classify review sentiments. We implemented a simple version of the tokenizer
    and dataset loader. We also developed the full training cycle of our classification
    head.
  prefs: []
  type: TYPE_NORMAL
- en: We used transfer learning instead of fine-tuning to utilize less computational
    resources because the fine-tuning technique usually involves re-training a full
    pre-trained model on a new dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss how to save and load model parameters.
    We will also look at the different APIs that exist in machine learning libraries
    for this purpose. Saving and loading model parameters can be quite an important
    part of the training process because it allows us to stop and restore training
    at an arbitrary moment. Also, saved model parameters can be used for evaluation
    purposes after the model has been trained.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PyTorch documentation: [https://pytorch.org/cppdocs/](https://pytorch.org/cppdocs/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hugging Face BERT model documentation: [https://huggingface.co/docs/transformers/model_doc/bert](https://huggingface.co/docs/transformers/model_doc/bert)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An illustrated Transformer explanation: [https://jalammar.github.io/illustrated-transformer](https://jalammar.github.io/illustrated-transformer)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Attention Is All You Need*, Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
    Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A list of already pre-trained BERT-like models for sentiment analysis: [https://huggingface.co/models?other=sentiment-analysis](https://huggingface.co/models?other=sentiment-analysis)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 4: Production and Deployment Challenges'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The crucial feature of C++ is the ability of the program to be compiled and
    run on a variety of hardware platforms. You can train your complex **machine learning**
    (**ML**) model on the fastest GPU in the data center and deploy it to tiny mobile
    devices with limited resources. This part will show you how to use C++ APIs of
    various ML frameworks to save and load trained models, and how to track and visualize
    a training process, which is crucial for ML practitioners to be able to control
    and check a model’s training performance. Also, we will learn how to build programs
    that use ML models on Android devices; in particular, we will create an object
    detection system that uses a device’s camera.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part comprises the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 12*](B19849_12.xhtml#_idTextAnchor660), *Exporting and Importing
    Models*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 13*](B19849_13.xhtml#_idTextAnchor689), *Tracking and Visualizing
    ML Experiments*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 14*](B19849_14.xhtml#_idTextAnchor702), *Deploying Models on a Mobile
    Platform*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
