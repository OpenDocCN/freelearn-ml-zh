- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Sentiment Analysis with BERT and Transfer Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 BERT 和迁移学习进行情感分析
- en: The **Transformer architecture** is a **neural network model** that has gained
    significant popularity in **natural language processing** (**NLP**). It was first
    introduced in a paper by Vaswani et al. in 2017\. The main advantage of the Transformer
    is its ability to handle parallel processing, which makes it faster than RNNs.
    Another important advantage of the Transformer is its ability to handle long-range
    dependencies in sequences. This is achieved through the use of attention mechanisms,
    which allow the model to focus on specific parts of the input when generating
    the output.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**Transformer 架构**是一种**神经网络模型**，在**自然语言处理**（**NLP**）领域获得了显著的流行。它首次在 Vaswani
    等人于 2017 年发表的一篇论文中提出。Transformer 的主要优势是其处理并行处理的能力，这使得它比 RNNs 更快。Transformer 的另一个重要优势是它处理序列中长距离依赖的能力。这是通过使用注意力机制实现的，允许模型在生成输出时关注输入的特定部分。'
- en: In recent years, the Transformer has been applied to a wide range of NLP tasks,
    including machine translation, question-answering, and summarization. Its success
    can be attributed to its simplicity, scalability, and effectiveness in capturing
    long-term dependencies. However, like any model, the Transformer also has some
    limitations, such as its high computational cost and reliance on large amounts
    of data for training. Despite these limitations, the Transformer remains a powerful
    tool for NLP researchers and practitioners. One of the factors that makes it possible
    is its ability to use and adapt already pre-trained networks for particular tasks
    with a much lower amount of computational resources and training data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，Transformer 已被应用于广泛的 NLP 任务，包括机器翻译、问答和摘要。其成功可以归因于其简单性、可扩展性和在捕捉长期依赖方面的有效性。然而，像任何模型一样，Transformer
    也有一些局限性，例如其高计算成本和对大量训练数据的依赖。尽管存在这些局限性，Transformer 仍然是 NLP 研究人员和从业者的一项强大工具。使其成为可能的一个因素是其能够使用和适应已经预训练的网络，以特定任务以更低的计算资源和训练数据量。
- en: There are two main approaches to transfer learning, known as **fine-tuning**
    and **transfer learning**. Fine-tuning is a process that further adjusts a pre-trained
    model to better suit a specific task or dataset. It involves unfreezing some or
    all of the layers in the pre-trained model and training them on the new data.
    The process of transfer learning typically involves taking a pre-trained model,
    removing the final layers that are specific to the original task, and adding new
    layers or modifying the existing ones to fit the new task. The parameters of the
    model’s hidden layers are usually frozen during the training phase. This is one
    of the main differences from fine-tuning.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 转移学习主要有两种方法，称为**微调**和**迁移学习**。微调是一个进一步调整预训练模型以更好地适应特定任务或数据集的过程。这涉及到解冻预训练模型中的某些或所有层，并在新数据上训练它们。迁移学习的过程通常涉及取一个预训练模型，移除针对原始任务特定的最终层，并添加新层或修改现有层以适应新任务。模型隐藏层的参数通常在训练阶段被冻结。这是与微调的主要区别之一。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: A general overview of the Transformer architecture
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer 架构的一般概述
- en: A brief discussion of Transformer’s main components and how they work together
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对 Transformer 的主要组件及其协同工作方式的简要讨论
- en: An example of how to apply the transfer learning technique to build a new model
    for sentiment analysis
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将迁移学习技术应用于构建新的情感分析模型的示例
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The following are the technical requirements for this chapter:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的技术要求如下：
- en: The PyTorch library
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 库
- en: A modern C++ compiler with C++20 support
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持 C++20 的现代 C++ 编译器
- en: The CMake build system version >= 3.22
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CMake 构建系统版本 >= 3.22
- en: 'The code files for this chapter can be found in the following GitHub repo:
    [https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-C-second-edition/tree/master/Chapter11/pytorch](https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-C-second-edition/tree/master/Chapter11/pytorch).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在以下 GitHub 仓库中找到：[https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-C-second-edition/tree/master/Chapter11/pytorch](https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-C-second-edition/tree/master/Chapter11/pytorch)。
- en: 'To configure the development environment, please follow the instructions described
    in the following document: [https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/blob/main/env_scripts/README.md](https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/blob/main/env_scripts/README.md).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 请按照以下文档中描述的说明配置开发环境：[https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/blob/main/env_scripts/README.md](https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/blob/main/env_scripts/README.md)。
- en: Also, you can explore the scripts in that folder to see configuration details.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您还可以探索该文件夹中的脚本以查看配置细节。
- en: 'To build the example project for this chapter, you can use the following script:
    [https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/blob/main/build_scripts/build_ch11.sh](https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/blob/main/build_scripts/build_ch11.sh).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建本章的示例项目，您可以使用以下脚本：[https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/blob/main/build_scripts/build_ch11.sh](https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/blob/main/build_scripts/build_ch11.sh)。
- en: An overview of the Transformer architecture
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer架构概述
- en: 'The Transformer is a type of neural network architecture that was first introduced
    in the paper *Attention Is All You Need* by Google researchers. It has become
    widely used in NLP and other domains due to its ability to process long-range
    dependencies and attention mechanisms. The following scheme shows the general
    Transformer architecture:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer是一种神经网络架构，它首次由谷歌研究人员在论文《Attention Is All You Need》中提出。由于其能够处理长距离依赖关系和注意力机制，它已经在NLP和其他领域得到广泛应用。以下方案展示了Transformer的一般架构：
- en: '![Figure 11.1 – The Transformer architecture](img/B19849_11_1.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图11.1 – Transformer架构](img/B19849_11_1.jpg)'
- en: Figure 11.1 – The Transformer architecture
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1 – Transformer架构
- en: 'There are two main components of the Transformer architecture: the encoder
    and the decoder. The encoder processes the input sequence, while the decoder generates
    the output sequence. Common elements of these Transformer components are the following:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构有两个主要组件：编码器和解码器。编码器处理输入序列，而解码器生成输出序列。这些Transformer组件的常见元素如下：
- en: '**Self-attention**: The model uses self-attention mechanisms to learn the relationships
    between different parts of an input sequence. This allows it to capture long-distance
    dependencies, which are important for understanding context in natural languages.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自注意力机制**：该模型使用自注意力机制来学习输入序列不同部分之间的关系。这使得它能够捕捉到长距离依赖关系，这对于理解自然语言中的上下文非常重要。'
- en: '**Cross-attention**: This is a type of attention mechanism that is used when
    working with two or more sequences. In this case, the elements from one sequence
    attend to elements from another sequence, allowing the model to learn the relationships
    between the two inputs. It’s used for communication between the encoder and decoder
    parts.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交叉注意力机制**：这是一种在处理两个或更多序列时使用的注意力机制。在这种情况下，一个序列的元素关注另一个序列的元素，从而使模型能够学习两个输入之间的关系。它用于编码器和解码器部分之间的通信。'
- en: '**Multi-head attention**: Instead of using a single attention mechanism, the
    Transformer uses multiple attention heads, each with its own weights. This helps
    to model different aspects of the input sequence and improve the model’s representational
    power.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多头注意力**：Transformer不是使用单个注意力机制，而是使用多个注意力头，每个头都有其自己的权重。这有助于模拟输入序列的不同方面，并提高模型的表示能力。'
- en: '**Positional encoding**: Since the Transformer does not use recurrent or convolutional
    layers, it needs a way to preserve positional information about the input sequences.
    Positional encoding is used to add this information to the inputs. This is done
    by adding sine and cosine functions of different frequencies to the embedding
    vectors.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**位置编码**：由于Transformer不使用循环或卷积层，它需要一种方法来保留输入序列的位置信息。位置编码用于将此信息添加到输入中。这是通过向嵌入向量添加不同频率的正弦和余弦函数来实现的。'
- en: '**Feedforward neural networks**: Between the attention layers, there are fully
    connected feedforward neural networks. These networks help to transform the output
    of the attention layer into a more meaningful representation.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前馈神经网络**：在注意力层之间，存在全连接的前馈神经网络。这些网络有助于将注意力层的输出转换为更有意义的表示。'
- en: '**Residual connections and normalization**: Like many deep learning models,
    the Transformer includes residual connections and batch normalization to improve
    training stability and convergence.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**残差连接和归一化**：与许多深度学习模型一样，Transformer 包含残差连接和批量归一化，以提高训练的稳定性和收敛性。'
- en: Let’s see in detail the main differences and tasks that are solved by the encoder
    and decoder parts.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看编码器和解码器部分解决的主要差异和任务。
- en: Encoder
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器
- en: The **encoder** part of a Transformer is responsible for encoding the input
    data into a fixed-length vector representation, known as an embedding. This embedding
    captures the important features and information from the input and represents
    it in a more abstract form. The encoder typically consists of multiple layers
    of self-attention mechanisms and feedforward networks. Each layer of the encoder
    helps to refine and improve the representation of the input, capturing more complex
    patterns and dependencies.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 的 **编码器** 部分负责将输入数据编码为固定长度的向量表示，称为嵌入。这个嵌入捕获了输入中的重要特征和信息，并以更抽象的形式表示它。编码器通常由多层自注意力机制和前馈网络组成。编码器的每一层都有助于细化并改进输入的表示，捕获更复杂的模式和依赖关系。
- en: 'In terms of internal representations, the encoder produces embeddings that
    can be either contextual or positional. Contextual embeddings focus on capturing
    semantic and syntactic information from the text, while positional embeddings
    encode information about the order and position of words in the sequence. Both
    types of embeddings play a role in capturing the context and relationships between
    words in a sentence:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部表示方面，编码器产生的嵌入可以是上下文或位置的。上下文嵌入专注于从文本中捕获语义和句法信息，而位置嵌入编码关于序列中单词的顺序和位置的信息。这两种类型的嵌入都在捕捉句子中单词的上下文和关系方面发挥作用：
- en: Contextual embeddings allow the model to understand the meaning of words based
    on their context, taking into account surrounding words and their relationships
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文嵌入允许模型根据上下文理解单词的意义，考虑到周围的单词及其关系
- en: Positional embeddings, on the other hand, provide information about the position
    of each word in the sequence, helping the model understand the order and structure
    of the text
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置嵌入另一方面提供关于序列中每个单词位置的信息，帮助模型理解文本的顺序和结构
- en: Together, contextual and positional embeddings in the encoder help the Transformer
    to better understand and represent the input data, enabling it to generate more
    accurate and meaningful outputs in downstream tasks.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在编码器中，上下文和位置嵌入共同帮助 Transformer 更好地理解和表示输入数据，使其能够在下游任务中生成更准确和有意义的输出。
- en: Decoder
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码器
- en: The **decoder** part of a Transformer takes the encoded representation as input
    and generates the final output. It consists of layers of attention and feedforward
    networks, similar to the encoder. However, it also includes additional layers
    that allow it to predict and generate outputs. The decoder predicts the next token
    in a sequence based on the encoded representation of the input sequence and its
    own previous predictions.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 的 **解码器** 部分以编码表示作为输入并生成最终输出。它由与编码器类似的多层注意力和前馈网络组成。然而，它还包括额外的层，使其能够预测和生成输出。解码器根据输入序列的编码表示及其自己的先前预测来预测序列中的下一个标记。
- en: The output probabilities of the Transformer decoder represent the likelihood
    of each possible token being the next word in the sequence. These probabilities
    are calculated using a softmax function, which assigns a probability value to
    each token based on its relevance to the context.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 解码器的输出概率代表每个可能的标记作为序列中下一个单词的可能性。这些概率使用 softmax 函数计算，该函数根据标记与上下文的相关性为其分配概率值。
- en: During training, the decoder uses the encoded embedding to generate predictions
    and compare them with the true output. The difference between the predicted and
    true outputs is used to update the parameters of the decoder and improve its performance.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，解码器使用编码嵌入生成预测并将其与真实输出进行比较。预测输出和真实输出之间的差异用于更新解码器的参数并提高其性能。
- en: 'Output sampling is an essential part of the decoder process. It involves selecting
    the next token based on output probabilities. There are many methods for sampling
    the output. The following list shows some of the popular ones:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 输出采样是解码器过程的一个重要部分。它涉及根据输出概率选择下一个标记。有许多采样输出的方法。以下列表显示了其中一些流行的方法：
- en: '**Greedy search**: This is the simplest method of sampling, where the most
    probable token at each step is selected based on the softmax probability distribution
    of the token values. While it is fast and easy to implement, it may not always
    find the optimal solution.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**贪婪搜索**：这是最简单的采样方法，在每个步骤根据标记值的softmax概率分布选择最可能的标记。虽然这种方法快速且易于实现，但它可能并不总是找到最优解。'
- en: '**Top-k sampling**: Top-k sampling selects the top *k* tokens with the highest
    probabilities from the softmax distribution at each step, instead of selecting
    the most probable token. This method can help to diversify the samples and prevent
    the model from getting stuck in a local optimum.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Top-k采样**：Top-k采样在每个步骤从softmax分布中选择概率最高的前*k*个标记，而不是选择最可能的标记。这种方法可以帮助多样化样本，防止模型陷入局部最优。'
- en: '**Nucleus sampling**: Nucleus sampling, also known as top-p sampling, is a
    variant of top-k sampling that selects a subset of tokens from the top of the
    softmax distribution based on their probabilities. By selecting multiple tokens
    within a range of probabilities, nucleus sampling can improve the diversity and
    coverage of the output.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**核采样**：核采样，也称为top-p采样，是top-k采样的一个变体，它根据概率从softmax分布的顶部选择一个子集的标记。通过在概率范围内选择多个标记，核采样可以提高输出的多样性和覆盖率。'
- en: You now have an understanding of how the main Transformer components work and
    what elements are used within it. But this leaves the topic of how inputs are
    preprocessed before the decoder takes them to process uncovered. Let’s see how
    input text can be converted into Transformer input.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在已经了解了主要Transformer组件的工作原理以及其中使用的元素。但这也留下了在解码器处理之前如何对输入进行预处理的话题。让我们看看如何将输入文本转换为Transformer输入。
- en: Tokenization
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分词
- en: Tokenization is the process of breaking down a sequence of text into smaller
    units, called tokens. These tokens can be individual words, subwords, or even
    characters, depending on the specific task and model architecture. For example,
    in the sentence *I love eating pizza*, the tokens would be *I*, *love*, *eating*,
    and *pizza*.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 分词是将文本序列分解成更小的单元，称为标记的过程。这些标记可以是单个单词、子词，甚至字符，具体取决于特定任务和模型架构。例如，在句子“我爱吃披萨”中，标记将是“I”，“爱”，“吃”，和“披萨”。
- en: 'There are many tokenization methods used in Transformer models. The most used
    are the following:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在Transformer模型中使用了多种分词方法。最常用的有以下几种：
- en: '**Word tokenization**: This method splits the text into individual words. It
    is the most common approach and is suitable for tasks such as translation and
    text classification.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单词分词**：这种方法将文本分割成单个单词。这是最常见的方法，适用于翻译和文本分类等任务。'
- en: '**Subword tokenization**: In this method, the text is split into smaller units,
    called **subwords**. This can improve performance on tasks where words are often
    misspelled or truncated, such as machine translation.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**子词分词**：在这种方法中，文本被分割成更小的单元，称为**子词**。这可以提高在单词经常拼写错误或截断的任务上的性能，如机器翻译。'
- en: '**Character tokenization**: This method breaks down the text into individual
    characters. It can be useful for tasks that require fine-grained analysis, such
    as sentiment analysis.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**字符分词**：这种方法将文本分解成单个字符。对于需要细粒度分析的任务，如情感分析，这可能很有用。'
- en: The choice of tokenization method depends on the characteristics of the dataset
    and the requirements of the task.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 分词方法的选取取决于数据集的特征和任务的需求数据。
- en: 'In the following subsection, we will use the BERT model, which uses `[CLS]`
    (classify) and `[SEP]` (end). These tokens serve specific purposes in the model’s
    architecture. Here’s a step-by-step explanation of the WordPiece tokenization
    algorithm:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的小节中，我们将使用BERT模型，该模型使用`[CLS]`（分类）和`[SEP]`（分隔符）。这些标记在模型架构中具有特定用途。以下是WordPiece分词算法的逐步解释：
- en: '**Initial vocabulary**: Start with a small vocabulary that includes special
    tokens used by the model and the initial alphabet. The initial alphabet contains
    all the characters present at the beginning of a word and the characters inside
    a word preceded by the WordPiece prefix.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**初始词汇表**：从一个包含模型使用的特殊标记和初始字母表的小型词汇表开始。初始字母表包含单词开头和WordPiece前缀后的所有字符。'
- en: '`##` for BERT) to each character within a word, resulting in a split such as
    `w ##o ##r ##d`. This creates subwords from each character in the original word.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`##`（用于BERT）将每个单词中的每个字符分开，例如将“word”分割成“w ##o ##r ##d”。这将从原始单词中的每个字符创建子词。'
- en: '`score = (freq_of_pair)/(freq_of_first_element×freq_of_second_element)`. This
    prioritizes the merging of pairs where the individual parts are less frequent
    in the vocabulary.'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`score = (freq_of_pair)/(freq_of_first_element×freq_of_second_element)`。这优先合并那些个体部分在词汇表中出现频率较低的配对。'
- en: '**Merging pairs**: The algorithm merges pairs with high scores, which means
    that the algorithm merges pairs that occur less frequently into individual elements.'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**合并对**：算法合并得分高的对，这意味着算法将出现频率较低的合并对合并成单个元素。'
- en: '**Iterative merging**: The process repeats until a desired number of merge
    operations has been performed or a predefined threshold is reached. At this point,
    the final vocabulary is created.'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**迭代合并**：这个过程会重复进行，直到完成所需数量的合并操作或达到预定义的阈值。此时，创建最终的词汇表。'
- en: With the vocabulary, we can tokenize any word from the input in a similar way
    as we did previously, during the vocabulary construction. So, at first, we search
    for the whole word in the vocabulary. If we don’t find it, we remove a character
    from the beginning preceding a subword with the `##` prefix and search again.
    This process is continued until we find a subword. If we don’t find any tokens
    for a word in a vocabulary, we usually skip this word.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用词汇表，我们可以以类似于我们在词汇构建过程中之前所做的方式对输入中的任何单词进行标记化。因此，首先我们在词汇表中搜索整个单词。如果我们找不到，我们就从前面带有`##`前缀的子词开始移除一个字符，然后再次搜索。这个过程会一直持续到我们找到一个子词。如果我们在一个词汇表中找不到任何单词的标记，我们通常会跳过这个单词。
- en: The vocabulary is typically represented as a dictionary or lookup table that
    maps each word to a unique integer index. The vocabulary size is an important
    factor in the performance of a Transformer, as it determines the complexity of
    the model and its ability to handle different types of text.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇表通常表示为一个字典或查找表，将每个单词映射到唯一的整数索引。词汇表大小是Transformer性能的一个重要因素，因为它决定了模型的复杂性和处理不同类型文本的能力。
- en: Word embeddings
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单词嵌入
- en: Even though we use tokenization to convert words into numbers, it doesn't provide
    any semantic meaning to the neural network. To be able to represent semantic proximity,
    we can use embedding. Embedding is where we map an arbitrary entity to a specific
    vector, for example, a node in a graph, an object in a picture, or the definition
    of a word. A set of embedding vectors can be treated as vector meaning space.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们使用标记化将单词转换为数字，但这并不为神经网络提供任何语义意义。为了能够表示语义邻近性，我们可以使用嵌入。嵌入是将任意实体映射到特定向量的地方，例如，图中的一个节点，图片中的一个对象，或单词的定义。一组嵌入向量可以被视为向量意义空间。
- en: There are many approaches to creating embeddings for words. For example, the
    ones most known in classical NLP are Word2Vec and GloVe, which are based on statistical
    analysis and are actually standalone models.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 创建单词嵌入有许多方法。例如，在经典NLP中最知名的是Word2Vec和GloVe，它们基于统计分析，实际上是独立的模型。
- en: For the Transformer, a different approach was proposed. With tokens as input,
    we pass them into the MLP layer, which gives embedding vectors for each of the
    tokens as output. This layer is trained along with the rest of the model and is
    an internal model component. In this way, we can have embeddings that are more
    fine-tuned for specific training data and particular tasks.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Transformer，提出了不同的方法。以标记作为输入，我们将它们传递到MLP层，该层为每个标记输出嵌入向量。这一层与模型的其他部分一起训练，是内部模型组件。这样，我们可以拥有针对特定训练数据和特定任务的更精细调整的嵌入。
- en: Using the encoder and decoder parts separately
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分别使用编码器和解码器部分
- en: The original Transformer architecture has both parts, the encoder and the decoder.
    But recently, it has been found that these parts can be used separately to solve
    different tasks. The two most well-known base architectures based on them are
    BERT and GPT.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的Transformer架构包含编码器和解码器两部分。但最近发现，这些部分可以分别用于解决不同的任务。基于它们的两个最著名的基架构是BERT和GPT。
- en: '**BERT** stands for **bidirectional encoder representations from transformers**.
    It’s a state-of-the-art NLP model that uses a bidirectional approach to understand
    the context of words in a sentence. Unlike traditional models that only consider
    words in one direction, BERT can look at words both before and after a given word
    to better understand their meaning. It is based only on the encoder Transformer
    part. This makes it particularly useful for tasks that require understanding the
    context, such as semantic similarity and text classification. Also, it is designed
    to understand context in both directions, which means it can consider both the
    previous and following words in a sentence.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**BERT**代表**来自变换器的双向编码器表示**。它是一个最先进的NLP模型，使用双向方法来理解句子中单词的上下文。与仅考虑一个方向的单词的传统模型不同，BERT可以查看给定单词之前和之后的单词，以更好地理解其含义。它仅基于编码器变换器部分。这使得它在需要理解上下文的任务中特别有用，例如语义相似度和文本分类。此外，它旨在理解双向上下文，这意味着它可以考虑句子中的前一个和后一个单词。'
- en: On the other hand, **GPT**, which stands for **generative pre-trained transformer**,
    is a generative model based only on the decoder Transformer part. It is designed
    to generate human-like text by predicting the next word in a sequence.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，**GPT**，即**生成式预训练变换器**，是一个仅基于解码器变换器部分的生成模型。它通过预测序列中的下一个单词来生成类似人类的文本。
- en: In the next section, we will develop a sentiment analysis model with the PyTorch
    library using BERT as the base model.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将使用PyTorch库和BERT作为基础模型来开发一个情感分析模型。
- en: Sentiment analysis example with BERT
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于BERT的情感分析示例
- en: In this section, we are going to build a machine learning model that can detect
    review sentiment (detect whether a review is positive or negative) using PyTorch.
    As a training set, we are going to use the Large Movie Review Dataset, which contains
    a set of 25,000 movie reviews for training and 25,000 for testing, both of which
    are highly polarized.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建一个机器学习模型，该模型可以使用PyTorch检测评论情感（检测评论是正面还是负面）。作为训练集，我们将使用大型电影评论数据集，其中包含用于训练的25,000条电影评论和用于测试的25,000条评论，两者都高度两极分化。
- en: As we said before, we will use an already pre-trained BERT model. BERT was chosen
    due to its ability to understand context and relationships between words, making
    it particularly effective for tasks such as question-answering, sentiment analysis,
    and text classification. Let’s remember that transfer learning is a machine learning
    approach that involves transferring knowledge from a pre-trained model to a new
    or different problem domain. It is used when there is a lack of labeled data for
    the specific task at hand, or when training a model from scratch would be too
    computationally expensive.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所说，我们将使用已经预训练的BERT模型。BERT之所以被选中，是因为它能够理解单词之间的上下文和关系，这使得它在问答、情感分析和文本分类等任务中特别有效。让我们记住，迁移学习是一种机器学习方法，涉及将知识从预训练模型转移到新的或不同的问题域。当特定任务缺乏标记数据时，或者从头开始训练模型过于计算昂贵时，会使用迁移学习。
- en: 'Applying the transfer learning algorithm involves the following steps:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 应用迁移学习算法包括以下步骤：
- en: '**Selection of a pre-trained model**: The model should be chosen based on relevance
    to the task.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择预训练模型**：应根据任务的相关性来选择模型。'
- en: '**Adding a new task-specific head**: This could be, for example, a combination
    of fully connected linear layers with a final softmax for classification.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**添加新的任务特定头**：例如，这可能是全连接线性层与最终的softmax分类的组合。'
- en: '**Freezing the pre-trained parameters**: Freezing the parameters allows the
    model to retain its pre-learned knowledge.'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**冻结预训练参数**：冻结参数允许模型保留其预学习的知识。'
- en: '**Training on the new dataset**: The model is trained using a combination of
    the pre-trained weights and the new data, allowing it to learn specific features
    and patterns relevant to the new domain in new layers.'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**在新数据集上训练**：模型使用预训练权重和新数据的组合进行训练，使其能够在新的层中学习与新领域相关的特定特征和模式。'
- en: We know that BERT-like models are used to extract some semantic knowledge from
    input data; in our case, it will be text. Also, BERT-like models usually represent
    extracted knowledge in the form of embedding vectors. These vectors can be used
    to train a new model head, for example, for a classification task. We will follow
    the previously described steps.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道BERT类模型用于从输入数据中提取一些语义知识；在我们的案例中，它将是文本。BERT类模型通常以嵌入向量的形式表示提取的知识。这些向量可以用来训练新的模型头，例如，用于分类任务。我们将遵循之前描述的步骤。
- en: Exporting the model and vocabulary
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导出模型和词汇表
- en: 'The traditional method of using some pre-trained model in C++ using the PyTorch
    library is to load this model as a TorchScript. A common approach to getting this
    script is to trace a model available in Python and save it. There are a lot of
    pre-trained models on the [https://huggingface.co/](https://huggingface.co/) site.
    Also, these models are available with the Python API. So, let’s write a simple
    Python program to export the base BERT model:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PyTorch库在C++中通过某些预训练模型的传统方法是将此模型作为TorchScript加载。获取此脚本的一种常见方法是通过追踪Python中可用的模型并保存它。在[https://huggingface.co/](https://huggingface.co/)网站上有很多预训练模型。此外，这些模型还提供了Python
    API。因此，让我们编写一个简单的Python程序来导出基础BERT模型：
- en: 'The following code snippet shows how to import the required Python modules
    and load a pre-trained model for tracing:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码片段展示了如何导入所需的Python模块并加载用于追踪的预训练模型：
- en: '[PRE0]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We imported the `BertModel` and `BertTokenizer` classes from the `transformers`
    module, which is the library from Hugging Face that allows us to work with different
    Transformer-based models. We used the `bert-base-cased` model, which is the raw
    BERT model that was trained to understand general language semantics on large
    corpus of texts, and we also loaded the tokenizer module specialized for BERT
    models—BertTokenizer. Notice also that we used the `torchscript=True` parameter
    to be able to trace and save the model. This parameter tells the library to use
    operators and modules suitable for Torch JIT tracing.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们从`transformers`模块中导入了`BertModel`和`BertTokenizer`类，这是Hugging Face的库，允许我们使用不同的基于Transformer的模型。我们使用了`bert-base-cased`模型，这是在大型文本语料库上训练以理解通用语言语义的原始BERT模型，我们还加载了专门针对BERT模型的分词器模块——BertTokenizer。注意，我们还使用了`torchscript=True`参数来能够追踪和保存模型。此参数告诉库使用适合Torch
    JIT追踪的操作符和模块。
- en: 'Now that we have the loaded tokenizer object, we can tokenize some sample text
    for tracing as follows:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们有了加载的分词器对象，我们可以按照以下方式对一些样本文本进行分词以进行追踪：
- en: '[PRE1]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, we defined the maximum number of tokens that can be generated, which is
    `128`. The BERT model we loaded can process a maximum of 512 tokens at once, so
    you should configure this number for your task, for example, you can use a smaller
    number of tokens to satisfy performance restrictions on embedded devices. Also,
    we told the tokenizer to truncate longer sequences and pad shorter sequences to
    `max_length`. We also made the tokenizer return PyTorch tensors by specifying
    `return_tensors="pt"`.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们定义了可以生成的最大标记数，即`128`。我们加载的BERT模型一次可以处理最多512个标记，因此您应该根据您的任务配置此数字，例如，您可以使用更少的标记数量以满足嵌入式设备上的性能限制。此外，我们告诉分词器截断较长的序列并填充较短的序列到`max_length`。我们还通过指定`return_tensors="pt"`使分词器返回PyTorch张量。
- en: 'We used two values returned from the tokenizer: `input_ids`, which is the token
    values, and `attention_mask`, which is a binary mask filled with `1` for real
    tokens and `0` for padded tokens that shouldn’t be processed.'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用了分词器返回的两个值：`input_ids`，它是标记值，以及`attention_mask`，它是一个二进制掩码，对于真实标记填充`1`，对于不应处理的填充标记填充`0`。
- en: 'Now that we have tokens and masks, we can export the model as follows:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们有了标记和掩码，我们可以按照以下方式导出模型：
- en: '[PRE2]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We switched the model into evaluation mode because it will be used for tracing,
    not for training. Then, we used the `torch.jit.trace` function to trace the model
    on sample input that is the tuple of our generated tokens and attention mask.
    We used the `save` method of the traced module to save the model script into a
    file.
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将模型切换到评估模式，因为它将被用于追踪，而不是用于训练。然后，我们使用`torch.jit.trace`函数在样本输入上追踪模型，样本输入是我们生成的标记和注意力掩码的元组。我们使用追踪模块的`save`方法将模型脚本保存到文件中。
- en: 'As well as the model, we also have to export the tokenizer vocabulary, as follows:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了模型之外，我们还需要导出分词器的词汇表，如下所示：
- en: '[PRE3]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, we just went through all the available tokens in the tokenizer object
    and saved them as `[value - id]` pairs listed in the text file.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只是遍历了分词器对象中所有可用的令牌，并将它们作为`[value - id]`对保存到文本文件中。
- en: Implementing the tokenizer
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现分词器
- en: 'We can load the saved scripted model directly with the PyTorch C++ API, but
    we can’t do the same with the tokenizer. Also, there is no tokenizer implementation
    in the PyTorch C++ API. So, we have to implement the tokenizer by ourselves:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接使用PyTorch C++ API加载保存的脚本模型，但不能对分词器做同样的事情。此外，PyTorch C++ API中没有分词器的实现。因此，我们必须自己实现分词器：
- en: 'The simplest tokenizer can actually be implemented easily. It can have the
    following definition for the header file:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最简单的分词器实际上可以很容易地实现。它可以为头文件定义以下内容：
- en: '[PRE4]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We defined the `Tokenizer` class with a constructor that takes the name of the
    vocabulary file and the maximum length of the token sequence to produce. We also
    defined a single method, `tokenize`, that takes input text as an argument.
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们定义了一个`Tokenizer`类，它有一个构造函数，该构造函数接受词汇文件名和要生成的令牌序列的最大长度。我们还定义了一个单独的方法`tokenize`，它接受输入文本作为参数。
- en: 'The constructor can be implemented as follows:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构造函数可以如下实现：
- en: '[PRE5]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We simply opened the given text file and read it line by line. We split each
    line into two components, which are the token string value and the corresponding
    ID. These components are delimited with the space character. Also, we converted
    `id` from a `string` to an `integer` value. All parsed token ID pairs were saved
    into the `std::unordered_map` container to be able to search for the token ID
    effectively.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们简单地打开给定的文本文件，逐行读取。我们将每一行分割成两个组件，即令牌字符串值和相应的ID。这些组件由空格字符分隔。此外，我们将`id`从`string`转换为`integer`值。所有解析的令牌ID对都保存到`std::unordered_map`容器中，以便能够有效地搜索令牌ID。
- en: 'The `tokenize` method implementation is slightly complex. We define it as follows:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`tokenize`方法的实现稍微复杂一些。我们定义如下：'
- en: '[PRE6]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here, we got the special token ID values from the loaded vocabulary. These token
    IDs are needed by the BERT model to correctly process inputs.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们从加载的词汇表中获得了特殊的令牌ID值。这些令牌ID是BERT模型正确处理输入所必需的。
- en: 'The PAD token is used to mark empty tokens in the case when our input text
    is too short. It can be done as follows:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们的输入文本太短时，PAD令牌用于标记空令牌。可以这样做：
- en: '[PRE7]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As in a Python program, we created two vectors, one for token IDs and one for
    the attention mask. We used `pad_token_id` as the default value for token IDs
    and filled the attention mask with zeros. Then, we put `start_token_id` as the
    first element and put the corresponding value in the attention mask.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 就像在Python程序中一样，我们创建了两个向量，一个用于令牌ID，另一个用于注意力掩码。我们使用`pad_token_id`作为令牌ID的默认值，并用零填充注意力掩码。然后，我们将`start_token_id`作为第一个元素，并在注意力掩码中放入相应的值。
- en: 'Having defined output containers, we now define the intermediate objects for
    input text processing, as follows:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义了输出容器后，我们现在定义输入文本处理的中间对象，如下所示：
- en: '[PRE8]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We moved our input text string into a `stringstream` object to be able to read
    it word by word. We also defined the corresponding word string object.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将输入文本字符串移动到`stringstream`对象中，以便能够逐词读取。我们还定义了相应的单词字符串对象。
- en: 'The top-level processing cycle can be defined as follows:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 顶层处理周期可以如下定义：
- en: '[PRE9]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here, we used the `getline` function to split an input string stream into words
    using the space character as a delimiter. This is a simple approach to splitting
    input text. Usually, tokenizers use more complex strategies for splitting. But
    it’s enough for our task and our dataset. Also, we added the check that if we
    reach the maximum sequence length, we stop text processing, so we truncate it.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们使用了`getline`函数，通过空格字符作为分隔符将输入字符串流分割成单词。这是一种简单的分割输入文本的方法。通常，分词器会使用更复杂的策略进行分割。但对于我们的任务和我们的数据集来说，这已经足够了。此外，我们还添加了检查，如果我们达到最大序列长度，我们就停止文本处理，因此我们截断它。
- en: 'Then, we have to identify the longest possible prefix in the first word that
    exists in the vocabulary; it can be a whole word. The implementation starts as
    follows:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们必须识别第一个单词中最长可能的、存在于词汇表中的前缀；它可能是一个完整的单词。实现开始如下：
- en: '[PRE10]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, we defined the `start` variable to track the beginning of the prefix of
    a word, it's initialized with 0 which is the current word's beginning position.
    We defined the `end` variable to track the end of the prefix, it's initialized
    with the current word length as the word's last position. Initially, they pointed
    to the beginning and the end of the word. Then, in the internal loop, we continuously
    reduced the size of the prefix by decrementing the `end` variable. After each
    decrement, if we didn’t find the prefix in the vocabulary, we repeated the process
    until the end of the word. Also, we made it so that after a successful token search,
    we swapped the `start` and `end` variables to split the word and continue prefix
    searches for the remaining part of the word. This was done because a word can
    consist of several tokens. Also, in this code, we made a check for the maximum
    token sequence length to stop the whole token search process.
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following step is where we carry out a prefix search in the vocabulary:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Here, we extracted the prefix from the original word by using the `substr` function.
    If the `start` variable is not `0`, we are working with the internal part of the
    word, so the addition of the `##` special prefix is carried out. We used the `find`
    method of the unordered map container to find a token (prefix). If the search
    was successful, we placed the token ID in the next position of the `input_ids`
    container, made a corresponding mark in `attention_mask`, increased the `input_id`
    index to move the current sequence position, and broke the loop to start working
    with the following word part.
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After implementing the code for filling in the input IDs and attention mask,
    we put them into PyTorch tensor objects and return the output as follows:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Before converting input IDs and masks into tensors, we finalized the token ID
    sequence with the `end_token_id` value. Then, we used the `torch.tensor` function
    to create tensor objects. This function can take different inputs, and one of
    them is just `std::vector` with numeric values. Also, we used the `unsqueeze`
    function to add the batch dimension to tensors. We also returned the final tensors
    as a standard pair.
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the following subsection, we will implement a dataset loader class using
    the implemented tokenizer.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the dataset loader
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have to develop parser and data loader classes to move the dataset to memory
    in a format suitable for use with PyTorch:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the parser. The dataset we have is organized as follows: there
    are two folders for the training and testing sets, and each of these folders contains
    two child folders, named `pos` and `neg`, which is where the positive and negative
    review files are placed, respectively. Each file in the dataset contains exactly
    one review, and its sentiment is determined by the folder it’s placed in. In the
    following code sample, we will define the interface for the reader class:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We defined two vectors, `pos_samples_` and `neg_samples_`, which contain the
    reviews that were read from the corresponding folders.
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We will assume that the object of this class should be initialized with the
    path to the root folder where one of the datasets is placed (the training set
    or testing set). We can initialize this in the following way:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The most important parts of this class are the `constructor` and the `read_directory`
    methods.
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The constructor is the main point wherein we fill the containers, `pos_samples_`
    and `neg_samples_`, with actual reviews from the `pos` and `neg` folders:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `read_directory` method implements the logic for iterating files in the
    given directory and reads them as follows:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We used the standard library directory iterator class, `fs::directory_iterator`,
    to get every file in the folder. The object of this class returns the object of
    the `fs::directory_entry` class, and this object can be used to determine whether
    this is a regular file with the `is_regular_file` method. We got the file path
    of this entry with the `path` method. We read the whole file to one string object
    using the `rdbuf` method of the `std::ifstream` type object.
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that the `ImdbReader` class has been implemented, we can go further and
    start the dataset implementation. Our dataset class should return a pair of items:
    one representing the tokenized text and another the sentiment value. Also, we
    need to develop a custom function to convert the vector of tensors in a batch
    into one single tensor. This function is required if we want to make PyTorch compatible
    with our custom training data.'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s define the `ImdbSample` type for a custom training data sample. We will
    use this with the `torch::data::Dataset` type:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '`ImdbData` represents the training data and has two tensors for a token sequence
    and an attention mask. `ImdbSample` represents the whole sample with a target
    value. A tensor contains `1` or `0` for positive or negative sentiment, respectively.'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following code snippet shows the `ImdbDataset` class’ declaration:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We inherited our dataset class from the `torch::data::Dataset` class so that
    we can use it for data loader initialization. The PyTorch data loader object is
    responsible for sampling random training objects and making batches from them.
    The objects of our `ImdbDataset` class should be initialized with the root dataset
    path for the `ImdbReader` and `Tokenizer` objects. The constructor implementation
    is trivial; we just initialize the reader and store a pointer to the tokenizer.
    Notice that we used the pointer to the tokenizer to share it among the train and
    test datasets later. We overrode two methods from the `torch::data::Dataset` class:
    the `get` and `size` methods.'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following code shows how we implement the `size` method:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The `size` method returns the number of reviews in the `ImdbReader` object.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The `get` method has a more complicated implementation than the previous method,
    as shown in the following code:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: First, we got the review text and sentiment value from the given index (the
    function argument value). In the `size` method, we returned the total number of
    positive and negative reviews, so if the input index is greater than the number
    of positive reviews, then this index points to a negative one. Then, we subtracted
    the number of positive reviews from it.
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After we got the correct index, we also got the corresponding text review, assigned
    its address to the `review` pointer, and initialized the `target` tensor. The
    `torch::tensor` function was used to initialize the `target` tensor. This function
    takes an arbitrary numeric value and tensor options such as the required type.
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: With the review text, we just used the tokenizer object to create two tensors
    with token IDs and a sentiment mask. They are packed in the `tokenizer_out` pair
    object. We returned the pair of training tensors and one target tensor.
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To be able to effectively use batched training, we create a special class so
    that PyTorch is able to convert our non-standard sample type into a batch tensor.
    In the simplest case, we will get the `std::vector` object of training samples
    instead of a single batch tensor. It was done as follows:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We inherited our class from the special PyTorch `torch::data::transforms::Collation`
    type and specialized it with the template parameter of our `ImdbExample` class.
    Having such a class, we overrode the virtual `apply_batch` function with an implementation
    that takes as input the `std::vector` object containing `ImdbExample` objects
    and returns the single `ImdbExample` object. It means that we merged all input
    IDs, attention masks, and target tensors in three separate tensors. This was done
    by creating three separate containers for input IDs, attention masks, and target
    tensors. They were filled in a simple loop over the input samples. Then, we just
    used the `torch::stack` function to merge (stack) these containers in single tensors.
    This class will be used in the DataLoader type object construction later.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the model
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The next step is to create a `model` class. We already have the exported model
    that we are going to use as a pre-trained part. We create a simple classification
    head with two linear fully connected layers and one dropout layer for regularization:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'The header file for this class will look as follows:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We included the `torch\script.h` header file to use the `torch::jit::script::Module`
    class. The instance of this class will be used as a representation of the previously
    exported BERT model. See the `bert_` member variable. Also, we defined member
    variables for the linear and dropout layers as instances of the `torch::jit::script::Module`
    class. We inherited our `ModelImpl` class from the `torch::nn::Module` class to
    integrate it into the PyTorch auto-gradient system.
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The constructor implementation looks as follows:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We used `torch::jit::load` to load the model that we exported from Python. This
    function takes a single argument—the model filename. Also, we initialized the
    dropout and linear layers and registered them in the parent `torch::nn::Module`
    object. The `fc1_` linear layer is the input one; it takes the 768-dimensional
    output of the BERT model. The `fc2_` linear layer is the output layer. It processes
    the internal 512-dimensional state into two classes.
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用了`torch::jit::load`来加载我们从Python导出的模型。这个函数接受一个参数——模型文件名。此外，我们还初始化了dropout和线性层，并在父`torch::nn::Module`对象中注册了它们。`fc1_`线性层是输入层；它接收BERT模型的768维输出。`fc2_`线性层是输出层。它将内部512维状态处理成两个类别。
- en: 'The main model functionality is implemented in the `forward` function as follows:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主模型功能是在`forward`函数中实现的，如下所示：
- en: '[PRE24]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This function takes the input token IDs plus the corresponding attention mask
    and returns a two-dimensional tensor with classification results. We interpret
    the sentiment analysis as a classification task. The `forward` implementation
    has two parts. One part is where we preprocess inputs with the loaded BERT model;
    this preprocessing is just an inference with pretrained BERT model backbone. The
    second part is where we pass the BERT output through our classification head.
    This is the trainable part. To use the BERT model, that is, the `torch::jit:script::Module`
    object, we packed inputs into the `std::vector` container of the `torch::jit::Ivalue`
    objects. The conversion from `torch::Tensor` was done automatically.
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此函数接收输入标记ID以及相应的注意力掩码，并返回一个包含分类结果的二维张量。我们将情感分析视为一个分类任务。`forward`实现有两个部分。一部分是我们使用加载的BERT模型对输入进行预处理；这种预处理只是使用预训练的BERT模型主干进行推理。第二部分是我们将BERT输出通过我们的分类头。这是可训练的部分。要使用BERT模型，即`torch::jit:script::Module`对象，我们将输入打包到`torch::jit::Ivalue`对象的`std::vector`容器中。从`torch::Tensor`的转换是自动完成的。
- en: Then, we used standard for PyTorch `forward` function for inference. This function
    returns the `torch::jit` tuple object; the return type actually depends on the
    initial model that was traced. So, to get the PyTorch tensor from the `torch::jit`
    value object, we explicitly used the `toTuple` method to say how to interpret
    the output result. Then, we accessed the second tuple element by using the `elements`
    method, which provides the indexing operator for tuple elements. Finally, to get
    the tensor, we used the `toTensor` method of the `jit::Ivalue` object, which is
    a tuple element.
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，我们使用了PyTorch的标准`forward`函数进行推理。这个函数返回一个`torch::jit`元组对象；返回类型实际上取决于最初被追踪的模型。因此，要从`torch::jit`值对象中获取PyTorch张量，我们显式地使用了`toTuple`方法来说明如何解释输出结果。然后，我们通过使用`elements`方法访问第二个元组元素，该方法为元组元素提供了索引操作符。最后，为了获取张量，我们使用了`jit::Ivalue`对象的`toTensor`方法，这是一个元组元素。
- en: The BERT model we used returns two tensors. The first one represents the embedding
    values for input tokens, and the second one is the pooled output. Pooled output
    is the embeddings of the `[CLS]` token, for the input text. The linear layer weights
    that produced this output were trained from the next sentence prediction (classification)
    objective during BERT pre-training. So, these are the ideal values to use in the
    following text classification tasks. That is why we took the second element of
    the tuple returned by the BERT model.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用的BERT模型返回两个张量。第一个表示输入标记的嵌入值，第二个是池化输出。池化输出是输入文本的`[CLS]`标记的嵌入。产生此输出的线性层权重是在BERT预训练期间从下一个句子预测（分类）目标中训练出来的。因此，这些是在以下文本分类任务中使用的理想值。这就是为什么我们取BERT模型返回的元组的第二个元素。
- en: The second part of the `forward` function is also simple. We passed the BERT
    output to the `fc1_` linear layer followed by the `relu` activation function.
    After this operation, we got the `512` internal hidden state. Then, this state
    was processed by the `dropout_` module to introduce some regularization into the
    model. The final stage was the use of the `fc2_` output linear module, which returns
    a two-dimensional vector followed by the `softmax` function. The `softmax` function
    converts logits into probability values with the range `[0,1]` because raw logits
    from a linear layer can have arbitrary values and need to convert them into target
    values.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`forward`函数的第二部分同样简单。我们将BERT的输出传递给`fc1_`线性层，随后是`relu`激活函数。在此操作之后，我们得到了`512`个内部隐藏状态。然后，这个状态被`dropout_`模块处理，以向模型中引入一些正则化。最后阶段是使用`fc2_`输出线性模块，它返回一个二维向量，随后是`softmax`函数。`softmax`函数将logits转换为范围在`[0,1]`之间的概率值，因为来自线性层的原始logits可以具有任意值，需要将它们转换为目标值。'
- en: Now, we have described all the components required for the training process.
    Let’s see how the model training can be implemented.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经描述了训练过程所需的所有组件。让我们看看模型训练是如何实现的。
- en: Training the model
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'The first step in training is creating the dataset object, which can be done
    as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的第一步是创建数据集对象，可以按照以下方式完成：
- en: '[PRE25]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We created the `tokenizer` and `train_dataset` objects just by passing the
    paths to the corresponding files. The test dataset can be created in the same
    way with the same `tokenizer` object. Now that we have the dataset, we create
    a data loader as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅通过传递对应文件的路径就创建了`tokenizer`和`train_dataset`对象。测试数据集也可以用相同的方式创建，使用相同的`tokenizer`对象。现在我们有了数据集，我们创建数据加载器如下：
- en: '[PRE26]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We specified the size of the batch and used the `make_data_loader` function
    to create the data loader object. This object uses the dataset to effectively
    load and organize training samples in batches. We used the transformation `map`
    function of the `train_dataset` object with an instance of our collation `Stack`
    class to allow PyTorch to merge our training samples into tensor batches. Also,
    we specified some data loader options, that is, the batch size and the number
    of worker threads used to load and preprocess data.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指定了批大小，并使用`make_data_loader`函数创建了数据加载器对象。此对象使用数据集有效地加载和组织批量训练样本。我们使用`train_dataset`对象的`map`转换函数和一个我们的`Stack`类实例来允许PyTorch将我们的训练样本合并成张量批次。此外，我们还指定了一些数据加载器选项，即批大小和用于加载数据和预处理的线程数。
- en: 'We create the model object as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建模型对象如下：
- en: '[PRE27]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We used the `torch::cuda::is_available` function to determine whether the CUDA
    device is available in a system and initialized the `device` variable correspondingly.
    Using a CUDA device can significantly improve the training and inference of a
    model. The model was created with the constructor that takes the path to the exported
    BERT model. After model object initialization, we moved this object to the particular
    device.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了`torch::cuda::is_available`函数来确定系统是否可用CUDA设备，并相应地初始化`device`变量。使用CUDA设备可以显著提高模型的训练和推理。模型是通过一个构造函数创建的，该构造函数接受导出BERT模型的路径。在模型对象初始化后，我们将此对象移动到特定的设备。
- en: 'The last component required for training is an optimizer, which we create as
    follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 训练所需的最后一个组件是一个优化器，我们创建如下：
- en: '[PRE28]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We used the `AdamW` optimizer, which is an improved version of the popular Adam
    optimizer. To construct the optimizer object, we passed the model parameters and
    the learning rate option as constructor arguments.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了`AdamW`优化器，这是流行的Adam优化器的一个改进版本。为了构建优化器对象，我们将模型参数和学习率选项作为构造函数参数传递。
- en: 'The training cycle can be defined as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 训练周期可以定义为以下内容：
- en: '[PRE29]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: There are two nested loops. One is over the epochs and there is another, an
    internal one, that is over the batches. At the beginning of each internal loop
    is when a new epoch starts. We switched the model into training mode. This switch
    can be done once, but usually, you have some testing code that switches the model
    into evaluation mode, so this switch returns the model to the required state.
    Here, we omitted the test code for simplicity. It looks pretty similar to the
    training one, the only difference being disabling gradient calculations. In the
    internal loop, we used simple range-based `for` loop C++ syntax to iterate over
    batches. For every batch, at first, we cleared the gradient values by calling
    the `zero_grad` function for the optimizer object. Then, we decoupled the batch
    into separate tensor objects. Also, we moved these tensors to a GPU device if
    one is available. This was done with the `.to(device)` calls. We removed an additional
    dimension from the model input tensors with the `squeeze` method. This dimension
    appeared during an automatic batch creation.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Once all the tensors were prepared, we made a prediction with our model that
    gave us the `output` tensor. This output was used in the `torch::cross_entropy_loss`
    loss function, which is usually used for multi-class classification. It takes
    a tensor with probabilities for every class and the one-hot-encoded labels tensor.
    Then, we used the `backward` method of the `loss` tensor to calculate gradients.
    Also, we clipped gradients with the `clip_grad_norm_` function by setting a top
    value limit, to prevent them from exploding. Once the gradients were ready, we
    used the optimizer `step` function to update model weights according to the optimizer
    algorithm.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: This architecture, with the settings we used, can result in more than 80% accuracy
    in the sentiment analysis of movie reviews in 500 training epochs.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced the Transformer architecture, a powerful model used
    in NLP and other fields of machine learning. We discussed the key components of
    the Transformer architecture, which include tokenization, embeddings, positional
    encoding, encoder, decoder, attention mechanisms, multi-head attention, cross-attention,
    residual connections, normalization layers, feedforward layers, and sampling techniques.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in the last part of this chapter, we developed an application so that
    we could perform a sentiment analysis of movie reviews. We applied the transfer
    learning technique to use the features learned by the pre-trained model in a new
    model designed for our specific task. We used the BERT model to produce a embedding
    representation of input texts and attached a linear layer classification head
    to classify review sentiments. We implemented a simple version of the tokenizer
    and dataset loader. We also developed the full training cycle of our classification
    head.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: We used transfer learning instead of fine-tuning to utilize less computational
    resources because the fine-tuning technique usually involves re-training a full
    pre-trained model on a new dataset.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss how to save and load model parameters.
    We will also look at the different APIs that exist in machine learning libraries
    for this purpose. Saving and loading model parameters can be quite an important
    part of the training process because it allows us to stop and restore training
    at an arbitrary moment. Also, saved model parameters can be used for evaluation
    purposes after the model has been trained.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PyTorch documentation: [https://pytorch.org/cppdocs/](https://pytorch.org/cppdocs/)'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hugging Face BERT model documentation: [https://huggingface.co/docs/transformers/model_doc/bert](https://huggingface.co/docs/transformers/model_doc/bert)'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An illustrated Transformer explanation: [https://jalammar.github.io/illustrated-transformer](https://jalammar.github.io/illustrated-transformer)'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Attention Is All You Need*, Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
    Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A list of already pre-trained BERT-like models for sentiment analysis: [https://huggingface.co/models?other=sentiment-analysis](https://huggingface.co/models?other=sentiment-analysis)'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 4: Production and Deployment Challenges'
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The crucial feature of C++ is the ability of the program to be compiled and
    run on a variety of hardware platforms. You can train your complex **machine learning**
    (**ML**) model on the fastest GPU in the data center and deploy it to tiny mobile
    devices with limited resources. This part will show you how to use C++ APIs of
    various ML frameworks to save and load trained models, and how to track and visualize
    a training process, which is crucial for ML practitioners to be able to control
    and check a model’s training performance. Also, we will learn how to build programs
    that use ML models on Android devices; in particular, we will create an object
    detection system that uses a device’s camera.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'This part comprises the following chapters:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 12*](B19849_12.xhtml#_idTextAnchor660), *Exporting and Importing
    Models*'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 13*](B19849_13.xhtml#_idTextAnchor689), *Tracking and Visualizing
    ML Experiments*'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 14*](B19849_14.xhtml#_idTextAnchor702), *Deploying Models on a Mobile
    Platform*'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
