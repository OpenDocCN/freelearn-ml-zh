- en: Topic Modeling and Sentiment Analysis in NLP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we're going to introduce some common topic modeling methods,
    discussing some applications. Topic modeling is a very important NLP section and
    its purpose is to extract semantic pieces of information out of a corpus of documents.
    We're going to discuss **latent semantic analysis**, one of most famous methods;
    it's based on the same philosophy already discussed for model-based recommendation
    systems. We'll also discuss its probabilistic variant, PLSA, which is aimed at
    building a latent factor probability model without any assumption of prior distributions.
    On the other hand, the **Latent Dirichlet Allocation** is a similar approach that
    assumes a prior Dirichlet distribution for latent variables. In the last section,
    we're going to discuss sentiment analysis with a concrete example based on a Twitter
    dataset.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Topic modeling
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main goal of topic modeling in natural language processing is to analyze
    a corpus in order to identify common topics among documents. In this context,
    even if we talk about semantics, this concept has a particular meaning, driven
    by a very important assumption. A topic derives from the usage of particular terms
    in the same document and it is confirmed by the multiplicity of different documents
    where the first condition is true.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, we don''t consider a human-oriented semantics but a statistical
    modeling that works with meaningful documents (this guarantees that the usage
    of terms is aimed to express a particular concept and, therefore, there''s a human
    semantic purpose behind them). For this reason, the starting point of all our
    methods is an occurrence matrix, normally defined as a document-term matrix (we
    have already discussed count vectorizing and tf-idf in [Chapter 12](d276f3b9-d2e3-4b98-b144-645f7ba08d36.xhtml),
    *Introduction to NLP*):'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/63977317-5b6c-4654-8148-96aef3833a58.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
- en: In many papers, this matrix is transposed (it's a term-document one); however,
    scikit-learn produces document-term matrices, and, to avoid confusion, we are
    going to consider this structure.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Latent semantic analysis
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The idea behind latent semantic analysis is factorizing *M[dw]* so as to extract
    a set of latent variables (this means that we can assume their existence but they
    cannot be observed directly) that work as connectors between the document and
    terms. As discussed in [Chapter 11](9c4b1425-663c-4995-8faf-c0823d854ef1.xhtml), *Introduction
    to Recommendation Systems*, a very common decomposition method is SVD:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/804f7c06-5366-4973-bd3f-42eb8fe4d3fb.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
- en: 'However, we''re not interested in a full decomposition; we are interested only
    in the subspace defined by the top *k* singular values:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/76837ca8-9a75-4c09-ae12-098c4a101123.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
- en: 'This approximation has the reputation of being the best one considering the
    Frobenius norm, so it guarantees a very high level of accuracy. When applying
    it to a document-term matrix, we obtain the following decomposition:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ddb9caa0-938b-4570-8aaa-06399d54d44e.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: 'Or, in a more compact way:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6376620d-c7ab-44a5-91e2-3ad7caf2f569.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
- en: 'Here, the first matrix defines a relationship among documents and *k* latent
    variables, and the second a relationship among *k* latent variables and words.
    Considering the structure of the original matrix and what is explained at the
    beginning of this chapter, we can consider the latent variables as **topics** that
    define a subspace where the documents are projected. A generic document can now
    be defined as:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/beda378a-ddfc-428e-a7e3-088a9dd30cdc.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: 'Furthermore, each topic becomes a linear combination of words. As the weight
    of many words is close to zero, we can decide to take only the top *r* words to
    define a topic; therefore, we get:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a768187-1982-4cfe-ae6a-1beb2f94e2bb.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: 'Here, each *h[ji]* is obtained after sorting the columns of *M[twk]*. To better
    understand the process, let''s show a complete example based on a subset of Brown
    corpus (500 documents from the `news` category):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After defining the corpus, we need to tokenize and vectorize using a tf-idf
    approach:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now it''s possible to apply an SVD to the `Xc` matrix (remember that in SciPy,
    the `V` matrix is already transposed):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As the corpus is not very small, it''s useful to set the parameter `full_matrices=False` to
    save computational time. We assume we have two topics, so we can extract our sub-matrices:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If we want to analyze the top 10 words per topic, we need to consider that:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1f988ce-2dc8-4c50-b703-e004e3bcec7d.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, we can obtain the most significant words per topic after sorting
    the matrix using the `get_feature_names()` method provided by the vectorizers:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In this case, we''re considering only non-negative values in the matrix `Vk`;
    however, as a topic is a mixture of words, the negative components should also be
    taken into account. In this case, we need to sort the absolute values of `Vk`:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If we want to analyze how a document is represented in this sub-space, we must
    use:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3dc27e85-a78a-4495-887f-69f2ada30f94.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: 'Let''s consider, for example, the first document of our corpus:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As we are working in a bidimensional space, it''s interesting to plot all the
    points corresponding to each document:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/22cc8148-a648-41a4-9d21-35bcc85deb1e.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: 'In the previous figure, we can see that many documents are correlated, with
    a small group of outliers. This is probably due to the fact that our choice of
    two topics is restrictive. If we repeat the same experiment using two Brown corpus
    categories (`news` and `fiction`), we observe a different behavior:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'I don''t repeat the remaining calculations because they are similar. (The only
    difference is that our corpus is now quite bigger and this leads to a longer computational
    time. For this reason, we''re going to discuss an alternative, which is much faster.)
    Plotting the points corresponding to the documents, we now get:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b2c24ea-f6a0-4df6-aab5-6b57d85ccfb6.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: Now it's easier to distinguish two groups, which are almost orthogonal (meaning
    that many documents belong to only one category). I suggest repeating this experiment
    with different corpora and ranks. Unfortunately, it's impossible to plot more
    than three dimensions, but it's always possible to check whether the sub-space
    describes the underlying semantics correctly using only numerical computations.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'As anticipated, the standard SciPy SVD implementation can be really slow when
    the occurrence matrix is huge; however, scikit-learn provides a truncated SVD
    implementation, `TruncatedSVD`, that works only with the sub-space. The result
    is much faster (it can directly manage sparse matrices too). Let''s repeat the
    previous experiments (with a complete corpus) using this class:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Through the `n_components` parameter, it''s possible to set the desired rank,
    discarding the remaining parts of the matrices. After fitting the model, we get
    the document-topic matrix *M[dtk]* directly as the output of the method `fit_transform()`,
    while the topic-word matrix *M[twk]* can be accessed using the instance variable
    `components_`:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The reader can verify how much faster this process can be; therefore, I suggest
    using a standard SVD implementation only when it''s needed to have access to the
    full matrices. Unfortunately, as is also written in the documentation, this method
    is very sensitive to the algorithm and the random state. It also suffers from
    a phenomenon called **sign indeterminacy**, which means that the signs of all
    components can change if a different random seed is used. I suggest you declare:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Do this with a fixed seed at the beginning of every file (even Jupyter notebooks)
    to be sure that it's possible to repeat the calculations and always obtain the
    same result.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, I advise repeating this experiment using **non-negative matrix factorization**,
    as described in [Chapter 3](7323017f-66c2-4b18-af1e-5d4dc28f9031.xhtml), *Feature
    Selection and Feature Engineering*.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Probabilistic latent semantic analysis
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The previous model was based on a deterministic approach, but it''s also possible
    to define a probabilistic model over the space determined by documents and words.
    In this case, we''re not making any assumption about Apriori probabilities (this
    will be done in the next approach), and we''re going to determine the parameters
    that maximize the log-likelihood of our model. In particular, consider the plate
    notation (if you want to know more about this technique, read [https://en.wikipedia.org/wiki/Plate_notation](https://en.wikipedia.org/wiki/Plate_notation))
    shown in the following figure:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4f6fddca-41f4-419f-8b5d-d88135beab30.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: We assume we have a corpus of *m* documents and each of them is composed of
    *n* words (both elements are observed and therefore represented as gray circles);
    however, we also assume the presence of a limited set of *k* common latent factors
    (topics) that link a document with a group of words (as they are not observed,
    the circle is white). As already written, we cannot observe them directly, but
    we're allowed to assume their existence.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'The joint probability to find a document with a particular word is:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bf99ba35-7325-46ac-9759-c0226c94c541.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, after introducing the latent factors, the conditional probability
    to find a word in a specific document can be written as:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f8ece24-825d-4ef1-9491-d0c10caaf96c.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
- en: 'The initial joint probability *P(d, w)* can be also expressed using the latent
    factors:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4f5761ae-c0d7-42b5-ba13-add58068af28.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: This includes the prior probability *P(t)*. As we don't want to work with it,
    it's preferable to use the expression *P(w|d)*. To determine the two conditional
    probability distributions, a common approach is the **expectation-maximization**
    (**EM**) strategy. A full description can be found in Hofmann T., *Unsupervised
    Learning by Probabilistic Latent Semantic Analysis*, Machine Learning 42, 177-196,
    2001, Kluwer Academic Publishers*. *In this context, we show only the final results
    without any proof.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'The log-likelihood can be written as:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8006da53-20d2-4e60-ae2e-0fdcac43e2e9.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
- en: 'Which becomes:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d7bcae69-f621-4ce8-8774-0e4120b69411.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
- en: '*M[dw]* is an occurrence matrix (normally obtained with a count vectorizer)
    and *M[dw](d, w)* is the frequency of the word *w* in document *d*. For simplicity,
    we are going to approximate it by excluding the first term (which doesn''t depend
    on *t[k]*):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00ab1edb-b077-4522-999a-e6bec52b90fb.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
- en: 'Moreover, it''s useful to introduce the conditional probability *P(t|d,w)*,
    which is the probability of a topic given a document and a word. The EM algorithm
    maximizes the expected complete log-likelihood under the posterior probability *P(t|d,w)*:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/47ae99fd-dbcf-462e-9d4d-fed24934c75b.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
- en: 'The **E** phase of the algorithm can be expressed as:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b106abc-630e-42f9-a621-29265ad5e83e.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
- en: It must be extended to all topics, words, and documents and must be normalized
    with the sum per topic in order to always have consistent probabilities.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'The **M** phase is split into two computations:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8ee6088d-59d4-461b-9e79-cbe4a96b7958.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
- en: Also in this case, the calculations must be extended to all topics, words, and
    documents. But in the first case, we sum by document and normalize by summing
    by word and document, while in the second, we sum by word and normalize by the
    length of the document.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm must be iterated until the log-likelihood stops increasing its
    magnitude. Unfortunately, scikit-learn doesn''t provide a PLSA implementation
    (maybe because the next strategy, LDA, is considered much more powerful and efficient),
    so we need to write some code from scratch. Let''s start by defining a small subset
    of the Brown corpus, taking 10 sentences from the `editorial` category and 10
    from the `fiction` one:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now we can vectorize using the `CountVectorizer` class:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'At this point, we can define the rank (we choose 2 for simplicity), two constants
    that will be used later, and the matrices to hold the probabilities *P(t|d)*,
    *P(w|t)*, and *P(t|d,w)*:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The two matrices *P(t|d)*, *P(w|t)* must be normalized so as to be coherent
    with the algorithm; the other one is initialized to zero. Now we can define the
    log-likelihood function:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'And finally the expectation-maximization functions:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the preceding function, when the normalization factor is 0, the probability
    *P(t|w, d)* is set to 0.0 for each topic:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The constants `alpha_1` and `alpha_2` are used when a normalization factor becomes
    0\. In that case, it can be useful to assign the probability a small value; therefore
    we divided the numerator for those constants. I suggest trying with different
    values so as to tune up the algorithm for different tasks.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we can try our algorithm with a limited number of iterations:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'It''s possible to verify the convergence after the 30th step. At this point,
    we can check the top five words per topic considering the *P(w|t)* conditional
    distribution sorted in descending mode per topic weight:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Latent Dirichlet Allocation
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous method, we didn''t make any assumptions about the topic prior
    to distribution and this can result in a limitation because the algorithm isn''t
    driven by any real-world intuition. LDA, instead, is based on the idea that a
    topic is characterized by a small ensemble of important words and normally a document
    doesn''t cover many topics. For this reason, the main assumption is that the prior
    topic distribution is a symmetric **Dirichlet **one. The probability density function
    is defined as:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f47820a-b6e1-4601-8fd1-793649391f08.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
- en: 'If the concentration parameter alpha is below 1.0, the distribution will be
    sparse as desired. This allows us to model topic-document and topic-word distributions,
    which will always be concentrated on a few values. In this way we can avoid the
    following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: The topic mixture assigned to a document could becoming flat (many topics with
    similar weight)
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The structure of a topic considering the word ensemble could becoming similar
    to a background (in fact, only a limited number of words must be important; otherwise
    the semantic boundaries fade out).
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using the plate notation, we can represent the relationship among documents,
    topics, and words as shown in the following figure:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2658f9fb-fac1-4aaf-b84c-1a9f2759b2a3.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
- en: In the previous figure, alpha is the Dirichlet parameter for the topic-document
    distribution, while gamma has the same role for the topic-word distribution. Theta,
    instead, is the topic distribution for a specific document, while beta is the
    topic distribution for a specific word.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'If we have a corpus of *m* documents and a vocabulary of *n* words (each document
    has *n[i]* words) and we assume to have *k* different topics, the generative algorithm
    can be described with the following steps:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'For each document, draw a sample (a topic mixture) from the topic-document
    distribution:'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/d3a4ad0a-5c17-479b-aeb6-9cf7084ab843.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
- en: 'For each topic, draw a sample from the from the topic-word distribution:'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/f56a7a90-aecd-472b-8236-c6b732851b3d.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
- en: 'Both parameters must be estimated. At this point, considering the occurrence
    matrix *M[dw]* and the notation *z[mn]* to define the topic assigned to the *n*-th
    word in the *m*-th document, we can iterate over documents (index *d*) and words
    (index *w*):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'A topic for document *d* and word *w* is chosen according to:'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/4d93ad6e-8bc5-4974-9ccb-5a178835345d.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
- en: 'A word is chosen according to:'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/b3ab9e42-ce8b-4269-a511-597a753c15fc.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
- en: 'In both cases, a categorical distribution is a one-trial multinomial one. A
    complete description of how the parameters are estimated is quite complex and
    it''s beyond the scope of this book; however, the main problem is finding the
    distribution of latent variables:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d339ef3f-7607-4030-bc4d-355da52b46e7.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
- en: 'The reader can find a lot more information in Blei D., Ng A., Jordan M., *Latent
    Dirichlet Allocation*, Journal of Machine Learning Research, 3, (2003) 993-1022*. *However,
    a very important difference between LDA and PLSA is about the generative ability
    of LDA, which allows working with unseen documents. In fact, the PLSA training
    process finds the optimal parameters *p(t|d)* only for the corpus, while LDA adopts
    random variables. It''s possible to understand this concept by defining the probability
    of theta (a topic mixture) as joint with a set of topics and a set of words, and
    conditioned to the model parameters:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c5691c82-1945-47db-95cb-fe698757a9a6.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
- en: 'As shown in the previously mentioned paper, the probability of a document (a
    set of words) conditioned to the model parameters, can be obtained by integration:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/32d90868-9ebe-4524-ba35-28207ace2197.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
- en: This expression shows the difference between PLSA and LDA. Once learned *p(t|d)*,
    PLSA cannot generalize, while LDA, sampling from the random variables, can always
    find a suitable topic mixture for an unseen document.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'scikit-learn provides a full LDA implementation through the class `LatentDirichletAllocation`.
    We''re going to use it with a bigger dataset (4,000 documents) built from a subset
    of the Brown corpus:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now we can vectorize, define, and train our LDA model by assuming that we have
    eight main topics:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In `CountVectorizer`, we added a regular expression to filter the tokens through
    the parameter `token_pattern`. This is useful as we are not using a full tokenizer
    and, in the corpus, there are also many numbers that we want to filter out. The
    class `LatentDirichletAllocation` allows us to specify the learning method (through
    `learning_method`), which can be either batch or online. We have chosen online
    because it's faster; however, both methods adopt variational Bayes to learn the
    parameters. The former adopts the whole dataset, while the latter works with mini-batches.
    The online option will be removed in the 0.20 release; therefore, you can see
    a deprecation warning when using it now. Both theta and beta Dirichlet parameters
    can be specified through `doc_topic_prior` (theta) and `topic_word_prior` (beta).
    The default value (adopted by us too) is 1.0 / `n_topics` . It's important to
    keep both values small and, in particular, less than 1.0 in order to encourage
    sparseness. The maximum number of iterations (`max_iter`) and other learning-related
    parameters can be applied by reading the built-in documentation or visiting [http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can test our model by extracting the top five keywords per topic. Just
    like `TruncatedSVD`, the topic-word distribution results are stored in the instance
    variable `components_`:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: There are some repetitions, probably due to the composition of some topics,
    and the reader can try different prior parameters to observe the changes. It's
    possible to do an experiment to check whether the model works correctly.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider two documents:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'They are quite different and so are their topic distributions:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We have a dominant topic `(0.85t[0])` for the first document and a mixture
    `(0.22t[0] + 0.22t[2 ]+ 0.42t[7])` for the second one. Now let''s consider the
    concatenation of both documents:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In the resulting document, as expected, the mixture has changed: `0.61t[0] +
    0.11t[2] + 0.21t[7]`. In other words, the algorithm introduced the previously
    dominant topic 5 (which is now stronger) by weakening both topic 2 and topic 7\.
    This is reasonable, because the length of the first document is less than the
    second one, and therefore topic 5 cannot completely cancel the other topics out.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One the most widespread applications of NLP is sentiment analysis of short texts
    (tweets, posts, comments, reviews, and so on). From a marketing viewpoint, it's
    very important to understand the semantics of these pieces of information in terms
    of the sentiment expressed. As you can understand, this task can be very easy
    when the comment is precise and contains only a set of positive/negative words,
    but it becomes more complex when in the same sentence there are different propositions
    that can conflict with each other. For example, *I loved that hotel. It was a
    wonderful experience* is clearly a positive comment, while *The hotel is good,
    however, the restaurant was bad and, even if the waiters were kind, I had to fight
    with a receptionist to have another pillow*. In this case, the situation is more
    difficult to manage, because there are both positive and negative elements, resulting
    in a neutral review. For this reason, many applications aren't based on a binary
    decision but admit intermediate levels (at least one to express the neutrality).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）最广泛的应用之一是对短文本（推文、帖子、评论、评论等）的情感分析。从市场营销的角度来看，理解这些信息片段所表达的情感语义非常重要。正如你可以理解的，当评论精确且只包含一组正面/负面词汇时，这项任务可以非常简单，但当同一句子中有可能相互冲突的不同命题时，它就变得更加复杂。例如，*我喜欢那家酒店。那是一次美妙的经历*显然是一个积极的评论，而*这家酒店不错，然而，餐厅很糟糕，即使服务员很友好，我不得不与前台服务员争论再要一个枕头*。在这种情况下，情况更加难以管理，因为既有积极的也有消极的元素，导致一个中性的评论。因此，许多应用不是基于二元决策，而是承认中间级别（至少一个来表达中立）。
- en: These kind of problems are normally supervised (as we're going to do), but there
    are also cheaper and more complex solutions. The simplest way to evaluate the
    sentiment is to look for particular keywords. This dictionary-based approach is
    fast and, together with a good stemmer, can immediately mark positive and negative
    documents. On the flip side, it doesn't consider the relationship among terms
    and cannot learn how to weight the different components. For example, *Lovely
    day, bad mood* will result in a neutral (+1, -1), while with a supervised approach
    it's possible to make the model learn that *mood* is very important and *bad mood* will
    normally drive to a negative sentiment. Other approaches (much more complex) are
    based on topic modeling (you can now understand how to apply LSA or LDA to determine
    the underlying topics in terms of positivity or negativity); however, they need
    further steps to use topic-word and topic-document distributions. It can be helpful
    in the real semantics of a comment, where, for example, a positive adjective is
    normally used together with other similar components (like verbs). Say, *Lovely
    hotel, I'm surely coming back*. In this case (if the number of samples is big
    enough), a topic can emerge from the combination of words such as *lovely* or
    *amazing* and (positive) verbs such as *returning* or *coming back*.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的问题通常是监督性的（正如我们即将要做的那样），但也有更便宜且更复杂的解决方案。评估情感的最简单方法就是寻找特定的关键词。这种基于字典的方法速度快，并且与一个好的词干提取器结合使用，可以立即标记出正面和负面的文档。然而，它不考虑术语之间的关系，也不能学习如何权衡不同的组成部分。例如，*美好的一天，糟糕的心情*将导致中性（+1，-1），而使用监督方法，模型可以学习到*mood*非常重要，并且*糟糕的心情*通常会导致负面情感。其他方法（更为复杂）基于主题建模（你现在可以理解如何应用LSA或LDA来确定基于积极或消极的潜在主题）；然而，它们需要进一步步骤来使用主题-词和主题-文档分布。在现实语义中，这可能很有帮助，例如，一个积极的形容词通常与其他类似成分（如动词）一起使用。比如说，*这家酒店很棒，我肯定会再回来*。在这种情况下（如果样本数量足够大），一个主题可以从诸如*lovely*或*amazing*等词语的组合中产生，以及（积极的）动词，如*returning*或*coming
    back*。
- en: An alternative is to consider the topic distribution of positive and negative
    documents and work with a supervised approach in the topic sub-space. Other approaches
    include deep-learning techniques (such as Word2Vec or Doc2Vec) and are based on
    the idea of generating a vectorial space where similar words are close to each
    other, in order to easily manage synonyms. For example, if the training set contains
    the sentence *Lovely hotel* but it doesn't contain *Wonderful hotel*, a Word2Vec
    model can learn from other examples that *lovely* and *wonderful* are very close;
    therefore the new document *Wonderful hotel* is immediately classified using the
    knowledge provided by the first comment. An introduction to this technique, together
    with some technical papers, can be found at [https://code.google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now consider our example, which is based on a subset of the *Twitter
    Sentiment Analysis Training Corpus* dataset. In order to speed up the process,
    we have limited the experiment to 1,00,000 tweets. After downloading the file
    (see the box at the end of this paragraph), it''s necessary to parse it (using
    the UTF-8 encoding):'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The `dataset` variable must contain the full path to the CSV file. This procedure
    reads all the lines skipping the first one (which is the header), and stores each
    tweet as a new list entry in the `corpus` variable, and the corresponding sentiment
    (which is binary, 0 or 1) in the `labels` variable. At this point, we proceed
    as usual, tokenizing, vectorizing, and preparing the training and test sets:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We have chosen to include dots together with letters and numbers in the `RegexpTokenizer`
    instance because they are useful for expressing particular emotions. Moreover,
    the n-gram range has been set to (1, 2), so we include bigrams (the reader can
    try with trigrams too). At this point, we can train a random forest:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now we can produce some metrics to evaluate the model:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The performances are not excellent (it''s possible to achieve better accuracies
    using Word2Vec); however, they are acceptable for many tasks. In particular, a
    78% recall means that the number of false negatives is about 20% and it can be
    useful when using sentiment analysis for an automatic processing task (in many
    cases, the risk threshold to auto-publish a negative review is quite a bit lower,
    and, therefore, a better solution must be employed). The performances can be also
    confirmed by the corresponding ROC curve:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d74b2df8-0327-4453-bd56-9633db59673e.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
- en: The *Twitter Sentiment Analysis Training Corpus* dataset (as a CSV file) used
    in the example can be downloaded from [http://thinknook.com/wp-content/uploads/2012/09/Sentiment-Analysis-Dataset.zip](http://thinknook.com/wp-content/uploads/2012/09/Sentiment-Analysis-Dataset.zip).
    Considering the amount of data, the training process can be very long (even taking
    hours on slower machines).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: VADER sentiment analysis with NLTK
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the English language, NLTK provides an already trained model called **VADER**
    (**Valence Aware Dictionary and sEntiment Reasoner**) that works in a slightly
    different way and adopts a rule engine together with a lexicon to infer the sentiment
    intensity of a piece of text. More information and details can be found in Hutto
    C.J., Gilbert E., *VADER: A Parsimonious Rule-based Model for Sentiment Analysis
    of Social Media Text*, AAAI, 2014*.*'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'The NLTK version uses the `SentimentIntensityAnalyzer` class and can immediately
    be used to have a polarity sentiment measure made up of four components:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Positive factor
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Negative factor
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neutral factor
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compound factor
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first three don''t need any explanation, while the last one is a particular
    measure (a normalized overall score), which is computed as:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2db0e565-d2f1-453e-a70d-698edba300ee.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
- en: 'Here, *Sentiment(w[i])* is the score valence of the word *w[i]* and alpha is
    a normalization coefficient that should approximate the maximum expected value
    (the default value set in NLTK is 15). The usage of this class is immediate, as
    the following snippet can confirm:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The NLTK Vader implementation uses the library Twython for some functionalities.
    Even though it's not necessary, in order to avoid a warning, it's possible to
    install it using pip (`pip install twython`).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hofmann T., *Unsupervised Learning by Probabilistic Latent Semantic Analysis*,
    Machine Learning 42, 177-196, 2001, Kluwer Academic Publishers.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blei D., Ng A., Jordan M., *Latent Dirichlet Allocation, Journal of Machine
    Learning Research*, 3, (2003) 993-1022.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hutto C.J., Gilbert E., *VADER: A Parsimonious Rule-based Model for Sentiment
    Analysis of Social Media Text*, AAAI, 2014.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced topic modeling. We discussed latent semantic
    analysis based on truncated SVD, probabilistic latent semantic analysis (which
    aims to build a model without assumptions about latent factor prior probabilities),
    and latent Dirichlet allocation, which outperformed the previous method and is
    based on the assumption that the latent factor has a sparse prior Dirichlet distribution.
    This means that a document normally covers only a limited number of topics and
    a topic is characterized only by a few important words.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: In the last section, we discussed sentiment analysis of documents, which is
    aimed at determining whether a piece of text expresses a positive or negative
    feeling. In order to show a feasible solution, we built a classifier based on
    an NLP pipeline and a random forest with average performances that can be used
    in many real-life situations.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we're going to briefly introduce deep learning, together
    with the TensorFlow framework. As this topic alone requires a dedicated book,
    our goal is to define the main concepts with some practical examples. If the reader
    wants to have further information, at the end of the chapter, a complete reference
    list will be provided.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
