- en: Topic Modeling and Sentiment Analysis in NLP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理中的主题建模和情感分析
- en: In this chapter, we're going to introduce some common topic modeling methods,
    discussing some applications. Topic modeling is a very important NLP section and
    its purpose is to extract semantic pieces of information out of a corpus of documents.
    We're going to discuss **latent semantic analysis**, one of most famous methods;
    it's based on the same philosophy already discussed for model-based recommendation
    systems. We'll also discuss its probabilistic variant, PLSA, which is aimed at
    building a latent factor probability model without any assumption of prior distributions.
    On the other hand, the **Latent Dirichlet Allocation** is a similar approach that
    assumes a prior Dirichlet distribution for latent variables. In the last section,
    we're going to discuss sentiment analysis with a concrete example based on a Twitter
    dataset.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍一些常见的主题建模方法，并讨论一些应用。主题建模是自然语言处理的一个重要部分，其目的是从文档语料库中提取语义信息。我们将讨论**潜在语义分析**，这是最著名的方法之一；它基于已经讨论过的基于模型的推荐系统的相同哲学。我们还将讨论其概率变体PLSA，它旨在构建一个没有先验分布假设的潜在因子概率模型。另一方面，**潜在狄利克雷分配**是一种类似的方法，它假设潜在变量具有先验狄利克雷分布。在最后一节中，我们将通过基于Twitter数据集的具体示例来讨论情感分析。
- en: Topic modeling
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主题建模
- en: The main goal of topic modeling in natural language processing is to analyze
    a corpus in order to identify common topics among documents. In this context,
    even if we talk about semantics, this concept has a particular meaning, driven
    by a very important assumption. A topic derives from the usage of particular terms
    in the same document and it is confirmed by the multiplicity of different documents
    where the first condition is true.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理中主题建模的主要目标是分析语料库，以识别文档之间的共同主题。在这种情况下，即使我们谈论语义，这个概念也有一个特定的含义，它是由一个非常重要的假设驱动的。一个主题来源于同一文档中特定术语的使用，并且通过多个不同文档中第一个条件成立来得到证实。
- en: 'In other words, we don''t consider a human-oriented semantics but a statistical
    modeling that works with meaningful documents (this guarantees that the usage
    of terms is aimed to express a particular concept and, therefore, there''s a human
    semantic purpose behind them). For this reason, the starting point of all our
    methods is an occurrence matrix, normally defined as a document-term matrix (we
    have already discussed count vectorizing and tf-idf in [Chapter 12](d276f3b9-d2e3-4b98-b144-645f7ba08d36.xhtml),
    *Introduction to NLP*):'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们不考虑面向人类的语义，而是一种与有意义的文档一起工作的统计模型（这保证了术语的使用旨在表达特定的概念，因此，它们背后有人的语义目的）。因此，我们所有方法的起点都是一个发生矩阵，通常定义为文档-词矩阵（我们已经在[第12章](d276f3b9-d2e3-4b98-b144-645f7ba08d36.xhtml)，《自然语言处理导论》中讨论了计数向量化tf-idf）：
- en: '![](img/63977317-5b6c-4654-8148-96aef3833a58.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/63977317-5b6c-4654-8148-96aef3833a58.png)'
- en: In many papers, this matrix is transposed (it's a term-document one); however,
    scikit-learn produces document-term matrices, and, to avoid confusion, we are
    going to consider this structure.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多论文中，这个矩阵是转置的（它是一个词-文档矩阵）；然而，scikit-learn生成文档-词矩阵，为了避免混淆，我们将考虑这种结构。
- en: Latent semantic analysis
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 潜在语义分析
- en: 'The idea behind latent semantic analysis is factorizing *M[dw]* so as to extract
    a set of latent variables (this means that we can assume their existence but they
    cannot be observed directly) that work as connectors between the document and
    terms. As discussed in [Chapter 11](9c4b1425-663c-4995-8faf-c0823d854ef1.xhtml), *Introduction
    to Recommendation Systems*, a very common decomposition method is SVD:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在语义分析背后的思想是将*M[dw]*分解，以提取一组潜在变量（这意味着我们可以假设它们的存在，但它们不能直接观察到）。正如在[第11章](9c4b1425-663c-4995-8faf-c0823d854ef1.xhtml)，《推荐系统导论》中讨论的那样，一个非常常见的分解方法是SVD：
- en: '![](img/804f7c06-5366-4973-bd3f-42eb8fe4d3fb.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/804f7c06-5366-4973-bd3f-42eb8fe4d3fb.png)'
- en: 'However, we''re not interested in a full decomposition; we are interested only
    in the subspace defined by the top *k* singular values:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们并不对完全分解感兴趣；我们只对由前*k*个奇异值定义的子空间感兴趣：
- en: '![](img/76837ca8-9a75-4c09-ae12-098c4a101123.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/76837ca8-9a75-4c09-ae12-098c4a101123.png)'
- en: 'This approximation has the reputation of being the best one considering the
    Frobenius norm, so it guarantees a very high level of accuracy. When applying
    it to a document-term matrix, we obtain the following decomposition:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这个近似在考虑Frobenius范数的情况下享有最佳声誉，因此它保证了非常高的精度。当将其应用于文档-词矩阵时，我们得到以下分解：
- en: '![](img/ddb9caa0-938b-4570-8aaa-06399d54d44e.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ddb9caa0-938b-4570-8aaa-06399d54d44e.png)'
- en: 'Or, in a more compact way:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，以更紧凑的方式：
- en: '![](img/6376620d-c7ab-44a5-91e2-3ad7caf2f569.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6376620d-c7ab-44a5-91e2-3ad7caf2f569.png)'
- en: 'Here, the first matrix defines a relationship among documents and *k* latent
    variables, and the second a relationship among *k* latent variables and words.
    Considering the structure of the original matrix and what is explained at the
    beginning of this chapter, we can consider the latent variables as **topics** that
    define a subspace where the documents are projected. A generic document can now
    be defined as:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，第一个矩阵定义了文档和 *k* 个潜在变量之间的关系，第二个则定义了 *k* 个潜在变量和单词之间的关系。考虑到原始矩阵的结构以及本章开头所解释的内容，我们可以将潜在变量视为**主题**，它们定义了一个子空间，文档被投影到这个子空间中。现在，一个通用的文档可以这样定义：
- en: '![](img/beda378a-ddfc-428e-a7e3-088a9dd30cdc.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/beda378a-ddfc-428e-a7e3-088a9dd30cdc.png)'
- en: 'Furthermore, each topic becomes a linear combination of words. As the weight
    of many words is close to zero, we can decide to take only the top *r* words to
    define a topic; therefore, we get:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，每个主题都成为单词的线性组合。由于许多单词的权重接近于零，我们可以决定只取前 *r* 个单词来定义一个主题；因此，我们得到：
- en: '![](img/8a768187-1982-4cfe-ae6a-1beb2f94e2bb.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8a768187-1982-4cfe-ae6a-1beb2f94e2bb.png)'
- en: 'Here, each *h[ji]* is obtained after sorting the columns of *M[twk]*. To better
    understand the process, let''s show a complete example based on a subset of Brown
    corpus (500 documents from the `news` category):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，每个 *h[ji]* 都是在对 *M[twk]* 的列进行排序后得到的。为了更好地理解这个过程，让我们基于布朗语料库的一个子集（来自 `news`
    类别的 500 篇文档）展示一个完整的示例：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After defining the corpus, we need to tokenize and vectorize using a tf-idf
    approach:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义语料库之后，我们需要使用 tf-idf 方法进行分词和向量化：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now it''s possible to apply an SVD to the `Xc` matrix (remember that in SciPy,
    the `V` matrix is already transposed):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以对 `Xc` 矩阵应用 SVD（记住在 SciPy 中，`V` 矩阵已经转置）：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As the corpus is not very small, it''s useful to set the parameter `full_matrices=False` to
    save computational time. We assume we have two topics, so we can extract our sub-matrices:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于语料库不是非常小，设置参数 `full_matrices=False` 以节省计算时间是很有用的。我们假设有两个主题，因此我们可以提取我们的子矩阵：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If we want to analyze the top 10 words per topic, we need to consider that:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要分析每个主题的前 10 个单词，我们需要考虑以下几点：
- en: '![](img/a1f988ce-2dc8-4c50-b703-e004e3bcec7d.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a1f988ce-2dc8-4c50-b703-e004e3bcec7d.png)'
- en: 'Therefore, we can obtain the most significant words per topic after sorting
    the matrix using the `get_feature_names()` method provided by the vectorizers:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以通过使用向量器提供的 `get_feature_names()` 方法对矩阵进行排序后，获得每个主题的最显著单词：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In this case, we''re considering only non-negative values in the matrix `Vk`;
    however, as a topic is a mixture of words, the negative components should also be
    taken into account. In this case, we need to sort the absolute values of `Vk`:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们只考虑矩阵 `Vk` 中的非负值；然而，由于主题是单词的混合，负成分也应该被考虑。在这种情况下，我们需要对 `Vk` 的绝对值进行排序：
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If we want to analyze how a document is represented in this sub-space, we must
    use:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要分析一个文档在这个子空间中的表示，我们必须使用：
- en: '![](img/3dc27e85-a78a-4495-887f-69f2ada30f94.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3dc27e85-a78a-4495-887f-69f2ada30f94.png)'
- en: 'Let''s consider, for example, the first document of our corpus:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们考虑我们语料库的第一个文档：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As we are working in a bidimensional space, it''s interesting to plot all the
    points corresponding to each document:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在处理一个二维空间，绘制每个文档对应的所有点是有趣的：
- en: '![](img/22cc8148-a648-41a4-9d21-35bcc85deb1e.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/22cc8148-a648-41a4-9d21-35bcc85deb1e.png)'
- en: 'In the previous figure, we can see that many documents are correlated, with
    a small group of outliers. This is probably due to the fact that our choice of
    two topics is restrictive. If we repeat the same experiment using two Brown corpus
    categories (`news` and `fiction`), we observe a different behavior:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到许多文档是相关的，有一个小的异常值组。这可能是由于我们选择两个主题的限制性。如果我们使用布朗语料库的两个类别（`news` 和
    `fiction`）重复相同的实验，我们会观察到不同的行为：
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'I don''t repeat the remaining calculations because they are similar. (The only
    difference is that our corpus is now quite bigger and this leads to a longer computational
    time. For this reason, we''re going to discuss an alternative, which is much faster.)
    Plotting the points corresponding to the documents, we now get:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我不再重复剩余的计算，因为它们是相似的。（唯一的区别是，我们的语料库现在相当大，这导致计算时间更长。因此，我们将讨论一个替代方案，它要快得多。）绘制文档对应的点，我们现在得到：
- en: '![](img/2b2c24ea-f6a0-4df6-aab5-6b57d85ccfb6.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2b2c24ea-f6a0-4df6-aab5-6b57d85ccfb6.png)'
- en: Now it's easier to distinguish two groups, which are almost orthogonal (meaning
    that many documents belong to only one category). I suggest repeating this experiment
    with different corpora and ranks. Unfortunately, it's impossible to plot more
    than three dimensions, but it's always possible to check whether the sub-space
    describes the underlying semantics correctly using only numerical computations.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在更容易区分两组，它们几乎是正交的（这意味着许多文档只属于一个类别）。我建议使用不同的语料库和秩重复这个实验。不幸的是，不可能绘制超过三个维度，但总是可以使用仅数值计算来检查子空间是否正确描述了潜在的语义。
- en: 'As anticipated, the standard SciPy SVD implementation can be really slow when
    the occurrence matrix is huge; however, scikit-learn provides a truncated SVD
    implementation, `TruncatedSVD`, that works only with the sub-space. The result
    is much faster (it can directly manage sparse matrices too). Let''s repeat the
    previous experiments (with a complete corpus) using this class:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，当发生矩阵很大时，标准的SciPy SVD实现可能会非常慢；然而，scikit-learn提供了一个截断SVD实现，`TruncatedSVD`，它仅与子空间一起工作。结果是速度更快（它还可以直接管理稀疏矩阵）。让我们使用这个类重复之前的实验（使用完整的语料库）：
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Through the `n_components` parameter, it''s possible to set the desired rank,
    discarding the remaining parts of the matrices. After fitting the model, we get
    the document-topic matrix *M[dtk]* directly as the output of the method `fit_transform()`,
    while the topic-word matrix *M[twk]* can be accessed using the instance variable
    `components_`:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`n_components`参数，可以设置所需的秩，丢弃矩阵的其余部分。在拟合模型后，我们可以直接将文档-主题矩阵*M[dtk]*作为`fit_transform()`方法的输出获得，而主题-单词矩阵*M[twk]*可以通过实例变量`components_`访问：
- en: '[PRE9]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The reader can verify how much faster this process can be; therefore, I suggest
    using a standard SVD implementation only when it''s needed to have access to the
    full matrices. Unfortunately, as is also written in the documentation, this method
    is very sensitive to the algorithm and the random state. It also suffers from
    a phenomenon called **sign indeterminacy**, which means that the signs of all
    components can change if a different random seed is used. I suggest you declare:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 读者可以验证这个过程可以有多快；因此，我建议只有在需要访问完整矩阵时才使用标准的SVD实现。不幸的是，正如文档中所述，这种方法对算法和随机状态非常敏感。它还受到称为**符号不确定性**的现象的影响，这意味着如果使用不同的随机种子，所有组件的符号都可能改变。我建议你声明：
- en: '[PRE10]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Do this with a fixed seed at the beginning of every file (even Jupyter notebooks)
    to be sure that it's possible to repeat the calculations and always obtain the
    same result.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个文件的开始处使用固定的种子（即使是Jupyter笔记本）以确保可以重复计算并始终获得相同的结果。
- en: Moreover, I advise repeating this experiment using **non-negative matrix factorization**,
    as described in [Chapter 3](7323017f-66c2-4b18-af1e-5d4dc28f9031.xhtml), *Feature
    Selection and Feature Engineering*.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我建议使用**非负矩阵分解**重复这个实验，如第3章所述，*特征选择与特征工程*。
- en: Probabilistic latent semantic analysis
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率潜在语义分析
- en: 'The previous model was based on a deterministic approach, but it''s also possible
    to define a probabilistic model over the space determined by documents and words.
    In this case, we''re not making any assumption about Apriori probabilities (this
    will be done in the next approach), and we''re going to determine the parameters
    that maximize the log-likelihood of our model. In particular, consider the plate
    notation (if you want to know more about this technique, read [https://en.wikipedia.org/wiki/Plate_notation](https://en.wikipedia.org/wiki/Plate_notation))
    shown in the following figure:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的模型是基于确定性方法，但也可以在由文档和单词确定的范围内定义一个概率模型。在这种情况下，我们不对Apriori概率做出任何假设（这将在下一个方法中完成），我们将确定最大化我们模型对数似然参数的参数。特别是，考虑以下图中所示的板符号（如果你想了解更多关于这种技术的信息，请阅读[https://en.wikipedia.org/wiki/Plate_notation](https://en.wikipedia.org/wiki/Plate_notation)）：
- en: '![](img/4f6fddca-41f4-419f-8b5d-d88135beab30.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4f6fddca-41f4-419f-8b5d-d88135beab30.png)'
- en: We assume we have a corpus of *m* documents and each of them is composed of
    *n* words (both elements are observed and therefore represented as gray circles);
    however, we also assume the presence of a limited set of *k* common latent factors
    (topics) that link a document with a group of words (as they are not observed,
    the circle is white). As already written, we cannot observe them directly, but
    we're allowed to assume their existence.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设我们有一个包含 *m* 个文档的语料库，并且每个文档由 *n* 个单词组成（这两个元素都是观察到的，因此用灰色圆圈表示）；然而，我们还假设存在一组有限的
    *k* 个共同潜在因子（主题），它们将文档与一组单词联系起来（由于它们没有被观察到，圆圈是白色的）。正如已经写过的，我们无法直接观察到它们，但我们允许假设它们的存在。
- en: 'The joint probability to find a document with a particular word is:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 找到具有特定单词的文档的联合概率是：
- en: '![](img/bf99ba35-7325-46ac-9759-c0226c94c541.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bf99ba35-7325-46ac-9759-c0226c94c541.png)'
- en: 'Therefore, after introducing the latent factors, the conditional probability
    to find a word in a specific document can be written as:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在引入潜在因子后，找到特定文档中单词的条件概率可以写成：
- en: '![](img/5f8ece24-825d-4ef1-9491-d0c10caaf96c.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5f8ece24-825d-4ef1-9491-d0c10caaf96c.png)'
- en: 'The initial joint probability *P(d, w)* can be also expressed using the latent
    factors:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 初始联合概率 *P(d, w)* 也可以用潜在因子表示：
- en: '![](img/4f5761ae-c0d7-42b5-ba13-add58068af28.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4f5761ae-c0d7-42b5-ba13-add58068af28.png)'
- en: This includes the prior probability *P(t)*. As we don't want to work with it,
    it's preferable to use the expression *P(w|d)*. To determine the two conditional
    probability distributions, a common approach is the **expectation-maximization**
    (**EM**) strategy. A full description can be found in Hofmann T., *Unsupervised
    Learning by Probabilistic Latent Semantic Analysis*, Machine Learning 42, 177-196,
    2001, Kluwer Academic Publishers*. *In this context, we show only the final results
    without any proof.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括先验概率 *P(t)*。由于我们不想处理它，因此使用表达式 *P(w|d)* 更为可取。为了确定两个条件概率分布，一个常见的方法是 **期望最大化**（**EM**）策略。完整的描述可以在Hofmann
    T.的《基于概率潜在语义分析的无监督学习》，机器学习42，177-196，2001，Kluwer学术出版社中找到。*在此上下文中，我们只展示最终结果，而不提供任何证明。
- en: 'The log-likelihood can be written as:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对数似然可以写成：
- en: '![](img/8006da53-20d2-4e60-ae2e-0fdcac43e2e9.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8006da53-20d2-4e60-ae2e-0fdcac43e2e9.png)'
- en: 'Which becomes:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 变成：
- en: '![](img/d7bcae69-f621-4ce8-8774-0e4120b69411.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d7bcae69-f621-4ce8-8774-0e4120b69411.png)'
- en: '*M[dw]* is an occurrence matrix (normally obtained with a count vectorizer)
    and *M[dw](d, w)* is the frequency of the word *w* in document *d*. For simplicity,
    we are going to approximate it by excluding the first term (which doesn''t depend
    on *t[k]*):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*M[dw]* 是一个出现矩阵（通常通过计数向量器获得），而 *M[dw](d, w)* 是单词 *w* 在文档 *d* 中的频率。为了简化，我们将通过排除第一个项（它不依赖于
    *t[k]*）来近似它：'
- en: '![](img/00ab1edb-b077-4522-999a-e6bec52b90fb.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00ab1edb-b077-4522-999a-e6bec52b90fb.png)'
- en: 'Moreover, it''s useful to introduce the conditional probability *P(t|d,w)*,
    which is the probability of a topic given a document and a word. The EM algorithm
    maximizes the expected complete log-likelihood under the posterior probability *P(t|d,w)*:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，引入条件概率 *P(t|d,w)* 也是有用的，它是给定文档和单词的主题概率。EM算法在后验概率 *P(t|d,w)* 下最大化期望的完整对数似然：
- en: '![](img/47ae99fd-dbcf-462e-9d4d-fed24934c75b.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/47ae99fd-dbcf-462e-9d4d-fed24934c75b.png)'
- en: 'The **E** phase of the algorithm can be expressed as:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的 **E** 阶段可以表示为：
- en: '![](img/9b106abc-630e-42f9-a621-29265ad5e83e.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9b106abc-630e-42f9-a621-29265ad5e83e.png)'
- en: It must be extended to all topics, words, and documents and must be normalized
    with the sum per topic in order to always have consistent probabilities.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 必须扩展到所有主题、单词和文档，并且必须按主题求和进行归一化，以确保始终具有一致的概率。
- en: 'The **M** phase is split into two computations:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**M** 阶段分为两个计算：'
- en: '![](img/8ee6088d-59d4-461b-9e79-cbe4a96b7958.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8ee6088d-59d4-461b-9e79-cbe4a96b7958.png)'
- en: Also in this case, the calculations must be extended to all topics, words, and
    documents. But in the first case, we sum by document and normalize by summing
    by word and document, while in the second, we sum by word and normalize by the
    length of the document.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个情况下，计算必须扩展到所有主题、单词和文档。但在第一种情况下，我们是按文档求和并按单词和文档进行归一化，而在第二种情况下，我们是按单词求和并按文档长度进行归一化。
- en: 'The algorithm must be iterated until the log-likelihood stops increasing its
    magnitude. Unfortunately, scikit-learn doesn''t provide a PLSA implementation
    (maybe because the next strategy, LDA, is considered much more powerful and efficient),
    so we need to write some code from scratch. Let''s start by defining a small subset
    of the Brown corpus, taking 10 sentences from the `editorial` category and 10
    from the `fiction` one:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 算法必须迭代，直到对数似然停止增加其幅度。不幸的是，scikit-learn没有提供PLSA实现（也许是因为下一个策略LDA被认为更强大和高效），因此我们需要从头编写一些代码。让我们首先定义布朗语料库的一个小子集，从`editorial`类别中取10个句子，从`fiction`类别中取10个：
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now we can vectorize using the `CountVectorizer` class:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用`CountVectorizer`类进行向量化：
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'At this point, we can define the rank (we choose 2 for simplicity), two constants
    that will be used later, and the matrices to hold the probabilities *P(t|d)*,
    *P(w|t)*, and *P(t|d,w)*:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以定义排名（为了简单起见，我们选择2），两个稍后将要使用的常数，以及用于存储概率 *P(t|d)*，*P(w|t)* 和 *P(t|d,w)*
    的矩阵：
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The two matrices *P(t|d)*, *P(w|t)* must be normalized so as to be coherent
    with the algorithm; the other one is initialized to zero. Now we can define the
    log-likelihood function:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 两个矩阵 *P(t|d)*，*P(w|t)* 必须归一化，以便与算法保持一致；另一个初始化为零。现在我们可以定义对数似然函数：
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'And finally the expectation-maximization functions:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，期望最大化函数：
- en: '[PRE15]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the preceding function, when the normalization factor is 0, the probability
    *P(t|w, d)* is set to 0.0 for each topic:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的函数中，当归一化因子为0时，每个主题的概率 *P(t|w, d)* 被设置为0.0：
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The constants `alpha_1` and `alpha_2` are used when a normalization factor becomes
    0\. In that case, it can be useful to assign the probability a small value; therefore
    we divided the numerator for those constants. I suggest trying with different
    values so as to tune up the algorithm for different tasks.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当归一化因子变为0时，常数 `alpha_1` 和 `alpha_2` 被使用。在这种情况下，分配一个小的概率值可能是有用的；因此，我们为这些常数除以分子。我建议尝试不同的值，以便调整算法以适应不同的任务。
- en: 'At this point, we can try our algorithm with a limited number of iterations:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以尝试我们的算法，限制迭代次数：
- en: '[PRE17]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'It''s possible to verify the convergence after the 30th step. At this point,
    we can check the top five words per topic considering the *P(w|t)* conditional
    distribution sorted in descending mode per topic weight:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在第30步之后，可以验证收敛性。在这个时候，我们可以检查每个主题按主题权重降序排列的 *P(w|t)* 条件分布的前五项单词：
- en: '[PRE18]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Latent Dirichlet Allocation
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 潜在狄利克雷分配
- en: 'In the previous method, we didn''t make any assumptions about the topic prior
    to distribution and this can result in a limitation because the algorithm isn''t
    driven by any real-world intuition. LDA, instead, is based on the idea that a
    topic is characterized by a small ensemble of important words and normally a document
    doesn''t cover many topics. For this reason, the main assumption is that the prior
    topic distribution is a symmetric **Dirichlet **one. The probability density function
    is defined as:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在先前的方法中，我们没有对主题先验分布做出任何假设，这可能导致限制，因为算法没有受到任何现实世界直觉的驱动。相反，LDA基于这样的观点：一个主题由一组重要的单词组成，通常一个文档不会涵盖许多主题。因此，主要假设是先验主题分布是对称的**狄利克雷**分布。概率密度函数定义为：
- en: '![](img/5f47820a-b6e1-4601-8fd1-793649391f08.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5f47820a-b6e1-4601-8fd1-793649391f08.png)'
- en: 'If the concentration parameter alpha is below 1.0, the distribution will be
    sparse as desired. This allows us to model topic-document and topic-word distributions,
    which will always be concentrated on a few values. In this way we can avoid the
    following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果浓度参数alpha小于1.0，分布将是稀疏的，正如所期望的那样。这允许我们模拟主题-文档和主题-单词分布，这些分布将始终集中在少数几个值上。这样我们就可以避免以下情况：
- en: The topic mixture assigned to a document could becoming flat (many topics with
    similar weight)
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分配给文档的主题混合可能会变得平坦（许多主题具有相似权重）
- en: The structure of a topic considering the word ensemble could becoming similar
    to a background (in fact, only a limited number of words must be important; otherwise
    the semantic boundaries fade out).
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑到单词集合，一个主题的结构可能会变得类似于背景（实际上，只有有限数量的单词必须是重要的；否则语义边界会变得模糊）。
- en: 'Using the plate notation, we can represent the relationship among documents,
    topics, and words as shown in the following figure:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 使用板状符号，我们可以表示文档、主题和单词之间的关系，如下面的图所示：
- en: '![](img/2658f9fb-fac1-4aaf-b84c-1a9f2759b2a3.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2658f9fb-fac1-4aaf-b84c-1a9f2759b2a3.png)'
- en: In the previous figure, alpha is the Dirichlet parameter for the topic-document
    distribution, while gamma has the same role for the topic-word distribution. Theta,
    instead, is the topic distribution for a specific document, while beta is the
    topic distribution for a specific word.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，alpha 是主题-文档分布的 Dirichlet 参数，而 gamma 在主题-单词分布中具有相同的作用。Theta 相反，是特定文档的主题分布，而
    beta 是特定单词的主题分布。
- en: 'If we have a corpus of *m* documents and a vocabulary of *n* words (each document
    has *n[i]* words) and we assume to have *k* different topics, the generative algorithm
    can be described with the following steps:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个包含 *m* 个文档和 *n* 个单词（每个文档有 *n[i]* 个单词）的语料库，并且我们假设有 *k* 个不同的主题，生成算法可以用以下步骤描述：
- en: 'For each document, draw a sample (a topic mixture) from the topic-document
    distribution:'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个文档，从主题-文档分布中抽取一个样本（一个主题混合）：
- en: '![](img/d3a4ad0a-5c17-479b-aeb6-9cf7084ab843.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d3a4ad0a-5c17-479b-aeb6-9cf7084ab843.png)'
- en: 'For each topic, draw a sample from the from the topic-word distribution:'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个主题，从主题-单词分布中抽取一个样本：
- en: '![](img/f56a7a90-aecd-472b-8236-c6b732851b3d.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f56a7a90-aecd-472b-8236-c6b732851b3d.png)'
- en: 'Both parameters must be estimated. At this point, considering the occurrence
    matrix *M[dw]* and the notation *z[mn]* to define the topic assigned to the *n*-th
    word in the *m*-th document, we can iterate over documents (index *d*) and words
    (index *w*):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 必须估计两个参数。在此阶段，考虑到发生矩阵 *M[dw]* 和表示 *m*-th 文档中 *n*-th 单词所分配的主题的符号 *z[mn]*，我们可以遍历文档（索引
    *d*）和单词（索引 *w*）：
- en: 'A topic for document *d* and word *w* is chosen according to:'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据文档 *d* 和单词 *w* 选择一个主题：
- en: '![](img/4d93ad6e-8bc5-4974-9ccb-5a178835345d.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4d93ad6e-8bc5-4974-9ccb-5a178835345d.png)'
- en: 'A word is chosen according to:'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据以下标准选择一个单词：
- en: '![](img/b3ab9e42-ce8b-4269-a511-597a753c15fc.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b3ab9e42-ce8b-4269-a511-597a753c15fc.png)'
- en: 'In both cases, a categorical distribution is a one-trial multinomial one. A
    complete description of how the parameters are estimated is quite complex and
    it''s beyond the scope of this book; however, the main problem is finding the
    distribution of latent variables:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，一个分类分布是一个单次多项式分布。参数估计的完整描述相当复杂，超出了本书的范围；然而，主要问题是找到潜在变量的分布：
- en: '![](img/d339ef3f-7607-4030-bc4d-355da52b46e7.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d339ef3f-7607-4030-bc4d-355da52b46e7.png)'
- en: 'The reader can find a lot more information in Blei D., Ng A., Jordan M., *Latent
    Dirichlet Allocation*, Journal of Machine Learning Research, 3, (2003) 993-1022*. *However,
    a very important difference between LDA and PLSA is about the generative ability
    of LDA, which allows working with unseen documents. In fact, the PLSA training
    process finds the optimal parameters *p(t|d)* only for the corpus, while LDA adopts
    random variables. It''s possible to understand this concept by defining the probability
    of theta (a topic mixture) as joint with a set of topics and a set of words, and
    conditioned to the model parameters:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 读者可以在 Blei D.，Ng A.，Jordan M.，*潜在狄利克雷分配*，《机器学习研究杂志》，3，(2003) 993-1022 中找到更多信息。然而，LDA
    和 PLSA 之间一个非常重要的区别是 LDA 的生成能力，它允许处理未见过的文档。实际上，PLSA 训练过程只为语料库找到最优参数 *p(t|d)*，而
    LDA 采用随机变量。可以通过定义 theta（一个主题混合）的概率为与一组主题和一组单词的联合，并给定模型参数来理解这个概念：
- en: '![](img/c5691c82-1945-47db-95cb-fe698757a9a6.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c5691c82-1945-47db-95cb-fe698757a9a6.png)'
- en: 'As shown in the previously mentioned paper, the probability of a document (a
    set of words) conditioned to the model parameters, can be obtained by integration:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述的论文所示，给定模型参数的文档（一组单词）的条件概率可以通过积分获得：
- en: '![](img/32d90868-9ebe-4524-ba35-28207ace2197.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/32d90868-9ebe-4524-ba35-28207ace2197.png)'
- en: This expression shows the difference between PLSA and LDA. Once learned *p(t|d)*,
    PLSA cannot generalize, while LDA, sampling from the random variables, can always
    find a suitable topic mixture for an unseen document.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表达式显示了 PLSA 和 LDA 之间的区别。一旦学习到 *p(t|d)*，PLSA 就无法泛化，而 LDA 通过从随机变量中采样，总能找到一个适合未见文档的合适主题混合。
- en: 'scikit-learn provides a full LDA implementation through the class `LatentDirichletAllocation`.
    We''re going to use it with a bigger dataset (4,000 documents) built from a subset
    of the Brown corpus:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 通过 `LatentDirichletAllocation` 类提供了一个完整的 LDA 实现。我们将使用从布朗语料库的子集构建的更大的数据集（4,000
    个文档）来使用它：
- en: '[PRE19]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now we can vectorize, define, and train our LDA model by assuming that we have
    eight main topics:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过假设我们有八个主要主题来向量化、定义和训练我们的 LDA 模型：
- en: '[PRE20]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In `CountVectorizer`, we added a regular expression to filter the tokens through
    the parameter `token_pattern`. This is useful as we are not using a full tokenizer
    and, in the corpus, there are also many numbers that we want to filter out. The
    class `LatentDirichletAllocation` allows us to specify the learning method (through
    `learning_method`), which can be either batch or online. We have chosen online
    because it's faster; however, both methods adopt variational Bayes to learn the
    parameters. The former adopts the whole dataset, while the latter works with mini-batches.
    The online option will be removed in the 0.20 release; therefore, you can see
    a deprecation warning when using it now. Both theta and beta Dirichlet parameters
    can be specified through `doc_topic_prior` (theta) and `topic_word_prior` (beta).
    The default value (adopted by us too) is 1.0 / `n_topics` . It's important to
    keep both values small and, in particular, less than 1.0 in order to encourage
    sparseness. The maximum number of iterations (`max_iter`) and other learning-related
    parameters can be applied by reading the built-in documentation or visiting [http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在`CountVectorizer`中，我们通过参数`token_pattern`添加了一个正则表达式来过滤标记。这很有用，因为我们没有使用完整的分词器，在语料库中也有许多我们想要过滤掉的数字。`LatentDirichletAllocation`类允许我们指定学习方法（通过`learning_method`），可以是批处理或在线。我们选择了在线，因为它更快；然而，两种方法都采用变分贝叶斯来学习参数。前者采用整个数据集，而后者使用小批量。在线选项将在0.20版本中删除；因此，现在使用它时，您可能会看到弃用警告。theta和beta
    Dirichlet参数可以通过`doc_topic_prior`（theta）和`topic_word_prior`（beta）来指定。默认值（我们也是如此）是`1.0
    / n_topics`。保持这两个值都很小，特别是小于1.0，以鼓励稀疏性。最大迭代次数(`max_iter`)和其他学习相关参数可以通过阅读内置文档或访问[http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)来应用。
- en: 'Now we can test our model by extracting the top five keywords per topic. Just
    like `TruncatedSVD`, the topic-word distribution results are stored in the instance
    variable `components_`:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过提取每个主题的前五个关键词来测试我们的模型。就像`TruncatedSVD`一样，主题-词分布结果存储在实例变量`components_`中：
- en: '[PRE21]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: There are some repetitions, probably due to the composition of some topics,
    and the reader can try different prior parameters to observe the changes. It's
    possible to do an experiment to check whether the model works correctly.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 存在一些重复，这可能是由于某些主题的组成，读者可以尝试不同的先验参数来观察变化。可以进行实验来检查模型是否工作正确。
- en: 'Let''s consider two documents:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑两份文档：
- en: '[PRE22]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'They are quite different and so are their topic distributions:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 它们相当不同，它们的主题分布也是如此：
- en: '[PRE23]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We have a dominant topic `(0.85t[0])` for the first document and a mixture
    `(0.22t[0] + 0.22t[2 ]+ 0.42t[7])` for the second one. Now let''s consider the
    concatenation of both documents:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一份文档，我们有一个主导主题`(0.85t[0])`，而对于第二份文档，有一个混合`(0.22t[0] + 0.22t[2 ]+ 0.42t[7])`。现在让我们考虑这两份文档的拼接：
- en: '[PRE24]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In the resulting document, as expected, the mixture has changed: `0.61t[0] +
    0.11t[2] + 0.21t[7]`. In other words, the algorithm introduced the previously
    dominant topic 5 (which is now stronger) by weakening both topic 2 and topic 7\.
    This is reasonable, because the length of the first document is less than the
    second one, and therefore topic 5 cannot completely cancel the other topics out.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成的文档中，正如预期的那样，混合比例已经发生了变化：`0.61t[0] + 0.11t[2] + 0.21t[7]`。换句话说，算法通过削弱主题2和主题7，引入了之前占主导地位的主题5（现在变得更加强大）。这是合理的，因为第一份文档的长度小于第二份，因此主题5不能完全抵消其他主题。
- en: Sentiment analysis
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 情感分析
- en: One the most widespread applications of NLP is sentiment analysis of short texts
    (tweets, posts, comments, reviews, and so on). From a marketing viewpoint, it's
    very important to understand the semantics of these pieces of information in terms
    of the sentiment expressed. As you can understand, this task can be very easy
    when the comment is precise and contains only a set of positive/negative words,
    but it becomes more complex when in the same sentence there are different propositions
    that can conflict with each other. For example, *I loved that hotel. It was a
    wonderful experience* is clearly a positive comment, while *The hotel is good,
    however, the restaurant was bad and, even if the waiters were kind, I had to fight
    with a receptionist to have another pillow*. In this case, the situation is more
    difficult to manage, because there are both positive and negative elements, resulting
    in a neutral review. For this reason, many applications aren't based on a binary
    decision but admit intermediate levels (at least one to express the neutrality).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）最广泛的应用之一是对短文本（推文、帖子、评论、评论等）的情感分析。从市场营销的角度来看，理解这些信息片段所表达的情感语义非常重要。正如你可以理解的，当评论精确且只包含一组正面/负面词汇时，这项任务可以非常简单，但当同一句子中有可能相互冲突的不同命题时，它就变得更加复杂。例如，*我喜欢那家酒店。那是一次美妙的经历*显然是一个积极的评论，而*这家酒店不错，然而，餐厅很糟糕，即使服务员很友好，我不得不与前台服务员争论再要一个枕头*。在这种情况下，情况更加难以管理，因为既有积极的也有消极的元素，导致一个中性的评论。因此，许多应用不是基于二元决策，而是承认中间级别（至少一个来表达中立）。
- en: These kind of problems are normally supervised (as we're going to do), but there
    are also cheaper and more complex solutions. The simplest way to evaluate the
    sentiment is to look for particular keywords. This dictionary-based approach is
    fast and, together with a good stemmer, can immediately mark positive and negative
    documents. On the flip side, it doesn't consider the relationship among terms
    and cannot learn how to weight the different components. For example, *Lovely
    day, bad mood* will result in a neutral (+1, -1), while with a supervised approach
    it's possible to make the model learn that *mood* is very important and *bad mood* will
    normally drive to a negative sentiment. Other approaches (much more complex) are
    based on topic modeling (you can now understand how to apply LSA or LDA to determine
    the underlying topics in terms of positivity or negativity); however, they need
    further steps to use topic-word and topic-document distributions. It can be helpful
    in the real semantics of a comment, where, for example, a positive adjective is
    normally used together with other similar components (like verbs). Say, *Lovely
    hotel, I'm surely coming back*. In this case (if the number of samples is big
    enough), a topic can emerge from the combination of words such as *lovely* or
    *amazing* and (positive) verbs such as *returning* or *coming back*.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的问题通常是监督性的（正如我们即将要做的那样），但也有更便宜且更复杂的解决方案。评估情感的最简单方法就是寻找特定的关键词。这种基于字典的方法速度快，并且与一个好的词干提取器结合使用，可以立即标记出正面和负面的文档。然而，它不考虑术语之间的关系，也不能学习如何权衡不同的组成部分。例如，*美好的一天，糟糕的心情*将导致中性（+1，-1），而使用监督方法，模型可以学习到*mood*非常重要，并且*糟糕的心情*通常会导致负面情感。其他方法（更为复杂）基于主题建模（你现在可以理解如何应用LSA或LDA来确定基于积极或消极的潜在主题）；然而，它们需要进一步步骤来使用主题-词和主题-文档分布。在现实语义中，这可能很有帮助，例如，一个积极的形容词通常与其他类似成分（如动词）一起使用。比如说，*这家酒店很棒，我肯定会再回来*。在这种情况下（如果样本数量足够大），一个主题可以从诸如*lovely*或*amazing*等词语的组合中产生，以及（积极的）动词，如*returning*或*coming
    back*。
- en: An alternative is to consider the topic distribution of positive and negative
    documents and work with a supervised approach in the topic sub-space. Other approaches
    include deep-learning techniques (such as Word2Vec or Doc2Vec) and are based on
    the idea of generating a vectorial space where similar words are close to each
    other, in order to easily manage synonyms. For example, if the training set contains
    the sentence *Lovely hotel* but it doesn't contain *Wonderful hotel*, a Word2Vec
    model can learn from other examples that *lovely* and *wonderful* are very close;
    therefore the new document *Wonderful hotel* is immediately classified using the
    knowledge provided by the first comment. An introduction to this technique, together
    with some technical papers, can be found at [https://code.google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是考虑正负文档的主题分布，并在主题子空间中使用监督方法。其他方法包括深度学习技术（如Word2Vec或Doc2Vec），其基于生成一个向量空间，其中相似的词彼此靠近，以便容易管理同义词。例如，如果训练集包含句子*Lovely
    hotel*，但它不包含*Wonderful hotel*，Word2Vec模型可以从其他示例中学习到*lovely*和*wonderful*非常接近；因此，新的文档*Wonderful
    hotel*可以立即使用第一评论提供的信息进行分类。关于这项技术的介绍和一些技术论文可以在[https://code.google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/)找到。
- en: 'Let''s now consider our example, which is based on a subset of the *Twitter
    Sentiment Analysis Training Corpus* dataset. In order to speed up the process,
    we have limited the experiment to 1,00,000 tweets. After downloading the file
    (see the box at the end of this paragraph), it''s necessary to parse it (using
    the UTF-8 encoding):'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑我们的例子，它基于*Twitter Sentiment Analysis Training Corpus*数据集的一个子集。为了加快过程，我们将实验限制在100,000条推文。下载文件后（见本段末尾的框），需要解析它（使用UTF-8编码）：
- en: '[PRE25]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The `dataset` variable must contain the full path to the CSV file. This procedure
    reads all the lines skipping the first one (which is the header), and stores each
    tweet as a new list entry in the `corpus` variable, and the corresponding sentiment
    (which is binary, 0 or 1) in the `labels` variable. At this point, we proceed
    as usual, tokenizing, vectorizing, and preparing the training and test sets:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`dataset`变量必须包含CSV文件的完整路径。此过程读取所有行，跳过第一行（它是标题），并将每条推文作为新的列表条目存储在`corpus`变量中，相应的情感（二进制，0或1）存储在`labels`变量中。在这个阶段，我们像往常一样进行，标记化、向量化，并准备训练集和测试集：'
- en: '[PRE26]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We have chosen to include dots together with letters and numbers in the `RegexpTokenizer`
    instance because they are useful for expressing particular emotions. Moreover,
    the n-gram range has been set to (1, 2), so we include bigrams (the reader can
    try with trigrams too). At this point, we can train a random forest:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择在`RegexpTokenizer`实例中包括点和字母数字，因为它们对于表达特定情绪很有用。此外，n-gram的范围已被设置为（1，2），因此我们包括了二元组（读者也可以尝试三元组）。在这个阶段，我们可以训练一个随机森林：
- en: '[PRE27]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now we can produce some metrics to evaluate the model:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以生成一些指标来评估模型：
- en: '[PRE28]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The performances are not excellent (it''s possible to achieve better accuracies
    using Word2Vec); however, they are acceptable for many tasks. In particular, a
    78% recall means that the number of false negatives is about 20% and it can be
    useful when using sentiment analysis for an automatic processing task (in many
    cases, the risk threshold to auto-publish a negative review is quite a bit lower,
    and, therefore, a better solution must be employed). The performances can be also
    confirmed by the corresponding ROC curve:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 性能并不出色（使用Word2Vec可以实现更好的准确率）；然而，对于许多任务来说是可以接受的。特别是，78%的召回率意味着错误负例的数量大约是20%，当使用情感分析进行自动处理任务时可能很有用（在许多情况下，自动发布负面评论的风险阈值相当低，因此必须采用更好的解决方案）。性能也可以通过相应的ROC曲线来确认：
- en: '![](img/d74b2df8-0327-4453-bd56-9633db59673e.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d74b2df8-0327-4453-bd56-9633db59673e.png)'
- en: The *Twitter Sentiment Analysis Training Corpus* dataset (as a CSV file) used
    in the example can be downloaded from [http://thinknook.com/wp-content/uploads/2012/09/Sentiment-Analysis-Dataset.zip](http://thinknook.com/wp-content/uploads/2012/09/Sentiment-Analysis-Dataset.zip).
    Considering the amount of data, the training process can be very long (even taking
    hours on slower machines).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 示例中使用的*Twitter Sentiment Analysis Training Corpus*数据集（CSV文件格式）可以从[http://thinknook.com/wp-content/uploads/2012/09/Sentiment-Analysis-Dataset.zip](http://thinknook.com/wp-content/uploads/2012/09/Sentiment-Analysis-Dataset.zip)下载。考虑到数据量，训练过程可能非常长（甚至在较慢的机器上可能需要数小时）。
- en: VADER sentiment analysis with NLTK
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: VADER情感分析与NLTK
- en: 'For the English language, NLTK provides an already trained model called **VADER**
    (**Valence Aware Dictionary and sEntiment Reasoner**) that works in a slightly
    different way and adopts a rule engine together with a lexicon to infer the sentiment
    intensity of a piece of text. More information and details can be found in Hutto
    C.J., Gilbert E., *VADER: A Parsimonious Rule-based Model for Sentiment Analysis
    of Social Media Text*, AAAI, 2014*.*'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '对于英语语言，NLTK提供了一个已经训练好的模型，称为 **VADER**（**Valence Aware Dictionary and sEntiment
    Reasoner**），它以略不同的方式工作，并采用规则引擎与词典一起推断文本的情感强度。更多信息和细节可以在 Hutto C.J.，Gilbert E.，*VADER:
    A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text*，AAAI，2014*
    中找到。'
- en: 'The NLTK version uses the `SentimentIntensityAnalyzer` class and can immediately
    be used to have a polarity sentiment measure made up of four components:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK 版本使用 `SentimentIntensityAnalyzer` 类，可以直接使用，以获得由四个组成部分组成的极性情感度量：
- en: Positive factor
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正面因素
- en: Negative factor
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负面因素
- en: Neutral factor
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中性因素
- en: Compound factor
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复合因素
- en: 'The first three don''t need any explanation, while the last one is a particular
    measure (a normalized overall score), which is computed as:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 前三项无需解释，而最后一个是特定度量（一个归一化的总体得分），其计算方式如下：
- en: '![](img/2db0e565-d2f1-453e-a70d-698edba300ee.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2db0e565-d2f1-453e-a70d-698edba300ee.png)'
- en: 'Here, *Sentiment(w[i])* is the score valence of the word *w[i]* and alpha is
    a normalization coefficient that should approximate the maximum expected value
    (the default value set in NLTK is 15). The usage of this class is immediate, as
    the following snippet can confirm:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*Sentiment(w[i])* 是单词 *w[i]* 的情感得分，alpha 是一个归一化系数，它应该近似最大预期值（NLTK中默认设置为15）。这个类的使用非常直接，以下代码片段可以证实：
- en: '[PRE29]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The NLTK Vader implementation uses the library Twython for some functionalities.
    Even though it's not necessary, in order to avoid a warning, it's possible to
    install it using pip (`pip install twython`).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK Vader 实现使用 Twython 库的一些功能。尽管这不是必需的，但为了避免警告，可以使用 pip 安装它（`pip install twython`）。
- en: References
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Hofmann T., *Unsupervised Learning by Probabilistic Latent Semantic Analysis*,
    Machine Learning 42, 177-196, 2001, Kluwer Academic Publishers.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hofmann T.，*Unsupervised Learning by Probabilistic Latent Semantic Analysis*，Machine
    Learning 42，177-196，2001，Kluwer Academic Publishers。
- en: Blei D., Ng A., Jordan M., *Latent Dirichlet Allocation, Journal of Machine
    Learning Research*, 3, (2003) 993-1022.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blei D.，Ng A.，Jordan M.，*Latent Dirichlet Allocation, Journal of Machine Learning
    Research*，3，(2003) 993-1022。
- en: 'Hutto C.J., Gilbert E., *VADER: A Parsimonious Rule-based Model for Sentiment
    Analysis of Social Media Text*, AAAI, 2014.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hutto C.J.，Gilbert E.，*VADER: A Parsimonious Rule-based Model for Sentiment
    Analysis of Social Media Text*，AAAI，2014。'
- en: Summary
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced topic modeling. We discussed latent semantic
    analysis based on truncated SVD, probabilistic latent semantic analysis (which
    aims to build a model without assumptions about latent factor prior probabilities),
    and latent Dirichlet allocation, which outperformed the previous method and is
    based on the assumption that the latent factor has a sparse prior Dirichlet distribution.
    This means that a document normally covers only a limited number of topics and
    a topic is characterized only by a few important words.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了主题建模。我们讨论了基于截断 SVD 的潜在语义分析、概率潜在语义分析（旨在构建一个不假设潜在因素先验概率的模型）以及潜在狄利克雷分配，后者优于前一种方法，并基于潜在因素具有稀疏先验狄利克雷分布的假设。这意味着一个文档通常只覆盖有限数量的主题，而一个主题只由少数几个重要单词来表征。
- en: In the last section, we discussed sentiment analysis of documents, which is
    aimed at determining whether a piece of text expresses a positive or negative
    feeling. In order to show a feasible solution, we built a classifier based on
    an NLP pipeline and a random forest with average performances that can be used
    in many real-life situations.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一节中，我们讨论了文档的情感分析，其目的是确定一段文本是否表达了一种积极的或消极的感觉。为了展示一个可行的解决方案，我们基于一个 NLP 管道和一个随机森林构建了一个分类器，其平均性能可用于许多现实生活中的情况。
- en: In the next chapter, we're going to briefly introduce deep learning, together
    with the TensorFlow framework. As this topic alone requires a dedicated book,
    our goal is to define the main concepts with some practical examples. If the reader
    wants to have further information, at the end of the chapter, a complete reference
    list will be provided.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将简要介绍深度学习以及 TensorFlow 框架。由于这个主题本身就需要一本专门的书籍，我们的目标是定义一些主要概念，并通过一些实际例子进行说明。如果读者想要了解更多信息，本章末尾将提供一个完整的参考文献列表。
