<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Analyzing Text Data</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>Preprocessing data using tokenization</li>
<li>Stemming text data</li>
<li>Converting text to its base form using lemmatization</li>
<li>Dividing text using chunking</li>
<li>Building a bag-of-words model</li>
<li>Building a text classifier</li>
<li>Identifying the gender of a name</li>
<li>Analyzing the sentiment of a sentence</li>
<li>Identifying patterns in text using topic modeling</li>
<li>Parts of speech tagging with spaCy</li>
<li>Word2Vec using gensim</li>
<li>Shallow learning for spam detection</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p class="mce-root">To complete the recipes in this chapter, you will need the following files (available on GitHub):</p>
<ul>
<li><kbd><span>tokenizer.py</span></kbd></li>
<li><kbd><span>stemmer.py</span></kbd></li>
<li><kbd><span>lemmatizer.py</span></kbd></li>
<li><kbd><span>chunking.py</span></kbd></li>
<li><kbd><span>bag_of_words.py</span></kbd></li>
<li><kbd><span>tfidf.py</span></kbd></li>
<li><kbd><kbd><span>gender_identification.py</span></kbd></kbd></li>
<li><kbd><span>sentiment_analysis.py</span></kbd></li>
<li><kbd><span>topic_modeling.py</span></kbd></li>
<li><kbd>data_topic_modeling.txt</kbd></li>
<li><kbd>PosTagging.py</kbd></li>
<li><kbd><span>GensimWord2Vec.py</span></kbd></li>
<li><kbd><span>LogiTextClassifier.py</span></kbd></li>
<li><kbd>spam.csv</kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p>Text analysis and <strong>natural language processing</strong> (<strong>NLP</strong>) are an integral part of modern artificial intelligence systems. Computers are good at understanding rigidly structured data with limited variety. However, when we deal with unstructured, free-form text, things begin to get difficult. Developing NLP applications is challenging because computers have a hard time understanding the underlying concepts. There are also many subtle variations to the way that we communicate things. These can be in the form of dialects, context, slang, and so on.</p>
<p>In order to solve this problem, NLP applications are developed based on machine learning. These algorithms detect patterns in text data so that we can extract insights from them. Artificial intelligence companies make heavy use of NLP and text analysis in order to deliver relevant results. Some of the most common applications of NLP include search engines, sentiment analysis, topic modeling, part-of-speech tagging, and entity recognition. The goal of NLP is to develop a set of algorithms so that we can interact with computers in plain English. If we can achieve this, then we won't need programming languages to instruct computers on what they should do. In this chapter, we will look at a few recipes that focus on text analysis and how we can extract meaningful information from text data.</p>
<p>We will use a Python package called <strong>Natural Language Toolkit</strong> (<strong>NLTK</strong>) heavily in this chapter. Make sure that you install this before you proceed:</p>
<ul>
<li>You can find the installation steps at <a href="http://www.nltk.org/install.html"><span class="URLPACKT">http://www.nltk.org/install.html</span></a>.</li>
<li>You will also need to install <kbd>NLTK Data</kbd>, which contains many corpora and trained models. This is an integral part of text analysis! You can find the installation steps at <a href="http://www.nltk.org/data.html"><span class="URLPACKT">http://www.nltk.org/data.html</span></a>.</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preprocessing data using tokenization</h1>
                </header>
            
            <article>
                
<p><strong>Tokenization</strong> is the process of dividing text into a set of meaningful pieces. These pieces are called <strong>tokens</strong>. For example, we can divide a chunk of text into words, or we can divide it into sentences. Depending on the task at hand, we can define our own conditions to divide the input text into meaningful tokens. Let's take a look at how to do this.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root">Tokenization is the first step in the computational analysis of the text and involves dividing the sequences of characters into minimal units of analysis called <strong>tokens</strong>. Tokens include various categories of text parts (words, punctuation, numbers, and so on), and can also be complex units (such as dates). In this recipe, we will illustrate how to divide a complex sentence into many tokens.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's look at how to preprocess data using tokenization:</p>
<ol>
<li>Create a new Python file and add the following lines (the full code is in the <kbd>tokenizer.py</kbd> file that's already been provided to you). Let's <kbd>import</kbd> the package and corpora:</li>
</ol>
<pre style="padding-left: 60px">import nltk<br/>nltk.download('punkt')</pre>
<ol start="2">
<li>Let's define some sample <kbd>text</kbd> for analysis:</li>
</ol>
<pre style="padding-left: 60px">text = "Are you curious about tokenization? Let's see how it works! We need to analyze a couple of sentences with punctuations to see it in action." </pre>
<ol start="3">
<li>Let's start with sentence tokenization. NLTK provides a sentence tokenizer, so let's <kbd>import</kbd> that:</li>
</ol>
<pre style="padding-left: 60px"># Sentence tokenization 
from nltk.tokenize import sent_tokenize </pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="4">
<li>Run the sentence tokenizer on the input <kbd>text</kbd> and extract the tokens:</li>
</ol>
<pre style="padding-left: 60px">sent_tokenize_list = sent_tokenize(text) </pre>
<ol start="5">
<li>Print the list of sentences to see whether it works correctly:</li>
</ol>
<pre style="padding-left: 60px">print("Sentence tokenizer:")<br/>print(sent_tokenize_list) </pre>
<ol start="6">
<li>Word tokenization is very commonly used in NLP. NLTK comes with a couple of different word tokenizers. Let's start with the basic word tokenizer:</li>
</ol>
<pre style="padding-left: 60px"># Create a new word tokenizer 
from nltk.tokenize import word_tokenize 
 
print("Word tokenizer:")<br/>print(word_tokenize(text)) </pre>
<ol start="7">
<li>If you want to split this punctuation into separate tokens, then you will need to use the <kbd>WordPunct</kbd> tokenizer:</li>
</ol>
<pre># Create a new WordPunct tokenizer 
from nltk.tokenize import WordPunctTokenizer 
 
word_punct_tokenizer = WordPunctTokenizer() 
print("Word punct tokenizer:")<br/>print(word_punct_tokenizer.tokenize(text)) </pre>
<ol start="8">
<li>If you run this code, you will see the following output on your Terminal:</li>
</ol>
<pre>Sentence tokenizer:<br/>['Are you curious about tokenization?', "Let's see how it works!", 'We need to analyze a couple of sentences with punctuations to see it in action.']<br/><br/>Word tokenizer:<br/>['Are', 'you', 'curious', 'about', 'tokenization', '?', 'Let', "'s", 'see', 'how', 'it', 'works', '!', 'We', 'need', 'to', 'analyze', 'a', 'couple', 'of', 'sentences', 'with', 'punctuations', 'to', 'see', 'it', 'in', 'action', '.']<br/><br/>Word punct tokenizer:<br/>['Are', 'you', 'curious', 'about', 'tokenization', '?', 'Let', "'", 's', 'see', 'how', 'it', 'works', '!', 'We', 'need', 'to', 'analyze', 'a', 'couple', 'of', 'sentences', 'with', 'punctuations', 'to', 'see', 'it', 'in', 'action', '.']</pre>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>In this recipe, we illustrated how to divide a complex sentence into many tokens. To do this, three methods of the <kbd>nltk.tokenize</kbd> package were used—<kbd>sent_tokenize</kbd>, <kbd>word_tokenize</kbd>, and <kbd>WordPunctTokenizer</kbd>:</span></p>
<ul>
<li class="mce-root"><kbd>sent_tokenize</kbd> <span>returns a sentence-tokenized copy of text, using NLTK's recommended sentence tokenizer.</span></li>
<li class="mce-root"><kbd>word_tokenize</kbd> <span>tokenizes a string to split off punctuation other than periods.</span></li>
<li class="mce-root"><kbd>WordPunctTokenizer</kbd> <span>tokenizes a text into a sequence of alphabetic and non-alphabetic characters, using the regexp <kbd>\w+|[^\w\s]+</kbd>.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">Tokenization is a procedure that, depending on the language that is analyzed, can be an extremely complex task. In English, for example, we could be content to consider taking sequences of characters that do not have spaces and the various punctuation marks. Languages such as Japanese or Chinese, in which words are not separated by spaces but the union of different symbols, can completely change the meaning, and the task is much more complex. But in general, even in languages with words separated by spaces, precise criteria must be defined, as punctuation is often ambiguous.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root">The official documentation of the <kbd>nltk.tokenize</kbd> package: <a href="https://www.nltk.org/api/nltk.tokenize.html">https://www.nltk.org/api/nltk.tokenize.html</a></li>
<li><em>Tokenization</em> (from the Natural Language Processing Group at Stanford University): <a href="https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html">https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html</a></li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stemming text data</h1>
                </header>
            
            <article>
                
<p>When we deal with a text document, we encounter different forms of words. Consider the word <strong>play.</strong> This word can appear in various forms, such as play, plays, player, playing, and so on. These are basically families of words with similar meanings. During text analysis, it's useful to extract the base forms of these words. This will help us to extract some statistics to analyze the overall text. The goal of <strong>stemming</strong> is to reduce these different forms into a common base form. This uses a heuristic process to cut off the ends of words in order to extract the base form. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this recipe, we will use the <kbd>nltk.stem</kbd> package that offers a processing interface for removing morphological affixes from words. Different stemmers are available for different languages. For the English language, we will use <kbd>PorterStemmer</kbd>, <kbd>LancasterStemmer</kbd>, and <kbd>SnowballStemmer</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's look at how to stem text data:</p>
<ol>
<li>Create a new Python file and import the following packages (the full code is in the <kbd>stemmer.py</kbd> file that's already been provided to you):</li>
</ol>
<pre style="padding-left: 60px">from nltk.stem.porter import PorterStemmer 
from nltk.stem.lancaster import LancasterStemmer 
from nltk.stem.snowball import SnowballStemmer </pre>
<ol start="2">
<li>Let's define a few <kbd>words</kbd> to play with, as follows:</li>
</ol>
<pre style="padding-left: 60px">words = ['table', 'probably', 'wolves', 'playing', 'is',  
        'dog', 'the', 'beaches', 'grounded', 'dreamt', 'envision'] </pre>
<ol start="3">
<li>We'll define a list of <kbd>stemmers</kbd> that we want to use:</li>
</ol>
<pre style="padding-left: 60px"># Compare different stemmers stemmers = ['PORTER', 'LANCASTER', 'SNOWBALL'] </pre>
<ol start="4">
<li>Initialize the required objects for all three stemmers:</li>
</ol>
<pre style="padding-left: 60px">stemmer_porter = PorterStemmer() stemmer_lancaster = LancasterStemmer() stemmer_snowball = SnowballStemmer('english') </pre>
<ol start="5">
<li>In order to print the output data in a neat tabular form, we need to format it in the correct way:</li>
</ol>
<pre style="padding-left: 60px">formatted_row = '{:&gt;16}' * (len(stemmers) + 1) 
print('\n', formatted_row.format('WORD', *stemmers), '\n')</pre>
<ol start="6">
<li>Let's iterate through the list of <kbd>words</kbd> and stem them by using the three stemmers:</li>
</ol>
<pre style="padding-left: 60px">for word in words: stemmed_words = [stemmer_porter.stem(word), stemmer_lancaster.stem(word), stemmer_snowball.stem(word)] print(formatted_row.format(word, *stemmed_words)) </pre>
<ol start="7">
<li>If you run this code, you will see the following output in your Terminal. Observe how the <span class="packt_screen">LANCASTER</span> stemmer behaves differently for a couple of words:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1027 image-border" src="assets/02ff3262-46ca-4b94-9475-f565c52a98ad.png" style="width:39.25em;height:14.83em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>All three stemming algorithms basically aim at achieving the same thing. The difference between the three stemming algorithms is basically the level of strictness with which they operate. If you observe the output, you will see that the <span class="packt_screen">LANCASTER</span> stemmer is stricter than the other two stemmers. The <span class="packt_screen">PORTER</span> stemmer is the least in terms of strictness, and the <span class="packt_screen">LANCASTER</span> is the strictest. The stemmed words that we get from the <span class="packt_screen">LANCASTER</span> stemmer tend to get confusing and obfuscated. The algorithm is really fast, but it will reduce the words a lot. So, a good rule of thumb is to use the <span class="packt_screen">SNOWBALL</span> stemmer.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">Stemming is the process of reducing the inflected form of a word to its root form, called the <strong>stem</strong>. The stem doesn't necessarily correspond to the morphological root (lemma) of the word: it's normally sufficient that the related words are mapped to the same stem, even if the latter isn't a valid root for the word. The creation of a stemming algorithm has been a prevalent issue in computer science. The stemming process is applied in search engines for query expansion, and in other natural language processing problems. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The official home page for distribution of the Porter Stemming Algorithm, written and maintained by its author, Martin Porter: <a href="https://tartarus.org/martin/PorterStemmer/">https://tartarus.org/martin/PorterStemmer/</a></li>
<li class="mce-root">The official documentation of the <kbd>nltk.stem</kbd> package: <a href="https://www.nltk.org/api/nltk.stem.html">https://www.nltk.org/api/nltk.stem.html</a></li>
<li class="mce-root"><em>Stemming</em> (from Wikipedia): <a href="https://en.wikipedia.org/wiki/Stemming">https://en.wikipedia.org/wiki/Stemming</a> </li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Converting text to its base form using lemmatization</h1>
                </header>
            
            <article>
                
<p>The goal of lemmatization is also to reduce words to their base forms, but this is a more structured approach. In the previous recipe, you saw that the base words that we obtained using stemmers don't really make sense. For example, the word <span class="packt_screen">wolves</span> was reduced to <span class="packt_screen">wolv</span>, which is not a real word. Lemmatization solves this problem by doing things with a vocabulary and morphological analysis of words. It removes inflectional word endings, such as -ing or -ed, and returns the base form of a word. This base form is known as the lemma. If you lemmatize the word <kbd>wolves</kbd>, you will get <kbd>wolf</kbd> as the output. The output depends on whether the token is a verb or a noun. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this recipe, we will use the <kbd>nltk.stem</kbd> package to reducing a word's inflected form to its canonical form, called a <strong>lemma</strong>.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's look at how to convert text to its base form using lemmatization:</p>
<ol>
<li>Create a new Python file and import the following package (the full code is in the <kbd>lemmatizer.py</kbd> file that's already been provided to you):</li>
</ol>
<pre style="padding-left: 60px">import nltk<br/>nltk.download('wordnet')<br/>from nltk.stem import WordNetLemmatizer </pre>
<ol start="2">
<li>Let's define the same set of words that we used during stemming:</li>
</ol>
<pre style="padding-left: 60px">words = ['table', 'probably', 'wolves', 'playing', 'is', 'dog', 'the', 'beaches', 'grounded', 'dreamt', 'envision'] </pre>
<ol start="3">
<li>We will compare two lemmatizers: the <kbd>NOUN</kbd> and <kbd>VERB</kbd> lemmatizers. Let's list them:</li>
</ol>
<pre style="padding-left: 60px"># Compare different lemmatizers 
lemmatizers = ['NOUN LEMMATIZER', 'VERB LEMMATIZER'] </pre>
<ol start="4">
<li>Create the object based on the <kbd>WordNet</kbd> lemmatizer:</li>
</ol>
<pre style="padding-left: 60px">lemmatizer_wordnet = WordNetLemmatizer() </pre>
<ol start="5">
<li>In order to print the output in a tabular form, we need to format it in the right way:</li>
</ol>
<pre style="padding-left: 60px">formatted_row = '{:&gt;24}' * (len(lemmatizers) + 1) 
print('\n', formatted_row.format('WORD', *lemmatizers), '\n') </pre>
<ol start="6">
<li>Iterate through the words and lemmatize them:</li>
</ol>
<pre style="padding-left: 60px">for word in words: 
    lemmatized_words = [lemmatizer_wordnet.lemmatize(word, pos='n'), 
           lemmatizer_wordnet.lemmatize(word, pos='v')] 
    print(formatted_row.format(word, *lemmatized_words)) </pre>
<ol start="7">
<li>If you run this code, you will see the following output:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1029 image-border" src="assets/8f65ff6c-7ca0-40a1-91fc-126bc6e796ef.png" style="width:33.83em;height:12.83em;"/></p>
<p style="padding-left: 60px"><span>Observe how the </span><kbd>NOUN</kbd><span> and </span><kbd>VERB</kbd><span> lemmatizers differ when they lemmatize the word, as shown in the preceding screenshot</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">Lemmatization is the process of reducing a word's inflected form to its canonical form, called a lemma. In the processing of natural language, lemmatization is the algorithmic process that automatically determines the word of a given word. The process may involve other language processing activities, such as morphological and grammatical analysis. In many languages, words appear in different inflected forms. The combination of the canonical form with its part of speech is called the <strong>lexeme</strong> of the word. A lexeme is, in structural lexicology, the minimum unit that constitutes the lexicon of a language. Hence, every lexicon of a language may correspond to its registration in a dictionary in the form of a lemma.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">In NLTK, for lemmatization, <kbd>WordNet</kbd> is available, but this resource is limited to the English language. It's a large lexical database of the English language. In this package, names, verbs, adjectives, and adverbs are grouped into sets of cognitive synonyms (<strong>synsets</strong>), each of which expresses a distinct concept. The synsets are interconnected by means of semantic and lexical conceptual relationships. The resulting network of significantly related words and concepts can be navigated with the browser. <kbd>WordNet</kbd> groups words according to their meanings, connecting not only word forms (strings of letters) but specific words. Hence, the words that are in close proximity to each other in the network are semantically disambiguated. In addition, <kbd>WordNet</kbd> labels the semantic relationships between words.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><span><em>Keras 2.x Projects</em> by Giuseppe Ciaburro,</span> from Packt Publishing</li>
<li>The official documentation of the <kbd>nltk.stem</kbd> package: <a href="https://www.nltk.org/api/nltk.stem.html">https://www.nltk.org/api/nltk.stem.html</a></li>
<li><em>Lemmatisation</em> (from Wikipedia): <a href="https://en.wikipedia.org/wiki/Lemmatisation">https://en.wikipedia.org/wiki/Lemmatisation</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dividing text using chunking</h1>
                </header>
            
            <article>
                
<p><strong>Chunking</strong> refers to dividing the input text into pieces, which are based on any random condition. This is different from tokenization in the sense that there are no constraints, and the chunks do not need to be meaningful at all. This is used very frequently during text analysis. While dealing with large text documents, it's better to do it in chunks.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's look at how to divide text by using chunking:</p>
<ol>
<li>Create a new Python file and import the following packages (the full code is in the <kbd>chunking.py</kbd> file that's already been provided to you):</li>
</ol>
<pre style="padding-left: 90px">import numpy as np <br/>nltk.download('brown')
from nltk.corpus import brown </pre>
<ol start="2">
<li>Let's define a function to <kbd>split</kbd> the text into chunks. The first step is to divide the text based on spaces:</li>
</ol>
<pre style="padding-left: 60px"># Split a text into chunks  
def splitter(data, num_words): 
    words = data.split(' ') 
    output = [] </pre>
<ol start="3">
<li>Initialize a couple of required variables:</li>
</ol>
<pre>    cur_count = 0 
    cur_words = [] </pre>
<p class="mce-root"/>
<ol start="4">
<li>Let's iterate through the <kbd>words</kbd>:</li>
</ol>
<pre>    for word in words: 
        cur_words.append(word) 
        cur_count += 1 </pre>
<ol start="5">
<li>Once you have hit the required number of words, reset the variables:</li>
</ol>
<pre>        if cur_count == num_words: 
            output.append(' '.join(cur_words)) 
            cur_words = [] 
            cur_count = 0 </pre>
<ol start="6">
<li>Append the chunks to the <kbd>output</kbd> variable, and return it:</li>
</ol>
<pre>    output.append(' '.join(cur_words) ) 
 
    return output </pre>
<ol start="7">
<li>We can now define the main function. Load the data from the <kbd>brown</kbd> corpus. We will use the first 10,000 words:</li>
</ol>
<pre>if __name__=='__main__': 
    # Read the data from the Brown corpus 
    data = ' '.join(brown.words()[:10000]) </pre>
<ol start="8">
<li>Define the number of words in each chunk:</li>
</ol>
<pre>    # Number of words in each chunk  
    num_words = 1700 </pre>
<ol start="9">
<li>Initialize a couple of relevant variables:</li>
</ol>
<pre>    chunks = [] 
    counter = 0 </pre>
<ol start="10">
<li>Call the <kbd>splitter</kbd> function on this text <kbd>data</kbd> and <kbd>print</kbd> the output:</li>
</ol>
<pre>    text_chunks = splitter(data, num_words) 
 
    print("Number of text chunks =", len(text_chunks)) </pre>
<ol start="11">
<li>If you run this code, you will see the number of chunks that were generated printed in the Terminal:</li>
</ol>
<pre style="padding-left: 60px"><strong>Number of text chunks = 6</strong></pre>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>Chunking</span> (also <span>called <strong>shallow parsing</strong></span>) is the analysis of a proposition, which is formed in a simple form by a subject and a predicate. The subject is typically a noun phrase, while the predicate is a verbal phrase formed by a verb with zero or more complements and adverbs. A chunk is made up of one or more adjacent tokens.</p>
<p><span>There are numerous approaches to the problem of chunking. For example, in the assigned </span><span>task, a chunk is represented as a group of words delimited by square brackets, for which a </span><span>tag representing the type of chunk is indicated. The dataset that was used was derived from a given </span><span>corpora by taking the part related to journal articles and extracting chunks of information from the </span><span>syntactic trees of the corpora.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>The Brown University Standard Corpus of Present-Day American English (or simply, the <kbd>brown</kbd> corpus) is a corpus that was compiled in the 1960s by Henry Kucera and W. Nelson Francis at Brown University, Providence, Rhode Island. It contains 500 text extracts in English, obtained from works published in the United States of America in 1961, for a total of about one million words.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root">The official documentation of the <kbd>nltk.corpus</kbd> package: <a href="https://www.nltk.org/api/nltk.corpus.html">https://www.nltk.org/api/nltk.corpus.html</a></li>
<li class="mce-root"><em>Basics of Natural Language Processing</em> (from the University of Zagreb): <a href="https://www.fer.unizg.hr/_download/repository/TAR-02-NLP.pdf">https://www.fer.unizg.hr/_download/repository/TAR-02-NLP.pdf</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a bag-of-words model</h1>
                </header>
            
            <article>
                
<p>When it comes to dealing with text documents that consist of millions of words, converting them into numerical representations is necessary. The reason for this is to make them usable for machine learning algorithms. These algorithms need numerical data so that they can analyze them and output meaningful information. This is where the <strong>bag-of-words</strong> approach comes into the picture. This is basically a model that learns a vocabulary from all of the words in all the documents. It models each document by building a histogram of all of the words in the document.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this recipe, we will build a bag-of-words model to extract a document term matrix, using the <kbd>sklearn.feature_extraction.text</kbd> package.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's look at how to build a bag-of-words model, as follows:</p>
<ol>
<li>Create a new Python file and import the following packages (the full code is in the <kbd>bag_of_words.py</kbd> file that's already been provided to you):</li>
</ol>
<pre style="padding-left: 60px">import numpy as np 
from nltk.corpus import brown 
from chunking import splitter </pre>
<ol start="2">
<li>Let's define the <kbd>main</kbd> function. Load the input <kbd>data</kbd> from the <kbd>brown</kbd> corpus:</li>
</ol>
<pre style="padding-left: 60px">if __name__=='__main__': 
    # Read the data from the Brown corpus 
    data = ' '.join(brown.words()[:10000]) </pre>
<ol start="3">
<li>Divide the text data into five chunks:</li>
</ol>
<pre>    # Number of words in each chunk  
    num_words = 2000 
 
    chunks = [] 
    counter = 0 
 
    text_chunks = splitter(data, num_words) </pre>
<ol start="4">
<li>Create a dictionary that is based on these text chunks:</li>
</ol>
<pre>    for text in text_chunks: 
        chunk = {'index': counter, 'text': text} 
        chunks.append(chunk) 
        counter += 1 </pre>
<ol start="5">
<li>The next step is to extract a document term matrix. This is basically a matrix that counts the number of occurrences of each word in the document. We will use <kbd>scikit-learn</kbd> to do this because it has better provisions, compared to NLTK, for this particular task. Import the following package:</li>
</ol>
<pre>    # Extract document term matrix 
    from sklearn.feature_extraction.text import CountVectorizer </pre>
<ol start="6">
<li>Define the object and extract the document term matrix:</li>
</ol>
<pre>    vectorizer = CountVectorizer(min_df=5, max_df=.95) 
    doc_term_matrix = vectorizer.fit_transform([chunk['text'] for chunk in chunks]) </pre>
<ol start="7">
<li>Extract the vocabulary from the <kbd>vectorizer</kbd> object and print it:</li>
</ol>
<pre>    vocab = np.array(vectorizer.get_feature_names()) 
    print("Vocabulary:")<br/>    print(vocab)</pre>
<ol start="8">
<li>Print the <kbd>Document term matrix</kbd>:</li>
</ol>
<pre>    print("Document term matrix:") 
    chunk_names = ['Chunk-0', 'Chunk-1', 'Chunk-2', 'Chunk-3', 'Chunk-4'] </pre>
<ol start="9">
<li>To print it in a tabular form, you will need to format this, as follows:</li>
</ol>
<pre>    formatted_row = '{:&gt;12}' * (len(chunk_names) + 1) 
    print('\n', formatted_row.format('Word', *chunk_names), '\n') </pre>
<ol start="10">
<li>Iterate through the words and print the number of times each word has occurred in different chunks:</li>
</ol>
<pre>    for word, item in zip(vocab, doc_term_matrix.T): 
        # 'item' is a 'csr_matrix' data structure 
        output = [str(x) for x in item.data] 
        print(formatted_row.format(word, *output)) </pre>
<ol start="11">
<li>If you run this code, you will see two main things printed in the Terminal. The first output is the vocabulary, as shown in the following screenshot:</li>
</ol>
<pre>Vocabulary:<br/>['about' 'after' 'against' 'aid' 'all' 'also' 'an' 'and' 'are' 'as' 'at'<br/> 'be' 'been' 'before' 'but' 'by' 'committee' 'congress' 'did' 'each'<br/> 'education' 'first' 'for' 'from' 'general' 'had' 'has' 'have' 'he'<br/> 'health' 'his' 'house' 'in' 'increase' 'is' 'it' 'last' 'made' 'make'<br/> 'may' 'more' 'no' 'not' 'of' 'on' 'one' 'only' 'or' 'other' 'out' 'over'<br/> 'pay' 'program' 'proposed' 'said' 'similar' 'state' 'such' 'take' 'than'<br/> 'that' 'the' 'them' 'there' 'they' 'this' 'time' 'to' 'two' 'under' 'up'<br/> 'was' 'were' 'what' 'which' 'who' 'will' 'with' 'would' 'year' 'years']</pre>
<ol start="12">
<li>The second thing is the <span class="packt_screen">Document term matrix</span>, which is pretty long. The first few lines will look like the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1030 image-border" src="assets/e469a5e4-b7b8-4b75-bb9e-9f70e7f68682.png" style="width:43.67em;height:23.00em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Consider the following sentences:</p>
<ul>
<li><strong>Sentence 1</strong>: The brown dog is running.</li>
<li><strong>Sentence 2</strong>: The black dog is in the black room.</li>
<li><strong>Sentence 3</strong>: Running in the room is forbidden.</li>
</ul>
<p>If you consider all three of these sentences, you will have the following nine unique words:</p>
<ul>
<li>the</li>
<li>brown</li>
<li>dog</li>
<li>is</li>
<li>running</li>
<li>black</li>
<li>in</li>
<li>room</li>
<li>forbidden</li>
</ul>
<p>Now, let's convert each sentence into a histogram, using the count of words in each sentence. Each feature vector will be nine-dimensional, because we have nine unique words:</p>
<ul>
<li><strong>Sentence 1</strong>: [1, 1, 1, 1, 1, 0, 0, 0, 0]</li>
<li><strong>Sentence 2</strong>: [2, 0, 1, 1, 0, 2, 1, 1, 0]</li>
<li><strong>Sentence 3</strong>: [0, 0, 0, 1, 1, 0, 1, 1, 1]</li>
</ul>
<p>Once we have extracted these feature vectors, we can use machine learning algorithms to analyze them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">The bag-of-words model is a method that's used in information retrieval and in the processing of the natural language in order to represent documents by ignoring the word order. In this model, each document is considered to contain words, similar to a stock exchange; this allows for the management of these words based on lists, where each stock contains certain words from a list.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The official documentation of the <kbd>sklearn.feature_extraction.text.CountVectorizer()</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html">https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html</a></li>
<li>The <em>Bag-of-Words Model</em> (from Wikipedia): <a href="https://en.wikipedia.org/wiki/Bag-of-words_model">https://en.wikipedia.org/wiki/Bag-of-words_model</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a text classifier</h1>
                </header>
            
            <article>
                
<p>The main aim of text classification is to sort text documents into different classes. This is a vital analysis technique in NLP. We will use a technique that is based on a statistic called <strong>tf-idf</strong>, which stands for <strong>term frequency</strong>-<strong>inverse document frequency</strong>. This is an analysis tool that helps us to understand how important a word is to a document in a set of documents. This serves as a feature vector that's used to categorize documents. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this recipe, we will use the term frequency-inverse document frequency method to evaluate the importance of a word for a document in a collection or a corpus, and to build a text classifier.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's look at how to build a text classifier:</p>
<ol>
<li>Create a new Python file and import the following package (the full code is in the <kbd>tfidf.py</kbd> file that's already been provided to you):</li>
</ol>
<pre style="padding-left: 60px">from sklearn.datasets import fetch_20newsgroups </pre>
<ol start="2">
<li>Let's select a list of categories and name them using a dictionary mapping. These categories are available as a part of the news groups dataset that we just imported:</li>
</ol>
<pre style="padding-left: 60px">category_map = {'misc.forsale': 'Sales', 'rec.motorcycles': 'Motorcycles', <br/> 'rec.sport.baseball': 'Baseball', 'sci.crypt': 'Cryptography', <br/> 'sci.space': 'Space'}</pre>
<ol start="3">
<li>Load the training data based on the categories that we just defined:</li>
</ol>
<pre style="padding-left: 60px">training_data = fetch_20newsgroups(subset='train',  
        categories=category_map.keys(), shuffle=True, random_state=7) </pre>
<ol start="4">
<li>Import the feature extractor:</li>
</ol>
<pre style="padding-left: 60px"># Feature extraction 
from sklearn.feature_extraction.text import CountVectorizer </pre>
<ol start="5">
<li>Extract the features by using the training data:</li>
</ol>
<pre style="padding-left: 60px">vectorizer = CountVectorizer() X_train_termcounts = vectorizer.fit_transform(training_data.data) print("Dimensions of training data:", X_train_termcounts.shape)</pre>
<ol start="6">
<li>We are now ready to train the classifier. We will use the multinomial Naive Bayes classifier:</li>
</ol>
<pre style="padding-left: 60px"># Training a classifier 
from sklearn.naive_bayes import MultinomialNB 
from sklearn.feature_extraction.text import TfidfTransformer </pre>
<ol start="7">
<li>Define a couple of random input sentences:</li>
</ol>
<pre style="padding-left: 60px">input_data = [ "The curveballs of right handed pitchers tend to curve to the left", "Caesar cipher is an ancient form of encryption", "This two-wheeler is really good on slippery roads" 
] </pre>
<ol start="8">
<li>Define the <kbd>tfidf_transformer</kbd> object and train it:</li>
</ol>
<pre style="padding-left: 60px"># tf-idf transformer 
tfidf_transformer = TfidfTransformer() 
X_train_tfidf = tfidf_transformer.fit_transform(X_train_termcounts) </pre>
<p class="mce-root"/>
<ol start="9">
<li>Once we have the feature vectors, train the multinomial Naive Bayes classifier using this data:</li>
</ol>
<pre># Multinomial Naive Bayes classifier 
classifier = MultinomialNB().fit(X_train_tfidf, training_data.target) </pre>
<ol start="10">
<li>Transform the input data using the word counts:</li>
</ol>
<pre style="padding-left: 60px">X_input_termcounts = vectorizer.transform(input_data) </pre>
<ol start="11">
<li>Transform the input data using the <kbd>tfidf_transformer</kbd> module:</li>
</ol>
<pre style="padding-left: 60px">X_input_tfidf = tfidf_transformer.transform(X_input_termcounts) </pre>
<ol start="12">
<li>Predict the output categories of these input sentences by using the trained classifier:</li>
</ol>
<pre style="padding-left: 60px"># Predict the output categories 
predicted_categories = classifier.predict(X_input_tfidf) </pre>
<ol start="13">
<li>Print the output, as follows:</li>
</ol>
<pre style="padding-left: 60px"># Print the outputs 
for sentence, category in zip(input_data, predicted_categories): 
    print('\nInput:', sentence, '\nPredicted category:', \<br/>            category_map[training_data.target_names[category]])</pre>
<ol start="14">
<li>If you run this code, you will see the following output printed in your Terminal:</li>
</ol>
<pre>Dimensions of training data: (2968, 40605)<br/><br/>Input: The curveballs of right handed pitchers tend to curve to the left <br/>Predicted category: Baseball<br/><br/>Input: Caesar cipher is an ancient form of encryption <br/>Predicted category: Cryptography<br/><br/>Input: This two-wheeler is really good on slippery roads <br/>Predicted category: Motorcycles</pre>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The tf-idf technique is used frequently in information retrieval. The goal is to understand the importance of each word within a document. We want to identify words that occur many times in a document. At the same time, common words such as <strong>is</strong> and <strong>be</strong> don't really reflect the nature of the content. So, we need to extract the words that are true indicators. The importance of each word increases as the count increases. At the same time, as it appears a lot, the frequency of this word increases, too. These two things tend to balance each other out. We extract the term counts from each sentence. Once we have converted this to a feature vector, we can train the classifier to categorize these sentences.</p>
<p>The <strong>term frequency</strong> (<strong>TF</strong>) measures how frequently a word occurs in a given document. As multiple documents differ in length, the numbers in the histogram tend to vary a lot. So, we need to normalize this so that it becomes a level playing field. To achieve normalization, we can divide the term-frequency by the total number of words in a given document. The <strong>inverse document frequency</strong> (<strong>IDF</strong>) measures the importance of a given word. When we compute the TF, all words are considered to be equally important. To counterbalance the frequencies of commonly occurring words, we need to weigh them down and scale up the rare ones. We need to calculate the ratio of the number of documents with the given word and divide it by the total number of documents. The IDF is calculated by taking the negative algorithm of this ratio.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>Simple words, such as <strong>is</strong> or <strong>the</strong>, tend to appear a lot in various documents. However, this doesn't mean that we can characterize the document based on these words. At the same time, if a word appears a single time, that is not useful, either. So, we look for words that appear a number of times, but not so much that they become noisy. This is formulated in the tf-idf technique and is used to classify documents. Search engines frequently use this tool to order search results by relevance.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root">Refer to the official documentation of the <kbd>sklearn.feature_extraction.text.TfidfTransformer()</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html">https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html</a></li>
<li>Refer to the following page: <em>What does tf-idf mean?</em>: <a href="http://www.tfidf.com/">http://www.tfidf.com/</a></li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Identifying the gender of a name</h1>
                </header>
            
            <article>
                
<p>Identifying the gender of a name is an interesting task in NLP. We will use the heuristic that the last few characters in a name is its defining characteristic. For example, if the name ends with <strong>la</strong>, it's most likely a female name, such as Angela or Layla. On the other hand, if the name ends with <strong>im</strong>, it's most likely a male name, such as Tim or Jim. As we aren't sure of the exact number of characters to use, we will experiment with this. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this recipe, we will use the names corpora to extract labeled names, and then we will classify the gender based on the final part of the name.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's look at how to identify the gender:</p>
<ol>
<li>Create a new Python file and import the following packages (the full code is in the <kbd>gender_identification.py</kbd> file that's already been provided to you):</li>
</ol>
<pre style="padding-left: 60px">import nltk<br/>nltk.download('names')<br/><br/>import random from nltk.corpus import names from nltk import NaiveBayesClassifier from nltk.classify import accuracy as nltk_accuracy </pre>
<ol start="2">
<li>We need to define a function to extract features from input words:</li>
</ol>
<pre style="padding-left: 60px"># Extract features from the input word def gender_features(word, num_letters=2): return {'feature': word[-num_letters:].lower()} </pre>
<ol start="3">
<li>Let's define the main function. We need some labeled training data:</li>
</ol>
<pre style="padding-left: 60px">if __name__=='__main__': 
    # Extract labeled names 
    labeled_names = ([(name, 'male') for name in names.words('male.txt')] + 
            [(name, 'female') for name in names.words('female.txt')]) </pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<ol start="4">
<li>Seed the random number generator and shuffle the training data:</li>
</ol>
<pre>    random.seed(7) 
    random.shuffle(labeled_names) </pre>
<ol start="5">
<li>Define some input names to play with:</li>
</ol>
<pre>    input_names = ['Leonardo', 'Amy', 'Sam'] </pre>
<ol start="6">
<li>As we don't know how many ending characters we need to consider, we will sweep the parameter space from <kbd>1</kbd> to <kbd>5</kbd>. Each time, we will extract the features, as follows:</li>
</ol>
<pre style="padding-left: 60px">    # Sweeping the parameter space 
    for i in range(1, 5): 
        print('\nNumber of letters:', i) 
        featuresets = [(gender_features(n, i), gender) for (n, gender) in labeled_names] </pre>
<ol start="7">
<li>Divide this into train and test datasets:</li>
</ol>
<pre style="padding-left: 60px">        train_set, test_set = featuresets[500:], featuresets[:500] </pre>
<ol start="8">
<li>We will use the Naive Bayes classifier to do this:</li>
</ol>
<pre>        classifier = NaiveBayesClassifier.train(train_set) </pre>
<ol start="9">
<li>Evaluate the <kbd>classifier</kbd> model for each value in the parameter space:</li>
</ol>
<pre style="padding-left: 60px">        # Print classifier accuracy 
        print('Accuracy ==&gt;', str(100 * nltk_accuracy(classifier, test_set)) + str('%')) 
 
# Predict outputs for new inputs 
        for name in input_names: 
            print(name, '==&gt;', classifier.classify(gender_features(name, i))) </pre>
<ol start="10">
<li>If you run this code, you will see the following output printed in your Terminal:</li>
</ol>
<pre style="padding-left: 60px"><strong>Number of letters: 1</strong><br/><strong>Accuracy ==&gt; 76.2%</strong><br/><strong>Leonardo ==&gt; male</strong><br/><strong>Amy ==&gt; female</strong><br/><strong>Sam ==&gt; male</strong><br/><br/><strong>Number of letters: 2</strong><br/><strong>Accuracy ==&gt; 78.6%</strong><br/><strong>Leonardo ==&gt; male</strong><br/><strong>Amy ==&gt; female</strong><br/><strong>Sam ==&gt; male</strong><br/><br/><strong>Number of letters: 3</strong><br/><strong>Accuracy ==&gt; 76.6%</strong><br/><strong>Leonardo ==&gt; male</strong><br/><strong>Amy ==&gt; female</strong><br/><strong>Sam ==&gt; female</strong><br/><br/><strong>Number of letters: 4</strong><br/><strong>Accuracy ==&gt; 70.8%</strong><br/><strong>Leonardo ==&gt; male</strong><br/><strong>Amy ==&gt; female</strong><br/><strong>Sam ==&gt; female</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>In this recipe, we used the names corpus to extract labeled names, and then we classified the gender based on the final part of the name. A Naive Bayes classifier is a supervised learning classifier that uses Bayes' theorem to build the model. This topic was addressed in the <em>Building a Naive Bayes classifier</em> recipe in</span> <a href="102c5690-d978-4a56-a586-1f741cde6b3d.xhtml"/><a href="102c5690-d978-4a56-a586-1f741cde6b3d.xhtml">Chapter 2</a><em>, Constructing a Classifier</em><span>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>The Bayesian classifier is called naive because it ingenuously assumes that the presence or absence of a particular characteristic in a given class of interest is not related to the presence or absence of other characteristics, greatly simplifying the calculation. Let's go ahead and build a Naive Bayes classifier.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><a href="102c5690-d978-4a56-a586-1f741cde6b3d.xhtml">Chapter 2</a><em>, Constructing a Classifier</em></li>
<li><span><em>Keras 2.x Projects</em> by Giuseppe Ciaburro, from</span> Packt Publishing</li>
<li><span>The official documentation of the </span><kbd>nltk.classify</kbd><em><span> </span></em>package:<span> </span><a href="http://www.nltk.org/api/nltk.classify.html?highlight=naivebayesclassifier">http://www.nltk.org/api/nltk.classify.html?highlight=naivebayesclassifier</a></li>
<li><em>Bayes' Theorem</em> (from Stanford Encyclopedia of Philosophy):<span> </span><a href="https://plato.stanford.edu/entries/bayes-theorem/">https://plato.stanford.edu/entries/bayes-theorem/</a></li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Analyzing the sentiment of a sentence</h1>
                </header>
            
            <article>
                
<p><strong>Sentiment analysis</strong> is one of the most popular applications of NLP. Sentiment analysis refers to the process of determining whether a given piece of text is positive or negative. In some variations, we consider neutral as a third option. This technique is commonly used to discover how people feel about a particular topic. This is used to analyze the sentiments of users in various forms, such as marketing campaigns, social media, e-commerce, and so on.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this recipe, we will analyze the sentiment of a sentence by using a Naive Bayes classifier, starting with the data contained in the <kbd>movie_reviews</kbd> corpus.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's look at how to analyze the sentiment of a sentence:</p>
<ol>
<li>Create a new Python file and import the following packages (the full code is in the <kbd>sentiment_analysis.py</kbd> file that's already been provided to you):</li>
</ol>
<pre style="padding-left: 60px">import nltk.classify.util 
from nltk.classify import NaiveBayesClassifier 
from nltk.corpus import movie_reviews </pre>
<ol start="2">
<li>Define a function to extract the features:</li>
</ol>
<pre style="padding-left: 60px">def extract_features(word_list): 
    return dict([(word, True) for word in word_list]) </pre>
<ol start="3">
<li>We need training data for this, so we will use the movie reviews in NLTK:</li>
</ol>
<pre style="padding-left: 60px">if __name__=='__main__': 
    # Load positive and negative reviews   
    positive_fileids = movie_reviews.fileids('pos') 
    negative_fileids = movie_reviews.fileids('neg') </pre>
<p class="mce-root"/>
<ol start="4">
<li>Let's separate them into positive and negative reviews:</li>
</ol>
<pre>    features_positive = [(extract_features(movie_reviews.words(fileids=[f])),  
            'Positive') for f in positive_fileids] 
    features_negative = [(extract_features(movie_reviews.words(fileids=[f])),  
            'Negative') for f in negative_fileids] </pre>
<ol start="5">
<li>Divide the data into train and test datasets:</li>
</ol>
<pre style="padding-left: 60px">    # Split the data into train and test (80/20) 
    threshold_factor = 0.8 
    threshold_positive = int(threshold_factor * len(features_positive)) 
    threshold_negative = int(threshold_factor * len(features_negative)) </pre>
<ol start="6">
<li>Extract the features:</li>
</ol>
<pre style="padding-left: 60px">    features_train = features_positive[:threshold_positive] + features_negative[:threshold_negative] 
    features_test = features_positive[threshold_positive:] + features_negative[threshold_negative:]   
    print("Number of training datapoints:", len(features_train))<br/>    print("Number of test datapoints:", len(features_test))</pre>
<ol start="7">
<li>We will use a <kbd>NaiveBayesClassifier</kbd>. Define the object and train it:</li>
</ol>
<pre>    # Train a Naive Bayes classifier 
    classifier = NaiveBayesClassifier.train(features_train) 
    print("Accuracy of the classifier:", nltk.classify.util.accuracy(classifier, features_test))</pre>
<ol start="8">
<li>The <kbd>classifier</kbd> object contains the most informative words that it obtained during analysis. These words basically have a strong say in what's classified as a positive or a negative review. Let's print them out:</li>
</ol>
<pre>    print("Top 10 most informative words:")<br/>    for item in classifier.most_informative_features()[:10]:<br/>        print(item[0])</pre>
<p class="mce-root"/>
<ol start="9">
<li>Create a couple of random input sentences:</li>
</ol>
<pre>    # Sample input reviews 
    input_reviews = [ 
        "It is an amazing movie",  
        "This is a dull movie. I would never recommend it to anyone.", 
        "The cinematography is pretty great in this movie",  
        "The direction was terrible and the story was all over the place"  
    ] </pre>
<ol start="10">
<li>Run the classifier on those input sentences and obtain the predictions:</li>
</ol>
<pre>    print("Predictions:") 
    for review in input_reviews: 
        print("Review:", review) 
        probdist = classifier.prob_classify(extract_features(review.split())) 
        pred_sentiment = probdist.max() </pre>
<ol start="11">
<li>Print the output:</li>
</ol>
<pre>        print("Predicted sentiment:", pred_sentiment) <br/>        print("Probability:", round(probdist.prob(pred_sentiment), 2))</pre>
<ol start="12">
<li>If you run this code, you will see three main things printed in the Terminal. The first is the accuracy, as shown in the following code snippet:</li>
</ol>
<pre style="padding-left: 60px"><strong>Number of training datapoints: 1600</strong><br/><strong>Number of test datapoints: 400</strong><br/><strong>Accuracy of the classifier: 0.735</strong></pre>
<ol start="13">
<li>The next item is a list of the most informative words:</li>
</ol>
<pre style="padding-left: 60px"><strong>Top 10 most informative words:</strong><br/><br/><strong>outstanding</strong><br/><strong>insulting</strong><br/><strong>vulnerable</strong><br/><strong>ludicrous</strong><br/><strong>uninvolving</strong><br/><strong>astounding</strong><br/><strong>avoids</strong><br/><strong>fascination</strong><br/><strong>seagal</strong><br/><strong>anna</strong></pre>
<p class="mce-root"/>
<ol start="14">
<li>The last item is the list of predictions, which are based on the input sentences:</li>
</ol>
<pre style="padding-left: 60px"><strong>Predictions:</strong><br/><br/><strong>Review: It is an amazing movie</strong><br/><strong>Predicted sentiment: Positive</strong><br/><strong>Probability: 0.61</strong><br/><br/><strong>Review: This is a dull movie. I would never recommend it to anyone.</strong><br/><strong>Predicted sentiment: Negative</strong><br/><strong>Probability: 0.77</strong><br/><br/><strong>Review: The cinematography is pretty great in this movie</strong><br/><strong>Predicted sentiment: Positive</strong><br/><strong>Probability: 0.67</strong><br/><br/><strong>Review: The direction was terrible and the story was all over the place</strong><br/><strong>Predicted sentiment: Negative</strong><br/><strong>Probability: 0.63</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>We used NLTK's Naive Bayes classifier for our task here. In the feature extractor function, we basically extracted all the unique words. However, the NLTK classifier needs the data to be arranged in the form of a dictionary. Hence, we arranged it in such a way that the NLTK <kbd>classifier</kbd> object can ingest it. Once we divided the data into training and testing datasets, we trained the classifier to categorize the sentences into positive and negative ones. </p>
<p><span>If you look at the top informative words, you can see that we have words such as <span class="packt_screen">outstanding</span> to indicate positive reviews and words such as <span class="packt_screen">insulting</span> to indicate negative reviews. This is interesting information, because it tells us what words are being used to indicate strong reactions.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>The term sentiment analysis refers to the use of NLP techniques, text analysis, and computational linguistics to find information in written or spoken text sources. If this subjective information is taken from large amounts of data, and therefore from the opinions of large groups of people, sentiment analysis can also be called <strong>opinion mining</strong>.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root">The official documentation of the <kbd>nltk.corpus</kbd> package: <a href="https://www.nltk.org/api/nltk.corpus.html">https://www.nltk.org/api/nltk.corpus.html</a></li>
<li><span>The official documentation of</span><span> the </span><kbd>nltk.classify</kbd> package: <a href="http://www.nltk.org/api/nltk.classify.html?highlight=naivebayesclassifier">http://www.nltk.org/api/nltk.classify.html?highlight=naivebayesclassifier</a></li>
<li><em>Sentiment Analysis</em> (from Stanford University): <a href="https://web.stanford.edu/class/cs124/lec/sentiment.pdf">https://web.stanford.edu/class/cs124/lec/sentiment.pdf</a><a href="https://web.stanford.edu/class/cs124/lec/sentiment.pdf"/></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Identifying patterns in text using topic modeling</h1>
                </header>
            
            <article>
                
<p><strong>Topic modeling</strong> refers to the process of identifying hidden patterns in text data. The goal is to uncover a hidden thematic structure in a collection of documents. This will help us to organize our documents in a better way, so that we can use them for analysis. This is an active area of research in NLP. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this recipe, we will <span>use a library called </span><kbd>gensim</kbd><span> to identify patterns in text, using topic modeling. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's look at how to identify patterns in text by using topic modeling:</p>
<ol>
<li>Create a new Python file and import the following packages (the full code is in the <kbd>topic_modeling.py</kbd> file that's already been provided to you):</li>
</ol>
<pre style="padding-left: 60px">from nltk.tokenize import RegexpTokenizer   
from nltk.stem.snowball import SnowballStemmer 
from gensim import models, corpora 
from nltk.corpus import stopwords </pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="2">
<li>Define a function to load the input data. We will use the <kbd>data_topic_modeling.txt</kbd> text file that has already been provided to you:</li>
</ol>
<pre style="padding-left: 60px"># Load input data 
def load_data(input_file): 
    data = [] 
    with open(input_file, 'r') as f: 
        for line in f.readlines(): 
            data.append(line[:-1]) 
 
    return data </pre>
<ol start="3">
<li>Let's define a <kbd>class</kbd> to preprocess the text. This preprocessor will take care of creating the required objects and extracting the relevant features from the input text:</li>
</ol>
<pre style="padding-left: 60px"># Class to preprocess text 
class Preprocessor(object): 
    # Initialize various operators 
    def __init__(self): 
        # Create a regular expression tokenizer 
        self.tokenizer = RegexpTokenizer(r'\w+') </pre>
<ol start="4">
<li>We need a list of stop words so that we can exclude them from analysis. These are common words, such as <strong>in</strong>, <strong>the</strong>, <strong>is</strong>, and so on:</li>
</ol>
<pre>        # get the list of stop words  
        self.stop_words_english = stopwords.words('english') </pre>
<ol start="5">
<li>Define the <kbd>SnowballStemmer</kbd> module:</li>
</ol>
<pre>        # Create a Snowball stemmer  
        self.stemmer = SnowballStemmer('english') </pre>
<ol start="6">
<li>Define a processor function that takes care of tokenization, stop word removal, and stemming:</li>
</ol>
<pre>    # Tokenizing, stop word removal, and stemming 
    def process(self, input_text): 
        # Tokenize the string 
        tokens = self.tokenizer.tokenize(input_text.lower()) </pre>
<p class="mce-root"/>
<ol start="7">
<li>Remove the stop words from the text:</li>
</ol>
<pre>        # Remove the stop words  
        tokens_stopwords = [x for x in tokens if not x in self.stop_words_english] </pre>
<ol start="8">
<li>Perform stemming on the tokens:</li>
</ol>
<pre>        # Perform stemming on the tokens  
        tokens_stemmed = [self.stemmer.stem(x) for x in tokens_stopwords] </pre>
<ol start="9">
<li>Return the processed tokens:</li>
</ol>
<pre>        return tokens_stemmed </pre>
<ol start="10">
<li>We are now ready to define the main function. Load the input data from the text file:</li>
</ol>
<pre>if __name__=='__main__': 
    # File containing linewise input data  
    input_file = 'data_topic_modeling.txt' 
 
    # Load data 
    data = load_data(input_file) </pre>
<ol start="11">
<li>Define an object that is based on the class that we defined:</li>
</ol>
<pre>    # Create a preprocessor object 
    preprocessor = Preprocessor() </pre>
<ol start="12">
<li>We need to process the text in the file and extract the processed tokens:</li>
</ol>
<pre>    # Create a list for processed documents 
    processed_tokens = [preprocessor.process(x) for x in data] </pre>
<ol start="13">
<li>Create a dictionary that is based on tokenized documents so that it can be used for topic modeling:</li>
</ol>
<pre>    # Create a dictionary based on the tokenized documents 
    dict_tokens = corpora.Dictionary(processed_tokens) </pre>
<ol start="14">
<li>We need to create a document term matrix using the processed tokens, as follows:</li>
</ol>
<pre>    # Create a document term matrix 
    corpus = [dict_tokens.doc2bow(text) for text in processed_tokens] </pre>
<p class="mce-root"/>
<ol start="15">
<li>Let's suppose that we know that the text can be divided into two topics. We will use a technique called <strong>latent Dirichlet allocation</strong> (<strong>LDA</strong>) for topic modeling. Define the required parameters and initialize the <kbd>LdaModel</kbd> object:</li>
</ol>
<pre>    # Generate the LDA model based on the corpus we just created 
    num_topics = 2 
    num_words = 4 
 
    ldamodel = models.ldamodel.LdaModel(corpus,  
            num_topics=num_topics, id2word=dict_tokens, passes=25) </pre>
<ol start="16">
<li>Once this has identified the two topics, we can see how it's separating these two topics by looking at the most contributed words:</li>
</ol>
<pre>    print("Most contributing words to the topics:")<br/>    for item in ldamodel.print_topics(num_topics=num_topics, num_words=num_words):<br/>        print ("Topic", item[0], "==&gt;", item[1])</pre>
<ol start="17">
<li>The full code is in the <kbd>topic_modeling.py</kbd> file. If you run this code, you will see the following printed in your Terminal:</li>
</ol>
<pre style="padding-left: 60px"><strong>Most contributing words to the topics:</strong><br/><strong>Topic 0 ==&gt; 0.057*"need" + 0.034*"order" + 0.034*"work" + 0.034*"modern"</strong><br/><strong>Topic 1 ==&gt; 0.057*"need" + 0.034*"train" + 0.034*"club" + 0.034*"develop"</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><strong>Topic modeling</strong> works by identifying the important words or themes in a document. These words tend to determine what the topic is about. We use a regular expression tokenizer, because we just want the words, without any punctuation or other kinds of tokens. Hence, we use this to extract the tokens. The stop word removal is another important step, because this helps us to eliminate the noise caused by words such as <strong>is</strong> or <strong>the</strong>. After that, we need to stem the words to get to their base forms. This entire thing is packaged as a preprocessing block in text analysis tools. That is what we are doing here, as well!</p>
<p>We use a technique called LDA to model the topics. LDA basically represents the documents as a mixture of different topics that tend to spit out words. These words are spat out with certain probabilities. The goal is to find these topics! This is a generative model that tries to find the set of topics that are responsible for the generation of the given set of documents. </p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>As you can see from the output, we have words such as <span class="packt_screen">talent</span> and <span class="packt_screen">train</span> to characterize the sports topic, whereas we have <span class="packt_screen">encrypt</span> to characterize the cryptography topic. We are working with a really small text file, which is why some words might seem less relevant. Obviously, the accuracy will improve if you work with a larger dataset.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root">The official documentation of the <kbd>gensim</kbd> library: <a href="https://radimrehurek.com/gensim/install.html">https://radimrehurek.com/gensim/install.html</a></li>
<li><em>Introduction to Latent Dirichlet Allocation</em> (from MIT): <a href="http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/">http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/</a></li>
<li><em>Topic Modeling</em> (from Columbia University): <a href="http://www.cs.columbia.edu/~blei/topicmodeling.html">http://www.cs.columbia.edu/~blei/topicmodeling.html</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Parts of speech tagging with spaCy</h1>
                </header>
            
            <article>
                
<p><strong>Parts</strong>-<strong>of</strong>-<strong>speech tagging</strong> (<strong>PoS tagging</strong>) is the process of labeling the words that correspond to particular lexical categories. The common linguistic categories include nouns, verbs, adjectives, articles, pronouns, adverbs, conjunctions, and so on.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this recipe, we will use a library called <kbd>spacy</kbd> to perform <span>PoS tagging</span>. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's look at how to perform PoS tagging using <kbd>spacy</kbd>:</p>
<ol>
<li>Create a new Python file and import the following packages (the full code is in the<span> </span><kbd>PosTagging.py</kbd><span> </span>file that's already been provided to you):</li>
</ol>
<pre style="padding-left: 60px">import spacy</pre>
<ol start="2">
<li>Load the <span><kbd>en_core_web_sm</kbd> model</span>:</li>
</ol>
<pre style="padding-left: 60px">nlp = spacy.load('en_core_web_sm')</pre>
<p class="mce-root"/>
<ol start="3">
<li>Let's define an input text:</li>
</ol>
<pre style="padding-left: 60px"> Text = nlp(u'We catched fish, and talked, and we took a swim now and then to keep off sleepiness')</pre>
<p style="padding-left: 60px">As a source, I used a passage based on the novel <em>The Adventures of Huckleberry Finn</em> by Mark Twain.</p>
<ol start="4">
<li>Finally, we will perform a <span>PoS tagging</span>:</li>
</ol>
<pre style="padding-left: 60px">for token in Text:<br/>    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,<br/>          token.shape_, token.is_alpha, token.is_stop) </pre>
<ol start="5">
<li>The following results are returned:</li>
</ol>
<pre style="padding-left: 60px"><strong>We -PRON- PRON PRP nsubj Xx True False</strong><br/><strong>catched catch VERB VBD ROOT xxxx True False</strong><br/><strong>fish fish NOUN NN dobj xxxx True False</strong><br/><strong>, , PUNCT , punct , False False</strong><br/><strong>and and CCONJ CC cc xxx True True</strong><br/><strong>talked talk VERB VBD conj xxxx True False</strong><br/><strong>, , PUNCT , punct , False False</strong><br/><strong>and and CCONJ CC cc xxx True True</strong><br/><strong>we -PRON- PRON PRP nsubj xx True True</strong><br/><strong>took take VERB VBD conj xxxx True False</strong><br/><strong>a a DET DT det x True True</strong><br/><strong>swim swim NOUN NN dobj xxxx True False</strong><br/><strong>now now ADV RB advmod xxx True True</strong><br/><strong>and and CCONJ CC cc xxx True True</strong><br/><strong>then then ADV RB advmod xxxx True True</strong><br/><strong>to to PART TO aux xx True True</strong><br/><strong>keep keep VERB VB conj xxxx True True</strong><br/><strong>off off PART RP prt xxx True True</strong><br/><strong>sleepiness sleepiness NOUN NN dobj xxxx True False</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>PoS tagging involves assigning a tag to each word of a document/corpus. The choice of the tagset to use depends on the language. The input is a string of words and the tagset to be used, and the output is the association of the best tag with each word. There may be multiple tags compatible with a word (<strong>ambiguity</strong>). The task of the PoS tagger is to solve these ambiguities by choosing the most appropriate tags, based on the context in which the word is located.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p><span>To perform a PoS</span><span> tagging, we used the <kbd>spacy</kbd> library. This library extracts linguistic features, such as PoS tags, dependency labels, and named entities, customizing the tokenizer and working with the rule-based matcher.</span></p>
<p class="mce-root"><span>To install the <kbd>en_core_web_sm</kbd> model, use the following code:</span></p>
<pre><strong>$ python -m spacy download en</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root">The official documentation of the <kbd>spacy</kbd> library: <a href="https://spacy.io/usage/linguistic-features">https://spacy.io/usage/linguistic-features</a></li>
<li><em>Parts-of-Speech Tagging</em> (from New York University): <a href="https://cs.nyu.edu/courses/fall16/CSCI-UA.0480-006/lecture4-hmm.pdf">https://cs.nyu.edu/courses/fall16/CSCI-UA.0480-006/lecture4-hmm.pdf</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Word2Vec using gensim</h1>
                </header>
            
            <article>
                
<p><strong>Word embedding</strong> allows us to memorize both the semantic and syntactic information of words, starting with an unknown corpus and constructing a vector space in which the vectors of words are closer if the words occur in the same linguistic contexts, that is, if they are recognized as semantically similar. Word2Vec is a set of templates that are used to produce word embedding; the package was originally created in C by Tomas Mikolov, and was then implemented in Python and Java.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this recipe, we will use the <kbd>gensim</kbd> library to build a<span> Word2Vec model</span>. </p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's look at how to perform word embedding by using <kbd>gensim</kbd>:</p>
<ol>
<li>Create a new Python file and import the following packages (the full code is in the <kbd>GensimWord2Vec.py</kbd><span> </span>file that's already been provided to you):</li>
</ol>
<pre style="padding-left: 60px">import gensim<br/>from nltk.corpus import abc</pre>
<ol start="2">
<li>Build a model based on the Word2Vec methodology:</li>
</ol>
<pre style="padding-left: 60px">model= gensim.models.Word2Vec(abc.sents())</pre>
<ol start="3">
<li>Let's extract the vocabulary from the data and put it into a <kbd>list</kbd>:</li>
</ol>
<pre style="padding-left: 60px"> X= list(model.wv.vocab)</pre>
<ol start="4">
<li>Now, we will find similarities with the word <kbd>'science'</kbd>:</li>
</ol>
<pre>data=model.wv.most_similar('science')</pre>
<ol start="5">
<li>Finally, we will <kbd>print</kbd> the <kbd>data</kbd>:</li>
</ol>
<pre style="padding-left: 60px">print(data)</pre>
<p style="padding-left: 60px">The following results will be returned:</p>
<pre><strong>[('law', 0.938495397567749), ('general', 0.9232532382011414), ('policy', 0.9198083877563477), ('agriculture', 0.918685793876648), ('media', 0.9151924252510071), ('discussion', 0.9143469929695129), ('practice', 0.9138249754905701), ('reservoir', 0.9102856516838074), ('board', 0.9069126844406128), ('tight', 0.9067160487174988)]</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Word2Vec is a simple two-layer artificial neural network that was designed to process natural language; the algorithm requires a corpus in the input and returns a set of vectors that represent the semantic distributions of words in the text. For each word contained in the corpus, in a univocal way, a vector is constructed in order to represent it as a point in the created multidimensional space. In this space, the words will be closer if they are recognized as semantically more similar.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>In this recipe, we used the Australian National Corpus (<kbd>abc</kbd>), a great collection of language data, both text-based and digital. To use this corpus, you must download it with the following code:</p>
<pre style="padding-left: 60px" class="mce-root">import nltk<br/>nltk.download('abc')</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root">The official documentation of the <kbd>gensim</kbd> library: <a href="https://radimrehurek.com/gensim/">https://radimrehurek.com/gensim/</a></li>
<li><em>Word2vec</em> (from Wikipedia): <a href="https://en.wikipedia.org/wiki/Word2vec">https://en.wikipedia.org/wiki/Word2vec</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Shallow learning for spam detection</h1>
                </header>
            
            <article>
                
<p><strong>Spamming</strong> means sending large amounts of unwanted messages (usually commercial). It can be implemented through any medium, but the most commonly used are email and SMS. The main purpose of spamming is advertising, from the most common commercial offers to proposals for the sale of illegal material, such as pirated software and drugs without a prescription, and from questionable financial projects to genuine attempts at fraud.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this recipe, we will use a logistic regression model for spam detection. To do this, a collection of labeled SMS messages collected for mobile phone spam research will be used. This dataset comprises of 5,574 real English non-encoded messages, tagged according to whether they are legitimate (<kbd>ham</kbd>) or spamming (<kbd>spam</kbd>). </p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's look at how to perform shallow learning for spam detection:</p>
<ol>
<li>Create a new Python file and import the following packages (the full code is in the <kbd>LogiTextClassifier.py</kbd> file that's already been provided to you):</li>
</ol>
<pre style="padding-left: 60px">import pandas as pd<br/>from sklearn.feature_extraction.text import TfidfVectorizer<br/>from sklearn.linear_model.logistic import LogisticRegression<br/>from sklearn.model_selection import train_test_split</pre>
<ol start="2">
<li>Load the <kbd>spam.csv</kbd> file that was provided to you:</li>
</ol>
<pre style="padding-left: 60px">df = pd.read_csv('spam.csv', sep=',',header=None, encoding='latin-1')</pre>
<ol start="3">
<li>Let's extract the data for training and testing:</li>
</ol>
<pre style="padding-left: 60px">X_train_raw, X_test_raw, y_train, y_test = train_test_split(df[1],df[0])</pre>
<ol start="4">
<li>We need to <kbd>vectorize</kbd> the text data contained in the DataFrame:</li>
</ol>
<pre style="padding-left: 60px">vectorizer = TfidfVectorizer()<br/>X_train = vectorizer.fit_transform(X_train_raw)</pre>
<ol start="5">
<li>We can now build the logistic regression model:</li>
</ol>
<pre style="padding-left: 60px">classifier = LogisticRegression(solver='lbfgs', multi_class='multinomial')<br/>classifier.fit(X_train, y_train)</pre>
<ol start="6">
<li>Define two SMS messages as test data:</li>
</ol>
<pre style="padding-left: 60px">X_test = vectorizer.transform( ['Customer Loyalty Offer:The NEW Nokia6650 Mobile from ONLY å£10 at TXTAUCTION!', 'Hi Dear how long have we not heard.'] )</pre>
<ol start="7">
<li>Finally, we will perform a prediction by using the model:</li>
</ol>
<pre style="padding-left: 60px">predictions = classifier.predict(X_test)<br/>print(predictions)</pre>
<p style="padding-left: 60px">The following results will be returned:</p>
<pre style="padding-left: 60px"><strong>['spam' 'ham']</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 60px">These indicate that the first SMS was identified as <kbd>spam</kbd>, while the second SMS was identified as <kbd>ham</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Logistic regression analysis is a method for estimating the regression function that best links the probability of a dichotomous attribute with a set of explanatory variables. <strong>Logistic assault</strong> is a nonlinear regression model that's used when the dependent variable is dichotomous. The objective of the model is to establish the probability with which an observation can generate one or the other values of the dependent variable; it can also be used to classify the observations into two categories, according to their characteristics.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">In addition to the measurement scale of the dependent variable, logistic regression analysis is distinguished from linear regression because a normal distribution of <em>y</em> is assumed for this, whereas if <em>y</em> is dichotomous, its distribution is obviously binomial. Similarly, in linear regression analysis, the <em>y</em> estimate obtained from the regression varies from -∞ to + ∞, while in logistic regression analysis, the <em>y</em> estimate varies between 0 and 1.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root">The official documentation of the <kbd>sklearn.linear_model.LogisticRegression()</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html</a></li>
<li class="mce-root">
<p class="mce-root">The official documentation of the <kbd>sklearn.feature_extraction.text.TfidfVectorizer()</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html">https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html</a></p>
</li>
<li class="mce-root">
<p class="mce-root"><span><em>Regression Analysis with R</em> by Giuseppe Ciaburro,</span><span> from Packt Publishing</span></p>
</li>
<li class="mce-root">
<p class="mce-root"><em>Logistic Regression</em> <span>(from the University of Sheffield):</span> <a href="https://www.sheffield.ac.uk/polopoly_fs/1.233565!/file/logistic_regression_using_SPSS_level1_MASH.pdf">https://www.sheffield.ac.uk/polopoly_fs/1.233565!/file/logistic_regression_using_SPSS_level1_MASH.pdf</a></p>
</li>
</ul>


            </article>

            
        </section>
    </body></html>