- en: Chapter 2. Analyzing Images to Recognize a Face
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*"We can use the Computer Vision API to prove to our clients the reliability
    of the data, so they can be confident making important business decisions based
    on that information."*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- Leendert de Voogd, CEO of Vigiglobe'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In the previous chapter, you were briefly introduced to Microsoft Cognitive
    Services. Throughout this chapter, we will dive into image-based APIs from the
    vision API. We will learn how to perform image analysis. Moving on, we will dive
    deeper into the Face API, which we briefly looked at in the previous chapter,
    and we will learn how you can identify people. Next, we will learn how to use
    the Face API to recognize emotions in faces. Finally, we will learn about the
    different ways to moderate content.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing images to identify content, metadata, and adult ratings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recognizing celebrities in images and reading text in images.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Diving into the Face API:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning to find the likelihood of two faces belonging to the same person
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Grouping faces based on visual similarities and searching similar faces
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying a person from a face
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Recognizing emotions
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Content moderation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze an image using the Computer Vision API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Computer Vision API allows us to process an image and retrieve information
    about it. It relies on advanced algorithms to analyze the content of the image
    in different ways, based on our needs.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this section, we will learn how to take advantage of this API. We
    will look at the different ways to analyze an image through standalone examples.
    Some of the features we will cover will also be incorporated into our end-to-end
    application in a later chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Calling any of the APIs will return one of the following response codes:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Code | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `200` | Information of the extracted features in JSON format. |'
  prefs: []
  type: TYPE_TB
- en: '| `400` | Typically, this means bad request. It may be an invalid image URL,
    an image that is too small or too large, an invalid image format, or any other
    errors to do with the request body. |'
  prefs: []
  type: TYPE_TB
- en: '| `415` | Unsupported media type. |'
  prefs: []
  type: TYPE_TB
- en: '| `500` | Possible errors may include a failure to process the image, image
    processing timing out, or an internal server error. |'
  prefs: []
  type: TYPE_TB
- en: Setting up a chapter example project
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we go into the specifics of the API, we need to create an example project
    for this chapter. This project will contain all of the examples, which will not
    be put into the end-to-end application at this stage:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you have not already done so, sign up for an API key for Computer Vision
    by visiting [https://portal.azure.com](https://portal.azure.com).
  prefs: []
  type: TYPE_NORMAL
- en: Create a new project in Visual Studio using the template we created in [Chapter
    1](ch01.html "Chapter 1. Getting Started with Microsoft Cognitive Services"),
    *Getting Started with Microsoft Cognitive Services*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Right-click on the project and choose **Manage NuGet Packages**. Search for
    the `Microsoft.ProjectOxford.Vision` package and install it into the project,
    as shown in the following screenshot:![Setting up a chapter example project](img/B12373_02_01.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create the following `UserControls` files and add them into the `ViewModel`
    folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`CelebrityView.xaml`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DescriptionView.xaml`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ImageAnalysisView.xaml`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OcrView.xaml`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ThumbnailView.xaml`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Also, add the corresponding `ViewModel` instances from the following list into
    the `ViewModel` folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`CelebrityViewModel.cs`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DescriptionViewModel.cs`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ImageAnalysisViewModel.cs`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OcrViewModel.cs`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ThumbnailViewModel.cs`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Go through the newly created `ViewModel` instances and make sure that all classes
    are public.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will switch between the different views using a `TabControl` tag. Open the
    `MainView.xaml` file and add the following in the precreated `Grid` tag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This will add a tab bar at the top of the application that will allow you to
    navigate between the different views.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will add the properties and members required in our `MainViewModel.cs`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the variable used to access the Computer Vision API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code declares a private variable holding the `CelebrityViewModel`
    object. It also declares the `public` property that we use to access the `ViewModel`
    in our `View`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Following the same pattern, add properties for the rest of the created `ViewModel`
    instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'With all the properties in place, create the `ViewModel` instances in our constructor
    using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note how we first create the `VisionServiceClient` object with the API key that
    we signed up for earlier and the root URI, as described in [Chapter 1](ch01.html
    "Chapter 1. Getting Started with Microsoft Cognitive Services"), *Getting Started
    with Microsoft Cognitive Services*. This is then injected into all the `ViewModel`
    instances to be used there.
  prefs: []
  type: TYPE_NORMAL
- en: 'This should now compile and present you with the application shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Setting up a chapter example project](img/B12373_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Generic image analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We start enabling generic image analysis by adding a UI to the `ImageAnalysis.xaml`
    file. All the Computer Vision example UIs will be built in the same manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'The UI should have two columns, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The first one will contain the image selection, while the second one will display
    our results.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the left-hand column, we create a vertically oriented `StackPanel` label.
    To this, we add a label and a `ListBox` label. The list box will display a list
    of visual features that we can add to our analysis query. Note how we have a `SelectionChanged`
    event hooked up in the `ListBox` label in the following code. This will be added
    behind the code, and will be covered shortly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The list box will be able to select multiple items, and the items will be gathered
    in the `ViewModel`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the same stack panel, we also add a button element and an image element.
    These will allow us to browse for an image, show it, and analyze it. Both the
    `Button` command and the image source are bound to the corresponding properties
    in the `ViewModel`, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We also add another vertically oriented stack panel. This will be placed in
    the right-hand column. It contains a title label, as well as a textbox, bound
    to the analysis result in our `ViewModel`, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we want to add our `SelectionChanged` event handler to our code-behind.
    Open the `ImageAnalysisView.xaml.cs` file and add the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The first line of the function will give us the current `DataContext`, which
    is the `MainViewModel` class. We access the `ImageAnalysisVm` property, which
    is our `ViewModel`, and clear the selected visual features list.
  prefs: []
  type: TYPE_NORMAL
- en: 'From there, we loop through the selected items from our list box. All items
    will be added to the `SelectedFeatures` list in our `ViewModel`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Open the `ImageAnalysisViewModel.cs` file. Make sure that the class inherits
    the `ObservableObject` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Declare a `private` variable, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This will be used to access the Computer Vision API, and it is initialized through
    the constructor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we declare a private variable and the corresponding property for our
    list of visual features, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In a similar manner, create a `BitmapImage` variable and property called `ImageSource`.
    Create a list of `VisualFeature` types called `SelectedFeatures` and a string
    called `AnalysisResult`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to declare the property for our button, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'With that in place, we create our constructor, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The constructor takes one parameter, the `IVisionServiceClient` object, which
    we have created in our `MainViewModel` file. It assigns that parameter to the
    variable that we created earlier. Then we call an `Initialize` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `Initialize` function, we fetch all the values from the `VisualFeature`
    variable of the `enum` type. These values are added to the features list, which
    is displayed in the UI. We also created our button, and now that we have done
    so, we need to create the corresponding action, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The first lines of the preceding code are similar to what we did in [Chapter
    1](ch01.html "Chapter 1. Getting Started with Microsoft Cognitive Services"),
    *Getting Started with Microsoft Cognitive Services*. We open a file browser and
    get the selected image.
  prefs: []
  type: TYPE_NORMAL
- en: 'With an image selected, we run an analyze on it, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We call the `AnalyzeImageAsync` function of our `_visionClient`. This function
    has four overloads, all of which are quite similar. In our case, we pass on the
    image as a `Stream` type and the `SelectedFeatures` list, containing the `VisualFeatures`
    variable to analyze.
  prefs: []
  type: TYPE_NORMAL
- en: 'The request parameters are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Parameter | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Image (required)** |'
  prefs: []
  type: TYPE_TB
- en: Can be uploaded in the form of a raw image binary or URL.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be JPEG, PNG, GIF, or BMP.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: File size must be less than 4 MB.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image dimensions must be at least 50 x 50 pixels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Visual features (optional)** | A list indicating the visual feature types
    to return. It can include categories, tags, descriptions, faces, image types,
    color, and whether or not it is adult content. |'
  prefs: []
  type: TYPE_TB
- en: '| **Details (optional)** | A list indicating what domain-specific details to
    return. |'
  prefs: []
  type: TYPE_TB
- en: The response to this request is the `AnalysisResult` string.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then check to see if the result is `null`. If it is not, we call a function
    to parse it and assign the result to our `AnalysisResult` string, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Remember to close the `try` clause and finish the method with the corresponding
    `catch` clause.
  prefs: []
  type: TYPE_NORMAL
- en: The `AnalysisResult` string contains data according to the visual features requested
    in the API call.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data in the `AnalysisResult` variable is described in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Visual feature | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Categories** | Images are categorized according to a defined taxonomy.
    This includes everything from animals, buildings, and outdoors, to people. |'
  prefs: []
  type: TYPE_TB
- en: '| **Tags** | Images are tagged with a list of words related to the content.
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Description** | This contains a full sentence describing the image. |'
  prefs: []
  type: TYPE_TB
- en: '| **Faces** | This detects faces in images and contains face coordinates, gender,
    and age. |'
  prefs: []
  type: TYPE_TB
- en: '| **ImageType** | This detects whether an image is clipart or a line drawing.
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Color** | This contains information about dominant colors, accent colors,
    and whether or not the image is in black and white. |'
  prefs: []
  type: TYPE_TB
- en: '| **Adult** | This detects whether an image is pornographic in nature and whether
    or not it is racy. |'
  prefs: []
  type: TYPE_TB
- en: 'To retrieve data, for example for categories, you can use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'A successful call would present us with the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generic image analysis](img/B12373_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Sometimes, you may only be interested in the image description. In such cases,
    it is wasteful to ask for the kind of full analysis that we have just done. By
    calling the following function, you will get an array of descriptions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In this call, we have specified a URL for the image and the number of descriptions
    to return. The first parameter must always be included, but it may be an image
    upload instead of a URL. The second parameter is optional, and in cases where
    it is not provided, it defaults to one.
  prefs: []
  type: TYPE_NORMAL
- en: A successful query will result in an `AnalysisResult` object, which is the same
    as the one that was described in the preceding code. In this case, it will only
    contain the request ID, image metadata, and an array of captions. Each caption
    contains an image description and the confidence of that description being correct.
  prefs: []
  type: TYPE_NORMAL
- en: We will add this form of image analysis to our smart-house application in a
    later chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Recognizing celebrities using domain models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the features of the Computer Vision API is the ability to recognize domain-specific
    content. At the time of writing, the API only supports celebrity recognition,
    where it is able to recognize around 200,000 celebrities.
  prefs: []
  type: TYPE_NORMAL
- en: For this example, we choose to use an image from the internet. The UI will then
    need a textbox to input the URL. It will need a button to load the image and perform
    the domain analysis. There should be an image element to see the image and a textbox
    to output the result.
  prefs: []
  type: TYPE_NORMAL
- en: The corresponding `ViewModel` should have two `string` properties for the URL
    and the analysis result. It should have a `BitmapImage` property for the image
    and an `ICommand` property for our button.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add a `private` variable for the `IVisionServiceClient` type at the start of
    the `ViewModel`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This should be assigned in the constructor, which will take a parameter of the
    `IVisionServiceClient` type.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we need a URL to fetch an image from the internet, we need to initialize
    the `Icommand` property with both an action and a predicate. The latter checks
    whether the URL property is set or not, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The `LoadAndFindCelebrity` load creates a `Uri` with the given URL. Using this,
    it creates a `BitmapImage` and assigns this to `ImageSource`, the `BitmapImage`
    property, as shown in the following code. The image should be visible in the UI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We call the `AnalyzeImageInDomainAsync` type with the given URL, as shown in
    the following code. The first parameter we pass in is the image URL. Alternatively,
    this could have been an image that was opened as a `Stream` type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The second parameter is the domain model name, which is in a `string` format.
    As an alternative, we could have used a specific `Model` object, which can be
    retrieved by calling the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This would return an array of `Models`, which we can display and select from.
    As there is only one available at this time, there is no point in doing so.
  prefs: []
  type: TYPE_NORMAL
- en: The result from `AnalyzeImageInDomainAsync` is an object of the `AnalysisInDomainResult`
    type. This object will contain the request ID, metadata of the image, and the
    result, containing an array of celebrities. In our case, we simply output the
    entire result array. Each item in this array will contain the name of the celebrity,
    the confidence of a match, and the face rectangle in the image. Do try it in the
    example code provided.
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing optical character recognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For some tasks, **optical character recognition** (**OCR**) can be very useful.
    Say that you took a photo of a receipt. Using OCR, you can read the amount from
    the photo itself and have it automatically added to accounting.
  prefs: []
  type: TYPE_NORMAL
- en: OCR will detect text in images and extract machine-readable characters. It will
    automatically detect language. Optionally, the API will detect image orientation
    and correct it before reading the text.
  prefs: []
  type: TYPE_NORMAL
- en: 'To specify a language, you need to use the **BCP-47** language code. At the
    time of writing, the following languages are supported: simplified Chinese, traditional
    Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Greek, Hungarian,
    Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish,
    Turkish, Arabic, Romanian, Cyrillic Serbian, Latin Serbian, and Slovak.'
  prefs: []
  type: TYPE_NORMAL
- en: In the code example, the UI will have an image element. It will also have a
    button to load the image and detect text. The result will be printed to a textbox
    element.
  prefs: []
  type: TYPE_NORMAL
- en: The `ViewModel` will need a `string` property for the result, a `BitmapImage`
    property for the image, and an `ICommand` property for the button.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add a `private` variable to the `ViewModel` for the Computer Vision API, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The constructor should have one parameter of the `IVisionServiceClient` type,
    which should be assigned to the preceding variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a function as a command for our button. Call it `BrowseAndAnalyze` and
    have it accept `object` as the parameter. Then, open a file browser and find an
    image to analyze. With the image selected, we run the OCR analysis, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: With the image opened as a `Stream` type, we call the `RecognizeTextAsync` method.
    In this case, we pass on the image as a `Stream` type, but we could just as easily
    have passed on a URL to an image.
  prefs: []
  type: TYPE_NORMAL
- en: Two more parameters may be specified in this call. First, you can specify the
    language of the text. The default is unknown, which means that the API will try
    to detect the language automatically. Second, you can specify whether or not the
    API should detect the orientation of the image. The default is set to `false`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the call succeeds, it will return data in the form of an `OcrResults` object.
    We send this result to a function, the `PrintOcrResult` function, where we will
    parse it and print the text, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we create a `StringBuilder` object, which will hold all the text. The
    first content we add to it is the language of the text in the image, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The result has an array, which contains the `Regions` property. Each item represents
    recognized text, and each region contains multiple lines. The `line` variables
    are arrays, where each item represents recognized text. Each line contains an
    array of the `Words` property. Each item in this array represents a recognized
    word.
  prefs: []
  type: TYPE_NORMAL
- en: 'With all the words appended to the `StringBuilder` function, we return it as
    a string. This will then be printed in the UI, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Utilizing optical character recognition](img/B12373_02_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The result also contains the orientation and angle of the text. Combining this
    with the bounding box, also included, you can mark each word in the original image.
  prefs: []
  type: TYPE_NORMAL
- en: Generating image thumbnails
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In today's world, we, as developers, have to consider different screen sizes
    when displaying images. The Computer Vision API offers some help with this by
    providing the ability to generate thumbnails.
  prefs: []
  type: TYPE_NORMAL
- en: Thumbnail generation, in itself, is not that big a deal. What makes the API
    clever is that it analyzes the image and determines the region of interest.
  prefs: []
  type: TYPE_NORMAL
- en: It will also generate smart cropping coordinates. This means that if the specified
    aspect ratio differs from the original, it will crop the image, with a focus on
    the interesting regions.
  prefs: []
  type: TYPE_NORMAL
- en: In the example code, the UI consists of two image elements and one button. The
    first image is the image in its original size. The second is for the generated
    thumbnail, which we specify to be 250 x 250 pixels in size.
  prefs: []
  type: TYPE_NORMAL
- en: The `View` model will need the corresponding properties, two `BitmapImages`
    methods to act as image sources, and one `ICommand` property for our button command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a private variable in the `ViewModel`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: This will be our API access point. The constructor should accept an `IVisionServiceClient`
    object, which should be assigned to the preceding variable.
  prefs: []
  type: TYPE_NORMAL
- en: For the `ICommand` property, we create a function, `BrowseAndAnalyze`, accepting
    an `object` parameter. We do not need to check whether we can execute the command.
    We will browse for an image each time.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `BrowseAndAnalyze` function, we open a file dialog and select an image.
    When we have the image file path, we can generate our thumbnail, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: We open the image file so that we have a `Stream` type. This stream is the first
    parameter in our call to the `GetThumbnailAsync` method. The next two parameters
    indicate the width and height that we want for our thumbnail.
  prefs: []
  type: TYPE_NORMAL
- en: By default, the API call will use smart cropping, so we do not have to specify
    it. If we have a case where we do not want smart cropping, we could add a `bool`
    variable as the fourth parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the call succeeds, we get a `byte` array back. This is the image data. If
    it contains data, we pass it on to a new function, `CreateThumbnail`, to create
    a `BitmapImage` object from it, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: To create an image from a `byte` array, we create a `MemoryStream` object from
    it. We make sure that we start at the beginning of the array.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create a `BitmapImage` object and begin to initialize it. We specify
    the `CacheOption` and set the `StreamSource` to the `MemoryStream` variables we
    created earlier. Finally, we stop the `BitmapImage` initialization and assign
    the image to our `Thumbnail` property, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Close up the `try` clause and add the corresponding `catch` clause. You should
    now be able to generate thumbnails.
  prefs: []
  type: TYPE_NORMAL
- en: Diving deep into the Face API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Face API has two main features. The first one is face detection and the
    other is face recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Face detection allows us to detect up to 64 faces in one image. We have already
    seen the basic usage. The features of face recognition are implied in its name:
    using it, we can detect whether two faces belong to the same person. We can find
    similar faces, or one in particular, and we can group similar faces. We will learn
    how to do all of this in the following sections.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When calling any of the APIs, it will respond with one of the following responses:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Code | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `200` | Successful call. It returns an array containing data related to the
    API call. |'
  prefs: []
  type: TYPE_TB
- en: '| `400` | Request body is invalid. This can be a number of errors, depending
    on the API call. Typically, the request code is invalid. |'
  prefs: []
  type: TYPE_TB
- en: '| `401` | Access denied because of an invalid subscription key. The key may
    be wrong or the account/subscription plan may be blocked. |'
  prefs: []
  type: TYPE_TB
- en: '| `403` | Out of call volume data. You have made all the available calls to
    the API for this month. |'
  prefs: []
  type: TYPE_TB
- en: '| `415` | Invalid media type. |'
  prefs: []
  type: TYPE_TB
- en: '| `429` | Rate limit is exceeded. You will need to wait a period of time (less
    than one minute in the free preview) before you try again. |'
  prefs: []
  type: TYPE_TB
- en: Retrieving more information from the detected faces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [Chapter 1](ch01.html "Chapter 1. Getting Started with Microsoft Cognitive
    Services"), *Getting Started with Microsoft Cognitive Services*, we learned the
    very basic form of face detection. In the example, we retrieved a `Face` array.
    This contained information on all faces that were found in an image. In that specific
    example, we obtained information about the face rectangle, face ID, face landmarks,
    and age.
  prefs: []
  type: TYPE_NORMAL
- en: 'When calling the API, there are four request parameters, as shown in the following
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Parameter | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `image` |'
  prefs: []
  type: TYPE_TB
- en: The image in which to search for faces. It will either be in the form of a URL
    or binary data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supported formats are JPEG, PNG, GIF, and BMP.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum file size is 4 MB.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size of detectable faces is between 36 x 36 pixels and 4096 x 4096 pixels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `return FaceId` (optional) | Boolean value. This specifies whether the response
    should include the face ID or not. |'
  prefs: []
  type: TYPE_TB
- en: '| `return FaceLandmarks` (optional) | Boolean value. This specifies whether
    the response should include `FaceLandmarks` in detected faces. |'
  prefs: []
  type: TYPE_TB
- en: '| `return FaceAttributes` (optional) |'
  prefs: []
  type: TYPE_TB
- en: String value. This is a comma-separated string containing all face attributes
    that are to be analyzed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supported attributes are age, gender, head pose, smile, facial hair, emotion,
    and glasses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These attributes are still experimental, and should be treated as such.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: If a face is successfully discovered, it will expire in 24 hours. When calling
    other parts of the Face API, you are often required to have a face ID as an input.
    In those cases, we need to detect a face first, followed by the call to the API
    we wish to use, using the detected face as a parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Using this knowledge, I challenge you to play around with the example in [Chapter
    1](ch01.html "Chapter 1. Getting Started with Microsoft Cognitive Services"),
    *Getting Started with Microsoft Cognitive Services*. Draw a rectangle around the
    face. Mark the eyes in the image.
  prefs: []
  type: TYPE_NORMAL
- en: Deciding whether two faces belong to the same person
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To decide whether two faces belong to the same person, we are going to call
    the `Verify` function of the API. The API allows us to detect when two faces are
    of the same person, which is called **face-to-face verification**. Detecting whether
    a face belongs to a specific person is called **face-to-person verification**.
  prefs: []
  type: TYPE_NORMAL
- en: The UI will consist of three button elements, two image elements, and one text
    block element. Two of the buttons will be used to browse for images, which are
    then shown in each image element. The last button will run the verification. The
    text block will output the result.
  prefs: []
  type: TYPE_NORMAL
- en: Lay out the UI how you want and bind the different elements to properties in
    the `ViewModel`, as we have done previously. In the `ViewModel`, there should
    be two `BitmapImage` properties for the image elements. There should be one `string`
    property, containing the verification result. Finally, there should be three `ICommand`
    properties, one for each of our buttons.
  prefs: []
  type: TYPE_NORMAL
- en: Remember to add the UI to the `MainView.xaml` file as a new `TabItem`. In addition,
    add the `ViewModel` to the `MainViewModel.cs` file, where you will also need to
    add a new variable for the `FaceServiceClient` variable. This should be created
    with the Face API key, which we signed up for in [Chapter 1](ch01.html "Chapter 1. Getting
    Started with Microsoft Cognitive Services"), *Getting Started with Microsoft Cognitive
    Services*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `ViewModel`, we need to declare the following three `private` variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: We have seen the first one before; it will access the Face API. The two `Guid`
    variables will be assigned when we have run the face detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'The constructor accepts one parameter, which is our `FaceServiceClient` object.
    This is assigned to the previously created variable, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'From the constructor, we call the `Initialize` function to create the `DelegateCommand`
    properties, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The browse commands do not need to be disabled at any point, so we just pass
    on the command function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Both functions will look similar. We call another function to browse for an
    image and detect a face. To separate each image, we pass on the image number.
  prefs: []
  type: TYPE_NORMAL
- en: The `BrowseImageAsync` function will accept an `int` type as a parameter. It
    returns a `BitmapImage` object, which we assign to the `BitmapImage` property
    bound to our UI. The first part opens a browse dialog and returns the selected
    image. We will jump in when we have the image and the path to that image.
  prefs: []
  type: TYPE_NORMAL
- en: 'We open the image as a `Stream` object. The `Stream` object is used in the
    API call to detect faces. When we call the API, we can use the default call, as
    it will return the value we are interested in, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'When the detection process has completed, we check to see which image this
    is and assign the `FaceId` parameter to the correct `Guid` variable using the
    following code. For this example, we are assuming that there will be only one
    face per image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Finish off the function by adding catch clauses as you see fit. You also need
    to create and return a `BitmapImage` parameter from the selected image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before the button for the face verification is enabled, we perform a check
    to see if both face IDs have been set using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The `VerifyFace` function is not a complex one, as you can see in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'With the face IDs set, we can make a call to the `VerifyAsync` function of
    the API. We pass on the face IDs as parameters and get a `VerifyResult` object
    in return. We use this object to provide the output, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'A successful call will return a code `200` response. The response data is a
    `bool` type variable, `isIdentical`, and a number, `confidence`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deciding whether two faces belong to the same person](img/B12373_02_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: At the time of writing, the `NuGet` package for the Face API only allows for
    face-to-face verification. If we were calling directly to the REST API, we could
    have utilized face-to-person verification as well.
  prefs: []
  type: TYPE_NORMAL
- en: To use face-to-person verification, only one image is required. You will need
    to pass on the face ID for that image. You will also need to pass on a person
    group ID, and a person ID. These are to specify a specific person group to search
    in and a certain person within that group. We will cover person groups and persons
    later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Finding similar faces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using the Face API, you can find faces similar to a provided face. The API allows
    for two search modes. Match person mode is the default mode. This will match faces
    to the same person, according to an internal same-person threshold. The other
    is match face mode, which will ignore the same-person threshold. This returns
    matches that are similar, but the similarity may be low.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the example code provided, we have three buttons in our UI: one for generating
    a face list, another for adding faces to the list, and, finally, one to find similar
    faces. We need a textbox to specify a name for the face list. For convenience,
    we add a list box, outputting the persisted face IDs from the face list. We also
    add an image element to show the image we are checking, and a textbox outputting
    the result.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the corresponding `ViewModel`, we need to add a `BitmapImage` property for
    the image element. We need two `string` properties: one for our face-list name
    and one for the API call result. To get data to our list box, we need an `ObservableCollection`
    property containing `Guids`. The buttons need to be hooked up to individual `ICommand`
    properties.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We declare two `private` variables at the start of the `ViewModel`, as shown
    in the following code. The first one is a `bool` variable to indicate whether
    or not the face list already exists. The other is used to access the Face API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The constructor should accept the `FaceServiceClient` parameter, which it assigns
    to the preceding variable. It will then call an `Initialize` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: First, we initialize the `FaceListName` property to `Chapter2`. Next, we create
    the command objects, specifying actions and predicates.
  prefs: []
  type: TYPE_NORMAL
- en: 'We finish the `Initialize` function by calling two functions, as shown in the
    following code. One checks whether the face list exists, while the second updates
    the list of face IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'To check whether a given face list exists, we first need to get a list of all
    face lists. We do this by calling the `ListFaceListsAsync` method, which will
    return a `FaceListMetadata` array. We make sure that the result has data before
    we loop through the array, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Each `FaceListMetadata` array, from the resultant array, contains a face-list
    ID, a name of the face list, and user-provided data. For this example, we are
    just interested in the name. If the face-list name that we have specified is the
    name of any face list returned, we set the `_faceListExists` parameter to `true`,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: If the face list exists, we can update the list of face IDs.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the faces in a face list, we need to get the face list first. This is
    done with a call to the Face API''s function, the `GetFaceListAsync` method. This
    requires the face-list ID to be passed as a parameter. The face-list ID needs
    to be in lowercase or digits, and can contain a maximum of 64 characters. For
    the sake of simplicity, we use the face-list name as the face ID, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: The result of this API call is a `FaceList` object, containing the face-list
    ID and face-list name. It also contains user-provided data and an array of persisted
    faces.
  prefs: []
  type: TYPE_NORMAL
- en: 'We check whether we have any data and then get the array of persisted faces.
    Looping through this array, we are able to get the `PersistedFaceId` parameter
    (as a `guid` variable) and user-provided data of each item. The persisted face
    ID is added to the `FaceIds ObservableCollection`, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Finish the function by adding the corresponding `catch` clause.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the face list does not exist and we have specified a face-list name, then
    we can create a new face list, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: First, we check to see that the face list does not exist. Using the `_faceServiceClient`
    parameter, you are required to pass on a face-list ID, a face-list name, and user
    data. As seen previously, the face-list ID needs to be lowercase characters or
    digits.
  prefs: []
  type: TYPE_NORMAL
- en: Using the REST API, the user parameter is optional, and as such, you would not
    have to provide it.
  prefs: []
  type: TYPE_NORMAL
- en: After we have created a face list, we want to ensure that it exists. We do this
    by a call to the previously created `DoesFaceListExistAsync` function. Add the
    `catch` clause to finish the function.
  prefs: []
  type: TYPE_NORMAL
- en: If the named face list exists, we can add faces to this list. Add the `AddExampleFacesToList`
    function. It should accept `object` as a parameter. I will leave the details of
    adding the images up to you. In the provided example, we get a list of images
    from a given directory and loop through it.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the file path of a given image, we open the image as a `Stream`. To optimize
    it for our similarity operation, we find the `FaceRectangle` parameter in an image.
    As there should be only one face per image in the face list, we select the first
    element in the `Face` array, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Adding the face to the face list is as simple as calling the `AddFaceToFaceListAsync`
    function. We need to specify the face-list ID and the image. The image may come
    from a `Stream` (as in our case) or a URL. Optionally, we can add user data and
    the face rectangle of the image, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The result of the API call is an `AddPersistedFaceResult` variable. This contains
    the persisted face ID, which is different from a face ID in the `DetectAsync`
    call. A face added to a face list will not expire until it is deleted.
  prefs: []
  type: TYPE_NORMAL
- en: We finish the function by calling the `UpdateFaceGuidsAsync` method.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we create our `FindSimilarFace` function, also accepting `object` as
    a parameter. To be able to search for similar faces, we need a face ID (the `Guid`
    variable) from the `DetectAsync` method. This can be called with a local image
    or from a URL. The example code opens a file browser and allows the user to browse
    for an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the face ID, we can search for similar faces, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: We call the `FindSimilarAsync` function. The first parameter is the face ID
    of the face we specified. The next parameter is the face-list ID, and the final
    parameter is the number of candidate faces returned. The default for this is 20,
    so it is often best to specify a number.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of using a face list to find similar faces, you can use an array of
    the `Guid` variable. That array should contain face IDs retrieved from the `DetectAsync`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, the NuGet API package only supports match person mode.
    If you are using the REST API directly, you can specify the mode as a parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the mode selected, the result will contain either the face ID or
    the persisted face ID of similar faces. It will also contain the confidence of
    the similarity of the given face.
  prefs: []
  type: TYPE_NORMAL
- en: 'To delete a face from the face list, call the following function in the Face
    API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'To delete a face list, call the following function in the Face API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'To update a face list, call the following function in the Face API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Grouping similar faces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you have several images of faces, one thing you may want to do is group the
    faces. Typically, you will want to group faces based on similarity, which is a
    feature the Face API provides.
  prefs: []
  type: TYPE_NORMAL
- en: By providing the API with a list of face IDs, it will respond with one or more
    groups. One group consists of faces that are similar looking. Usually, this means
    that the faces belong to the same person. Faces that cannot find any similar counterparts
    are placed in a group we'll call `MessyGroup`.
  prefs: []
  type: TYPE_NORMAL
- en: Create a new `View` called `FaceGroupingView.xaml`. The `View` should have six
    image elements, with corresponding titles and textboxes for face IDs. It should
    also have a button for our group command and a textbox to output the grouping
    result.
  prefs: []
  type: TYPE_NORMAL
- en: In the corresponding `FaceGroupingViewModel.xaml` `View` model, you should add
    the `BitmapImage` properties for all images. You should also add the `string`
    properties for the face IDs and one for the result. There is also a need for an
    `ICommand` property.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the start of the `ViewModel`, we declare some `private` variables, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: The first one is used to access the Face API. The second one contains a list
    of strings that in turn contain the location of our images. The last list contains
    the detected face IDs.
  prefs: []
  type: TYPE_NORMAL
- en: The constructor accepts a parameter of the `FaceServiceClient` type. It assigns
    it to the corresponding variable and calls the `Initialize` function. This creates
    our `ICommand` object and calls a function to add our images to the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the function that adds images, we add hardcoded image paths to our `_imageFiles`
    list. For this example, we add six. Using a `for` loop, we generate each `BitmapImage`
    property. When we have an image, we want to detect faces in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'We do not need any more data than the generated face ID, which we know is stored
    for 24 hours after detection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Assuming that there is only one face per image, we add that face ID to our `_faceIds`
    list. The image, face ID, and current iteration number in the loop are passed
    on to a new function, `CreateImageSources`. This function contains a `switch`
    case based on the iteration number. Based on the number, we assign the image and
    face ID to the corresponding image and image ID property. This is then shown in
    the UI.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have a button to group the images. To group the images, we call the Face
    API''s `GroupAsync` method, passing on an array of face IDs, as shown in the following
    code. The array of face IDs must contain at least two elements, and it cannot
    contain more than 1,000 elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The response is a `GroupResult` type, which may contain one or more groups,
    as well as the messy group. We check to see whether there is a response and then
    we parse it, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Before looking at the `ParseGroupResult` method, add the corresponding `catch`
    clause and close-up `GroupFaces` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'When parsing the results, we first create a `StringBuilder` class to hold our
    text. Then we get the `groups` from the result. A group is an array of face IDs
    of the images in that group. All groups are stored in a list, and we append the
    number of groups to the `StringBuilder` class, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'We loop through the list of groups. Inside this loop, we loop through each
    item in the group. For the sake of readability, we have a helper function to find
    the image name from the ID. It finds the index in our `_faceIds` list. This is
    then used in the image name, so if the index is `2`, the image name would be `Image
    3`. For this to give the intended effect, you must have placed the images in a
    logical order, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The `GroupResult` method may also contain a `MessyGroup` array. This is an
    array of `Guid` variables containing the face IDs in that group. We loop through
    this array and append the image name, the same way we did with the regular groups,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'We end the function by returning the `StringBuilder` function''s text, which
    will output it to the screen, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Make sure that the `ViewModel` instances have been created in the `MainViewModel.cs`
    file. Also, make sure that the `View` has been added as a `TabItem` property in
    the `MainView.xaml` file. Compile and test the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are using the sample images provided, you may end up with something
    like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Grouping similar faces](img/B12373_02_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Adding identification to our smart-house application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a part of our smart-house application, we want the application to recognize
    who we are. Doing so opens up the opportunity to get responses and actions from
    the application, tailored to you.
  prefs: []
  type: TYPE_NORMAL
- en: Creating our smart-house application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Create a new project for the smart-house application, based on the MVVM template
    we created earlier.
  prefs: []
  type: TYPE_NORMAL
- en: With the new project created, add the `Microsoft.ProjectOxford.Face` NuGet package.
  prefs: []
  type: TYPE_NORMAL
- en: As we will be building this application throughout this book, we will start
    small. In the `MainView.xaml` file, add a `TabControl` property containing two
    items. The two items should be two user controls, one called the `AdministrationView.xaml`
    file and the other called the `HomeView.xaml` file.
  prefs: []
  type: TYPE_NORMAL
- en: The administration control will be where we administer different parts of the
    application. The home control will be the starting point and the main control
    to use.
  prefs: []
  type: TYPE_NORMAL
- en: Add corresponding `ViewModel` instances to the `Views`. Make sure they are declared
    and created in `MainViewModel.cs`, as we have seen throughout this chapter. Make
    sure that the application compiles and runs before moving on.
  prefs: []
  type: TYPE_NORMAL
- en: Adding people to be identified
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we can go on to identify a person, we need to have something to identify
    them from. To identify a person, we need a `PersonGroup` property. This is a group
    that contains several `Persons` properties.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a view
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the administration control, we will execute several operations in this regard.
    The UI should contain two textbox elements, two list box elements, and six buttons.
    The two textbox elements will allow us to input a name for the person group and
    a name for the person. One list box will list all person groups that we have available.
    The other will list all the persons in any given group.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have buttons for each of the operations that we want to execute, which are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Add person group
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delete person group
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train person group
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add person
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delete person
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add person face
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `View` model should have two `ObservableCollection` properties: one of
    a `PersonGroup` type and the other of a `Person` type. We should also add three
    `string` properties. One will be for our person group name, the other for our
    person name. The last will hold some status text. We also want a `PersonGroup`
    property for the selected person group. Finally, we want a `Person` property holding
    the selected person.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our `View` model, we want to add a `private` variable for the `FaceServiceClient`
    method, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'This should be assigned in the constructor, which should accept a parameter
    of a `FaceServiceClient` type. It should also call an initialization function,
    which will initialize six `ICommand` properties. These maps to the buttons, created
    earlier. The initialization function should call the `GetPersonGroups` function
    to list all person groups available, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'The `ListPersonGroupsAsync` function does not take any parameters, and returns
    a `PersonGroup` array if successfully executed, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: We then check to see whether the array contains any elements. If it does, we
    clear out the existing `PersonGroups` list. Then we loop through each item of
    the `PersonGroup` array and add them to the `PersonGroups` list.
  prefs: []
  type: TYPE_NORMAL
- en: If no person groups exist, we can add a new one by filling in a name. The name
    you fill in here will also be used as a person group ID. This means that it can
    include numbers and English lowercase letters, the "-" character (hyphen), and
    the "_" character (underscore). The maximum length is 64 characters. When it is
    filled in, we can add a person group.
  prefs: []
  type: TYPE_NORMAL
- en: Adding person groups
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we call the `DoesPersonGroupExistAsync` function, specifying `PersonGroupName`
    as a parameter, as shown in the following code. If this is `true`, then the name
    we have given already exists, and as such, we are not allowed to add it. Note
    how we call the `ToLower` function on the name. This is so we are sure that the
    ID is in lowercase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'If the person group does not exist, we call the `CreatePersonGroupAsync` function,
    as shown in the following code. Again, we specify the `PersonGroupName` as lowercase
    in the first parameter. This represents the ID of the group. The second parameter
    indicates the name we want. We end the function by calling the `GetPersonGroups`
    function again, so we get the newly added group in our list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: The `DoesPersonGroupExistAsync` function makes one API call. It tries to call
    the `GetPersonGroupAsync` function, with the person group ID specified as a parameter.
    If the resultant `PersonGroup` list is anything but `null`, we return `true`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To delete a person group, a group must be selected as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: The API call to the `DeletePersonGroupAsync` function requires a person group
    ID as a parameter. We get this from the selected person group. If no exception
    is caught, then the call has completed successfully, and we call the `GetPersonGroups`
    function to update our list.
  prefs: []
  type: TYPE_NORMAL
- en: 'When a person group is selected from the list, we make sure that we call the
    `GetPersons` function. This will update the list of persons, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: We make sure the selected person group is not `null`. If it is not, we clear
    our `persons` list. The API call to the `GetPersonsAsync` function requires a
    person group ID as a parameter. A successful call will result in a `Person` array.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the resultant array contains any elements, we loop through it. Each `Person`
    object is added to our `persons` list, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Adding new persons
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If no persons exist, we can add new ones. To add a new one, a person group
    must be selected, and a name of the person must be filled in. With this in place,
    we can click on the **Add** button:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: The API call to the `CreatePersonAsync` function requires a person group ID
    as the first parameter. The next parameter is the name of the person. Optionally,
    we can add user data as a third parameter. In this case, it should be a string.
    When a new person has been created, we update the `persons` list by calling the
    `GetPersons` function again.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we have selected a person group and a person, then we will be able to delete
    that person, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: To delete a person, we make a call to the `DeletePersonAsync` function. This
    requires the person group ID of the person group the person lives in. It also
    requires the ID of the person we want to delete. If no exceptions are caught,
    then the call succeeded, and we call the `GetPersons` function to update our person
    list.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our administration control now looks similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Adding new persons](img/B12373_02_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Associating faces with a person
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before we can identify a person, we need to associate faces with that person.
    With a given person group and person selected, we can add faces. To do so, we
    open a file dialog. When we have an image file, we can add the face to the person,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: We open the image file as a `Stream`. This file is passed on as the third parameter
    in our call to the `AddPersonFaceAsync` function. Instead of a stream, we could
    have passed a URL to an image.
  prefs: []
  type: TYPE_NORMAL
- en: The first parameter in the call is the person group ID of the group in which
    the person lives. The next parameter is the person ID.
  prefs: []
  type: TYPE_NORMAL
- en: Some optional parameters to include are user data in the form of a string and
    a `FaceRectangle` parameter for the image. The `FaceRectangle` parameter is required
    if there is more than one face in the image.
  prefs: []
  type: TYPE_NORMAL
- en: A successful call will result in an `AddPersistedFaceResult` object. This contains
    the persisted face ID for the person.
  prefs: []
  type: TYPE_NORMAL
- en: Each person can have a maximum of 248 faces associated with it. The more faces
    you can add, the more likely it is that you will receive a solid identification
    later. The faces that you add should from slightly different angles.
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With enough faces associated with the persons, we need to train the person group.
    This is a task that is required after any change to a person or person group.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can train a person group when one has been selected, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'The call to the `TrainPersonGroupAsync` function takes a person group ID as
    a parameter, as shown in the following code. It does not return anything, and
    it may take a while to execute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'We want to ensure that the training completed successfully. To do so, we call
    the `GetPersonGroupTrainingStatusAsync` function inside a `while` loop. This call
    requires a person group ID, and a successful call results in a `TrainingStatus`
    object, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: We check the status and we show the result if it is not running. If the training
    is still running, we wait for one second and run the check again.
  prefs: []
  type: TYPE_NORMAL
- en: When the training has succeeded, we are ready to identify people.
  prefs: []
  type: TYPE_NORMAL
- en: Additional functionality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are a few API calls that we have not looked at, which will be mentioned
    briefly in the following bullet list:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To update a person group, call the following; this function does not return
    anything:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To get a person''s face, call the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: A successful call returns the persisted face ID and user-provided data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To delete a person''s face, call the following; this call does not return anything:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To update a person, call the following; this call does not return anything:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To update a person''s face, call the following; this call does not return anything:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Identifying a person
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To identify a person, we are first going to upload an image. Open the `HomeView.xaml`
    file and add a `ListBox` element to the UI. This will contain the person groups
    to choose from when identifying a person. We will need to add a button element
    to find an image, upload it, and identify the person. A `TextBox` element is added
    to show the working response. For our own convenience, we also add an image element
    to show the image we are using.
  prefs: []
  type: TYPE_NORMAL
- en: In the `View` model, add an `ObservableCollection` property of a `PersonGroup`
    type. We need to add a property for the selected `PersonGroup` type. Also, add
    a `BitmapImage` property for our image, and a string property for the response.
    We will also need an `ICommand` property for our button.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add a `private` variable for the `FaceServiceClient` type, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'This will be assigned in our constructor, which should accept a parameter of
    a `FaceServiceClient` type. From the constructor, call on the `Initialize` function
    to initialize everything, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: First, we call the `GetPersonGroups` function to retrieve all the person groups.
    This function makes a call to the `ListPersonGroupsAsync` API, which we saw earlier.
    The result is added to our `PersonGroup` list's `ObservableCollection` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we create our `ICommand` object. The `CanUploadOwnerImage` function will
    return `true` if we have selected an item from the `PersonGroup` list. If we have
    not, it will return `false`, and we will not be able to identify anyone.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `UploadOwnerImage` function, we first browse to an image and then load
    it. With an image loaded and a file path available, we can start to identify the
    person in the image, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'We open the image as a `Stream` type, as shown in the following code. Using
    this, we detect faces in the image. From the detected faces, we get all the face
    IDs in an array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: The array of face IDs will be sent as the second parameter to the `IdentifyAsync`
    API call. Remember that when we detect a face, it is stored for 24 hours. Proceeding
    to use the corresponding face ID will make sure that the service knows which face
    to use for identification.
  prefs: []
  type: TYPE_NORMAL
- en: The first parameter used is the ID of the person group we have selected. The
    last parameter in the call is the number of candidates returned. As we do not
    want to identify more than one person at a time, we specify one. Because of this,
    we should ensure that there is only one face in the image we upload.
  prefs: []
  type: TYPE_NORMAL
- en: 'A successful API call will result in an array of the `IdentifyResult` parameter,
    as shown in the following code. Each item in this array will contain candidates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'We loop through the array of results, as shown in the following code. If we
    do not have any candidates, we just break out of the loop. If, however, we do
    have candidates, we get the `PersonId` parameter of the first candidate (we asked
    for only one candidate earlier, so this is okay):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'With the `personId` parameter, we get a single `Person` object, using the API
    to call the `GetPersonAsync` function. If the call is successful, we print a welcome
    message to the correct person (as shown in the following screenshot) and break
    out of the loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Identifying a person](img/B12373_02_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Knowing your mood using the Face API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Face API allows you to recognize emotions from faces.
  prefs: []
  type: TYPE_NORMAL
- en: Research has shown that there are some key emotions that can be classified as
    cross-cultural. These are happiness, sadness, surprise, anger, fear, contempt,
    disgust, and neutral. All of these are detected by the API, which allows your
    applications to respond in a more personalized way by knowing the user's mood.
  prefs: []
  type: TYPE_NORMAL
- en: We will learn how to recognize emotions from images so that our smart-house
    application can know our mood.
  prefs: []
  type: TYPE_NORMAL
- en: Getting images from a web camera
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine that there are several cameras around your house. The smart-house application
    can see what your mood is at any time. By knowing this, it can utilize the mood
    to better predict your needs.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to add web-camera capabilities to our application. If you do not
    have a web camera, you can follow along, but load images using the techniques
    we have already seen.
  prefs: []
  type: TYPE_NORMAL
- en: First we need to add a NuGet package to our smart-house application. Search
    for `OpenCvSharp3-AnyCPU` and install the package by **shimat**. This is a package
    that allows for the processing of images, and is utilized by the next dependency
    we are going to add.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the example code provided, there is a project called `VideoFrameAnalyzer`.
    This is a project written by Microsoft that allows us to grab frame-by-frame images
    from a web camera. Using this, we are able to analyze emotions in our application.
    The use case we will execute is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting images from a web camera](img/B12373_02_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In our `HomeView.xaml` file, add two new buttons. One will be to start the web
    camera while the other will be to stop it.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the corresponding `View` model, add two `ICommand` properties for each of
    the buttons. Also add the following `private` members:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: The first one is a `FrameGrabber` object, which is from the `VideoFrameAnalyzer`
    project. The `static` member is an array of parameters for images, and is used
    when fetching web camera images. Additionally, we need to add a `CameraResult`
    class, which should be within the `ViewModel` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'We initialize the `EmotionScores` to `null`, as shown in the following code.
    This is done so that new emotion scores always will be assigned from the most
    resent analysis result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'Add an initialization of the `_frameGrabber` member in the constructor and
    add the following in the `Initialization` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: Each time a new frame is provided from the camera, an event is raised.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we receive new frames, we want to create a `BitmapImage` from it to show
    it in the UI. To do so requires us to invoke the action from the current dispatcher,
    as the event is triggered from a background thread, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: We get the `BitmapSource` of the `Frame` and create some required variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `encoder` we created, we add the `bitmapSource` and save it to the
    `memoryStream`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'This `memoryStream` is then assigned to the `BitmapImage` we created, as shown
    in the following code. This is in turn assigned to the `ImageSource`, which will
    show the frame in the UI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: As this event will be triggered a lot, we will get a fluent stream in the UI,
    and it will seem like it is a direct video feed.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our `Initialization` function, we will also need to create our `ICommand`
    for the buttons, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'To be able to start the camera, we need to have selected a person group, and
    we need to have at least one camera available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'To start a camera, we need to specify which camera to use and how often we
    want to trigger an analysis using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: If no camera is specified in `StartProcessingCameraAsync`, the first one available
    is chosen by default.
  prefs: []
  type: TYPE_NORMAL
- en: We will get back to the analysis part of this process soon.
  prefs: []
  type: TYPE_NORMAL
- en: 'To stop the camera, we run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: Letting the smart house know your mood
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now have a video from the web camera available for our use.
  prefs: []
  type: TYPE_NORMAL
- en: In the `FrameGrabber` class, there is a `Func`, which will be used for analysis
    functions. We need to create the function that will be passed on this that will
    enable emotions to be recognized.
  prefs: []
  type: TYPE_NORMAL
- en: Create a new function, `EmotionAnalysisAsync`, that accepts a `VideoFrame` as
    a parameter. The return type should be `Task<CameraResult>` and the function should
    be marked as `async`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `frame` we get as a parameter is used to create a `MemoryStream` containing
    the current frame. This will be in the JPG file format. We will find a face in
    this image, and we want to ensure that we specify that we want emotion attributes
    using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'A successful call will result in an object containing all the emotion scores,
    as shown in the following code. The scores are what we want to return:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: Catch any exceptions that may be thrown, returning `null` when they are.
  prefs: []
  type: TYPE_NORMAL
- en: We need to assign the `Initialize` function to the `Func`. We also need to add
    an event handler each time we have a new result.
  prefs: []
  type: TYPE_NORMAL
- en: 'When a new result is obtained, we grab the `EmotionScore` that is received,
    as shown in the following code. If it is `null` or does not contain any elements,
    then we do not want to do anything else:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code, we parse the emotion scores in `AnalyseEmotions`, which
    we will look at in a bit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: Using the result from `AnalyseEmotions`, we print a string to the result to
    indicate the current mood. This will need to be invoked from the current dispatcher,
    as the event has been triggered in another thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the current mood in a readable format, we parse the emotion scores in
    `AnalyseEmotions` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: With the `Scores` we get, we call a `ToRankedList` function. This will return
    a list of `KeyValuePair`, containing each emotion, along with the corresponding
    confidence. The first one will be the most likely, the second will be the second
    most likely, and so on. We only care about the most likely one, so we select it.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the top emotion score selected, we use a `switch` statement to find the
    correct emotion. This is returned and printed to the result, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'The last piece of the puzzle is to make sure that the analysis is being executed
    at a specified interval. In the `StartCamera` function, add the following line,
    just before calling `StartProcessingCamera`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: This will trigger an emotion analysis to be called every fifth second.
  prefs: []
  type: TYPE_NORMAL
- en: 'When I have a smile on my face, the application now knows that I am happy and
    can provide further interaction accordingly. If we compile and run the example,
    we should get results like those shown in the following screenshots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Letting the smart house know your mood](img/B12373_02_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As my mood changes to neutral, the application detects this as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Letting the smart house know your mood](img/B12373_02_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Automatically moderating user content
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using the content moderator API, we can add monitoring to user-generated content.
    The API is created to assist with flags and to assess and filter offensive and
    unwanted content.
  prefs: []
  type: TYPE_NORMAL
- en: Types of content moderation APIs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will quickly go through the key features of the moderation APIs in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A reference to the documentation for all APIs can be found at [https://docs.microsoft.com/nb-no/azure/cognitive-services/content-moderator/api-reference](https://docs.microsoft.com/nb-no/azure/cognitive-services/content-moderator/api-reference).
  prefs: []
  type: TYPE_NORMAL
- en: Image moderation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The image moderation API allows you to moderate images for adult and inappropriate
    content. It can also extract textual content and detect faces in images.
  prefs: []
  type: TYPE_NORMAL
- en: When using the API to evaluate inappropriate content, the API will take an image
    as input. Based on the image, it will return a Boolean value, indicating whether
    the image is appropriate or not. It will also contain a corresponding confidence
    score between 0 and 1\. The Boolean value is set based on a set of default thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: If the image contains any text, the API will use OCR to extract the text. It
    will then look for the same adult or racy content as text moderation, which we
    will get to shortly.
  prefs: []
  type: TYPE_NORMAL
- en: Some content-based applications may not want to display any personally identifiable
    information, in which case it can be wise to detect faces in images. Based on
    the information retrieved in the face-detection evaluation, you can ensure that
    no user content contains images of people.
  prefs: []
  type: TYPE_NORMAL
- en: Text moderation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using the text moderation API, you can screen text against custom and shared
    lists of text. It is able to detect personally identifiable information and profanity
    in text. In this case, personally identifiable information is the presence of
    information such as email addresses, phone numbers, and mailing addresses.
  prefs: []
  type: TYPE_NORMAL
- en: When you submit a text to be moderated, the API can detect the language used,
    if it is not stated. Screening text will automatically correct any misspelled
    words (to catch deliberately misspelled words). The results will contain the location
    of profanities and personal identifiable information in the text, as well as the
    original text, autocorrected text, and the language. Using these results, you
    can moderate content appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: Moderation tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are three ways to moderate content, enabled by the content moderator:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Human moderation**: Using teams and community to manually moderate all content'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated moderation**: Utilizing machine learning and AI to moderate at
    scale with no human interaction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hybrid moderation**: A combination of the preceding two, where people typically
    occasionally do reviews'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The common scenario used is the last one. This is where machine learning is
    used to automate the moderation process and teams of people can review the moderation.
    Microsoft have created a review tool to ease this process. This allows you to
    see through all the items for review in a web browser while using the APIs in
    your application. We will look into this tool in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Using the
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'l, head over to [https://contentmoderator.cognitive.microsoft.com/](https://contentmoderator.cognitive.microsoft.com/).
    From here, you can sign in using your Microsoft account. On your first sign-in,
    you will need to register by adding your name to the account. You will then go
    on to create a *review team*, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using the](img/B12373_02_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You can do this by selecting the region and entering a team name. You can optionally
    enter the email addresses of other people who should be part of the team. Click
    on **Create Team**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once in, you will be presented with the following dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using the](img/B12373_02_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You will be presented with the total number of images and textual content that
    are for review. You will also be presented with the total number of completed
    and pending reviews. The dashboard also lists the users that have completed reviews,
    as well as any tags used for content.
  prefs: []
  type: TYPE_NORMAL
- en: 'By selecting the **Try** option in the menu, you have the option to upload
    images or text to execute moderation online. Do this by either uploading an image
    or entering sample text in the textbox. Once done, you can select the **Review**
    option, where you will be presented with the following screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using the](img/B12373_02_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If the given content is either adult content or racist, you can click on the
    **a** or **r** buttons, respectively. For text, any profanities will be displayed.
    Once you are done marking reviews, click on **Next**. This will go through a process
    of moderating the given content.
  prefs: []
  type: TYPE_NORMAL
- en: Other tools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Apart from the APIs and the review tool, there are two other tools you can
    use, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**List manager API**: Using custom lists of images and text to moderate pre-identified
    content that you don''t wish to scan for repeatedly'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Workflow API**: Using this API, you can define conditional logic and actions
    to specify the policies used by your specific content'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To use any of these APIs, or to use the moderator APIs, you can make calls
    to specific REST APIs. To do so, you will need to use an API key and a base URL.
    These settings can be found under **Settings | Credentials** on the review tool
    website, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Other tools](img/B12373_02_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Building your own image classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Custom Vision** service allows you to build your own image classifiers.
    There might be cases where you require special images to use the image APIs. Such
    cases may be from a factory, where the equipment you need to recognize is not
    very available. You can start to build a prototype, using as little
  prefs: []
  type: TYPE_NORMAL
- en: Building a classifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To build a classifier, you will need to create a new project. Doing so will
    allow you to specify what category the images will be in. You will also select
    the classification type and project type.
  prefs: []
  type: TYPE_NORMAL
- en: Moving on, you will need to upload images. This can be done through the web
    page or through a REST API. All images must be tagged so that the classifier will
    recognize similar images later.
  prefs: []
  type: TYPE_NORMAL
- en: Once all images (at least 50) are uploaded, you must train your model. Once
    the training is complete, you will be presented with a precision percentage per
    tag. This is a measurement of the accuracy of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On the website, you can test your models. Doing so will allow you to upload
    images, which will be classified by the model. If it turns out that the model
    performs poorly, you can improve the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Improving the model involves uploading more images. Some general guidelines
    to improve the model are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Have enough images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make sure that the balance between tags is good (so that there is an equal number
    of images per tag)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a diverse set of images for training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use images that have been used for prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inspect the predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the trained model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once you are happy with the model, you can use it for predictions. The model
    can be used in one of the two following ways:'
  prefs: []
  type: TYPE_NORMAL
- en: With a REST API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Export it to a model file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first choice involves uploading an image. Calling the generated endpoint
    for your model, along with the image data, will result in a prediction. The result
    will contain the predicted tags, ordered by their probability.
  prefs: []
  type: TYPE_NORMAL
- en: The second choice allows you to run the prediction offline. This means that
    you can utilize different frameworks, such as TensorFlow, CoreML, and ONNX, for
    different platforms. How to use the model with these frameworks is beyond the
    scope of this book. The downside of using an offline model is that the accuracy
    may suffer a bit compared to the online version.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we took a deep dive into a big part of the vision APIs. You
    first learned how to get good descriptions of images. Next, you learned how to
    recognize celebrities and text in images, and you learned how to generate thumbnails.
    Following this, we moved on to the Face API, where we got more information about
    detected faces. We found out how to verify whether two faces were the same. After
    this, you learned how to find similar faces and group similar faces. Then we added
    identification to our smart-house application, allowing it to know who we are.
    We also added the ability to recognize emotions in faces. We took a quick look
    into the content moderator to see how you can add automatic moderation to user-generated
    content. Finally, we briefly looked at the Custom Vision service, and how you
    can use it to generate specific prediction models.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will continue with the final vision API. We will focus on videos,
    learning what the video indexer API has to offer.
  prefs: []
  type: TYPE_NORMAL
