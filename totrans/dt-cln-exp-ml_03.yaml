- en: '*Chapter 2*: Examining Bivariate and Multivariate Relationships between Features
    and Targets'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we'll look at the correlation between possible features and
    target variables. Bivariate exploratory analysis, using crosstabs (two-way frequencies),
    correlations, scatter plots, and grouped boxplots can uncover key issues for modeling.
    Common issues include high correlation between features and non-linear relationships
    between features and the target variable. We will use pandas methods for bivariate
    analysis and Matplotlib for visualizations in this chapter. We will also discuss
    the implications of what we find in terms of feature engineering and modeling.
  prefs: []
  type: TYPE_NORMAL
- en: We will also use multivariate techniques to understand the relationship between
    features. This includes leaning on some machine learning algorithms to identify
    possibly problematic observations. After, we will provide tentative recommendations
    for eliminating certain observations from our modeling, as well as for transforming
    key features.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Identifying outliers and extreme values in bivariate relationships
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using scatter plots to view bivariate relationships between continuous features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using grouped boxplots to view bivariate relationships between continuous and
    categorical features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using linear regression to identify data points with significant influence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using K-nearest neighbors to find outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Isolation Forest to find outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will rely heavily on the pandas and Matplotlib libraries, but you
    don't require any prior knowledge of these. If you have installed Python from
    a scientific distribution, such as Anaconda or WinPython, then these libraries
    have probably already been installed. We will also be using Seaborn for some of
    our graphics and the statsmodels library for some summary statistics. If you need
    to install any of the packages, you can do so by running `pip install [package
    name]` from a terminal window or Windows PowerShell. The code for this chapter
    can be found in this book's GitHub repository at [https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning](https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning).
  prefs: []
  type: TYPE_NORMAL
- en: Identifying outliers and extreme values in bivariate relationships
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is hard to develop a reliable model without having a good sense of the bivariate
    relationships in our data. We not only care about the relationship between particular
    features and target variables but also about how features move together. If features
    are highly correlated, then modeling their independent effect becomes tricky or
    unnecessary. This may be a challenge, even if the features are highly correlated
    over just a range of values.
  prefs: []
  type: TYPE_NORMAL
- en: Having a good understanding of bivariate relationships is also important for
    identifying outliers. A value might be unexpected, even if it is not an extreme
    value. This is because some values for a feature are unusual when a second feature
    has certain values. This is easy to illustrate when one feature is categorical
    and the other is continuous.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the number of bird sightings per day over
    several years but shows different distributions for the two sites. One site has
    a (mean) sightings per day of 33, while the other has 52\. (This is a fictional
    example that''s been pulled from my *Python Data Cleaning Cookbook*.) The overall
    mean (not shown) is 42\. What should we make of a value of 58 for daily sightings?
    Is it an outlier? This depends on which of the two sites was being observed. If
    there were 58 sightings in a day at site A, 58 would be an unusually high number.
    However, this wouldn''t be true for site B, where 58 sightings would not be very
    different from the mean for that site:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – Daily Bird Sightings ](img/B17978_02_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – Daily Bird Sightings
  prefs: []
  type: TYPE_NORMAL
- en: 'This hints at a useful rule of thumb: whenever a feature of interest is correlated
    with another feature, we should take that relationship into account when we''re
    trying to identify outliers (or any modeling with that feature, actually). It
    is helpful to state this a little more precisely and extend it to cases where
    both features are continuous. If we assume a linear relationship between feature
    *x* and feature *y*, we can describe that relationship with the familiar *y =
    mx + b* equation, where *m* is the slope and *b* is the *y*-intercept. Then, we
    can expect the value of *y* to be somewhere close to *x* times the estimated slope,
    plus the *y*-intercept. Unexpected values are those that deviate substantially
    from this relationship, where the value of *y* is much higher or lower than what
    would be predicted, given the value of *x*. This can be extended to multiple *x*,
    or predictor, variables.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will learn how to identify outliers and unexpected values
    by examining the relationship a feature has with another feature. In subsequent
    sections of this chapter, we will use multivariate techniques to make additional
    improvements to our outlier detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will work with data based on COVID-19 cases by country in this section.
    The dataset contains cases and deaths per million people in the population. We
    will treat both columns as possible targets. It also contains demographic data
    for each country, such as GDP per capita, median age, and diabetes prevalence.
    Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Our World in Data provides COVID-19 public-use data at [https://ourworldindata.org/coronavirus-source-data](https://ourworldindata.org/coronavirus-source-data).
    The dataset that's being used in this section was downloaded on July 9, 2021\.
    There are more columns in the data than I have included. I created the `region`
    column based on country.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by loading the COVID-19 dataset and looking at how it is structured.
    We will also import the Matplotlib and Seaborn libraries since we will do a couple
    of visualizations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A great place to start with our examination of bivariate relationships is with
    correlations. First, let''s create a DataFrame that contains a few key features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can get the Pearson correlation matrix for these features. There is
    a strong positive correlation of 0.71 between cases and deaths per million. The
    percentage of the population that''s aged 65 or older is positively correlated
    with cases and deaths, at 0.53 for both. Life expectancy is also highly correlated
    with cases per million. There seems to be at least some correlation of **gross
    domestic product** (**GDP**) per person with cases:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It is worth noting the correlation between possible features, such as between
    life expectancy and GDP per capita (0.68) and life expectancy and those aged 65
    or older (0.73).
  prefs: []
  type: TYPE_NORMAL
- en: 'It can be helpful to see the correlation matrix as a heat map. This can be
    done by passing the correlation matrix to the Seaborn `heatmap` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This creates the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – Heat map of COVID data, with the strongest correlations in red
    and peach ](img/B17978_02_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – Heat map of COVID data, with the strongest correlations in red
    and peach
  prefs: []
  type: TYPE_NORMAL
- en: We want to pay attention to the cells shown with warmer colors – in this case,
    mainly peach. I find that using a heat map helps me keep correlations in mind
    when modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: All the color images contained in this book can be downloaded. Check the *Preface*
    of this book for the respective link.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a closer look at the relationship between total cases per million
    and deaths per million. One way to get a better sense of this than with just a
    correlation coefficient is by comparing the high and low values for each and seeing
    how they move together. In the following code, we''re using the `qcut` method
    to create a categorical feature with five values distributed relatively evenly,
    from very low to very high, for cases. We have done the same for deaths:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can use the `crosstab` function to view the number of countries for each
    quintile of cases and quintile of deaths. As we would expect, most of the countries
    are along the diagonal. There are 27 countries with very low cases and very low
    deaths, and 25 countries with very high cases and very high deaths. The interesting
    counts are those not on the diagonal, such as the four countries with very high
    cases but only medium deaths, nor the one with medium cases and very high deaths.
    Let''s also look at the means of our features so that we can reference them later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s take a closer look at the countries away from the diagonal. Four countries
    – Cyprus, Kuwait, Maldives, and Qatar – have fewer deaths per million than average
    but well above average cases per million. Interestingly, all four countries are
    very small in terms of population; three of the four have population densities
    far below the average of 453; again, three of the four have people aged 65 or
    older percentages that are much lower than average:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s take a closer look at the country with more deaths than we would have
    expected based on cases. For Mexico, the number of cases per million are well
    below average, while the number of deaths per million are quite a bit above average:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Correlation coefficients and heat maps are a good place to start when we want
    to get a sense of the bivariate relationships in our dataset. However, it can
    be hard to visualize the relationship between continuous variables with just a
    correlation coefficient. This is particularly true when the relationship is not
    linear – that is, when it varies based on the ranges of a feature. We can often
    improve our understanding of the relationship between two features with a scatter
    plot. We will do that in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Using scatter plots to view bivariate relationships between continuous features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll learn how to get a scatter plot of our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use scatter plots to get a more complete picture of the relationship
    between two features than what can be detected by a correlation coefficient alone.
    This is particularly useful when that relationship changes across certain ranges
    of the data. In this section, we will create scatter plots of some of the same
    features we examined in the previous section. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is helpful to plot a regression line through the data points. We can do
    this with Seaborn''s `regplot` method. Let''s load the COVID-19 data again, along
    with the Matplotlib and Seaborn libraries, and generate a scatter plot of `total_cases_mill`
    by `total_deaths_mill`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – Total COVID Cases and Deaths by Country ](img/B17978_02_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – Total COVID Cases and Deaths by Country
  prefs: []
  type: TYPE_NORMAL
- en: The regression line is an estimate of the relationship between cases per million
    and deaths per million. The slope of the line indicates how much we can expect
    deaths per million to increase with a 1-unit increase in cases per million. Those
    points on the scatter plot that are significantly above the regression line should
    be examined more closely.
  prefs: []
  type: TYPE_NORMAL
- en: 'The country with deaths per million near 6,000 and cases per million below
    75,000 is clearly an outlier. Let''s take a closer look:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we can see that the outlier country is Peru. Peru does have above-average
    cases per million, but its number of deaths per million is still much greater
    than would be expected given the number of cases. If we draw a line that's perpendicular
    to the *x* axis at 62,830, we can see that it crosses the regression line at about
    1,000 deaths per million, which is far fewer than the 5,876 for Peru. The only
    other values in the data for Peru that also stand out as very different from the
    dataset averages are population density and GDP per person, both of which are
    substantially lower than average. Here, none of our features may help us explain
    the high number of deaths in Peru.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When creating a scatter plot, it is common to put a feature or predictor variable
    on the *x* axis and a target variable on the *y* axis. If a regression line is
    drawn, then that represents the increase in the target that's been predicted by
    a 1-unit increase in the predictor. But scatter plots can also be used to examine
    the relationship between two predictors or two possible targets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking back at how we defined an outlier in [*Chapter 1*](B17978_01_ePub.xhtml#_idTextAnchor014),
    *Examining the Distribution of Features and Targets*, an argument can be made
    that Peru is an outlier. But we still have more work to do before we can come
    to that conclusion. Peru is not the only country with points on the scatter plot
    far above or below the regression line. It is generally a good idea to investigate
    many of these points. Let''s take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating scatter plots that contain most of the key continuous features can
    help us identify other possible outliers and better visualize the correlations
    we observed in the first section of this chapter. Let''s create scatter plots
    of people who are aged 65 and older and GDP per capita with total cases per million:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Age 65 Plus and GDP with Cases Per Million ](img/B17978_02_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 – Age 65 Plus and GDP with Cases Per Million
  prefs: []
  type: TYPE_NORMAL
- en: These scatter plots show that some countries that had very high cases per million
    had values close to what we would expect, given the age of the population or the
    GDP. These are extreme values, but not necessarily outliers as we have defined
    them.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to use scatter plots to illustrate the relationships between
    two features and a target, all in one graphic. Let's return to the land temperatures
    data that we worked with in the previous chapter to explore this.
  prefs: []
  type: TYPE_NORMAL
- en: Data Note
  prefs: []
  type: TYPE_NORMAL
- en: The land temperature dataset contains the average temperature readings (in Celsius)
    in 2019 from over 12,000 stations across the world, though the majority of the
    stations are in the United States. The dataset was retrieved from the Global Historical
    Climatology Network integrated database. It has been made available for public
    use by the United States National Oceanic and Atmospheric Administration at [https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-monthly-version-4](https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-monthly-version-4).
  prefs: []
  type: TYPE_NORMAL
- en: 'We expect the average temperature at a weather station to be impacted by both
    latitude and elevation. Let''s say that our previous analysis showed that elevation
    does not start having much of an impact on temperature until approximately the
    1,000-meter mark. We can split the `landtemps` DataFrame into low- and high-elevation
    stations, with 1,000 meters as the threshold. In the following code, we can see
    that this gives us 9,538 low-elevation stations with an average temperature of
    12.16 degrees Celsius, and 2,557 high-elevation stations with an average temperature
    of 7.58:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can visualize the relationship between elevation and latitude and temperature
    in one scatter plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following scatter plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5 – Latitude and Average Temperature in 2019 ](img/B17978_02_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 – Latitude and Average Temperature in 2019
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that the temperatures gradually decrease as the distance from
    the equator (measured in latitude) increases. We can also see that high-elevation
    weather stations (those with red dots) are generally below low-elevation stations
    – that is, they have lower temperatures at similar latitudes.
  prefs: []
  type: TYPE_NORMAL
- en: 'There also seems to be at least some difference in slope between high- and
    low-elevation stations. Temperatures appear to decline more quickly as latitude
    increases with high-elevation stations. We can draw two regression lines through
    the scatter plot – one for high and one for low-elevation stations – to get a
    clearer picture of this. To simplify the code a bit, let''s create a categorical
    feature, `elevation_group`, for low- and high-elevation stations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6 – Latitude and Average Temperature in 2019 with regression lines
    ](img/B17978_02_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 – Latitude and Average Temperature in 2019 with regression lines
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see the steeper negative slope for high-elevation stations.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to see a scatter plot with two continuous features and a continuous
    target, rather than forcing one of the features to be dichotomous, as we did in
    the previous example, we can take advantage of Matplotlib''s 3D functionality:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following three-dimensional scatter plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7 – Latitude, Temperature, and Elevation in 2019 ](img/B17978_02_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 – Latitude, Temperature, and Elevation in 2019
  prefs: []
  type: TYPE_NORMAL
- en: Scatter plots are a go-to visualization for teasing out relationships between
    continuous features. We get a better sense of those relationships than correlation
    coefficients alone can reveal. However, we need a very different visualization
    if we are examining the relationship between a continuous feature and a categorical
    one. Grouped boxplots are useful in those cases. We will learn how to create grouped
    boxplots with Matplotlib in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Using grouped boxplots to view bivariate relationships between continuous and
    categorical features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Grouped boxplots are an underappreciated visualization. They are helpful when
    we're examining the relationship between continuous and categorical features since
    they show how the distribution of a continuous feature can vary by the values
    of the categorical feature.
  prefs: []
  type: TYPE_NORMAL
- en: We can explore this by returning to the **National Longitudinal Survey** (**NLS**)
    data we worked with in the previous chapter. The NLS has one observation per survey
    respondent but collects annual data on education and employment (data for each
    year is captured in different columns).
  prefs: []
  type: TYPE_NORMAL
- en: Data Note
  prefs: []
  type: TYPE_NORMAL
- en: As stated in [*Chapter 1*](B17978_01_ePub.xhtml#_idTextAnchor014), *Examining
    the Distribution of Features and Targets*, the NLS of Youth is conducted by the
    United States Bureau of Labor Statistics. Separate files for SPSS, Stata, and
    SAS can be downloaded from the respective repository. The NLS data can be downloaded
    from [https://www.nlsinfo.org/investigator/pages/search](https://www.nlsinfo.org/investigator/pages/search).
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to create grouped bloxplots:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Among the many columns in the NLS DataFrame, there''s `highestdegree` and `weeksworked17`,
    which represent the highest degree the respondent earned and the number of weeks
    the person worked in 2017, respectively. Let''s look at the distribution of weeks
    worked for each value of the degree that was earned. First, we must define a function,
    `gettots`, to get the descriptive statistics we want. Then, we must pass a `groupby`
    series object, `groupby([''highestdegree''])[''weeksworked17'']`, to that function
    using `apply`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we can see how different the distribution of weeks worked for people with
    less than a high school degree is from that distribution for people with a bachelor's
    degree or more. For those with no degree, more than 25% had 0 weeks worked. For
    those with a bachelor's degree, even those at the 25th percentile worked 45 weeks
    during the year. The interquartile range covers the whole distribution for individuals
    with no degree (0 to 52), but only a small part of the range for individuals with
    bachelor's degrees (45 to 52).
  prefs: []
  type: TYPE_NORMAL
- en: We should also make note of the class imbalance for `highestdegree`. The counts
    get quite small after master's degrees and the counts for high school degrees
    are nearly twice that of the next largest group. We will likely need to collapse
    some categories before we do any modeling with this data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Grouped boxplots make the differences in distributions even clearer. Let''s
    create some with the same data. We will use Seaborn for this plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8 – Boxplots of Weeks Worked by Highest Degree ](img/B17978_02_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 – Boxplots of Weeks Worked by Highest Degree
  prefs: []
  type: TYPE_NORMAL
- en: The grouped boxplots illustrate the dramatic difference in interquartile range
    for weeks worked by the degree earned. At the associate's degree level (a 2-year
    college degree in the United States) or above, there are values below the whiskers,
    represented by dots. Below the associate's degree level, the boxplots do not identify
    any outliers or extreme values. For example, a 0 weeks worked value is not an
    extreme value for someone with no degree, but it is for someone with an associate's
    degree or more.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also use grouped boxplots to illustrate how the distribution of COVID-19
    cases varies by region. Let''s also add a swarmplot to view the data points since
    there aren''t too many of them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9 – Boxplots of Total Cases Per Million by Region ](img/B17978_02_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 – Boxplots of Total Cases Per Million by Region
  prefs: []
  type: TYPE_NORMAL
- en: These grouped boxplots show just how much the median cases per million varies
    by region, from East Africa and East Asia on the low end to Eastern Europe and
    Western Europe on the high end. Extremely high values for East Asia are *below*
    the first quartile for Western Europe. We should probably avoid drawing too many
    conclusions beyond that since the counts for most regions (the number of countries)
    are fairly small.
  prefs: []
  type: TYPE_NORMAL
- en: So far in this chapter, we have focused mainly on bivariate relationships between
    features, as well as those between a feature and a target. The statistics and
    visualizations we have generated will inform the modeling we will do. We are already
    getting a sense of likely features, their influence on targets, and how the distributions
    of some features change with the values of another feature.
  prefs: []
  type: TYPE_NORMAL
- en: We will explore multivariate relationships in the remaining sections of this
    chapter. We want to have some sense of how multiple features move together before
    we begin our modeling. Do some features no longer matter once other features are
    included? Which observations pull on our parameter estimates more than others,
    and what are the implications for model fitting? Similarly, which observations
    are not like the others, because they either have invalid values or because they
    seem to be capturing a completely different phenomenon than the other observations?
    We will begin to answer those questions in the next three sections. Although we
    will not get any definitive answers until we construct our models, we can start
    making difficult modeling decisions by anticipating them.
  prefs: []
  type: TYPE_NORMAL
- en: Using linear regression to identify data points with significant influence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is not unusual to find that a few observations have a surprisingly high degree
    of influence on our model, our parameter estimates, and our predictions. This
    may or may not be desirable. Observations with significant influence may be unhelpful
    if they reflect a different social or natural process than the rest of the data
    does. For example, let's say we have a dataset of flying animals that migrate
    a great distance, and this is almost exclusively bird species, except for data
    on monarch butterflies. If we are using the wing architecture as a predictor of
    migration distance, the monarch butterfly data should probably be removed.
  prefs: []
  type: TYPE_NORMAL
- en: We should return to the distinction we made in the first section between an
    extreme value and an outlier. We mentioned that an outlier can be thought of as
    an observation with feature values, or relationships between feature values, that
    are so unusual that they cannot help explain relationships in the rest of the
    data. An extreme value, on the other hand, may reflect a natural and explainable
    trend in a feature, or the same relationship between features that has been observed
    throughout the data.
  prefs: []
  type: TYPE_NORMAL
- en: Distinguishing between an outlier and an extreme value matters most with observations
    that have a high influence on our model. A standard measure of influence in regression
    analysis is **Cook's Distance** (**Cook's D**). This gives us a measure of how
    much our predictions would change if an observation were to be removed from the
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s construct a relatively straightforward multivariate regression model
    in this section with the COVID-19 data we have been using, and then generate a
    Cook''s D value for each observation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s load the COVID-19 data and the Matplotlib and statsmodels libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s look at the distribution of total cases per million in population
    and some possible predictors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Next, let's define a function, `getlm`, that uses statsmodels to run a linear
    regression model and generate influence statistics, including Cook's D. This function
    takes a DataFrame, the name of the target column, and the column names for the
    features (it is customary to refer to a target as *y* and features as *X*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will use `dropna` to drop any observations where one of the features has
    a missing value. The function returns the estimated coefficients (along with `pvalues`),
    the influence measures for each observation, and the full regression results (`lm`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can call the `getlm` function while specifying the total cases per
    million as the target and population density (people per square mile), age 65
    plus the percentage, GDP per capita, and diabetes prevalence as predictors. Then,
    we can print the parameter estimates. Ordinarily, we would want to look at a full
    summary of the model, which can be generated with `lm.summary()`. We''ll skip
    that here for ease of understanding:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The coefficients for population density, age 65 plus, and GDP are all significant
    at the 95% level (have p-values less than 0.05). The result for population density
    is interesting since our bivariate analysis did not reveal a relationship between
    population density and cases per million. The coefficient indicates a 6.9-point
    reduction in cases per million, with a 1-point increase in people per square mile.
    Put more broadly, more crowded countries have fewer cases per million people once
    we control for the percentage of people that are 65 or older and their GDP per
    capita. This could be spurious, or it could be a relationship that can only be
    detected with multivariate analysis. (It could also be that population density
    is highly correlated with a feature that has a greater effect on cases per million,
    but that feature has been left out of the model. This would give us a biased coefficient
    estimate for population density.)
  prefs: []
  type: TYPE_NORMAL
- en: We can use the influence DataFrame that we created in our call to `getlm` to
    take a closer look at those observations with a high Cook's D. One way of defining
    a high Cook's D is by using three times the mean value for Cook's D for all observations.
    Let's create a `covidtotalsoutliers` DataFrame with all the values above that
    threshold.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There were 13 countries with Cook's D values above the threshold. Let's print
    out the first five in descending order of the Cook's D value. Bahrain and Maldives
    are in the top quarter of the distribution for cases (see the descriptives we
    printed earlier in this section). They also have high population densities and
    low percentages of age 65 or older. All else being equal, we would expect lower
    cases per million for those two countries, given what our model says about the
    relationship between population density and age to cases. Bahrain does have a
    very high GDP per capita, however, which our model tells us is associated with
    high case numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Singapore and Hong Kong have extremely high population densities and below-average
    cases per million, particularly Hong Kong. These two locations, alone, may account
    for the direction of the population density coefficient. They both also have very
    high GDP per capita values, which might be a drag on that coefficient. It may
    just be that our model should not include locations that are city-states:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'So, let''s take a look at our regression model estimates if we remove Hong
    Kong and Singapore:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The big change in the model is that the population density coefficient has now
    changed direction. This demonstrates how sensitive the population density estimate
    is to outlier observations whose feature and target values may not be generalizable
    to the rest of the data. In this case, that might be true for city-states such
    as Hong Kong and Singapore.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generating influence measures with linear regression is a very useful technique,
    and it has the advantage that it is fairly easy to interpret, as we have seen.
    However, it does have one important disadvantage: it assumes a linear relationship
    between features, and that features are normally distributed. This is often not
    the case. We also needed to understand the relationships in the data enough to
    create *labels*, to identify total cases per million as the target. This is not
    always possible either. In the next two sections, we''ll look at machine learning
    algorithms for outlier detection that do not make these assumptions.'
  prefs: []
  type: TYPE_NORMAL
- en: Using K-nearest neighbors to find outliers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning tools can help us identify observations that are unlike others
    when we have unlabeled data – that is, when there is no target or dependent variable.
    Even when selecting targets and features is relatively straightforward, it might
    be helpful to identify outliers without making any assumptions about relationships
    between features, or the distribution of features.
  prefs: []
  type: TYPE_NORMAL
- en: Although we typically use **K-nearest neighbors** (**KNN**) with labeled data,
    for classification or regression problems, we can use it to identify anomalous
    observations. These are observations where there is the greatest difference between
    their values and their nearest neighbors' values. KNN is a very popular algorithm
    because it is intuitive, makes few assumptions about the structure of the data,
    and is quite flexible. The main disadvantage of KNN is that it is not as efficient
    as many other approaches, particularly parametric techniques such as linear regression.
    We will discuss these advantages in much greater detail in [*Chapter 9*](B17978_09_ePub.xhtml#_idTextAnchor113),
    *K-Nearest Neighbors, Decision Tree, Random Forest, and Gradient Boosted Regression*,
    and [*Chapter 12*](B17978_12_ePub.xhtml#_idTextAnchor144), *K-Nearest Neighbors
    for Classification*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use **PyOD**, short for **Python outlier detection**, to identify countries
    in the COVID-19 data that are significantly different from others. PyOD can use
    several algorithms to identify outliers, including KNN. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to import the KNN module from PyOD and `StandardScaler` from
    the `sklearn` preprocessing utility functions. We also load the COVID-19 data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we standardize the data, which is important when we have features with
    very different ranges, from over 100,000 for total cases per million and GDP per
    capita to less than 20 for diabetes prevalence and age 65 and older. We can use
    scikit-learn''s standard scaler, which converts each feature value into a z-score,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17978_02_0011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B17978_02_002.png) is the value for the *i*th observation of the
    *j*th feature, ![](img/B17978_02_003.png) is the mean for feature ![](img/B17978_02_004.png),
    and ![](img/B17978_02_005.png) is the standard deviation for that feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the scaler for just the features we will be including in our model,
    and then drop all observations that are missing values for one or more features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can run the model and generate predictions and anomaly scores. First,
    we must set `contamination` to `0.1` to indicate that we want 10% of observations
    to be identified as outliers. This is pretty arbitrary but not a bad starting
    point. After using the `fit` method to run the KNN algorithm, we get predictions
    (1 if an outlier, 0 if an inlier) and an anomaly score, which is the basis of
    the prediction (in this case, the top 10% of anomaly scores will get a prediction
    of 1):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can combine the two NumPy arrays with the predictions and anomaly scores
    – `y_pred` and `y_scores`, respectively – and convert them into the columns of
    a DataFrame. This makes it easier to view the range of anomaly scores and their
    associated predictions. 18 countries have been identified as outliers (this is
    a result of setting `contamination` to `0.1`). Outliers have anomaly scores of
    1.77 to 9.34, while inliers have scores of 0.11 to 1.74:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s take a closer look at the countries with the highest anomaly scores:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Several of the locations we identified as having high influence in the previous
    section have high anomaly scores, including Singapore, Hong Kong, Bahrain, and
    Maldives. This is more evidence that we need to take a closer look at the data
    for these countries. Perhaps there is invalid data or there are theoretical reasons
    why they are very different than the rest of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the linear model in the previous section, there is no defined target.
    We include both total cases per million and total deaths per million in this case.
    Peru has been identified as an outlier here, though it was not with the linear
    model. This is partly because of Peru's very high deaths per million, which is
    the highest in the dataset (we did not use deaths per million in our linear regression
    model).
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that Japan is not on this list of outliers. Let''s take a look at its
    anomaly score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The anomaly score is the 15th highest in the dataset. Compare this with the
    4th highest Cook's D score for Japan from the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: It is interesting to compare these results with a similar analysis we could
    conduct with Isolation Forest. We will do that in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This has been a very simplified example of the approach we would take with a
    typical machine learning project. The most important omission here is that we
    are conducting our analysis on the full dataset. For reasons we will discuss at
    the beginning of [*Chapter 4*](B17978_04_ePub.xhtml#_idTextAnchor043), *Encoding,
    Transforming, and Scaling Features*, we want to split our data into training and
    testing datasets very early in the process. We will learn how to incorporate outlier
    detection in a machine learning pipeline in the remaining chapters of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Using Isolation Forest to find outliers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Isolation Forest** is a relatively new machine learning technique for identifying
    anomalies. It has quickly become popular, partly because its algorithm is optimized
    to find outliers, rather than normal values. It finds outliers by successively
    partitioning the data until a data point has been isolated. Points that require
    fewer partitions to be isolated receive higher anomaly scores. This process turns
    out to be fairly easy on system resources. In this section, we will learn how
    to use it to detect outlier COVID-19 cases and deaths.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Isolation Forest is a good alternative to KNN, particularly when we''re working
    with large datasets. The efficiency of the algorithm allows it to handle large
    samples and a high number of features. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can do an analysis similar to the one in the previous section with Isolation
    Forest rather than KNN. Let''s start by loading scikit-learn''s `StandardScaler`
    and `IsolationForest` modules, as well as the COVID-19 data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we must standardize the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we are ready to run our anomaly detection model. The `n_estimators` parameter
    indicates how many trees to build. Setting `max_features` to `1.0` will use all
    of our features. The `predict` method gives us the anomaly prediction, which is
    `-1` for an anomaly. This is based on the anomaly score, which we can get using
    `decision_function`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s take a closer look at the outliers (we will also create a DataFrame
    of the inliers to use in a later step). We sort by anomaly score and show the
    countries with the highest (most negative) score. Singapore, Hong Kong, Bahrain,
    Qatar, and Peru are, again, the most anomalous:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It''s helpful to look at a visualization of the outliers and inliers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10 – Isolation Forest Anomaly Detection – GDP Per Capita and Cases
    Per Million ](img/B17978_02_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 – Isolation Forest Anomaly Detection – GDP Per Capita and Cases
    Per Million
  prefs: []
  type: TYPE_NORMAL
- en: Although we are only able to see three dimensions with this visualization, the
    plot does illustrate some of what makes an outlier an outlier. We expect cases
    to increase as the GDP per capita and the age 65 plus percentage increase. We
    can see that the outliers deviate from the expected pattern, having cases per
    million noticeably above or below countries with similar GDPs and age 65 plus
    values.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we used bivariate and multivariate statistical techniques and
    visualizations to get a better sense of bivariate relationships among features.
    We looked at common statistics, such as the Pearson correlation. We also examined
    bivariate relationships through visualizations, with scatter plots when both features
    are continuous, and with grouped boxplots when one feature is categorical. The
    last three sections of this chapter explored multivariate techniques for examining
    relationships and identifying outliers, including machine learning algorithms
    such as KNN and Isolation Forest.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a good sense of the distribution of our data, we are ready
    to start engineering our features, including imputing missing values and encoding,
    transforming, and scaling our variables. This will be our focus for the next two
    chapters.
  prefs: []
  type: TYPE_NORMAL
