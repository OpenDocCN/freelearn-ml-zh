["```py\n\nfrom matplotlib import pyplot as plt\n\nimport numpy as np\n\nnp.random.seed(4711)  # for repeatability \n\nc1 = np.random.multivariate_normal([10, 0], [[3, 1], [1, 4]], size=[100,])\n\nl1 = np.zeros(100)\n\nl2 = np.ones(100)\n\nc2 = np.random.multivariate_normal([0, 10], [[3, 1], [1, 4]], size=[100,])\n\n#add noise:\n\nnp.random.seed(1)  # for repeatability \n\nnoise1x = np.random.normal(0,2,100)\n\nnoise1y = np.random.normal(0,8,100)\n\nnoise2 = np.random.normal(0,8,100)\n\nc1[:,0] += noise1x\n\nc1[:,1] += noise1y\n\nc2[:,1] += noise2\n\nfig = plt.figure(figsize=(20,15))\n\nax = fig.add_subplot(111)\n\nax.set_xlabel('x',fontsize=30)\n\nax.set_ylabel('y',fontsize=30)\n\nfig.suptitle('classes',fontsize=30)\n\nlabels = np.concatenate((l1,l2),)\n\nX = np.concatenate((c1, c2),)\n\npp1= ax.scatter(c1[:,0], c1[:,1],cmap='prism',s=50,color='r')\n\npp2= ax.scatter(c2[:,0], c2[:,1],cmap='prism',s=50,color='g')\n\nax.legend((pp1,pp2),('class 1', 'class2'),fontsize=35)\n\nfig.savefig('classes.png')\n\n```", "```py\n\nimport numpy as np\n\nfrom sklearn import mixture\n\nfrom scipy.cluster.hierarchy import linkage\n\nfrom scipy.cluster.hierarchy import fcluster\n\nfrom sklearn.cluster import KMeans\n\nfrom sklearn.cluster import MeanShift\n\nfrom matplotlib import pyplot as plt\n\nfig.clf()#reset plt\n\nfig, ((axis1, axis2), (axis3, axis4)) = plt.subplots(2, 2, sharex='col', sharey='row')\n\n#k-means\n\nkmeans = KMeans(n_clusters=2)\n\nkmeans.fit(X)\n\npred_kmeans = kmeans.labels_\n\nplt.scatter(X[:,0], X[:,1], c=kmeans.labels_, cmap='prism')  # plot points with cluster dependent colors\n\naxis1.scatter(X[:,0], X[:,1], c=kmeans.labels_, cmap='prism')\n\naxis1.set_ylabel('y',fontsize=40)\n\naxis1.set_title('k-means',fontsize=20)\n\n#mean-shift\n\nms = MeanShift(bandwidth=7)\n\nms.fit(X)\n\npred_ms = ms.labels_\n\naxis2.scatter(X[:,0], X[:,1], c=pred_ms, cmap='prism')\n\naxis2.set_title('mean-shift',fontsize=20)\n\n#gaussian mixture\n\ng = mixture.GMM(n_components=2)\n\ng.fit(X) \n\npred_gmm = g.predict(X)\n\naxis3.scatter(X[:,0], X[:,1], c=pred_gmm, cmap='prism')\n\naxis3.set_xlabel('x',fontsize=40)\n\naxis3.set_ylabel('y',fontsize=40)\n\naxis3.set_title('gaussian mixture',fontsize=20)\n\n#hierarchical\n\n# generate the linkage matrix\n\nZ = linkage(X, 'ward')\n\nmax_d = 110\n\npred_h = fcluster(Z, max_d, criterion='distance')\n\naxis4.scatter(X[:,0], X[:,1], c=pred_h, cmap='prism')\n\naxis4.set_xlabel('x',fontsize=40)\n\naxis4.set_title('hierarchical ward',fontsize=20)\n\nfig.set_size_inches(18.5,10.5)\n\nfig.savefig('comp_clustering.png', dpi=100)\n\n```", "```py\n\nfrom scipy.cluster.hierarchy import dendrogram\n\nfig = plt.figure(figsize=(20,15))\n\nplt.title('Hierarchical Clustering Dendrogram',fontsize=30)\n\nplt.xlabel('data point index (or cluster index)',fontsize=30)\n\nplt.ylabel('distance (ward)',fontsize=30)\n\ndendrogram(\n\n Z,\n\n truncate_mode='lastp',  # show only the last p merged clusters\n\n p=12,\n\n leaf_rotation=90.,\n\n leaf_font_size=12.,\n\n show_contracted=True,\n\n)\n\nfig.savefig('dendrogram.png')\n\n```", "```py\n\nfrom sklearn.metrics import homogeneity_completeness_v_measure\n\nfrom sklearn.metrics import silhouette_score\n\nres = homogeneity_completeness_v_measure(labels,pred_kmeans)\n\nprint 'kmeans measures, homogeneity:',res[0],' completeness:',res[1],' v-measure:',res[2],' silhouette score:',silhouette_score(X,pred_kmeans)\n\nres = homogeneity_completeness_v_measure(labels,pred_ms)\n\nprint 'mean-shift measures, homogeneity:',res[0],' completeness:',res[1],' v-measure:',res[2],' silhouette score:',silhouette_score(X,pred_ms)\n\nres = homogeneity_completeness_v_measure(labels,pred_gmm)\n\nprint 'gaussian mixture model measures, homogeneity:',res[0],' completeness:',res[1],' v-measure:',res[2],' silhouette score:',silhouette_score(X,pred_gmm)\n\nres = homogeneity_completeness_v_measure(labels,pred_h)\n\nprint 'hierarchical (ward) measures, homogeneity:',res[0],' completeness:',res[1],' v-measure:',res[2],' silhouette score:',silhouette_score(X,pred_h)\n\nThe preceding code produces the following output:\n\nkmeans measures, homogeneity: 0.25910415428  completeness: 0.259403626429  v-measure: 0.259253803872  silhouette score: 0.409469791511\n\nmean-shift measures, homogeneity: 0.657373750073  completeness: 0.662158204648  v-measure: 0.65975730345  silhouette score: 0.40117810244\n\ngaussian mixture model measures, homogeneity: 0.959531296098  completeness: 0.959600517797  v-measure: 0.959565905699  silhouette score: 0.380255218681\n\nhierarchical (ward) measures, homogeneity: 0.302367273976  completeness: 0.359334499592  v-measure: 0.32839867574  silhouette score: 0.356446705251\n\n```", "```py\n\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\n\n#line y = 2*x\n\nx = np.arange(1,101,1).astype(float)\n\ny = 2*np.arange(1,101,1).astype(float)\n\n#add noise\n\nnoise = np.random.normal(0, 10, 100)\n\ny += noise\n\nfig = plt.figure(figsize=(10,10))\n\n#plot\n\nplt.plot(x,y,'ro')\n\nplt.axis([0,102, -20,220])\n\nplt.quiver(60, 100,10-0, 20-0, scale_units='xy', scale=1)\n\nplt.arrow(60, 100,10-0, 20-0,head_width=2.5, head_length=2.5, fc='k', ec='k')\n\nplt.text(70, 110, r'$v^1$', fontsize=20)\n\n#save\n\nax = fig.add_subplot(111)\n\nax.axis([0,102, -20,220])\n\nax.set_xlabel('x',fontsize=40)\n\nax.set_ylabel('y',fontsize=40)\n\nfig.suptitle('2 dimensional dataset',fontsize=40)\n\nfig.savefig('pca_data.png')\n\n```", "```py\n\nmean_x = np.mean(x)\n\nmean_y = np.mean(y)\n\nu_x = (x- mean_x)/np.std(x)\n\nu_y = (y-mean_y)/np.std(y)\n\nsigma = np.cov([u_x,u_y])\n\n```", "```py\n\neig_vals, eig_vecs = np.linalg.eig(sigma)\n\neig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i])\n\n for i in range(len(eig_vals))]\n\neig_pairs.sort()\n\neig_pairs.reverse()\n\nv1 = eig_pairs[0][1]\n\nprint v1\n\narray([ 0.70710678,  0.70710678]\n\n```", "```py\n\nx_v1 = v1[0]*np.std(x)+mean_x\n\ny_v1 = v1[1]*np.std(y)+mean_y\n\nprint 'slope:',(y_v1-1)/(x_v1-1)\n\nslope: 2.03082418796\n\n```", "```py\n\nX = np.array([u_x,u_y])\n\nX = X.T\n\nprint X.shape\n\n(100,2)\n\n```", "```py\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=1)\n\npca.fit(X)\n\nv1_sklearn = pca.components_[0]\n\nprint v1_sklearn\n\n[ 0.70710678  0.70710678]\n\n```", "```py\n\n#transform in reduced space\n\nX_red_sklearn = pca.fit_transform(X)\n\nW = np.array(v1.reshape(2,1))\n\nX_red = W.T.dot(X.T)\n\n#check the reduced matrices are equal\n\nassert X_red.T.all() == X_red_sklearn.all(), 'problem with the pca algorithm'\n\n```"]