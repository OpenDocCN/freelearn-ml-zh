- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Utilizing Tools and Packages for Active ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will discuss a range of Python libraries, frameworks, and
    tools commonly used in active ML. These resources are instrumental to implementing
    a variety of active ML techniques. The content of this chapter is structured to
    be informative and useful for individuals at different levels of expertise, from
    beginners to experienced programmers. The aim is to provide a solid understanding
    of the tools we will cover in order to effectively incorporate active ML techniques
    into your projects.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, the focus will be on understanding Python packages
    for active ML. We will use the popular Python libraries `scikit-learn` and `modAL`.
    You’ll learn about their functionalities and how they can be applied to active
    ML scenarios. We will also explore a range of active ML tools. In addition to
    the tools covered in previous sections of the book, this chapter will introduce
    some additional active ML tools. Each tool will be presented with an overview
    of its features and potential applications, helping you to understand how they
    fit into different active ML contexts.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will discuss the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Mastering Python packages for enhanced active ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting familiar with the active ML tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the exercises in this chapter, you will need to install these packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'And you will need the following imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Mastering Python packages for enhanced active ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section offers a comprehensive overview of two popular Python packages
    known for their capabilities in facilitating active ML: `scikit-learn`, a versatile
    and user-friendly library, is foundational in the ML community for its extensive
    array of traditional ML tools. On the other hand, `modAL`, specifically designed
    for active ML, builds upon `scikit-learn`’s robust framework to introduce more
    dynamic, data-efficient learning techniques. Together, these packages represent
    a powerful toolkit for anyone looking to leverage the strengths of active ML methodologies.'
  prefs: []
  type: TYPE_NORMAL
- en: scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While not exclusively for active ML, **scikit-learn** ([https://scikit-learn.org/stable/index.html](https://scikit-learn.org/stable/index.html))
    is a foundational package in Python’s machine learning ecosystem. It offers a
    broad range of algorithms and tools that are often used in conjunction with active
    ML packages – a vast collection of algorithms for classification, regression,
    clustering, and dimensionality reduction. It also provides tools for model evaluation
    and data preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: '`scikit-learn` is typically used as a base for model development and is often
    integrated with active ML packages for model training and evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: For instance, `scikit-learn` can be used to perform customer segmentation in
    marketing by clustering customers based on purchasing behavior, demographics,
    and engagement metrics. K-means clustering, a popular algorithm in `scikit-learn`,
    helps in identifying distinct customer groups for targeted marketing campaigns.
    Active ML can be incorporated by iteratively refining the clustering model. For
    instance, marketing analysts can label ambiguous cases where the clustering algorithm
    is uncertain, improving the model’s accuracy over time.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s illustrate this with a simulated example.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we perform the initial clustering with `KMeans`. We start by defining
    some mock customer data (age, annual income):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we use `KMeans` for clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'And we predict the cluster for each customer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We have segmented our customers into two clusters based on age and annual income.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we set up the active ML section. Let’s assume that we have a larger,
    unlabeled dataset of customer features called `X_unlabeled`. In the context of
    our customer segmentation scenario using `KMeans`, unlabeled data would consist
    of customer records with the same features we used for clustering (in our case,
    age and annual income), but without any assigned cluster labels. This data is
    what we’ll use to apply and refine our clustering and classification models in
    an active ML framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We need a model (a classifier) to make predictions on this unlabeled data.
    Let’s use a simple classifier called `LogisticRegression` for illustration. We
    initialize this classifier and use the clusters as labels to train it on our initial
    dataset (`X`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we implement the active ML loop. In each iteration, the classifier predicts
    labels for the unlabeled data. First, we need to create a `obtain_labels` placeholder
    function where we obtain the true labels for the selected data points. In a real-world
    scenario, this function would involve a process to acquire the actual labels,
    such as conducting surveys or expert analysis. Since we’re creating a simulated
    example, we design this function to randomly assign labels based on some assumed
    logic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'For our active ML loop, we need to choose how many iterations we want to go
    through and how many samples will be labeled in each iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now create our active ML loop that will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Select instances for which the classifier is least confident.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Obtain true labels for these instances (in practice, this might involve manual
    labeling or additional data collection).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the classifier with these new labels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Periodically update the `KMeans` model with the newly labeled data to refine
    the customer segments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following is a code snippet that helps us to achieve this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code returns the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Our active ML loop iterates a specified number of times, each time selecting
    the least confident predictions made by the classifier, obtaining labels for these
    instances, and then updating the classifier and `KMeans` model with the new data.
    Remember, the `obtain_labels` function is a simplification. In a real application,
    obtaining labels would involve an oracle manually labeling the samples as we described
    in [*Chapter 3*](B21789_03.xhtml#_idTextAnchor040), *Managing the Human in* *the
    Loop*.
  prefs: []
  type: TYPE_NORMAL
- en: modAL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`scikit-learn`. It allows for easy integration of active learning strategies
    into existing ML workflows. It provides various active ML strategies such as uncertainty
    sampling and query by committee. It also supports custom query strategies and
    easy integration with `scikit-learn` models.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, it is ideal for tasks such as image classification and regression
    where active ML can efficiently select informative samples for annotation.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at an example where we classify images from the popular `torchvision`
    datasets. Given the large volume of images, active ML can help prioritize which
    images should be labeled manually. We will use the `modAL` framework’s uncertainty
    sampling query strategy. It will be able to identify the most informative images
    (those where the classifier is most uncertain) and query them for labeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'We implement a `load_images` function to read images from the dataset directory,
    then we convert them to grayscale and flatten the images for training. Indeed,
    we need to transform the image data into a format compatible with `RandomForest`,
    so each image, which is a 2D array, is *flattened* into a 1D array. This means
    converting the image into a long vector of pixel values. For our grayscale images
    of size 32x32 pixels, the flattened form will be a vector of 1,024 elements (32x32):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, for our example, we split the dataset into initial labeled data with
    the images stored in `X_initial` and the labels stored in `y_initial` and unlabeled
    data as `X_unlabeled`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We are starting our example with 12,500 labeled images and 37,500 unlabeled
    images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we initialize the `modAL` active learner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we simulate the querying of the labels with the following five-iteration
    loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding loop returns the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The loop represents five iterations of active ML. In each iteration, the model
    queries the dataset to label new instances. The learner queries the `X_unlabeled`
    training data and returns the `query_idx` index and `query_instance` instance
    of the sample it is most uncertain about. Then, the learner is taught using the
    instance it queried. In a real-world scenario, this step would involve obtaining
    the label for the queried instance from an oracle (such as a human annotator).
    However, in this simulated example, the label is directly taken from the `y_all`
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: This example illustrates the process of active ML using `modAL`, where the model
    actively queries specific instances to learn from, rather than passively learning
    from a static dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '`modAL` is a great Python package that allows us to implement complex active
    ML methods easily. For example, let’s create a use case of active ML using the
    `modAL` package, specifically focusing on **committee-based** algorithms. In this
    example, we’ll use a committee of classifiers to query the most informative samples
    from an unlabeled dataset. As a reminder, we defined the *query-by-committee approaches*
    in [*Chapter 2*](B21789_02.xhtml#_idTextAnchor027)*, Designing Query* *Strategy
    Frameworks*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, let’s use the Iris dataset (*Fisher, R. A.. (1988). Iris.
    UCI Machine Learning Repository*. [https://doi.org/10.24432/C56C76](https://doi.org/10.24432/C56C76)),
    a common choice for classification tasks. The Iris dataset is a classic dataset
    in machine learning and statistics, often used for demonstrating classification
    algorithms. The dataset contains 150 samples of iris flowers. Each sample has
    four features: sepal length, sepal width, petal length, and petal width. These
    features are measurements in centimeters of the respective parts of the iris plant.
    There are three species (classes) of iris plants in the dataset: Iris setosa,
    Iris virginica, and Iris versicolor. Each class has 50 samples, making the dataset
    evenly balanced among the three classes. The typical task with the Iris dataset
    is a multiclass classification problem. The goal is to predict the species of
    an iris plant based on measurements of its sepals and petals.'
  prefs: []
  type: TYPE_NORMAL
- en: We will use a committee of K-Nearest Neighbors classifiers. The committee will
    use the **query-by-committee (QBC) strategy** to select data points about which
    it has the most disagreement.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by loading the Iris dataset (from the datasets available with `scikit-learn`)
    and creating an initial small labeled dataset and a larger unlabeled dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We initialize twenty `ActiveLearner` instances, each with a `RandomForestClassifier`,
    and combine them into a `Committee`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The active ML loop uses the `vote_entropy_sampling` strategy to select the sample
    about which the committee members disagree the most.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how our active ML loop looks like for five iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The `query` method of the `Committee` object is used to select the most informative
    sample from the unlabeled `X_unlabeled` dataset. The committee, consisting of
    multiple learners, uses its internal query strategy, `vote_entropy_sampling`,
    to determine which instance in `X_unlabeled` it finds most valuable for learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'The selected sample in each iteration is used to teach (retrain) all the committee’s
    learners. After each query, the performance of the committee is evaluated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This example demonstrates how to use a committee of learners with `modAL` to
    actively improve model performance by querying the most informative samples. The
    committee’s diverse opinions help in selecting samples that are more informative
    for learning, thus improving the overall model more efficiently. We observe in
    our output, for example, that the committee score improved from 0.96 to 0.973.
  prefs: []
  type: TYPE_NORMAL
- en: In active ML, especially when using a committee-based approach as in the preceding
    example, the expectation is generally that the performance of the model (or in
    this case, the committee of models) will improve over the iterations. This improvement
    is expected because the committee is being trained on increasingly informative
    samples, which are selected based on the committee’s uncertainty or disagreement.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, a few points are worth noting:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Incremental improvement**: The increase in performance might not be linear
    or consistent across all iterations. In some iterations, the model may improve
    significantly, while in others, the improvement might be minimal or even stagnant.
    We can see this in our example, where the committee score went from 0.96 to 0.94,
    and then back up to 0.973.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Depends on data and model**: The rate and consistency of improvement depend
    on the nature of the data and the effectiveness of the learning algorithm. For
    some datasets or configurations, the improvement might be rapid and consistent,
    while for others, it might be slower or less predictable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Diminishing returns**: As the most informative samples are added to the training
    set, the remaining unlabeled samples may become less informative, leading to diminishing
    returns in terms of performance improvement in later iterations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modAL` function uses the accuracy by default) is a measure of how well the
    committee’s combined prediction aligns with the true labels. As the committee
    is exposed to more representative samples of the data, its predictions should
    become more accurate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluation method**: The method of evaluating the committee’s performance
    can also affect perceived improvements. If the evaluation is done on a static
    test set, the improvements may be more evident. However, if the evaluation is
    on the training set (including newly added samples), the improvements might be
    less pronounced due to increasing complexity or variance in the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, while an increase in the committee’s performance score over a number
    of iterations is a common expectation in active ML, the actual pattern of improvement
    can vary based on various factors. Regular monitoring and adjustments might be
    necessary to ensure the active ML process is yielding the desired results, as
    we saw in [*Chapter 6*](B21789_06.xhtml#_idTextAnchor078), *Evaluating and* *Enhancing
    Efficiency*.
  prefs: []
  type: TYPE_NORMAL
- en: The choice of the right Python package for active ML depends on the specific
    requirements of the task at hand, including the type of data, the ML model being
    used, and the desired active learning strategy. Integrating these packages effectively
    can lead to more efficient data labeling, faster model convergence, and overall
    better performance of ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will explore tools that can be easily used to perform active ML on
    unlabeled data such as Encord Active, Lightly, Cleanlab, Voxel51, and UBIAI.
  prefs: []
  type: TYPE_NORMAL
- en: Getting familiar with the active ML tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Throughout this book, we’ve introduced and discussed several key active ML
    tools and labeling platforms, including Lightly, Encord, LabelBox, Snorkel AI,
    Prodigy, `modAL`, and Roboflow. To further enhance your understanding and assist
    you in selecting the most suitable tool for your specific project needs, let’s
    revisit these tools with expanded insights and introduce a few additional ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '`scikit-learn`. It stands out for its extensive range of query strategies,
    which can be tailored to various active ML scenarios. Whether you are dealing
    with classification, regression, or clustering tasks, `modAL` provides a robust
    and intuitive interface for implementing active learning workflows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Label** **Studio** ([https://docs.humansignal.com/guide/active_learning.     html?__hstc=90244869.a32555b92661e36e5f4b3b8a0f2cc99a.170621 0819596.1706210819596.1706210819596.1&__hssc=90244869.2.1706     210819596&__hsfp=3755259113&_gl=1*1i1r2ib*_ga*MTE1NzM0NDQ4Ny 4xNzA2MjEwODE5*_ga_NQELN45JRH*MTcwNjIxMDgxOS4xLjEuMTcwNj     IxMDgzNC4wLjAuMA](https://docs.humansignal.com/guide/active_learning.html?__hstc=90244869.a32555b92661e36e5f4b3b8a0f2cc99a.1706210819596.1706210819596.1706210819596.1&__hssc=90244869.2.1706210819596&__hsfp=3755259113&_gl=1*1i1r2ib*_ga*MTE1NzM0NDQ4Ny4xNzA2MjEwODE5*_ga_NQELN45JRH*MTcwNjIxMDgxOS4xLjEuMTcwNjIxMDgzNC4wLjAuMA)):
    An open source, multi-type data labeling tool, Label Studio excels in its adaptability
    to different forms of data, including text, images, and audio. It allows for the
    integration of ML models into the labeling process, thereby enhancing labeling
    efficiency through active ML. Its flexibility extends to customizable labeling
    interfaces, making it suitable for a broad range of applications in data annotation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prodigy** ([https://prodi.gy/](https://prodi.gy/)): Prodigy offers a unique
    blend of active ML and human-in-the-loop approaches. It’s a highly efficient annotation
    tool, particularly for refining training data for NLP models. Its real-time feedback
    loop allows for rapid iteration and model improvement, making it an ideal choice
    for projects that require quick adaptation and precision in data annotation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lightly**([https://www.lightly.ai/](https://www.lightly.ai/)): Specializing
    in image datasets, Lightly uses active ML to identify the most representative
    and diverse set of images for training. This ensures that models are trained on
    a balanced and varied dataset, leading to improved generalization and performance.
    Lightly is particularly useful for projects where data is abundant but labeling
    resources are limited.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Encord Active** ([https://encord.com/active](https://encord.com/active)):
    Focused on active ML for image and video data, Encord Active is integrated within
    a comprehensive labeling platform. It streamlines the labeling process by identifying
    and prioritizing the most informative samples, thereby enhancing efficiency and
    reducing the manual annotation workload. This platform is particularly beneficial
    for large-scale computer vision projects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cleanlab** ([https://cleanlab.ai/](https://cleanlab.ai/)): Cleanlab stands
    out for its ability to detect, quantify, and rectify label errors in datasets.
    This capability is invaluable for active ML, where the quality of the labeled
    data directly impacts model performance. It offers a systematic approach to ensuring
    data integrity, which is crucial for training robust and reliable models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Voxel51** ([https://voxel51.com/blog/supercharge-your-annotation-workflow-with-active-learning](https://voxel51.com/blog/supercharge-your-annotation-workflow-with-active-learning)):
    With a focus on video and image data, Voxel51 provides an active ML platform that
    prioritizes the most informative data for labeling. This enhances the annotation
    workflow, making it more efficient and effective. The platform is particularly
    adept at handling complex, large-scale video datasets, offering powerful tools
    for video analytics and ML'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**UBIAI** ([https://ubiai.tools/active-learning-2](https://ubiai.tools/active-learning-2)):
    UBIAI is a tool that specializes in text annotation and supports active ML. It
    simplifies the process of training and deploying NLP models by streamlining the
    annotation workflow. Its active ML capabilities ensure that the most informative
    text samples are prioritized for annotation, thus improving model accuracy with
    fewer labeled examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Snorkel AI** ([https://snorkel.ai](https://snorkel.ai)): Renowned for its
    novel approach to creating, modeling, and managing training data, Snorkel AI uses
    a technique called weak supervision. This method combines various labeling sources
    to reduce the dependency on large labeled datasets, complementing active ML strategies
    to create efficient training data pipelines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deepchecks** ([https://deepchecks.com/importance-of-active-learning-in-machine-learning](https://deepchecks.com/importance-of-active-learning-in-machine-learning)):
    Deepchecks offers a comprehensive suite of validation checks that are essential
    in an active ML context. These checks ensure the quality and diversity of datasets
    and models, thereby facilitating the development of more accurate and robust ML
    systems. It’s an essential tool for maintaining data integrity and model reliability
    throughout the ML lifecycle.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LabelBox** ([https://labelbox.com/guides/the-guide-to-getting-started-with-active-learning](https://labelbox.com/guides/the-guide-to-getting-started-with-active-learning)):
    As a comprehensive data labeling platform, LabelBox excels in managing the entire
    data labeling process. It provides a suite of tools for creating, managing, and
    iterating on labeled data, applicable to a wide range of data types such as images,
    videos, and text. Its support for active learning methodologies further enhances
    the efficiency of the labeling process, making it an ideal choice for large-scale
    ML projects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Roboflow** ([https://docs.roboflow.com/api-reference/active-learning](https://docs.roboflow.com/api-reference/active-learning)):
    Designed for computer vision projects, Roboflow streamlines the process of preparing
    image data. It is especially valuable for tasks involving image recognition and
    object detection. Roboflow’s focus on easing the preparation, annotation, and
    management of image data makes it a key resource for teams and individuals working
    in the field of computer vision.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each tool in this extended list brings unique capabilities to the table, addressing
    specific challenges in ML projects. From image and video annotation to text processing
    and data integrity checks, these tools provide the necessary functionalities to
    enhance project efficiency and efficacy through active ML strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter has provided a comprehensive exploration of the various Python
    libraries, frameworks, and tools essential for active ML. By navigating through
    the intricacies of popular libraries such as `scikit-learn` and `modAL`, we have
    explored their capabilities and how they can be effectively applied in active
    ML scenarios. Additionally, this chapter has expanded your toolkit by introducing
    a range of other active ML tools, each with its own unique features and potential
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Whether you are a beginner taking your first steps in active ML or an experienced
    programmer seeking to refine your skills, this chapter aimed to equip you with
    a solid foundation in the tools and techniques of active ML. The knowledge gained
    here is not just theoretical; it is a practical guide to help you master Python
    packages for enhanced active ML and to familiarize yourself with a broad spectrum
    of active ML tools. This understanding will enable you to select and apply the
    most appropriate tools for your specific ML projects, enhancing the efficiency
    and effectiveness of your models.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have reached the end of the book. But remember you have
    just started your journey in the world of active ML. As you move forward in your
    ML journey, remember that the field of active ML is continually evolving. Staying
    informed about new developments, tools, and techniques will be key to maintaining
    a cutting-edge approach in your work. The tools and concepts covered in this chapter
    provide a strong basis for further exploration and innovation in the exciting
    and dynamic field of active ML.
  prefs: []
  type: TYPE_NORMAL
