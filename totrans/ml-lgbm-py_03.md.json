["```py\nimport lightgbm as lgb\n```", "```py\ndataset = datasets.fetch_covtype()\nX_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, random_state=179)\ntraining_set = lgb.Dataset(X_train, y_train - 1)\ntest_set = lgb.Dataset(X_test, y_test - 1)\n```", "```py\nparams = {\n    'boosting_type': 'gbdt',\n    'objective': 'multiclass',\n    'num_classes': '7',\n    'metric': {'auc_mu'},\n    'num_leaves': 120,\n    'learning_rate': 0.09,\n    'force_row_wise': True,\n    'verbose': 0\n}\n```", "```py\nmetrics = {}\ncallbacks = [\n    lgb.log_evaluation(period=15),\n    lgb.record_evaluation(metrics),\n    lgb.early_stopping(15),\n    lgb.reset_parameter(learning_rate=learning_rate_decay(0.09, 0.999))\n]\n```", "```py\ndef learning_rate_decay(initial_lr, decay_rate):\n    def _decay(iteration):\n        return initial_lr * (decay_rate ** iteration)\n    return _decay\n```", "```py\ngbm = lgb.train(params, training_set, num_boost_round=150, valid_sets=test_set, callbacks=callbacks)\n```", "```py\ny_pred = np.argmax(gbm.predict(X_test, num_iteration=gbm.best_iteration), axis=1)\nf1_score(y_test - 1, y_pred, average=\"macro\")\n```", "```py\nlgb.plot_metric(metrics, 'auc_mu')\n```", "```py\ndataset = datasets.fetch_covtype()\nX_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, random_state=179)\n```", "```py\nmetrics = {}\ncallbacks = [\n    lgb.log_evaluation(period=15),\n    lgb.record_evaluation(metrics),\n    lgb.early_stopping(15),\n    lgb.reset_parameter(learning_rate=learning_rate_decay(0.09, 0.999))\n]\n```", "```py\nmodel = lgb.LGBMClassifier(\n    boosting_type='gbdt',\n    n_estimators=150,\n    num_leaves=120,\n    learning_rate=0.09,\n    force_row_wise=True\n)\n```", "```py\nmodel = model.fit(X_train, y_train, eval_set=(X_test, y_test), eval_metric='auc_mu', callbacks=callbacks)\n```", "```py\nf1_score(y_test, model.predict(X_test), average=\"macro\")\n```", "```py\ngrid = {\n    'learning_rate': [0.001, 0.01, 0.1, 0.2, 0,5],\n    'num_rounds': [20, 40, 60, 80, 100],\n    'num_leaves': [2, 16, 32, 64, 128, 256],\n}\n```", "```py\ndf = pd.read_csv(\"students/data.csv\", sep=\";\")\nprint(f\"Shape: {df.shape}\")\ndf.sample(10)\n```", "```py\ndf.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4424 entries, 0 to 4423\nData columns (total 35 columns):\n #   Column                                          Non-Null Count  Dtype\n---  ------                                          --------------  -----\n 0   Marital status                                  4424 non-null   int64\n 1   Application mode                                4424 non-null   int64\n 2   Application order                               4424 non-null   int64\n…\n```", "```py\ndf.isnull().sum()\n```", "```py\ndf.loc[df.duplicated()]\n```", "```py\nsns.countplot(data=df, x='Target')\n```", "```py\nsns.heatmap(df.corr(), cmap='coolwarm')\n```", "```py\nnationalities = df.groupby(['Nacionality', 'Target']).size().reset_index().pivot(columns='Target', index='Nacionality', values=0)\nnationalities_total = nationalities.sum(axis=1)\nnationalities_total = nationalities_total.sort_values(ascending=True)\nnationalities.loc[nationalities_total.index].plot(kind='barh', stacked=True)\n```", "```py\nsns.countplot(data=df, x='Gender', hue='Target', hue_order=['Dropout', 'Enrolled', 'Graduate'])\nplt.xticks(ticks=[0,1], labels=['Female','Male'])\n```", "```py\ndf.columns = df.columns.str.strip().str.replace(' ', '_')\ndf = df.drop(columns=[\"Nacionality\", \"International\"], axis=1)\ndf[\"Target\"]=df[\"Target\"].map({\n    \"Dropout\":0,\n    \"Enrolled\":1,\n    \"Graduate\":2\n})\nX = df.drop(columns=[\"Target\"], axis=1)\ny = df[\"Target\"]\n```", "```py\ndef gbdt_parameter_optimization():\n    params = {\n        \"max_depth\": [-1, 32, 128],\n        \"n_estimators\": [50, 100, 150],\n        \"min_child_samples\": [10, 20, 30],\n        \"learning_rate\": [0.001, 0.01, 0.1],\n        \"num_leaves\": [32, 64, 128]\n    }\n    model = lgb.LGBMClassifier(force_row_wise=True, boosting_type=\"gbdt\", verbose=-1)\n    grid_search = GridSearchCV(estimator=model, param_grid=params, cv=5, verbose=10)\n    grid_search.fit(X, y)\n    return grid_search\nresults = gbdt_parameter_optimization()\nprint(results.best_params_)\nprint(results.best_score_)\n```", "```py\nmodel = lgb.LGBMClassifier(force_row_wise=True, boosting_type=\"gbdt\", learning_rate=0.1, max_depth=-1, min_child_samples=10, n_estimators=100, num_leaves=32, verbose=-1)\nscores = cross_val_score(model, X, y, scoring=\"f1_macro\")\nscores.mean()\n```"]