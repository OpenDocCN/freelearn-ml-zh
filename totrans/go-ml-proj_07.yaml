- en: Convolutional Neural Networks - MNIST Handwriting Recognition
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络 - MNIST 手写识别
- en: In the previous chapter, I posited a scenario where you are a postal worker
    trying to recognize handwriting. In that, we ended up with a neural network built
    on top of Gorgonia. In this chapter, we'll look at the same scenario, but we'll
    augment our ideas of what a neural network is and write a more advanced neural
    network, one that was, until very recently, state of the art.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我提出了一种场景，即你是一名邮递员，试图识别手写体。在那里，我们最终构建了一个基于 Gorgonia 的神经网络。在本章中，我们将探讨相同的场景，但我们将扩展我们对神经网络的理解，并编写一个更先进的神经网络，这是一个直到最近仍然是尖端技术的神经网络。
- en: Specifically, in this chapter, we are going to build a **Convolutional Neural
    Network** (**CNN**). A CNN is a type of deep learning network that has been popular
    in recent years.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，在本章中，我们将构建一个 **卷积神经网络**（**CNN**）。CNN 是一种近年来流行的深度学习网络。
- en: Everything you know about neurons is wrong
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你所知道的关于神经元的一切都是错误的
- en: In the previous chapter, I mentioned that everything you know about neural networks
    is wrong. Here, I repeat that claim. Most literature out there on a neural network
    starts with a comparison with biological neurones and ends there. This leads readers
    to often assume that it is. I'd like to make a point that artificial neural networks
    are *nothing* like their biological namesake.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我提到关于神经网络的你知道的一切都是错误的。在这里，我重申这一说法。大多数关于神经网络的文献都是从与生物神经元的比较开始的，并以此结束。这导致读者经常假设它是。我想指出，人工神经网络与它们的生物同名物没有任何相似之处。
- en: Instead, in the last chapter, I spent a significant amount of the chapter describing
    linear algebra, and explained that the twist is that you can express almost any
    **machine learning** (**ML**) problem as linear algebra. I shall continue to do
    so in this chapter.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，在上一章中，我花了很多时间描述线性代数，并解释说，转折点是你可以将几乎任何 **机器学习**（**ML**）问题表达为线性代数。我将在本章继续这样做。
- en: Rather than think of artificial neural networks as analogies of real-life neural
    networks, I personally encourage you to think of artificial neural networks as
    mathematical equations. The non-linearities introduced by the activation functions,
    combined with linear combinations allows for artificial neural networks to be
    able to approximate any function.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 与其将人工神经网络视为现实生活神经网络的类比，我个人鼓励你将人工神经网络视为数学方程式。激活函数引入的非线性性与线性组合相结合，使得人工神经网络能够近似任何函数。
- en: Neural networks – a redux
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络 - 重新审视
- en: 'The fundamental understanding that neural networks are mathematical expressions
    leads to really simple and easy implementations of neural networks. Recall from
    the previous chapter that a neural network can be written like this:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对神经网络的基本理解是它们是数学表达式，这导致了神经网络简单易行的实现。回想一下上一章，神经网络可以写成这样：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If we rewrite the code as a mathematical equation, we can write a neural network
    like this:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将代码重写为一个数学方程式，我们可以写出如下神经网络：
- en: '![](img/a67670dc-893a-4308-9b31-462f96a5a349.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a67670dc-893a-4308-9b31-462f96a5a349.png)'
- en: A side note: ![](img/86262bf8-91aa-45b1-a951-f2db5da20de6.png)is the same as ![](img/d6732aaf-6223-4477-af91-f022c81b9b92.png).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下：![图片](img/86262bf8-91aa-45b1-a951-f2db5da20de6.png)与![图片](img/d6732aaf-6223-4477-af91-f022c81b9b92.png)相同。
- en: 'We can simply write it out using Gorgonia, like this:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 Gorgonia 简单地写出它，如下所示：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The preceding code is a representation of the following neural network in images:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码是以下神经网络在图像中的表示：
- en: '![](img/60f87787-65ce-40e1-b7a6-f4037719a3c5.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/60f87787-65ce-40e1-b7a6-f4037719a3c5.png)'
- en: The middle layer consists of 800 hidden units.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 中间层由 800 个隐藏单元组成。
- en: Of course, the preceding code hides a lot of things. You can't really expect
    a neural network from scratch in fewer than 20 lines, can you? To understand what
    is happening, we need to take a brief detour into understanding what Gorgonia
    is.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，前面的代码隐藏了很多东西。你不可能在少于 20 行代码中从头开始构建一个神经网络，对吧？为了理解正在发生的事情，我们需要简要地了解一下 Gorgonia
    是什么。
- en: Gorgonia
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Gorgonia
- en: Gorgonia is a library that provides primitives for working with mathematical
    expressions specific to deep learning. When working with a ML related project,
    you will start to find yourself more introspective about the world, and questioning
    assumptions all the time. This is a good thing.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Gorgonia 是一个库，它提供了用于处理深度学习特定数学表达式的原语。当与机器学习相关的项目一起工作时，你将开始发现自己对世界的洞察力更强，并且总是质疑假设。这是好事。
- en: 'Consider what happens in your mind when you read the following mathematical
    expression:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑当你阅读以下数学表达式时，你心中的想法：
- en: '![](img/2fbb360a-4c40-41b7-987d-ac97f71891a4.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2fbb360a-4c40-41b7-987d-ac97f71891a4.png)'
- en: You should instantly think *hang on, that's false*. Why does your brain think
    this?
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该立刻想到“等等，这是错误的”。为什么你的大脑会这样想？
- en: 'That''s mainly because your brain evaluated the mathematical expression. In
    general, there are three parts to the expression: the left-hand side, the equal
    symbol, and the right-hand side. Your brain evaluated each part separately and
    then evaluated the expression as false.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这主要是因为你的大脑评估了数学表达式。一般来说，表达式有三个部分：左边、等号和右边。你的大脑分别评估每一部分，然后评估整个表达式为假。
- en: 'When we read mathematical expressions, we automatically evaluate the expressions
    in our mind that we take evaluation for granted. In Gorgonia, what we take for
    granted is made explicit. There are two general *parts* to using Gorgonia: defining
    an expression and evaluating an expression.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们阅读数学表达式时，我们会自动在心中评估这些表达式，并且我们理所当然地认为这是评估。在 Gorgonia 中，我们理所当然的事情被明确化了。使用 Gorgonia
    有两个一般的 *部分*：定义表达式和评估表达式。
- en: Since you are most probably a programmer, you can think of the first part as
    writing a program, and the second part can be thought of as running a program.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你很可能是程序员，你可以把第一部分看作是编写程序，而第二部分可以看作是运行程序。
- en: When describing a neural network in Gorgonia, it's often instructive to imagine
    yourself writing in another programming language, one that is specific to building
    neural networks. This is because the patterns used in Gorgonia are not unlike
    a new programming language. Indeed, Gorgonia was built from ground-up with the
    idea that it's a programming language without a syntactical frontend. As such,
    in this section, I will often ask you to imagine writing in another Go-like language.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当在 Gorgonia 中描述神经网络时，想象自己用另一种编程语言编写代码通常是有益的，这种语言是专门用于构建神经网络的。这是因为 Gorgonia 中使用的模式与一种新的编程语言非常相似。事实上，Gorgonia
    是从零开始构建的，其理念是它是一种没有语法前端的编程语言。因此，在本节中，我经常会要求你想象自己在另一种类似 Go 的语言中编写代码。
- en: Why?
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么？
- en: 'A good question to ask is *why?* Why bother with this separation of processes?
    After all, the preceding code could be rewritten as the previous chapter''s `Predict`
    function:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好问题是“为什么？”为什么要费心分离这个过程？毕竟，前面的代码可以被重写为上一章的 `Predict` 函数：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, we define the network in Go, and when we run the Go code, the neural network
    is run as it is being defined. What's the problem we face that we need to introduce
    the idea of separating the definition of the neural network and running it? We've
    already seen the problem when we wrote the `Train` method.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们用 Go 语言定义网络，当我们运行 Go 代码时，神经网络就像定义时一样运行。我们面临的问题是什么，需要引入将神经网络定义和运行分离的想法？我们已经看到了当我们编写
    `Train` 方法时的这个问题。
- en: 'If you recall, in the last chapter, I said that writing the `Train` method
    requires us to actually copy and paste code from the `Predict` method. To refresh
    your memory, here''s the `Train` method:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得，在上一个章节中，我说过编写 `Train` 方法需要我们实际上从 `Predict` 方法中复制和粘贴代码。为了刷新你的记忆，以下是 `Train`
    方法：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Let's go through an exercise of refactoring to highlight the problem. Taking
    off our ML hat for a bit, and putting on our software engineer hat, let's see
    how we can refactor `Train` and `Predict`, even if conceptually. We see in the
    `Train` method that we need access to `act0` and `pred` in order to backpropagate
    the errors. Where in `Predict` `act0` and `pred` are terminal values (that is,
    we don't use them after the function has returned), in `Train`, they are not.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过重构练习来突出问题。暂时摘下我们的机器学习帽子，戴上软件工程师的帽子，看看我们如何重构 `Train` 和 `Predict`，即使是在概念上。我们在
    `Train` 方法中看到，我们需要访问 `act0` 和 `pred` 来反向传播错误。在 `Predict` 中，`act0` 和 `pred` 是终端值（也就是说，函数返回后我们不再使用它们），而在
    `Train` 中则不是。
- en: 'So, here, we can create a new method; let''s call it `fwd`:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，在这里，我们可以创建一个新的方法；让我们称它为 `fwd`：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'And we can refactor `Predict` to look like this:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将 `Predict` 重构为如下所示：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'And the `Train` method would look like this:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`Train` 方法将看起来像这样：'
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This looks better. What exactly are we doing here? We are programming. We are
    rearranging one form of syntax into another form of syntax but we are not changing
    the semantics, the meaning of the program. The refactored program has exactly
    the same meaning as the pre-refactored program.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这个看起来更好。我们在这里到底在做什么呢？我们在编程。我们在将一种语法形式重新排列成另一种语法形式，但我们并没有改变语义，即程序的意义。重构后的程序与未重构前的程序具有完全相同的意义。
- en: Programming
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编程
- en: Wait a minute, you might say to yourself. What do I mean by *the meaning of
    the program?* This is a surprisingly deep topic that involves a whole branch of
    mathematics known as **homotopy**. But for all practical intents and purposes
    of this chapter, let's define the *meaning* of a program to be the extensional
    definition of the program. If two programs compile and run, take the exact same
    inputs, and return the same exact output every time, we say two programs are equal.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 等一下，你可能自己会想。我说的“程序的意义”是什么意思？这是一个非常深奥的话题，涉及到整个数学分支，称为**同伦**。但就本章的所有实际目的而言，让我们将程序的意义定义为程序的扩展定义。如果两个程序编译并运行，接受相同的输入，并且每次都返回相同的精确输出，那么我们说两个程序是相等的。
- en: 'These two programs would be equal:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个程序将是相等的：
- en: '| **Program A** | **Program B** |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| **程序 A** | **程序 B** |'
- en: '| --- | --- |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `fmt.Println("Hello World")` | `fmt.Printf("Hello " + "World\n")` |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| `fmt.Println("Hello World")` | `fmt.Printf("Hello " + "World\n")` |'
- en: 'Intentionally, if we visualize the programs as an **Abstract Syntax Tree**
    (**AST**), they look slightly different:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 故意地，如果我们把程序可视化为一个 **抽象语法树**（**AST**），它们看起来略有不同：
- en: '![](img/ec6b6303-8a9f-448e-9cae-10d9465a6c30.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ec6b6303-8a9f-448e-9cae-10d9465a6c30.png)'
- en: '![](img/80b016f8-e352-4740-8f0d-c65c83060bea.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/80b016f8-e352-4740-8f0d-c65c83060bea.png)'
- en: The syntax for both programs are different, but they are semantically the same.
    We can refactor program B into program A, by eliminating the `+`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 两个程序的语法不同，但它们的语义是相同的。我们可以通过消除 `+` 将程序 B 重构为程序 A。
- en: 'But note what we did here: we took a program and represented it as an AST.
    Through syntax, we manipulated the AST. This is the essence of programming.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 但请注意我们在这里做了什么：我们取了一个程序，并以抽象语法树（AST）的形式表示它。通过语法，我们操作了 AST。这就是编程的本质。
- en: What is a tensor? – part 2
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是张量？ – 第二部分
- en: 'In the previous chapter, there was an info box that introduced the concept
    of a tensor. That info box was a little simplified. If you google what a tensor
    is, you will get very conflicting results, which only serve to confuse. I don''t
    want to add to the confusion. Instead, I shall only briefly touch on tensors in
    a way that will be relevant to our project, and in a way very much like how a
    typical textbook on Euclidean geometry introduces the concept of a point: by holding
    it to be self-evident from use cases.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，有一个信息框介绍了张量的概念。那个信息框有点简化。如果你在谷歌上搜索什么是张量，你会得到非常矛盾的结果，这些结果只会让人更加困惑。我不想增加困惑。相反，我将简要地触及张量，使其与我们项目相关，并且以一种非常类似于典型欧几里得几何教科书介绍点概念的方式：通过将其视为从用例中显而易见。
- en: 'Likewise, we will hold tensors to be self-evident from use. First, we will
    look at the concept of multiplication:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们将从用例中认为张量是显而易见的。首先，我们将看看乘法的概念：
- en: 'First, let''s define a vector: ![](img/a86a3ad1-6ff3-4633-b115-91b671301d9f.png).
    You can think of it as this diagram:'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，让我们定义一个向量：![](img/a86a3ad1-6ff3-4633-b115-91b671301d9f.png)。你可以把它想象成这个图：
- en: '![](img/1d1d9e18-693e-42c1-9b0e-12a6aff3c24c.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d1d9e18-693e-42c1-9b0e-12a6aff3c24c.png)'
- en: 'Next, let''s multiply the vector by a scalar value: ![](img/90b2efe0-f4c4-441f-9276-a22d5023c6bc.png).
    The result is something like this:'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，让我们将向量乘以一个标量值：![](img/90b2efe0-f4c4-441f-9276-a22d5023c6bc.png)。结果是类似这样的：
- en: '![](img/6b4e7f1c-38da-44ca-a8f5-cc1d5e185ccc.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6b4e7f1c-38da-44ca-a8f5-cc1d5e185ccc.png)'
- en: 'There are two observations:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个观察点：
- en: The general direction of the arrow doesn't change.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 箭头的总体方向没有改变。
- en: Only the length changes. In physics terms, this is called the magnitude. If
    the vector represents the distance travelled, you would have traveled twice the
    distance along the same direction.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有长度发生变化。在物理术语中，这被称为大小。如果向量代表行进的距离，你将沿着相同方向行进两倍的距离。
- en: 'So, how would you change directions by using multiplications alone? What do
    you have to multiply to change directions? Let''s try the following matrix, which
    we will call *T*, for transformation:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你如何仅通过乘法来改变方向呢？你需要乘以什么来改变方向？让我们尝试以下矩阵，我们将称之为 *T*，用于变换：
- en: '![](img/5262facb-1ac0-4216-b920-b4e20e163d78.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5262facb-1ac0-4216-b920-b4e20e163d78.png)'
- en: 'Now if we multiply the transformation matrix with the vector, we get the following:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在如果我们用变换矩阵乘以向量，我们得到以下结果：
- en: '![](img/93630693-f819-481c-b93d-4fa5b9ffca44.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/93630693-f819-481c-b93d-4fa5b9ffca44.png)'
- en: 'And if we plot out the starting vector and the ending vector, we get the resultant
    output:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们绘制起始向量和结束向量，我们得到的结果如下：
- en: '![](img/b70d5c40-fdf6-479a-9405-1f361beef36e.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b70d5c40-fdf6-479a-9405-1f361beef36e.png)'
- en: As we can see, the direction has changed. The magnitude too has changed.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，方向已经改变。大小也发生了变化。
- en: Now, you might be saying, *hang on, isn't this just Linear Algebra 101?*. Yes,
    it is. But to really understand a tensor, we must learn how to construct one.
    The matrix that we just used is also a tensor of rank-2\. The proper name for
    a tensor of rank-2 is a **dyad**.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可能会说，“等等，这不是线性代数101的内容吗？”是的，它是。但为了真正理解张量，我们必须学习如何构造它。我们刚才使用的矩阵也是一个秩为2的张量。秩为2的张量的正确名称是
    **二重积**。
- en: Why the mixing of naming conventions? Here's a bit of fun trivia. When I was
    writing the earliest versions of Gorgonia, I was musing about the terrible naming
    conventions computer science has had, a fact that Bjarne Stroustrup himself lamented.
    The canonical name for a rank-2 tensor is called a **dyad**, but can be represented
    as a matrix. I was struggling to properly call it; after all, there are power
    in names and to name it is to tame it.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么会有命名约定的混合？这里有一点点有趣的趣闻。当我编写 Gorgonia 最早版本的时候，我在思考计算机科学糟糕的命名约定，这是 Bjarne Stroustrup
    本人也曾哀叹的事实。秩为2的张量的标准名称是 **二重积**，但它可以表示为一个矩阵。我一直在努力给它一个合适的名字；毕竟，名字中蕴含着力量，命名就是驯服。
- en: At around the same time as I was developing the earliest versions of Gorgonia,
    I was following a most excellent BBC TV series called **Orphan Black**, in which
    the Dyad Institute is the primary foe of the protagonists. They were quite villainous
    and that clearly left an impact in my mind. I decided against naming it thus.
    In retrospect, this seemed like a rather silly decision.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 大约在我开发 Gorgonia 最早版本的同时，我正在追一部非常优秀的 BBC 电视系列剧 **Orphan Black**，其中 Dyad 学院是主角的主要敌人。他们相当邪恶，这显然在我的脑海中留下了深刻印象。我决定不这样命名它。回顾起来，这似乎是一个相当愚蠢的决定。
- en: 'Now let''s consider the transformation dyad. You can think of the dyad as a
    vector *u* times a vector *v*. To write it out in equation form:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑变换二重积。你可以把二重积想象成向量 *u* 乘以向量 *v*。用方程式表示出来：
- en: '![](img/d7f28330-0c81-4a14-bff1-67c135e3574d.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d7f28330-0c81-4a14-bff1-67c135e3574d.png)'
- en: 'At this point, you may be familiar with the previous chapter''s notion of linear
    algebra. You might think to yourself: if two vectors multiply, that''d end up
    with a scalar value, no? If so, how would you multiply two vectors and get a matrix
    out of it?'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你可能已经熟悉了上一章的线性代数概念。你可能会想：“如果两个向量相乘，那会得到一个标量值，对吗？如果是这样，你怎么乘以两个向量并得到一个矩阵呢？”
- en: 'Here, we''d need to introduce a new type of multiplication: the outer product
    (and by contrast, the multiplication introduced in the previous chapter is an
    inner product). We write outer products with this symbol: ![](img/0fb20035-e86b-4189-9050-11b52ee9f24d.png).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们需要引入一种新的乘法类型：外积（相比之下，上一章中引入的乘法是内积）。我们用这个符号来表示外积：![](img/0fb20035-e86b-4189-9050-11b52ee9f24d.png)。
- en: 'Specifically speaking, the outer product, also known as a dyad product, is
    defined as such:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，外积，也称为二重积，定义为如下：
- en: '![](img/e6158f0e-afbe-4214-a383-6a9f8d1a8d38.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e6158f0e-afbe-4214-a383-6a9f8d1a8d38.png)'
- en: We won't be particularly interested in the specifics of *u* and *v* in this
    chapter. However, being able to construct a dyad from its constituent vectors
    is an integral part of what a tensor is all about.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们不会特别关注 *u* 和 *v* 的具体细节。然而，能够从其组成向量构造二重积是张量概念的一个基本组成部分。
- en: 'Specifically, we can replace *T* with *uv*:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们可以将 *T* 替换为 *uv*：
- en: '![](img/862fd95c-5ca3-470f-a68d-7ae0fa0690ca.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/862fd95c-5ca3-470f-a68d-7ae0fa0690ca.png)'
- en: Now we get ![](img/c1e433e0-9b4e-475b-bd90-2d3d079cd553.png) as the scalar magnitude
    change and *u* as the directional change.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们得到 ![](img/c1e433e0-9b4e-475b-bd90-2d3d079cd553.png) 作为标量大小变化，*u* 作为方向变化。
- en: So what is the big fuss with tensors? I can give two reasons.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，张量究竟有什么大惊小怪的？我可以给出两个原因。
- en: Firstly, the idea that dyads can be formed from vectors generalizes upward.
    A three-tensor, or triad can be formed by a dyad product *uvw*, a four-tensor
    or a tetrad can be formed by a dyad product *uvwx*, and so on and so forth. This
    affords us a mental shortcut that will be very useful to us when we see shapes
    that are associated with tensors.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，从向量中可以形成二元的想法可以向上推广。一个三张量，或三元组，可以通过二元乘积 *uvw* 形成，一个四张量或四元组可以通过二元乘积 *uvwx*
    形成，以此类推。这为我们提供了一个心理捷径，当我们看到与张量相关的形状时，这将非常有用。
- en: 'The useful mental model of what a tensor can be thought as is the following:
    a vector is like a list of things, a dyad is like a list of vectors, a triad is
    like a list of dyads, and so on and so forth. This is absolutely helpful when
    thinking of images, like those that we''ve seen in the previous chapter:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 将张量可以想象成什么的有用心理模型如下：一个向量就像一个事物列表，一个二元组就像一个向量列表，一个三元组就像一个二元组列表，以此类推。这在思考图像时非常有帮助，就像我们在上一章中看到的那样：
- en: An image can be seen as a (28, 28) matrix. A list of ten images would have the
    shape (10, 28, 28). If we wanted to arrange the images in such a way that it's
    a list of lists of ten images, it'd have a shape of (10, 10, 28, 28).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一张图像可以看作是一个 (28, 28) 矩阵。十个图像的列表将具有形状 (10, 28, 28)。如果我们想以这样的方式排列图像，使其成为十个图像的列表的列表，那么它的形状将是
    (10, 10, 28, 28)。
- en: 'All this comes with a caveat of course: a tensor can only be defined in the
    presence of transformation. As a physics professor once told me: *that which transforms
    like a tensor is a tensor*. A tensor devoid of any transformation is just an *n*-dimensional
    array of data. The data must transform, or flow from tensor to tensor in an equation.
    In this regards, I think that TensorFlow is a ridiculously well-named product.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这一切都有一个前提：张量只能在变换存在的情况下定义。正如一位物理教授曾经告诉我的：“那些像张量一样变换的东西就是张量”。没有任何变换的张量只是一个
    *n*- 维数据数组。数据必须变换，或者在一个方程中从张量流向张量。在这方面，我认为TensorFlow是一个极其恰当命名的产品。
- en: For more information on tensors, I would recommend the relatively dense text
    book, *Linear Algebra and Geometry* by Kostrikin (I failed to finish this book,
    but it was this book that gave me what I believe to be a strong-ish understanding
    of tensors). More on the flow of tensors can be found in Spivak's *Manifold Calculus*.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 关于张量的更多信息，我推荐相对密集的教科书，Kostrikin的《线性代数与几何》（我未能完成这本书，但正是这本书给了我一个我认为相当强的张量理解）。关于张量流的信息可以在Spivak的《微分几何》中找到。
- en: All expressions are graphs
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 所有表达式都是图
- en: Now we can finally return to the preceding example.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们终于可以回到前面的例子了。
- en: 'Our problem, if you recall, is that we had to specify the neural network twice:
    once for prediction and once for learning purposes. We then refactored the program
    so that we don''t have to specify the network twice. Additionally, we had to manually
    write out the expression for the backpropagation. This is error prone, especially
    when dealing with larger neural networks like the one we''re about to build in
    this chapter. Is there a better way? The answer is yes.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你记得，我们的问题是我们必须指定神经网络两次：一次用于预测，一次用于学习目的。然后我们重构了程序，这样我们就不必两次指定网络。此外，我们必须手动编写反向传播的表达式。这很容易出错，尤其是在处理像我们在本章将要构建的这样的大型神经网络时。有没有更好的方法？答案是肯定的。
- en: Once we understand and fully internalize that neural networks are essentially
    mathematical expressions, we can take the learning's from tensors, and model a
    neural network where the entire neural network is a flow of tensors.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们理解和完全内化了神经网络本质上是一种数学表达式的观点，我们就可以从张量中吸取经验，并构建一个神经网络，其中整个神经网络是张量流。
- en: Recall that tensors can only be defined in the presence of transformation; then,
    any operation that transforms tensor(s), used in concert with data structures
    that hold data are tensors. Also, recall that computer programs can be represented
    as abstract syntax trees. Mathematical expressions can be represented as a program.
    Therefore, mathematical expressions can also be represented as an abstract syntax
    tree.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，张量只能在变换存在的情况下定义；那么，任何用于变换张量（们）的操作，与持有数据的结构一起使用时，都是张量。此外，回想一下，计算机程序可以表示为抽象语法树。数学表达式可以表示为一个程序。因此，数学表达式也可以表示为抽象语法树。
- en: More accurate, however, is that mathematical expressions can be expressed as
    a graph; a directed acyclic graph, to be specific. We call this the **expression
    graph**.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，更准确地说，数学表达式可以表示为图；具体来说，是一个有向无环图。我们称之为**表达式图**。
- en: 'This distinction matters. Trees cannot share nodes. Graphs can. Let''s consider,
    for example, the following mathematical expression:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这种区别很重要。树不能共享节点。图可以。让我们考虑以下数学表达式：
- en: '![](img/773f21bd-f1bd-4f85-bed9-d3b4351558ad.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/773f21bd-f1bd-4f85-bed9-d3b4351558ad.png)'
- en: 'Here are the representations as a graph and as a tree:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是图和树的表示：
- en: '![](img/042d0364-e315-454a-8a18-85ca98bcb637.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/042d0364-e315-454a-8a18-85ca98bcb637.png)'
- en: On the left, we have a directed acyclic graph, and on the right, we have a tree.
    Note that in the tree variant of the mathematical equation, there are repeat nodes.
    Both are rooted at ![](img/bc041d3c-1879-480d-b2c1-ee4e3e65713e.png). The arrow
    should be read as *depends on*. ![](img/1ffd83f8-66fd-437d-ab43-bc22d9502e7e.png)
    depends on two other nodes, ![](img/0d90444f-20a9-498d-8d65-11d5c6b644fe.png)
    and ![](img/d4029dca-e311-4e99-ad95-9648fbdf599d.png), and so on and so forth.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在左边，我们有一个有向无环图，在右边，我们有一个树。请注意，在数学方程的树变体中，有重复的节点。两者都以![图片](img/bc041d3c-1879-480d-b2c1-ee4e3e65713e.png)为根。箭头应该读作*依赖于*。![图片](img/1ffd83f8-66fd-437d-ab43-bc22d9502e7e.png)依赖于两个其他节点，![图片](img/0d90444f-20a9-498d-8d65-11d5c6b644fe.png)和![图片](img/d4029dca-e311-4e99-ad95-9648fbdf599d.png)，等等。
- en: Both the graph and tree are valid representations of the same mathematical equation,
    of course.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图和树都是同一数学方程的有效表示，当然。
- en: Why bother representing a mathematical expression as a graph or a tree? Recall
    that an abstract syntax tree represents a computation. If a mathematical expression,
    represented as a graph or a tree, has a shared notion of computation, then it
    also represents an abstract syntax tree.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要把数学表达式表示为图或树呢？回想一下，抽象语法树表示一个计算。如果一个数学表达式，以图或树的形式表示，具有共享的计算概念，那么它也代表了一个抽象语法树。
- en: Indeed, we can take each node in the graph or tree, and perform a computation
    on it. If each node is a representation of a computation, then logic holds that
    fewer nodes means faster computations (and less memory usage). Therefore, we should
    prefer to use the directed acyclic graph representation.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 的确，我们可以对图或树中的每个节点进行计算。如果每个节点是计算的表示，那么逻辑上就越少的节点意味着计算越快（以及更少的内存使用）。因此，我们应该更喜欢使用有向无环图表示。
- en: 'And now we come to the major benefit of representing a mathematical expression
    as a graph: we get differentiation for free.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来到了将数学表达式表示为图的主要好处：我们能够免费获得微分。
- en: If you recall from the previous chapter, backpropagation is essentially differentiating
    the cost with regards to the inputs. The gradients, once calculated, can then
    be used to update the values of the weights themselves. Having a graph structure,
    we wouldn't have to write the backpropagation parts. Instead, if we have a virtual
    machine that executes the graph, starting at the leaves and moving toward the
    root, the virtual machine can automatically perform differentiation on the values
    as it traverses the graph from leaf to root.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您从上一章回忆起来，反向传播本质上是对输入的成本进行微分。一旦计算了梯度，就可以用来更新权重的值。有了图结构，我们就不必编写反向传播的部分。相反，如果我们有一个执行图的虚拟机，从叶子节点开始，向根节点移动，虚拟机可以自动在遍历图从叶子到根的过程中对值进行微分。
- en: Alternatively, if we don't want to do automatic differentiation, we can also
    perform symbolic differentiation by manipulating the graph in the same way that
    we manipulated the AST in the *What is programming* section, by adding and coalescing
    nodes.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果我们不想进行自动微分，我们也可以通过操纵图来执行符号微分，就像我们在“什么是编程”部分中操纵AST一样，通过添加和合并节点。
- en: 'In this way, we can now shift our view of a neural network to this:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式，我们现在可以将我们对神经网络的看法转移到这个：
- en: '![](img/145f53a8-6df1-467b-9981-1b1ec6976d5f.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/145f53a8-6df1-467b-9981-1b1ec6976d5f.png)'
- en: Describing a neural network
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 描述神经网络
- en: 'Now let''s get back to the task of writing a neural network and thinking of
    it in terms of a mathematical expression expressed as a graph. Recall that the
    code looks something like this:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们回到编写神经网络的任务，并以图表示的数学表达式来思考它。回想一下，代码看起来像这样：
- en: '[PRE7]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now let's go through this code.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来分析这段代码。
- en: First, we create a new expression graph with `g := G.NewGraph()`. An expression
    graph is a holder object to hold the mathematical expression. Why would we want
    an expression graph? The mathematical expression that represents a neural network
    is contained in the `*gorgonia.ExpressionGraph` object.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用`g := G.NewGraph()`创建一个新的表达式图。表达式图是一个持有数学表达式的对象。我们为什么想要一个表达式图呢？表示神经网络的数学表达式包含在`*gorgonia.ExpressionGraph`对象中。
- en: Mathematical expressions are only interesting if we use variables. ![](img/65a9f649-813c-4549-bf9e-216a9ab70c7f.png)
    is quite an uninteresting expression because you can't do much with this expression.
    The only thing you can do with it is to evaluate the expression and see if it
    returns true or false. ![](img/dfaa4a00-b8a2-4fea-98fe-e69fc78d969c.png) is slightly
    more interesting. But, then again, *a* can only be *1*.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 数学表达式只有在我们使用变量时才有意思。![](img/65a9f649-813c-4549-bf9e-216a9ab70c7f.png)是一个非常无趣的表达式，因为你无法用这个表达式做很多事情。你可以做的唯一事情是评估这个表达式，看看它返回的是真还是假。![](img/dfaa4a00-b8a2-4fea-98fe-e69fc78d969c.png)稍微有趣一些。但再次强调，*a*只能为*1*。
- en: Consider, however, the expression ![](img/f4cd11b6-41d0-4a24-bbd3-657288d1aa15.png).
    With two variables, it suddenly becomes a lot more interesting. The values that
    *a* and *b* can take are dependent on one another, and there is a whole range
    of possible pairs of numbers that can fit into *a *and *b*.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，考虑一下这个表达式！[](img/f4cd11b6-41d0-4a24-bbd3-657288d1aa15.png)。有了两个变量，它突然变得更有趣。*a*和*b*可以取的值相互依赖，并且存在一系列可能的数字对可以适合到*a*和*b*中。
- en: 'Recall that each layer of neural network is just a mathematical expression
    that reads like this: ![](img/3f3d99df-f160-4131-b517-a183b5f40033.png). In this
    case, *w*, *x*, and *b* are variables. So, we create them. Note that in this case,
    Gorgonia treats variables as a programming language does: you have to tell the
    system what the variable represents.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，神经网络中的每一层仅仅是一个类似这样的数学表达式：![](img/3f3d99df-f160-4131-b517-a183b5f40033.png)。在这种情况下，*w*、*x*和*b*是变量。因此，我们创建了它们。请注意，在这种情况下，Gorgonia将变量处理得就像编程语言一样：你必须告诉系统变量代表什么。
- en: In Go, you would do that by typing `var x Foo`, which tells the Go compiler
    that `x` should be a type `Foo`. In Gorgonia, the mathematical variables are declared
    by using `NewMatrix`, `NewVector`, `NewScalar`, and `NewTensor`. `x := G.NewMatrix(g,
    Float, G.WithName, G.WithShape(N, 728))` simply says `x` is a matrix in expression
    graph `g` with a name `x`, and has a shape of `(N, 728)`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在Go语言中，你会通过输入`var x Foo`来完成这个操作，这告诉Go编译器`x`应该是一个类型`Foo`。在Gorgonia中，数学变量通过使用`NewMatrix`、`NewVector`、`NewScalar`和`NewTensor`来声明。`x
    := G.NewMatrix(g, Float, G.WithName, G.WithShape(N, 728))`简单地说，`x`是表达式图`g`中的一个名为`x`的矩阵，其形状为`(N,
    728)`。
- en: Here, readers may observe that `728` is a familiar number. In fact, what this
    tells us is that `x` represents the input, which is `N` images. `x`, therefore,
    is a matrix of *N* rows, where each row represents a single image (728 floating
    points).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，读者可能会注意到`728`是一个熟悉的数字。实际上，这告诉我们`x`代表输入，即`N`张图像。因此，`x`是一个包含*N*行的矩阵，其中每一行代表一张单独的图像（728个浮点数）。
- en: 'The eagle-eyed reader would note that `w` and `b` have extra options, where
    the declaration of `x` does not. You see, `NewMatrix` simply declares the variable
    in the expression graph. There is no value associated with it. This allows for
    flexibility when the value is attached to a variable. However, with regards to
    the weight matrix, we want to start the equation with some initial values. `G.WithInit(G.Uniform(1.0))`
    is a construction option that populates the weight matrix with values pulled from
    a uniform distribution with a gain of `1.0`. If you imagine yourself coding in
    another language specific to building neural networks, it''d look something like
    this: `var w Matrix(728, 800) = Uniform(1.0)`.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 留意细节的读者会注意到`w`和`b`有额外的选项，而`x`的声明没有。你看，`NewMatrix`只是声明了表达式图中的变量。它没有与之关联的值。这允许在值附加到变量时具有灵活性。然而，关于权重矩阵，我们希望方程从一些初始值开始。`G.WithInit(G.Uniform(1.0))`是一个构造选项，它使用具有增益`1.0`的均匀分布的值填充权重矩阵。如果你想象自己在另一种专门用于构建神经网络的编程语言中编码，它看起来可能像这样：`var
    w Matrix(728, 800) = Uniform(1.0)`。
- en: Following that, we simply write out the mathematical equation: ![](img/4cdbb766-d42f-4356-bf64-0644b8de86ab.png)
    is simply a matrix multiplication between ![](img/4979cf68-4f3a-4b41-9ec1-5c4ecd4d150d.png)
    and ![](img/641b17c8-e982-4a2d-a7e1-9affa6afae46.png); hence, `xw, _ := G.Mul(x,
    w)`. At this point, it should be clarified that we are merely describing the computation
    that is supposed to happen. It is yet to happen. In this way, it is not dissimilar
    to writing a program; writing code does not equal running the program.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之后，我们只需写出数学方程式：![](img/4cdbb766-d42f-4356-bf64-0644b8de86ab.png) 简单来说，就是矩阵乘法，它是
    ![](img/4979cf68-4f3a-4b41-9ec1-5c4ecd4d150d.png) 和 ![](img/641b17c8-e982-4a2d-a7e1-9affa6afae46.png)
    之间的乘法；因此，`xw, _ := G.Mul(x, w)`. 在这一点上，应该明确的是，我们只是在描述应该发生的计算。它尚未发生。这种方式与编写程序并无太大区别；编写代码并不等同于运行程序。
- en: '`G.Mul` and most operations in Gorgonia actually returns an error. For the
    purposes of this demonstration, we''re ignoring any errors that may arise from
    symbolically multiplying `x` and `w`. What could possibly go wrong with simple
    multiplication? Well, we''re dealing with matrix multiplication, so the shapes
    must have matching inner dimensions. A (N, 728) matrix can only be multiplied
    by a (728, M) matrix, which leads to an (N, M) matrix. If the second matrix does
    not have 728 rows, then an error will happen. So, in real production code, error
    handling is a** must**.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`G.Mul` 和 Gorgonia 中的大多数操作实际上都会返回一个错误。为了演示的目的，我们忽略了从符号上乘以 `x` 和 `w` 可能产生的任何错误。简单的乘法可能出错吗？嗯，我们处理的是矩阵乘法，所以形状必须具有匹配的内部维度。一个
    (N, 728) 矩阵只能与一个 (728, M) 矩阵相乘，这将导致一个 (N, M) 矩阵。如果第二个矩阵没有 728 行，那么将发生错误。因此，在实际的生产代码中，错误处理是**必须的**。'
- en: 'Speaking of *must*, Gorgonia comes with a utility function, called, **G.Must**.
    Taking a cue from the `text/template` and `html/template` libraries found in the
    standard library, the `G.Must` function panics when an error occur. To use, simply
    write this: `xw := G.Must(G.Mul(x,w))`.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 说到**必须**，Gorgonia 提供了一个名为 **G.Must** 的实用函数。从标准库中找到的 `text/template` 和 `html/template`
    库中汲取灵感，当发生错误时，`G.Must` 函数会引发恐慌。要使用，只需编写这个：`xw := G.Must(G.Mul(x,w))`。
- en: After the inputs are multiplied with the weights, we add to the biases using
    `G.Add(xw, b)`. Again, errors may occur, but in this example, we're eliding the
    checks of errors.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在将输入与权重相乘之后，我们使用 `G.Add(xw, b)` 将偏差加到上面。同样，可能会发生错误，但在这个例子中，我们省略了错误检查。
- en: 'Lastly, we take the result and perform a non-linearity: a sigmoid function,
    with `G.Sigmoid(xwb)`. This layer is now complete. Its shape, if you follow, would
    be (N, 800).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将结果与非线性函数：sigmoid 函数，通过 `G.Sigmoid(xwb)` 进行处理。这一层现在已经完成。如果你跟着走，它的形状将是 (N,
    800)。
- en: The completed layer is then used as an input for the following layer. The next
    layer has a similar layout as the first layer, except instead of a sigmoid non-linearity,
    a `G.SoftMax` is used. This ensures that each row in the resulting matrix sums
    1.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 完成的层随后被用作下一层的输入。下一层的布局与第一层相似，只是没有使用 sigmoid 非线性，而是使用了 `G.SoftMax`。这确保了结果矩阵中的每一行总和为
    1。
- en: One-hot vector
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单热向量
- en: 'Perhaps, not so coincidentally, the last layer has the shape of (N, 10). N
    is the number of input images (which we''ve gotten from `x`) ; that''s fairly
    self-explanatory. It also means that there is a clean mapping from input to output.
    What''s not self-explanatory is the 10\. Why 10? Simply put, there are 10 possible
    numbers we want to predict - 0, 1, 2, 3, 4, 5, 6, 7, 8, 9:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 也许并非巧合，最后一层的形状是 (N, 10)。N 是输入图像的数量（我们从 `x` 中获得）；这一点相当直观。这也意味着输入到输出的映射是清晰的。不那么直观的是
    10。为什么是 10？简单来说，我们想要预测 10 个可能的数字 - 0, 1, 2, 3, 4, 5, 6, 7, 8, 9：
- en: '![](img/c8df17fe-0447-45c6-9949-e9ceb0e44c7b.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c8df17fe-0447-45c6-9949-e9ceb0e44c7b.png)'
- en: The preceding diagram is an example result matrix. Recall that we used `G.SoftMax`
    to ensure that each row sums up to 1\. Therefore, we can interpret the numbers
    in each column of each row to be the probability that it is the specific digit
    that we're predicting. To find the digit we're predicting, simply find the highest
    probability in each column.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 上一图是一个示例结果矩阵。回想一下，我们使用了 `G.SoftMax` 来确保每一行的总和为 1。因此，我们可以将每一行每一列的数字解释为预测特定数字的概率。要找到我们预测的数字，只需找到每一列的最高概率即可。
- en: In the previous chapter, I introduced the concept of one-hot vector encoding.
    To recap, it takes a slice of labels and returns a matrix.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我介绍了单热向量编码的概念。为了回顾，它接受一个标签切片并返回一个矩阵。
- en: '![](img/3a72b6c1-df67-44cf-a666-3c493e8dc6fc.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3a72b6c1-df67-44cf-a666-3c493e8dc6fc.png)'
- en: 'Now, this is clearly a matter of encoding. Who''s to say that column 0 would
    have to represent 0? We could of course come up with a completely crazy encoding
    like such and the neural network would still work:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这显然是一个编码问题。谁又能说列0必须代表0呢？我们当然可以想出一个完全疯狂的编码方式，比如这样，神经网络仍然可以工作：
- en: '![](img/f68f2ab0-abf5-4b28-8ff1-3fd91db7c3a7.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f68f2ab0-abf5-4b28-8ff1-3fd91db7c3a7.png)'
- en: Of course, we would not be using such a scheme for encoding; it would be a massive
    source of programmer error. Instead, we would go for the standard encoding of
    a one-hot vector.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们不会使用这样的编码方案；这将是一个巨大的编程错误来源。相反，我们将采用一热向量的标准编码。
- en: I hope this has given you a taste of how powerful the notion of an expression
    graph can be. One thing we haven't touched upon yet is the execution of the graph.
    How do you run a graph? We'll look further into that in the next section.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这已经让你感受到了表达式图概念的力量。我们还没有涉及到的是图的执行。你该如何运行一个图呢？我们将在下一节进一步探讨这个问题。
- en: The project
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 项目
- en: With all that done, it's time to get on to the project! Once again, we are going
    to recognize handwritten digits. But this time around, we're going to build a
    CNN for that. Instead of just using the `tensor` package of Gorgonia, this time
    we're going to use all of Gorgonia.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的准备工作都完成之后，是时候开始项目了！再次强调，我们将识别手写数字。但这一次，我们将构建一个CNN来完成这个任务。这一次，我们不仅会使用Gorgonia的`tensor`包，还会使用Gorgonia的所有功能。
- en: Once again, to install Gorgonia, simply run `go get -u gorgonia.org/gorgonia`
    and `go get -u gorgonia.org/tensor`.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 再次提醒，要安装 Gorgonia，只需运行 `go get -u gorgonia.org/gorgonia` 和 `go get -u gorgonia.org/tensor`。
- en: Getting the data
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取数据
- en: 'The data is the same data as in the previous chapter: the MNIST dataset. It
    can be found in the repository for this chapter, and we''ll be using a function
    we wrote in the previous chapter to acquire the data:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 数据与上一章相同：MNIST数据集。它可以在本章的仓库中找到，我们将使用上一章编写的函数来获取数据：
- en: '[PRE8]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Other things from the previous chapter
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 上一章的其他内容
- en: 'Obviously, there is a lot from the previous chapter that we can reuse:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们可以从上一章中重用很多内容：
- en: The range normalization function (`pixelWeight`) and its isometric counterpart
    (`reversePixelWeight`)
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 范围归一化函数（`pixelWeight`）及其等距对应函数（`reversePixelWeight`）
- en: '`prepareX` and `prepareY`'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prepareX` 和 `prepareY`'
- en: The `visualize` function
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`visualize` 函数'
- en: 'For convenience sake, here they are again:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便，这里再次列出：
- en: '[PRE9]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: CNNs
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNNs
- en: What we will be building is a CNN. So, what is a Convolutional Neural Network?
    As its name suggests, it's a neural network, not unlike the one we have built
    in the previous chapter. So, clearly, there are elements that are similar. There
    are also elements that are not similar, for if they were similar, we wouldn't
    have this chapter.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要构建的是一个CNN。那么，什么是卷积神经网络呢？正如其名所示，它是一个神经网络，与我们之前构建的神经网络类似。所以，显然，它们有一些相似之处。也有一些不同之处，因为如果它们相似，我们就不需要这一章了。
- en: What are convolutions?
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是卷积？
- en: The main difference between the neural network we built in the previous chapter
    and a CNN is the convolutional layer. Recall that the neural network was able
    to learn features related to digits. In order to be more accurate, the neural
    network layers need to learn more specific features. One way to do this is to
    add more layers; more layers would lead to more features being learned, giving
    rise to deep learning.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在上一章构建的神经网络相比，CNN的主要区别在于卷积层。回想一下，神经网络能够学习与数字相关的特征。为了更精确，神经网络层需要学习更具体的特征。实现这一目标的一种方法就是添加更多的层；更多的层会导致学习到更多的特征，从而产生深度学习。
- en: 'On a spring evening of 1877, people dressed in what modern-day people would
    consider as *black-tie* gathered at the Royal Institute, in London. The speaker
    for the evening was Francis Galton, the same Galton we met in [Chapter 1](3d68e167-a44d-4195-a270-f8180ff8f85f.xhtml),
    *How to Solve All Machine Learning Problems*. In his talk, Galton brought out
    a curious device, which he called a **quincunx**. It was a vertical wooden board
    with wooden pegs sticking out of it, arranged in a uniform, but interleaved manner.
    The front of it was covered with glass and there was an opening at the top. Tiny
    balls are then dropped from the top and as they hit the pegs, bounce left or right,
    and fall to the corresponding chutes. This continues until the balls collect at
    the bottom:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在1877年一个春天的傍晚，穿着现代人们认为是*黑色礼服*的人们聚集在伦敦的皇家学会。晚上的演讲者是弗朗西斯·高尔顿，也就是我们在[第一章](3d68e167-a44d-4195-a270-f8180ff8f85f.xhtml)，*如何解决所有机器学习问题*中遇到的高尔顿。在他的演讲中，高尔顿展示了一个奇特的装置，他称之为**五点阵**。这是一个垂直的木制板，上面有木钉均匀地交错排列。前面覆盖着玻璃，顶部有一个开口。然后从顶部滴下微小的球，当它们击中木钉时，会向左或向右弹跳，并落入相应的斜槽中。这个过程一直持续到球收集到底部：
- en: '![](img/254eef09-5de0-42b6-b2a8-076a13d440e4.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/254eef09-5de0-42b6-b2a8-076a13d440e4.png)'
- en: A curious shape begins to form. It's the shape modern statisticians have come
    to recognize as the binomial distribution. Most statistical textbooks end the
    story about here. The quincunx, now known as the Galton Board, illustrates, very
    clearly and firmly, the idea of the central limit theorem.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好奇的形状开始形成。这是现代统计学家已经认识到的二项分布的形状。大多数统计教材的故事就在这里结束。五点阵，现在被称为高尔顿板，非常清晰和坚定地说明了中心极限定理的概念。
- en: 'Our story, of course, doesn''t end there. Recall in [Chapter 1](3d68e167-a44d-4195-a270-f8180ff8f85f.xhtml), *How
    to Solve All Machine Learning Problems*, that I mentioned that Galton was very
    much interested in hereditary issues. A few years earlier, Galton had published
    a book called *Hereditary Genius*. He had collected data on *eminent* persons
    in Great Britain across the preceding centuries, and much to his dismay, he found
    that *eminent* parentage tended to lead to un-eminent children. He called this
    a **reversion to the mediocre**:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们的故事并没有结束。回想一下[第一章](3d68e167-a44d-4195-a270-f8180ff8f85f.xhtml)，*如何解决所有机器学习问题*，我提到高尔顿非常关注遗传问题。几年前，高尔顿出版了一本名为*遗传天才*的书。他收集了前几个世纪英国*杰出*人物的数据，让他非常沮丧的是，他发现*杰出*的父系往往会导致不杰出的子女。他把这称为**回归到平庸**：
- en: '![](img/7a38fc9b-c6ce-4fb6-bd87-0bea52d33083.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7a38fc9b-c6ce-4fb6-bd87-0bea52d33083.png)'
- en: '![](img/3e483077-8c75-442f-9367-efcde7a3003f.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3e483077-8c75-442f-9367-efcde7a3003f.png)'
- en: And, yet, he reasoned, the mathematics doesn't show such things! He explained
    this by showing off a quincunx with two layers. A two-layered quincunx was a stand-in
    for the generational effect. The top layer would essentially be the distribution
    of a feature (say, height). Upon dropping to the second layer, the beads would
    cause the distribution to *flatten out*, which is not what he had observed. Instead,
    he surmised that there has to be another factor which causes the regression to
    the mean. To illustrate his idea, he installed chutes as the controlling factor,
    which causes a regression to the mean. A mere 40 years later, the rediscovery
    of Mendel's pea experiments would reveal genetics to be the factor. That is a
    story for another day.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，他推理道，数学并没有显示出这样的事情！他通过展示一个两层结构的五点阵来解释这一点。两层结构的五点阵是代际效应的替代品。顶层基本上是特征的分布（比如说，身高）。当下降到第二层时，珠子会导致分布*扁平化*，而这并不是他所观察到的。相反，他推测必须存在另一个因素，导致回归到平均值。为了说明他的想法，他安装了斜槽作为控制因素，这会导致回归到平均值。仅仅40年后，孟德尔的豌豆实验的重新发现将揭示遗传是这一因素。那是一个另外的故事。
- en: 'What we''re interested in is why the distribution would *flatten out*. While
    the standard *it''s physics!* would suffice as an answer, there remains interesting
    questions that we could ask. Let''s look at a simplified depiction:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感兴趣的是为什么分布会*扁平化*。虽然标准的*它是物理！*可以作为一个答案，但仍然存在一些有趣的问题我们可以问。让我们看看一个简化的描述：
- en: '![](img/5458700e-52d3-41e2-8859-61799bc48284.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5458700e-52d3-41e2-8859-61799bc48284.png)'
- en: 'Here, we evaluate the probability that the ball will drop and hit a position.
    The curve indicates the probability of the ball landing at position B. Now, we
    add a second layer:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们评估球落下并击中某个位置的概率。曲线表示球落在位置B的概率。现在，我们添加一个第二层：
- en: '![](img/56a539a1-3b5f-4ffa-b039-02562e06a206.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/56a539a1-3b5f-4ffa-b039-02562e06a206.png)'
- en: Say, from the previous layer, the ball landed at position 2\. Now, what is the
    probability that the ball's final resting place is at position D?
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 假设，从上一层，球落在位置2。那么，球最终静止在位置D的概率是多少？
- en: 'To calculate this, we need to know all the possible ways that the ball can
    end up at position D. Limiting our option to A to D only, here they are:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算这个，我们需要知道球最终到达位置D的所有可能方式。限制我们的选项只从A到D，这里它们是：
- en: '| **Level 1 Position** | **L1 Horizontal Distance** | **Level 2 position**
    | **L2 Horizontal Distance** |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| **Level 1 Position** | **L1 Horizontal Distance** | **Level 2 position**
    | **L2 Horizontal Distance** |'
- en: '| A | 0 | D | 3 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| A | 0 | D | 3 |'
- en: '| B | 1 | D | 2 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| B | 1 | D | 2 |'
- en: '| C | 2 | D | 1 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| C | 2 | D | 1 |'
- en: '| D | 3 | D | 0 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| D | 3 | D | 0 |'
- en: Now we can ask the question in terms of probability. The horizontal distances
    in the table are an encoding that allows us to ask the question probabilistically
    and generically. The probability of the ball travelling horizontally by one unit
    can be represented as *P(1)*, the probability of the ball travelling horizontally
    by two units can be represented as *P(2)*, and so on.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以用概率来提问。表中的水平距离是一种编码，允许我们以概率和通用的方式提问。球水平移动一个单位的概率可以表示为*P(1)*，球水平移动两个单位的概率可以表示为*P(2)*，依此类推。
- en: 'And to calculate the probability that the ball ends up in D after two levels
    is essentially summing up all the probabilities:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算球在两个级别后最终落在D的概率，本质上就是将所有概率加起来：
- en: '![](img/e0fe1ff2-0715-4167-926b-587d0954a6cc.png).'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/e0fe1ff2-0715-4167-926b-587d0954a6cc.png)。'
- en: 'We can write it as such:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以写成这样：
- en: '![](img/5da38259-9ba6-49f3-ac2c-8680c3f24f75.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5da38259-9ba6-49f3-ac2c-8680c3f24f75.png)'
- en: We can read it as the probability of the final distance being *$c = a+b$* is
    the sum of *$P_1(a)$*, with the probability of level 1, where the ball traveled
    horizontally by *$a$* and *$P_2(b)$*, with the probability of level 2, where the
    ball traveled horizontally by *$b$*.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将其理解为最终距离为*$c = a+b$*的概率是*$P_1(a)$*的和，其中1级水平，球水平移动了*$a$*，以及*$P_2(b)$*的和，其中2级水平，球水平移动了*$b$*。
- en: 'And this is the typical definition of convolution:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是卷积的典型定义：
- en: '![](img/64c598e7-27fd-40c9-80d3-5df72c34d70c.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/64c598e7-27fd-40c9-80d3-5df72c34d70c.png)'
- en: 'If the integral scares you, we can equivalently rewrite this as a summation
    operation (this is only valid because we are considering discrete values; for
    continuous real values, integrations have to be used):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如果积分让你感到害怕，我们可以等效地将其重写为求和操作（这仅在我们考虑离散值时有效；对于连续实数值，必须使用积分）：
- en: '![](img/becbdaea-2d47-4a0a-bdff-452aca041ced.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/becbdaea-2d47-4a0a-bdff-452aca041ced.png)'
- en: 'Now, if you squint very carefully, this equation looks a lot like the preceding
    probability equation. Instead of ![](img/01f5c5c4-2cf9-4e7f-be8b-3a2230927b42.png),
    we can rewrite it as ![](img/5b3ffbce-5e13-4fd8-91dd-29ac6bc959c7.png):'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你非常仔细地眯着眼睛看，这个方程看起来非常像前面的概率方程。用![](img/01f5c5c4-2cf9-4e7f-be8b-3a2230927b42.png)代替，我们可以将其重写为![](img/5b3ffbce-5e13-4fd8-91dd-29ac6bc959c7.png)：
- en: '![](img/154d9ba7-91db-4d97-a194-846644eacbbf.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/154d9ba7-91db-4d97-a194-846644eacbbf.png)'
- en: And what are probabilities, but functions? There is, after all, a reason we
    write probabilities in the format $P(a)$. We can indeed genericize the probability
    equation to the convolution definition.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 而概率是什么呢，但函数？毕竟，我们之所以用$P(a)$的格式写概率，是有原因的。我们确实可以将概率方程泛化到卷积定义。
- en: 'However, for now, let''s strengthen our intuitions about what convolutions
    are. For that, we''ll keep the notion that the function we''re talking about has
    probabilities. First, we should note that the probability of the ball ending up
    in a particular location is dependent on where it starts. But imagine if the platform
    for the second platform moves horizontally:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，现在让我们加强我们对卷积的理解。为此，我们将保持我们讨论的函数具有概率的概念。首先，我们应该注意球最终落在特定位置的概率取决于它开始的位置。但想象一下，如果第二个平台的平台水平移动：
- en: '![](img/a9663b8d-da9a-4422-8960-c544f9d25494.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a9663b8d-da9a-4422-8960-c544f9d25494.png)'
- en: Now the probability of the final resting place of the ball is highly dependent
    on where the initial starting position is, as well as where the second layer's
    starting position is. The ball may not even land on the bottom!
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在球的最终静止位置高度依赖于初始起始位置，以及第二层起始位置。球甚至可能不会落在底部！
- en: 'So, here''s a good mental shortcut of thinking about convolutions: t''s as
    if one function in one layer is *sliding* across a second function.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这里有一个关于卷积的良好心理捷径：就像一个层中的函数在另一个函数上*滑动*一样。
- en: So, convolutions are what cause the *flattening* of Galton's quincunx. In essence,
    it is a function that slides on top of the probability function, flattening it
    out as it moves along the horizontal dimension. This is a one-dimensional convolution;
    the ball only travels along one dimension.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，卷积是导致高尔顿方阵*展平*的原因。本质上，这是一个在水平维度上滑动的函数，它在移动过程中将概率函数展平。这是一个一维卷积；球只沿着一个维度移动。
- en: 'A two-dimensional convolution is similar to a one-dimensional convolution.
    Instead, there are two *distances* or metrics that we''re considering for each
    layer:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 二维卷积与一维卷积类似。相反，对于每一层，我们考虑两个*距离*或度量：
- en: '![](img/6178477d-0195-4954-8b6e-053bd86510d4.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6178477d-0195-4954-8b6e-053bd86510d4.png)'
- en: 'But this equation is nigh impenetrable. Instead, here''s a convenient series
    of pictures of how it works, step by step:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 但这个方程几乎无法理解。相反，这里有一系列如何逐步工作的方便图片：
- en: 'Convolution (Step 1):'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积（步骤1）：
- en: '![](img/158dfe31-000c-4a9b-98bf-fc56db88a2ef.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/158dfe31-000c-4a9b-98bf-fc56db88a2ef.png)'
- en: 'Convolution (Step 2):'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积（步骤2）：
- en: '![](img/34bd277c-9d68-4f3f-ab2e-80a458721b2c.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/34bd277c-9d68-4f3f-ab2e-80a458721b2c.png)'
- en: 'Convolution (Step 3):'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积（步骤3）：
- en: '![](img/339ed5ff-b018-4da1-b985-78a312a791e9.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/339ed5ff-b018-4da1-b985-78a312a791e9.png)'
- en: 'Convolution (Step 4):'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积（步骤4）：
- en: '![](img/cc64c27d-e73c-4246-87eb-c401076e359c.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cc64c27d-e73c-4246-87eb-c401076e359c.png)'
- en: 'Convolution (Step 5):'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积（步骤5）：
- en: '![](img/53bef645-f45c-4182-8450-e4a07570cc73.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/53bef645-f45c-4182-8450-e4a07570cc73.png)'
- en: 'Convolution (Step 6):'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积（步骤6）：
- en: '![](img/f4262f45-418b-4821-bc34-3e6c91c3d5e1.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f4262f45-418b-4821-bc34-3e6c91c3d5e1.png)'
- en: 'Convolution (Step 7):'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积（步骤7）：
- en: '![](img/d84707bf-ac1a-4c60-a99d-2f0541273fe0.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d84707bf-ac1a-4c60-a99d-2f0541273fe0.png)'
- en: 'Convolution (Step 8):'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积（步骤8）：
- en: '![](img/2890e9fd-7b9c-4d82-8eb9-0ddf3c39310f.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2890e9fd-7b9c-4d82-8eb9-0ddf3c39310f.png)'
- en: 'Convolution (Step 9):'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积（步骤9）：
- en: '![](img/02889f5c-c066-4583-9839-78843d460c6f.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/02889f5c-c066-4583-9839-78843d460c6f.png)'
- en: Again, you can think of this as sliding a function that slides over another
    function (the input) in two dimensions. The function that slides, performs the
    standard linear algebra transformation of multiplication followed by addition.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，你可以将这想象为在二维空间中滑动一个函数，该函数在另一个函数（输入）上滑动。滑动的函数执行标准的线性代数变换，即乘法后加法。
- en: 'You can see this in action in an image-processing example that is undoubtedly
    very common: Instagram.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在一个图像处理示例中看到这一点，这个示例无疑是常见的：Instagram。
- en: How Instagram filters work
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Instagram滤镜的工作原理
- en: 'I am going to assume that you are familiar with Instagram. If not, I both envy
    and pity you; but here''s the gist of Instagram: it''s a photo sharing service
    that has a selling point of allowing users to apply filters to their images. The
    filters would change the color of the images, often to enhance the subject.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我假设你熟悉Instagram。如果不熟悉，我既羡慕又同情你；但这里是Instagram的要点：它是一个照片分享服务，其卖点在于允许用户对其图像应用过滤器。这些过滤器会改变图像的颜色，通常是为了增强主题。
- en: How do those filters work? Convolutions!
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这些过滤器是如何工作的？卷积！
- en: 'For example, let''s define a filter:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们定义一个过滤器：
- en: '![](img/834e573b-df92-43e3-b22c-ca53a720ce3f.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/834e573b-df92-43e3-b22c-ca53a720ce3f.png)'
- en: 'To convolve, we simply slide the filter across the following diagram (it''s
    a *very* famous artwork by an artist called Piet Chew):'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行卷积，我们只需将过滤器滑动到以下图中（这是一位名叫皮特·丘的艺术家的一幅非常著名的艺术品）：
- en: '![](img/3306b7c3-de14-471f-947e-87e1c204875c.jpeg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3306b7c3-de14-471f-947e-87e1c204875c.jpeg)'
- en: 'Applying the preceding filter would yield something such as the following:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 应用前面的过滤器会产生如下效果：
- en: '![](img/cfaa8048-1e5d-4d65-a5ea-603a23d245e3.jpeg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cfaa8048-1e5d-4d65-a5ea-603a23d245e3.jpeg)'
- en: Yes, the filter blurs images!
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，过滤器会模糊图像！
- en: 'Here''s an example written in Go to emphasize the idea:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个用Go编写的示例，以强调这个想法：
- en: '[PRE10]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The function is quite slow and inefficient, of course. Gorgonia itself comes
    with a much more sophisticated algorithm
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 函数当然相当慢且效率低下。Gorgonia自带一个更复杂的算法
- en: Back to neural networks
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回到神经网络
- en: OK, so we now know that convolutions are important in the use of filters. But
    how does this relate to neural networks?
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们知道卷积在过滤器使用中很重要。但这与神经网络有什么关系呢？
- en: Recall that a neural network is defined as a linear transform (![](img/8868b16c-e984-46bd-ae8f-1434e9b705e7.png))
    with a non-linearity applied on it (written as ![](img/7d355b95-e418-44e3-a7a5-cad40667f0f1.png)).
    Note that *x*, the input image, is acted upon as a whole. This would be like having
    a single filter across the entire image. But what if we could process the image
    one small section at a time?
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，神经网络被定义为作用于其上的非线性应用（![](img/8868b16c-e984-46bd-ae8f-1434e9b705e7.png)）的线性变换。注意，*x*，输入图像，作为一个整体被作用。这就像在整个图像上有一个单一的过滤器。但如果我们能一次处理图像的一小部分会怎样呢？
- en: To add to that, in the preceding section, I showed how a simple filter could
    be used to blur an image. Filters could also be used to sharpen an image, picking
    out features that matter and blurring out features that don't. So, what if a machine
    could learn what filter to create?
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些，在前一节中，我展示了如何使用一个简单的过滤器来模糊图像。过滤器也可以用来锐化图像，突出重要的特征，同时模糊掉不重要的特征。那么，如果一台机器能够学会创建什么样的过滤器呢？
- en: 'That''s the reason why we would want to use a convolution in a neural network:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么我们想在神经网络中使用卷积的原因：
- en: Convolutions act on small parts of the image at a time, leaving only features
    that matter
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积一次作用于图像的小部分，只留下重要的特征
- en: We can learn the specific filters
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以学习特定的过滤器
- en: This gives a lot of fine-tuned control to the machine. Now, instead of a rough
    feature detector that works on the whole image at once, we can build many filters,
    each specializing to a specific feature, thus allowing us to extract the features
    necessary for the classification of numbers.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这给了机器很多精细的控制。现在，我们不再需要一个同时作用于整个图像的粗糙特征检测器，我们可以构建许多过滤器，每个过滤器专门针对一个特定的特征，从而允许我们提取出对数字分类必要的特征。
- en: Max-pooling
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Max-pooling
- en: Now we have in our minds a conceptual machine that will learn the filters that
    it needs to apply to extract features from an image. But, at the same time, we
    don't want the machine to overfit on the learning. A filter that is overly specific
    to the training data is not useful in real life. If a filter learns, for example,
    that all human faces have two eyes, a nose, and a mouth, and that's all, it wouldn't
    be able to classify a picture of a person with half their face obscured.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们心中有一个概念性的机器，它会学习它需要应用到图像上以提取特征的过滤器。但是，同时，我们不想让机器过度拟合学习。一个对训练数据过度具体的过滤器在现实生活中是没有用的。例如，如果一个过滤器学会所有的人类面孔都有两只眼睛、一个鼻子和一个嘴巴，那就结束了，它将无法分类一个半张脸被遮挡的人的图片。
- en: So, in an attempt to teach a ML algorithm to be able to generalize better, we
    simply give it less information. Max-pooling is one such process, as is *dropout*
    (see the next section).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了尝试教会机器学习算法更好地泛化，我们只是给它更少的信息。Max-pooling是这样一个过程，*dropout*（见下一节）也是如此。
- en: 'How max pooling works is it partitions the input data into non-overlapping
    regions, and simply finds the maximum value of that region:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Max-pooling的工作原理是将输入数据分成非重叠的区域，并简单地找到该区域的最大值：
- en: '![](img/4cc49a97-54a3-4bb8-a123-fe8bc7b60321.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4cc49a97-54a3-4bb8-a123-fe8bc7b60321.png)'
- en: There is, of course, an implicit understanding that this definitely changes
    the shape of the output. In fact, you will observe that it shrinks the image.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，有一个隐含的理解，这肯定会改变输出的形状。实际上，你会观察到它缩小了图像。
- en: Dropout
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dropout
- en: 'The result after max-pooling is minimum information within the output. But
    this may still be too much information; the machine may still overfit. Therefore,
    a very interesting quandary arises: what if some of the activations were randomly
    zeroed?'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: Max-pooling后的结果是输出中的最小信息。但这可能仍然信息过多；机器可能仍然会过度拟合。因此，出现了一个非常有趣的问题：如果随机将一些激活置零会怎样？
- en: This is the basis of dropout. It's a remarkably simple idea that improves upon
    the machine learning algorithm's ability to generalize, simply by having deleterious
    effects on information. With every iteration, random activations are zeroed. This
    forces the algorithm to only learn what is really important. How it does so involves
    structural algebra and is a story for another day.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是Dropout的基础。这是一个非常简单但能提高机器学习算法泛化能力的方法，它通过影响信息来达到目的。在每次迭代中，随机激活被置零。这迫使算法只学习真正重要的东西。它是如何做到这一点的涉及到结构代数，这是另一个故事。
- en: For the purposes of this project, Gorgonia actually handles dropout by means
    of element-wise multiplication by a randomly generated matrix of 1s and 0s.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目来说，Gorgonia实际上是通过使用随机生成的1s和0s矩阵进行逐元素乘法来处理Dropout的。
- en: Describing a CNN
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 描述CNN
- en: 'Having said all that, the neural network is very easy to build. First, we define
    a neural network as such:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 说了这么多，构建神经网络是非常简单的。首先，我们这样定义一个神经网络：
- en: '[PRE11]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here, we defined a neural network with four layers. A convnet layer is similar
    to a linear layer in many ways. It can, for example, be written as an equation:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了一个具有四层的神经网络。卷积层在许多方面类似于线性层。例如，它可以写成方程：
- en: '![](img/25dbe445-4aa7-4544-8e5d-c6c58ea5b98a.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/25dbe445-4aa7-4544-8e5d-c6c58ea5b98a.png)'
- en: Note that in this specific example, I consider dropout and max-pool to be part
    of the same layer. In many literatures, they are considered to be separate layers.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这个特定的例子中，我考虑dropout和max-pool是同一层的部分。在许多文献中，它们被认为是独立的层。
- en: I personally do not see the necessity to consider them as separate layers. After
    all, everything is just a mathematical equation; composing functions comes naturally.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我个人认为没有必要将它们视为独立的层。毕竟，一切只是数学方程；函数的组合是自然而然的。
- en: 'A mathematical equation on its own without structure is quite meaningless.
    Unfortunately, we do not have technology usable enough to simply define the structure
    of a data type (the hotness is in dependently-typed languages, such as Idris,
    but they are not yet at the level of usability or performance that is necessary
    for deep learning). Instead, we have to constrain our data structure by providing
    a function to define a `convnet`:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 一个没有结构的数学方程本身是相当没有意义的。不幸的是，我们并没有足够的技术来简单地定义数据类型（类型依赖性语言，如Idris，在这方面很有前景，但它们还没有达到深度学习所需的可用性或性能水平）。相反，我们必须通过提供一个函数来定义`convnet`来约束我们的数据结构：
- en: '[PRE12]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We'll start with `dt`. This is essentially a global variable denoting what data
    type we would like to work in. For the purposes of this project, we can use `var
    dt = tensor.Float64`, to indicate that we would like to work with `float64` throughout
    the entire project. This allows us to immediately reuse the functions from the
    previous chapter without having to handle different data types. Note that if we
    do plan to use `float32`, the computation speed immediately doubles. In the repository
    to this chapter, you might note that the code uses `float32`.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从`dt`开始。这本质上是一个全局变量，表示我们希望在哪种数据类型下工作。对于这个项目的目的，我们可以使用`var dt = tensor.Float64`来表示我们希望在项目的整个过程中使用`float64`。这允许我们立即重用上一章中的函数，而无需处理不同的数据类型。注意，如果我们确实计划使用`float32`，计算速度会立即加倍。在本书的代码库中，你可能会注意到代码使用了`float32`。
- en: We'll start with `d0` all the way to `d3`. This is fairly simple. For the first
    three layers, we want 20% of the activations to be randomly zeroed. But for the
    last layer, we want 55% of the activations to be randomly zeroed. In really broad
    strokes, this causes an information bottleneck, which will cause the machine to
    learn only the really important features.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从`d0`一直到最后`d3`开始。这相当简单。对于前三层，我们希望20%的激活随机置零。但对于最后一层，我们希望55%的激活随机置零。从非常粗略的角度来看，这会导致信息瓶颈，这将导致机器只学习真正重要的特征。
- en: Take a look at how `w0` is defined. Here, we're saying `w0` is a variable called
    `w0`. It is a tensor with the shape of (32, 1, 3, 3). This is typically called
    the **Number of Batches, Channels, Height, Width** (**NCHW**/**BCHW**) format.
    In short, what we're saying is that there are 32 filters we wish to learn, each
    filter has a height and width of (3, 3), and it has one color channel. MNIST is,
    after all, black and white.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 看看`w0`是如何定义的。在这里，我们说`w0`是一个名为`w0`的变量。它是一个形状为(32, 1, 3, 3)的张量。这通常被称为**批次数量、通道、高度、宽度**（**NCHW**/**BCHW**）格式。简而言之，我们说的是我们希望学习32个过滤器，每个过滤器的高度和宽度为(3,
    3)，并且它有一个颜色通道。MNIST毕竟只有黑白。
- en: BCHW is not the only format! Some deep learning frameworks prefer to use BHWC
    formats. The reason for preferring one format over another is purely operational.
    Some convolution algorithms work better with NCHW; some work better with BHWC.
    The ones in Gorgonia works only in BCHW.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: BCHW不是唯一的格式！一些深度学习框架更喜欢使用BHWC格式。选择一种格式而不是另一种格式纯粹是出于操作上的考虑。一些卷积算法与NCHW配合得更好；一些与BHWC配合得更好。Gorgonia中的那些只支持BCHW。
- en: The choice of a 3 x 3 filter is purely unprincipled but not without precedence.
    You could choose a 5 x 5 filter, or a 2 x 1 filter, or really, a filter of any
    shape. However, it has to be said that a 3 x 3 filter is probably the most universal
    filter that can work on all sorts of images. Square filters of these sorts are
    common in image-processing algorithms, so it is in accordance to such traditions
    that we chose a 3 x 3.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 3 x 3 滤波器的选择纯粹是无原则的，但并非没有先例。你可以选择 5 x 5 滤波器，或者 2 x 1 滤波器，或者实际上，任何形状的滤波器。然而，必须说的是，3
    x 3 滤波器可能是最通用的滤波器，可以在各种图像上工作。这类正方形滤波器在图像处理算法中很常见，因此我们选择 3 x 3 是遵循这样的传统。
- en: 'The weights for the higher layers start to look a bit more interesting. For
    example, `w1` has a shape of (64, 32, 3, 3). Why? In order to understand why,
    we need to explore the interplay between the activation functions and the shapes.
    Here''s the entire forward function of the `convnet`:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 高层权重开始看起来更有趣。例如，`w1` 的形状为 (64, 32, 3, 3)。为什么？为了理解为什么，我们需要探索激活函数和形状之间的相互作用。以下是
    `convnet` 的整个前向函数：
- en: '[PRE13]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: It should be noted that convolution layers do change the shape of the inputs.
    Given an (N, 1, 28, 28) input, the `Conv2d` function will return a (N, 32, 28,
    28) output, precisely because there are now 32 filters. The `MaxPool2d` will return
    an output with the shape of (N, 32, 14, 14); recall that the purpose of max-pooling
    is to reduce the amount of information in the neural network. It just happens
    that max-pooling with a shape of (2, 2) will nicely halve the length and width
    of the image (and reduce the amount of information by four times).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 应该注意的是，卷积层确实会改变输入的形状。给定一个 (N, 1, 28, 28) 的输入，`Conv2d` 函数将返回一个 (N, 32, 28, 28)
    的输出，这正是因为现在有 32 个滤波器。`MaxPool2d` 将返回一个形状为 (N, 32, 14, 14) 的输出；回想一下，最大池化的目的是减少神经网络中的信息量。碰巧的是，形状为
    (2, 2) 的最大池化将很好地将图像的长度和宽度减半（并将信息量减少四倍）。
- en: 'The output of layer 0 would have a shape of (N, 32, 14, 14). If we stick to
    our explanations of our shapes from earlier, where it was in the format of (N,
    C, H, W), we would be quite stumped. What does it mean to have 32 channels? To
    answer that, let''s look at how we encode a color image in terms of BCHW:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 第 0 层的输出形状为 (N, 32, 14, 14)。如果我们坚持我们之前对形状的解释，即格式为 (N, C, H, W)，我们可能会感到困惑。32
    个通道是什么意思？为了回答这个问题，让我们看看我们是如何根据 BCHW 编码彩色图像的：
- en: '![](img/68a7c029-efcf-489f-8097-ad069dcf4be9.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/68a7c029-efcf-489f-8097-ad069dcf4be9.png)'
- en: Note that we encode it as three separate layers, stacked onto one another. This
    is a clue as to how to think about having 32 channels. Of course, each of the
    32 channels as the result of applying each of the 32 filters; the extracted features,
    so to speak. The result can, of course, be stacked in the same way color channels
    be stacked.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们将其编码为三个单独的层，堆叠在一起。这是关于如何思考有 32 个通道的一个线索。当然，每个 32 个通道都是应用每个 32 个滤波器的结果；可以说是提取的特征。结果当然可以以相同的方式堆叠，就像颜色通道一样。
- en: For the most part, however, the mere act of symbol pushing is all that is required
    to build a deep learning system; no real intelligence is required. This, of course
    mirrors, the Chinese Room Puzzle thought experiment, and I have quite a bit to
    say on that, though it's not really the time nor the place.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在很大程度上，仅仅进行符号推演就足以构建一个深度学习系统；不需要真正的智能。这当然反映了中国房间难题的思想实验，我对这一点有很多话要说，尽管现在并不是时候也不是地方。
- en: The more interesting parts is in the construction of Layer 3\. Layers 1 and
    2 are constructed very similarly to Layer 0, but Layer 3 has a slightly different
    construction. The reason is because the output of Layer 2 is a rank-4 tensor,
    but in order to perform matrix multiplication, it needs to be reshaped into a
    rank-2 tensor.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 更有趣的部分在于第三层的构建。第一层和第二层的构建与第 0 层非常相似，但第三层的构建略有不同。原因是第 2 层的输出是一个秩为 4 的张量，但为了执行矩阵乘法，它需要被重塑为秩为
    2 的张量。
- en: Lastly, the final layer, which decodes the output, uses a softmax activation
    function to ensure that the result we get is probability.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，解码输出的最后一层使用 softmax 激活函数来确保我们得到的结果是概率。
- en: And really, there you have it. A CNN, written in a very neat way that does not
    obfuscate the mathematical definitions.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这就是你所看到的。一个用非常整洁的方式编写的 CNN，它并没有模糊数学定义。
- en: Backpropagation
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播
- en: 'For the convnet to learn, what is required is backpropagation, which propagates
    the errors, and a gradient descent function to update the weight matrices. To
    do this is relatively simple with Gorgonia, so simple that we can actually put
    it into our main function without impacting understandability:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让卷积神经网络学习，所需的是反向传播，它传播误差，以及一个梯度下降函数来更新权重矩阵。在Gorgonia中这样做相对简单，简单到我们甚至可以在主函数中实现它而不影响可读性：
- en: '[PRE14]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'For the errors, we use a simple cross-entropy by multiplying the expected output
    element-wise and then averaging it, as shown in this snippet:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 对于误差，我们使用简单的交叉熵，通过逐元素相乘期望输出并平均，如本片段所示：
- en: '[PRE15]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Following that, we simply call `gorgonia.Grad(cost, m.learnables()...)`, which
    performs symbolic backpropagation. What is `m.learnables()`?, you may ask. It''s
    simply the variables that we wish the machine to learn. The definition is as such:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之后，我们只需调用`gorgonia.Grad(cost, m.learnables()...)`，它执行符号反向传播。你可能想知道`m.learnables()`是什么？它只是我们希望机器学习的变量。定义如下：
- en: '[PRE16]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Again, it's fairly simple.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，这相当简单。
- en: One additional comment I want the reader to note is `gorgonia.Read(cost, &costVal)`.
    `Read` is one of the more confusing parts of Gorgonia. But when framed correctly,
    it is quite simple to understand.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个我想让读者注意的评论是`gorgonia.Read(cost, &costVal)`。`Read`是Gorgonia中较为复杂的一部分。但是，当正确地构建框架时，它相当容易理解。
- en: Earlier, in the section *Describing a neural network*, I likened Gorgonia to
    writing in another programming language. If so, then `Read` is the equivalent
    of `io.WriteFile`. What `gorgonia.Read(cost, &costVal)` says is that when the
    mathematical expression gets evaluated, make a copy of the result of `cost` and
    store it in `costVal`. This is necessary because of the way mathematical expressions
    are evaluated within the Gorgonia system.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 早期，在*描述神经网络*这一节中，我把Gorgonia比作用另一种编程语言进行编写。如果是这样，那么`Read`就相当于`io.WriteFile`。`gorgonia.Read(cost,
    &costVal)`所表达的意思是，当数学表达式被评估时，将`cost`的结果复制并存储在`costVal`中。这是由于Gorgonia系统中数学表达式评估的方式所必需的。
- en: Why is it called `Read` instead of `Write`? I initially modeled Gorgonia to
    be quite monadic (in the Haskell sense of monad), and as a result, one would *read
    out* a value. After a span of three years, the name sort of stuck.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么叫`Read`而不是`Write`？我最初将Gorgonia建模为相当单调的（在Haskell单调的概念中），因此人们会*读取*一个值。经过三年的发展，这个名字似乎已经固定下来。
- en: Running the neural network
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行神经网络
- en: Observe that up to this point, we've merely described the computations we need
    to perform. The neural network doesn't actually run; this is simply a description
    on the neural network to run.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，到目前为止，我们仅仅描述了我们需要执行的运算。神经网络实际上并没有运行；这只是在描述要运行的神经网络。
- en: 'We need to be able to evaluate the mathematical expression. In order to do
    so, we need to compile the expression into a program that can be executed. Here''s
    the code to do it:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要能够评估数学表达式。为了做到这一点，我们需要将表达式编译成一个可执行的程序。以下是实现这一点的代码：
- en: '[PRE17]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'It''s not strictly necessary to call `gorgonia.Compile(g)`. This was done for
    pedagogical reasons, to showcase that the mathematical expression can indeed be
    compiled down into an assembly-like program. In production systems, I often just
    do something like this: `vm := gorgonia.NewTapeMachine(g, gorgonia.BindDualValues(m.learnables()...))`.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`gorgonia.Compile(g)`并不是严格必要的。这样做是为了教学目的，展示数学表达式确实可以被编译成类似汇编的程序。在生产系统中，我经常这样做：`vm
    := gorgonia.NewTapeMachine(g, gorgonia.BindDualValues(m.learnables()...))`。
- en: There are two provided `vm` types in Gorgonia, each representing different modes
    of computation. In this project, we're merely using `NewTapeMachine` to get a
    `*gorgonia.tapeMachine`. The function to create a `vm` takes many options, and
    the `BindDualValues` option simply binds the gradients of each of the variables
    in the models to the variables themselves. This allows for cheaper gradient descent.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: Gorgonia提供了两种`vm`类型，每种类型代表不同的计算模式。在这个项目中，我们仅仅使用`NewTapeMachine`来获取`*gorgonia.tapeMachine`。创建`vm`的函数有很多选项，而`BindDualValues`选项只是将模型中每个变量的梯度绑定到变量本身。这允许更便宜的梯度下降。
- en: 'Lastly, note that a `VM` is a resource. You should think of a `VM` as if it
    were an external CPU, a computing resource. It is good practice to close any external
    resources after we use them and, fortunately, Go has a very convenient way of
    handling cleanups: `defer vm.Close()`.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意，`VM`是一种资源。你应该把`VM`想象成一个外部的CPU，一个计算资源。在使用完外部资源后关闭它们是一个好的实践，幸运的是，Go有一个非常方便的方式来处理清理：`defer
    vm.Close()`。
- en: 'Before we move on to talk about gradient descent, here''s what the compiled
    program looks like, in pseudo-assembly:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: batches := numExamples / bs
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: log.Printf("Batches %d", batches)
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: bar := pb.New(batches)
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: bar.SetRefreshRate(time.Second)
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: bar.SetMaxWidth(80)
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: for i := 0; i < *epochs; i++ {
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: bar.Prefix(fmt.Sprintf("Epoch %d", i))
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: bar.Set(0)
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: bar.Start()
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: for b := 0; b < batches; b++ {
  id: totrans-296
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: start := b * bs
  id: totrans-297
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: end := start + bs
  id: totrans-298
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: if start >= numExamples {
  id: totrans-299
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: break
  id: totrans-300
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-301
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: if end > numExamples {
  id: totrans-302
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: end = numExamples
  id: totrans-303
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: var xVal, yVal tensor.Tensor
  id: totrans-305
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: if xVal, err = inputs.Slice(sli{start, end}); err != nil {
  id: totrans-306
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: log.Fatal("Unable to slice x")
  id: totrans-307
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-308
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: if yVal, err = targets.Slice(sli{start, end}); err != nil {
  id: totrans-309
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: log.Fatal("Unable to slice y")
  id: totrans-310
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-311
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: if err = xVal.(*tensor.Dense).Reshape(bs, 1, 28, 28); err != nil {
  id: totrans-312
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: log.Fatalf("Unable to reshape %v", err)
  id: totrans-313
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-314
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: gorgonia.Let(x, xVal)
  id: totrans-315
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: gorgonia.Let(y, yVal)
  id: totrans-316
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: if err = vm.RunAll(); err != nil {
  id: totrans-317
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'log.Fatalf("Failed at epoch  %d: %v", i, err)'
  id: totrans-318
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-319
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: solver.Step(gorgonia.NodesToValueGrads(m.learnables()))
  id: totrans-320
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: vm.Reset()
  id: totrans-321
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: bar.Increment()
  id: totrans-322
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-323
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: log.Printf("Epoch %d | cost %v", i, costVal)
  id: totrans-324
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: testImgs, err := readImageFile(os.Open("t10k-images.idx3-ubyte"))
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: if err != nil {
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: log.Fatal(err)
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: testlabels, err := readLabelFile(os.Open("t10k-labels.idx1-ubyte"))
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: if err != nil {
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: log.Fatal(err)
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: testData := prepareX(testImgs)
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: testLbl := prepareY(testlabels)
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: shape := testData.Shape()
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: visualize(testData, 10, 10, "testData.png")
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: var correct, total float32
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: numExamples = shape[0]
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: batches = numExamples / bs
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: for b := 0; b < batches; b++ {
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: start := b * bs
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: end := start + bs
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: if start >= numExamples {
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: break
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: if end > numExamples {
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: end = numExamples
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: var oneimg, onelabel tensor.Tensor
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: if oneimg, err = testData.Slice(sli{start, end}); err != nil {
  id: totrans-353
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: log.Fatalf("Unable to slice images (%d, %d)", start, end)
  id: totrans-354
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-355
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: if onelabel, err = testLbl.Slice(sli{start, end}); err != nil {
  id: totrans-356
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: log.Fatalf("Unable to slice labels (%d, %d)", start, end)
  id: totrans-357
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-358
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: if err = oneimg.(*tensor.Dense).Reshape(bs, 1, 28, 28); err != nil {
  id: totrans-359
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: log.Fatalf("Unable to reshape %v", err)
  id: totrans-360
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-361
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: gorgonia.Let(x, oneimg)
  id: totrans-362
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: gorgonia.Let(y, onelabel)
  id: totrans-363
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: if err = vm.RunAll(); err != nil {
  id: totrans-364
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: log.Fatal("Predicting (%d, %d) failed %v", start, end, err)
  id: totrans-365
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-366
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: label, _ := onelabel.(*tensor.Dense).Argmax(1)
  id: totrans-367
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: predicted, _ := m.outVal.(*tensor.Dense).Argmax(1)
  id: totrans-368
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: lblData := label.Data().([]int)
  id: totrans-369
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: for i, p := range predicted.Data().([]int) {
  id: totrans-370
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: if p == lblData[i] {
  id: totrans-371
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: correct++
  id: totrans-372
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-373
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: total++
  id: totrans-374
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-375
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'fmt.Printf("Correct/Totals: %v/%v = %1.3f\n", correct, total, correct/total)'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: label, _ := onelabel.(*tensor.Dense).Argmax(1)
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: predicted, _ := m.outVal.(*tensor.Dense).Argmax(1)
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: lblData := label.Data().([]int)
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: for i, p := range predicted.Data().([]int) {
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: if p == lblData[i] {
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: correct++
  id: totrans-384
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-385
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: total++
  id: totrans-386
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we wrote our own `argmax` function. Gorgonia's tensor
    package actually does provide a handy method for doing just that. But in order
    to understand what is going on, we will need to first look at the results.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: The shape of `m.outVal` is (N, 10), where N is the batch size. The same shape
    also shows for `onelabel`.  (N, 10) means N rows of 10 columns. What can these
    10 columns be? Well, of course they're the encoded numbers! So what we want is
    to find the maximum values amongst the column for each row. And that's the first
    dimension. Hence when a call to `.ArgMax()` is made, we specify 1 as the axis.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '`m.outVal`的形状是(N, 10)，其中N是批量大小。相同的形状也适用于`onelabel`。 (N, 10)意味着有N行，每行有10列。这10列可能是什么？当然，它们是编码的数字！所以我们要做的是找到每行的列中的最大值。这就是第一维。因此，当调用`.ArgMax()`时，我们指定1作为轴。'
- en: Therefore the result of the `.Argmax()` calls will have a shape (N). For each
    value in that vector, if they are the same for `lblData` and `predicted`, then
    we increment the `correct` counter. This gives us a way to count accuracy.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`.Argmax()`调用的结果将具有形状(N)。对于该向量中的每个值，如果它们对于`lblData`和`predicted`是相同的，那么我们就增加`correct`计数器。这为我们提供了一种计算准确度的方法。
- en: Accuracy
  id: totrans-392
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准确度
- en: We use accuracy because the previous chapter used accuracy. This allows us to
    have a apples-to-apples comparison. Additionally you may note that there is a
    lack of cross validation. That will be left as an exercise to the reader.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用准确度是因为上一章使用了准确度。这使得我们可以进行苹果对苹果的比较。此外，你可能还会注意到缺乏交叉验证。这将被留给读者作为练习。
- en: After training the neural network for two hours on a batch size of 50 and 150
    epochs, I'm pleased to say I got a 99.87% accuracy. And this isn't even state
    of the art!
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 在对批量大小为50和150个周期的神经网络训练了两个小时后，我很高兴地说，我得到了99.87%的准确度。这甚至还不是最先进的！
- en: In the previous chapter, it took just 6.5 minutes to get a 97% accuracy. That
    additional 2% accuracy required a lot more time. This is a factor in real life.
    Often business decisions are a big factor in choosing ML algorithm.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，仅用6.5分钟就达到了97%的准确度。而额外提高2%的准确度则需要更多的时间。这在现实生活中是一个因素。通常，商业决策是选择机器学习算法的一个重要因素。
- en: Summary
  id: totrans-396
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about neural networks and studied about the Gorgonia
    library in detail. Then we learned how to recognize handwritten digits using a
    CNN.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了神经网络，并详细研究了Gorgonia库。然后我们学习了如何使用CNN识别手写数字。
- en: In the next chapter, we're going to strengthen our intuition about what can
    be done with computer vision, by building a multiple facial-detection system in
    Go.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将通过在Go中构建一个多面部检测系统来加强我们对计算机视觉可以做什么的直觉。
