- en: Convolutional Neural Networks - MNIST Handwriting Recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, I posited a scenario where you are a postal worker
    trying to recognize handwriting. In that, we ended up with a neural network built
    on top of Gorgonia. In this chapter, we'll look at the same scenario, but we'll
    augment our ideas of what a neural network is and write a more advanced neural
    network, one that was, until very recently, state of the art.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, in this chapter, we are going to build a **Convolutional Neural
    Network** (**CNN**). A CNN is a type of deep learning network that has been popular
    in recent years.
  prefs: []
  type: TYPE_NORMAL
- en: Everything you know about neurons is wrong
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, I mentioned that everything you know about neural networks
    is wrong. Here, I repeat that claim. Most literature out there on a neural network
    starts with a comparison with biological neurones and ends there. This leads readers
    to often assume that it is. I'd like to make a point that artificial neural networks
    are *nothing* like their biological namesake.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, in the last chapter, I spent a significant amount of the chapter describing
    linear algebra, and explained that the twist is that you can express almost any
    **machine learning** (**ML**) problem as linear algebra. I shall continue to do
    so in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than think of artificial neural networks as analogies of real-life neural
    networks, I personally encourage you to think of artificial neural networks as
    mathematical equations. The non-linearities introduced by the activation functions,
    combined with linear combinations allows for artificial neural networks to be
    able to approximate any function.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks – a redux
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The fundamental understanding that neural networks are mathematical expressions
    leads to really simple and easy implementations of neural networks. Recall from
    the previous chapter that a neural network can be written like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If we rewrite the code as a mathematical equation, we can write a neural network
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a67670dc-893a-4308-9b31-462f96a5a349.png)'
  prefs: []
  type: TYPE_IMG
- en: A side note: ![](img/86262bf8-91aa-45b1-a951-f2db5da20de6.png)is the same as ![](img/d6732aaf-6223-4477-af91-f022c81b9b92.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can simply write it out using Gorgonia, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code is a representation of the following neural network in images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/60f87787-65ce-40e1-b7a6-f4037719a3c5.png)'
  prefs: []
  type: TYPE_IMG
- en: The middle layer consists of 800 hidden units.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the preceding code hides a lot of things. You can't really expect
    a neural network from scratch in fewer than 20 lines, can you? To understand what
    is happening, we need to take a brief detour into understanding what Gorgonia
    is.
  prefs: []
  type: TYPE_NORMAL
- en: Gorgonia
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gorgonia is a library that provides primitives for working with mathematical
    expressions specific to deep learning. When working with a ML related project,
    you will start to find yourself more introspective about the world, and questioning
    assumptions all the time. This is a good thing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider what happens in your mind when you read the following mathematical
    expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2fbb360a-4c40-41b7-987d-ac97f71891a4.png)'
  prefs: []
  type: TYPE_IMG
- en: You should instantly think *hang on, that's false*. Why does your brain think
    this?
  prefs: []
  type: TYPE_NORMAL
- en: 'That''s mainly because your brain evaluated the mathematical expression. In
    general, there are three parts to the expression: the left-hand side, the equal
    symbol, and the right-hand side. Your brain evaluated each part separately and
    then evaluated the expression as false.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When we read mathematical expressions, we automatically evaluate the expressions
    in our mind that we take evaluation for granted. In Gorgonia, what we take for
    granted is made explicit. There are two general *parts* to using Gorgonia: defining
    an expression and evaluating an expression.'
  prefs: []
  type: TYPE_NORMAL
- en: Since you are most probably a programmer, you can think of the first part as
    writing a program, and the second part can be thought of as running a program.
  prefs: []
  type: TYPE_NORMAL
- en: When describing a neural network in Gorgonia, it's often instructive to imagine
    yourself writing in another programming language, one that is specific to building
    neural networks. This is because the patterns used in Gorgonia are not unlike
    a new programming language. Indeed, Gorgonia was built from ground-up with the
    idea that it's a programming language without a syntactical frontend. As such,
    in this section, I will often ask you to imagine writing in another Go-like language.
  prefs: []
  type: TYPE_NORMAL
- en: Why?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A good question to ask is *why?* Why bother with this separation of processes?
    After all, the preceding code could be rewritten as the previous chapter''s `Predict`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Here, we define the network in Go, and when we run the Go code, the neural network
    is run as it is being defined. What's the problem we face that we need to introduce
    the idea of separating the definition of the neural network and running it? We've
    already seen the problem when we wrote the `Train` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you recall, in the last chapter, I said that writing the `Train` method
    requires us to actually copy and paste code from the `Predict` method. To refresh
    your memory, here''s the `Train` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Let's go through an exercise of refactoring to highlight the problem. Taking
    off our ML hat for a bit, and putting on our software engineer hat, let's see
    how we can refactor `Train` and `Predict`, even if conceptually. We see in the
    `Train` method that we need access to `act0` and `pred` in order to backpropagate
    the errors. Where in `Predict` `act0` and `pred` are terminal values (that is,
    we don't use them after the function has returned), in `Train`, they are not.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, here, we can create a new method; let''s call it `fwd`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'And we can refactor `Predict` to look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'And the `Train` method would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This looks better. What exactly are we doing here? We are programming. We are
    rearranging one form of syntax into another form of syntax but we are not changing
    the semantics, the meaning of the program. The refactored program has exactly
    the same meaning as the pre-refactored program.
  prefs: []
  type: TYPE_NORMAL
- en: Programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Wait a minute, you might say to yourself. What do I mean by *the meaning of
    the program?* This is a surprisingly deep topic that involves a whole branch of
    mathematics known as **homotopy**. But for all practical intents and purposes
    of this chapter, let's define the *meaning* of a program to be the extensional
    definition of the program. If two programs compile and run, take the exact same
    inputs, and return the same exact output every time, we say two programs are equal.
  prefs: []
  type: TYPE_NORMAL
- en: 'These two programs would be equal:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Program A** | **Program B** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `fmt.Println("Hello World")` | `fmt.Printf("Hello " + "World\n")` |'
  prefs: []
  type: TYPE_TB
- en: 'Intentionally, if we visualize the programs as an **Abstract Syntax Tree**
    (**AST**), they look slightly different:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec6b6303-8a9f-448e-9cae-10d9465a6c30.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/80b016f8-e352-4740-8f0d-c65c83060bea.png)'
  prefs: []
  type: TYPE_IMG
- en: The syntax for both programs are different, but they are semantically the same.
    We can refactor program B into program A, by eliminating the `+`.
  prefs: []
  type: TYPE_NORMAL
- en: 'But note what we did here: we took a program and represented it as an AST.
    Through syntax, we manipulated the AST. This is the essence of programming.'
  prefs: []
  type: TYPE_NORMAL
- en: What is a tensor? – part 2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, there was an info box that introduced the concept
    of a tensor. That info box was a little simplified. If you google what a tensor
    is, you will get very conflicting results, which only serve to confuse. I don''t
    want to add to the confusion. Instead, I shall only briefly touch on tensors in
    a way that will be relevant to our project, and in a way very much like how a
    typical textbook on Euclidean geometry introduces the concept of a point: by holding
    it to be self-evident from use cases.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Likewise, we will hold tensors to be self-evident from use. First, we will
    look at the concept of multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s define a vector: ![](img/a86a3ad1-6ff3-4633-b115-91b671301d9f.png).
    You can think of it as this diagram:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/1d1d9e18-693e-42c1-9b0e-12a6aff3c24c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, let''s multiply the vector by a scalar value: ![](img/90b2efe0-f4c4-441f-9276-a22d5023c6bc.png).
    The result is something like this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/6b4e7f1c-38da-44ca-a8f5-cc1d5e185ccc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are two observations:'
  prefs: []
  type: TYPE_NORMAL
- en: The general direction of the arrow doesn't change.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only the length changes. In physics terms, this is called the magnitude. If
    the vector represents the distance travelled, you would have traveled twice the
    distance along the same direction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, how would you change directions by using multiplications alone? What do
    you have to multiply to change directions? Let''s try the following matrix, which
    we will call *T*, for transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5262facb-1ac0-4216-b920-b4e20e163d78.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now if we multiply the transformation matrix with the vector, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/93630693-f819-481c-b93d-4fa5b9ffca44.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And if we plot out the starting vector and the ending vector, we get the resultant
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b70d5c40-fdf6-479a-9405-1f361beef36e.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, the direction has changed. The magnitude too has changed.
  prefs: []
  type: TYPE_NORMAL
- en: Now, you might be saying, *hang on, isn't this just Linear Algebra 101?*. Yes,
    it is. But to really understand a tensor, we must learn how to construct one.
    The matrix that we just used is also a tensor of rank-2\. The proper name for
    a tensor of rank-2 is a **dyad**.
  prefs: []
  type: TYPE_NORMAL
- en: Why the mixing of naming conventions? Here's a bit of fun trivia. When I was
    writing the earliest versions of Gorgonia, I was musing about the terrible naming
    conventions computer science has had, a fact that Bjarne Stroustrup himself lamented.
    The canonical name for a rank-2 tensor is called a **dyad**, but can be represented
    as a matrix. I was struggling to properly call it; after all, there are power
    in names and to name it is to tame it.
  prefs: []
  type: TYPE_NORMAL
- en: At around the same time as I was developing the earliest versions of Gorgonia,
    I was following a most excellent BBC TV series called **Orphan Black**, in which
    the Dyad Institute is the primary foe of the protagonists. They were quite villainous
    and that clearly left an impact in my mind. I decided against naming it thus.
    In retrospect, this seemed like a rather silly decision.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s consider the transformation dyad. You can think of the dyad as a
    vector *u* times a vector *v*. To write it out in equation form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d7f28330-0c81-4a14-bff1-67c135e3574d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At this point, you may be familiar with the previous chapter''s notion of linear
    algebra. You might think to yourself: if two vectors multiply, that''d end up
    with a scalar value, no? If so, how would you multiply two vectors and get a matrix
    out of it?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we''d need to introduce a new type of multiplication: the outer product
    (and by contrast, the multiplication introduced in the previous chapter is an
    inner product). We write outer products with this symbol: ![](img/0fb20035-e86b-4189-9050-11b52ee9f24d.png).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically speaking, the outer product, also known as a dyad product, is
    defined as such:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e6158f0e-afbe-4214-a383-6a9f8d1a8d38.png)'
  prefs: []
  type: TYPE_IMG
- en: We won't be particularly interested in the specifics of *u* and *v* in this
    chapter. However, being able to construct a dyad from its constituent vectors
    is an integral part of what a tensor is all about.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we can replace *T* with *uv*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/862fd95c-5ca3-470f-a68d-7ae0fa0690ca.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we get ![](img/c1e433e0-9b4e-475b-bd90-2d3d079cd553.png) as the scalar magnitude
    change and *u* as the directional change.
  prefs: []
  type: TYPE_NORMAL
- en: So what is the big fuss with tensors? I can give two reasons.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, the idea that dyads can be formed from vectors generalizes upward.
    A three-tensor, or triad can be formed by a dyad product *uvw*, a four-tensor
    or a tetrad can be formed by a dyad product *uvwx*, and so on and so forth. This
    affords us a mental shortcut that will be very useful to us when we see shapes
    that are associated with tensors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The useful mental model of what a tensor can be thought as is the following:
    a vector is like a list of things, a dyad is like a list of vectors, a triad is
    like a list of dyads, and so on and so forth. This is absolutely helpful when
    thinking of images, like those that we''ve seen in the previous chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: An image can be seen as a (28, 28) matrix. A list of ten images would have the
    shape (10, 28, 28). If we wanted to arrange the images in such a way that it's
    a list of lists of ten images, it'd have a shape of (10, 10, 28, 28).
  prefs: []
  type: TYPE_NORMAL
- en: 'All this comes with a caveat of course: a tensor can only be defined in the
    presence of transformation. As a physics professor once told me: *that which transforms
    like a tensor is a tensor*. A tensor devoid of any transformation is just an *n*-dimensional
    array of data. The data must transform, or flow from tensor to tensor in an equation.
    In this regards, I think that TensorFlow is a ridiculously well-named product.'
  prefs: []
  type: TYPE_NORMAL
- en: For more information on tensors, I would recommend the relatively dense text
    book, *Linear Algebra and Geometry* by Kostrikin (I failed to finish this book,
    but it was this book that gave me what I believe to be a strong-ish understanding
    of tensors). More on the flow of tensors can be found in Spivak's *Manifold Calculus*.
  prefs: []
  type: TYPE_NORMAL
- en: All expressions are graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we can finally return to the preceding example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our problem, if you recall, is that we had to specify the neural network twice:
    once for prediction and once for learning purposes. We then refactored the program
    so that we don''t have to specify the network twice. Additionally, we had to manually
    write out the expression for the backpropagation. This is error prone, especially
    when dealing with larger neural networks like the one we''re about to build in
    this chapter. Is there a better way? The answer is yes.'
  prefs: []
  type: TYPE_NORMAL
- en: Once we understand and fully internalize that neural networks are essentially
    mathematical expressions, we can take the learning's from tensors, and model a
    neural network where the entire neural network is a flow of tensors.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that tensors can only be defined in the presence of transformation; then,
    any operation that transforms tensor(s), used in concert with data structures
    that hold data are tensors. Also, recall that computer programs can be represented
    as abstract syntax trees. Mathematical expressions can be represented as a program.
    Therefore, mathematical expressions can also be represented as an abstract syntax
    tree.
  prefs: []
  type: TYPE_NORMAL
- en: More accurate, however, is that mathematical expressions can be expressed as
    a graph; a directed acyclic graph, to be specific. We call this the **expression
    graph**.
  prefs: []
  type: TYPE_NORMAL
- en: 'This distinction matters. Trees cannot share nodes. Graphs can. Let''s consider,
    for example, the following mathematical expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/773f21bd-f1bd-4f85-bed9-d3b4351558ad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here are the representations as a graph and as a tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/042d0364-e315-454a-8a18-85ca98bcb637.png)'
  prefs: []
  type: TYPE_IMG
- en: On the left, we have a directed acyclic graph, and on the right, we have a tree.
    Note that in the tree variant of the mathematical equation, there are repeat nodes.
    Both are rooted at ![](img/bc041d3c-1879-480d-b2c1-ee4e3e65713e.png). The arrow
    should be read as *depends on*. ![](img/1ffd83f8-66fd-437d-ab43-bc22d9502e7e.png)
    depends on two other nodes, ![](img/0d90444f-20a9-498d-8d65-11d5c6b644fe.png)
    and ![](img/d4029dca-e311-4e99-ad95-9648fbdf599d.png), and so on and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: Both the graph and tree are valid representations of the same mathematical equation,
    of course.
  prefs: []
  type: TYPE_NORMAL
- en: Why bother representing a mathematical expression as a graph or a tree? Recall
    that an abstract syntax tree represents a computation. If a mathematical expression,
    represented as a graph or a tree, has a shared notion of computation, then it
    also represents an abstract syntax tree.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, we can take each node in the graph or tree, and perform a computation
    on it. If each node is a representation of a computation, then logic holds that
    fewer nodes means faster computations (and less memory usage). Therefore, we should
    prefer to use the directed acyclic graph representation.
  prefs: []
  type: TYPE_NORMAL
- en: 'And now we come to the major benefit of representing a mathematical expression
    as a graph: we get differentiation for free.'
  prefs: []
  type: TYPE_NORMAL
- en: If you recall from the previous chapter, backpropagation is essentially differentiating
    the cost with regards to the inputs. The gradients, once calculated, can then
    be used to update the values of the weights themselves. Having a graph structure,
    we wouldn't have to write the backpropagation parts. Instead, if we have a virtual
    machine that executes the graph, starting at the leaves and moving toward the
    root, the virtual machine can automatically perform differentiation on the values
    as it traverses the graph from leaf to root.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, if we don't want to do automatic differentiation, we can also
    perform symbolic differentiation by manipulating the graph in the same way that
    we manipulated the AST in the *What is programming* section, by adding and coalescing
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this way, we can now shift our view of a neural network to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/145f53a8-6df1-467b-9981-1b1ec6976d5f.png)'
  prefs: []
  type: TYPE_IMG
- en: Describing a neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let''s get back to the task of writing a neural network and thinking of
    it in terms of a mathematical expression expressed as a graph. Recall that the
    code looks something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now let's go through this code.
  prefs: []
  type: TYPE_NORMAL
- en: First, we create a new expression graph with `g := G.NewGraph()`. An expression
    graph is a holder object to hold the mathematical expression. Why would we want
    an expression graph? The mathematical expression that represents a neural network
    is contained in the `*gorgonia.ExpressionGraph` object.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematical expressions are only interesting if we use variables. ![](img/65a9f649-813c-4549-bf9e-216a9ab70c7f.png)
    is quite an uninteresting expression because you can't do much with this expression.
    The only thing you can do with it is to evaluate the expression and see if it
    returns true or false. ![](img/dfaa4a00-b8a2-4fea-98fe-e69fc78d969c.png) is slightly
    more interesting. But, then again, *a* can only be *1*.
  prefs: []
  type: TYPE_NORMAL
- en: Consider, however, the expression ![](img/f4cd11b6-41d0-4a24-bbd3-657288d1aa15.png).
    With two variables, it suddenly becomes a lot more interesting. The values that
    *a* and *b* can take are dependent on one another, and there is a whole range
    of possible pairs of numbers that can fit into *a *and *b*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that each layer of neural network is just a mathematical expression
    that reads like this: ![](img/3f3d99df-f160-4131-b517-a183b5f40033.png). In this
    case, *w*, *x*, and *b* are variables. So, we create them. Note that in this case,
    Gorgonia treats variables as a programming language does: you have to tell the
    system what the variable represents.'
  prefs: []
  type: TYPE_NORMAL
- en: In Go, you would do that by typing `var x Foo`, which tells the Go compiler
    that `x` should be a type `Foo`. In Gorgonia, the mathematical variables are declared
    by using `NewMatrix`, `NewVector`, `NewScalar`, and `NewTensor`. `x := G.NewMatrix(g,
    Float, G.WithName, G.WithShape(N, 728))` simply says `x` is a matrix in expression
    graph `g` with a name `x`, and has a shape of `(N, 728)`.
  prefs: []
  type: TYPE_NORMAL
- en: Here, readers may observe that `728` is a familiar number. In fact, what this
    tells us is that `x` represents the input, which is `N` images. `x`, therefore,
    is a matrix of *N* rows, where each row represents a single image (728 floating
    points).
  prefs: []
  type: TYPE_NORMAL
- en: 'The eagle-eyed reader would note that `w` and `b` have extra options, where
    the declaration of `x` does not. You see, `NewMatrix` simply declares the variable
    in the expression graph. There is no value associated with it. This allows for
    flexibility when the value is attached to a variable. However, with regards to
    the weight matrix, we want to start the equation with some initial values. `G.WithInit(G.Uniform(1.0))`
    is a construction option that populates the weight matrix with values pulled from
    a uniform distribution with a gain of `1.0`. If you imagine yourself coding in
    another language specific to building neural networks, it''d look something like
    this: `var w Matrix(728, 800) = Uniform(1.0)`.'
  prefs: []
  type: TYPE_NORMAL
- en: Following that, we simply write out the mathematical equation: ![](img/4cdbb766-d42f-4356-bf64-0644b8de86ab.png)
    is simply a matrix multiplication between ![](img/4979cf68-4f3a-4b41-9ec1-5c4ecd4d150d.png)
    and ![](img/641b17c8-e982-4a2d-a7e1-9affa6afae46.png); hence, `xw, _ := G.Mul(x,
    w)`. At this point, it should be clarified that we are merely describing the computation
    that is supposed to happen. It is yet to happen. In this way, it is not dissimilar
    to writing a program; writing code does not equal running the program.
  prefs: []
  type: TYPE_NORMAL
- en: '`G.Mul` and most operations in Gorgonia actually returns an error. For the
    purposes of this demonstration, we''re ignoring any errors that may arise from
    symbolically multiplying `x` and `w`. What could possibly go wrong with simple
    multiplication? Well, we''re dealing with matrix multiplication, so the shapes
    must have matching inner dimensions. A (N, 728) matrix can only be multiplied
    by a (728, M) matrix, which leads to an (N, M) matrix. If the second matrix does
    not have 728 rows, then an error will happen. So, in real production code, error
    handling is a** must**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Speaking of *must*, Gorgonia comes with a utility function, called, **G.Must**.
    Taking a cue from the `text/template` and `html/template` libraries found in the
    standard library, the `G.Must` function panics when an error occur. To use, simply
    write this: `xw := G.Must(G.Mul(x,w))`.'
  prefs: []
  type: TYPE_NORMAL
- en: After the inputs are multiplied with the weights, we add to the biases using
    `G.Add(xw, b)`. Again, errors may occur, but in this example, we're eliding the
    checks of errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we take the result and perform a non-linearity: a sigmoid function,
    with `G.Sigmoid(xwb)`. This layer is now complete. Its shape, if you follow, would
    be (N, 800).'
  prefs: []
  type: TYPE_NORMAL
- en: The completed layer is then used as an input for the following layer. The next
    layer has a similar layout as the first layer, except instead of a sigmoid non-linearity,
    a `G.SoftMax` is used. This ensures that each row in the resulting matrix sums
    1.
  prefs: []
  type: TYPE_NORMAL
- en: One-hot vector
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Perhaps, not so coincidentally, the last layer has the shape of (N, 10). N
    is the number of input images (which we''ve gotten from `x`) ; that''s fairly
    self-explanatory. It also means that there is a clean mapping from input to output.
    What''s not self-explanatory is the 10\. Why 10? Simply put, there are 10 possible
    numbers we want to predict - 0, 1, 2, 3, 4, 5, 6, 7, 8, 9:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c8df17fe-0447-45c6-9949-e9ceb0e44c7b.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram is an example result matrix. Recall that we used `G.SoftMax`
    to ensure that each row sums up to 1\. Therefore, we can interpret the numbers
    in each column of each row to be the probability that it is the specific digit
    that we're predicting. To find the digit we're predicting, simply find the highest
    probability in each column.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, I introduced the concept of one-hot vector encoding.
    To recap, it takes a slice of labels and returns a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3a72b6c1-df67-44cf-a666-3c493e8dc6fc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, this is clearly a matter of encoding. Who''s to say that column 0 would
    have to represent 0? We could of course come up with a completely crazy encoding
    like such and the neural network would still work:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f68f2ab0-abf5-4b28-8ff1-3fd91db7c3a7.png)'
  prefs: []
  type: TYPE_IMG
- en: Of course, we would not be using such a scheme for encoding; it would be a massive
    source of programmer error. Instead, we would go for the standard encoding of
    a one-hot vector.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this has given you a taste of how powerful the notion of an expression
    graph can be. One thing we haven't touched upon yet is the execution of the graph.
    How do you run a graph? We'll look further into that in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With all that done, it's time to get on to the project! Once again, we are going
    to recognize handwritten digits. But this time around, we're going to build a
    CNN for that. Instead of just using the `tensor` package of Gorgonia, this time
    we're going to use all of Gorgonia.
  prefs: []
  type: TYPE_NORMAL
- en: Once again, to install Gorgonia, simply run `go get -u gorgonia.org/gorgonia`
    and `go get -u gorgonia.org/tensor`.
  prefs: []
  type: TYPE_NORMAL
- en: Getting the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The data is the same data as in the previous chapter: the MNIST dataset. It
    can be found in the repository for this chapter, and we''ll be using a function
    we wrote in the previous chapter to acquire the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Other things from the previous chapter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Obviously, there is a lot from the previous chapter that we can reuse:'
  prefs: []
  type: TYPE_NORMAL
- en: The range normalization function (`pixelWeight`) and its isometric counterpart
    (`reversePixelWeight`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prepareX` and `prepareY`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `visualize` function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For convenience sake, here they are again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What we will be building is a CNN. So, what is a Convolutional Neural Network?
    As its name suggests, it's a neural network, not unlike the one we have built
    in the previous chapter. So, clearly, there are elements that are similar. There
    are also elements that are not similar, for if they were similar, we wouldn't
    have this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: What are convolutions?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main difference between the neural network we built in the previous chapter
    and a CNN is the convolutional layer. Recall that the neural network was able
    to learn features related to digits. In order to be more accurate, the neural
    network layers need to learn more specific features. One way to do this is to
    add more layers; more layers would lead to more features being learned, giving
    rise to deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'On a spring evening of 1877, people dressed in what modern-day people would
    consider as *black-tie* gathered at the Royal Institute, in London. The speaker
    for the evening was Francis Galton, the same Galton we met in [Chapter 1](3d68e167-a44d-4195-a270-f8180ff8f85f.xhtml),
    *How to Solve All Machine Learning Problems*. In his talk, Galton brought out
    a curious device, which he called a **quincunx**. It was a vertical wooden board
    with wooden pegs sticking out of it, arranged in a uniform, but interleaved manner.
    The front of it was covered with glass and there was an opening at the top. Tiny
    balls are then dropped from the top and as they hit the pegs, bounce left or right,
    and fall to the corresponding chutes. This continues until the balls collect at
    the bottom:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/254eef09-5de0-42b6-b2a8-076a13d440e4.png)'
  prefs: []
  type: TYPE_IMG
- en: A curious shape begins to form. It's the shape modern statisticians have come
    to recognize as the binomial distribution. Most statistical textbooks end the
    story about here. The quincunx, now known as the Galton Board, illustrates, very
    clearly and firmly, the idea of the central limit theorem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our story, of course, doesn''t end there. Recall in [Chapter 1](3d68e167-a44d-4195-a270-f8180ff8f85f.xhtml), *How
    to Solve All Machine Learning Problems*, that I mentioned that Galton was very
    much interested in hereditary issues. A few years earlier, Galton had published
    a book called *Hereditary Genius*. He had collected data on *eminent* persons
    in Great Britain across the preceding centuries, and much to his dismay, he found
    that *eminent* parentage tended to lead to un-eminent children. He called this
    a **reversion to the mediocre**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a38fc9b-c6ce-4fb6-bd87-0bea52d33083.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/3e483077-8c75-442f-9367-efcde7a3003f.png)'
  prefs: []
  type: TYPE_IMG
- en: And, yet, he reasoned, the mathematics doesn't show such things! He explained
    this by showing off a quincunx with two layers. A two-layered quincunx was a stand-in
    for the generational effect. The top layer would essentially be the distribution
    of a feature (say, height). Upon dropping to the second layer, the beads would
    cause the distribution to *flatten out*, which is not what he had observed. Instead,
    he surmised that there has to be another factor which causes the regression to
    the mean. To illustrate his idea, he installed chutes as the controlling factor,
    which causes a regression to the mean. A mere 40 years later, the rediscovery
    of Mendel's pea experiments would reveal genetics to be the factor. That is a
    story for another day.
  prefs: []
  type: TYPE_NORMAL
- en: 'What we''re interested in is why the distribution would *flatten out*. While
    the standard *it''s physics!* would suffice as an answer, there remains interesting
    questions that we could ask. Let''s look at a simplified depiction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5458700e-52d3-41e2-8859-61799bc48284.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we evaluate the probability that the ball will drop and hit a position.
    The curve indicates the probability of the ball landing at position B. Now, we
    add a second layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56a539a1-3b5f-4ffa-b039-02562e06a206.png)'
  prefs: []
  type: TYPE_IMG
- en: Say, from the previous layer, the ball landed at position 2\. Now, what is the
    probability that the ball's final resting place is at position D?
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate this, we need to know all the possible ways that the ball can
    end up at position D. Limiting our option to A to D only, here they are:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Level 1 Position** | **L1 Horizontal Distance** | **Level 2 position**
    | **L2 Horizontal Distance** |'
  prefs: []
  type: TYPE_TB
- en: '| A | 0 | D | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| B | 1 | D | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| C | 2 | D | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| D | 3 | D | 0 |'
  prefs: []
  type: TYPE_TB
- en: Now we can ask the question in terms of probability. The horizontal distances
    in the table are an encoding that allows us to ask the question probabilistically
    and generically. The probability of the ball travelling horizontally by one unit
    can be represented as *P(1)*, the probability of the ball travelling horizontally
    by two units can be represented as *P(2)*, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'And to calculate the probability that the ball ends up in D after two levels
    is essentially summing up all the probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e0fe1ff2-0715-4167-926b-587d0954a6cc.png).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can write it as such:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5da38259-9ba6-49f3-ac2c-8680c3f24f75.png)'
  prefs: []
  type: TYPE_IMG
- en: We can read it as the probability of the final distance being *$c = a+b$* is
    the sum of *$P_1(a)$*, with the probability of level 1, where the ball traveled
    horizontally by *$a$* and *$P_2(b)$*, with the probability of level 2, where the
    ball traveled horizontally by *$b$*.
  prefs: []
  type: TYPE_NORMAL
- en: 'And this is the typical definition of convolution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/64c598e7-27fd-40c9-80d3-5df72c34d70c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If the integral scares you, we can equivalently rewrite this as a summation
    operation (this is only valid because we are considering discrete values; for
    continuous real values, integrations have to be used):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/becbdaea-2d47-4a0a-bdff-452aca041ced.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, if you squint very carefully, this equation looks a lot like the preceding
    probability equation. Instead of ![](img/01f5c5c4-2cf9-4e7f-be8b-3a2230927b42.png),
    we can rewrite it as ![](img/5b3ffbce-5e13-4fd8-91dd-29ac6bc959c7.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/154d9ba7-91db-4d97-a194-846644eacbbf.png)'
  prefs: []
  type: TYPE_IMG
- en: And what are probabilities, but functions? There is, after all, a reason we
    write probabilities in the format $P(a)$. We can indeed genericize the probability
    equation to the convolution definition.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, for now, let''s strengthen our intuitions about what convolutions
    are. For that, we''ll keep the notion that the function we''re talking about has
    probabilities. First, we should note that the probability of the ball ending up
    in a particular location is dependent on where it starts. But imagine if the platform
    for the second platform moves horizontally:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a9663b8d-da9a-4422-8960-c544f9d25494.png)'
  prefs: []
  type: TYPE_IMG
- en: Now the probability of the final resting place of the ball is highly dependent
    on where the initial starting position is, as well as where the second layer's
    starting position is. The ball may not even land on the bottom!
  prefs: []
  type: TYPE_NORMAL
- en: 'So, here''s a good mental shortcut of thinking about convolutions: t''s as
    if one function in one layer is *sliding* across a second function.'
  prefs: []
  type: TYPE_NORMAL
- en: So, convolutions are what cause the *flattening* of Galton's quincunx. In essence,
    it is a function that slides on top of the probability function, flattening it
    out as it moves along the horizontal dimension. This is a one-dimensional convolution;
    the ball only travels along one dimension.
  prefs: []
  type: TYPE_NORMAL
- en: 'A two-dimensional convolution is similar to a one-dimensional convolution.
    Instead, there are two *distances* or metrics that we''re considering for each
    layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6178477d-0195-4954-8b6e-053bd86510d4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But this equation is nigh impenetrable. Instead, here''s a convenient series
    of pictures of how it works, step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Convolution (Step 1):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/158dfe31-000c-4a9b-98bf-fc56db88a2ef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Convolution (Step 2):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/34bd277c-9d68-4f3f-ab2e-80a458721b2c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Convolution (Step 3):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/339ed5ff-b018-4da1-b985-78a312a791e9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Convolution (Step 4):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cc64c27d-e73c-4246-87eb-c401076e359c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Convolution (Step 5):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/53bef645-f45c-4182-8450-e4a07570cc73.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Convolution (Step 6):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f4262f45-418b-4821-bc34-3e6c91c3d5e1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Convolution (Step 7):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d84707bf-ac1a-4c60-a99d-2f0541273fe0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Convolution (Step 8):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2890e9fd-7b9c-4d82-8eb9-0ddf3c39310f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Convolution (Step 9):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02889f5c-c066-4583-9839-78843d460c6f.png)'
  prefs: []
  type: TYPE_IMG
- en: Again, you can think of this as sliding a function that slides over another
    function (the input) in two dimensions. The function that slides, performs the
    standard linear algebra transformation of multiplication followed by addition.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see this in action in an image-processing example that is undoubtedly
    very common: Instagram.'
  prefs: []
  type: TYPE_NORMAL
- en: How Instagram filters work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I am going to assume that you are familiar with Instagram. If not, I both envy
    and pity you; but here''s the gist of Instagram: it''s a photo sharing service
    that has a selling point of allowing users to apply filters to their images. The
    filters would change the color of the images, often to enhance the subject.'
  prefs: []
  type: TYPE_NORMAL
- en: How do those filters work? Convolutions!
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s define a filter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/834e573b-df92-43e3-b22c-ca53a720ce3f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To convolve, we simply slide the filter across the following diagram (it''s
    a *very* famous artwork by an artist called Piet Chew):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3306b7c3-de14-471f-947e-87e1c204875c.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Applying the preceding filter would yield something such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cfaa8048-1e5d-4d65-a5ea-603a23d245e3.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Yes, the filter blurs images!
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example written in Go to emphasize the idea:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The function is quite slow and inefficient, of course. Gorgonia itself comes
    with a much more sophisticated algorithm
  prefs: []
  type: TYPE_NORMAL
- en: Back to neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OK, so we now know that convolutions are important in the use of filters. But
    how does this relate to neural networks?
  prefs: []
  type: TYPE_NORMAL
- en: Recall that a neural network is defined as a linear transform (![](img/8868b16c-e984-46bd-ae8f-1434e9b705e7.png))
    with a non-linearity applied on it (written as ![](img/7d355b95-e418-44e3-a7a5-cad40667f0f1.png)).
    Note that *x*, the input image, is acted upon as a whole. This would be like having
    a single filter across the entire image. But what if we could process the image
    one small section at a time?
  prefs: []
  type: TYPE_NORMAL
- en: To add to that, in the preceding section, I showed how a simple filter could
    be used to blur an image. Filters could also be used to sharpen an image, picking
    out features that matter and blurring out features that don't. So, what if a machine
    could learn what filter to create?
  prefs: []
  type: TYPE_NORMAL
- en: 'That''s the reason why we would want to use a convolution in a neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutions act on small parts of the image at a time, leaving only features
    that matter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can learn the specific filters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This gives a lot of fine-tuned control to the machine. Now, instead of a rough
    feature detector that works on the whole image at once, we can build many filters,
    each specializing to a specific feature, thus allowing us to extract the features
    necessary for the classification of numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Max-pooling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we have in our minds a conceptual machine that will learn the filters that
    it needs to apply to extract features from an image. But, at the same time, we
    don't want the machine to overfit on the learning. A filter that is overly specific
    to the training data is not useful in real life. If a filter learns, for example,
    that all human faces have two eyes, a nose, and a mouth, and that's all, it wouldn't
    be able to classify a picture of a person with half their face obscured.
  prefs: []
  type: TYPE_NORMAL
- en: So, in an attempt to teach a ML algorithm to be able to generalize better, we
    simply give it less information. Max-pooling is one such process, as is *dropout*
    (see the next section).
  prefs: []
  type: TYPE_NORMAL
- en: 'How max pooling works is it partitions the input data into non-overlapping
    regions, and simply finds the maximum value of that region:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4cc49a97-54a3-4bb8-a123-fe8bc7b60321.png)'
  prefs: []
  type: TYPE_IMG
- en: There is, of course, an implicit understanding that this definitely changes
    the shape of the output. In fact, you will observe that it shrinks the image.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The result after max-pooling is minimum information within the output. But
    this may still be too much information; the machine may still overfit. Therefore,
    a very interesting quandary arises: what if some of the activations were randomly
    zeroed?'
  prefs: []
  type: TYPE_NORMAL
- en: This is the basis of dropout. It's a remarkably simple idea that improves upon
    the machine learning algorithm's ability to generalize, simply by having deleterious
    effects on information. With every iteration, random activations are zeroed. This
    forces the algorithm to only learn what is really important. How it does so involves
    structural algebra and is a story for another day.
  prefs: []
  type: TYPE_NORMAL
- en: For the purposes of this project, Gorgonia actually handles dropout by means
    of element-wise multiplication by a randomly generated matrix of 1s and 0s.
  prefs: []
  type: TYPE_NORMAL
- en: Describing a CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Having said all that, the neural network is very easy to build. First, we define
    a neural network as such:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we defined a neural network with four layers. A convnet layer is similar
    to a linear layer in many ways. It can, for example, be written as an equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/25dbe445-4aa7-4544-8e5d-c6c58ea5b98a.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that in this specific example, I consider dropout and max-pool to be part
    of the same layer. In many literatures, they are considered to be separate layers.
  prefs: []
  type: TYPE_NORMAL
- en: I personally do not see the necessity to consider them as separate layers. After
    all, everything is just a mathematical equation; composing functions comes naturally.
  prefs: []
  type: TYPE_NORMAL
- en: 'A mathematical equation on its own without structure is quite meaningless.
    Unfortunately, we do not have technology usable enough to simply define the structure
    of a data type (the hotness is in dependently-typed languages, such as Idris,
    but they are not yet at the level of usability or performance that is necessary
    for deep learning). Instead, we have to constrain our data structure by providing
    a function to define a `convnet`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We'll start with `dt`. This is essentially a global variable denoting what data
    type we would like to work in. For the purposes of this project, we can use `var
    dt = tensor.Float64`, to indicate that we would like to work with `float64` throughout
    the entire project. This allows us to immediately reuse the functions from the
    previous chapter without having to handle different data types. Note that if we
    do plan to use `float32`, the computation speed immediately doubles. In the repository
    to this chapter, you might note that the code uses `float32`.
  prefs: []
  type: TYPE_NORMAL
- en: We'll start with `d0` all the way to `d3`. This is fairly simple. For the first
    three layers, we want 20% of the activations to be randomly zeroed. But for the
    last layer, we want 55% of the activations to be randomly zeroed. In really broad
    strokes, this causes an information bottleneck, which will cause the machine to
    learn only the really important features.
  prefs: []
  type: TYPE_NORMAL
- en: Take a look at how `w0` is defined. Here, we're saying `w0` is a variable called
    `w0`. It is a tensor with the shape of (32, 1, 3, 3). This is typically called
    the **Number of Batches, Channels, Height, Width** (**NCHW**/**BCHW**) format.
    In short, what we're saying is that there are 32 filters we wish to learn, each
    filter has a height and width of (3, 3), and it has one color channel. MNIST is,
    after all, black and white.
  prefs: []
  type: TYPE_NORMAL
- en: BCHW is not the only format! Some deep learning frameworks prefer to use BHWC
    formats. The reason for preferring one format over another is purely operational.
    Some convolution algorithms work better with NCHW; some work better with BHWC.
    The ones in Gorgonia works only in BCHW.
  prefs: []
  type: TYPE_NORMAL
- en: The choice of a 3 x 3 filter is purely unprincipled but not without precedence.
    You could choose a 5 x 5 filter, or a 2 x 1 filter, or really, a filter of any
    shape. However, it has to be said that a 3 x 3 filter is probably the most universal
    filter that can work on all sorts of images. Square filters of these sorts are
    common in image-processing algorithms, so it is in accordance to such traditions
    that we chose a 3 x 3.
  prefs: []
  type: TYPE_NORMAL
- en: 'The weights for the higher layers start to look a bit more interesting. For
    example, `w1` has a shape of (64, 32, 3, 3). Why? In order to understand why,
    we need to explore the interplay between the activation functions and the shapes.
    Here''s the entire forward function of the `convnet`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: It should be noted that convolution layers do change the shape of the inputs.
    Given an (N, 1, 28, 28) input, the `Conv2d` function will return a (N, 32, 28,
    28) output, precisely because there are now 32 filters. The `MaxPool2d` will return
    an output with the shape of (N, 32, 14, 14); recall that the purpose of max-pooling
    is to reduce the amount of information in the neural network. It just happens
    that max-pooling with a shape of (2, 2) will nicely halve the length and width
    of the image (and reduce the amount of information by four times).
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of layer 0 would have a shape of (N, 32, 14, 14). If we stick to
    our explanations of our shapes from earlier, where it was in the format of (N,
    C, H, W), we would be quite stumped. What does it mean to have 32 channels? To
    answer that, let''s look at how we encode a color image in terms of BCHW:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/68a7c029-efcf-489f-8097-ad069dcf4be9.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that we encode it as three separate layers, stacked onto one another. This
    is a clue as to how to think about having 32 channels. Of course, each of the
    32 channels as the result of applying each of the 32 filters; the extracted features,
    so to speak. The result can, of course, be stacked in the same way color channels
    be stacked.
  prefs: []
  type: TYPE_NORMAL
- en: For the most part, however, the mere act of symbol pushing is all that is required
    to build a deep learning system; no real intelligence is required. This, of course
    mirrors, the Chinese Room Puzzle thought experiment, and I have quite a bit to
    say on that, though it's not really the time nor the place.
  prefs: []
  type: TYPE_NORMAL
- en: The more interesting parts is in the construction of Layer 3\. Layers 1 and
    2 are constructed very similarly to Layer 0, but Layer 3 has a slightly different
    construction. The reason is because the output of Layer 2 is a rank-4 tensor,
    but in order to perform matrix multiplication, it needs to be reshaped into a
    rank-2 tensor.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, the final layer, which decodes the output, uses a softmax activation
    function to ensure that the result we get is probability.
  prefs: []
  type: TYPE_NORMAL
- en: And really, there you have it. A CNN, written in a very neat way that does not
    obfuscate the mathematical definitions.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the convnet to learn, what is required is backpropagation, which propagates
    the errors, and a gradient descent function to update the weight matrices. To
    do this is relatively simple with Gorgonia, so simple that we can actually put
    it into our main function without impacting understandability:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'For the errors, we use a simple cross-entropy by multiplying the expected output
    element-wise and then averaging it, as shown in this snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Following that, we simply call `gorgonia.Grad(cost, m.learnables()...)`, which
    performs symbolic backpropagation. What is `m.learnables()`?, you may ask. It''s
    simply the variables that we wish the machine to learn. The definition is as such:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Again, it's fairly simple.
  prefs: []
  type: TYPE_NORMAL
- en: One additional comment I want the reader to note is `gorgonia.Read(cost, &costVal)`.
    `Read` is one of the more confusing parts of Gorgonia. But when framed correctly,
    it is quite simple to understand.
  prefs: []
  type: TYPE_NORMAL
- en: Earlier, in the section *Describing a neural network*, I likened Gorgonia to
    writing in another programming language. If so, then `Read` is the equivalent
    of `io.WriteFile`. What `gorgonia.Read(cost, &costVal)` says is that when the
    mathematical expression gets evaluated, make a copy of the result of `cost` and
    store it in `costVal`. This is necessary because of the way mathematical expressions
    are evaluated within the Gorgonia system.
  prefs: []
  type: TYPE_NORMAL
- en: Why is it called `Read` instead of `Write`? I initially modeled Gorgonia to
    be quite monadic (in the Haskell sense of monad), and as a result, one would *read
    out* a value. After a span of three years, the name sort of stuck.
  prefs: []
  type: TYPE_NORMAL
- en: Running the neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Observe that up to this point, we've merely described the computations we need
    to perform. The neural network doesn't actually run; this is simply a description
    on the neural network to run.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to be able to evaluate the mathematical expression. In order to do
    so, we need to compile the expression into a program that can be executed. Here''s
    the code to do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s not strictly necessary to call `gorgonia.Compile(g)`. This was done for
    pedagogical reasons, to showcase that the mathematical expression can indeed be
    compiled down into an assembly-like program. In production systems, I often just
    do something like this: `vm := gorgonia.NewTapeMachine(g, gorgonia.BindDualValues(m.learnables()...))`.'
  prefs: []
  type: TYPE_NORMAL
- en: There are two provided `vm` types in Gorgonia, each representing different modes
    of computation. In this project, we're merely using `NewTapeMachine` to get a
    `*gorgonia.tapeMachine`. The function to create a `vm` takes many options, and
    the `BindDualValues` option simply binds the gradients of each of the variables
    in the models to the variables themselves. This allows for cheaper gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, note that a `VM` is a resource. You should think of a `VM` as if it
    were an external CPU, a computing resource. It is good practice to close any external
    resources after we use them and, fortunately, Go has a very convenient way of
    handling cleanups: `defer vm.Close()`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we move on to talk about gradient descent, here''s what the compiled
    program looks like, in pseudo-assembly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: batches := numExamples / bs
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: log.Printf("Batches %d", batches)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: bar := pb.New(batches)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: bar.SetRefreshRate(time.Second)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: bar.SetMaxWidth(80)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: for i := 0; i < *epochs; i++ {
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: bar.Prefix(fmt.Sprintf("Epoch %d", i))
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: bar.Set(0)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: bar.Start()
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: for b := 0; b < batches; b++ {
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: start := b * bs
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: end := start + bs
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: if start >= numExamples {
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: break
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: if end > numExamples {
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: end = numExamples
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: var xVal, yVal tensor.Tensor
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: if xVal, err = inputs.Slice(sli{start, end}); err != nil {
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: log.Fatal("Unable to slice x")
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: if yVal, err = targets.Slice(sli{start, end}); err != nil {
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: log.Fatal("Unable to slice y")
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: if err = xVal.(*tensor.Dense).Reshape(bs, 1, 28, 28); err != nil {
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: log.Fatalf("Unable to reshape %v", err)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: gorgonia.Let(x, xVal)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: gorgonia.Let(y, yVal)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: if err = vm.RunAll(); err != nil {
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'log.Fatalf("Failed at epoch  %d: %v", i, err)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: solver.Step(gorgonia.NodesToValueGrads(m.learnables()))
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: vm.Reset()
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: bar.Increment()
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: log.Printf("Epoch %d | cost %v", i, costVal)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: testImgs, err := readImageFile(os.Open("t10k-images.idx3-ubyte"))
  prefs: []
  type: TYPE_NORMAL
- en: if err != nil {
  prefs: []
  type: TYPE_NORMAL
- en: log.Fatal(err)
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: testlabels, err := readLabelFile(os.Open("t10k-labels.idx1-ubyte"))
  prefs: []
  type: TYPE_NORMAL
- en: if err != nil {
  prefs: []
  type: TYPE_NORMAL
- en: log.Fatal(err)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: testData := prepareX(testImgs)
  prefs: []
  type: TYPE_NORMAL
- en: testLbl := prepareY(testlabels)
  prefs: []
  type: TYPE_NORMAL
- en: shape := testData.Shape()
  prefs: []
  type: TYPE_NORMAL
- en: visualize(testData, 10, 10, "testData.png")
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: var correct, total float32
  prefs: []
  type: TYPE_NORMAL
- en: numExamples = shape[0]
  prefs: []
  type: TYPE_NORMAL
- en: batches = numExamples / bs
  prefs: []
  type: TYPE_NORMAL
- en: for b := 0; b < batches; b++ {
  prefs: []
  type: TYPE_NORMAL
- en: start := b * bs
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: end := start + bs
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: if start >= numExamples {
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: break
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: if end > numExamples {
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: end = numExamples
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: var oneimg, onelabel tensor.Tensor
  prefs: []
  type: TYPE_NORMAL
- en: if oneimg, err = testData.Slice(sli{start, end}); err != nil {
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: log.Fatalf("Unable to slice images (%d, %d)", start, end)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: if onelabel, err = testLbl.Slice(sli{start, end}); err != nil {
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: log.Fatalf("Unable to slice labels (%d, %d)", start, end)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: if err = oneimg.(*tensor.Dense).Reshape(bs, 1, 28, 28); err != nil {
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: log.Fatalf("Unable to reshape %v", err)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: gorgonia.Let(x, oneimg)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: gorgonia.Let(y, onelabel)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: if err = vm.RunAll(); err != nil {
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: log.Fatal("Predicting (%d, %d) failed %v", start, end, err)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: label, _ := onelabel.(*tensor.Dense).Argmax(1)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: predicted, _ := m.outVal.(*tensor.Dense).Argmax(1)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: lblData := label.Data().([]int)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: for i, p := range predicted.Data().([]int) {
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: if p == lblData[i] {
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: correct++
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: total++
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'fmt.Printf("Correct/Totals: %v/%v = %1.3f\n", correct, total, correct/total)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: label, _ := onelabel.(*tensor.Dense).Argmax(1)
  prefs: []
  type: TYPE_NORMAL
- en: predicted, _ := m.outVal.(*tensor.Dense).Argmax(1)
  prefs: []
  type: TYPE_NORMAL
- en: lblData := label.Data().([]int)
  prefs: []
  type: TYPE_NORMAL
- en: for i, p := range predicted.Data().([]int) {
  prefs: []
  type: TYPE_NORMAL
- en: if p == lblData[i] {
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: correct++
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: total++
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we wrote our own `argmax` function. Gorgonia's tensor
    package actually does provide a handy method for doing just that. But in order
    to understand what is going on, we will need to first look at the results.
  prefs: []
  type: TYPE_NORMAL
- en: The shape of `m.outVal` is (N, 10), where N is the batch size. The same shape
    also shows for `onelabel`.  (N, 10) means N rows of 10 columns. What can these
    10 columns be? Well, of course they're the encoded numbers! So what we want is
    to find the maximum values amongst the column for each row. And that's the first
    dimension. Hence when a call to `.ArgMax()` is made, we specify 1 as the axis.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore the result of the `.Argmax()` calls will have a shape (N). For each
    value in that vector, if they are the same for `lblData` and `predicted`, then
    we increment the `correct` counter. This gives us a way to count accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We use accuracy because the previous chapter used accuracy. This allows us to
    have a apples-to-apples comparison. Additionally you may note that there is a
    lack of cross validation. That will be left as an exercise to the reader.
  prefs: []
  type: TYPE_NORMAL
- en: After training the neural network for two hours on a batch size of 50 and 150
    epochs, I'm pleased to say I got a 99.87% accuracy. And this isn't even state
    of the art!
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, it took just 6.5 minutes to get a 97% accuracy. That
    additional 2% accuracy required a lot more time. This is a factor in real life.
    Often business decisions are a big factor in choosing ML algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about neural networks and studied about the Gorgonia
    library in detail. Then we learned how to recognize handwritten digits using a
    CNN.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we're going to strengthen our intuition about what can
    be done with computer vision, by building a multiple facial-detection system in
    Go.
  prefs: []
  type: TYPE_NORMAL
