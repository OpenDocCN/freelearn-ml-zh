- en: Convolutional Neural Networks - MNIST Handwriting Recognition
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络 - MNIST 手写识别
- en: In the previous chapter, I posited a scenario where you are a postal worker
    trying to recognize handwriting. In that, we ended up with a neural network built
    on top of Gorgonia. In this chapter, we'll look at the same scenario, but we'll
    augment our ideas of what a neural network is and write a more advanced neural
    network, one that was, until very recently, state of the art.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我提出了一种场景，即你是一名邮递员，试图识别手写体。在那里，我们最终构建了一个基于 Gorgonia 的神经网络。在本章中，我们将探讨相同的场景，但我们将扩展我们对神经网络的理解，并编写一个更先进的神经网络，这是一个直到最近仍然是尖端技术的神经网络。
- en: Specifically, in this chapter, we are going to build a **Convolutional Neural
    Network** (**CNN**). A CNN is a type of deep learning network that has been popular
    in recent years.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，在本章中，我们将构建一个 **卷积神经网络**（**CNN**）。CNN 是一种近年来流行的深度学习网络。
- en: Everything you know about neurons is wrong
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你所知道的关于神经元的一切都是错误的
- en: In the previous chapter, I mentioned that everything you know about neural networks
    is wrong. Here, I repeat that claim. Most literature out there on a neural network
    starts with a comparison with biological neurones and ends there. This leads readers
    to often assume that it is. I'd like to make a point that artificial neural networks
    are *nothing* like their biological namesake.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我提到关于神经网络的你知道的一切都是错误的。在这里，我重申这一说法。大多数关于神经网络的文献都是从与生物神经元的比较开始的，并以此结束。这导致读者经常假设它是。我想指出，人工神经网络与它们的生物同名物没有任何相似之处。
- en: Instead, in the last chapter, I spent a significant amount of the chapter describing
    linear algebra, and explained that the twist is that you can express almost any
    **machine learning** (**ML**) problem as linear algebra. I shall continue to do
    so in this chapter.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，在上一章中，我花了很多时间描述线性代数，并解释说，转折点是你可以将几乎任何 **机器学习**（**ML**）问题表达为线性代数。我将在本章继续这样做。
- en: Rather than think of artificial neural networks as analogies of real-life neural
    networks, I personally encourage you to think of artificial neural networks as
    mathematical equations. The non-linearities introduced by the activation functions,
    combined with linear combinations allows for artificial neural networks to be
    able to approximate any function.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 与其将人工神经网络视为现实生活神经网络的类比，我个人鼓励你将人工神经网络视为数学方程式。激活函数引入的非线性性与线性组合相结合，使得人工神经网络能够近似任何函数。
- en: Neural networks – a redux
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络 - 重新审视
- en: 'The fundamental understanding that neural networks are mathematical expressions
    leads to really simple and easy implementations of neural networks. Recall from
    the previous chapter that a neural network can be written like this:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对神经网络的基本理解是它们是数学表达式，这导致了神经网络简单易行的实现。回想一下上一章，神经网络可以写成这样：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If we rewrite the code as a mathematical equation, we can write a neural network
    like this:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将代码重写为一个数学方程式，我们可以写出如下神经网络：
- en: '![](img/a67670dc-893a-4308-9b31-462f96a5a349.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a67670dc-893a-4308-9b31-462f96a5a349.png)'
- en: A side note: ![](img/86262bf8-91aa-45b1-a951-f2db5da20de6.png)is the same as ![](img/d6732aaf-6223-4477-af91-f022c81b9b92.png).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下：![图片](img/86262bf8-91aa-45b1-a951-f2db5da20de6.png)与![图片](img/d6732aaf-6223-4477-af91-f022c81b9b92.png)相同。
- en: 'We can simply write it out using Gorgonia, like this:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 Gorgonia 简单地写出它，如下所示：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The preceding code is a representation of the following neural network in images:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码是以下神经网络在图像中的表示：
- en: '![](img/60f87787-65ce-40e1-b7a6-f4037719a3c5.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/60f87787-65ce-40e1-b7a6-f4037719a3c5.png)'
- en: The middle layer consists of 800 hidden units.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 中间层由 800 个隐藏单元组成。
- en: Of course, the preceding code hides a lot of things. You can't really expect
    a neural network from scratch in fewer than 20 lines, can you? To understand what
    is happening, we need to take a brief detour into understanding what Gorgonia
    is.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，前面的代码隐藏了很多东西。你不可能在少于 20 行代码中从头开始构建一个神经网络，对吧？为了理解正在发生的事情，我们需要简要地了解一下 Gorgonia
    是什么。
- en: Gorgonia
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Gorgonia
- en: Gorgonia is a library that provides primitives for working with mathematical
    expressions specific to deep learning. When working with a ML related project,
    you will start to find yourself more introspective about the world, and questioning
    assumptions all the time. This is a good thing.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Gorgonia 是一个库，它提供了用于处理深度学习特定数学表达式的原语。当与机器学习相关的项目一起工作时，你将开始发现自己对世界的洞察力更强，并且总是质疑假设。这是好事。
- en: 'Consider what happens in your mind when you read the following mathematical
    expression:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑当你阅读以下数学表达式时，你心中的想法：
- en: '![](img/2fbb360a-4c40-41b7-987d-ac97f71891a4.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2fbb360a-4c40-41b7-987d-ac97f71891a4.png)'
- en: You should instantly think *hang on, that's false*. Why does your brain think
    this?
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该立刻想到“等等，这是错误的”。为什么你的大脑会这样想？
- en: 'That''s mainly because your brain evaluated the mathematical expression. In
    general, there are three parts to the expression: the left-hand side, the equal
    symbol, and the right-hand side. Your brain evaluated each part separately and
    then evaluated the expression as false.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这主要是因为你的大脑评估了数学表达式。一般来说，表达式有三个部分：左边、等号和右边。你的大脑分别评估每一部分，然后评估整个表达式为假。
- en: 'When we read mathematical expressions, we automatically evaluate the expressions
    in our mind that we take evaluation for granted. In Gorgonia, what we take for
    granted is made explicit. There are two general *parts* to using Gorgonia: defining
    an expression and evaluating an expression.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们阅读数学表达式时，我们会自动在心中评估这些表达式，并且我们理所当然地认为这是评估。在 Gorgonia 中，我们理所当然的事情被明确化了。使用 Gorgonia
    有两个一般的 *部分*：定义表达式和评估表达式。
- en: Since you are most probably a programmer, you can think of the first part as
    writing a program, and the second part can be thought of as running a program.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你很可能是程序员，你可以把第一部分看作是编写程序，而第二部分可以看作是运行程序。
- en: When describing a neural network in Gorgonia, it's often instructive to imagine
    yourself writing in another programming language, one that is specific to building
    neural networks. This is because the patterns used in Gorgonia are not unlike
    a new programming language. Indeed, Gorgonia was built from ground-up with the
    idea that it's a programming language without a syntactical frontend. As such,
    in this section, I will often ask you to imagine writing in another Go-like language.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当在 Gorgonia 中描述神经网络时，想象自己用另一种编程语言编写代码通常是有益的，这种语言是专门用于构建神经网络的。这是因为 Gorgonia 中使用的模式与一种新的编程语言非常相似。事实上，Gorgonia
    是从零开始构建的，其理念是它是一种没有语法前端的编程语言。因此，在本节中，我经常会要求你想象自己在另一种类似 Go 的语言中编写代码。
- en: Why?
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么？
- en: 'A good question to ask is *why?* Why bother with this separation of processes?
    After all, the preceding code could be rewritten as the previous chapter''s `Predict`
    function:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好问题是“为什么？”为什么要费心分离这个过程？毕竟，前面的代码可以被重写为上一章的 `Predict` 函数：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, we define the network in Go, and when we run the Go code, the neural network
    is run as it is being defined. What's the problem we face that we need to introduce
    the idea of separating the definition of the neural network and running it? We've
    already seen the problem when we wrote the `Train` method.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们用 Go 语言定义网络，当我们运行 Go 代码时，神经网络就像定义时一样运行。我们面临的问题是什么，需要引入将神经网络定义和运行分离的想法？我们已经看到了当我们编写
    `Train` 方法时的这个问题。
- en: 'If you recall, in the last chapter, I said that writing the `Train` method
    requires us to actually copy and paste code from the `Predict` method. To refresh
    your memory, here''s the `Train` method:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得，在上一个章节中，我说过编写 `Train` 方法需要我们实际上从 `Predict` 方法中复制和粘贴代码。为了刷新你的记忆，以下是 `Train`
    方法：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Let's go through an exercise of refactoring to highlight the problem. Taking
    off our ML hat for a bit, and putting on our software engineer hat, let's see
    how we can refactor `Train` and `Predict`, even if conceptually. We see in the
    `Train` method that we need access to `act0` and `pred` in order to backpropagate
    the errors. Where in `Predict` `act0` and `pred` are terminal values (that is,
    we don't use them after the function has returned), in `Train`, they are not.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过重构练习来突出问题。暂时摘下我们的机器学习帽子，戴上软件工程师的帽子，看看我们如何重构 `Train` 和 `Predict`，即使是在概念上。我们在
    `Train` 方法中看到，我们需要访问 `act0` 和 `pred` 来反向传播错误。在 `Predict` 中，`act0` 和 `pred` 是终端值（也就是说，函数返回后我们不再使用它们），而在
    `Train` 中则不是。
- en: 'So, here, we can create a new method; let''s call it `fwd`:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，在这里，我们可以创建一个新的方法；让我们称它为 `fwd`：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'And we can refactor `Predict` to look like this:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将 `Predict` 重构为如下所示：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'And the `Train` method would look like this:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`Train` 方法将看起来像这样：'
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This looks better. What exactly are we doing here? We are programming. We are
    rearranging one form of syntax into another form of syntax but we are not changing
    the semantics, the meaning of the program. The refactored program has exactly
    the same meaning as the pre-refactored program.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Programming
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Wait a minute, you might say to yourself. What do I mean by *the meaning of
    the program?* This is a surprisingly deep topic that involves a whole branch of
    mathematics known as **homotopy**. But for all practical intents and purposes
    of this chapter, let's define the *meaning* of a program to be the extensional
    definition of the program. If two programs compile and run, take the exact same
    inputs, and return the same exact output every time, we say two programs are equal.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'These two programs would be equal:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '| **Program A** | **Program B** |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
- en: '| `fmt.Println("Hello World")` | `fmt.Printf("Hello " + "World\n")` |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
- en: 'Intentionally, if we visualize the programs as an **Abstract Syntax Tree**
    (**AST**), they look slightly different:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec6b6303-8a9f-448e-9cae-10d9465a6c30.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: '![](img/80b016f8-e352-4740-8f0d-c65c83060bea.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: The syntax for both programs are different, but they are semantically the same.
    We can refactor program B into program A, by eliminating the `+`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'But note what we did here: we took a program and represented it as an AST.
    Through syntax, we manipulated the AST. This is the essence of programming.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: What is a tensor? – part 2
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, there was an info box that introduced the concept
    of a tensor. That info box was a little simplified. If you google what a tensor
    is, you will get very conflicting results, which only serve to confuse. I don''t
    want to add to the confusion. Instead, I shall only briefly touch on tensors in
    a way that will be relevant to our project, and in a way very much like how a
    typical textbook on Euclidean geometry introduces the concept of a point: by holding
    it to be self-evident from use cases.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'Likewise, we will hold tensors to be self-evident from use. First, we will
    look at the concept of multiplication:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s define a vector: ![](img/a86a3ad1-6ff3-4633-b115-91b671301d9f.png).
    You can think of it as this diagram:'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/1d1d9e18-693e-42c1-9b0e-12a6aff3c24c.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: 'Next, let''s multiply the vector by a scalar value: ![](img/90b2efe0-f4c4-441f-9276-a22d5023c6bc.png).
    The result is something like this:'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/6b4e7f1c-38da-44ca-a8f5-cc1d5e185ccc.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
- en: 'There are two observations:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: The general direction of the arrow doesn't change.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only the length changes. In physics terms, this is called the magnitude. If
    the vector represents the distance travelled, you would have traveled twice the
    distance along the same direction.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, how would you change directions by using multiplications alone? What do
    you have to multiply to change directions? Let''s try the following matrix, which
    we will call *T*, for transformation:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5262facb-1ac0-4216-b920-b4e20e163d78.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
- en: 'Now if we multiply the transformation matrix with the vector, we get the following:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/93630693-f819-481c-b93d-4fa5b9ffca44.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: 'And if we plot out the starting vector and the ending vector, we get the resultant
    output:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b70d5c40-fdf6-479a-9405-1f361beef36e.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: As we can see, the direction has changed. The magnitude too has changed.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Now, you might be saying, *hang on, isn't this just Linear Algebra 101?*. Yes,
    it is. But to really understand a tensor, we must learn how to construct one.
    The matrix that we just used is also a tensor of rank-2\. The proper name for
    a tensor of rank-2 is a **dyad**.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Why the mixing of naming conventions? Here's a bit of fun trivia. When I was
    writing the earliest versions of Gorgonia, I was musing about the terrible naming
    conventions computer science has had, a fact that Bjarne Stroustrup himself lamented.
    The canonical name for a rank-2 tensor is called a **dyad**, but can be represented
    as a matrix. I was struggling to properly call it; after all, there are power
    in names and to name it is to tame it.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: At around the same time as I was developing the earliest versions of Gorgonia,
    I was following a most excellent BBC TV series called **Orphan Black**, in which
    the Dyad Institute is the primary foe of the protagonists. They were quite villainous
    and that clearly left an impact in my mind. I decided against naming it thus.
    In retrospect, this seemed like a rather silly decision.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s consider the transformation dyad. You can think of the dyad as a
    vector *u* times a vector *v*. To write it out in equation form:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d7f28330-0c81-4a14-bff1-67c135e3574d.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
- en: 'At this point, you may be familiar with the previous chapter''s notion of linear
    algebra. You might think to yourself: if two vectors multiply, that''d end up
    with a scalar value, no? If so, how would you multiply two vectors and get a matrix
    out of it?'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we''d need to introduce a new type of multiplication: the outer product
    (and by contrast, the multiplication introduced in the previous chapter is an
    inner product). We write outer products with this symbol: ![](img/0fb20035-e86b-4189-9050-11b52ee9f24d.png).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically speaking, the outer product, also known as a dyad product, is
    defined as such:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e6158f0e-afbe-4214-a383-6a9f8d1a8d38.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
- en: We won't be particularly interested in the specifics of *u* and *v* in this
    chapter. However, being able to construct a dyad from its constituent vectors
    is an integral part of what a tensor is all about.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we can replace *T* with *uv*:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/862fd95c-5ca3-470f-a68d-7ae0fa0690ca.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
- en: Now we get ![](img/c1e433e0-9b4e-475b-bd90-2d3d079cd553.png) as the scalar magnitude
    change and *u* as the directional change.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: So what is the big fuss with tensors? I can give two reasons.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, the idea that dyads can be formed from vectors generalizes upward.
    A three-tensor, or triad can be formed by a dyad product *uvw*, a four-tensor
    or a tetrad can be formed by a dyad product *uvwx*, and so on and so forth. This
    affords us a mental shortcut that will be very useful to us when we see shapes
    that are associated with tensors.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'The useful mental model of what a tensor can be thought as is the following:
    a vector is like a list of things, a dyad is like a list of vectors, a triad is
    like a list of dyads, and so on and so forth. This is absolutely helpful when
    thinking of images, like those that we''ve seen in the previous chapter:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: An image can be seen as a (28, 28) matrix. A list of ten images would have the
    shape (10, 28, 28). If we wanted to arrange the images in such a way that it's
    a list of lists of ten images, it'd have a shape of (10, 10, 28, 28).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'All this comes with a caveat of course: a tensor can only be defined in the
    presence of transformation. As a physics professor once told me: *that which transforms
    like a tensor is a tensor*. A tensor devoid of any transformation is just an *n*-dimensional
    array of data. The data must transform, or flow from tensor to tensor in an equation.
    In this regards, I think that TensorFlow is a ridiculously well-named product.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: For more information on tensors, I would recommend the relatively dense text
    book, *Linear Algebra and Geometry* by Kostrikin (I failed to finish this book,
    but it was this book that gave me what I believe to be a strong-ish understanding
    of tensors). More on the flow of tensors can be found in Spivak's *Manifold Calculus*.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: All expressions are graphs
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we can finally return to the preceding example.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'Our problem, if you recall, is that we had to specify the neural network twice:
    once for prediction and once for learning purposes. We then refactored the program
    so that we don''t have to specify the network twice. Additionally, we had to manually
    write out the expression for the backpropagation. This is error prone, especially
    when dealing with larger neural networks like the one we''re about to build in
    this chapter. Is there a better way? The answer is yes.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Once we understand and fully internalize that neural networks are essentially
    mathematical expressions, we can take the learning's from tensors, and model a
    neural network where the entire neural network is a flow of tensors.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Recall that tensors can only be defined in the presence of transformation; then,
    any operation that transforms tensor(s), used in concert with data structures
    that hold data are tensors. Also, recall that computer programs can be represented
    as abstract syntax trees. Mathematical expressions can be represented as a program.
    Therefore, mathematical expressions can also be represented as an abstract syntax
    tree.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: More accurate, however, is that mathematical expressions can be expressed as
    a graph; a directed acyclic graph, to be specific. We call this the **expression
    graph**.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'This distinction matters. Trees cannot share nodes. Graphs can. Let''s consider,
    for example, the following mathematical expression:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这种区别很重要。树不能共享节点。图可以。让我们考虑以下数学表达式：
- en: '![](img/773f21bd-f1bd-4f85-bed9-d3b4351558ad.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/773f21bd-f1bd-4f85-bed9-d3b4351558ad.png)'
- en: 'Here are the representations as a graph and as a tree:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是图和树的表示：
- en: '![](img/042d0364-e315-454a-8a18-85ca98bcb637.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/042d0364-e315-454a-8a18-85ca98bcb637.png)'
- en: On the left, we have a directed acyclic graph, and on the right, we have a tree.
    Note that in the tree variant of the mathematical equation, there are repeat nodes.
    Both are rooted at ![](img/bc041d3c-1879-480d-b2c1-ee4e3e65713e.png). The arrow
    should be read as *depends on*. ![](img/1ffd83f8-66fd-437d-ab43-bc22d9502e7e.png)
    depends on two other nodes, ![](img/0d90444f-20a9-498d-8d65-11d5c6b644fe.png)
    and ![](img/d4029dca-e311-4e99-ad95-9648fbdf599d.png), and so on and so forth.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在左边，我们有一个有向无环图，在右边，我们有一个树。请注意，在数学方程的树变体中，有重复的节点。两者都以![图片](img/bc041d3c-1879-480d-b2c1-ee4e3e65713e.png)为根。箭头应该读作*依赖于*。![图片](img/1ffd83f8-66fd-437d-ab43-bc22d9502e7e.png)依赖于两个其他节点，![图片](img/0d90444f-20a9-498d-8d65-11d5c6b644fe.png)和![图片](img/d4029dca-e311-4e99-ad95-9648fbdf599d.png)，等等。
- en: Both the graph and tree are valid representations of the same mathematical equation,
    of course.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图和树都是同一数学方程的有效表示，当然。
- en: Why bother representing a mathematical expression as a graph or a tree? Recall
    that an abstract syntax tree represents a computation. If a mathematical expression,
    represented as a graph or a tree, has a shared notion of computation, then it
    also represents an abstract syntax tree.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要把数学表达式表示为图或树呢？回想一下，抽象语法树表示一个计算。如果一个数学表达式，以图或树的形式表示，具有共享的计算概念，那么它也代表了一个抽象语法树。
- en: Indeed, we can take each node in the graph or tree, and perform a computation
    on it. If each node is a representation of a computation, then logic holds that
    fewer nodes means faster computations (and less memory usage). Therefore, we should
    prefer to use the directed acyclic graph representation.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 的确，我们可以对图或树中的每个节点进行计算。如果每个节点是计算的表示，那么逻辑上就越少的节点意味着计算越快（以及更少的内存使用）。因此，我们应该更喜欢使用有向无环图表示。
- en: 'And now we come to the major benefit of representing a mathematical expression
    as a graph: we get differentiation for free.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来到了将数学表达式表示为图的主要好处：我们能够免费获得微分。
- en: If you recall from the previous chapter, backpropagation is essentially differentiating
    the cost with regards to the inputs. The gradients, once calculated, can then
    be used to update the values of the weights themselves. Having a graph structure,
    we wouldn't have to write the backpropagation parts. Instead, if we have a virtual
    machine that executes the graph, starting at the leaves and moving toward the
    root, the virtual machine can automatically perform differentiation on the values
    as it traverses the graph from leaf to root.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您从上一章回忆起来，反向传播本质上是对输入的成本进行微分。一旦计算了梯度，就可以用来更新权重的值。有了图结构，我们就不必编写反向传播的部分。相反，如果我们有一个执行图的虚拟机，从叶子节点开始，向根节点移动，虚拟机可以自动在遍历图从叶子到根的过程中对值进行微分。
- en: Alternatively, if we don't want to do automatic differentiation, we can also
    perform symbolic differentiation by manipulating the graph in the same way that
    we manipulated the AST in the *What is programming* section, by adding and coalescing
    nodes.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果我们不想进行自动微分，我们也可以通过操纵图来执行符号微分，就像我们在“什么是编程”部分中操纵AST一样，通过添加和合并节点。
- en: 'In this way, we can now shift our view of a neural network to this:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式，我们现在可以将我们对神经网络的看法转移到这个：
- en: '![](img/145f53a8-6df1-467b-9981-1b1ec6976d5f.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/145f53a8-6df1-467b-9981-1b1ec6976d5f.png)'
- en: Describing a neural network
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 描述神经网络
- en: 'Now let''s get back to the task of writing a neural network and thinking of
    it in terms of a mathematical expression expressed as a graph. Recall that the
    code looks something like this:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们回到编写神经网络的任务，并以图表示的数学表达式来思考它。回想一下，代码看起来像这样：
- en: '[PRE7]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now let's go through this code.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来分析这段代码。
- en: First, we create a new expression graph with `g := G.NewGraph()`. An expression
    graph is a holder object to hold the mathematical expression. Why would we want
    an expression graph? The mathematical expression that represents a neural network
    is contained in the `*gorgonia.ExpressionGraph` object.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用`g := G.NewGraph()`创建一个新的表达式图。表达式图是一个持有数学表达式的对象。我们为什么想要一个表达式图呢？表示神经网络的数学表达式包含在`*gorgonia.ExpressionGraph`对象中。
- en: Mathematical expressions are only interesting if we use variables. ![](img/65a9f649-813c-4549-bf9e-216a9ab70c7f.png)
    is quite an uninteresting expression because you can't do much with this expression.
    The only thing you can do with it is to evaluate the expression and see if it
    returns true or false. ![](img/dfaa4a00-b8a2-4fea-98fe-e69fc78d969c.png) is slightly
    more interesting. But, then again, *a* can only be *1*.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 数学表达式只有在我们使用变量时才有意思。![](img/65a9f649-813c-4549-bf9e-216a9ab70c7f.png)是一个非常无趣的表达式，因为你无法用这个表达式做很多事情。你可以做的唯一事情是评估这个表达式，看看它返回的是真还是假。![](img/dfaa4a00-b8a2-4fea-98fe-e69fc78d969c.png)稍微有趣一些。但再次强调，*a*只能为*1*。
- en: Consider, however, the expression ![](img/f4cd11b6-41d0-4a24-bbd3-657288d1aa15.png).
    With two variables, it suddenly becomes a lot more interesting. The values that
    *a* and *b* can take are dependent on one another, and there is a whole range
    of possible pairs of numbers that can fit into *a *and *b*.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，考虑一下这个表达式！[](img/f4cd11b6-41d0-4a24-bbd3-657288d1aa15.png)。有了两个变量，它突然变得更有趣。*a*和*b*可以取的值相互依赖，并且存在一系列可能的数字对可以适合到*a*和*b*中。
- en: 'Recall that each layer of neural network is just a mathematical expression
    that reads like this: ![](img/3f3d99df-f160-4131-b517-a183b5f40033.png). In this
    case, *w*, *x*, and *b* are variables. So, we create them. Note that in this case,
    Gorgonia treats variables as a programming language does: you have to tell the
    system what the variable represents.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，神经网络中的每一层仅仅是一个类似这样的数学表达式：![](img/3f3d99df-f160-4131-b517-a183b5f40033.png)。在这种情况下，*w*、*x*和*b*是变量。因此，我们创建了它们。请注意，在这种情况下，Gorgonia将变量处理得就像编程语言一样：你必须告诉系统变量代表什么。
- en: In Go, you would do that by typing `var x Foo`, which tells the Go compiler
    that `x` should be a type `Foo`. In Gorgonia, the mathematical variables are declared
    by using `NewMatrix`, `NewVector`, `NewScalar`, and `NewTensor`. `x := G.NewMatrix(g,
    Float, G.WithName, G.WithShape(N, 728))` simply says `x` is a matrix in expression
    graph `g` with a name `x`, and has a shape of `(N, 728)`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在Go语言中，你会通过输入`var x Foo`来完成这个操作，这告诉Go编译器`x`应该是一个类型`Foo`。在Gorgonia中，数学变量通过使用`NewMatrix`、`NewVector`、`NewScalar`和`NewTensor`来声明。`x
    := G.NewMatrix(g, Float, G.WithName, G.WithShape(N, 728))`简单地说，`x`是表达式图`g`中的一个名为`x`的矩阵，其形状为`(N,
    728)`。
- en: Here, readers may observe that `728` is a familiar number. In fact, what this
    tells us is that `x` represents the input, which is `N` images. `x`, therefore,
    is a matrix of *N* rows, where each row represents a single image (728 floating
    points).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，读者可能会注意到`728`是一个熟悉的数字。实际上，这告诉我们`x`代表输入，即`N`张图像。因此，`x`是一个包含*N*行的矩阵，其中每一行代表一张单独的图像（728个浮点数）。
- en: 'The eagle-eyed reader would note that `w` and `b` have extra options, where
    the declaration of `x` does not. You see, `NewMatrix` simply declares the variable
    in the expression graph. There is no value associated with it. This allows for
    flexibility when the value is attached to a variable. However, with regards to
    the weight matrix, we want to start the equation with some initial values. `G.WithInit(G.Uniform(1.0))`
    is a construction option that populates the weight matrix with values pulled from
    a uniform distribution with a gain of `1.0`. If you imagine yourself coding in
    another language specific to building neural networks, it''d look something like
    this: `var w Matrix(728, 800) = Uniform(1.0)`.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 留意细节的读者会注意到`w`和`b`有额外的选项，而`x`的声明没有。你看，`NewMatrix`只是声明了表达式图中的变量。它没有与之关联的值。这允许在值附加到变量时具有灵活性。然而，关于权重矩阵，我们希望方程从一些初始值开始。`G.WithInit(G.Uniform(1.0))`是一个构造选项，它使用具有增益`1.0`的均匀分布的值填充权重矩阵。如果你想象自己在另一种专门用于构建神经网络的编程语言中编码，它看起来可能像这样：`var
    w Matrix(728, 800) = Uniform(1.0)`。
- en: Following that, we simply write out the mathematical equation: ![](img/4cdbb766-d42f-4356-bf64-0644b8de86ab.png)
    is simply a matrix multiplication between ![](img/4979cf68-4f3a-4b41-9ec1-5c4ecd4d150d.png)
    and ![](img/641b17c8-e982-4a2d-a7e1-9affa6afae46.png); hence, `xw, _ := G.Mul(x,
    w)`. At this point, it should be clarified that we are merely describing the computation
    that is supposed to happen. It is yet to happen. In this way, it is not dissimilar
    to writing a program; writing code does not equal running the program.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '`G.Mul` and most operations in Gorgonia actually returns an error. For the
    purposes of this demonstration, we''re ignoring any errors that may arise from
    symbolically multiplying `x` and `w`. What could possibly go wrong with simple
    multiplication? Well, we''re dealing with matrix multiplication, so the shapes
    must have matching inner dimensions. A (N, 728) matrix can only be multiplied
    by a (728, M) matrix, which leads to an (N, M) matrix. If the second matrix does
    not have 728 rows, then an error will happen. So, in real production code, error
    handling is a** must**.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'Speaking of *must*, Gorgonia comes with a utility function, called, **G.Must**.
    Taking a cue from the `text/template` and `html/template` libraries found in the
    standard library, the `G.Must` function panics when an error occur. To use, simply
    write this: `xw := G.Must(G.Mul(x,w))`.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: After the inputs are multiplied with the weights, we add to the biases using
    `G.Add(xw, b)`. Again, errors may occur, but in this example, we're eliding the
    checks of errors.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we take the result and perform a non-linearity: a sigmoid function,
    with `G.Sigmoid(xwb)`. This layer is now complete. Its shape, if you follow, would
    be (N, 800).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: The completed layer is then used as an input for the following layer. The next
    layer has a similar layout as the first layer, except instead of a sigmoid non-linearity,
    a `G.SoftMax` is used. This ensures that each row in the resulting matrix sums
    1.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: One-hot vector
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Perhaps, not so coincidentally, the last layer has the shape of (N, 10). N
    is the number of input images (which we''ve gotten from `x`) ; that''s fairly
    self-explanatory. It also means that there is a clean mapping from input to output.
    What''s not self-explanatory is the 10\. Why 10? Simply put, there are 10 possible
    numbers we want to predict - 0, 1, 2, 3, 4, 5, 6, 7, 8, 9:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c8df17fe-0447-45c6-9949-e9ceb0e44c7b.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram is an example result matrix. Recall that we used `G.SoftMax`
    to ensure that each row sums up to 1\. Therefore, we can interpret the numbers
    in each column of each row to be the probability that it is the specific digit
    that we're predicting. To find the digit we're predicting, simply find the highest
    probability in each column.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, I introduced the concept of one-hot vector encoding.
    To recap, it takes a slice of labels and returns a matrix.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3a72b6c1-df67-44cf-a666-3c493e8dc6fc.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
- en: 'Now, this is clearly a matter of encoding. Who''s to say that column 0 would
    have to represent 0? We could of course come up with a completely crazy encoding
    like such and the neural network would still work:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f68f2ab0-abf5-4b28-8ff1-3fd91db7c3a7.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
- en: Of course, we would not be using such a scheme for encoding; it would be a massive
    source of programmer error. Instead, we would go for the standard encoding of
    a one-hot vector.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: I hope this has given you a taste of how powerful the notion of an expression
    graph can be. One thing we haven't touched upon yet is the execution of the graph.
    How do you run a graph? We'll look further into that in the next section.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: The project
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With all that done, it's time to get on to the project! Once again, we are going
    to recognize handwritten digits. But this time around, we're going to build a
    CNN for that. Instead of just using the `tensor` package of Gorgonia, this time
    we're going to use all of Gorgonia.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Once again, to install Gorgonia, simply run `go get -u gorgonia.org/gorgonia`
    and `go get -u gorgonia.org/tensor`.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Getting the data
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The data is the same data as in the previous chapter: the MNIST dataset. It
    can be found in the repository for this chapter, and we''ll be using a function
    we wrote in the previous chapter to acquire the data:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Other things from the previous chapter
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Obviously, there is a lot from the previous chapter that we can reuse:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: The range normalization function (`pixelWeight`) and its isometric counterpart
    (`reversePixelWeight`)
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prepareX` and `prepareY`'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `visualize` function
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For convenience sake, here they are again:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: CNNs
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What we will be building is a CNN. So, what is a Convolutional Neural Network?
    As its name suggests, it's a neural network, not unlike the one we have built
    in the previous chapter. So, clearly, there are elements that are similar. There
    are also elements that are not similar, for if they were similar, we wouldn't
    have this chapter.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: What are convolutions?
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main difference between the neural network we built in the previous chapter
    and a CNN is the convolutional layer. Recall that the neural network was able
    to learn features related to digits. In order to be more accurate, the neural
    network layers need to learn more specific features. One way to do this is to
    add more layers; more layers would lead to more features being learned, giving
    rise to deep learning.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'On a spring evening of 1877, people dressed in what modern-day people would
    consider as *black-tie* gathered at the Royal Institute, in London. The speaker
    for the evening was Francis Galton, the same Galton we met in [Chapter 1](3d68e167-a44d-4195-a270-f8180ff8f85f.xhtml),
    *How to Solve All Machine Learning Problems*. In his talk, Galton brought out
    a curious device, which he called a **quincunx**. It was a vertical wooden board
    with wooden pegs sticking out of it, arranged in a uniform, but interleaved manner.
    The front of it was covered with glass and there was an opening at the top. Tiny
    balls are then dropped from the top and as they hit the pegs, bounce left or right,
    and fall to the corresponding chutes. This continues until the balls collect at
    the bottom:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/254eef09-5de0-42b6-b2a8-076a13d440e4.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
- en: A curious shape begins to form. It's the shape modern statisticians have come
    to recognize as the binomial distribution. Most statistical textbooks end the
    story about here. The quincunx, now known as the Galton Board, illustrates, very
    clearly and firmly, the idea of the central limit theorem.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'Our story, of course, doesn''t end there. Recall in [Chapter 1](3d68e167-a44d-4195-a270-f8180ff8f85f.xhtml), *How
    to Solve All Machine Learning Problems*, that I mentioned that Galton was very
    much interested in hereditary issues. A few years earlier, Galton had published
    a book called *Hereditary Genius*. He had collected data on *eminent* persons
    in Great Britain across the preceding centuries, and much to his dismay, he found
    that *eminent* parentage tended to lead to un-eminent children. He called this
    a **reversion to the mediocre**:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a38fc9b-c6ce-4fb6-bd87-0bea52d33083.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
- en: '![](img/3e483077-8c75-442f-9367-efcde7a3003f.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
- en: And, yet, he reasoned, the mathematics doesn't show such things! He explained
    this by showing off a quincunx with two layers. A two-layered quincunx was a stand-in
    for the generational effect. The top layer would essentially be the distribution
    of a feature (say, height). Upon dropping to the second layer, the beads would
    cause the distribution to *flatten out*, which is not what he had observed. Instead,
    he surmised that there has to be another factor which causes the regression to
    the mean. To illustrate his idea, he installed chutes as the controlling factor,
    which causes a regression to the mean. A mere 40 years later, the rediscovery
    of Mendel's pea experiments would reveal genetics to be the factor. That is a
    story for another day.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'What we''re interested in is why the distribution would *flatten out*. While
    the standard *it''s physics!* would suffice as an answer, there remains interesting
    questions that we could ask. Let''s look at a simplified depiction:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5458700e-52d3-41e2-8859-61799bc48284.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
- en: 'Here, we evaluate the probability that the ball will drop and hit a position.
    The curve indicates the probability of the ball landing at position B. Now, we
    add a second layer:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56a539a1-3b5f-4ffa-b039-02562e06a206.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
- en: Say, from the previous layer, the ball landed at position 2\. Now, what is the
    probability that the ball's final resting place is at position D?
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate this, we need to know all the possible ways that the ball can
    end up at position D. Limiting our option to A to D only, here they are:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '| **Level 1 Position** | **L1 Horizontal Distance** | **Level 2 position**
    | **L2 Horizontal Distance** |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
- en: '| A | 0 | D | 3 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
- en: '| B | 1 | D | 2 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
- en: '| C | 2 | D | 1 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
- en: '| D | 3 | D | 0 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
- en: Now we can ask the question in terms of probability. The horizontal distances
    in the table are an encoding that allows us to ask the question probabilistically
    and generically. The probability of the ball travelling horizontally by one unit
    can be represented as *P(1)*, the probability of the ball travelling horizontally
    by two units can be represented as *P(2)*, and so on.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'And to calculate the probability that the ball ends up in D after two levels
    is essentially summing up all the probabilities:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e0fe1ff2-0715-4167-926b-587d0954a6cc.png).'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'We can write it as such:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5da38259-9ba6-49f3-ac2c-8680c3f24f75.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
- en: We can read it as the probability of the final distance being *$c = a+b$* is
    the sum of *$P_1(a)$*, with the probability of level 1, where the ball traveled
    horizontally by *$a$* and *$P_2(b)$*, with the probability of level 2, where the
    ball traveled horizontally by *$b$*.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'And this is the typical definition of convolution:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/64c598e7-27fd-40c9-80d3-5df72c34d70c.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
- en: 'If the integral scares you, we can equivalently rewrite this as a summation
    operation (this is only valid because we are considering discrete values; for
    continuous real values, integrations have to be used):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/becbdaea-2d47-4a0a-bdff-452aca041ced.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: 'Now, if you squint very carefully, this equation looks a lot like the preceding
    probability equation. Instead of ![](img/01f5c5c4-2cf9-4e7f-be8b-3a2230927b42.png),
    we can rewrite it as ![](img/5b3ffbce-5e13-4fd8-91dd-29ac6bc959c7.png):'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/154d9ba7-91db-4d97-a194-846644eacbbf.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
- en: And what are probabilities, but functions? There is, after all, a reason we
    write probabilities in the format $P(a)$. We can indeed genericize the probability
    equation to the convolution definition.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'However, for now, let''s strengthen our intuitions about what convolutions
    are. For that, we''ll keep the notion that the function we''re talking about has
    probabilities. First, we should note that the probability of the ball ending up
    in a particular location is dependent on where it starts. But imagine if the platform
    for the second platform moves horizontally:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a9663b8d-da9a-4422-8960-c544f9d25494.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: Now the probability of the final resting place of the ball is highly dependent
    on where the initial starting position is, as well as where the second layer's
    starting position is. The ball may not even land on the bottom!
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'So, here''s a good mental shortcut of thinking about convolutions: t''s as
    if one function in one layer is *sliding* across a second function.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: So, convolutions are what cause the *flattening* of Galton's quincunx. In essence,
    it is a function that slides on top of the probability function, flattening it
    out as it moves along the horizontal dimension. This is a one-dimensional convolution;
    the ball only travels along one dimension.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'A two-dimensional convolution is similar to a one-dimensional convolution.
    Instead, there are two *distances* or metrics that we''re considering for each
    layer:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6178477d-0195-4954-8b6e-053bd86510d4.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
- en: 'But this equation is nigh impenetrable. Instead, here''s a convenient series
    of pictures of how it works, step by step:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'Convolution (Step 1):'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/158dfe31-000c-4a9b-98bf-fc56db88a2ef.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
- en: 'Convolution (Step 2):'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/34bd277c-9d68-4f3f-ab2e-80a458721b2c.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
- en: 'Convolution (Step 3):'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/339ed5ff-b018-4da1-b985-78a312a791e9.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
- en: 'Convolution (Step 4):'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cc64c27d-e73c-4246-87eb-c401076e359c.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: 'Convolution (Step 5):'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/53bef645-f45c-4182-8450-e4a07570cc73.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
- en: 'Convolution (Step 6):'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f4262f45-418b-4821-bc34-3e6c91c3d5e1.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
- en: 'Convolution (Step 7):'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d84707bf-ac1a-4c60-a99d-2f0541273fe0.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: 'Convolution (Step 8):'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2890e9fd-7b9c-4d82-8eb9-0ddf3c39310f.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
- en: 'Convolution (Step 9):'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02889f5c-c066-4583-9839-78843d460c6f.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
- en: Again, you can think of this as sliding a function that slides over another
    function (the input) in two dimensions. The function that slides, performs the
    standard linear algebra transformation of multiplication followed by addition.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see this in action in an image-processing example that is undoubtedly
    very common: Instagram.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: How Instagram filters work
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I am going to assume that you are familiar with Instagram. If not, I both envy
    and pity you; but here''s the gist of Instagram: it''s a photo sharing service
    that has a selling point of allowing users to apply filters to their images. The
    filters would change the color of the images, often to enhance the subject.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: How do those filters work? Convolutions!
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s define a filter:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/834e573b-df92-43e3-b22c-ca53a720ce3f.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
- en: 'To convolve, we simply slide the filter across the following diagram (it''s
    a *very* famous artwork by an artist called Piet Chew):'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3306b7c3-de14-471f-947e-87e1c204875c.jpeg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
- en: 'Applying the preceding filter would yield something such as the following:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cfaa8048-1e5d-4d65-a5ea-603a23d245e3.jpeg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
- en: Yes, the filter blurs images!
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example written in Go to emphasize the idea:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The function is quite slow and inefficient, of course. Gorgonia itself comes
    with a much more sophisticated algorithm
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Back to neural networks
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OK, so we now know that convolutions are important in the use of filters. But
    how does this relate to neural networks?
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Recall that a neural network is defined as a linear transform (![](img/8868b16c-e984-46bd-ae8f-1434e9b705e7.png))
    with a non-linearity applied on it (written as ![](img/7d355b95-e418-44e3-a7a5-cad40667f0f1.png)).
    Note that *x*, the input image, is acted upon as a whole. This would be like having
    a single filter across the entire image. But what if we could process the image
    one small section at a time?
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: To add to that, in the preceding section, I showed how a simple filter could
    be used to blur an image. Filters could also be used to sharpen an image, picking
    out features that matter and blurring out features that don't. So, what if a machine
    could learn what filter to create?
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'That''s the reason why we would want to use a convolution in a neural network:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Convolutions act on small parts of the image at a time, leaving only features
    that matter
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can learn the specific filters
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This gives a lot of fine-tuned control to the machine. Now, instead of a rough
    feature detector that works on the whole image at once, we can build many filters,
    each specializing to a specific feature, thus allowing us to extract the features
    necessary for the classification of numbers.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Max-pooling
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we have in our minds a conceptual machine that will learn the filters that
    it needs to apply to extract features from an image. But, at the same time, we
    don't want the machine to overfit on the learning. A filter that is overly specific
    to the training data is not useful in real life. If a filter learns, for example,
    that all human faces have two eyes, a nose, and a mouth, and that's all, it wouldn't
    be able to classify a picture of a person with half their face obscured.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: So, in an attempt to teach a ML algorithm to be able to generalize better, we
    simply give it less information. Max-pooling is one such process, as is *dropout*
    (see the next section).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'How max pooling works is it partitions the input data into non-overlapping
    regions, and simply finds the maximum value of that region:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4cc49a97-54a3-4bb8-a123-fe8bc7b60321.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
- en: There is, of course, an implicit understanding that this definitely changes
    the shape of the output. In fact, you will observe that it shrinks the image.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Dropout
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The result after max-pooling is minimum information within the output. But
    this may still be too much information; the machine may still overfit. Therefore,
    a very interesting quandary arises: what if some of the activations were randomly
    zeroed?'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: This is the basis of dropout. It's a remarkably simple idea that improves upon
    the machine learning algorithm's ability to generalize, simply by having deleterious
    effects on information. With every iteration, random activations are zeroed. This
    forces the algorithm to only learn what is really important. How it does so involves
    structural algebra and is a story for another day.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: For the purposes of this project, Gorgonia actually handles dropout by means
    of element-wise multiplication by a randomly generated matrix of 1s and 0s.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Describing a CNN
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Having said all that, the neural network is very easy to build. First, we define
    a neural network as such:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here, we defined a neural network with four layers. A convnet layer is similar
    to a linear layer in many ways. It can, for example, be written as an equation:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/25dbe445-4aa7-4544-8e5d-c6c58ea5b98a.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
- en: Note that in this specific example, I consider dropout and max-pool to be part
    of the same layer. In many literatures, they are considered to be separate layers.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: I personally do not see the necessity to consider them as separate layers. After
    all, everything is just a mathematical equation; composing functions comes naturally.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'A mathematical equation on its own without structure is quite meaningless.
    Unfortunately, we do not have technology usable enough to simply define the structure
    of a data type (the hotness is in dependently-typed languages, such as Idris,
    but they are not yet at the level of usability or performance that is necessary
    for deep learning). Instead, we have to constrain our data structure by providing
    a function to define a `convnet`:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We'll start with `dt`. This is essentially a global variable denoting what data
    type we would like to work in. For the purposes of this project, we can use `var
    dt = tensor.Float64`, to indicate that we would like to work with `float64` throughout
    the entire project. This allows us to immediately reuse the functions from the
    previous chapter without having to handle different data types. Note that if we
    do plan to use `float32`, the computation speed immediately doubles. In the repository
    to this chapter, you might note that the code uses `float32`.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: We'll start with `d0` all the way to `d3`. This is fairly simple. For the first
    three layers, we want 20% of the activations to be randomly zeroed. But for the
    last layer, we want 55% of the activations to be randomly zeroed. In really broad
    strokes, this causes an information bottleneck, which will cause the machine to
    learn only the really important features.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Take a look at how `w0` is defined. Here, we're saying `w0` is a variable called
    `w0`. It is a tensor with the shape of (32, 1, 3, 3). This is typically called
    the **Number of Batches, Channels, Height, Width** (**NCHW**/**BCHW**) format.
    In short, what we're saying is that there are 32 filters we wish to learn, each
    filter has a height and width of (3, 3), and it has one color channel. MNIST is,
    after all, black and white.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: BCHW is not the only format! Some deep learning frameworks prefer to use BHWC
    formats. The reason for preferring one format over another is purely operational.
    Some convolution algorithms work better with NCHW; some work better with BHWC.
    The ones in Gorgonia works only in BCHW.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: The choice of a 3 x 3 filter is purely unprincipled but not without precedence.
    You could choose a 5 x 5 filter, or a 2 x 1 filter, or really, a filter of any
    shape. However, it has to be said that a 3 x 3 filter is probably the most universal
    filter that can work on all sorts of images. Square filters of these sorts are
    common in image-processing algorithms, so it is in accordance to such traditions
    that we chose a 3 x 3.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'The weights for the higher layers start to look a bit more interesting. For
    example, `w1` has a shape of (64, 32, 3, 3). Why? In order to understand why,
    we need to explore the interplay between the activation functions and the shapes.
    Here''s the entire forward function of the `convnet`:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: It should be noted that convolution layers do change the shape of the inputs.
    Given an (N, 1, 28, 28) input, the `Conv2d` function will return a (N, 32, 28,
    28) output, precisely because there are now 32 filters. The `MaxPool2d` will return
    an output with the shape of (N, 32, 14, 14); recall that the purpose of max-pooling
    is to reduce the amount of information in the neural network. It just happens
    that max-pooling with a shape of (2, 2) will nicely halve the length and width
    of the image (and reduce the amount of information by four times).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of layer 0 would have a shape of (N, 32, 14, 14). If we stick to
    our explanations of our shapes from earlier, where it was in the format of (N,
    C, H, W), we would be quite stumped. What does it mean to have 32 channels? To
    answer that, let''s look at how we encode a color image in terms of BCHW:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/68a7c029-efcf-489f-8097-ad069dcf4be9.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
- en: Note that we encode it as three separate layers, stacked onto one another. This
    is a clue as to how to think about having 32 channels. Of course, each of the
    32 channels as the result of applying each of the 32 filters; the extracted features,
    so to speak. The result can, of course, be stacked in the same way color channels
    be stacked.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: For the most part, however, the mere act of symbol pushing is all that is required
    to build a deep learning system; no real intelligence is required. This, of course
    mirrors, the Chinese Room Puzzle thought experiment, and I have quite a bit to
    say on that, though it's not really the time nor the place.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: The more interesting parts is in the construction of Layer 3\. Layers 1 and
    2 are constructed very similarly to Layer 0, but Layer 3 has a slightly different
    construction. The reason is because the output of Layer 2 is a rank-4 tensor,
    but in order to perform matrix multiplication, it needs to be reshaped into a
    rank-2 tensor.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, the final layer, which decodes the output, uses a softmax activation
    function to ensure that the result we get is probability.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: And really, there you have it. A CNN, written in a very neat way that does not
    obfuscate the mathematical definitions.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the convnet to learn, what is required is backpropagation, which propagates
    the errors, and a gradient descent function to update the weight matrices. To
    do this is relatively simple with Gorgonia, so simple that we can actually put
    it into our main function without impacting understandability:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'For the errors, we use a simple cross-entropy by multiplying the expected output
    element-wise and then averaging it, as shown in this snippet:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Following that, we simply call `gorgonia.Grad(cost, m.learnables()...)`, which
    performs symbolic backpropagation. What is `m.learnables()`?, you may ask. It''s
    simply the variables that we wish the machine to learn. The definition is as such:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Again, it's fairly simple.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: One additional comment I want the reader to note is `gorgonia.Read(cost, &costVal)`.
    `Read` is one of the more confusing parts of Gorgonia. But when framed correctly,
    it is quite simple to understand.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Earlier, in the section *Describing a neural network*, I likened Gorgonia to
    writing in another programming language. If so, then `Read` is the equivalent
    of `io.WriteFile`. What `gorgonia.Read(cost, &costVal)` says is that when the
    mathematical expression gets evaluated, make a copy of the result of `cost` and
    store it in `costVal`. This is necessary because of the way mathematical expressions
    are evaluated within the Gorgonia system.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Why is it called `Read` instead of `Write`? I initially modeled Gorgonia to
    be quite monadic (in the Haskell sense of monad), and as a result, one would *read
    out* a value. After a span of three years, the name sort of stuck.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Running the neural network
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Observe that up to this point, we've merely described the computations we need
    to perform. The neural network doesn't actually run; this is simply a description
    on the neural network to run.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to be able to evaluate the mathematical expression. In order to do
    so, we need to compile the expression into a program that can be executed. Here''s
    the code to do it:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'It''s not strictly necessary to call `gorgonia.Compile(g)`. This was done for
    pedagogical reasons, to showcase that the mathematical expression can indeed be
    compiled down into an assembly-like program. In production systems, I often just
    do something like this: `vm := gorgonia.NewTapeMachine(g, gorgonia.BindDualValues(m.learnables()...))`.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: There are two provided `vm` types in Gorgonia, each representing different modes
    of computation. In this project, we're merely using `NewTapeMachine` to get a
    `*gorgonia.tapeMachine`. The function to create a `vm` takes many options, and
    the `BindDualValues` option simply binds the gradients of each of the variables
    in the models to the variables themselves. This allows for cheaper gradient descent.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, note that a `VM` is a resource. You should think of a `VM` as if it
    were an external CPU, a computing resource. It is good practice to close any external
    resources after we use them and, fortunately, Go has a very convenient way of
    handling cleanups: `defer vm.Close()`.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we move on to talk about gradient descent, here''s what the compiled
    program looks like, in pseudo-assembly:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: batches := numExamples / bs
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: log.Printf("Batches %d", batches)
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: bar := pb.New(batches)
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: bar.SetRefreshRate(time.Second)
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: bar.SetMaxWidth(80)
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: for i := 0; i < *epochs; i++ {
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: bar.Prefix(fmt.Sprintf("Epoch %d", i))
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: bar.Set(0)
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: bar.Start()
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: for b := 0; b < batches; b++ {
  id: totrans-296
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: start := b * bs
  id: totrans-297
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: end := start + bs
  id: totrans-298
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: if start >= numExamples {
  id: totrans-299
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: break
  id: totrans-300
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-301
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: if end > numExamples {
  id: totrans-302
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: end = numExamples
  id: totrans-303
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: var xVal, yVal tensor.Tensor
  id: totrans-305
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: if xVal, err = inputs.Slice(sli{start, end}); err != nil {
  id: totrans-306
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: log.Fatal("Unable to slice x")
  id: totrans-307
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-308
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: if yVal, err = targets.Slice(sli{start, end}); err != nil {
  id: totrans-309
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: log.Fatal("Unable to slice y")
  id: totrans-310
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-311
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: if err = xVal.(*tensor.Dense).Reshape(bs, 1, 28, 28); err != nil {
  id: totrans-312
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: log.Fatalf("Unable to reshape %v", err)
  id: totrans-313
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-314
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: gorgonia.Let(x, xVal)
  id: totrans-315
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: gorgonia.Let(y, yVal)
  id: totrans-316
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: if err = vm.RunAll(); err != nil {
  id: totrans-317
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'log.Fatalf("Failed at epoch  %d: %v", i, err)'
  id: totrans-318
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-319
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: solver.Step(gorgonia.NodesToValueGrads(m.learnables()))
  id: totrans-320
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: vm.Reset()
  id: totrans-321
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: bar.Increment()
  id: totrans-322
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-323
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: log.Printf("Epoch %d | cost %v", i, costVal)
  id: totrans-324
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: testImgs, err := readImageFile(os.Open("t10k-images.idx3-ubyte"))
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: if err != nil {
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: log.Fatal(err)
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: testlabels, err := readLabelFile(os.Open("t10k-labels.idx1-ubyte"))
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: if err != nil {
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: log.Fatal(err)
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: testData := prepareX(testImgs)
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: testLbl := prepareY(testlabels)
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: shape := testData.Shape()
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: visualize(testData, 10, 10, "testData.png")
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: var correct, total float32
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: numExamples = shape[0]
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: batches = numExamples / bs
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: for b := 0; b < batches; b++ {
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: start := b * bs
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: end := start + bs
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: if start >= numExamples {
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: break
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: if end > numExamples {
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: end = numExamples
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: var oneimg, onelabel tensor.Tensor
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: if oneimg, err = testData.Slice(sli{start, end}); err != nil {
  id: totrans-353
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: log.Fatalf("Unable to slice images (%d, %d)", start, end)
  id: totrans-354
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-355
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: if onelabel, err = testLbl.Slice(sli{start, end}); err != nil {
  id: totrans-356
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: log.Fatalf("Unable to slice labels (%d, %d)", start, end)
  id: totrans-357
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-358
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: if err = oneimg.(*tensor.Dense).Reshape(bs, 1, 28, 28); err != nil {
  id: totrans-359
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: log.Fatalf("Unable to reshape %v", err)
  id: totrans-360
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-361
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: gorgonia.Let(x, oneimg)
  id: totrans-362
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: gorgonia.Let(y, onelabel)
  id: totrans-363
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: if err = vm.RunAll(); err != nil {
  id: totrans-364
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: log.Fatal("Predicting (%d, %d) failed %v", start, end, err)
  id: totrans-365
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-366
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: label, _ := onelabel.(*tensor.Dense).Argmax(1)
  id: totrans-367
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: predicted, _ := m.outVal.(*tensor.Dense).Argmax(1)
  id: totrans-368
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: lblData := label.Data().([]int)
  id: totrans-369
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: for i, p := range predicted.Data().([]int) {
  id: totrans-370
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: if p == lblData[i] {
  id: totrans-371
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: correct++
  id: totrans-372
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-373
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: total++
  id: totrans-374
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-375
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'fmt.Printf("Correct/Totals: %v/%v = %1.3f\n", correct, total, correct/total)'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: label, _ := onelabel.(*tensor.Dense).Argmax(1)
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: predicted, _ := m.outVal.(*tensor.Dense).Argmax(1)
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: lblData := label.Data().([]int)
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: for i, p := range predicted.Data().([]int) {
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: if p == lblData[i] {
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: correct++
  id: totrans-384
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-385
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: total++
  id: totrans-386
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we wrote our own `argmax` function. Gorgonia's tensor
    package actually does provide a handy method for doing just that. But in order
    to understand what is going on, we will need to first look at the results.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: The shape of `m.outVal` is (N, 10), where N is the batch size. The same shape
    also shows for `onelabel`.  (N, 10) means N rows of 10 columns. What can these
    10 columns be? Well, of course they're the encoded numbers! So what we want is
    to find the maximum values amongst the column for each row. And that's the first
    dimension. Hence when a call to `.ArgMax()` is made, we specify 1 as the axis.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: Therefore the result of the `.Argmax()` calls will have a shape (N). For each
    value in that vector, if they are the same for `lblData` and `predicted`, then
    we increment the `correct` counter. This gives us a way to count accuracy.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  id: totrans-392
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We use accuracy because the previous chapter used accuracy. This allows us to
    have a apples-to-apples comparison. Additionally you may note that there is a
    lack of cross validation. That will be left as an exercise to the reader.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: After training the neural network for two hours on a batch size of 50 and 150
    epochs, I'm pleased to say I got a 99.87% accuracy. And this isn't even state
    of the art!
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, it took just 6.5 minutes to get a 97% accuracy. That
    additional 2% accuracy required a lot more time. This is a factor in real life.
    Often business decisions are a big factor in choosing ML algorithm.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-396
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about neural networks and studied about the Gorgonia
    library in detail. Then we learned how to recognize handwritten digits using a
    CNN.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we're going to strengthen our intuition about what can
    be done with computer vision, by building a multiple facial-detection system in
    Go.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
