- en: Beating CAPTCHAs with Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Images pose interesting and difficult challenges for data miners. Until recently,
    only small amounts of progress were made with analyzing images for extracting
    information. However recently, such as with the progress made on self-driving
    cars, significant advances have been made in a very short time-frame. The latest
    research is providing algorithms that can understand images for commercial surveillance,
    self-driving vehicles, and person identification.
  prefs: []
  type: TYPE_NORMAL
- en: There is lots of raw data in an image, and the standard method for encoding
    images - pixels - isn't that informative by itself. Images and photos can be blurry,
    too close to the targets, too dark, too light, scaled, cropped, skewed, or any
    other of a variety of problems that cause havoc for a computer system trying to
    extract useful information. Neural networks can combine these lower level features
    into higher level patterns that are more able to generalize and deal with these
    issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we look at extracting text data from images by using neural
    networks for predicting each letter in the CAPTCHA. CAPTCHAs are images designed
    to be easy for humans to solve and hard for a computer to solve, as per the acronym:
    **Completely Automated Public Turing test to tell Computers and Humans Apart**.
    Many websites use them for registration and commenting systems to stop automated
    programs flooding their site with fake accounts and spam comments.'
  prefs: []
  type: TYPE_NORMAL
- en: These tests help stop programs (bots) using websites, such as a bot intent on
    automatically signing up new people to a website. We play the part of such a spammer,
    trying to get around a CAPTCHA-protected system for posting messages to an online
    forum. The website is protected by a CAPTCHA, meaning we can't post unless we
    pass the test.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics covered in this chapter include:'
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating our own dataset of CAPTCHAs and letters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The scikit-image library for working with image data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting basic features from images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using neural networks for larger-scale classification tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving performance using postprocessing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Artificial neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Artificial neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Neural networks are a class of algorithm that was originally designed based
    on the way that human brains work. However, modern advances are generally based
    on mathematics rather than biological insights. A neural network is a collection
    of neurons that are connected together. Each neuron is a simple function of its
    inputs, which are combined using some function to generate an output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06162_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The functions that define a neuron''s processing can be any standard function,
    such as a linear combination of the inputs, and is called the **activation function**.
    For the commonly used learning algorithms to work, we need the activation function
    to be *derivable* and  *smooth*. A frequently used activation function is the
    **logistic function**, which is defined by the following equation (*k* is often
    simply 1, *x* is the inputs into the neuron, and L is normally 1, that is, the
    maximum value of the function):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06162_08_02.png)'
  prefs: []
  type: TYPE_IMG
- en: The value of this graph, from -6 to +6, is shown below. The red lines indicate
    that the value is 0.5 when *x* is zero, but the function quickly climbs to 1.0
    as x increases, and quickly drops to -1.0 when x decreases.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06162_08_03.png)'
  prefs: []
  type: TYPE_IMG
- en: Each individual neuron receives its inputs and then computes the output based
    on these values. Neural networks can be considered as a collection of these neurons
    connected together, and they can be very powerful for data mining applications.
    The combinations of these neurons, how they fit together, and how they combine
    to learn a model are one of the most powerful concepts in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For data mining applications, the arrangement of neurons is usually in **layers**.
    The first layer is called the **input layer** and takes its input from samples
    in the data. The outputs of each of these neurons are computed and then passed
    along to the neurons in the next layer. This is called a **feed-forward neural
    network**. We will refer to these simply as **neural networks** for this chapter,
    as they are the most common type used and the only type used in this chapter.
    There are other types of neural networks too that are used for different applications.
    We will see another type of network in [Chapter 11](4b36fb74-3c44-430a-ac71-92412fc77a7b.xhtml)*,
    Object Detection in Images Using Deep Neural Networks*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The outputs of one layer become the inputs of the next layer, continuing until
    we reach the final layer: the **output layer**. These outputs represent the predictions
    of the neural network as the classification. Any layer of neurons between the
    input layer and the output layer is referred to as a **hidden layer**, as they
    learn a representation of the data not intuitively interpretable by humans. Most
    neural networks have at least three layers, although most modern applications
    use networks with many more layers than that.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06162_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Typically, we consider fully connected layers. The outputs of each neuron in
    a layer go to all neurons in the next layer. While we do define a fully connected
    network, many of the weights will be set to zero during the training process,
    effectively removing these links. Additionally, many of these weights might retain
    very small values, even after training.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to being one of the conceptually simpler forms for neural networks,
    fully connected neural networks are also simpler and more efficient to program
    than other connection patterns.
  prefs: []
  type: TYPE_NORMAL
- en: See [Chapter 11](4b36fb74-3c44-430a-ac71-92412fc77a7b.xhtml), *Object Detection
    in images using Deep Neural Networks*,  for an investigation into different types
    of neural networks, including layers built specifically for image processing.
  prefs: []
  type: TYPE_NORMAL
- en: As the function of the neurons is normally the logistic function, and the neurons
    are fully connected to the next layer, the parameters for building and training
    a neural network must be other factors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first factor for neural networks is in the building phase: the size and
    shape of the neural network. This includes how many layers the neural network
    has and how many neurons it has in each hidden layer (the size of the input and
    output layers is usually dictated by the dataset).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second parameter for neural networks is determined in the training phase:
    the weight of the connections between neurons. When one neuron connects to another,
    this connection has an associated weight that is multiplied by the signal (the
    output of the first neuron). If the connection has a weight of 0.8, the neuron
    is activated, and it outputs a value of 1, the resulting input to the next neuron
    is 0.8\. If the first neuron is not activated and has a value of 0, this stays
    at 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The combination of an appropriately sized network and well-trained weights determines
    how accurate the neural network can be when making classifications. The word *appropriately*
    in the previous sentence also doesn't necessarily mean bigger, as neural networks
    that are too large can take a long time to train and can more easily over-fit
    the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Weights can be set randomly to start with but are then updated during the training
    phase. Setting weights to zero is normally not a good idea, as all neurons in
    the network act similarly to begin with! Having randomly set weights gives each
    neuron a different *role* in the learning process that can be improved with training.
  prefs: []
  type: TYPE_NORMAL
- en: A neural network in this configuration is a classifier that can then be used
    to predict the target of a data sample based on the inputs, much like the classification
    algorithms we have used in previous chapters. But first, we need a dataset to
    train and test with.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks are, by a margin, the biggest area of advancement in data mining
    in recent years. This might make you think: *Why bother learning any other type
    of classification algorithm?* While neural networks are state of the art in pretty
    much every domain (at least, right now), the reason to learn other classifiers
    is that neural networks often require larger amounts of data to work well, and
    they take a long time to learn. If you don't have **big data**, you will probably
    get better results from another algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, to spice up things a little, let us take on the role of the
    bad guy. We want to create a program that can beat CAPTCHAs, allowing our comment
    spam program to advertise on someone's website. It should be noted that our CAPTCHAs
    will be a little easier than those used on the web today and that spamming isn't
    a very nice thing to do.
  prefs: []
  type: TYPE_NORMAL
- en: We play the bad guy today, but please *don't* use this against real world sites.
    One reason to "play the bad guy" is to help improve the security of our website,
    by looking for issues with it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our experiment will simplify a CAPTCHA to be individual English words of four
    letters only, as shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06162_08_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our goal will be to create a program that can recover the word from images
    like this. To do this, we will use four steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Break the image into individual letters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Classify each individual letter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recombine the letters to form a word.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rank words with a dictionary to try to fix errors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our CAPTCHA-busting algorithm will make the following assumptions. First, the
    word will be a whole and valid four-character English word (in fact, we use the
    same dictionary for creating and busting CAPTCHAs). Second, the word will only
    contain uppercase letters. No symbols, numbers, or spaces will be used.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to make the problem slightly harder than simply identifying letters,
    by performing a shear transform to the text, along with varying rates of shearing and
    scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Drawing basic CAPTCHAs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we can start classifying CAPTCHAs, we first need a dataset to learn from.
    In this section, we will be generating our own data to perform the data mining
    on.
  prefs: []
  type: TYPE_NORMAL
- en: In more real-world applications, you'll be wanting to use an existing CAPTCHA
    service to generate the data, but for our purposes in this chapter, our own data
    will be sufficient. One of the issues that can arise is that we code in our assumptions
    around how the data works when we create the dataset ourselves, and then carry
    those same assumptions over to our data mining training.
  prefs: []
  type: TYPE_NORMAL
- en: Our goal here is to draw an image with a word on it, along with a shear transform.
    We are going to use the PIL library to draw our CAPTCHAs and the `scikit-image`
    library to perform the shear transform. The `scikit-image` library can read images
    in a NumPy array format that PIL can export to, allowing us to use both libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both PIL and scikit-image can be installed via Anaconda. However, I recommend
    getting PIL through its replacement called **pillow**:'
  prefs: []
  type: TYPE_NORMAL
- en: conda install pillow scikit-image
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the necessary libraries and modules. We import NumPy and the
    Image drawing functions as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we create our base function for generating CAPTCHAs. This function takes
    a word and a shear value (which is normally between 0 and 0.5) to return an image
    in a NumPy array format. We allow the user to set the size of the resulting image,
    as we will use this function for single-letter training samples as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In this function, we create a new image using L for the format, which means
    black-and-white pixels only, and create an instance of the `ImageDraw` class.
    This allows us to draw on this image using PIL. We then load the font, draw the
    text, and perform a `scikit-image` shear transform on it.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can get the Coval font I used from the Open Font Library at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://openfontlibrary.org/en/font/bretan](http://openfontlibrary.org/en/font/bretan)'
  prefs: []
  type: TYPE_NORMAL
- en: Download the `.zip` file and extract the `Coval-Black.otf` file into the same
    directory as your Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'From here, we can now generate images quite easily and use `pyplot` to display
    them. First, we use our inline display for the matplotlib graphs and import `pyplot`.
    The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is the image shown at the start of this section: our CAPTCHA. Here
    are some other examples with different shear and scale values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B06162_08_12-2-e1493021528419.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B06162_08_13-e1493021566785.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is a variant scaled to `1.5` sized. While it looks similar to the BONE
    image above, note the *x*-axis and *y*-axis values are larger:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B06162_08_14-e1493021653291.png)'
  prefs: []
  type: TYPE_IMG
- en: Splitting the image into individual letters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our CAPTCHAs are words. Instead of building a classifier that can identify
    the thousands and thousands of possible words, we will break the problem down
    into a smaller problem: predicting letters.'
  prefs: []
  type: TYPE_NORMAL
- en: Our experiment is in English, and all uppercase, meaning we have 26 classes
    to predict from for each letter. If you try these experiments in other languages,
    keep in mind the number of output classes will have to change.
  prefs: []
  type: TYPE_NORMAL
- en: The first step in our algorithm for beating these CAPTCHAs involves segmenting
    the word to discover each of the letters within it. To do this, we are going to
    create a function that finds contiguous sections of black pixels in the image
    and extract them as subimages. These are (or at least should be) our letters.
    The `scikit-image` function has tools for performing these operations.
  prefs: []
  type: TYPE_NORMAL
- en: Our function will take an image, and return a list of sub-images, where each
    sub-image is a letter from the original word in the image. The first thing we
    need to do is to detect where each letter is. To do this, we will use the label
    function in `scikit-image`, which finds connected sets of pixels that have the
    same value. This has analogies to our connected component discovery in [Chapter
    7](67684c84-b6f7-4f09-b0ce-cabe2d2c373d.xhtml)*, Follow Recommendations Using
    Graph Mining*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then get the subimages from the example CAPTCHA using this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also view each of these subimages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The result will look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06162_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, our image segmentation does a reasonable job, but the results
    are still quite messy, with bits of previous letters showing. This is fine, and
    almost preferable. While training on data with regular noise makes our training
    worse, training on data with random noise can actually make it better. One reason
    is that the underlying data mining model learns the important aspects, namely
    the non-noise parts instead of specific noise inherent in the training data set.
    It is a fine line between too much and too little noise, and this can be hard
    to properly model. Testing on validation sets is a good way to ensure your training
    is improving.
  prefs: []
  type: TYPE_NORMAL
- en: 'One important note is that this code is not consistent in finding letters.
    Lower shear values typically result in accurately segmented images. For example,
    here is the code to segment the WOOF example from above:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B06162_08_15-e1493021829708.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In contrast, higher shear values are not segmented properly. For example, here
    is the BARK example from before:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06162_08_16.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice the large overlap caused by the square segmentation. One suggestion for
    an improvement on this chapter's code is to improve our segmentation by finding
    non-square segments.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a training dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using the functions we have already defined, we can now create a dataset of
    letters, each with different shear values. From this, we will train a neural network
    to recognize each letter from the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first set up our random state and an array that holds the options for letters,
    shear values and scale values that we will randomly select from. There isn''t
    much surprise here, but if you haven''t used NumPy''s `arange` function before,
    it is similar to Python''s `range` function—except this one works with NumPy arrays
    and allows the step to be a float. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We then create a function (for generating a single sample in our training dataset)
    that randomly selects a letter, a shear value, and a scale value selected from
    the available options.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We return the image of the letter, along with the target value representing
    the letter in the image. Our classes will be 0 for A, 1 for B, 2 for C, and so
    on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Outside the function block, we can now call this code to generate a new sample
    and then show it using `pyplot`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The resulting image has just a single letter, with a random shear and random
    scale value.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06162_08_17-e1493023909718.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can now generate all of our data by calling this several thousand times.
    We then put the data into NumPy arrays, as they are easier to work with than lists.
    The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Our targets are integer values between 0 and 26, with each representing a letter
    of the alphabet. Neural networks don''t usually support multiple values from a
    single neuron, instead preferring to have multiple outputs, each with values 0
    or 1\. We perform one-hot-encoding of the targets, giving us a target array that
    has 26 outputs per sample, using values near 1 if that letter is likely and near
    0 otherwise. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: From this output, we know that our neural network's output layer will have 26
    neurons. The goal of the neural network is to determine which of these neurons
    to fire, based on a given input--the pixels that compose the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The library we are going to use doesn''t support sparse arrays, so we need
    to turn our sparse matrix into a dense NumPy array. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we perform a train/test split to later evaluate our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Training and classifying
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are now going to build a neural network that will take an image as input
    and try to predict which (single) letter is in the image.
  prefs: []
  type: TYPE_NORMAL
- en: We will use the training set of single letters we created earlier. The dataset
    itself is quite simple. We have a 20-by-20-pixel image, each pixel 1 (black) or
    0 (white). These represent the 400 features that we will use as inputs into the
    neural network. The outputs will be 26 values between 0 and 1, where higher values
    indicate a higher likelihood that the associated letter (the first neuron is A,
    the second is B, and so on) is the letter represented by the input image.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to use the scikit-learn's `MLPClassifier` for our neural network
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will need a recent version of `scikit-learn` to use MLPClassifier. If the
    below import statement fails, try again after updating scikit-learn. You can do
    this using the following Anaconda command:'
  prefs: []
  type: TYPE_NORMAL
- en: '`conda update scikit-learn`'
  prefs: []
  type: TYPE_NORMAL
- en: 'As for other `scikit-learn` classifiers, we import the model type and create
    a new one. The constructor below specifies that we create one hidden layer with
    100 nodes in it. The size of the input and output layers is determined at training
    time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'To see the internal parameters of the neural network, we can use the `get_params()`
    function. This function exists on all `scikit-learn` models. Here is the output
    from the above model. Many of these parameters can improve training or the speed
    of training. For example, increasing the learning rate will train the model faster,
    at the risk of missing optimal values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we fit our model using the standard scikit-learn interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Our model has now learned weights between each of the layers. We can view those
    weights by examining `clf.coefs_`, which is a list of NumPy arrays that join each
    of the layers. For example, the weights between the input layer with 400 neurons
    (from each of our pixels) to the hidden layer with 100 neurons (a parameter we
    set), can be obtained using `clf.coefs_[0]`. In addition, the weights between
    the hidden layer and the output layer (with 26 neurons) can be obtained using
    `clf.coefs_[1]`. These weights, together with the parameters above, wholly define
    our trained network.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now use that trained network to predict our test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we evaluate the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The result is 0.96, which is pretty impressive. This version of the F1 score
    is based on the macro-average, which computes the individual F1 score for each
    class, and then averages them without considering the size of each class.
  prefs: []
  type: TYPE_NORMAL
- en: 'To examine these individual class results, we can view the classification report:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The results from my experiment are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The final `f1-score` for this report is shown on the bottom right, the second
    last number - 0.99\. This is the micro-average, where the `f1-score` is computed
    for each sample and then the mean is computed. This form makes more sense for
    relatively similar class sizes, while the macro-average makes more sense for imbalanced
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: Pretty simple from an API perspective, as `scikit-learn` hides all of the complexity.
    However what actually happened in the backend? How do we train a neural network?
  prefs: []
  type: TYPE_NORMAL
- en: Back-propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training a neural network is specifically focused on the following things.
  prefs: []
  type: TYPE_NORMAL
- en: The first is the size and shape of the network - how many layers, what sized
    layers and what error functions they use. While types of neural networks exists
    that can alter their size and shape, the most common type, a feed-forward neural
    network, rarely has this capability. Instead, its size is fixed at initialization
    time, which in this chapter is 400 neurons in the first layer, 100 in the hidden
    layer and 26 in the final layer. Training for the shape is usually the job of
    a meta-algorithm that trains a set of neural networks and determines which is
    the most effective, outside of training the networks themselves.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second part of training a neural network is to alter the weights between
    neurons. In a standard neural network, nodes from one layer are attached to nodes
    of the next layer by edges with a specific weight. These can be initialized randomly
    (although several smarter methods do exist such as autoencoders), but need to
    be adjusted to allow the network to *learn* the relationship between training
    samples and training classes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This adjusting of weights was one of the key issues holding back very-early
    neural networks, before an algorithm called **back propagation** was developed
    to solve the issue.
  prefs: []
  type: TYPE_NORMAL
- en: The **back propagation** (**backprop**) algorithm is a way of assigning blame
    to each neuron for incorrect predictions. First, we consider the usage of a neural
    network, where we feed a sample into the input layer and see which of the output
    layer's neurons fire, as *forward propagation*. Back propagation goes backwards
    from the output layer to the input layer, assigning blame to each weight in the
    network, in proportion to the effect that weight has on any errors that the network
    makes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The amount of change is based on two aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: Neuron activation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The gradient of the activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first is the degree to which the neuron was *activated*. Neurons that fire
    with high (absolute) values are considered to have a great impact on the result,
    while those that fired with small (absolute) values have a low impact on the result.
    Due to this, weights around neurons that fire with high values are changed more
    than those around small values.
  prefs: []
  type: TYPE_NORMAL
- en: The second aspect to the amount that weights change is proportional to the *gradient
    of the activation function*. Many neural networks you use will have the same activation
    function for all neurons, but there are lots of situations where it makes sense
    to have different activation functions for different layers of neurons (or more
    rarely, different activation functions in the same layer). The gradient of the
    activation function, combined with the activation of the neuron, and the error
    assigned to that neuron, together form the amount that the weights are changed.
  prefs: []
  type: TYPE_NORMAL
- en: I've skipped over the maths involved in back propagation, as the focus of this
    book is on practical usage. As you increase your usage of neural networks, it
    pays to know more about what goes on inside the algorithm. I recommend looking
    into the details of the back-prop algorithm, which can be understood with some
    basic knowledge of gradients and derivatives.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a classifier for predicting individual letters, we now move
    onto the next step in our plan - predicting words. To do this, we want to predict
    each letter from each of these segments, and put those predictions together to
    form the predicted word from a given CAPTCHA.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our function will accept a CAPTCHA and the trained neural network, and it will
    return the predicted word:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now test on a word using the following code. Try different words and
    see what sorts of errors you get, but keep in mind that our neural network only
    knows about capital letters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We can codify this into a function, allowing us to perform predictions more
    easily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The returned results specify whether the prediction is correct, the original
    word, and the predicted word. This code correctly predicts the word GENE, but
    makes mistakes with other words. How accurate is it? To test, we will create a
    dataset with a whole bunch of four-letter English words from NLTK. The code is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Install NLTK using Anaconda: conda install nltk'
  prefs: []
  type: TYPE_NORMAL
- en: 'After installation, and before using it in code, you will need to download
    the corpus using:'
  prefs: []
  type: TYPE_NORMAL
- en: python -c "import nltk; nltk.download('words')"
  prefs: []
  type: TYPE_NORMAL
- en: 'The words instance here is actually a corpus object, so we need to call `words()`
    on it to extract the individual words from this corpus. We also filter to get
    only four-letter words from this list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then iterate over all of the words to see how many we get correct by
    simply counting the correct and incorrect predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The results I get are 3,342 correct and 2,170 incorrect for an accuracy of just
    over 62 percent. From our original 99 percent per-letter accuracy, this is a big
    decline. What happened?
  prefs: []
  type: TYPE_NORMAL
- en: 'The reasons for this decline are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: The first factor to impact is our accuracy. All other things being equal, if
    we have four letters, and 99 percent accuracy per-letter, then we can expect about
    a 96 percent success rate (all other things being equal) getting four letters
    in a row (0.99⁴&ap;0.96). A single error in a single letter's prediction results
    in the wrong word being predicted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second impact is the shear value. Our dataset chose randomly between shear
    values of 0 to 0.5\. The previous test used a shear of 0.2\. For a value of 0,
    I get 75 percent accuracy; for a shear of 0.5, the result is much worse at 2.5
    percent. The higher the shear, the lower the performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third impact is that often words are incorrectly segmented. Another issue
    is that some vowels are commonly mistaken, causing more errors than can be expected
    by the above error rates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s examine the second of these issues, and map the relationship between
    shear and performance. First, we turn our evaluation code into a function that
    is dependent on a given shear value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Then, we take a list of shear values and then use this function to evaluate
    the accuracy for each value. Note that this code will take a while to run, approximately
    30 minutes depending on your hardware.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, plot the result using matplotlib:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B06162_08_18-1.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see that there is a severe drop in performance as the shear value increases
    past 0.4\. Normalizing the input would help, with tasks such as image rotation
    and unshearing the input.
  prefs: []
  type: TYPE_NORMAL
- en: Another surprising option to address issues with shear is to increase the amount
    of training data with high shear values, which can lead to the model learning
    a more generalized output.
  prefs: []
  type: TYPE_NORMAL
- en: We look into improving the accuracy using post-processing in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Improving accuracy using a dictionary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rather than just returning the given prediction, we can check whether the word
    actually exists in our dictionary. If it does, then that is our prediction. If
    it isn't in the dictionary, we can try and find a word that is similar to it and
    predict that instead. Note that this strategy relies on our assumption that all
    CAPTCHA words will be valid English words, and therefore this strategy wouldn't
    work for a random sequence of characters. This is one reason why some CAPTCHAs
    don't use words.
  prefs: []
  type: TYPE_NORMAL
- en: There is one issue here—how do we determine the closest word? There are many
    ways to do this. For instance, we can compare the lengths of words. Two words
    that have a similar length could be considered more similar. However, we commonly
    consider words to be similar if they have the same letters in the same positions.
    This is where the **edit distance** comes in.
  prefs: []
  type: TYPE_NORMAL
- en: Ranking mechanisms for word similarity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **Levenshtein edit distance** is a commonly used method for comparing two
    short strings to see how similar they are. It isn''t very scalable, so it isn''t
    commonly used for very long strings. The edit distance computes the number of
    steps it takes to go from one word to another. The steps can be one of the following
    three actions:'
  prefs: []
  type: TYPE_NORMAL
- en: Insert a new letter into the word at any position
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delete any letter from the word
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Substitute a letter for another one
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The minimum number of actions needed to transform the first word into the second
    is given as the distance. Higher values indicate that the words are less similar.
  prefs: []
  type: TYPE_NORMAL
- en: 'This distance is available in NLTK as `nltk.metrics.edit_distance`. We can
    call it using only two strings and it returns the edit distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: When used with different words, the edit distance is quite a good approximation
    to what many people would intuitively feel are similar words. The edit distance
    is great for testing spelling mistakes, dictation errors, and name matching (where
    you can mix up your Marc and Mark spelling quite easily).
  prefs: []
  type: TYPE_NORMAL
- en: 'However, it isn''t very good for our case. We don''t really expect letters
    to be moved around, just individual letter comparisons to be wrong. For this reason,
    we will create a different distance metric, which is simply the number of letters
    in the same positions that are incorrect. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: We subtract the value from the length of the prediction word (which is four)
    to make it a distance metric where lower values indicate more similarity between
    the words.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can now test our improved prediction function using similar code to before.
    First, we define a prediction function, which also takes our list of valid words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We compute the distance between our predicted word and each other word in the dictionary,
    and sort it by distance (lowest first). The changes in our testing code are in
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code will take a while to run (computing all of the distances
    will take some time) but the net result is 3,037 samples correct and 2,476 samples
    incorrect. This is an accuracy of 71.5 percent for a boost of nearly 10 percentage
    points!
  prefs: []
  type: TYPE_NORMAL
- en: Looking for a challenge? Update the `predict_captcha` function to return the
    probabilities assigned to each letter. By default, the letter with the highest
    probability is chosen for each letter in a word. If that doesn't work, choose
    the next most probable word, by multiplying the per-letter probabilities together.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we worked with images in order to use simple pixel values to
    predict the letter being portrayed in a CAPTCHA. Our CAPTCHAs were a bit simplified;
    we only used complete four-letter English words. In practice, the problem is much
    harder--as it should be! With some improvements, it would be possible to solve
    much harder CAPTCHAs with neural networks and a methodology similar to what we
    discussed. The `scikit-image` library contains lots of useful functions for extracting
    shapes from images, functions for improving contrast, and other image tools that
    will help.
  prefs: []
  type: TYPE_NORMAL
- en: We took our larger problem of predicting words, and created a smaller and simple
    problem of predicting letters. From here, we were able to create a feed-forward
    neural network to accurately predict which letter was in the image. At this stage,
    our results were very good with 97 percent accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks are simply connected sets of neurons, which are basic computation
    devices consisting of a single function. However, when you connect these together,
    they can solve incredibly complex problems. Neural networks are the basis for
    deep learning, which is one of the most effective areas of data mining at the
    moment.
  prefs: []
  type: TYPE_NORMAL
- en: Despite our great per-letter accuracy, the performance when predicting a word
    drops to just over 60 percent when trying to predict a whole word. We improved
    our accuracy using a dictionary, searching for the best matching word. To do this,
    we considered the commonly used edit distance; however, we simplified it because
    we were only concerned with individual mistakes on letters, not insertions or
    deletions. This improvement netted some benefit, but there are still many improvements
    you could try to further boost the accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: To take the concepts in this chapter further, investigate changing the neural
    network structure, by adding more hidden layers, or changing the shape of those
    layers. Investigate the impact this has on the result. Further, try creating a
    more difficult CAPTCHA--does this drop the accuracy? Can you build a more complicated
    network to learn it?
  prefs: []
  type: TYPE_NORMAL
- en: Data mining problems such as the CAPTCHA example show that an initial problem
    statement, such as *guess this word*, can be broken into individual subtasks that
    can be performed using data mining. Further, those subtasks can be combined in
    a few different ways, such as with the use of external information. In this chapter,
    we combined our letter prediction with a dictionary of valid words to provide
    a final response, giving better accuracy than letter prediction alone.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will continue with string comparisons. We will attempt
    to determine which author (out of a set of authors) wrote a particular document--using
    only the content and no other information!
  prefs: []
  type: TYPE_NORMAL
