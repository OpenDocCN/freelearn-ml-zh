<html><head></head><body>
  <div id="_idContainer296" class="Basic-Text-Frame">
    <h1 class="chapterNumber">11</h1>
    <h1 id="_idParaDest-297" class="chapterTitle">Bias Mitigation and Causal Inference Methods</h1>
    <p class="normal">In <em class="chapterRef">Chapter 6</em>, <em class="italic">Anchors and Counterfactual Explanations</em>, we examined fairness and its connection to decision-making but limited to post hoc model interpretation methods. In <em class="chapterRef">Chapter 10</em>, <em class="italic">Feature Selection and Engineering for Interpretability</em>, we broached the topic of cost-sensitivity, which often relates to balance or fairness. In this chapter, we will engage with methods that will balance data and tune models for fairness.</p>
    <p class="normal">With a credit card default dataset, we will learn how to leverage target visualizers such as class balance to detect undesired bias, then how to reduce it via preprocessing methods such as reweighting and disparate impact remover for in-processing and equalized odds for post-processing. Extending from the topics of <em class="chapterRef">Chapter 6</em>, <em class="italic">Anchors and Counterfactual Explanations</em>, and <em class="chapterRef">Chapter 10</em>, <em class="italic">Feature Selection and Engineering for Interpretability</em>, we will also study how policy decisions can have unexpected, counterintuitive, or detrimental effects. A decision, in the context of hypothesis testing, is called a <strong class="keyWord">treatment</strong>. For <a id="_idIndexMarker1157"/>many decision-making scenarios, it is critical to estimate their effect and make sure this estimate is reliable.</p>
    <p class="normal">Therefore, we will hypothesize treatments for reducing credit card default for the most vulnerable populations and leverage causal modeling to <a id="_idIndexMarker1158"/>determine its <strong class="keyWord">Average Treatment Effects</strong> (<strong class="keyWord">ATE</strong>) and <strong class="keyWord">Conditional Average Treatment Effects</strong> (<strong class="keyWord">CATE</strong>). Finally, we<a id="_idIndexMarker1159"/> will test causal assumptions and the robustness of estimates using a variety of methods.</p>
    <p class="normal">These are the main topics we are going to cover:</p>
    <ul>
      <li class="bulletList">Detecting bias</li>
      <li class="bulletList">Mitigating bias</li>
      <li class="bulletList">Creating a causal model</li>
      <li class="bulletList">Understanding heterogeneous treatment effects</li>
      <li class="bulletList">Testing estimate robustness</li>
    </ul>
    <h1 id="_idParaDest-298" class="heading-1">Technical requirements</h1>
    <p class="normal">This chapter’s example uses the <code class="inlineCode">mldatasets</code>, <code class="inlineCode">pandas</code>, <code class="inlineCode">numpy</code>, <code class="inlineCode">sklearn</code>, <code class="inlineCode">lightgbm</code>, <code class="inlineCode">xgboost</code>, <code class="inlineCode">matplotlib</code>, <code class="inlineCode">seaborn</code>, <code class="inlineCode">xai</code>, <code class="inlineCode">aif360</code>, <code class="inlineCode">econml</code>, and <code class="inlineCode">dowhy</code> libraries. Instructions on how to install all these libraries are in the preface. </p>
    <div class="note">
      <p class="normal">The code for this chapter is located here:</p>
      <p class="normal"><a href="https://packt.link/xe6ie"><span class="url">https://packt.link/xe6ie</span></a></p>
    </div>
    <h1 id="_idParaDest-299" class="heading-1">The mission</h1>
    <p class="normal">Over 2.8 billion credit cards <a id="_idIndexMarker1160"/>are circulating worldwide, and we collectively spend over $25 trillion (US) on them every year (<a href="https://www.ft.com/content/ad826e32-2ee8-11e9-ba00-0251022932c8"><span class="url">https://www.ft.com/content/ad826e32-2ee8-11e9-ba00-0251022932c8</span></a>). This is an astronomical amount, no doubt, but the credit card industry’s size is best measured not by what is spent, but by what is owed. Card issuers such as banks make the bulk of their money from interest. So, the over $60 trillion owed by consumers (2022), of which credit card debt is a sizable portion, provides a steady income to lenders in the form of interest. It could be argued this is good for business, but it also poses ample risk because if a borrower defaults before the principal plus operation costs have been repaid, the lender could lose money, especially once they’ve exhausted legal avenues to collect the debt.</p>
    <p class="normal">When there’s a credit bubble, this problem is compounded because an unhealthy level of debt can compromise lenders’ finances and take their stakeholders down with them when the bubble bursts. Such was the case with the 2008 housing bubble, also known as the subprime mortgage crisis. These bubbles often begin with speculation on growth and seeking unqualified demand to fuel that growth. In the case of the mortgage crisis, the banks offered mortgages to people with no proven capacity to repay. They also, sadly, targeted minorities who had their entire net worth wiped out once the bubble burst. Financial crises and depressions, and every calamity in between, tend to affect those that are most vulnerable at much higher rates.</p>
    <p class="normal">Credit cards have also been involved in catastrophic bubbles, notably in South Korea in 2003 (<a href="https://www.bis.org/repofficepubl/arpresearch_fs_200806.10.pdf"><span class="url">https://www.bis.org/repofficepubl/arpresearch_fs_200806.10.pdf</span></a>) and Taiwan in 2006. This chapter will examine data from 2005, leading to the Taiwanese credit card crisis. By 2006, delinquent credit card debt reached $268 billion owed by over 700,000 people. Just over 3% of the Taiwanese population could not pay even the credit card’s minimum balance and colloquially were known as <strong class="keyWord">credit card slaves</strong>. Significant societal ramifications ensued, such as a sharp increase in homelessness, drug trafficking/abuse, and even suicide. In the aftermath of the 1997 Asian financial crisis, suicide steadily increased around the region. A 23% jump between 2005 and 2006 pushed Taiwan’s suicide rate to the world’s second highest.</p>
    <p class="normal">If we trace <a id="_idIndexMarker1161"/>back the crisis to its root causes, it was about new card-issuing banks having exhausted a saturated real-estate market, slashing requirements to obtain credit cards, which at the time were poorly regulated by authorities. </p>
    <p class="normal">It hit younger people the most because they typically have less income and experience in managing money. In 2005, the Taiwanese Financial Supervisory Commission issued new regulations to raise credit card applicants’ requirements, preventing new credit card slaves. However, more policies would be needed to attend to the debt and the debtors already in the system. </p>
    <p class="normal">Authorities started discussing the creation of <strong class="keyWord">asset management corporations</strong> (<strong class="keyWord">AMCs</strong>) to take bad debts from the balance sheet of banks. They also wanted to pass a <em class="italic">debtors’ repayment regulation</em> that would provide a framework to negotiate a reasonable repayment plan. Neither of these policies were codified into law untill 2006.</p>
    <p class="normal">Hypothetically, let’s say it’s August 2005, and you have come from the future armed with novel machine learning and causal inference methods! A Taiwanese bank wants to create a classification model to predict customers that will default on their loans. They have provided you with a dataset of 30,000 of their credit card customers. Regulators are still drafting the laws, so there’s an opportunity to propose policies that benefit both the bank and the debtors. When the laws have passed, using the classification model, they can then anticipate which debts they should sell to the AMCs and, with the causal model, estimate which policies would benefit other customers and the bank, but they want to do this fairly and robustly—this is your mission!</p>
    <h1 id="_idParaDest-300" class="heading-1">The approach</h1>
    <p class="normal">The bank has<a id="_idIndexMarker1162"/> stressed to you how important it is that there’s fairness embedded in your methods because the regulators and the public at large want assurance that banks will not cause any more harm. Their reputation depends on it too, because in recent months, the media has been relentless in blaming them for dishonest and predatory lending practices, causing distrust in consumers. For this reason, they want to use state-of-the-art robustness testing to demonstrate that the prescribed policies will alleviate the problem. Your proposed approach includes the following points:</p>
    <ul>
      <li class="bulletList">Younger lenders have been reported to be more prone to defaulting on repayment, so you expect to find age bias, but you will also <em class="italic">look for bias</em> with other protected groups such as gender.</li>
      <li class="bulletList">Once you have detected bias, you can <em class="italic">mitigate bias</em> with preprocessing, in-processing, and post-processing algorithms using the <strong class="keyWord">AI Fairness 360</strong> (<strong class="keyWord">AIF360</strong>) library. In this process, you will train different models with each algorithm, assess their fairness, and choose the fairest model.</li>
      <li class="bulletList">To be able to understand the impact of policies, the bank has conducted an experiment on a small portion of customers. With the experimental results, you can fit a <em class="italic">causal model</em> through the <code class="inlineCode">dowhy</code> library, which will identify the <em class="italic">causal effect</em>. These effects are broken down further by the causal model to reveal the heterogeneous treatment effects.</li>
      <li class="bulletList">Then, you can <em class="italic">assess the heterogeneous treatment effects</em> to understand them and decide which treatment is the most effective.</li>
      <li class="bulletList">Lastly, to <em class="italic">ensure that your conclusions are robust</em>, you will refute the results with several methods to see if the effects hold.</li>
    </ul>
    <p class="normal">Let’s dig in!</p>
    <h1 id="_idParaDest-301" class="heading-1">The preparations</h1>
    <p class="normal">You will find <a id="_idIndexMarker1163"/>the code for this example here: <a href="https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python/blob/master/Chapter11/CreditCardDefaults.ipynb"><span class="url">https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python/blob/master/Chapter11/CreditCardDefaults.ipynb</span></a>.</p>
    <h2 id="_idParaDest-302" class="heading-2">Loading the libraries</h2>
    <p class="normal">To run this <a id="_idIndexMarker1164"/>example, you need to install the following libraries:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">mldatasets</code> to load the dataset</li>
      <li class="bulletList"><code class="inlineCode">pandas</code> and <code class="inlineCode">numpy</code> to manipulate it</li>
      <li class="bulletList"><code class="inlineCode">sklearn</code> (scikit-learn), <code class="inlineCode">xgboost</code>, <code class="inlineCode">aif360</code>, and <code class="inlineCode">lightgbm</code> to split the data and fit the models</li>
      <li class="bulletList"><code class="inlineCode">matplotlib</code>, <code class="inlineCode">seaborn</code>, and <code class="inlineCode">xai</code> to visualize the interpretations</li>
      <li class="bulletList"><code class="inlineCode">econml</code> and <code class="inlineCode">dowhy</code> for causal inference</li>
    </ul>
    <p class="normal">You should load all of them first, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> math
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> mldatasets
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> tqdm.notebook <span class="hljs-keyword">import</span> tqdm
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> model_selection, tree
<span class="hljs-keyword">import</span> lightgbm <span class="hljs-keyword">as</span> lgb
<span class="hljs-keyword">import</span> xgboost <span class="hljs-keyword">as</span> xgb
<span class="hljs-keyword">from</span> aif360.datasets <span class="hljs-keyword">import</span> BinaryLabelDataset
<span class="hljs-keyword">from</span> aif360.metrics <span class="hljs-keyword">import</span> BinaryLabelDatasetMetric,\
                           ClassificationMetric
<span class="hljs-keyword">from</span> aif360.algorithms.preprocessing <span class="hljs-keyword">import</span> Reweighing,\
                                           DisparateImpactRemover
<span class="hljs-keyword">from</span> aif360.algorithms.inprocessing <span class="hljs-keyword">import</span> ExponentiatedGradientReduction, GerryFairClassifier
<span class="hljs-keyword">from</span> aif360.algorithms.postprocessing.\
                      calibrated_eq_odds_postprocessing \
                            <span class="hljs-keyword">import</span> CalibratedEqOddsPostprocessing
<span class="hljs-keyword">from</span> aif360.algorithms.postprocessing.eq_odds_postprocessing\
                            <span class="hljs-keyword">import</span> EqOddsPostprocessing
<span class="hljs-keyword">from</span> econml.dr <span class="hljs-keyword">import</span> LinearDRLearner
<span class="hljs-keyword">import</span> dowhy
<span class="hljs-keyword">from</span> dowhy <span class="hljs-keyword">import</span> CausalModel
<span class="hljs-keyword">import</span> xai
<span class="hljs-keyword">from</span> networkx.drawing.nx_pydot <span class="hljs-keyword">import</span> to_pydot
<span class="hljs-keyword">from</span> IPython.display <span class="hljs-keyword">import</span> Image, display
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
</code></pre>
    <h2 id="_idParaDest-303" class="heading-2">Understanding and preparing the data</h2>
    <p class="normal">We load <a id="_idIndexMarker1165"/>the data like this into a DataFrame called <code class="inlineCode">ccdefault_all_df</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">ccdefault_all_df = mldatasets.load(<span class="hljs-string">"cc-default"</span>, prepare=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">There should be 30,000 records and 31 columns. We can verify this is the case with <code class="inlineCode">info()</code>, like this:</p>
    <pre class="programlisting code"><code class="hljs-code">ccdefault_all_df.info()
</code></pre>
    <p class="normal">The preceding code outputs the following:</p>
    <pre class="programlisting con"><code class="hljs-con">Int64Index: 30000 entries, 1 to 30000
Data columns (total 31 columns):
#   Column            Non-Null Count  Dtype  
---  ------            --------------  -----  
0   CC_LIMIT_CAT      30000 non-null  int8   
1   EDUCATION         30000 non-null  int8   
2   MARITAL_STATUS    30000 non-null  int8   
3   GENDER            30000 non-null  int8   
4   AGE_GROUP         30000 non-null  int8   
5   pay_status_1      30000 non-null  int8   
6   pay_status_2      30000 non-null  int8   
7   pay_status_3      30000 non-null  int8   
8   pay_status_4      30000 non-null  int8   
9   pay_status_5      30000 non-null  int8   
10  pay_status_6      30000 non-null  int8   
11  paid_pct_1        30000 non-null  float64
12  paid_pct_2        30000 non-null  float64
13  paid_pct_3        30000 non-null  float64
14  paid_pct_4        30000 non-null  float64
15  paid_pct_5        30000 non-null  float64
16  paid_pct_6        30000 non-null  float64
17  bill1_over_limit  30000 non-null  float64
18  IS_DEFAULT        30000 non-null  int8   
19  _AGE              30000 non-null  int16  
20  _spend            30000 non-null  int32  
21  _tpm              30000 non-null  int16  
22  _ppm              30000 non-null  int16  
23  _RETAIL           30000 non-null  int8   
24  _URBAN            30000 non-null  int8   
25  _RURAL            30000 non-null  int8   
26  _PREMIUM          30000 non-null  int8   
27  _TREATMENT        30000 non-null  int8   
28  _LTV              30000 non-null  float64
29  _CC_LIMIT         30000 non-null  int32  
30  _risk_score       30000 non-null  float64
</code></pre>
    <p class="normal">The <a id="_idIndexMarker1166"/>output checks out. All features are numerical, with no missing values because we used <code class="inlineCode">prepare=True</code>, which ensures that all null values are imputed. Categorical features are all <code class="inlineCode">int8</code> because they have already been encoded.</p>
    <h3 id="_idParaDest-304" class="heading-3">The data dictionary</h3>
    <p class="normal">There<a id="_idIndexMarker1167"/> are 30 features, but we won’t use them together because 18 of them are for the bias mitigation exercise, and the remaining 12 that start with an underscore (_) are for the causal inference exercise. Soon, we will split the data into the corresponding datasets for each exercise. It’s important to note that lowercase features have to do with each client’s transactional history, whereas client account or target features are uppercase.</p>
    <p class="normal">We will use the following features in the <em class="italic">bias mitigation exercise</em>:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">CC_LIMIT_CAT</code>: ordinal; the credit card limit (<code class="inlineCode">_CC_LIMIT</code>) separated into eight approximately equally distributed quartiles</li>
      <li class="bulletList"><code class="inlineCode">EDUCATION</code>: ordinal; the customer’s educational attainment level (<code class="inlineCode">0</code>: Other, <code class="inlineCode">1</code>: High School, <code class="inlineCode">2</code>: Undergraduate, <code class="inlineCode">3</code>: Graduate)</li>
      <li class="bulletList"><code class="inlineCode">MARITAL_STATUS</code>: nominal; the customer’s marital status (<code class="inlineCode">0</code>: Other, <code class="inlineCode">1</code>: Single, <code class="inlineCode">2</code>: Married)</li>
      <li class="bulletList"><code class="inlineCode">GENDER</code>: nominal; the gender of the customer (<code class="inlineCode">1</code>: Male, <code class="inlineCode">2</code>: Female)</li>
      <li class="bulletList"><code class="inlineCode">AGE GROUP</code>: binary; denoting if the customer belongs to a privileged age group (<code class="inlineCode">1</code>: privileged (26-47 years old), <code class="inlineCode">0</code>: underprivileged (every other age))</li>
      <li class="bulletList"><code class="inlineCode">pay_status_1</code> <code class="inlineCode">pay_status_6</code>: ordinal; the repayment status for the previous six periods from April, <code class="inlineCode">pay_status_6</code>, to August 2005, <code class="inlineCode">pay_status_1</code> (<code class="inlineCode">-1</code>: payment on time, <code class="inlineCode">1</code>: payment is 1 month delayed, <code class="inlineCode">2</code>: payment is 2 months delayed <code class="inlineCode">8</code>: 8 months delayed, <code class="inlineCode">9</code>: 9 months and above)</li>
      <li class="bulletList"><code class="inlineCode">paid_pct_1</code> <code class="inlineCode">paid_pct_6</code>: continuous; the percentage of the bill due each month from April, <code class="inlineCode">paid_pct_6</code>, to August 2005, <code class="inlineCode">paid_pct_1</code>, that was paid</li>
      <li class="bulletList"><code class="inlineCode">bill1_over_limit</code>: continuous; the last bill’s ratio in August 2005 over the corresponding credit limit</li>
      <li class="bulletList"><code class="inlineCode">IS_DEFAULT</code>: binary; target variable; whether the customer defaulted</li>
    </ul>
    <p class="normal">These are the features<a id="_idIndexMarker1168"/> we will use only in the <em class="italic">causal inference exercise</em>:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">_AGE</code>: continuous; the age in years of the customer.</li>
      <li class="bulletList"><code class="inlineCode">_spend</code>: continuous; how much was spent by each customer in <strong class="keyWord">New Taiwan Dollar</strong> (<strong class="keyWord">NT$</strong>).</li>
      <li class="bulletList"><code class="inlineCode">_tpm</code>: continuous; median transactions per month made by the customer with the credit card over the previous 6 months.</li>
      <li class="bulletList"><code class="inlineCode">_ppm</code>: continuous; median purchases per month made by the customer with the credit card over the previous 6 months.</li>
      <li class="bulletList"><code class="inlineCode">_RETAIL</code>: binary; if the customer is a retail customer, instead of a customer obtained through their employer.</li>
      <li class="bulletList"><code class="inlineCode">_URBAN</code>: binary; if the customer is an urban customer.</li>
      <li class="bulletList"><code class="inlineCode">_RURAL</code>: binary; if the customer is a rural customer.</li>
      <li class="bulletList"><code class="inlineCode">_PREMIUM</code>: binary; if the customer is “premium.” Premium customers get cashback offers and other spending incentives.</li>
      <li class="bulletList"><code class="inlineCode">_TREATMENT</code>: nominal; the intervention or policy prescribed to each customer (<code class="inlineCode">-1</code>: Not part of the experiment, <code class="inlineCode">0</code>: Control group, <code class="inlineCode">1</code>: Lower Credit Limit, <code class="inlineCode">2</code>: Payment Plan, <code class="inlineCode">3</code>: Payment Plan and Credit Limit).</li>
      <li class="bulletList"><code class="inlineCode">_LTV</code>: continuous; the outcome of the intervention, which is the lifetime value estimated in <em class="italic">NT$</em> given the credit payment behavior over the previous 6 months.</li>
      <li class="bulletList"><code class="inlineCode">_CC_LIMIT</code>: continuous; the original credit card limit in <em class="italic">NT$</em> that the customer had before the treatment. Bankers expect the outcome of the treatment to be greatly impacted by this feature.</li>
      <li class="bulletList"><code class="inlineCode">_risk_score</code>: continuous; the risk score that the bank computed 6 months prior for each customer based on credit card bills’ ratio over their credit card limit. It’s like <code class="inlineCode">bill1_over_limit</code> except it’s a weighted average of 6 months of payment history, and it was produced 5 months before choosing the treatment.</li>
    </ul>
    <p class="normal">We will explain the causal inference features a bit more and their purpose in the following sections. Meanwhile, let’s break down the <code class="inlineCode">_TREATMENT</code> feature by its values with <code class="inlineCode">value_counts()</code> to understand how we will split this dataset, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">ccdefault_all_df._TREATMENT.value_counts()
</code></pre>
    <p class="normal">The preceding code outputs the following:</p>
    <pre class="programlisting con"><code class="hljs-con">-1    28904
3      274
2      274
1      274
0      274
</code></pre>
    <p class="normal">Most of the observations are treatment <code class="inlineCode">-1</code>, so they are not part of the causal inference. The remainder was split evenly between the three treatments (<code class="inlineCode">1-3</code>) and the control group (<code class="inlineCode">0</code>). Naturally, we will use these four groups for the causal inference exercise. However, since the<a id="_idIndexMarker1169"/> control group wasn’t prescribed treatment, we can use it in our bias mitigation exercise along with the <code class="inlineCode">-1</code> treatments. We have to be careful to exclude customers whose behaviors were manipulated in the bias mitigation exercise. The whole point is to predict which customers are most likely to default under “business as usual” circumstances while attempting to reduce bias.</p>
    <h3 id="_idParaDest-305" class="heading-3">Data preparation</h3>
    <p class="normal">Our single data <a id="_idIndexMarker1170"/>preparation step, for now, is to split the datasets, which can be easily done by subsetting the <code class="inlineCode">pandas</code> DataFrames using the <code class="inlineCode">_TREATMENT</code> column. We will create one DataFrame for each exercise with this subsetting: bias mitigation (<code class="inlineCode">ccdefault_bias_df</code>) and causal inference (<code class="inlineCode">ccdefault_causal_df</code>). These can be seen in the following code snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">ccdefault_bias_df = ccdefault_all_df[
    ccdefault_all_df._TREATMENT &lt; <span class="hljs-number">1</span>
]
ccdefault_causal_df =ccdefault_all_df[
    ccdefault_all_df._TREATMENT &gt;= <span class="hljs-number">0</span>
]
</code></pre>
    <p class="normal">We will do a few other data preparation steps in the in-depth sections but, for now, we are good to go to get started!</p>
    <h1 id="_idParaDest-306" class="heading-1">Detecting bias</h1>
    <p class="normal">There <a id="_idIndexMarker1171"/>are many sources of bias in machine learning. As outlined in <em class="chapterRef">Chapter 1</em>, <em class="italic">Interpretation, Interpretability, and Explainability; and Why Does It All Matter?</em>, there are ample sources of bias. Those <a id="_idIndexMarker1172"/>rooted in the <em class="italic">truths</em> that the data represents, such as systemic and structural ones, lead to prejudice bias in the data. There are also biases rooted in the data, such as sample, exclusion, association, and measurement biases. Lastly, there are biases in the insights we derive from data or models we have to be careful with, such as conservatism bias, salience bias, and fundamental attribution error.</p>
    <p class="normal">For this example, to properly disentangle so many bias levels, we ought to connect our data to census data for Taiwan in 2005 and historical lending data split by demographics. Then, using these external datasets, control for credit card contract conditions, as well as gender, income, and other demographic data to ascertain if young people, in particular, were targeted for high-interest credit cards they shouldn’t have qualified for. We would also need to trace the dataset to the authors and consult with them and the domain experts to examine the dataset for bias-related data quality issues. Ideally, these steps would be necessary to validate the hypothesis, but that would be a monumental task requiring several chapters’ worth of explanation.</p>
    <p class="normal">Therefore, in the spirit of expediency, we take the premise of this chapter at face value. That is, due to predatory lending practices, certain age groups are more vulnerable to credit card default, not through any fault of their own. We will also take at face value the quality of the dataset. With these caveats in place, it means that if we find disparities between age groups in the data or any model derived from this data, it can be attributed solely to predatory lending practices.</p>
    <p class="normal">There are also two types of fairness, outlined here:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Procedural fairness</strong>: This <a id="_idIndexMarker1173"/>is about fair or equal treatment. It’s hard to define this term legally because it depends so much on the context.</li>
      <li class="bulletList"><strong class="keyWord">Outcome fairness</strong>: This<a id="_idIndexMarker1174"/> is solely about measuring fair outcomes.</li>
    </ul>
    <p class="normal">These two concepts aren’t mutually exclusive since the procedure may be fair but the outcome unfair, or vice versa. In this example, the unfair <em class="italic">procedure</em> was the offering of high-interest credit cards to unqualified customers. Nevertheless, we are going to focus on outcome fairness in this chapter.</p>
    <p class="normal">When we discuss bias in machine learning, it will impact <em class="italic">protected</em> groups, and within these groups, there will be <em class="italic">privileged</em> and <em class="italic">underprivileged</em> groups. The latter is a group that is adversely impacted by bias. There are also many ways in which bias is manifested, and thus<a id="_idIndexMarker1175"/> addressed, as follows:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Representation</strong>: There<a id="_idIndexMarker1176"/> can be a lack of representation or an overrepresentation of the underprivileged group. The model will learn either too little or too much about this group, compared to others.</li>
      <li class="bulletList"><strong class="keyWord">Distribution</strong>: Differences in the<a id="_idIndexMarker1177"/> distribution of features between groups can lead the model to make biased associations that can impact model outcomes either directly or indirectly.</li>
      <li class="bulletList"><strong class="keyWord">Probability</strong>: For<a id="_idIndexMarker1178"/> classification problems, class balance discrepancies between groups such as those discussed in <em class="chapterRef">Chapter 6</em>, <em class="italic">Anchors and Counterfactual Explanations</em>, can lead to the model learning that one group has a higher probability of being part of one class or another. These can be easily observed through confusion matrices or by comparing their classification metrics, such as false positive or false negative rates.</li>
      <li class="bulletList"><strong class="keyWord">Hybrid</strong>: A <a id="_idIndexMarker1179"/>combination of any of the preceding manifestations.</li>
    </ul>
    <p class="normal">Strategies for any bias manifestation are discussed in the <em class="italic">Mitigating bias</em> section, but the kind we address in the chapter pertains to disparities with probability for our main protected attribute (<code class="inlineCode">_AGE</code>). We will observe this through these means:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Visualizing dataset bias</strong>: Observing disparities in the data for the protected feature through visualizations.</li>
      <li class="bulletList"><strong class="keyWord">Quantifying dataset bias</strong>: Measuring bias using fairness metrics.</li>
      <li class="bulletList"><strong class="keyWord">Quantifying model bias</strong>: We will train a classification model and use other fairness metrics designed for models.</li>
    </ul>
    <p class="normal">Model bias can be visualized, as we have done already in <em class="chapterRef">Chapter 6</em>, <em class="italic">Anchors and Counterfactual Explanations</em>, or as we will do in <em class="chapterRef">Chapter 12</em>, <em class="italic">Monotonic Constraints and Model Tuning for Interpretability</em>. We will quickly explore some other visualizations later in this chapter, in a subsection called <em class="italic">Tying it all together!</em> Without further ado, let’s move on to the practical portion of this section.</p>
    <h2 id="_idParaDest-307" class="heading-2">Visualizing dataset bias</h2>
    <p class="normal">The data <a id="_idIndexMarker1180"/>itself tells the story of how probable it is that one group belongs to a positive class versus another. If it’s a categorical feature, these probabilities can be obtained by dividing the <code class="inlineCode">value_counts()</code> function for the positive class over all classes. For instance, for gender, we could do this:</p>
    <pre class="programlisting code"><code class="hljs-code">ccdefault_bias_df[
    ccdefault_bias_df.<span class="code-highlight"><strong class="hljs-slc">IS_DEFAULT==1</strong></span>
].<span class="code-highlight"><strong class="hljs-slc">GENDER.value_counts()</strong></span>/ccdefault_bias_df.<span class="code-highlight"><strong class="hljs-slc">GENDER.value_counts()</strong></span>
</code></pre>
    <p class="normal">The preceding snippet produces the following output, which shows that males have, on average, a higher probability of defaulting on their credit card:</p>
    <pre class="programlisting con"><code class="hljs-con">2    0.206529
1    0.241633
</code></pre>
    <p class="normal">The code for doing this for a continuous feature is a bit more complicated. It is recommended that you use <code class="inlineCode">pandas</code>' <code class="inlineCode">qcut</code> to divide the feature into quartiles first and then use the same approach used for categorical features. Fortunately, the <code class="inlineCode">plot_prob_progression</code> function does this for you and plots the progression of probabilities for each quartile. The first attribute is a <code class="inlineCode">pandas</code> series, an array or list with the protected feature (<code class="inlineCode">_AGE</code>), and the second is the same but for the target feature (<code class="inlineCode">IS_DEFAULT</code>). We then choose the number of intervals (<code class="inlineCode">x_intervals</code>) that we are setting as quartiles (<code class="inlineCode">use_quantiles=True</code>). </p>
    <p class="normal">The rest of the attributes are aesthetic, such as the label, title, and adding a <code class="inlineCode">mean_line</code>. The code can be seen in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">mldatasets.<span class="code-highlight"><strong class="hljs-slc">plot_prob_progression(</strong></span>
    ccdefault_bias_df.<span class="code-highlight"><strong class="hljs-slc">_AGE</strong></span>,
    ccdefault_bias_df.<span class="code-highlight"><strong class="hljs-slc">IS_DEFAULT</strong></span>,
    <span class="code-highlight"><strong class="hljs-slc">x_intervals</strong></span>=<span class="hljs-number">8</span>,
    use_quantiles=<span class="hljs-literal">True</span>,
    xlabel=<span class="hljs-string">'Age'</span>,
    <span class="code-highlight"><strong class="hljs-slc">mean_line</strong></span>=<span class="hljs-literal">True</span>,
    title=<span class="hljs-string">'Probability of Default by Age'</span>
)
</code></pre>
    <p class="normal">The preceding code produced the following output, which depicts how the youngest (<code class="inlineCode">21-25</code>) and oldest (<code class="inlineCode">47-79</code>) are most likely to default. All other groups represent just over one standard deviation from the mean:</p>
    <p class="packt_figref"><img src="../Images/B18406_11_01.png" alt="Chart, line chart  Description automatically generated"/></p>
    <p class="packt_figref">Figure 11.1: Probability of CC default by _AGE</p>
    <p class="normal">We can call <a id="_idIndexMarker1181"/>the youngest and oldest quartiles the underprivileged group and all others the privileged group. In order to detect and mitigate unfairness, it is best to code them as a binary feature—and we have done just that with <code class="inlineCode">AGE_GROUP</code>. We can leverage <code class="inlineCode">plot_prob_progression</code> again, but this time with <code class="inlineCode">AGE_GROUP</code> instead of <code class="inlineCode">AGE</code>, and we will <code class="inlineCode">replace</code> the numbers with labels we can interpret more easily. The code can be seen in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">mldatasets.<span class="code-highlight"><strong class="hljs-slc">plot_prob_progression(</strong></span>
    ccdefault_bias_df.<span class="code-highlight"><strong class="hljs-slc">AGE_GROUP</strong></span>.<span class="code-highlight"><strong class="hljs-slc">replace</strong></span>({<span class="hljs-number">0</span>:<span class="hljs-string">'</span><span class="hljs-string">21-25,48+'</span>,<span class="hljs-number">1</span>:<span class="hljs-string">'26-47'</span>}),
    ccdefault_bias_df.<span class="code-highlight"><strong class="hljs-slc">IS_DEFAULT</strong></span>,
    xlabel=<span class="hljs-string">'Age Group'</span>,
    title=<span class="hljs-string">'Probability of Default by Age Group'</span>,
    <span class="code-highlight"><strong class="hljs-slc">mean_line</strong></span>=<span class="hljs-literal">True</span>
)
</code></pre>
    <p class="normal">The preceding snippet produced the following output, in which the disparities between both groups are pretty evident:</p>
    <p class="packt_figref"><img src="../Images/B18406_11_02.png" alt="Chart, line chart  Description automatically generated"/></p>
    <p class="packt_figref">Figure 11.2: Probability of CC default by AGE_GROUP</p>
    <p class="normal">Next, let’s <a id="_idIndexMarker1182"/>bring <code class="inlineCode">GENDER</code> back into the picture. We can employ <code class="inlineCode">plot_prob_contour_map</code>, which is like <code class="inlineCode">plot_prob_progression</code> but in two dimensions, color-coding the probabilities instead of drawing a line. So, the first two attributes are the features we want on the <em class="italic">x</em>-axis (<code class="inlineCode">GENDER</code>) and <em class="italic">y</em>-axis (<code class="inlineCode">AGE_GROUP</code>), and the third is the target (<code class="inlineCode">IS_DEFAULT</code>). Since both our features are binary, it is best to use <code class="inlineCode">plot_type='grid'</code> as opposed to <code class="inlineCode">contour</code>. The code can be seen in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">mldatasets.plot_prob_contour_map(
    ccdefault_bias_df.<span class="code-highlight"><strong class="hljs-slc">GENDER</strong></span>.replace({<span class="hljs-number">1</span>:<span class="hljs-string">'Male'</span>,<span class="hljs-number">2</span>:<span class="hljs-string">'Female'</span>}),
    ccdefault_bias_df.<span class="code-highlight"><strong class="hljs-slc">AGE_GROUP</strong></span>.replace({<span class="hljs-number">0</span>:<span class="hljs-string">'21-25,48+'</span>,<span class="hljs-number">1</span>:<span class="hljs-string">'26-47'</span>}),
    ccdefault_bias_df.<span class="code-highlight"><strong class="hljs-slc">IS_DEFAULT</strong></span>,
    xlabel=<span class="hljs-string">'Gender'</span>,
    ylabel=<span class="hljs-string">'Age Group'</span>,
    annotate=<span class="hljs-literal">True</span>,
    plot_type=<span class="hljs-string">'grid'</span>,
    title=<span class="hljs-string">'Probability of Default by Gender/Age Group'</span>
)
</code></pre>
    <p class="normal">The preceding snippet generates the following output. It is immediately evident how the most privileged <a id="_idIndexMarker1183"/>group is 26- 47-year-old females, followed by their male counterparts at about 3-4% apart. The same happens with the underprivileged age group:</p>
    <p class="packt_figref"><img src="../Images/B18406_11_03.png" alt="Chart, treemap chart  Description automatically generated"/></p>
    <p class="packt_figref">Figure 11.3: Probability grid of CC default by GENDER and AGE_GROUP</p>
    <p class="normal">The gender difference is an interesting observation, and we could present a number of hypotheses as to why females default less. Are they just simply better at managing debt? Does<a id="_idIndexMarker1184"/> it have to do with their marital status or education? We won’t dig deeper into these questions. Given that we only know of age-based discrimination, we will only use <code class="inlineCode">AGE_GROUP</code> in privilege groups but keep <code class="inlineCode">GENDER</code> a protected attribute, which will be factored in some fairness metrics we will monitor. Speaking of metrics, we will quantify dataset bias next.</p>
    <h2 id="_idParaDest-308" class="heading-2">Quantifying dataset bias</h2>
    <p class="normal">There <a id="_idIndexMarker1185"/>are three categories of fairness metrics, outlined here:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Individual fairness</strong>: How close individual observations are to their peers in the data. Distance metrics such as <em class="italic">Euclidean</em> and <em class="italic">Manhattan distance</em> can serve this purpose.</li>
      <li class="bulletList"><strong class="keyWord">Group fairness</strong>: How labels or outcomes between groups are, on average, distant from each other. This can be measured either in the data or for a model.</li>
      <li class="bulletList"><strong class="keyWord">Both</strong>: A few metrics measure entropy or variance by factoring inequality both in-group and between groups, such as the <em class="italic">Theil index</em> and the <em class="italic">coefficient of variation</em>.</li>
    </ul>
    <p class="normal">We will focus exclusively on group fairness metrics in this chapter.</p>
    <p class="normal">Before we compute fairness metrics, there are a few pending data preparation steps. Let’s make sure the dataset we will use for the bias mitigation exercise (<code class="inlineCode">ccdefault_bias_df</code>) only has the pertinent columns, which are those that don’t begin with an underscore (<code class="inlineCode">"_"</code>). On the other hand, the causal inference exercise will include only the underscored columns plus <code class="inlineCode">AGE_GROUP</code> and <code class="inlineCode">IS_DEFAULT</code>. The code can be seen in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">cols_bias_l = ccdefault_all_df.columns[
    <span class="code-highlight"><strong class="hljs-slc">~ccdefault_all_df.columns.str.startswith('_')</strong></span>
].tolist()
cols_causal_l = [<span class="code-highlight"><strong class="hljs-slc">'AGE_GROUP'</strong></span>,<span class="code-highlight"><strong class="hljs-slc">'IS_DEFAULT'</strong></span>] +\
    ccdefault_all_df.columns[
        <span class="code-highlight"><strong class="hljs-slc">ccdefault_all_df.columns.str.startswith('_')</strong></span>
    ].tolist()
ccdefault_bias_df = ccdefault_bias_df[<span class="code-highlight"><strong class="hljs-slc">cols_bias_l</strong></span>]
ccdefault_causal_df = ccdefault_causal_df[<span class="code-highlight"><strong class="hljs-slc">cols_causal_l</strong></span>]
</code></pre>
    <p class="normal">Also, it’s more important to quantify dataset bias on the training data because that is the data the model will learn from, so let’s go ahead and split the data into train and test <code class="inlineCode">X</code> and <code class="inlineCode">y</code> pairs. We do this after we have, of course, initialized the random seed to aim for some reproducibility. The code can be seen in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">rand = <span class="hljs-number">9</span>
os.environ[<span class="hljs-string">'PYTHONHASHSEED'</span>]=<span class="hljs-built_in">str</span>(rand)
np.random.seed(rand)
y = ccdefault_bias_df[<span class="code-highlight"><strong class="hljs-slc">'IS_DEFAULT'</strong></span>]
X = ccdefault_bias_df.drop([<span class="code-highlight"><strong class="hljs-slc">'IS_DEFAULT'</strong></span>], axis=<span class="hljs-number">1</span>).copy()
X_train, X_test, y_train, y_test = model_selection.<span class="code-highlight"><strong class="hljs-slc">train_test_split</strong></span>(
    X, y, test_size=<span class="hljs-number">0.25</span>, random_state=rand
)
</code></pre>
    <p class="normal">Even though<a id="_idIndexMarker1186"/> we will use the <code class="inlineCode">pandas</code> data we just split for training and performance evaluation, the library we will use for this exercise, called AIF360, abstracts datasets into base classes. These classes include the data converted to a <code class="inlineCode">numpy</code> array and store attributes related to fairness. </p>
    <p class="normal">For regression, AIF360 has <code class="inlineCode">RegressionDataset</code>, but for this binary classification example, we will use <code class="inlineCode">BinaryLabelDataset</code>. You can initialize it with the <code class="inlineCode">pandas</code> DataFrame with both features and labels (<code class="inlineCode">X_train.join(y_train)</code>). Then, you specify the name of the label (<code class="inlineCode">label_names</code>) and protected attributes (<code class="inlineCode">protected_attribute_names</code>), and it is recommended that you enter a value for <code class="inlineCode">favorable_label</code> and <code class="inlineCode">unfavorable_label</code>, which tells AIF360 which label values are preferred so that it factors them into how it assesses fairness. As confusing as it may seem, positive and, in contrast, negative in binary classification only pertain to what we are trying to predict—the positive class—and not whether it is a favorable outcome. The code can be seen in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">train_ds = <span class="code-highlight"><strong class="hljs-slc">BinaryLabelDataset</strong></span>(
    df=<span class="code-highlight"><strong class="hljs-slc">X_train.join(y_train)</strong></span>,
    label_names=[<span class="hljs-string">'IS_DEFAULT'</span>],
    protected_attribute_names=[<span class="hljs-string">'AGE_GROUP'</span>, <span class="hljs-string">'GENDER'</span>],
    favorable_label=<span class="hljs-number">0</span>,
    unfavorable_label=<span class="hljs-number">1</span>
)
test_ds = <span class="code-highlight"><strong class="hljs-slc">BinaryLabelDataset</strong></span>(
    <span class="code-highlight"><strong class="hljs-slc">df=X_test.join(y_test)</strong></span>,
    label_names=[<span class="hljs-string">'IS_DEFAULT'</span>],
    protected_attribute_names=[<span class="hljs-string">'AGE_GROUP'</span>, <span class="hljs-string">'GENDER'</span>],
    favorable_label=<span class="hljs-number">0</span>, unfavorable_label=<span class="hljs-number">1</span>
)
</code></pre>
    <p class="normal">Next, we create arrays for <code class="inlineCode">underprivileged groups</code> and <code class="inlineCode">privileged_groups</code>. Those in <code class="inlineCode">AGE_GROUP=1</code> have a lower probability of default, so they are privileged, and vice versa. Then, with these and the abstracted dataset for training (<code class="inlineCode">train_ds</code>), we can initialize a metrics class via <code class="inlineCode">BinaryLabelDatasetMetric</code>. This class has functions for computing several group fairness metrics, judging the data alone. We will output three of them and then explain<a id="_idIndexMarker1187"/> what they mean. The code can be seen in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">underprivileged_groups=[<span class="code-highlight"><strong class="hljs-slc">{'AGE_GROUP': 0}</strong></span>]
privileged_groups=[<span class="code-highlight"><strong class="hljs-slc">{'AGE_GROUP': 1}</strong></span>]
metrics_train_ds = <span class="code-highlight"><strong class="hljs-slc">BinaryLabelDatasetMetric</strong></span>(
    train_ds,
    unprivileged_groups=underprivileged_groups,
    privileged_groups=privileged_groups
)
<span class="hljs-built_in">print</span>(<span class="hljs-string">'Statistical Parity Difference (SPD): %.4f'</span> %
      metrics_train_ds.<span class="code-highlight"><strong class="hljs-slc">statistical_parity_difference</strong></span>())
<span class="hljs-built_in">print</span>(<span class="hljs-string">'Disparate Impact (DI): %.4f'</span> % 
      metrics_train_ds.<span class="code-highlight"><strong class="hljs-slc">disparate_impact</strong></span>())
<span class="hljs-built_in">print</span>(<span class="hljs-string">'Smoothed Empirical Differential Fairness (SEDF): %.4f'</span> %\
      metrics_train_ds.<span class="code-highlight"><strong class="hljs-slc">smoothed_empirical_differential_fairness</strong></span>())
</code></pre>
    <p class="normal">The preceding snippet generates the following output:</p>
    <pre class="programlisting con"><code class="hljs-con">Statistical Parity Difference (SPD):               -0.0437
Disparate Impact (DI):                              0.9447
Smoothed Empirical Differential Fairness (SEDF):    0.3514
</code></pre>
    <p class="normal">Now, let’s explain what each metric means, as follows:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Statistical Parity Difference</strong> (<strong class="keyWord">SPD</strong>): Also known as<a id="_idIndexMarker1188"/> the <strong class="keyWord">mean difference</strong>, this<a id="_idIndexMarker1189"/> is the difference between the mean probability of favorable outcomes between underprivileged and privileged groups. A negative number represents unfairness to the underprivileged group and a positive number is better, yet a number closer to zero represents a fair outcome with no significant difference between the privileged and underprivileged groups. It’s computed with the following formula, where <em class="italic">f</em> is the value for the favorable class, <em class="italic">D</em> is the group of the customer, and <em class="italic">Y</em> is whether the customer will default or not:</li>
    </ul>
    <p class="center"><img src="../Images/B18406_11_001.png" alt="" role="presentation"/></p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Disparate Impact</strong> (<strong class="keyWord">DI</strong>): DI is <a id="_idIndexMarker1190"/>exactly like SPD except it’s the ratio, not the difference. And, as ratios go, the closer to one the better for the underprivileged group. In other words, one represents a fair outcome between groups with no difference, below one means unfavorable outcomes to the underprivileged group compared to the privileged group, and over one means favorable outcomes to the underprivileged group compared to the privileged group. The formula is shown here:</li>
    </ul>
    <p class="center"><img src="../Images/B18406_11_002.png" alt="" role="presentation"/></p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Smoothed Empirical Differential Fairness</strong> (<strong class="keyWord">SEDF</strong>): This fairness metric is one of the many newer <a id="_idIndexMarker1191"/>ones from a paper called <em class="italic">“An Intersectional Definition of Fairness.”</em> Unlike the previous two metrics, it’s not restricted to the predetermined privileged and underprivileged groups, but it’s extended to include all the categories in the protected attributes—in this case, the four in <em class="italic">Figure 11.3</em>. The authors of the paper argue that fairness is particularly tricky when you have a crosstab of protected attributes. This occurs<a id="_idIndexMarker1192"/> because of <strong class="keyWord">Simpson’s paradox</strong>, which is that one group can be advantaged or disadvantaged in aggregate but not when subdivided into crosstabs. We won’t get<a id="_idIndexMarker1193"/> into the math, but their method accounts for this possibility while measuring a sensible level of fairness in intersectional scenarios. To interpret it, zero represents absolute fairness, and the farther from zero it is, the less fair it is.</li>
    </ul>
    <p class="normal">Next, we will quantify group fairness metrics for a model.</p>
    <h2 id="_idParaDest-309" class="heading-2">Quantifying model bias</h2>
    <p class="normal">Before<a id="_idIndexMarker1194"/> we compute metrics, we will need to train a model. To that end, we will initialize a LightGBM classifier (<code class="inlineCode">LGBMClassifier</code>) with optimal hyperparameters (<code class="inlineCode">lgb_params</code>). These have already been hyperparameter-tuned for us (more details on how to do this in <em class="chapterRef">Chapter 12</em>, <em class="italic">Monotonic Constraints and Model Tuning for Interpretability</em>). </p>
    <p class="normal">Please note that these parameters include <code class="inlineCode">scale_pos_weight</code>, which is for class weighting. Since this is an unbalanced classification task, this is an essential parameter to leverage so that the classifier is cost-sensitive-trained, penalizing one form of misclassification over another. Once the classifier is initialized, it is <code class="inlineCode">fit</code> and evaluated with <code class="inlineCode">evaluate_class_mdl</code>, which returns a dictionary with predictive performance metrics that we can store in a model dictionary (<code class="inlineCode">cls_mdls</code>). The code can be seen in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">cls_mdls = {}
lgb_params = {
    <span class="hljs-string">'learning_rate'</span>: <span class="hljs-number">0.4</span>,
    <span class="hljs-string">'reg_alpha'</span>: <span class="hljs-number">21</span>,
    <span class="hljs-string">'reg_lambda'</span>: <span class="hljs-number">1</span>,
    <span class="code-highlight"><strong class="hljs-slc">'scale_pos_weight'</strong></span>: <span class="hljs-number">1.8</span>
}
lgb_base_mdl = lgb.LGBMClassifier(
    random_seed=rand,
    max_depth=<span class="hljs-number">6</span>,
    num_leaves=<span class="hljs-number">33</span>,
    **lgb_params
)
lgb_base_mdl.fit(X_train, y_train)
<span class="code-highlight"><strong class="hljs-slc">cls_mdls</strong></span>[<span class="hljs-string">'lgb_0_base'</span>] = mldatasets.<span class="code-highlight"><strong class="hljs-slc">evaluate_class_mdl</strong></span>(
    lgb_base_mdl,
    X_train,
    X_test,
    y_train,
    y_test,
    plot_roc=<span class="hljs-literal">False</span>,
    plot_conf_matrix=<span class="hljs-literal">True</span>,
    show_summary=<span class="hljs-literal">True</span>,
    ret_eval_dict=<span class="hljs-literal">True</span>
)
</code></pre>
    <p class="normal">The preceding snippet of code outputs <em class="italic">Figure 11.4</em>. The <code class="inlineCode">scale_pos_weight</code> parameter ensures a healthier balance between false positives in the top-right corner and false negatives at the bottom left. As a result, precision and recall aren’t too far off from each other. We favor high precision for a problem such as this one because we want to maximize true positives, but, not at the great expense of recall, so a balance between both is critical. While hyperparameter tuning, the <code class="inlineCode">F1</code> score, and the <strong class="keyWord">Matthews correlation coefficient</strong> (<strong class="keyWord">MCC</strong>) are<a id="_idIndexMarker1195"/> useful metrics to use to this end. The evaluation of the LightGBM base model is shown here:</p>
    <figure class="mediaobject"><img src="../Images/B18406_11_04.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 11.4: Evaluation of the LightGBM base model</p>
    <p class="normal">Next, let’s <a id="_idIndexMarker1196"/>compute the fairness metrics for the model. To do this, we need to make a “deep” copy (<code class="inlineCode">deepcopy=True</code>) of the AIF360 dataset, but we change the <code class="inlineCode">labels</code> and <code class="inlineCode">scores</code> to be those predicted by our model. The <code class="inlineCode">compute_aif_metrics</code> function employs the <code class="inlineCode">ClassificationMetric</code> class of AIF360 to do for the model what <code class="inlineCode">BinaryLabelDatasetMetric</code> did for the dataset. However, it doesn’t engage with the model directly. It computes fairness using the original dataset (<code class="inlineCode">test_ds</code>) and the modified one with the model’s predictions (<code class="inlineCode">test_pred_ds</code>). The <code class="inlineCode">compute_aif_metrics</code> function creates a dictionary with several precalculated metrics (<code class="inlineCode">metrics_test_dict</code>) and the metric class (<code class="inlineCode">metrics_test_cls</code>), which can be used to obtain metrics one by one. The code can be seen in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">test_pred_ds = test_ds.copy(deepcopy=<span class="hljs-literal">True</span>)
test_pred_ds.labels =\
    cls_mdls[<span class="hljs-string">'lgb_0_base'</span>][<span class="hljs-string">'preds_test'</span>].reshape(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)
test_pred_ds.scores = \
    cls_mdls[<span class="hljs-string">'lgb_0_base'</span>][<span class="hljs-string">'probs_test'</span>].reshape(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)
metrics_test_dict, metrics_test_cls = \
    mldatasets.<span class="code-highlight"><strong class="hljs-slc">compute_aif_metrics</strong></span>(
        test_ds,
        test_pred_ds,
        unprivileged_groups=underprivileged_groups,
        privileged_groups=privileged_groups
    )
cls_mdls[<span class="hljs-string">'lgb_0_base'</span>].<span class="code-highlight"><strong class="hljs-slc">update</strong></span>(metrics_test_dict)
<span class="hljs-built_in">print</span>(<span class="hljs-string">'Statistical Parity Difference (SPD): %.4f'</span> %
      metrics_test_cls.<span class="code-highlight"><strong class="hljs-slc">statistical_parity_difference</strong></span>())
<span class="hljs-built_in">print</span>(<span class="hljs-string">'Disparate Impact (DI): %.4f'</span> %
      metrics_test_cls.<span class="code-highlight"><strong class="hljs-slc">disparate_impact</strong></span>())
<span class="hljs-built_in">print</span>(<span class="hljs-string">'Average Odds Difference (AOD): %.4f'</span> %
      metrics_test_cls.<span class="code-highlight"><strong class="hljs-slc">average_odds_difference</strong></span>())
<span class="hljs-built_in">print</span>(<span class="hljs-string">'Equal Opportunity Difference (EOD): %.4f'</span> %
      metrics_test_cls.<span class="code-highlight"><strong class="hljs-slc">equal_opportunity_difference</strong></span>())
print('Differential Fairness Bias Amplification(DFBA): %.4f' % \
    metrics_test_cls.<span class="code-highlight"><strong class="hljs-slc">differential_fairness_bias_amplification</strong></span>())
</code></pre>
    <p class="normal">The preceding <a id="_idIndexMarker1197"/>snippet generates the following output:</p>
    <pre class="programlisting con"><code class="hljs-con">Statistical Parity Difference (SPD):               -0.0679
Disparate Impact (DI):                                      0.9193
Average Odds Difference (AOD):                        -0.0550
Equal Opportunity Difference (EOD):                 -0.0265
Differential Fairness Bias Amplification (DFBA):    0.2328
</code></pre>
    <p class="normal">Now, putting the metrics we already explained aside, let’s explain what the other ones mean, as follows:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Average Odds Difference</strong> (<strong class="keyWord">AOD</strong>): The<a id="_idIndexMarker1198"/> difference between <strong class="keyWord">False-Positive Rates</strong> (<strong class="keyWord">FPR</strong>) averaged <a id="_idIndexMarker1199"/>with the difference <a id="_idIndexMarker1200"/>between <strong class="keyWord">False-Negative Rates</strong> (<strong class="keyWord">FNR</strong>) for both privileged and underprivileged groups. Negative means there’s a disadvantage for the underprivileged group, and the closer to zero, the better. The formula is shown here:</li>
    </ul>
    <p class="normal"><img src="../Images/B18406_11_003.png" alt="" role="presentation"/></p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Equal Opportunity Difference</strong> (<strong class="keyWord">EOD</strong>): It’s<a id="_idIndexMarker1201"/> only the <strong class="keyWord">True Positive Rate</strong> (<strong class="keyWord">TPR</strong>) differences <a id="_idIndexMarker1202"/>of AOD, so it’s only useful to measure the <em class="italic">opportunity</em> for TPRs. As with<a id="_idIndexMarker1203"/> AOD, negative confirms a disadvantage for the underprivileged group, and the closer the value is to zero means there is no significant difference between groups. The formula is shown here:</li>
    </ul>
    <p class="normal"><img src="../Images/B18406_11_004.png" alt="" role="presentation"/></p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Differential Fairness Bias Amplification</strong> (<strong class="keyWord">DFBA</strong>): This <a id="_idIndexMarker1204"/>metric was defined in the same paper as SEDF, and similarly has zero as the baseline of fairness and is also intersectional. However, it only measures the difference in unfairness in proportion between the model and the data in a phenomenon called bias amplification. In other words, the value represents how much more the model increases unfairness compared to the original data.</li>
    </ul>
    <p class="normal">If you compare the model’s <code class="inlineCode">SPD</code> and <code class="inlineCode">DI</code> metrics to that of the data, they are indeed worse. No surprise there, because it’s to be expected since model-learned representations tend to amplify bias. You can confirm this with the <code class="inlineCode">DFBA</code> metrics. As for <code class="inlineCode">AOD</code> and <code class="inlineCode">EOD</code>, they tend to be in the same neighborhood as the <code class="inlineCode">SPD</code> metrics, but ideally, the <code class="inlineCode">EOD</code> metric is substantially closer to zero than the <code class="inlineCode">AOD</code> metric because we care more about TPRs in this example.</p>
    <p class="normal">Next, we will go over methods to mitigate bias in the model.</p>
    <h1 id="_idParaDest-310" class="heading-1">Mitigating bias</h1>
    <p class="normal">We can <a id="_idIndexMarker1205"/>mitigate<a id="_idIndexMarker1206"/> bias at three different levels with methods that operate at these individual levels:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Preprocessing</strong>: These <a id="_idIndexMarker1207"/>are interventions to detect and remove bias from the training <strong class="keyWord">data</strong> before training the model. Methods that leverage pre-processing have the advantage that they tackle bias at the source. On the other hand, any undetected bias could still be amplified by the model.</li>
      <li class="bulletList"><strong class="keyWord">In-processing</strong>: These<a id="_idIndexMarker1208"/> methods mitigate bias during the <strong class="keyWord">model training</strong> and are, therefore, highly dependent on the model and tend to not be model-agnostic like the pre-processing and post-processing methods. They also require hyperparameter tuning to calibrate fairness metrics.</li>
      <li class="bulletList"><strong class="keyWord">Post-processing</strong>: These <a id="_idIndexMarker1209"/>methods mitigate bias during <strong class="keyWord">model inference</strong>. In <em class="chapterRef">Chapter 6</em>, <em class="italic">Anchors and Counterfactual Explanations</em>, we touched on the subject of using the What-If tool to choose the right thresholds (see <em class="italic">Figure 6.13</em> in that chapter), and we manually adjusted them to achieve parity with false positives. Just as we did then, post-processing methods aim to detect and correct fairness directly in the outcomes, but what adjustments to make will depend on which metrics matter most to your problem. Post-processing methods have the advantage that they can tackle outcome unfairness where it can have the greatest impact, but since it’s disconnected from the rest of the model development, it can distort things.</li>
    </ul>
    <p class="normal">Please note <a id="_idIndexMarker1210"/>that bias mitigation methods can hurt predictive performance, so there’s often a trade-off. There can be opposing goals, especially in cases where the data is reflective of a biased truth. We can choose to aim for a better truth instead: a righteous one—<em class="italic">the one we want, not the one we have</em>.</p>
    <p class="normal">This section will explain several methods for each level but will only implement and evaluate two for each. Also, we won’t do it in this chapter, but you can combine different kinds of methods to maximize mitigation—for instance, you could use a preprocessing method to de-bias the data, then train a model with it, and lastly, use a post-processing method to remove bias added by the model.</p>
    <h2 id="_idParaDest-311" class="heading-2">Preprocessing bias mitigation methods</h2>
    <p class="normal">These are<a id="_idIndexMarker1211"/> some of the most important preprocessing or data-specific bias mitigation methods:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Unawareness</strong>: Also <a id="_idIndexMarker1212"/>known<a id="_idIndexMarker1213"/> as <strong class="keyWord">suppression</strong>. The most straightforward way to remove bias is to exclude biased features from the dataset, but it’s a naïve approach because you assume that bias is strictly contained in those features.</li>
      <li class="bulletList"><strong class="keyWord">Feature engineering</strong>: Sometimes, continuous <a id="_idIndexMarker1214"/>features capture bias because there are so many sparse areas where the model can fill voids with assumptions or learn from outliers. It can do the same with interactions. Feature engineering can place guardrails. We will discuss this topic in <em class="chapterRef">Chapter 12</em>, <em class="italic">Monotonic Constraints and Model Tuning for Interpretability</em>.</li>
      <li class="bulletList"><strong class="keyWord">Balancing</strong>: Also <a id="_idIndexMarker1215"/>known as <strong class="keyWord">resampling</strong>. On<a id="_idIndexMarker1216"/> their own, representation problems are relatively easy to fix by balancing the dataset. The XAI library (<a href="https://github.com/EthicalML/xai"><span class="url">https://github.com/EthicalML/xai</span></a>) has a <code class="inlineCode">balance</code> function that does this by random downsampling and upsampling of group representations. Downsampling, or under-sampling, is what we typically call sampling, which is just taking a certain percentage of the observations, whereas upsampling, or over-sampling, creates a certain percentage of random duplicates. Some strategies synthetically upsample rather <a id="_idIndexMarker1217"/>than duplicate, such as the <strong class="keyWord">Synthetic Minority Oversampling TEchnique</strong> (<strong class="keyWord">SMOTE</strong>). However, we must caution that it’s always preferable to downsample than upsample if you have enough data. It’s best not to use only the balancing strategy if there are other possible bias problems.</li>
      <li class="bulletList"><strong class="keyWord">Relabeling</strong>: Also <a id="_idIndexMarker1218"/>known as <strong class="keyWord">massaging</strong>, this <a id="_idIndexMarker1219"/>is having an algorithm change the labels for observations that appear to be most biased, resulting in <em class="italic">massaged data</em> by ranking them. Usually, this is performed with a Naïve-Bayes classifier, and to maintain class<a id="_idIndexMarker1220"/> distribution, it not only promotes some observations but demotes an equal amount.</li>
      <li class="bulletList"><strong class="keyWord">Reweighing</strong>: This <a id="_idIndexMarker1221"/>method similarly ranks observations as relabeling does, but instead of flipping their labels it derives a weight for each one, which we can then implement in the learning process. Much like class weights are applied to each class, sample weights are applied to each observation or sample. Many regressors and classifiers, <code class="inlineCode">LGBMClassifier</code> included, support sample weights. Even though, technically, reweighting doesn’t touch the data and solution applied to the model, it is a preprocessing method because we detected bias in the data.</li>
      <li class="bulletList"><strong class="keyWord">Disparate impact remover</strong>: The <a id="_idIndexMarker1222"/>authors of this method were very careful to abide by legal definitions of bias and preserve the integrity of the data without changing the labels or the protected attributes. It implements a repair process that attempts to remove bias in the remaining features. It’s an excellent process to use whenever we suspect that’s where most of the bias is located—that is, the features are highly correlated with the protected attributes, but it doesn’t address bias elsewhere. In any case, it’s a good baseline to use to understand how much of the bias is non-protected features.</li>
      <li class="bulletList"><strong class="keyWord">Learning fair representations</strong>: This leverages an adversarial learning framework. There’s<a id="_idIndexMarker1223"/> a generator (autoencoder) that creates representations of the data excluding the protected attribute, and a critic whose goal is that the learned representations within privileged and underprivileged groups are as close as possible.</li>
      <li class="bulletList"><strong class="keyWord">Optimized preprocessing for discrimination prevention</strong>: This method produces <a id="_idIndexMarker1224"/>transformations through mathematical optimization of the data in such a way that overall probability distributions are maintained. At the same time, the correlation between protected attributes and the target is nullified. The result of this process is data that is distorted slightly to de-bias it.</li>
    </ul>
    <p class="normal">Given that <a id="_idIndexMarker1225"/>there are so many pre-processing methods, we will only employ two of them in this chapter. Still, if you are interested in using the ones we won’t cover, they are available in the AIF360 library, and you can read about them in their documentation (<a href="https://aif360.res.ibm.com/"><span class="url">https://aif360.res.ibm.com/</span></a>).</p>
    <h3 id="_idParaDest-312" class="heading-3">The Reweighing method</h3>
    <p class="normal">The <code class="inlineCode">Reweighing</code> method<a id="_idIndexMarker1226"/> is fairly simple to implement. You initialize it by specifying the groups, then <code class="inlineCode">fit</code> and <code class="inlineCode">transform</code> the data as you would with any scikit-learn encoder or scaler. For those that aren’t familiar with <code class="inlineCode">fit</code>, the algorithm learns how to transform the provided data, and <code class="inlineCode">transform</code> uses what was learned to transform it. The code can be seen in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">reweighter= <span class="code-highlight"><strong class="hljs-slc">Reweighing</strong></span>(
    unprivileged_groups=underprivileged_groups,
    privileged_groups=privileged_groups
)
reweighter.<span class="code-highlight"><strong class="hljs-slc">fit</strong></span>(train_ds)
train_rw_ds = reweighter.<span class="code-highlight"><strong class="hljs-slc">transform</strong></span>(train_ds)
</code></pre>
    <p class="normal">The transformation derived from this process doesn’t change the data but creates weights for each observation. The AIF360 library is equipped to factor these weights into the calculations of fairness, so we can use <code class="inlineCode">BinaryLabelDatasetMetric</code>, as we have before, to compute different metrics. The code can be seen in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">metrics_train_rw_ds = <span class="code-highlight"><strong class="hljs-slc">BinaryLabelDatasetMetric</strong></span>(
    train_rw_ds,
    unprivileged_groups=underprivileged_groups,
    privileged_groups=privileged_groups
)
<span class="hljs-built_in">print</span>(<span class="hljs-string">'Statistical Parity Difference (SPD): %.4f'</span> %
      metrics_train_rw_ds.<span class="code-highlight"><strong class="hljs-slc">statistical_parity_difference</strong></span>())
<span class="hljs-built_in">print</span>(<span class="hljs-string">'Disparate Impact (DI): %.4f'</span> %
       metrics_train_rw_ds.<span class="code-highlight"><strong class="hljs-slc">disparate_impact</strong></span>())
<span class="hljs-built_in">print</span>(<span class="hljs-string">'Smoothed Empirical Differential Fairness(SEDF): %.4f'</span>%
metrics_train_rw_ds.<span class="code-highlight"><strong class="hljs-slc">smoothed_empirical_differential_fairness</strong></span>())
</code></pre>
    <p class="normal">The preceding code outputs the following:</p>
    <pre class="programlisting con"><code class="hljs-con">Statistical Parity Difference (SPD):                    -0.0000
Disparate Impact (DI):                                   1.0000
Smoothed Empirical Differential Fairness (SEDF):    0.1942
</code></pre>
    <p class="normal">The weights have a perfect effect on SPD and DI, making them absolutely fair from those metrics’ standpoints. However, note that SEDF is better than before, but not zero. This is because privileged and underprivileged groups only pertain to the <code class="inlineCode">AGE_GROUP</code> protected attribute, but not <code class="inlineCode">GENDER</code>. SEDF is a measure of intersectional fairness that reweighting does not address.</p>
    <p class="normal">You <a id="_idIndexMarker1227"/>would think that adding weights to observations would adversely impact predictive performance. However, this method was designed to maintain balance. In an unweighted dataset, all observations have a weight of one, and therefore the average of all the weights is one. While reweighting changes the weights for observations, the mean is still approximately one. You can check this is the case by taking the absolute difference in the mean of <code class="inlineCode">instance_weights</code> between the original dataset and the reweighted one. It should be infinitesimal. The code can be seen in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">np.<span class="code-highlight"><strong class="hljs-slc">abs</strong></span>(train_ds.<span class="code-highlight"><strong class="hljs-slc">instance_weights.mean</strong></span>() -\
       train_rw_ds.<span class="code-highlight"><strong class="hljs-slc">instance_weights.mean</strong></span>()) &lt; <span class="hljs-number">1e-6</span>
</code></pre>
    <p class="normal">So, how can you apply <code class="inlineCode">instance_weights</code>?, you ask. Many model classes have a lesser-known attribute in the <code class="inlineCode">fit</code> method, called <code class="inlineCode">sample_weight</code>. You simply plug it in there, and while training, it will learn from observations in accordance with the respective weights. This method is shown in the following code snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">lgb_rw_mdl = lgb.LGBMClassifier(
    random_seed=rand,
    max_depth=<span class="hljs-number">6</span>,
    num_leaves=<span class="hljs-number">33</span>,
    **lgb_params
)
lgb_rw_mdl.fit(
    X_train,
    y_train,
    <span class="code-highlight"><strong class="hljs-slc">sample_weight</strong></span>=train_rw_ds.instance_weights
)
</code></pre>
    <p class="normal">We can evaluate this model as we have with the base model, with <code class="inlineCode">evaluate_class_mdl</code>. However, when we calculate the fairness metrics with <code class="inlineCode">compute_aif_metrics</code>, we will save them in the model dictionary. Instead of looking at each method’s outcomes one by one, we <a id="_idIndexMarker1228"/>will compare them at the end of the section. Have a look at the following code snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">cls_mdls[<span class="hljs-string">'lgb_1_rw'</span>] = mldatasets.<span class="code-highlight"><strong class="hljs-slc">evaluate_class_mdl</strong></span>(
    lgb_rw_mdl,
    train_rw_ds.features,
    X_test,
    train_rw_ds.labels,
    y_test,
    plot_roc=<span class="hljs-literal">False</span>,
    plot_conf_matrix=<span class="hljs-literal">True</span>,
    show_summary=<span class="hljs-literal">True</span>,
    ret_eval_dict=<span class="hljs-literal">True</span>
)
test_pred_rw_ds = test_ds.copy(deepcopy=<span class="hljs-literal">True</span>)
test_pred_rw_ds.labels = cls_mdls[<span class="hljs-string">'lgb_1_rw'</span>][<span class="hljs-string">'preds_test'</span>
    ].reshape(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)
test_pred_rw_ds.scores = cls_mdls[<span class="hljs-string">'lgb_1_rw'</span>][<span class="hljs-string">'probs_test'</span>
    ].reshape(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)
metrics_test_rw_dict, _ = mldatasets.<span class="code-highlight"><strong class="hljs-slc">compute_aif_metrics</strong></span>(
    test_ds,
    test_pred_rw_ds,
    unprivileged_groups=underprivileged_groups,
    privileged_groups=privileged_groups
)
cls_mdls[<span class="hljs-string">'lgb_1_rw'</span>].update(metrics_test_rw_dict)
</code></pre>
    <p class="normal">The preceding snippet outputs the confusion matrix and performance metrics, as shown in <em class="italic">Figure 11.5</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_11_05.png" alt="Chart, waterfall chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 11.5: Evaluation of the LightGBM reweighted model</p>
    <p class="normal">If you<a id="_idIndexMarker1229"/> compare <em class="italic">Figure 11.5</em> to <em class="italic">Figure 11.4</em>, you can conclude that there’s not much difference in predictive performance between the reweighted and the base model. This outcome was expected, but it’s still good to verify it. Some bias-mitigation methods can adversely impact predictive performance, but reweighing did not. Neither should <strong class="keyWord">Disparate Impact</strong> (<strong class="keyWord">DI</strong>) remover, for that matter, which we will discuss next!</p>
    <h3 id="_idParaDest-313" class="heading-3">The disparate impact remover method</h3>
    <p class="normal">This method<a id="_idIndexMarker1230"/> focuses on bias not located in the protected attribute (<code class="inlineCode">AGE_GROUP</code>), so we will have to delete this feature during the process. To that end, we will need its index—in other words, what position it has within the list of columns. We can save this position (<code class="inlineCode">protected_index</code>) as a variable, like this:</p>
    <pre class="programlisting code"><code class="hljs-code">protected_index = train_ds.feature_names.index(<span class="hljs-string">'AGE_GROUP'</span>)
</code></pre>
    <p class="normal">DI remover is <a id="_idIndexMarker1231"/>parametric. It requires a repair level between zero and one, so we need to find the optimal one. To that end, we can iterate through an array with different values for repair level (<code class="inlineCode">levels</code>), initialize <code class="inlineCode">DisparateImpactRemover</code> with each <code class="inlineCode">level</code>, and <code class="inlineCode">fit_transform</code> the data, which will de-bias the data. However, we then train the model without the protected attribute and use <code class="inlineCode">BinaryLabelDatasetMetric</code> to assess the <code class="inlineCode">disparate_impact</code>. Remember that DI is a ratio, so it’s a metric that can be between over and under one, and an optimal DI is closest to one. Therefore, as we iterate across different repair levels, we will continuously save the model whose DI is closest to one. We will also append the DIs into an array for later use. Have a look at the following code snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">di = np.array([])
train_dir_ds = <span class="hljs-literal">None</span>
test_dir_ds = <span class="hljs-literal">None</span>
lgb_dir_mdl = <span class="hljs-literal">None</span>
X_train_dir = <span class="hljs-literal">None</span>
X_test_dir = <span class="hljs-literal">None</span>
levels = np.hstack(
    [np.linspace(<span class="hljs-number">0.</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">41</span>), np.linspace(<span class="hljs-number">0.2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">9</span>)]
)
<span class="hljs-keyword">for</span> level <span class="hljs-keyword">in</span> tqdm(levels):
    di_remover = <span class="code-highlight"><strong class="hljs-slc">DisparateImpactRemover</strong></span>(repair_level=level)
    train_dir_ds_i = di_remover.<span class="code-highlight"><strong class="hljs-slc">fit_transform</strong></span>(train_ds)
    test_dir_ds_i = di_remover.<span class="code-highlight"><strong class="hljs-slc">fit_transform</strong></span>(test_ds)
    X_train_dir_i = np.<span class="code-highlight"><strong class="hljs-slc">delete</strong></span>(
        train_dir_ds_i.features,
        protected_index,
        axis=<span class="hljs-number">1</span>
    )
    X_test_dir_i = np.<span class="code-highlight"><strong class="hljs-slc">delete</strong></span>(
        test_dir_ds_i.features,
        protected_index,
        axis=<span class="hljs-number">1</span>
    )
    lgb_dir_mdl_i = lgb.<span class="code-highlight"><strong class="hljs-slc">LGBMClassifier</strong></span>(
        random_seed=rand,
        max_depth=<span class="hljs-number">5</span>,
        num_leaves=<span class="hljs-number">33</span>,
        **lgb_params
    )
    lgb_dir_mdl_i.<span class="code-highlight"><strong class="hljs-slc">fit</strong></span>(X_train_dir_i, train_dir_ds_i.labels)
    test_dir_ds_pred_i = test_dir_ds_i.copy()
    test_dir_ds_pred_i.labels = lgb_dir_mdl_i.predict(
        X_test_dir_i
    )
    metrics_test_dir_ds = <span class="code-highlight"><strong class="hljs-slc">BinaryLabelDatasetMetric</strong></span>(
        test_dir_ds_pred_i,
        unprivileged_groups=underprivileged_groups,
        privileged_groups=privileged_groups
    )
    di_i = metrics_test_dir_ds.disparate_impact()
    <span class="hljs-keyword">if</span> (di.shape[<span class="hljs-number">0</span>]==<span class="hljs-number">0</span>) <span class="hljs-keyword">or</span> (np.<span class="hljs-built_in">min</span>(np.<span class="hljs-built_in">abs</span>(di-<span class="hljs-number">1</span>)) &gt;= <span class="hljs-built_in">abs</span>(di_i-<span class="hljs-number">1</span>)):
        <span class="hljs-built_in">print</span>(<span class="hljs-built_in">abs</span>(di_i-<span class="hljs-number">1</span>))
        train_dir_ds = train_dir_ds_i
        test_dir_ds = test_dir_ds_i
        X_train_dir = X_train_dir_i
        X_test_dir = X_test_dir_i
        lgb_dir_mdl = lgb_dir_mdl_i
    di = np.append(np.array(di), di_i)
</code></pre>
    <p class="normal">To observe<a id="_idIndexMarker1232"/> the DI at different repair levels, we can use the following code, and if you want to zoom in on the area where the best DI is located, just uncomment the <code class="inlineCode">xlim</code> line:</p>
    <pre class="programlisting code"><code class="hljs-code">plt.plot(<span class="code-highlight"><strong class="hljs-slc">levels</strong></span>, <span class="code-highlight"><strong class="hljs-slc">di</strong></span>, marker=<span class="hljs-string">'o'</span>)
</code></pre>
    <p class="normal">The preceding code generates the following output. As you can tell by this, there’s an optimal repair level somewhere between 0 and 0.1 because that’s where it gets closest to one:</p>
    <p class="packt_figref"><img src="../Images/B18406_11_06.png" alt="Chart, line chart  Description automatically generated"/></p>
    <p class="packt_figref">Figure 11.6: DI at different DI remover repair levels</p>
    <p class="normal">Now, let’s <a id="_idIndexMarker1233"/>evaluate the best DI-repaired model with <code class="inlineCode">evaluate_class_mdl</code> and compute the fairness metrics (<code class="inlineCode">compute_aif_metrics</code>). We won’t even plot the confusion matrix this time, but we will save all results into the <code class="inlineCode">cls_mdls</code> dictionary for later inspection. The code can be seen in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">cls_mdls[<span class="hljs-string">'lgb_1_dir'</span>] = mldatasets.<span class="code-highlight"><strong class="hljs-slc">evaluate_class_mdl</strong></span>(
    lgb_dir_mdl,
    X_train_dir,
    X_test_dir,
    train_dir_ds.labels,
    test_dir_ds.labels,
    plot_roc=<span class="hljs-literal">False</span>,
    plot_conf_matrix=<span class="hljs-literal">False</span>,
    show_summary=<span class="hljs-literal">False</span>,
    ret_eval_dict=<span class="hljs-literal">True</span>
)
test_pred_dir_ds = test_ds.copy(deepcopy=<span class="hljs-literal">True</span>)
test_pred_dir_ds.labels = cls_mdls[<span class="hljs-string">'lgb_1_dir'</span>][<span class="hljs-string">'preds_test'</span>
].reshape(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)
metrics_test_dir_dict, _ = mldatasets.<span class="code-highlight"><strong class="hljs-slc">compute_aif_metrics</strong></span>(
    test_ds,
    test_pred_dir_ds,
    unprivileged_groups=underprivileged_groups,
    privileged_groups=privileged_groups
)
cls_mdls[<span class="hljs-string">'lgb_1_dir'</span>].<span class="code-highlight"><strong class="hljs-slc">update</strong></span>(metrics_test_dir_dict)
</code></pre>
    <p class="normal">The <a id="_idIndexMarker1234"/>next link in the chain after data is the model, so even if we de-bias the data, the model introduces bias on its own, thus it makes sense to train models that are equipped to deal with it, which is what we will learn how to do next!</p>
    <h2 id="_idParaDest-314" class="heading-2">In-processing bias mitigation methods</h2>
    <p class="normal">These are some <a id="_idIndexMarker1235"/>of the most important in-processing or model-specific bias mitigation methods:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Cost-sensitive training</strong>: We are already incorporating this method into every LightGBM model <a id="_idIndexMarker1236"/>trained in this chapter through the <code class="inlineCode">scale_pos_weight</code> parameter. It’s typically used in imbalanced classification problems and is simply seen as a means to improve accuracy for minor classes. However, given that imbalances with classes tend to favor some groups over others, this method can also be used to mitigate bias, but there are no guarantees that it will. It can be incorporated as class weights or by creating a custom loss function. The implementation will vary according to the model class and what costs are associated with the bias. If they grow linearly with misclassifications, the class weighting will suffice, but otherwise, a custom loss function is recommended.</li>
      <li class="bulletList"><strong class="keyWord">Constraints</strong>: Many <a id="_idIndexMarker1237"/>model classes support monotonic and interaction constraints, and <strong class="keyWord">TensorFlow Lattice</strong> (<strong class="keyWord">TFL</strong>) offers <a id="_idIndexMarker1238"/>more advanced custom shape constraints. These ensure that relationships between features and targets are restricted to a certain pattern, placing guardrails at the model level. There are many reasons you would want to employ them, but chief among them is to mitigate bias. We will discuss this topic in <em class="chapterRef">Chapter 12</em>, <em class="italic">Monotonic Constraints and Model Tuning for Interpretability</em>.</li>
      <li class="bulletList"><strong class="keyWord">Prejudice remover regularizer</strong>: This<a id="_idIndexMarker1239"/> method defines prejudice as the statistical dependence between the sensitive and target variables. However, the aim of this method is to minimize indirect prejudice, which excludes the prejudice that can be avoided by simply removing the sensitive variable. Therefore, the<a id="_idIndexMarker1240"/> method starts by quantifying it with a <strong class="keyWord">Prejudice Index</strong> (<strong class="keyWord">PI</strong>), which<a id="_idIndexMarker1241"/> is the mutual information between the target and sensitive variable. Incidentally, we covered mutual information in <em class="chapterRef">Chapter 10</em>, <em class="italic">Feature Selection and Engineering for Interpretability</em>. Then, along with L2, the PI is incorporated into a custom regularization term. In theory, any model classifier can regularize using the PI-based regularizer, but the only implementation, so far, uses logistic regression.</li>
      <li class="bulletList"><strong class="keyWord">Gerry fair classifier</strong>: This <a id="_idIndexMarker1242"/>is inspired by the concept <a id="_idIndexMarker1243"/>of <strong class="keyWord">fairness gerrymandering</strong>, which has the appearance of fairness in one group but lacks fairness when subdivided into subgroups. The algorithm leverages a <strong class="keyWord">fictitious play</strong> game-theory-inspired approach <a id="_idIndexMarker1244"/>in which you have a zero-sum game between a <em class="italic">learner</em> and an <em class="italic">auditor</em>. The learner minimizes the prediction error and aggregate fairness-based penalty term. The auditor takes it one step further by penalizing the learner based on the worst outcomes observed in the most unfairly treated subgroup. </li>
    </ul>
    <p class="normal">The game’s objective is to achieve a <strong class="keyWord">Nash equilibrium</strong>, which <a id="_idIndexMarker1245"/>is achieved when two non-cooperative players with possibly contradictory aims reach a solution that partially satisfies both. In this case, the learner gets a minimal prediction error and aggregate unfairness, and the auditor gets minimal subgroup unfairness. The implementation of this method is model-agnostic.</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Adversarial debiasing</strong>: Similar<a id="_idIndexMarker1246"/> to the gerry fair classifier, adversarial debiasing leverages two opposing actors, but this time it’s with two neural networks: a predictor and an adversary. We maximize the predictor’s ability to predict the target while minimizing the adversary’s ability to predict the protected feature, thus increasing the equality of odds between privileged and underprivileged groups.</li>
      <li class="bulletList"><strong class="keyWord">Exponentiated gradient reduction</strong>: This method automates cost-sensitive optimization<a id="_idIndexMarker1247"/> by reducing it to a sequence of such problems and using fairness constraints concerning protected attributes such as demographic parity or equalized odds. It is model-agnostic but limited only to scikit-learn-compatible <a id="_idIndexMarker1248"/>binary classifiers.</li>
    </ul>
    <p class="normal">Given that there are so many in-processing methods, we will only employ two of them in this chapter. Still, if you are interested in using ones we won’t cover, they are available in the AIF360 library and documentation.</p>
    <h3 id="_idParaDest-315" class="heading-3">The exponentiated gradient reduction method</h3>
    <p class="normal">The <code class="inlineCode">ExponentiatedGradientReduction</code> method <a id="_idIndexMarker1249"/>is an implementation of cost-sensitive training with constraints . We initialize it with a base estimator, the maximum number of iterations to perform (<code class="inlineCode">max_iter</code>) and specify the disparity <code class="inlineCode">constraints</code> to use. Then, we <code class="inlineCode">fit</code> it. This method can be seen in the following code snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">lgb_egr_mdl = ExponentiatedGradientReduction(
    estimator=lgb_base_mdl,
    max_iter=<span class="hljs-number">50</span>,
    constraints=<span class="hljs-string">'DemographicParity'</span>
)
lgb_egr_mdl.fit(train_ds)
</code></pre>
    <p class="normal">We can use the <code class="inlineCode">predict</code> function to get the training and test predictions and then employ <code class="inlineCode">evaluate_class_metrics_mdl</code> and <code class="inlineCode">compute_aif_metrics</code> to obtain predictive performance and fairness metrics, respectively. We place both into the <code class="inlineCode">cls_mdls</code> dictionary, as illustrated in the following code snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">train_pred_egr_ds = lgb_egr_mdl.<span class="code-highlight"><strong class="hljs-slc">predict</strong></span>(train_ds)
test_pred_egr_ds = lgb_egr_mdl.<span class="code-highlight"><strong class="hljs-slc">predict</strong></span>(test_ds)
cls_mdls[<span class="hljs-string">'lgb_2_egr'</span>] = mldatasets.<span class="code-highlight"><strong class="hljs-slc">evaluate_class_metrics_mdl</strong></span>(
    lgb_egr_mdl,
    train_pred_egr_ds.labels,
    test_pred_egr_ds.scores,
    test_pred_egr_ds.labels,
    y_train,
    y_test
)
metrics_test_egr_dict, _ = mldatasets.<span class="code-highlight"><strong class="hljs-slc">compute_aif_metrics</strong></span>(
    test_ds,
    test_pred_egr_ds,
    unprivileged_groups=underprivileged_groups,
    privileged_groups=privileged_groups
)
cls_mdls[<span class="hljs-string">'lgb_2_egr'</span>].<span class="code-highlight"><strong class="hljs-slc">update</strong></span>(metrics_test_egr_dict)
</code></pre>
    <p class="normal">Next, we <a id="_idIndexMarker1250"/>will learn about a partially model-agnostic in-processing method that takes into account intersectionality.</p>
    <h3 id="_idParaDest-316" class="heading-3">The gerry fair classifier method</h3>
    <p class="normal">The gerry fair classifier<a id="_idIndexMarker1251"/> is partially model-agnostic. It only supports linear models, <strong class="keyWord">Support Vector Machines</strong> (<strong class="keyWord">SVMs</strong>), kernel regression, and <a id="_idIndexMarker1252"/>decision trees. We initialize <code class="inlineCode">GerryFairClassifier</code> by defining a regularization strength (<code class="inlineCode">C</code>), a fairness approximation for early stopping (<code class="inlineCode">gamma</code>), whether to be verbose (<code class="inlineCode">printflag</code>), the maximum number of iterations (<code class="inlineCode">max_iters</code>), the model (<code class="inlineCode">predictor</code>), and the fairness notion to employ (<code class="inlineCode">fairness_def</code>). We will use the fairness notion of false negatives (<code class="inlineCode">"FN"</code>) to compute the fairness violations’ weighted disparity. Once it’s been initialized, all we need to do is <code class="inlineCode">fit</code> it and enable <code class="inlineCode">early_termination</code> to stop if it hasn’t improved in five iterations. The code is shown in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">dt_gf_mdl = <span class="code-highlight"><strong class="hljs-slc">GerryFairClassifier</strong></span>(
    C=<span class="hljs-number">100</span>,
    gamma=<span class="hljs-number">.005</span>,
    max_iters=<span class="hljs-number">50</span>,
    <span class="code-highlight"><strong class="hljs-slc">fairness_def</strong></span>=<span class="hljs-string">'FN'</span>,
    printflag=<span class="hljs-literal">True</span>,
    <span class="code-highlight"><strong class="hljs-slc">predictor</strong></span>=tree.DecisionTreeRegressor(max_depth=<span class="hljs-number">3</span>)
)
dt_gf_mdl.<span class="code-highlight"><strong class="hljs-slc">fit</strong></span>(train_ds, early_termination=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">We can use the <code class="inlineCode">predict</code> function to get the training and test predictions and then employ <code class="inlineCode">evaluate_class_metrics_mdl</code> and <code class="inlineCode">compute_aif_metrics</code> to obtain predictive performance and fairness metrics, respectively. We place both into the <code class="inlineCode">cl_smdls</code> dictionary, as illustrated in the following code snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">train_pred_gf_ds = dt_gf_mdl.<span class="code-highlight"><strong class="hljs-slc">predict</strong></span>(train_ds, threshold=<span class="hljs-literal">False</span>)
test_pred_gf_ds = dt_gf_mdl.<span class="code-highlight"><strong class="hljs-slc">predict</strong></span>(test_ds, threshold=<span class="hljs-literal">False</span>)
cls_mdls[<span class="hljs-string">'dt_2_gf'</span>] = mldatasets.evaluate_class_metrics_mdl(
    dt_gf_mdl,
    train_pred_gf_ds.labels,
    <span class="hljs-literal">None</span>,
    test_pred_gf_ds.labels,
    y_train,
    y_test
)
metrics_test_gf_dict, _ = mldatasets.<span class="code-highlight"><strong class="hljs-slc">compute_aif_metrics</strong></span>(
    test_ds,
    test_pred_gf_ds,
    unprivileged_groups=underprivileged_groups,
    privileged_groups=privileged_groups
)
cls_mdls[<span class="hljs-string">'dt_2_gf'</span>].<span class="code-highlight"><strong class="hljs-slc">update</strong></span>(metrics_test_gf_dict)
</code></pre>
    <p class="normal">The<a id="_idIndexMarker1253"/> next and last link in the chain after the model is inference, so even if you de-bias the data and the model there might be some bias left, thus it makes sense to deal with it in this stage too, which is what we will learn how to do next!</p>
    <h2 id="_idParaDest-317" class="heading-2">Post-processing bias mitigation methods</h2>
    <p class="normal">These are some of the<a id="_idIndexMarker1254"/> most important <a id="_idIndexMarker1255"/>post-processing or inference-specific bias mitigation methods:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Prediction abstention</strong>: This <a id="_idIndexMarker1256"/>has many potential benefits such as fairness, safety, and controlling costs, but which one applies will depend on your problem. Typically, a model will return all predictions, even low-confidence ones—that is, predictions that are close to the classification threshold or when the model returns confidence intervals that fall outside of a predetermined threshold. When fairness is involved, if we change predictions to <strong class="keyWord">I don’t know</strong> (<strong class="keyWord">IDK</strong>) in low-confidence regions, the model will likely become fairer as a side-effect when we assess fairness metrics only against predictions that were made. It is also possible to make prediction abstention an in-processing method. A paper called <em class="italic">Predict Responsibly: Increasing Fairness by Learning to Defer</em> discusses two approaches to do this, by training a model to either <strong class="keyWord">punt</strong> (learn to predict IDK) or <strong class="keyWord">defer</strong> (predict IDK when the odds of being correct are lower than an expert opinion). Another paper called <em class="italic">The Utility of Abstaining in Binary Classification</em> employs a reinforcement learning framework called <strong class="keyWord">Knows What It Knows</strong> (<strong class="keyWord">KWIK</strong>), which<a id="_idIndexMarker1257"/> has self-awareness of its mistakes but allows for abstentions.</li>
      <li class="bulletList"><strong class="keyWord">Equalized odds postprocessing</strong>: Also<a id="_idIndexMarker1258"/> known as disparate mistreatment, this ensures that privileged and underprivileged groups have equal treatment for misclassifications, whether false-positive or false-negative. It finds optimal probability thresholds with which changing the labels equalizes the odds between groups.</li>
      <li class="bulletList"><strong class="keyWord">Calibrated equalized odds postprocessing</strong>: Instead of changing the labels, this method <a id="_idIndexMarker1259"/>modifies the probability estimates so that they are on average equal. It calls this calibration. However, this constraint cannot be satisfied for false positives and false negatives concurrently, so you are forced to prefer one over the other. Therefore, it is advantageous in cases where recall is far more important than precision or vice versa, and there are benefits to calibrating the estimated probabilities.</li>
      <li class="bulletList"><strong class="keyWord">Reject option classification</strong>: This <a id="_idIndexMarker1260"/>method leverages the intuition that predictions around the decision boundary tend to be the least fair. It then finds an optimal band around the decision boundary for which flipping the labels for underprivileged and privileged groups yields the most equitable outcomes.</li>
    </ul>
    <p class="normal">We will only <a id="_idIndexMarker1261"/>employ two of these post-processing methods in this chapter. Reject option classification is available in the AIF360 library and documentation.</p>
    <h3 id="_idParaDest-318" class="heading-3">The equalized odds post-processing method</h3>
    <p class="normal">The<a id="_idIndexMarker1262"/> equalized odds post-processing method (<code class="inlineCode">EqOddsPostprocessing</code>) is initialized with the groups we want to equalize odds for and the random <code class="inlineCode">seed</code>. Then, we <code class="inlineCode">fit</code> it. Note that fitting takes two datasets: the original one (<code class="inlineCode">test_ds</code>) and then the dataset with predictions for our base model (<code class="inlineCode">test_pred_ds</code>). What <code class="inlineCode">fit</code> does is compute the optimal probability thresholds. Then, <code class="inlineCode">predict</code> creates a new dataset where these thresholds have changed the <code class="inlineCode">labels</code>. The code can be seen in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">epp = <span class="code-highlight"><strong class="hljs-slc">EqOddsPostprocessing</strong></span>(
    privileged_groups=privileged_groups,
    unprivileged_groups=underprivileged_groups,
    seed=rand
)
epp = epp.<span class="code-highlight"><strong class="hljs-slc">fit</strong></span>(test_ds, test_pred_ds)
test_pred_epp_ds = epp.<span class="code-highlight"><strong class="hljs-slc">predict</strong></span>(test_pred_ds)
</code></pre>
    <p class="normal">We<a id="_idIndexMarker1263"/> can employ <code class="inlineCode">evaluate_class_metrics_mdl</code> and <code class="inlineCode">compute_aif_metrics</code> to obtain predictive performance and fairness metrics <a id="_idIndexMarker1264"/>for <strong class="keyWord">Equal-Proportion Probability</strong> (<strong class="keyWord">EPP</strong>), respectively. We place both into the <code class="inlineCode">cls_mdls</code> dictionary. The code can be seen in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">cls_mdls[<span class="hljs-string">'lgb_3_epp'</span>] = mldatasets.<span class="code-highlight"><strong class="hljs-slc">evaluate_class_metrics_mdl</strong></span>(
    lgb_base_mdl,
    cls_mdls[<span class="hljs-string">'lgb_0_base'</span>][<span class="hljs-string">'preds_train'</span>],
    test_pred_epp_ds.scores,
    test_pred_epp_ds.labels,
    y_train,
    y_test
)
metrics_test_epp_dict, _ = mldatasets.<span class="code-highlight"><strong class="hljs-slc">compute_aif_metrics</strong></span>(
    test_ds,
    test_pred_epp_ds,
    unprivileged_groups=underprivileged_groups,
    privileged_groups=privileged_groups
)
cls_mdls[<span class="hljs-string">'</span><span class="hljs-string">lgb_3_epp'</span>].<span class="code-highlight"><strong class="hljs-slc">update</strong></span>(metrics_test_epp_dict)
</code></pre>
    <p class="normal">Next, we will learn about another post-processing method. The main difference is that it calibrates the probability scores rather than only changing the predicted labels.</p>
    <h3 id="_idParaDest-319" class="heading-3">The calibrated equalized odds postprocessing method</h3>
    <p class="normal">Calibrated <a id="_idIndexMarker1265"/>equalized odds (<code class="inlineCode">CalibratedEqOddsPostprocessing</code>) is implemented exactly like equalized odds, except it has one more crucial attribute (<code class="inlineCode">cost_constraint</code>). This attribute defines which constraint to satisfy since it cannot make the scores fair for FPRs and FNRs simultaneously. We choose FPR and then <code class="inlineCode">fit</code>, <code class="inlineCode">predict</code>, and <code class="inlineCode">evaluate</code>, as we did for equalized odds. The code can be seen in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">cpp = <span class="code-highlight"><strong class="hljs-slc">CalibratedEqOddsPostprocessing</strong></span>(
    privileged_groups=privileged_groups,
    unprivileged_groups=underprivileged_groups,
    <span class="code-highlight"><strong class="hljs-slc">cost_constraint</strong></span>=<span class="hljs-string">"fpr"</span>,
    seed=rand
)
cpp = cpp.<span class="code-highlight"><strong class="hljs-slc">fit</strong></span>(test_ds, test_pred_ds)
test_pred_cpp_ds = cpp.<span class="code-highlight"><strong class="hljs-slc">predict</strong></span>(test_pred_ds)
cls_mdls[<span class="hljs-string">'lgb_3_cpp'</span>] = mldatasets.<span class="code-highlight"><strong class="hljs-slc">evaluate_class_metrics_mdl</strong></span>(
    lgb_base_mdl,
    cls_mdls[<span class="hljs-string">'</span><span class="hljs-string">lgb_0_base'</span>][<span class="hljs-string">'preds_train'</span>],
    test_pred_cpp_ds.scores,
    test_pred_cpp_ds.labels,
    y_train,
    y_test
)
metrics_test_cpp_dict, _ = mldatasets.<span class="code-highlight"><strong class="hljs-slc">compute_aif_metrics</strong></span>(
    test_ds,
    test_pred_cpp_ds,
    unprivileged_groups=underprivileged_groups,
    privileged_groups=privileged_groups
)
cls_mdls[<span class="hljs-string">'lgb_3_cpp'</span>].<span class="code-highlight"><strong class="hljs-slc">update</strong></span>(metrics_test_cpp_dict)
</code></pre>
    <p class="normal">Now that <a id="_idIndexMarker1266"/>we have tried six bias mitigation methods, two at every level, we can compare them against each other and the base model!</p>
    <h2 id="_idParaDest-320" class="heading-2">Tying it all together!</h2>
    <p class="normal">To compare the<a id="_idIndexMarker1267"/> metrics for all the methods, we can take the dictionary (<code class="inlineCode">cls_mdls</code>) and place it in the DataFrame (<code class="inlineCode">cls_metrics_df</code>). We are only interested in a few performance metrics and most of the fairness metrics recorded. Then, we output the DataFrame sorted by test accuracy and with all the fairness metrics color-coded. The code can be seen in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">cls_metrics_df = pd.DataFrame.from_dict(cls_mdls, <span class="hljs-string">'index'</span>)[
    [
        <span class="hljs-string">'accuracy_train'</span>,
        <span class="hljs-string">'accuracy_test'</span>,
        <span class="hljs-string">'</span><span class="hljs-string">f1_test'</span>,
        <span class="hljs-string">'mcc_test'</span>,
        <span class="hljs-string">'SPD'</span>,
        <span class="hljs-string">'DI'</span>,
        <span class="hljs-string">'AOD'</span>,
        <span class="hljs-string">'EOD'</span>,
        <span class="hljs-string">'DFBA'</span>
    ]
]
metrics_fmt_dict = <span class="hljs-built_in">dict</span>(
    <span class="hljs-built_in">zip</span>(cls_metrics_df.columns,[<span class="hljs-string">'{:.1%}'</span>]*<span class="hljs-number">3</span>+ [<span class="hljs-string">'{:.3f}'</span>]*<span class="hljs-number">6</span>)
)
cls_metrics_df.sort_values(
    by=<span class="hljs-string">'accuracy_test'</span>,
    ascending=<span class="hljs-literal">False</span>
).style.<span class="hljs-built_in">format</span>(metrics_fmt_dict)
</code></pre>
    <p class="normal">The preceding snippet outputs the following DataFrame:</p>
    <p class="packt_figref"><img src="../Images/B18406_11_07.png" alt="" role="presentation"/></p>
    <p class="packt_figref">Figure 11.7: Comparison of all bias mitigation methods with different fairness metrics</p>
    <p class="normal"><em class="italic">Figure 11.7</em> shows that most methods yielded models that are fairer than the base model for SPD, DI, AOD, and EOD. Calibrated equalized odds post-processing (<code class="inlineCode">lgb_3_cpp</code>) was the exception, but it had one of the best DFBAs but it yielded a suboptimal DI because of the lopsided nature of the calibration. Note that this method is particularly good <a id="_idIndexMarker1268"/>at achieving parity for FPR or FNR while calibrating scores, but none of these fairness metrics are useful for picking up on this. Instead, you could create a metric that’s the ratio between FPRs, as we did in <em class="chapterRef">Chapter 6</em>, <em class="italic">Anchors and Counterfactual Explanations</em>. Incidentally, this would be the perfect use case for <strong class="keyWord">Calibrated Equalized Odds</strong> (<strong class="keyWord">CPP</strong>). </p>
    <p class="normal">The method that obtained the best SPD, DI, AOD, and DFBA, and the second-best EOD was equalized odds post-processing (<code class="inlineCode">lgb_3_epp</code>), so let’s visualize fairness for it using XAI’s plots. To this end, we first create a DataFrame with the test examples (<code class="inlineCode">test_df</code>) and then use <code class="inlineCode">replace</code> to make an <code class="inlineCode">AGE_GROUP</code> categorical and obtain the list of categorical columns (<code class="inlineCode">cat_cols_l</code>). Then, we can compare different metrics (<code class="inlineCode">metrics_plot</code>) using the true labels (<code class="inlineCode">y_test</code>), predicted probability scores for the EPP model, the DataFrame (<code class="inlineCode">test_df</code>), the protected attribute (<code class="inlineCode">cross_cols</code>), and categorical columns. We can do the same for the <strong class="keyWord">Receiver Operating Characteristic</strong> (<strong class="keyWord">ROC</strong>) plot (<code class="inlineCode">roc_plot</code>) and the <strong class="keyWord">Precision-Recall</strong> (<strong class="keyWord">PR</strong>) plot (<code class="inlineCode">pr_plot</code>). The code can be seen in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">test_df = ccdefault_bias_df.loc[<span class="code-highlight"><strong class="hljs-slc">X_test.index</strong></span>]
test_df[<span class="hljs-string">'</span><span class="hljs-string">AGE_GROUP'</span>] = test_df.AGE_GROUP.<span class="code-highlight"><strong class="hljs-slc">replace</strong></span>(
    {<span class="hljs-number">0</span>:<span class="hljs-string">'underprivileged'</span>, <span class="hljs-number">1</span>:<span class="hljs-string">'privileged'</span>}
)
cat_cols_l = ccdefault_bias_df.dtypes[<span class="code-highlight"><strong class="hljs-slc">lambda x: x==np.int8</strong></span>
                                      ].index.tolist()
_ = xai.<span class="code-highlight"><strong class="hljs-slc">metrics_plot</strong></span>(
    y_test,cls_mdls[<span class="hljs-string">'lgb_3_epp'</span>][<span class="hljs-string">'probs_test'</span>],
    df=test_df, cross_cols=[<span class="hljs-string">'AGE_GROUP'</span>],
    categorical_cols=cat_cols_l
)
_ = xai.<span class="code-highlight"><strong class="hljs-slc">roc_plot</strong></span>(
    y_test, cls_mdls[<span class="hljs-string">'lgb_3_epp'</span>][<span class="hljs-string">'</span><span class="hljs-string">probs_test'</span>],
    df=test_df, cross_cols=[<span class="hljs-string">'AGE_GROUP'</span>],
    categorical_cols=cat_cols_l
)
_ = xai.<span class="code-highlight"><strong class="hljs-slc">pr_plot</strong></span>(
    y_test,
    cls_mdls[<span class="hljs-string">'lgb_3_epp'</span>][<span class="hljs-string">'probs_test'</span>],
    df=test_df, cross_cols=[<span class="hljs-string">'AGE_GROUP'</span>],
    categorical_cols=cat_cols_l
)
</code></pre>
    <p class="normal">The preceding snippet outputs the three plots in <em class="italic">Figure 11.8</em>. The first one shows that even the fairest model still has some disparities between both groups, especially between precision and recall and, by extension, F1 score, which is their average. However, the ROC curve shows how close both groups are from an FPR versus a TPR standpoint. The third plot is where the disparities in precision and recall become even more evident. This all demonstrates how hard it is to keep a fair balance on all fronts! Some methods are best for making one aspect perfect but nothing else, while others are pretty good on a<a id="_idIndexMarker1269"/> handful of aspects but nothing else. Despite the shortcomings of the methods, most of them achieved a sizable improvement. Ultimately, choosing methods will depend on what you most care about, and combining <a id="_idIndexMarker1270"/>them is also recommended for maximum effect! The output is shown here:</p>
    <figure class="mediaobject"><img src="../Images/B18406_11_08.png" alt="" role="presentation"/></figure>
    <figure class="mediaobject">Figure 11.8: Plots demonstrating fairness for the fairest model</figure>
    <p class="normal">We’ve concluded the bias mitigation exercise and will move on to the causal inference exercise, where we will discuss how to ensure fair and robust policies.</p>
    <h1 id="_idParaDest-321" class="heading-1">Creating a causal model</h1>
    <p class="normal">Decision-making <a id="_idIndexMarker1271"/>will often involve understanding cause and effect. If the effect is desirable, you can decide <a id="_idIndexMarker1272"/>to replicate its cause, or otherwise avoid it. You can change something on purpose to observe how it changes outcomes, or trace an accidental effect back to its cause, or simulate which change will produce the most beneficial impact. Causal inference can help us do all this by creating causal graphs and models. These tie all variables together and estimate effects to make more principled decisions. However, to properly assess the impact of a cause, whether by design or accident, you’ll need to separate its effect from confounding variables.</p>
    <p class="normal">The <a id="_idIndexMarker1273"/>reason causal inference is relevant to this chapter is that the bank’s policy decisions have the power to impact cardholder livelihoods significantly and, given the rise in suicides, even life and death. Therefore, there’s a moral imperative to assess policy decisions with the utmost care.</p>
    <p class="normal">The Taiwanese bank conducted a lending policy experiment for 6 months. The bank saw the writing on the wall and knew that the customers with the highest risk of default would somehow be written off their balance sheets in a way that diminished those customers’ financial obligations. Therefore, the experiment’s focus only involved what the bank considered salvageable, which were low-to-mid risk-of-default customers, and now that the experiment has ended, they want to understand how the following policies have impacted customer behavior:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Lower credit limit</strong>: Some customers had their credit limit reduced by 25%.</li>
      <li class="bulletList"><strong class="keyWord">Payment plan</strong>: They were given 6 months to pay back their current credit card debt. In other words, the debt was split up into six parts, and every month they would have to pay one part.</li>
      <li class="bulletList"><strong class="keyWord">Both measures</strong>: A reduction in credit limit and the payment plan.</li>
    </ul>
    <p class="normal">Also, prevailing credit card interest rates in Taiwan were around 16-20% in 2005, but the bank caught wind that these would be capped at 4% by the Taiwanese Financial Supervisory Commission. Therefore, they ensured all customers in the experiment were automatically provided with interest rates at that level. Some bank executives thought this would only aggravate the indebtedness and create more “credit card slaves” in the process. These concerns prompted the proposal to conduct the experiment with a lower credit card limit as a countermeasure. On the other hand, the payment plan was devised to understand whether debt relief gave customers breathing room to use the card without fear.</p>
    <p class="normal">On the <a id="_idIndexMarker1274"/>business side, the rationale was that a healthy level of spending needed to be encouraged because with lower interest rates, the bulk of the profits would come from payment processing, cashback partnerships, and other sources tied to spending and, in turn, increased customer longevity. Yet, this would also be beneficial to customers because if they were more profitable as spenders than as debtors, it meant the incentives were in place to keep them from becoming the latter. All this justified the use of estimated lifetime value (<code class="inlineCode">_LTV</code>) as a proxy metric for how the experiment’s outcome benefited both the bank and its customers. For years, the bank has been using a reasonably accurate calculation to estimate how much value a credit card holder will provide to the bank given their spending and payment history, and parameters such as limits and interest rates.</p>
    <p class="normal">In the <a id="_idIndexMarker1275"/>parlance <a id="_idIndexMarker1276"/>of experimental design, the chosen policy is called a <strong class="keyWord">treatment</strong>, and along with the three treated groups, there’s a control group that wasn’t prescribed a treatment—that is, no change in policy at all, not even the lower interest rates. Before we move forward, let’s first initialize a list with the treatment names (<code class="inlineCode">treatment_names</code>) and one that includes even the control group (<code class="inlineCode">all_treatment_names</code>), as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">treatment_names = [
    <span class="hljs-string">'Lower Credit Limit'</span>,
    <span class="hljs-string">'Payment Plan'</span>,
    <span class="hljs-string">'Payment Plan &amp;Credit Limit'</span>
]
all_treatment_names = np.array([<span class="hljs-string">"None"</span>] + treatment_names)
</code></pre>
    <p class="normal">Now, let’s examine the results of the experiment to help us design an optimal causal model.</p>
    <h2 id="_idParaDest-322" class="heading-2">Understanding the results of the experiment</h2>
    <p class="normal">A fairly intuitive <a id="_idIndexMarker1277"/>way of assessing the effectiveness of a treatment is by comparing their outcomes. We want to know the answers to the following two simple questions:</p>
    <ul>
      <li class="bulletList">Did the treatment decrease the default rate compared to the control group?</li>
      <li class="bulletList">Were the spending behaviors conducive to an increase in lifetime value estimates?</li>
    </ul>
    <p class="normal">We can visualize both in a single plot. To this end, we obtain a <code class="inlineCode">pandas</code> series with the percentage for each group that defaulted (<code class="inlineCode">pct_s</code>), then another one with the sum of lifetime values for each group (<code class="inlineCode">ltv_s</code>) in thousands of NTD (<code class="inlineCode">K$</code>). We put both series into a <code class="inlineCode">pandas</code> DataFrame and <code class="inlineCode">plot</code> it, as illustrated in the following code snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">pct_s = ccdefault_causal_df[
    ccdefault_causal_df.IS_DEFAULT==<span class="hljs-number">1</span>]
    .groupby([<span class="hljs-string">'_TREATMENT'</span>])
    .size()
    /ccdefault_causal_df.groupby([<span class="hljs-string">'_TREATMENT'</span>]).size()
ltv_s = ccdefault_causal_df.groupby(
    [<span class="hljs-string">'_TREATMENT'</span>])[<span class="hljs-string">'_LTV'</span>].<span class="hljs-built_in">sum</span>()/<span class="hljs-number">1000</span>
plot_df = pd.DataFrame(
    {<span class="hljs-string">'% Defaulted'</span>:pct_s,
     <span class="hljs-string">'Total LTV, K$'</span>:ltv_s}
)
plot_df.index = all_treatment_names
ax = plot_df.plot(secondary_y=[<span class="hljs-string">'Total LTV, K$'</span>], figsize=(<span class="hljs-number">8</span>,<span class="hljs-number">5</span>))
ax.get_legend().set_bbox_to_anchor((<span class="hljs-number">0.7</span>, <span class="hljs-number">0.99</span>))<br/>plt.grid(<span class="hljs-literal">False</span>)
</code></pre>
    <p class="normal">The <a id="_idIndexMarker1278"/>preceding snippet outputs the plot shown in <em class="italic">Figure 11.9</em>. It can be inferred that all treatments fare better than the control group. The lowering of the credit limit on its own decreases the default rate by over 12% and more than doubles the estimated <code class="inlineCode">LTV</code>, while the payment plan only decreases the defaults by 3% and increases the LTV by about 85%. However, both policies combined quadrupled the control group’s LTV and reduced the default rate by nearly 15%! The output can be seen here:</p>
    <p class="packt_figref"><img src="../Images/B18406_11_09.png" alt="Chart, line chart  Description automatically generated"/></p>
    <p class="packt_figref">Figure 11.9: Outcomes for treatment experiment with different credit policies</p>
    <p class="normal">Before <a id="_idIndexMarker1279"/>bank executives rejoice that they have found the winning policy, we must examine how they distributed it among the credit cardholders in the experiment. We learned that they chose the treatment according to the risk factor, which is measured by the <code class="inlineCode">_risk_score</code> variable. However, lifetime value is largely affected by the credit limit available (<code class="inlineCode">_CC_LIMIT</code>), so we must take that into account. One way to understand the distribution is by plotting both variables against each other in a scatter plot color-coded by <code class="inlineCode">_TREATMENT</code>. The code for this can be seen in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">sns.<span class="code-highlight"><strong class="hljs-slc">scatterplot</strong></span>(
    x=ccdefault_causal_df[<span class="hljs-string">'</span><span class="hljs-string">_CC_LIMIT'</span>].values,
    y=ccdefault_causal_df[<span class="hljs-string">'_risk_score'</span>].values,
    hue=all_treatment_names[ccdefault_causal_df[<span class="hljs-string">'_TREATMENT'</span>].values],
    hue_order = all_treatment_names
)
</code></pre>
    <p class="normal">The preceding code generated the plot in <em class="italic">Figure 11.10</em>. It shows that the three treatments correspond to different risk levels, while the control group (<code class="inlineCode">None</code>) is spread out more vertically. The choice to assign treatments based on risk level also meant that they unevenly distributed the treatments based on <code class="inlineCode">_CC_LIMIT</code>. We ought to ask ourselves if this experiment’s biased conditions make it even viable to interpret the outcomes. Have a look at the following output:</p>
    <p class="packt_figref"><img src="../Images/B18406_11_10.png" alt="Chart, scatter chart  Description automatically generated"/></p>
    <p class="packt_figref">Figure 11.10: Risk factors versus original credit limit</p>
    <p class="normal">The <a id="_idIndexMarker1280"/>scatterplot in <em class="italic">Figure 11.10</em> demonstrates the stratification of the treatments across risk factors. However, scatter plots can be challenging to interpret to understand <a id="_idIndexMarker1281"/>distributions. For that, it’s best to use a <strong class="keyWord">K</strong><strong class="keyWord">ernel Density Estimate</strong> (<strong class="keyWord">KDE</strong>) plot. So, let’s see how <code class="inlineCode">_CC_LIMIT</code> and lifetime value (<code class="inlineCode">_LTV</code>) is distributed across all treatments with Seaborn’s <code class="inlineCode">displot</code>. Have a look at the following code snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">sns.<span class="code-highlight"><strong class="hljs-slc">displot</strong></span>(
    ccdefault_causal_df,
    x=<span class="hljs-string">"_CC_LIMIT"</span>,
    hue=<span class="hljs-string">"_TREATMENT"</span>,
    kind=<span class="hljs-string">"kde"</span>,
    fill=<span class="hljs-literal">True</span>
)
sns.<span class="code-highlight"><strong class="hljs-slc">displot</strong></span>(
    ccdefault_causal_df,
    x=<span class="hljs-string">"_LTV"</span>,
    hue=<span class="hljs-string">"_TREATMENT"</span>,
    kind=<span class="hljs-string">"kde"</span>, fill=<span class="hljs-literal">True</span>
)
</code></pre>
    <p class="normal">The <a id="_idIndexMarker1282"/>preceding snippet produced the two KDE plots in <em class="italic">Figure 11.11</em>. We can easily tell how far apart all four distributions are for both plots, mostly regarding treatment #3 (<strong class="keyWord">Payment Plan &amp; Lower Credit Limit</strong>), which tends to be centered significantly more to the right and has a longer and fatter right tail. You can view the output here:</p>
    <figure class="mediaobject"><img src="../Images/B18406_11_11.png" alt="Chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 11.11: KDE distributions for _CC_LIMIT and _LTV by _TREATMENT</p>
    <p class="normal">Ideally, when you design an experiment such as this, you should aim for equal distribution among all groups based on any pertinent factors that could alter the outcomes. However, this might not always be feasible, either because of logistical or strategic constraints. In this case, the outcome (<code class="inlineCode">_LTV</code>) varies according to customer credit card limits (<code class="inlineCode">_CC_LIMIT</code>), the <strong class="keyWord">heterogeneity feature</strong>—in other words, the varying feature that directly impacts the treatment effect, also known as the <strong class="keyWord">heterogeneous treatment effect modifier</strong>. We<a id="_idIndexMarker1283"/> can create a causal model that includes both the <code class="inlineCode">_TREATMENT</code> feature and the effect modifier (<code class="inlineCode">_CC_LIMIT</code>).</p>
    <h2 id="_idParaDest-323" class="heading-2">Understanding causal models</h2>
    <p class="normal">The causal model <a id="_idIndexMarker1284"/>we will build can be separated into four components, as follows:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Outcome</strong> (<em class="italic">Y</em>): The outcome variable(s) of the causal model.</li>
      <li class="bulletList"><strong class="keyWord">Treatments</strong> (<em class="italic">T</em>): The treatment variable(s) that influences the outcome.</li>
      <li class="bulletList"><strong class="keyWord">Effect modifiers</strong> (<em class="italic">X</em>): The variable(s) that influences the effect’s heterogeneity conditioning it. It sits in between the treatment and the outcome.</li>
      <li class="bulletList"><strong class="keyWord">Controls</strong> (<em class="italic">W</em>): Also known as <strong class="keyWord">common causes</strong> or <strong class="keyWord">confounders</strong>. They are the features that influence both<a id="_idIndexMarker1285"/> the outcome and the treatment.</li>
    </ul>
    <p class="normal">We will start by identifying each one of these components in the data as separate <code class="inlineCode">pandas</code> DataFrames, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="code-highlight"><strong class="hljs-slc">W</strong></span> = ccdefault_causal_df[
    [
      <span class="hljs-string">'_spend'</span>,<span class="hljs-string">'_tpm'</span>, <span class="hljs-string">'_ppm'</span>, <span class="hljs-string">'_RETAIL'</span>,<span class="hljs-string">'_URBAN'</span>, <span class="hljs-string">'_RURAL'</span>,
      <span class="hljs-string">'_PREMIUM'</span>
<span class="hljs-string">    </span>]
]
<span class="code-highlight"><strong class="hljs-slc">X</strong></span> = ccdefault_causal_df[[<span class="hljs-string">'_CC_LIMIT'</span>]]
<span class="code-highlight"><strong class="hljs-slc">T</strong></span> = ccdefault_causal_df[[<span class="hljs-string">'_TREATMENT'</span>]]
<span class="code-highlight"><strong class="hljs-slc">Y</strong></span> = ccdefault_causal_df[[<span class="hljs-string">'_LTV'</span>]]
</code></pre>
    <p class="normal">We will use <a id="_idIndexMarker1286"/>the <strong class="keyWord">Doubly Robust Learning</strong> (<strong class="keyWord">DRL</strong>) method to estimate the treatment effects. It’s called “doubly” because it leverages two models, as follows:</p>
    <ul>
      <li class="bulletList">It predicts the outcome with a <em class="italic">regression model</em>, as illustrated here:</li>
    </ul>
    <p class="center"><img src="../Images/B18406_11_005.png" alt="" role="presentation"/></p>
    <ul>
      <li class="bulletList">It predicts the treatment with a <em class="italic">propensity model</em>, as illustrated here:</li>
    </ul>
    <p class="center"><img src="../Images/B18406_11_006.png" alt="" role="presentation"/></p>
    <p class="normal">It’s also <em class="italic">robust</em> because of the final stage, which combines both models while maintaining many desirable statistical properties such as confidence intervals and asymptotic<a id="_idIndexMarker1287"/> normality. More formally, the estimation leverages regression model <em class="italic">g</em> and propensity model <em class="italic">p</em> conditional on treatment <em class="italic">t</em>, like this:</p>
    <p class="center"><img src="../Images/B18406_11_007.png" alt="" role="presentation"/></p>
    <p class="normal">It also does this:</p>
    <p class="center"><img src="../Images/B18406_11_008.png" alt="" role="presentation"/></p>
    <p class="normal">The goal is to derive<a id="_idIndexMarker1288"/> the <strong class="keyWord">Conditional Average Treatment Effect</strong> (<strong class="keyWord">CATE</strong>) denoted as <img src="../Images/B18406_11_009.png" alt="" role="presentation"/> associated with each treatment <strong class="keyWord">t</strong> given heterogeneous effect <strong class="keyWord">X</strong>. First, the DRL method de-biases the regression model by applying the inverse propensity, like this:</p>
    <p class="center"><img src="../Images/B18406_11_010.png" alt="" role="presentation"/></p>
    <p class="normal">How exactly to estimate coefficients <img src="../Images/B18406_11_011.png" alt="" role="presentation"/> from model <img src="../Images/B18406_11_012.png" alt="" role="presentation"/> will depend on the DRL variant employed. We will use a linear variant (<code class="inlineCode">LinearDRLearner</code>) so that it returns coefficients and intercepts, which can be easily interpreted. It derives <img src="../Images/B18406_11_013.png" alt="" role="presentation"/> by running <strong class="keyWord">ordinary linear regression</strong> (<strong class="keyWord">OLS</strong>) for<a id="_idIndexMarker1289"/> the outcome differences between a treatment <em class="italic">t</em> and the control <img src="../Images/B18406_11_014.png" alt="" role="presentation"/> on <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">t</sub> . This intuitively makes sense because the estimated effect of a treatment minus the estimated effect of the absence of a<a id="_idIndexMarker1290"/> treatment (t = 0) is the <em class="italic">net</em> effect of said treatment.</p>
    <p class="normal">Now, with all the theory out of the way, let’s dig in!</p>
    <h2 id="_idParaDest-324" class="heading-2">Initializing the linear doubly robust learner</h2>
    <p class="normal">We can<a id="_idIndexMarker1291"/> initialize a <code class="inlineCode">LinearDRLearner</code> from the <code class="inlineCode">econml</code> library, which we call <code class="inlineCode">drlearner</code>, by specifying any scikit-learn-compatible regressor (<code class="inlineCode">model_regression</code>) and classifier (<code class="inlineCode">model_propensity</code>). We will use XGBoost for both, but note that the classifier has an <code class="inlineCode">objective=multi:softmax</code> attribute. Remember that we have multiple treatments, so it’s a multiclass classification problem. The code can be seen in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">drlearner = <span class="code-highlight"><strong class="hljs-slc">LinearDRLearner</strong></span>(
    model_regression=xgb.XGBRegressor(learning_rate=<span class="hljs-number">0.1</span>),
    model_propensity=xgb.XGBClassifier(learning_rate=<span class="hljs-number">0.1</span>,
    max_depth=<span class="hljs-number">2</span>,
    objective=<span class="hljs-string">"multi:softmax"</span>),
    random_state=rand
)
</code></pre>
    <p class="normal">If you want to understand what both the regression and propensity model are doing, you can easily fit <code class="inlineCode">xgb.XGBRegressor().fit(W.join(X),Y)</code> and <code class="inlineCode">xgb.XGBClassifier(objective="multi:softmax").fit(W.join(X), T)</code> models. We won’t do this now but if you are curious, you could evaluate their performance and even run feature importance methods to understand what influences their predictions individually. The causal model brings them together with the DRL framework, leading to different conclusions.</p>
    <h2 id="_idParaDest-325" class="heading-2">Fitting the causal model</h2>
    <p class="normal">We can <a id="_idIndexMarker1292"/>use <code class="inlineCode">fit</code> in the <code class="inlineCode">drlearner</code> to fit the causal model leveraging the <code class="inlineCode">dowhy</code> wrapper of <code class="inlineCode">econml</code>. The first attributes are the <code class="inlineCode">Y</code>, <code class="inlineCode">T</code>, <code class="inlineCode">X</code>, and <code class="inlineCode">Y</code> components: <code class="inlineCode">pandas</code> DataFrames. Optionally, you can provide variable names for each of these components: the column names of each of the <code class="inlineCode">pandas</code> DataFrames. Lastly, we would like to estimate the treatment effects. Optionally, we can provide the effect modifiers (<code class="inlineCode">X</code>) to do this with, and we will use half of this data to do so, as illustrated in the following code snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">causal_mdl = drlearner.dowhy.fit(
    Y,
    T,
    X=X,
    W=W,
    outcome_names=Y.columns.to_list(),
    treatment_names=T.columns.to_list(),
    feature_names=X.columns.to_list(),
    confounder_names=W.columns.to_list(),
    target_units=X.iloc[:<span class="hljs-number">550</span>].values
)
</code></pre>
    <p class="normal">With the causal model initialized, we can visualize it. The <code class="inlineCode">pydot</code> library with <code class="inlineCode">pygraphviz</code> can do this for us. Please note that this library is difficult to configure in some environments, so it might not load and show you the much less attractive default graphic instead with <code class="inlineCode">view_model</code>. Don’t worry if this happens. Have a look at the following code snippet:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">try</span>:
    display(<span class="code-highlight"><strong class="hljs-slc">Image</strong></span>(to_pydot(causal_mdl._graph._graph).create_png()))
<span class="hljs-keyword">except</span>:
    causal_mdl.<span class="code-highlight"><strong class="hljs-slc">view_model</strong></span>()
</code></pre>
    <p class="normal">The code in the preceding snippet outputs the model diagram shown here. With it, you can appreciate how all the variables connect:</p>
    <p class="packt_figref"><img src="../Images/B18406_11_12.png" alt="Diagram  Description automatically generated"/></p>
    <p class="packt_figref">Figure 11.12: Causal model diagram</p>
    <p class="normal">The causal <a id="_idIndexMarker1293"/>model has already been fitted, so let’s examine and interpret the results, shall we?</p>
    <h1 id="_idParaDest-326" class="heading-1">Understanding heterogeneous treatment effects</h1>
    <p class="normal">Firstly, it’s important<a id="_idIndexMarker1294"/> to note <a id="_idIndexMarker1295"/>how the <code class="inlineCode">dowhy</code> wrapper of <code class="inlineCode">econml</code> has cut down on a few steps with the <code class="inlineCode">dowhy.fit</code> method. Usually, when you build a <code class="inlineCode">CausalModel</code> such as this one directly with <code class="inlineCode">dowhy</code>, it has a method called <code class="inlineCode">identify_effect</code> that derives the probability expression for the effect to be estimated (the <em class="italic">identified estimand</em>). In this case, this is called <a id="_idIndexMarker1296"/>the <strong class="keyWord">Average Treatment Effect</strong> (<strong class="keyWord">ATE</strong>). Then, another method called <code class="inlineCode">estimate_effect</code> takes this expression and the models it’s supposed to tie together (regression and propensity). With them, it computes both the ATE, <img src="../Images/B18406_11_015.png" alt="" role="presentation"/>, and CATE, <img src="../Images/B18406_11_016.png" alt="" role="presentation"/>, for every outcome <em class="italic">i</em> and treatment <em class="italic">t</em>. However, since we used the wrapper to <code class="inlineCode">fit</code> the causal model, it automatically takes care of both the identification and estimation steps.</p>
    <p class="normal">You can access the identified ATE with the <code class="inlineCode">identified_estimand_</code> property and the estimate results with the <code class="inlineCode">estimate_</code> property for the causal model. The code can be seen in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">identified_ate = causal_mdl.identified_estimand_
<span class="hljs-built_in">print</span>(identified_ate)
drlearner_estimate = causal_mdl.estimate_
<span class="hljs-built_in">print</span>(drlearner_estimate)
</code></pre>
    <p class="normal">The code shown in <a id="_idIndexMarker1297"/>the preceding snippet outputs<a id="_idIndexMarker1298"/> the <strong class="keyWord">estimand expression</strong> for <code class="inlineCode">identified_estimand_</code>, which is a derivation of the expected value for <img src="../Images/B18406_11_017.png" alt="" role="presentation"/>, with some assumptions. Then, the causal-realized <code class="inlineCode">estimate_</code> returns the ATE for treatment #1, as illustrated in the following code snippet:</p>
    <pre class="programlisting con"><code class="hljs-con">Estimand type: nonparametric-ate
### Estimand : 1
Estimand name: backdoor1 (Default)
Estimand expression:
      d 
─────────────(E[_LTV|_RETAIL,_URBAN,_PREMIUM,_RURAL,_CC_LIMIT,
d[_TREATMENT]
])
Estimand assumption 1, Unconfoundedness: If U→{_TREATMENT} and U→_LTV then \ P(_LTV|_TREATMENT,_RETAIL,_URBAN,_PREMIUM,_RURAL,_CC_LIMIT,_spend,_ppm,_tpm,U) = \ P(_LTV|_TREATMENT,_RETAIL,_URBAN,_PREMIUM,_RURAL,_CC_LIMIT,_spend,_ppm,_tpm)
*** Causal Estimate ***
## Identified estimand
Estimand type: nonparametric-ate
## Realized estimand
b:_LTV ~ _TREATMENT + _RETAIL + _URBAN + _PREMIUM + _RURAL + \ _CC_LIMIT + _spend + _ppm + _tpm | _CC_LIMIT
Target units:
## Estimate
Mean value: 7227.904763676559
Effect estimates: [6766.07978487 7337.39526574 7363.36013004
                   7224.20893104 7500.84310705 7221.40328496]
</code></pre>
    <p class="normal">Next, we can iterate across all treatments in the causal model and return a summary for each treatment, like this:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(causal_mdl._d_t[<span class="hljs-number">0</span>]):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">"Treatment: %s"</span> % treatment_names[i])
     display(econml_mdl.<span class="code-highlight"><strong class="hljs-slc">summary</strong></span>(T=i+<span class="hljs-number">1</span>))
</code></pre>
    <p class="normal">The <a id="_idIndexMarker1299"/>preceding code outputs three linear regression summaries. The first one looks like this:</p>
    <figure class="mediaobject"><img src="../Images/B18406_11_13.png" alt="Graphical user interface, table  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 11.13: Summary of one of the treatments</p>
    <p class="normal">To get a <a id="_idIndexMarker1300"/>better sense of the coefficients and intercepts, we can plot them with their respective confidence intervals. To do this, we first create an index of treatments (<code class="inlineCode">idxs</code>). There are three treatments, so this is just an array of numbers between 0 and 2. Then, place all the coefficients (<code class="inlineCode">coef_</code>) and intercepts (<code class="inlineCode">intercept_</code>) into an array using list comprehension. However, it’s a bit more complicated for the 90% confidence intervals for both coefficients and intercepts because <code class="inlineCode">coef__interval</code> and <code class="inlineCode">intercept__interval</code> return the lower and upper bounds of these intervals. We need the length of the margin of error in both directions, not the bounds. We deduct the coefficient and intercepts from these bounds to obtain their respective margin of error, as illustrated in the following code snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">idxs = np.arange(<span class="hljs-number">0</span>, causal_mdl._d_t[<span class="hljs-number">0</span>])
coefs = np.hstack([causal_mdl.<span class="code-highlight"><strong class="hljs-slc">coef_</strong></span>(T=i+<span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> idxs])
intercepts = np.hstack(
    [causal_mdl.<span class="code-highlight"><strong class="hljs-slc">intercept_</strong></span>(T=i+<span class="hljs-number">1</span>)<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> idxs]
)
coefs_err = np.hstack(
    [causal_mdl.<span class="code-highlight"><strong class="hljs-slc">coef__interval</strong></span>(T=i+<span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> idxs]
)
coefs_err[<span class="hljs-number">0</span>, :] = coefs - coefs_err[<span class="hljs-number">0</span>, :]
coefs_err[<span class="hljs-number">1</span>, :] = coefs_err[<span class="hljs-number">1</span>, :] - coefs
intercepts_err = np.vstack(
    [causal_mdl.<span class="code-highlight"><strong class="hljs-slc">intercept__interval</strong></span>(T=i+<span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> idxs]
).Tintercepts_err[<span class="hljs-number">0</span>, :] = intercepts - intercepts_err[<span class="hljs-number">0</span>, :]
intercepts_err[<span class="hljs-number">1</span>, :] = intercepts_err[<span class="hljs-number">1</span>, :] - intercepts
</code></pre>
    <p class="normal">Next, we<a id="_idIndexMarker1301"/> plot the coefficients for each treatment and respective errors using <code class="inlineCode">errorbar</code>. We can do the same with the intercepts as another subplot, as illustrated in the following code snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">ax1 = plt.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)
plt.errorbar(<span class="code-highlight"><strong class="hljs-slc">idxs</strong></span>, <span class="code-highlight"><strong class="hljs-slc">coefs</strong></span>, <span class="code-highlight"><strong class="hljs-slc">coefs_err</strong></span>, fmt=<span class="hljs-string">"o"</span>)
plt.xticks(idxs, treatment_names)
plt.setp(ax1.get_xticklabels(), visible=<span class="hljs-literal">False</span>)
plt.title(<span class="hljs-string">"Coefficients"</span>)
plt.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)
plt.errorbar(<span class="code-highlight"><strong class="hljs-slc">idxs</strong></span>, <span class="code-highlight"><strong class="hljs-slc">intercepts</strong></span>, <span class="code-highlight"><strong class="hljs-slc">intercepts_err</strong></span>, fmt=<span class="hljs-string">"o"</span>)
plt.xticks(idxs, treatment_names)
plt.title(<span class="hljs-string">"</span><span class="hljs-string">Intercepts"</span>)
</code></pre>
    <p class="normal">The <a id="_idIndexMarker1302"/>preceding snippet outputs the following:</p>
    <figure class="mediaobject"><img src="../Images/B18406_11_14.png" alt="A picture containing chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 11.14: Coefficients and intercepts for all treatments</p>
    <p class="normal">With <em class="italic">Figure 11.14</em>, you can <a id="_idIndexMarker1303"/>appreciate how relatively large the margin of error is for all intercepts and coefficients. Nonetheless, it’s pretty clear that from the coefficients alone, treatments keep getting marginally better when read from left to right. But before we conclude that <strong class="keyWord">Payment Plan &amp; Lower Credit Limit</strong> is the <a id="_idIndexMarker1304"/>best policy, we must consider the intercept, which is lower for this treatment than the first one. Essentially, this means that a <a id="_idIndexMarker1305"/>customer with a minimal credit card limit is likely to improve lifetime value more with the first policy because the coefficients are multiplied by the limit, whereas the intercept is the starting point. Given that there’s no one best policy for all customers, let’s examine how to choose policies for each, using the causal model.</p>
    <h2 id="_idParaDest-327" class="heading-2">Choosing policies</h2>
    <p class="normal">We can <a id="_idIndexMarker1306"/>decide on a credit policy on a customer basis using the <code class="inlineCode">const_marginal_effect</code> method, which takes the <em class="italic">X</em> effect modifier (<code class="inlineCode">_CC_LIMIT</code>) and computes the counterfactual CATE, <img src="../Images/B18406_11_018.png" alt="" role="presentation"/>. In other words, it returns the estimated <code class="inlineCode">_LTV</code> for all treatments for all observations in <em class="italic">X</em>.</p>
    <p class="normal">However, they don’t all cost the same. Setting up a payment plan requires administrative and legal costs of about <em class="italic">NT$</em>1,000 per contract, and according to the bank’s actuarial department, lowering the credit limit by 25 has an opportunity cost estimated at <em class="italic">NT$</em>72 per average payment per month (<code class="inlineCode">_ppm</code>) over the lifetime of the customer. To factor these costs, we can set up a simple <code class="inlineCode">lambda</code> function that takes the payment plan costs for all treatments and adds them to the variable credit limit costs, which, naturally, is multiplied by <code class="inlineCode">_ppm</code>. Given an array with credit card limits of <em class="italic">n</em> length, the cost function returns an array of (<em class="italic">n</em>, 3) dimensions with a cost for each treatment. Then, we obtain the counterfactual CATE and deduct the costs (<code class="inlineCode">treatment_effect_minus_costs</code>). Then, we expand the array to include a column of zeros representing the <strong class="keyWord">None</strong> treatment and use <code class="inlineCode">argmax</code> to return each customer’s recommended treatment index (<code class="inlineCode">recommended_T</code>), as illustrated in the following code snippet:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="code-highlight"><strong class="hljs-slc">cost_fn</strong></span> = <span class="hljs-keyword">lambda</span> X: np.repeat(
    np.array([[<span class="hljs-number">0</span>, <span class="hljs-number">1000</span>, <span class="hljs-number">1000</span>]]),
    X.shape[<span class="hljs-number">0</span>], axis=<span class="hljs-number">0</span>) + (np.repeat(np.array([[<span class="hljs-number">72</span>, <span class="hljs-number">0</span>, <span class="hljs-number">72</span>]]),
    X.shape[<span class="hljs-number">0</span>], axis=<span class="hljs-number">0</span>)
    *X._ppm.values.reshape(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)
)
<span class="code-highlight"><strong class="hljs-slc">treatment_effect_minus_costs</strong></span> = causal_mdl.const_marginal_effect(
    X=X.values) - <span class="code-highlight"><strong class="hljs-slc">cost_fn</strong></span>(ccdefault_causal_df)
treatment_effect_minus_costs = np.hstack(
    [
        np.zeros(X.shape),
        <span class="code-highlight"><strong class="hljs-slc">treatment_effect_minus_costs</strong></span>
    ]
)
recommended_T = np.<span class="code-highlight"><strong class="hljs-slc">argmax</strong></span>(treatment_effect_minus_costs, axis=<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">We can use <code class="inlineCode">scatterplot</code> <code class="inlineCode">_CC_LIMIT</code> and <code class="inlineCode">_ppm</code>, color-coded by the recommended treatment to observe the customer’s optimal credit policy, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">sns.scatterplot(
    x=ccdefault_causal_df[<span class="hljs-string">'_CC_LIMIT'</span>].values,
    y=ccdefault_causal_df[<span class="hljs-string">'_ppm'</span>].values,
    hue=all_treatment_names[recommended_T],
    hue_order=all_treatment_names
)
plt.title(<span class="hljs-string">"</span><span class="hljs-string">Optimal Credit Policy by Customer"</span>)
plt.xlabel(<span class="hljs-string">"Original Credit Limit"</span>)
plt.ylabel(<span class="hljs-string">"Payments/month"</span>)
</code></pre>
    <p class="normal">The <a id="_idIndexMarker1307"/>preceding snippet outputs the following scatterplot:</p>
    <figure class="mediaobject"><img src="../Images/B18406_11_15.png" alt="Chart, scatter chart  Description automatically generated"/></figure>
    <figure class="mediaobject">Figure 11.15: Optimal credit policy by customer depending on original credit limit and card usage</figure>
    <p class="normal">It’s evident in <em class="italic">Figure 11.15</em> that “None” (no treatment) is never recommended for any customer. This fact holds even when costs aren’t deducted—you can remove <code class="inlineCode">cost_fn</code> from <code class="inlineCode">treatment_effect_minus_costs</code> and rerun the code that outputs the plot to verify that treatment is always prescribed regardless of the costs. You can deduce that all treatments are beneficial to customers, some more than others. And, of course, some treatments benefit the bank more than others, depending on the customer. There’s a thin line to tread here.</p>
    <p class="normal">One of the <a id="_idIndexMarker1308"/>biggest concerns is fairness to customers, especially those that the bank wronged the most: the underprivileged age group. Just because one policy is more costly to the bank than another, it should not preclude the opportunity to access other policies. One way to assess this would be with a percentage-stacked bar plot for all recommended policies. That way, we can observe how the recommended policy is split between privileged and underprivileged groups. Have a look at the following code snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">ccdefault_causal_df[<span class="hljs-string">'recommended_T'</span>] = recommended_T
plot_df = ccdefault_causal_df.groupby(
    [<span class="hljs-string">'recommended_T'</span>,<span class="hljs-string">'AGE_GROUP'</span>]).size().reset_index()
plot_df[<span class="hljs-string">'AGE_GROUP'</span>] = plot_df.AGE_GROUP.<span class="code-highlight"><strong class="hljs-slc">replace</strong></span>(
    {<span class="hljs-number">0</span>:<span class="hljs-string">'underprivileged'</span>, <span class="hljs-number">1</span>:<span class="hljs-string">'privileged'</span>}
)
plot_df = plot_df.pivot(
    columns=<span class="hljs-string">'AGE_GROUP'</span>,
    index=<span class="hljs-string">'recommended_T'</span>,
    values=<span class="hljs-number">0</span>
)
plot_df.index = treatment_names
plot_df = plot_df.apply(<span class="hljs-keyword">lambda</span> r: <span class="code-highlight"><strong class="hljs-slc">r/r.sum()*100</strong></span>, axis=<span class="hljs-number">1</span>)
plot_df.plot.bar(stacked=<span class="hljs-literal">True</span>, rot=<span class="hljs-number">0</span>)
plt.xlabel(<span class="hljs-string">'Optimal Policy'</span>)
plt.ylabel(<span class="hljs-string">'%'</span>)
</code></pre>
    <p class="normal">The code in the preceding snippet outputs the following:</p>
    <p class="packt_figref"><img src="../Images/B18406_11_16.png" alt="" role="presentation"/></p>
    <p class="packt_figref">Figure 11.16: Fairness of optimal policy distributions</p>
    <p class="normal"><em class="italic">Figure 11.16</em> shows <a id="_idIndexMarker1309"/>how privileged groups are at a higher proportion assigned one of the policies with the <strong class="keyWord">Payment Plan</strong>. This <a id="_idIndexMarker1310"/>disparity is primarily due to the bank’s costs being a factor, so if the bank were to absorb some of these costs, it could make it fairer. But what would be a fair solution? Choosing credit policies is an example of procedural fairness, and there are many possible definitions. Does equal treatment literally mean equal treatment or proportional treatment? Does it encompass notions of freedom of choice too? What if a customer prefers one policy over another? Should they be allowed to switch? Whatever the definition is, it can be resolved with help from the causal model. We can assign all customers the same policy, or the distribution of recommended policies can be calibrated so that proportions are equal, or every customer can choose between the first and second most optimal policy. There are so many ways to go about it!</p>
    <h1 id="_idParaDest-328" class="heading-1">Testing estimate robustness</h1>
    <p class="normal">The <code class="inlineCode">dowhy</code> library <a id="_idIndexMarker1311"/>comes with four <a id="_idIndexMarker1312"/>methods to test the robustness of the estimated causal effect, outlined as follows:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Random common cause</strong>: Adding<a id="_idIndexMarker1313"/> a randomly generated confounder. If the estimate is robust, the ATE should not change too much.</li>
      <li class="bulletList"><strong class="keyWord">Placebo treatment refuter</strong>: Replacing<a id="_idIndexMarker1314"/> treatments with random variables (placebos). If the estimate is robust, the ATE should be close to zero.</li>
      <li class="bulletList"><strong class="keyWord">Data subset refuter</strong>: Removing<a id="_idIndexMarker1315"/> a random subset of the data. If the estimator generalizes well, the ATE should not change too much.</li>
      <li class="bulletList"><strong class="keyWord">Add unobserved common cause</strong>: Adding <a id="_idIndexMarker1316"/>an unobserved confounder that is associated with both the treatment and outcome. The estimator assumes some level of unconfoundedness but adding more should bias the estimates. Depending on the strength of the confounder’s effect, it should have an equal impact on the ATE.</li>
    </ul>
    <p class="normal">We will test robustness with the first two next.</p>
    <h2 id="_idParaDest-329" class="heading-2">Adding a random common cause</h2>
    <p class="normal">This method<a id="_idIndexMarker1317"/> is the easiest to implement by calling <code class="inlineCode">refute_estimate </code>with <code class="inlineCode">method_name="random_common_cause"</code>. This will return a summary that you can print. Have a look at the following code snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">ref_random = causal_mdl.refute_estimate(
    method_name=<span class="hljs-string">"random_common_cause"</span>
)
<span class="hljs-built_in">print</span>(ref_random)
</code></pre>
    <p class="normal">The code in the preceding snippet outputs the following:</p>
    <pre class="programlisting con"><code class="hljs-con">Refute: Add a Random Common Cause
Estimated effect:7227.904763676559
New effect:7241.433599647397
</code></pre>
    <p class="normal">The preceding output tells us that a new common cause, or <em class="italic">W</em> variable, doesn’t have a sizable impact on the ATE.</p>
    <h2 id="_idParaDest-330" class="heading-2">Replacing the treatment variable with a random variable</h2>
    <p class="normal">With<a id="_idIndexMarker1318"/> this method, we will replace the treatment variable with noise. If the treatment correlates robustly with the outcome, this should bring the average effect to zero. To implement it, we also call the <code class="inlineCode">refute_estimate</code> function but with <code class="inlineCode">placebo_treatment_refuter</code> for the method. We must also specify the <code class="inlineCode">placebo_type</code> and the number of simulations (<code class="inlineCode">num_simulations</code>). The placebo type we will use is <code class="inlineCode">permute</code>, and the more simulations the better, but this will also take longer. The code can be seen in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">ref_placebo = causal_mdl.refute_estimate(
    method_name="placebo_treatment_refuter",
    placebo_type="permute", num_simulations=<span class="hljs-number">20</span>
)
<span class="hljs-built_in">print</span>(ref_placebo)
</code></pre>
    <p class="normal">The preceding code outputs the following:</p>
    <pre class="programlisting con"><code class="hljs-con">Refute: Use a Placebo Treatment
Estimated effect:7227.904763676559
New effect:381.05420029741083
p value:0.32491556283289624
</code></pre>
    <p class="normal">As you can tell by the preceding output, the new effect is close to zero. However, given that the p-value is above 0.05, we cannot reject the null hypothesis that ascertains that the ATE is greater than zero. This tells us that the estimated causal effect is not very robust. We can likely improve it by adding relevant confounders or by using a different causal model, but also, the experimental design had flaws that we cannot fix, such as the biased way the bank prescribed the treatments according to the risk factor.</p>
    <h1 id="_idParaDest-331" class="heading-1">Mission accomplished</h1>
    <p class="normal">The mission of this chapter was twofold, as outlined here:</p>
    <ul>
      <li class="bulletList">Create a fair predictive model to predict which customers are most likely to default.</li>
      <li class="bulletList">Create a robust causal model to estimate which policies are most beneficial to customers and the bank.</li>
    </ul>
    <p class="normal">Regarding the first goal, we have produced four models with bias mitigation methods that are objectively fairer than the base model, according to four fairness metrics (SPD, DI, AOD, EOD)—when comparing privileged and underprivileged age groups. However, only two of these models are intersectionally fairer using both age group and gender, according to DFBA (see <em class="italic">Figure 11.7</em>). We can still improve fairness significantly by combining methods, yet any one of the four models improves the base model.</p>
    <p class="normal">As for the second goal, the causal inference framework determined that any of the policies tested is better than no policy for both parties. Hooray! However, it yielded estimates that didn’t establish a single winning one. Still, as expected, the recommended policy varies according to the customer’s credit limit—on the other hand, if we aim to maximize bank profitability, we must factor in the average use of credit cards. The question of profitability presents two goals that we must reconcile: prescribing the recommended policies that benefit either the customer or the bank the most.</p>
    <p class="normal">For this reason, how to be procedurally fair is a complicated question with many possible answers, and any of the solutions would involve the bank absorbing some of the costs associated with implementing the policies. As for robustness, despite the flawed experiment, we can conclude that our estimates have a mediocre level of robustness, passing one robustness test but not the other. That being said, it all depends on what we consider robust enough to validate our findings. Ideally, we would ask the bank to start a new unbiased experiment but waiting another 6 months might not be feasible. </p>
    <p class="normal">In data science, we often find ourselves working with flawed experiments and biased data and have to make the most of it. Causal inference provides a way to do so by disentangling cause and effect, complete with estimates and their respective confidence intervals. We can then offer findings with all the disclaimers so that decision-makers can make informed decisions. Biased decisions lead to biased outcomes, so the moral imperative of tackling bias can start by shaping decision-making.</p>
    <h1 id="_idParaDest-332" class="heading-1">Summary</h1>
    <p class="normal">After reading this chapter, you should understand how bias can be detected visually and with metrics, both in data and models, then mitigated through preprocessing, in-processing, and post-processing methods. We also learned about causal inference by estimating heterogeneous treatment effects, making fair policy decisions with them, and testing their robustness. In the next chapter, we also discuss bias but learn how to tune models to meet several objectives, including fairness.</p>
    <h1 id="_idParaDest-333" class="heading-1">Dataset sources</h1>
    <p class="normal">Yeh, I. C., &amp; Lien, C. H. (2009). <em class="italic">The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients</em>. Expert Systems with Applications, 36(2), 2473-2480: <a href="https://dl.acm.org/doi/abs/10.1016/j.eswa.2007.12.020"><span class="url">https://dl.acm.org/doi/abs/10.1016/j.eswa.2007.12.020</span></a></p>
    <h1 id="_idParaDest-334" class="heading-1">Further reading</h1>
    <ul>
      <li class="bulletList">Chang, C., Chang, H.H., and Tien, J., 2017, <em class="italic">A Study on the Coping Strategy of Financial Supervisory Organization under Information Asymmetry: Case Study of Taiwan’s Credit Card Market</em>. Universal Journal of Management, 5, 429-436: <a href="http://doi.org/10.13189/ujm.2017.050903"><span class="url">http://doi.org/10.13189/ujm.2017.050903</span></a></li>
      <li class="bulletList">Foulds, J., and Pan, S., 2020, <em class="italic">An Intersectional Definition of Fairness</em>. 2020 IEEE 36th International Conference on Data Engineering (ICDE), 1918-1921: <a href="https://arxiv.org/abs/1807.08362"><span class="url">https://arxiv.org/abs/1807.08362</span></a></li>
      <li class="bulletList">Kamiran, F., and Calders, T., 2011, <em class="italic">Data preprocessing techniques for classification without discrimination</em>. Knowledge and Information Systems, 33, 1-33: <a href="https://link.springer.com/article/10.1007/s10115-011-0463-8"><span class="url">https://link.springer.com/article/10.1007/s10115-011-0463-8</span></a></li>
      <li class="bulletList">Feldman, M., Friedler, S., Moeller, J., Scheidegger, C., and Venkatasubramanian, S., 2015, <em class="italic">Certifying and Removing DI</em>. Proceedings of the 21st ACM SIGKDD International Conference on Knowledge Discovery and Data Mining: <a href="https://arxiv.org/abs/1412.3756"><span class="url">https://arxiv.org/abs/1412.3756</span></a></li>
      <li class="bulletList">Kamishima, T., Akaho, S., Asoh, H., and Sakuma, J., 2012, <em class="italic">Fairness-Aware Classifier with Prejudice Remover Regularizer</em>. ECML/PKDD: <a href="https://dl.acm.org/doi/10.5555/3120007.3120011"><span class="url">https://dl.acm.org/doi/10.5555/3120007.3120011</span></a></li>
      <li class="bulletList">A. Agarwal, A. Beygelzimer, M. Dudik, J. Langford, and H. Wallach, <em class="italic">A Reductions Approach to Fair Classification</em>, International Conference on Machine Learning, 2018. <a href="https://arxiv.org/pdf/1803.02453.pdf"><span class="url">https://arxiv.org/pdf/1803.02453.pdf</span></a></li>
      <li class="bulletList">Kearns, M., Neel, S., Roth, A., and Wu, Z., 2018, <em class="italic">Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness</em>. ICML: <a href="https://arxiv.org/pdf/1711.05144.pdf"><span class="url">https://arxiv.org/pdf/1711.05144.pdf</span></a></li>
      <li class="bulletList">Pleiss, G., Raghavan, M., Wu, F., Kleinberg, J., and Weinberger, K.Q., 2017, <em class="italic">On Fairness and Calibration</em>. NIPS: <a href="https://arxiv.org/abs/1709.02012"><span class="url">https://arxiv.org/abs/1709.02012</span></a></li>
      <li class="bulletList">Foster, D. and Syrgkanis, V., 2019, <em class="italic">Orthogonal Statistical Learning</em>. ICML: <a href="http://arxiv.org/abs/1901.09036"><span class="url">http://arxiv.org/abs/1901.09036</span></a></li>
    </ul>
    <h1 class="heading-1">Learn more on Discord</h1>
    <p class="normal">To join the Discord community for this book – where you can share feedback, ask the author questions, and learn about new releases – follow the QR code below:</p>
    <p class="normal"><a href="Chapter_11.xhtml"><span class="url">https://packt.link/inml</span></a></p>
    <p class="normal"><img src="../Images/QR_Code107161072033138125.png" alt="" role="presentation"/></p>
  </div>
</body></html>