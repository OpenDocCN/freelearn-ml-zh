<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Convolutional Neural Networks - MNIST Handwriting Recognition</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the previous chapter, I posited a scenario where you are a postal worker trying to recognize handwriting. In that, we ended up with a neural network built on top of Gorgonia. In this chapter, we'll look at the same scenario, but we'll augment our ideas of what a neural network is and write a more advanced neural network, one that was, until very recently, state of the art.</p>
<p class="mce-root">Specifically, in this chapter, we are going to build a <strong>Convolutional Neural Network</strong> (<strong>CNN</strong>). A CNN is a type of deep learning network that has been popular in recent years.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Everything you know about neurons is wrong </h1>
                </header>
            
            <article>
                
<p class="mce-root">In the previous chapter, I mentioned that everything you know about neural networks is wrong. Here, I repeat that claim. Most literature out there on a neural network starts with a comparison with biological neurones and ends there. This leads readers to often assume that it is. I'd like to make a point that artificial neural networks are <em>nothing</em> like their biological namesake.</p>
<p class="mce-root">Instead, in the last chapter, I spent a significant amount of the chapter describing linear algebra, and explained that the twist is that you can express almost any <strong>machine learning</strong> (<strong>ML</strong>) problem as linear algebra. I shall continue to do so in this chapter.</p>
<p class="mce-root">Rather than think of artificial neural networks as analogies of real-life neural networks, I personally encourage you to think of artificial neural networks as mathematical equations. The non-linearities introduced by the activation functions, combined with linear combinations allows for artificial neural networks to be able to approximate any function.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Neural networks – a redux</h1>
                </header>
            
            <article>
                
<p class="mce-root">The fundamental understanding that neural networks are mathematical expressions leads to really simple and easy implementations of neural networks. Recall from the previous chapter that a neural network can be written like this:</p>
<pre class="mce-root">func affine(weights [][]float64, inputs []float64) []float64 {<br/>  return activation(matVecMul(weights, inputs))<br/>}</pre>
<p class="mce-root">If we rewrite the code as a mathematical equation, we can write a neural network like this:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/a67670dc-893a-4308-9b31-462f96a5a349.png" style="width:11.25em;height:1.92em;" width="1350" height="230"/></p>
<div class="mce-root packt_tip">A side note: <img class="fm-editor-equation" src="Images/86262bf8-91aa-45b1-a951-f2db5da20de6.png" style="width:2.42em;height:1.50em;" width="290" height="180"/>is the same as <img class="fm-editor-equation" src="Images/d6732aaf-6223-4477-af91-f022c81b9b92.png" style="width:2.00em;height:1.00em;" width="240" height="120"/>.</div>
<p class="mce-root">We can simply write it out using Gorgonia, like this:</p>
<pre>import (<br/>  G "gorgonia.org/gorgonia"<br/>)<br/><br/>var Float tensor.Float = tensor.Float64<br/>func main() {<br/>  g := G.NewGraph()<br/>  x := G.NewMatrix(g, Float, G.WithName("x"), G.WithShape(N, 728))<br/>  w := G.NewMatrix(g, Float, G.WithName("w"), G.WithShape(728, 800), <br/>       G.WithInit(G.Uniform(1.0)))<br/>  b := G.NewMatrix(g, Float, G.WithName("b"), G.WithShape(N, 800), <br/>       G.WithInit(G.Zeroes()))<br/>  xw, _ := G.Mul(x, w)<br/>  xwb, _ := G.Add(xw, b)<br/>  act, _ := G.Sigmoid(xwb)<br/><br/>  w2 := G.NewMatrix(g, Float, G.WithName("w2"), G.WithShape(800, 10), <br/>        G.WithInit(G.Uniform(1.0)))<br/>  b2 := G.NewMatrix(g, Float, G.WithName("b2"), G.WithShape(N, 10), <br/>        G.WithInit(G.Zeroes()))<br/>  xw2, _ := G.Mul(act, w2)<br/>  xwb2, _ := G.Add(xw2, b2)<br/>  sm, _ := G.SoftMax(xwb2)<br/>}</pre>
<p class="mce-root">The preceding code is a representation of the following neural network in images:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-321 image-border" src="Images/60f87787-65ce-40e1-b7a6-f4037719a3c5.png" style="width:30.83em;height:23.00em;" width="1322" height="985"/></p>
<p class="mce-root">The middle layer consists of 800 hidden units.</p>
<p class="mce-root">Of course, the preceding code hides a lot of things. You can't really expect a neural network from scratch in fewer than 20 lines, can you? To understand what is happening, we need to take a brief detour into understanding what Gorgonia is.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Gorgonia</h1>
                </header>
            
            <article>
                
<p class="mce-root">Gorgonia is a library that provides primitives for working with mathematical expressions specific to deep learning. When working with a ML related project, you will start to find yourself more introspective about the world, and questioning assumptions all the time. This is a good thing.</p>
<p class="mce-root">Consider what happens in your mind when you read the following mathematical expression:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/2fbb360a-4c40-41b7-987d-ac97f71891a4.png" style="width:6.25em;height:1.42em;" width="750" height="170"/></p>
<p class="mce-root">You should instantly think <em>hang on, that's false</em>. Why does your brain think this?</p>
<p class="mce-root">That's mainly because your brain evaluated the mathematical expression. In general, there are three parts to the expression: the left-hand side, the equal symbol, and the right-hand side. Your brain evaluated each part separately and then evaluated the expression as false.</p>
<p class="mce-root">When we read mathematical expressions, we automatically evaluate the expressions in our mind that we take evaluation for granted. In Gorgonia, what we take for granted is made explicit. There are two general <em>parts</em> to using Gorgonia: defining an expression and evaluating an expression.</p>
<p class="mce-root">Since you are most probably a programmer, you can think of the first part as writing a program, and the second part can be thought of as running a program.</p>
<p class="mce-root">When describing a neural network in Gorgonia, it's often instructive to imagine yourself writing in another programming language, one that is specific to building neural networks. This is because the patterns used in Gorgonia are not unlike a new programming language. Indeed, Gorgonia was built from ground-up with the idea that it's a programming language without a syntactical frontend. As such, in this section, I will often ask you to imagine writing in another Go-like language.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Why?</h1>
                </header>
            
            <article>
                
<p class="mce-root">A good question to ask is <em>why?</em> Why bother with this separation of processes? After all, the preceding code could be rewritten as the previous chapter's <kbd>Predict</kbd> function:</p>
<pre class="mce-root">func (nn *NN) Predict(a tensor.Tensor) (int, error) {<br/>  if a.Dims() != 1 {<br/>    return nil, errors.New("Expected a vector")<br/>  }<br/><br/>  var m maybe<br/>  act0 := m.sigmoid(m.matVecMul(nn.hidden, a))<br/>  pred := m.sigmoid(m.matVecMul(nn.final, act0))<br/>  if m.err != nil {<br/>    return -1, m.err<br/>  }<br/>  return argmax(pred.Data().([]float64)), nil<br/>}</pre>
<p class="mce-root">Here, we define the network in Go, and when we run the Go code, the neural network is run as it is being defined. What's the problem we face that we need to introduce the idea of separating the definition of the neural network and running it? We've already seen the problem when we wrote the <kbd>Train</kbd> method.</p>
<p class="mce-root">If you recall, in the last chapter, I said that writing the <kbd>Train</kbd> method requires us to actually copy and paste code from the <kbd>Predict</kbd> method. To refresh your memory, here's the <kbd>Train</kbd> method:</p>
<pre class="mce-root">// X is the image, Y is a one hot vector<br/>func (nn *NN) Train(x, y tensor.Tensor, learnRate float64) (cost float64, err error) {<br/>  // predict<br/>  var m maybe<br/>  m.reshape(x, s.Shape()[0], 1)<br/>  m.reshape(y, 10, 1)<br/>  act0 := m.sigmoid(m.matmul(nn.hidden, x))<br/>  pred := m.sigmoid(m.matmul(nn.final, act0))<br/><br/>  // backpropagation.<br/>  outputErrors := m.sub(y, pred))<br/>  cost = sum(outputErrors.Data().([]float64))<br/><br/>  hidErrs := m.do(func() (tensor.Tensor, error) {<br/>    if err := nn.final.T(); err != nil {<br/>      return nil, err<br/>    }<br/>    defer nn.final.UT()<br/>    return tensor.MatMul(nn.final, outputErrors)<br/>  })<br/>  dpred := m.mul(m.dsigmoid(pred), outputErrors, tensor.UseUnsafe())<br/>  dpred_dfinal := m.dmatmul(outputErrors, act0)<br/>    if err := act0.T(); err != nil {<br/>      return nil, err<br/>    }<br/>    defer act0.UT()<br/>    return tensor.MatMul(outputErrors, act0)<br/>  })<br/><br/>  m.reshape(m.mul(hidErrs, m.dsigmoid(act0), tensor.UseUnsafe()), <br/>                  hidErrs.Shape()[0], 1)<br/>  dcost_dhidden := m.do(func() (tensor.Tensor, error) {<br/>    if err := x.T(); err != nil {<br/>      return nil, err<br/>    }<br/>    defer x.UT()<br/>    return tensor.MatMul(hidErrs, x)<br/>  })<br/><br/>  // gradient update<br/>  m.mul(dpred_dfinal, learnRate, tensor.UseUnsafe())<br/>  m.mul(dcost_dhidden, learnRate, tensor.UseUnsafe())<br/>  m.add(nn.final, dpred_dfinal, tensor.UseUnsafe())<br/>  m.add(nn.hidden, dcost_dhidden, tensor.UseUnsafe())<br/>  return cost, m.err<br/>}</pre>
<p class="mce-root">Let's go through an exercise of refactoring to highlight the problem. Taking off our ML hat for a bit, and putting on our software engineer hat, let's see how we can refactor <kbd>Train</kbd> and <kbd>Predict</kbd>, even if conceptually. We see in the <kbd>Train</kbd> method that we need access to <kbd>act0</kbd> and <kbd>pred</kbd> in order to backpropagate the errors. Where in <kbd>Predict</kbd> <kbd>act0</kbd> and <kbd>pred</kbd> are terminal values (that is, we don't use them after the function has returned), in <kbd>Train</kbd>, they are not.</p>
<p class="mce-root">So, here, we can create a new method; let's call it <kbd>fwd</kbd>:</p>
<pre class="mce-root">func (nn *NN) fwd(x tensor.Tensor) (act0, pred tensor.Tensor, err error) {<br/>  var m maybe<br/>  m.reshape(x, s.Shape()[0], 1)<br/>  act0 := m.sigmoid(m.matmul(nn.hidden, x))<br/>  pred := m.sigmoid(m.matmul(nn.final, act0))<br/>  return act0, pred, m.err<br/>}</pre>
<p class="mce-root">And we can refactor <kbd>Predict</kbd> to look like this:</p>
<pre class="mce-root">func (nn *NN) Predict(a tensor.Tensor) (int, error) {<br/>  if a.Dims() != 1 {<br/>    return nil, errors.New("Expected a vector")<br/>  }<br/><br/>  var err error<br/>  var pred tensor.Tensor<br/>  if _, pred, err = nn.fwd(a); err!= nil {<br/>    return -1, err<br/>  }<br/>  return argmax(pred.Data().([]float64)), nil<br/>}</pre>
<p class="mce-root">And the <kbd>Train</kbd> method would look like this:</p>
<pre class="mce-root">// X is the image, Y is a one hot vector<br/>func (nn *NN) Train(x, y tensor.Tensor, learnRate float64) (cost float64, err error) {<br/>  // predict<br/>  var act0, pred tensor.Tensor<br/>  if act0, pred, err = nn.fwd(); err != nil {<br/>    return math.Inf(1), err<br/>  }<br/><br/>  var m maybe<br/>  m.reshape(y, 10, 1)<br/>  // backpropagation.<br/>  outputErrors := m.sub(y, pred))<br/>  cost = sum(outputErrors.Data().([]float64))<br/><br/>  hidErrs := m.do(func() (tensor.Tensor, error) {<br/>    if err := nn.final.T(); err != nil {<br/>      return nil, err<br/>    }<br/>    defer nn.final.UT()<br/>    return tensor.MatMul(nn.final, outputErrors)<br/>  })<br/>  dpred := m.mul(m.dsigmoid(pred), outputErrors, tensor.UseUnsafe())<br/>  dpred_dfinal := m.dmatmul(outputErrors, act0)<br/>    if err := act0.T(); err != nil {<br/>      return nil, err<br/>    }<br/>    defer act0.UT()<br/>    return tensor.MatMul(outputErrors, act0)<br/>  })<br/><br/>  m.reshape(m.mul(hidErrs, m.dsigmoid(act0), tensor.UseUnsafe()), <br/>                  hidErrs.Shape()[0], 1)<br/>  dcost_dhidden := m.do(func() (tensor.Tensor, error) {<br/>    if err := x.T(); err != nil {<br/>      return nil, err<br/>    }<br/>    defer x.UT()<br/>    return tensor.MatMul(hidErrs, x)<br/>  })<br/><br/>  // gradient update<br/>  m.mul(dpred_dfinal, learnRate, tensor.UseUnsafe())<br/>  m.mul(dcost_dhidden, learnRate, tensor.UseUnsafe())<br/>  m.add(nn.final, dpred_dfinal, tensor.UseUnsafe())<br/>  m.add(nn.hidden, dcost_dhidden, tensor.UseUnsafe())<br/>  return cost, m.err<br/>}</pre>
<p class="mce-root">This looks better. What exactly are we doing here? We are programming. We are rearranging one form of syntax into another form of syntax but we are not changing the semantics, the meaning of the program. The refactored program has exactly the same meaning as the pre-refactored program.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Programming</h1>
                </header>
            
            <article>
                
<p class="mce-root">Wait a minute, you might say to yourself. What do I mean by <em>the meaning of the program?</em> This is a surprisingly deep topic that involves a whole branch of mathematics known as <strong>homotopy</strong>. But for all practical intents and purposes of this chapter, let's define the <em>meaning</em> of a program to be the extensional definition of the program. If two programs compile and run, take the exact same inputs, and return the same exact output every time, we say two programs are equal.</p>
<p class="mce-root">These two programs would be equal:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<thead>
<tr>
<td>
<p><strong>Program A</strong></p>
</td>
<td>
<p><strong>Program B</strong></p>
</td>
</tr>
</thead>
<tbody>
<tr>
<td>
<p><kbd>fmt.Println("Hello World")</kbd></p>
</td>
<td>
<p><kbd>fmt.Printf("Hello " + "World\n")</kbd></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p class="mce-root">Intentionally, if we visualize the programs as an <strong>Abstract Syntax Tree</strong> (<strong>AST</strong>), they look slightly different:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-503 image-border" src="Images/ec6b6303-8a9f-448e-9cae-10d9465a6c30.png" style="width:36.67em;height:15.92em;" width="798" height="347"/></p>
<p class="CDPAlignCenter CDPAlign"><img style="font-size: 1em;width:35.75em;height:18.33em;" class="alignnone size-full wp-image-504 image-border" src="Images/80b016f8-e352-4740-8f0d-c65c83060bea.png" width="866" height="443"/></p>
<p class="mce-root">The syntax for both programs are different, but they are semantically the same. We can refactor program B into program A, by eliminating the <kbd>+</kbd>.</p>
<p class="mce-root">But note what we did here: we took a program and represented it as an AST. Through syntax, we manipulated the AST. This is the essence of programming.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">What is a tensor? – part 2</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the previous chapter, there was an info box that introduced the concept of a tensor. That info box was a little simplified. If you google what a tensor is, you will get very conflicting results, which only serve to confuse. I don't want to add to the confusion. Instead, I shall only briefly touch on tensors in a way that will be relevant to our project, and in a way very much like how a typical textbook on Euclidean geometry introduces the concept of a point: by holding it to be self-evident from use cases.</p>
<p class="mce-root">Likewise, we will hold tensors to be self-evident from use. First, we will look at the concept of multiplication:</p>
<ul>
<li class="mce-root">First, let's define a vector: <img class="fm-editor-equation" src="Images/a86a3ad1-6ff3-4633-b115-91b671301d9f.png" style="width:4.17em;height:1.00em;" width="900" height="220"/>. You can think of it as this diagram:</li>
</ul>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-334 image-border" src="Images/1d1d9e18-693e-42c1-9b0e-12a6aff3c24c.png" style="width:12.50em;height:8.25em;" width="498" height="325"/></p>
<p class="mce-root"/>
<ul>
<li class="mce-root">Next, let's multiply the vector by a scalar value: <img class="fm-editor-equation" src="Images/90b2efe0-f4c4-441f-9276-a22d5023c6bc.png" style="width:0.50em;height:0.92em;" width="90" height="160"/>. The result is something like this:</li>
</ul>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-335 image-border" src="Images/6b4e7f1c-38da-44ca-a8f5-cc1d5e185ccc.png" style="width:12.50em;height:10.75em;" width="546" height="468"/></p>
<p class="mce-root">There are two observations:</p>
<ul>
<li class="mce-root">The general direction of the arrow doesn't change.</li>
<li class="mce-root">Only the length changes. In physics terms, this is called the magnitude. If the vector represents the distance travelled, you would have traveled twice the distance along the same direction.</li>
</ul>
<p class="mce-root">So, how would you change directions by using multiplications alone? What do you have to multiply to change directions? Let's try the following matrix, which we will call <em>T</em>, for transformation:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/5262facb-1ac0-4216-b920-b4e20e163d78.png" style="width:9.33em;height:3.42em;" width="1310" height="480"/></p>
<p class="mce-root">Now if we multiply the transformation matrix with the vector, we get the following:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/93630693-f819-481c-b93d-4fa5b9ffca44.png" style="width:11.58em;height:7.50em;" width="1870" height="1210"/></p>
<p class="mce-root"/>
<p class="mce-root">And if we plot out the starting vector and the ending vector, we get the resultant output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-594 image-border" src="Images/b70d5c40-fdf6-479a-9405-1f361beef36e.png" style="width:25.08em;height:24.50em;" width="798" height="780"/></p>
<p class="mce-root">As we can see, the direction has changed. The magnitude too has changed.</p>
<p class="mce-root">Now, you might be saying, <em>hang on, isn't this just Linear Algebra 101?</em>. Yes, it is. But to really understand a tensor, we must learn how to construct one. The matrix that we just used is also a tensor of rank-2. The proper name for a tensor of rank-2 is a <strong>dyad</strong>.</p>
<div class="mce-root packt_infobox">Why the mixing of naming conventions? Here's a bit of fun trivia. When I was writing the earliest versions of Gorgonia, I was musing about the terrible naming conventions computer science has had, a fact that Bjarne Stroustrup himself lamented. The canonical name for a rank-2 tensor is called a <strong>dyad</strong>, but can be represented as a matrix. I was struggling to properly call it; after all, there are power in names and to name it is to tame it.<br/>
At around the same time as I was developing the earliest versions of Gorgonia, I was following a most excellent BBC TV series called <strong>Orphan Black</strong>, in which the Dyad Institute is the primary foe of the protagonists. They were quite villainous and that clearly left an impact in my mind. I decided against naming it thus. In retrospect, this seemed like a rather silly decision.</div>
<p class="mce-root">Now let's consider the transformation dyad. You can think of the dyad as a vector <em>u</em> times a vector <em>v</em>. To write it out in equation form:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/d7f28330-0c81-4a14-bff1-67c135e3574d.png" style="width:4.92em;height:5.00em;" width="690" height="700"/></p>
<p class="mce-root">At this point, you may be familiar with the previous chapter's notion of linear algebra. You might think to yourself: if two vectors multiply, that'd end up with a scalar value, no? If so, how would you multiply two vectors and get a matrix out of it?</p>
<p class="mce-root">Here, we'd need to introduce a new type of multiplication: the outer product (and by contrast, the multiplication introduced in the previous chapter is an inner product). We write outer products with this symbol: <img class="fm-editor-equation" src="Images/0fb20035-e86b-4189-9050-11b52ee9f24d.png" style="width:1.25em;height:1.33em;" width="280" height="290"/>.</p>
<p class="mce-root">Specifically speaking, the outer product, also known as a dyad product, is defined as such:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/e6158f0e-afbe-4214-a383-6a9f8d1a8d38.png" style="width:15.83em;height:9.58em;" width="2300" height="1390"/></p>
<p class="mce-root">We won't be particularly interested in the specifics of <em>u</em> and <em>v</em> in this chapter. However, being able to construct a dyad from its constituent vectors is an integral part of what a tensor is all about.</p>
<p class="mce-root">Specifically, we can replace <em>T</em> with <em>uv</em>:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/862fd95c-5ca3-470f-a68d-7ae0fa0690ca.png" style="width:7.92em;height:7.92em;" width="1190" height="1190"/></p>
<p class="mce-root">Now we get <img class="fm-editor-equation" src="Images/c1e433e0-9b4e-475b-bd90-2d3d079cd553.png" style="width:0.58em;height:0.92em;" width="110" height="170"/> as the scalar magnitude change and <em>u</em> as the directional change.</p>
<p class="mce-root">So what is the big fuss with tensors? I can give two reasons.</p>
<p class="mce-root">Firstly, the idea that dyads can be formed from vectors generalizes upward. A three-tensor, or triad can be formed by a dyad product <em>uvw</em>, a four-tensor or a tetrad can be formed by a dyad product <em>uvwx</em>, and so on and so forth. This affords us a mental shortcut that will be very useful to us when we see shapes that are associated with tensors.</p>
<p class="mce-root">The useful mental model of what a tensor can be thought as is the following: a vector is like a list of things, a dyad is like a list of vectors, a triad is like a list of dyads, and so on and so forth. This is absolutely helpful when thinking of images, like those that we've seen in the previous chapter:</p>
<p class="mce-root">An image can be seen as a (28, 28) matrix. A list of ten images would have the shape (10, 28, 28). If we wanted to arrange the images in such a way that it's a list of lists of ten images, it'd have a shape of (10, 10, 28, 28).</p>
<p class="mce-root">All this comes with a caveat of course: a tensor can only be defined in the presence of transformation. As a physics professor once told me: <em>that which transforms like a tensor is a tensor</em>. A tensor devoid of any transformation is just an <em>n</em>-dimensional array of data. The data must transform, or flow from tensor to tensor in an equation. In this regards, I think that TensorFlow is a ridiculously well-named product.</p>
<div class="mce-root packt_tip">For more information on tensors, I would recommend the relatively dense text book, <em>Linear Algebra and Geometry</em> by Kostrikin (I failed to finish this book, but it was this book that gave me what I believe to be a strong-ish understanding of tensors). More on the flow of tensors can be found in Spivak's <em>Manifold Calculus</em>.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">All expressions are graphs</h1>
                </header>
            
            <article>
                
<p class="mce-root">Now we can finally return to the preceding example.</p>
<p class="mce-root">Our problem, if you recall, is that we had to specify the neural network twice: once for prediction and once for learning purposes. We then refactored the program so that we don't have to specify the network twice. Additionally, we had to manually write out the expression for the backpropagation. This is error prone, especially when dealing with larger neural networks like the one we're about to build in this chapter. Is there a better way? The answer is yes.</p>
<p class="mce-root">Once we understand and fully internalize that neural networks are essentially mathematical expressions, we can take the learning's from tensors, and model a neural network where the entire neural network is a flow of tensors.</p>
<p class="mce-root">Recall that tensors can only be defined in the presence of transformation; then, any operation that transforms tensor(s), used in concert with data structures that hold data are tensors. Also, recall that computer programs can be represented as abstract syntax trees. Mathematical expressions can be represented as a program. Therefore, mathematical expressions can also be represented as an abstract syntax tree.</p>
<p class="mce-root">More accurate, however, is that mathematical expressions can be expressed as a graph; a directed acyclic graph, to be specific. We call this the <strong>expression graph</strong>.</p>
<p class="mce-root">This distinction matters. Trees cannot share nodes. Graphs can. Let's consider, for example, the following mathematical expression:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/773f21bd-f1bd-4f85-bed9-d3b4351558ad.png" style="width:9.17em;height:1.67em;" width="1100" height="200"/></p>
<p class="mce-root">Here are the representations as a graph and as a tree:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-505 image-border" src="Images/042d0364-e315-454a-8a18-85ca98bcb637.png" style="width:65.58em;height:28.92em;" width="787" height="347"/></p>
<p class="mce-root">On the left, we have a directed acyclic graph, and on the right, we have a tree. Note that in the tree variant of the mathematical equation, there are repeat nodes. Both are rooted at <img class="fm-editor-equation" src="Images/bc041d3c-1879-480d-b2c1-ee4e3e65713e.png" style="width:6.92em;height:1.25em;" width="1100" height="200"/>. The arrow should be read as <em>depends on</em>. <img class="fm-editor-equation" src="Images/1ffd83f8-66fd-437d-ab43-bc22d9502e7e.png" style="width:6.92em;height:1.25em;" width="1100" height="200"/> depends on two other nodes, <img class="fm-editor-equation" src="Images/0d90444f-20a9-498d-8d65-11d5c6b644fe.png" style="width:6.42em;height:1.17em;" width="940" height="170"/> and <img class="fm-editor-equation" src="Images/d4029dca-e311-4e99-ad95-9648fbdf599d.png" style="width:6.17em;height:1.08em;" width="970" height="170"/>, and so on and so forth.</p>
<p class="mce-root">Both the graph and tree are valid representations of the same mathematical equation, of course.</p>
<p class="mce-root">Why bother representing a mathematical expression as a graph or a tree? Recall that an abstract syntax tree represents a computation. If a mathematical expression, represented as a graph or a tree, has a shared notion of computation, then it also represents an abstract syntax tree.</p>
<p class="mce-root">Indeed, we can take each node in the graph or tree, and perform a computation on it. If each node is a representation of a computation, then logic holds that fewer nodes means faster computations (and less memory usage). Therefore, we should prefer to use the directed acyclic graph representation.</p>
<p class="mce-root">And now we come to the major benefit of representing a mathematical expression as a graph: we get differentiation for free.</p>
<p class="mce-root">If you recall from the previous chapter, backpropagation is essentially differentiating the cost with regards to the inputs. The gradients, once calculated, can then be used to update the values of the weights themselves. Having a graph structure, we wouldn't have to write the backpropagation parts. Instead, if we have a virtual machine that executes the graph, starting at the leaves and moving toward the root, the virtual machine can automatically perform differentiation on the values as it traverses the graph from leaf to root.</p>
<p class="mce-root">Alternatively, if we don't want to do automatic differentiation, we can also perform symbolic differentiation by manipulating the graph in the same way that we manipulated the AST in the <em>What is programming</em> section, by adding and coalescing nodes.</p>
<p class="mce-root">In this way, we can now shift our view of a neural network to this:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="Images/145f53a8-6df1-467b-9981-1b1ec6976d5f.png" style="width:13.67em;height:20.83em;" width="227" height="347"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Describing a neural network</h1>
                </header>
            
            <article>
                
<p class="mce-root">Now let's get back to the task of writing a neural network and thinking of it in terms of a mathematical expression expressed as a graph. Recall that the code looks something like this:</p>
<pre class="mce-root">import (<br/>  G "gorgonia.org/gorgonia"<br/>)<br/><br/>var Float tensor.Float = tensor.Float64<br/>func main() {<br/>  g := G.NewGraph()<br/>  x := G.NewMatrix(g, Float, G.WithName("x"), G.WithShape(N, 728))<br/>  w := G.NewMatrix(g, Float, G.WithName("w"), G.WithShape(728, 800), <br/>                   G.WithInit(G.Uniform(1.0)))<br/>  b := G.NewMatrix(g, Float, G.WithName("b"), G.WithShape(N, 800), <br/>                   G.WithInit(G.Zeroes()))<br/>  xw, _ := G.Mul(x, w)<br/>  xwb, _ := G.Add(xw, b)<br/>  act, _ := G.Sigmoid(xwb)<br/><br/>  w2 := G.NewMatrix(g, Float, G.WithName("w2"), G.WithShape(800, 10), <br/>                    G.WithInit(G.Uniform(1.0)))<br/>  b2 := G.NewMatrix(g, Float, G.WithName("b2"), G.WithShape(N, 10),  <br/>                    G.WithInit(G.Zeroes()))<br/>  xw2, _ := G.Mul(act, w2)<br/>  xwb2, _ := G.Add(xw2, b2)<br/>  sm, _ := G.SoftMax(xwb2)<br/>}</pre>
<p class="mce-root">Now let's go through this code.</p>
<p class="mce-root">First, we create a new expression graph with <kbd>g := G.NewGraph()</kbd>. An expression graph is a holder object to hold the mathematical expression. Why would we want an expression graph? The mathematical expression that represents a neural network is contained in the <kbd>*gorgonia.ExpressionGraph</kbd> object.</p>
<p class="mce-root">Mathematical expressions are only interesting if we use variables. <img class="fm-editor-equation" src="Images/65a9f649-813c-4549-bf9e-216a9ab70c7f.png" style="width:4.42em;height:1.00em;" width="750" height="170"/> is quite an uninteresting expression because you can't do much with this expression. The only thing you can do with it is to evaluate the expression and see if it returns true or false. <img class="fm-editor-equation" src="Images/dfaa4a00-b8a2-4fea-98fe-e69fc78d969c.png" style="width:4.83em;height:1.08em;" width="760" height="170"/> is slightly more interesting. But, then again, <em>a</em> can only be <em>1</em>.</p>
<p class="mce-root">Consider, however, the expression <img class="fm-editor-equation" src="Images/f4cd11b6-41d0-4a24-bbd3-657288d1aa15.png" style="width:4.75em;height:1.08em;" width="750" height="170"/>. With two variables, it suddenly becomes a lot more interesting. The values that <em>a</em> and <em>b</em> can take are dependent on one another, and there is a whole range of possible pairs of numbers that can fit into <em>a </em>and <em>b</em>.</p>
<p class="mce-root">Recall that each layer of neural network is just a mathematical expression that reads like this: <img class="fm-editor-equation" src="Images/3f3d99df-f160-4131-b517-a183b5f40033.png" style="width:7.83em;height:1.33em;" width="1350" height="230"/>. In this case, <em>w</em>, <em>x</em>, and <em>b</em> are variables. So, we create them. Note that in this case, Gorgonia treats variables as a programming language does: you have to tell the system what the variable represents.</p>
<p class="mce-root">In Go, you would do that by typing <kbd>var x Foo</kbd>, which tells the Go compiler that <kbd>x</kbd> should be a type <kbd>Foo</kbd>. In Gorgonia, the mathematical variables are declared by using <kbd>NewMatrix</kbd>, <kbd>NewVector</kbd>, <kbd>NewScalar</kbd>, and <kbd>NewTensor</kbd>. <kbd>x := G.NewMatrix(g, Float, G.WithName, G.WithShape(N, 728))</kbd> simply says <kbd>x</kbd> is a matrix in expression graph <kbd>g</kbd> with a name <kbd>x</kbd>, and has a shape of <kbd>(N, 728)</kbd>.</p>
<p class="mce-root">Here, readers may observe that <kbd>728</kbd> is a familiar number. In fact, what this tells us is that <kbd>x</kbd> represents the input, which is <kbd>N</kbd> images. <kbd>x</kbd>, therefore, is a matrix of <em>N</em> rows, where each row represents a single image (728 floating points).</p>
<p class="mce-root">The eagle-eyed reader would note that <kbd>w</kbd> and <kbd>b</kbd> have extra options, where the declaration of <kbd>x</kbd> does not. You see, <kbd>NewMatrix</kbd> simply declares the variable in the expression graph. There is no value associated with it. This allows for flexibility when the value is attached to a variable. However, with regards to the weight matrix, we want to start the equation with some initial values. <kbd>G.WithInit(G.Uniform(1.0))</kbd> is a construction option that populates the weight matrix with values pulled from a uniform distribution with a gain of <kbd>1.0</kbd>. If you imagine yourself coding in another language specific to building neural networks, it'd look something like this: <kbd>var w Matrix(728, 800) = Uniform(1.0)</kbd>.</p>
<p class="mce-root">Following that, we simply write out the mathematical equation: <img class="fm-editor-equation" src="Images/4cdbb766-d42f-4356-bf64-0644b8de86ab.png" style="width:2.00em;height:1.00em;" width="240" height="120"/> is simply a matrix multiplication between <img class="fm-editor-equation" src="Images/4979cf68-4f3a-4b41-9ec1-5c4ecd4d150d.png" style="width:0.92em;height:0.92em;" width="110" height="110"/> and <img class="fm-editor-equation" src="Images/641b17c8-e982-4a2d-a7e1-9affa6afae46.png" style="width:1.08em;height:1.00em;" width="130" height="120"/>; hence, <kbd>xw, _ := G.Mul(x, w)</kbd>. At this point, it should be clarified that we are merely describing the computation that is supposed to happen. It is yet to happen. In this way, it is not dissimilar to writing a program; writing code does not equal running the program.</p>
<p class="mce-root"><kbd>G.Mul</kbd> and most operations in Gorgonia actually returns an error. For the purposes of this demonstration, we're ignoring any errors that may arise from symbolically multiplying <kbd>x</kbd> and <kbd>w</kbd>. What could possibly go wrong with simple multiplication? Well, we're dealing with matrix multiplication, so the shapes must have matching inner dimensions. A (N, 728) matrix can only be multiplied by a (728, M) matrix, which leads to an (N, M) matrix. If the second matrix does not have 728 rows, then an error will happen. So, in real production code, error handling is a<strong> must</strong>.</p>
<p class="mce-root">Speaking of <em>must</em>, Gorgonia comes with a utility function, called, <strong>G.Must</strong>. Taking a cue from the <kbd>text/template</kbd> and <kbd>html/template</kbd> libraries found in the standard library, the <kbd>G.Must</kbd> function panics when an error occur. To use, simply write this: <kbd>xw := G.Must(G.Mul(x,w))</kbd>.</p>
<p class="mce-root">After the inputs are multiplied with the weights, we add to the biases using <kbd>G.Add(xw, b)</kbd>. Again, errors may occur, but in this example, we're eliding the checks of errors.</p>
<p class="mce-root">Lastly, we take the result and perform a non-linearity: a sigmoid function, with <kbd>G.Sigmoid(xwb)</kbd>. This layer is now complete. Its shape, if you follow, would be (N, 800).</p>
<p class="mce-root">The completed layer is then used as an input for the following layer. The next layer has a similar layout as the first layer, except instead of a sigmoid non-linearity, a <kbd>G.SoftMax</kbd> is used. This ensures that each row in the resulting matrix sums 1.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">One-hot vector</h1>
                </header>
            
            <article>
                
<p class="mce-root">Perhaps, not so coincidentally, the last layer has the shape of (N, 10). N is the number of input images (which we've gotten from <kbd>x</kbd>) ; that's fairly self-explanatory. It also means that there is a clean mapping from input to output. What's not self-explanatory is the 10. Why 10? Simply put, there are 10 possible numbers we want to predict - 0, 1, 2, 3, 4, 5, 6, 7, 8, 9:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-542 image-border" src="Images/c8df17fe-0447-45c6-9949-e9ceb0e44c7b.png" style="width:19.42em;height:15.83em;" width="448" height="365"/></p>
<p class="mce-root">The preceding diagram is an example result matrix. Recall that we used <kbd>G.SoftMax</kbd> to ensure that each row sums up to 1. Therefore, we can interpret the numbers in each column of each row to be the probability that it is the specific digit that we're predicting. To find the digit we're predicting, simply find the highest probability in each column.</p>
<p class="mce-root"/>
<p class="mce-root">In the previous chapter, I introduced the concept of one-hot vector encoding. To recap, it takes a slice of labels and returns a matrix.</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/3a72b6c1-df67-44cf-a666-3c493e8dc6fc.png" style="width:14.92em;height:29.25em;" width="262" height="514"/></p>
<p class="mce-root">Now, this is clearly a matter of encoding. Who's to say that column 0 would have to represent 0? We could of course come up with a completely crazy encoding like such and the neural network would still work:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/f68f2ab0-abf5-4b28-8ff1-3fd91db7c3a7.png" style="width:11.83em;height:24.67em;" width="252" height="526"/></p>
<p class="mce-root">Of course, we would not be using such a scheme for encoding; it would be a massive source of programmer error. Instead, we would go for the standard encoding of a one-hot vector.</p>
<p class="mce-root">I hope this has given you a taste of how powerful the notion of an expression graph can be. One thing we haven't touched upon yet is the execution of the graph. How do you run a graph? We'll look further into that in the next section.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The project</h1>
                </header>
            
            <article>
                
<p class="mce-root">With all that done, it's time to get on to the project! Once again, we are going to recognize handwritten digits. But this time around, we're going to build a CNN for that. Instead of just using the <kbd>tensor</kbd> package of Gorgonia, this time we're going to use all of Gorgonia.</p>
<p class="mce-root">Once again, to install Gorgonia, simply run <kbd>go get -u gorgonia.org/gorgonia</kbd> and <kbd>go get -u gorgonia.org/tensor</kbd>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting the data</h1>
                </header>
            
            <article>
                
<p class="mce-root">The data is the same data as in the previous chapter: the MNIST dataset. It can be found in the repository for this chapter, and we'll be using a function we wrote in the previous chapter to acquire the data:</p>
<pre class="mce-root">// Image holds the pixel intensities of an image.<br/>// 255 is foreground (black), 0 is background (white).<br/>type RawImage []byte<br/><br/>// Label is a digit label in 0 to 9<br/>type Label uint8<br/><br/><br/>const numLabels = 10<br/>const pixelRange = 255<br/><br/>const (<br/>  imageMagic = 0x00000803<br/>  labelMagic = 0x00000801<br/>  Width = 28<br/>  Height = 28<br/>)<br/><br/>func readLabelFile(r io.Reader, e error) (labels []Label, err error) {<br/>  if e != nil {<br/>    return nil, e<br/>  }<br/><br/>  var magic, n int32<br/>  if err = binary.Read(r, binary.BigEndian, &amp;magic); err != nil {<br/>    return nil, err<br/>  }<br/>  if magic != labelMagic {<br/>    return nil, os.ErrInvalid<br/>  }<br/>  if err = binary.Read(r, binary.BigEndian, &amp;n); err != nil {<br/>    return nil, err<br/>  }<br/>  labels = make([]Label, n)<br/>  for i := 0; i &lt; int(n); i++ {<br/>    var l Label<br/>    if err := binary.Read(r, binary.BigEndian, &amp;l); err != nil {<br/>      return nil, err<br/>    }<br/>    labels[i] = l<br/>  }<br/>  return labels, nil<br/>}<br/><br/>func readImageFile(r io.Reader, e error) (imgs []RawImage, err error) {<br/>  if e != nil {<br/>    return nil, e<br/>  }<br/><br/>  var magic, n, nrow, ncol int32<br/>  if err = binary.Read(r, binary.BigEndian, &amp;magic); err != nil {<br/>    return nil, err<br/>  }<br/>  if magic != imageMagic {<br/>    return nil, err /*os.ErrInvalid*/<br/>  }<br/>  if err = binary.Read(r, binary.BigEndian, &amp;n); err != nil {<br/>    return nil, err<br/>  }<br/>  if err = binary.Read(r, binary.BigEndian, &amp;nrow); err != nil {<br/>    return nil, err<br/>  }<br/>  if err = binary.Read(r, binary.BigEndian, &amp;ncol); err != nil {<br/>    return nil, err<br/>  }<br/>  imgs = make([]RawImage, n)<br/>  m := int(nrow * ncol)<br/>  for i := 0; i &lt; int(n); i++ {<br/>    imgs[i] = make(RawImage, m)<br/>    m_, err := io.ReadFull(r, imgs[i])<br/>    if err != nil {<br/>      return nil, err<br/>    }<br/>    if m_ != int(m) {<br/>      return nil, os.ErrInvalid<br/>    }<br/>  }<br/> return imgs, nil</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Other things from the previous chapter</h1>
                </header>
            
            <article>
                
<p class="mce-root">Obviously, there is a lot from the previous chapter that we can reuse:</p>
<ul>
<li class="mce-root">The range normalization function (<kbd>pixelWeight</kbd>) and its isometric counterpart (<kbd>reversePixelWeight</kbd>)</li>
<li class="mce-root"><kbd>prepareX</kbd> and <kbd>prepareY</kbd></li>
<li class="mce-root">The <kbd>visualize</kbd> function</li>
</ul>
<p class="mce-root">For convenience sake, here they are again:</p>
<pre>func pixelWeight(px byte) float64 {<br/>    retVal := (float64(px) / 255 * 0.999) + 0.001<br/>    if retVal == 1.0 {<br/>        return 0.999<br/>    }<br/>    return retVal<br/>}<br/>func reversePixelWeight(px float64) byte {<br/>    return byte(((px - 0.001) / 0.999) * 255)<br/>}<br/>func prepareX(M []RawImage) (retVal tensor.Tensor) {<br/>    rows := len(M)<br/>    cols := len(M[0])<br/><br/>    b := make([]float64, 0, rows*cols)<br/>    for i := 0; i &lt; rows; i++ {<br/>        for j := 0; j &lt; len(M[i]); j++ {<br/>            b = append(b, pixelWeight(M[i][j]))<br/>        }<br/>    }<br/>    return tensor.New(tensor.WithShape(rows, cols), tensor.WithBacking(b))<br/>}<br/>func prepareY(N []Label) (retVal tensor.Tensor) {<br/>    rows := len(N)<br/>    cols := 10<br/><br/>    b := make([]float64, 0, rows*cols)<br/>    for i := 0; i &lt; rows; i++ {<br/>        for j := 0; j &lt; 10; j++ {<br/>            if j == int(N[i]) {<br/>                b = append(b, 0.999)<br/>            } else {<br/>                b = append(b, 0.001)<br/>            }<br/>        }<br/>    }<br/>    return tensor.New(tensor.WithShape(rows, cols), tensor.WithBacking(b))<br/>}<br/>func visualize(data tensor.Tensor, rows, cols int, filename string) (err error) {<br/>    N := rows * cols<br/><br/>    sliced := data<br/>    if N &gt; 1 {<br/>        sliced, err = data.Slice(makeRS(0, N), nil) // data[0:N, :] in python<br/>        if err != nil {<br/>            return err<br/>        }<br/>    }<br/><br/>    if err = sliced.Reshape(rows, cols, 28, 28); err != nil {<br/>        return err<br/>    }<br/><br/>    imCols := 28 * cols<br/>    imRows := 28 * rows<br/>    rect := image.Rect(0, 0, imCols, imRows)<br/>    canvas := image.NewGray(rect)<br/><br/>    for i := 0; i &lt; cols; i++ {<br/>        for j := 0; j &lt; rows; j++ {<br/>            var patch tensor.Tensor<br/>            if patch, err = sliced.Slice(makeRS(i, i+1), makeRS(j,  <br/>                                         j+1)); err != nil {<br/>                return err<br/>            }<br/><br/>            patchData := patch.Data().([]float64)<br/>            for k, px := range patchData {<br/>                x := j*28 + k%28<br/>                y := i*28 + k/28<br/>                c := color.Gray{reversePixelWeight(px)}<br/>                canvas.Set(x, y, c)<br/>            }<br/>        }<br/>    }<br/><br/>    var f io.WriteCloser<br/>    if f, err = os.Create(filename); err != nil {<br/>        return err<br/>    }<br/><br/>    if err = png.Encode(f, canvas); err != nil {<br/>        f.Close()<br/>        return err<br/>    }<br/><br/>    if err = f.Close(); err != nil {<br/>        return err<br/>    }<br/>    return nil<br/>}</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">CNNs</h1>
                </header>
            
            <article>
                
<p class="mce-root">What we will be building is a CNN. So, what is a Convolutional Neural Network? As its name suggests, it's a neural network, not unlike the one we have built in the previous chapter. So, clearly, there are elements that are similar. There are also elements that are not similar, for if they were similar, we wouldn't have this chapter.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">What are convolutions?</h1>
                </header>
            
            <article>
                
<p class="mce-root">The main difference between the neural network we built in the previous chapter and a CNN is the convolutional layer. Recall that the neural network was able to learn features related to digits. In order to be more accurate, the neural network layers need to learn more specific features. One way to do this is to add more layers; more layers would lead to more features being learned, giving rise to deep learning.</p>
<p class="mce-root">On a spring evening of 1877, people dressed in what modern-day people would consider as <em>black-tie</em> gathered at the Royal Institute, in London. The speaker for the evening was Francis Galton, the same Galton we met in <a href="3d68e167-a44d-4195-a270-f8180ff8f85f.xhtml" target="_blank"/><a href="3d68e167-a44d-4195-a270-f8180ff8f85f.xhtml" target="_blank">Chapter 1</a>, <em>How to Solve All Machine Learning Problems</em>. In his talk, Galton brought out a curious device, which he called a <strong>quincunx</strong>. It was a vertical wooden board with wooden pegs sticking out of it, arranged in a uniform, but interleaved manner. The front of it was covered with glass and there was an opening at the top. Tiny balls are then dropped from the top and as they hit the pegs, bounce left or right, and fall to the corresponding chutes. This continues until the balls collect at the bottom:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-506 image-border" src="Images/254eef09-5de0-42b6-b2a8-076a13d440e4.png" style="width:17.33em;height:29.33em;" width="467" height="791"/></p>
<p class="mce-root">A curious shape begins to form. It's the shape modern statisticians have come to recognize as the binomial distribution. Most statistical textbooks end the story about here. The quincunx, now known as the Galton Board, illustrates, very clearly and firmly, the idea of the central limit theorem.</p>
<p class="mce-root">Our story, of course, doesn't end there. Recall in <a href="3d68e167-a44d-4195-a270-f8180ff8f85f.xhtml" target="_blank">Chapter 1</a>, <em>How to Solve All Machine Learning Problems</em>, that I mentioned that Galton was very much interested in hereditary issues. A few years earlier, Galton had published a book called <em>Hereditary Genius</em>. He had collected data on <em>eminent</em> persons in Great Britain across the preceding centuries, and much to his dismay, he found that <em>eminent</em> parentage tended to lead to un-eminent children. He called this a <strong>reversion to the mediocre</strong>:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-324 image-border" src="Images/7a38fc9b-c6ce-4fb6-bd87-0bea52d33083.png" style="width:23.92em;height:37.50em;" width="1142" height="1788"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-325 image-border" src="Images/3e483077-8c75-442f-9367-efcde7a3003f.png" style="width:34.67em;height:28.08em;" width="1493" height="1207"/></p>
<p class="mce-root">And, yet, he reasoned, the mathematics doesn't show such things! He explained this by showing off a quincunx with two layers. A two-layered quincunx was a stand-in for the generational effect. The top layer would essentially be the distribution of a feature (say, height). Upon dropping to the second layer, the beads would cause the distribution to <em>flatten out</em>, which is not what he had observed. Instead, he surmised that there has to be another factor which causes the regression to the mean. To illustrate his idea, he installed chutes as the controlling factor, which causes a regression to the mean. A mere 40 years later, the rediscovery of Mendel's pea experiments would reveal genetics to be the factor. That is a story for another day.</p>
<p class="mce-root">What we're interested in is why the distribution would <em>flatten out</em>. While the standard <em>it's physics!</em> would suffice as an answer, there remains interesting questions that we could ask. Let's look at a simplified depiction:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-290 image-border" src="Images/5458700e-52d3-41e2-8859-61799bc48284.png" style="width:17.25em;height:11.92em;" width="515" height="355"/></p>
<p class="mce-root">Here, we evaluate the probability that the ball will drop and hit a position. The curve indicates the probability of the ball landing at position B. Now, we add a second layer:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-291 image-border" src="Images/56a539a1-3b5f-4ffa-b039-02562e06a206.png" style="width:17.17em;height:14.67em;" width="548" height="470"/></p>
<p class="mce-root">Say, from the previous layer, the ball landed at position 2. Now, what is the probability that the ball's final resting place is at position D?</p>
<p class="mce-root">To calculate this, we need to know all the possible ways that the ball can end up at position D. Limiting our option to A to D only, here they are:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td style="width: 19%" class="CDPAlignCenter CDPAlign">
<p><strong>Level 1 Position</strong></p>
</td>
<td style="width: 25%" class="CDPAlignCenter CDPAlign">
<p><strong>L1 Horizontal Distance</strong></p>
</td>
<td style="width: 24.3223%" class="CDPAlignCenter CDPAlign">
<p><strong>Level 2 position</strong></p>
</td>
<td style="width: 24.6777%" class="CDPAlignCenter CDPAlign">
<p><strong>L2 Horizontal Distance</strong></p>
</td>
</tr>
<tr>
<td style="width: 19%" class="CDPAlignCenter CDPAlign">
<p>A</p>
</td>
<td style="width: 25%" class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td style="width: 24.3223%" class="CDPAlignCenter CDPAlign">
<p>D</p>
</td>
<td style="width: 24.6777%" class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
</tr>
<tr>
<td style="width: 19%" class="CDPAlignCenter CDPAlign">
<p>B</p>
</td>
<td style="width: 25%" class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td style="width: 24.3223%" class="CDPAlignCenter CDPAlign">
<p>D</p>
</td>
<td style="width: 24.6777%" class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
</tr>
<tr>
<td style="width: 19%" class="CDPAlignCenter CDPAlign">
<p>C</p>
</td>
<td style="width: 25%" class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td style="width: 24.3223%" class="CDPAlignCenter CDPAlign">
<p>D</p>
</td>
<td style="width: 24.6777%" class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
</tr>
<tr>
<td style="width: 19%" class="CDPAlignCenter CDPAlign">
<p>D</p>
</td>
<td style="width: 25%" class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td style="width: 24.3223%" class="CDPAlignCenter CDPAlign">
<p>D</p>
</td>
<td style="width: 24.6777%" class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root">Now we can ask the question in terms of probability. The horizontal distances in the table are an encoding that allows us to ask the question probabilistically and generically. The probability of the ball travelling horizontally by one unit can be represented as <em>P(1)</em>, the probability of the ball travelling horizontally by two units can be represented as <em>P(2)</em>, and so on.</p>
<p class="mce-root">And to calculate the probability that the ball ends up in D after two levels is essentially summing up all the probabilities:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/e0fe1ff2-0715-4167-926b-587d0954a6cc.png" style="width:42.08em;height:1.58em;" width="6380" height="240"/>.</p>
<p class="mce-root">We can write it as such:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/5da38259-9ba6-49f3-ac2c-8680c3f24f75.png" style="width:17.00em;height:1.92em;" width="2570" height="290"/></p>
<p class="mce-root">We can read it as the probability of the final distance being <em>$c = a+b$</em> is the sum of <em>$P_1(a)$</em>, with the probability of level 1, where the ball traveled horizontally by <em>$a$</em> and <em>$P_2(b)$</em>, with the probability of level 2, where the ball traveled horizontally by <em>$b$</em>.</p>
<p class="mce-root">And this is the typical definition of convolution:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/64c598e7-27fd-40c9-80d3-5df72c34d70c.png" style="width:16.33em;height:2.92em;" width="2470" height="440"/></p>
<p class="mce-root">If the integral scares you, we can equivalently rewrite this as a summation operation (this is only valid because we are considering discrete values; for continuous real values, integrations have to be used):</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/becbdaea-2d47-4a0a-bdff-452aca041ced.png" style="width:17.00em;height:2.08em;" width="2370" height="290"/></p>
<p class="mce-root">Now, if you squint very carefully, this equation looks a lot like the preceding probability equation. Instead of <img class="fm-editor-equation" src="Images/01f5c5c4-2cf9-4e7f-be8b-3a2230927b42.png" style="width:0.67em;height:1.42em;" width="80" height="170"/>, we can rewrite it as <img class="fm-editor-equation" src="Images/5b3ffbce-5e13-4fd8-91dd-29ac6bc959c7.png" style="width:3.42em;height:1.25em;" width="410" height="150"/>:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/154d9ba7-91db-4d97-a194-846644eacbbf.png" style="width:14.83em;height:1.92em;" width="2250" height="290"/></p>
<p class="mce-root">And what are probabilities, but functions? There is, after all, a reason we write probabilities in the format $P(a)$. We can indeed genericize the probability equation to the convolution definition.</p>
<p class="mce-root">However, for now, let's strengthen our intuitions about what convolutions are. For that, we'll keep the notion that the function we're talking about has probabilities. First, we should note that the probability of the ball ending up in a particular location is dependent on where it starts. But imagine if the platform for the second platform moves horizontally:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-292 image-border" src="Images/a9663b8d-da9a-4422-8960-c544f9d25494.png" style="width:35.25em;height:13.58em;" width="1350" height="521"/></p>
<p class="mce-root">Now the probability of the final resting place of the ball is highly dependent on where the initial starting position is, as well as where the second layer's starting position is. The ball may not even land on the bottom!</p>
<p class="mce-root">So, here's a good mental shortcut of thinking about convolutions: t's as if one function in one layer is <em>sliding</em> across a second function.</p>
<p class="mce-root">So, convolutions are what cause the <em>flattening</em> of Galton's quincunx. In essence, it is a function that slides on top of the probability function, flattening it out as it moves along the horizontal dimension. This is a one-dimensional convolution; the ball only travels along one dimension.</p>
<p class="mce-root">A two-dimensional convolution is similar to a one-dimensional convolution. Instead, there are two <em>distances</em> or metrics that we're considering for each layer:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/6178477d-0195-4954-8b6e-053bd86510d4.png" style="width:23.50em;height:2.00em;" width="3410" height="290"/></p>
<p class="mce-root"/>
<p class="mce-root">But this equation is nigh impenetrable. <span class="fontstyle0">Instead, here's a convenient series of pictures of how it works, step by step</span>:</p>
<p>Convolution (Step 1):</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-510 image-border" src="Images/158dfe31-000c-4a9b-98bf-fc56db88a2ef.png" style="width:24.25em;height:17.75em;" width="871" height="636"/></p>
<p><span>Convolution (Step 2):</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-511 image-border" src="Images/34bd277c-9d68-4f3f-ab2e-80a458721b2c.png" style="width:25.08em;height:18.33em;" width="871" height="636"/></p>
<p class="mce-root"/>
<p><span>Convolution (Step 3):</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-512 image-border" src="Images/339ed5ff-b018-4da1-b985-78a312a791e9.png" style="width:29.00em;height:21.17em;" width="871" height="636"/></p>
<p><span>Convolution (Step 4):</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-513 image-border" src="Images/cc64c27d-e73c-4246-87eb-c401076e359c.png" style="width:29.00em;height:21.17em;" width="871" height="636"/></p>
<p class="mce-root"/>
<p><span>Convolution (Step 5):</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-514 image-border" src="Images/53bef645-f45c-4182-8450-e4a07570cc73.png" style="width:30.25em;height:22.08em;" width="871" height="636"/></p>
<p><span>Convolution (Step 6):</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-515 image-border" src="Images/f4262f45-418b-4821-bc34-3e6c91c3d5e1.png" style="width:29.83em;height:21.75em;" width="871" height="636"/></p>
<p class="mce-root"/>
<p><span>Convolution (Step 7):</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-516 image-border" src="Images/d84707bf-ac1a-4c60-a99d-2f0541273fe0.png" style="width:31.08em;height:22.67em;" width="871" height="636"/></p>
<p><span>Convolution (Step 8):</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-517 image-border" src="Images/2890e9fd-7b9c-4d82-8eb9-0ddf3c39310f.png" style="width:31.42em;height:23.00em;" width="871" height="636"/></p>
<p class="mce-root"/>
<p><span>Convolution (Step 9):</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-518 image-border" src="Images/02889f5c-c066-4583-9839-78843d460c6f.png" style="width:31.92em;height:23.33em;" width="871" height="636"/></p>
<p class="mce-root">Again, you can think of this as sliding a function that slides over another function (the input) in two dimensions. The function that slides, performs the standard linear algebra transformation of multiplication followed by addition.</p>
<p class="mce-root">You can see this in action in an image-processing example that is undoubtedly very common: Instagram.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How Instagram filters work</h1>
                </header>
            
            <article>
                
<p class="mce-root">I am going to assume that you are familiar with Instagram. If not, I both envy and pity you; but here's the gist of Instagram: it's a photo sharing service that has a selling point of allowing users to apply filters to their images. The filters would change the color of the images, often to enhance the subject.</p>
<p class="mce-root">How do those filters work? Convolutions!</p>
<p class="mce-root">For example, let's define a filter:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/834e573b-df92-43e3-b22c-ca53a720ce3f.png" style="width:12.92em;height:7.33em;" width="1550" height="880"/></p>
<p class="mce-root">To convolve, we simply slide the filter across the following diagram (it's a <em>very</em> famous artwork by an artist called Piet Chew):</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="Images/3306b7c3-de14-471f-947e-87e1c204875c.jpeg" style="width:10.92em;height:11.00em;" width="500" height="500"/></p>
<p>Applying the preceding filter would yield something such as the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/cfaa8048-1e5d-4d65-a5ea-603a23d245e3.jpeg" style="width:10.67em;height:10.75em;" width="500" height="500"/></p>
<p>Yes, the filter blurs images!</p>
<p class="mce-root">Here's an example written in Go to emphasize the idea:</p>
<pre class="mce-root">func main() {<br/>  kb := []float64{<br/>    1 / 16.0, 1 / 8.0, 1 / 16.0,<br/>    1 / 8.0, 1 / 4.0, 1 / 8.0,<br/>    1 / 16.0, 1 / 8.0, 1 / 16.0,<br/>  }<br/>  k := tensor.New(tensor.WithShape(3,3), tensor.WithBacking(kb))<br/><br/>  for _, row := range imgIt {<br/>    for j, px := range row {<br/>      var acc float64<br/><br/>      for _, krow := range kIt {<br/>        for _, kpx := range krow {<br/>          acc += px * kpx <br/>        }<br/>      }<br/>      row[j] = acc<br/>    }<br/>  }<br/>}</pre>
<p class="mce-root">The function is quite slow and inefficient, of course. Gorgonia itself comes with a much more sophisticated algorithm</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Back to neural networks</h1>
                </header>
            
            <article>
                
<p class="mce-root">OK, so we now know that convolutions are important in the use of filters. But how does this relate to neural networks?</p>
<p class="mce-root">Recall that a neural network is defined as a linear transform (<img class="fm-editor-equation" src="Images/8868b16c-e984-46bd-ae8f-1434e9b705e7.png" style="width:3.92em;height:1.25em;" width="600" height="190"/>) with a non-linearity applied on it (written as <img class="fm-editor-equation" src="Images/7d355b95-e418-44e3-a7a5-cad40667f0f1.png" style="width:5.58em;height:1.50em;" width="850" height="230"/>). Note that <em>x</em>, the input image, is acted upon as a whole. This would be like having a single filter across the entire image. But what if we could process the image one small section at a time?</p>
<p class="mce-root">To add to that, in the preceding section, I showed how a simple filter could be used to blur an image. Filters could also be used to sharpen an image, picking out features that matter and blurring out features that don't. So, what if a machine could learn what filter to create?</p>
<p class="mce-root">That's the reason why we would want to use a convolution in a neural network:</p>
<ul>
<li class="mce-root">Convolutions act on small parts of the image at a time, leaving only features that matter</li>
<li class="mce-root">We can learn the specific filters</li>
</ul>
<p class="mce-root">This gives a lot of fine-tuned control to the machine. Now, instead of a rough feature detector that works on the whole image at once, we can build many filters, each specializing to a specific feature, thus allowing us to extract the features necessary for the classification of numbers.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Max-pooling</h1>
                </header>
            
            <article>
                
<p class="mce-root">Now we have in our minds a conceptual machine that will learn the filters that it needs to apply to extract features from an image. But, at the same time, we don't want the machine to overfit on the learning. A filter that is overly specific to the training data is not useful in real life. If a filter learns, for example, that all human faces have two eyes, a nose, and a mouth, and that's all, it wouldn't be able to classify a picture of a person with half their face obscured.</p>
<p class="mce-root">So, in an attempt to teach a ML algorithm to be able to generalize better, we simply give it less information. Max-pooling is one such process, as is <em>dropout</em> (see the next section).</p>
<p class="mce-root">How max pooling works is it partitions the input data into non-overlapping regions, and simply finds the maximum value of that region:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-332 image-border" src="Images/4cc49a97-54a3-4bb8-a123-fe8bc7b60321.png" style="width:26.83em;height:18.67em;" width="1564" height="1091"/></p>
<p class="mce-root">There is, of course, an implicit understanding that this definitely changes the shape of the output. In fact, you will observe that it shrinks the image.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Dropout</h1>
                </header>
            
            <article>
                
<p class="mce-root">The result after max-pooling is minimum information within the output. But this may still be too much information; the machine may still overfit. Therefore, a very interesting quandary arises: what if some of the activations were randomly zeroed?</p>
<p class="mce-root">This is the basis of dropout. It's a remarkably simple idea that improves upon the machine learning algorithm's ability to generalize, simply by having deleterious effects on information. With every iteration, random activations are zeroed. This forces the algorithm to only learn what is really important. How it does so involves structural algebra and is a story for another day.</p>
<p class="mce-root">For the purposes of this project, Gorgonia actually handles dropout by means of element-wise multiplication by a randomly generated matrix of 1s and 0s.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Describing a CNN</h1>
                </header>
            
            <article>
                
<p class="mce-root">Having said all that, the neural network is very easy to build. First, we define a neural network as such:</p>
<pre class="mce-root">type convnet struct {<br/>    g                  *gorgonia.ExprGraph<br/>    w0, w1, w2, w3, w4 *gorgonia.Node // weights. the number at the back indicates which layer it's used for<br/>    d0, d1, d2, d3     float64        // dropout probabilities<br/><br/>    out    *gorgonia.Node<br/>    outVal gorgonia.Value<br/>}</pre>
<p class="mce-root">Here, we defined a neural network with four layers. A convnet layer is similar to a linear layer in many ways. It can, for example, be written as an equation:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/25dbe445-4aa7-4544-8e5d-c6c58ea5b98a.png" style="width:18.33em;height:1.58em;" width="2780" height="240"/></p>
<p class="mce-root">Note that in this specific example, I consider dropout and max-pool to be part of the same layer. In many literatures, they are considered to be separate layers.</p>
<p class="mce-root">I personally do not see the necessity to consider them as separate layers. After all, everything is just a mathematical equation; composing functions comes naturally.</p>
<p class="mce-root">A mathematical equation on its own without structure is quite meaningless. Unfortunately, we do not have technology usable enough to simply define the structure of a data type (the hotness is in dependently-typed languages, such as Idris, but they are not yet at the level of usability or performance that is necessary for deep learning). Instead, we have to constrain our data structure by providing a function to define a <kbd>convnet</kbd>:</p>
<pre class="mce-root">func newConvNet(g *gorgonia.ExprGraph) *convnet {<br/>  w0 := gorgonia.NewTensor(g, dt, 4, gorgonia.WithShape(32, 1, 3, 3), <br/>                 gorgonia.WithName("w0"),    <br/>                 gorgonia.WithInit(gorgonia.GlorotN(1.0)))<br/>  w1 := gorgonia.NewTensor(g, dt, 4, gorgonia.WithShape(64, 32, 3, 3), <br/>                 gorgonia.WithName("w1"),  <br/>                 gorgonia.WithInit(gorgonia.GlorotN(1.0)))<br/>  w2 := gorgonia.NewTensor(g, dt, 4, gorgonia.WithShape(128, 64, 3, 3), <br/>                 gorgonia.WithName("w2"), <br/>                 gorgonia.WithInit(gorgonia.GlorotN(1.0)))<br/>  w3 := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(128*3*3, 625), <br/>                 gorgonia.WithName("w3"), <br/>                 gorgonia.WithInit(gorgonia.GlorotN(1.0)))<br/>  w4 := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(625, 10), <br/>                 gorgonia.WithName("w4"), <br/>                 gorgonia.WithInit(gorgonia.GlorotN(1.0)))<br/>  return &amp;convnet{<br/>    g: g,<br/>    w0: w0,<br/>    w1: w1,<br/>    w2: w2,<br/>    w3: w3,<br/>    w4: w4,<br/><br/>    d0: 0.2,<br/>    d1: 0.2,<br/>    d2: 0.2,<br/>    d3: 0.55,<br/>  }<br/>}</pre>
<p class="mce-root">We'll start with <kbd>dt</kbd>. This is essentially a global variable denoting what data type we would like to work in. For the purposes of this project, we can use <kbd>var dt = tensor.Float64</kbd>, to indicate that we would like to work with <kbd>float64</kbd> throughout the entire project. This allows us to immediately reuse the functions from the previous chapter without having to handle different data types. Note that if we do plan to use <kbd>float32</kbd>, the computation speed immediately doubles. In the repository to this chapter, you might note that the code uses <kbd>float32</kbd>.</p>
<p class="mce-root">We'll start with <kbd>d0</kbd> all the way to <kbd>d3</kbd>. This is fairly simple. For the first three layers, we want 20% of the activations to be randomly zeroed. But for the last layer, we want 55% of the activations to be randomly zeroed. In really broad strokes, this causes an information bottleneck, which will cause the machine to learn only the really important features.</p>
<p class="mce-root">Take a look at how <kbd>w0</kbd> is defined. Here, we're saying <kbd>w0</kbd> is a variable called <kbd>w0</kbd>. It is a tensor with the shape of (32, 1, 3, 3). This is typically called the <strong>Number of Batches, Channels, Height, Width</strong> (<strong>NCHW</strong>/<strong>BCHW</strong>) format. In short, what we're saying is that there are 32 filters we wish to learn, each filter has a height and width of (3, 3), and it has one color channel. MNIST is, after all, black and white.</p>
<div class="mce-root packt_infobox">BCHW is not the only format! Some deep learning frameworks prefer to use BHWC formats. The reason for preferring one format over another is purely operational. Some convolution algorithms work better with NCHW; some work better with BHWC. The ones in Gorgonia works only in BCHW.</div>
<p class="mce-root">The choice of a 3 x 3 filter is purely unprincipled but not without precedence. You could choose a 5 x 5 filter, or a 2 x 1 filter, or really, a filter of any shape. However, it has to be said that a 3 x 3 filter is probably the most universal filter that can work on all sorts of images. Square filters of these sorts are common in image-processing algorithms, so it is in accordance to such traditions that we chose a 3 x 3.</p>
<p class="mce-root">The weights for the higher layers start to look a bit more interesting. For example, <kbd>w1</kbd> has a shape of (64, 32, 3, 3). Why? In order to understand why, we need to explore the interplay between the activation functions and the shapes. Here's the entire forward function of the <kbd>convnet</kbd>:</p>
<pre class="mce-root">// This function is particularly verbose for educational reasons. In reality, you'd wrap up the layers within a layer struct type and perform per-layer activations<br/>func (m *convnet) fwd(x *gorgonia.Node) (err error) {<br/>    var c0, c1, c2, fc *gorgonia.Node<br/>    var a0, a1, a2, a3 *gorgonia.Node<br/>    var p0, p1, p2 *gorgonia.Node<br/>    var l0, l1, l2, l3 *gorgonia.Node<br/><br/>    // LAYER 0<br/>    // here we convolve with stride = (1, 1) and padding = (1, 1),<br/>    // which is your bog standard convolution for convnet<br/>    if c0, err = gorgonia.Conv2d(x, m.w0, tensor.Shape{3, 3}, []int{1, 1}, []int{1, 1}, []int{1, 1}); err != nil {<br/>        return errors.Wrap(err, "Layer 0 Convolution failed")<br/>    }<br/>    if a0, err = gorgonia.Rectify(c0); err != nil {<br/>        return errors.Wrap(err, "Layer 0 activation failed")<br/>    }<br/>    if p0, err = gorgonia.MaxPool2D(a0, tensor.Shape{2, 2}, []int{0, 0}, []int{2, 2}); err != nil {<br/>        return errors.Wrap(err, "Layer 0 Maxpooling failed")<br/>    }<br/>    if l0, err = gorgonia.Dropout(p0, m.d0); err != nil {<br/>        return errors.Wrap(err, "Unable to apply a dropout")<br/>    }<br/><br/>    // Layer 1<br/>    if c1, err = gorgonia.Conv2d(l0, m.w1, tensor.Shape{3, 3}, []int{1, 1}, []int{1, 1}, []int{1, 1}); err != nil {<br/>        return errors.Wrap(err, "Layer 1 Convolution failed")<br/>    }<br/>    if a1, err = gorgonia.Rectify(c1); err != nil {<br/>        return errors.Wrap(err, "Layer 1 activation failed")<br/>    }<br/>    if p1, err = gorgonia.MaxPool2D(a1, tensor.Shape{2, 2}, []int{0, 0}, []int{2, 2}); err != nil {<br/>        return errors.Wrap(err, "Layer 1 Maxpooling failed")<br/>    }<br/>    if l1, err = gorgonia.Dropout(p1, m.d1); err != nil {<br/>        return errors.Wrap(err, "Unable to apply a dropout to layer 1")<br/>    }<br/><br/>    // Layer 2<br/>    if c2, err = gorgonia.Conv2d(l1, m.w2, tensor.Shape{3, 3}, []int{1, 1}, []int{1, 1}, []int{1, 1}); err != nil {<br/>        return errors.Wrap(err, "Layer 2 Convolution failed")<br/>    }<br/>    if a2, err = gorgonia.Rectify(c2); err != nil {<br/>        return errors.Wrap(err, "Layer 2 activation failed")<br/>    }<br/>    if p2, err = gorgonia.MaxPool2D(a2, tensor.Shape{2, 2}, []int{0, 0}, []int{2, 2}); err != nil {<br/>        return errors.Wrap(err, "Layer 2 Maxpooling failed")<br/>    }<br/>    log.Printf("p2 shape %v", p2.Shape())<br/><br/>    var r2 *gorgonia.Node<br/>    b, c, h, w := p2.Shape()[0], p2.Shape()[1], p2.Shape()[2], p2.Shape()[3]<br/>    if r2, err = gorgonia.Reshape(p2, tensor.Shape{b, c * h * w}); err != nil {<br/>        return errors.Wrap(err, "Unable to reshape layer 2")<br/>    }<br/>    log.Printf("r2 shape %v", r2.Shape())<br/>    if l2, err = gorgonia.Dropout(r2, m.d2); err != nil {<br/>        return errors.Wrap(err, "Unable to apply a dropout on layer 2")<br/>    }<br/><br/>    // Layer 3<br/>    if fc, err = gorgonia.Mul(l2, m.w3); err != nil {<br/>        return errors.Wrapf(err, "Unable to multiply l2 and w3")<br/>    }<br/>    if a3, err = gorgonia.Rectify(fc); err != nil {<br/>        return errors.Wrapf(err, "Unable to activate fc")<br/>    }<br/>    if l3, err = gorgonia.Dropout(a3, m.d3); err != nil {<br/>        return errors.Wrapf(err, "Unable to apply a dropout on layer 3")<br/>    }<br/><br/>    // output decode<br/>    var out *gorgonia.Node<br/>    if out, err = gorgonia.Mul(l3, m.w4); err != nil {<br/>        return errors.Wrapf(err, "Unable to multiply l3 and w4")<br/>    }<br/>    m.out, err = gorgonia.SoftMax(out)<br/>    gorgonia.Read(m.out, &amp;m.outVal)<br/>    return<br/>}</pre>
<p class="mce-root">It should be noted that convolution layers do change the shape of the inputs. Given an (N, 1, 28, 28) input, the <kbd>Conv2d</kbd> function will return a (N, 32, 28, 28) output, precisely because there are now 32 filters. The <kbd>MaxPool2d</kbd> will return an output with the shape of (N, 32, 14, 14); recall that the purpose of max-pooling is to reduce the amount of information in the neural network. It just happens that max-pooling with a shape of (2, 2) will nicely halve the length and width of the image (and reduce the amount of information by four times).</p>
<p class="mce-root">The output of layer 0 would have a shape of (N, 32, 14, 14). If we stick to our explanations of our shapes from earlier, where it was in the format of (N, C, H, W), we would be quite stumped. What does it mean to have 32 channels? To answer that, let's look at how we encode a color image in terms of BCHW:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-341 image-border" src="Images/68a7c029-efcf-489f-8097-ad069dcf4be9.png" style="width:17.75em;height:15.42em;" width="550" height="473"/></p>
<p class="mce-root">Note that we encode it as three separate layers, stacked onto one another. This is a clue as to how to think about having 32 channels. Of course, each of the 32 channels as the result of applying each of the 32 filters; the extracted features, so to speak. The result can, of course, be stacked in the same way color channels be stacked.</p>
<p class="mce-root">For the most part, however, the mere act of symbol pushing is all that is required to build a deep learning system; no real intelligence is required. This, of course mirrors, the Chinese Room Puzzle thought experiment, and I have quite a bit to say on that, though it's not really the time nor the place.</p>
<p class="mce-root">The more interesting parts is in the construction of Layer 3. Layers 1 and 2 are constructed very similarly to Layer 0, but Layer 3 has a slightly different construction. The reason is because the output of Layer 2 is a rank-4 tensor, but in order to perform matrix multiplication, it needs to be reshaped into a rank-2 tensor.</p>
<p class="mce-root">Lastly, the final layer, which decodes the output, uses a softmax activation function to ensure that the result we get is probability.</p>
<p class="mce-root">And really, there you have it. A CNN, written in a very neat way that does not obfuscate the mathematical definitions.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Backpropagation</h1>
                </header>
            
            <article>
                
<p class="mce-root">For the convnet to learn, what is required is backpropagation, which propagates the errors, and a gradient descent function to update the weight matrices. To do this is relatively simple with Gorgonia, so simple that we can actually put it into our main function without impacting understandability:</p>
<pre class="mce-root">func main() {<br/>    flag.Parse()<br/>    parseDtype()<br/>    imgs, err := readImageFile(os.Open("train-images-idx3-ubyte"))<br/>    if err != nil {<br/>        log.Fatal(err)<br/>    }<br/>    labels, err := readLabelFile(os.Open("train-labels-idx1-ubyte"))<br/>    if err != nil {<br/>        log.Fatal(err)<br/>    }<br/><br/>    inputs := prepareX(imgs)<br/>    targets := prepareY(labels)<br/><br/>    // the data is in (numExamples, 784).<br/>    // In order to use a convnet, we need to massage the data<br/>    // into this format (batchsize, numberOfChannels, height, width).<br/>    //<br/>    // This translates into (numExamples, 1, 28, 28).<br/>    //<br/>    // This is because the convolution operators actually understand height and width.<br/>    //<br/>    // The 1 indicates that there is only one channel (MNIST data is black and white).<br/>    numExamples := inputs.Shape()[0]<br/>    bs := *batchsize<br/><br/>    if err := inputs.Reshape(numExamples, 1, 28, 28); err != nil {<br/>        log.Fatal(err)<br/>    }<br/>    g := gorgonia.NewGraph()<br/>    x := gorgonia.NewTensor(g, dt, 4, gorgonia.WithShape(bs, 1, 28, 28), gorgonia.WithName("x"))<br/>    y := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(bs, 10), gorgonia.WithName("y"))<br/>    m := newConvNet(g)<br/>    if err = m.fwd(x); err != nil {<br/>        log.Fatalf("%+v", err)<br/>    }<br/>    losses := gorgonia.Must(gorgonia.HadamardProd(m.out, y))<br/>    cost := gorgonia.Must(gorgonia.Mean(losses))<br/>    cost = gorgonia.Must(gorgonia.Neg(cost))<br/><br/>    // we wanna track costs<br/>    var costVal gorgonia.Value<br/>    gorgonia.Read(cost, &amp;costVal)<br/><br/>    if _, err = gorgonia.Grad(cost, m.learnables()...); err != nil {<br/>        log.Fatal(err)<br/>    }</pre>
<p class="mce-root">For the errors, we use a simple cross-entropy by multiplying the expected output element-wise and then averaging it, as shown in this snippet:</p>
<pre class="mce-root">    losses := gorgonia.Must(gorgonia.HadamardProd(m.out, y))<br/>    cost := gorgonia.Must(gorgonia.Mean(losses))<br/>    cost = gorgonia.Must(gorgonia.Neg(cost))</pre>
<p class="mce-root">Following that, we simply call <kbd>gorgonia.Grad(cost, m.learnables()...)</kbd>, which performs symbolic backpropagation. What is <kbd>m.learnables()</kbd>?, you may ask. It's simply the variables that we wish the machine to learn. The definition is as such:</p>
<pre class="mce-root">func (m *convnet) learnables() gorgonia.Nodes {<br/>    return gorgonia.Nodes{m.w0, m.w1, m.w2, m.w3, m.w4}<br/>}</pre>
<p class="mce-root">Again, it's fairly simple.</p>
<p class="mce-root">One additional comment I want the reader to note is <kbd>gorgonia.Read(cost, &amp;costVal)</kbd>. <kbd>Read</kbd> is one of the more confusing parts of Gorgonia. But when framed correctly, it is quite simple to understand.</p>
<p class="mce-root">Earlier, in the section <em>Describing a neural network</em>, I likened Gorgonia to writing in another programming language. If so, then <kbd>Read</kbd> is the equivalent of <kbd>io.WriteFile</kbd>. What <kbd>gorgonia.Read(cost, &amp;costVal)</kbd> says is that when the mathematical expression gets evaluated, make a copy of the result of <kbd>cost</kbd> and store it in <kbd>costVal</kbd>. This is necessary because of the way mathematical expressions are evaluated within the Gorgonia system.</p>
<div class="mce-root packt_infobox">Why is it called <kbd>Read</kbd> instead of <kbd>Write</kbd>? I initially modeled Gorgonia to be quite monadic (in the Haskell sense of monad), and as a result, one would <em>read out</em> a value. After a span of three years, the name sort of stuck.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Running the neural network</h1>
                </header>
            
            <article>
                
<p class="mce-root">Observe that up to this point, we've merely described the computations we need to perform. The neural network doesn't actually run; this is simply a description on the neural network to run.</p>
<p class="mce-root">We need to be able to evaluate the mathematical expression. In order to do so, we need to compile the expression into a program that can be executed. Here's the code to do it:</p>
<pre class="mce-root">    vm := gorgonia.NewTapeMachine(g, <br/>        gorgonia.WithPrecompiled(prog, locMap), <br/>        gorgonia.BindDualValues(m.learnables()...))<br/>    solver := gorgonia.NewRMSPropSolver(gorgonia.WithBatchSize(float64(bs)))<br/>    defer vm.Close()</pre>
<p class="mce-root">It's not strictly necessary to call <kbd>gorgonia.Compile(g)</kbd>. This was done for pedagogical reasons, to showcase that the mathematical expression can indeed be compiled down into an assembly-like program. In production systems, I often just do something like this: <kbd>vm := gorgonia.NewTapeMachine(g, gorgonia.BindDualValues(m.learnables()...))</kbd>.</p>
<p class="mce-root">There are two provided <kbd>vm</kbd> types in Gorgonia, each representing different modes of computation. In this project, we're merely using <kbd>NewTapeMachine</kbd> to get a <kbd>*gorgonia.tapeMachine</kbd>. The function to create a <kbd>vm</kbd> takes many options, and the <kbd>BindDualValues</kbd> option simply binds the gradients of each of the variables in the models to the variables themselves. This allows for cheaper gradient descent.</p>
<p class="mce-root">Lastly, note that a <kbd>VM</kbd> is a resource. You should think of a <kbd>VM</kbd> as if it were an external CPU, a computing resource. It is good practice to close any external resources after we use them and, fortunately, Go has a very convenient way of handling cleanups: <kbd>defer vm.Close()</kbd>.</p>
<p class="mce-root">Before we move on to talk about gradient descent, here's what the compiled program looks like, in pseudo-assembly:</p>
<pre class="mce-root"><br/> Instructions:<br/> 0 loadArg 0 (x) to CPU0<br/> 1 loadArg 1 (y) to CPU1<br/> 2 loadArg 2 (w0) to CPU2<br/> 3 loadArg 3 (w1) to CPU3<br/> 4 loadArg 4 (w2) to CPU4<br/> 5 loadArg 5 (w3) to CPU5<br/> 6 loadArg 6 (w4) to CPU6<br/> 7 im2col&lt;(3,3), (1, 1), (1,1) (1, 1)&gt; [CPU0] CPU7 false false false<br/> 8 Reshape(32, 9) [CPU2] CPU8 false false false<br/> 9 Reshape(78400, 9) [CPU7] CPU7 false true false<br/> 10 Alloc Matrix float64(78400, 32) CPU9<br/> 11 A × Bᵀ [CPU7 CPU8] CPU9 true false true<br/> 12 DoWork<br/> 13 Reshape(100, 28, 28, 32) [CPU9] CPU9 false true false<br/> 14 Aᵀ{0, 3, 1, 2} [CPU9] CPU9 false true false<br/> 15 const 0 [] CPU10 false false false<br/> 16 &gt;= true [CPU9 CPU10] CPU11 false false false<br/> 17 ⊙ false [CPU9 CPU11] CPU9 false true false<br/> 18 MaxPool{100, 32, 28, 28}(kernel: (2, 2), pad: (0, 0), stride: (2, <br/>                             2)) [CPU9] CPU12 false false false<br/> 19 0(0, 1) - (100, 32, 14, 14) [] CPU13 false false false<br/> 20 const 0.2 [] CPU14 false false false<br/> 21 &gt; true [CPU13 CPU14] CPU15 false false false<br/> 22 ⊙ false [CPU12 CPU15] CPU12 false true false<br/> 23 const 5 [] CPU16 false false false<br/> 24 ÷ false [CPU12 CPU16] CPU12 false true false<br/> 25 im2col&lt;(3,3), (1, 1), (1,1) (1, 1)&gt; [CPU12] CPU17 false false false<br/> 26 Reshape(64, 288) [CPU3] CPU18 false false false<br/> 27 Reshape(19600, 288) [CPU17] CPU17 false true false<br/> 28 Alloc Matrix float64(19600, 64) CPU19<br/> 29 A × Bᵀ [CPU17 CPU18] CPU19 true false true<br/> 30 DoWork<br/> 31 Reshape(100, 14, 14, 64) [CPU19] CPU19 false true false<br/> 32 Aᵀ{0, 3, 1, 2} [CPU19] CPU19 false true false<br/> 33 &gt;= true [CPU19 CPU10] CPU20 false false false<br/> 34 ⊙ false [CPU19 CPU20] CPU19 false true false<br/> 35 MaxPool{100, 64, 14, 14}(kernel: (2, 2), pad: (0, 0), stride: (2, <br/>                             2)) [CPU19] CPU21 false false false<br/> 36 0(0, 1) - (100, 64, 7, 7) [] CPU22 false false false<br/> 37 &gt; true [CPU22 CPU14] CPU23 false false false<br/> 38 ⊙ false [CPU21 CPU23] CPU21 false true false<br/> 39 ÷ false [CPU21 CPU16] CPU21 false true false<br/> 40 im2col&lt;(3,3), (1, 1), (1,1) (1, 1)&gt; [CPU21] CPU24 false false false<br/> 41 Reshape(128, 576) [CPU4] CPU25 false false false<br/> 42 Reshape(4900, 576) [CPU24] CPU24 false true false<br/> 43 Alloc Matrix float64(4900, 128) CPU26<br/> 44 A × Bᵀ [CPU24 CPU25] CPU26 true false true<br/> 45 DoWork<br/> 46 Reshape(100, 7, 7, 128) [CPU26] CPU26 false true false<br/> 47 Aᵀ{0, 3, 1, 2} [CPU26] CPU26 false true false<br/> 48 &gt;= true [CPU26 CPU10] CPU27 false false false<br/> 49 ⊙ false [CPU26 CPU27] CPU26 false true false<br/> 50 MaxPool{100, 128, 7, 7}(kernel: (2, 2), pad: (0, 0), stride: (2, <br/>                            2)) [CPU26] CPU28 false false false<br/> 51 Reshape(100, 1152) [CPU28] CPU28 false true false<br/> 52 0(0, 1) - (100, 1152) [] CPU29 false false false<br/> 53 &gt; true [CPU29 CPU14] CPU30 false false false<br/> 54 ⊙ false [CPU28 CPU30] CPU28 false true false<br/> 55 ÷ false [CPU28 CPU16] CPU28 false true false<br/> 56 Alloc Matrix float64(100, 625) CPU31<br/> 57 A × B [CPU28 CPU5] CPU31 true false true<br/> 58 DoWork<br/> 59 &gt;= true [CPU31 CPU10] CPU32 false false false<br/> 60 ⊙ false [CPU31 CPU32] CPU31 false true false<br/> 61 0(0, 1) - (100, 625) [] CPU33 false false false<br/> 62 const 0.55 [] CPU34 false false false<br/> 63 &gt; true [CPU33 CPU34] CPU35 false false false<br/> 64 ⊙ false [CPU31 CPU35] CPU31 false true false<br/> 65 const 1.8181818181818181 [] CPU36 false false false<br/> 66 ÷ false [CPU31 CPU36] CPU31 false true false<br/> 67 Alloc Matrix float64(100, 10) CPU37<br/> 68 A × B [CPU31 CPU6] CPU37 true false true<br/> 69 DoWork<br/> 70 exp [CPU37] CPU37 false true false<br/> 71 Σ[1] [CPU37] CPU38 false false false<br/> 72 SizeOf=10 [CPU37] CPU39 false false false<br/> 73 Repeat[1] [CPU38 CPU39] CPU40 false false false<br/> 74 ÷ false [CPU37 CPU40] CPU37 false true false<br/> 75 ⊙ false [CPU37 CPU1] CPU37 false true false<br/> 76 Σ[0 1] [CPU37] CPU41 false false false<br/> 77 SizeOf=100 [CPU37] CPU42 false false false<br/> 78 SizeOf=10 [CPU37] CPU43 false false false<br/> 79 ⊙ false [CPU42 CPU43] CPU44 false false false<br/> 80 ÷ false [CPU41 CPU44] CPU45 false false false<br/> 81 neg [CPU45] CPU46 false false false<br/> 82 DoWork<br/> 83 Read CPU46 into 0xc43ca407d0<br/> 84 Free CPU0<br/> Args: 11 | CPU Memories: 47 | GPU Memories: 0<br/> CPU Mem: 133594448 | GPU Mem []<br/> ```</pre>
<p class="mce-root">Printing the program allows you to actually have a feel for the complexity of the neural network. At 84 instructions, the convnet is among the simpler programs I've seen. However, there are quite a few expensive operations, which would inform us quite a bit about how long each run would take. This output also tells us roughly how many bytes of memory will be used: 133594448 bytes, or 133 megabytes.</p>
<p class="mce-root">Now it's time to talk about, gradient descent. Gorgonia comes with a number of gradient descent solvers. For this project, we'll be using the RMSProp algorithm. So, we create a solver by calling <kbd>solver := gorgonia.NewRMSPropSolver(gorgonia.WithBatchSize(float64(bs)))</kbd>. Because we are planning to perform our operations in batches, we should correct the solver by providing it the batch size, lest the solver overshoots its target.</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/4c1d9844-845b-4a62-aed4-e0074809e3cd.png" style="width:15.75em;height:15.08em;" width="281" height="269"/></p>
<p class="mce-root">To run the neural network, we simply run it for a number of epochs (which is passed in as an argument to the program):</p>
<pre class="mce-root">    batches := numExamples / bs<br/>    log.Printf("Batches %d", batches)<br/>    bar := pb.New(batches)<br/>    bar.SetRefreshRate(time.Second)<br/>    bar.SetMaxWidth(80)<br/><br/>    for i := 0; i &lt; *epochs; i++ {<br/>        bar.Prefix(fmt.Sprintf("Epoch %d", i))<br/>        bar.Set(0)<br/>        bar.Start()<br/>        for b := 0; b &lt; batches; b++ {<br/>            start := b * bs<br/>            end := start + bs<br/>            if start &gt;= numExamples {<br/>                break<br/>            }<br/>            if end &gt; numExamples {<br/>                end = numExamples<br/>            }<br/><br/>            var xVal, yVal tensor.Tensor<br/>            if xVal, err = inputs.Slice(sli{start, end}); err != nil {<br/>                log.Fatal("Unable to slice x")<br/>            }<br/><br/>            if yVal, err = targets.Slice(sli{start, end}); err != nil {<br/>                log.Fatal("Unable to slice y")<br/>            }<br/>            if err = xVal.(*tensor.Dense).Reshape(bs, 1, 28, 28); err != nil {<br/>                log.Fatalf("Unable to reshape %v", err)<br/>            }<br/><br/>            gorgonia.Let(x, xVal)<br/>            gorgonia.Let(y, yVal)<br/>            if err = vm.RunAll(); err != nil {<br/>                log.Fatalf("Failed at epoch  %d: %v", i, err)<br/>            }<br/>            solver.Step(gorgonia.NodesToValueGrads(m.learnables()))<br/>            vm.Reset()<br/>            bar.Increment()<br/>        }<br/>        log.Printf("Epoch %d | cost %v", i, costVal)<br/>    }</pre>
<p class="mce-root">Because I was feeling a bit fancy, I decided to add a progress bar to track the progress. To do so, I'm using <kbd>cheggaaa/pb.v1</kbd> as the library to draw a progress bar. To install it, simply run <kbd>go get gopkg.in/cheggaaa/pb.v1</kbd> and to use it, simply add <kbd>import "gopkg.in/cheggaaa/pb.v1</kbd> in the imports.</p>
<p class="mce-root">The rest is fairly straightforward. From the training dataset, we slice out a small portion of it (specifically, we slice out <kbd>bs</kbd> rows). Because our program takes a rank-4 tensor as an input, the data has to be reshaped to <kbd>xVal.(*tensor.Dense).Reshape(bs, 1, 28, 28)</kbd>.</p>
<p class="mce-root">Finally, we feed the value into the function by using <kbd>gorgonia.Let</kbd>. Where <kbd>gorgonia.Read</kbd> reads a value out from the execution environment, <kbd>gorgonia.Let</kbd> puts a value into the execution environment. After which, <kbd>vm.RunAll()</kbd> executes the program, evaluating the mathematical function. As a programmed and intentional side-effect, each call to <kbd>vm.RunAll()</kbd> will populate the cost value into <kbd>costVal</kbd>.</p>
<p class="mce-root">Once the equation has been evaluated, this also means that the variables of the equation are now ready to be updated. As such, we use <kbd>solver.Step(gorgonia.NodesToValueGrads(m.learnables()))</kbd> to perform the actual gradient updates. After this, <kbd>vm.Reset()</kbd> is called to reset the VM state, ready for its next iteration.</p>
<p>Gorgonia in general, is pretty efficient. In the current version as this book was written, it managed to use all eight cores in my CPU as shown here:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-507 image-border" src="Images/7b1b4faf-4ecf-4a20-8fe5-6aef1eb035d4.png" style="width:85.58em;height:41.67em;" width="1027" height="500"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Testing</h1>
                </header>
            
            <article>
                
<p>Of course we'd have to test our neural network.</p>
<p>First we load up the testing data:</p>
<pre>testImgs, err := readImageFile(os.Open("t10k-images.idx3-ubyte"))<br/>if err != nil {<br/>   log.Fatal(err)<br/>}<br/><br/>testlabels, err := readLabelFile(os.Open("t10k-labels.idx1-ubyte"))<br/>  if err != nil {<br/>     log.Fatal(err)<br/>  }<br/><br/>testData := prepareX(testImgs)<br/>testLbl := prepareY(testlabels)<br/>shape := testData.Shape()<br/>visualize(testData, 10, 10, "testData.png")</pre>
<p>In the last line, we visualize the test data to ensure that we do indeed have the correct dataset:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/1f8e92fd-bd9f-4382-a9d2-1a9fc82433cc.png" style="width:19.33em;height:19.42em;" width="280" height="280"/></p>
<p>Then we have the main testing loop. Do observe that it's extremely similar to the training loop - because it's the same neural network!</p>
<pre>var correct, total float32<br/>numExamples = shape[0]<br/>batches = numExamples / bs<br/>for b := 0; b &lt; batches; b++ {<br/>    start := b * bs<br/>    end := start + bs<br/>    if start &gt;= numExamples {<br/>       break<br/>    }<br/>    if end &gt; numExamples {<br/>       end = numExamples<br/>    }<br/><br/>var oneimg, onelabel tensor.Tensor<br/>        if oneimg, err = testData.Slice(sli{start, end}); err != nil {<br/>            log.Fatalf("Unable to slice images (%d, %d)", start, end)<br/>        }<br/>        if onelabel, err = testLbl.Slice(sli{start, end}); err != nil {<br/>            log.Fatalf("Unable to slice labels (%d, %d)", start, end)<br/>        }<br/>        if err = oneimg.(*tensor.Dense).Reshape(bs, 1, 28, 28); err != nil {<br/>            log.Fatalf("Unable to reshape %v", err)<br/>        }<br/><br/>        gorgonia.Let(x, oneimg)<br/>        gorgonia.Let(y, onelabel)<br/>        if err = vm.RunAll(); err != nil {<br/>            log.Fatal("Predicting (%d, %d) failed %v", start, end, err)<br/>        }<br/>        label, _ := onelabel.(*tensor.Dense).Argmax(1)<br/>        predicted, _ := m.outVal.(*tensor.Dense).Argmax(1)<br/>        lblData := label.Data().([]int)<br/>        for i, p := range predicted.Data().([]int) {<br/>            if p == lblData[i] {<br/>                correct++<br/>            }<br/>            total++<br/>        }<br/>    }<br/><br/>    fmt.Printf("Correct/Totals: %v/%v = %1.3f\n", correct, total, correct/total)</pre>
<p>One difference is in the following snippet:</p>
<pre>label, _ := onelabel.(*tensor.Dense).Argmax(1)<br/>predicted, _ := m.outVal.(*tensor.Dense).Argmax(1)<br/>lblData := label.Data().([]int)<br/>for i, p := range predicted.Data().([]int) {<br/>    if p == lblData[i] {<br/>          correct++<br/>          }<br/>        total++<br/>   }</pre>
<p>In the previous chapter, we wrote our own <kbd>argmax</kbd> function. Gorgonia's tensor package actually does provide a handy method for doing just that. But in order to understand what is going on, we will need to first look at the results.</p>
<p>The shape of <kbd>m.outVal</kbd> is (N, 10), where N is the batch size. The same shape also shows for <kbd>onelabel</kbd>.  (N, 10) means N rows of 10 columns. What can these 10 columns be? Well, of course they're the encoded numbers! So what we want is to find the maximum values amongst the column for each row. And that's the first dimension. Hence when a call to <kbd>.ArgMax()</kbd> is made, we specify 1 as the axis.</p>
<p>Therefore the result of the <kbd>.Argmax()</kbd> calls will have a shape (N). For each value in that vector, if they are the same for <kbd>lblData</kbd> and <kbd>predicted</kbd>, then we increment the <kbd>correct</kbd> counter. This gives us a way to count accuracy.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Accuracy</h1>
                </header>
            
            <article>
                
<p>We use accuracy because the previous chapter used accuracy. This allows us to have a apples-to-apples comparison. Additionally you may note that there is a lack of cross validation. That will be left as an exercise to the reader.</p>
<p>After training the neural network for two hours on a batch size of 50 and 150 epochs, I'm pleased to say I got a 99.87% accuracy. And this isn't even state of the art!</p>
<p>In the previous chapter, it took just 6.5 minutes to get a 97% accuracy. That additional 2% accuracy required a lot more time. This is a factor in real life. Often business decisions are a big factor in choosing ML algorithm.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned about neural networks and studied about the Gorgonia library in detail. Then we learned how to recognize handwritten digits using a CNN.</p>
<p>In the next chapter, <span>w</span><span>e're going to strengthen our intuition about what can be done with computer vision, by building a multiple facial-detection system in Go</span><span>.</span></p>


            </article>

            
        </section>
    </div>



  </body></html>