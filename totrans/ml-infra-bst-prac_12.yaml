- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Types of Machine Learning Systems – Feature-Based and Raw Data-Based (Deep Learning)
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习系统类型——基于特征和基于原始数据（深度学习）
- en: In the previous chapters, we learned about data, noise, features, and visualization.
    Now, it’s time to move on to machine learning models. There is no such thing as
    one model, but there are plenty of them – starting from the classical models such
    as random forest to deep learning models for vision systems to generative AI models
    such as GPT.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们学习了数据、噪声、特征和可视化。现在，是时候转向机器学习模型了。没有一种单一的模型，但有很多种——从经典的模型，如随机森林，到用于视觉系统的深度学习模型，再到生成式AI模型，如GPT。
- en: The convolutional and GPT models are called deep learning models. Their name
    comes from the fact that they use raw data as input and the first layers of the
    models include feature extraction layers. They are also designed to progressively
    learn more abstract features as the input data moves through these models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积和GPT模型被称为深度学习模型。它们的名称来源于它们使用原始数据作为输入，并且模型的第一层包括特征提取层。它们还设计为随着输入数据通过这些模型而逐步学习更抽象的特征。
- en: This chapter demonstrates each of these types of models and progresses from
    classical machine learning to generative AI models.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章演示了这些模型类型中的每一种，并从经典机器学习到生成式AI模型逐步推进。
- en: 'In this chapter, we’ll cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Why do we need different types of models?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么我们需要不同类型的模型？
- en: Classical machine learning models and systems, such as random forest, decision
    tree, and logistic regression
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经典的机器学习模型和系统，例如随机森林、决策树和逻辑回归
- en: Deep learning models for vision systems, convolutional neural models, and **You
    Only Look Once** (**YOLO**) models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于视觉系统的深度学习模型、卷积神经网络模型和**你只需看一次**（**YOLO**）模型
- en: '**General Pretrained Transformers** (**GPT**) models'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通用预训练转换器**（**GPT**）模型'
- en: Why do we need different types of models?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么我们需要不同类型的模型？
- en: So far, we have invested a significant amount of effort in data processing while
    focusing on tasks such as noise reduction and annotation. However, we have yet
    to delve into the models that are employed to work with this processed data. While
    we briefly mentioned different types of models based on data annotation, including
    supervised, unsupervised, and reinforced learning, we have not thoroughly explored
    the user’s perspective when it comes to utilizing these models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们在数据处理上投入了大量的努力，同时专注于诸如噪声减少和标注等任务。然而，我们还没有深入研究用于处理这些处理数据的模型。虽然我们简要提到了基于数据标注的不同类型模型，包括监督学习、无监督学习和强化学习，但我们还没有彻底探讨用户在利用这些模型时的视角。
- en: It is important to consider the perspective of the user when employing machine
    learning models for working with data. The user’s needs, preferences, and specific
    requirements play a crucial role in selecting and utilizing the appropriate models.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用机器学习模型处理数据时，考虑用户的视角非常重要。用户的需求、偏好和具体要求在选择和使用适当的模型中起着至关重要的作用。
- en: From the user’s standpoint, it becomes essential to assess factors such as model
    interpretability, ease of integration, computational efficiency, and scalability.
    Depending on the application and use case, the user might prioritize different
    aspects of the models, such as accuracy, speed, or the ability to handle large-scale
    datasets.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 从用户的角度来看，评估诸如模型可解释性、集成简便性、计算效率和可扩展性等因素变得至关重要。根据应用和用例，用户可能会优先考虑模型的不同方面，例如准确性、速度或处理大规模数据集的能力。
- en: Furthermore, the user’s domain expertise and familiarity with the underlying
    algorithms impact the selection and evaluation of models. Some users might prefer
    simpler, more transparent models that offer interpretability and comprehensibility,
    while others might be willing to trade interpretability for improved predictive
    performance using more complex models such as deep learning networks.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，用户的领域专业知识和对底层算法的熟悉程度会影响模型的选择和评估。一些用户可能更喜欢简单、更透明的模型，这些模型提供可解释性和可理解性，而其他人可能愿意为了使用更复杂的模型（如深度学习网络）来提高预测性能而牺牲可解释性。
- en: Considering the user’s perspective enables a more holistic approach to model
    selection and deployment. It involves actively involving the user in the decision-making
    process, gathering feedback, and continuously refining the models to meet their
    specific needs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑用户的视角使模型选择和部署的方法更加全面。这涉及到积极地将用户纳入决策过程，收集反馈，并持续改进模型以满足他们的特定需求。
- en: By incorporating the user’s perspective into the discussion, we can ensure that
    the models we choose not only satisfy technical requirements but also align with
    the user’s expectations and objectives, ultimately enhancing the effectiveness
    and usability of the entire system.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将用户的视角纳入讨论中，我们可以确保我们选择的模型不仅满足技术要求，而且与用户的期望和目标相一致，从而最终提高整个系统的有效性和可用性。
- en: Therefore, moving forward, we’ll explore how different types of users interact
    with and benefit from various machine learning models while considering their
    specific requirements, preferences, and domain expertise. We’ll start with the
    classical machine learning models, which are historically the first ones.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在未来的工作中，我们将探讨不同类型的用户如何与各种机器学习模型互动并从中受益，同时考虑他们的具体需求、偏好和领域专业知识。我们将从经典的机器学习模型开始，这些模型在历史上是最先出现的。
- en: Classical machine learning models
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 经典机器学习模型
- en: Classical machine learning models require pre-processed data in the form of
    tables and matrices. Classical machine learning models, such as random forest,
    linear regression, and support vector machines, require a clear set of predictors
    and classes to find patterns. Due to this, our pre-processing pipelines need to
    be manually designed for the task at hand.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 经典机器学习模型需要以表格和矩阵的形式预处理数据。例如，随机森林、线性回归和支持向量机等经典机器学习模型需要一个清晰的预测器和类别集合来发现模式。因此，我们需要为手头的任务手动设计预处理管道。
- en: 'From the user’s perspective, these systems are designed in a very classical
    way – there is a user interface, an engine for data processing (our classical
    machine learning model), and an output. This is depicted in *Figure 9**.1*:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从用户的视角来看，这些系统是以非常经典的方式设计的——有一个用户界面、数据处理引擎（我们的经典机器学习模型）和输出。这如图 *图9**.1* 所示：
- en: '![Figure 9.1 – Elements of a machine learning system](img/B19548_09_1.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图9.1 – 机器学习系统的要素](img/B19548_09_1.jpg)'
- en: Figure 9.1 – Elements of a machine learning system
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 – 机器学习系统的要素
- en: '*Figure 9**.1* shows that there are three elements – the input prompt, the
    model, and the output. For most such systems, the input prompt is a set of properties
    that are provided for the model. The user fills in some sort of form and the system
    provides an answer. It can be a form for predicting the price of land or a system
    for loans, applying for a job, finding the best car, and so on.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9**.1* 展示了有三个要素——输入提示、模型和输出。对于大多数这样的系统，输入提示是为模型提供的一组属性。用户填写某种形式的表格，系统提供答案。它可以是一个预测土地价格的表格，或者是一个贷款、求职、寻找最佳汽车的系统，等等。'
- en: 'The source code for such a system may look something like this:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的系统的源代码可能看起来像这样：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This fragment of code requires a model to be already trained and only uses it
    for making predictions. The main line that uses the model is the line in boldface.
    The rest of the code fragment is for processing the input and the last line is
    for communicating the output.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码需要模型已经训练好，并且只使用它来进行预测。使用模型的主要行是加粗的行。代码片段的其余部分用于处理输入，最后一行用于输出通信。
- en: 'In modern ecosystems, the power of machine learning models comes from the ability
    to change models without the need to change a lot of code. The majority of classical
    machine learning models use this fit/predict interface, which enables just that.
    So, which machine learning models can we use? There are just too many of them
    to provide an exhaustive list. However, certain groups of these models have certain
    properties:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代生态系统中，机器学习模型的力量来自于无需大量更改代码就能改变模型的能力。大多数经典机器学习模型使用这种拟合/预测接口，正是这种接口使得这一点成为可能。那么，我们可以使用哪些机器学习模型呢？它们的种类实在太多，无法一一列举。然而，这些模型中的一些群体具有某些特性：
- en: '**Regression models** group machine learning models that are used for predicting
    a class value. They can be used both for classification (classifying a module
    to be defect-prone or not) and prediction tasks (predicting the number of defects
    in a module). These models are based on finding the best curve to fit the given
    data.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归模型**将用于预测类别值的机器学习模型分组。它们既可以用于分类（将模块分类为易出故障或不），也可以用于预测任务（预测模块中的缺陷数量）。这些模型基于找到最佳曲线来拟合给定数据。'
- en: '**Tree-based models** group models that are based on finding differences in
    the dataset as if we wrote a set of if-then statements. The logical conditions
    for these if-then statements are based on the statistical properties of the data.
    These models are good for both classification and prediction models.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于树的模型**将基于在数据集中寻找差异的模型分组，就像我们编写了一系列的if-then语句。这些if-then语句的逻辑条件基于数据的统计特性。这些模型适用于分类和预测模型。'
- en: '**Clustering algorithms** group models that are based on finding similarities
    in the data and grouping similar entities. They are often unsupervised and require
    some experimentation to find the right set of parameters (for example, the number
    of clusters).'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类算法**将基于在数据中寻找相似性和将相似实体分组的模型分组。它们通常是未监督的，并且需要一些实验来找到正确的参数集（例如，簇的数量）。'
- en: '**Neural networks** group all kinds of neural networks that can be used for
    classical machine learning tasks. These algorithms require us to design and train
    the neural network model.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经网络**将所有可用于经典机器学习任务的神经网络分组。这些算法需要我们设计和训练神经网络模型。'
- en: We can select these models based on their properties and test them to find the
    best one. However, if we include hyperparameter training, this process is very
    time-consuming and effort-intensive. Therefore, I strongly recommend using AutoML
    approaches for this. AutoML is a group of algorithms that utilize the fit/predict
    interface for machine learning models to find the best model automatically. By
    exploring the plethora of models, they can find the model that is the best for
    the dataset. We say this is with an asterisk. Sometimes, the human ability to
    understand the data and its properties beats most automated machine learning p[rocesses
    (https://metrics.blogg.gu](https://metrics.blogg.gu.se/?p=682).se/?p=682).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据这些模型的特点来选择它们，并通过测试找到最佳模型。然而，如果我们包括超参数训练，这个过程将非常耗时且费力。因此，我强烈推荐使用AutoML方法。AutoML是一组算法，它们利用机器学习模型的fit/predict接口自动寻找最佳模型。通过探索众多模型，它们可以找到最适合数据集的模型。我们用星号表示这一点。有时，人类理解数据和其特性的能力会超过大多数自动机器学习过程（见[博客文章](https://metrics.blogg.gu.se/?p=682)）。
- en: So, here is my first best practice for this chapter.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这是本章的第一个最佳实践。
- en: 'Best practice #50'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳实践#50
- en: Use AutoML as your first choice when you’re training classical machine learning
    models.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当您在训练经典机器学习模型时，请将AutoML作为首选。
- en: 'Using AutoML is very simple and can be illustrated with the following fragment
    of code (from the documentation of auto-sklearn):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 使用AutoML非常简单，以下是从auto-sklearn文档中的代码片段可以说明这一点：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The preceding fragment illustrates how easy it is to use the auto-sklearn toolkit
    to find the best model. Please note that this toolkit has been designed for Linux-based
    systems only. To use it on the Microsoft Windows operating system, I recommend
    using **Windows Subsystem for Linux 2.0** (**WSL 2**). The interface hides the
    best model in such a way that the user does not even have to see which model is
    the best for the data at hand.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的片段说明了使用auto-sklearn工具包寻找最佳模型是多么容易。请注意，这个工具包仅设计用于基于Linux的系统。要在Microsoft Windows操作系统上使用它，我建议使用**Windows
    Subsystem for Linux 2.0**（**WSL 2**）。该界面以这种方式隐藏最佳模型，以至于用户甚至不需要看到哪个模型最适合当前的数据。
- en: '`import autosklearn.classification` imports the auto-sklearn module specifically
    for classification tasks. `cls = autosklearn.classification.AutoSklearnClassifier()`
    initializes an instance of the `AutoSklearnClassifier` class, which represents
    the AutoML classifier in `autosklearn`. It creates an object that will be used
    to search for the best classifier and its hyperparameters automatically. `cls.fit(X_train,
    y_train)` fits `AutoSklearnClassifier` to the training data. It automatically
    explores different classifiers and their hyperparameter configurations to find
    the best model based on the provided `X_train` (features) and `y_train` (target
    labels). It trains the AutoML model on the provided training dataset.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`import autosklearn.classification`导入专门用于分类任务的auto-sklearn模块。`cls = autosklearn.classification.AutoSklearnClassifier()`初始化`AutoSklearnClassifier`类的一个实例，它代表`autosklearn`中的AutoML分类器。它创建一个对象，该对象将用于自动搜索最佳分类器和其超参数。`cls.fit(X_train,
    y_train)`将`AutoSklearnClassifier`拟合到训练数据。它自动探索不同的分类器和它们的超参数配置，以根据提供的`X_train`（特征）和`y_train`（目标标签）找到最佳模型。它在提供的训练数据集上训练AutoML模型。'
- en: '`predictions = cls.predict(X_test)` uses the fitted `AutoSklearnClassifier`
    to make predictions on the `X_test` dataset. It applies the best-found model from
    the previous step to the test data and assigns the predicted labels to the `predictions`
    variable.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`predictions = cls.predict(X_test)` 使用拟合的 `AutoSklearnClassifier` 对 `X_test`
    数据集进行预测。它将上一步找到的最佳模型应用于测试数据，并将预测的标签分配给 `predictions` 变量。'
- en: 'Let’s apply auto-sklearn on the same dataset that we used for visualization
    in [*Chapter 6*](B19548_06.xhtml#_idTextAnchor074):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在[*第6章*](B19548_06.xhtml#_idTextAnchor074)中使用的相同数据集上应用auto-sklearn：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We’ll use the same code we used previously:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用之前使用的相同代码：
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once we have trained the model, we can inspect it – for example, by asking
    auto-sklearn to provide us with information about the best model – using the `print(cls.sprint_statistics())`
    command. The results are as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们训练了模型，我们可以检查它——例如，通过使用 `print(cls.sprint_statistics())` 命令让auto-sklearn提供有关最佳模型的信息。结果如下：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This information shows us that the toolkit has tested `1273` algorithms and
    that `59` of them crashed. This means that they were not compatible with the dataset
    provided by us.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这条信息显示工具包已测试了 `1273` 个算法，其中有 `59` 个崩溃。这意味着它们与我们提供的数据集不兼容。
- en: We can also ask the toolkit to provide us with the best model by using the `print(cls.show_models())`
    command. This command provides a long list of the models that are used for ensemble
    learning and their weight on the final score. Finally, we can ask for the accuracy
    score for the test data by using `print(f\"Accuracy score {sklearn.metrics.accuracy_score(y_test,
    predictions):.2f}\")`. For this dataset, the accuracy score is 0.59 for the test
    data, which is not a lot. However, this is the model that’s obtained by using
    the best ensemble. If we ask the model to provide us with the accuracy score for
    the training data, we’ll get 0.79, which is much higher, but that’s because the
    model is very well optimized.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用 `print(cls.show_models())` 命令让工具包为我们提供最佳模型。此命令提供了一系列用于集成学习的模型及其在最终得分中的权重。最后，我们可以使用
    `print(f\"Accuracy score {sklearn.metrics.accuracy_score(y_test, predictions):.2f}\")`
    来获取测试数据的准确度分数。对于这个数据集，测试数据的准确度分数为0.59，这并不多。然而，这是通过使用最佳集成获得的模型。如果我们要求模型提供训练数据的准确度分数，我们将得到0.79，这要高得多，但这是因为模型非常优化。
- en: Later in this book, we’ll explore these algorithms and learn how they behave
    for tasks in software engineering and beyond.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的后面部分，我们将探讨这些算法，并学习它们在软件工程及其它任务中的行为。
- en: Convolutional neural networks and image processing
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络和图像处理
- en: The classical machine learning models are quite powerful, but they are limited
    in their input. We need to pre-process it so that it’s a set of feature vectors.
    They are also limited in their ability to learn – they are one-shot learners.
    We can only train them once and we cannot add more training. If more training
    is required, we need to train these models from the very beginning.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的机器学习模型相当强大，但在输入方面有限。我们需要预处理它，使其成为一组特征向量。它们在学习能力上也有限——它们是一次性学习者。我们只能训练它们一次，并且不能添加更多训练。如果需要更多训练，我们需要从头开始训练这些模型。
- en: The classical machine learning models are also considered to be rather limited
    in their ability to handle complex structures, such as images. Images, as we have
    learned before, have at least two different dimensions and they can have three
    channels of information – red, green, and blue. In more complex applications,
    the images can contain data from LiDAR or geospatial data that can provide meta-information
    about the images.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的机器学习模型在处理复杂结构，如图像的能力上也被认为相当有限。正如我们之前所学的，图像至少有两个不同的维度，并且可以包含三个信息通道——红色、绿色和蓝色。在更复杂的应用中，图像可以包含来自激光雷达或地理空间数据的数据，这些数据可以提供关于图像的元信息。
- en: So, to handle images, more complex models are needed. One of these models is
    the YOLO model. It’s considered to be state-of-the-art in the area of object detection
    due to its great balance between accuracy and speed.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了处理图像，需要更复杂的模型。其中之一是YOLO模型。由于其准确性和速度之间取得了很好的平衡，YOLO模型被认为在目标检测领域处于最前沿。
- en: Let’s take a look at how we can utilize a pre-trained YOLO v5 model from Hugging
    Face. Here, I would like to provide my next best practice.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何利用Hugging Face中的预训练YOLO v5模型。在这里，我想提供我的下一个最佳实践。
- en: 'Best practice #51'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '最佳实践 #51'
- en: Use pre-trained models from Hugging Face or TensorFlow Hub to start with.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 从Hugging Face或TensorFlow Hub使用预训练模型开始。
- en: 'Using a pre-trained model has a few advantages:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练模型有几个优点：
- en: First of all, it allows us to use the network as a benchmark for our pipeline.
    We can experiment with it and understand its limitations before we move forward
    and start training it.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，它允许我们将网络作为我们管道的基准。在继续前进并开始训练它之前，我们可以对其进行实验并了解其局限性。
- en: Second, it provides us with the possibility to add more training for the existing,
    proven-in-use models that others have also used.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，它为我们提供了为现有、经过实际使用验证的模型添加更多训练的可能性，这些模型也被其他人使用过。
- en: Finally, it provides us with the possibility to share our models with the community
    to support the ethical and responsible development of artificial intelligence.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，它为我们提供了与社区分享我们的模型的可能性，以支持人工智能的道德和负责任的发展。
- en: 'The following code fragment installs the `YoLo` model and instantiates it:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段安装 `YoLo` 模型并实例化它：
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The first few lines load the YOLOv5 model from the specified source – that is,
    `fcakyon/yolov5s-v7.0` – using the `load` function. They assign the loaded model
    to the variable model, which can be used to perform object detection. The `model.conf`
    parameter sets the confidence threshold for **non-maximum suppression** (**NMS**),
    which is used to filter out detections below this confidence level. In this case,
    it is set to 0.25, meaning that only detections with a confidence score above
    0.25 will be considered.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 前几行使用 `load` 函数从指定的源加载 YOLOv5 模型——即 `fcakyon/yolov5s-v7.0` ——并将加载的模型分配给变量 `model`，该变量可用于执行目标检测。`model.conf`
    参数设置了 **非极大值抑制**（**NMS**）的置信度阈值，该阈值用于过滤掉低于此置信度水平的检测。在这种情况下，它被设置为 0.25，这意味着只有置信度分数高于
    0.25 的检测将被考虑。
- en: The `model.iou` parameter sets the `model.agnostic` parameter determines whether
    NMS is class-agnostic or not. If it’s set to `False`, NMS will consider class
    labels during suppression, which means that if two bounding boxes have the same
    coordinates but different labels, they will not be considered duplicates. Here,
    it is set to `False`. The `model.multi_label` parameter controls whether NMS allows
    multiple labels per bounding box. If it’s set to `False`, each box will be assigned
    a single label with the highest confidence score. Here, it is set to `False`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`model.iou` 参数设置了 `model.agnostic` 参数确定 NMS 是否为类别无关。如果设置为 `False`，NMS 将在抑制过程中考虑类别标签，这意味着如果两个边界框具有相同的坐标但不同的标签，它们将不会被考虑为重复。在这里，它被设置为
    `False`。`model.multi_label` 参数控制 NMS 是否允许每个边界框有多个标签。如果设置为 `False`，每个框将被分配一个具有最高置信度分数的单个标签。在这里，它被设置为
    `False`。'
- en: Finally, the `model.max_det` parameter sets the maximum number of detections
    allowed per image. In this case, it is set to `1000`, meaning that only the top
    1,000 detections (sorted by confidence score) will be kept.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`model.max_det` 参数设置了每张图像允许的最大检测数量。在这种情况下，它被设置为 `1000`，这意味着只有前 1,000 个检测（按置信度分数排序）将被保留。
- en: 'Now, we can perform inferences – that is, detect objects using the network
    – but first, we must load the image:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以执行推理——即使用网络检测对象——但首先，我们必须加载图像：
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This code fragment loads the image file located at `./test_image.jpg` using
    the `open` function from PIL’s Image module. It creates an instance of the `Image`
    class representing the image.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码片段使用 PIL 的 Image 模块的 `open` 函数加载位于 `./test_image.jpg` 的图像文件。它创建了一个表示图像的 `Image`
    类实例。
- en: 'Once the image has been loaded, you can apply various transformations to pre-process
    it before feeding it to the YOLOv5 model for object detection. This might involve
    resizing, normalization, or other pre-processing steps, depending on the model’s
    requirements:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦加载了图像，你可以在将其馈送到 YOLOv5 模型进行目标检测之前对其进行各种转换以进行预处理。这可能涉及调整大小、归一化或其他预处理步骤，具体取决于模型的要求：
- en: '[PRE7]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The preceding code fragment performs object detection in the first few lines
    and then draws the image, together with the bounding boxes of the detected object.
    In our case, this is the result of the preceding code fragment:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码片段在第一行执行目标检测，然后绘制图像，以及检测到的对象的边界框。在我们的例子中，这是前面代码片段的结果：
- en: '![Figure 9.2 – Objects detected in the image](img/B19548_09_2.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.2 – 图像中检测到的对象](img/B19548_09_2.jpg)'
- en: Figure 9.2 – Objects detected in the image
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – 图像中检测到的对象
- en: Please note that the model identifies the car as a truck, perhaps because of
    the presence of the additional luggage on the back of the car. The source of the
    image is Pixabay. The figure shows that object detection does not identify the
    object correctly. However, this is not a problem. We can take this pre-trained
    model and train it even more. However, that is the topic of the next few chapters,
    so we won’t cover it here.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，该模型将汽车识别为卡车，可能是因为汽车后部存在额外的行李。图像来源是Pixabay。图示表明，目标检测未能正确识别对象。然而，这并不是问题。我们可以使用这个预训练模型并进一步训练它。然而，这将是下一章的主题，所以我们在这里不会涉及。
- en: 'Best practice #52'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳实践#52
- en: Work with pre-trained networks to identify their limitations and then train
    the network on your own dataset.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 与预训练网络合作，识别它们的局限性，然后在您的数据集上训练网络。
- en: I strongly recommend using the pre-trained models to start with and then train
    the network on your own data. This ability of deep learning models to continue
    training is a great property that we can utilize when designing machine learning-based
    systems. In this way, we get the best of both worlds – our systems can detect
    generic objects while being better at detecting objects that our system specifies.
    This kind of approach is often used in designing automotive systems.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈建议从预训练模型开始使用，然后在您的数据集上训练网络。深度学习模型能够继续训练的能力是我们设计基于机器学习的系统时可以利用的伟大特性。这样，我们就能兼得两者之优——我们的系统可以检测通用对象，同时在检测系统指定的对象方面表现得更好。这种方法通常用于设计汽车系统。
- en: Let’s look at other types of deep learning systems that use pre-training and
    add one more layer of complexity – prompt engineering.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看其他类型的深度学习系统，这些系统使用预训练并增加了一层复杂性——提示工程。
- en: BERT and GPT models
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BERT和GPT模型
- en: BERT and GPT models use raw data as input and their main output is one predicted
    word. This word can be predicted both in the middle of a sentence and at the end
    of it. This means that the products that are designed around these models need
    to process data differently than in the other models.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: BERT和GPT模型使用原始数据作为输入，它们的主要输出是一个预测的单词。这个单词可以在句中预测，也可以在句尾预测。这意味着围绕这些模型设计的产物需要以不同于其他模型的方式处理数据。
- en: '*Figure 9**.3* provides an overview of this kind of processing with a focus
    on both prompt engineering in the beginning and output processing in the end.
    This figure shows the machine learning models based on the BERT or GPT architecture
    in the center. This is an important aspect, but it only provides a very small
    element of the entire system (or tool).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9**.3* 提供了这种处理方法的概述，重点关注开始时的提示工程和结束时的输出处理。此图显示了位于中心的基于BERT或GPT架构的机器学习模型。这是一个重要方面，但它只提供了整个系统（或工具）的非常小的一部分。'
- en: The tool’s workflow starts on the left-hand side with input processing. For
    the user, it is a prompt that asks the model to do something, such as `"Write
    a function that reverses a string in C"`. The tool turns that prompt into a useful
    input for the model – it can find a similar C program as input for the model and
    add the `<mask>` token to the end of that program to make the model predict the
    output.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 工具的工作流程从左侧的输入处理开始。对于用户来说，这是一个提示，要求模型执行某些操作，例如：“用C语言编写一个反转字符串的函数”。工具将这个提示转换为模型的有用输入——它可以找到与模型输入相似的C程序，并在该程序末尾添加`<mask>`标记，使模型预测输出。
- en: 'Then, the model makes the prediction – one token at a time. The tool needs
    to loop the output until something useful is generated. The tool needs to define
    the stop criteria and it needs to process the output into a useful one for the
    user:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，模型进行预测——一次一个标记。工具需要循环输出，直到生成有用的内容。工具需要定义停止标准，并将输出处理成对用户有用的形式：
- en: '![Figure 9.3 – Overview of the design of modern deep learning tools where the
    machine learning model is only a small (but important) part of the tool](img/B19548_09_3.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图9.3 – 现代深度学习工具设计概述，其中机器学习模型只是工具的一个小（但重要）部分](img/B19548_09_3.jpg)'
- en: Figure 9.3 – Overview of the design of modern deep learning tools where the
    machine learning model is only a small (but important) part of the tool
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 – 现代深度学习工具设计概述，其中机器学习模型只是工具的一个小（但重要）部分
- en: 'If we send this prompt to ChatGPT, we get the following response in three parts.
    First, we get the actual code of the function, which reverses the string in C:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将这个提示发送到ChatGPT，我们会得到以下三部分的响应。首先，我们得到函数的实际代码，该函数用C语言反转字符串：
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This function does not have any comments generated, so ChatGPT uses the summarization
    pipeline to describe the model in the following way:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数没有生成任何注释，所以ChatGPT使用摘要管道以以下方式描述模型：
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This explanation is supposed to replace the documentation for the code, which
    needs to be added for good programming practice.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这个说明应该替换代码的文档，这需要添加以符合良好的编程实践。
- en: 'Finally, the tool provides us with an example of the code that tests this function:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这个工具为我们提供了一个测试这个函数的代码示例：
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The last part of the response is also generated as part of the prompt. This
    is because the test code is usually present in similar situations in the dataset.
    Now, if our prompt is something a bit more than just reversing a string, maybe
    in a different programming language, such as C#, we may get a different structure
    of the response. For example, if our prompt is `Write a function that creates
    3000 random numbers in C# and prints them to the console`, then the response will
    only contain the C# code for the function, not for the test code:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 响应的最后部分也是作为提示的一部分生成的。这是因为测试代码通常在数据集中以类似的情况存在。现在，如果我们的提示不仅仅是字符串反转，也许是在不同的编程语言中，比如C#，我们可能会得到不同的响应结构。例如，如果我们的提示是`Write
    a function that creates 3000 random numbers in C# and prints them to the console`，那么响应将只包含该函数的C#代码，而不是测试代码：
- en: '[PRE11]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The explanations are also generated, but not the code that tests this function.
    Instead, the function is wrapped as `class Program` and there is no `main()` function
    to test it.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 也生成了说明，但没有测试这个函数的代码。相反，这个函数被封装为`class Program`，并且没有`main()`函数来测试它。
- en: Using language models in software systems
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在软件系统中使用语言模型
- en: Using products such as ChatGPT is great, but they are also limited to the purpose
    for which they were designed. Now, we can use models like this from scratch using
    the Hugging Face interface. In the following code example, we can see how we can
    use a model dedicated to a specific task – recognizing design patterns – to complete
    the text – that is, writing the signature of a Singleton design pattern. This
    illustrates how language models (including GPT-3/4) work with text under the hood.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ChatGPT等产品很棒，但它们也局限于它们被设计的目的。现在，我们可以使用Hugging Face界面从头开始使用这样的模型。在下面的代码示例中，我们可以看到如何使用专门用于特定任务（如识别设计模式）的模型来完成文本——即编写Singleton设计模式的签名。这说明了语言模型（包括GPT-3/4）在底层是如何与文本工作的。
- en: 'In the following code fragment, we’re importing the model from the Hugging
    Face library and instantiating it. The model has been pre-trained on a set of
    dedicated singleton programs and constructed synthetically by adding random code
    from the Linux kernel source code as code of a Singleton class in C++:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码片段中，我们正在从Hugging Face库导入模型并实例化它。该模型已经在一系列专门的单一程序上进行了预训练，并通过添加来自Linux内核源代码的随机代码作为C++中单一类代码来合成构建：
- en: '[PRE12]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This code imports the necessary modules from the Transformers library from Hugging
    Face. Then, it loads the tokenizer and the model for the pre-trained SingBERTa.
    The tokenizer is responsible for converting text into numerical tokens, and the
    model is a pre-trained language model specifically designed for **masked language
    modeling** (**MLM**) tasks. It loads the model from the pre-trained SingBERTa.
    After, it imports the feature extraction pipeline from the Transformers library.
    The feature extraction pipeline allows us to easily extract contextualized embeddings
    from the model.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码从Hugging Face的Transformers库中导入必要的模块。然后，它加载了预训练的SingBERTa的标记器和模型。标记器负责将文本转换为数值标记，而模型是一个专门为**掩码语言建模**（**MLM**）任务设计的预训练语言模型。它从预训练的SingBERTa中加载模型。之后，它从Transformers库中导入特征提取管道。特征提取管道使我们能够轻松地从模型中提取上下文化的嵌入。
- en: 'Overall, this code sets up the necessary components for us to use the SingBERTa
    model for various natural language processing tasks, such as text tokenization,
    MLM, and feature extraction. The following code fragment does just that – it creates
    the pipeline for filling in the blanks. This means that the model is prepared
    to predict the next word in the sentence:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这段代码为我们设置了使用SingBERTa模型进行各种自然语言处理任务（如文本分词、MLM和特征提取）所必需的组件。下面的代码片段正是这样做的——它创建了填充空白的管道。这意味着模型已经准备好预测句子中的下一个单词：
- en: '[PRE13]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can use this pipeline by using the `fill_mask("static Singleton:: <mask>")`
    command, which results in the following output:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '我们可以通过使用`fill_mask("static Singleton:: <mask>")`命令来使用这个管道，这将产生以下输出：'
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The preceding output shows that the best prediction is the `f` token. This is
    correct since the training example used `f` as the name of the functions that
    were synthetically added to the Singleton class (`Singleton::f1()`, for example).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的输出显示，最佳预测是`f`标记。这是正确的，因为训练示例使用了`f`作为添加到Singleton类中的函数的名称（例如`Singleton::f1()`）。
- en: If we want to expand these predictions, just like the ChatGPT code generation
    feature, we need to loop the preceding code and generate one token at a time,
    thus filling in the program. There is no guarantee that the program will compile,
    so post-processing could essentially select only these constructs (from the list
    of tokens provided), which would lead to a compiling piece of code. We could even
    add features for testing this code, thus making our product smarter and smarter,
    without the need to create a larger model.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要扩展这些预测，就像ChatGPT的代码生成功能一样，我们需要循环前面的代码，一次生成一个标记，从而填充程序。无法保证程序能够编译，因此后处理基本上只能选择这些结构（从提供的标记列表中），这将导致一段可编译的代码。我们甚至可以添加测试此代码的功能，从而使我们的产品越来越智能，而无需创建更大的模型。
- en: Hence, here is my last best practice for this chapter.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这是本章的最后一个最佳实践。
- en: 'Best practice #53'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '最佳实践 #53'
- en: Instead of looking for more complex models, create a smarter pipeline.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 不要寻找更复杂的模型，而是创建一个更智能的管道。
- en: Working with a good pipeline can make a good model into a great software product.
    By providing the right prompt (the beginning of the text to make the prediction),
    we can create an output that is useful for the use case that our product fulfills.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 与一个好的管道一起工作可以使一个好的模型变成一个优秀的软件产品。通过提供正确的提示（用于预测的文本开头），我们可以创建一个对我们产品所满足的使用案例有用的输出。
- en: Summary
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we got a glimpse of what machine learning models look like
    from the inside, at least from the perspective of a programmer. This illustrated
    the major differences in how we construct machine learning-based software.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们窥见了机器学习模型从内部的样子，至少是从程序员的角度来看。这说明了我们在构建基于机器学习的软件时存在的重大差异。
- en: In classical models, we need to create a lot of pre-processing pipelines so
    that the model gets the right input. This means that we need to make sure that
    the data has the right properties and is in the right format; we need to work
    with the output to turn the predictions into something more useful.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在经典模型中，我们需要创建大量的预处理管道，以确保模型获得正确的输入。这意味着我们需要确保数据具有正确的属性，并且处于正确的格式；我们需要与输出一起工作，将预测转化为更有用的东西。
- en: In deep learning models, the data is pre-processed in a more streamlined way.
    The models can prepare the images and the text. Therefore, the software engineers’
    task is to focus on the product and its use case rather than monitoring concept
    drift, data preparation, and post-processing.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习模型中，数据以更流畅的方式进行预处理。模型可以准备图像和文本。因此，软件工程师的任务是专注于产品和其使用案例，而不是监控概念漂移、数据准备和后处理。
- en: In the next chapter, we’ll continue looking at examples of training machine
    learning models – both the classical ones and, most importantly, the deep learning
    ones.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续探讨训练机器学习模型的示例——既包括经典的，也包括最重要的深度学习模型。
- en: References
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*Staron, M. and W. Meding. Short-term defect inflow prediction in large software
    project-an initial evaluation. In International Conference on Empirical Assessment
    in Software Engineering (**EASE). 2007.*'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Staron, M. 和 W. Meding. 在大型软件项目中短期缺陷流入预测的初步评估。在《国际软件工程经验评估会议》（EASE）中。2007年。*'
- en: '*Prykhodko, S. Developing the software defect prediction models using regression
    analysis based on normalizing transformations. In Modern problems in testing of
    the applied software (PTTAS-2016), Abstracts of the Research and Practice Seminar,
    Poltava,* *Ukraine. 2016.*'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Prykhodko, S. 基于归一化变换的回归分析开发软件缺陷预测模型。在《现代应用软件测试问题》（PTTAS-2016）中，研究与实践研讨会摘要，波尔塔瓦，乌克兰。2016年。*'
- en: '*Ochodek, M., et al.,* *Chapter 8* *Recognizing Lines of Code Violating Company-Specific
    Coding Guidelines Using Machine Learning**. In Accelerating Digital Transformation:
    10 Years of Software Center. 2022, Springer.* *p. 211-251.*'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Ochodek, M. 等，* 第8章 *使用机器学习识别违反公司特定编码指南的代码行*。在《加速数字化转型：软件中心10年》中。2022年，Springer。*
    第211-251页。*'
- en: '*Ibrahim, D.R., R. Ghnemat, and A. Hudaib. Software defect prediction using
    feature selection and random forest algorithm. In 2017 International Conference
    on New Trends in Computing Sciences (ICTCS).* *2017\. IEEE.*'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Ibrahim, D.R.，R. Ghnemat，和A. Hudaib。使用特征选择和随机森林算法进行软件缺陷预测。在2017年国际计算科学新趋势会议（ICTCS）上。2017\.
    IEEE.*'
- en: '*Ochodek, M., M. Staron, and W. Meding,* *Chapter 9* *SimSAX: A Measure of
    Project Similarity Based on Symbolic Approximation Method and Software Defect
    Inflow. In Accelerating Digital Transformation: 10 Years of Software Center. 2022,
    Springer.* *p. 253-283.*'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Ochodek, M.，M. Staron，和W. Meding，* *第9章* *SimSAX：基于符号近似方法和软件缺陷流入的项目相似度度量。在加速数字化转型：软件中心10年。2022，Springer。*
    *p. 253-283.*'
- en: '*Phan, V.A., Learning Stretch-Shrink Latent Representations With Autoencoder
    and K-Means for Software Defect Prediction. IEEE Access, 2022\. 10:* *p. 117827-117835.*'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Phan, V.A.，使用自动编码器和K-Means学习拉伸-收缩潜在表示以进行软件缺陷预测。IEEE Access，2022\. 10:* *p.
    117827-117835.*'
- en: '*Staron, M., et al., Machine learning to support code reviews in continuous
    integration. Artificial Intelligence Methods For Software Engineering, 2021:*
    *p. 141-167.*'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Staron, M.，等人，机器学习支持持续集成中的代码审查。软件工程人工智能方法，2021:* *p. 141-167.*'
- en: '*Li, J., et al. Software defect prediction via convolutional neural network.
    In 2017 IEEE International Conference on Software Quality, Reliability and Security
    (QRS).* *2017\. IEEE.*'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Li, J.，等人。通过卷积神经网络进行软件缺陷预测。在2017年IEEE国际软件质量、可靠性和安全性会议（QRS）上。2017\. IEEE.*'
- en: '*Feurer, M., et al., Efficient and robust automated machine learning. Advances
    in neural information processing systems,* *2015\. 28.*'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Feurer, M.，等人，高效且鲁棒的自动机器学习。神经网络信息处理系统进展，* *2015\. 28.*'
- en: '*Feurer, M., et al., Auto-sklearn 2.0: Hands-free automl via meta-learning.
    The Journal of Machine Learning Research, 2022\. 23(1):* *p. 11936-11996.*'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Feurer, M.，等人，Auto-sklearn 2.0：通过元学习实现免手动的自动机器学习。机器学习研究杂志，2022\. 23(1):* *p.
    11936-11996.*'
- en: '*Redmon, J., et al. You only look once: Unified, real-time object detection.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern* *Recognition.
    2016.*'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Redmon, J.，等人。你只看一次：统一、实时目标检测。在IEEE计算机视觉和模式识别会议论文集中。2016.*'
- en: '*Staron, M., Automotive software architectures.* *2021: Springer.*'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Staron, M., 《汽车软件架构》.* *2021: Springer.*'
- en: '*Gamma, E., et al., Design patterns: elements of reusable object-oriented software.
    1995: Pearson* *Deutschland GmbH.*'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Gamma, E., 等人，设计模式：可重用面向对象软件的元素。1995: Pearson* *Deutschland GmbH.*'
