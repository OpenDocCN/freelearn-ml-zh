- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Types of Machine Learning Systems – Feature-Based and Raw Data-Based (Deep Learning)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we learned about data, noise, features, and visualization.
    Now, it’s time to move on to machine learning models. There is no such thing as
    one model, but there are plenty of them – starting from the classical models such
    as random forest to deep learning models for vision systems to generative AI models
    such as GPT.
  prefs: []
  type: TYPE_NORMAL
- en: The convolutional and GPT models are called deep learning models. Their name
    comes from the fact that they use raw data as input and the first layers of the
    models include feature extraction layers. They are also designed to progressively
    learn more abstract features as the input data moves through these models.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter demonstrates each of these types of models and progresses from
    classical machine learning to generative AI models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Why do we need different types of models?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classical machine learning models and systems, such as random forest, decision
    tree, and logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning models for vision systems, convolutional neural models, and **You
    Only Look Once** (**YOLO**) models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**General Pretrained Transformers** (**GPT**) models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why do we need different types of models?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have invested a significant amount of effort in data processing while
    focusing on tasks such as noise reduction and annotation. However, we have yet
    to delve into the models that are employed to work with this processed data. While
    we briefly mentioned different types of models based on data annotation, including
    supervised, unsupervised, and reinforced learning, we have not thoroughly explored
    the user’s perspective when it comes to utilizing these models.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to consider the perspective of the user when employing machine
    learning models for working with data. The user’s needs, preferences, and specific
    requirements play a crucial role in selecting and utilizing the appropriate models.
  prefs: []
  type: TYPE_NORMAL
- en: From the user’s standpoint, it becomes essential to assess factors such as model
    interpretability, ease of integration, computational efficiency, and scalability.
    Depending on the application and use case, the user might prioritize different
    aspects of the models, such as accuracy, speed, or the ability to handle large-scale
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the user’s domain expertise and familiarity with the underlying
    algorithms impact the selection and evaluation of models. Some users might prefer
    simpler, more transparent models that offer interpretability and comprehensibility,
    while others might be willing to trade interpretability for improved predictive
    performance using more complex models such as deep learning networks.
  prefs: []
  type: TYPE_NORMAL
- en: Considering the user’s perspective enables a more holistic approach to model
    selection and deployment. It involves actively involving the user in the decision-making
    process, gathering feedback, and continuously refining the models to meet their
    specific needs.
  prefs: []
  type: TYPE_NORMAL
- en: By incorporating the user’s perspective into the discussion, we can ensure that
    the models we choose not only satisfy technical requirements but also align with
    the user’s expectations and objectives, ultimately enhancing the effectiveness
    and usability of the entire system.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, moving forward, we’ll explore how different types of users interact
    with and benefit from various machine learning models while considering their
    specific requirements, preferences, and domain expertise. We’ll start with the
    classical machine learning models, which are historically the first ones.
  prefs: []
  type: TYPE_NORMAL
- en: Classical machine learning models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classical machine learning models require pre-processed data in the form of
    tables and matrices. Classical machine learning models, such as random forest,
    linear regression, and support vector machines, require a clear set of predictors
    and classes to find patterns. Due to this, our pre-processing pipelines need to
    be manually designed for the task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the user’s perspective, these systems are designed in a very classical
    way – there is a user interface, an engine for data processing (our classical
    machine learning model), and an output. This is depicted in *Figure 9**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Elements of a machine learning system](img/B19548_09_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – Elements of a machine learning system
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9**.1* shows that there are three elements – the input prompt, the
    model, and the output. For most such systems, the input prompt is a set of properties
    that are provided for the model. The user fills in some sort of form and the system
    provides an answer. It can be a form for predicting the price of land or a system
    for loans, applying for a job, finding the best car, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The source code for such a system may look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This fragment of code requires a model to be already trained and only uses it
    for making predictions. The main line that uses the model is the line in boldface.
    The rest of the code fragment is for processing the input and the last line is
    for communicating the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'In modern ecosystems, the power of machine learning models comes from the ability
    to change models without the need to change a lot of code. The majority of classical
    machine learning models use this fit/predict interface, which enables just that.
    So, which machine learning models can we use? There are just too many of them
    to provide an exhaustive list. However, certain groups of these models have certain
    properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Regression models** group machine learning models that are used for predicting
    a class value. They can be used both for classification (classifying a module
    to be defect-prone or not) and prediction tasks (predicting the number of defects
    in a module). These models are based on finding the best curve to fit the given
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tree-based models** group models that are based on finding differences in
    the dataset as if we wrote a set of if-then statements. The logical conditions
    for these if-then statements are based on the statistical properties of the data.
    These models are good for both classification and prediction models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clustering algorithms** group models that are based on finding similarities
    in the data and grouping similar entities. They are often unsupervised and require
    some experimentation to find the right set of parameters (for example, the number
    of clusters).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Neural networks** group all kinds of neural networks that can be used for
    classical machine learning tasks. These algorithms require us to design and train
    the neural network model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can select these models based on their properties and test them to find the
    best one. However, if we include hyperparameter training, this process is very
    time-consuming and effort-intensive. Therefore, I strongly recommend using AutoML
    approaches for this. AutoML is a group of algorithms that utilize the fit/predict
    interface for machine learning models to find the best model automatically. By
    exploring the plethora of models, they can find the model that is the best for
    the dataset. We say this is with an asterisk. Sometimes, the human ability to
    understand the data and its properties beats most automated machine learning p[rocesses
    (https://metrics.blogg.gu](https://metrics.blogg.gu.se/?p=682).se/?p=682).
  prefs: []
  type: TYPE_NORMAL
- en: So, here is my first best practice for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #50'
  prefs: []
  type: TYPE_NORMAL
- en: Use AutoML as your first choice when you’re training classical machine learning
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using AutoML is very simple and can be illustrated with the following fragment
    of code (from the documentation of auto-sklearn):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The preceding fragment illustrates how easy it is to use the auto-sklearn toolkit
    to find the best model. Please note that this toolkit has been designed for Linux-based
    systems only. To use it on the Microsoft Windows operating system, I recommend
    using **Windows Subsystem for Linux 2.0** (**WSL 2**). The interface hides the
    best model in such a way that the user does not even have to see which model is
    the best for the data at hand.
  prefs: []
  type: TYPE_NORMAL
- en: '`import autosklearn.classification` imports the auto-sklearn module specifically
    for classification tasks. `cls = autosklearn.classification.AutoSklearnClassifier()`
    initializes an instance of the `AutoSklearnClassifier` class, which represents
    the AutoML classifier in `autosklearn`. It creates an object that will be used
    to search for the best classifier and its hyperparameters automatically. `cls.fit(X_train,
    y_train)` fits `AutoSklearnClassifier` to the training data. It automatically
    explores different classifiers and their hyperparameter configurations to find
    the best model based on the provided `X_train` (features) and `y_train` (target
    labels). It trains the AutoML model on the provided training dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '`predictions = cls.predict(X_test)` uses the fitted `AutoSklearnClassifier`
    to make predictions on the `X_test` dataset. It applies the best-found model from
    the previous step to the test data and assigns the predicted labels to the `predictions`
    variable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s apply auto-sklearn on the same dataset that we used for visualization
    in [*Chapter 6*](B19548_06.xhtml#_idTextAnchor074):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll use the same code we used previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have trained the model, we can inspect it – for example, by asking
    auto-sklearn to provide us with information about the best model – using the `print(cls.sprint_statistics())`
    command. The results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This information shows us that the toolkit has tested `1273` algorithms and
    that `59` of them crashed. This means that they were not compatible with the dataset
    provided by us.
  prefs: []
  type: TYPE_NORMAL
- en: We can also ask the toolkit to provide us with the best model by using the `print(cls.show_models())`
    command. This command provides a long list of the models that are used for ensemble
    learning and their weight on the final score. Finally, we can ask for the accuracy
    score for the test data by using `print(f\"Accuracy score {sklearn.metrics.accuracy_score(y_test,
    predictions):.2f}\")`. For this dataset, the accuracy score is 0.59 for the test
    data, which is not a lot. However, this is the model that’s obtained by using
    the best ensemble. If we ask the model to provide us with the accuracy score for
    the training data, we’ll get 0.79, which is much higher, but that’s because the
    model is very well optimized.
  prefs: []
  type: TYPE_NORMAL
- en: Later in this book, we’ll explore these algorithms and learn how they behave
    for tasks in software engineering and beyond.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional neural networks and image processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The classical machine learning models are quite powerful, but they are limited
    in their input. We need to pre-process it so that it’s a set of feature vectors.
    They are also limited in their ability to learn – they are one-shot learners.
    We can only train them once and we cannot add more training. If more training
    is required, we need to train these models from the very beginning.
  prefs: []
  type: TYPE_NORMAL
- en: The classical machine learning models are also considered to be rather limited
    in their ability to handle complex structures, such as images. Images, as we have
    learned before, have at least two different dimensions and they can have three
    channels of information – red, green, and blue. In more complex applications,
    the images can contain data from LiDAR or geospatial data that can provide meta-information
    about the images.
  prefs: []
  type: TYPE_NORMAL
- en: So, to handle images, more complex models are needed. One of these models is
    the YOLO model. It’s considered to be state-of-the-art in the area of object detection
    due to its great balance between accuracy and speed.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at how we can utilize a pre-trained YOLO v5 model from Hugging
    Face. Here, I would like to provide my next best practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #51'
  prefs: []
  type: TYPE_NORMAL
- en: Use pre-trained models from Hugging Face or TensorFlow Hub to start with.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a pre-trained model has a few advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: First of all, it allows us to use the network as a benchmark for our pipeline.
    We can experiment with it and understand its limitations before we move forward
    and start training it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second, it provides us with the possibility to add more training for the existing,
    proven-in-use models that others have also used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, it provides us with the possibility to share our models with the community
    to support the ethical and responsible development of artificial intelligence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code fragment installs the `YoLo` model and instantiates it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The first few lines load the YOLOv5 model from the specified source – that is,
    `fcakyon/yolov5s-v7.0` – using the `load` function. They assign the loaded model
    to the variable model, which can be used to perform object detection. The `model.conf`
    parameter sets the confidence threshold for **non-maximum suppression** (**NMS**),
    which is used to filter out detections below this confidence level. In this case,
    it is set to 0.25, meaning that only detections with a confidence score above
    0.25 will be considered.
  prefs: []
  type: TYPE_NORMAL
- en: The `model.iou` parameter sets the `model.agnostic` parameter determines whether
    NMS is class-agnostic or not. If it’s set to `False`, NMS will consider class
    labels during suppression, which means that if two bounding boxes have the same
    coordinates but different labels, they will not be considered duplicates. Here,
    it is set to `False`. The `model.multi_label` parameter controls whether NMS allows
    multiple labels per bounding box. If it’s set to `False`, each box will be assigned
    a single label with the highest confidence score. Here, it is set to `False`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the `model.max_det` parameter sets the maximum number of detections
    allowed per image. In this case, it is set to `1000`, meaning that only the top
    1,000 detections (sorted by confidence score) will be kept.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can perform inferences – that is, detect objects using the network
    – but first, we must load the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This code fragment loads the image file located at `./test_image.jpg` using
    the `open` function from PIL’s Image module. It creates an instance of the `Image`
    class representing the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the image has been loaded, you can apply various transformations to pre-process
    it before feeding it to the YOLOv5 model for object detection. This might involve
    resizing, normalization, or other pre-processing steps, depending on the model’s
    requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code fragment performs object detection in the first few lines
    and then draws the image, together with the bounding boxes of the detected object.
    In our case, this is the result of the preceding code fragment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Objects detected in the image](img/B19548_09_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – Objects detected in the image
  prefs: []
  type: TYPE_NORMAL
- en: Please note that the model identifies the car as a truck, perhaps because of
    the presence of the additional luggage on the back of the car. The source of the
    image is Pixabay. The figure shows that object detection does not identify the
    object correctly. However, this is not a problem. We can take this pre-trained
    model and train it even more. However, that is the topic of the next few chapters,
    so we won’t cover it here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #52'
  prefs: []
  type: TYPE_NORMAL
- en: Work with pre-trained networks to identify their limitations and then train
    the network on your own dataset.
  prefs: []
  type: TYPE_NORMAL
- en: I strongly recommend using the pre-trained models to start with and then train
    the network on your own data. This ability of deep learning models to continue
    training is a great property that we can utilize when designing machine learning-based
    systems. In this way, we get the best of both worlds – our systems can detect
    generic objects while being better at detecting objects that our system specifies.
    This kind of approach is often used in designing automotive systems.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at other types of deep learning systems that use pre-training and
    add one more layer of complexity – prompt engineering.
  prefs: []
  type: TYPE_NORMAL
- en: BERT and GPT models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: BERT and GPT models use raw data as input and their main output is one predicted
    word. This word can be predicted both in the middle of a sentence and at the end
    of it. This means that the products that are designed around these models need
    to process data differently than in the other models.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9**.3* provides an overview of this kind of processing with a focus
    on both prompt engineering in the beginning and output processing in the end.
    This figure shows the machine learning models based on the BERT or GPT architecture
    in the center. This is an important aspect, but it only provides a very small
    element of the entire system (or tool).'
  prefs: []
  type: TYPE_NORMAL
- en: The tool’s workflow starts on the left-hand side with input processing. For
    the user, it is a prompt that asks the model to do something, such as `"Write
    a function that reverses a string in C"`. The tool turns that prompt into a useful
    input for the model – it can find a similar C program as input for the model and
    add the `<mask>` token to the end of that program to make the model predict the
    output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, the model makes the prediction – one token at a time. The tool needs
    to loop the output until something useful is generated. The tool needs to define
    the stop criteria and it needs to process the output into a useful one for the
    user:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – Overview of the design of modern deep learning tools where the
    machine learning model is only a small (but important) part of the tool](img/B19548_09_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – Overview of the design of modern deep learning tools where the
    machine learning model is only a small (but important) part of the tool
  prefs: []
  type: TYPE_NORMAL
- en: 'If we send this prompt to ChatGPT, we get the following response in three parts.
    First, we get the actual code of the function, which reverses the string in C:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This function does not have any comments generated, so ChatGPT uses the summarization
    pipeline to describe the model in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This explanation is supposed to replace the documentation for the code, which
    needs to be added for good programming practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the tool provides us with an example of the code that tests this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The last part of the response is also generated as part of the prompt. This
    is because the test code is usually present in similar situations in the dataset.
    Now, if our prompt is something a bit more than just reversing a string, maybe
    in a different programming language, such as C#, we may get a different structure
    of the response. For example, if our prompt is `Write a function that creates
    3000 random numbers in C# and prints them to the console`, then the response will
    only contain the C# code for the function, not for the test code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The explanations are also generated, but not the code that tests this function.
    Instead, the function is wrapped as `class Program` and there is no `main()` function
    to test it.
  prefs: []
  type: TYPE_NORMAL
- en: Using language models in software systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using products such as ChatGPT is great, but they are also limited to the purpose
    for which they were designed. Now, we can use models like this from scratch using
    the Hugging Face interface. In the following code example, we can see how we can
    use a model dedicated to a specific task – recognizing design patterns – to complete
    the text – that is, writing the signature of a Singleton design pattern. This
    illustrates how language models (including GPT-3/4) work with text under the hood.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code fragment, we’re importing the model from the Hugging
    Face library and instantiating it. The model has been pre-trained on a set of
    dedicated singleton programs and constructed synthetically by adding random code
    from the Linux kernel source code as code of a Singleton class in C++:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This code imports the necessary modules from the Transformers library from Hugging
    Face. Then, it loads the tokenizer and the model for the pre-trained SingBERTa.
    The tokenizer is responsible for converting text into numerical tokens, and the
    model is a pre-trained language model specifically designed for **masked language
    modeling** (**MLM**) tasks. It loads the model from the pre-trained SingBERTa.
    After, it imports the feature extraction pipeline from the Transformers library.
    The feature extraction pipeline allows us to easily extract contextualized embeddings
    from the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, this code sets up the necessary components for us to use the SingBERTa
    model for various natural language processing tasks, such as text tokenization,
    MLM, and feature extraction. The following code fragment does just that – it creates
    the pipeline for filling in the blanks. This means that the model is prepared
    to predict the next word in the sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use this pipeline by using the `fill_mask("static Singleton:: <mask>")`
    command, which results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The preceding output shows that the best prediction is the `f` token. This is
    correct since the training example used `f` as the name of the functions that
    were synthetically added to the Singleton class (`Singleton::f1()`, for example).
  prefs: []
  type: TYPE_NORMAL
- en: If we want to expand these predictions, just like the ChatGPT code generation
    feature, we need to loop the preceding code and generate one token at a time,
    thus filling in the program. There is no guarantee that the program will compile,
    so post-processing could essentially select only these constructs (from the list
    of tokens provided), which would lead to a compiling piece of code. We could even
    add features for testing this code, thus making our product smarter and smarter,
    without the need to create a larger model.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, here is my last best practice for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #53'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of looking for more complex models, create a smarter pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Working with a good pipeline can make a good model into a great software product.
    By providing the right prompt (the beginning of the text to make the prediction),
    we can create an output that is useful for the use case that our product fulfills.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we got a glimpse of what machine learning models look like
    from the inside, at least from the perspective of a programmer. This illustrated
    the major differences in how we construct machine learning-based software.
  prefs: []
  type: TYPE_NORMAL
- en: In classical models, we need to create a lot of pre-processing pipelines so
    that the model gets the right input. This means that we need to make sure that
    the data has the right properties and is in the right format; we need to work
    with the output to turn the predictions into something more useful.
  prefs: []
  type: TYPE_NORMAL
- en: In deep learning models, the data is pre-processed in a more streamlined way.
    The models can prepare the images and the text. Therefore, the software engineers’
    task is to focus on the product and its use case rather than monitoring concept
    drift, data preparation, and post-processing.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll continue looking at examples of training machine
    learning models – both the classical ones and, most importantly, the deep learning
    ones.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Staron, M. and W. Meding. Short-term defect inflow prediction in large software
    project-an initial evaluation. In International Conference on Empirical Assessment
    in Software Engineering (**EASE). 2007.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Prykhodko, S. Developing the software defect prediction models using regression
    analysis based on normalizing transformations. In Modern problems in testing of
    the applied software (PTTAS-2016), Abstracts of the Research and Practice Seminar,
    Poltava,* *Ukraine. 2016.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ochodek, M., et al.,* *Chapter 8* *Recognizing Lines of Code Violating Company-Specific
    Coding Guidelines Using Machine Learning**. In Accelerating Digital Transformation:
    10 Years of Software Center. 2022, Springer.* *p. 211-251.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ibrahim, D.R., R. Ghnemat, and A. Hudaib. Software defect prediction using
    feature selection and random forest algorithm. In 2017 International Conference
    on New Trends in Computing Sciences (ICTCS).* *2017\. IEEE.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ochodek, M., M. Staron, and W. Meding,* *Chapter 9* *SimSAX: A Measure of
    Project Similarity Based on Symbolic Approximation Method and Software Defect
    Inflow. In Accelerating Digital Transformation: 10 Years of Software Center. 2022,
    Springer.* *p. 253-283.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Phan, V.A., Learning Stretch-Shrink Latent Representations With Autoencoder
    and K-Means for Software Defect Prediction. IEEE Access, 2022\. 10:* *p. 117827-117835.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Staron, M., et al., Machine learning to support code reviews in continuous
    integration. Artificial Intelligence Methods For Software Engineering, 2021:*
    *p. 141-167.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Li, J., et al. Software defect prediction via convolutional neural network.
    In 2017 IEEE International Conference on Software Quality, Reliability and Security
    (QRS).* *2017\. IEEE.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Feurer, M., et al., Efficient and robust automated machine learning. Advances
    in neural information processing systems,* *2015\. 28.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Feurer, M., et al., Auto-sklearn 2.0: Hands-free automl via meta-learning.
    The Journal of Machine Learning Research, 2022\. 23(1):* *p. 11936-11996.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Redmon, J., et al. You only look once: Unified, real-time object detection.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern* *Recognition.
    2016.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Staron, M., Automotive software architectures.* *2021: Springer.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Gamma, E., et al., Design patterns: elements of reusable object-oriented software.
    1995: Pearson* *Deutschland GmbH.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
