<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer216">&#13;
			<h1 id="_idParaDest-229" class="chapter-number"><a id="_idTextAnchor235"/>11</h1>&#13;
			<h1 id="_idParaDest-230"><a id="_idTextAnchor236"/>Statistical Estimation</h1>&#13;
			<p>In this chapter, we will introduce you to a range of statistical techniques that enable you to make inferences and estimations using both numerical and categorical data. We will explore key concepts and methods, such as hypothesis testing, confidence intervals, and estimation techniques, that empower us to make generalizations about populations from a <span class="No-Break">given sample.</span></p>&#13;
			<p>By the end of this chapter, you will grasp the core concepts of statistical inference and be able to perform hypothesis testing in <span class="No-Break">different scenarios.</span></p>&#13;
			<p>In this chapter, we will cover the following <span class="No-Break">main topics:</span></p>&#13;
			<ul>&#13;
				<li>Statistical inference for <span class="No-Break">categorical data</span></li>&#13;
				<li>Statistical inference for <span class="No-Break">numerical data</span></li>&#13;
				<li>Constructing the bootstrapped <span class="No-Break">confidence interval</span></li>&#13;
				<li>Introducing the central limit theorem used <span class="No-Break">in t-distribution</span></li>&#13;
				<li>Constructing the confidence interval for the population mean using <span class="No-Break">the t-distribution</span></li>&#13;
				<li>Performing hypothesis testing for <span class="No-Break">two means</span></li>&#13;
				<li><span class="No-Break">Introducing ANOVA</span></li>&#13;
			</ul>&#13;
			<p>To run the code in this chapter, you will need to have the latest versions of the <span class="No-Break">following packages:</span></p>&#13;
			<ul>&#13;
				<li><span class="No-Break"><strong class="source-inline">dplyr</strong></span><span class="No-Break">, 1.0.10</span></li>&#13;
				<li><span class="No-Break"><strong class="source-inline">ggplot2</strong></span><span class="No-Break">, 3.4.0</span></li>&#13;
				<li><span class="No-Break"><strong class="source-inline">socviz</strong></span><span class="No-Break">, 1.2</span></li>&#13;
				<li><span class="No-Break"><strong class="source-inline">infer</strong></span><span class="No-Break">, 1.0.4</span></li>&#13;
			</ul>&#13;
			<p>Please note that the versions mentioned in the preceding list are the latest ones at the time I am writing this book. All the code and data for this chapter is available <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_11/working.R"><span class="No-Break">https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_11/working.R</span></a><span class="No-Break">.</span></p>&#13;
			<h1 id="_idParaDest-231"><a id="_idTextAnchor237"/>Statistical inference for categorical data</h1>&#13;
			<p>A categorical variable<a id="_idIndexMarker1001"/> has distinct categories or levels, rather than numerical values. Categorical data is <a id="_idIndexMarker1002"/>common in our daily lives, such as gender (male or female, although a modern view may differ), type of property <a id="_idIndexMarker1003"/>sales (new property or resale), and industry. The ability to make sound inferences about these variables is thus essential for drawing meaningful conclusions and making well-informed decisions in <span class="No-Break">diverse contexts.</span></p>&#13;
			<p>Being a categorical variable often means we cannot pass it to a <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) model<a id="_idIndexMarker1004"/> without additional preprocessing. Take the industry variable, for example. Instead of passing the categorical values (<strong class="source-inline">string</strong> values such as <strong class="source-inline">"finance"</strong> or <strong class="source-inline">"technology"</strong>) to the model, a common approach is to one-hot encode the variable into multiple columns, with each column corresponding to a specific industry, indicating a binary value of <strong class="source-inline">0</strong> <span class="No-Break">or </span><span class="No-Break"><strong class="source-inline">1</strong></span><span class="No-Break">.</span></p>&#13;
			<p>In this section, we will explore various statistical techniques designed specifically to handle categorical data, enabling us to derive valuable insights and make inferences about populations based on available samples. We will also discuss important concepts, such as proportions, independence, and goodness of fit, which form the foundation for understanding and working with categorical variables, covering both cases with a single parameter and<a id="_idIndexMarker1005"/> <span class="No-Break">multiple parameters.</span></p>&#13;
			<p>Let us start by discussing the inference for a <span class="No-Break">single parameter.</span></p>&#13;
			<h2 id="_idParaDest-232"><a id="_idTextAnchor238"/>Statistical inference for a single parameter</h2>&#13;
			<p>A <a id="_idIndexMarker1006"/>population parameter, the subject of interest and to be inferred, is a fixed quantity that describes a particular statistical attribute of a population, including the mean, proportion, or standard deviation. This quantity often stays hidden from us. For example, in order to get the most popular major in a university, we need to count the number of enrolled students in each major across the whole university and then return the major with the <span class="No-Break">biggest count.</span></p>&#13;
			<p>In the context of statistical inference for a single parameter, we aim to estimate this unknown parameter or test hypotheses about its value based on the information gathered from a sample. In other words, we would use statistical inference tools to infer unknown population parameters based on the known sample at hand. In the previous example, we would infer the most popular major of the whole university by a limited sample of students enrolled in a specific <span class="No-Break">academic year.</span></p>&#13;
			<p>Let us first explore <a id="_idIndexMarker1007"/>the <strong class="bold">General Social Survey</strong> (<span class="No-Break"><strong class="bold">GSS</strong></span><span class="No-Break">) dataset.</span></p>&#13;
			<h2 id="_idParaDest-233"><a id="_idTextAnchor239"/>Introducing the General Social Survey dataset</h2>&#13;
			<p>The GSS is a <a id="_idIndexMarker1008"/>comprehensive dataset widely used by researchers and policymakers to understand social, cultural, and political trends in the United States. The GSS has been continued by the <strong class="bold">National Opinion Research Center</strong> (<strong class="bold">NORC</strong>) at the<a id="_idIndexMarker1009"/> University of Chicago since 1972, with the objective of collecting data on a broad range of topics, including attitudes, behaviors, and opinions on <span class="No-Break">various issues.</span></p>&#13;
			<p>Let us load the GSS dataset from the <strong class="source-inline">socviz</strong> package (remember to install this package <span class="No-Break">via </span><span class="No-Break"><strong class="source-inline">install.packages("socviz")</strong></span><span class="No-Break">):</span></p>&#13;
			<pre class="source-code">&#13;
library(socviz)&#13;
data(gss_lon)</pre>			<p>The GSS dataset is now stored in the <strong class="source-inline">gss_lon</strong> variable, which contains a total of 62,466 rows and 25 columns, as <span class="No-Break">shown here:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; dim(gss_lon)&#13;
62466    25</pre>			<p>The GSS dataset <a id="_idIndexMarker1010"/>contains numerous variables that cover diverse topics, such as education, income, family structure, political beliefs, and religious affiliation. Let us examine the structure of the dataset using the <strong class="source-inline">glimpse()</strong> function from the <strong class="source-inline">dplyr</strong> package, designed to help you quickly explore and understand the structure of <span class="No-Break">the data:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; glimpse(gss_lon)</pre>			<p><span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.1</em> shows a screenshot of the first few <span class="No-Break">variables returned.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer200" class="IMG---Figure">&#13;
					<img src="Images/B18680_11_001.jpg" alt="Figure 11.1 – Showing the first few rows of the result from running the glimpse() function" width="1064" height="350"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1 – Showing the first few rows of the result from running the glimpse() function</p>&#13;
			<p>Next, we will look at calculating a specific statistic based on a <span class="No-Break">categorical variable.</span></p>&#13;
			<h2 id="_idParaDest-234"><a id="_idTextAnchor240"/>Calculating the sample proportion</h2>&#13;
			<p>The <strong class="source-inline">siblings</strong> column in<a id="_idIndexMarker1011"/> the dataset is a categorical variable that tracks the number of siblings in the family. In the following exercise, we would like to calculate the proportion of survey respondents whose family has two siblings in the latest <span class="No-Break">year, 2016.</span></p>&#13;
			<h3>Exercise 11.1 – calculating the sample proportion of siblings</h3>&#13;
			<p>In this exercise, we first<a id="_idIndexMarker1012"/> obtain a summary of the <strong class="source-inline">siblings</strong> column and subset the dataset to focus on the year 2016, which will then be used to calculate the proportion of surveys with a specific number of siblings in <span class="No-Break">the family:</span></p>&#13;
			<ol>&#13;
				<li>Obtain a summary of the <strong class="source-inline">siblings</strong> column using the <span class="No-Break"><strong class="source-inline">summary()</strong></span><span class="No-Break"> function:</span><pre class="source-code">&#13;
&gt;&gt;&gt; summary(gss_lon$siblings)&#13;
    0     1     2     3     4     5    6+  NA's&#13;
 3047 10152 11313  9561  7024  5066 14612  1691</pre><p class="list-inset">The result suggests that most surveys are conducted for families with six siblings <span class="No-Break">or more!</span></p></li>				<li>Subset the <a id="_idIndexMarker1013"/>dataset for the <span class="No-Break">year 2016:</span><pre class="source-code">&#13;
gss2016 = gss_lon %&gt;% filter(year == 2016)&#13;
Plot the count of siblings in a bar chart using ggplot().&#13;
ggplot(gss2016, aes(x = siblings)) +&#13;
  geom_bar() +&#13;
  labs(title = "Frequency count of siblings", x = "Number of siblings", y = "Count") +&#13;
  theme(text = element_text(size = 16))</pre><p class="list-inset">Running the code generates the chart in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">.</span></p></li>			</ol>&#13;
			<div>&#13;
				<div id="_idContainer201" class="IMG---Figure">&#13;
					<img src="Images/B18680_11_002.jpg" alt="Figure 11.2 – Visualizing the frequency count of the number of siblings in a bar chart" width="1095" height="912"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2 – Visualizing the frequency count of the number of siblings in a bar chart</p>&#13;
			<ol>&#13;
				<li value="3">Calculate the <a id="_idIndexMarker1014"/>proportion of surveys with <span class="No-Break">two siblings:</span><pre class="source-code">&#13;
p_hat = gss2016 %&gt;%&#13;
  summarize(prop_2sib = mean(siblings=="2", na.rm=TRUE)) %&gt;%&#13;
  pull()&#13;
&gt;&gt;&gt; p_hat&#13;
0.208246</pre><p class="list-inset">Here, we use the <strong class="source-inline">summarize()</strong> function to calculate the mean of a series of binary values, which corresponds to the proportion of surveys with two siblings. We then use the <strong class="source-inline">pull()</strong> function to obtain the proportion from the <span class="No-Break">resulting DataFrame.</span></p></li>			</ol>&#13;
			<p>We use the sample <a id="_idIndexMarker1015"/>proportion to estimate the population statistic. In other words, we calculate the proportion of families with two siblings based on the available samples to approximate the corresponding proportion if we were to calculate the same based on all the data in the population. Such an estimate comes with a confidence interval that quantifies the list of possible values for the <span class="No-Break">population </span><span class="No-Break"><a id="_idIndexMarker1016"/></span><span class="No-Break">proportion.</span></p>&#13;
			<p>The next section shows how to calculate the confidence interval for the <span class="No-Break">sample proportion.</span></p>&#13;
			<h2 id="_idParaDest-235"><a id="_idTextAnchor241"/>Calculating the confidence interval</h2>&#13;
			<p>The confidence interval is <a id="_idIndexMarker1017"/>an important tool in making inferences about the population parameters based on sample data. A confidence interval provides an estimated range within which a population parameter, such as proportion, is likely to be found with a specified confidence level, such as 95%. When working with sample proportions, calculating confidence intervals allows us to understand the true proportion in the population better and gauge the uncertainty associated with the estimation of the <span class="No-Break">population proportion.</span></p>&#13;
			<p>We can use the<a id="_idIndexMarker1018"/> following steps to calculate the <span class="No-Break">confidence interval:</span></p>&#13;
			<ol>&#13;
				<li>Calculate the sample proportion, <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable"> </span> (pronounced as p-hat). This is the value we calculated based on the sample data in 2016. In other contexts, <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable"> </span> is calculated by dividing the number of successes (for the attribute of interest) by the total <span class="No-Break">sample size.</span></li>&#13;
				<li>Determine the desired level of confidence, commonly denoted as <span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">α</span><span class="_-----MathTools-_Math_Base">)</span> x 100%, where <span class="_-----MathTools-_Math_Variable">α</span> represents the level of significance. In other words, it is the probability of rejecting the null hypothesis when it is true. The most frequently used confidence levels are 90%, 95%, <span class="No-Break">and 99%.</span></li>&#13;
				<li>Calculate the standard error of the sample proportion, which is given by the <span class="No-Break">following formula:</span></li>&#13;
			</ol>&#13;
			<p><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Variable">E</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">√</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span></p>&#13;
			<p class="list-inset">Here, the standard error also corresponds to the standard deviation of the sample proportion, which is assumed to follow a Bernoulli distribution with a success probability of <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable"> </span> (recall the introduction of Bernoulli distribution in the previous chapter). Such calculation relies on two assumptions: the observations in the samples are independent and there are sufficient observations in the sample. A common rule of thumb for checking the second assumption is to ensure both <span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">&gt;</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">10</span> and <span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">&gt;</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number">10</span></span><span class="No-Break">.</span></p>&#13;
			<p class="list-inset">Alternatively, instead of assuming a Bernoulli distribution, we can use the bootstrap<a id="_idIndexMarker1019"/> procedure to estimate the standard error without any distributional assumption. Bootstrap is a non-parametric method that involves resampling the data with replacement to create new samples, calculating the statistic of interest (in this case, the proportion) for each resampled dataset, and estimating the standard error from the variability of the calculated statistics across the <span class="No-Break">resampled datasets.</span></p>&#13;
			<p>4.	Find the critical value (z-score) corresponding to the preset confidence level. This can be done using the <strong class="source-inline">qnorm()</strong> function, which gives us the quantiles of the standard <span class="No-Break">normal distribution.</span></p>&#13;
			<p>5.	Compute the <strong class="bold">margin of error</strong> (<strong class="bold">ME</strong>) as<a id="_idIndexMarker1020"/> the product of the standard error and the <span class="No-Break">critical value:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">M</span><span class="_-----MathTools-_Math_Variable">E</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Variable">E</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol">*</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">z</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Operator">_</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal">s</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal">c</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal">o</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal">r</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal">e</span></span></p>&#13;
			<p>6.	Calculate the confidence interval by adding and subtracting the ME from the sample proportion, giving <span class="No-Break">the following:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">w</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">l</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">M</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">E</span></span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">U</span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">l</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">M</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">E</span></span></p>&#13;
			<p>The confidence<a id="_idIndexMarker1021"/> interval provides a list of possible values for the population proportion according to the specific confidence level. See <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.3</em> for a summary of the <span class="No-Break">calculation process.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer202" class="IMG---Figure">&#13;
					<img src="Images/B18680_11_003.jpg" alt="Figure 11.3 – Summarizing the process of calculating the confidence interval based on sample proportion" width="959" height="398"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3 – Summarizing the process of calculating the confidence interval based on sample proportion</p>&#13;
			<p>Let us stay with the bootstrap procedure a little longer. Without assuming any specific distribution, the bootstrap procedure is a flexible approach that can provide more accurate estimates of the standard error, especially for small sample sizes or when the data is not well behaved. However, It can be computationally intensive, especially for large datasets or<a id="_idIndexMarker1022"/> when many bootstrap replications <span class="No-Break">are generated.</span></p>&#13;
			<p><span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.4</em> provides a schematic overview of the bootstrap procedure. <span class="No-Break">Let’s review:</span></p>&#13;
			<ol>&#13;
				<li>First, we start with the whole dataset and specify the variable of interest, which is the <strong class="source-inline">siblings</strong> variable in this case. This is achieved via the <span class="No-Break"><strong class="source-inline">specify()</strong></span><span class="No-Break"> function.</span></li>&#13;
				<li>Next, we draw samples from the variable with replacement, where the new sample will be the same size as the original dataset. Such resampling introduces randomness to the <span class="No-Break">resulting dataset.</span></li>&#13;
				<li>We repeat the process many times, leading to a collection of bootstrapped artificial datasets using the <span class="No-Break"><strong class="source-inline">generate()</strong></span><span class="No-Break"> function.</span></li>&#13;
				<li>For each replicated dataset, we will calculate the sample statistic of interest, which is the proportion of observations with two siblings in this case. This is done via the <span class="No-Break"><strong class="source-inline">calculate()</strong></span><span class="No-Break"> function.</span></li>&#13;
				<li>These <a id="_idIndexMarker1023"/>sample statistics derived using repeated sampling of the original dataset will then form a distribution, called the bootstrapped distribution (plotted via <strong class="source-inline">ggplot()</strong>), whose standard deviation (extracted via the <strong class="source-inline">summarize()</strong> function) will be a good approximation of the <span class="No-Break">standard error.</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer203" class="IMG---Figure">&#13;
					<img src="Images/B18680_11_004.jpg" alt="Figure 11.4 – The schematic overview of obtaining the standard error using the bootstrap procedure" width="848" height="545"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.4 – The schematic overview of obtaining the standard error using the bootstrap procedure</p>&#13;
			<p>The bootstrapped samples convey different levels of uncertainty in the sample statistic and jointly form a density distribution of multiple artificial sample statistics. The standard deviation of the bootstrapped distribution then gives the standard error of the sample statistic. Note that functions such as <strong class="source-inline">specify()</strong>, <strong class="source-inline">generate()</strong>, and <strong class="source-inline">calculate()</strong> all come from the <strong class="source-inline">infer</strong> package in R. Remember to install this package before continuing with the <span class="No-Break">following code.</span></p>&#13;
			<p>Let us go through the following exercise to understand the bootstrap procedure for calculating the <span class="No-Break">confidence interval.</span></p>&#13;
			<h3>Exercise 11.2 – calculating the confidence interval via bootstrap</h3>&#13;
			<p>In this exercise, we will explore<a id="_idIndexMarker1024"/> calculating the confidence interval<a id="_idIndexMarker1025"/> of the sample proportion. The confidence interval includes the list of estimates within which the true population proportion may assume, given the observed samples. It is a way to quantify the uncertainty in estimating the population proportion based on the actual observations. Besides a step-by-step walk-through of the calculation process using bootstrap, we will also compare the result with the alternative approach using the assumed <span class="No-Break">Bernoulli distribution:</span></p>&#13;
			<ol>&#13;
				<li>Build a set of bootstrapped sample statistics using the specify-generate-calculate procedure from the <strong class="source-inline">infer</strong> package described earlier. Remember to build a binary variable to indicate the binary condition of having an observation with <span class="No-Break">two siblings:</span><pre class="source-code">&#13;
library(infer)&#13;
gss2016 = gss2016 %&gt;%&#13;
  mutate(siblings_two_ind = if_else(siblings=="2","Y","N")) %&gt;%&#13;
  filter(!is.na(siblings_two_ind))&#13;
bs = gss2016 %&gt;%&#13;
  specify(response = siblings_two_ind,&#13;
          success = "Y") %&gt;%&#13;
  generate(reps = 500,&#13;
           type = "bootstrap") %&gt;%&#13;
  calculate(stat = "prop")</pre><p class="list-inset">Here, we first create a binary indicator variable using the <strong class="source-inline">if_else()</strong> function to denote whether the family in the current survey has two siblings. We also remove rows with <strong class="source-inline">NA</strong> values in this column. Next, we use the <strong class="source-inline">specify()</strong> function to indicate the <strong class="source-inline">siblings_two_ind</strong> variable of interest and the level that corresponds to a success. We then use the <strong class="source-inline">generate()</strong> function to generate <strong class="source-inline">500</strong> bootstrapped samples, and use the <strong class="source-inline">calculate()</strong> function to obtain the corresponding sample statistic (proportion of success) in each bootstrapped sample by setting <strong class="source-inline">stat = "</strong><span class="No-Break"><strong class="source-inline">prop"</strong></span><span class="No-Break">.</span></p><p class="list-inset">Let us observe<a id="_idIndexMarker1026"/> the contents in the <a id="_idIndexMarker1027"/>bootstrapped <span class="No-Break">sample statistics:</span></p><pre class="source-code">&gt;&gt;&gt; bs&#13;
Response: siblings_two_ind (factor)&#13;
# A tibble: 500 × 2&#13;
   replicate  stat&#13;
       &lt;int&gt; &lt;dbl&gt;&#13;
 1         1 0.205&#13;
 2         2 0.209&#13;
 3         3 0.218&#13;
 4         4 0.189&#13;
 5         5 0.207&#13;
 6         6 0.205&#13;
 7         7 0.221&#13;
 8         8 0.214&#13;
 9         9 0.212&#13;
10        10 0.212&#13;
# … with 490 more rows&#13;
# i Use `print(n = ...)` to see more rows</pre><p class="list-inset">The result shows that the <strong class="source-inline">bs</strong> object is a <strong class="source-inline">tibble</strong> DataFrame with 500 rows (corresponding to the total number of the bootstrapped sample) and 2 columns. The first column (<strong class="source-inline">replicate</strong>) denotes the number of bootstrapped samples, and the second column (<strong class="source-inline">stat</strong>) indicates the proportion of success (that is, the number of rows with <strong class="source-inline">siblings_two_ind==2</strong> divided by the total number of rows) in the <span class="No-Break">bootstrapped sample.</span></p></li>				<li>Plot the<a id="_idIndexMarker1028"/> bootstrapped sample<a id="_idIndexMarker1029"/> statistics in a <span class="No-Break">density plot:</span><pre class="source-code">&#13;
&gt;&gt;&gt; ggplot(bs, aes(x = stat)) +&#13;
  geom_density() +&#13;
  labs(title = "Density plot of the sample proportions", x = "Sample proportion", y = "Density") +&#13;
  theme(text = element_text(size = 16))</pre></li>			</ol>&#13;
			<p>Running the code generates the plot in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.5</em>. The spread of this distribution, which relates to the standard deviation, directly determines the magnitude of the standard error. Also, if we were to increase the number of bootstrapped samples, we would expect a smoother <span class="No-Break">density curve.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer204" class="IMG---Figure">&#13;
					<img src="Images/B18680_11_005.jpg" alt="Figure 11.5 – Visualizing the density plot of all bootstrapped sample proportions" width="1042" height="868"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.5 – Visualizing the density plot of all bootstrapped sample proportions</p>&#13;
			<ol>&#13;
				<li>Calculate the<a id="_idIndexMarker1030"/> standard error as the<a id="_idIndexMarker1031"/> standard deviation of the empirical distribution based on the bootstrapped <span class="No-Break">sample proportions:</span><pre class="source-code">&#13;
SE = bs %&gt;%&#13;
  summarise(sd(stat)) %&gt;%&#13;
  pull()&#13;
&gt;&gt;&gt; SE&#13;
0.007181953</pre><p class="list-inset">Here, we use the <strong class="source-inline">sd()</strong> function to calculate the standard deviation of the <strong class="source-inline">stat</strong> column in <strong class="source-inline">bs</strong>, and then return the value via the <strong class="source-inline">pull()</strong> function. The standard error will then be scaled by the predetermined z-score and subtracted from and added to the original sample proportion to obtain the <span class="No-Break">confidence interval.</span></p></li>				<li>Calculate the confidence interval of the original sample proportion with a 95% <span class="No-Break">confidence interval:</span><pre class="source-code">&#13;
&gt;&gt;&gt; c(p_hat - 2*SE, p_hat + 2*SE)&#13;
0.1938821 0.2226099</pre><p class="list-inset">Here, since a 95% confidence level corresponds to a z-score of 2, we multiply it with the standard error before subtracting from and adding to the original sample proportion (<strong class="source-inline">p_hat</strong>) to obtain the <span class="No-Break">confidence interval.</span></p></li>				<li>Calculate the <a id="_idIndexMarker1032"/>confidence interval using <a id="_idIndexMarker1033"/>the structure information by assuming a Bernoulli distribution for the probability <span class="No-Break">of success:</span><pre class="source-code">&#13;
SE2 = sqrt(p_hat*(1-p_hat)/nrow(gss2016))&#13;
&gt;&gt;&gt; c(p_hat - 2*SE2, p_hat + 2*SE2)&#13;
0.193079 0.223413</pre><p class="list-inset">Here, we use the explicit form of the variance of the Bernoulli distribution to calculate the standard error. The result shows a fairly similar confidence interval compared with the one obtained using the <span class="No-Break">bootstrap approach.</span></p></li>			</ol>&#13;
			<p>The confidence interval provides a measure of uncertainty for our estimate of the unknown population proportion using the observed sample proportion. Let us look at how to interpret the confidence interval in the <span class="No-Break">next section.</span></p>&#13;
			<h2 id="_idParaDest-236"><a id="_idTextAnchor242"/>Interpreting the confidence interval of the sample proportion</h2>&#13;
			<p>Interpreting the <a id="_idIndexMarker1034"/>confidence interval of the sample <a id="_idIndexMarker1035"/>proportion involves understanding the meaning of the interval and the associated confidence level. In our previous example, the bootstrap approach reports a confidence interval of <strong class="source-inline">[0.1938821, 0.2226099]</strong>. There are two levels of interpretation for this <span class="No-Break">confidence interval.</span></p>&#13;
			<p>First, the range of the confidence interval suggests that the true population proportion of families with two siblings is likely to fall between 19.39% and 22.26%. This range is based on the sample data and estimates the uncertainty in the <span class="No-Break">true proportion.</span></p>&#13;
			<p>Second, the 95% confidence interval means that if we were to conduct the survey many times (either in 2016 or other years), we would generate different random samples of the same size, based on which we can calculate the 95% confidence interval for each sample. Among these artificial samples, we will obtain a collection of intervals, and approximately 95% of them would include the true population proportion within <span class="No-Break">the interval.</span></p>&#13;
			<p>Note that<a id="_idIndexMarker1036"/> the confidence interval is <a id="_idIndexMarker1037"/>still an estimate, and the true population proportion may fall outside the calculated interval. However, the confidence interval provides a useful way to quantify the uncertainty in the estimate and gives a list of plausible values for the true population proportion based on the <span class="No-Break">observed samples.</span></p>&#13;
			<p>The next section introduces hypothesis testing for the <span class="No-Break">sample proportion.</span></p>&#13;
			<h2 id="_idParaDest-237"><a id="_idTextAnchor243"/>Hypothesis testing for the sample proportion</h2>&#13;
			<p>Hypothesis testing for the <a id="_idIndexMarker1038"/>sample proportion is very much related to the confidence interval introduced in a previous section, which captures the level of uncertainty in the estimate for the unknown proportion based on the population data. Naturally, a sample with fewer observations leads to a wide confidence interval. Hypothesis testing for the sample proportion aims to determine whether there is enough evidence in a sample to support or reject a claim about the population proportion. The process starts with a null hypothesis (H0), which represents the baseline assumption about the population proportion. Correspondingly, there is an alternative hypothesis (H1) that represents the claim or statement we are testing against the null hypothesis. Hypothesis testing then compares the observed sample proportion to a specified null hypothesis in order to assess whether we have enough evidence to reject the null hypothesis in favor of the <span class="No-Break">alternative hypothesis.</span></p>&#13;
			<p>Let us go through an overview of the procedure involved in carrying out <span class="No-Break">hypothesis testing:</span></p>&#13;
			<ol>&#13;
				<li><strong class="bold">Formulate the hypothesis</strong>. In this step, we set up the null hypothesis (H0) and alternative hypothesis (H1). The null hypothesis often says there is no effect, and the situation remains the status quo, as indicated by an equality sign in H0. On the other hand, the alternative hypothesis states that there is an effect or difference, as indicated by an inequality sign in H1. For example, we can set the following hypotheses for H0 <span class="No-Break">and H1:</span><ul><li>H0: <span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span> (the population proportion is equal to a specified value, <span class="No-Break"><span class="_-----MathTools-_Math_Variable">p</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">0</span></span><span class="No-Break">)</span></li><li>H1: <span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">≠</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span> (the population proportion is not equal to <span class="No-Break"><span class="_-----MathTools-_Math_Variable">p</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">0</span></span><span class="No-Break">)</span></li></ul></li>&#13;
				<li><strong class="bold">Choose a significance level (</strong><span class="_-----MathTools-_Math_Symbol_Extended">𝜶</span><strong class="bold">)</strong>. The significance level is a probability threshold we use to reject the null hypothesis when it is true. Widely used significance levels include 0.05 (5%) and <span class="No-Break">0.01 (1%).</span></li>&#13;
				<li><strong class="bold">Calculate the test statistic</strong>. Now that we observe a sample proportion based on the actual data, we can calculate the probability of observing such a sample proportion <em class="italic">if</em> the null hypothesis were true. This starts with calculating the test statistic (z-score) for the<a id="_idIndexMarker1039"/> sample proportion using the <span class="No-Break">following formula:</span></li>&#13;
			</ol>&#13;
			<p><span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">√</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base">__________</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">/</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span></p>&#13;
			<p class="list-inset">where <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable"> </span> is the sample proportion, <span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span> is the population proportion assuming the null hypothesis, and <span class="_-----MathTools-_Math_Variable">n</span> is the sample size. There are two things to note there. First, the denominator resembles the standard deviation based on the sample proportion covered earlier. Indeed, we are assuming a Bernoulli distribution with a success probability of <span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span>. With a total of <span class="_-----MathTools-_Math_Variable">n</span> observations, the standard deviation for the sample proportion variable is <span class="_-----MathTools-_Math_Base">√</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base">__________</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">/</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span>. Second, the whole term corresponds to the process of converting a number into a z-score of a specific distribution, a topic covered in the previous chapter. Here, we assume a normal distribution with mean <span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span> and standard deviation <span class="_-----MathTools-_Math_Base">√</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base">__________</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">/</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span>. We can then convert the observed sample proportion <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable"> </span> to the corresponding z-score for ease of calculation <span class="No-Break">later on.</span></p>&#13;
			<p class="list-inset">Note that we can also use the bootstrap approach to calculate the empirical p-value under the <span class="No-Break">null hypothesis.</span></p>&#13;
			<p>4.	<strong class="bold">Determine the p-value</strong>. The z-score is a measure that falls on a standard Gaussian distribution. It is a test statistic, and we are often interested in the probability of observing the test statistic at this or an even more extreme value. This is called the p-value, denoted as <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable"> </span>, when we assume the null hypothesis is true. In other words, we try to assess how likely it is to observe some phenomenon, assuming the null hypothesis is true. If the probability of observing <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable"> </span> or an even more extreme number is very small, we have confidence that the null hypothesis is false, and we can reject H0 in favor <span class="No-Break">of H1.</span></p>&#13;
			<p class="list-inset">Note that for a two-tailed test, we can also calculate the p-value using the standard normal distribution and doubling the <span class="No-Break">single-side probability:</span></p>&#13;
			<p class="list-inset">p-value = <span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">Z</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">&gt;</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">|</span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">z</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">|</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span></p>&#13;
			<p>5.	<strong class="bold">Make a decision</strong>. Finally, we <a id="_idIndexMarker1040"/>compare the p-value to the preset significance level (<span class="_-----MathTools-_Math_Variable">α</span>) and use the following rule to make <span class="No-Break">a decision.</span></p>&#13;
			<p class="list-inset">If the p-value ≤ <span class="_-----MathTools-_Math_Variable">α</span>, reject the null hypothesis in favor of the alternative hypothesis. Doing so suggests that there is enough evidence to suggest that the population proportion differs from the hypothesized proportion <span class="No-Break"><span class="_-----MathTools-_Math_Variable">p</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">0</span></span><span class="No-Break">.</span></p>&#13;
			<p class="list-inset">If the p-value &gt; <span class="_-----MathTools-_Math_Variable">α</span>, fail to reject the null hypothesis. This means that there is not enough evidence to suggest that the population proportion is different from <span class="No-Break"><span class="_-----MathTools-_Math_Variable">p</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">0</span></span><span class="No-Break">.</span></p>&#13;
			<p>Conducting the hypothesis testing follows a similar process. The only difference is the use of the <strong class="source-inline">hypothesise()</strong> function (placed after <strong class="source-inline">specify()</strong>), which serves as a null hypothesis. We then perform the same bootstrap procedure to obtain a density plot of the bootstrapped sample proportions, followed by calculating the total probability of obtaining a proportion at least as extreme as the one indicated in the <span class="No-Break">null hypothesis.</span></p>&#13;
			<p>Let us go through an exercise to review the process of performing hypothesis testing for the <span class="No-Break">sample proportion.</span></p>&#13;
			<h3>Exercise 11.3 – performing hypothesis testing for the sample proportion</h3>&#13;
			<p>In<a id="_idIndexMarker1041"/> this exercise, we will set up a hypothetical<a id="_idIndexMarker1042"/> population proportion in a null hypothesis and test the validity of this hypothesis based on the observed <span class="No-Break">sample proportion:</span></p>&#13;
			<ol>&#13;
				<li>Plot the frequency count of families with and without two siblings in 2016 in a <span class="No-Break">bar plot:</span><pre class="source-code">&#13;
gss2016 %&gt;%&#13;
  ggplot(aes(x = siblings_two_ind)) +&#13;
  geom_bar() +&#13;
  labs(title = "Frequency count of families with two siblings", x = "Have two siblings", y = "Count") +&#13;
  theme(text = element_text(size = 16))</pre><p class="list-inset">Running the code generates the plot in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.6</em>, which shows that families with two siblings account for around ¼ of <span class="No-Break">all families.</span></p></li>			</ol>&#13;
			<div>&#13;
				<div id="_idContainer205" class="IMG---Figure">&#13;
					<img src="Images/B18680_11_006.jpg" alt="Figure 11.6 – Visualizing the frequency count of families with two siblings" width="1058" height="926"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.6 – Visualizing the frequency count of families with two siblings</p>&#13;
			<ol>&#13;
				<li value="2">Calculate <a id="_idIndexMarker1043"/>the sample <a id="_idIndexMarker1044"/>proportion of families with <span class="No-Break">two siblings:</span><pre class="source-code">&#13;
p_hat = gss2016 %&gt;%&#13;
  summarize(mean(siblings_two_ind=="Y")) %&gt;%&#13;
  pull()&#13;
&gt;&gt;&gt; p_hat&#13;
0.208246</pre><p class="list-inset">Here, we first build a series of binary outcomes using <strong class="source-inline">siblings_two_ind=="Y"</strong>. Taking the average of this column gives the ratio of <strong class="source-inline">TRUE</strong> values, which gets executed in a <strong class="source-inline">summarize()</strong> context. We then extract the value of the sample proportion <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">pull()</strong></span><span class="No-Break">.</span></p></li>				<li>Use the<a id="_idIndexMarker1045"/> specify-hypothesise-generate-calculate<a id="_idIndexMarker1046"/> procedure to generate a collection of bootstrapped sample proportions under the null hypothesis, which specifies a population proportion <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">0.19</strong></span><span class="No-Break">:</span><pre class="source-code">&#13;
null = gss2016 %&gt;%&#13;
  specify(response = siblings_two_ind,&#13;
          success = "Y") %&gt;%&#13;
  hypothesise(null = "point",&#13;
              p = 0.19) %&gt;%&#13;
  generate(reps = 500,&#13;
           type = "draw") %&gt;%&#13;
  calculate(stat = "prop")&#13;
&gt;&gt;&gt; null&#13;
Response: siblings_two_ind (factor)&#13;
Null Hypothesis: point&#13;
# A tibble: 500 × 2&#13;
   replicate  stat&#13;
   &lt;fct&gt;     &lt;dbl&gt;&#13;
 1 1         0.179&#13;
 2 2         0.193&#13;
 3 3         0.176&#13;
 4 4         0.181&#13;
 5 5         0.181&#13;
 6 6         0.198&#13;
 7 7         0.191&#13;
 8 8         0.189&#13;
 9 9         0.194&#13;
10 10        0.189&#13;
# … with 490 more rows&#13;
# i Use `print(n = ...)` to see more rows</pre></li>				<li>Generate <a id="_idIndexMarker1047"/>the density plot<a id="_idIndexMarker1048"/> of the bootstrapped sample proportions along with the proportion suggested by the null hypothesis via a <span class="No-Break">vertical line:</span><pre class="source-code">&#13;
ggplot(null, aes(x = stat)) +&#13;
  geom_density() +&#13;
  geom_vline(xintercept = p_hat,&#13;
             color = "red") +&#13;
  labs(title = "Density plot using bootstrap", x = "Sample proportion", y = "Density") +&#13;
  theme(text = element_text(size = 16))</pre><p class="list-inset">Running the code generates the plot in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.7</em>. The probability of observing a value at least as extreme as the one indicated by the red line (according to the null hypothesis) is thus the total area under the density curve toward the right of the red line. We then double the result to account for the <span class="No-Break">opposite direction.</span></p></li>			</ol>&#13;
			<div>&#13;
				<div id="_idContainer206" class="IMG---Figure">&#13;
					<img src="Images/B18680_11_007.jpg" alt="Figure 11.7 – Visualizing the density plot of the bootstrapped sample proportions for hypothesis testing" width="1132" height="980"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.7 – Visualizing the density plot of the bootstrapped sample proportions for hypothesis testing</p>&#13;
			<ol>&#13;
				<li value="5">Calculate <a id="_idIndexMarker1049"/><span class="No-Break">the</span><span class="No-Break"><a id="_idIndexMarker1050"/></span><span class="No-Break"> p-value:</span><pre class="source-code">&#13;
&gt;&gt;&gt; null %&gt;%&#13;
  summarise(mean(stat &gt; p_hat)) %&gt;%&#13;
  pull()* 2&#13;
0.02</pre><p class="list-inset">Since this <a id="_idIndexMarker1051"/>result is less than the<a id="_idIndexMarker1052"/> preset significance level of 5%, we have sufficient evidence to favor the alternative hypothesis and reject the null hypothesis. In other words, the assumed 19% is statistically different from the true population proportion with a confidence level of up to 95%. We can therefore draw the conclusion that the true population proportion is <span class="No-Break">not 19%.</span></p></li>			</ol>&#13;
			<p>The next section looks at the inference for the difference in sample proportions between two <span class="No-Break">categorical variables.</span></p>&#13;
			<h2 id="_idParaDest-238"><a id="_idTextAnchor244"/>Inference for the difference in sample proportions</h2>&#13;
			<p>The setting <a id="_idIndexMarker1053"/>now is that we have two categorical variables. Take gender and degree, for example. The data will report a proportion of degree holders for both females and males. A natural question to ask is whether males are more likely to get a degree than females. A particular dataset will report a snapshot of these proportions, which may or may not suggest a higher percentage of degree holders are males. The tools from hypothesis testing could then come in to answer the following question: if males are a higher proportion of degree holders in the dataset, is such difference statistically significant? In other words, are males more likely to get a degree than females, or vice versa? This section attempts to answer this type <span class="No-Break">of question.</span></p>&#13;
			<p>Inference for the difference in sample proportions between two categorical variables (for example, gender and degree) involves comparing the proportions of samples for each level in two different populations. This type of analysis is commonly used in experiments or observational studies to determine the existence of a significant difference in proportions between two groups. The main goal is to estimate the difference between the population proportions and determine whether this difference is <span class="No-Break">statistically significant.</span></p>&#13;
			<p>The procedure for hypothesis testing is similar to before. We first formulate the null hypothesis, which assumes no difference between the proportion of the two populations, that is, <span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span>, or <span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span>. The alternative hypothesis then states that their difference is not zero; that is, <span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">≠</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span>. Next, we choose a specific significance level and calculate the sample statistic (difference in sample proportion, including the pooled proportion between the two categorical variables) and the test statistic (via either a closed-form expression based on the assumed distribution or using the bootstrap method). Finally, we obtain the p-value and decide whether the observed result under the null<a id="_idIndexMarker1054"/> hypothesis possesses statistical significance <span class="No-Break">or not.</span></p>&#13;
			<p>Let us go through a concrete exercise following our <span class="No-Break">previous example.</span></p>&#13;
			<h3>Exercise 11.4 – performing hypothesis testing for the difference in sample proportions</h3>&#13;
			<p>In this<a id="_idIndexMarker1055"/> exercise, we focus on <a id="_idIndexMarker1056"/>how to conduct hypothesis testing for the difference in the sample proportion between gender and status of higher degree. Here, we define a higher degree as a bachelor’s and above. The proportion of higher-degree holders will likely differ between the male and female groups, and we will test whether such a difference is significant given the <span class="No-Break">observed data:</span></p>&#13;
			<ol>&#13;
				<li>Add a binary column called <strong class="source-inline">higher_degree</strong> to the previous DataFrame, <strong class="source-inline">gss2016</strong>, to indicate the status of higher degree, including bachelor’s <span class="No-Break">and above:</span><pre class="source-code">&#13;
gss2016 = gss2016 %&gt;%&#13;
  mutate(higher_degree = if_else(degree %in% c("Bachelor","Graduate"), "Y", "N"))</pre></li>				<li>Print the ratio between the two levels for <strong class="source-inline">gender</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">higher_degree</strong></span><span class="No-Break">:</span><pre class="source-code">&#13;
&gt;&gt;&gt; table(gss2016$higher_degree)&#13;
   N    Y&#13;
2008  854&#13;
&gt;&gt;&gt; table(gss2016$sex)&#13;
  Male Female&#13;
  1274   1588</pre></li>				<li>Plot these counts in a <span class="No-Break">bar chart:</span><pre class="source-code">&#13;
ggplot(gss2016, aes(x = sex, fill=higher_degree)) +&#13;
  geom_bar() +&#13;
  labs(title = "Frequency count for gender and degree", x = "Gender", y = "Count") +&#13;
  theme(text = element_text(size = 16))</pre><p class="list-inset">Running the code generates the chart in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.8</em></span><span class="No-Break">.</span></p></li>			</ol>&#13;
			<div>&#13;
				<div id="_idContainer207" class="IMG---Figure">&#13;
					<img src="Images/B18680_11_008.jpg" alt="Figure 11.8 – Visualizing the frequency count of gender and higher-degree status" width="1068" height="751"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.8 – Visualizing the frequency count of gender and higher-degree status</p>&#13;
			<p class="list-inset">We can also <a id="_idIndexMarker1057"/>plot them in <a id="_idIndexMarker1058"/>percentages by specifying <strong class="source-inline">position = "fill"</strong> in the <span class="No-Break"><strong class="source-inline">geom_bar()</strong></span><span class="No-Break"> function:</span></p>&#13;
			<pre class="source-code">&#13;
ggplot(gss2016, aes(x = sex, fill=higher_degree)) +&#13;
  geom_bar(position = "fill") +&#13;
  labs(title = "Sample proportions for gender and degree", x = "Gender", y = "Ratio") +&#13;
  theme(text = element_text(size = 16))</pre>			<p class="list-inset">Running the code generates the chart in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.9</em>, which suggests no obvious difference in the proportion of higher-degree holders between the male and <span class="No-Break">female groups.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer208" class="IMG---Figure">&#13;
					<img src="Images/B18680_11_009.jpg" alt="Figure 11.9 – Visualizing the frequency count of gender and higher-degree status" width="1118" height="788"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.9 – Visualizing the frequency count of gender and higher-degree status</p>&#13;
			<ol>&#13;
				<li value="4">Calculate<a id="_idIndexMarker1059"/> the difference<a id="_idIndexMarker1060"/> in sample proportions of higher-degree holders between males <span class="No-Break">and females:</span><pre class="source-code">&#13;
p_hats = gss2016 %&gt;%&#13;
  group_by(sex) %&gt;%&#13;
  summarise(mean(higher_degree=="Y", na.rm=TRUE)) %&gt;%&#13;
  pull()&#13;
d_hat = diff(p_hats)&#13;
&gt;&gt;&gt; d_hat&#13;
0.007288771</pre><p class="list-inset">The result also shows that the difference is quite small, with the female group being 0.7% higher than the male group (refer to the slightly higher blue bar of the female group in the previous figure). Let us see whether such a difference is <span class="No-Break">statistically significant.</span></p></li>				<li>Generate <a id="_idIndexMarker1061"/>one bootstrap <a id="_idIndexMarker1062"/>sample set under the null hypothesis, which states that there is no difference in the ratio of higher-degree holders between the male and <span class="No-Break">female groups:</span><pre class="source-code">&#13;
gss2016 %&gt;%&#13;
  specify(&#13;
    response = higher_degree,&#13;
    explanatory = sex,&#13;
    success = "Y"&#13;
  ) %&gt;%&#13;
  hypothesise(null = "independence") %&gt;%&#13;
  generate(reps = 1, type = "permute")&#13;
Response: higher_degree (factor)&#13;
Explanatory: sex (factor)&#13;
Null Hypothesis: independence&#13;
# A tibble: 2,862 × 3&#13;
# Groups:   replicate [1]&#13;
   higher_degree sex    replicate&#13;
   &lt;fct&gt;         &lt;fct&gt;      &lt;int&gt;&#13;
 1 N             Male           1&#13;
 2 N             Male           1&#13;
 3 Y             Male           1&#13;
 4 N             Female         1&#13;
 5 N             Female         1&#13;
 6 N             Female         1&#13;
 7 Y             Male           1&#13;
 8 N             Female         1&#13;
 9 N             Male           1&#13;
10 N             Male           1&#13;
# … with 2,852 more rows&#13;
# i Use `print(n = ...)` to see more rows</pre><p class="list-inset">Here, we use <strong class="source-inline">higher_degree</strong> as the response variable and <strong class="source-inline">sex</strong> as the explanatory variable in a logistic regression setting (to be introduced in <a href="B18680_13.xhtml#_idTextAnchor279"><span class="No-Break"><em class="italic">Chapter 13</em></span></a>). Under the null hypothesis, we randomly sample from the original dataset and create a new artificial dataset of the <span class="No-Break">same shape.</span></p></li>				<li>Repeat the<a id="_idIndexMarker1063"/> same bootstrap <a id="_idIndexMarker1064"/>sampling procedures 500 times and calculate the difference in sample proportions of higher-degree holders between female and male groups (note the sequence here) for each set of <span class="No-Break">bootstrapped samples:</span><pre class="source-code">&#13;
null = gss2016 %&gt;%&#13;
  specify(&#13;
    higher_degree ~ sex,&#13;
    success = "Y"&#13;
  ) %&gt;%&#13;
  hypothesise(null = "independence") %&gt;%&#13;
  generate(reps = 500, type = "permute") %&gt;%&#13;
  calculate(stat = "diff in props", order = c("Female", "Male"))&#13;
&gt;&gt;&gt; null&#13;
Response: higher_degree (factor)&#13;
Explanatory: sex (factor)&#13;
Null Hypothesis: independence&#13;
# A tibble: 500 × 2&#13;
   replicate     stat&#13;
       &lt;int&gt;    &lt;dbl&gt;&#13;
 1         1  0.00870&#13;
 2         2  0.00587&#13;
 3         3 -0.00120&#13;
 4         4  0.0228&#13;
 5         5  0.00446&#13;
 6         6 -0.00827&#13;
 7         7 -0.0366&#13;
 8         8  0.0129&#13;
 9         9  0.0172&#13;
10        10 -0.00261&#13;
# … with 490 more rows&#13;
# i Use `print(n = ...)` to see more rows</pre></li>				<li>Plot these <a id="_idIndexMarker1065"/>bootstrapped <a id="_idIndexMarker1066"/>sample statistics in a density curve and plot the observed difference as a vertical <span class="No-Break">red line:</span><pre class="source-code">&#13;
ggplot(null, aes(x = stat)) +&#13;
  geom_density() +&#13;
  geom_vline(xintercept = d_hat, color = "red") +&#13;
  labs(x = "Difference in sample proportion (female - male)", y = "Count") +&#13;
  theme(text = element_text(size = 16))</pre><p class="list-inset">Running the code generates the plot in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.10</em>, which shows that the red line is not located toward the extreme side of the empirical distribution. This suggests that the p-value, which will be calculated next, may <span class="No-Break">be high.</span></p></li>			</ol>&#13;
			<div>&#13;
				<div id="_idContainer209" class="IMG---Figure">&#13;
					<img src="Images/B18680_11_010.jpg" alt="Figure 11.10 – Showing the density plot for the bootstrapped sample statistics and observed differences" width="1108" height="813"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.10 – Showing the density plot for the bootstrapped sample statistics and observed differences</p>&#13;
			<ol>&#13;
				<li value="8">Compute <a id="_idIndexMarker1067"/>the <span class="No-Break">two-tailed </span><span class="No-Break"><a id="_idIndexMarker1068"/></span><span class="No-Break">p-value:</span><pre class="source-code">&#13;
null %&gt;%&#13;
  summarize(pval = 2 * mean(stat &gt; d_hat)) %&gt;%&#13;
  pull()&#13;
0.608</pre><p class="list-inset">The result<a id="_idIndexMarker1069"/> shows a pretty high p-value, which suggests that we lack sufficient evidence to<a id="_idIndexMarker1070"/> reject the null hypothesis. In other words, there is not enough information to suggest that the proportion of higher-degree holders between males and females <span class="No-Break">is different.</span></p></li>			</ol>&#13;
			<p>The hypothesis testing relies on a predefined significance level. That significance level, denoted as <span class="_-----MathTools-_Math_Variable">α</span>, has something to do with the statistical error of the procedure. The next section introduces two common types of statistical error when performing <span class="No-Break">hypothesis testing.</span></p>&#13;
			<h2 id="_idParaDest-239"><a id="_idTextAnchor245"/>Type I and Type II errors</h2>&#13;
			<p>There are two types of errors when conducting hypothesis testing and making a decision about the null hypothesis (H0) and the alternative hypothesis (H1). They are called Type I and Type <span class="No-Break">II errors.</span></p>&#13;
			<p>The Type I error<a id="_idIndexMarker1071"/> refers to false positives. It happens when the null hypothesis is true but mistakenly rejected. In other words, we find evidence in our sample data that suggests a significant effect or difference exists and we favor the alternative hypothesis, even though it does not actually exist in the population. We denote the probability of experiencing a Type I error as <span class="_-----MathTools-_Math_Variable">α</span>. It is also called the significance level, which was set to <strong class="source-inline">0.05</strong> in the previous example. A 5% significance level means that there is a 5% chance of rejecting the null hypothesis when it is true. The significance level thus represents the probability of committing a false <span class="No-Break">positive error.</span></p>&#13;
			<p>The Type II error<a id="_idIndexMarker1072"/> focuses on the false negative case. It occurs when we fail to reject a false null hypothesis. In other words, we do not find evidence in our sample data to reject the null hypothesis, even though it does exist in the population. The probability of making a Type II error is denoted by <span class="_-----MathTools-_Math_Variable">β</span>, which is also referred to as the power of the test. The complement of the power, denoted as <span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span>, represents the probability of rejecting the null hypothesis when it <span class="No-Break">is false.</span></p>&#13;
			<p>Type I errors involve falsely rejecting the null hypothesis, while Type II errors involve failing to reject the null hypothesis when false. Both types of errors are important considerations in hypothesis testing because they can lead to incorrect conclusions. To minimize the risk of these errors, we can make a careful choice regarding the significance level (<span class="_-----MathTools-_Math_Variable">α</span>) and also ensure that their study has sufficient power (<span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span>). The power of a test depends on the sample size, the effect size (which is a quantitative measure of the magnitude of an empirical relationship between variables), and the chosen significance level. Larger sample sizes and larger effect sizes both increase the power of a test, reducing the likelihood of Type <span class="No-Break">II errors.</span></p>&#13;
			<p><span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.11</em> provides an overview of the different types of outcomes in a hypothesis test. Note that the false positive and false negative are related to the quality of the decision. Depending on the type of a false decision, we would classify the errors as either Type I <a id="_idIndexMarker1073"/>or Type <span class="No-Break">II</span><span class="No-Break"><a id="_idIndexMarker1074"/></span><span class="No-Break"> errors.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer210" class="IMG---Figure">&#13;
					<img src="Images/B18680_11_011.jpg" alt="Figure 11.11 – Overview of different types of outcomes in a hypothesis test" width="1067" height="366"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.11 – Overview of different types of outcomes in a hypothesis test</p>&#13;
			<p>The next section introduces the chi-square test, which tests the independence of two <span class="No-Break">categorical variables.</span></p>&#13;
			<h2 id="_idParaDest-240"><a id="_idTextAnchor246"/>Testing the independence of two categorical variables</h2>&#13;
			<p>To check the independence of<a id="_idIndexMarker1075"/> two categorical variables, the process involves checking the existence of a statistically significant relationship between them. One common procedure is the chi-square test for independence. It works by comparing the observed frequencies in a contingency table with the expected frequencies under the assumption <span class="No-Break">of independence.</span></p>&#13;
			<p>Let us first review the contingency table for two <span class="No-Break">categorical variables.</span></p>&#13;
			<h2 id="_idParaDest-241"><a id="_idTextAnchor247"/>Introducing the contingency table</h2>&#13;
			<p>A contingency table, also<a id="_idIndexMarker1076"/> known as a cross-tabulation or crosstab, is a table used to display the frequency distribution of two or more categorical variables. It summarizes the relationships between the variables by showing how their categories intersect or co-occur in the data. It provides a good summary of the relationships between <span class="No-Break">categorical variables.</span></p>&#13;
			<p>Let us stick with the example of the relationship between gender and degree. This time, we will look at all types of degrees, as shown in the <span class="No-Break">following code:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; table(gss2016$degree)&#13;
Lt High School    High School Junior College       Bachelor        Graduate&#13;
           328           1459            215            536             318</pre>			<p>To indicate its relationship with gender, we can plot the degree together with gender in a stacked bar plot <span class="No-Break">as before:</span></p>&#13;
			<pre class="source-code">&#13;
ggplot(gss2016, aes(x = sex, fill=degree)) +&#13;
  geom_bar() +&#13;
  labs(title = "Frequency count for gender and degree", x = "Gender", y = "Count") +&#13;
  theme(text = element_text(size = 16))</pre>			<p>Running the code generates the plot in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.12</em></span><span class="No-Break">.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer211" class="IMG---Figure">&#13;
					<img src="Images/B18680_11_012.jpg" alt="Figure 11.12 – Visualizing the relationship between gender and degree in a bar plot" width="1209" height="751"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.12 – Visualizing the relationship between gender and degree in a bar plot</p>&#13;
			<p>However, the figure provides no<a id="_idIndexMarker1077"/> information on the exact count for each category. To obtain the exact frequency for each category of the two variables, we can use the <span class="No-Break">contingency table:</span></p>&#13;
			<pre class="source-code">&#13;
tab = gss2016 %&gt;%&#13;
  select(sex, degree) %&gt;%&#13;
  table()&#13;
&gt;&gt;&gt; tab&#13;
        degree&#13;
sex      Lt High School High School Junior College Bachelor Graduate&#13;
  Male              147         661             89      243      132&#13;
  Female            181         798            126      293      186</pre>			<p>Here, we used the <strong class="source-inline">table()</strong> function to generate the contingency table after selecting both <strong class="source-inline">sex</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">degree</strong></span><span class="No-Break">.</span></p>&#13;
			<p>The next section introduces the chi-square test to test for the independence between these two <span class="No-Break">categorical variables.</span></p>&#13;
			<h2 id="_idParaDest-242"><a id="_idTextAnchor248"/>Applying the chi-square test for independence between two categorical variables</h2>&#13;
			<p>The chi-square test<a id="_idIndexMarker1078"/> is a statistical test used to decide a possibly significant relationship (dependence) between two categorical variables in a collection of observed samples. It can be used to test for independence or goodness of fit. In this chapter, we focus mainly on the test for independence between two categorical variables. The test compares the observed frequencies with the expected ones in a contingency table, assuming that the variables are independent. If the observed and expected frequencies are significantly different, the test suggests that the variables are not independent; in other words, they are dependent on <span class="No-Break">each other.</span></p>&#13;
			<p>Following the<a id="_idIndexMarker1079"/> same approach as before, we can generate an artificial bootstrapped dataset to obtain a sample statistic, called the chi-square statistic. This dataset is generated by permuting the original dataset under the assumption of independence in the null hypothesis. In the following code, we generate one permutated dataset of the same shape as the original dataset, assuming independence under the <span class="No-Break">null hypothesis:</span></p>&#13;
			<pre class="source-code">&#13;
perm_1 = gss2016 %&gt;%&#13;
  # Specify the variables of interest&#13;
  specify(degree ~ sex) %&gt;%&#13;
  # Set up the null hypothesis&#13;
  hypothesize(null = "independence") %&gt;%&#13;
  # Generate a single permuted dataset&#13;
  generate(reps = 1, type = "permute")&#13;
&gt;&gt;&gt; perm_1&#13;
Response: degree (factor)&#13;
Explanatory: sex (factor)&#13;
Null Hypothesis: independence&#13;
# A tibble: 2,856 × 3&#13;
# Groups:   replicate [1]&#13;
   degree         sex    replicate&#13;
   &lt;fct&gt;          &lt;fct&gt;      &lt;int&gt;&#13;
 1 Junior College Male           1&#13;
 2 Bachelor       Male           1&#13;
 3 High School    Male           1&#13;
 4 High School    Female         1&#13;
 5 High School    Female         1&#13;
 6 High School    Female         1&#13;
 7 Graduate       Male           1&#13;
 8 Bachelor       Female         1&#13;
 9 High School    Male           1&#13;
10 High School    Male           1&#13;
# … with 2,846 more rows&#13;
# i Use `print(n = ...)` to see more rows</pre>			<p>Next, we<a id="_idIndexMarker1080"/> create 500 permutated datasets and extract the corresponding <span class="No-Break">chi-square statistic:</span></p>&#13;
			<pre class="source-code">&#13;
null_spac = gss2016 %&gt;%&#13;
  specify(degree ~ sex) %&gt;%&#13;
  hypothesize(null = "independence") %&gt;%&#13;
  generate(reps = 500, type = "permute") %&gt;%&#13;
  calculate(stat = "Chisq")&#13;
&gt;&gt;&gt; null_spac&#13;
Response: degree (factor)&#13;
Explanatory: sex (factor)&#13;
Null Hypothesis: independence&#13;
# A tibble: 500 × 2&#13;
   replicate  stat&#13;
       &lt;int&gt; &lt;dbl&gt;&#13;
 1         1  3.50&#13;
 2         2  1.11&#13;
 3         3 14.0&#13;
 4         4  4.62&#13;
 5         5  1.41&#13;
 6         6  1.41&#13;
 7         7  9.69&#13;
 8         8  4.17&#13;
 9         9  5.97&#13;
10        10  2.86&#13;
# … with 490 more rows&#13;
# i Use `print(n = ...)` to see more rows</pre>			<p>To run<a id="_idIndexMarker1081"/> the test, we obtain the expected frequency for each cell in the contingency table under the assumption of independence between the categorical variables. The expected frequency for a cell is computed as (<em class="italic">row sum * column sum</em>)<em class="italic"> / </em><span class="No-Break"><em class="italic">overall sum</em></span><span class="No-Break">:</span></p>&#13;
			<pre class="source-code">&#13;
# calculate expected frequency table&#13;
row_totals = rowSums(tab)&#13;
col_totals = colSums(tab)&#13;
overall_total = sum(tab)&#13;
expected = outer(row_totals, col_totals) / overall_total&#13;
&gt;&gt;&gt; expected&#13;
       Lt High School High School Junior College Bachelor Graduate&#13;
Male          146.084    649.8067        95.7563 238.7227 141.6303&#13;
Female        181.916    809.1933       119.2437 297.2773 176.3697</pre>			<p>Here, we first obtain the row-wise and column-wise sum, as well as the total sum. We then use the <strong class="source-inline">outer()</strong> function to obtain the outer product between these two vectors, which is then scaled by the total sum to obtain the expected frequency count in <span class="No-Break">each cell.</span></p>&#13;
			<p>Now, we compute the observed chi-square statistic based on the <span class="No-Break">available samples:</span></p>&#13;
			<pre class="source-code">&#13;
# Compute chi-square statistic&#13;
observed_chi_square = sum((tab - expected)^2 / expected)&#13;
&gt;&gt;&gt; observed_chi_square&#13;
2.536349</pre>			<p>We can then plot the observed chi-square statistic within the density curve of previous bootstrapped sample statistics to get a sense of where the observed statistic is located, based on which we will be able to calculate the <span class="No-Break">corresponding p-value:</span></p>&#13;
			<pre class="source-code">&#13;
ggplot(null_spac, aes(x = stat)) +&#13;
  geom_density() +&#13;
  geom_vline(xintercept = observed_chi_square, color = "red") +&#13;
  labs(title = "Density curve of bootstrapped chi-square statistic", x = "Chi-square statistic", y = "Density") +&#13;
  theme(text = element_text(size = 16))</pre>			<p>Running the <a id="_idIndexMarker1082"/>code generates the plot in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.13</em>, which shows a <span class="No-Break">high p-value.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer212" class="IMG---Figure">&#13;
					<img src="Images/B18680_11_013.jpg" alt="Figure 11.13 – Visualizing the density curve of bootstrapped chi-square statistics and the observed statistic" width="1153" height="686"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.13 – Visualizing the density curve of bootstrapped chi-square statistics and the observed statistic</p>&#13;
			<p>Now we can calculate the p-value. As shown in the following code, the p-value of <strong class="source-inline">0.72</strong> is indeed quite high, and thus there is no sufficient evidence to reject the <span class="No-Break">null hypothesis:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; null_spac %&gt;%&#13;
  summarize(pval = 2 * mean(stat &lt; observed_chi_square)) %&gt;%&#13;
  pull()&#13;
0.72</pre>			<p>In the next section, we will shift to look at statistical inference for <span class="No-Break">numerical data.</span></p>&#13;
			<h1 id="_idParaDest-243"><a id="_idTextAnchor249"/>Statistical inference for numerical data</h1>&#13;
			<p>In this section, we <a id="_idIndexMarker1083"/>will switch to look at statistical inference using numerical data. We will cover two approaches. The first approach relies on the bootstrapping procedure and permutes the original dataset to create additional artificial datasets, which can then be used to derive the confidence intervals. The second approach uses a theoretical assumption on the distribution of the bootstrapped samples and relies on the t-distribution to achieve the same result. We will learn how to perform a t-test, derive a confidence interval, and conduct<a id="_idIndexMarker1084"/> an <strong class="bold">analysis of </strong><span class="No-Break"><strong class="bold">variance</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">ANOVA</strong></span><span class="No-Break">).</span></p>&#13;
			<p>As discussed earlier, bootstrapping<a id="_idIndexMarker1085"/> is a non-parametric resampling method that allows us to estimate the sampling distribution of a particular statistic, such as the mean, median, or proportion, as in the previous section. This is achieved by repeatedly drawing random samples with replacement from the original data. By doing so, we can calculate confidence intervals and perform hypothesis tests without relying on specific <span class="No-Break">distributional assumptions.</span></p>&#13;
			<p>Additionally, the t-distribution<a id="_idIndexMarker1086"/> is a probability distribution used for hypothesis testing if the sample size is small and the standard deviation of the population data remains unknown. It is a more general approach that assumes the bootstrapped samples follow a specific distribution. We will then use this distribution to estimate confidence intervals and perform the <span class="No-Break">hypothesis test.</span></p>&#13;
			<p>The t-test<a id="_idIndexMarker1087"/> is a widely used statistical test that allows us to compare the mean values of two groups or test whether the mean of a single group is equal to a specific value. This time, our interest is the mean of a group since the variable is numeric. The test relies on the t-distribution and takes into account the sample sizes, sample means, and <span class="No-Break">sample variances.</span></p>&#13;
			<p>Confidence intervals<a id="_idIndexMarker1088"/> offer a list of possible values, where the true population statistic, such as the mean or proportion, is likely to lie, with a specified level of confidence (specified by the significance <span class="No-Break">level </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">α</span></span><span class="No-Break">).</span></p>&#13;
			<p>Finally, ANOVA extends the<a id="_idIndexMarker1089"/> t-test used when there are more than two groups to compare. ANOVA helps us determine possible significant differences among the group means by dividing the total variability of the observed data into two parts: between-group variability and within-group variability. It tests the null hypothesis that the mean values of all groups are equal. If the null hypothesis is rejected, we can continue to identify which specific group means differ from <span class="No-Break">each other.</span></p>&#13;
			<p>Let us start with generating a bootstrap distribution for <span class="No-Break">the median.</span></p>&#13;
			<h2 id="_idParaDest-244"><a id="_idTextAnchor250"/>Generating a bootstrap distribution for the median</h2>&#13;
			<p>As <a id="_idIndexMarker1090"/>discussed earlier, when building a bootstrap <a id="_idIndexMarker1091"/>distribution for a single statistic, we first generate a collection of bootstrap samples via sampling with replacement, and then record the relevant statistic (in this case, the median) of <span class="No-Break">each distribution.</span></p>&#13;
			<p>Let us go through an exercise to build the collection of <span class="No-Break">bootstrap samples.</span></p>&#13;
			<h3>Exercise 11.5 – generating a bootstrap distribution for the sample median</h3>&#13;
			<p>In this exercise, we will<a id="_idIndexMarker1092"/> apply the same specify-generate-calculate workflow using the <strong class="source-inline">infer</strong> package to generate a bootstrap distribution for<a id="_idIndexMarker1093"/> the sample median using the <span class="No-Break"><strong class="source-inline">mtcars</strong></span><span class="No-Break"> dataset.</span></p>&#13;
			<p>Load the <strong class="source-inline">mtcars</strong> dataset and view <span class="No-Break">its structure:</span></p>&#13;
			<pre class="source-code">&#13;
data(mtcars)&#13;
&gt;&gt;&gt; str(mtcars)&#13;
'data.frame':  32 obs. of  11 variables:&#13;
 $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...&#13;
 $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...&#13;
 $ disp: num  160 160 108 258 360 ...&#13;
 $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...&#13;
 $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...&#13;
 $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...&#13;
 $ qsec: num  16.5 17 18.6 19.4 17 ...&#13;
 $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...&#13;
 $ am  : num  1 1 1 0 0 0 0 0 0 0 ...&#13;
 $ gear: num  4 4 4 3 3 3 3 4 4 4 ...&#13;
 $ carb: num  4 4 1 1 2 1 4 2 2 4 ...</pre>			<p>The result<a id="_idIndexMarker1094"/> shows that we have a dataset with <a id="_idIndexMarker1095"/>32 rows and 11 columns. In the following steps, we will use the <strong class="source-inline">mpg</strong> variable and generate a bootstrap distribution of <span class="No-Break">its median:</span></p>&#13;
			<ol>&#13;
				<li>Generate 10,000 bootstrap <a id="_idIndexMarker1096"/>samples according to the <strong class="source-inline">mpg</strong> variable and obtain the median <a id="_idIndexMarker1097"/>of <span class="No-Break">all samples:</span><pre class="source-code">&#13;
bs &lt;- mtcars %&gt;%&#13;
  specify(response = mpg) %&gt;%&#13;
  generate(reps = 10000, type = "bootstrap") %&gt;%&#13;
  calculate(stat = "median")&#13;
&gt;&gt;&gt; bs&#13;
Response: mpg (numeric)&#13;
# A tibble: 10,000 × 2&#13;
   replicate  stat&#13;
       &lt;int&gt; &lt;dbl&gt;&#13;
 1         1  21.4&#13;
 2         2  22.2&#13;
 3         3  20.4&#13;
 4         4  17.8&#13;
 5         5  19.2&#13;
 6         6  19.2&#13;
 7         7  18.4&#13;
 8         8  20.4&#13;
 9         9  19.0&#13;
10        10  21.4&#13;
# … with 9,990 more rows&#13;
# i Use `print(n = ...)` to see more rows</pre></li>			</ol>&#13;
			<p>Here, we<a id="_idIndexMarker1098"/> specify <strong class="source-inline">stat = "median"</strong> in the <strong class="source-inline">calculate()</strong> function to extract <a id="_idIndexMarker1099"/>the median in each <span class="No-Break">bootstrap sample.</span></p>&#13;
			<p>2.	Plot the bootstrap distribution as a density curve of the bootstrapped <span class="No-Break">sample statistics:</span></p>&#13;
			<pre class="source-code">&#13;
ggplot(bs, aes(x = stat)) +&#13;
  geom_density() +&#13;
  labs(title = "Density plot for bootstrapped median", x = "Median", y = "Probability") +&#13;
  theme(text = element_text(size = 16))</pre>			<p class="list-inset">Running the code generates the plot in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.14</em></span><span class="No-Break">.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer213" class="IMG---Figure">&#13;
					<img src="Images/B18680_11_014.jpg" alt="Figure 11.14 – Visualizing the density curve of the bootstrapped sample median" width="715" height="544"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.14 – Visualizing the density curve of the bootstrapped sample median</p>&#13;
			<p>The next section looks at constructing the bootstrapped <span class="No-Break">confidence interval.</span></p>&#13;
			<h1 id="_idParaDest-245"><a id="_idTextAnchor251"/>Constructing the bootstrapped confidence interval</h1>&#13;
			<p>We have<a id="_idIndexMarker1100"/> looked at how to construct the bootstrapped confidence interval using the standard error method. This involves adding and subtracting the scaled standard error from the observed sample statistic. It turns out that there is another, simpler method, which just uses the percentile of the bootstrap distribution to obtain the <span class="No-Break">confidence interval.</span></p>&#13;
			<p>Let us continue with the previous example. Say we would like to calculate the 95% confidence interval of the previous bootstrap distribution. We can achieve this by calculating the upper and lower quantiles (97.5% and 2.5%, respectively) of the bootstrap distribution. The following code <span class="No-Break">achieves this:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; bs %&gt;%&#13;
  summarize(&#13;
    l = quantile(stat, 0.025),&#13;
    u = quantile(stat, 0.975)&#13;
  )&#13;
# A tibble: 1 × 2&#13;
      l     u&#13;
  &lt;dbl&gt; &lt;dbl&gt;&#13;
1  16.6  21.4</pre>			<p>Let us also calculate the bootstrap confidence interval using the standard error method, as shown in the <span class="No-Break">following code:</span></p>&#13;
			<pre class="source-code">&#13;
SE = bs %&gt;%&#13;
  summarise(sd(stat)) %&gt;%&#13;
  pull()&#13;
observed_median = median(mtcars$mpg)&#13;
&gt;&gt;&gt; c(observed_median - 2*SE, observed_median + 2*SE)&#13;
16.64783 21.75217</pre>			<p>As expected, the result is close to the one obtained using the percentile method. However, the<a id="_idIndexMarker1101"/> standard error method is a more accurate method than the <span class="No-Break">percentile method.</span></p>&#13;
			<p>The next section covers re-centering a bootstrap distribution upon testing a <span class="No-Break">null hypothesis.</span></p>&#13;
			<h2 id="_idParaDest-246"><a id="_idTextAnchor252"/>Re-centering a bootstrap distribution</h2>&#13;
			<p>The <a id="_idIndexMarker1102"/>bootstrap distribution from the previous section is generated by randomly sampling the original dataset with replacement. Each set of bootstrap samples maintains the same size as the original sample sets. However, we cannot directly use this bootstrap distribution for <span class="No-Break">hypothesis testing.</span></p>&#13;
			<p>Upon introducing a null hypothesis, what we did in the previous hypothesis test section for two categorical variables is re-generated a new bootstrap distribution under the null hypothesis. We then place the observed sample statistic as a vertical red line along the bootstrap distribution to calculate the p-value, representing the probability of experiencing a phenomenon at least as extreme as the observed sample statistic. The only additional step is to generate the bootstrap distribution under the <span class="No-Break">null hypothesis.</span></p>&#13;
			<p>When generating bootstrap samples under the null hypothesis, the main idea is to remove the effect we are testing for and create samples, assuming the null hypothesis is true. In other words, we create samples that would be expected if there were no difference between the groups. For example, when comparing means between two groups, we would subtract the overall mean from each observation to center the data around 0 before performing the random sampling <span class="No-Break">with replacement.</span></p>&#13;
			<p>There is another way to achieve this. Recall that the original bootstrap distribution, by design, is centered around the observed sample statistic. Upon introducing the null hypothesis, we could simply move the original bootstrap distribution to be centered around the statistic in the null hypothesis, which is the null value. This shifted bootstrap distribution represents the same distribution if we were to remove the effect in the original dataset and then perform bootstrap sampling again. We can then place the observed sample statistic along the shifted bootstrap distribution to calculate the corresponding p-value, which represents the ratio of simulations that generate a sample statistic at least as <a id="_idIndexMarker1103"/>favorable to the alternative hypothesis as the actual sample statistic. <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.15</em> demonstrates <span class="No-Break">this process.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer214" class="IMG---Figure">&#13;
					<img src="Images/B18680_11_015.jpg" alt="Figure 11.15 – Shifting the bootstrap distribution to be centered around the null value" width="768" height="360"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.15 – Shifting the bootstrap distribution to be centered around the null value</p>&#13;
			<p>Let us generate the<a id="_idIndexMarker1104"/> bootstrap distribution for hypothesis testing for the previous example. We want to test the null hypothesis with a population median of 16 for the <strong class="source-inline">mpg</strong> variable. The following code generates the bootstrapped sample statistics, where we specify the null value via <strong class="source-inline">med = 16</strong> and the point estimate with <strong class="source-inline">null = "point"</strong> in the <span class="No-Break"><strong class="source-inline">hypothesize()</strong></span><span class="No-Break"> function:</span></p>&#13;
			<pre class="source-code">&#13;
bs = mtcars %&gt;%&#13;
  specify(response = mpg) %&gt;%&#13;
  hypothesize(null = "point", med = 16) %&gt;%&#13;
  generate(reps = 10000, type = "bootstrap") %&gt;%&#13;
  calculate(stat = "median")&#13;
&gt;&gt;&gt; bs&#13;
Response: mpg (numeric)&#13;
Null Hypothesis: point&#13;
# A tibble: 10,000 × 2&#13;
   replicate  stat&#13;
       &lt;int&gt; &lt;dbl&gt;&#13;
 1         1  16&#13;
 2         2  16.2&#13;
 3         3  18&#13;
 4         4  16.2&#13;
 5         5  16&#13;
 6         6  16.2&#13;
 7         7  16&#13;
 8         8  16&#13;
 9         9  17.8&#13;
10        10  16&#13;
# … with 9,990 more rows&#13;
# i Use `print(n = ...)` to see more rows</pre>			<p>Now, we plot these <a id="_idIndexMarker1105"/>bootstrapped sample statistics in a density plot, along with the observed sample statistic as a vertical <span class="No-Break">red line:</span></p>&#13;
			<pre class="source-code">&#13;
ggplot(bs, aes(x = stat)) +&#13;
  geom_density() +&#13;
  geom_vline(xintercept = median(mtcars$mpg), color = "red") +&#13;
  labs(title = "Density curve of bootstrapped median", x = "Sample median", y = "Density") +&#13;
  theme(text = element_text(size = 16))</pre>			<p>Running the code generates the plot in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.16</em>, which shows a <span class="No-Break">small p-value.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer215" class="IMG---Figure">&#13;
					<img src="Images/B18680_11_016.jpg" alt="Figure 11.16 – Density plot of bootstrapped sample medians and observed sample median (vertical red line)" width="719" height="549"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.16 – Density plot of bootstrapped sample medians and observed sample median (vertical red line)</p>&#13;
			<p>In the next section, we will cover another <a id="_idIndexMarker1106"/>distribution-based inference approach based on the <strong class="bold">central limit </strong><span class="No-Break"><strong class="bold">theorem</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">CLT</strong></span><span class="No-Break">).</span></p>&#13;
			<h1 id="_idParaDest-247"><a id="_idTextAnchor253"/>Introducing the central limit theorem used in t-distribution</h1>&#13;
			<p>The CLT says<a id="_idIndexMarker1107"/> that the distribution from the sum (or average) of many independent and identically distributed random variables would jointly form a normal distribution, regardless of the underlying distribution of these individual variables. Due to the CLT, normal distribution is often used to approximate the sampling distribution of various statistics, such as the sample mean and the <span class="No-Break">sample proportion.</span></p>&#13;
			<p>The t-distribution<a id="_idIndexMarker1108"/> is related to the CLT in the context of statistical inference. When we’re estimating a population mean from a sample, we often have no access to the true standard deviation of the population. Instead, we resort to the sample standard deviation as an estimate. In this case, the sampling distribution of the sample mean doesn’t follow a normal distribution, but rather a t-distribution. In other words, when we extract the sample mean from a set of observed samples, and we are unsure of the population standard deviation (as is often the case when working with actual data), the sample mean can be modeled as a realization from <span class="No-Break">the t-distribution.</span></p>&#13;
			<p>The t-distribution <a id="_idIndexMarker1109"/>is a family of continuous probability distributions that are symmetric and bell-shaped, which shows similarity to the normal distribution. However, the t-distribution shows heavier tails, which accounts for the greater uncertainty due to estimating the population standard deviation from the observed data. That is, observations of a t-distribution are more likely to fall into distant tails (such as beyond two standard deviations away from the mean) than the normal distribution. The shape of the t-distribution relies<a id="_idIndexMarker1110"/> on the <strong class="bold">degrees of freedom</strong> (<strong class="bold">df</strong>), which depends on the sample size and determines the thickness of the tails. As more samples are collected, the df moves up, and the t-distribution gradually approximates the <span class="No-Break">normal distribution.</span></p>&#13;
			<p>We briefly covered the <strong class="source-inline">qt()</strong> function used to find the cutoffs under the t-distribution in the previous chapter. Now let us go through an exercise to get more familiar with calculations related to <span class="No-Break">the t-distribution.</span></p>&#13;
			<h3>Exercise 11.6 – understanding the t-distribution</h3>&#13;
			<p>In this <a id="_idIndexMarker1111"/>exercise, we will use the <strong class="source-inline">pt()</strong> function to find probabilities under the t-distribution. For a given cutoff quantile value, <strong class="source-inline">q</strong>, and a given <strong class="source-inline">df</strong>, the <strong class="source-inline">pt(q, df)</strong> function gives us the probability under the t-distribution with <strong class="source-inline">df</strong> for values of <strong class="source-inline">t</strong> less than <strong class="source-inline">q</strong>. In other words, we have <span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">&lt;</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span> <strong class="source-inline">pt(q = T, df)</strong>. We can also use the <strong class="source-inline">qt()</strong> function to find the quantiles for a specific probability under the t-distribution. That is, if <span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">&lt;</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span>, then <span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span> <span class="No-Break"><strong class="source-inline">qt(p, df)</strong></span><span class="No-Break">:</span></p>&#13;
			<ol>&#13;
				<li>Find the probability under the t-distribution with 10 df <span class="No-Break">below </span><span class="No-Break"><strong class="source-inline">T=3</strong></span><span class="No-Break">:</span><pre class="source-code">&#13;
x = pt(3, df = 10)&#13;
&gt;&gt;&gt; x&#13;
0.9933282</pre></li>				<li>Find the probability under the t-distribution with 10 df <span class="No-Break">above </span><span class="No-Break"><strong class="source-inline">T=3</strong></span><span class="No-Break">:</span><pre class="source-code">&#13;
y = 1 - x&#13;
&gt;&gt;&gt; y&#13;
0.006671828</pre><p class="list-inset">Note that we first calculate the probability of being below a specific cutoff value under the t-distribution, and then take the complement to find the probability above <span class="No-Break">the threshold.</span></p></li>				<li>Find the probability under the t-distribution with <strong class="source-inline">100</strong> df <span class="No-Break">above </span><span class="No-Break"><strong class="source-inline">T=3</strong></span><span class="No-Break">:</span><pre class="source-code">&#13;
z = 1 - pt(3, df = 100)&#13;
&gt;&gt;&gt; z&#13;
0.001703958</pre><p class="list-inset">Since <strong class="source-inline">df=100</strong> has a better approximation to the normal distribution than <strong class="source-inline">df=10</strong>, the resulting probability, <strong class="source-inline">z</strong>, is thus smaller <span class="No-Break">than </span><span class="No-Break"><strong class="source-inline">y</strong></span><span class="No-Break">.</span></p></li>				<li>Find the 95<span class="superscript">th</span> percentile of the t-distribution with <span class="No-Break">10 df:</span><pre class="source-code">&#13;
d = qt(0.95, df = 10)&#13;
&gt;&gt;&gt; d&#13;
1.812461</pre></li>				<li>Find the cutoff <a id="_idIndexMarker1112"/>value that bounds the upper end of the middle 95<span class="superscript">th</span> percentile of the t-distribution with <span class="No-Break"><strong class="source-inline">10</strong></span><span class="No-Break"> df:</span><pre class="source-code">&#13;
e = qt(0.975, df = 10)&#13;
&gt;&gt;&gt; e&#13;
2.228139</pre><p class="list-inset">Here, the upper end of the middle 95<span class="superscript">th</span> percentile refers to the <span class="No-Break">97.5</span><span class="No-Break"><span class="superscript">th</span></span><span class="No-Break"> percentile.</span></p></li>				<li>Find the cutoff value that bounds the upper end of the middle 95<span class="superscript">th</span> percentile of the t-distribution with <span class="No-Break"><strong class="source-inline">100</strong></span><span class="No-Break"> df:</span><pre class="source-code">&#13;
f = qt(0.975, df = 100)&#13;
&gt;&gt;&gt; f&#13;
1.983972</pre></li>			</ol>&#13;
			<p>The next section discusses how to construct the confidence interval for the population mean using <span class="No-Break">the t-distribution.</span></p>&#13;
			<h1 id="_idParaDest-248"><a id="_idTextAnchor254"/>Constructing the confidence interval for the population mean using the t-distribution</h1>&#13;
			<p>Let us<a id="_idIndexMarker1113"/> review the<a id="_idIndexMarker1114"/> process of statistical inference for the population mean. We start with a limited sample, from which we can derive the sample mean. Since we want to estimate the population mean, we would like to perform statistical inference based on the observed sample mean and quantify the range where the population statistic <span class="No-Break">may exist.</span></p>&#13;
			<p>For example, the average miles per gallon, shown in the following code, is around 20 in the <span class="No-Break"><strong class="source-inline">mtcars</strong></span><span class="No-Break"> dataset:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; mean(mtcars$mpg)&#13;
20.09062</pre>			<p>Given this result, we won’t be surprised to encounter another similar dataset with an average <strong class="source-inline">mpg</strong> of 19 or 21. However, we would be surprised if the value is 5, 50, or even 100. When assessing a new collection of samples, we need a way to quantify the variability of the sample mean across multiple samples. We have learned two ways to do this: use the bootstrap approach to simulate artificial samples or use the CLT to approximate such variability. We will focus on the CLT approach in <span class="No-Break">this section.</span></p>&#13;
			<p>According to the CLT, the sample mean of any sampling distribution would be approximately normally distributed, regardless of the original distribution. In other words, we have <span class="No-Break">the following:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">∼</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">μ</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Variable">E</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">σ</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">√</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">)</span></p>&#13;
			<p>Note that this is a theoretical distribution we are unable to obtain. For example, the population standard deviation, <span class="_-----MathTools-_Math_Variable">σ</span>, stays unknown, and we only have access to the observed samples. Instead, we would estimate the standard error using the sample standard deviation, <span class="_-----MathTools-_Math_Variable">s</span>, giving <span class="No-Break">the following:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">∼</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">μ</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Variable">E</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">√</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">)</span></p>&#13;
			<p>We would then employ the t-distribution of <span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span> degree of freedom to make an inference for the population mean as it gives thicker tails due to the additional uncertainty introduced <span class="No-Break">by </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">s</span></span><span class="No-Break">.</span></p>&#13;
			<p>In addition, note that the approximation using the CLT relies on a few assumptions. For example, the samples need to be independent of each other. This is often satisfied when the samples are randomly selected, or if the samples account for less than 10% of the total population if they are selected without replacement. The sample size also needs to be larger to account for potential skewness in <span class="No-Break">the samples.</span></p>&#13;
			<p>We can <a id="_idIndexMarker1115"/>construct the 95% confidence<a id="_idIndexMarker1116"/> interval using the <strong class="source-inline">t.test()</strong> function, as shown in the <span class="No-Break">following code:</span></p>&#13;
			<pre class="source-code">&#13;
# Construct 95% CI for avg mpg&#13;
&gt;&gt;&gt; t.test(mtcars$mpg)&#13;
  One Sample t-test&#13;
data:  mtcars$mpg&#13;
t = 18.857, df = 31, p-value &lt; 2.2e-16&#13;
alternative hypothesis: true mean is not equal to 0&#13;
95 percent confidence interval:&#13;
 17.91768 22.26357&#13;
sample estimates:&#13;
mean of x&#13;
 20.09062</pre>			<p>Here, we are performing a one-sample t-test, where the default null hypothesis states that the population mean is 0. The result shows a very small p-value, suggesting that we could reject the null hypothesis in favor of the alternative hypothesis; that is, the population mean is not 0. The 95% confidence interval (between <strong class="source-inline">17.91768</strong> and <strong class="source-inline">22.26357</strong>) is also<a id="_idIndexMarker1117"/> constructed<a id="_idIndexMarker1118"/> based on the t-distribution with a <strong class="source-inline">df</strong> of <strong class="source-inline">31</strong> and a t-statistic <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">18.857</strong></span><span class="No-Break">.</span></p>&#13;
			<p>The next section reviews the hypothesis testing for two means using both bootstrap simulation and <span class="No-Break">t-test approximation.</span></p>&#13;
			<h1 id="_idParaDest-249"><a id="_idTextAnchor255"/>Performing hypothesis testing for two means</h1>&#13;
			<p>In this section, we will explore the<a id="_idIndexMarker1119"/> process of comparing two sample means using hypothesis testing. When comparing two sample means, we want to determine whether a significant difference exists between the means of two distinct populations <span class="No-Break">or groups.</span></p>&#13;
			<p>Suppose now we have two groups of samples. These two groups could represent a specific value before and after treatment for each sample. Our objective is thus to compare the sample statistics of these two groups, such as the sample mean, and determine whether the treatment has an effect. To do this, we can perform a hypothesis test to compare mean values from the two independent distributions using either bootstrap simulation or <span class="No-Break">t-test approximation.</span></p>&#13;
			<p>When using the t-test in the hypothesis test to compare the mean values of two independent samples, the two-sample t-test assumes normal distribution for the data, and that the variances of the two populations are equal. However, in cases where these assumptions may not hold, alternative non-parametric tests or resampling methods, such as bootstrap, can be employed to make inferences about the <span class="No-Break">population means.</span></p>&#13;
			<p>Let us go through an exercise to see these two methods of hypothesis testing <span class="No-Break">in play.</span></p>&#13;
			<h3>Exercise 11.7 – comparing two means</h3>&#13;
			<p>In this exercise, we<a id="_idIndexMarker1120"/> will explore two approaches (t-test and bootstrap) to compare two sample means and calculate the confidence interval of the difference in <span class="No-Break">sample means:</span></p>&#13;
			<ol>&#13;
				<li>Generate a dummy dataset that consists of two groups <span class="No-Break">of samples:</span><pre class="source-code">&#13;
# Define two samples&#13;
sample1 = c(10, 12, 14, 16, 18)&#13;
sample2 = c(15, 17, 19, 21, 23)&#13;
# Combine samples into a data frame&#13;
data = tibble(&#13;
  value = c(sample1, sample2),&#13;
  group = factor(rep(c("Group 1", "Group 2"), each = length(sample1)))&#13;
)&#13;
&gt;&gt;&gt; data&#13;
# A tibble: 10 × 2&#13;
   value group&#13;
   &lt;dbl&gt; &lt;fct&gt;&#13;
 1    10 Group 1&#13;
 2    12 Group 1&#13;
 3    14 Group 1&#13;
 4    16 Group 1&#13;
 5    18 Group 1&#13;
 6    15 Group 2&#13;
 7    17 Group 2&#13;
 8    19 Group 2&#13;
 9    21 Group 2&#13;
10    23 Group 2</pre><p class="list-inset">Here, we<a id="_idIndexMarker1121"/> created a <strong class="source-inline">tibble</strong> DataFrame with the <strong class="source-inline">value</strong> column indicating the sample observation and the <strong class="source-inline">group</strong> column indicating the group number. We would like to assess the difference in the sample mean between these <span class="No-Break">two groups.</span></p></li>				<li>Perform<a id="_idIndexMarker1122"/> bootstrap sampling <strong class="source-inline">1000</strong> times and calculate the bootstrap statistics under the null hypothesis that these two groups are independent of each other, and there is no difference in <span class="No-Break">their means:</span><pre class="source-code">&#13;
bootstrap_results = data %&gt;%&#13;
  specify(response = value, explanatory = group) %&gt;%&#13;
  hypothesize(null = "independence") %&gt;%&#13;
  generate(reps = 1000, type = "bootstrap") %&gt;%&#13;
  calculate(stat = "diff in means", order = c("Group 1", "Group 2"))&#13;
&gt;&gt;&gt; bootstrap_results&#13;
Response: value (numeric)&#13;
Explanatory: group (factor)&#13;
Null Hypothesis: independence&#13;
# A tibble: 1,000 × 2&#13;
   replicate  stat&#13;
       &lt;int&gt; &lt;dbl&gt;&#13;
 1         1 -7.5&#13;
 2         2 -6.17&#13;
 3         3 -5&#13;
 4         4 -2.20&#13;
 5         5 -8.05&#13;
 6         6 -4.2&#13;
 7         7 -3.5&#13;
 8         8 -6.67&#13;
 9         9 -4.2&#13;
10        10 -6.90&#13;
# … with 990 more rows&#13;
# i Use `print(n = ...)` to see more rows</pre><p class="list-inset">Here, the pipeline for the hypothesis test starts by specifying the response (<strong class="source-inline">value</strong>) and explanatory (<strong class="source-inline">group</strong>) variables, setting up the null hypothesis, generating bootstrap samples under the null hypothesis, and then calculating the test statistic (in this case, the difference in means) for each bootstrap sample. The null hypothesis states that we assume the sample mean values for both groups come from the same population, and that any observed difference is merely due <span class="No-Break">to chance.</span></p></li>				<li>Calculate <a id="_idIndexMarker1123"/>the confidence interval based on the <span class="No-Break">bootstrap statistics:</span><pre class="source-code">&#13;
ci = bootstrap_results %&gt;%&#13;
  filter(!is.na(stat)) %&gt;%&#13;
  get_confidence_interval(level = 0.95, type = "percentile")&#13;
&gt;&gt;&gt; ci&#13;
# A tibble: 1 × 2&#13;
  lower_ci upper_ci&#13;
     &lt;dbl&gt;    &lt;dbl&gt;&#13;
1       -9    -1.17</pre></li>				<li>Perform a two-sample t-test using the <span class="No-Break"><strong class="source-inline">t.test()</strong></span><span class="No-Break"> function:</span><pre class="source-code">&#13;
t_test_result = t.test(sample1, sample2)&#13;
&gt;&gt;&gt; t_test_result&#13;
   Welch Two Sample t-test&#13;
data:  sample1 and sample2&#13;
t = -2.5, df = 8, p-value = 0.03694&#13;
alternative hypothesis: true difference in means is not equal to 0&#13;
95 percent confidence interval:&#13;
 -9.6120083 -0.3879917&#13;
sample estimates:&#13;
mean of x mean of y&#13;
       14        19</pre><p class="list-inset">The result <a id="_idIndexMarker1124"/>shows that the 95% confidence interval based on the t-distribution is close but still different from the one obtained via bootstrap sampling. We can also perform the t-test by passing in the <span class="No-Break">model form:</span></p><pre class="source-code">t_test_result2 = t.test(value ~ group, data = data)&#13;
&gt;&gt;&gt; t_test_result2&#13;
    Welch Two Sample t-test&#13;
data:  value by group&#13;
t = -2.5, df = 8, p-value = 0.03694&#13;
alternative hypothesis: true difference in means between group Group 1 and group Group 2 is not equal to 0&#13;
95 percent confidence interval:&#13;
 -9.6120083 -0.3879917&#13;
sample estimates:&#13;
mean in group Group 1 mean in group Group 2&#13;
                   14                    19</pre></li>			</ol>&#13;
			<p>The next section introduces ANOVA, or the analysis <span class="No-Break">of variance.</span></p>&#13;
			<h1 id="_idParaDest-250"><a id="_idTextAnchor256"/>Introducing ANOVA</h1>&#13;
			<p><strong class="bold">ANOVA</strong> is <a id="_idIndexMarker1125"/>a statistical hypothesis testing method used to compare the means of more than two groups, which extends the two-sample t-test discussed in the previous section. The goal of ANOVA is to test potential significant differences among the group means (the between-group variability) while accounting for the variability within each group (the <span class="No-Break">within-group variability).</span></p>&#13;
			<p>ANOVA relies on the F-statistic in hypothesis testing. The F-statistic is a ratio of two estimates of variance: the between-group variance and the within-group variance. The between-group variance measures the differences among the group means, while the within-group variance represents the variability within each group. The F-statistic can be calculated based on these two <span class="No-Break">group variances.</span></p>&#13;
			<p>In hypothesis testing, the null hypothesis for ANOVA states that all group means are equal, and any observed differences are due to chance. The alternative hypothesis, on the other hand, suggests that at least one group’s mean differs from the others. If the F-statistic is sufficiently large, the between-group variance is significantly greater than the within-group variance, which provides evidence against the <span class="No-Break">null hypothesis.</span></p>&#13;
			<p>Let us look at a concrete example. We first load the <strong class="source-inline">PlantGrowth</strong> dataset, which contains the weights of plants after they have been subjected to three <span class="No-Break">different treatments:</span></p>&#13;
			<pre class="source-code">&#13;
data(PlantGrowth)&#13;
&gt;&gt;&gt; str(PlantGrowth)&#13;
'data.frame':   30 obs. of  2 variables:&#13;
 $ weight: num  4.17 5.58 5.18 6.11 4.5 4.61 5.17 4.53 5.33 5.14 ...&#13;
 $ group : Factor w/ 3 levels "ctrl","trt1",..: 1 1 1 1 1 1 1 1 1 1 ...</pre>			<p>Next, we perform the one-way ANOVA test using the same specify-hypothesize-generate-calculate procedure. Specifically, we first specify the response variable (<strong class="source-inline">weight</strong>) and the explanatory variable (<strong class="source-inline">group</strong>). We then set up the null hypothesis, stating no difference in the means of the groups, using <strong class="source-inline">hypothesize(null = "independence")</strong>. Next, we <a id="_idIndexMarker1126"/>generate 1,000 permuted datasets using <strong class="source-inline">generate(reps = 1000, type = "permute")</strong>. Finally, we calculate the F-statistic for each permuted dataset using <strong class="source-inline">calculate(stat = "</strong><span class="No-Break"><strong class="source-inline">F")</strong></span><span class="No-Break">:</span></p>&#13;
			<pre class="source-code">&#13;
anova_results = PlantGrowth %&gt;%&#13;
  specify(response = weight, explanatory = group) %&gt;%&#13;
  hypothesize(null = "independence") %&gt;%&#13;
  generate(reps = 1000, type = "permute") %&gt;%&#13;
  calculate(stat = "F")&#13;
&gt;&gt;&gt; anova_results&#13;
Response: weight (numeric)&#13;
Explanatory: group (factor)&#13;
Null Hypothesis: independence&#13;
# A tibble: 1,000 × 2&#13;
   replicate  stat&#13;
       &lt;int&gt; &lt;dbl&gt;&#13;
 1         1 0.162&#13;
 2         2 0.198&#13;
 3         3 1.18&#13;
 4         4 0.328&#13;
 5         5 1.21&#13;
 6         6 3.00&#13;
 7         7 1.93&#13;
 8         8 0.605&#13;
 9         9 0.446&#13;
10        10 1.10&#13;
# … with 990 more rows&#13;
# i Use `print(n = ...)` to see more rows</pre>			<p>Last, we can<a id="_idIndexMarker1127"/> calculate the p-value using the observed F-statistic and the distribution of the F-statistics obtained from the permuted datasets. When the p-value is smaller than the preset significance level (for example, <strong class="source-inline">0.05</strong>), we could reject the null hypothesis and say that there is a significant difference among the means of <span class="No-Break">the groups:</span></p>&#13;
			<pre class="source-code">&#13;
p_value = anova_results %&gt;%&#13;
  get_p_value(obs_stat = anova_results, direction = "right") %&gt;%&#13;
  pull()&#13;
&gt;&gt;&gt; p_value&#13;
0.376</pre>			<p>The result suggests that we do not have enough confidence to reject the <span class="No-Break">null hypothesis.</span></p>&#13;
			<h1 id="_idParaDest-251"><a id="_idTextAnchor257"/>Summary</h1>&#13;
			<p>In this chapter, we covered different types of statistical inferences for hypothesis testing, targeting both numerical and categorical data. We introduced inference methods for a single variable, two variables, and multiple variables, using either proportion (for categorical variable) or mean (for numerical variable) as the sample statistic. The hypothesis testing procedure, including both the parametric approach using model-based approximation and the non-parametric approach using bootstrap-based simulations, offers valuable tools such as the confidence interval and p-value. These tools allow us to make a decision about whether we can reject the null hypothesis in favor of the alternative hypothesis. Such a decision also relates to the Type I and Type <span class="No-Break">II errors.</span></p>&#13;
			<p>In the next chapter, we will cover one of the most widely used statistical and ML models: <span class="No-Break">linear regression.</span></p>&#13;
		</div>&#13;
	</div></body></html>