- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Optimizing and Managing Machine Learning Models for Edge Deployment
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化和管理适用于边缘部署的机器学习模型
- en: Every **Machine Learning** (**ML**) practitioner knows that the ML development
    life cycle is an extremely iterative process, from gathering, exploring, and engineering
    the right features for our algorithm, to training, tuning, and optimizing the
    ML model for deployment. As ML practitioners, we spend up to 80% of our time getting
    the right data for training the ML model, with the last 20% actually training
    and tuning the ML model. By the end of the process, we are all probably so relieved
    that we finally have an optimized ML model that we often don’t pay enough attention
    to exactly how the resultant model is deployed. It is, therefore, important to
    realize that where and how the trained model gets deployed has a significant impact
    on the overall ML use case. For example, let’s say that our ML use case was specific
    to **Autonomous Vehicles** (**AVs**), specifically a **Computer Vision** (**CV**)
    model that was trained to detect other vehicles. Once our CV model has been trained
    and optimized, we can deploy it.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 每个机器学习（ML）从业者都知道，机器学习开发生命周期是一个极其迭代的过程，从收集、探索和为我们的算法工程正确的特征，到训练、调整和优化机器学习模型以进行部署。作为机器学习从业者，我们花费高达80%的时间来获取训练机器学习模型所需的数据，最后20%的时间实际上是在训练和调整机器学习模型。在过程结束时，我们可能都感到非常欣慰，因为我们终于有一个优化的机器学习模型，我们往往没有足够关注最终模型是如何部署的。因此，重要的是要认识到，训练模型部署的位置和方式对整个机器学习用例有重大影响。例如，假设我们的机器学习用例是针对**自动驾驶汽车**（AVs），特别是训练来检测其他车辆的**计算机视觉**（CV）模型。一旦我们的CV模型被训练和优化，我们就可以部署它。
- en: '*But where do we deploy it? Do we deploy it on the vehicle itself, or do we
    deploy it on the same infrastructure used to train* *the model?*'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*但是我们在哪里部署它？是部署在车辆本身上，还是部署在训练* *模型* *所使用的相同基础设施上？*'
- en: Well, if we deploy the model onto the same infrastructure we used to train the
    model, we will also need to ensure that the vehicle can connect to this infrastructure.
    We will also need to ensure that the connectivity from the vehicle to the model
    is sufficiently robust and performant to ensure that model inference results are
    timely. It would be disastrous if the vehicle was unable to detect an oncoming
    vehicle in time. So, in this use case, it might be better to execute model inferences
    on the vehicle itself; that way, we won’t need to worry about network connectivity,
    resilience, bandwidth, and latency between the vehicle and the ML model. Essentially,
    deploying and executing ML model inferences on the edge devices.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，如果我们将模型部署到我们用于训练模型的相同基础设施上，我们还需要确保车辆可以连接到这个基础设施。我们还需要确保从车辆到模型的连接足够稳健和高效，以确保模型推理结果及时。如果车辆无法及时检测到迎面而来的车辆，那将是灾难性的。因此，在这种情况下，在车辆本身上执行模型推理可能更好；这样，我们就无需担心车辆和机器学习模型之间的网络连接、弹性、带宽和延迟。本质上，就是在边缘设备上部署和执行机器学习模型推理。
- en: 'However, deploying and managing ML models at the edge imposes additional complexities
    on the ML development life cycle. So, in this chapter, we will be reviewing some
    of these complexities, and by means of a practical example, we will see exactly
    how to optimize, manage, and deploy the ML model for the edge. Thus, we will be
    covering the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在边缘部署和管理机器学习模型给机器学习开发生命周期带来了额外的复杂性。因此，在本章中，我们将回顾一些这些复杂性，并通过一个实际示例，我们将看到如何确切地优化、管理和部署边缘的机器学习模型。因此，我们将涵盖以下主题：
- en: Understanding edge computing
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解边缘计算
- en: Reviewing the key considerations for optimal edge deployments
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 审查最佳边缘部署的关键考虑因素
- en: Designing an architecture for optimal edge deployments
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计用于最佳边缘部署的架构
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To work through the hands-on examples within this chapter, you should have
    the following prerequisites:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成本章中的动手示例，你应该具备以下先决条件：
- en: A web browser (for the best experience, it is recommended that you use a Chrome
    or Firefox browser)
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络浏览器（为了获得最佳体验，建议您使用Chrome或Firefox浏览器）
- en: Access to the AWS account that you’ve used in [*Chapter 5*](B18493_05.xhtml#_idTextAnchor095),
    *Data Analysis*
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问你在[*第5章*](B18493_05.xhtml#_idTextAnchor095)，*数据分析*中使用的AWS账户
- en: Access to the Amazon SageMaker Studio development environment that we created
    in [*Chapter 5*](B18493_05.xhtml#_idTextAnchor095), *Data Analysis*
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问我们在[*第5章*](B18493_05.xhtml#_idTextAnchor095)，*数据分析*中创建的Amazon SageMaker Studio开发环境
- en: Example code for this chapter is provided in the companion GitHub repository
    ([https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter08](https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter08))
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章的示例代码可在配套的GitHub仓库中找到（[https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter08](https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter08)）
- en: Understanding edge computing
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解边缘计算
- en: To understand how we can optimize, manage, and deploy ML models for the edge,
    we need to first understand what edge computing is. Edge computing is a pattern
    or type of architecture that brings data storage mechanisms, and computing resources
    closer to the actual source of the data. So, by bringing these resources closer
    to the data itself, we are fundamentally improving the responsiveness of the overall
    application and removing the requirement to provide optimal and resilient network
    bandwidth.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解我们如何优化、管理和部署边缘的ML模型，我们首先需要了解什么是边缘计算。边缘计算是一种模式或架构类型，它将数据存储机制和计算资源更靠近实际的数据源。因此，通过将这些资源靠近数据本身，我们从根本上提高了整体应用程序的响应性，并消除了提供最佳和弹性网络带宽的需求。
- en: Therefore, if we refer to the AV example highlighted at the outset of this chapter,
    by moving the CV model closer to the source of the data, basically the live camera
    feed, we are able to detect other vehicles in real time. Consequently, instead
    of having our application make a connection to the infrastructure that hosts the
    trained model, we send the camera feed to the ML model, retrieve the inferences,
    and finally, have the application take some action based on the results.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们参考本章开头强调的AV示例，通过将CV模型靠近数据源，即实时的摄像头视频流，我们能够实时检测其他车辆。因此，我们不需要让我们的应用程序连接到托管训练模型的底层基础设施，而是将摄像头视频发送到ML模型，检索推理结果，最后，根据结果让应用程序采取一些行动。
- en: Now, using an edge computing architecture, we can send the camera feed directly
    to the trained CV model, running on the compute resources inside the vehicle itself,
    and have the application take some action based on the retrieved inference results
    in real time. Hence, by using an edge computing architecture, we have alleviated
    any unnecessary application latency introduced by having to connect to the infrastructure
    hosting the CV model. Subsequently, we have allowed the vehicle to react to other
    detected vehicles in real time. Additionally, we have removed the dependency on
    a resilient and optical network connection.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用边缘计算架构，我们可以直接将摄像头视频发送到车辆内部的计算资源上运行的训练好的CV模型，并根据实时检索到的推理结果让应用程序采取一些行动。因此，通过使用边缘计算架构，我们减轻了连接到CV模型托管基础设施所带来的任何不必要的应用程序延迟。随后，我们允许车辆实时对检测到的其他车辆做出反应。此外，我们消除了对弹性光纤网络连接的依赖。
- en: However, while the edge computing architecture provides improved application
    response times, the architecture itself also introduces additional complexities,
    especially related to its design and implementation. So, in the next section,
    we will review some of the key considerations that need to be accounted for when
    optimally deploying an ML model to the edge.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管边缘计算架构提供了改进的应用程序响应时间，但该架构本身也引入了额外的复杂性，尤其是在其设计和实现方面。因此，在下一节中，我们将回顾在最佳部署ML模型到边缘时需要考虑的一些关键因素。
- en: Reviewing the key considerations for optimal edge deployments
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查最佳边缘部署的关键考虑因素
- en: 'As we saw in the previous two chapters, there are several key factors that
    need to be taken into account when designing an appropriate architecture for training
    as well as deploying ML models at scale. In both these chapters, we also saw how
    Amazon SageMaker can be used to implement an effective ephemeral infrastructure
    for executing these tasks. Hence, in a later part of this chapter, we will also
    review how SageMaker can be used to deploy ML models to the edge at scale. Nonetheless,
    before we can dive into edge deployments with SageMaker, it is important to review
    some of the key factors that influence the successful deployment of an ML model
    at the edge:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如前两章所述，在设计适用于大规模训练和部署机器学习模型的适当架构时，需要考虑几个关键因素。在这两章中，我们还看到了如何使用Amazon SageMaker来实现执行这些任务的有效临时基础设施。因此，在本章的后续部分，我们还将回顾如何使用SageMaker在边缘大规模部署机器学习模型。然而，在我们深入使用SageMaker进行边缘部署之前，回顾一些影响边缘部署机器学习模型成功的关键因素是很重要的：
- en: Efficiency
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 效率
- en: Performance
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能
- en: Reliability
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可靠性
- en: Security
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全性
- en: While not all the mentioned factors may influence how an edge architecture is
    designed and may not be vital to the ML use case, it is important to at least
    consider them. So, let’s start by examining the significance of efficiency within
    the edge architecture design.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然并非所有提到的因素都会影响边缘架构的设计，也不一定对机器学习用例至关重要，但至少考虑它们是很重要的。因此，让我们首先考察效率在边缘架构设计中的重要性。
- en: Efficiency
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 效率
- en: Efficiency, by definition, is the ratio, or percentage, of output correlated
    with the input. When the ML model is deployed at the edge, it makes the execution
    closer to the application, as well as to the input data being used to generate
    the inferences. Therefore, we can say that deploying an ML model to the edge makes
    it efficient by default. However, this assumption is based on the fact that the
    ML model only provides inference results based on the input data and doesn’t need
    to perform any preprocessing of the input data beforehand.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 按定义，效率是输出与输入之间的比率或百分比。当机器学习模型在边缘部署时，它使执行更接近应用程序，以及用于生成推理的输入数据。因此，我们可以这样说，将机器学习模型部署到边缘默认使其变得高效。然而，这个假设是基于机器学习模型仅根据输入数据提供推理结果，并且不需要在之前对输入数据进行任何预处理的事实。
- en: For example, if we refer to the CV model example, if the image data provided
    to the ML model had to be preprocessed, for instance, the images needed to be
    resized, or the image tensors needed to be normalized, then this preprocessing
    step introduces more work for the ML model. Therefore, this ML model isn’t as
    efficient as one that just provides the inference result without any preprocessing.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们参考CV模型示例，如果提供给机器学习模型的图像数据需要预处理，例如，图像需要调整大小，或者图像张量需要归一化，那么这个预处理步骤为机器学习模型引入了更多的工作。因此，这个机器学习模型不如一个仅提供推理结果而不进行任何预处理的模型高效。
- en: So, when designing an architecture for edge deployments, we need to reduce the
    amount of unnecessary work being performed on the input data, to further streamline
    that inference result and therefore make the inference as efficient as possible.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在设计边缘部署的架构时，我们需要减少在输入数据上执行的不必要工作，以进一步简化推理结果，从而使推理尽可能高效。
- en: Performance
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能
- en: Measuring performance is similar to measuring efficiency, except that it’s not
    a ratio but rather a measurement of quality. Thus, when it comes to measuring
    the quality of an ML model deployed to the edge, we measure how quickly the model
    provides an inference result and how true the inference result is. So, just as
    with efficiency, having the ML model inference results closer to the data source
    does improve the overall performance, but there are trade-offs that are specific
    to the ML model use case that also need to be considered.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 性能的衡量与效率的衡量类似，只不过它不是一个比率，而是一个质量的测量。因此，当衡量部署到边缘的机器学习模型的质量时，我们衡量模型提供推理结果的速度以及推理结果的准确性。因此，就像效率一样，将机器学习模型的推理结果更接近数据源确实可以提高整体性能，但还需要考虑特定于机器学习用例的权衡。
- en: To illustrate using the CV use case example, we may have to compromise on the
    quality of the model’s inference results by compressing, or pruning the model
    architecture, essentially making it smaller to fit into the limited memory and
    run on the limited processing capacity of an edge computing device. Additionally,
    while most CV algorithms require GPU resources for training, as well as inference,
    we may not be able to provide GPU resources to edge devices.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以CV用例为例，我们可能需要通过压缩或剪枝模型架构来妥协模型推理结果的质量，使其更小，以便适应有限的内存，并在边缘计算设备的有限处理能力上运行。此外，尽管大多数CV算法在训练和推理时都需要GPU资源，但我们可能无法为边缘设备提供GPU资源。
- en: So, when designing an architecture for edge deployments, we need to anticipate
    what computing resources are available at the edge and explore how to refactor
    the trained ML model to ensure that it will fit on the edge device and provide
    the best inference results in the shortest amount of time.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在设计边缘部署的架构时，我们需要预测边缘可用的计算资源，并探索如何重构训练好的机器学习模型，以确保它能够适应边缘设备，并在最短的时间内提供最佳的推理结果。
- en: Reliability
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可靠性
- en: Depending on the ML use case and how the model’s inference results are used,
    reliability may not be a crucial factor influencing the design of the edge architecture.
    For instance, if we consider the AV use case, having the ability to detect other
    vehicles in proximity is a matter of life and death for the passenger. Alternatively,
    not being able to use an ML model to predict future temperature fluctuations on
    a smart thermostat may be an inconvenience but not necessarily a critical factor
    influencing the design of the edge architecture.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 根据机器学习用例以及模型推理结果的使用方式，可靠性可能不是影响边缘架构设计的决定性因素。例如，如果我们考虑自动驾驶用例，检测附近其他车辆对乘客来说是生死攸关的问题。另一方面，如果不能使用机器学习模型来预测智能恒温器上的未来温度波动，可能只是不便，但并不一定是影响边缘架构设计的决定性因素。
- en: Being able to detect as well as alert when a deployed ML model fails are crucial
    aspects of the overall reliability of the edge architecture. Thus, the ability
    to manage ML models at the edge becomes a crucial element of the overall reliability
    of the edge architecture. Other factors that may influence the reliability and
    manageability of the architecture are the communication technologies in use. These
    technologies may provide different levels of reliability and may require multiple
    different types. For example, in the AV use case, the vehicle may use cellular
    connectivity as the primary communication technology, and should this fail, a
    satellite link may be used as a backup.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 能够检测到部署的机器学习模型失败并发出警报是边缘架构整体可靠性的关键方面。因此，在边缘管理机器学习模型的能力成为边缘架构整体可靠性的关键要素。可能影响架构可靠性和可管理性的其他因素是所使用的通信技术。这些技术可能提供不同级别的可靠性，可能需要多种不同类型。例如，在自动驾驶用例中，车辆可能将蜂窝连接作为主要通信技术，如果这失败了，可以使用卫星链路作为备份。
- en: So, when designing an architecture for edge deployments, reliability might not
    be a critical factor, but having the ability to manage models deployed to the
    edge is also essential to the overall scalability of the architecture.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在设计边缘部署的架构时，可靠性可能不是一个关键因素，但能够管理部署到边缘的模型也是架构整体可扩展性的一个重要因素。
- en: Security
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安全性
- en: As is the case with reliability, security may not be a crucial factor influencing
    the edge architecture’s design, and more specific to the use case itself. For
    example, it may be necessary to encrypt all data stored on the edge architecture.
    With respect to the ML model deployed on the edge architecture, this means that
    all inference data (both requests and response data to and from the ML model)
    must be encrypted if persisted on the edge architecture. Additionally, any data
    transmitted between components internally and externally to the architecture must
    be encrypted as well.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 就像可靠性一样，安全性可能不是影响边缘架构设计的决定性因素，而是更具体地取决于用例本身。例如，可能需要加密存储在边缘架构上的所有数据。至于部署在边缘架构上的机器学习模型，这意味着所有推理数据（包括机器模型请求和响应数据）都必须在边缘架构上加密。此外，架构内部和外部传输的任何数据也必须加密。
- en: It is important to bear in mind that there is a shift from a security management
    perspective from a centralized to a decentralized trust model and that compute
    resources within the edge architecture are constrained by size and performance
    capabilities. Consequently, the choice in the types of encryption is limited,
    as advanced encryption requires additional compute resources.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要记住，从安全管理角度来看，存在从集中式信任模型到去中心化信任模型的转变，并且边缘架构中的计算资源受限于大小和性能能力。因此，加密类型的选择有限，因为高级加密需要额外的计算资源。
- en: Now that we have reviewed some of the key factors that influence the design
    of an optimal edge architecture for ML model deployment, in the next section,
    we will dive into building an optimal edge architecture using Amazon SageMaker,
    as well as other AWS services that specialize in edge device management.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经回顾了一些影响机器学习模型部署最佳边缘架构设计的关键因素，在下一节中，我们将深入探讨如何使用Amazon SageMaker以及其他专注于边缘设备管理的AWS服务来构建最佳边缘架构。
- en: Designing an architecture for optimal edge deployments
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计最佳边缘部署的架构
- en: While there are a number of key factors that influence the edge architecture
    design, as was highlighted in the previous section, there is also a critical capability
    necessary to enable these factors, namely the ability to build, deploy, and manage
    the device software at the edge. Additionally, we also need the ability to manage
    the application, in essence, the ML model deployed to run on the edge devices.
    Consequently, AWS provides both of these management capabilities using a dedicated
    device management service called **AWS IoT Greengrass** ([https://aws.amazon.com/greengrass/](https://aws.amazon.com/greengrass/)),
    as well as the ML model management capability built into Amazon SageMaker called
    **Amazon SageMaker Edge** ([https://aws.amazon.com/sagemaker/edge](https://aws.amazon.com/sagemaker/edge)).
    AWS IoT Greengrass is a service provided by AWS to deploy software to remote devices
    at scale without firmware updates. *Figure 8**.1* shows an example of an architecture
    that leverages both Greengrass and SageMaker to support edge deployments.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有许多关键因素会影响边缘架构设计，正如前文所述，但还需要一个关键能力来启用这些因素，即构建、部署和管理边缘设备软件的能力。此外，我们还需要管理应用的能力，本质上是指部署到边缘设备上运行的机器学习模型。因此，AWS通过一个名为**AWS
    IoT Greengrass**的专用设备管理服务提供了这两种管理能力，以及内置在Amazon SageMaker中的机器学习模型管理能力，称为**Amazon
    SageMaker Edge**([https://aws.amazon.com/greengrass/](https://aws.amazon.com/greengrass/))和[https://aws.amazon.com/sagemaker/edge](https://aws.amazon.com/sagemaker/edge))。AWS
    IoT Greengrass是AWS提供的一项服务，可以在无需固件更新的情况下大规模部署软件到远程设备。*图8.1*展示了利用Greengrass和SageMaker支持边缘部署的架构示例。
- en: '![Figure 8.1 – Architecture for edge deployments using Amazon SageMaker and
    IoT Greengrass](img/B18493_08_001.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 8.1 – 使用Amazon SageMaker和IoT Greengrass进行边缘部署的架构](img/B18493_08_001.jpg)'
- en: Figure 8.1 – Architecture for edge deployments using Amazon SageMaker and IoT
    Greengrass
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 8.1 – 使用Amazon SageMaker和IoT Greengrass进行边缘部署的架构
- en: As you can see from *Figure 8**.1*, a typical architecture is divided into two
    separate and individual architectures. One for the cloud-based components and
    one for the edge components. At a high level, the cloud environment is used to
    build, deploy, and manage the use case application being deployed to the edge.
    The corresponding edge environment, or the corporate data center, in this case,
    is where the edge devices reside, and in turn, execute upon the ML use case by
    running the supported ML models. From an ML use case perspective, *Figure 8**.1*
    also shows the cameras attached to the edge server device, allowing any video
    captured to be streamed to the ML model in order for the model to classify the
    objects detected in the video frames. *Figure 8**.1* shows a simplistic flow for
    the CV use case, from training an ML model on SageMaker to deploying it to the
    edge server using Greengrass, and then managing and monitoring the solution. But
    building out the actual solution is very complicated.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 8**.1* 所示，典型的架构分为两个独立的架构。一个用于基于云的组件，另一个用于边缘组件。从高层次来看，云环境用于构建、部署和管理部署到边缘的用例应用程序。相应的边缘环境，或在此情况下企业数据中心，是边缘设备所在之处，并依次通过运行支持的
    ML 模型来执行 ML 用例。从 ML 用例的角度来看，*图 8**.1* 还显示了连接到边缘服务器设备的摄像头，允许任何捕获的视频流到 ML 模型，以便模型对视频帧中检测到的对象进行分类。*图
    8**.1* 展示了 CV 用例的简化流程，从在 SageMaker 上训练 ML 模型到使用 Greengrass 部署到边缘服务器，然后管理和监控解决方案。但构建实际解决方案非常复杂。
- en: So, to illustrate this complexity, in the next section, we are going to build
    out this architecture by breaking out each component, starting with the corporate
    data center or edge architecture.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了说明这种复杂性，在下一节中，我们将通过分解每个组件来构建此架构，从企业数据中心或边缘架构开始。
- en: Building the edge components
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建边缘组件
- en: As highlighted in the previous section, the corporate data center serves as
    the edge location for our CV use case. Inside this edge location, we have a number
    of cameras, connected to a compute device, or edge server, that runs the CV model.
    Since we don’t have access to a corporate data center, within the context of the
    book, we will simulate building out the edge environment using an **Elastic Compute
    Cloud** (**EC2**) instance.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如前文所述，企业数据中心作为我们的 CV 用例的边缘位置。在这个边缘位置内部，我们有一些摄像头，连接到计算设备或边缘服务器，运行 CV 模型。由于我们没有访问企业数据中心，在本书的背景下，我们将使用
    **弹性计算云** （**EC2**） 实例来模拟构建边缘环境。
- en: Note
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'If you are not accustomed to working with EC2 instances, you can familiarize
    yourself with them by referencing the following documentation: [https://aws.amazon.com/ec2/getting-started/](https://aws.amazon.com/ec2/getting-started/).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不习惯使用 EC2 实例，您可以通过参考以下文档来熟悉它们：[https://aws.amazon.com/ec2/getting-started/](https://aws.amazon.com/ec2/getting-started/)。
- en: 'The following steps will demonstrate how to set up the edge server using an
    EC2 instance and configure the necessary Greengrass software as well as the required
    security permissions. Let’s get started with setting up the appropriate **Identity
    and Access Management** (**IAM**) roles and permissions:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将演示如何使用 EC2 实例设置边缘服务器，并配置必要的 Greengrass 软件以及所需的安全权限。让我们开始设置适当的 **身份和访问管理**
    （**IAM**） 角色和权限：
- en: Log into your AWS account and open the IAM console ([https://console.aws.amazon.com/iam/home](https://console.aws.amazon.com/iam/home)).
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录您的 AWS 账户并打开 IAM 控制台 ([https://console.aws.amazon.com/iam/home](https://console.aws.amazon.com/iam/home))。
- en: Once the IAM console is open, use the left-hand navigation panel and click on
    **Roles** to open the **Roles** dashboard. Then click on the **Create role** button
    in the top-right corner.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦打开 IAM 控制台，请使用左侧导航面板并点击 **角色** 以打开 **角色** 仪表板。然后点击右上角的 **创建角色** 按钮。
- en: Once the **Create role** wizard starts, select **AWS Service** as the **Trusted**
    **entity** type.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦启动 **创建角色** 向导，请选择 **AWS 服务** 作为 **受信任** **实体** 类型。
- en: For the **Use case**, select **EC2** and click on the **Next** button.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 **用例**，选择 **EC2** 并点击 **下一步** 按钮。
- en: On the **Add permissions** page, click on the **Create policy** button in the
    top right to open the **Create** **policy** page.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **添加权限** 页面上，点击右上角的 **创建策略** 按钮以打开 **创建** **策略** 页面。
- en: On the `SageMakerGreenGrassV2MinimalResourcePolicy.json` file in GitHub ([https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/blob/main/Chapter08/SageMakerGreenGrassV2MinimalResourcePolicy.json](https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/blob/main/Chapter08/SageMakerGreenGrassV2MinimalResourcePolicy.json)),
    and make sure to update the `<account_id>` tag in the policy with your AWS account
    ID.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在GitHub上的`SageMakerGreenGrassV2MinimalResourcePolicy.json`文件（[https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/blob/main/Chapter08/SageMakerGreenGrassV2MinimalResourcePolicy.json](https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/blob/main/Chapter08/SageMakerGreenGrassV2MinimalResourcePolicy.json)），并确保将策略中的`<account_id>`标签更新为您的AWS账户ID。
- en: Click on the **Next:** **Tags** button.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**下一步：****标签**按钮。
- en: Click on the **Next:** **Review** button.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**下一步：****审查**按钮。
- en: On the `SageMakerGreenGrassV2MinimalResourcePolicy`, and click the **Create**
    **policy** button.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`SageMakerGreenGrassV2MinimalResourcePolicy`上，然后点击**创建****策略**按钮。
- en: Go back to the **Add permission** page from *step 5*, and refresh the page to
    capture the newly created IAM policy.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从**步骤5**的**添加权限**页面返回，刷新页面以捕获新创建的IAM策略。
- en: Search for the `SageMakerGreenGrassV2MinimalResourcePolicy` policy in the search
    bar, and once found, select the checkbox for the policy to add the permission,
    then click the **Next** button.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在搜索栏中搜索`SageMakerGreenGrassV2MinimalResourcePolicy`策略，一旦找到，选择策略的复选框以添加权限，然后点击**下一步**按钮。
- en: On the `SageMakerGreenGrassV2MinimalResourceRole`, as the **Role name**.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`SageMakerGreenGrassV2MinimalResourceRole`上，作为**角色名称**。
- en: Once again, using the `SageMakerGreenGrassV2MinimalResourceRole` role to open
    the **Role** **summary** page.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次使用`SageMakerGreenGrassV2MinimalResourceRole`角色打开**角色****摘要**页面。
- en: Now, click the **Add permissions** dropdown and select the **Attach** **policies**
    option.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，点击**添加权限**下拉菜单并选择**附加****策略**选项。
- en: Search for the `AmazonSageMakerEdgeDeviceFleetPolicy` policy, and click on the
    checkbox to select this policy.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在搜索栏中搜索`AmazonSageMakerEdgeDeviceFleetPolicy`策略，并点击复选框以选择此策略。
- en: Repeat the process shown in *step 15*, except this time, select the checkbox
    for the **AmazonSageMakerFullAccess**, **AWSIoTLogging**, **AmazonS3FullAccess**,
    **AWSIoTRuleActions**, **AWSIoTThingsRegistration**, and **AmazonSSMManagedInstanceCore**
    policies.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复**步骤15**中显示的过程，但这次选择**AmazonSageMakerFullAccess**、**AWSIoTLogging**、**AmazonS3FullAccess**、**AWSIoTRuleActions**、**AWSIoTThingsRegistration**和**AmazonSSMManagedInstanceCore**策略的复选框。
- en: With these policies selected, click the **Attach** **policies** button.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择这些策略后，点击**附加****策略**按钮。
- en: In the `policy` option.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`策略`选项中。
- en: 'On the **Create policy** page, click the **JSON** tab and add the following
    policy statement:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**创建策略**页面，点击**JSON**选项卡并添加以下策略声明：
- en: '[PRE0]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Click the **Review** **policy** button.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**审查****策略**按钮。
- en: Name the policy `GreengrassComponentAccessPolicy` and click the **Create** **policy**
    button.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将策略命名为`GreengrassComponentAccessPolicy`，然后点击**创建****策略**按钮。
- en: In the **Role summary** page, click the **Trust relationships** tab and click
    the **Edit trust** **policy** button.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**角色摘要**页面，点击**信任关系**选项卡，然后点击**编辑信任****策略**按钮。
- en: 'Replace the existing policy with the following trust policy statement:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用以下信任策略声明替换现有的策略：
- en: '[PRE14]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now that we have set up the necessary permissions, next we can configure the
    edge server with the following steps showing us how to configure the EC2 instance:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置了必要的权限，接下来我们可以按照以下步骤配置边缘服务器，这些步骤展示了如何配置EC2实例：
- en: Before creating the EC2 instance, we need to configure the necessary scripts
    that customize the EC2 instance as an edge server. To provide easy command line
    access to AWS resources, open the **AWS CloudShell** console ([https://console.aws.amazon.com/cloudshell/home](https://console.aws.amazon.com/cloudshell/home)).
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在创建EC2实例之前，我们需要配置必要的脚本，以将EC2实例定制为边缘服务器。为了提供对AWS资源的简单命令行访问，打开**AWS CloudShell**控制台（[https://console.aws.amazon.com/cloudshell/home](https://console.aws.amazon.com/cloudshell/home)）。
- en: 'Once the CloudShell console has been initialized in the browser, clone the
    companion GitHub repository by running the following command:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦浏览器中的CloudShell控制台初始化完成，通过运行以下命令克隆配套的GitHub仓库：
- en: '[PRE40]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Next, we create an S3 bucket and store the configuration scripts for the EC2
    instance by running the following commands:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个S3存储桶，并通过运行以下命令存储EC2实例的配置脚本：
- en: '[PRE41]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Run the following command to capture the name of the S3 bucket containing the
    EC2 configuration scripts:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令以捕获包含EC2配置脚本的S3存储桶的名称：
- en: '[PRE44]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Note
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Make sure to remember the name of the S3 bucket containing the EC2 instance
    configuration scripts, as it will be used in a later step.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保记住包含 EC2 实例配置脚本的 S3 存储桶名称，因为它将在后续步骤中使用。
- en: Open the EC2 management console ([https://console.aws.amazon.com/ec2/v2/home](https://console.aws.amazon.com/ec2/v2/home))
    in a browser tab.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在浏览器标签页中打开 EC2 管理控制台 ([https://console.aws.amazon.com/ec2/v2/home](https://console.aws.amazon.com/ec2/v2/home))。
- en: Once the EC2 console is open, click the **Launch** **instance** button.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦打开 EC2 控制台，点击 **启动实例**按钮。
- en: In the `edge-server` as the name for this instance.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `edge-server` 作为此实例的名称。
- en: Select AMI as **amzn2-ami-kernel-5.10-hvm-2.0.20220426.0-x86_64-gp2** and instance
    type as **c5.large**.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 AMI 为 **amzn2-ami-kernel-5.10-hvm-2.0.20220426.0-x86_64-gp2**，实例类型为 **c5.large**。
- en: Scroll down to the `20` GiB for **Root volume**.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚动到 `20` GiB 的 **根卷**。
- en: Using the **IAM instance profile** drop-down box in the **Advance details**
    section, select the **SageMakerGreenGrassV2MinimalResourceRole** role.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **高级详情**部分的 **IAM 实例配置文件**下拉框中，选择 **SageMakerGreenGrassV2MinimalResourceRole**
    角色。
- en: 'In the **User data** text box of the **Advance details** section, paste the
    following Bash code:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **高级详情**部分的 **用户数据**文本框中，粘贴以下 Bash 代码：
- en: '[PRE45]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Note
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Make sure to replace the S3 bucket name with the name of your S3 bucket from
    the output of *step 4*.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 确保将 S3 存储桶名称替换为步骤 4 输出中的您的 S3 存储桶名称。
- en: Click the **Launch instance** button to create the EC2 instance.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **启动实例**按钮以创建 EC2 实例。
- en: Wait 10 minutes after the EC2 instance is in the active state before logging
    in to the EC2 instance, as the Bash script in the user data will need some time
    to install the necessary packages and build the Docker image for AWS IoT Greengrass
    software.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在登录到 EC2 实例之前，请等待 10 分钟，因为用户数据中的 Bash 脚本需要一些时间来安装必要的软件包并构建 AWS IoT Greengrass
    软件的 Docker 镜像。
- en: 'Log into the EC2 instance, and ensure you are in the `/home/ec2-user` directory.
    Using `env` file, and ensure the `AWS_REGION` variable is set to the current AWS
    Region being used. For example, the following output shows the `env` file configured
    for the `us-east-1` Region:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录到 EC2 实例，并确保您位于 `/home/ec2-user` 目录。使用 `env` 文件，并确保 `AWS_REGION` 变量设置为当前使用的
    AWS 区域。例如，以下输出显示了为 `us-east-1` 区域配置的 `env` 文件：
- en: '[PRE61]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Note
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You can also customize the `THING_NAME` and `THING_GROUP_NAME` parameters. However,
    make sure these variables are in lowercase.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以自定义 `THING_NAME` 和 `THING_GROUP_NAME` 参数。但是，请确保这些变量是小写。
- en: Save and exit the `env` file.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存并退出 `env` 文件。
- en: 'Run the Docker container by executing the following command:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行以下命令运行 Docker 容器：
- en: '[PRE70]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Note
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'In case you need to restart the Greengrass V2 Docker container, make sure to
    get the new credentials by running the following command:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要重新启动 Greengrass V2 Docker 容器，请确保通过运行以下命令获取新的凭据：
- en: '`$ rm credentials #remove the old` `credentials file`'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ rm credentials # 删除旧的` `credentials 文件`'
- en: '`$ sudo python3 getResourceTempCredentials.py # create credentials file with`
    `new credentials`'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ sudo python3 getResourceTempCredentials.py # 使用` `new credentials` 创建凭据文件`'
- en: '`$ ./``dockerRun.sh`'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ ./``dockerRun.sh`'
- en: 'Open another terminal window, log in to the EC2 instance, and run the following
    command to verify that Greengrass V2 is running and retrieve the container ID:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开另一个终端窗口，登录到 EC2 实例，并运行以下命令以验证 Greengrass V2 是否正在运行并检索容器 ID：
- en: '[PRE71]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'You can then run the following command to access the container and explore
    the AWS IoT Greengrass Core software running inside the container:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您可以运行以下命令来访问容器并探索容器内运行的 AWS IoT Greengrass Core 软件：
- en: '[PRE72]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Note
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When you use `docker exec` to run commands inside the Docker container, these
    are not captured in the Docker logs. As a best practice, we recommend you log
    your commands in the Docker logs so that you can look into the state of the Greengrass
    Docker container in case you need to troubleshoot any issues.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 当您使用 `docker exec` 在 Docker 容器内运行命令时，这些命令不会记录在 Docker 日志中。作为最佳实践，我们建议您将命令记录在
    Docker 日志中，以便在需要调试任何问题时可以查看 Greengrass Docker 容器的状态。
- en: Run the following command in a different terminal. It will attach your terminal’s
    input, output, and error to the container running currently. This will help you
    to view and control the container from your terminal.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在不同的终端中运行以下命令。它将您的终端的输入、输出和错误附加到当前运行的容器。这将帮助您从终端查看和控制容器。
- en: '[PRE73]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: By executing these steps, we have effectively configured an EC2 instance to
    run our ML model as an edge server. So, with the edge components of our architecture
    successfully built, we can move on to building the ML model.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行这些步骤，我们实际上已经配置了一个 EC2 实例来运行我们的机器学习模型作为边缘服务器。因此，随着我们架构的边缘组件成功构建，我们可以继续构建机器学习模型。
- en: Building the ML model
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建机器学习模型
- en: Building the ML model involves training an optimal ML model to suit our business
    use case. As we’ve seen in previous chapters, Amazon SageMaker provides us with
    distinct capabilities that allow us to ingest and process the necessary training
    data, as well as train and optimize the best ML model. Additionally, we saw that
    SageMaker also allows us to deploy and host these models in the cloud, and as
    we will see, SageMaker also allows us to deploy and manage ML models at the edge.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 构建机器学习模型涉及训练一个适合我们业务用例的最优机器学习模型。正如我们在前面的章节中看到的，Amazon SageMaker 为我们提供了独特的功能，使我们能够摄取和处理必要的训练数据，以及训练和优化最佳的机器学习模型。此外，我们还看到
    SageMaker 允许我们在云中部署和托管这些模型，而且正如我们将要看到的，SageMaker 还允许我们在边缘部署和管理机器学习模型。
- en: 'The following steps will walk you through how to build an ML model that suits
    our use case, compile the model for an edge environment, and then deploy the model
    to the edge, all using SageMaker:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将指导您如何构建一个适合我们用例的机器学习模型，为边缘环境编译模型，然后将模型部署到边缘，所有这些操作都使用 SageMaker：
- en: Within your AWS account, open the Amazon SageMaker management console ([https://console.aws.amazon.com/sagemaker/home](https://console.aws.amazon.com/sagemaker/home)).
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您的 AWS 账户中，打开 Amazon SageMaker 管理控制台 ([https://console.aws.amazon.com/sagemaker/home](https://console.aws.amazon.com/sagemaker/home))。
- en: Launch the SageMaker Studio IDE.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动 SageMaker Studio IDE。
- en: Note
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on how to create and launch the SageMaker Studio IDE, please
    refer to the *Setting up EMR and SageMaker Studio* section of [*Chapter 5*](B18493_05.xhtml#_idTextAnchor095),
    *Data Analysis*.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 关于如何创建和启动 SageMaker Studio IDE 的更多信息，请参阅 *第 5 章* 的 *设置 EMR 和 SageMaker Studio*
    部分，*数据分析*。
- en: Once the SageMaker Studio IDE has launched, use the `Applied-Machine-Learning-and-High-Performance-Computing-on-AWS`
    folder to expand it.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦 SageMaker Studio IDE 启动，使用 `Applied-Machine-Learning-and-High-Performance-Computing-on-AWS`
    文件夹展开它。
- en: Then, double-click on the `Chapter_8` folder to open it for browsing.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，双击 `Chapter_8` 文件夹以打开它进行浏览。
- en: Double-click on the `sagemaker_notebook` folder, and then launch the `1_compile_resnet_model_egde_manager.ipynb`
    notebook.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 双击 `sagemaker_notebook` 文件夹，然后启动 `1_compile_resnet_model_egde_manager.ipynb`
    笔记本。
- en: Once the notebook has started, use the menu bar to select the **Kernel** menu
    and then the **Restart Kernel and Run All** **Cells…** option.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦笔记本启动，使用菜单栏选择 **Kernel** 菜单，然后选择 **Restart Kernel and Run All Cells…** 选项。
- en: 'Once the notebook has run, we will have our image classification model running
    on the edge server and managed as part of a fleet. Nonetheless, let’s verify this
    by reviewing some of the important code cells in the notebook. The first part
    of the notebook downloads an already optimized or pre-trained `sagemaker_model`:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本运行完毕后，我们的图像分类模型将在边缘服务器上运行，并作为机群的一部分进行管理。尽管如此，让我们通过回顾笔记本中的一些重要代码单元来验证这一点。笔记本的第一部分下载了一个已经优化或预训练的
    `sagemaker_model`：
- en: '[PRE74]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'With the model object defined, we then use SageMaker Neo ([https://docs.aws.amazon.com/sagemaker/latest/dg/neo.html](https://docs.aws.amazon.com/sagemaker/latest/dg/neo.html))
    to compile a model that suites the specific compute architecture of the edge device,
    in this case, our X86_64 Linux edge server:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义了模型对象之后，我们使用 SageMaker Neo ([https://docs.aws.amazon.com/sagemaker/latest/dg/neo.html](https://docs.aws.amazon.com/sagemaker/latest/dg/neo.html))
    编译一个适合边缘设备特定计算架构的模型，在这种情况下，是我们的 X86_64 Linux 边缘服务器：
- en: '[PRE75]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Note
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Make sure to take note of the S3 path for the compiled model, as we will be
    using this path to deploy the model package.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保注意编译模型的 S3 路径，因为我们将会使用这个路径来部署模型包。
- en: 'After compiling the model for the edge server, we verify the model’s functionality
    by deploying it as a SageMaker-hosted endpoint, and then using the following code
    to generate a sample inference for a test image:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在为边缘服务器编译了模型之后，我们通过将其作为 SageMaker 托管的端点部署来验证模型的功能，然后使用以下代码为测试图像生成一个样本推理：
- en: '[PRE76]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Once we’ve verified that the model functions correctly, essentially being able
    to classify the image correctly, we can then package the model as shown in the
    following code snippet:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们验证了模型能够正确地工作，即能够正确地分类图像，我们就可以按照以下代码片段所示打包模型：
- en: '[PRE77]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Finally, once the model has been packaged, we can use the `create_device_fleet()`
    method to create a manageable fleet of edge devices to host the newly compiled
    ML model and then use the `register_device()` method to initialize our EC2 edge
    server as a registered or managed edge device that runs our ML model:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一旦模型打包完成，我们可以使用`create_device_fleet()`方法创建一个可管理的边缘设备编队来托管新编译的ML模型，然后使用`register_device()`方法初始化我们的EC2边缘服务器作为已注册或管理的边缘设备，该设备运行我们的ML模型：
- en: '[PRE78]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: Once the model has been trained and compiled and the edge server registered
    as a SageMaker-managed edge device, we can go ahead and deploy the model package
    to the edge server.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型经过训练和编译，并将边缘服务器注册为SageMaker管理的边缘设备，我们就可以继续部署模型包到边缘服务器。
- en: Deploying the model package
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署模型包
- en: 'To deploy the model package to the edge server, we will register it as a Greengrass
    component and then deploy the component to the edge server using the Greengrass
    console. The following steps will walk us through how to do this:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 要将模型包部署到边缘服务器，我们将将其注册为Greengrass组件，然后使用Greengrass控制台将组件部署到边缘服务器。以下步骤将指导我们如何完成此操作：
- en: Using a web browser, open the AWS IoT management console ([https://console.aws.amazon.com/iot/home](https://console.aws.amazon.com/iot/home)).
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用网页浏览器打开AWS IoT管理控制台（[https://console.aws.amazon.com/iot/home](https://console.aws.amazon.com/iot/home)）。
- en: In the **Manage** section of the left-hand navigation panel, expand the **Greengrass
    devices** option and click on **Components**.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在左侧导航面板的**管理**部分，展开**Greengrass设备**选项，然后点击**组件**。
- en: Click on the **Create** **component** button.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**创建** **组件**按钮。
- en: After the `com.greengrass.SageMakerEdgeManager.ImageClassification.Model.json`
    file contents into the **Recipe** box.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`com.greengrass.SageMakerEdgeManager.ImageClassification.Model.json`文件内容输入到**配方**框中。
- en: Note
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The `com.greengrass.SageMakerEdgeManager.ImageClassification.Model.json` file
    can be found in the companion GitHub repository ([https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/blob/main/Chapter08/custom_component_recipes/com.greengrass.SageMakerEdgeManager.ImageClassification.Model.json](https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/blob/main/Chapter08/custom_component_recipes/com.greengrass.SageMakerEdgeManager.ImageClassification.Model.json)).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '`com.greengrass.SageMakerEdgeManager.ImageClassification.Model.json`文件可以在配套的GitHub仓库中找到（[https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/blob/main/Chapter08/custom_component_recipes/com.greengrass.SageMakerEdgeManager.ImageClassification.Model.json](https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/blob/main/Chapter08/custom_component_recipes/com.greengrass.SageMakerEdgeManager.ImageClassification.Model.json)）。'
- en: Update the S3 location of the packaged model under the `Artifacts` tag of the
    JSON file to match the S3 path for the compiled model.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新JSON文件中`Artifacts`标签下打包模型的S3位置，以匹配编译模型的S3路径。
- en: Note
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The S3 location of the packaged model is the output from the `create_compilation_job()`
    method used in the previous section.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 打包模型的S3位置是上一节中使用的`create_compilation_job()`方法的输出。
- en: Click on the **Create** **component** button.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**创建** **组件**按钮。
- en: 'Go to your S3 bucket and create the `artifacts` folder, and inside it, create
    another folder with the name `com.greengrass.SageMakerEdgeManager.ImageClassification`.
    Your S3 path should look like this: `s3://<bucket_name>/artifacts/com.greengrass.SageMakerEdgeManager.ImageClassification/`.'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往您的S3存储桶，创建`artifacts`文件夹，并在其中创建一个名为`com.greengrass.SageMakerEdgeManager.ImageClassification`的另一个文件夹。您的S3路径应如下所示：`s3://<bucket_name>/artifacts/com.greengrass.SageMakerEdgeManager.ImageClassification/`。
- en: Upload the `image_classification.zip` and `installer.sh` files from the GitHub
    repo ([https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter08](https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter08))
    to the S3 location defined in *step 7*.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从GitHub仓库（[https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter08](https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter08)）上传`image_classification.zip`和`installer.sh`文件到第7步中定义的S3位置。
- en: Update the S3 location of the `image_classification.zip` and `installer.sh`
    files under the `Artifacts` tag of the JSON file to match the S3 path defined
    in *step 8*.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新JSON文件中`Artifacts`标签下`image_classification.zip`和`installer.sh`文件的S3位置，以匹配第8步中定义的S3路径。
- en: Repeat *step 3* and *step 4* for the **com.greengrass.SageMakerEdgeManager.ImageClassification**
    component.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于**com.greengrass.SageMakerEdgeManager.ImageClassification**组件，重复第3步和第4步。
- en: Note
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The `com.greengrass.SageMakerEdgeManager.ImageClassification.json` file can
    be found in the companion GitHub repository ([https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/blob/main/Chapter08/custom_component_recipes/com.greengrass.SageMakerEdgeManager.ImageClassification.json](https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/blob/main/Chapter08/custom_component_recipes/com.greengrass.SageMakerEdgeManager.ImageClassification.json)).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '`com.greengrass.SageMakerEdgeManager.ImageClassification.json` 文件可以在配套的 GitHub
    仓库([https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/blob/main/Chapter08/custom_component_recipes/com.greengrass.SageMakerEdgeManager.ImageClassification.json](https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/blob/main/Chapter08/custom_component_recipes/com.greengrass.SageMakerEdgeManager.ImageClassification.json))中找到。'
- en: 'With the Greengrass components registered, we can now deploy them to the edge
    server and run image classification inference using the components that we just
    created. Deploying these components downloads a SageMaker Neo-compiled pre-trained
    ML model and installs the SageMaker Edge Manager agent on the edge server. However,
    before we can actually perform the deployment of these components, we have to
    subscribe to the notification topic in order to view and manage any inference
    tasks on the edge server. The following steps will walk us through how to subscribe
    to a topic:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在注册了 Greengrass 组件后，我们现在可以将它们部署到边缘服务器，并使用我们刚刚创建的组件运行图像分类推理。部署这些组件将下载 SageMaker
    Neo 编译的预训练 ML 模型，并在边缘服务器上安装 SageMaker Edge Manager 代理。然而，在我们实际部署这些组件之前，我们必须订阅通知主题，以便查看和管理边缘服务器上的任何推理任务。以下步骤将指导我们如何订阅主题：
- en: Go to the AWS IoT console ([https://console.aws.amazon.com/iot/](https://console.aws.amazon.com/iot/))
    and click on **MQTT test client** from the **Test** option of the left-hand navigation
    panel.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往 AWS IoT 控制台([https://console.aws.amazon.com/iot/](https://console.aws.amazon.com/iot/))，然后在左侧导航面板的
    **测试** 选项中点击 **MQTT 测试客户端**。
- en: In the `gg/sageMakerEdgeManager/image-classification`.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `gg/sageMakerEdgeManager/image-classification`。
- en: Click the **Subscribe** button.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **订阅** 按钮。
- en: With the ability to monitor and manage any inference requests to the ML model
    running on the edge server in place, we can deploy the Greengrass components.
    The following steps will show us how to do this (they are also highlighted in
    the AWS Greengrass Developer guide at [https://docs.aws.amazon.com/greengrass/v2/developerguide/greengrass-v2-developer-guide.pdf):](https://docs.aws.amazon.com/greengrass/v2/developerguide/greengrass-v2-developer-guide.pdf):)
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 由于可以监控和管理运行在边缘服务器上的 ML 模型的任何推理请求，我们可以部署 Greengrass 组件。以下步骤将展示如何进行此操作（这些步骤也在
    AWS Greengrass 开发者指南[https://docs.aws.amazon.com/greengrass/v2/developerguide/greengrass-v2-developer-guide.pdf](https://docs.aws.amazon.com/greengrass/v2/developerguide/greengrass-v2-developer-guide.pdf)中突出显示）：）
- en: In the AWS IoT Greengrass console ([https://console.aws.amazon.com/greengrass](https://console.aws.amazon.com/greengrass))
    navigation menu, choose **Deployments**, and then choose the deployment for the
    target device.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 AWS IoT Greengrass 控制台([https://console.aws.amazon.com/greengrass](https://console.aws.amazon.com/greengrass))
    导航菜单中，选择 **部署**，然后选择目标设备的部署。
- en: On the deployment page, choose **Revise** and then choose **Revise deployment**.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在部署页面，选择 **修订**，然后选择 **修订部署**。
- en: On the **Specify target** page, click **Next**.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **指定目标** 页面，点击 **下一步**。
- en: Under the **My components** option of the **Select components** page, select
    both the **com.greengrass.SageMakerEdgeManager.ImageClassification**, and **com.greengrass.SageMakerEdgeManager.ImageClassification.Model**
    components.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **选择组件** 页面的 **我的组件** 选项下，选择 **com.greengrass.SageMakerEdgeManager.ImageClassification**
    和 **com.greengrass.SageMakerEdgeManager.ImageClassification.Model** 组件。
- en: Under **Public components**, turn off the **Show only selected components**
    toggle, and then select the **aws.greengrass.SageMakerEdgeManager** component.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **公共组件** 下，关闭 **仅显示所选组件** 切换，然后选择 **aws.greengrass.SageMakerEdgeManager**
    组件。
- en: Click **Next**.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **下一步**。
- en: On the **Configure components** page, select the **aws.greengrass.SageMakerEdgeManager**
    component, and choose **Configure component**.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **配置组件** 页面，选择 **aws.greengrass.SageMakerEdgeManager** 组件，并选择 **配置组件**。
- en: 'Under **Configuration update**, in **Configuration to merge**, enter the following
    configuration:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **配置更新** 下的 **要合并的配置** 中，输入以下配置：
- en: '[PRE79]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Note
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Replace `device-fleet-name` and the S3 bucket name with the corresponding values
    you created when creating the device fleet in the notebook.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `device-fleet-name` 和 S3 存储桶名称替换为在笔记本中创建设备编队时创建的相应值。
- en: Choose **Confirm**, and then choose **Next**.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**确认**，然后选择**下一步**。
- en: On the **Configure advanced settings** page, keep the default configuration
    settings and choose **Next**.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**配置高级设置**页面，保持默认配置设置并选择**下一步**。
- en: On the **Review** page, choose **Deploy**.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**审查**页面，选择**部署**。
- en: 'The deployment can take several minutes to complete. After the components have
    been deployed, we can view, manage, and monitor the ML model inference results
    in the component log of the Greengrass Core device, as well as in the AWS IoT
    MQTT client of the AWS IoT console. To view the inference results in the component
    log of the Greengrass Core device, log into the edge server EC2 instance and run
    the following command:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 部署可能需要几分钟才能完成。组件部署完成后，我们可以在Greengrass Core设备的组件日志以及AWS IoT控制台的AWS IoT MQTT客户端中查看、管理和监控机器学习模型推理结果。要查看Greengrass
    Core设备的组件日志中的推理结果，请登录到边缘服务器EC2实例并运行以下命令：
- en: '[PRE83]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'You should see `Top 5 predictions with score 0.3 or above` in the logs similar
    to the one shown here:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该在日志中看到类似于以下内容的“得分0.3或以上的前5个预测”：
- en: '[PRE84]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Alternatively, you can also view the results in **MQTT test client** on the
    AWS IoT console ([https://console.aws.amazon.com/iot/](https://console.aws.amazon.com/iot/))
    by clicking **MQTT test client** from the **Test** option on the left-hand navigation
    panel. In the **Subscriptions** section, you will see the prediction results,
    as shown in the following screenshot:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您也可以通过点击左侧导航面板上的**测试**选项中的**MQTT测试客户端**在AWS IoT控制台上查看结果（[https://console.aws.amazon.com/iot/](https://console.aws.amazon.com/iot/)）。在**订阅**部分，您将看到预测结果，如下面的截图所示：
- en: '![Figure 8.2 – Inference results on MQTT test client on AWS IoT console](img/B18493_08_002.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![图8.2 – 在AWS IoT控制台MQTT测试客户端上的推理结果](img/B18493_08_002.jpg)'
- en: Figure 8.2 – Inference results on MQTT test client on AWS IoT console
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 – 在AWS IoT控制台MQTT测试客户端上的推理结果
- en: 'If you can’t see inference results in the MQTT client, the deployment might
    have failed or it did not reach the core device. This can occur primarily due
    to two reasons: your core device is not connected to the network, or it doesn’t
    have the right permissions to execute the component. To troubleshoot it, you can
    run the following command on your core device. This command will open the AWS
    IoT Greengrass Core software log file, which includes logs from the Greengrass
    Core device’s deployment service.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在MQTT客户端看不到推理结果，部署可能已失败或未达到核心设备。这主要可能由两个原因造成：您的核心设备未连接到网络，或者它没有执行组件的正确权限。要排查问题，您可以在核心设备上运行以下命令。此命令将打开AWS
    IoT Greengrass Core软件日志文件，其中包含Greengrass Core设备部署服务的日志。
- en: '[PRE85]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: Note
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'For more information, see the troubleshooting documentation: [https://docs.aws.amazon.com/greengrass/v2/developerguide/ml-troubleshooting.html](https://docs.aws.amazon.com/greengrass/v2/developerguide/ml-troubleshooting.html).'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 如需更多信息，请参阅故障排除文档：[https://docs.aws.amazon.com/greengrass/v2/developerguide/ml-troubleshooting.html](https://docs.aws.amazon.com/greengrass/v2/developerguide/ml-troubleshooting.html)。
- en: With the Greengrass components now deployed to the edge server, we have successfully
    deployed our ML model to the edge. Furthermore, by leveraging the capabilities
    of AWS IoT Greengrass, as well as Amazon SageMaker, we have not only compiled
    the ML to function *efficiently* and ensure *performance* on the edge device but
    also established a mechanism to *manage* and *monitor* the environment. As you
    will recall from the *Reviewing the key considerations for optimal edge deployments*
    section, these are the key factors that make up an optimal edge architecture.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 现在Greengrass组件已部署到边缘服务器，我们已经成功将我们的机器学习模型部署到边缘。此外，通过利用AWS IoT Greengrass和Amazon
    SageMaker的能力，我们不仅使机器学习在边缘设备上**高效**运行并确保**性能**，还建立了一种**管理**和**监控**环境的机制。正如您在**回顾最佳边缘部署的关键考虑因素**部分所回忆的那样，这些是构成最佳边缘架构的关键因素。
- en: Summary
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced you to the concept of deploying ML models outside
    of the cloud, primarily on an edge architecture. To lay the foundation for how
    to accomplish an edge deployment, we also examined what an edge architecture is,
    as well as the most important factors that need to be considered when designing
    an edge architecture, namely efficiency, performance, and reliability.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们向您介绍了将机器学习模型部署在云之外的概念，主要是边缘架构。为了奠定边缘部署的基础，我们还探讨了边缘架构是什么，以及在设计边缘架构时需要考虑的最重要因素，即效率、性能和可靠性。
- en: With these factors in mind, we explored how the AWS IoT Greengrass, as well
    as Amazon SageMaker services, can be used to build an optimal ML model package
    in the cloud, compiled to run efficiently on an edge device, and then deployed
    to the edge environment, in a reliable manner. In doing so, we also highlighted
    just how crucial the ability to manage and monitor both the edge devices, as well
    as the deployed ML models is to create an optimal edge architecture.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些因素，我们探讨了如何使用AWS IoT Greengrass以及Amazon SageMaker服务在云中构建一个最优的机器学习模型包，编译后能够在边缘设备上高效运行，并且以可靠的方式部署到边缘环境中。在这个过程中，我们还强调了管理和监控边缘设备以及部署的机器学习模型对于创建最优边缘架构的重要性。
- en: In the next chapter, we will continue along the lines of performance monitoring
    and optimization of deployed ML models.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续探讨部署的机器学习模型的性能监控和优化。
