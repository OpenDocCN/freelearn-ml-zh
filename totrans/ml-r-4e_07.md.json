["```py\n> concrete <- read.csv(\"concrete.csv\")\n> str(concrete) \n```", "```py\n'data.frame': 1030 obs. of  9 variables:\n $ cement      : num  141 169 250 266 155 ...\n $ slag        : num  212 42.2 0 114 183.4 ...\n $ ash         : num  0 124.3 95.7 0 0 ...\n $ water       : num  204 158 187 228 193 ...\n $ superplastic: num  0 10.8 5.5 0 9.1 0 0 6.4 0 9 ...\n $ coarseagg   : num  972 1081 957 932 1047 ...\n $ fineagg     : num  748 796 861 670 697 ...\n $ age         : int  28 14 28 28 28 90 7 56 28 28 ...\n $ strength    : num  29.9 23.5 29.2 45.9 18.3 ... \n```", "```py\n> normalize <- function(x) {\n    return((x - min(x)) / (max(x) - min(x)))\n} \n```", "```py\n> concrete_norm <- as.data.frame(lapply(concrete, normalize)) \n```", "```py\n> summary(concrete_norm$strength) \n```", "```py\n Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.2664  0.4001  0.4172  0.5457  1.0000 \n```", "```py\n> summary(concrete$strength) \n```", "```py\n Min.  1st Qu.  Median    Mean   3rd Qu.    Max. \n   2.33   23.71    34.45   35.82    46.13     82.60 \n```", "```py\n> concrete_train <- concrete_norm[1:773, ]\n> concrete_test <- concrete_norm[774:1030, ] \n```", "```py\n> set.seed(12345)\n> concrete_model <- neuralnet(strength ~ cement + slag +\n  ash + water + superplastic + coarseagg + fineagg + age,\n  data = concrete_train) \n```", "```py\n> plot(concrete_model) \n```", "```py\n> model_results <- compute(concrete_model, concrete_test[1:8]) \n```", "```py\n> predicted_strength <- model_results$net.result \n```", "```py\n> cor(predicted_strength, concrete_test$strength) \n```", "```py\n [,1]\n[1,] 0.8064656 \n```", "```py\n> set.seed(12345)\n> concrete_model2 <- neuralnet(strength ~ cement + slag +\n                               ash + water + superplastic +\n                               coarseagg + fineagg + age,\n                               data = concrete_train, hidden = 5) \n```", "```py\n> plot(concrete_model2) \n```", "```py\n> model_results2 <- compute(concrete_model2, concrete_test[1:8])\n> predicted_strength2 <- model_results2$net.result\n> cor(predicted_strength2, concrete_test$strength) \n```", "```py\n [,1]\n[1,] 0.9244533426 \n```", "```py\n> softplus <- function(x) { log(1 + exp(x)) } \n```", "```py\n> set.seed(12345)\n> concrete_model3 <- neuralnet(strength ~ cement + slag +\n                               ash + water + superplastic +\n                               coarseagg + fineagg + age,\n                               data = concrete_train,\n                               hidden = c(5, 5),\n                               act.fct = softplus) \n```", "```py\n> plot(concrete_model3) \n```", "```py\n> model_results3 <- compute(concrete_model3, concrete_test[1:8])\n> predicted_strength3 <- model_results3$net.result\n> cor(predicted_strength3, concrete_test$strength) \n```", "```py\n [,1]\n[1,] 0.9348395359 \n```", "```py\n> strengths <- data.frame(\n    actual = concrete$strength[774:1030],\n    pred = predicted_strength3\n  ) \n```", "```py\n> head(strengths, n = 3)\n    actual         pred\n774  30.14 0.2860639091\n775  44.40 0.4777304648\n776  24.50 0.2840964250 \n```", "```py\n> cor(strengths$pred, strengths$actual) \n```", "```py\n[1] 0.9348395 \n```", "```py\n> cor(strengths$pred, concrete_test$strength) \n```", "```py\n[1] 0.9348395 \n```", "```py\n> unnormalize <- function(x) {\n    return(x * (max(concrete$strength) -\n            min(concrete$strength)) + min(concrete$strength))\n} \n```", "```py\n> strengths$pred_new <- unnormalize(strengths$pred)\n> strengths$error_pct <- (strengths$pred_new - strengths$actual) / \n                            strengths$actual \n```", "```py\n> head(strengths, n = 3)\n    actual      pred pred_new  error_pct\n774  30.14 0.2860639 25.29235 -0.16083776\n775  44.40 0.4777305 40.67742 -0.08384179\n776  24.50 0.2840964 25.13442 -0.02589470 \n```", "```py\n> cor(strengths$pred_new, strengths$actual) \n```", "```py\n[1] 0.9348395 \n```", "```py\n> letters <- read.csv(\"letterdata.csv\", stringsAsFactors = TRUE)\n> str(letters) \n```", "```py\n'data.frame':\t20000 obs. of  17 variables:\n $ letter: Factor w/ 26 levels \"A\",\"B\",\"C\",\"D\",..: 20 9 4 14 ...\n $ xbox  : int  2 5 4 7 2 4 4 1 2 11 ...\n $ ybox  : int  8 12 11 11 1 11 2 1 2 15 ...\n $ width : int  3 3 6 6 3 5 5 3 4 13 ...\n $ height: int  5 7 8 6 1 8 4 2 4 9 ...\n $ onpix : int  1 2 6 3 1 3 4 1 2 7 ...\n $ xbar  : int  8 10 10 5 8 8 8 8 10 13 ...\n $ ybar  : int  13 5 6 9 6 8 7 2 6 2 ...\n $ x2bar : int  0 5 2 4 6 6 6 2 2 6 ...\n $ y2bar : int  6 4 6 6 6 9 6 2 6 2 ...\n $ xybar : int  6 13 10 4 6 5 7 8 12 12 ...\n $ x2ybar: int  10 3 3 4 5 6 6 2 4 1 ...\n $ xy2bar: int  8 9 7 10 9 6 6 8 8 9 ...\n $ xedge : int  0 2 3 6 1 0 2 1 1 8 ...\n $ xedgey: int  8 8 7 10 7 8 8 6 6 1 ...\n $ yedge : int  0 4 3 2 5 9 7 2 1 1 ...\n $ yedgex: int  8 10 9 8 10 7 10 7 7 8 ... \n```", "```py\n> letters_train <- letters[1:16000, ]\n> letters_test  <- letters[16001:20000, ] \n```", "```py\n> library(kernlab)\n> letter_classifier <- ksvm(letter ~ ., data = letters_train,\n                            kernel = \"vanilladot\") \n```", "```py\n> letter_classifier \n```", "```py\nSupport Vector Machine object of class \"ksvm\"\nSV type: C-svc  (classification)\n parameter : cost C = 1\nLinear (vanilla) kernel function.\nNumber of Support Vectors : 7037\nObjective Function Value : -14.1746 -20.0072 -23.5628 -6.2009 -7.5524\n-32.7694 -49.9786 -18.1824 -62.1111 -32.7284 -16.2209...\nTraining error : 0.130062 \n```", "```py\n> letter_predictions <- predict(letter_classifier, letters_test) \n```", "```py\n> head(letter_predictions) \n```", "```py\n[1] U N V X N H\nLevels: A B C D E F G H I J K L M N O P Q R S T U V W X Y Z \n```", "```py\n> table(letter_predictions, letters_test$letter) \n```", "```py\nletter_predictions   A   B   C   D   E\n                 A 144   0   0   0   0\n                 B   0 121   0   5   2\n                 C   0   0 120   0   4\n                 D   2   2   0 156   0\n                 E   0   0   5   0 127 \n```", "```py\n> agreement <- letter_predictions == letters_test$letter \n```", "```py\n> table(agreement) \n```", "```py\nagreement\nFALSE  TRUE\n643 3357 \n```", "```py\n> prop.table(table(agreement)) \n```", "```py\nagreement\n  FALSE    TRUE\n0.16075 0.83925 \n```", "```py\n> set.seed(12345)\n> letter_classifier_rbf <- ksvm(letter ~ ., data = letters_train,\n                                kernel = \"rbfdot\") \n```", "```py\n> letter_predictions_rbf <- predict(letter_classifier_rbf,\n                                    letters_test) \n```", "```py\n> agreement_rbf <- letter_predictions_rbf == letters_test$letter\n> table(agreement_rbf) \n```", "```py\nagreement_rbf\nFALSE  TRUE\n  278  3722 \n```", "```py\n> prop.table(table(agreement_rbf)) \n```", "```py\nagreement_rbf\n  FALSE    TRUE\n0.0695 0.9305 \n```", "```py\n> cost_values <- c(1, seq(from = 5, to = 40, by = 5))\n> accuracy_values <- sapply(cost_values, function(x) {\n    set.seed(12345)\n    m <- ksvm(letter ~ ., data = letters_train,\n              kernel = \"rbfdot\", C = x)\n    pred <- predict(m, letters_test)\n    agree <- ifelse(pred == letters_test$letter, 1, 0)\n    accuracy <- sum(agree) / nrow(letters_test)\n    return (accuracy)\n  })\n> plot(cost_values, accuracy_values, type = \"b\") \n```"]