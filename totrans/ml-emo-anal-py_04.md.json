["```py\n    <w c5=NN1Â Â hw=factsheetÂ Â pos= SUBST >FACTSHEET </w>\n    ```", "```py\n    <w c5=DTQÂ Â hw=whatÂ Â pos= PRON >WHAT </w>\n    ```", "```py\n1Â Â Â whatÂ Â Â Â whatÂ Â Â Â PRONÂ Â Â Â PronType=Int,RelÂ Â 0Â Â Â Â Â Â Â rootÂ Â Â Â _2Â Â Â isÂ Â Â Â Â Â beÂ Â Â Â Â Â AUXÂ Â Â Â Â Mood=IndÂ Â Â Â Â Â Â Â Â Â 1Â Â Â Â Â Â Â copÂ Â Â Â Â _\n```", "```py\ndef reader(path, dataFileReader, pattern=re.compile('.*')):Â Â Â Â if isinstance(pattern, str):\nÂ Â Â Â Â Â Â Â pattern = re.compile(pattern)\nÂ Â Â Â if isinstance(path, list):\nÂ Â Â Â Â Â Â Â # If what you're looking at is a list of file names,\nÂ Â Â Â Â Â Â Â # look inside it and return the things you find there\nÂ Â Â Â Â Â Â Â for f in path:\nÂ Â Â Â Â Â Â Â Â Â Â Â for r in reader(f, dataFileReader, pattern=pattern):\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â yield r\nÂ Â Â Â elif os.path.isdir(path):\nÂ Â Â Â Â Â Â Â # If what you're looking at is a directory,\nÂ Â Â Â Â Â Â Â # look inside it and return the things you find there\nÂ Â Â Â Â Â Â Â for f in sorted(os.listdir(path)):\nÂ Â Â Â Â Â Â Â Â Â Â Â for r in reader(os.path.join(path, f),\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â dataFileReader, pattern=pattern):\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â yield r\nÂ Â Â Â else:\nÂ Â Â Â Â Â Â Â # If it's a datafile, check that its name matches the pattern\nÂ Â Â Â Â Â Â Â # and then use the dataFileReader to extract what you want\nÂ Â Â Â Â Â Â Â if pattern.match(path):\nÂ Â Â Â Â Â Â Â Â Â Â Â for r in dataFileReader(path):\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â yield r\n```", "```py\n>>> r = reader(BNC, BNCWordReader, pattern=r'.*\\.xml')>>> l = list(r)\n```", "```py\n<s n= 1 ><w c5= NN1Â Â hw= factsheetÂ Â pos= SUBST >FACTSHEET </w><w c5= DTQÂ Â hw= whatÂ Â pos= PRON >WHAT </w><w c5= VBZÂ Â hw= beÂ Â pos= VERB >IS </w><w c5= NN1Â Â hw= aidsÂ Â pos= SUBST >AIDS</w><c c5= PUN >?</c></s>\n```", "```py\nBNCWORD = re.compile('<(?P<tagtype>w|c).*?>(?P<form>.*?)\\s*</(?P=tagtype)>')Get raw text from BNC leaf files\ndef BNCWordReader(data):\nÂ Â Â Â for i in BNCWORD.finditer(open(data).read()):\nÂ Â Â Â Â Â Â Â yield i.group( form )\n```", "```py\nINPUT STRING: ÙÙŠLOOK-UP WORD: fy\nÂ Â Â Â Â Comment:\nÂ Â Â Â Â Â Â INDEX: P1W1\n* SOLUTION 1: (fiy) [fiy_1] fiy/PREP\nÂ Â Â Â Â (GLOSS): in\nÂ Â SOLUTION 2: (fiy~a) [fiy_1] fiy/PREP+ya/PRON_1S\nÂ Â Â Â Â (GLOSS): in + me\nÂ Â SOLUTION 3: (fiy) [fiy_2] Viy/ABBREV\nÂ Â Â Â Â (GLOSS): V.\nINPUT STRING: Ø³ÙˆØ³Ø©\nLOOK-UP WORD: swsp\nÂ Â Â Â Â Comment:\nÂ Â Â Â Â Â Â INDEX: P1W2\n* SOLUTION 1: (suwsap) [suws_1] suws/NOUN+ap/NSUFF_FEM_SG\nÂ Â Â Â Â (GLOSS): woodworm:licorice + [fem.sg.]\nÂ Â SOLUTION 2: (suwsapu) [suws_1] suws/NOUN+ap/NSUFF_FEM_SG+u/CASE_DEF_NOM\nÂ Â Â Â Â (GLOSS): woodworm:licorice + [fem.sg.] + [def.nom.]\nâ€¦\n```", "```py\nPATBWordPattern = re.compile(\"INPUT STRING: (?P<form>\\S*)\")def PATBWordReader(path):\nÂ Â Â Â for i in PATBWordPattern.finditer(open(path).read()):\nÂ Â Â Â Â Â Â Â yield i.group( form )\n```", "```py\n... Ù„ÙˆÙ†Øº Ø¨ÙŠØªØ´ ( Ø§Ù„ÙˆÙ„Ø§ÙŠØ§Øª Ø§Ù„Ù…ØªØ­Ø¯Ø© ) 15-7 ( Ø§Ù Ø¨\n```", "```py\ndef WordReader(pattern, path):Â Â Â Â for i in pattern.finditer(open(path).read()):\nÂ Â Â Â Â Â Â Â yield i.group( form )\nfrom functools import partial\nPATBWordReader = partial(WordReader, PATBWordPattern)\nBNCWordReader = partial(WordReader, BNCWordPattern)\n```", "```py\nmysql> describe sentiments;+-----------+--------------+------+-----+---------+-------+\n| FieldÂ Â Â Â Â | TypeÂ Â Â Â Â Â Â Â Â | Null | Key | Default | Extra |\n+-----------+--------------+------+-----+---------+-------+\n| annotator | int(11)Â Â Â Â Â Â | YESÂ Â |Â Â Â Â Â | NULLÂ Â Â Â |Â Â Â Â Â Â Â |\n| tweetÂ Â Â Â Â | int(11)Â Â Â Â Â Â | YESÂ Â |Â Â Â Â Â | NULLÂ Â Â Â |Â Â Â Â Â Â Â |\n| sentiment | varchar(255) | YESÂ Â |Â Â Â Â Â | NULLÂ Â Â Â |Â Â Â Â Â Â Â |\n+-----------+--------------+------+-----+---------+-------+\n3 rows in set (0.01 sec)\nmysql> select * from sentiments where tweet < 3;\n+-----------+-------+-----------------------+\n| annotator | tweet | sentimentÂ Â Â Â Â Â Â Â Â Â Â Â Â |\n+-----------+-------+-----------------------+\n|Â Â Â Â Â Â Â Â 19 |Â Â Â Â Â 1 | loveÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â |\n|Â Â Â Â Â Â Â Â Â 1 |Â Â Â Â Â 1 | anger+dissatisfaction |\n|Â Â Â Â Â Â Â Â Â 8 |Â Â Â Â Â 1 | anger+dissatisfaction |\n|Â Â Â Â Â Â Â Â Â 2 |Â Â Â Â Â 2 | love+joyÂ Â Â Â Â Â Â Â Â Â Â Â Â Â |\n|Â Â Â Â Â Â Â Â 19 |Â Â Â Â Â 2 | loveÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â |\n|Â Â Â Â Â Â Â Â Â 6 |Â Â Â Â Â 2 | love+joy+optimismÂ Â Â Â Â |\n+-----------+-------+-----------------------\n```", "```py\n>>> DB = MySQLdb.connect(db= centement , autocommit=True, charset= UTF8 )>>> cursor = DB.cursor()\n>>> data = pandas.read_sql( select * from sentiments where tweet < 3 , DB)\n>>> data\nÂ Â Â annotatorÂ Â tweetÂ Â Â Â Â Â Â Â Â Â Â Â Â Â sentiment\n0Â Â Â Â Â Â Â Â Â 19Â Â Â Â Â Â 1Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â love\n1Â Â Â Â Â Â Â Â Â Â 1Â Â Â Â Â Â 1Â Â anger+dissatisfaction\n2Â Â Â Â Â Â Â Â Â Â 8Â Â Â Â Â Â 1Â Â anger+dissatisfaction\n3Â Â Â Â Â Â Â Â Â Â 2Â Â Â Â Â Â 2Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â love+joy\n4Â Â Â Â Â Â Â Â Â 19Â Â Â Â Â Â 2Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â love\n5Â Â Â Â Â Â Â Â Â Â 6Â Â Â Â Â Â 2Â Â Â Â Â Â love+joy+optimism\n>>> data.to_json()\n'{ annotator :{ 0 :19, 1 :1, 2 :8, 3 :2, 4 :19, 5 :6}, tweet :{ 0 :1, 1 :1, 2 :1, 3 :2, 4 :2, 5 :2}, sentiment :{ 0 : love , 1 : anger+dissatisfaction , 2 : anger+dissatisfaction , 3 : love+joy , 4 : love , 5 : love+joy+optimism }}'\n>>> print(data.to_csv())\n,annotator,tweet,sentiment\n0,19,1,love\n1,1,1,anger+dissatisfaction\n2,8,1,anger+dissatisfaction\n3,2,2,love+joy\n4,19,2,love\n5,6,2,love+joy+optimism\n```", "```py\nIDÂ Â TweetÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â angerÂ Â sadnessÂ Â surprise0Â Â Â 2017-En-21441 Worry is a dowÂ Â Â Â Â 0Â Â Â Â Â 1Â Â Â Â Â Â Â Â Â Â 0\n1Â Â Â 2017-En-31535Â Â Whatever you dÂ Â Â Â 0Â Â Â Â Â 0Â Â Â Â Â Â Â Â Â Â 0\n2Â Â Â 2017-En-22190Â Â No but that'sÂ Â Â Â Â 0Â Â Â Â Â 0Â Â Â Â Â Â Â Â Â Â 1\n3Â Â Â 2017-En-20221Â Â Do you think hÂ Â Â Â 0Â Â Â Â Â 0Â Â Â Â Â Â Â Â Â Â 0\n4Â Â Â 2017-En-22180Â Â Rooneys effingÂ Â Â Â 1Â Â Â Â Â 0Â Â Â Â Â Â Â Â Â Â 0\n6830Â Â 2017-En-21383Â Â @nicky57672 HiÂ Â 0Â Â Â Â Â 0Â Â Â Â Â Â Â Â Â Â 0\n6831Â Â 2017-En-41441Â Â @andreamitchelÂ Â 0Â Â Â Â Â 0Â Â Â Â Â Â Â Â Â Â 1\n6832Â Â 2017-En-10886Â Â @isthataspiderÂ Â 0Â Â Â Â Â 1Â Â Â Â Â Â Â Â Â Â 0\n6833Â Â 2017-En-40662Â Â i wonder how aÂ Â 0Â Â Â Â Â 0Â Â Â Â Â Â Â Â Â Â 1\n6834Â Â 2017-En-31003Â Â I'm highly aniÂ Â 0Â Â Â Â Â 0Â Â Â Â Â Â Â Â Â Â 0\n```", "```py\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â as well as in a stock marketÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â in share prices in the stock market\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â to the harsher side of stock market life\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â apart from the stock market crash of two\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â There was a huge stockmarket crash in October\nÂ Â Â Â Â Â Â Â Â Â Â of being replaced in a sudden stockmarket coup\nÂ Â Â Â Â Â Â Â Â Â Â in the days that followed the stockmarket crash of October\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â The stockmarket crash of 1987 is\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â was to be a major battle ground between\nÂ Â Â Â Â Â Â Â Â Â Â of politics as an ideological battle ground and by her\nÂ Â Â Â Â Â Â Â Â Â Â Â Â likely to form the election battle ground\nÂ Â Â Â Â Â Â Â Â Â Â Â Â likely to form the election battle ground\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â surveying the battleground with quiet\nÂ Â Â Â Â Â Industry had become an ideological battleground\nÂ Â Â Â Â Â Â Â Â of London as a potential savage battleground is confirmed by\nÂ Â Â Â Â Â Â Â Â Â previous evening it had been a battleground for people who\n```", "```py\nÂ Â Â Â Â Â Â an example about lifting a heavy weight and doing aÂ Â Â Â Â Â Â Â Â Â Â Â Â Â I was n't lifting a heavy weight\nÂ Â Â Â Â Â Â Â Â Â Â Â 's roster there was a heavy weight of expectation for the\nÂ Â Â half-conscious levels he was a heavy weight upon me of a perhaps\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â the heavyweight boxing champion of the\nÂ Â Â Â Â Â up a stirring finale to the heavyweight contest\nÂ Â Â of the main contenders for the heavyweight Bath title\na former British and Commonwealth heavyweight boxing champion\nÂ Â a new sound broadcasting system under study\nÂ Â Â Â Â Â Â Â Â Â Â many of the plants now under study phase come to fruition '\nÂ Â Â Â to the Brazilian auto workers under study at that particular time\nÂ Â Â Â Â Â in Guanxi province has been under study since the 1950s\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â and Jack 's understudy can take over as the maid\nÂ Â Â to be considered as an England understudy for the Polish expedition\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â His Equity-required understudy received $800 per â€” \nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â will now understudy all positions along the\n```", "```py\nENGLISHPATTERN = re.compile(r\"\"\"(?P<word>(\\d+,?)+((\\.|:)\\d+)?K?|(Mr|Mrs|Dr|Prof|St|Rd)\\.?|([A-Za-z_](?!n't))*[A-Za-z]|n't|\\.|\\?|,|\\$|Â£|&|:|!|\"|-|â€“|[^a-zA-Z\\s]+)\"\"\")\n```", "```py\n>>> tokenise(\"Mr. Jones bought it for Â£5.3K!\")['Mr.', 'Jones', 'bought', 'it', 'for', 'Â£', '5.3K', '!']\n>>> tokenise(\"My cat is lucky the RSPCA weren't open at 3am last night!!!\n#fuming ğŸ˜¡ğŸ±\")\n['My', 'cat', 'is', 'lucky', 'the', 'RSPCA', 'were', \"n't\", 'open', 'at', '3', 'am', 'last', 'night', '!', '!', '!', '#', 'fuming', 'ğŸ˜¡ğŸ±']\n```", "```py\nARABICPATTERN = re.compile(r\"(?P<word>(\\d+,?)+(\\.\\d+)?|[ØŸÛ¼]+|\\.|\\?|,|\\$|Â£|&|!|'|\\\"|\\S+)\")CHINESEPATTERN =\nre.compile(r\"(?P<word>(\\d+,?)+(\\.\\d+)?|[ä¸€-é¾¥]|.|\\?|,|\\$|Â£|&|!|'|\\\"|)\")\n```", "```py\nfrom utilities import *from nltk.corpus import wordnet\nPREFIXES = {\"\", \"un\", \"dis\", \"re\"}\nSUFFIXES = {\"\", \"ing\", \"s\", \"ed\", \"en\", \"er\", \"est\", \"ly\", \"ion\"}\nPATTERN = re.compile(\"(?P<form>[a-z]{3,}) (?P<pos>n|v|r|a) \")\ndef readAllWords():\nÂ Â Â Â return set(wordnet.all_lemma_names())\nALLWORDS = readAllWords()\ndef stem(form, prefixes=PREFIXES, words=ALLWORDS, suffixes=SUFFIXES):\nÂ Â Â Â for i in range(len(form)):\nÂ Â Â Â Â Â Â Â if form[:i] in prefixes:\nÂ Â Â Â Â Â Â Â Â Â Â Â for j in range(i+1, len(form)+1):\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â if form[i:j] in words:\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â if form[j:] in suffixes:\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â yield (\"%s-%s+%s\"%(form[:i],\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â form[i:j], form[j:])).strip(\"+-\")\nROOTPATTERN = re.compile(\"^(.*-)?(?P<root>.*?)(\\+.*)?$\")\ndef sortstem(w):\nÂ Â Â Â return ROOTPATTERN.match(w).group(\"root\")\ndef allstems(form, prefixes=PREFIXES, words=ALLWORDS, suffixes=SUFFIXES):\nÂ Â Â Â return sorted(stem(form, prefixes=PREFIXES,\nÂ Â Â Â Â Â Â Â words=ALLWORDS, suffixes=SUFFIXES), key=sortstem)\n```", "```py\n>>> from chapter4 import stem1>>> stem1.stem(\"unexpected\")\n<generator object stem at 0x7f947a418890>\n```", "```py\n>>> list(stem1.stem(\"unexpected\"))['unexpected', 'un-expect+ed', 'un-expected']\n```", "```py\n>>> list(stem1.stem(\"uneaten\"))['un-eat+en']\n```", "```py\n>>> stem1.allstems(\"unexpected\")['un-expect+ed', 'un-expected', 'unexpected']\n```", "```py\nÂ Â Â Â MORPHOLOGICAL_SUBSTITUTIONS = {Â Â Â Â Â Â Â Â NOUN: [\nÂ Â Â Â Â Â Â Â Â Â Â Â ('s', ''),\nÂ Â Â Â Â Â Â Â Â Â Â Â ('ses', 's'),\nÂ Â Â Â Â Â Â Â Â Â Â Â ('ves', 'f'),\nÂ Â Â Â Â Â Â Â Â Â Â Â ('xes', 'x'),\nÂ Â Â Â Â Â Â Â Â Â Â Â ('zes', 'z'),\nÂ Â Â Â Â Â Â Â Â Â Â Â ('ches', 'ch'),\nÂ Â Â Â Â Â Â Â Â Â Â Â ('shes', 'sh'),\nÂ Â Â Â Â Â Â Â Â Â Â Â ('men', 'man'),\nÂ Â Â Â Â Â Â Â Â Â Â Â ('ies', 'y'),\nÂ Â Â Â Â Â Â Â ],\nÂ Â Â Â Â Â Â Â VERB: [\nÂ Â Â Â Â Â Â Â Â Â Â Â ('s', ''),\nÂ Â Â Â Â Â Â Â Â Â Â Â ('ies', 'y'),\nÂ Â Â Â Â Â Â Â Â Â Â Â ('es', 'e'),\nÂ Â Â Â Â Â Â Â Â Â Â Â ('es', ''),\nÂ Â Â Â Â Â Â Â Â Â Â Â ('ed', 'e'),\nÂ Â Â Â Â Â Â Â Â Â Â Â ('ed', ''),\nÂ Â Â Â Â Â Â Â Â Â Â Â ('ing', 'e'),\nÂ Â Â Â Â Â Â Â Â Â Â Â ('ing', ''),\nÂ Â Â Â Â Â Â Â ],\nÂ Â Â Â Â Â Â Â ADJ: [('er', ''), ('est', ''), ('er', 'e'), ('est', 'e')],\nÂ Â Â Â Â Â Â Â ADV: [],\nÂ Â Â Â }\n```", "```py\nSPELLINGRULES = \"\"\"ee X:(d|n) ==> ee + e X\nC y X:ing ==> C ie + X\nC X:ly ==> C le + X\ni X:e(d|r|st?)|ly ==> y + X\nX:((d|g|t)o)|x|s(h|s)|ch es ==> X + s\nX0 (?!(?P=X0)) C X1:ed|en|ing ==> X0 C e + X1\nC0 V C1 C1 X:e(d|n|r)|ing ==> C0 V C1 + X\nC0 V C1 X:(s|$) ==> C0 V C1 + X\n\"\"\"\n```", "```py\n>>> from chapter4 import stem2>>> list(stem2.stem(\"kitted\"))\n['kit+ed']\n```", "```py\n>>> stem2.allstems(\"hitting\")['hit+ing', 'hitting']\n```", "```py\n>>> stem2.allstems(\"unreconstructed\")['un-re-construct+ed', 'un-reconstruct+ed', 'un-reconstructed', 'unreconstructed']\n>>> stem2.allstems(\"restrictions\")\n['restrict+ion+s', 'restriction+s', 're-strict+ion+s']\n```", "```py\nPREFIXES = fixaffixes(Â Â Â Â {\"un\": \"(v->tns)->(v->tns), (a->cmp)->(a->cmp)\",\nÂ Â Â Â Â \"re\": \"(v->tns)->(v->tns)\",\nÂ Â Â Â Â \"dis\": \"(v->tns)->(v->tns)\"})\nSUFFIXES = fixaffixes(\nÂ Â Â Â {\"\": \"tns, num, cmp\",\nÂ Â Â Â Â \"ing\": \"tns\",\nÂ Â Â Â Â \"ed\": \"tns\",\nÂ Â Â Â Â \"s\": \"tns, num\",\nÂ Â Â Â Â \"en\": \"tns\",\nÂ Â Â Â Â \"est\": \"cmp\",\nÂ Â Â Â Â \"ly\": \"r<-(a->cmp), r<-v\",\nÂ Â Â Â Â \"er\": \"(n->num)<-(v->tns), cmp\",,\nÂ Â Â Â Â \"ment\": \"(n->num)<-(v->tns)\"\nÂ Â Â Â Â \"ness\": \"(n->num)<-(v->tns)\"})\n```", "```py\n>>> from chapter4 import stem3>>> stem3.allstems(\"smaller\")[0]\n('small+er', ['a'])\n>>> stem3.allstems(\"redevelopments\")[0]\n('re-develop+ment+s', ['n'])\n>>> stem3.allstems(\"unsurprisingly\")[0]\n('un-surprise+ing++ly', ['r'])\n>>> stem3.allstems(\"unreconstructedly\")[0]\n('un-re-construct+ed++ly', ['r'])\n>>> stem3.allstems(\"reconstructions\")[0]\n('re-construct+ion+s', ['n'])\n>>> stem3.allstems(\"unreconstructions\")[0]\nTraceback (most recent call last):\nÂ Â File \"<stdin>\", line 1, in <module>\nIndexError: list index out of range\n```", "```py\nFSUFFIXES = fixaffixes({Â Â Â Â \"\": \"gen, num\",\nÂ Â Â Â \"s\": \"num\",\nÂ Â Â Â \"e\": \"gen\",})\n```", "```py\nFSPELLING = \"\"\"ive ==> if+e\n\"\"\"\n```", "```py\nFSUFFIXES = {Â Â Â Â \"\": \"gen, num\", \"s\": \"num\", \"e\": \"gen, person\",\nÂ Â Â Â \"er\": \"mood\", \"\": \"mood\",\nÂ Â Â Â \"ez\": \"person\", \"ais\": \"person\", \"a\": \"person\", \"ai\": \"person\",\nÂ Â Â Â \"aient\": \"person\", \"ait\": \"person\", \"as\": \"person\",\"Ã¢t\": \"person\",\nÂ Â Â Â \"asse\": \"person\", \"asses\": \"person\", \"ent\": \"person\", \"es\": \"person\",\nÂ Â Â Â \"iez\": \"person\", \"ions\": \"person\", \"ons\": \"person\", \"ont\": \"person\",\nÂ Â Â Â }\n```", "```py\nSUFFIXES = fixaffixes(Â Â Â Â {\"\": \"tense[finite=infinitive]; tense[finite=tensed, tense=present]\"\nÂ Â Â Â Â \"ing\": \"tense[finite=participle, tense=present]\",\nÂ Â Â Â Â \"ed\": \"tense[finite=participle, tense=present, voice=passive];\nÂ Â Â Â Â Â Â Â Â Â Â Â tense[tense=past, voice=active]\",\nÂ Â Â Â Â \"s\": \"tense[finite=tensed, tense=present, number=singular,\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â person=third];\nÂ Â Â Â Â \"en\": \"tense[finite=participle]\",\nÂ Â Â Â Â Â ...\nÂ Â Â Â Â \"ly\": \"r<-v[finite=participle, tense=present]\",\nÂ Â Â Â Â ...\n})\n```", "```py\n    ;--- Ab(1)\n    ```", "```py\n    ;; >ab~-ui_1\n    ```", "```py\n    >b (Ø£Ø¨) >ab~ (Ø£ÙØ¨Ù‘) PV_V desire;aspire\n    ```", "```py\n    >bb (Ø£Ø¨Ø¨) >abab (Ø£ÙØ¨ÙØ¨) PV_C desire;aspire\n    ```", "```py\n    &b (Ø¤Ø¨) &ub~ (Ø¤ÙØ¨Ù‘) IV_Vd desire;aspire\n    ```", "```py\n    >bb (Ø£Ø¨Ø¨) >obub (Ø£Ù’Ø¨ÙØ¨) IV_C desire;aspire\n    ```", "```py\n    }b (Ø¦Ø¨) }ib~ (Ø¦ÙØ¨Ù‘) IV_Vd desire;aspire\n    ```", "```py\n    >bb (Ø£Ø¨Ø¨) >obib (Ø£Ù’Ø¨ÙØ¨) IV_C desire;aspire\n    ```", "```py\n>>> from basics.readers import *>>> from chapter4 import compounds\n>>> l = list(reader(BNC, BNCWordReader, pattern=\".*/[A-Z\\d]*\\.xml\"))\n>>> pmi, pmiTable, words, pairs = compounds.doItAllPMI(l)\n111651731 words\n760415 distinct words found (111651731 tokens)\nGetting pairs\n67372 pairs found\nCalculating PMI\n>>> thresholded = compounds.thresholdpmi(pmi, 300)\n>>> printall(thresholded[:20])\n(14.880079898248782, 'inter-alia', 306)\n(14.10789557602586, 'ulcerative-colitis', 708)\n(13.730483221346029, 'vice-versa', 667)\n(13.600053898897935, 'gall-bladder', 603)\n(13.564948792663655, 'amino-acids', 331)\n(13.490100806659854, 'ad-hoc', 485)\n(12.956064741908307, 'carbon-dioxide', 976)\n(12.935141767901545, 'sq-km', 499)\n(12.872023194200782, 'biopsy-specimens', 306)\n(12.766406621309034, 'da-da', 499)\n(12.55829842681955, 'mentally-handicapped', 564)\n(12.46079297927814, 'ethnic-minorities', 336)\n(12.328294856503494, 'et-al', 2963)\n(12.273447636994682, 'global-warming', 409)\n(12.183953515076327, 'bodily-harm', 361)\n(12.097267289044826, 'ozone-layer', 320)\n(12.083121068394941, 'ha-ha', 665)\n(12.01519057467734, 'activating-factor', 311)\n(12.005309794347232, 'desktop-publishing', 327)\n(11.972306035897368, 'tens-thousands', 341)\n```", "```py\n>>> pmiTable['crime-prevention'](10.540598239864938, 202)\n>>> pmiTable['greenhouse-gases']\n(12.322885857554724, 120)\n```", "```py\n@PM @KF Very misleading heading.#anxious don't know why #worry (: slowly going #mad hahahahahahahahaha\n```"]