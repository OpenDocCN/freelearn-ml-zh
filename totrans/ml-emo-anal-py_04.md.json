["```py\n    <w c5=NN1  hw=factsheet  pos= SUBST >FACTSHEET </w>\n    ```", "```py\n    <w c5=DTQ  hw=what  pos= PRON >WHAT </w>\n    ```", "```py\n1   what    what    PRON    PronType=Int,Rel  0       root    _2   is      be      AUX     Mood=Ind          1       cop     _\n```", "```py\ndef reader(path, dataFileReader, pattern=re.compile('.*')):    if isinstance(pattern, str):\n        pattern = re.compile(pattern)\n    if isinstance(path, list):\n        # If what you're looking at is a list of file names,\n        # look inside it and return the things you find there\n        for f in path:\n            for r in reader(f, dataFileReader, pattern=pattern):\n                yield r\n    elif os.path.isdir(path):\n        # If what you're looking at is a directory,\n        # look inside it and return the things you find there\n        for f in sorted(os.listdir(path)):\n            for r in reader(os.path.join(path, f),\n                            dataFileReader, pattern=pattern):\n                yield r\n    else:\n        # If it's a datafile, check that its name matches the pattern\n        # and then use the dataFileReader to extract what you want\n        if pattern.match(path):\n            for r in dataFileReader(path):\n                yield r\n```", "```py\n>>> r = reader(BNC, BNCWordReader, pattern=r'.*\\.xml')>>> l = list(r)\n```", "```py\n<s n= 1 ><w c5= NN1  hw= factsheet  pos= SUBST >FACTSHEET </w><w c5= DTQ  hw= what  pos= PRON >WHAT </w><w c5= VBZ  hw= be  pos= VERB >IS </w><w c5= NN1  hw= aids  pos= SUBST >AIDS</w><c c5= PUN >?</c></s>\n```", "```py\nBNCWORD = re.compile('<(?P<tagtype>w|c).*?>(?P<form>.*?)\\s*</(?P=tagtype)>')Get raw text from BNC leaf files\ndef BNCWordReader(data):\n    for i in BNCWORD.finditer(open(data).read()):\n        yield i.group( form )\n```", "```py\nINPUT STRING: فيLOOK-UP WORD: fy\n     Comment:\n       INDEX: P1W1\n* SOLUTION 1: (fiy) [fiy_1] fiy/PREP\n     (GLOSS): in\n  SOLUTION 2: (fiy~a) [fiy_1] fiy/PREP+ya/PRON_1S\n     (GLOSS): in + me\n  SOLUTION 3: (fiy) [fiy_2] Viy/ABBREV\n     (GLOSS): V.\nINPUT STRING: سوسة\nLOOK-UP WORD: swsp\n     Comment:\n       INDEX: P1W2\n* SOLUTION 1: (suwsap) [suws_1] suws/NOUN+ap/NSUFF_FEM_SG\n     (GLOSS): woodworm:licorice + [fem.sg.]\n  SOLUTION 2: (suwsapu) [suws_1] suws/NOUN+ap/NSUFF_FEM_SG+u/CASE_DEF_NOM\n     (GLOSS): woodworm:licorice + [fem.sg.] + [def.nom.]\n…\n```", "```py\nPATBWordPattern = re.compile(\"INPUT STRING: (?P<form>\\S*)\")def PATBWordReader(path):\n    for i in PATBWordPattern.finditer(open(path).read()):\n        yield i.group( form )\n```", "```py\n... لونغ بيتش ( الولايات المتحدة ) 15-7 ( اف ب\n```", "```py\ndef WordReader(pattern, path):    for i in pattern.finditer(open(path).read()):\n        yield i.group( form )\nfrom functools import partial\nPATBWordReader = partial(WordReader, PATBWordPattern)\nBNCWordReader = partial(WordReader, BNCWordPattern)\n```", "```py\nmysql> describe sentiments;+-----------+--------------+------+-----+---------+-------+\n| Field     | Type         | Null | Key | Default | Extra |\n+-----------+--------------+------+-----+---------+-------+\n| annotator | int(11)      | YES  |     | NULL    |       |\n| tweet     | int(11)      | YES  |     | NULL    |       |\n| sentiment | varchar(255) | YES  |     | NULL    |       |\n+-----------+--------------+------+-----+---------+-------+\n3 rows in set (0.01 sec)\nmysql> select * from sentiments where tweet < 3;\n+-----------+-------+-----------------------+\n| annotator | tweet | sentiment             |\n+-----------+-------+-----------------------+\n|        19 |     1 | love                  |\n|         1 |     1 | anger+dissatisfaction |\n|         8 |     1 | anger+dissatisfaction |\n|         2 |     2 | love+joy              |\n|        19 |     2 | love                  |\n|         6 |     2 | love+joy+optimism     |\n+-----------+-------+-----------------------\n```", "```py\n>>> DB = MySQLdb.connect(db= centement , autocommit=True, charset= UTF8 )>>> cursor = DB.cursor()\n>>> data = pandas.read_sql( select * from sentiments where tweet < 3 , DB)\n>>> data\n   annotator  tweet              sentiment\n0         19      1                   love\n1          1      1  anger+dissatisfaction\n2          8      1  anger+dissatisfaction\n3          2      2               love+joy\n4         19      2                   love\n5          6      2      love+joy+optimism\n>>> data.to_json()\n'{ annotator :{ 0 :19, 1 :1, 2 :8, 3 :2, 4 :19, 5 :6}, tweet :{ 0 :1, 1 :1, 2 :1, 3 :2, 4 :2, 5 :2}, sentiment :{ 0 : love , 1 : anger+dissatisfaction , 2 : anger+dissatisfaction , 3 : love+joy , 4 : love , 5 : love+joy+optimism }}'\n>>> print(data.to_csv())\n,annotator,tweet,sentiment\n0,19,1,love\n1,1,1,anger+dissatisfaction\n2,8,1,anger+dissatisfaction\n3,2,2,love+joy\n4,19,2,love\n5,6,2,love+joy+optimism\n```", "```py\nID  Tweet                        anger  sadness  surprise0   2017-En-21441 Worry is a dow     0     1          0\n1   2017-En-31535  Whatever you d    0     0          0\n2   2017-En-22190  No but that's     0     0          1\n3   2017-En-20221  Do you think h    0     0          0\n4   2017-En-22180  Rooneys effing    1     0          0\n6830  2017-En-21383  @nicky57672 Hi  0     0          0\n6831  2017-En-41441  @andreamitchel  0     0          1\n6832  2017-En-10886  @isthataspider  0     1          0\n6833  2017-En-40662  i wonder how a  0     0          1\n6834  2017-En-31003  I'm highly ani  0     0          0\n```", "```py\n                         as well as in a stock market                  in share prices in the stock market\n                  to the harsher side of stock market life\n                          apart from the stock market crash of two\n                        There was a huge stockmarket crash in October\n           of being replaced in a sudden stockmarket coup\n           in the days that followed the stockmarket crash of October\n                                     The stockmarket crash of 1987 is\n                       was to be a major battle ground between\n           of politics as an ideological battle ground and by her\n             likely to form the election battle ground\n             likely to form the election battle ground\n                           surveying the battleground with quiet\n      Industry had become an ideological battleground\n         of London as a potential savage battleground is confirmed by\n          previous evening it had been a battleground for people who\n```", "```py\n       an example about lifting a heavy weight and doing a              I was n't lifting a heavy weight\n            's roster there was a heavy weight of expectation for the\n   half-conscious levels he was a heavy weight upon me of a perhaps\n                              the heavyweight boxing champion of the\n      up a stirring finale to the heavyweight contest\n   of the main contenders for the heavyweight Bath title\na former British and Commonwealth heavyweight boxing champion\n  a new sound broadcasting system under study\n           many of the plants now under study phase come to fruition '\n    to the Brazilian auto workers under study at that particular time\n      in Guanxi province has been under study since the 1950s\n                      and Jack 's understudy can take over as the maid\n   to be considered as an England understudy for the Polish expedition\n              His Equity-required understudy received $800 per — \n                         will now understudy all positions along the\n```", "```py\nENGLISHPATTERN = re.compile(r\"\"\"(?P<word>(\\d+,?)+((\\.|:)\\d+)?K?|(Mr|Mrs|Dr|Prof|St|Rd)\\.?|([A-Za-z_](?!n't))*[A-Za-z]|n't|\\.|\\?|,|\\$|£|&|:|!|\"|-|–|[^a-zA-Z\\s]+)\"\"\")\n```", "```py\n>>> tokenise(\"Mr. Jones bought it for £5.3K!\")['Mr.', 'Jones', 'bought', 'it', 'for', '£', '5.3K', '!']\n>>> tokenise(\"My cat is lucky the RSPCA weren't open at 3am last night!!!\n#fuming 😡🐱\")\n['My', 'cat', 'is', 'lucky', 'the', 'RSPCA', 'were', \"n't\", 'open', 'at', '3', 'am', 'last', 'night', '!', '!', '!', '#', 'fuming', '😡🐱']\n```", "```py\nARABICPATTERN = re.compile(r\"(?P<word>(\\d+,?)+(\\.\\d+)?|[؟ۼ]+|\\.|\\?|,|\\$|£|&|!|'|\\\"|\\S+)\")CHINESEPATTERN =\nre.compile(r\"(?P<word>(\\d+,?)+(\\.\\d+)?|[一-龥]|.|\\?|,|\\$|£|&|!|'|\\\"|)\")\n```", "```py\nfrom utilities import *from nltk.corpus import wordnet\nPREFIXES = {\"\", \"un\", \"dis\", \"re\"}\nSUFFIXES = {\"\", \"ing\", \"s\", \"ed\", \"en\", \"er\", \"est\", \"ly\", \"ion\"}\nPATTERN = re.compile(\"(?P<form>[a-z]{3,}) (?P<pos>n|v|r|a) \")\ndef readAllWords():\n    return set(wordnet.all_lemma_names())\nALLWORDS = readAllWords()\ndef stem(form, prefixes=PREFIXES, words=ALLWORDS, suffixes=SUFFIXES):\n    for i in range(len(form)):\n        if form[:i] in prefixes:\n            for j in range(i+1, len(form)+1):\n                if form[i:j] in words:\n                    if form[j:] in suffixes:\n                        yield (\"%s-%s+%s\"%(form[:i],\n                        form[i:j], form[j:])).strip(\"+-\")\nROOTPATTERN = re.compile(\"^(.*-)?(?P<root>.*?)(\\+.*)?$\")\ndef sortstem(w):\n    return ROOTPATTERN.match(w).group(\"root\")\ndef allstems(form, prefixes=PREFIXES, words=ALLWORDS, suffixes=SUFFIXES):\n    return sorted(stem(form, prefixes=PREFIXES,\n        words=ALLWORDS, suffixes=SUFFIXES), key=sortstem)\n```", "```py\n>>> from chapter4 import stem1>>> stem1.stem(\"unexpected\")\n<generator object stem at 0x7f947a418890>\n```", "```py\n>>> list(stem1.stem(\"unexpected\"))['unexpected', 'un-expect+ed', 'un-expected']\n```", "```py\n>>> list(stem1.stem(\"uneaten\"))['un-eat+en']\n```", "```py\n>>> stem1.allstems(\"unexpected\")['un-expect+ed', 'un-expected', 'unexpected']\n```", "```py\n    MORPHOLOGICAL_SUBSTITUTIONS = {        NOUN: [\n            ('s', ''),\n            ('ses', 's'),\n            ('ves', 'f'),\n            ('xes', 'x'),\n            ('zes', 'z'),\n            ('ches', 'ch'),\n            ('shes', 'sh'),\n            ('men', 'man'),\n            ('ies', 'y'),\n        ],\n        VERB: [\n            ('s', ''),\n            ('ies', 'y'),\n            ('es', 'e'),\n            ('es', ''),\n            ('ed', 'e'),\n            ('ed', ''),\n            ('ing', 'e'),\n            ('ing', ''),\n        ],\n        ADJ: [('er', ''), ('est', ''), ('er', 'e'), ('est', 'e')],\n        ADV: [],\n    }\n```", "```py\nSPELLINGRULES = \"\"\"ee X:(d|n) ==> ee + e X\nC y X:ing ==> C ie + X\nC X:ly ==> C le + X\ni X:e(d|r|st?)|ly ==> y + X\nX:((d|g|t)o)|x|s(h|s)|ch es ==> X + s\nX0 (?!(?P=X0)) C X1:ed|en|ing ==> X0 C e + X1\nC0 V C1 C1 X:e(d|n|r)|ing ==> C0 V C1 + X\nC0 V C1 X:(s|$) ==> C0 V C1 + X\n\"\"\"\n```", "```py\n>>> from chapter4 import stem2>>> list(stem2.stem(\"kitted\"))\n['kit+ed']\n```", "```py\n>>> stem2.allstems(\"hitting\")['hit+ing', 'hitting']\n```", "```py\n>>> stem2.allstems(\"unreconstructed\")['un-re-construct+ed', 'un-reconstruct+ed', 'un-reconstructed', 'unreconstructed']\n>>> stem2.allstems(\"restrictions\")\n['restrict+ion+s', 'restriction+s', 're-strict+ion+s']\n```", "```py\nPREFIXES = fixaffixes(    {\"un\": \"(v->tns)->(v->tns), (a->cmp)->(a->cmp)\",\n     \"re\": \"(v->tns)->(v->tns)\",\n     \"dis\": \"(v->tns)->(v->tns)\"})\nSUFFIXES = fixaffixes(\n    {\"\": \"tns, num, cmp\",\n     \"ing\": \"tns\",\n     \"ed\": \"tns\",\n     \"s\": \"tns, num\",\n     \"en\": \"tns\",\n     \"est\": \"cmp\",\n     \"ly\": \"r<-(a->cmp), r<-v\",\n     \"er\": \"(n->num)<-(v->tns), cmp\",,\n     \"ment\": \"(n->num)<-(v->tns)\"\n     \"ness\": \"(n->num)<-(v->tns)\"})\n```", "```py\n>>> from chapter4 import stem3>>> stem3.allstems(\"smaller\")[0]\n('small+er', ['a'])\n>>> stem3.allstems(\"redevelopments\")[0]\n('re-develop+ment+s', ['n'])\n>>> stem3.allstems(\"unsurprisingly\")[0]\n('un-surprise+ing++ly', ['r'])\n>>> stem3.allstems(\"unreconstructedly\")[0]\n('un-re-construct+ed++ly', ['r'])\n>>> stem3.allstems(\"reconstructions\")[0]\n('re-construct+ion+s', ['n'])\n>>> stem3.allstems(\"unreconstructions\")[0]\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nIndexError: list index out of range\n```", "```py\nFSUFFIXES = fixaffixes({    \"\": \"gen, num\",\n    \"s\": \"num\",\n    \"e\": \"gen\",})\n```", "```py\nFSPELLING = \"\"\"ive ==> if+e\n\"\"\"\n```", "```py\nFSUFFIXES = {    \"\": \"gen, num\", \"s\": \"num\", \"e\": \"gen, person\",\n    \"er\": \"mood\", \"\": \"mood\",\n    \"ez\": \"person\", \"ais\": \"person\", \"a\": \"person\", \"ai\": \"person\",\n    \"aient\": \"person\", \"ait\": \"person\", \"as\": \"person\",\"ât\": \"person\",\n    \"asse\": \"person\", \"asses\": \"person\", \"ent\": \"person\", \"es\": \"person\",\n    \"iez\": \"person\", \"ions\": \"person\", \"ons\": \"person\", \"ont\": \"person\",\n    }\n```", "```py\nSUFFIXES = fixaffixes(    {\"\": \"tense[finite=infinitive]; tense[finite=tensed, tense=present]\"\n     \"ing\": \"tense[finite=participle, tense=present]\",\n     \"ed\": \"tense[finite=participle, tense=present, voice=passive];\n            tense[tense=past, voice=active]\",\n     \"s\": \"tense[finite=tensed, tense=present, number=singular,\n                 person=third];\n     \"en\": \"tense[finite=participle]\",\n      ...\n     \"ly\": \"r<-v[finite=participle, tense=present]\",\n     ...\n})\n```", "```py\n    ;--- Ab(1)\n    ```", "```py\n    ;; >ab~-ui_1\n    ```", "```py\n    >b (أب) >ab~ (أَبّ) PV_V desire;aspire\n    ```", "```py\n    >bb (أبب) >abab (أَبَب) PV_C desire;aspire\n    ```", "```py\n    &b (ؤب) &ub~ (ؤُبّ) IV_Vd desire;aspire\n    ```", "```py\n    >bb (أبب) >obub (أْبُب) IV_C desire;aspire\n    ```", "```py\n    }b (ئب) }ib~ (ئِبّ) IV_Vd desire;aspire\n    ```", "```py\n    >bb (أبب) >obib (أْبِب) IV_C desire;aspire\n    ```", "```py\n>>> from basics.readers import *>>> from chapter4 import compounds\n>>> l = list(reader(BNC, BNCWordReader, pattern=\".*/[A-Z\\d]*\\.xml\"))\n>>> pmi, pmiTable, words, pairs = compounds.doItAllPMI(l)\n111651731 words\n760415 distinct words found (111651731 tokens)\nGetting pairs\n67372 pairs found\nCalculating PMI\n>>> thresholded = compounds.thresholdpmi(pmi, 300)\n>>> printall(thresholded[:20])\n(14.880079898248782, 'inter-alia', 306)\n(14.10789557602586, 'ulcerative-colitis', 708)\n(13.730483221346029, 'vice-versa', 667)\n(13.600053898897935, 'gall-bladder', 603)\n(13.564948792663655, 'amino-acids', 331)\n(13.490100806659854, 'ad-hoc', 485)\n(12.956064741908307, 'carbon-dioxide', 976)\n(12.935141767901545, 'sq-km', 499)\n(12.872023194200782, 'biopsy-specimens', 306)\n(12.766406621309034, 'da-da', 499)\n(12.55829842681955, 'mentally-handicapped', 564)\n(12.46079297927814, 'ethnic-minorities', 336)\n(12.328294856503494, 'et-al', 2963)\n(12.273447636994682, 'global-warming', 409)\n(12.183953515076327, 'bodily-harm', 361)\n(12.097267289044826, 'ozone-layer', 320)\n(12.083121068394941, 'ha-ha', 665)\n(12.01519057467734, 'activating-factor', 311)\n(12.005309794347232, 'desktop-publishing', 327)\n(11.972306035897368, 'tens-thousands', 341)\n```", "```py\n>>> pmiTable['crime-prevention'](10.540598239864938, 202)\n>>> pmiTable['greenhouse-gases']\n(12.322885857554724, 120)\n```", "```py\n@PM @KF Very misleading heading.#anxious don't know why #worry (: slowly going #mad hahahahahahahahaha\n```"]