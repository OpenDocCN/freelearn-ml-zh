["```py\n    > ind = cut(1:nrow(churnTrain), breaks=10, labels=F)\n\n    ```", "```py\n    > accuracies = c()\n    > for (i in 1:10) {\n    +   fit = svm(churn ~., churnTrain[ind != i,])\n    +   predictions = predict(fit, churnTrain[ind == i, ! names(churnTrain) %in% c(\"churn\")])\n    +   correct_count = sum(predictions == churnTrain[ind == i,c(\"churn\")])\n    +   accuracies = append(correct_count / nrow(churnTrain[ind == i,]), accuracies)\n    + }\n\n    ```", "```py\n    > accuracies\n     [1] 0.9341317 0.8948949 0.8978979 0.9459459 0.9219219 0.9281437 0.9219219 0.9249249 0.9189189 0.9251497\n\n    ```", "```py\n    > mean(accuracies)\n    [1] 0.9213852\n\n    ```", "```py\n> for (i in 1:10) {\n+   fit = naiveBayes(churn ~., churnTrain[ind != i,])\n+   predictions = predict(fit, churnTrain[ind == i, ! names(churnTrain) %in% c(\"churn\")])\n+   correct_count = sum(predictions == churnTrain[ind == i,c(\"churn\")])\n+   accuracies = append(correct_count / nrow(churnTrain[ind == i,]), accuracies)\n+ }\n\n```", "```py\n    > tuned = tune.svm(churn~., data = trainset, gamma = 10^-2, cost = 10^2, tunecontrol=tune.control(cross=10))\n\n    ```", "```py\n    > summary(tuned)\n\n    Error estimation of 'svm' using 10-fold cross validation: 0.08164651\n\n    ```", "```py\n    > tuned$performances\n     gamma cost      error dispersion\n    1  0.01  100 0.08164651 0.02437228\n\n    ```", "```py\n    > svmfit = tuned$best.model\n    > table(trainset[,c(\"churn\")], predict(svmfit))\n\n     yes   no\n     yes  234  108\n     no    13 1960\n\n    ```", "```py\n    > ?e1071::tune\n\n    ```", "```py\n    > control = trainControl(method=\"repeatedcv\", number=10, repeats=3)\n\n    ```", "```py\n    > model = train(churn~., data=trainset, method=\"rpart\", preProcess=\"scale\", trControl=control)\n\n    ```", "```py\n    > model\n    CART \n\n    2315 samples\n     16 predictor\n     2 classes: 'yes', 'no' \n\n    Pre-processing: scaled \n    Resampling: Cross-Validated (10 fold, repeated 3 times) \n\n    Summary of sample sizes: 2084, 2083, 2082, 2084, 2083, 2084, ... \n\n    Resampling results across tuning parameters:\n\n     cp      Accuracy  Kappa  Accuracy SD  Kappa SD\n     0.0556  0.904     0.531  0.0236       0.155 \n     0.0746  0.867     0.269  0.0153       0.153 \n     0.0760  0.860     0.212  0.0107       0.141 \n\n    Accuracy was used to select the optimal model using the largest value.\n    The final value used for the model was cp = 0.05555556.\n\n    ```", "```py\n    > ?trainControl\n\n    ```", "```py\n    > importance = varImp(model, scale=FALSE)\n    > importance\n    rpart variable importance\n\n     Overall\n    number_customer_service_calls 116.015\n    total_day_minutes             106.988\n    total_day_charge              100.648\n    international_planyes          86.789\n    voice_mail_planyes             25.974\n    total_eve_charge               23.097\n    total_eve_minutes              23.097\n    number_vmail_messages          19.885\n    total_intl_minutes              6.347\n    total_eve_calls                 0.000\n    total_day_calls                 0.000\n    total_night_charge              0.000\n    total_intl_calls                0.000\n    total_intl_charge               0.000\n    total_night_minutes             0.000\n    total_night_calls               0.000\n\n    ```", "```py\n    > plot(importance)\n\n    ```", "```py\n> library(rpart)\n> model.rp = rpart(churn~., data=trainset)\n> model.rp$variable.importance\n total_day_minutes              total_day_charge \n 111.645286                    110.881583 \nnumber_customer_service_calls            total_intl_minutes \n 58.486651                     48.283228 \n total_intl_charge              total_eve_charge \n 47.698379                     47.166646 \n total_eve_minutes            international_plan \n 47.166646                     42.194508 \n total_intl_calls         number_vmail_messages \n 36.730344                     19.884863 \n voice_mail_plan             total_night_calls \n 19.884863                      7.195828 \n total_eve_calls            total_night_charge \n 3.553423                      1.754547 \n total_night_minutes               total_day_calls \n 1.754547                      1.494986 \n\n```", "```py\n    > install.packages(\"rminer\")\n    > library(rminer)\n\n    ```", "```py\n    > model=fit(churn~.,trainset,model=\"svm\")\n\n    ```", "```py\n    > VariableImportance=Importance(model,trainset,method=\"sensv\")\n\n    ```", "```py\n    > L=list(runs=1,sen=t(VariableImportance$imp),sresponses=VariableImportance$sresponses)\n    > mgraph(L,graph=\"IMP\",leg=names(trainset),col=\"gray\",Grid=10)\n\n    ```", "```py\n    > ?rminer::fit\n\n    ```", "```py\n    > new_train = trainset[,! names(churnTrain) %in% c(\"churn\", \"international_plan\", \"voice_mail_plan\")]\n\n    ```", "```py\n    >cor_mat = cor(new_train)\n\n    ```", "```py\n    > highlyCorrelated = findCorrelation(cor_mat, cutoff=0.75)\n\n    ```", "```py\n    > names(new_train)[highlyCorrelated]\n    [1] \"total_intl_minutes\"  \"total_day_charge\"    \"total_eve_minutes\"   \"total_night_minutes\"\n\n    ```", "```py\n    > intl_plan = model.matrix(~ trainset.international_plan - 1, data=data.frame(trainset$international_plan))\n    > colnames(intl_plan) = c(\"trainset.international_planno\"=\"intl_no\", \"trainset.international_planyes\"= \"intl_yes\")\n\n    ```", "```py\n    > voice_plan = model.matrix(~ trainset.voice_mail_plan - 1, data=data.frame(trainset$voice_mail_plan))\n    > colnames(voice_plan) = c(\"trainset.voice_mail_planno\" =\"voice_no\", \"trainset.voice_mail_planyes\"=\"voidce_yes\")\n\n    ```", "```py\n    > trainset$international_plan = NULL\n    > trainset$voice_mail_plan = NULL\n    > trainset = cbind(intl_plan,voice_plan, trainset)\n\n    ```", "```py\n    > intl_plan = model.matrix(~ testset.international_plan - 1, data=data.frame(testset$international_plan))\n    > colnames(intl_plan) = c(\"testset.international_planno\"=\"intl_no\", \"testset.international_planyes\"= \"intl_yes\")\n\n    ```", "```py\n    > voice_plan = model.matrix(~ testset.voice_mail_plan - 1, data=data.frame(testset$voice_mail_plan))\n    > colnames(voice_plan) = c(\"testset.voice_mail_planno\" =\"voice_no\", \"testset.voice_mail_planyes\"=\"voidce_yes\")\n\n    ```", "```py\n    > testset$international_plan = NULL\n    > testset$voice_mail_plan = NULL\n    > testset = cbind(intl_plan,voice_plan, testset)\n\n    ```", "```py\n    > ldaControl = rfeControl(functions = ldaFuncs, method = \"cv\")\n\n    ```", "```py\n    > ldaProfile = rfe(trainset[, !names(trainset) %in% c(\"churn\")], trainset[,c(\"churn\")],sizes = c(1:18), rfeControl = ldaControl)\n    > ldaProfile\n\n    Recursive feature selection\n\n    Outer resampling method: Cross-Validated (10 fold) \n\n    Resampling performance over subset size:\n\n     Variables Accuracy  Kappa AccuracySD KappaSD Selected\n     1   0.8523 0.0000   0.001325 0.00000 \n     2   0.8523 0.0000   0.001325 0.00000 \n     3   0.8423 0.1877   0.015468 0.09787 \n     4   0.8462 0.2285   0.016593 0.09610 \n     5   0.8466 0.2384   0.020710 0.09970 \n     6   0.8466 0.2364   0.019612 0.09387 \n     7   0.8458 0.2315   0.017551 0.08670 \n     8   0.8458 0.2284   0.016608 0.09536 \n     9   0.8475 0.2430   0.016882 0.10147 \n     10   0.8514 0.2577   0.014281 0.08076 \n     11   0.8518 0.2587   0.014124 0.08075 \n     12   0.8544 0.2702   0.015078 0.09208        *\n     13   0.8544 0.2721   0.015352 0.09421 \n     14   0.8531 0.2663   0.018428 0.11022 \n     15   0.8527 0.2652   0.017958 0.10850 \n     16   0.8531 0.2684   0.017897 0.10884 \n     17   0.8531 0.2684   0.017897 0.10884 \n     18   0.8531 0.2684   0.017897 0.10884 \n\n    The top 5 variables (out of 12):\n     total_day_charge, total_day_minutes, intl_no, number_customer_service_calls, total_eve_charge\n\n    ```", "```py\n    > plot(ldaProfile, type = c(\"o\", \"g\"))\n\n    ```", "```py\n    > ldaProfile$optVariables\n     [1] \"total_day_charge\" \n     [2] \"total_day_minutes\" \n     [3] \"intl_no\" \n     [4] \"number_customer_service_calls\"\n     [5] \"total_eve_charge\" \n     [6] \"total_eve_minutes\" \n     [7] \"voidce_yes\" \n     [8] \"total_intl_calls\" \n     [9] \"number_vmail_messages\" \n    [10] \"total_intl_charge\" \n    [11] \"total_intl_minutes\" \n    [12] \"total_night_minutes\" \n\n    ```", "```py\n    > ldaProfile$fit\n    Call:\n    lda(x, y)\n\n    Prior probabilities of groups:\n     yes        no \n    0.1477322 0.8522678 \n\n    Group means:\n     total_day_charge total_day_minutes   intl_no\n    yes         35.00143          205.8877 0.7046784\n    no          29.62402          174.2555 0.9351242\n     number_customer_service_calls total_eve_charge\n    yes                      2.204678         18.16702\n    no                       1.441460         16.96789\n     total_eve_minutes voidce_yes total_intl_calls\n    yes          213.7269  0.1666667         4.134503\n    no           199.6197  0.2954891         4.514445\n     number_vmail_messages total_intl_charge\n    yes              5.099415          2.899386\n    no               8.674607          2.741343\n     total_intl_minutes total_night_minutes\n    yes           10.73684            205.4640\n    no            10.15119            201.4184\n\n    Coefficients of linear discriminants:\n     LD1\n    total_day_charge               0.715025524\n    total_day_minutes             -0.130486470\n    intl_no                        2.259889324\n    number_customer_service_calls -0.421997335\n    total_eve_charge              -2.390372793\n    total_eve_minutes              0.198406977\n    voidce_yes                     0.660927935\n    total_intl_calls               0.066240268\n    number_vmail_messages         -0.003529233\n    total_intl_charge              2.315069869\n    total_intl_minutes            -0.693504606\n    total_night_minutes           -0.002127471\n\n    ```", "```py\n    > postResample(predict(ldaProfile, testset[, !names(testset) %in% c(\"churn\")]), testset[,c(\"churn\")])\n    Accuracy     Kappa\n    0.8605108 0.2672027\n\n    ```", "```py\n    caretFuncs      SVM (caret)\n    lmFuncs     lm (base)\n    rfFuncs         RF(randomForest)\n    treebagFuncs     DT (ipred)\n    ldaFuncs       lda(base)\n    nbFuncs       NB(klaR)\n    gamFuncs      gam(gam)\n\n    ```", "```py\n    > library(car)\n    > data(Quartet)\n\n    ```", "```py\n    > plot(Quartet$x, Quartet$y3)\n    > lmfit = lm(Quartet$y3~Quartet$x)\n    > abline(lmfit, col=\"red\")\n\n    ```", "```py\n    > predicted= predict(lmfit, newdata=Quartet[c(\"x\")])\n\n    ```", "```py\n    > actual = Quartet$y3\n    > rmse = (mean((predicted - actual)^2))^0.5\n    > rmse\n    [1] 1.118286\n\n    ```", "```py\n    > mu = mean(actual)\n    > rse = mean((predicted - actual)^2) / mean((mu - actual)^2) \n    > rse\n    [1] 0.333676\n\n    ```", "```py\n    > rsquare = 1 - rse\n    > rsquare\n    [1] 0.666324\n\n    ```", "```py\n    > library(MASS)\n    > plot(Quartet$x, Quartet$y3)\n    > rlmfit = rlm(Quartet$y3~Quartet$x)\n    > abline(rlmfit, col=\"red\")\n\n    ```", "```py\n    > predicted = predict(rlmfit, newdata=Quartet[c(\"x\")])\n\n    ```", "```py\n    > actual = Quartet$y3\n    > rmse = (mean((predicted - actual)^2))^0.5\n    > rmse\n    [1] 1.279045\n\n    ```", "```py\n    > mu = mean(actual)\n    > rse =mean((predicted - actual)^2) / mean((mu - actual)^2) \n    > rse\n    [1] 0.4365067\n\n    ```", "```py\n    > rsquare = 1 - rse\n    > rsquare\n    [1] 0.5634933\n\n    ```", "```py\n> tune(lm, y3~x, data = Quartet)\nError estimation of 'lm' using 10-fold cross validation: 2.33754\n\n```", "```py\n    > svm.model= train(churn ~ .,\n    +                   data = trainset,\n    +                   method = \"svmRadial\")\n\n    ```", "```py\n    > svm.pred = predict(svm.model, testset[,! names(testset) %in% c(\"churn\")])\n\n    ```", "```py\n    > table(svm.pred, testset[,c(\"churn\")])\n\n    svm.pred yes  no\n     yes  73  16\n     no   68 861\n\n    ```", "```py\n    > confusionMatrix(svm.pred, testset[,c(\"churn\")])\n    Confusion Matrix and Statistics\n\n     Reference\n    Prediction yes  no\n     yes  73  16\n     no   68 861\n\n     Accuracy : 0.9175 \n     95% CI : (0.8989, 0.9337)\n     No Information Rate : 0.8615 \n     P-Value [Acc > NIR] : 2.273e-08 \n\n     Kappa : 0.5909 \n     Mcnemar's Test P-Value : 2.628e-08 \n\n     Sensitivity : 0.51773 \n     Specificity : 0.98176 \n     Pos Pred Value : 0.82022 \n     Neg Pred Value : 0.92680 \n     Prevalence : 0.13851 \n     Detection Rate : 0.07171 \n     Detection Prevalence : 0.08743 \n     Balanced Accuracy : 0.74974 \n\n     'Positive' Class : yes \n\n    ```", "```py\n    > install.packages(\"ROCR\")\n    > library(ROCR)\n\n    ```", "```py\n    > svmfit=svm(churn~ ., data=trainset, prob=TRUE)\n\n    ```", "```py\n    >pred=predict(svmfit,testset[, !names(testset) %in% c(\"churn\")], probability=TRUE)\n\n    ```", "```py\n    > pred.prob = attr(pred, \"probabilities\") \n    > pred.to.roc = pred.prob[, 2] \n\n    ```", "```py\n    > pred.rocr = prediction(pred.to.roc, testset$churn)\n\n    ```", "```py\n    > perf.rocr = performance(pred.rocr, measure = \"auc\", x.measure = \"cutoff\") \n    > perf.tpr.rocr = performance(pred.rocr, \"tpr\",\"fpr\") \n\n    ```", "```py\n    > plot(perf.tpr.rocr, colorize=T,main=paste(\"AUC:\",(perf.rocr@y.values)))\n\n    ```", "```py\n    > install.packages(\"pROC\")\n    > library(\"pROC\")\n\n    ```", "```py\n    > control = trainControl(method = \"repeatedcv\",\n    +                            number = 10,\n    +                            repeats = 3,\n    +                            classProbs = TRUE,\n    +                            summaryFunction = twoClassSummary)\n\n    ```", "```py\n    > glm.model= train(churn ~ .,\n    +                     data = trainset,\n    +                     method = \"glm\",\n    +                     metric = \"ROC\",\n    +                     trControl = control)\n\n    ```", "```py\n    > svm.model= train(churn ~ .,\n    +                   data = trainset,\n    +                   method = \"svmRadial\",\n    +                   metric = \"ROC\",\n    +                   trControl = control)\n\n    ```", "```py\n    > rpart.model= train(churn ~ .,\n    +                   data = trainset,\n    +                   method = \"rpart\",\n    +                   metric = \"ROC\",\n    +                   trControl = control)\n\n    ```", "```py\n    > glm.probs = predict(glm.model, testset[,! names(testset) %in% c(\"churn\")], type = \"prob\")\n    > svm.probs = predict(svm.model, testset[,! names(testset) %in% c(\"churn\")], type = \"prob\")\n    > rpart.probs = predict(rpart.model, testset[,! names(testset) %in% c(\"churn\")], type = \"prob\")\n\n    ```", "```py\n    > glm.ROC = roc(response = testset[,c(\"churn\")],\n    +                predictor =glm.probs$yes,\n    +                levels = levels(testset[,c(\"churn\")]))\n    > plot(glm.ROC, type=\"S\", col=\"red\") \n\n    Call:\n    roc.default(response = testset[, c(\"churn\")], predictor = glm.probs$yes,     levels = levels(testset[, c(\"churn\")]))\n\n    Data: glm.probs$yes in 141 controls (testset[, c(\"churn\")] yes) > 877 cases (testset[, c(\"churn\")] no).\n    Area under the curve: 0.82\n\n    > svm.ROC = roc(response = testset[,c(\"churn\")],\n    +                predictor =svm.probs$yes,\n    +                levels = levels(testset[,c(\"churn\")]))\n    > plot(svm.ROC, add=TRUE, col=\"green\") \n\n    Call:\n    roc.default(response = testset[, c(\"churn\")], predictor = svm.probs$yes,     levels = levels(testset[, c(\"churn\")]))\n\n    Data: svm.probs$yes in 141 controls (testset[, c(\"churn\")] yes) > 877 cases (testset[, c(\"churn\")] no).\n    Area under the curve: 0.9233\n\n    > rpart.ROC = roc(response = testset[,c(\"churn\")],\n    +                predictor =rpart.probs$yes,\n    +                levels = levels(testset[,c(\"churn\")]))\n    > plot(rpart.ROC, add=TRUE, col=\"blue\")\n\n    Call:\n    roc.default(response = testset[, c(\"churn\")], predictor = rpart.probs$yes,     levels = levels(testset[, c(\"churn\")]))\n\n    Data: rpart.probs$yes in 141 controls (testset[, c(\"churn\")] yes) > 877 cases (testset[, c(\"churn\")] no).\n    Area under the curve: 0.7581\n\n    ```", "```py\n    > help(package=\"pROC\")\n\n    ```", "```py\n    > cv.values = resamples(list(glm = glm.model, svm=svm.model, rpart = rpart.model))\n\n    ```", "```py\n    > summary(cv.values)\n\n    Call:\n    summary.resamples(object = cv.values)\n\n    Models: glm, svm, rpart \n    Number of resamples: 30 \n\n    ROC \n     Min. 1st Qu. Median   Mean 3rd Qu.   Max. NA's\n    glm   0.7206  0.7847 0.8126 0.8116  0.8371 0.8877    0\n    svm   0.8337  0.8673 0.8946 0.8929  0.9194 0.9458    0\n    rpart 0.2802  0.7159 0.7413 0.6769  0.8105 0.8821    0\n\n    Sens \n     Min. 1st Qu. Median   Mean 3rd Qu.   Max. NA's\n    glm   0.08824  0.2000 0.2286 0.2194  0.2517 0.3529    0\n    svm   0.44120  0.5368 0.5714 0.5866  0.6424 0.7143    0\n    rpart 0.20590  0.3742 0.4706 0.4745  0.5929 0.6471    0\n\n    Spec \n     Min. 1st Qu. Median   Mean 3rd Qu.   Max. NA's\n    glm   0.9442  0.9608 0.9746 0.9701  0.9797 0.9949    0\n    svm   0.9442  0.9646 0.9746 0.9740  0.9835 0.9949    0\n    rpart 0.9492  0.9709 0.9797 0.9780  0.9848 0.9949    0\n\n    ```", "```py\n    > dotplot(cv.values, metric = \"ROC\")\n\n    ```", "```py\n    > bwplot(cv.values, layout = c(3, 1))\n\n    ```"]