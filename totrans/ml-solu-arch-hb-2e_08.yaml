- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building a Data Science Environment Using AWS ML Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While some organizations opt to build their own ML platforms using open-source
    technologies, many other organizations prefer to leverage fully managed ML services
    as the foundation for their ML platforms. In this chapter, we will delve into
    the fully managed ML services offered by AWS. Specifically, you will learn about
    **Amazon SageMaker**, and other related services for building a data science environment
    for data scientists. We will examine various components of SageMaker, such as
    SageMaker Studio, SageMaker Training, and SageMaker Hosting. Additionally, we
    will delve into the architectural framework for constructing a data science environment
    and provide a hands-on exercise to guide you through the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, this chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker overview
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data science environment architecture using SageMaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices for building a data science environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hands-on lab – building a data science environment using AWS services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you will need access to an AWS account and have the following
    AWS services for the hands-on lab:'
  prefs: []
  type: TYPE_NORMAL
- en: Amazon S3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon SageMaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon ECR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will also need to download the dataset from [https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news](https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news).
  prefs: []
  type: TYPE_NORMAL
- en: The sample source code used in this chapter can be found at [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter08](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter08).
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Amazon SageMaker offers ML functionalities that cover the entire ML lifecycle,
    spanning from initial experimentation to production deployment and ongoing monitoring.
    It caters to various roles, such as data scientists, data analysts, and MLOps
    engineers. The following diagram showcases the key SageMaker features that support
    the complete data science journey for different personas:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B20836_08_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: SageMaker capabilities'
  prefs: []
  type: TYPE_NORMAL
- en: Within SageMaker, data scientists have access to an array of features and services
    to support different ML tasks. These include Studio notebooks for model building,
    Data Wrangler for visual data preparation, the Processing service for large-scale
    data processing and transformation, the Training service, the Tuning service for
    model tuning, and the Hosting service for model hosting. With these tools, data
    scientists can handle various ML responsibilities, such as data preparation, model
    building and training, model tuning, and conducting model integration testing.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, data analysts can utilize SageMaker Canvas, a user-friendly
    model-building service that requires little to no coding. This visual interface
    empowers analysts to train models effortlessly. Additionally, they can use Studio
    notebooks for lightweight data analysis and processing.
  prefs: []
  type: TYPE_NORMAL
- en: MLOps engineers play a crucial role in managing and governing the ML environment.
    They are responsible for automating ML workflows and can leverage SageMaker Pipelines,
    Model Registry, and endpoint monitoring to achieve this. Furthermore, MLOps engineers
    configure the processing, training, and hosting infrastructure to ensure smooth
    operations for both interactive usage by data scientists and automated operations.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, our focus will center on data science environments catered
    specifically to data scientists. Subsequently, in the following chapter, we will
    delve into the administration, governance, and automation of ML infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Data science environment architecture using SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data scientists use data science environments to iterate different data science
    experiments with various datasets and algorithms. These environments require essential
    tools like Jupyter Notebook to author and execute code, data processing engines
    for handling large-scale data processing and feature engineering, and model training
    services for training models at scale. Additionally, an effective data science
    environment should include utilities for managing and tracking different experimentation
    runs, enabling researchers to organize and monitor their experiments effectively.
    To manage artifacts such as source code and Docker images, the data scientists
    also need a code repository and a Docker container repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates a basic data science environment architecture
    that uses Amazon SageMaker and other supporting services:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Data science environment architecture ](img/B20836_08_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: Data science environment architecture'
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker has multiple data science environments, including Studio, which is
    the primary data science development environment for data scientists, RStudio
    for R users, and Canvas for users who want a no-code/low-code development environment
    for building ML models. You also have access to TensorBoard, a popular tool for
    monitoring and visualizing model metrics such as loss and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s explore how data scientists can leverage the different components
    of SageMaker to accomplish data science tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Onboarding SageMaker users
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The primary SageMaker user interface for data scientists is SageMaker Studio,
    a data science **integrated development environment** (**IDE**). It provides core
    features such as hosted notebooks for running experiments, as well as access to
    different backend services such as data wrangling, model training, and model hosting
    services from a single user interface. It is the main interface for data scientists
    to interact with most of SageMaker’s functionality. It also provides a Python
    SDK for interacting with its backend services programmatically from Python notebooks
    or scripts. The following diagram shows the key components of SageMaker Studio:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – SageMaker Studio architecture ](img/B20836_08_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.3: SageMaker Studio architecture'
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker Studio implements the concept of domains to segregate user environments.
    A domain represents a collection of user profiles, each equipped with specific
    configurations, such as the AWS IAM role used when running Jupyter notebook, tags,
    and access permission to SageMaker Canvas. Within each domain, a Studio user can
    access different Studio applications such as JupyterLab, Code Editor, or Canvas
    through a user profile.
  prefs: []
  type: TYPE_NORMAL
- en: To run certain Studio applications such as JupyterLab and Code Editor, you need
    to create SageMaker Studio spaces. SageMaker Studio spaces are used to manage
    the storage and resource needs of these applications. Each space has a 1:1 relationship
    with an instance of an application, and is composed of resources such as a storage
    volume, application type, and image the application is based on. A space can be
    either private or shared.
  prefs: []
  type: TYPE_NORMAL
- en: To begin using SageMaker, the initial step is to onboard users into SageMaker
    Studio. The onboarding process starts by creating a SageMaker domain, if one doesn’t
    already exist. For the single-user scenario, SageMaker provides a quick domain
    setup option. With the quick setup option, SageMaker automatically configures
    the new domain with default settings for fast setup. For advanced multi-user scenarios,
    SageMaker provides an advanced domain setup option. With the advanced setting,
    you can configure various aspects of the domain for enterprise adoption, such
    as the authentication method, access to different services, networking configuration,
    and data encryption keys. Following that, user profiles are created within the
    domain, and users are granted access to these profiles. Once the user profiles
    are set up, users can launch a Studio environment by selecting their respective
    user profile in a domain. This allows them to start working within SageMaker Studio.
  prefs: []
  type: TYPE_NORMAL
- en: Launching Studio applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are multiple applications available inside Studio, including JupyterLab,
    RStudio, Canvas, and Code Editor. You also have the option to launch the previous
    Studio Classic experience.
  prefs: []
  type: TYPE_NORMAL
- en: In the SageMaker Studio environment, initiating a Jupyter notebook is a straightforward
    process. Begin by selecting the JupyterLab application, followed by the creation
    of a JupyterLab space. A space, serving as a named, self-contained, and durable
    storage container, can be attached to the JupyterLab application. During the space
    creation, you have the flexibility to choose the server instance type and the
    Docker image for your JupyterLab. Once the space is successfully created, you
    can proceed to launch the JupyterLab application and initiate a notebook within
    it. The JupyterLab notebook now has an AI-powered programming assistant (Jupyternaut)
    feature that you can use to ask programming questions. For example, you can ask
    questions such as “How do I import a Python library?” or “How do I train a model
    using SageMaker?”.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, launching the Canvas application involves selecting it within the
    SageMaker Studio environment. As a no-code ML tool, Canvas provides features for
    data preparation, model training, inference, and workflow automation. While primarily
    designed for data analysts with a limited background in data science and ML, it
    also proves to be valuable for experienced data scientists aiming to quickly establish
    baseline models for diverse datasets. Users can leverage Canvas’s ready-to-use
    models for predictions without model building or choose to create custom models
    tailored to specific business problems. Ready-to-use models cover various use
    cases like language detection and document analysis. For custom models, Canvas
    supports building diverse model types using tabular and image data for personalized
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Importing data, whether from local or external sources such as S3, Amazon Redshift,
    or Databricks, is supported. Additionally, Canvas facilitates data preparation
    through Data Wrangler and offers generative AI foundation models for initiating
    conversational chats.
  prefs: []
  type: TYPE_NORMAL
- en: The Code Editor, built on Code-OSS and Visual Studio Code Open Source, facilitates
    writing, testing, debugging, and running analytics and ML code. To launch the
    Code Editor, simply select the application within Studio and create a dedicated
    private space. This space operates on a single **Amazon Elastic Compute Cloud**
    (**Amazon EC2**) instance for computing and a corresponding **Amazon Elastic Block
    Store** (**Amazon EBS**) volume for storage. All elements within the space, including
    code, Git profiles, and environment variables, are stored on this unified Amazon
    EBS volume.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Preparing and processing data for model training is an essential step in the
    ML lifecycle to optimize model performance. To do this in the Studio environment,
    you can install and use your preferred library packages directly inside your Studio
    notebook. SageMaker also provides several data wrangling and processing services
    to assist with data preparation, including SageMaker Data Wrangler, and integrated
    data preparation with Amazon EMR and Glue for large-scale data preparation and
    processing.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing data interactively with SageMaker Data Wrangler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SageMaker Data Wrangler is a fully managed service that helps data scientists
    and engineers prepare and analyze their data for ML. With a graphical user interface,
    it facilitates data preparation tasks, such as data cleaning, feature engineering,
    feature selection, and visualization.
  prefs: []
  type: TYPE_NORMAL
- en: To use Data Wrangler, you construct a data flow, a pipeline that connects the
    dataset, transformation, and analysis. When you create and run a data flow, Data
    Wrangler spins up an EC2 instance to run the transformation and analysis. Each
    data flow is associated with an EC2 instance. A data flow normally starts with
    a data import step. Data Wrangler allows you to import data from multiple data
    sources, including Amazon S3, Athena, Amazon Redshift, Amazon EMR, Databricks,
    Snowflake, as well as **Software-as-a-Service** (**SaaS**) platforms such as Datadog,
    GitHub, and Stripe.
  prefs: []
  type: TYPE_NORMAL
- en: After the data is imported, you can use Data Wrangler to clean and explore the
    data and perform feature engineering with a built-in transform. You can also use
    the preconfigured visualization templates to understand data and detect outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can assess the data quality with built-in reports. The following diagram
    shows the flow and architecture of Data Wrangler:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a data processing process  Description automatically generated](img/B20836_08_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.4: SageMaker Data Wrangler architecture'
  prefs: []
  type: TYPE_NORMAL
- en: When utilizing Data Wrangler, the first step involves importing sample data
    from various data sources to perform data processing and transformations. After
    completing the necessary transformation steps, you have the option to export a
    recipe. This recipe can then be executed by SageMaker Processing, which processes
    the entire dataset based on the defined transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, you have the option to export the transformation steps into a
    notebook file. This notebook file can be used to initiate a SageMaker Processing
    job, allowing for the data to be processed and transformed. The resulting output
    can be either directed to SageMaker Feature Store or stored in Amazon S3 for further
    usage and analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing data at scale interactively
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When dealing with large-scale data analysis, transformation, and preparation
    tasks, SageMaker offers built-in integration with Amazon EMR and AWS Glue. This
    built-in integration allows you to manage and handle large-scale interactive data
    preparation. By leveraging Amazon EMR, you can process and analyze massive datasets,
    while AWS Glue provides a serverless capability to prepare data at scale, as well
    as providing easy access to Glue data catalogs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Within the Studio notebook environment, you have the capability to discover
    and establish connections with their existing Amazon EMR clusters. This enables
    them to interactively explore, visualize, and prepare large-scale data for ML
    tasks, leveraging powerful tools such as Apache Spark, Apache Hive, and Presto.
    The following diagram shows how SageMaker integration with EMR works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a software development process  Description automatically generated](img/B20836_08_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.5: SageMaker integration with EMR'
  prefs: []
  type: TYPE_NORMAL
- en: As a user, you connect to an existing EMR cluster from your Studio notebooks.
    If there is no EMR cluster available, you can self-provision one directly from
    the Studio environment by choosing a predefined template created by system administrators.
    System administrators use AWS Service Catalog to define parameterized templates
    for data scientists to use. Once your notebook is connected to an EMR cluster,
    you can run Spark commands or code inside your notebook cells.
  prefs: []
  type: TYPE_NORMAL
- en: AWS Glue interactive sessions is a serverless service that provides you with
    the tools to collect, transform, cleanse, and prepare the data. The built-in integration
    between SageMaker and Glue interactive sessions allows you to run interactive
    sessions for data preparation interactively using Glue as the backend.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a diagram  Description automatically generated](img/B20836_08_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.6: SageMaker integration with AWS Glue interactive sessions'
  prefs: []
  type: TYPE_NORMAL
- en: To use Glue interactive sessions inside the Studio notebook, you choose the
    built-in Glue PySpark or Glue Spark kernel when you create your Studio notebook.
    After initialization, you can browse the Glue data catalog, run large queries,
    and interactively analyze and prepare data using Spark, all within your Studio
    notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Both EMR and Glue offer similar capabilities for large-scale interactive data
    processing. Glue interactive sessions are a good choice if you are looking for
    a quick and serverless way to run Spark sessions. EMR provides more robust capabilities
    and the flexibility to configure your compute cluster for optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Processing data as separate jobs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SageMaker Processing provides a separate infrastructure for large-scale data
    processing such as data cleaning and feature engineering for large datasets as
    separate backend jobs. It can be accessed directly from a notebook environment
    via the SageMaker Python SDK or Boto3 SDK. SageMaker Processing uses Docker container
    images to run data processing jobs. Several built-in containers, such as scikit-learn
    containers and Spark containers, are provided out of the box. You also have the
    option to use your custom containers for processing. The following diagram shows
    the SageMaker Processing architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – SageMaker Processing architecture ](img/B20836_08_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.7: SageMaker Processing architecture'
  prefs: []
  type: TYPE_NORMAL
- en: When a SageMaker Processing job is initiated, the processing container is pulled
    from Amazon ECR and loaded into the EC2 compute cluster. The data in S3 is copied
    over to the storage attached to the compute nodes for the data processing scripts
    to access and process. Once the processing procedure is completed, the output
    data is copied back to the S3 output location.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker Processing provides several processors for data processing, including
    a Spark processor, scikit-learn processor, and your own customer processor by
    bringing your containers. SageMaker Processing also supports processors for different
    ML frameworks, including PyTorch, TensorFlow, MXNet, Hugging Face, and XGBoost.
    You can use one of the processors if you need to use library packages as part
    of the processing script.
  prefs: []
  type: TYPE_NORMAL
- en: Creating, storing, and sharing features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When data scientists perform feature engineering for training data preparation,
    they often need to reuse the same features for different model tasks. Further,
    features generated could be used for both training and inference to help reduce
    the training-serving skew.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon SageMaker Feature Store is a service for sharing and managing ML features
    for ML development and serving. Feature Store is a centralized store for features
    and associated metadata so features can be discovered and reused. It has an online
    component as well as an offline component. The online store is used for low-latency
    real-time inference use cases, and the offline store is used for training and
    batch inference. The following diagram shows how Feature Store works. As you can
    see, it is quite similar to the architecture of other open-source alternatives,
    such as Feast, with the key difference being it is fully managed.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a software store  Description automatically generated](img/B20836_08_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.8: SageMaker Feature Store'
  prefs: []
  type: TYPE_NORMAL
- en: To begin, you first read and process the raw data. The data is then ingested
    into online and offline stores via streaming, or directly to the offline store
    via batch. Feature Store uses the concept called `FeatureGroup` to store and manage
    features. A `FeatureGroup` is a collection of features that are defined via a
    schema in Feature Store, describing the structure and metadata associated with
    a record. You can visualize a feature group as a table in which each column is
    a feature, with a unique identifier for each row.
  prefs: []
  type: TYPE_NORMAL
- en: The online store is specifically optimized for real-time predictions, offering
    low-latency reads and high-throughput writes. It is ideal for scenarios that require
    quick access to feature data for real-time inference. On the other hand, the offline
    store is designed for batch predictions and model training purposes. It operates
    as an append-only store, allowing historical feature data to be stored and accessed.
    The offline store is particularly useful for storing and serving features during
    exploration and model training processes.
  prefs: []
  type: TYPE_NORMAL
- en: Training ML models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once you have prepared the training data, you are all set to train the model.
    Data scientists can use the SageMaker Training service to handle model training
    that requires dedicated training infrastructure and instance types. The Training
    service is also ideal for large-scale distributed training using multiple nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start training, you first store your training data in storage such as Amazon
    S3, Amazon EFS, or Amazon FSx. You choose different storage options based on your
    specific requirements, such as cost and latency. S3 is the most common one, suitable
    for the majority of model training needs. With S3, you have multiple modes to
    ingest data from S3 to the training infrastructure:'
  prefs: []
  type: TYPE_NORMAL
- en: '**File mode**: This is the default input model whereby SageMaker downloads
    the training data from S3 to a local directory in the training instance. The training
    starts when the full dataset has been downloaded. With this mode, the training
    instance must have sufficient local storage space to fit the entire dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pipe mode**: With this mode, data is streamed directly from an S3 data source.
    This can provide faster start times and better throughput than the file mode.
    This model also reduces the size of the storage volumes attached to the training
    instances. It only needs enough space to store the final model artifacts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fast file mode**: This mode is the newer and simpler-to-use replacement of
    pipe mode. At the start of the training, the data files are identified but not
    downloaded. Training can start without waiting for the entire dataset to download.
    Fast file mode exposes S3 objects using a POSIX-compliant file system interface,
    as if the files are available on the local disk of the training instances. It
    streams the S3 data on demand as the training script consumes it, meaning that
    you don’t need to fit the training data into the training instance storage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to S3, you can also store your training data in Amazon FSx for Lustre.
    You want to use FSx when you have high-throughput and low-latency data retrieval
    requirements for your training data. With FSx, you can scale to hundreds of gigabytes
    of throughput and millions of **Inputs/Outputs Operations Per Second** (**IOPS**)
    with low-latency file retrieval. When a training job is started, it mounts FSx
    to the training instance file system as a local drive. FSx allows the training
    job to run faster as it takes less time to read the files and it does not need
    to copy data to the training instance’s local storage like S3 file mode. EFS provides
    similar functionality as FSx, but at a lower throughput and higher latency.
  prefs: []
  type: TYPE_NORMAL
- en: If you already have data in your EFS system, you can directly launch a training
    job using data in EFS without data movement. This reduces the training start time.
    Compared to FSx, EFS is less costly but has lower throughput and higher latency.
  prefs: []
  type: TYPE_NORMAL
- en: Once the training data is ready in the storage system of your choice, you can
    kick off the training job using the AWS Boto3 SDK or the SageMaker Python SDK.
    To run the training job, you need to provide configuration details such as the
    training Docker image’s URL from ECR, the training script location, the framework
    version, the training dataset location, and the S3 model output location, as well
    as infrastructure details such as the compute instance type and number, as well
    as networking details. The following sample code shows how to configure and kick
    off a training job using the SageMaker SDK.
  prefs: []
  type: TYPE_NORMAL
- en: 'The SageMaker Training service uses containers as a core technology for training
    management. All training jobs are executed inside containers hosted on SageMaker
    training infrastructure. The following diagram shows the architecture of the SageMaker
    Training service:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – SageMaker Training Service architecture ](img/B20836_08_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.9: SageMaker Training Service architecture'
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker provides various managed containers for model training using different
    ML algorithms and ML frameworks. Firstly, there is a list of built-in containerized
    algorithms for different ML tasks such as computer vision, NLP, forecasting, and
    common tabular regression and classifications. With these built-in algorithms,
    you only need to provide the training data location. SageMaker also provides a
    list of managed framework containers, such as containers for scikit-learn, TensorFlow,
    and PyTorch. With a managed framework container, in addition to providing data
    sources and infrastructure specifications, you also need to provide a training
    script that runs the model training loop.
  prefs: []
  type: TYPE_NORMAL
- en: If the built-in algorithms and framework containers do not meet your needs,
    you can bring your own custom container for model training. This container needs
    to contain the model training scripts, as well as all the dependencies required
    to run the training loop.
  prefs: []
  type: TYPE_NORMAL
- en: By default, SageMaker tracks all training jobs and their associated metadata,
    such as algorithms, input training dataset URLs, hyperparameters, and model output
    locations. Training jobs also emit system metrics and algorithm metrics to AWS
    CloudWatch for monitoring. Training logs are also sent to CloudWatch Logs for
    inspection and analysis needs. This metadata is critical for lineage tracking
    and reproducibility.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning ML models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To optimize the model’s performance, you also need to try different hyperparameters,
    such as a learning rate for gradient descent and model training. An algorithm
    can contain a large number of hyperparameters, and tuning them manually would
    be a highly labor-intensive task. The SageMaker Tuning service works with SageMaker
    training jobs to tune model training hyperparameters automatically. The following
    four types of hyperparameter tuning strategies are supported by the service:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Grid search**: Grid search is an exhaustive search technique that systematically
    explores a predefined set of hyperparameter values over a specified range for
    each hyperparameter. It creates a grid of all possible combinations and evaluates
    the model’s performance for each configuration using cross-validation or a validation
    set. Grid search is highly focused, but it can be highly inefficient due to the
    large number of combinations, especially when the hyperparameter dimension is
    high.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random search**: Random search is a popular and effective hyperparameter
    optimization technique that offers an alternative to exhaustive methods like grid
    search. Unlike grid search, which evaluates all possible combinations of hyperparameters
    within predefined ranges, random search takes a more stochastic approach. Instead
    of covering the entire search space systematically, it randomly samples hyperparameter
    values from defined distributions for each hyperparameter. Random search can be
    more efficient, compared to grid search; however, it might not always find the
    best combination of hyperparameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bayesian search**: This is where the hyperparameter search is treated like
    a regression problem, where the inputs for regression are the values of the hyperparameters
    and the output is the model’s performance metric once the model has been trained
    using the input values. The tuning service uses the values that have been collected
    from the training jobs to predict the next set of values that would produce model
    improvement. Compared to random search, Bayesian search is more efficient as it
    uses a model to focus on the most promising search spaces of hyperparameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hyperband**: Hyperband leverages the concept of bandit algorithms and successive
    halving to make the search process more effective and resource-efficient. It begins
    by randomly sampling a large number of hyperparameter configurations and then
    dividing them into multiple “bands” or sets. In each band, the configurations
    are evaluated through a predefined number of iterations, eliminating poorly performing
    ones at regular intervals. The surviving configurations are then promoted to the
    next band, where they are given additional iterations to fine-tune their performance.
    This process continues, gradually increasing the resources allocated to promising
    configurations while efficiently discarding underperforming ones. Hyperband is
    known to be more efficient than other methods, and it can find good combinations
    of hyperparameters with fewer iterations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The SageMaker tuning service works with SageMaker training jobs to optimize
    the hyperparameters. It works by sending different input hyperparameter values
    to the training jobs and picking the hyperparameter values that return the best
    model metrics. The following diagram shows how the SageMaker tuning service works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a training process  Description automatically generated](img/B20836_08_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.10: SageMaker tuning architecture'
  prefs: []
  type: TYPE_NORMAL
- en: To use the SageMaker tuning service, you create a tuning job and specify configuration
    details such as tuning strategy, objective metric to optimize, hyperparameters
    to tune and their ranges, max number of training jobs to run, and the number of
    jobs to run in parallel. Subsequently, the tuning job will kick off a number of
    training jobs. Depending on the tuning strategy, the tuning job will pass different
    hyperparameters to the training jobs to execute. The training metrics from the
    training jobs will be used by the tuning job to determine what hyperparameters
    to use in order to optimize the model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying ML models for testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data scientists normally do not deploy models for client application consumption
    directly. However, data scientists sometimes need to test the performance of models
    trained with the SageMaker training service, and they will need to deploy these
    models to an API endpoint for testing. This is especially needed for models of
    large sizes where they can’t be evaluated in a notebook instance. SageMaker provides
    a dedicated service for model hosting and its architecture is illustrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a server  Description automatically generated](img/B20836_08_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.11: SageMaker hosting architecture'
  prefs: []
  type: TYPE_NORMAL
- en: 'The SageMaker Hosting service offers multiple model inference options for different
    needs, from real-time inference to batch inference. The following are some options
    available for data scientists to use for model hosting evaluation and testing:'
  prefs: []
  type: TYPE_NORMAL
- en: One of the most common model serving use cases is low-latency prediction in
    real time on a sustained basis. For this use case, you should consider the **real-time
    inference** option from the SageMaker hosting service. SageMaker real-time inference
    provides multiple options for model hosting, including single-model hosting, multiple-model
    hosting in a single container behind one endpoint, and multiple-model hosting
    using different containers behind one endpoint.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sometimes, you have models that are used for intermittent predictions with idle
    periods between traffic bursts. For this type of requirement, you can consider
    the **Serverless** option from the SageMaker hosting service. The key benefit
    of using Serverless Inference is the removal of infrastructure configuration and
    management overhead. It is also more cost effective since you don’t need to pay
    for infrastructure when it is not used, unlike real-time inference, where you
    need to pay for the infrastructure whether there is inference traffic or not.
    However, there might be a delay caused by cold-starts with Serverless Inference
    when the model is invoked for the first time or there is extended idle time between
    invocations to allow SageMaker Serverless Inference to launch instances.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sometimes, the payload size of an inference can be very large, and it takes
    a long time to generate the prediction. In this case, a real-time endpoint won’t
    work due to the large payload size and extended time for the inference. For this
    type of use case, you can consider **the asynchronous inference** option of SageMaker
    hosting. Asynchronous inference queues the incoming requests and processes them
    asynchronously (as the name suggests). When using asynchronous inference, the
    input data and prediction output are stored in S3 instead of sending the payload
    and getting the response directly from the endpoint API. When the prediction is
    complete, you get a notification from the AWS SNS service.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another model inference pattern is **batch inference**. This is when you have
    a large number of inferences to do, and they don’t need individual predictions
    to be generated and returned. For example, you might need to run a propensity-to-buy
    model for a large number of users and store the output in a database for downstream
    consumption. For this usage pattern, you can consider the SageMaker Batch Transform
    feature for model inference. Batch inference is also more cost effective as you
    only need to spin up the infrastructure when running the batch job.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having discussed the various tasks involved in completing an ML project, ranging
    from data preparation to model deployment using the different SageMaker features,
    now let’s briefly talk about the need for automation. Data scientists frequently
    engage in iterative experimentation and model development, involving different
    datasets, new features, and various training scripts. Keeping track of configurations
    and model metrics for each run becomes essential. To streamline and automate these
    repetitive tasks, automation pipelines can be constructed, supporting various
    ML processes like data processing, model training, and model testing.
  prefs: []
  type: TYPE_NORMAL
- en: There are several tools available for automation and orchestration, including
    SageMaker’s Pipelines feature. It allows the creation of a **Directed Acyclic
    Graph** (**DAG**) to efficiently orchestrate and automate ML workflows. SageMaker
    Pipelines shares similarities with Airflow, an open-source workflow orchestration
    tool discussed in *Chapter 7*, *Open-Source ML Platforms*.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, AWS Step Functions serves as an alternative option for building
    automated workflow orchestration, providing flexibility and scalability to accommodate
    diverse ML tasks. By leveraging these tools, data scientists can enhance efficiency,
    reproducibility, and organization in their ML workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices for building a data science environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data science environments are meant for data scientists to perform quick experimentations
    using a wide range of ML frameworks and libraries. The following are some best
    practices to follow when providing such an environment for your data scientists:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Run large-scale model training using the SageMaker Training service instead
    of Studio notebooks**: SageMaker Studio notebooks are meant for quick experimentation
    with small datasets. While it is possible to provision large EC2 instances for
    certain large model training jobs, it is not cost effective to always keep a large
    EC2 instance running for a notebook all the time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Abstract infrastructure configuration details from data scientists**: There
    are many infrastructure configurations to consider when using SageMaker, such
    as networking configuration, IAM roles, encryption keys, EC2 instance types, and
    storage options. To make the lives of data scientists easier, abstract these details
    away from the data scientists. For example, instead of having the data scientists
    enter specific networking configurations, have those details stored as environment
    variables or custom SDK options for data scientists to choose from.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Create self-service provisioning**: To prevent provisioning bottlenecks,
    consider building a self-service provisioning capability to streamline user onboarding.
    For example, use AWS Service Catalog to create an ML product for automated user
    onboarding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use Studio notebook local mode for quick model training job testing**: SageMaker
    has support for local mode, meaning you can mimic running training jobs locally
    in your Studio notebook. With SageMaker Training jobs, there is an overhead of
    spinning up separate infrastructure. Running these tests locally can help speed
    up experimentation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Set up guardrails**: This helps prevent data scientists from making mistakes
    such as using the wrong instance types for model training or forgetting to use
    data encryption keys. You can use AWS service control policies to help with guardrail
    management.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clean up unused resources**: Regularly review and clean up unused notebooks,
    endpoints, and other resources to avoid unnecessary costs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use Spot instances**: For cost optimization, consider using Amazon EC2 Spot
    Instances for training jobs if possible. Spot instances can significantly reduce
    training costs while maintaining high performance. However, since Spot instances
    can be taken away unexpectedly, it is important to enable training checkpoints,
    so training can resume at the last checkpoint instead of restarting training from
    the beginning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use built-in algorithms and managed containers for training**: SageMaker
    provides a list of built-in algorithms for different ML tasks and managed training
    containers for different ML frameworks. Taking advantage of these pre-existing
    resources can substantially reduce the engineering effort required, eliminating
    the need to build your own algorithms from scratch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Build automated pipelines for repeatable ML experimentation, model building,
    and model testing**: Having an automated pipeline can greatly reduce the manual
    effort and improve the tracking of different experiments. Consider different orchestration
    technology options based on your technology standards and preferences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By adhering to these best practices, you can make the most of SageMaker Studio’s
    capabilities, streamline your ML workflows, and ensure cost-effective and secure
    usage of the platform.
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on exercise – building a data science environment using AWS services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The primary goal of this lab is to offer practical, hands-on experience with
    the various SageMaker tools. Once you are familiar with the core functionality
    in this lab, you should independently explore other features such as Code Editor
    and RStudio.
  prefs: []
  type: TYPE_NORMAL
- en: Problem statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As an ML solutions architect, you have been tasked with building a data science
    environment on AWS for the data scientists in the equity research department.
    The data scientists in the equity research department have several NLP problems,
    such as detecting the sentiment of financial phrases. Once you have created the
    environment for the data scientists, you also need to build a proof of concept
    to show the data scientists how to build and train an NLP model using the environment.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset description
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The data scientists have indicated that they like to use the BERT model to
    solve sentiment analysis problems, and they plan to use the financial phrases
    dataset to establish some initial benchmarks for the model: [https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news](https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news).'
  prefs: []
  type: TYPE_NORMAL
- en: Lab instructions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this lab, you will begin by establishing a SageMaker domain and user profile
    to facilitate user onboarding to SageMaker Studio. Additionally, the lab encompasses
    learning to train a deep learning model through both the JupyterLab notebook directly
    and the SageMaker training job service.
  prefs: []
  type: TYPE_NORMAL
- en: The final step will involve deploying the trained model utilizing the SageMaker
    hosting service. You will also explore SageMaker Canvas to learn how to train
    an ML model without any coding. After the lab, you will be able to use SageMaker
    as a data science tool for various experimentation, model training, and model
    deployment tasks. SageMaker has many other features.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive in!
  prefs: []
  type: TYPE_NORMAL
- en: Setting up SageMaker Studio
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Follow these steps to set up a SageMaker Studio environment:'
  prefs: []
  type: TYPE_NORMAL
- en: To create a SageMaker Studio environment, we need to set up a domain and a user
    profile in the respective AWS Region. Navigate to the SageMaker management console,
    once you’ve logged in to the AWS Management Console, and click on the **Studio**
    link on the left.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the right-hand side of the screen, click on the **Create a SageMaker Domain**
    button. Choose the **Setup for Single User** option and click on **Set up.** It
    will take a few minutes for the domain and a default user profile to be created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To start the Studio environment for the newly created user, click on the **Studio**
    link again, select the user profile you just created, and click on **Open Studio**
    to launch Studio. It will take a few minutes for the Studio environment to appear.
    Once everything is ready, you will see a screen that is similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20836_08_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.12: Studio UI'
  prefs: []
  type: TYPE_NORMAL
- en: Launching a JupyterLab notebook
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, we need to launch a JupyterLab application within the SageMaker Studio
    UI so we can have a Jupyter Notebook environment for authoring model-building
    scripts and training ML models. Continue with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Select the **JupterLab** application under the **Applications** section in the
    left navigation pane.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Create JupyterLab Space** on the right-hand side to create a space
    for the JupyterLab. Provide a name for the space on the subsequent pop-up screen
    and click **Create space**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next screen, change the storage to **20 GB**, select **ml.g5.xLarge**,
    and keep all other configurations the same, and then click on **Run Space**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It will take some time for the space to be created. Once it is ready, click
    on **Open JupyterLab** to launch it, and it will be launched in a separate tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training the BERT model in the Jupyter notebook
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this part of the hands-on exercise, we will train a financial sentiment analysis
    NLP model using the BERT transformer, which we learned about in *Chapter 3*, *Exploring
    ML Algorithms*. To get started, create a new notebook to author our code by selecting
    **File** > **New** > **Notebook** from the menu dropdown. When prompted to select
    a kernel, pick **Python 3 (ipykernel)**. You can rename the file so that it has
    a more meaningful name by selecting **File** > **Rename Notebook** from the menu.
    Download the dataset from Kaggle at [https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news](https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news).
    Note that you will need a Kaggle account to download it. Once it’s been downloaded,
    you should see an `archive.zip` file. Unzip the file using unzip tool on your
    local machine.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s upload the `data` file to the Studio notebook. Create a new folder
    called `data` in the same folder where the new notebook is located and upload
    it to the `data` directory using the **File Upload** utility (the up arrow icon)
    in Studio UI. Select `all-data.csv` to upload.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s install some additional packages for our exercise. Run the following
    code block inside the notebook cell to install the transformer package. The transformer
    package provides a list of pre-trained transformers such as BERT. You will use
    these transformers to fine-tune an ML task. Note that some of the code block samples
    are not complete. You can find the complete code samples at [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter08/bert-financial-sentiment.ipynb](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter08/bert-financial-sentiment.ipynb):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Restart the kernel of the notebook after installing `ipywidgets`. Next, import
    some libraries into the notebook and set up the logger for logging purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are ready to load the `data` file and process it. The following code
    block loads the `data` file and splits the data into train and test datasets.
    We will select the first two columns from the file and name them `sentiment` and
    `article`. The `sentiment` column is the label column. It contains three different
    unique values (`negative`, `neutral`, and `positive`). Since they are string values,
    we will convert them into integers (`0, 1, 2`) using `OrdinalEncoder` from the
    scikit-learn library. We also need to determine the max length of the article
    column. The max length is used to prepare the input for the transformer since
    the transformer requires a fixed length:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will build a list of utility functions to support the data loading
    and model training. We need to feed data to the transformer model in batches.
    The following `get_data_loader()` function loads the dataset into the PyTorch
    `DataLoader` class with a specified batch size. Note that we also encode the articles
    into tokens with the `BertTokenizer` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following `train()` function will run the training loop using the `BertForSequenceClassification`
    class. We will use the pre-trained BERT model for fine-tuning, instead of training
    from scratch. We will feed one batch of data to the BERT model at a time. Note
    that we will also check if there is a GPU device on the server. If there is one,
    we will use the `cuda` device for GPU training, instead of `cpu` for CPU training.
    We need to manually move the data and BERT model to the same target device using
    the `.to(device)` function so that the training can happen on the target device
    with the data residing in memory on the same device. The optimizer we’re using
    here is AdamW, which is a variant of the gradient descent optimization algorithm.
    The training loop will run through the number of epochs specified. One epoch runs
    through the entire training dataset once:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We also want to test the model’s performance using a separate test dataset
    during training. To do this, we will implement the following `test()` function,
    which is called by the `train()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have all the functions needed to load and process data, run the training
    loop, and measure the model metrics using a test dataset. With that, we can kick
    off the training process. We will use the `args` variable to set up various values,
    such as batch size, data location, and learning rate, to be used by the training
    loop and the testing loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Once you have run the preceding code, you should see training stats for each
    batch and epoch. The model will also be saved in the specified directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s see how the trained model can be used to make predictions directly.
    To do this, we must implement several utility functions. The following `input_fn()`
    function takes input in JSON format and outputs an input vector that represents
    the string input and its associated mask. The output will be sent to the model
    for prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following `predict_fn()` function takes `input_data` returned by `input_fn()`
    and uses the trained model to generate the prediction. Note that we will also
    use a GPU if a GPU device is available on the server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, run the following code to generate a prediction. Replace the value of
    the article with different financial text to see the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: With SageMaker, you have different options to train ML models. For quick experiments
    and light model building, the Jupyter Notebook environment is sufficient for many
    model training tasks. For more resource-demanding ML training tasks, we need to
    consider dedicated training resources for model training. In the next section,
    let us look at an alternative way to train the BERT model using the SageMaker
    training service.
  prefs: []
  type: TYPE_NORMAL
- en: Training the BERT model with the SageMaker Training service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the previous section, you trained the BERT model directly inside a GPU-based
    Jupyter notebook. Instead of provisioning a GPU-based notebook instance, you can
    provision a less costly CPU-based instance and send the model training task to
    the SageMaker Training service. To use the SageMaker Training service, you need
    to make some minor changes to the training script and create a separate launcher
    script to kick off the training. As we discussed in the *Training ML models* section,
    there are three main approaches to training a model in SageMaker. Since SageMaker
    provides a managed container for PyTorch, we will use the managed container approach
    to train the model. With this approach, you will need to provide the following
    inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: A training script as the entry point, as well as dependencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An IAM role to be used by the training job
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Infrastructure details such as the instance type and number
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A data (training/validation/testing) location in S3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A model output location in S3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameters for training the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When a training job is started, the SageMaker Training service will perform
    the following tasks in sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: Launch the EC2 instances needed for the training job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the data from S3 to the training host.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the appropriate managed container from the SageMaker ECR registry and
    run the container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the training script and dependencies to the training container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the training script and pass the hyperparameters as command-line arguments
    to the training script. The training script will load the training/validation/testing
    data from specific directories in the container, run the training loop, and save
    the model to a specific directory in the container. Several environment variables
    will be set in the container to provide configuration details, such as directories
    for the data and model output, to the training script.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the training script exits with success, the SageMaker Training service
    will copy the saved model artifacts from the container to the model output location
    in S3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, let’s create the following training script, name it `train.py`, and save
    it in a new directory called `code`. Note that the training script is almost the
    same as the code in the *Training the BERT model in the Jupyter notebook* section.
    We have added an `if __name__ == "__main__":` section at the end. This section
    contains the code for reading the values of the command-line arguments and the
    values of the system environment variables such as SageMaker’s data directory
    (`SM_CHANNEL_TRAINING`), the model output directory (`SM_MODEL_DIR`), and the
    number of GPUs (`SM_NUM_GPUS`) available on the host. The following code sample
    is not complete. You can find the complete code sample at [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter08/code/train.py](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter08/code/train.py):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding script requires library packages that are not available in the
    managed training container. You can install custom library packages using the
    `requirement.txt` file. Create a `requirement.txt` file with the following code
    and save it in the `code` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s create a launcher notebook for kicking off the training job using
    the SageMaker Training service. The launcher notebook will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Upload the training and test datasets to the S3 bucket and folders.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set up the SageMaker PyTorch estimator using the SageMaker SDK to configure
    the training job.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kick off the SageMaker training job.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a new notebook called `bert-financial-sentiment-launcher.ipynb` in the
    folder where the `code` folder is located and copy the following code block into
    the notebook one cell at a time. When you’re prompted to choose a kernel, pick
    the **Python 3 (ipykernel)** kernel.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code specifies the S3 bucket to be used for saving the training
    and testing dataset, as well as the model artifacts. You can use the bucket that
    was created earlier in the *Setting up SageMaker Studio* section, when the Studio
    domain was configured. The training and test datasets we created earlier will
    be uploaded to the bucket. The `get_execution_role()` function returns the IAM
    role associated with the notebook, which we will use to run the training job later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we must set up the SageMaker PyTorch estimator and kick off the training
    job. Note that you can also specify the PyTorch framework version and Python version
    to set up the container. For simplicity, we are passing the name of the training
    file and test file, as well as the max length, as hyperparameters. The `train.py`
    file can also be modified to look them up dynamically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Once the training job has been completed, you can go to the SageMaker management
    console to access the training job’s details and metadata. Training jobs also
    send outputs to CloudWatch Logs and CloudWatch metrics. You can navigate to these
    logs by clicking on the respective links on the training job details page.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this step, we will deploy the trained model to a SageMaker RESTful endpoint
    so that it can be integrated with downstream applications. We will use the managed
    PyTorch serving container to host the model. With the managed PyTorch serving
    container, you can provide an inference script to process the request data before
    it is sent to the model for inference, as well as controlling how to call the
    model for inference. Let’s create a new script called `inference.py` in the `code`
    folder that contains the following code block. As you have probably noticed, we
    have used the same functions that we used in *Training the BERT model in the Jupyter
    notebook* section for the predictions. Note that you need to use the same function
    signatures for these two functions as SageMaker will be looking for the exact
    function name and parameter lists. You can find the complete source code at [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter08/code/inference.py](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter08/code/inference.py):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to modify the `bert-financial-sentiment-launcher.ipynb` file
    to create the endpoint. You can deploy trained models from the SageMaker `estimator`
    class directly. Here, however, we want to show you how to deploy a model that’s
    been trained previously, as this is the most likely deployment scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'After the model has been deployed, we can call the model endpoint to generate
    some predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Try out different phrases and see if the model predicts the sentiment correctly.
    You can also access the endpoint’s details by navigating to the SageMaker management
    console and clicking on the endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid any ongoing costs for the endpoint, let’s delete it. Run the following
    command in a new cell to delete the endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Congratulations – you have finished building a basic data science environment
    and used it to train and deploy an NLP model to detect its sentiment! If you don’t
    want to keep this environment to avoid any associated costs, make sure that you
    shut down any instances of the SageMaker Studio notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s explore SageMaker Canvas and see how to use it to build custom ML
    models without any coding.
  prefs: []
  type: TYPE_NORMAL
- en: Building ML models with SageMaker Canvas
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this lab, we will train a customer churn classification model using Canvas’s
    custom model feature. We will go through the full cycle from dataset creation/selection,
    model training, and model analysis to prediction generation and model deployment.
  prefs: []
  type: TYPE_NORMAL
- en: To get started, we first need to launch a SageMaker Canvas environment. To do
    that, go back to your Studio environment, select **Canvas** under the **Applications**
    section, and click on the **Run Canvas** button in the right-hand pane. It is
    going to take 8-10 minutes for the Canvas environment to become available. When
    the Canvas status changes to **Running**, click on **Open Canvas**, and you will
    see a screen similar to *Figure 8.13*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20836_08_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.13: SageMaker Canvas'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will guide you through the rest of the lab:'
  prefs: []
  type: TYPE_NORMAL
- en: Select the **My models** icon on the left-hand pane to start building a custom
    model using a custom dataset. You should see a screen similar to *Figure 8.14*:![](img/B20836_08_14.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 8.14: Canvas My models screen'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click on the **New model** button,provide a name for the model, select **Predictive
    analysis** as the problem type, and click on **Create**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the dataset from [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter08/churn.csv](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter08/churn.csv)
    and save it to your local machine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the **Select dataset** screen, click on the **Create data** link in the top-right
    corner. Provide a name for the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next screen, click on **Select files from your local computer** and navigate
    to the `churn.csv` file you downloaded in *step 4* to upload the file. After the
    file is uploaded, click on **Create dataset** to continue.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next screen, check the dataset you have just created, and click on **Select
    datase**t to continue.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next screen, you will be asked to select a target column to predict.
    Select the **Exited** column from the dataset. Also, you should uncheck some of
    the source columns, such as surname and row number, as they are not relevant to
    model training. Finally, select **Quick build** to build a model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next screen, you will see some information about the duration of the
    build and the build type. Now you will need to wait for the build process to complete.
    When it is complete, you will see a screen similar to the following figure. You
    will be able to review the various training metrics, such as accuracy, precision,
    and recall. You will also be able to see the impact of different source columns
    on the target columns.![](img/B20836_08_15.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 8.15: Model training results'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Since model performance optimization is not our primary goal here, we will click
    on **Predict** to generate some predictions on the next screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next screen, select the **Single prediction** option, and change the
    values for some of the fields, such as age, salary, and credit score, to see how
    they impact the results by clicking on **Update**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lastly, we will deploy the model to an endpoint so it can be used by other applications.
    To deploy, you simply click on the **Deploy** button to deploy the model. You
    will have the option to select an instance type and the number of instances for
    the model. Upon successful deployment, a deployment URL will be made available
    for other applications to use.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Congratulations on completing the lab! You have effectively trained and deployed
    a binary classification model using your custom dataset without any code, using
    SageMaker Canvas. This no-code ML tool facilitates a swift initiation into ML
    projects, even for individuals without prior ML knowledge. You now have first-hand
    experience with how Canvas automates numerous tasks for you, ranging from algorithm
    selection and model training to model deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored how a data science environment can provide a scalable
    infrastructure for experimentation, model training, and model deployment for testing
    purposes. You learned about the core architecture components for building a fully
    managed data science environment using AWS services such as Amazon SageMaker,
    Amazon ECR, and Amazon S3\. You practiced setting up a data science environment
    and trained and deployed an NLP model using both SageMaker Studio notebooks and
    the SageMaker Training service. You have also developed hands-on experience with
    SageMaker Canvas to automate ML tasks from model building to model deployment.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you should be able to talk about the key components of a data
    science environment, as well as how to build one using AWS services and use it
    for model building, training, and deployment. In the next chapter, we will talk
    about how to build an enterprise ML platform for scale through automation.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/mlsah](https://packt.link/mlsah )'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code70205728346636561.png)'
  prefs: []
  type: TYPE_IMG
