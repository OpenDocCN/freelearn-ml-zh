["```py\n[akozlov@Alexanders-MacBook-Pro ~]$ scala\nWelcome to Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40).\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala>\n\n```", "```py\n[akozlov@Alexanders-MacBook-Pro]$ scala\nâ€¦\nscala> import scala.sys.process._\nimport scala.sys.process._\nscala> val histogram = ( \"gzcat chapter01/data/clickstream/clickstream_sample.tsv.gz\"  #|  \"cut -f 10\" #| \"sort\" #|  \"uniq -c\" #| \"sort -k1nr\" ).lineStream\nhistogram: Stream[String] = Stream(7731 http://www.mycompany.com/us/en_us/, ?)\nscala> histogram take(10) foreach println \n7731 http://www.mycompany.com/us/en_us/\n3843 http://mycompanyplus.mycompany.com/plus/\n2734 http://store.mycompany.com/us/en_us/?l=shop,men_shoes\n2400 http://m.mycompany.com/us/en_us/\n1750 http://store.mycompany.com/us/en_us/?l=shop,men_mycompanyid\n1556 http://www.mycompany.com/us/en_us/c/mycompanyid?sitesrc=id_redir\n1530 http://store.mycompany.com/us/en_us/\n1393 http://www.mycompany.com/us/en_us/?cp=USNS_KW_0611081618\n1379 http://m.mycompany.com/us/en_us/?ref=http%3A%2F%2Fwww.mycompany.com%2F\n1230 http://www.mycompany.com/us/en_us/c/running\n\n```", "```py\nscala> import scala.sys.process._\nimport scala.sys.process._\nscala> val nums = ( \"gzcat chapter01/data/clickstream/clickstream_sample.tsv.gz\"  #|  \"cut -f 6\" ).lineStream\nnums: Stream[String] = Stream(0, ?) \nscala> val m = nums.map(_.toDouble).min\nm: Double = 0.0\nscala> val m = nums.map(_.toDouble).sum/nums.size\nm: Double = 3.6883642764024662\nscala> val m = nums.map(_.toDouble).max\nm: Double = 33.0\n\n```", "```py\nimport scala.util.Random\nimport util.Properties\n\nval threshold = 0.05\n\nval lines = scala.io.Source.fromFile(\"chapter01/data/iris/in.txt\").getLines\nval newLines = lines.filter(_ =>\n    Random.nextDouble() <= threshold\n)\n\nval w = new java.io.FileWriter(new java.io.File(\"out.txt\"))\nnewLines.foreach { s =>\n    w.write(s + Properties.lineSeparator)\n}\nw.close\n```", "```py\nimport scala.reflect.ClassTag\nimport scala.util.Random\nimport util.Properties\n\ndef reservoirSample[T: ClassTag](input: Iterator[T],k: Int): Array[T] = {\n  val reservoir = new Array[T](k)\n  // Put the first k elements in the reservoir.\n  var i = 0\n  while (i < k && input.hasNext) {\n    val item = input.next()\n    reservoir(i) = item\n    i += 1\n  }\n\n  if (i < k) {\n    // If input size < k, trim the array size\n    reservoir.take(i)\n  } else {\n    // If input size > k, continue the sampling process.\n    while (input.hasNext) {\n      val item = input.next\n      val replacementIndex = Random.nextInt(i)\n      if (replacementIndex < k) {\n        reservoir(replacementIndex) = item\n      }\n      i += 1\n    }\n    reservoir\n  }\n}\n\nval numLines=15\nval w = new java.io.FileWriter(new java.io.File(\"out.txt\"))\nval lines = io.Source.fromFile(\"chapter01/data/iris/in.txt\").getLines\nreservoirSample(lines, numLines).foreach { s =>\n    w.write(s + scala.util.Properties.lineSeparator)\n}\nw.close\n```", "```py\nval origLinesRdd = sc.textFile(\"file://...\")\nval keyedRdd = origLines.keyBy(r => r.split(\",\")(0))\nval fractions = keyedRdd.countByKey.keys.map(r => (r, 0.1)).toMap\nval sampledWithKey = keyedRdd.sampleByKeyExact(fractions)\nval sampled = sampledWithKey.map(_._2).collect\n```", "```py\nimport scala.util.hashing.MurmurHash3._\n\nval markLow = 0\nval markHigh = 4096\nval seed = 12345\n\ndef consistentFilter(s: String): Boolean = {\n  val hash = stringHash(s.split(\" \")(0), seed) >>> 16\n  hash >= markLow && hash < markHigh\n}\n\nval w = new java.io.FileWriter(new java.io.File(\"out.txt\"))\nval lines = io.Source.fromFile(\"chapter01/data/iris/in.txt\").getLines\nlines.filter(consistentFilter).foreach { s =>\n     w.write(s + Properties.lineSeparator)\n}\nw.close\n```", "```py\n[akozlov@Alexanders-MacBook-Pro]$ wget http://s3.eu-central-1.amazonaws.com/spark-notebook/zip/spark-notebook-0.6.3-scala-2.11.7-spark-1.6.1-hadoop-2.6.4-with-hive-with-parquet.zip\n...\n[akozlov@Alexanders-MacBook-Pro]$ unzip -d ~/ spark-notebook-0.6.3-scala-2.11.7-spark-1.6.1-hadoop-2.6.4-with-hive-with-parquet.zip\n...\n[akozlov@Alexanders-MacBook-Pro]$ ln -sf ~/ spark-notebook-0.6.3-scala-2.11.7-spark-1.6.1-hadoop-2.6.4-with-hive-with-parquet ~/spark-notebook\n[akozlov@Alexanders-MacBook-Pro]$ cp chapter01/notebook/Chapter01.snb ~/spark-notebook/notebooks\n[akozlov@Alexanders-MacBook-Pro]$ cp chapter01/ data/kddcup/kddcup.parquet ~/spark-notebook\n[akozlov@Alexanders-MacBook-Pro]$ cd ~/spark-notebook\n[akozlov@Alexanders-MacBook-Pro]$ bin/spark-notebook \nPlay server process ID is 2703\n16/04/14 10:43:35 INFO play: Application started (Prod)\n16/04/14 10:43:35 INFO play: Listening for HTTP on /0:0:0:0:0:0:0:0:9000\n...\n\n```", "```py\nval labelCount = df.groupBy(\"lbl\").count().collect\nlabelCount.toList.map(row => (row.getString(0), row.getLong(1)))\n```", "```py\nsqlContext.sql(\"SELECT lbl, protocol_type, min(duration), avg(duration), stddev(duration), max(duration) FROM parquet.`kddcup.parquet` group by lbl, protocol_type\")\n```", "```py\nval pct = sqlContext.sql(\"SELECT duration FROM parquet.`kddcup.parquet` where protocol_type = 'udp'\").rdd.map(_.getLong(0)).cache\npct.top((0.05*pct.count).toInt).last\n```", "```py\n[akozlov@Alexanders-MacBook-Pro]$ pbcopy < chapter01/data/iris/in.txt\n\n```"]