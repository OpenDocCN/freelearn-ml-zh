<html><head></head><body><div class="chapter" title="Chapter&#xA0;11.&#xA0;Deep Learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch11"/>Chapter 11. Deep Learning</h1></div></div></div><p>In this chapter, we will cover the following recipe:</p><p>Recurrent neural networks - predicting periodic signals</p><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec52"/>Introduction</h1></div></div></div><p>Most of the machine learning algorithms work well due to predefined representations and input features. Machine learning algorithms optimize weights to best make a final prediction, while representation learning attempt to automatically learn good features or representations. Deep learning algorithms attempt to learn at multiple levels of representation by increasing complexity. Deep architectures are composed of multiple levels of non-linear operations, such as neural nets with many hidden layers. The main goal of deep learning techniques is to learn feature hierarchies. Deep learning techniques can be divided into three major classes; deep networks for unsupervised or generative learning, deep networks for supervised learning and hybrid deep networks   </p></div></div>
<div class="section" title="Recurrent neural networks - predicting periodic signals"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec53"/>Recurrent neural networks - predicting periodic signals</h1></div></div></div><p>Oscillators are circuits that produce specific, periodic waveforms such as square, triangular, sawtooth, and sinusoidal. In order to generate output, oscillators generally use some form of active device-lamp, which is surrounded by resistors, capacitors, and inductors. Two main classes of oscillators are relaxation and sinusoidal. Triangular, sawtooth and other non-sinusoidal waveforms are generated using relaxation oscillators, while sinusoidal oscillators consist of amplifiers with external components to generate oscillation. Normally, no harmonics are present in pure sine waves and they consist of a single frequency.</p><div class="section" title="Getting ready..."><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec66"/>Getting ready...</h2></div></div></div><p>The task is to predict a cosine from a noisy sine wave. 5Hz frequency waves are used for the sine wave with some normally distributed noise and a smooth cosine wave. The dataset created is a set of 10 sequences, each of which consists of 40 observations.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec67"/>How to do it...</h2></div></div></div><p>The following packages need to be loaded as the first step to be carried out:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; install.packages("rnn")</strong></span>
<span class="strong"><strong>    &gt; library(rnn)</strong></span>
</pre><p>Setting the initial seed as a random number for the purpose of reproducibility:</p><pre class="programlisting">
<span class="strong"><strong>&gt; set.seed(10)</strong></span>
</pre><p>Initializing the required frequency:</p><pre class="programlisting">
<span class="strong"><strong>&gt; f &lt;- 5</strong></span>
</pre><p>Creating the vector required:</p><pre class="programlisting">
<span class="strong"><strong>&gt; w &lt;- 2*pi*f</strong></span>
</pre><p>Generating sequences: The <code class="literal">seq()</code> function generates regular sequences. <code class="literal">0.005</code> is the starting value while <code class="literal">2</code> is the ending value. <code class="literal">by=0.005</code> determines the incremental sequence:</p><pre class="programlisting">
<span class="strong"><strong>&gt; t &lt;- seq(0.005,2,by=0.005)</strong></span>
</pre><p>Generating <code class="literal">sin</code> and <code class="literal">cos</code> values:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; x &lt;- sin(t*w) + rnorm(200, 0, 0.25)</strong></span>
<span class="strong"><strong>    &gt; y &lt;- cos(t*w)</strong></span>
</pre><p>Generating samples of time series: The <code class="literal">matrix()</code> function creates a matrix from <code class="literal">x</code> and <code class="literal">y</code> values. <code class="literal">nrow = 40</code> indicates the number of rows required:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; X &lt;- matrix(x, nrow = 40)</strong></span>
<span class="strong"><strong>    &gt; Y &lt;- matrix(y, nrow = 40)</strong></span>
</pre><p>Plotting the noisy wave:. The <code class="literal">plot()</code> function is a generic function for the plotting of R objects. The <code class="literal">as.vector(X)</code> data frame is passed as a function value. <code class="literal">type='l'</code> signifies lines:</p><pre class="programlisting">
<span class="strong"><strong>&gt; plot(as.vector(X), col='blue', type='l', ylab = "x-matrix, y-matrix", main = "Noisy waves")</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_001.jpg" alt="How to do it..."/></div><p>
</p><pre class="programlisting">
<span class="strong"><strong>&gt; lines(as.vector(Y), col = "red")</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_002.jpg" alt="How to do it..."/></div><p>
</p><p>Standardizing the values of <code class="literal">X</code>. The range of values lies between 0 and 1:</p><pre class="programlisting">
<span class="strong"><strong>&gt; X &lt;- (X - min(X)) / (max(X) - min(X))</strong></span>
</pre><p>Printing the values of <code class="literal">X</code>:</p><pre class="programlisting">
<span class="strong"><strong>&gt; X</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_003.jpg" alt="How to do it..."/></div><p>
</p><p>Standardizing the values of <code class="literal">Y</code>. The range of values lies between 0 and 1:</p><pre class="programlisting">
<span class="strong"><strong>&gt; X &lt;- (X - min(X)) / (max(X) - min(X))</strong></span>
</pre><p>Printing the values of <code class="literal">X</code>:</p><pre class="programlisting">
<span class="strong"><strong>&gt; X</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_004.jpg" alt="How to do it..."/></div><p>
</p><p>Transposing the values of <code class="literal">X</code> and <code class="literal">Y</code>:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; X &lt;- t(X)</strong></span>
<span class="strong"><strong>    &gt; Y &lt;- t(Y)</strong></span>
</pre><p>Creating training and testing sets:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; train &lt;- 1:8</strong></span>
<span class="strong"><strong>    &gt; test &lt;- 9:10</strong></span>
</pre><p>Training the recurrent neural network. <code class="literal">Y = Y[train,]</code> signifies an array of output values. <code class="literal">X = X[train,]</code> signifies an array of input values. <code class="literal">learningrate = 0.05</code> means the rate to be applied for weight iteration. <code class="literal">hidden_dim = 16</code> is the dimension of hidden layers. <code class="literal">numepochs = 1500</code> is the number of times the whole dataset undergoes training.</p><p>This phase will take time. The time taken depends on the learning rate, the number of dimensions, and the number of times the whole dataset undergoes training:</p><pre class="programlisting">
<span class="strong"><strong>&gt; model &lt;- trainr(Y = Y[train,],X = X[train,],learningrate = 0.05,hidden_dim = 16,numepochs = 1500)</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_005.jpg" alt="How to do it..."/></div><p>
</p><p>Predicting the output of the recurrent neural network:</p><pre class="programlisting">
<span class="strong"><strong>&gt; Y_predicted &lt;- predictr(model, X)</strong></span>
</pre><p>Plotting the <span class="strong"><strong>Actual values vs the Predicted values</strong></span>. The output constitutes the training set and the testing set:</p><pre class="programlisting">
<span class="strong"><strong>&gt; plot(as.vector(t(Y)), col = 'red', type = 'l', main = "Actual values vs Predicted values", ylab = "Y, Y-predicted")</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_006.jpg" alt="How to do it..."/></div><p>
</p><pre class="programlisting">
<span class="strong"><strong>&gt; lines(as.vector(t(Y_predicted)), type = 'l', col = 'blue')</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_007.jpg" alt="How to do it..."/></div><p>
</p><p>Plotting the <span class="strong"><strong>Actual values vs the Predicted values</strong></span>. The output constitutes the testing set only:</p><pre class="programlisting">
<span class="strong"><strong>&gt; plot(as.vector(t(Y[test,])), col = 'red', type='l', main = "Actual vs predicted: testing set", ylab = "Y,Y-predicted")</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_008.jpg" alt="How to do it..."/></div><p>
</p><pre class="programlisting">
<span class="strong"><strong>&gt; lines(as.vector(t(Y_predicted[test,])), type = 'l', col = 'blue')</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_009.jpg" alt="How to do it..."/></div><p>
</p></div></div></body></html>