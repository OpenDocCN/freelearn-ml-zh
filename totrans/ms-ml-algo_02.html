<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Introduction to Semi-Supervised Learning</h1>
                </header>
            
            <article>
                
<p class="mce-root">Semi-supervised learning is a machine learning branch that tries to solve problems with both labeled and unlabeled data with an approach that employs concepts belonging to clustering and classification methods. The high availability of unlabeled samples, in contrast with the difficulty of labeling huge datasets correctly, drove many researchers to investigate the best approaches that allow extending the knowledge provided by the labeled samples to a larger unlabeled population without loss of accuracy. In this chapter, we're going to introduce this branch and, in particular, we will discuss:</p>
<ul>
<li>The semi-supervised scenario</li>
<li>The assumptions needed to efficiently operate in such a scenario</li>
<li>The different approaches to semi-supervised learning</li>
<li>Generative Gaussian mixtures algorithm</li>
<li>Contrastive pessimistic likelihood estimation approach</li>
<li><strong>Semi-supervised Support Vector Machines</strong> (<strong>S<sup>3</sup>VM</strong>)</li>
<li><strong>Transductive Support Vector Machines</strong> (<strong>TSVM</strong>)</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Semi-supervised scenario</h1>
                </header>
            
            <article>
                
<p>A typical semi-supervised scenario is not very different from a supervised one. Let's suppose we have a data generating process, <em>p<sub>data</sub></em>:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/163ed14d-061e-48f3-89d5-693b530d0525.png" style="width:13.75em;height:1.58em;"/></p>
<p>However, contrary to a supervised approach, we have only a limited number <em>N</em> of samples drawn from <em>p<sub>data</sub></em> and provided with a label, as follows:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/933c6eb6-6b35-4f83-b713-adaabda6ce98.png" style="width:22.17em;height:3.92em;"/></p>
<p class="mce-root">Instead, we have a larger amount (<em>M</em>) of unlabeled samples drawn from the marginal distribution <em>p(x)</em>:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/882d2b8f-70b6-4891-92b6-6cca8e919657.png" style="width:22.75em;height:1.83em;"/></p>
<p>In general, there are no restrictions on the values of <em>N</em> and <em>M;</em> however, a semi-supervised problem arises when the number of unlabeled samples is much higher than the number of complete samples. If we can draw <em>N</em> &gt;&gt; <em>M</em> labeled samples from <em>p<sub>data</sub></em>, it's probably useless to keep on working with semi-supervised approaches and preferring classical supervised methods is likely to be the best choice. The extra complexity we need is justified by <em>M</em> &gt;&gt; <em>N</em>, which is a common condition in all those situations where the amount of available unlabeled data is large, while the number of correctly labeled samples is quite a lot lower. For example, we can easily access millions of free images but detailed labeled datasets are expensive and include only a limited subset of possibilities. However, is it always possible to apply semi-supervised learning to improve our models? The answer to this question is almost obvious: unfortunately no. As a rule of thumb, we can say that if the knowledge of <em>X<sub>u</sub></em> increases our knowledge about the prior distribution <em>p(x)</em>, a semi-supervised algorithm is likely to perform better than a purely supervised (and thus limited to <em>X<sub>l</sub></em>) counterpart. On the other hand, if the unlabeled samples are drawn from different distributions, the final result can be quite a lot worse. In real cases, it's not so immediately necessary to decide whether a semi-supervised algorithm is the best choice; therefore, cross-validation and comparisons are the best practices to employ when evaluating a scenario.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transductive learning</h1>
                </header>
            
            <article>
                
<p>When a semi-supervised model is aimed at finding the labels for the unlabeled samples, the approach is called transductive learning. In this case, we are not interested in modeling the whole distribution <em>p(x|y)</em>, which implies determining the density of both datasets, but rather in finding <em>p(y|x)</em> only for the unlabeled points. In many cases, this strategy can be time-saving and it's always preferable when our goal is more oriented at improving our knowledge about the unlabeled dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Inductive learning</h1>
                </header>
            
            <article>
                
<p>Contrary to transductive learning, inductive learning<strong> </strong>considers all the <em>X</em> samples and tries to determine a complete <em>p(x|y)</em> or a function <em>y=f(x)</em> that can map both labeled and unlabeled points to their corresponding labels. In general, this method is more complex and requires more computational time; therefore, according to <em>Vapnik's principle</em>, if not required or necessary, it's always better to pick the most pragmatic solution and, possibly, expand it if the problem requires further details.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Semi-supervised assumptions</h1>
                </header>
            
            <article>
                
<p>As explained in the previous section, semi-supervised learning is not guaranteed to improve a supervised model. A wrong choice could lead to a dramatic worsening in performance; however, it's possible to state some fundamental assumptions which are required for semi-supervised learning to work properly. They are not always mathematically proven theorems, but rather empirical observations that justify the choice of an approach otherwise completely arbitrary.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Smoothness assumption</h1>
                </header>
            
            <article>
                
<p>Let's consider a real-valued function <em>f(x)</em> and the corresponding metric spaces <em>X</em> and <em>Y</em>. Such a function is said to be Lipschitz-continuous if:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5a428b7c-fac1-4bac-acc0-b0c195912935.png" style="width:31.92em;height:1.50em;"/></p>
<p>In other words, if two points <em>x<sub>1</sub></em> and <em>x<sub>2</sub></em> are near, the corresponding output values <em>y<sub>1</sub></em> and <em>y<sub>2</sub></em> cannot be arbitrarily far from each other. This condition is fundamental in regression problems where a generalization is often required for points that are between training samples. For example, if we need to predict the output for a point <em>x<sub>t</sub></em> : <em>x</em><sub><em>1</em> </sub>&lt; <em>x<sub>t</sub></em> &lt; <em>x<sub>2</sub></em> and the regressor is Lipschitz-continuous, we can be sure that <em>y<sub>t</sub></em> will be correctly bounded by <em>y<sub>1</sub></em> and <em>y<sub>2</sub></em>. This condition is often called general smoothness, but in semi-supervised it's useful to add a restriction (correlated with the cluster assumption): if two points are in a high density region (cluster) and they are close, then the corresponding outputs must be close too. This extra condition is very important because, if two samples are in a low density region they can belong to different clusters and their labels can be very different. This is not always true, but it's useful to include this constraint to allow some further assumptions in many definitions of semi-supervised models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cluster assumption</h1>
                </header>
            
            <article>
                
<p>This assumption is strictly linked to the previous one and it's probably easier to accept. It can be expressed with a chain of interdependent conditions. Clusters are high density regions; therefore, if two points are close, they are likely to belong to the same cluster and their labels must be the same. Low density regions are separation spaces; therefore, samples belonging to a low density region are likely to be boundary points and their classes can be different. To better understand this concept, it's useful to think about supervised SVM: only the support vectors should be in low density regions. Let's consider the following bidimensional example:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/4f8fbd46-2ca9-4bd9-9266-13e3125c1913.png" style="width:54.25em;height:32.50em;"/></div>
<p class="mce-root">In a semi-supervised scenario, we couldn't know the label of a point belonging to a high density region; however, if it is close enough to a labeled point that it's possible to build a ball where all the points have the same average density, we are allowed to predict the label of our test sample. Instead, if we move to a low-density region, the process becomes harder, because two points can be very close but with different labels. We are going to discuss the semi-supervised, low-density separation problem at the end of this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Manifold assumption</h1>
                </header>
            
            <article>
                
<p>This is the less intuitive assumption, but it can be extremely useful to reduce the complexity of many problems. First of all, we can provide a non-rigorous definition of a manifold. An <em>n</em>-manifold is a topological space that is globally curved, but locally homeomorphic to an <em>n</em>-dimensional Euclidean space. In the following diagram, there's an example of a manifold: the surface of a sphere in <em>ℜ<sup>3</sup></em>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/78afa71a-9c17-4388-8ce7-dc7ec95c1df4.png" style="width:41.33em;height:24.67em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">2D manifold obtained from a spherical surface</div>
<p>The small patch around <em>P</em> (for <em>ε</em> → <em>0</em>) can be mapped to a flat circular surface. Therefore, the properties of a manifold are locally based on the Euclidean geometry, while, globally, they need a proper mathematical extension which is beyond the scope of this book (further information can be found in <em><span>Semi-supervised learning on Riemannian manifolds</span></em>,<em><span> Belkin M., Niyogi P.</span></em>, <em><span>Machine Learning 56</span></em>, <em><span>2004</span></em>).</p>
<p>The manifold assumption states that <em>p</em>-dimensional samples (where <em>p</em> &gt;&gt; <em>1</em>) approximately lie on a <em>q</em>-dimensional manifold with <em>p</em> &lt;&lt; <em>q</em>. Without excessive mathematical rigor, we can say that, for example, if we have <em>N</em> <em>1000</em>-dimensional bounded vectors, they are enclosed into a <em>1000</em>-dimensional hypercube with edge-length equal to <em>r</em>. The corresponding <em>n</em>-volume is <em>r<sup>p</sup> = r<sup>1000</sup></em>, therefore, the probability of filling the entire space is very small (and decreases with <em>p</em>). What we observe, instead, is a high density on a lower dimensional manifold. For example, if we look at the Earth from space, we might think that its inhabitants are uniformly distributed over the whole volume. We know that this is false and, in fact, we can create maps and atlases which are represented on two-dimensional manifolds. It doesn't make sense to use three-dimensional vectors to map the position of a human being. It's easier to use a projection and work with latitude and longitude.</p>
<p>This assumption <span>authorizes us to apply dimensionality reduction methods in order to avoid the</span> <em>Curse of Dimensionality</em>, theorized by Bellman (in <em>Dynamic Programming and Markov Process, Ronald A. Howard</em>, <em>The MIT Press</em>).<span> In the scope of machine learning, the main consequence of such an effect is that when the dimensionality of the samples increases, in order to achieve a high accuracy, it's necessary to use more and more samples.</span> Moreover, Hughes observed (the phenomenon has been named after him and it's presented in the paper <em>Hughes G. F., On the mean accuracy of statistical pattern recognizers, IEEE Transactions on Information Theory</em>, <em>1968</em>, <em>14/1</em>) that the accuracy of statistical classifiers is inversely proportional to the dimensionality of the samples. This means that whenever it's possible to work on lower dimensional manifolds (in particular in semi-supervised scenarios), two advantages are achieved:</p>
<ul>
<li>Less computational time and memory consumption</li>
<li>Higher classification accuracy</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generative Gaussian mixtures</h1>
                </header>
            
            <article>
                
<p>Generative Gaussian mixtures is an inductive algorithm for semi-supervised clustering. Let's suppose we have a labeled dataset (<em>X<sub>l</sub></em>, <em>Y<sub>l</sub></em>) containing <em>N</em> samples (drawn from <em>p<sub>data</sub></em>) and an unlabeled dataset <em>X<sub>u</sub></em> containing <em>M</em> &gt;&gt; <em>N</em> samples (drawn from the marginal distribution <em>p(x)</em>). It's not necessary that <em>M</em> &gt;&gt; <em>N</em>, but we want to create a real semi-supervised scenario, with only a few labeled samples. Moreover, we are assuming that all unlabeled samples are consistent with <em>p<sub>data</sub></em>. This can seem like a vicious cycle, but without this assumption, the procedure does not have a strong mathematical foundation. Our goal is to determine a complete <em>p(x|y)</em> distribution using a generative model. In general, it's possible to use different priors, but we are now employing multivariate Gaussians to model our data:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4594a146-4eaf-4d21-9205-85bab6323788.png" style="width:23.42em;height:3.92em;"/></p>
<p>Thus, our model parameters are means and covariance matrices for all Gaussians. In other contexts, it's possible to use binomial or multinomial distributions. However, the procedure doesn't change; therefore, let's assume that it's possible to approximate <em>p(x|y)</em> with a parametrized distribution <em>p(x|y</em>, <em>θ)</em>. We can achieve this goal by minimizing the Kullback-Leibler divergence between the two distributions:</p>
<p class="mce-root CDPAlignCenter CDPAlign packt_figref"><img class="fm-editor-equation" src="assets/3149ad3a-070e-4f76-9370-e7dcf0c428dc.png" style="width:33.75em;height:3.67em;"/></p>
<p>In <a href="8d541a43-8790-4a91-a79b-e48496f75d90.xhtml">Chapter 5</a>, <em><span>EM Algorithm and Applications</span></em> we are going to show that this is equivalent to maximizing the likelihood of the dataset. To obtain the likelihood, it's necessary to define the number of expected Gaussians (which is known from the labeled samples) and a weight-vector that represents the marginal probability of a specific Gaussian:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4da88076-3c81-49ec-aecd-269f48a9c11e.png" style="width:21.50em;height:1.50em;"/></p>
<p>Using the Bayes' theorem, we get:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3712c8bb-c9fc-42c6-b8e0-a73b9373a13f.png" style="width:16.67em;height:1.67em;"/></p>
<p>As we are working with both labeled and unlabeled samples, the previous expression has a double interpretation:</p>
<ul>
<li>For unlabeled samples, it is computed by multiplying the <em>i<sup>th</sup></em> Gaussian weight times the probability <em>p(x<sub>j</sub>)</em> relative to the <em>i<sup>th</sup></em> Gaussian distribution.</li>
<li> For labeled samples, it can be represented by a vector p = [0, 0, ... 1, ... 0, 0] where 1 is the <em>i<sup>th</sup></em> element. In this way, we force our model to trust the labeled samples in order to find the best parameter values that maximize the likelihood on the whole dataset.</li>
</ul>
<p>With this distinction, we can consider a single log-likelihood function where the term <em>f<sub>w</sub>(y<sub>i</sub>|x<sub>j</sub>)</em> has been substituted by a per sample weight:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/fc30daf5-a259-4ead-b28f-952f4f76e7de.png" style="width:36.83em;height:3.08em;"/></p>
<p>It's possible to maximize the log-likelihood using the EM algorithm (see <a href="8d541a43-8790-4a91-a79b-e48496f75d90.xhtml">Chapter 5</a>, <em>EM Algorithm and Applications</em>). In this context, we provide the steps directly:</p>
<ul>
<li><em>p(y<sub>i</sub>|x<sub>j</sub>,θ,w)</em> is computed according to the previously explained method</li>
<li>The parameters of the Gaussians are updated using these rules:</li>
</ul>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3fdcf390-48e9-44b8-b80f-9455d7a941b0.png" style="width:13.00em;height:3.33em;"/></p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ce430896-d1fb-478b-b706-2312a5e536a7.png" style="width:14.92em;height:4.00em;"/></p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0a188d67-68d1-410f-81d0-b6fb4978651a.png" style="width:23.67em;height:3.83em;"/></p>
<p><em>N</em> is the total number of samples. The procedure must be iterated until the parameters stop modifying or the modifications are lower than a fixed threshold.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of a generative Gaussian mixture</h1>
                </header>
            
            <article>
                
<p>We can now implement this model in Python using a simple bidimensional dataset, created using the <kbd>make_blobs()</kbd> function provided by Scikit-Learn:</p>
<pre>from sklearn.datasets import make_blobs<br/><br/>nb_samples = 1000<br/>nb_unlabeled = 750<br/><br/>X, Y = make_blobs(n_samples=nb_samples, n_features=2, centers=2, cluster_std=2.5, random_state=100)<br/><br/>unlabeled_idx = np.random.choice(np.arange(0, nb_samples, 1), replace=False, size=nb_unlabeled)<br/>Y[unlabeled_idx] = -1</pre>
<p class="mce-root">We have created 1,000 samples belonging to 2 classes. 750 points have then been randomly selected to become our unlabeled dataset (the corresponding class has been set to -1). We can now initialize two Gaussian distributions by defining their mean, covariance, and weight. One possibility is to use random values:</p>
<pre>import numpy as np<br/><br/># First Gaussian<br/>m1 = np.random.uniform(-7.5, 10.0, size=2)<br/>c1 = np.random.uniform(5.0, 15.0, size=(2, 2))<br/>c1 = np.dot(c1, c1.T)<br/>q1 = 0.5<br/><br/># Second Gaussian<br/>m2 = np.random.uniform(-7.5, 10.0, size=2)<br/>c2 = np.random.uniform(5.0, 15.0, size=(2, 2))<br/>c2 = np.dot(c2, c2.T)<br/>q2 = 0.5</pre>
<p>However, as the covariance matrices must be positive semi definite, it's useful to alter the random values (by multiplying each matrix by the corresponding transpose) or to set hard-coded initial parameters. In this case, we could pick the following example:</p>
<pre>import numpy as np<br/><br/># First Gaussian<br/>m1 = np.array([-3.0, -4.5])<br/>c1 = np.array([[25.0, 5.0], <br/>               [5.0, 35.0]])<br/>q1 = 0.5<br/><br/># Second Gaussian<br/>m2 = np.array([5.0, 10.0])<br/>c2 = np.array([[25.0, -10.0], <br/>               [-10.0, 25.0]])<br/>q2 = 0.5</pre>
<p>The resulting plot is shown in the following graph, where the small diamonds represent the unlabeled points and the bigger dots, the samples belonging to the known classes:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/233b53fa-0a97-46fd-b3b4-db83cd146b71.png" style="width:51.67em;height:37.92em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Initial configuration of the Gaussian mixture</div>
<p>The two Gaussians are represented by the concentric ellipses. We can now execute the training procedure. For simplicity, we repeat the update for a fixed number of iterations. The reader can easily modify the code in order to introduce a threshold:</p>
<pre>from scipy.stats import multivariate_normal<br/><br/>nb_iterations = 5<br/><br/>for i in range(nb_iterations):<br/>    Pij = np.zeros((nb_samples, 2))<br/>    <br/>    for i in range(nb_samples):<br/>        if Y[i] == -1:<br/>            p1 = multivariate_normal.pdf(X[i], m1, c1, allow_singular=True) * q1<br/>            p2 = multivariate_normal.pdf(X[i], m2, c2, allow_singular=True) * q2<br/>            Pij[i] = [p1, p2] / (p1 + p2)<br/>        <br/>        else:<br/>            Pij[i, :] = [1.0, 0.0] if Y[i] == 0 else [0.0, 1.0]<br/>            <br/>    n = np.sum(Pij, axis=0)<br/>    m = np.sum(np.dot(Pij.T, X), axis=0)<br/>    <br/>    m1 = np.dot(Pij[:, 0], X) / n[0]<br/>    m2 = np.dot(Pij[:, 1], X) / n[1]<br/>    <br/>    q1 = n[0] / float(nb_samples)<br/>    q2 = n[1] / float(nb_samples)<br/>    <br/>    c1 = np.zeros((2, 2))<br/>    c2 = np.zeros((2, 2))<br/><br/>    for t in range(nb_samples):<br/>        c1 += Pij[t, 0] * np.outer(X[t] - m1, X[t] - m1)<br/>        c2 += Pij[t, 1] * np.outer(X[t] - m2, X[t] - m2)<br/>    <br/>    c1 /= n[0]<br/>    c2 /= n[1]</pre>
<p>The first thing at the beginning of each cycle is to initialize the <kbd>Pij</kbd> matrix that will be used to store the <em>p(y<sub>i</sub>|x<sub>j</sub></em><span><em>,θ,w)</em> values. Then, for each sample, we can compute <em>p(y<sub>i</sub>|x<sub>j</sub>,θ,w)</em> considering whether it's labeled or not. The Gaussian probability is computed using the SciPy function <kbd>multivariate_normal.pdf()</kbd>. When the whole <em>P<sub>ij</sub></em> matrix has been populated, we can update the parameters (means and covariance matrix) of both Gaussians and the relative weights. The algorithm is very fast; after five iterations, we get the stable state represented in the following graph:</span></p>
<div class="CDPAlignCenter CDPAlign"><span><img src="assets/23d5ef13-ea8a-4b9a-9158-ad546f3c2a11.png" style="width:50.83em;height:37.25em;"/></span></div>
<p>The two Gaussians have perfectly mapped the space by setting their parameters so as to cover the high-density regions. We can check for some unlabeled points, as follows:</p>
<pre>print(np.round(X[Y==-1][0:10], 3))<br/><br/>[[  1.67    7.204]
 [ -1.347  -5.672]
 [ -2.395  10.952]
 [ -0.261   6.526]
 [  1.053   8.961]
 [ -0.579  -7.431]
 [  0.956   9.739]
 [ -5.889   5.227]
 [ -2.761   8.615]
 [ -1.777   4.717]]</pre>
<p>It's easy to locate them in the previous plot. The corresponding classes can be obtained through the last <em>P<sub>ij</sub></em> matrix:</p>
<pre>print(np.round(Pij[Y==-1][0:10], 3))<br/><br/>[[ 0.002  0.998]
 [ 1.     0.   ]
 [ 0.     1.   ]
 [ 0.003  0.997]
 [ 0.     1.   ]
 [ 1.     0.   ]
 [ 0.     1.   ]
 [ 0.007  0.993]
 [ 0.     1.   ]
 [ 0.02   0.98 ]]</pre>
<p>This immediately verifies that they have been correctly labeled and assigned to the right cluster. This algorithm is very fast and produces excellent results in terms of density estimation. In <a href="8d541a43-8790-4a91-a79b-e48496f75d90.xhtml">Chapter 5</a>, <em><span>EM Algorithm and Applications</span></em>, we are going to discuss a general version of this algorithm, explaining the complete training procedure based on the EM algorithm.</p>
<div class="packt_infobox">In all the examples that involve random numbers, the seed is set equal to 1,000 (<kbd>np.random.seed(1000)</kbd>). Other values or subsequent experiments without resetting it can yield slightly different results.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Weighted log-likelihood</h1>
                </header>
            
            <article>
                
<p>In the previous example, we have considered a single log-likelihood for both labeled and unlabeled samples:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/cc579607-466f-438a-85ad-3c92800a5816.png" style="width:37.83em;height:3.17em;"/></div>
<p>This is equivalent to saying that we trust the unlabeled points just like the labeled ones. However, in some contexts, this assumption can lead to completely wrong estimations, as shown in the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f4a6098c-6199-4920-8626-3cfa01b2869c.png" style="width:51.00em;height:37.33em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Biased final Gaussian mixture configuration</div>
<p>In this case, the means and covariance matrices of both Gaussian distributions have been biased by the unlabeled points and the resulting density estimation is clearly wrong. When this phenomenon happens, the best thing to do is to consider a double weighted log-likelihood. If the first <em>N</em> samples are labeled and the following <em>M</em> are unlabeled, the log-likelihood can be expressed as follows:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d1ed52c3-6911-4b8b-9934-123e08707c50.png" style="width:38.25em;height:4.25em;"/></p>
<p>In the previous formula, the term <em>λ</em>, if less than 1, can underweight the unlabeled terms, giving more importance to the labeled dataset. The modifications to the algorithm are trivial because each unlabeled weight has to be scaled according to <span><em>λ</em>, reducing its estimated probability. In <em>Semi-Supervised Learning</em></span>, <em>Chapelle O.</em>,<em> Schölkopf B.</em>, <em>Zien A.</em>, (<em>edited by</em>), <em>The MIT Press</em>,<em> </em>the reader can find a very detailed discussion about the choice of <span><em>λ</em>. There are no golden rules; however, a possible strategy could be based on the cross-validation performed on the labeled dataset. Another (more complex) approach is to consider different increasing values of <em>λ</em> and pick the first one where the log-likelihood is maximum. I recommend the aforementioned book for further details and strategies.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Contrastive pessimistic likelihood estimation</h1>
                </header>
            
            <article>
                
<p>As explained at the beginning of this chapter, in many real life problems, it's cheaper to retrieve unlabeled samples, rather than correctly labeled ones. For this reason, many researchers worked to find out the best strategies to carry out a semi-supervised classification that could outperform the supervised counterpart. The idea is to train a classifier with a few labeled samples and then improve its accuracy after adding weighted unlabeled samples. One of the best results is the <strong>Contrastive Pessimistic Likelihood Estimation</strong> (<strong>CPLE</strong>) algorithm, proposed by M. Loog (in <em>Loog M.</em>,<em> Contrastive Pessimistic Likelihood Estimation for Semi-Supervised Classification</em>, <em>arXiv:1503.00269</em>).</p>
<p>Before explaining this algorithm, an introduction is necessary. If we have a labeled dataset (<em>X</em>, <em>Y</em>) containing <em>N</em> samples, it's possible to define the log-likelihood cost function of a generic estimator, as follows:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/cb4facc0-a6eb-4f12-b726-6237372c39e1.png" style="width:17.17em;height:3.08em;"/></p>
<p>After training the model, it should be possible to determine <em>p(y<sub>i</sub>|x<sub>i</sub>, θ)</em>, which is the probability of a label given a sample <em>x<sub>i</sub></em>. However, some classifiers are not based on this approach (like SVM) and evaluate the right class, for example, by checking the sign of a parametrized function <em>f(x<sub>i</sub>, </em><span><em>θ)</em>. As CPLE is a generic framework that can be used with any classification algorithm when the probabilities are not available, it's useful to implement a technique called Platt scaling, which allows transforming the decision function into a probability through a parametrized sigmoid. For a binary classifier, it can be expressed as follows:</span></p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e9c6f607-6508-4207-855d-67cff8e13904.png" style="width:19.58em;height:3.33em;"/></p>
<p><em>α</em> and <em>β</em> are parameters that must be learned in order to maximize the likelihood. Luckily Scikit-Learn provides the method <kbd>predict_proba()</kbd>, which returns the probabilities for all classes. Platt scaling is performed automatically or on demand; for example, the SCV classifier needs to have the parameter <kbd>probability=True</kbd> in order to compute the probability mapping. I always recommend checking the documentation before implementing a custom solution.</p>
<p>We can consider a full dataset, made up of labeled and unlabeled samples. For simplicity, we can reorganize the original dataset, so that the first <em>N</em> samples are labeled, while the next <em>M</em> are unlabeled:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/852b786f-f2d6-4166-b49f-bfc70fdc98c9.png" style="width:33.58em;height:1.92em;"/></p>
<p>As we don't know the labels for all <em>x<sup>u</sup></em> samples, we can decide to use <em>M</em> <em>k</em>-dimensional (k is the number of classes) soft-labels <em>q<sub>i</sub></em> that can be optimized during the training process:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/90fb619a-a877-455d-bf7e-77922cc55eb7.png" style="width:31.75em;height:3.17em;"/></p>
<p>The second condition in the previous formula is necessary to guarantee that each <em>q<sub>i</sub></em> represents a discrete probability (all the elements must sum up to 1.0). The complete log-likelihood cost function can, therefore, be expressed as follows:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3fdf450f-1852-4bfa-8336-19bd92532055.png" style="width:35.00em;height:4.42em;"/></p>
<p>The first term represents the log-likelihood for the supervised part, while the second one is responsible for the unlabeled points. If we train a classifier with only the labeled samples, excluding the second addend, we get a parameter set <em>θ<sub>sup</sub></em>. CPLE defines a contrastive condition (as a log-likelihood too), by defining the improvement in the total cost function given by the semi-supervised approach, compared to the supervised solution:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ea514fd2-d071-4331-885f-2ab7b4334e98.png" style="width:27.50em;height:1.58em;"/></p>
<p>This condition allows imposing that the semi-supervised solution must outperform the supervised one, in fact, maximizing it; we both increase the first term and reduce the second one, obtaining a proportional increase of CL (the term <em>contrastive</em> is very common in machine learning and it normally indicates a condition which is achieved as the difference between two opposite constraints). If CL doesn't increase, it probably means that the unlabeled samples have not been drawn from the marginal distribution <em>p(x)</em> extracted from <em>p<sub>data</sub></em>.</p>
<p>Moreover, in the previous expression, we have implicitly used soft-labels, but as they are initially randomly chosen and there's no ground truth to support their values, it's a good idea not to trust them by imposing a pessimistic condition (as another log-likelihood):</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3fcae4b3-c343-44cf-85b9-2d5bbb3b128c.png" style="width:27.50em;height:1.67em;"/></p>
<p>By imposing this constraint, we try to find the soft-labels that minimize the contrastive log-likelihood; that's why this is defined as a pessimistic approach. It can seem a contradiction; however, trusting soft-labels can be dangerous, because the semi-supervised log-likelihood could be increased even with a large percentage of misclassification. Our goal is to find the best parameter set that is able to guarantee the highest accuracy starting from the supervised baseline (which has been obtained using the labeled samples) and improving it, without forgetting the structural features provided by the labeled samples.</p>
<p>Therefore, our final goal can be expressed as follows:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/36e7e7a5-d681-4f86-becb-ca5c6cbffac4.png" style="width:21.17em;height:1.75em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of contrastive pessimistic likelihood estimation</h1>
                </header>
            
            <article>
                
<p>We are going to implement the CPLE algorithm in Python using a subset extracted from the MNIST dataset. For simplicity, we are going to use only the samples representing the digits 0 and 1:</p>
<pre>from sklearn.datasets import load_digits<br/><br/>import numpy as np<br/><br/>X_a, Y_a = load_digits(return_X_y=True)<br/><br/>X = np.vstack((X_a[Y_a == 0], X_a[Y_a == 1]))<br/>Y = np.vstack((np.expand_dims(Y_a, axis=1)[Y_a==0], np.expand_dims(Y_a, axis=1)[Y_a==1]))<br/><br/>nb_samples = X.shape[0]<br/>nb_dimensions = X.shape[1]<br/>nb_unlabeled = 150<br/>Y_true = np.zeros((nb_unlabeled,))<br/><br/>unlabeled_idx = np.random.choice(np.arange(0, nb_samples, 1), replace=False, size=nb_unlabeled)<br/>Y_true = Y[unlabeled_idx].copy()<br/>Y[unlabeled_idx] = -1</pre>
<p>After creating the restricted dataset (<em>X</em>, <em>Y</em>) which contain 360 samples, we randomly select 150 samples (about 42%) to become unlabeled (the corresponding <em>y</em> is -1). At this point, we can measure the performance of logistic regression trained only on the labeled dataset:</p>
<pre>from sklearn.linear_model import LogisticRegression<br/><br/>lr_test = LogisticRegression()<br/>lr_test.fit(X[Y.squeeze() != -1], Y[Y.squeeze() != -1].squeeze())<br/>unlabeled_score = lr_test.score(X[Y.squeeze() == -1], Y_true)<br/><br/>print(unlabeled_score)<br/>0.573333333333</pre>
<p>So, the logistic regression shows 57% accuracy for the classification of the unlabeled samples. We can also evaluate the cross-validation score on the whole dataset (before removing some random labels):</p>
<pre>from sklearn.model_selection import cross_val_score<br/><br/>total_cv_scores = cross_val_score(LogisticRegression(), X, Y.squeeze(), cv=10)<br/><br/>print(total_cv_scores)<br/>[ 0.48648649  0.51351351  0.5         0.38888889  0.52777778  0.36111111
  0.58333333  0.47222222  0.54285714  0.45714286]</pre>
<p>Thus, the classifier achieves an average 48% accuracy when using 10 folds (each test set contains 36 samples) if all the labels are known.</p>
<p>We can now implement a CPLE algorithm. The first thing is to initialize a <kbd>LogisticRegression</kbd> instance and the soft-labels:</p>
<pre>lr = LogisticRegression()<br/>q0 = np.random.uniform(0, 1, size=nb_unlabeled)</pre>
<p><em>q0</em> is a random array of values bounded in the half-open interval [0, 1]; therefore, we also need a converter to transform <em>q<sub>i</sub></em> into an actual binary label:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0015cb62-47fa-45fd-9646-7d08d9804844.png" style="width:11.33em;height:3.00em;"/></p>
<p>We can achieve this using the NumPy function <kbd>np.vectorize()</kbd>, which allows us to apply a transformation to all the elements of a vector:</p>
<pre>trh = np.vectorize(lambda x: 0.0 if x &lt; 0.5 else 1.0)</pre>
<p>In order to compute the log-likelihood, we need also a weighted log-loss (similar to the Scikit-Learn function <kbd>log_loss()</kbd>, which, however, computes the negative log-likelihood but doesn't support weights):</p>
<pre>def weighted_log_loss(yt, p, w=None, eps=1e-15):<br/>    if w is None:<br/>        w_t = np.ones((yt.shape[0], 2))<br/>    else:<br/>        w_t = np.vstack((w, 1.0 - w)).T<br/>    <br/>    Y_t = np.vstack((1.0 - yt.squeeze(), yt.squeeze())).T<br/>    L_t = np.sum(w_t * Y_t * np.log(np.clip(p, eps, 1.0 - eps)), axis=1)<br/>    <br/>    return np.mean(L_t)</pre>
<p>This function computes the following expression:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8a0191f5-780c-4f33-a0ce-f5b3ecd791f3.png" style="width:29.67em;height:3.42em;"/></p>
<p>We need also a function to build the dataset with variable soft-labels <em>q<sub>i</sub></em>:</p>
<pre>def build_dataset(q):<br/>    Y_unlabeled = trh(q)<br/>    <br/>    X_n = np.zeros((nb_samples, nb_dimensions))<br/>    X_n[0:nb_samples - nb_unlabeled] = X[Y.squeeze()!=-1]<br/>    X_n[nb_samples - nb_unlabeled:] = X[Y.squeeze()==-1]<br/>    <br/>    Y_n = np.zeros((nb_samples, 1))<br/>    Y_n[0:nb_samples - nb_unlabeled] = Y[Y.squeeze()!=-1]<br/>    Y_n[nb_samples - nb_unlabeled:] = np.expand_dims(Y_unlabeled, axis=1)<br/>    <br/>    return X_n, Y_n</pre>
<p>At this point, we can define our contrastive log-likelihood:</p>
<pre>def log_likelihood(q):<br/>    X_n, Y_n = build_dataset(q)<br/>    Y_soft = trh(q)<br/>    <br/>    lr.fit(X_n, Y_n.squeeze())<br/>    <br/>    p_sup = lr.predict_proba(X[Y.squeeze() != -1])<br/>    p_semi = lr.predict_proba(X[Y.squeeze() == -1])<br/>    <br/>    l_sup = weighted_log_loss(Y[Y.squeeze() != -1], p_sup)<br/>    l_semi = weighted_log_loss(Y_soft, p_semi, q)<br/>    <br/>    return l_semi - l_sup</pre>
<p>This method will be called by the optimizer, passing a different <em>q</em> vector each time. The first step is building the new dataset and computing <kbd>Y_soft</kbd>, which are the labels corresponding to <em>q</em>. Then the logistic regression classifier is trained with with the dataset (as <kbd>Y_n</kbd> is a (k, 1) array, it's necessary to squeeze it to avoid a warning. The same thing is done when using <em>Y</em> as a boolean indicator). At this point, it's possible to compute both <em>p<sub>sup</sub></em> and <em>p<sub>semi</sub></em> using the method <kbd>predict_proba()</kbd> and, finally, we can compute the semi-supervised and supervised log-loss, which is the term, a function of <em>q<sub>i</sub></em>, that we want to minimize, while the maximization of <em>θ</em> is done implicitly when training the logistic regression.</p>
<p>The optimization is carried out using the BFGS algorithm implemented in SciPy:</p>
<pre>from scipy.optimize import fmin_bfgs<br/><br/>q_end = fmin_bfgs(f=log_likelihood, x0=q0, maxiter=5000, disp=False)</pre>
<p class="mce-root">This is a very fast algorithm, but the user is encouraged to experiment with methods or libraries. The two parameters we need in this case are <kbd>f</kbd>, which is the function to minimize, and <kbd>x0</kbd>, which is the initial condition for the independent variables. <kbd>maxiter</kbd> is useful for avoiding an excessive number of iterations when no improvements are achieved. Once the optimization is complete, <kbd>q_end</kbd> contains the optimal soft-labels. We can, therefore, rebuild our dataset:</p>
<pre>X_n, Y_n = build_dataset(q_end)</pre>
<p>With this final configuration, we can retrain the logistic regression and check the cross-validation accuracy:</p>
<pre>final_semi_cv_scores = cross_val_score(LogisticRegression(), X_n, Y_n.squeeze(), cv=10)<br/><br/>print(final_semi_cv_scores)<br/>[ 1.          1.          0.89189189  0.77777778  0.97222222  0.88888889
  0.61111111  0.88571429  0.94285714  0.48571429]</pre>
<p>The semi-supervised solution based on the CPLE algorithms achieves an average 84% accuracy, outperforming, as expected, the supervised approach. The reader can try other examples using different classifiers, such SVM or Decision Trees, and verify when CPLE allows obtaining higher accuracy than other supervised algorithms.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Semi-supervised Support Vector Machines (S3VM)</h1>
                </header>
            
            <article>
                
<p>When we discussed the cluster assumption, we also defined the low-density regions as boundaries and the corresponding problem as low-density separation. A common supervised classifier which is based on this concept is a <strong>Support Vector Machine</strong> (<strong>SVM</strong>), the objective of which is to maximize the distance between the dense regions where the samples must be. For a complete description of linear and kernel-based SVMs, please refer to <em>Bonaccorso G.</em>, <em>Machine Learning Algorithms</em>, <em>Packt Publishing</em>; however, it's useful to remind yourself of the basic model for a linear SVM with slack variables <em>ξ<sub>i</sub></em>:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4ce95bed-96a5-44eb-a7d7-8eefada51472.png" style="width:26.50em;height:4.00em;"/></p>
<p>This model is based on the assumptions that <em>y<sub>i</sub></em> can be either -1 or 1. The slack variables <em><span>ξ</span><sub>i</sub></em> or soft-margins are variables, one for each sample, introduced to reduce the <em>strength</em> imposed by the original condition (<em>min ||w||</em>), which is based on a hard margin that misclassifies all the samples that are on the wrong side. They are defined by the Hinge loss, as follows:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7732ffee-8269-4bac-971f-7a4a1b4f1448.png" style="width:17.75em;height:1.83em;"/></p>
<p>With those variables, we allow some points to overcome the limit without being misclassified if they remain within a distance controlled by the corresponding slack variable (which is also minimized during the training phase, so as to avoid uncontrollable growth). In the following diagram, there's a schematic representation of this process:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b5093fc7-34e5-4ab3-8186-76a30ecc15b6.png" style="width:58.67em;height:41.33em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">SVM generic scenario</div>
<p>The last elements of each high-density regions are the support vectors. Between them, there's a low-density region (it can also be zero-density in some cases) where our separating hyperplane lies. In <a href="acff0775-f21c-4b6d-8ef2-c78713e21364.xhtml">Chapter 1</a>, <em>Machine Learning Model Fundamentals</em>, we defined the concept of <em>empirical risk</em> as a proxy for expected risk; therefore, we can turn the SVM problem into the minimization of empirical risk under the Hinge cost function (with or without Ridge Regularization on w):</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/fe754389-2954-4aeb-be7d-8a4105339c56.png" style="width:27.17em;height:3.33em;"/></p>
<p class="mce-root">Theoretically, every function which is always bounded by two hyperplanes containing the support vectors is a good classifier, but we need to minimize the empirical risk (and, so, the expected risk); therefore we look for the maximum margin between high-density regions. This model is able to separate two dense regions with irregular boundaries and, by adopting a kernel function, also in non-linear scenarios. The natural question, at this point, is about the best strategy to integrate labeled and unlabeled samples when we need to solve this kind of problem in a semi-supervised scenario.</p>
<p>The first element to consider is the ratio: if we have a low percentage of unlabeled points, the problem is mainly supervised and the generalization ability learned using the training set should be enough to correctly classify all the unlabeled points. On the other hand, if the number of unlabeled samples is much larger, we return to an almost pure clustering scenario (like the one discussed in the paragraph about the Generative Gaussian mixtures). In order to exploit the strength of semi-supervised methods in low-density separation problems, therefore, we should consider situations where the ratio labeled/unlabeled is about 1.0. However, even if we have the predominance of a class (for example, if we have a huge unlabeled dataset and only a few labeled samples), it's always possible to use the algorithms we're going to discuss, even if, sometimes, their performance could be equal to or lower than a pure supervised/clustering solution. Transductive SMVs, for example, showed better accuracies when the labeled/unlabeled ratio is very small, while other methods can behave in a completely different way. However, when working with semi-supervised learning (and its assumptions), it is always important to bear in mind that each problem is supervised and unsupervised at the same time and the best solution must be evaluated in every different context.</p>
<p>A solution for this problem is offered by the <em>Semi-Supervised SVM</em> (also known as <em>S<sup>3</sup>VM</em>) algorithm. If we have <em>N</em> labeled samples and <em>M</em> unlabeled samples, the objective function becomes as follows:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8914f477-cc93-413c-a2af-cc0946bbe32c.png" style="width:30.33em;height:4.42em;"/></p>
<p>The first term imposes the standard SVM condition about the maximum separation distance, while the second block is divided into two parts:</p>
<ul>
<li>We need to add <em>N</em> slack variables <em>η<sub>i</sub></em><span> </span>to guarantee a soft-margin for the labeled samples.</li>
<li>At the same time, we have to consider the unlabeled points, which could be classified as +1 or -1. Therefore, we have two corresponding slack-variable sets <em>ξ<sub>i</sub></em><span> </span>and <em>z<sub>i</sub></em>. However, we want to find the smallest variable for each possible pair, so as to be sure that the unlabeled sample is placed on the sub-space where the maximum accuracy is achieved.</li>
</ul>
<p>The constraints necessary to solve the problems become as follows:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/00e27ea5-de91-4860-8d55-45cbd0e09fda.png" style="width:34.08em;height:5.33em;"/></p>
<p>The first constraint is limited to the labeled points and it's the same as a supervised SVM. The following two, instead, take into account the possibility that an unlabeled sample could be classified as +1 or -1. Let's suppose, for example, that the label <em>y<sub>j</sub></em><span> </span>for the sample <em>x<sub>j</sub></em><span> </span>should be +1 and the first member of the second inequality is a positive number <em>K</em> (so the corresponding term of the third equation is <em>-K</em>). It's easy to verify that the first slack variable is <em><span>ξ</span><sub>i</sub> ≥ 1 - K,</em> while the second one is <em>z<sub>j</sub> ≥ 1 + K</em>.</p>
<p>Therefore, in the objective, <em><span>ξ</span></em><sub><em>i</em> </sub>is chosen to be minimized. This method is inductive and yields good (if not excellent) performances; however, it has a very high computational cost and should be solved using optimized (native) libraries. Unfortunately, it is a non-convex problem and there are no standard methods to solve it so it always reaches the optimal configuration.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of S3VM</h1>
                </header>
            
            <article>
                
<p>We now implement an S<sup>3</sup>VM in Python using the SciPy optimization methods, which are mainly based on C and FORTRAN implementations. The reader can try it with other libraries such as NLOpt and LIBSVM and compare the results. A possibility suggested by Bennet and Demiriz is to use the L1-norm for w, so as to linearize the objective function; however, this choice seems to produce good results only for small datasets. We are going to keep the original formulation based on the L2-norm, using an <span><strong>Sequential Least Squares Programming</strong> (</span><strong>SLSQP</strong>) algorithm to optimize the objective. </p>
<p>Let's start by creating a bidimensional dataset with both labeled and unlabeled samples:</p>
<pre>from sklearn.datasets import make_classification<br/><br/>nb_samples = 500<br/>nb_unlabeled = 200<br/><br/>X, Y = make_classification(n_samples=nb_samples, n_features=2, n_redundant=0, random_state=1000)<br/>Y[Y==0] = -1<br/>Y[nb_samples - nb_unlabeled:nb_samples] = 0</pre>
<p>For simplicity (and without any impact, because the samples are shuffled), we set last 200 samples as unlabeled (<em>y = 0</em>). The corresponding plot is shown in the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/866c542c-e327-4590-a18f-2864b229a397.png" style="width:47.25em;height:35.25em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Original labeled and unlabeled dataset</div>
<p>The crosses represent unlabeled points, which are spread throughout the entire dataset. At this point we need to initialize all variables required for the optimization problem:</p>
<pre>import numpy as np<br/><br/>w = np.random.uniform(-0.1, 0.1, size=X.shape[1])<br/>eta = np.random.uniform(0.0, 0.1, size=nb_samples - nb_unlabeled)<br/>xi = np.random.uniform(0.0, 0.1, size=nb_unlabeled)<br/>zi = np.random.uniform(0.0, 0.1, size=nb_unlabeled)<br/>b = np.random.uniform(-0.1, 0.1, size=1)<br/>C = 1.0<br/><br/>theta0 = np.hstack((w, eta, xi, zi, b))</pre>
<p>As the optimization algorithm requires a single array, we have stacked all vectors into a horizontal array <kbd>theta0</kbd> using the <kbd>np.hstack()</kbd> function. We also need to vectorize the <kbd>min()</kbd> function in order to apply it to arrays:</p>
<pre>vmin = np.vectorize(lambda x1, x2: x1 if x1 &lt;= x2 else x2)</pre>
<p>Now, we can define the objective function:</p>
<pre>def svm_target(theta, Xd, Yd):<br/>    wt = theta[0:2].reshape((Xd.shape[1], 1))<br/>    <br/>    s_eta = np.sum(theta[2:2 + nb_samples - nb_unlabeled])<br/>    s_min_xi_zi = np.sum(vmin(theta[2 + nb_samples - nb_unlabeled:2 + nb_samples], <br/>                              theta[2 + nb_samples:2 + nb_samples + nb_unlabeled]))<br/>    <br/>    return C * (s_eta + s_min_xi_zi) + 0.5 * np.dot(wt.T, wt)</pre>
<p>The arguments are the current <kbd>theta</kbd> vector and the complete datasets <kbd>Xd</kbd> and <kbd>Yd</kbd>. The dot product of <em>w</em> has been multiplied by 0.5 to keep the conventional notation used for supervised SVMs. The constant can be omitted without any impact. At this point, we need to define all the constraints, as they are based on the slack variables; each function (which shares the same parameters of the objectives) is parametrized with an index, <kbd>idx</kbd>. The labeled constraint is as follows:</p>
<pre>def labeled_constraint(theta, Xd, Yd, idx):<br/>    wt = theta[0:2].reshape((Xd.shape[1], 1))<br/>    <br/>    c = Yd[idx] * (np.dot(Xd[idx], wt) + theta[-1]) + \<br/>    theta[2:2 + nb_samples - nb_unlabeled][idx] - 1.0<br/>    <br/>    return (c &gt;= 0)[0]</pre>
<p>The unlabeled constraints, instead, are as follows:</p>
<pre>def unlabeled_constraint_1(theta, Xd, idx):<br/>    wt = theta[0:2].reshape((Xd.shape[1], 1))<br/>    <br/>    c = np.dot(Xd[idx], wt) - theta[-1] + \<br/>        theta[2 + nb_samples - nb_unlabeled:2 + nb_samples][idx - nb_samples + nb_unlabeled] - 1.0<br/>        <br/>    return (c &gt;= 0)[0]<br/><br/>def unlabeled_constraint_2(theta, Xd, idx):<br/>    wt = theta[0:2].reshape((Xd.shape[1], 1))<br/>    <br/>    c = -(np.dot(Xd[idx], wt) - theta[-1]) + \<br/>        theta[2 + nb_samples:2 + nb_samples + nb_unlabeled ][idx - nb_samples + nb_unlabeled] - 1.0<br/>        <br/>    return (c &gt;= 0)[0]</pre>
<p>They are parametrized with the current <kbd>theta</kbd> vector, the <kbd>Xd</kbd> dataset, and an <kbd>idx</kbd> index. We need also to include the constraints for each slack variable (<em>≥ 0</em>):</p>
<pre>def eta_constraint(theta, idx):<br/>    return theta[2:2 + nb_samples - nb_unlabeled][idx] &gt;= 0<br/><br/>def xi_constraint(theta, idx):<br/>    return theta[2 + nb_samples - nb_unlabeled:2 + nb_samples][idx - nb_samples + nb_unlabeled] &gt;= 0<br/><br/>def zi_constraint(theta, idx):<br/>    return theta[2 + nb_samples:2 + nb_samples+nb_unlabeled ][idx - nb_samples + nb_unlabeled] &gt;= 0</pre>
<p>We can now set up the problem using the SciPy convention:</p>
<pre>svm_constraints = []<br/><br/>for i in range(nb_samples - nb_unlabeled):<br/>    svm_constraints.append({<br/>            'type': 'ineq',<br/>            'fun': labeled_constraint,<br/>            'args': (X, Y, i)<br/>        })<br/>    svm_constraints.append({<br/>            'type': 'ineq',<br/>            'fun': eta_constraint,<br/>            'args': (i,)<br/>        })<br/>    <br/>for i in range(nb_samples - nb_unlabeled, nb_samples):<br/>    svm_constraints.append({<br/>            'type': 'ineq',<br/>            'fun': unlabeled_constraint_1,<br/>            'args': (X, i)<br/>        })<br/>    svm_constraints.append({<br/>            'type': 'ineq',<br/>            'fun': unlabeled_constraint_2,<br/>            'args': (X, i)<br/>        })<br/>    svm_constraints.append({<br/>            'type': 'ineq',<br/>            'fun': xi_constraint,<br/>            'args': (i,)<br/>        })<br/>    svm_constraints.append({<br/>            'type': 'ineq',<br/>            'fun': zi_constraint,<br/>            'args': (i,)<br/>        })</pre>
<p>Each constraint is represented with a dictionary, where <kbd>type</kbd> is set to <kbd>ineq</kbd> to indicate that it is an inequality, <kbd>fun</kbd> points to the callable object and <kbd>args</kbd> contains all extra arguments (<kbd>theta</kbd> is the main x variable and it's automatically added). Using SciPy, it's possible to minimize the objective using the <strong>Sequential Least Squares Programming</strong> (<strong><span>SLSQP</span></strong>) or <strong>Constraint Optimization by Linear Approximation</strong> (<strong>COBYLA</strong>) algorithms. We preferred the former, because it works more rapidly and is more stable:</p>
<pre>from scipy.optimize import minimize<br/><br/>result = minimize(fun=svm_target, <br/>                  x0=theta0, <br/>                  constraints=svm_constraints, <br/>                  args=(X, Y), <br/>                  method='SLSQP', <br/>                  tol=0.0001, <br/>                  options={'maxiter': 1000})</pre>
<p>After the training process is complete, we can compute the labels for the unlabeled points:</p>
<pre>theta_end = result['x']<br/>w = theta_end[0:2]<br/>b = theta_end[-1]<br/><br/>Xu= X[nb_samples - nb_unlabeled:nb_samples]<br/>yu = -np.sign(np.dot(Xu, w) + b)</pre>
<p>In the next graph, it's possible to compare the initial plot (left) with the final one where all points have been assigned a label (right):</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a4b85e3e-4a9c-4209-98d0-8180b29d82df.png" style="width:53.17em;height:24.25em;"/></div>
<p>As you can see, S<sup>3</sup>VM succeeded in finding the right label for all unlabeled points, confirming the existence of two very dense regions for <em>x</em> between <em>[0, 2]</em> (square dots) and <em>y</em> between <em>[0, 2]</em> (circular dots).</p>
<div class="packt_infobox">NLOpt is a complete optimization library developed at MIT. It is available for different operating systems and programming languages. The website is <a href="https://nlopt.readthedocs.io" target="_blank">https://nlopt.readthedocs.io</a>. LIBSVM is an optimized library for solving SVM problems and it is adopted by Scikit-Learn together with LIBLINEAR. It's also available for different environments. The homepage is <a href="https://www.csie.ntu.edu.tw/~cjlin/libsvm/" target="_blank">https://www.csie.ntu.edu.tw/~cjlin/libsvm/.</a></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transductive Support Vector Machines (TSVM)</h1>
                </header>
            
            <article>
                
<p>Another approach to the same problem is offered by the TSVM, proposed by T. Joachims (in <em>Transductive Inference for Text Classification using Support Vector Machines</em>,<em> </em><em>Joachims T.</em>, <em>ICML Vol. 99/1999</em>). The idea is to keep the original objective with two sets of slack variables: the first for the labeled samples and the other for the unlabeled ones:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e896ce54-dc1b-4a81-9200-df1c2153320e.png" style="width:24.33em;height:4.33em;"/></p>
<p>As this is a transductive approach, we need to consider the unlabeled samples as variable-labeled ones (subject to the learning process), imposing a constraint similar to the supervised points. As for the previous algorithm, we assume we have <em>N</em> labeled samples and <em>M</em> unlabeled ones; therefore, the conditions become as follows:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e0adc106-9f38-4c21-8851-236dcdd6ef59.png" style="width:32.92em;height:5.92em;"/></p>
<p>The first constraint is the classical SVM one and it works only on labeled samples. The second one uses the variable <em>y<sup>(u)</sup><sub>j</sub></em> with the corresponding slack variables <em>ξ<sub>j</sub></em> to impose a similar condition on the unlabeled samples, while the third one is necessary to constrain the labels to being equal to -1 and 1.</p>
<p>Just like the semi-supervised SVMs, this algorithm is non-convex and it's useful to try different methods to optimize it. Moreover, the author, in the aforementioned paper, showed how TSVM works better when the test set (unlabeled) is large and the training set (labeled) is relatively small (when a standard supervised SVM is outperformed). On the other hand, with large training sets and small test sets, a supervised SVM (or other algorithms) are always preferable because they are faster and yield better accuracy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of TSVM</h1>
                </header>
            
            <article>
                
<p>In our Python implementation, we are going to use a bidimensional dataset similar to one employed in the previous method; however, in this case, we impose 400 unlabeled samples out of a total of 500 points:</p>
<pre>from sklearn.datasets import make_classification<br/><br/>nb_samples = 500<br/>nb_unlabeled = 400<br/><br/>X, Y = make_classification(n_samples=nb_samples, n_features=2, n_redundant=0, random_state=1000)<br/>Y[Y==0] = -1<br/>Y[nb_samples - nb_unlabeled:nb_samples] = 0</pre>
<p>The corresponding plot is shown in the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c9abc37a-205a-4a81-b90e-124f77f97c26.png" style="width:47.25em;height:35.25em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Original labeled and unlabeled dataset</div>
<p>The procedure is similar to the one we used before. First of all, we need to initialize our variables:</p>
<pre>import numpy as np<br/><br/>w = np.random.uniform(-0.1, 0.1, size=X.shape[1])<br/>eta_labeled = np.random.uniform(0.0, 0.1, size=nb_samples - nb_unlabeled)<br/>eta_unlabeled = np.random.uniform(0.0, 0.1, size=nb_unlabeled)<br/>y_unlabeled = np.random.uniform(-1.0, 1.0, size=nb_unlabeled)<br/>b = np.random.uniform(-0.1, 0.1, size=1)<br/><br/>C_labeled = 1.0<br/>C_unlabeled = 10.0<br/><br/>theta0 = np.hstack((w, eta_labeled, eta_unlabeled, y_unlabeled, b))</pre>
<p>In this case, we also need to define the <kbd>y_unlabeled</kbd> vector for variable-labels. The author also suggests using two C constants (<kbd>C_labeled</kbd> and <kbd>C_unlabeled</kbd>) in order to be able to weight the misclassification of labeled and unlabeled samples differently. We used a value of 1.0 for <kbd>C_labeled</kbd> and 10.0 for <kbd>C_unlabled</kbd>, because we want to penalize more the misclassification of unlabeled samples.</p>
<p>The objective function to optimize is as follows:</p>
<pre>def svm_target(theta, Xd, Yd):<br/>    wt = theta[0:2].reshape((Xd.shape[1], 1))<br/>    <br/>    s_eta_labeled = np.sum(theta[2:2 + nb_samples - nb_unlabeled])<br/>    s_eta_unlabeled = np.sum(theta[2 + nb_samples - nb_unlabeled:2 + nb_samples])<br/>    <br/>    return (C_labeled * s_eta_labeled) + (C_unlabeled * s_eta_unlabeled) + (0.5 * np.dot(wt.T, wt))</pre>
<p>While the labeled and unlabeled constraints are as follows:</p>
<pre>def labeled_constraint(theta, Xd, Yd, idx):<br/>    wt = theta[0:2].reshape((Xd.shape[1], 1))<br/>    <br/>    c = Yd[idx] * (np.dot(Xd[idx], wt) + theta[-1]) + \<br/>    theta[2:2 + nb_samples - nb_unlabeled][idx] - 1.0<br/>    <br/>    return (c &gt;= 0)[0]<br/><br/>def unlabeled_constraint(theta, Xd, idx):<br/>    wt = theta[0:2].reshape((Xd.shape[1], 1))<br/>    <br/>    c = theta[2 + nb_samples:2 + nb_samples + nb_unlabeled][idx - nb_samples + nb_unlabeled] * \<br/>        (np.dot(Xd[idx], wt) + theta[-1]) + \<br/>        theta[2 + nb_samples - nb_unlabeled:2 + nb_samples][idx - nb_samples + nb_unlabeled] - 1.0<br/>    <br/>    return (c &gt;= 0)[0]</pre>
<p>We need also to impose the constraints on the slack variables and on the <em>y<sup>(u)</sup></em>:</p>
<pre>def eta_labeled_constraint(theta, idx):<br/>    return theta[2:2 + nb_samples - nb_unlabeled][idx] &gt;= 0<br/><br/>def eta_unlabeled_constraint(theta, idx):<br/>    return theta[2 + nb_samples - nb_unlabeled:2 + nb_samples][idx - nb_samples + nb_unlabeled] &gt;= 0<br/><br/>def y_constraint(theta, idx):<br/>    return np.power(theta[2 + nb_samples:2 + nb_samples + nb_unlabeled][idx], 2) == 1.0</pre>
<p>As in the previous example, we can create the constraint dictionary needed by SciPy:</p>
<pre>svm_constraints = []<br/><br/>for i in range(nb_samples - nb_unlabeled):<br/>    svm_constraints.append({<br/>            'type': 'ineq',<br/>            'fun': labeled_constraint,<br/>            'args': (X, Y, i)<br/>        })<br/>    svm_constraints.append({<br/>            'type': 'ineq',<br/>            'fun': eta_labeled_constraint,<br/>            'args': (i,)<br/>        })<br/>    <br/>for i in range(nb_samples - nb_unlabeled, nb_samples):<br/>    svm_constraints.append({<br/>            'type': 'ineq',<br/>            'fun': unlabeled_constraint,<br/>            'args': (X, i)<br/>        })<br/>    svm_constraints.append({<br/>            'type': 'ineq',<br/>            'fun': eta_unlabeled_constraint,<br/>            'args': (i,)<br/>        })<br/><br/>for i in range(nb_unlabeled):<br/>    svm_constraints.append({<br/>            'type': 'eq',<br/>            'fun': y_constraint,<br/>            'args': (i,)<br/>        })</pre>
<p>In this case, the last constraint is an equality, because we want to force <em><span>y</span><sup>(u)</sup></em> to be equal either to -1 or 1. At this point, we minimize the objective function:</p>
<pre>from scipy.optimize import minimize<br/><br/>result = minimize(fun=svm_target, <br/>                  x0=theta0, <br/>                  constraints=svm_constraints, <br/>                  args=(X, Y), <br/>                  method='SLSQP', <br/>                  tol=0.0001, <br/>                  options={'maxiter': 1000})</pre>
<p>When the process is complete, we can compute the labels for the unlabeled samples and compare the plots:</p>
<pre>theta_end = result['x']<br/>w = theta_end[0:2]<br/>b = theta_end[-1]<br/><br/>Xu= X[nb_samples - nb_unlabeled:nb_samples]<br/>yu = -np.sign(np.dot(Xu, w) + b)</pre>
<p>The plot comparison is shown in the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/00c61ef5-5da6-4107-8c9a-36a2ec5a0115.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Original dataset (left). Final labeled dataset (right)</div>
<p>The misclassification (based on the density distribution) is slightly higher than S<sup>3</sup>VM, but it's possible to change the C values and the optimization method until the expected result has been reached. A good benchmark is provided by a supervised SVM, which can have better performances when the training set is huge enough (and when it represents the whole <em>p<sub>data</sub></em> correctly).</p>
<p>It's interesting to evaluate different combinations of the C parameters, starting from a standard supervised SVM. The dataset is smaller, with a high number of unlabeled samples:</p>
<pre>nb_samples = 100<br/>nb_unlabeled = 90<br/><br/>X, Y = make_classification(n_samples=nb_samples, n_features=2, n_redundant=0, random_state=100)<br/>Y[Y==0] = -1<br/>Y[nb_samples - nb_unlabeled:nb_samples] = 0</pre>
<p>We use the standard SVM implementation provided by Scikit-Learn (the <kbd>SVC()</kbd> class) with a linear kernel and <kbd>C=1.0</kbd>:</p>
<pre>from sklearn.svm import SVC<br/><br/>svc = SVC(kernel='linear', C=1.0)<br/>svc.fit(X[Y!=0], Y[Y!=0])<br/><br/>Xu_svc= X[nb_samples - nb_unlabeled:nb_samples]<br/>yu_svc = svc.predict(Xu_svc)</pre>
<p>The SVM is trained with the labeled samples and the vector <kbd>yu_svc</kbd> contains the prediction for the unlabeled samples. The resulting plot (in comparison with the original dataset) is shown in the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5d4839c3-9687-4f64-a00f-46f205834f25.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><span> Original dataset (left). Final labeled dataset (right) with C = 1.0</span></div>
<p>All the labeled samples are represented with bigger squares and circles. The result meets our expectations, but there's an area <em>(X [-1, 0] - Y [-2, -1])</em>, where the SVM decided to impose the <em>circle</em> class even if the unlabeled points are close to a square. This hypothesis can't be acceptable considering the clustering assumption; in fact, in a high-density region there are samples belonging to two classes. A similar (or even worse) result is obtained using an S<sup>3</sup>VM with <strong>CL=10</strong> and <strong>CU=5</strong>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a83b787d-d1ef-4344-8453-fda96b498443.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><span>Original dataset (left). Final labeled dataset (right) with C<sub>L</sub> = 10 and C<sub>U</sub> = 5</span></div>
<p>In this case, the classification accuracy is lower because the penalty for the unlabeled samples is lower than the one imposed on the labeled points. A supervised SVM has obviously better performances. Let's try <span>with <strong>C</strong></span><strong><sub>L</sub></strong><span><strong>=10</strong> and <strong>C</strong></span><strong><sub>U</sub></strong><span><strong>=50</strong>:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e36c7b7e-208f-4ca5-af4f-e6b97b134456.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><span>Original dataset (left). Final labeled dataset (right) with C</span><sub>L</sub><span> = 10 and C</span><sub>U</sub><span> = 50</span></div>
<p>Now, the penalty is quite a lot higher for the unlabeled samples and the result appears much more reasonable considering the clustering assumption. All the high-density regions are coherent and separated by low-density ones. These examples show how the value chosen for the parameters and the optimization method can dramatically change the result. My suggestion is to test several configurations (on sub-sampled datasets), before picking the final one. In <em>Semi-Supervised Learning</em>, <em>Chapelle O.</em>,<em> Schölkopf B.</em>, <em>Zien A.</em>, (<em>edited by</em>), <em>The MIT Press,</em> there are further details about possible optimization strategies, with strengths and weaknesses.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we introduced semi-supervised learning, starting from the scenario and the assumptions needed to justify the approaches. We discussed the importance of the smoothness assumption when working with both supervised and semi-supervised classifiers in order to guarantee a reasonable generalization ability. Then we introduced the clustering assumption, which is strictly related to the geometry of the datasets and allows coping with density estimation problems with a strong structural condition. Finally, we discussed the manifold assumption and its importance in order to avoid the curse of dimensionality.</p>
<p>The chapter continued by introducing a generative and inductive model: Generative Gaussian mixtures, which allow clustering labeled and unlabeled samples starting from the assumption that the prior probabilities are modeled by multivariate Gaussian distributions.</p>
<p>The next topic was about a very important algorithm: contrastive pessimistic likelihood estimation, which is an inductive, semi-supervised classification framework that can be adopted together with any supervised classifier. The main concept is to define a contrastive log-likelihood based on soft-labels (representing the probabilities for the unlabeled samples) and impose a pessimistic condition in order to minimize the trust in the soft-labels. The algorithm can find the best configuration that maximizes the log-likelihood, taking into account both labeled and unlabeled samples.</p>
<p>Another inductive classification approach is provided by the S<sup>3</sup>VM algorithm, which is an extension of the classical SVM approach, based on two extra optimization constraints to address the unlabeled samples. This method is relatively powerful, but it's non-convex and, therefore, very sensitive to the algorithms employed to minimize the objective function.</p>
<p>An alternative to <span>S</span><sup>3</sup><span>VM is provided by the TSVM, which tries to minimize the objective with a condition based on variable labels. The problem is, hence, divided into two parts: the supervised one, which is exactly the same as standard SVM, and the semi-supervised one, which has a similar structure but without fixed <em>y</em> labels. This problem is non-convex too and it's necessary to evaluate different optimization strategies to find the best trade-off between accuracy and computational complexity. In the reference section, there are some useful resources so you can examine all these problems in depth and find a suitable solution for each particular scenario.</span></p>
<p>In the next chapter, <a href="c23d1792-167f-416e-a848-fa7a10777697.xhtml" target="_blank">Chapter 3</a>, <em>Graph-Based Semi-Supervised Learning</em> we're continuing this exploration by discussing some important algorithms based on the structure underlying the dataset. In particular, we're going to employ graph theory to perform the propagation of labels to unlabeled samples and to reduce the dimensionality of datasets in non-linear contexts.</p>


            </article>

            
        </section>
    </body></html>