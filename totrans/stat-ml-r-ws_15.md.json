["```py\n\n    # Set seed for reproducibility\n    set.seed(123)\n    # Generate independent variable X\n    X = runif(100, min = 1, max = 100) # 100 random uniform numbers between 1 and 100\n    # Generate some noise\n    noise = rnorm(100, mean = 0, sd = 10) # 100 random normal numbers with mean 0 and standard deviation 10\n    # Generate dependent variable Y\n    Y = 5 + 0.5 * X + noise\n    # Combine X and Y into a data frame\n    data = data.frame(X, Y)\n    ```", "```py\n\n    # Fit a simple linear regression model\n    model = lm(Y ~ X, data = data)\n    # Print the model summary\n    >>> summary(model)\n    Call:\n    lm(formula = Y ~ X, data = data)\n    Residuals:\n         Min       1Q   Median       3Q      Max\n    -22.3797  -6.1323  -0.1973   5.9633  22.1723\n    Coefficients:\n                Estimate Std. Error t value Pr(>|t|)\n    (Intercept)  4.91948    1.99064   2.471   0.0152 *\n    X            0.49093    0.03453  14.218   <2e-16 ***\n    ---\n    Signif. codes:\n    0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n    Residual standard error: 9.693 on 98 degrees of freedom\n    Multiple R-squared:  0.6735,  Adjusted R-squared:  0.6702\n    F-statistic: 202.2 on 1 and 98 DF,  p-value: < 2.2e-16\n    ```", "```py\n\n    # Plot the data\n    plot(data$X, data$Y, main = \"Simple Linear Regression\", xlab = \"X\", ylab = \"Y\")\n    # Add the fitted regression line\n    abline(model, col = \"red\")\n    ```", "```py\n\n    # Load the data\n    data(mtcars)\n    # Build the model\n    model = lm(mpg ~ cyl + hp + wt, data = mtcars)\n    ```", "```py\n\n    # Print the summary of the model\n    >>> summary(model)\n    Call:\n    lm(formula = mpg ~ cyl + hp + wt, data = mtcars)\n    Residuals:\n        Min      1Q  Median      3Q     Max\n    -3.9290 -1.5598 -0.5311  1.1850  5.8986\n    Coefficients:\n                Estimate Std. Error t value Pr(>|t|)\n    (Intercept) 38.75179    1.78686  21.687  < 2e-16 ***\n    cyl         -0.94162    0.55092  -1.709 0.098480 .\n    hp          -0.01804    0.01188  -1.519 0.140015\n    wt          -3.16697    0.74058  -4.276 0.000199 ***\n    ---\n    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n    Residual standard error: 2.512 on 28 degrees of freedom\n    Multiple R-squared:  0.8431,  Adjusted R-squared:  0.8263\n    F-statistic: 50.17 on 3 and 28 DF,  p-value: 2.184e-11\n    ```", "```py\n\n    set.seed(123)\n    x1 = rnorm(100)\n    x2 = -3 * x1 + rnorm(100)\n    y = 2 + x1 + x2 + rnorm(100)\n    df = data.frame(y = y, x1 = x1, x2 = x2)\n    ```", "```py\n\n    # Single linear regression\n    single_reg = lm(y ~ x1, data = df)\n    >>> summary(single_reg)\n    Call:\n    lm(formula = y ~ x1, data = df)\n    Residuals:\n        Min      1Q  Median      3Q     Max\n    -2.7595 -0.8365 -0.0564  0.8597  4.3211\n    Coefficients:\n                Estimate Std. Error t value Pr(>|t|)\n    (Intercept)   2.0298     0.1379   14.72   <2e-16 ***\n    x1           -2.1869     0.1511  -14.47   <2e-16 ***\n    ---\n    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n    Residual standard error: 1.372 on 98 degrees of freedom\n    Multiple R-squared:  0.6813,  Adjusted R-squared:  0.678\n    F-statistic: 209.5 on 1 and 98 DF,  p-value: < 2.2e-16\n    ```", "```py\n\n    # Multiple linear regression\n    multi_reg = lm(y ~ x1 + x2, data = df)\n    >>> summary(multi_reg)\n    Call:\n    lm(formula = y ~ x1 + x2, data = df)\n    Residuals:\n        Min      1Q  Median      3Q     Max\n    -1.8730 -0.6607 -0.1245  0.6214  2.0798\n    Coefficients:\n                Estimate Std. Error t value Pr(>|t|)\n    (Intercept)  2.13507    0.09614  22.208  < 2e-16 ***\n    x1           0.93826    0.31982   2.934  0.00418 **\n    x2           1.02381    0.09899  10.342  < 2e-16 ***\n    ---\n    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n    Residual standard error: 0.9513 on 97 degrees of freedom\n    Multiple R-squared:  0.8484,  Adjusted R-squared:  0.8453\n    F-statistic: 271.4 on 2 and 97 DF,  p-value: < 2.2e-16\n    ```", "```py\n\n# Fit the model\nmodel <- lm(mpg ~ qsec + am, data = mtcars)\n# Display the summary of the model\n>>> summary(model)\nCall:\nlm(formula = mpg ~ qsec + am, data = mtcars)\nResiduals:\n    Min      1Q  Median      3Q     Max\n-6.3447 -2.7699  0.2938  2.0947  6.9194\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept) -18.8893     6.5970  -2.863  0.00771 **\nqsec          1.9819     0.3601   5.503 6.27e-06 ***\nam            8.8763     1.2897   6.883 1.46e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 3.487 on 29 degrees of freedom\nMultiple R-squared:  0.6868,  Adjusted R-squared:  0.6652\nF-statistic:  31.8 on 2 and 29 DF,  p-value: 4.882e-08\n```", "```py\n\n# Convert am to categorical var\nmtcars$am_cat = as.factor(mtcars$am)\n# Fit the model\nmodel <- lm(mpg ~ qsec + am_cat, data = mtcars)\n# Display the summary of the model\n>>> summary(model)\nCall:\nlm(formula = mpg ~ qsec + am_cat, data = mtcars)\nResiduals:\n    Min      1Q  Median      3Q     Max\n-6.3447 -2.7699  0.2938  2.0947  6.9194\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept) -18.8893     6.5970  -2.863  0.00771 **\nqsec          1.9819     0.3601   5.503 6.27e-06 ***\nam_cat1       8.8763     1.2897   6.883 1.46e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 3.487 on 29 degrees of freedom\nMultiple R-squared:  0.6868,  Adjusted R-squared:  0.6652\nF-statistic:  31.8 on 2 and 29 DF,  p-value: 4.882e-08\n```", "```py\n\n# Load required library\nlibrary(ggplot2)\n# Create new data frame for the predictions\nnewdata = data.frame(qsec = seq(min(mtcars$qsec), max(mtcars$qsec), length.out = 100),\n                      am_cat = c(rep(0, 100), rep(1, 100)))\nnewdata$am_cat = as.factor(newdata$am_cat)\n# Get predictions\nnewdata$mpg_pred = predict(model, newdata)\n# Plot the data and the regression lines\nggplot(data = mtcars, aes(x = qsec, y = mpg, color = am_cat)) +\n  geom_point() +\n  geom_line(data = newdata, aes(y = mpg_pred)) +\n  labs(title = \"mpg vs qsec by Transmission Type\",\n       x = \"Quarter Mile Time (qsec)\",\n       y = \"Miles per Gallon (mpg)\",\n       color = \"Transmission Type\") +\n  scale_color_discrete(labels = c(\"Automatic\", \"Manual\")) +\n  theme(text = element_text(size = 16),  # Default text size\n        title = element_text(size = 15),  # Title size\n        axis.title = element_text(size = 18),  # Axis title size\n        legend.title = element_text(size = 16),  # Legend title size\n        legend.text = element_text(size = 16), # Legend text size\n        legend.position = \"bottom\")  # Legend position\n```", "```py\n\n# Adding interaction term\nmodel_interaction <- lm(mpg ~ qsec * am_cat, data = mtcars)\n# Print model summary\n>>> summary(model_interaction)\nCall:\nlm(formula = mpg ~ qsec * am_cat, data = mtcars)\nResiduals:\n    Min      1Q  Median      3Q     Max\n-6.4551 -1.4331  0.1918  2.2493  7.2773\nCoefficient s:\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept)   -9.0099     8.2179  -1.096  0.28226\nqsec           1.4385     0.4500   3.197  0.00343 **\nam_cat1      -14.5107    12.4812  -1.163  0.25481\nqsec:am_cat1   1.3214     0.7017   1.883  0.07012 .\nSignif. code s:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 3.343 on 28 degrees of freedom\nMultiple R-squared:  0.722,  Adjusted R-squared:  0.6923\nF-statistic: 24.24 on 3 and 28 DF,  p-value: 6.129e-08\n```", "```py\n\n# Create scatter plot with two intersecting lines\nggplot(mtcars, aes(x = qsec, y = mpg, color = as.factor(am_cat))) +\n  geom_point() +\n  geom_smooth(method =\"\"l\"\", se = FALSE) + # fit separate regression lines per group\n  scale_color_discrete(name =\"\"Transmission Typ\"\",\n                       labels = c\"\"Automati\"\",\"\"Manua\"\")) +\n  labs(x =\"\"Quarter mile time (seconds\"\",\n       y =\"\"Miles per gallo\"\",\n       title =\"\"Separate regression lines fit for automatic and manual car\"\") +\n  theme(text = element_text(size = 16),\n        title = element_text(size = 15),\n        axis.title = element_text(size = 20),\n        legend.title = element_text(size = 16),\n        legend.text = element_text(size = 16))\n```", "```py\n\n    # Create a quadratic dataset\n    set.seed(1)\n    x = seq(-10, 10, by = 0.5)\n    y = x^2 + rnorm(length(x), sd = 5)\n    # Put it in a dataframe\n    df = data.frame(x = x, y = y)\n    Fit a simple linear regression to the data and print the summary.\n    lm1 <- lm(y ~ x, data = df)\n    >>> summary(lm1)\n    Call:\n    lm(formula = y ~ x, data = df)\n    Residuals:\n        Min      1Q  Median      3Q     Max\n    -43.060 -29.350  -5.451  19.075  64.187\n    Coefficient s:\n                Estimate Std. Error t value Pr(>|t|)\n    (Intercept) 35.42884    5.04627   7.021 2.01e-08 ***\n    x           -0.04389    0.85298  -0.051    0.959\n    Signif. code s:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n    Residual standard error: 32.31 on 39 degrees of freedom\n    Multiple R-squared:  6.787e-05,  Adjusted R-squared:  -0.02557\n    F-statistic: 0.002647 on 1 and 39 DF,  p-value: 0.9592\n    ```", "```py\n\n    lm2 <- lm(y ~ x + I(x^2), data = df)\n    >>> summary(lm2)\n    Call:\n    lm(formula = y ~ x + I(x^2), data = df)\n    Residuals:\n        Min      1Q  Median      3Q     Max\n    -11.700  -2.134  -0.078   2.992   7.247\n    Coefficients:\n                Estimate Std. Error t value Pr(>|t|)\n    (Intercept)  0.49663    1.05176   0.472    0.639\n    x           -0.04389    0.11846  -0.370    0.713\n    I(x^2)       0.99806    0.02241  44.543   <2e-16 ***\n    Signif. code s:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n    Residual standard error: 4.487 on 38 degrees of freedom\n    Multiple R-squared:  0.9812,  Adjusted R-squared:  0.9802\n    F-statistic: 992.1 on 2 and 38 DF,  p-value: < 2.2e-16\n    ```", "```py\n\n    ggplot(df, aes(x = x, y = y)) +\n      geom_point() +\n      geom_line(aes(y = linear_pred), color =\"\"blu\"\", linetype =\"\"dashe\"\") +\n      geom_line(aes(y = quadratic_pred), color =\"\"re\"\") +\n      labs(title =\"\"Scatter plot with linear and quadratic fit\"\",\n           x =\"\"\"\",\n           y =\"\"\"\") +\n      theme(text = element_text(size = 15)) +\n      scale_color_discrete(name =\"\"Mode\"\",\n                           labels = c\"«Linear Mode\"»,\"«Quadratic Mode\"»)) +\n      annotate\"\"tex\"\", x = 0, y = 40, label =\"\"Linear Mode\"\", color =\"\"blu\"\") +\n      annotate\"\"tex\"\", x = 6, y = 80, label =\"\"Quadratic Mode\"\", color =\"\"re\"\")\n    ```", "```py\n\n# Fit the original model\nmodel_original = lm(mpg ~ hp, data = mtcars)\n# Fit the log-transformed model\nmtcars$log_mpg = log(mtcars$mpg)\nmodel_log = lm(log_mpg ~ hp, data = mtcars)\n# Predictions from the original model\nmtcars$pred_original = predict(model_original, newdata = mtcars)\n# Predictions from the log-transformed model (back-transformed to the original scale using exp)\nmtcars$pred_log = exp(predict(model_log, newdata = mtcars))\nlibrary(tidyr)\nlibrary(dplyr)\n# Reshape data to long format\ndf_long <- mtcars %>%\n  gather(key =\"\"Mode\"\", value =\"\"Predictio\"\", pred_original, pred_log)\n# Create plot\nggplot(df_long, aes(x = hp, y = mpg)) +\n  geom_point(data = mtcars, aes(x = hp, y = mpg)) +\n  geom_line(aes(y = Prediction, color = Model)) +\n  labs(\n    x =\"\"Horsepower (hp\"\",\n    y =\"\"Miles per gallon (mpg\"\",\n    color =\"\"Mode\"\"\n  ) +\n  scale_color_manual(values = c\"\"pred_origina\"\" =\"\"blu\"\",\"\"pred_lo\"\" =\"\"re\"\")) +\n  theme(\n    legend.position =\"\"botto\"\",\n    text = element_text(size = 16),\n    legend.title = element_text(size = 16),\n    axis.text = element_text(size = 16),  # control the font size of axis labels\n    legend.text = element_text(size = 16)  # control the font size of legend text\n  )\n```", "```py\n\n# Set seed for reproducibility\nset.seed(123)\n# Generate synthetic data\nn = 100 # number of observations\nx = runif(n, -10, 10) # predictors\nbeta0 = 2 # intercept\nbeta1 = 3 # slope\nepsilon = rnorm(n, 0, 2) # random error term\ny = beta0 + beta1*x + epsilon # response variable\n# Design matrix X\nX = cbind(1, x)\n```", "```py\n\nbeta_hat = solve(t(X) %*% X) %*% t(X) %*% y\n>>> print(beta_hat)\n       [,1]\n  1.985344\nx 3.053152\n```", "```py\n\n# Fit linear regression model for comparison\nmodel = lm(y ~ x)\n>>> print(coef(model))\n(Intercept)           x\n   1.985344    3.053152\n```", "```py\n\n# install the package if not already installed\nif(!require(car)) install.packages('car')\n# load the package\nlibrary(car)\n# fit a linear model\nmodel = lm(mpg ~ hp + wt + disp, data = mtcars)\n# calculate VIF\nvif_values = vif(model)\n>>> print(vif_values)\n      hp       wt     disp\n2.736633 4.844618 7.324517\n```", "```py\n\n# Load library\nlibrary(lmtest)\n# Fit a simple linear regression model on mtcars dataset\nmodel = lm(mpg ~ wt + hp, data = mtcars)\n# Perform a Breusch-Pagan test to formally check for heteroskedasticity\n>>> bptest(model)\n  studentized Breusch-Pagan test\ndata:  model\nBP = 0.88072, df = 2, p-value = 0.6438\n```", "```py\n\n    # install the package if not already installed\n    if(!require(glmnet)) install.packages('glmnet')\n    library(glmnet)\n    ```", "```py\n\n    # Prepare data\n    data(mtcars)\n    X = as.matrix(mtcars[, -1]) # predictors\n    y = mtcars[, 1] # response\n    ```", "```py\n\n    # Fit ridge regression model\n    set.seed(123) # for reproducibility\n    ridge_model = glmnet(X, y, alpha = 0)\n    ```", "```py\n\n    # Use cross-validation to find the optimal lambda\n    cv_ridge = cv.glmnet(X, y, alpha = 0)\n    best_lambda = cv_ridge$lambda.min\n    >>> best_lambda\n    2.746789\n    ```", "```py\n\n    # Fit a new ridge regression model using the optimal lambda\n    opt_ridge_model = glmnet(X, y, alpha = 0, lambda = best_lambda)\n    # Get coefficients\n    ridge_coefs = coef(opt_ridge_model)[-1]  # remove intercept\n    >>> ridge_coefs\n    [1] -0.371840170 -0.005260088 -0.011611491  1.054511975 -1.233657799  0.162231830\n     [7]  0.771141047  1.623031037  0.544153807 -0.547436697\n    ```", "```py\n\n    # Ordinary least squares regression\n    ols_model = lm(mpg ~ ., data = mtcars)\n    # Get coefficients\n    ols_coefs = coef(ols_model)[-1] # remove intercept\n    >>> ols_coefs\n            cyl        disp          hp        drat          wt        qsec          vs\n    -0.11144048  0.01333524 -0.02148212  0.78711097 -3.71530393  0.82104075  0.31776281\n             am        gear        carb\n     2.52022689  0.65541302 -0.19941925\n    ```", "```py\n\n    plot(1:length(ols_coefs), ols_coefs, type=\"b\", col=\"blue\", pch=19, xlab=\"Coefficient\", ylab=\"Value\", ylim=c(min(ols_coefs, ridge_coefs), max(ols_coefs, ridge_coefs)))\n    lines(1:length(ridge_coefs), ridge_coefs, type=\"b\", col=\"red\", pch=19)\n    legend(\"bottomright\", legend=c(\"OLS\", \"Ridge\"), col=c(\"blue\", \"red\"), pch=19)\n    ```", "```py\n\n    lasso_model = glmnet(X, y, alpha = 1)\n    ```", "```py\n\n    # Use cross-validation to find the optimal lambda\n    cv_lasso = cv.glmnet(X, y, alpha = 1)\n    best_lambda = cv_lasso$lambda.min\n    >>> best_lambda\n    0.8007036\n    ```", "```py\n\n    # Fit a new lasso regression model using the optimal lambda\n    opt_lasso_model = glmnet(X, y, alpha = 1, lambda = best_lambda)\n    ```", "```py\n    # Get coefficients\n    lasso_coefs = coef(opt_lasso_model)[-1]  # remove intercept\n    >>> lasso_coefs\n    [1] -0.88547684  0.00000000 -0.01169485  0.00000000 -2.70853300  0.00000000  0.00000000\n     [8]  0.00000000  0.00000000  0.00000000\n    ```", "```py\n\n    plot(1:length(ols_coefs), ols_coefs, type=\"b\", col=\"blue\", pch=19, xlab=\"Coefficient\", ylab=\"Value\", ylim=c(min(ols_coefs, ridge_coefs), max(ols_coefs, ridge_coefs)))\n    lines(1:length(ridge_coefs), ridge_coefs, type=\"b\", col=\"red\", pch=19)\n    lines(1:length(lasso_coefs), lasso_coefs, type=\"b\", col=\"green\", pch=19)\n    legend(\"bottomright\", legend=c(\"OLS\", \"Ridge\", \"Lasso\"), col=c(\"blue\", \"red\", \"green\"), pch=19)\n    ```"]