- en: Selected Topics in Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习中的选讲主题
- en: In [Chapter 4](6fd48e9f-f2fd-4b29-a006-1b151de4960f.xhtml)*, **Training Neural
    Networks*, we looked at what an **artificial neural network** (**ANN**) is and
    how this kind of model is built. You can say that a deep neural network is an
    elongated version of an ANN; however, it has got its own set of challenges.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](6fd48e9f-f2fd-4b29-a006-1b151de4960f.xhtml)*“训练神经网络”中，我们探讨了什么是**人工神经网络**（ANN）以及这种模型是如何构建的。你可以认为深度神经网络是ANN的延伸版本；然而，它也有自己的一套挑战。
- en: 'In this chapter, we will learn about the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下主题：
- en: What is a deep neural network?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是深度神经网络？
- en: How to initialize parameters
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何初始化参数
- en: Adversarial networks—generative adversarial networks and Bayesian generative
    adversarial networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对抗网络——生成对抗网络和贝叶斯生成对抗网络
- en: Deep Gaussian processes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度高斯过程
- en: Hinton's Capsule network
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 霍inton的胶囊网络
- en: Deep neural networks
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度神经网络
- en: 'Let''s recap on what we learned in [Chapter 4](6fd48e9f-f2fd-4b29-a006-1b151de4960f.xhtml)*,
    Training Neural Networks*. A neural network is a machine emulation of the human
    brain that is seen as a set of algorithms that have been set out to extract patterns
    out of data. It has got three different layers:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下我们在[第4章](6fd48e9f-f2fd-4b29-a006-1b151de4960f.xhtml)*“训练神经网络”中学习的内容。神经网络是机器模拟人脑，被视为一套旨在从数据中提取模式的算法。它有三个不同的层：
- en: Input layer
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入层
- en: Hidden layer
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层
- en: Output layer
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层
- en: Sensory numerical data (in the form of a vector) passes through the input layer
    and then goes through the hidden layers to generate its own set of perceptions
    and reasoning to yield the final result in the output layer.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 感官数值数据（以向量的形式）通过输入层，然后通过隐藏层生成它自己的感知和推理，以在输出层产生最终结果。
- en: 'Can you recall what we learned in [Chapter 4](6fd48e9f-f2fd-4b29-a006-1b151de4960f.xhtml),
    *Training Neural Networks**, *regarding the number of layers in ANN and how we
    count them? When we have got the layers like the ones shown in the following diagram,
    can you count the number of layers? Remember, we always count just the hidden
    layer and the output layer. So, if somebody is asking you how many layers there
    are in your network, you don''t include the input layer while answering:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你能回忆起我们在[第4章](6fd48e9f-f2fd-4b29-a006-1b151de4960f.xhtml)“训练神经网络”中学到的内容，关于人工神经网络（ANN）中的层数以及我们如何计算它们吗？当我们有如下图中所示的层时，你能计算出层数吗？记住，我们只计算隐藏层和输出层。所以，如果有人问你你的网络中有多少层，在回答时不要包括输入层：
- en: '![](img/3c7c2035-a325-4e73-8071-99a1ae84c085.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3c7c2035-a325-4e73-8071-99a1ae84c085.png)'
- en: Yes, that's right—there are two layers in the preceding architecture. What about
    for the following network?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，没错——前面的架构中有两层。那么对于下面的网络呢？
- en: '![](img/20b65667-28a5-42c9-a8ba-e4d1ecfa8e30.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/20b65667-28a5-42c9-a8ba-e4d1ecfa8e30.png)'
- en: This network has got three layers, which includes two hidden layers. As the
    layers increase, the model becomes deeper.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个网络有三个层，包括两个隐藏层。随着层的增加，模型变得更深。
- en: Why do we need a deep learning model?
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么我们需要深度学习模型？
- en: A deep learning model is a highly non-linear model that has got multiple layers
    with multiple nodes acting in sequence to solve a business problem. Every layer
    has been assigned a different task.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型是一个高度非线性的模型，具有多层和多个节点按顺序工作以解决业务问题。每一层都被分配了不同的任务。
- en: 'For example, if we have got a face detection problem, hidden layer 1 finds
    out which edges are present in the image. Layer 2 finds out the combination of
    edges, which start taking the shape of eyes, a nose, and other parts. Layer 3
    enables the object models, which creates the shape of the face. The following
    diagram shows the different hidden layers:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们有一个面部检测问题，隐藏层1找出图像中存在的哪些边缘。层2找出边缘的组合，这些边缘开始呈现出眼睛、鼻子和其他部分的形状。层3使对象模型得以创建，从而形成人脸的形状。以下图表显示了不同的隐藏层：
- en: '![](img/25fca5bd-3031-4453-8260-dc708618b000.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/25fca5bd-3031-4453-8260-dc708618b000.png)'
- en: Here, we have got a logistic regression model, also known as a **single layer
    neural network**. Sometimes, it is also called the most **shallow network**. The
    second network that can be seen here has got a two-layer network. Again, it's
    a shallow network, but it's not as shallow as the previous one. The next architecture
    has got three layers, which is making things more interesting now. The network
    is getting deep now. The last architecture has got a six layer architecture, which
    is comprised of five hidden layers. The number of layers is getting even deeper.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们得到了一个逻辑回归模型，也称为**单层神经网络**。有时，它也被称作最**浅层网络**。这里可以看到的第二个网络有一个两层网络。再次，它是一个浅层网络，但不如前一个浅。下一个架构有三个层，这使得事情更有趣。网络现在变深了。最后一个架构有一个六层架构，由五个隐藏层组成。层数变得更深了。
- en: Deep neural network notation
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度神经网络符号
- en: 'The explanation of the notation is as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 符号的解释如下：
- en: '*l*: Number of layers is 4'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*l*：层数为 4'
- en: '*n^([l])*: Number of nodes in layer *l *'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*n^([l])*：第 *l* 层的节点数'
- en: 'For the following architecture, this is as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于以下架构，这是如下所示：
- en: '*n ^([0]):* Number of nodes in input layer, that is, 3'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*n ^([0])*：输入层的节点数，即 3'
- en: '*n ^([1])*: 5'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*n ^([1])*：5'
- en: '*n ^([2])*: 5'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*n ^([2])*：5'
- en: '*n ^([3]):* 3'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*n ^([3])*：3'
- en: '*n ^([4])*: 1'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*n ^([4])*：1'
- en: '*a ^([l])*: Activations in layer *l:*'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*a ^([l])*：第 *l* 层的激活值'
- en: '![](img/2dc827b1-5595-4a37-b9b8-d58cb1a98826.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2dc827b1-5595-4a37-b9b8-d58cb1a98826.png)'
- en: 'As we already know, the following equation goes through the layers:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已知的，以下方程通过各层：
- en: '*z = w^TX + b*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*z = w^TX + b*'
- en: 'Hence, we get the following results:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得到以下结果：
- en: 'Activation: *a = σ(z)*'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活：*a = σ(z)*
- en: '*w^([l])*: Weight in layer *l*'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*w^([l])*：第 *l* 层的权重'
- en: '*b^([l])*: Bias in layer *l*'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b^([l])*：第 *l* 层的偏置'
- en: Forward propagation in a deep network
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度网络的正向传播
- en: Let's see how these equations set up for layer 1 and layer 2\. If the training
    example set, X is (*x1*, *x2*, *x3*) for the preceding network.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些方程是如何为第 1 层和第 2 层设置的。如果训练示例集 X 是前述网络中的 (*x1*, *x2*, *x3*)。
- en: 'Let''s see how the equation comes along for layer 1:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看方程是如何应用于第 1 层的：
- en: '![](img/39d24283-bd5c-405f-b28c-30dbc286f3ba.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/39d24283-bd5c-405f-b28c-30dbc286f3ba.png)'
- en: 'The activation function for layer 1 is as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 第 1 层的激活函数如下：
- en: '![](img/fb5b4b19-d107-4c3b-b931-830f85a5f13d.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fb5b4b19-d107-4c3b-b931-830f85a5f13d.png)'
- en: 'The input can also be expressed as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 输入也可以表示如下：
- en: '![](img/efa63eef-38f6-4f0c-a623-5f61322c8531.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/efa63eef-38f6-4f0c-a623-5f61322c8531.png)'
- en: 'For layer 2, the input will be as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第 2 层，输入将是以下内容：
- en: '![](img/8824d96b-e31e-485e-b996-ea8995819a89.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8824d96b-e31e-485e-b996-ea8995819a89.png)'
- en: 'The activation function that''s applied here is as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这里应用到的激活函数如下：
- en: '![](img/de51ced3-93d0-4e7c-a632-668cb1a049b2.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/de51ced3-93d0-4e7c-a632-668cb1a049b2.png)'
- en: 'Similarly, for layer 3, the input that''s applied is as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，对于第 3 层，应用的输入如下：
- en: '![](img/87e6ec32-b73e-4e88-8ae5-e4c1d2ab84a3.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/87e6ec32-b73e-4e88-8ae5-e4c1d2ab84a3.png)'
- en: 'The activation function for the third layer is as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 第 3 层的激活函数如下：
- en: '![](img/4f055318-9468-48a3-833c-bfe8c4d84297.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4f055318-9468-48a3-833c-bfe8c4d84297.png)'
- en: 'Finally, here''s the input for the last layer:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这是最后一层的输入：
- en: '![](img/ad73712c-6c74-4216-9426-ea03e83737f9.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ad73712c-6c74-4216-9426-ea03e83737f9.png)'
- en: 'This is its activation:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是它的激活：
- en: '![](img/fb5e5016-951f-4828-b429-6641e712e412.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fb5e5016-951f-4828-b429-6641e712e412.png)'
- en: 'Hence, the generalized forward propagation equation turns out to be as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，广义正向传播方程如下：
- en: '![](img/9b0cc83d-c92e-459d-8c45-005037bbaa69.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9b0cc83d-c92e-459d-8c45-005037bbaa69.png)'
- en: '![](img/22c42f32-4024-4aaa-b7f0-909e019056ce.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/22c42f32-4024-4aaa-b7f0-909e019056ce.png)'
- en: Parameters W and b
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参数 W 和 b
- en: 'Let''s talk about the following architecture. First, let''s note down what
    we learned about in the previous section. Take a look at the following diagram:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论以下架构。首先，让我们记下我们在上一节中学到的内容。看一下以下图表：
- en: '![](img/b3ce16ca-11d7-4056-b2e9-5ac66bb93668.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b3ce16ca-11d7-4056-b2e9-5ac66bb93668.png)'
- en: 'Here, we can see the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们可以看到以下内容：
- en: '*l*: Number of layers: 6'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*l*：层数：6'
- en: '*n ^([l])*: Number of nodes in layer ![](img/c830fe26-cc8a-4f06-b7d6-42cda47290ae.png)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*n ^([l])*：第 *l* 层的节点数 ![图片](img/c830fe26-cc8a-4f06-b7d6-42cda47290ae.png)'
- en: '*n ^([0])*: Number of nodes in input layer: 3 ::'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*n ^([0])*：输入层的节点数：3 ::'
- en: '*n ^([1])*: Number of nodes in first layer: 4 ::'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*n ^([1])*：第一层的节点数：4 ::'
- en: 'The equation for this is as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程如下：
- en: '*n ^([2])= 4 :: n ^([3]) = 4 :: n ^([4]) = 4 :: n ^([5]) =3 :: n ^([6])= 1*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*n ^([2])= 4 :: n ^([3]) = 4 :: n ^([4]) = 4 :: n ^([5]) =3 :: n ^([6])= 1*'
- en: 'Implementing forward propagation would mean that hidden layer 1 can be expressed
    via the following equation:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 实现前向传播意味着隐藏层1可以通过以下方程表示：
- en: '![](img/02ba8710-56e1-443e-935c-7cfd7f5d91bd.png)…..(1)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/02ba8710-56e1-443e-935c-7cfd7f5d91bd.png)…..(1)'
- en: Can you determine the dimensions of z, w, and X for forward propagation?
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你能确定z、w和X的前向传播维度吗？
- en: Let's discuss this. *X* indicates the input layer vectors or nodes, and we know
    that there are 3 nodes. Can we find out the dimension of the input layer? Well,
    yes, it's (*n^([0])*, 1) – alternatively, you can say that it is (3,1).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来讨论这个问题。*X*表示输入层向量或节点，我们知道有3个节点。我们能找出输入层的维度吗？嗯，是的，它是(*n^([0])*, 1)——或者你也可以说它是(3,1)。
- en: What about for first hidden layer? Since the first hidden layer has got three
    nodes, the dimension of *z^([1])* will be (*n ^([1])*,1). This means that the
    dimension will be (4,1).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一隐藏层呢？由于第一隐藏层有三个节点，*z^([1])*的维度将是(*n^([1])*,1)。这意味着维度将是(4,1)。
- en: 'The dimensions of *z^([1] )*and X have been ascertained. By looking at the
    preceding equation, it is evident that the dimensions of *z^([1])* and *w^([1])**X* have
    to be the same (from linear algebra). So, can you come up with the dimension of
    *w^([1])*? We know from linear algebra that matrix multiplication between matrix
    1 and 2 is possible only when the number of columns of matrix 1 is equal to the
    number of rows of matrix 2\. So, the number of columns of *w^([1] )*has to be
    equal to the number of rows of matrix *X*. This will make the number of columns
    of *w^([1])*3\. However, as we''ve already discussed, the dimensions of *z^([1] )*and
    *w^([1])**X* have to be the same, and so the number of rows of the former should
    be equal to the number of rows of the latter. Hence, the number of rows of *w^([1])*will
    turn out to be 4\. Alright, we have got the dimension of *w^([1])* now, which
    is (4,3). To make this more general, we can also say that the dimension of *w^([1])* is
    (*n^([1])*,*n^([0])*). Similarly, the dimension of *w^([2])* will be equal to
    (*n^([2])*,*n^([1])*) or (number of nodes of the current layer, number of nodes
    of the previous layer). It will make the dimension of *w^([2] )*(4,4). Let''s
    generalize this. Let''s look at the dimension of the following equation:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*z^([1])*和X的维度已经被确定。通过观察前面的方程，很明显*z^([1])*和*w^([1])X*的维度必须相同（来自线性代数）。那么，你能推导出*w^([1])*的维度吗？我们知道，从线性代数的角度来看，矩阵1和矩阵2之间的乘法只有在矩阵1的列数等于矩阵2的行数时才可能。因此，*w^([1])*的列数必须等于矩阵X的行数。这将使得*w^([1])*的列数为3。然而，正如我们已经讨论过的，*z^([1])*和*w^([1])X*的维度必须相同，因此前者的行数应该等于后者的行数。因此，*w^([1])*的行数将变为4。好的，我们现在已经得到了*w^([1])*的维度，它是(4,3)。为了使这个结论更普遍，我们也可以说*w^([1])*的维度是(*n^([1]),*n^([0])*）。同样，*w^([2])*的维度将等于(*n^([2]),*n^([1])*）或者（当前层的节点数，前一层节点数）。这将使得*w^([2])*的维度为(4,4)。让我们来概括一下。让我们看看以下方程的维度：'
- en: '*w^([1])= (n^([1]),n^([l-1]))*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*w^([1])= (n^([1]),n^([l-1]))*'
- en: What about the dimension of bias *b^([1])*? Can you make use of linear algebra
    and figure that out? This must be a cake walk for you by now. Yes, you would have
    probably guessed it correctly by now. It is has the same dimension as *z^([1])*.
    Let me explain this, for everyone’s benefit. Going by the equation, the dimension
    of the left-hand side should be equal to the dimension of the right-hand side.
    Besides, *w^([1])**X + b^([1])* is an addition of two matrices and it is well-known
    that two matrices can only be added if they have the same dimension; that is,
    they must have the same number of rows and columns. So, the dimension of *b^([1] )*will
    be equal to *w^([1])**X*; in turn, it will be equal to *z^([1])* (which is (4,1)).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，偏置*b^([1])*的维度是什么？你能利用线性代数并找出它吗？现在这对你来说应该像吃蛋糕一样简单。是的，你现在可能已经正确猜到了。它具有与*z^([1])*相同的维度。让我来解释这一点，为了大家的利益。根据方程，左侧的维度应该等于右侧的维度。此外，*w^([1])X
    + b^([1])*是两个矩阵的和，众所周知，只有当两个矩阵具有相同的维度时才能相加；也就是说，它们必须有相同的行数和列数。因此，*b^([1])*的维度将等于*w^([1])X*；反过来，它将等于*z^([1])*（它是(4,1)）。
- en: In terms of generalization, the dimension of *b^([1])= (n^([1]), 1)*.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在一般化的意义上，*b^([1])*的维度是( n^([1]), 1)*。
- en: 'For backpropagation, this is as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于反向传播，情况如下：
- en: Dimension of *d**w^([l])= (n^([l]),n^([l-1]))*
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*d**w^([l])*的维度= (n^([l]),n^([l-1]))*'
- en: Dimension of *db**^([l])= (n^([l]), 1)*
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*db**^([l])*的维度= (n^([l]), 1)*'
- en: Forward and backward propagation
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前向和反向传播
- en: Let me show you how forward pass and backward pass work with the help of an
    example.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我通过一个例子向您展示正向传播和反向传播是如何工作的。
- en: 'We have got a network that has got two layers (1 hidden layer and 1 output
    layer). Every layer (including the input) has got two nodes. It has got bias nodes
    as well, as shown in the following diagram:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个具有两层（1个隐藏层和1个输出层）的网络。每个层（包括输入层）都有两个节点。它还有偏置节点，如下面的图所示：
- en: '![](img/fe2b3f04-2d32-478b-9b8b-4732b81ecf6d.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fe2b3f04-2d32-478b-9b8b-4732b81ecf6d.png)'
- en: 'The notations that are used in the preceding diagram are as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 上图中使用的符号如下：
- en: '*IL*: Input layer'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*IL*: 输入层'
- en: '*HL*: Hidden layer'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*HL*: 隐藏层'
- en: '*OL*: Output layer'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*OL*: 输出层'
- en: '*w*: Weight'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*w*: 权重'
- en: '*B*: Bias'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*B*: 偏置'
- en: We have got the values for all of the required fields. Let's feed this into
    the network and see how it flows. The activation function that's being used here
    is the sigmoid.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经得到了所有所需字段的值。让我们将这些值输入到网络中，看看它如何流动。这里使用的激活函数是Sigmoid函数。
- en: 'The input that''s given to the first node of the hidden layer is as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给隐藏层第一个节点的输入如下：
- en: '*InputHL1 = w1*IL1 + w3*IL2 + B1*'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*InputHL1 = w1*IL1 + w3*IL2 + B1*'
- en: '*InputHL1= (0.2*0.8)+(0.4*0.15) + 0.4 =0.62*'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*InputHL1= (0.2*0.8)+(0.4*0.15) + 0.4 =0.62*'
- en: 'The input that''s given to the second node of the hidden layer is as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给隐藏层第二个节点的输入如下：
- en: '*InputHL2 = w2*IL1 + w4*IL2 + B1*'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*InputHL2 = w2*IL1 + w4*IL2 + B1*'
- en: '*InputHL2 = (0.25*0.8) +(0.1*0.15) + 0.4 = 0.615*'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*InputHL2 = (0.25*0.8) +(0.1*0.15) + 0.4 = 0.615*'
- en: 'To find out the output, we will use our activation function, like so:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到输出，我们将使用我们的激活函数，如下所示：
- en: '*OutputHL1 = ![](img/8b156d03-3ef6-4e23-84bd-819530ddd10c.png)  = 0.650219*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*OutputHL1 = ![图片](img/8b156d03-3ef6-4e23-84bd-819530ddd10c.png)  = 0.650219*'
- en: '*OutputHL2 = ![](img/534e2915-fe06-4d9a-940a-c198ee8bfb4f.png)= 0.649081*'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '*OutputHL2 = ![图片](img/534e2915-fe06-4d9a-940a-c198ee8bfb4f.png)= 0.649081*'
- en: 'Now, these outputs will be passed on to the output layer as input. Let''s calculate
    the value of the input for the nodes in the output layer:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这些输出将作为输入传递到输出层。让我们计算输出层节点的输入值：
- en: '*InputOL1 = w5*Output_HL1 + w7*Output_HL2 + B2 = 0.804641*'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*InputOL1 = w5*Output_HL1 + w7*Output_HL2 + B2 = 0.804641*'
- en: '*InputOL2= w6*Output_HL1 + w8*Output_HL2 + B2= 0.869606*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*InputOL2= w6*Output_HL1 + w8*Output_HL2 + B2= 0.869606*'
- en: 'Now, let''s compute the output:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们计算输出：
- en: '*Output[OL1] = ![](img/24d98aeb-c845-4fc7-9cd0-271853b5256a.png) = 0.690966*'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*Output[OL1] = ![图片](img/24d98aeb-c845-4fc7-9cd0-271853b5256a.png) = 0.690966*'
- en: '*Output[OL2] =![](img/56e40684-a2a9-4320-8c70-47b23ae15bfd.png) = 0.704664*'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*Output[OL2] =![](img/56e40684-a2a9-4320-8c70-47b23ae15bfd.png) = 0.704664*'
- en: Error computation
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 误差计算
- en: 'We can now calculate the error for each output neuron using the square error
    function and sum them together to get the total error:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用平方误差函数计算每个输出神经元的误差，并将它们相加以得到总误差：
- en: '*Etotal = *![](img/ed8a52d3-9fa1-4903-9c32-db4474463294.png)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*Etotal = *![图片](img/ed8a52d3-9fa1-4903-9c32-db4474463294.png)'
- en: '*EOL1 = Error at first node of output layer =*![](img/4fb5246b-2012-47dc-960a-b7f7ab5c16e9.png)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '*EOL1 = 输出层第一个节点的误差 =*![图片](img/4fb5246b-2012-47dc-960a-b7f7ab5c16e9.png)'
- en: '*=0.021848*'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '*=0.021848*'
- en: '*EOL2 = Error at second node of output layer =* ![](img/1598226c-3004-42d0-9f4e-afcc5b93b24c.png)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*EOL2 = 输出层第二个节点的误差 =* ![图片](img/1598226c-3004-42d0-9f4e-afcc5b93b24c.png)'
- en: '*=0.182809*'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '*=0.182809*'
- en: '*Total Error = Etotal= EOL1 + EOL2 = 0.021848 + 0.182809 = 0.204657*'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '*Total Error = Etotal= EOL1 + EOL2 = 0.021848 + 0.182809 = 0.204657*'
- en: Backward propagation
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播
- en: The purpose of backpropagation is to update each of the weights in the network
    so that they cause the actual output to be closer to the target output, thereby
    minimizing the error for each output neuron and the network as a whole.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播的目的是更新网络中每个权重的值，以便它们使实际输出更接近目标输出，从而最小化每个输出神经元和整个网络的误差。
- en: Let's focus on an output layer first. We are supposed to find out the impact
    of change in w5 on the total error.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先关注输出层。我们应该找出w5的变化对总误差的影响。
- en: This will be decided by ![](img/19ebbc28-eeac-4d1d-b685-1f7adeafdfa3.png). It
    is the partial derivative of Etotal with respect to w5.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这将由![图片](img/19ebbc28-eeac-4d1d-b685-1f7adeafdfa3.png)决定。它是Etotal相对于w5的偏导数。
- en: 'Let''s apply the chain rule here:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们应用链式法则：
- en: '![](img/8b71d76e-07e9-49dc-bdaf-eda319eadd55.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8b71d76e-07e9-49dc-bdaf-eda319eadd55.png)'
- en: '![](img/2f68c391-d1a5-482f-a663-ab7b1ce167d7.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2f68c391-d1a5-482f-a663-ab7b1ce167d7.png)'
- en: '![](img/1143e171-c1c6-4fe7-9c8b-55bd3f9b062f.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1143e171-c1c6-4fe7-9c8b-55bd3f9b062f.png)'
- en: '*= 0.690966 – 0.9 = -0.209034*'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '*= 0.690966 – 0.9 = -0.209034*'
- en: '![](img/d7f4f53f-c931-452b-8e67-9ea901f3c615.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d7f4f53f-c931-452b-8e67-9ea901f3c615.png)'
- en: '![](img/4ef3d0fd-7a77-4c98-b36d-e80053d71f84.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4ef3d0fd-7a77-4c98-b36d-e80053d71f84.png)'
- en: '*= 0.213532*'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '*= 0.213532*'
- en: '*InputOL1 = w5*OutputHL1 + w7*OutputHL2 + B2*'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '*InputOL1 = w5*OutputHL1 + w7*OutputHL2 + B2*'
- en: '*![](img/d594ec43-ddbf-45a9-883e-19c75ad8a624.png)= 0.650219*'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '*![](img/d594ec43-ddbf-45a9-883e-19c75ad8a624.png)= 0.650219*'
- en: 'Now, let''s get back to the old equation:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到旧方程：
- en: '![](img/759a15cf-94d3-4f7f-9814-4590259b23bf.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/759a15cf-94d3-4f7f-9814-4590259b23bf.png)'
- en: '![](img/dd625504-e1a7-43f4-b88c-948fb580e10c.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/dd625504-e1a7-43f4-b88c-948fb580e10c.png)'
- en: 'To update the weight, we will use the following formula. We have set the learning
    rate to be *α = 0.1*:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更新权重，我们将使用以下公式。我们已将学习率设置为 *α = 0.1*：
- en: '![](img/76b005cf-55a2-4935-b7d5-9fd8d59b8ac0.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/76b005cf-55a2-4935-b7d5-9fd8d59b8ac0.png)'
- en: Similarly, ![](img/b27156ea-8276-45b1-92e3-e1a34a2d67bc.png) are supposed to
    be calculated. The approach remains the same. We will leave this to compute as
    it will help you in understanding the concepts better.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 同样， ![](img/b27156ea-8276-45b1-92e3-e1a34a2d67bc.png) 也应该被计算。方法保持不变。我们将留给你计算，因为它将帮助你更好地理解概念。
- en: When it comes down to the hidden layer and computing, the approach still remains
    the same. However, the formula will change a bit. I will help you with the formula,
    but the rest of the computation has to be done by you.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到隐藏层和计算时，方法仍然保持不变。然而，公式会有所变化。我会帮你弄清楚公式，但其余的计算必须由你自己完成。
- en: We will take *w1* here.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里取 *w1*。
- en: 'Let''s apply the chain rule here:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在这里应用链式法则：
- en: '![](img/c72c8107-c603-4256-98be-ec0d735c48ae.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c72c8107-c603-4256-98be-ec0d735c48ae.png)'
- en: This formula has to be utilized for *w2*, *w3*, and *w4*. Please ensure that
    you are doing partial differentiation of *E_total* with respect to other weights
    and, in the end, that you are using the learning rate formula to get the updated
    weight.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式必须用于 *w2*、*w3* 和 *w4*。请确保你对 *E_total* 进行其他权重的偏导数，最后使用学习率公式来获取更新的权重。
- en: Forward propagation equation
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前向传播方程
- en: 'We know the equations around it. If the input for this is *a^([l-1])*, then
    the output will be *a^([l])*. However, there is a cache part, which is nothing
    but z*^([l])*, as shown in the following diagram:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道周围的方程。如果这个输入是 *a^([l-1])*，那么输出将是 *a^([l])*。然而，还有一个缓存部分，它就是 z*^([l])*，如下图中所示：
- en: '![](img/bd54a557-21c3-42af-8627-540138cb1f1e.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bd54a557-21c3-42af-8627-540138cb1f1e.png)'
- en: Here, this breaks down into *w^([1])a^([l-1]) +b^([l])* (remember that *a^([0])* is
    equal to *X*).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，这分解为 *w^([1])a^([l-1]) +b^([l])* （记住 *a^([0])* 等于 *X*）。
- en: Backward propagation equation
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播方程
- en: 'The following equations would be required to execute backward propagation:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 执行反向传播需要以下方程：
- en: '![](img/ae157002-f173-49f2-8834-e62dde566376.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ae157002-f173-49f2-8834-e62dde566376.png)'
- en: '![](img/10786584-fc12-4a34-bc22-a86a3b48ce3e.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/10786584-fc12-4a34-bc22-a86a3b48ce3e.png)'
- en: 'These equations will give you an idea of what is going on behind the scenes.
    Here, a suffix, *d*, has been added, which is a representation of the partial
    derivative that acts during backward propagation:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方程将给你一个关于幕后发生的事情的思路。在这里，添加了一个后缀，*d*，它表示在反向传播期间起作用的偏导数：
- en: '![](img/484cc2b3-97cd-4819-bf37-e8a1449f47ff.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/484cc2b3-97cd-4819-bf37-e8a1449f47ff.png)'
- en: '![](img/2b119630-edde-4f04-8387-58b8cf1dc790.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2b119630-edde-4f04-8387-58b8cf1dc790.png)'
- en: Parameters and hyperparameters
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参数和超参数
- en: While we are getting on with building a deep learning model, you need to know
    how to keep a tab on both parameters and hyperparameters. But how well do we understand
    these?
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们着手构建深度学习模型时，你需要知道如何同时监控参数和超参数。但我们到底理解得有多好呢？
- en: When it comes down to parameters, we have got weights and biases. As we begin
    to train the network, one of the prime steps is to initialize the parameters.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到参数时，我们拥有权重和偏差。当我们开始训练网络时，首要步骤之一就是初始化参数。
- en: Bias initialization
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏置初始化
- en: It is a common practice to initialize the bias by zero as the symmetrical breaking
    of neurons is taken care of by the random weights' initialization.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化偏置为零是一种常见做法，因为神经元的对称打破由随机权重的初始化来处理。
- en: Hyperparameters
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数
- en: Hyperparameters are one of the building blocks of the deep learning network.
    It is an element that determines the optimal architecture of the network (for
    example, number of layers) and also a factor that is responsible for ensuring
    how the network will be trained.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数是深度学习网络的基本构建块之一。它是决定网络最佳架构（例如，层数）的元素，也是确保网络如何训练的因素。
- en: 'The following are the various hyperparameters of the deep learning network:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是深度学习网络的各个超参数：
- en: '**Learning rate**: This is responsible for determining the pace at which the
    network is trained. A slow learning rate ensures a smooth convergence, whereas
    a fast learning rate may not have smooth convergence.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率**：这是负责确定网络训练速度的。慢速学习率确保平滑收敛，而快速学习率可能不会平滑收敛。'
- en: '**Epoch**: The number of epochs is the number of times the whole training data
    is consumed by the network while training.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**周期**：周期数是在训练过程中网络消耗整个训练数据的次数。'
- en: '**Number of hidden layers**: This determines the structure of the model, which
    helps in achieving the optimal capacity of the model.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏层数量**：这决定了模型的结构，有助于实现模型的最佳容量。'
- en: '**Number of nodes (neurons)**: There should be a trade-off between the number
    of nodes to be used. It decides whether all of the necessary information has been
    extracted to produce the required output. Overfitting or underfitting will be
    decided by the number of nodes. Hence, it is advisable to use it with regularization.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点数（神经元）**：应权衡使用的节点数。它决定了是否已经提取了所有必要的信息以产生所需输出。节点数将决定过拟合或欠拟合。因此，建议与正则化一起使用。'
- en: '**Dropout**: Dropout is a regularization technique that''s used to increase
    generalizing power by avoiding overfitting. This was discussed in detail in [Chapter
    4](e994d382-9a54-427c-86db-caf852e5c084.xhtml), *Training Neural Networks*. The
    dropout value can be between 0.2 and 0.5.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Dropout**：Dropout是一种正则化技术，通过避免过拟合来提高泛化能力。这在[第4章](e994d382-9a54-427c-86db-caf852e5c084.xhtml)《训练神经网络》中进行了详细讨论。Dropout值可以在0.2到0.5之间。'
- en: '**Momentum**: This determines the direction of the next step toward convergence.
    With a value between 0.6 and 0.9, it handles oscillation.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动量**：这决定了向收敛方向下一步的方向。在0.6到0.9之间，它处理振荡。'
- en: '**Batch size**: This is the number of samples that are fed into the network,
    after which a parameter update happens. Typically, it is taken as 32, 64, 128,
    256.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量大小**：这是输入到网络中的样本数，之后发生参数更新。通常，它取32、64、128、256。'
- en: To find the optimal number of hyperparameters, it is prudent to deploy a grid
    search or random search.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到最佳的超参数数量，部署网格搜索或随机搜索是明智的。
- en: Use case – digit recognizer
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用例 - 数字识别器
- en: The **Modified National Institute of Standards and Technology** (**MNIST**) is
    in fact the dataset of computer vision for *hello world*. Considering its release
    in 1999, this dataset has served as the main fundamental basis for benchmarking
    classification algorithms.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**修改后的国家标准与技术研究院**（**MNIST**）实际上是为计算机视觉的“hello world”准备的。考虑到它在1999年的发布，这个数据集已经作为基准分类算法的主要基础。'
- en: 'Our goal is to correctly identify digits from a dataset of tens of thousands
    of handwritten images. We have curated a set of tutorial-style kernels that cover
    everything from regression to neural networks:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是从包含数万张手写图像的数据集中正确识别数字。我们精心制作了一套教程风格的内核，涵盖了从回归到神经网络的所有内容：
- en: '[PRE0]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 前一段代码的输出如下：
- en: '![](img/80896871-a1bb-46e5-9c71-5e2b39b1abb9.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/80896871-a1bb-46e5-9c71-5e2b39b1abb9.png)'
- en: '[PRE1]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here, we get the following output:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们得到以下输出：
- en: '![](img/cc8d0828-a5b6-42d9-a851-660c411b9025.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cc8d0828-a5b6-42d9-a851-660c411b9025.png)'
- en: '[PRE2]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here, we get the following output:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们得到以下输出：
- en: '![](img/0e8f70db-2c67-40c5-a9af-59a109e83736.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e8f70db-2c67-40c5-a9af-59a109e83736.png)'
- en: '[PRE3]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'By reshaping the image into 3 dimensions, we get the following:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将图像重塑为3维，我们得到以下结果：
- en: '[PRE4]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'By executing the following code, we will be able to see the numbered plot:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下代码后，我们将能够看到编号的图表：
- en: '[PRE5]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output is as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/cf01e7a0-dc45-4863-8a0b-870e9f549a52.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf01e7a0-dc45-4863-8a0b-870e9f549a52.png)'
- en: 'The sequential model is now as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序模型现在如下：
- en: '[PRE6]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'When we define the optimizer, we get the following output:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们定义优化器时，我们得到以下输出：
- en: '[PRE7]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'When we compile the model, we get the following output:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们编译模型时，我们得到以下输出：
- en: '[PRE8]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, we generate the image generator:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们生成图像生成器：
- en: '[PRE9]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output can be seen as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![](img/6c892255-d1c4-47f1-b62d-ff8b95726881.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6c892255-d1c4-47f1-b62d-ff8b95726881.png)'
- en: 'We predict the model as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预测模型如下：
- en: '[PRE10]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output can be seen as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![](img/5e3c6373-055f-4d8e-8465-d835d9774367.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5e3c6373-055f-4d8e-8465-d835d9774367.png)'
- en: Generative adversarial networks
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: '**Generative adversarial networks** (**GANs**) are another form of deep neural
    network architecture, and is a combination of two networks that compete and cooperate
    with each other. It was introduced by Ian Goodfellow and Yoshua Bengio in 2014.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成对抗网络（GANs**）是另一种深度神经网络架构，是两个相互竞争和合作的网络的组合。它由Ian Goodfellow和Yoshua Bengio于2014年提出。'
- en: GANs can learn to mimic any distribution of data, which ideally means that GANs
    can be taught to create an object that's similar to an existing one in any domain,
    such as images, music, speech, and prose. It can create photos of any object that
    has never existed before. They are robot artists in a sense, and their output
    is impressive.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: GANs可以学习模仿任何数据分布，这理想情况下意味着GANs可以被训练来创建与现有领域中的任何类似的对象，例如图像、音乐、语音和散文。它可以创建任何从未存在过的物体的照片。从某种意义上说，它们是机器人艺术家，它们的输出令人印象深刻。
- en: It falls under unsupervised learning wherein both of the networks learn their
    task upon training. One of the networks is called the **generator** and the other
    is called the **discriminator**.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 它属于无监督学习，其中两个网络在训练过程中各自学习其任务。其中一个网络被称为**生成器**，另一个被称为**判别器**。
- en: To make this more understandable, we can think of a **GAN** as a case of a counterfeiter
    (generator) and a cop (discriminator). At the outset, the counterfeiter shows
    the cop fake money. The cop works like a detective and finds out that it is a
    fake money (you can think of D as a detective too if you want to understand how
    a discriminator works). The cop passes his feedback to the counterfeiter, explaining
    why the money is fake. The counterfeiter makes a few adjustments and makes new,
    fake money based on the feedback it received. The cop says that the money is still
    fake and he shares his new feedback with the counterfeiter. The counterfeiter
    then attempts to make new, fake money based on the latest feedback. The cycle
    continues indefinitely until the cop is fooled by the fake money because it looks
    real. When a GAN model is being created, the generator and discriminator start
    to learn from scratch and from each other. It may seem that they are pitted against
    each other, but they are helping each other learn. The feedback mechanism between
    these two is helping the model to be more robust.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这更容易理解，我们可以将**生成对抗网络（GAN**）想象成一个伪造者（生成器）和警察（判别器）的案例。一开始，伪造者向警察展示假币。警察就像侦探一样，发现这些是假币（如果你想了解判别器是如何工作的，也可以把D想象成侦探）。警察将他的反馈传递给伪造者，解释为什么这些钱是假的。伪造者根据收到的反馈做一些调整，并基于这些反馈制作新的假币。警察说这些钱仍然是假的，并将他的新反馈与伪造者分享。然后，伪造者根据最新的反馈尝试制作新的假币。这个过程无限循环，直到警察被这些看起来真实的假币欺骗。当创建GAN模型时，生成器和判别器从零开始相互学习。它们似乎是对抗的，但实际上它们在互相帮助学习。这两个之间的反馈机制有助于模型变得更加鲁棒。
- en: 'A discriminator is quite a good learner, since it is capable of learning anything
    from the real world. That is, if you want it to learn about images of cats and
    dogs, and its 1,000 different categories where it''s asked to differentiate between
    the images, it will be able to do so without any hassle, like so:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器是一个非常优秀的学习者，因为它能够从现实世界中学习任何东西。也就是说，如果你想让它学习关于猫和狗的图片，以及它被要求区分的1,000个不同类别，它将能够轻松做到，就像这样：
- en: '![](img/5f4c7150-07a3-481e-a605-e43d9d713791.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5f4c7150-07a3-481e-a605-e43d9d713791.png)'
- en: Noise goes into the generator; then, the output of the generator goes through
    the discriminator and we get an output. Simultaneously, the discriminator is being
    trained on the images of dogs. However, in the very beginning, even the dog images
    can be classified by the discriminator as being non-dog images, and it picks up
    on this error. This error gets back propagated through the network.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声进入生成器；然后，生成器的输出通过判别器，我们得到一个输出。同时，判别器正在对狗的图片进行训练。然而，在最开始，即使是狗的图片也可能被判别器错误地分类为非狗图片，并注意到这个错误。这个错误通过网络反向传播。
- en: Hinton's Capsule network
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 霍inton的胶囊网络
- en: Geoffrey Hinton, the father of deep learning, created a huge stir in the space
    of deep learning by introducing a new network. This network was called the **Capsule
    Network** (**CapsNet**). An algorithm to train this network was also brought forth,
    which is called **dynamic routing** **between capsules**. For the first time,
    Hinton spoke about it in 2011 in the paper called **transforming autoencoder**.
    In 2017 November, a full paper was published by Hinton and his team regarding
    the Capsule network.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习之父杰弗里·辛顿通过引入一个新的网络在深度学习领域引起了巨大的轰动。这个网络被称为**胶囊网络**（**CapsNet**）。还提出了一个训练这个网络的算法，称为**胶囊之间的动态路由**。辛顿首次在2011年的论文《转换自编码器》中提到了它。2017年11月，辛顿和他的团队发表了一篇关于胶囊网络的完整论文。
- en: The Capsule Network and convolutional neural networks
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 胶囊网络和卷积神经网络
- en: The **convolutional neural network** (**CNN**) has been one of the most important
    milestones in the area of deep learning. It has got everyone excited and has been
    the cornerstone for new research, too. But, as they say, *Nothing is perfect in
    this world*. Nor is our beloved CNN.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNN**）是深度学习领域的一个重要里程碑。它让每个人都感到兴奋，也是新研究的基础。但是，正如人们所说，*世界上没有什么是完美的*。我们心爱的CNN也不例外。'
- en: 'Can you recall how CNNs work? The most important job of a CNN is to execute
    convolution. What this means is that once you pass an image through CNN, the features,
    such as edges and color gradients, are extracted from image pixels by the convolution
    layer. Other layers will combine these features into a more complex one. And on
    top of it, once the dense layer is kept, it enables the network to carry out the
    classification job. The following diagram shows the image that we are working
    on:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 你能回忆起CNN是如何工作的吗？CNN最重要的任务是执行卷积。这意味着一旦你将一张图片通过CNN，卷积层就会从图像像素中提取出特征，如边缘和颜色梯度。其他层会将这些特征组合成一个更复杂的特征。而且，一旦密集层被保留，它就使得网络能够执行分类任务。以下图表显示了我们在工作的图像：
- en: '![](img/c0ef9182-2bff-467b-b8e4-7dc60d19db61.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c0ef9182-2bff-467b-b8e4-7dc60d19db61.png)'
- en: 'The preceding diagram is a basic CNN network, which is being used to detect
    the car in the image. The following diagram shows the image of a car that is in
    perfect order, and a fragmented image of the same:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表是一个基本的CNN网络，它被用来检测图像中的汽车。以下图表显示了同一辆车的完整图像和碎片化图像：
- en: '![](img/d34d0009-4a1a-40ab-9935-52be22d7ee52.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d34d0009-4a1a-40ab-9935-52be22d7ee52.png)'
- en: Let's say we pass on these two images through the CNN network (to detect the
    car) – what will be the response of the network for both images? Can you ponder
    over this and come up with an answer? Just to help you, a car has got a number
    of components such as wheels, a windshield, a bonnet, and so on, but a car is
    deemed as a car to human eyes when all of these parts/components are set in order.
    However, for a CNN, only the features are important. A CNN doesn't take relative
    positional and orientational relationship into account. So, both of these images
    will be classified as a car by the network, even though this is not the case to
    the human eye.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们将这两张图片通过CNN网络（用于检测汽车）传递过去——网络对这两张图片的响应会是什么？你能思考一下并给出答案吗？为了帮助你，一辆车有几个组成部分，如车轮、挡风玻璃、引擎盖等，但人类眼睛认为当所有这些部分/组件按顺序排列时，它才是一辆车。然而，对于CNN来说，只有特征才是重要的。CNN不考虑相对位置和方向关系。因此，这两张图片都将被网络分类为汽车，尽管这在人类眼中并非如此。
- en: To make amends, CNNs include max-pooling, which helps in increasing the view
    of a higher layer's neurons, thus making the detection of higher order features
    possible. Max-pooling makes CNNs work, but at the same time, information loss
    also takes place. It's a big drawback of CNN.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弥补这一点，CNN包括最大池化，这有助于增加更高层神经元的视野，从而使得检测更高阶特征成为可能。最大池化使得CNN能够工作，但同时也发生了信息损失。这是CNN的一个大缺点。
- en: Summary
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we studied deep neural networks and why we need a deep learning
    model. We also learned about forward and backward prorogation, along with parameters
    and hyperparameters. We also talked about GANs, along with deep Gaussian processes,
    the Capsule Network, and CNNs.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们研究了深度神经网络以及为什么我们需要深度学习模型。我们还学习了前向和反向传播，以及参数和超参数。我们还讨论了GANs、深度高斯过程、胶囊网络和CNN。
- en: In the next chapter, we will study causal inference.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将研究因果推断。
