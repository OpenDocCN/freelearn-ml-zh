- en: Selected Topics in Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 4](6fd48e9f-f2fd-4b29-a006-1b151de4960f.xhtml)*, **Training Neural
    Networks*, we looked at what an **artificial neural network** (**ANN**) is and
    how this kind of model is built. You can say that a deep neural network is an
    elongated version of an ANN; however, it has got its own set of challenges.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is a deep neural network?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to initialize parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adversarial networks—generative adversarial networks and Bayesian generative
    adversarial networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep Gaussian processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton's Capsule network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s recap on what we learned in [Chapter 4](6fd48e9f-f2fd-4b29-a006-1b151de4960f.xhtml)*,
    Training Neural Networks*. A neural network is a machine emulation of the human
    brain that is seen as a set of algorithms that have been set out to extract patterns
    out of data. It has got three different layers:'
  prefs: []
  type: TYPE_NORMAL
- en: Input layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sensory numerical data (in the form of a vector) passes through the input layer
    and then goes through the hidden layers to generate its own set of perceptions
    and reasoning to yield the final result in the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Can you recall what we learned in [Chapter 4](6fd48e9f-f2fd-4b29-a006-1b151de4960f.xhtml),
    *Training Neural Networks**, *regarding the number of layers in ANN and how we
    count them? When we have got the layers like the ones shown in the following diagram,
    can you count the number of layers? Remember, we always count just the hidden
    layer and the output layer. So, if somebody is asking you how many layers there
    are in your network, you don''t include the input layer while answering:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3c7c2035-a325-4e73-8071-99a1ae84c085.png)'
  prefs: []
  type: TYPE_IMG
- en: Yes, that's right—there are two layers in the preceding architecture. What about
    for the following network?
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/20b65667-28a5-42c9-a8ba-e4d1ecfa8e30.png)'
  prefs: []
  type: TYPE_IMG
- en: This network has got three layers, which includes two hidden layers. As the
    layers increase, the model becomes deeper.
  prefs: []
  type: TYPE_NORMAL
- en: Why do we need a deep learning model?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A deep learning model is a highly non-linear model that has got multiple layers
    with multiple nodes acting in sequence to solve a business problem. Every layer
    has been assigned a different task.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if we have got a face detection problem, hidden layer 1 finds
    out which edges are present in the image. Layer 2 finds out the combination of
    edges, which start taking the shape of eyes, a nose, and other parts. Layer 3
    enables the object models, which creates the shape of the face. The following
    diagram shows the different hidden layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/25fca5bd-3031-4453-8260-dc708618b000.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we have got a logistic regression model, also known as a **single layer
    neural network**. Sometimes, it is also called the most **shallow network**. The
    second network that can be seen here has got a two-layer network. Again, it's
    a shallow network, but it's not as shallow as the previous one. The next architecture
    has got three layers, which is making things more interesting now. The network
    is getting deep now. The last architecture has got a six layer architecture, which
    is comprised of five hidden layers. The number of layers is getting even deeper.
  prefs: []
  type: TYPE_NORMAL
- en: Deep neural network notation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The explanation of the notation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*l*: Number of layers is 4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*n^([l])*: Number of nodes in layer *l *'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the following architecture, this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*n ^([0]):* Number of nodes in input layer, that is, 3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*n ^([1])*: 5'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*n ^([2])*: 5'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*n ^([3]):* 3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*n ^([4])*: 1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*a ^([l])*: Activations in layer *l:*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/2dc827b1-5595-4a37-b9b8-d58cb1a98826.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we already know, the following equation goes through the layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '*z = w^TX + b*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, we get the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Activation: *a = σ(z)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*w^([l])*: Weight in layer *l*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*b^([l])*: Bias in layer *l*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forward propagation in a deep network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's see how these equations set up for layer 1 and layer 2\. If the training
    example set, X is (*x1*, *x2*, *x3*) for the preceding network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how the equation comes along for layer 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/39d24283-bd5c-405f-b28c-30dbc286f3ba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The activation function for layer 1 is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fb5b4b19-d107-4c3b-b931-830f85a5f13d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The input can also be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/efa63eef-38f6-4f0c-a623-5f61322c8531.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For layer 2, the input will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8824d96b-e31e-485e-b996-ea8995819a89.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The activation function that''s applied here is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/de51ced3-93d0-4e7c-a632-668cb1a049b2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, for layer 3, the input that''s applied is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/87e6ec32-b73e-4e88-8ae5-e4c1d2ab84a3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The activation function for the third layer is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4f055318-9468-48a3-833c-bfe8c4d84297.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, here''s the input for the last layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad73712c-6c74-4216-9426-ea03e83737f9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is its activation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fb5e5016-951f-4828-b429-6641e712e412.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Hence, the generalized forward propagation equation turns out to be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0cc83d-c92e-459d-8c45-005037bbaa69.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/22c42f32-4024-4aaa-b7f0-909e019056ce.png)'
  prefs: []
  type: TYPE_IMG
- en: Parameters W and b
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s talk about the following architecture. First, let''s note down what
    we learned about in the previous section. Take a look at the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b3ce16ca-11d7-4056-b2e9-5ac66bb93668.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we can see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*l*: Number of layers: 6'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*n ^([l])*: Number of nodes in layer ![](img/c830fe26-cc8a-4f06-b7d6-42cda47290ae.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*n ^([0])*: Number of nodes in input layer: 3 ::'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*n ^([1])*: Number of nodes in first layer: 4 ::'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The equation for this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*n ^([2])= 4 :: n ^([3]) = 4 :: n ^([4]) = 4 :: n ^([5]) =3 :: n ^([6])= 1*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementing forward propagation would mean that hidden layer 1 can be expressed
    via the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02ba8710-56e1-443e-935c-7cfd7f5d91bd.png)…..(1)'
  prefs: []
  type: TYPE_IMG
- en: Can you determine the dimensions of z, w, and X for forward propagation?
  prefs: []
  type: TYPE_NORMAL
- en: Let's discuss this. *X* indicates the input layer vectors or nodes, and we know
    that there are 3 nodes. Can we find out the dimension of the input layer? Well,
    yes, it's (*n^([0])*, 1) – alternatively, you can say that it is (3,1).
  prefs: []
  type: TYPE_NORMAL
- en: What about for first hidden layer? Since the first hidden layer has got three
    nodes, the dimension of *z^([1])* will be (*n ^([1])*,1). This means that the
    dimension will be (4,1).
  prefs: []
  type: TYPE_NORMAL
- en: 'The dimensions of *z^([1] )*and X have been ascertained. By looking at the
    preceding equation, it is evident that the dimensions of *z^([1])* and *w^([1])**X* have
    to be the same (from linear algebra). So, can you come up with the dimension of
    *w^([1])*? We know from linear algebra that matrix multiplication between matrix
    1 and 2 is possible only when the number of columns of matrix 1 is equal to the
    number of rows of matrix 2\. So, the number of columns of *w^([1] )*has to be
    equal to the number of rows of matrix *X*. This will make the number of columns
    of *w^([1])*3\. However, as we''ve already discussed, the dimensions of *z^([1] )*and
    *w^([1])**X* have to be the same, and so the number of rows of the former should
    be equal to the number of rows of the latter. Hence, the number of rows of *w^([1])*will
    turn out to be 4\. Alright, we have got the dimension of *w^([1])* now, which
    is (4,3). To make this more general, we can also say that the dimension of *w^([1])* is
    (*n^([1])*,*n^([0])*). Similarly, the dimension of *w^([2])* will be equal to
    (*n^([2])*,*n^([1])*) or (number of nodes of the current layer, number of nodes
    of the previous layer). It will make the dimension of *w^([2] )*(4,4). Let''s
    generalize this. Let''s look at the dimension of the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*w^([1])= (n^([1]),n^([l-1]))*'
  prefs: []
  type: TYPE_NORMAL
- en: What about the dimension of bias *b^([1])*? Can you make use of linear algebra
    and figure that out? This must be a cake walk for you by now. Yes, you would have
    probably guessed it correctly by now. It is has the same dimension as *z^([1])*.
    Let me explain this, for everyone’s benefit. Going by the equation, the dimension
    of the left-hand side should be equal to the dimension of the right-hand side.
    Besides, *w^([1])**X + b^([1])* is an addition of two matrices and it is well-known
    that two matrices can only be added if they have the same dimension; that is,
    they must have the same number of rows and columns. So, the dimension of *b^([1] )*will
    be equal to *w^([1])**X*; in turn, it will be equal to *z^([1])* (which is (4,1)).
  prefs: []
  type: TYPE_NORMAL
- en: In terms of generalization, the dimension of *b^([1])= (n^([1]), 1)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For backpropagation, this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Dimension of *d**w^([l])= (n^([l]),n^([l-1]))*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimension of *db**^([l])= (n^([l]), 1)*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forward and backward propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let me show you how forward pass and backward pass work with the help of an
    example.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have got a network that has got two layers (1 hidden layer and 1 output
    layer). Every layer (including the input) has got two nodes. It has got bias nodes
    as well, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fe2b3f04-2d32-478b-9b8b-4732b81ecf6d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The notations that are used in the preceding diagram are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*IL*: Input layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*HL*: Hidden layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*OL*: Output layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*w*: Weight'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*B*: Bias'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have got the values for all of the required fields. Let's feed this into
    the network and see how it flows. The activation function that's being used here
    is the sigmoid.
  prefs: []
  type: TYPE_NORMAL
- en: 'The input that''s given to the first node of the hidden layer is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*InputHL1 = w1*IL1 + w3*IL2 + B1*'
  prefs: []
  type: TYPE_NORMAL
- en: '*InputHL1= (0.2*0.8)+(0.4*0.15) + 0.4 =0.62*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The input that''s given to the second node of the hidden layer is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*InputHL2 = w2*IL1 + w4*IL2 + B1*'
  prefs: []
  type: TYPE_NORMAL
- en: '*InputHL2 = (0.25*0.8) +(0.1*0.15) + 0.4 = 0.615*'
  prefs: []
  type: TYPE_NORMAL
- en: 'To find out the output, we will use our activation function, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '*OutputHL1 = ![](img/8b156d03-3ef6-4e23-84bd-819530ddd10c.png)  = 0.650219*'
  prefs: []
  type: TYPE_NORMAL
- en: '*OutputHL2 = ![](img/534e2915-fe06-4d9a-940a-c198ee8bfb4f.png)= 0.649081*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, these outputs will be passed on to the output layer as input. Let''s calculate
    the value of the input for the nodes in the output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '*InputOL1 = w5*Output_HL1 + w7*Output_HL2 + B2 = 0.804641*'
  prefs: []
  type: TYPE_NORMAL
- en: '*InputOL2= w6*Output_HL1 + w8*Output_HL2 + B2= 0.869606*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s compute the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Output[OL1] = ![](img/24d98aeb-c845-4fc7-9cd0-271853b5256a.png) = 0.690966*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Output[OL2] =![](img/56e40684-a2a9-4320-8c70-47b23ae15bfd.png) = 0.704664*'
  prefs: []
  type: TYPE_NORMAL
- en: Error computation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can now calculate the error for each output neuron using the square error
    function and sum them together to get the total error:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Etotal = *![](img/ed8a52d3-9fa1-4903-9c32-db4474463294.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '*EOL1 = Error at first node of output layer =*![](img/4fb5246b-2012-47dc-960a-b7f7ab5c16e9.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '*=0.021848*'
  prefs: []
  type: TYPE_NORMAL
- en: '*EOL2 = Error at second node of output layer =* ![](img/1598226c-3004-42d0-9f4e-afcc5b93b24c.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '*=0.182809*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Total Error = Etotal= EOL1 + EOL2 = 0.021848 + 0.182809 = 0.204657*'
  prefs: []
  type: TYPE_NORMAL
- en: Backward propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The purpose of backpropagation is to update each of the weights in the network
    so that they cause the actual output to be closer to the target output, thereby
    minimizing the error for each output neuron and the network as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: Let's focus on an output layer first. We are supposed to find out the impact
    of change in w5 on the total error.
  prefs: []
  type: TYPE_NORMAL
- en: This will be decided by ![](img/19ebbc28-eeac-4d1d-b685-1f7adeafdfa3.png). It
    is the partial derivative of Etotal with respect to w5.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s apply the chain rule here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b71d76e-07e9-49dc-bdaf-eda319eadd55.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/2f68c391-d1a5-482f-a663-ab7b1ce167d7.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/1143e171-c1c6-4fe7-9c8b-55bd3f9b062f.png)'
  prefs: []
  type: TYPE_IMG
- en: '*= 0.690966 – 0.9 = -0.209034*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d7f4f53f-c931-452b-8e67-9ea901f3c615.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/4ef3d0fd-7a77-4c98-b36d-e80053d71f84.png)'
  prefs: []
  type: TYPE_IMG
- en: '*= 0.213532*'
  prefs: []
  type: TYPE_NORMAL
- en: '*InputOL1 = w5*OutputHL1 + w7*OutputHL2 + B2*'
  prefs: []
  type: TYPE_NORMAL
- en: '*![](img/d594ec43-ddbf-45a9-883e-19c75ad8a624.png)= 0.650219*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s get back to the old equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/759a15cf-94d3-4f7f-9814-4590259b23bf.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/dd625504-e1a7-43f4-b88c-948fb580e10c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To update the weight, we will use the following formula. We have set the learning
    rate to be *α = 0.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/76b005cf-55a2-4935-b7d5-9fd8d59b8ac0.png)'
  prefs: []
  type: TYPE_IMG
- en: Similarly, ![](img/b27156ea-8276-45b1-92e3-e1a34a2d67bc.png) are supposed to
    be calculated. The approach remains the same. We will leave this to compute as
    it will help you in understanding the concepts better.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes down to the hidden layer and computing, the approach still remains
    the same. However, the formula will change a bit. I will help you with the formula,
    but the rest of the computation has to be done by you.
  prefs: []
  type: TYPE_NORMAL
- en: We will take *w1* here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s apply the chain rule here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c72c8107-c603-4256-98be-ec0d735c48ae.png)'
  prefs: []
  type: TYPE_IMG
- en: This formula has to be utilized for *w2*, *w3*, and *w4*. Please ensure that
    you are doing partial differentiation of *E_total* with respect to other weights
    and, in the end, that you are using the learning rate formula to get the updated
    weight.
  prefs: []
  type: TYPE_NORMAL
- en: Forward propagation equation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We know the equations around it. If the input for this is *a^([l-1])*, then
    the output will be *a^([l])*. However, there is a cache part, which is nothing
    but z*^([l])*, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bd54a557-21c3-42af-8627-540138cb1f1e.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, this breaks down into *w^([1])a^([l-1]) +b^([l])* (remember that *a^([0])* is
    equal to *X*).
  prefs: []
  type: TYPE_NORMAL
- en: Backward propagation equation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following equations would be required to execute backward propagation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae157002-f173-49f2-8834-e62dde566376.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/10786584-fc12-4a34-bc22-a86a3b48ce3e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'These equations will give you an idea of what is going on behind the scenes.
    Here, a suffix, *d*, has been added, which is a representation of the partial
    derivative that acts during backward propagation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/484cc2b3-97cd-4819-bf37-e8a1449f47ff.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/2b119630-edde-4f04-8387-58b8cf1dc790.png)'
  prefs: []
  type: TYPE_IMG
- en: Parameters and hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While we are getting on with building a deep learning model, you need to know
    how to keep a tab on both parameters and hyperparameters. But how well do we understand
    these?
  prefs: []
  type: TYPE_NORMAL
- en: When it comes down to parameters, we have got weights and biases. As we begin
    to train the network, one of the prime steps is to initialize the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Bias initialization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is a common practice to initialize the bias by zero as the symmetrical breaking
    of neurons is taken care of by the random weights' initialization.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hyperparameters are one of the building blocks of the deep learning network.
    It is an element that determines the optimal architecture of the network (for
    example, number of layers) and also a factor that is responsible for ensuring
    how the network will be trained.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the various hyperparameters of the deep learning network:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Learning rate**: This is responsible for determining the pace at which the
    network is trained. A slow learning rate ensures a smooth convergence, whereas
    a fast learning rate may not have smooth convergence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Epoch**: The number of epochs is the number of times the whole training data
    is consumed by the network while training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of hidden layers**: This determines the structure of the model, which
    helps in achieving the optimal capacity of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of nodes (neurons)**: There should be a trade-off between the number
    of nodes to be used. It decides whether all of the necessary information has been
    extracted to produce the required output. Overfitting or underfitting will be
    decided by the number of nodes. Hence, it is advisable to use it with regularization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dropout**: Dropout is a regularization technique that''s used to increase
    generalizing power by avoiding overfitting. This was discussed in detail in [Chapter
    4](e994d382-9a54-427c-86db-caf852e5c084.xhtml), *Training Neural Networks*. The
    dropout value can be between 0.2 and 0.5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Momentum**: This determines the direction of the next step toward convergence.
    With a value between 0.6 and 0.9, it handles oscillation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch size**: This is the number of samples that are fed into the network,
    after which a parameter update happens. Typically, it is taken as 32, 64, 128,
    256.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To find the optimal number of hyperparameters, it is prudent to deploy a grid
    search or random search.
  prefs: []
  type: TYPE_NORMAL
- en: Use case – digit recognizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Modified National Institute of Standards and Technology** (**MNIST**) is
    in fact the dataset of computer vision for *hello world*. Considering its release
    in 1999, this dataset has served as the main fundamental basis for benchmarking
    classification algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our goal is to correctly identify digits from a dataset of tens of thousands
    of handwritten images. We have curated a set of tutorial-style kernels that cover
    everything from regression to neural networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/80896871-a1bb-46e5-9c71-5e2b39b1abb9.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cc8d0828-a5b6-42d9-a851-660c411b9025.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0e8f70db-2c67-40c5-a9af-59a109e83736.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'By reshaping the image into 3 dimensions, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'By executing the following code, we will be able to see the numbered plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cf01e7a0-dc45-4863-8a0b-870e9f549a52.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The sequential model is now as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'When we define the optimizer, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'When we compile the model, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we generate the image generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output can be seen as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c892255-d1c4-47f1-b62d-ff8b95726881.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We predict the model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output can be seen as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5e3c6373-055f-4d8e-8465-d835d9774367.png)'
  prefs: []
  type: TYPE_IMG
- en: Generative adversarial networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Generative adversarial networks** (**GANs**) are another form of deep neural
    network architecture, and is a combination of two networks that compete and cooperate
    with each other. It was introduced by Ian Goodfellow and Yoshua Bengio in 2014.'
  prefs: []
  type: TYPE_NORMAL
- en: GANs can learn to mimic any distribution of data, which ideally means that GANs
    can be taught to create an object that's similar to an existing one in any domain,
    such as images, music, speech, and prose. It can create photos of any object that
    has never existed before. They are robot artists in a sense, and their output
    is impressive.
  prefs: []
  type: TYPE_NORMAL
- en: It falls under unsupervised learning wherein both of the networks learn their
    task upon training. One of the networks is called the **generator** and the other
    is called the **discriminator**.
  prefs: []
  type: TYPE_NORMAL
- en: To make this more understandable, we can think of a **GAN** as a case of a counterfeiter
    (generator) and a cop (discriminator). At the outset, the counterfeiter shows
    the cop fake money. The cop works like a detective and finds out that it is a
    fake money (you can think of D as a detective too if you want to understand how
    a discriminator works). The cop passes his feedback to the counterfeiter, explaining
    why the money is fake. The counterfeiter makes a few adjustments and makes new,
    fake money based on the feedback it received. The cop says that the money is still
    fake and he shares his new feedback with the counterfeiter. The counterfeiter
    then attempts to make new, fake money based on the latest feedback. The cycle
    continues indefinitely until the cop is fooled by the fake money because it looks
    real. When a GAN model is being created, the generator and discriminator start
    to learn from scratch and from each other. It may seem that they are pitted against
    each other, but they are helping each other learn. The feedback mechanism between
    these two is helping the model to be more robust.
  prefs: []
  type: TYPE_NORMAL
- en: 'A discriminator is quite a good learner, since it is capable of learning anything
    from the real world. That is, if you want it to learn about images of cats and
    dogs, and its 1,000 different categories where it''s asked to differentiate between
    the images, it will be able to do so without any hassle, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f4c7150-07a3-481e-a605-e43d9d713791.png)'
  prefs: []
  type: TYPE_IMG
- en: Noise goes into the generator; then, the output of the generator goes through
    the discriminator and we get an output. Simultaneously, the discriminator is being
    trained on the images of dogs. However, in the very beginning, even the dog images
    can be classified by the discriminator as being non-dog images, and it picks up
    on this error. This error gets back propagated through the network.
  prefs: []
  type: TYPE_NORMAL
- en: Hinton's Capsule network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Geoffrey Hinton, the father of deep learning, created a huge stir in the space
    of deep learning by introducing a new network. This network was called the **Capsule
    Network** (**CapsNet**). An algorithm to train this network was also brought forth,
    which is called **dynamic routing** **between capsules**. For the first time,
    Hinton spoke about it in 2011 in the paper called **transforming autoencoder**.
    In 2017 November, a full paper was published by Hinton and his team regarding
    the Capsule network.
  prefs: []
  type: TYPE_NORMAL
- en: The Capsule Network and convolutional neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **convolutional neural network** (**CNN**) has been one of the most important
    milestones in the area of deep learning. It has got everyone excited and has been
    the cornerstone for new research, too. But, as they say, *Nothing is perfect in
    this world*. Nor is our beloved CNN.
  prefs: []
  type: TYPE_NORMAL
- en: 'Can you recall how CNNs work? The most important job of a CNN is to execute
    convolution. What this means is that once you pass an image through CNN, the features,
    such as edges and color gradients, are extracted from image pixels by the convolution
    layer. Other layers will combine these features into a more complex one. And on
    top of it, once the dense layer is kept, it enables the network to carry out the
    classification job. The following diagram shows the image that we are working
    on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c0ef9182-2bff-467b-b8e4-7dc60d19db61.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding diagram is a basic CNN network, which is being used to detect
    the car in the image. The following diagram shows the image of a car that is in
    perfect order, and a fragmented image of the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d34d0009-4a1a-40ab-9935-52be22d7ee52.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's say we pass on these two images through the CNN network (to detect the
    car) – what will be the response of the network for both images? Can you ponder
    over this and come up with an answer? Just to help you, a car has got a number
    of components such as wheels, a windshield, a bonnet, and so on, but a car is
    deemed as a car to human eyes when all of these parts/components are set in order.
    However, for a CNN, only the features are important. A CNN doesn't take relative
    positional and orientational relationship into account. So, both of these images
    will be classified as a car by the network, even though this is not the case to
    the human eye.
  prefs: []
  type: TYPE_NORMAL
- en: To make amends, CNNs include max-pooling, which helps in increasing the view
    of a higher layer's neurons, thus making the detection of higher order features
    possible. Max-pooling makes CNNs work, but at the same time, information loss
    also takes place. It's a big drawback of CNN.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we studied deep neural networks and why we need a deep learning
    model. We also learned about forward and backward prorogation, along with parameters
    and hyperparameters. We also talked about GANs, along with deep Gaussian processes,
    the Capsule Network, and CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will study causal inference.
  prefs: []
  type: TYPE_NORMAL
