- en: Appendix B. Probability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Essential concepts in probability are presented here in brief.
  prefs: []
  type: TYPE_NORMAL
- en: Axioms of probability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kolmogorov''s axioms of probability can be stated in terms of the sample space
    *S* of possible events, *E*1, *E*2, *E*3, …*E*n and the real-valued probability
    *P(E)* of an event *E*. The axioms are:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(E) ≥ 0 for all E* *ϵ* *S*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*P(S) = 1*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Axioms of probability](img/B05137_10_image059.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_IMG
- en: Together, these axioms say that probabilities cannot be negative numbers—impossible
    events have zero probability—no events outside the sample space are possible as
    it is the universe of possibilities under consideration, and that the probability
    of either of two mutually exclusive events occurring is equal to the sum of their
    individual probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Bayes' theorem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The probability of an event **E** conditioned on evidence **X** is proportional
    to the prior probability of the event and the likelihood of the evidence given
    that the event has occurred. This is Bayes'' Theorem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayes'' theorem](img/B05137_10_image062.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*P(X)* is the normalizing constant, which is also called the marginal probability
    of *X*. *P(E)* is the prior, and *P(X|E)* is the likelihood. *P(E|X)* is also
    called the posterior probability.'
  prefs: []
  type: TYPE_NORMAL
- en: Bayes' Theorem expressed in terms of the posterior and prior odds is known as
    Bayes' Rule.
  prefs: []
  type: TYPE_NORMAL
- en: Density estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Estimating the hidden probability density function of a random variable from
    sample data randomly drawn from the population is known as density estimation.
    Gaussian mixtures and kernel density estimates are examples used in feature engineering,
    data modeling, and clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a probability density function *f(X)* for a random variable *X*, the
    probabilities associated with the values of *X* can be found as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Density estimation](img/B05137_10_image069.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Density estimation can be parametric, where it is assumed that the data is drawn
    from a known family of distributions and *f(x)* is estimated by estimating the
    parameters of the distribution, for example, µ and σ² in the case of a normal
    distribution. The other approach is non-parametric, where no assumption is made
    about the distribution of the observed data and the data is allowed to determine
    the form of the distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Mean
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The long-run average value of a random variable is known as the expectation
    or mean. The sample mean is the corresponding average over the observed data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of a discrete random variable, the mean is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Mean](img/B05137_10_image073.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For example, the mean number of pips turning up on rolling a single fair die
    is 3.5.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a continuous random variable with probability density function *f(x)*,
    the mean is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Mean](img/B05137_10_image075.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Variance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Variance is the expectation of the square of the difference between the random
    variable and its mean.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the discrete case, with the mean defined as previously discussed, and with
    the probability mass function *p(x)*, the variance is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Variance](img/B05137_10_image077.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the continuous case, it is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Variance](img/B05137_10_image078.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Some continuous distributions do not have a mean or variance.
  prefs: []
  type: TYPE_NORMAL
- en: Standard deviation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Standard deviation is a measure of how spread out the data is in relation to
    its mean value. It is the square root of variance, and unlike variance, it is
    expressed in the same units as the data. The standard deviation in the case of
    discrete and continuous random variables are given here:,
  prefs: []
  type: TYPE_NORMAL
- en: Discrete case:![Standard deviation](img/B05137_10_image079.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous case:![Standard deviation](img/B05137_10_image080.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaussian standard deviation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The standard deviation of a sample drawn randomly from a larger population is
    a biased estimate of the population standard deviation. Based on the particular
    distribution, the correction to this biased estimate can differ. For a Gaussian
    or normal distribution, the variance is adjusted by a value of ![Gaussian standard
    deviation](img/B05137_10_image081.jpg) .
  prefs: []
  type: TYPE_NORMAL
- en: 'Per the definition given earlier, the biased estimate *s* is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gaussian standard deviation](img/B05137_10_image083.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, ![Gaussian standard deviation](img/B05137_10_image084.jpg)
    is the sample mean.
  prefs: []
  type: TYPE_NORMAL
- en: 'The unbiased estimate, which uses Bessel''s correction, is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gaussian standard deviation](img/B05137_10_image085.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Covariance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In a joint distribution of two random variables, the expectation of the product
    of the deviations of the random variables from their respective means is called
    the covariance. Thus, for two random variables **X** and **Y**, the equation is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Covariance](img/B05137_10_image087.jpg)'
  prefs: []
  type: TYPE_IMG
- en: = *E[XY] – μ*x *μ*y
  prefs: []
  type: TYPE_NORMAL
- en: If the two random variables are independent, then their covariance is zero.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation coefficient
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When the covariance is normalized by the product of the standard deviations
    of the two random variables, we get the correlation coefficient *ρ*[X,Y], also
    known as the Pearson product-moment correlation coefficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Correlation coefficient](img/B05137_10_image091.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The correlation coefficient can take values between -1 and 1 only. A coefficient
    of +1 means a perfect increasing linear relationship between the random variables.
    -1 means a perfect decreasing linear relationship. If the two variables are independent
    of each other, the Pearson's coefficient is 0.
  prefs: []
  type: TYPE_NORMAL
- en: Binomial distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Discrete probability distribution with parameters **n** and **p**. A random
    variable is a binary variable, with the probability of outcome given by **p**
    and **1 – p** in a single trial. The probability mass function gives the probability
    of **k** successes out of **n** independent trials.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters: *n, k*'
  prefs: []
  type: TYPE_NORMAL
- en: 'PMF:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Binomial distribution](img/B05137_10_image097.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Binomial distribution](img/B05137_10_image098.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is the Binomial coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mean: E[*X*] = *np*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Variance: *Var(X)* = *np*(1 – *p*)'
  prefs: []
  type: TYPE_NORMAL
- en: Poisson distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Poisson distribution gives the probability of the number of occurrences
    of an event in a given time period or in a given region of space.
  prefs: []
  type: TYPE_NORMAL
- en: Parameter λ, is the average number of occurrences in a given interval. The probability
    mass function of observing *k* events in that interval is
  prefs: []
  type: TYPE_NORMAL
- en: 'PMF:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Poisson distribution](img/B05137_10_image102.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Mean: E[*X*] = λ'
  prefs: []
  type: TYPE_NORMAL
- en: 'Variance: *Var(X)* = λ'
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Gaussian distribution, also known as the normal distribution, is a continuous
    probability distribution. Its probability density function is expressed in terms
    of the mean and variance as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gaussian distribution](img/B05137_10_image105.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Mean: µ'
  prefs: []
  type: TYPE_NORMAL
- en: 'Standard deviation: σ'
  prefs: []
  type: TYPE_NORMAL
- en: 'Variance: σ2'
  prefs: []
  type: TYPE_NORMAL
- en: 'The standard normal distribution is the case when the mean is 0 and the standard
    deviation is 1\. The PDF of the standard normal distribution is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gaussian distribution](img/B05137_10_image109.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Central limit theorem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The central limit theorem says that when you have several independent and identically
    distributed random variables with a distribution that has a well-defined mean
    and variance, the average value (or sum) over a large number of these observations
    is approximately normally distributed, irrespective of the parent distribution.
    Furthermore, the limiting normal distribution has the same mean as the parent
    distribution and a variance equal to the underlying variance divided by the sample
    size.
  prefs: []
  type: TYPE_NORMAL
- en: Given a random sample *X*1, *X*2, *X*3 … *X*n with *µ* = *E*[*X*i] and *σ*2
    *= Var(X*i*)*, the sample mean:![Central limit theorem](img/B05137_10_image113.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: is approximately normal ![Central limit theorem](img/B05137_10_image114.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: There are several variants of the central limit theorem where independence or
    the constraint of being identically distributed are relaxed, yet convergence to
    the normal distribution still follows.
  prefs: []
  type: TYPE_NORMAL
- en: Error propagation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose there is a random variable *X*, which is a function of multiple observations
    each with their own distributions. What can be said about the mean and variance
    of *X* given the corresponding values for measured quantities that make up *X*?
    This is the problem of error propagation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Say *x* is the quantity to be determined via observations of variables *u*,
    *v*, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '*x = f(u, v, )*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us assume that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Error propagation](img/B05137_10_image118.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The uncertainty in *x* in terms of the variances of *u*, *v*, and so on, can
    be expressed by the variance of *x*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Error propagation](img/B05137_10_image119.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'From the Taylor expansion of the variance of *x*, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Error propagation](img/B05137_10_image120.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Error propagation](img/B05137_10_image500.jpg) is the covariance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, we can determine the propagation error of the mean. Given *N* measurements
    with *x*[i] with uncertainties characterized by *s*i, the following can be written:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Error propagation](img/B05137_10_image125.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'With:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Error propagation](img/B05137_10_image126.jpg)'
  prefs: []
  type: TYPE_IMG
- en: These equations assume that the covariance is 0\.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose *s*i *= s* – that is, all observations have the same error.
  prefs: []
  type: TYPE_NORMAL
- en: Then, ![Error propagation](img/B05137_10_image128.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: Since ![Error propagation](img/B05137_10_image129.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, ![Error propagation](img/B05137_10_image134.jpg).
  prefs: []
  type: TYPE_NORMAL
