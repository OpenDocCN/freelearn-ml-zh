- en: '7'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: '7'
- en: Deep Learning with Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用神经网络进行深度学习
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够：
- en: Perform basic TensorFlow operations to solve various expressions
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行基本的TensorFlow操作以解决各种表达式
- en: Describe how aritifical neural networks work
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述人工神经网络的工作原理
- en: Train and test neural networks with TensorFlow
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TensorFlow训练和测试神经网络
- en: Implement deep learning neural network models with TensorFlow
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TensorFlow实现深度学习神经网络模型
- en: In this chapter, we'll detect a written digit using the TensorFlow library.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用TensorFlow库检测手写数字。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简介
- en: 'In this chapter, we will learn about another supervised learning technique.
    However, this time, instead of using a simple mathematical model such as classification
    or regression, we will use a completely different model: **neural networks** .
    Although we will use Neural Networks for supervised learning, note that Neural
    Networks can also model unsupervised learning techniques. The significance of
    this model increased in the last century, because in the past, the computation
    power required to use this model for supervised learning was not enough. Therefore,
    neural networks have emerged in practice in the last century.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习另一种监督学习技术。然而，这次，我们不会使用像分类或回归这样的简单数学模型，而将使用一个完全不同的模型：**神经网络**。虽然我们将使用神经网络进行监督学习，但请注意，神经网络也可以模拟无监督学习技术。这个模型的重要性在上个世纪有所增加，因为在过去，使用这个模型进行监督学习所需的计算能力不足。因此，在上个世纪，神经网络在实践中应运而生。
- en: TensorFlow for Python
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow for Python
- en: TensorFlow is one of the most important machine learning and open source libraries
    maintained by Google. The TensorFlow API is available in many languages, including
    Python, JavaScript, Java, and C. As TensorFlow supports supervised learning, we
    will use TensorFlow for building a graph model, and then use this model for prediction.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow是Google维护的最重要的人工智能和开源库之一。TensorFlow API在许多语言中可用，包括Python、JavaScript、Java和C。由于TensorFlow支持监督学习，我们将使用TensorFlow构建图模型，然后使用此模型进行预测。
- en: 'TensorFlow works with tensors. Some examples for tensors are:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow与张量一起工作。张量的例子包括：
- en: Scalar values such as a floating point number.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标量值，例如浮点数。
- en: A vector of arbitrary length.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长度任意的向量。
- en: A regular matrix, containing p times q values, where p and q are finite integers.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含p乘q个值的常规矩阵，其中p和q是有限的整数。
- en: A p x q x r generalized matrix-like structure, where p, q, r are finite integers.
    Imagine this construct as a rectangular object in three dimensional space with
    sides p, q, and r. The numbers in this data structure can be visualized in three
    dimensions.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个p x q x r的广义矩阵结构，其中p、q、r是有限的整数。想象这个结构在三维空间中是一个具有边长p、q和r的长方体对象。这个数据结构中的数字可以在三维空间中可视化。
- en: Observing the above four data structures, more complex, n-dimensional data structures
    can also be valid examples for tensors.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察上述四种数据结构，更复杂、n维数据结构也可以是张量的有效示例。
- en: We will stick to scalar, vector, and regular matrix tensors in this chapter.
    Within the scope of this chapter, think of tensors as scalar values, or arrays,
    or arrays of arrays.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将坚持使用标量、向量和常规矩阵张量。在本章范围内，将张量视为标量值、数组或数组的数组。
- en: TensorFlow is used to create artificial neural networks because it models its
    inputs, outputs, internal nodes, and directed edges between these nodes. TensorFlow
    also comes with mathematical functions to transform signals. These mathematical
    functions will also come handy when modeling when a neuron inside a neural network
    gets activated.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow用于创建人工神经网络，因为它模拟了其输入、输出、内部节点以及这些节点之间的有向边。TensorFlow还附带数学函数来转换信号。当神经网络中的神经元被激活时，这些数学函数在建模时也会很有用。
- en: Note
  id: totrans-21
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Tensors are array-like objects. Flow symbolizes the manipulation of tensor data.
    So, essentially, TensorFlow is an array data manipulation library.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 张量是类似数组的对象。流符号表示对张量数据的操作。因此，本质上，TensorFlow是一个数组数据处理库。
- en: The main use case for TensorFlow is artificial neural networks, as this field
    requires operation on big arrays and matrices. TensorFlow comes with many deep
    learning-related functions, and so it is an optimal environment for neural networks.
    TensorFlow is used for voice recognition, voice search, and it is also the brain
    behind translate.google.com. Later in this chapter, we will use TensorFlow to
    recognize written characters.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow的主要用途是人工神经网络，因为这个领域需要对大数组和大矩阵进行操作。TensorFlow附带了许多与深度学习相关的函数，因此它是神经网络的最佳环境。TensorFlow用于语音识别、语音搜索，也是translate.google.com背后的大脑。在本章的后面部分，我们将使用TensorFlow来识别手写字符。
- en: Installing TensorFlow in the Anaconda Navigator
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Anaconda Navigator中安装TensorFlow
- en: 'Let''s open the Anaconda Prompt and install TensorFlow using `pip` :'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打开Anaconda Prompt，使用`pip`安装TensorFlow：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Installation will take a few minutes because the package itself is quite big.
    If you prefer using your video card GPU instead of your CPU, you can also use
    `tensorflow-gpu` . Make sure that you only use the GPU version if you have a good
    enough graphics card for it.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 安装需要几分钟，因为包本身相当大。如果你更喜欢使用你的显卡GPU而不是CPU，你也可以使用`tensorflow-gpu`。确保只有在你有一个足够好的显卡时才使用GPU版本。
- en: 'Once you are done with the installation, you can import TensorFlow in IPython:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，你可以在IPython中导入TensorFlow：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: First, we will use TensorFlow to build a graph. The execution of this model
    is separated. This separation is important because execution is resource intensive
    and may therefore run on a server specialized in solving computation heavy problems.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用TensorFlow构建一个图。这个模型的执行是分开的。这种分离很重要，因为执行是资源密集型的，因此可能需要在专门解决计算密集型问题的服务器上运行。
- en: TensorFlow Operations
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TensorFlow操作
- en: 'TensorFlow provides many operations to manipulate data. A few examples of these
    operations are as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow提供了许多操作来操作数据。以下是一些这些操作的例子：
- en: '**Arithmetic operations** : `add` and `multiply`'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**算术运算**：`加`和`乘`'
- en: '**Exponential operations** : `exp` and `log`'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指数运算**：`exp`和`log`'
- en: '**Relational operations** : `greater` , `less` , and `equal`'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关系运算**：`大于`、`小于`和`等于`'
- en: '**Array operations** : `concat` , `slice` , and `split`'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数组操作**：`concat`、`slice`和`split`'
- en: '**Matrix operations** : `matrix_inverse` , `matrix_determinant` , and `matmul`'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**矩阵运算**：`matrix_inverse`、`matrix_determinant`和`matmul`'
- en: '**Neural network-related operations** : `sigmoid` , `ReLU` , and `softmax`'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经网络相关操作**：`sigmoid`、`ReLU`和`softmax`'
- en: 'Exercise 22: Using Basic Operations and TensorFlow constants'
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习22：使用基本操作和TensorFlow常量
- en: 'Use arithmetic operations in Tensorflow to solve the expression: *2 * 3 + 4*'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TensorFlow中的算术运算来解决表达式：*2 * 3 + 4*
- en: 'These operations can be used to build a graph. To understand more about TensorFlow
    constants and basic arithmetic operators, let''s consider a simple expression
    *2 * 3 + 4* the graph for this expression would be as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作可以用来构建图形。为了更深入地了解TensorFlow常量和基本算术运算符，让我们考虑一个简单的表达式*2 * 3 + 4*，这个表达式的图形如下：
- en: '![Figure 7.1 Graph of the expression 2*3+4](img/Image00062.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图7.1 表达式2*3+4的图形](img/Image00062.jpg)'
- en: 'Figure 7.1: Graph of the expression 2*3+4'
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.1：表达式2*3+4的图形
- en: 'Model this graph in TensorFlow by using the following code:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码在TensorFlow中模拟此图形：
- en: '[PRE2]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Once the graph is built, to perform calculations, we have to open a TensorFlow
    session and execute our nodes:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图形构建完成后，为了进行计算，我们必须打开一个TensorFlow会话并执行我们的节点：
- en: '[PRE3]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The intermediate and final results are printed to the console:'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 中间结果和最终结果将打印到控制台：
- en: '[PRE4]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Placeholders and Variables
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 占位符和变量
- en: Now that you can build expressions with TensorFlow, let's take things a step
    further and build placeholders and variables.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以使用TensorFlow构建表达式了，让我们更进一步，构建占位符和变量。
- en: Placeholders are substituted with a constant value when the execution of a session
    starts. Placeholders are essentially parameters that are substituted before solving
    an expression. Variables are values that might change during the execution of
    a session.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当会话开始执行时，占位符会被替换为一个常量值。占位符本质上是在解决表达式之前被替换的参数。变量是在会话执行过程中可能发生变化的值。
- en: 'Let''s create a parametrized expression with TensorFlow:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用TensorFlow创建一个参数化表达式：
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The output is `6.0` .
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是`6.0`。
- en: The `tf.global_variables_initializer()` call initialized the variable in `input3`
    to its default value, zero, after it was executed in `session.run` .
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.global_variables_initializer()`调用在`session.run`执行后，将`input3`中的变量初始化为其默认值，即零。'
- en: The sum was calculated inside another `session.run` statement by using the feed
    dictionary, thus using the constant `3.0` in place of the `input2` parameter.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用feed字典在另一个`session.run`语句中计算了总和，因此用常数`3.0`代替了`input2`参数。
- en: Note that in this specific example, the variable x is initialized to zero. The
    value of x does not change during the execution of the TensorFlow session. Later,
    when we will use TensorFlow to describe neural networks, we will define an optimization
    target, and the session will optimize the values of the variables to meet this
    target.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这个特定示例中，变量x被初始化为零。在TensorFlow会话执行期间，x的值不会改变。稍后，当我们使用TensorFlow来描述神经网络时，我们将定义一个优化目标，并且会话将优化变量的值以满足这个目标。
- en: Global Variables Initializer
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 全局变量初始化器
- en: As TensorFlow often makes use of matrix operations, it makes sense to learn
    how to initialize a matrix of random variables to a value that's randomly generated
    according to a normal distribution centered at zero.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 由于TensorFlow经常使用矩阵运算，因此学习如何初始化一个随机变量的矩阵到一个以零为中心的正态分布随机生成的值是有意义的。
- en: 'Not only matrices, but all global variables are initialized inside the session
    by calling `tf.global_variables_initializer()` :'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅矩阵，所有全局变量都是在会话内部通过调用`tf.global_variables_initializer()`来初始化的：
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As you can see, the initialization of a `tf.Variable` takes one argument: the
    value of `tf.random_normal([3,4])` .'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，`tf.Variable`的初始化需要一个参数：`tf.random_normal([3,4])`的值。
- en: Introduction to Neural Networks
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络简介
- en: Neural networks are the newest branch of AI. Neural networks are inspired by
    how the human brain works. Originally, they were invented in the 1940s by Warren
    McCulloch and Walter Pitts. The neural network was a mathematical model that was
    used for describing how the human brain can solve problems.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是人工智能的最新分支。神经网络受到人类大脑工作方式的启发。最初，它们是在20世纪40年代由沃伦·麦卡洛克和沃尔特·皮茨发明的。神经网络是一个数学模型，用于描述人类大脑如何解决问题。
- en: We will use the phrase artificial neural network when talking about the mathematical
    model and use biological neural network when talking about the human brain. Artificial
    neural networks are supervised learning algorithms.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论数学模型时，我们将使用“人工神经网络”这个短语，当我们谈论人类大脑时，我们将使用“生物神经网络”。人工神经网络是监督学习算法。
- en: The way a neural network learns is more complex compared to other classification
    or regression models. The neural network model has a lot of internal variables,
    and the relationship between the input and output variables may go through multiple
    internal layers. Neural networks have higher accuracy as compared to other supervised
    learning algorithms.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的学习方式比其他分类或回归模型更复杂。神经网络模型有很多内部变量，输入变量和输出变量之间的关系可能要通过多个内部层。与其他监督学习算法相比，神经网络具有更高的准确性。
- en: Note
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Mastering neural networks with TensorFlow is a complex process. The purpose
    of this section is to provide you with an introductory resource to get started.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 掌握TensorFlow中的神经网络是一个复杂的过程。本节的目的就是为你提供一个入门资源，帮助你开始学习。
- en: In this chapter, the main example we are going to use is the recognition of
    digits from an image. We are considering this image since it is small, and we
    have around 70,000 images available. The processing power required to process
    these images, is similar to that of a regular computer.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用的主要示例是从图像中识别数字。我们考虑这个图像是因为它很小，我们大约有70,000张图像可用。处理这些图像所需的处理能力与普通计算机相似。
- en: Artificial neural network works similar to human brain works. Dendroid in a
    human brain is connected to the nucleus and the nucleus is connected to the axon.
    Here, the dendroid acts as the inputs, nucleus is where the calculations occurs
    (weighted sum and the activation function) and the axon acts similar to the output.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络的工作原理与人类大脑的工作原理相似。在人类大脑中，树突连接到细胞核，细胞核连接到轴突。在这里，树突充当输入，细胞核是计算发生的地方（加权总和和激活函数），轴突的作用类似于输出。
- en: 'Then, we determine which neuron fires by passing the weighted sum to an activation
    function. If this function determines that a neuron has to fire, the signal appears
    in the output. This signal can be the input of other neurons in the network:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过将加权总和传递给激活函数来确定哪个神经元会激发。如果这个函数确定一个神经元必须激发，信号就会出现在输出中。这个信号可以是网络中其他神经元的输入：
- en: '![Figure 7.2 Diagram showing how the artificial neural network works](img/Image00063.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图7.2 展示人工神经网络工作原理的图](img/Image00063.jpg)'
- en: 'Figure 7.2: Diagram showing how the artificial neural network works'
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.2：展示人工神经网络工作原理的图
- en: 'Suppose `f` is the activation function, `x1` , `x2` , `x3` , and `x4` are the
    inputs, and their sum is weighted with the weights `w1` , `w2` , `w3` , and `w4`
    :'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 假设`f`是激活函数，`x1`，`x2`，`x3`和`x4`是输入，它们的和与权重`w1`，`w2`，`w3`和`w4`相乘：
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Assuming vector `x` is (`x1` , `x2` , `x3` , `x4` ) and vector `w` is (`w1`
    , `w2` , `w3` , `w4` ), we can write this equation as the scalar or dot product
    of these two vectors:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 假设向量`x`是(`x1`，`x2`，`x3`，`x4`)，向量`w`是(`w1`，`w2`，`w3`，`w4`)，我们可以将这个方程写成这两个向量的标量或点积：
- en: y = f(x ⋅ w)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: y = f(x ⋅ w)
- en: 'The construct we have defined is one neuron:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义的结构是一个神经元：
- en: 'Let''s hide the details of this neuron so that it becomes easier to construct
    a neural network:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们隐藏这个神经元的细节，以便更容易构建神经网络：
- en: '![Figure 7.4 Diagram that represents the hidden layer of a neuron](img/Image00064.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图7.4 表示神经元隐藏层的图](img/Image00064.jpg)'
- en: 'Figure 7.4: Diagram that represents the hidden layer of a neuron'
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.4：表示神经元隐藏层的图
- en: We can create multiple boxes and multiple output variables that may get activated
    as a result of reading the weighted average of inputs.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建多个框和多个输出变量，这些变量可能由于读取输入的加权平均值而被激活。
- en: 'Although in the following diagram there are arrows leading from all inputs
    to all boxes, bear in mind that the weights on the arrows might be zero. We still
    display these arrows in the diagram:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在下面的图中，所有输入都指向所有框的箭头，但请记住，箭头上的权重可能为零。我们仍然在图中显示这些箭头：
- en: '![](img/Image00065.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Image00065.jpg)'
- en: 'Figure 7.5: Diagram representing a neural network'
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.5：表示神经网络的图
- en: The boxes describing the relationship between the inputs and the outputs are
    referred to as a hidden layer. A neural network with one hidden layer is called
    a **regular neural network** .
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 描述输入和输出之间关系的框被称为隐藏层。只有一个隐藏层的神经网络被称为**常规神经网络**。
- en: 'When connecting inputs and outputs, we may have multiple hidden layers. A neural
    network with multiple layers is called a **deep neural network** :'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在连接输入和输出时，我们可能有多个隐藏层。具有多个层的神经网络被称为**深度神经网络**：
- en: '![Figure 7.6 A diagram representing a deep neural network](img/Image00066.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图7.6 表示深度神经网络的图](img/Image00066.jpg)'
- en: 'Figure 7.6: A diagram representing a deep neural network'
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.6：表示深度神经网络的图
- en: The term deep learning comes from the presence of multiple layers. When creating
    an artificial neural network, we can specify the number of hidden layers.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习这个术语来源于多层结构的存在。在创建人工神经网络时，我们可以指定隐藏层的数量。
- en: Biases
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 偏置
- en: 'Let''s see the model of a neuron in a neural network again:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次看看神经网络中神经元的模型：
- en: '![Figure 7.7 Diagram of neuron in neural network](img/Image00067.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图7.7 神经网络中神经元的图](img/Image00067.jpg)'
- en: 'Figure 7.7: Diagram of neuron in neural network'
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.7：神经网络中神经元的图
- en: 'We learned that the equation of this neuron is as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到这个神经元的方程如下：
- en: '[PRE8]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The problem with this equation is that there is no constant factor that depends
    on the inputs x1, x2, x3, and x4\. This implies that each neuron in a neural network,
    without bias, always produces this value whenever for each weight-input pair,
    their product is zero.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程的问题是没有依赖于输入x1，x2，x3和x4的常数因子。这意味着神经网络中的每个神经元，如果没有偏置，总是会在每个权重-输入对的乘积为零时产生这个值。
- en: 'Therefore, we add bias to the equation:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们在方程中添加偏置：
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The first equation is the verbose form, describing the role of each coordinate,
    weight coefficient, and bias. The second equation is the vector form, where x
    = (x1, x2, x3, x4) and w = (w1, w2, w3, w4). The dot operator between the vectors
    symbolizes the dot or scalar product of the two vectors. The two equations are
    equivalent. We will use the second form in practice because it is easier to define
    a vector of variables using TensorFlow than to define each variable one by one.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个方程是详细形式，描述了每个坐标、权重系数和偏置的作用。第二个方程是向量形式，其中x = (x1, x2, x3, x4)和w = (w1, w2,
    w3, w4)。向量之间的点运算符表示两个向量的点积或标量积。这两个方程是等价的。我们将在实践中使用第二种形式，因为它比逐个定义每个变量更容易使用TensorFlow定义变量向量。
- en: Similarly, for *w1* , *w2* , *w3* , and *w4* , the bias *b* is a variable, meaning
    that its value can change during the learning process.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，对于*w1*，*w2*，*w3*和*w4*，偏置*b*是一个变量，意味着它的值可以在学习过程中改变。
- en: With this constant factor built into each neuron, the neural network model becomes
    more flexible from the purpose of fitting a specific training dataset better.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个神经元都内置了这个常数因子，神经网络模型在拟合特定训练数据集方面变得更加灵活。
- en: Note
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: It may happen that the product `p = x1*w1 + x2*w2 + x3*w3 + x4*w4` is negative
    due to the presence of a few negative weights. We may still want to give the model
    the flexibility to fire a neuron with values above a given negative number. Therefore,
    adding a constant bias b = 5, for instance, can ensure that the neuron fires for
    values between -5 and 0 as well.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会发生这样的情况，由于存在一些负权重，乘积 `p = x1*w1 + x2*w2 + x3*w3 + x4*w4` 是负的。我们可能仍然希望模型具有灵活性，以在超过给定负数的值上激活神经元。因此，添加一个常数偏置
    b = 5，例如，可以确保神经元在 -5 到 0 之间的值上也能激活。
- en: Use Cases for Artificial Neural Networks
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 人工神经网络的用例
- en: 'Artificial neural networks have their place among supervised learning techniques.
    They can model both classification and regression problems. A classifier neural
    network seeks a relationship between features and labels. The features are the
    input variables, while each class the classifier can choose as a return value
    is a separate output. In the case of regression, the input variables are the features,
    while there is one single output: the predicted value. While traditional classification
    and regression techniques have their use cases in artificial intelligence, artificial
    neural networks are generally better at finding complex relationships between
    the inputs and the outputs.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络在监督学习技术中占有一席之地。它们可以模拟分类和回归问题。分类神经网络寻求特征和标签之间的关系。特征是输入变量，而分类器可以选择作为返回值的每个类别是一个单独的输出。在回归的情况下，输入变量是特征，而有一个单一的输出：预测值。虽然传统的分类和回归技术在人工智能中有其用例，但人工神经网络通常在寻找输入和输出之间的复杂关系方面表现得更好。
- en: Activation Functions
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 激活函数
- en: Different activation functions are used in neural networks. Without these functions,
    the neural network would be a linear model that could be easily described using
    matrix multiplication.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中使用了不同的激活函数。没有这些函数，神经网络将是一个可以用矩阵乘法轻松描述的线性模型。
- en: Activation functions of the neural network provide non-linearity. The most common
    activation functions are `sigmoid` and `tanh` (the hyperbolic tangent function).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的激活函数提供了非线性。最常用的激活函数是 `sigmoid` 和 `tanh`（双曲正切函数）。
- en: 'The formula of `sigmoid` is as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`sigmoid` 的公式如下：'
- en: '[PRE10]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let''s plot this function using `pyplot` :'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 `pyplot` 绘制这个函数：
- en: '[PRE11]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output is as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.8 Graph displaying the sigmoid curve](img/Image00068.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.8 显示 sigmoid 曲线的图形](img/Image00068.jpg)'
- en: 'Figure 7.8: Graph displaying the sigmoid curve'
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.8：显示 sigmoid 曲线的图形
- en: There are a few problems with the sigmoid function.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 函数存在一些问题。
- en: First, it may disproportionally amplify or dampen weights.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，它可能会不成比例地放大或衰减权重。
- en: Second, `sigmoid(0)` is not zero. This makes the learning process harder.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，`sigmoid(0)` 不为零。这使得学习过程更加困难。
- en: 'The formula of the hyperbolic tangent is as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 双曲正切的公式如下：
- en: '[PRE12]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can also plot this function like so:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以这样绘制这个函数：
- en: '[PRE13]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output is as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.9 Graph after plotting hyperbolic tangent](img/Image00069.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.9 绘制双曲正切后的图形](img/Image00069.jpg)'
- en: 'Figure 7.9: Graph after plotting hyperbolic tangent'
  id: totrans-127
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.9：绘制双曲正切后的图形
- en: Both functions add a little non-linearity to the values emitted by a neuron.
    The sigmoid function looks a bit smoother, while the tanh function gives slightly
    more edgy results.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个函数都给神经元发出的值增加了一点点非线性。sigmoid 函数看起来更平滑，而 tanh 函数给出稍微尖锐的结果。
- en: 'Another activation function has become popular lately: `ReLU` . `ReLU` stands
    for Rectified Linear Unit:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 最近另一种激活函数也变得流行起来：`ReLU`。`ReLU` 代表修正线性单元：
- en: '[PRE14]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Making the neural network model non-linear makes it easier for the model to
    approximate non-linear functions. Without these non-linear functions, regardless
    of the number of layers of the network, we would only be able to approximate linear
    problems:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 使神经网络模型非线性化使得模型更容易逼近非线性函数。没有这些非线性函数，无论网络有多少层，我们都只能逼近线性问题：
- en: '[PRE15]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output is as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.10 Graph displaying the ReLU function](img/Image00070.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.10 显示 ReLU 函数的图形](img/Image00070.jpg)'
- en: 'Figure 7.10: Graph displaying the ReLU function'
  id: totrans-135
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.10：显示 ReLU 函数的图形
- en: The `ReLU` activation function behaves surprisingly well from the perspective
    of quickly converging to the final values of the weights and biases of the neural
    network.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 从快速收敛到神经网络权重和偏差的最终值的角度来看，`ReLU` 激活函数表现得非常出色。
- en: 'We will use one more function in this chapter: `softmax` .'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用一个额外的函数：`softmax`。
- en: 'The `softmax` function shrinks the values of a list between *0* and *1* so
    that the sum of the elements of the list becomes *1* . The definition of the `softmax`
    function is as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`softmax` 函数将列表中的值缩小到 *0* 和 *1* 之间，使得列表中所有元素的和变为 *1*。`softmax` 函数的定义如下：'
- en: '[PRE16]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here is an example:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个例子：
- en: '[PRE17]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output is as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE18]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The `softmax` function can be used whenever we filter a list, not a single value.
    Each element of the list will be transformed.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们过滤列表而不是单个值时，可以使用 `softmax` 函数。列表中的每个元素都将被转换。
- en: Let's experiment with different activator functions. Observe how these functions
    dampen the weighted inputs by solving the following exercise.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试不同的激活函数。观察这些函数如何通过解决以下练习来抑制加权的输入。
- en: 'Exercise 23: Activation Functions'
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 23：激活函数
- en: 'Consider the following neural network:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下神经网络：
- en: '*y = f( 2 * x1 + 0.5 * x2 + 1.5 * x3 - 3 ).*'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '*y = f( 2 * x1 + 0.5 * x2 + 1.5 * x3 - 3 ).*'
- en: 'Assuming *x1* is 1 and *x2* is 2, calculate the value of *y* for the following
    x values: -1, 0, 1, 2, when:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 *x1* 是 1，*x2* 是 2，计算以下 x 值（-1，0，1，2）对应的 *y* 值：
- en: f is the `sigmoid` function
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: f 是 `sigmoid` 函数
- en: f is the `tanh` function
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: f 是 `tanh` 函数
- en: f is the `ReLU` function
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: f 是 `ReLU` 函数
- en: 'Perform the following steps:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤：
- en: 'Substitute the known coefficients:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代入已知的系数：
- en: '[PRE19]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Use the following three activator functions:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下三个激活函数：
- en: '[PRE20]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Calculate the sigmoid values, using the following commands:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令计算 `sigmoid` 值：
- en: '[PRE21]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The output is `0.5`
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果为 `0.5`
- en: '[PRE22]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The output is `0.8175744761936437`
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果为 `0.8175744761936437`
- en: '[PRE23]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The output is `0.9525741268224331`
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果为 `0.9525741268224331`
- en: '[PRE24]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The output is `0.9890130573694068`
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果为 `0.9890130573694068`
- en: '[PRE25]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The output is `0.9975273768433653`
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果为 `0.9975273768433653`
- en: 'As you can see, the changes are dampened quickly as the sum of the expression
    inside the `sigmoid` function increases. We expect the `tanh` function to have
    an even bigger dampening effect:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如您所见，当 `sigmoid` 函数内部的求和表达式增加时，变化迅速被抑制。我们期望 `tanh` 函数具有更大的抑制效果：
- en: '[PRE26]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The output is `0.0`
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果为 `0.0`
- en: '[PRE27]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The output is `0.9051482536448663`
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果为 `0.9051482536448663`
- en: '[PRE28]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The output is `0.9950547536867307`
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果为 `0.9950547536867307`
- en: '[PRE29]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The output is `0.9997532108480274`
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果为 `0.9997532108480274`
- en: '[PRE30]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The output is `0.9999877116507956`
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果为 `0.9999877116507956`
- en: 'Based on the characteristics of the `tanh` function, the output approaches
    the 1 asymptote faster than the sigmoid function. For *x3 = -2* , we calculate
    `f(0)` . While `sigmoid(0)` is *0.5* , `tanh(0)` is *0* . As opposed to the other
    two functions, the `ReLu` function does not dampen positive values:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据函数 `tanh` 的特性，输出比 `sigmoid` 函数更快地接近 1 的渐近线。对于 *x3 = -2*，我们计算 `f(0)`。而 `sigmoid(0)`
    是 *0.5*，`tanh(0)` 是 *0*。与另外两个函数不同，`ReLu` 函数不会抑制正值：
- en: '[PRE31]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The output is `0.0`
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果为 `0.0`
- en: '[PRE32]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The output is `1.5`
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果为 `1.5`
- en: '[PRE33]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The output is `3.0`
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果为 `3.0`
- en: '[PRE34]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The output is `4.5`
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果为 `4.5`
- en: '[PRE35]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The output is `6.0`
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果为 `6.0`
- en: Another advantage of the `ReLU` function is that its calculation is the easiest
    out of all of the activator functions.
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`ReLU` 函数的另一个优点是，它的计算是所有激活函数中最简单的。'
- en: Forward and Backward Propagation
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 前向和反向传播
- en: 'As artificial neural networks provide a supervised-learning technique, we have
    to train our model using training data. Training the network is the process of
    finding the weights belonging to each variable-input pair. The process of weight
    optimization consists of the repeated execution of two steps: forward propagation
    and backward propagation.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 由于人工神经网络提供了一种监督学习技术，我们必须使用训练数据来训练我们的模型。训练网络的过程是找到属于每个变量输入对的权重。权重优化的过程包括重复执行两个步骤：前向传播和反向传播。
- en: The names forward and backward propagation imply how these techniques work.
    We start by initializing the weights on the arrows of the neural network. Then,
    we apply forward propagation, followed by backward propagation.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传播和反向传播这两个名称暗示了这些技术的工作方式。我们首先初始化神经网络箭头上的权重。然后，我们进行前向传播，接着进行反向传播。
- en: '**Forward propagation** calculates output values based on input values. **Backward
    propagation** adjusts the weights and biases based on the margin of error measured
    between the label values created by the model and the actual label values in the
    training data. The rate of adjustment of the weights depend on the learning rate
    of the neural network. The higher the learning rate, the more the weights and
    biases are adjusted during the backward propagation. The momentum of the neural
    network determines how past results influence the upcoming values of weights and
    biases.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '**前向传播**根据输入值计算输出值。**反向传播**根据模型创建的标签值与训练数据中实际标签值之间的误差范围调整权重和偏差。权重调整的速率取决于神经网络的
    学习率。学习率越高，反向传播期间权重和偏差的调整就越多。神经网络的动量决定了过去的结果如何影响权重和偏差的即将到来的值。'
- en: Configuring a Neural Network
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置神经网络
- en: 'The following parameters are commonly used to create a neural network:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 以下参数通常用于创建神经网络：
- en: Number of hidden layers
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层的数量
- en: Number of nodes per hidden layer
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个隐藏层的节点数
- en: Activation function
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数
- en: Learning rate
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率
- en: Momentum
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动量
- en: Number of iterations for forward and backward propagation
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前向和反向传播的迭代次数
- en: Tolerance for error
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错误容忍度
- en: There are a few rules of thumb that can be used to determine the number of nodes
    per hidden layer. If your hidden layer contains more nodes than the size of your
    input, you risk overfitting the model. Often, a node count somewhere between the
    number of inputs and the number of outputs is reasonable.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些经验法则可以用来确定每个隐藏层的节点数。如果你的隐藏层包含的节点数多于输入的大小，你可能会使模型过拟合。通常，节点数在输入和输出之间是合理的。
- en: Importing the TensorFlow Digit Dataset
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 导入TensorFlow数字数据集
- en: Recognition of hand-written digits seems to be a simple task at first glance.
    However, this task is a simple classification problem with ten possible label
    values. TensorFlow provides an example dataset for the recognition of digits.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 初看起来，识别手写数字似乎是一个简单的任务。然而，这个任务是一个具有十个可能标签值的简单分类问题。TensorFlow提供了一个用于识别数字的示例数据集。
- en: Note
  id: totrans-208
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can read about this dataset on TensorFlow''s website here: [https://www.tensorflow.org/tutorials/](https://www.tensorflow.org/tutorials/)
    .'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在TensorFlow网站上阅读有关此数据集的信息：[https://www.tensorflow.org/tutorials/](https://www.tensorflow.org/tutorials/)。
- en: 'We will use `keras` to load the dataset. You can install it in the Anaconda
    Prompt by using the following command:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`keras`来加载数据集。你可以在Anaconda Prompt中使用以下命令进行安装：
- en: '[PRE36]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Remember, we will perform supervised learning on these datasets, so we will
    need training and testing data:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们将对这些数据集进行监督学习，因此我们需要训练和测试数据：
- en: '[PRE37]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The features are arrays containing the pixel values of a 28x28 image. The labels
    are one-digit integers between 0 and 9\. Let''s see the features and the label
    of the fifth element. We will use the same image library that we used in the previous
    section:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 特征是包含28x28图像像素值的数组。标签是介于0到9之间的一位整数。让我们看看第五个元素的特性和标签。我们将使用与上一节相同的图像库：
- en: '[PRE38]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '![Fig 7.11 Image for training](img/Image00071.jpg)'
  id: totrans-216
  prefs:
  - PREF_H6
  type: TYPE_IMG
  zh: '![图7.11 训练图像](img/Image00071.jpg)'
- en: 'Fig 7.11: Image for training'
  id: totrans-217
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.11：训练图像
- en: '[PRE39]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: In the activity at the end of this chapter, your task will be to create a neural
    network to classify these handwritten digits based on their values.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章末尾的活动结束时，你的任务将是创建一个神经网络，根据这些手写数字的值进行分类。
- en: Modeling Features and Labels
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 建模特征和标签
- en: We will go through the example of modeling features and labels for recognizing
    written numbers in the TensorFlow digit dataset.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过TensorFlow数字数据集中识别手写数字的特征和标签建模示例来讲解。
- en: We have a 28x28 pixel image as our input. The value of each image is either
    black or white. The feature set therefore consists of a vector of 28 * 28 = 784
    pixels.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以一个28x28像素的图像作为输入。每个图像的值要么是黑色，要么是白色。因此，特征集由一个28 * 28 = 784像素的向量组成。
- en: 'The images are grayscale and consist of images with colors ranging from 0 to
    255\. To process them, we need to scale the data. By dividing the training and
    testing features by 255.0, we ensure that our features are scaled between 0 and
    1:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图像为灰度图，颜色从0到255不等。为了处理它们，我们需要对数据进行缩放。通过将训练和测试特征除以255.0，我们确保我们的特征缩放在0到1之间：
- en: '[PRE40]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Notice that we could have a 28x28 square matrix to describe the features, but
    we would rather flatten the matrix and simply use a vector. This is because the
    neural network model normally handles one-dimensional data.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们可以有一个28x28的方阵来描述特征，但我们更愿意将矩阵展平并简单地使用一个向量。这是因为神经网络模型通常处理一维数据。
- en: 'Regarding the modeling of labels, many people think that it makes the most
    sense to model this problem with just one label: an integer value ranging from
    0 to 9\. This approach is problematic, because small errors in the calculation
    may result in completely different digits. We can imagine that a 5 is similar
    to a 6, so the adjacent values work really well here. However, in the case of
    1 and 7, a small error may make the neural network realize a 1 as a 2, or a 7
    as a 6\. This is highly confusing, and it may take a lot more time to train the
    neural network to make less errors with adjacent values.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 关于标签的建模，许多人认为用单个标签建模这个问题最有意义：一个介于0到9之间的整数值。这种方法是有问题的，因为计算中的小错误可能会导致完全不同的数字。我们可以想象，5和6是相似的，所以相邻的值在这里工作得很好。然而，在1和7的情况下，一个小错误可能会让神经网络将1识别为2，或者将7识别为6。这非常令人困惑，并且可能需要更多的时间来训练神经网络以减少相邻值的错误。
- en: More importantly, when our neural network classifier comes back with a result
    of 4.2, we may have as much trouble interpreting the answer as the hero in *The
    Hitchhiker's Guide to the Galaxy.* 4.2 is most likely a 4\. But if not, maybe
    it is a 5, or a 3, or a 6\. This is not how digit detection works.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，当我们的神经网络分类器返回结果为4.2时，我们可能像《银河系漫游指南》中的英雄一样难以解释这个答案。4.2很可能是4。但如果不是，它可能是一个5，或者一个3，或者一个6。这不是数字检测的工作方式。
- en: Therefore, it makes more sense to model this task using a vector of ten labels.
    When using TensorFlow for classification, it makes perfect sense to create one
    label for each possible class, with values ranging between 0 and 1\. These numbers
    describe probabilities that the read digit is classified as a member of the class
    the label represents.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用十个标签的向量来模拟这个任务更有意义。当使用TensorFlow进行分类时，为每个可能的类别创建一个标签，其值介于0和1之间，这是完全合理的。这些数字描述了读取的数字被分类为标签所代表的类别的成员的概率。
- en: For instance, the value `[0, 0.1, 0, 0, 0.9, 0, 0, 0, 0, 0]` indicates that
    our digit has a 90% of being a 4, and a 10% chance of it being a 2.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，值 `[0, 0.1, 0, 0, 0.9, 0, 0, 0, 0, 0]` 表示我们的数字有90%的可能性是4，有10%的可能性是2。
- en: In case of classification problems, we always use one output value per class.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类问题中，我们总是为每个类别使用一个输出值。
- en: Let's continue with the weights and biases. To connect 28*28 = 784 features
    and 10 labels, we need a 784 x 10 matrix of weights that has 784 rows and 10 columns.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续讨论权重和偏差。为了连接28*28 = 784个特征和10个标签，我们需要一个784行10列的权重矩阵。
- en: Therefore, the equation becomes `y = f( x ⋅ W + b )` , where x is a vector in
    a 784-dimensional space, W is a 784 x 10 matrix, and b is a vector of biases in
    ten dimensions. The y vector also contains ten coordinates. The f function is
    defined on vectors with ten coordinates, and it is applied on each coordinate.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，方程变为 `y = f( x ⋅ W + b )` ，其中x是一个784维空间中的向量，W是一个784 x 10的矩阵，b是一个包含十个维度的偏差向量。y向量也包含十个坐标。f函数定义在具有十个坐标的向量上，并且应用于每个坐标。
- en: Note
  id: totrans-233
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To transform a two-dimensional 28x28 matrix of data points to a one-dimensional
    vector of 28x28 elements, we need to flatten the matrix. As opposed to many other
    languages and libraries, Python does not have a flatten method.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将二维28x28的数据点矩阵转换为28x28元素的单一维向量，我们需要展平矩阵。与许多其他语言和库不同，Python没有展平方法。
- en: 'Since flattening is an easy task, let''s construct a flatten method:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 由于展平是一个简单的任务，让我们构建一个展平方法：
- en: '[PRE41]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The output is as follows:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE42]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Let''s flatten the features from a 28*28 matrix to a vector of a 784-dimensional
    space:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将28*28矩阵的特征展平到784维空间的向量：
- en: '[PRE43]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'To transfer the labels to a vector form, we need to perform normalization:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将标签转换为向量形式，我们需要进行归一化：
- en: '[PRE44]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: TensorFlow Modeling for Multiple Labels
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TensorFlow多标签建模
- en: 'We will now model the following equation in TensorFlow: `y = f( x ⋅ W + b )`'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将在TensorFlow中建模以下方程：`y = f( x ⋅ W + b )`
- en: 'After importing TensorFlow, we will define the features, labels, and weights:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入TensorFlow后，我们将定义特征、标签和权重：
- en: '[PRE45]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: We can simply write the equation `y = f( x ⋅ W + b )` if we know how to perform
    dot product multiplication using TensorFlow.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们知道如何使用TensorFlow执行点积乘法，我们可以简单地写出方程 `y = f( x ⋅ W + b )`。
- en: If we treat x as a *1x84* matrix, we can multiply it with the *784x10* W matrix
    using the `tf.matmul` function.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将x视为一个*1x84*矩阵，我们可以使用`tf.matmul`函数将其与*784x10*的W矩阵相乘。
- en: 'Therefore, our equation becomes the following: `y = f( tf.add( tf.matmul( x,
    W ), b ) )`'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的方程变为以下：`y = f( tf.add( tf.matmul( x, W ), b ) )`
- en: You might have noticed that x contains placeholders, while W and b are variables.
    This is because the values of x are given. We just need to substitute them in
    the equation. The task of TensorFlow is to optimize the values of W and b so that
    we maximize the probability that we read the right digi ts.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到x包含占位符，而W和b是变量。这是因为x的值是已知的。我们只需要在方程中替换它们。TensorFlow的任务是优化W和b的值，以最大化我们读取正确数字的概率。
- en: 'Let''s express the calculation of y in a function form:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将y的计算表达为一个函数形式：
- en: '[PRE46]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Note
  id: totrans-253
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'This is the place where we can define the activator function. In the activity
    at the end of this chapter, you are better off using the softmax activator function.
    This implies that you will have to replace sigmoid with softmax in the code: `f
    = tf.nn.softmax`'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这是定义激活函数的地方。在本章末尾的活动结束时，你最好使用softmax激活函数。这意味着你将不得不在代码中将sigmoid替换为softmax：`f
    = tf.nn.softmax`
- en: Optimizing the Variables
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化变量
- en: Placeholders symbolize the input. The task of TensorFlow is to optimize the
    variables.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 占位符代表输入。TensorFlow的任务是优化变量。
- en: 'To perform optimization, we need to use a cost function: cross-entropy. Cross-entropy
    has the following properties:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行优化，我们需要使用一个成本函数：交叉熵。交叉熵具有以下特性：
- en: Its value is zero if the predicted output matches the real output
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果预测输出与实际输出匹配，其值为零
- en: Its value is strictly positive afterward
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其值在之后是严格正的
- en: 'Our task is to minimize cross-entropy:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的任务是最小化交叉熵：
- en: '[PRE47]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Although the function computing y is called classify, we do not perform the
    actual classification here. Remember, we are using placeholders in the place of
    x, and the actual values are substituted while running the TensorFlow session.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然计算y的函数被称为classify，但我们在这里并没有执行实际的分类。记住，我们正在使用占位符代替x，实际值在运行TensorFlow会话时被替换。
- en: The `sigmoid_cross_entropy_with_logits` function takes two arguments to compare
    their values. The first argument is the label value, while the second argument
    is the result of the prediction.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '`sigmoid_cross_entropy_with_logits`函数接受两个参数来比较它们的值。第一个参数是标签值，而第二个参数是预测结果。'
- en: 'To calculate the cost, we have to call the `reduce_mean` method of TensorFlow:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算成本，我们必须调用TensorFlow的`reduce_mean`方法：
- en: '[PRE48]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Minimization of the cost goes through an optimizer. We will use the `GradientDescentOptimizer`
    with a learning rate. The learning rate is a parameter of the Neural Network that
    influences how fast the model adjusts:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 成本最小化通过一个优化器进行。我们将使用带有学习率的`GradientDescentOptimizer`。学习率是影响模型调整速度的神经网络参数：
- en: '[PRE49]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Optimization is not performed at this stage, as we are not running TensorFlow
    yet. We will perform optimization in the main loop.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段不执行优化，因为我们还没有运行TensorFlow。我们将在主循环中执行优化。
- en: 'If you are using a different activator function such as softmax, you will have
    to replace it in the source code. Instead of the following statement:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是不同的激活函数，如softmax，你将不得不在源代码中替换它。而不是以下语句：
- en: '[PRE50]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Use the following:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下内容：
- en: '`cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(`'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '`cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(`'
- en: '`logits=y,`'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '`logits=y,`'
- en: '`labels=y_true`'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '`labels=y_true`'
- en: '`)`'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '`)`'
- en: Note
  id: totrans-276
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The _v2 suffix in the method name. This is because the original `tf.nn.softmax_cross_entropy_with_logits`
    method is deprecated.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 方法名称中的_v2后缀。这是因为原始的`tf.nn.softmax_cross_entropy_with_logits`方法已被弃用。
- en: Training the TensorFlow Model
  id: totrans-278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练TensorFlow模型
- en: 'We need to create a TensorFlow session and run the model:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要创建一个TensorFlow会话并运行模型：
- en: '[PRE51]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'First, we initialize the variables using `tf.global_variables_initializer()`
    :'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用`tf.global_variables_initializer()`初始化变量：
- en: '[PRE52]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Then comes the optimization loop. We will determine the number of iterations
    and a batch size. In each iteration, we will randomly select a number of feature-label
    pairs equal to the batch size.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是优化循环。我们将确定迭代次数和批量大小。在每次迭代中，我们将随机选择与批量大小相等的特征-标签对。
- en: For demonstration purposes, instead of creating random batches, we will simply
    feed the upcoming hundred images each time a new iteration is started.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示目的，我们不会创建随机批次，而是在每次新迭代开始时简单地提供即将到来的前一百张图像。
- en: 'As we have 60,000 images in total, we could have up to 300 iterations and 200
    images per iteration. In reality, we will only run a few iterations, which means
    that we will only use a fraction of the available training data:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们总共有60,000张图像，我们可能会有多达300次迭代和每次迭代200张图像。实际上，我们只会运行几次迭代，这意味着我们只会使用部分可用的训练数据：
- en: '[PRE53]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Using the Model for Prediction
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用模型进行预测
- en: 'We can now use the trained model to perform prediction. The syntax is straightforward:
    we feed the test features to the dictionary of the session, and request the `classify(x)`
    value:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用训练好的模型进行预测。语法很简单：我们将测试特征输入到会话的字典中，并请求`classify(x)`值：
- en: '[PRE54]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Testing the Model
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试模型
- en: 'Now that our model has been trained and we can use it for prediction, it is
    time to test its performance:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练了模型，并且可以使用它进行预测，是时候测试其性能了：
- en: '[PRE55]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: We have to transfer the `labelsPredicted` values back to integers ranging from
    0 to 9 by taking the index of the largest value from each result. We will use
    a NumPy function to perform this transformation.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须通过从每个结果中取最大值的索引，将`labelsPredicted`值转换回0到9的整数范围。我们将使用NumPy函数来完成这个转换。
- en: 'The `argmax` function returns the index of its list or array argument that
    has the maximum value. The following is an example of this:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '`argmax`函数返回其列表或数组参数中具有最大值的索引。以下是一个示例：'
- en: '[PRE56]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The output is `2` .
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是`2`。
- en: Here is the second example with `argmax` functions
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是带有`argmax`函数的第二个示例
- en: np.argmax( [1, 0, 1])
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: np.argmax([1, 0, 1])
- en: The output is `0` .
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是`0`。
- en: 'Let''s perform the transformation:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行转换：
- en: '[PRE57]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'We can use the metrics that we learned about in the previous chapters using
    scikit-learn. Let''s calculate the confusion matrix first:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用我们在前几章中学到的scikit-learn中的度量。让我们先计算混淆矩阵：
- en: '[PRE58]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Randomizing the Sample Size
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机化样本大小
- en: 'Recall the training function of the neural network:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 回忆神经网络的训练函数：
- en: '[PRE59]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: The problem is that out of 60,000 numbers, we can only take 5 iterations. If
    we want to go beyond this threshold, we would run the risk of repeating these
    input sequences.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，在60,000个数字中，我们只能进行5次迭代。如果我们想超过这个阈值，我们就会面临重复这些输入序列的风险。
- en: We can maximize the effectiveness of using the training data by randomly selecting
    the values out of the training data.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过随机选择训练数据中的值来最大化使用训练数据的有效性。
- en: 'We can use the `random.sample` method for this purpose:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`random.sample`方法来完成这个目的：
- en: '[PRE60]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Note
  id: totrans-311
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'The random sample method randomly selects a given number of elements out of
    a list. For instance, in Hungary, the main national lottery works based on selecting
    5 numbers out of a pool of 90\. We can simulate a lottery round using the following
    expression:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 随机样本方法从列表中随机选择给定数量的元素。例如，在匈牙利，主要的国家级彩票是基于从90个数字中选择5个数字。我们可以使用以下表达式来模拟一轮彩票：
- en: '[PRE61]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The output is as follows:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE62]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Activity 14: Written Digit Detection'
  id: totrans-316
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动14：手写数字检测
- en: In this se ction, we will discuss how to provide more security for cryptocurrency
    traders via the detection of hand-written digits. We will be using assuming that
    you are a software developer at a new cryptocurrency trader platform. The latest
    security measure you are implementing requires the recognition of hand-written
    digits. Use the MNIST library to train a neural network to recognize digits. You
    can read more about this dataset at [https://www.tensorflow.org/tutorials/](https://www.tensorflow.org/tutorials/)
    .
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何通过检测手写数字为加密货币交易者提供更多安全性。我们将假设你是一家新加密货币交易平台上的软件开发者。你正在实施的最新安全措施需要识别手写数字。使用MNIST库训练一个神经网络来识别数字。你可以在[https://www.tensorflow.org/tutorials/](https://www.tensorflow.org/tutorials/)了解更多关于这个数据集的信息。
- en: 'Improve the accuracy of the model as much as possible by performing the following
    steps:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行以下步骤尽可能提高模型的准确度：
- en: Load the dataset and format the input.
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据集并格式化输入。
- en: Set up the TensorFlow graph. Instead of the sigmoid function, we will now use
    the `ReLU` function.
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置TensorFlow图。现在我们将使用`ReLU`函数而不是sigmoid函数。
- en: Train the model.
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型。
- en: Test the model and calculate the accuracy score.
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试模型并计算准确度得分。
- en: By re-running the code segment that's responsible for training the dataset,
    we can improve its accuracy. Run the code 50 times.
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过重新运行负责训练数据集的代码段，我们可以提高其准确性。运行代码50次。
- en: Print the confusion matrix.
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印混淆矩阵。
- en: At the end of the fiftieth run, the confusion matrix has improved.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在第五十次运行结束时，混淆矩阵已经改进。
- en: Not a bad result. More than 8 out of 10 digits were accurately recognized.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个坏的结果。超过8个数字被准确识别。
- en: Note
  id: totrans-327
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 298.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这个活动的解决方案可以在第298页找到。
- en: As you can see, neural networks do not improve linearly. It may appear that
    training the network brings little to no incremental improvement in accuracy for
    a while. Yet, after a certain threshold, a breakthrough happens, and the accuracy
    greatly increases.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，神经网络并不呈线性提高。可能看起来训练网络在一段时间内对准确性的增量改进很小或没有。然而，在达到某个阈值后，会出现突破，准确度会大幅提高。
- en: This behavior is analogous with studying for humans. You might also have trouble
    with neural networks right now. However, after getting deeply immersed in the
    material and trying a few exercises out, you will reach breakthrough after breakthrough,
    and your progress will speed up.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 这种行为与人类学习相似。你可能现在也会在神经网络方面遇到困难。然而，在深入材料并尝试一些练习之后，你将实现一次又一次的突破，你的进步将加速。
- en: Deep Learning
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习
- en: In this topic, we will increase the number of layers of the neural network.
    You may remember that we can add hidden layers to our graph. We will target improving
    the accuracy of our model by experimenting with hidden layers.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个主题中，我们将增加神经网络的层数。你可能记得我们可以向我们的图中添加隐藏层。我们将通过实验隐藏层来提高我们模型的准确性。
- en: Adding Layers
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加层
- en: 'Recall the diagram of neural networks with two hidden layers:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下具有两个隐藏层的神经网络图：
- en: '![Figure 7.12 Diagram showing two hidden layers in a neural network](img/Image00072.jpg)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![图7.12 展示神经网络中两个隐藏层的图](img/Image00072.jpg)'
- en: 'Figure 7.12: Diagram showing two hidden layers in a neural network'
  id: totrans-336
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.12：展示神经网络中两个隐藏层的图
- en: We can add a second layer to the equation by duplicating the weights and biases
    and making sure that the dimensions of the TensorFlow variables match. Note that
    in the first model, we transformed 784 features into 10 labels.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过复制权重和偏差并确保TensorFlow变量的维度匹配来在等式中添加第二个层。注意，在第一个模型中，我们将784个特征转换成了10个标签。
- en: In this model, we will transform 784 features into a specified number of outputs.
    We will then take these outputs and transform them into 10 labels.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模型中，我们将把784个特征转换成指定数量的输出。然后我们将这些输出转换成10个标签。
- en: Determining the node count of the added hidden layer is not exactly science.
    We will use a count of 200 in this example, as it is somewhere between the feature
    and label dimensions.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 确定添加的隐藏层的节点数并不完全是科学。在这个例子中，我们将使用200个节点，因为它位于特征和标签维度之间。
- en: As we have two layers, we will define two matrices `(W1, W2)` and vectors `(b1,
    b2)` for the weights and biases, respectively.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有两个层，我们将定义两个矩阵`(W1, W2)`和向量`(b1, b2)`来分别表示权重和偏差。
- en: 'First, we reduce the 784 input dots using `W1` and `b1` , and create 200 variable
    values. We feed these values as the input of the second layer and use `W2` and
    `b2` to create 10 label values:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用`W1`和`b1`减少784个输入点，并创建200个变量值。我们将这些值作为第二层的输入，并使用`W2`和`b2`创建10个标签值：
- en: '[PRE63]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: We can increase the number of layers if needed in this way. The output of layer
    n must be the input of layer n+1\. The rest of the code remains as it is.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，我们可以这样增加层数。层n的输出必须是层n+1的输入。其余的代码保持不变。
- en: Convolutional Neural Networks
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: '**Convolutional Neural Networks** (**CNNs** ) are artificial neural networks
    that are optimized for pattern recognition. CNNs are based on convolutional layers
    that are among the hidden layers of the deep neural network. A convolutional layer
    consists of neurons that transform their inputs using a convolution operation.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNNs**）是针对模式识别优化的人工神经网络。CNNs基于深度神经网络中的卷积层。卷积层由使用卷积操作转换其输入的神经元组成。'
- en: When using a convolution layer, we detect patterns in the image with an m*n
    matrix, where m and n are less than the width and the height of the image, respectively.
    When performing the convolution operation, we slide this m*n matrix over the image,
    matching every possibility. We calculate the scalar product of the m*n convolution
    filter and the pixel values of the 3x3 segment of the image our convolution filter
    is currently on. The convolution operation creates a new image from the original
    one, where the important aspects of our image are highlighted, and the less-important
    ones are blurred.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用卷积层时，我们使用一个m*n的矩阵在图像中检测模式，其中m和n分别小于图像的宽度和高度。在执行卷积操作时，我们将这个m*n矩阵在图像上滑动，匹配每一个可能性。我们计算m*n卷积滤波器与当前卷积滤波器所在的3x3图像像素值的标量积。卷积操作从原始图像创建一个新的图像，其中我们图像的重要方面被突出显示，不那么重要的方面则被模糊化。
- en: The convolution operation summarizes information on the window it is looking
    at. Therefore, it is an ideal operator for recognizing shapes in an image. Shapes
    can be anywhere on the image, and the convolution operator recognizes similar
    image information regardless of its exact position and orientation. Convolutional
    neural networks are outside the scope of this book, because it is a more advanced
    topic.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积操作总结了它所观察到的窗口上的信息。因此，它是一个识别图像中形状的理想操作符。形状可以出现在图像的任何位置，卷积操作符识别相似图像信息，无论其确切位置和方向如何。卷积神经网络超出了本书的范围，因为它是一个更高级的话题。
- en: 'Activity 15: Written Digit Detection with Deep Learning'
  id: totrans-348
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动十五：使用深度学习进行手写数字检测
- en: 'In this section, we will discuss how deep learning improves the performance
    of your model. We will be assuming that your boss is not satisfied with the results
    you presented in Activity 14 and has asked you to consider adding two hidden layers
    to your original model to determine whether new layers improve the accuracy of
    the model. To ensure that you are able to complete this activity correctly, you
    will need to be knowledgeable of deep learning:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论深度学习如何提高你模型的表现。我们假设你的老板对你提交的第14个活动的结果不满意，并要求你考虑在你的原始模型中添加两个隐藏层，以确定新层是否能提高模型的准确性。为了确保你能正确完成这个活动，你需要对深度学习有所了解：
- en: Execute the steps from the previous activity and measure the accuracy of the
    model.
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行上一个活动的步骤，并测量模型的准确率。
- en: Change the neural network by adding new layers. We will combine the `ReLU` and
    `softmax` activator functions.
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过添加新层来改变神经网络。我们将结合`ReLU`和`softmax`激活函数。
- en: Retrain the model.
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新训练模型。
- en: Evaluate the model. Find the accuracy score.
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型。找到准确率分数。
- en: Run the code 50 times.
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行代码50次。
- en: Print the confusion matrix.
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印混淆矩阵。
- en: This deep neural network behaves even more chaotically than the single layer
    one. It took 600 iterations of 200 samples to get from an accuracy of 0.572 to
    0.5723\. Not long after this iteration, we jumped from 0.6076 to 0.6834 in the
    same number of iterations.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 这个深度神经网络比单层神经网络表现得更加混沌。它经过600次迭代，每次迭代200个样本，从准确率0.572提升到0.5723。不久之后，在相同的迭代次数中，准确率从0.6076跃升至0.6834。
- en: Due to the flexibility of the deep neural network, we expect to reach an accuracy
    ceiling later than in the case of the simple model. Due to the complexity of a
    deep neural network, it is also more likely that it gets stuck at a local maximum
    for a long time.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 由于深度神经网络的灵活性，我们预计达到准确率上限的时间会比简单模型晚。由于深度神经网络的复杂性，它也更可能长时间陷入局部最优。
- en: Note
  id: totrans-358
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 302.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 这个活动的解决方案可以在第302页找到。
- en: Summary
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In this book, we have learned about the fundamentals of AI and applications
    of AI in chapter on principles of AI, then we wrote a Python code to model a Tic-Tac-Toe
    game.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们在“人工智能原理”章节学习了人工智能的基础知识以及人工智能的应用，然后我们编写了Python代码来模拟井字棋游戏。
- en: In the chapter AI with Search Techniques and Games, we solved the Tic-Tac-Toe
    game with game AI tools and search techniques. We learned about the search algorithms
    of Breadth First Search and Depth First Search. The A* algorithm helped students
    model a pathfinding problem. The chapter was concluded with modeling multiplayer
    games.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在“使用搜索技术和游戏的人工智能”这一章节中，我们使用游戏人工智能工具和搜索技术解决了井字棋游戏。我们学习了广度优先搜索和深度优先搜索的搜索算法。A*算法帮助学生建模路径查找问题。这一章节以建模多人游戏作为结束。
- en: 'In the next couple of chapters, we learned about supervised learning using
    regression and classification. These chapters included data preprocessing, train-test
    splitting, and models that were used in several real-life scenarios. Linear regression,
    polynomial regression, and Support Vector Machines all came in handy when it came
    to predicting stock data. Classification was performed using the k-nearest neighbor
    and Support Vector classifiers. Several activities helped students apply the basics
    of classification an interesting real-life use case: credit scoring.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几章中，我们学习了使用回归和分类进行监督学习。这些章节包括数据预处理、训练-测试分割以及在几个实际场景中使用的模型。当预测股票数据时，线性回归、多项式回归和支持向量机都非常有用。分类使用了k近邻和支持向量机分类器。几个活动帮助学生将分类的基本原理应用于一个有趣的现实生活用例：信用评分。
- en: In *Chapter 5* , *Using Trees for Predictive Analysis* , we were introduced
    to decision trees, random forests, and extremely randomized trees. This chapter
    introduced different means to evaluating the utility of models. We learned how
    to calculate the accuracy, precision, recall, and F1 Score of models. We also
    learned how to create the confusion matrix of a model. The models of this chapter
    were put into practice through the evaluation of car data.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第五章*，*使用树进行预测分析*中，我们介绍了决策树、随机森林和超随机树。本章介绍了评估模型效用性的不同方法。我们学习了如何计算模型的准确率、精确率、召回率和F1分数。我们还学习了如何创建模型的混淆矩阵。本章的模型通过评估汽车数据得到了实际应用。
- en: Unsupervised learning was introduced in *Chapter 6* , *Clustering* , along with
    the k-means and mean shift clustering algorithms. One interesting aspect of these
    algorithms is that the labels are not given in advance, but they are detected
    during the clustering process.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第六章*，*聚类*中介绍了无监督学习，以及k均值和均值漂移聚类算法。这些算法的一个有趣方面是，标签不是预先给出的，而是在聚类过程中检测到的。
- en: 'This book was concluded with Chapter 7, *Deep Learning with Neural Networks*
    , where neural networks and deep learning using TensorFlow was presented. We used
    these techniques on a real-life example: the detection of written digits.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 本书以第七章，*使用神经网络进行深度学习*结束，其中介绍了神经网络和TensorFlow的深度学习。我们使用这些技术在现实生活中的一个例子上应用了这些技术：手写数字的检测。
