- en: '7'
  prefs: []
  type: TYPE_NORMAL
- en: Deep Learning with Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Perform basic TensorFlow operations to solve various expressions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe how aritifical neural networks work
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train and test neural networks with TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement deep learning neural network models with TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we'll detect a written digit using the TensorFlow library.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about another supervised learning technique.
    However, this time, instead of using a simple mathematical model such as classification
    or regression, we will use a completely different model: **neural networks** .
    Although we will use Neural Networks for supervised learning, note that Neural
    Networks can also model unsupervised learning techniques. The significance of
    this model increased in the last century, because in the past, the computation
    power required to use this model for supervised learning was not enough. Therefore,
    neural networks have emerged in practice in the last century.'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow for Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TensorFlow is one of the most important machine learning and open source libraries
    maintained by Google. The TensorFlow API is available in many languages, including
    Python, JavaScript, Java, and C. As TensorFlow supports supervised learning, we
    will use TensorFlow for building a graph model, and then use this model for prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow works with tensors. Some examples for tensors are:'
  prefs: []
  type: TYPE_NORMAL
- en: Scalar values such as a floating point number.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A vector of arbitrary length.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A regular matrix, containing p times q values, where p and q are finite integers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A p x q x r generalized matrix-like structure, where p, q, r are finite integers.
    Imagine this construct as a rectangular object in three dimensional space with
    sides p, q, and r. The numbers in this data structure can be visualized in three
    dimensions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Observing the above four data structures, more complex, n-dimensional data structures
    can also be valid examples for tensors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will stick to scalar, vector, and regular matrix tensors in this chapter.
    Within the scope of this chapter, think of tensors as scalar values, or arrays,
    or arrays of arrays.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow is used to create artificial neural networks because it models its
    inputs, outputs, internal nodes, and directed edges between these nodes. TensorFlow
    also comes with mathematical functions to transform signals. These mathematical
    functions will also come handy when modeling when a neuron inside a neural network
    gets activated.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Tensors are array-like objects. Flow symbolizes the manipulation of tensor data.
    So, essentially, TensorFlow is an array data manipulation library.
  prefs: []
  type: TYPE_NORMAL
- en: The main use case for TensorFlow is artificial neural networks, as this field
    requires operation on big arrays and matrices. TensorFlow comes with many deep
    learning-related functions, and so it is an optimal environment for neural networks.
    TensorFlow is used for voice recognition, voice search, and it is also the brain
    behind translate.google.com. Later in this chapter, we will use TensorFlow to
    recognize written characters.
  prefs: []
  type: TYPE_NORMAL
- en: Installing TensorFlow in the Anaconda Navigator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s open the Anaconda Prompt and install TensorFlow using `pip` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Installation will take a few minutes because the package itself is quite big.
    If you prefer using your video card GPU instead of your CPU, you can also use
    `tensorflow-gpu` . Make sure that you only use the GPU version if you have a good
    enough graphics card for it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you are done with the installation, you can import TensorFlow in IPython:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: First, we will use TensorFlow to build a graph. The execution of this model
    is separated. This separation is important because execution is resource intensive
    and may therefore run on a server specialized in solving computation heavy problems.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Operations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TensorFlow provides many operations to manipulate data. A few examples of these
    operations are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Arithmetic operations** : `add` and `multiply`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exponential operations** : `exp` and `log`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Relational operations** : `greater` , `less` , and `equal`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Array operations** : `concat` , `slice` , and `split`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Matrix operations** : `matrix_inverse` , `matrix_determinant` , and `matmul`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Neural network-related operations** : `sigmoid` , `ReLU` , and `softmax`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Exercise 22: Using Basic Operations and TensorFlow constants'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Use arithmetic operations in Tensorflow to solve the expression: *2 * 3 + 4*'
  prefs: []
  type: TYPE_NORMAL
- en: 'These operations can be used to build a graph. To understand more about TensorFlow
    constants and basic arithmetic operators, let''s consider a simple expression
    *2 * 3 + 4* the graph for this expression would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 Graph of the expression 2*3+4](img/Image00062.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: Graph of the expression 2*3+4'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Model this graph in TensorFlow by using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the graph is built, to perform calculations, we have to open a TensorFlow
    session and execute our nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The intermediate and final results are printed to the console:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Placeholders and Variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that you can build expressions with TensorFlow, let's take things a step
    further and build placeholders and variables.
  prefs: []
  type: TYPE_NORMAL
- en: Placeholders are substituted with a constant value when the execution of a session
    starts. Placeholders are essentially parameters that are substituted before solving
    an expression. Variables are values that might change during the execution of
    a session.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a parametrized expression with TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The output is `6.0` .
  prefs: []
  type: TYPE_NORMAL
- en: The `tf.global_variables_initializer()` call initialized the variable in `input3`
    to its default value, zero, after it was executed in `session.run` .
  prefs: []
  type: TYPE_NORMAL
- en: The sum was calculated inside another `session.run` statement by using the feed
    dictionary, thus using the constant `3.0` in place of the `input2` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Note that in this specific example, the variable x is initialized to zero. The
    value of x does not change during the execution of the TensorFlow session. Later,
    when we will use TensorFlow to describe neural networks, we will define an optimization
    target, and the session will optimize the values of the variables to meet this
    target.
  prefs: []
  type: TYPE_NORMAL
- en: Global Variables Initializer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As TensorFlow often makes use of matrix operations, it makes sense to learn
    how to initialize a matrix of random variables to a value that's randomly generated
    according to a normal distribution centered at zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Not only matrices, but all global variables are initialized inside the session
    by calling `tf.global_variables_initializer()` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the initialization of a `tf.Variable` takes one argument: the
    value of `tf.random_normal([3,4])` .'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Neural networks are the newest branch of AI. Neural networks are inspired by
    how the human brain works. Originally, they were invented in the 1940s by Warren
    McCulloch and Walter Pitts. The neural network was a mathematical model that was
    used for describing how the human brain can solve problems.
  prefs: []
  type: TYPE_NORMAL
- en: We will use the phrase artificial neural network when talking about the mathematical
    model and use biological neural network when talking about the human brain. Artificial
    neural networks are supervised learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: The way a neural network learns is more complex compared to other classification
    or regression models. The neural network model has a lot of internal variables,
    and the relationship between the input and output variables may go through multiple
    internal layers. Neural networks have higher accuracy as compared to other supervised
    learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Mastering neural networks with TensorFlow is a complex process. The purpose
    of this section is to provide you with an introductory resource to get started.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, the main example we are going to use is the recognition of
    digits from an image. We are considering this image since it is small, and we
    have around 70,000 images available. The processing power required to process
    these images, is similar to that of a regular computer.
  prefs: []
  type: TYPE_NORMAL
- en: Artificial neural network works similar to human brain works. Dendroid in a
    human brain is connected to the nucleus and the nucleus is connected to the axon.
    Here, the dendroid acts as the inputs, nucleus is where the calculations occurs
    (weighted sum and the activation function) and the axon acts similar to the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we determine which neuron fires by passing the weighted sum to an activation
    function. If this function determines that a neuron has to fire, the signal appears
    in the output. This signal can be the input of other neurons in the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 Diagram showing how the artificial neural network works](img/Image00063.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.2: Diagram showing how the artificial neural network works'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Suppose `f` is the activation function, `x1` , `x2` , `x3` , and `x4` are the
    inputs, and their sum is weighted with the weights `w1` , `w2` , `w3` , and `w4`
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Assuming vector `x` is (`x1` , `x2` , `x3` , `x4` ) and vector `w` is (`w1`
    , `w2` , `w3` , `w4` ), we can write this equation as the scalar or dot product
    of these two vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: y = f(x ⋅ w)
  prefs: []
  type: TYPE_NORMAL
- en: 'The construct we have defined is one neuron:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s hide the details of this neuron so that it becomes easier to construct
    a neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 Diagram that represents the hidden layer of a neuron](img/Image00064.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.4: Diagram that represents the hidden layer of a neuron'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can create multiple boxes and multiple output variables that may get activated
    as a result of reading the weighted average of inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although in the following diagram there are arrows leading from all inputs
    to all boxes, bear in mind that the weights on the arrows might be zero. We still
    display these arrows in the diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00065.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.5: Diagram representing a neural network'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The boxes describing the relationship between the inputs and the outputs are
    referred to as a hidden layer. A neural network with one hidden layer is called
    a **regular neural network** .
  prefs: []
  type: TYPE_NORMAL
- en: 'When connecting inputs and outputs, we may have multiple hidden layers. A neural
    network with multiple layers is called a **deep neural network** :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 A diagram representing a deep neural network](img/Image00066.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.6: A diagram representing a deep neural network'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The term deep learning comes from the presence of multiple layers. When creating
    an artificial neural network, we can specify the number of hidden layers.
  prefs: []
  type: TYPE_NORMAL
- en: Biases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s see the model of a neuron in a neural network again:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 Diagram of neuron in neural network](img/Image00067.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.7: Diagram of neuron in neural network'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We learned that the equation of this neuron is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The problem with this equation is that there is no constant factor that depends
    on the inputs x1, x2, x3, and x4\. This implies that each neuron in a neural network,
    without bias, always produces this value whenever for each weight-input pair,
    their product is zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we add bias to the equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The first equation is the verbose form, describing the role of each coordinate,
    weight coefficient, and bias. The second equation is the vector form, where x
    = (x1, x2, x3, x4) and w = (w1, w2, w3, w4). The dot operator between the vectors
    symbolizes the dot or scalar product of the two vectors. The two equations are
    equivalent. We will use the second form in practice because it is easier to define
    a vector of variables using TensorFlow than to define each variable one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, for *w1* , *w2* , *w3* , and *w4* , the bias *b* is a variable, meaning
    that its value can change during the learning process.
  prefs: []
  type: TYPE_NORMAL
- en: With this constant factor built into each neuron, the neural network model becomes
    more flexible from the purpose of fitting a specific training dataset better.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It may happen that the product `p = x1*w1 + x2*w2 + x3*w3 + x4*w4` is negative
    due to the presence of a few negative weights. We may still want to give the model
    the flexibility to fire a neuron with values above a given negative number. Therefore,
    adding a constant bias b = 5, for instance, can ensure that the neuron fires for
    values between -5 and 0 as well.
  prefs: []
  type: TYPE_NORMAL
- en: Use Cases for Artificial Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Artificial neural networks have their place among supervised learning techniques.
    They can model both classification and regression problems. A classifier neural
    network seeks a relationship between features and labels. The features are the
    input variables, while each class the classifier can choose as a return value
    is a separate output. In the case of regression, the input variables are the features,
    while there is one single output: the predicted value. While traditional classification
    and regression techniques have their use cases in artificial intelligence, artificial
    neural networks are generally better at finding complex relationships between
    the inputs and the outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: Activation Functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Different activation functions are used in neural networks. Without these functions,
    the neural network would be a linear model that could be easily described using
    matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions of the neural network provide non-linearity. The most common
    activation functions are `sigmoid` and `tanh` (the hyperbolic tangent function).
  prefs: []
  type: TYPE_NORMAL
- en: 'The formula of `sigmoid` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s plot this function using `pyplot` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 Graph displaying the sigmoid curve](img/Image00068.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.8: Graph displaying the sigmoid curve'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are a few problems with the sigmoid function.
  prefs: []
  type: TYPE_NORMAL
- en: First, it may disproportionally amplify or dampen weights.
  prefs: []
  type: TYPE_NORMAL
- en: Second, `sigmoid(0)` is not zero. This makes the learning process harder.
  prefs: []
  type: TYPE_NORMAL
- en: 'The formula of the hyperbolic tangent is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also plot this function like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 Graph after plotting hyperbolic tangent](img/Image00069.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.9: Graph after plotting hyperbolic tangent'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Both functions add a little non-linearity to the values emitted by a neuron.
    The sigmoid function looks a bit smoother, while the tanh function gives slightly
    more edgy results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another activation function has become popular lately: `ReLU` . `ReLU` stands
    for Rectified Linear Unit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Making the neural network model non-linear makes it easier for the model to
    approximate non-linear functions. Without these non-linear functions, regardless
    of the number of layers of the network, we would only be able to approximate linear
    problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10 Graph displaying the ReLU function](img/Image00070.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.10: Graph displaying the ReLU function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `ReLU` activation function behaves surprisingly well from the perspective
    of quickly converging to the final values of the weights and biases of the neural
    network.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use one more function in this chapter: `softmax` .'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `softmax` function shrinks the values of a list between *0* and *1* so
    that the sum of the elements of the list becomes *1* . The definition of the `softmax`
    function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The `softmax` function can be used whenever we filter a list, not a single value.
    Each element of the list will be transformed.
  prefs: []
  type: TYPE_NORMAL
- en: Let's experiment with different activator functions. Observe how these functions
    dampen the weighted inputs by solving the following exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 23: Activation Functions'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Consider the following neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y = f( 2 * x1 + 0.5 * x2 + 1.5 * x3 - 3 ).*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming *x1* is 1 and *x2* is 2, calculate the value of *y* for the following
    x values: -1, 0, 1, 2, when:'
  prefs: []
  type: TYPE_NORMAL
- en: f is the `sigmoid` function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: f is the `tanh` function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: f is the `ReLU` function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Substitute the known coefficients:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the following three activator functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the sigmoid values, using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output is `0.5`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output is `0.8175744761936437`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output is `0.9525741268224331`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output is `0.9890130573694068`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output is `0.9975273768433653`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As you can see, the changes are dampened quickly as the sum of the expression
    inside the `sigmoid` function increases. We expect the `tanh` function to have
    an even bigger dampening effect:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output is `0.0`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output is `0.9051482536448663`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output is `0.9950547536867307`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output is `0.9997532108480274`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output is `0.9999877116507956`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Based on the characteristics of the `tanh` function, the output approaches
    the 1 asymptote faster than the sigmoid function. For *x3 = -2* , we calculate
    `f(0)` . While `sigmoid(0)` is *0.5* , `tanh(0)` is *0* . As opposed to the other
    two functions, the `ReLu` function does not dampen positive values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output is `0.0`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output is `1.5`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output is `3.0`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output is `4.5`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output is `6.0`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Another advantage of the `ReLU` function is that its calculation is the easiest
    out of all of the activator functions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Forward and Backward Propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As artificial neural networks provide a supervised-learning technique, we have
    to train our model using training data. Training the network is the process of
    finding the weights belonging to each variable-input pair. The process of weight
    optimization consists of the repeated execution of two steps: forward propagation
    and backward propagation.'
  prefs: []
  type: TYPE_NORMAL
- en: The names forward and backward propagation imply how these techniques work.
    We start by initializing the weights on the arrows of the neural network. Then,
    we apply forward propagation, followed by backward propagation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Forward propagation** calculates output values based on input values. **Backward
    propagation** adjusts the weights and biases based on the margin of error measured
    between the label values created by the model and the actual label values in the
    training data. The rate of adjustment of the weights depend on the learning rate
    of the neural network. The higher the learning rate, the more the weights and
    biases are adjusted during the backward propagation. The momentum of the neural
    network determines how past results influence the upcoming values of weights and
    biases.'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring a Neural Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following parameters are commonly used to create a neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of hidden layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of nodes per hidden layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Momentum
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of iterations for forward and backward propagation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tolerance for error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are a few rules of thumb that can be used to determine the number of nodes
    per hidden layer. If your hidden layer contains more nodes than the size of your
    input, you risk overfitting the model. Often, a node count somewhere between the
    number of inputs and the number of outputs is reasonable.
  prefs: []
  type: TYPE_NORMAL
- en: Importing the TensorFlow Digit Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recognition of hand-written digits seems to be a simple task at first glance.
    However, this task is a simple classification problem with ten possible label
    values. TensorFlow provides an example dataset for the recognition of digits.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can read about this dataset on TensorFlow''s website here: [https://www.tensorflow.org/tutorials/](https://www.tensorflow.org/tutorials/)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use `keras` to load the dataset. You can install it in the Anaconda
    Prompt by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember, we will perform supervised learning on these datasets, so we will
    need training and testing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The features are arrays containing the pixel values of a 28x28 image. The labels
    are one-digit integers between 0 and 9\. Let''s see the features and the label
    of the fifth element. We will use the same image library that we used in the previous
    section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '![Fig 7.11 Image for training](img/Image00071.jpg)'
  prefs:
  - PREF_H6
  type: TYPE_IMG
- en: 'Fig 7.11: Image for training'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: In the activity at the end of this chapter, your task will be to create a neural
    network to classify these handwritten digits based on their values.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling Features and Labels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will go through the example of modeling features and labels for recognizing
    written numbers in the TensorFlow digit dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We have a 28x28 pixel image as our input. The value of each image is either
    black or white. The feature set therefore consists of a vector of 28 * 28 = 784
    pixels.
  prefs: []
  type: TYPE_NORMAL
- en: 'The images are grayscale and consist of images with colors ranging from 0 to
    255\. To process them, we need to scale the data. By dividing the training and
    testing features by 255.0, we ensure that our features are scaled between 0 and
    1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we could have a 28x28 square matrix to describe the features, but
    we would rather flatten the matrix and simply use a vector. This is because the
    neural network model normally handles one-dimensional data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regarding the modeling of labels, many people think that it makes the most
    sense to model this problem with just one label: an integer value ranging from
    0 to 9\. This approach is problematic, because small errors in the calculation
    may result in completely different digits. We can imagine that a 5 is similar
    to a 6, so the adjacent values work really well here. However, in the case of
    1 and 7, a small error may make the neural network realize a 1 as a 2, or a 7
    as a 6\. This is highly confusing, and it may take a lot more time to train the
    neural network to make less errors with adjacent values.'
  prefs: []
  type: TYPE_NORMAL
- en: More importantly, when our neural network classifier comes back with a result
    of 4.2, we may have as much trouble interpreting the answer as the hero in *The
    Hitchhiker's Guide to the Galaxy.* 4.2 is most likely a 4\. But if not, maybe
    it is a 5, or a 3, or a 6\. This is not how digit detection works.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, it makes more sense to model this task using a vector of ten labels.
    When using TensorFlow for classification, it makes perfect sense to create one
    label for each possible class, with values ranging between 0 and 1\. These numbers
    describe probabilities that the read digit is classified as a member of the class
    the label represents.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, the value `[0, 0.1, 0, 0, 0.9, 0, 0, 0, 0, 0]` indicates that
    our digit has a 90% of being a 4, and a 10% chance of it being a 2.
  prefs: []
  type: TYPE_NORMAL
- en: In case of classification problems, we always use one output value per class.
  prefs: []
  type: TYPE_NORMAL
- en: Let's continue with the weights and biases. To connect 28*28 = 784 features
    and 10 labels, we need a 784 x 10 matrix of weights that has 784 rows and 10 columns.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the equation becomes `y = f( x ⋅ W + b )` , where x is a vector in
    a 784-dimensional space, W is a 784 x 10 matrix, and b is a vector of biases in
    ten dimensions. The y vector also contains ten coordinates. The f function is
    defined on vectors with ten coordinates, and it is applied on each coordinate.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To transform a two-dimensional 28x28 matrix of data points to a one-dimensional
    vector of 28x28 elements, we need to flatten the matrix. As opposed to many other
    languages and libraries, Python does not have a flatten method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since flattening is an easy task, let''s construct a flatten method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s flatten the features from a 28*28 matrix to a vector of a 784-dimensional
    space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'To transfer the labels to a vector form, we need to perform normalization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlow Modeling for Multiple Labels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will now model the following equation in TensorFlow: `y = f( x ⋅ W + b )`'
  prefs: []
  type: TYPE_NORMAL
- en: 'After importing TensorFlow, we will define the features, labels, and weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: We can simply write the equation `y = f( x ⋅ W + b )` if we know how to perform
    dot product multiplication using TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: If we treat x as a *1x84* matrix, we can multiply it with the *784x10* W matrix
    using the `tf.matmul` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, our equation becomes the following: `y = f( tf.add( tf.matmul( x,
    W ), b ) )`'
  prefs: []
  type: TYPE_NORMAL
- en: You might have noticed that x contains placeholders, while W and b are variables.
    This is because the values of x are given. We just need to substitute them in
    the equation. The task of TensorFlow is to optimize the values of W and b so that
    we maximize the probability that we read the right digi ts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s express the calculation of y in a function form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This is the place where we can define the activator function. In the activity
    at the end of this chapter, you are better off using the softmax activator function.
    This implies that you will have to replace sigmoid with softmax in the code: `f
    = tf.nn.softmax`'
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing the Variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Placeholders symbolize the input. The task of TensorFlow is to optimize the
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform optimization, we need to use a cost function: cross-entropy. Cross-entropy
    has the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: Its value is zero if the predicted output matches the real output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Its value is strictly positive afterward
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Our task is to minimize cross-entropy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Although the function computing y is called classify, we do not perform the
    actual classification here. Remember, we are using placeholders in the place of
    x, and the actual values are substituted while running the TensorFlow session.
  prefs: []
  type: TYPE_NORMAL
- en: The `sigmoid_cross_entropy_with_logits` function takes two arguments to compare
    their values. The first argument is the label value, while the second argument
    is the result of the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate the cost, we have to call the `reduce_mean` method of TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Minimization of the cost goes through an optimizer. We will use the `GradientDescentOptimizer`
    with a learning rate. The learning rate is a parameter of the Neural Network that
    influences how fast the model adjusts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Optimization is not performed at this stage, as we are not running TensorFlow
    yet. We will perform optimization in the main loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are using a different activator function such as softmax, you will have
    to replace it in the source code. Instead of the following statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(`'
  prefs: []
  type: TYPE_NORMAL
- en: '`logits=y,`'
  prefs: []
  type: TYPE_NORMAL
- en: '`labels=y_true`'
  prefs: []
  type: TYPE_NORMAL
- en: '`)`'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The _v2 suffix in the method name. This is because the original `tf.nn.softmax_cross_entropy_with_logits`
    method is deprecated.
  prefs: []
  type: TYPE_NORMAL
- en: Training the TensorFlow Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We need to create a TensorFlow session and run the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we initialize the variables using `tf.global_variables_initializer()`
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Then comes the optimization loop. We will determine the number of iterations
    and a batch size. In each iteration, we will randomly select a number of feature-label
    pairs equal to the batch size.
  prefs: []
  type: TYPE_NORMAL
- en: For demonstration purposes, instead of creating random batches, we will simply
    feed the upcoming hundred images each time a new iteration is started.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have 60,000 images in total, we could have up to 300 iterations and 200
    images per iteration. In reality, we will only run a few iterations, which means
    that we will only use a fraction of the available training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Using the Model for Prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can now use the trained model to perform prediction. The syntax is straightforward:
    we feed the test features to the dictionary of the session, and request the `classify(x)`
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Testing the Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that our model has been trained and we can use it for prediction, it is
    time to test its performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: We have to transfer the `labelsPredicted` values back to integers ranging from
    0 to 9 by taking the index of the largest value from each result. We will use
    a NumPy function to perform this transformation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `argmax` function returns the index of its list or array argument that
    has the maximum value. The following is an example of this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: The output is `2` .
  prefs: []
  type: TYPE_NORMAL
- en: Here is the second example with `argmax` functions
  prefs: []
  type: TYPE_NORMAL
- en: np.argmax( [1, 0, 1])
  prefs: []
  type: TYPE_NORMAL
- en: The output is `0` .
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s perform the transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the metrics that we learned about in the previous chapters using
    scikit-learn. Let''s calculate the confusion matrix first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Randomizing the Sample Size
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recall the training function of the neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: The problem is that out of 60,000 numbers, we can only take 5 iterations. If
    we want to go beyond this threshold, we would run the risk of repeating these
    input sequences.
  prefs: []
  type: TYPE_NORMAL
- en: We can maximize the effectiveness of using the training data by randomly selecting
    the values out of the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the `random.sample` method for this purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The random sample method randomly selects a given number of elements out of
    a list. For instance, in Hungary, the main national lottery works based on selecting
    5 numbers out of a pool of 90\. We can simulate a lottery round using the following
    expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Activity 14: Written Digit Detection'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this se ction, we will discuss how to provide more security for cryptocurrency
    traders via the detection of hand-written digits. We will be using assuming that
    you are a software developer at a new cryptocurrency trader platform. The latest
    security measure you are implementing requires the recognition of hand-written
    digits. Use the MNIST library to train a neural network to recognize digits. You
    can read more about this dataset at [https://www.tensorflow.org/tutorials/](https://www.tensorflow.org/tutorials/)
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'Improve the accuracy of the model as much as possible by performing the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the dataset and format the input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up the TensorFlow graph. Instead of the sigmoid function, we will now use
    the `ReLU` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Test the model and calculate the accuracy score.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By re-running the code segment that's responsible for training the dataset,
    we can improve its accuracy. Run the code 50 times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the confusion matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the end of the fiftieth run, the confusion matrix has improved.
  prefs: []
  type: TYPE_NORMAL
- en: Not a bad result. More than 8 out of 10 digits were accurately recognized.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 298.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, neural networks do not improve linearly. It may appear that
    training the network brings little to no incremental improvement in accuracy for
    a while. Yet, after a certain threshold, a breakthrough happens, and the accuracy
    greatly increases.
  prefs: []
  type: TYPE_NORMAL
- en: This behavior is analogous with studying for humans. You might also have trouble
    with neural networks right now. However, after getting deeply immersed in the
    material and trying a few exercises out, you will reach breakthrough after breakthrough,
    and your progress will speed up.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this topic, we will increase the number of layers of the neural network.
    You may remember that we can add hidden layers to our graph. We will target improving
    the accuracy of our model by experimenting with hidden layers.
  prefs: []
  type: TYPE_NORMAL
- en: Adding Layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recall the diagram of neural networks with two hidden layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.12 Diagram showing two hidden layers in a neural network](img/Image00072.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.12: Diagram showing two hidden layers in a neural network'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can add a second layer to the equation by duplicating the weights and biases
    and making sure that the dimensions of the TensorFlow variables match. Note that
    in the first model, we transformed 784 features into 10 labels.
  prefs: []
  type: TYPE_NORMAL
- en: In this model, we will transform 784 features into a specified number of outputs.
    We will then take these outputs and transform them into 10 labels.
  prefs: []
  type: TYPE_NORMAL
- en: Determining the node count of the added hidden layer is not exactly science.
    We will use a count of 200 in this example, as it is somewhere between the feature
    and label dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: As we have two layers, we will define two matrices `(W1, W2)` and vectors `(b1,
    b2)` for the weights and biases, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we reduce the 784 input dots using `W1` and `b1` , and create 200 variable
    values. We feed these values as the input of the second layer and use `W2` and
    `b2` to create 10 label values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: We can increase the number of layers if needed in this way. The output of layer
    n must be the input of layer n+1\. The rest of the code remains as it is.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Convolutional Neural Networks** (**CNNs** ) are artificial neural networks
    that are optimized for pattern recognition. CNNs are based on convolutional layers
    that are among the hidden layers of the deep neural network. A convolutional layer
    consists of neurons that transform their inputs using a convolution operation.'
  prefs: []
  type: TYPE_NORMAL
- en: When using a convolution layer, we detect patterns in the image with an m*n
    matrix, where m and n are less than the width and the height of the image, respectively.
    When performing the convolution operation, we slide this m*n matrix over the image,
    matching every possibility. We calculate the scalar product of the m*n convolution
    filter and the pixel values of the 3x3 segment of the image our convolution filter
    is currently on. The convolution operation creates a new image from the original
    one, where the important aspects of our image are highlighted, and the less-important
    ones are blurred.
  prefs: []
  type: TYPE_NORMAL
- en: The convolution operation summarizes information on the window it is looking
    at. Therefore, it is an ideal operator for recognizing shapes in an image. Shapes
    can be anywhere on the image, and the convolution operator recognizes similar
    image information regardless of its exact position and orientation. Convolutional
    neural networks are outside the scope of this book, because it is a more advanced
    topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 15: Written Digit Detection with Deep Learning'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will discuss how deep learning improves the performance
    of your model. We will be assuming that your boss is not satisfied with the results
    you presented in Activity 14 and has asked you to consider adding two hidden layers
    to your original model to determine whether new layers improve the accuracy of
    the model. To ensure that you are able to complete this activity correctly, you
    will need to be knowledgeable of deep learning:'
  prefs: []
  type: TYPE_NORMAL
- en: Execute the steps from the previous activity and measure the accuracy of the
    model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change the neural network by adding new layers. We will combine the `ReLU` and
    `softmax` activator functions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Retrain the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the model. Find the accuracy score.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the code 50 times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the confusion matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This deep neural network behaves even more chaotically than the single layer
    one. It took 600 iterations of 200 samples to get from an accuracy of 0.572 to
    0.5723\. Not long after this iteration, we jumped from 0.6076 to 0.6834 in the
    same number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the flexibility of the deep neural network, we expect to reach an accuracy
    ceiling later than in the case of the simple model. Due to the complexity of a
    deep neural network, it is also more likely that it gets stuck at a local maximum
    for a long time.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 302.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this book, we have learned about the fundamentals of AI and applications
    of AI in chapter on principles of AI, then we wrote a Python code to model a Tic-Tac-Toe
    game.
  prefs: []
  type: TYPE_NORMAL
- en: In the chapter AI with Search Techniques and Games, we solved the Tic-Tac-Toe
    game with game AI tools and search techniques. We learned about the search algorithms
    of Breadth First Search and Depth First Search. The A* algorithm helped students
    model a pathfinding problem. The chapter was concluded with modeling multiplayer
    games.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next couple of chapters, we learned about supervised learning using
    regression and classification. These chapters included data preprocessing, train-test
    splitting, and models that were used in several real-life scenarios. Linear regression,
    polynomial regression, and Support Vector Machines all came in handy when it came
    to predicting stock data. Classification was performed using the k-nearest neighbor
    and Support Vector classifiers. Several activities helped students apply the basics
    of classification an interesting real-life use case: credit scoring.'
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 5* , *Using Trees for Predictive Analysis* , we were introduced
    to decision trees, random forests, and extremely randomized trees. This chapter
    introduced different means to evaluating the utility of models. We learned how
    to calculate the accuracy, precision, recall, and F1 Score of models. We also
    learned how to create the confusion matrix of a model. The models of this chapter
    were put into practice through the evaluation of car data.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning was introduced in *Chapter 6* , *Clustering* , along with
    the k-means and mean shift clustering algorithms. One interesting aspect of these
    algorithms is that the labels are not given in advance, but they are detected
    during the clustering process.
  prefs: []
  type: TYPE_NORMAL
- en: 'This book was concluded with Chapter 7, *Deep Learning with Neural Networks*
    , where neural networks and deep learning using TensorFlow was presented. We used
    these techniques on a real-life example: the detection of written digits.'
  prefs: []
  type: TYPE_NORMAL
