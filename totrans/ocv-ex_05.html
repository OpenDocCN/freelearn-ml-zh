<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Automated Optical Inspection, Object Segmentation, and Detection</h1></div></div></div><p>In the previous chapter, we learned about histograms and filters that allowed us to understand image manipulation and create a photo application.</p><p>In this chapter, we will introduce you to the basic concepts of object segmentation and detection, which means isolation the objects that appear in an image for future processing and analysis.</p><p>In this chapter, we will cover the following topics:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Noise removal</li><li class="listitem" style="list-style-type: disc">The basics of light/background removal</li><li class="listitem" style="list-style-type: disc">The thresholding operation</li><li class="listitem" style="list-style-type: disc">A connected component for object segmentation</li><li class="listitem" style="list-style-type: disc">Finding contours for object segmentation</li></ul></div><p>The industry sector uses complex Computer Vision systems and hardware. Computer Vision tries to detect the problems and minimizes errors produced in the production process and increases the quality of final products.</p><p>In this sector, the name for Computer Vision tasks is <a id="id174" class="indexterm"/>
<strong>Automated Optical Inspection</strong> or AOI. This name appears in the inspection of printed circuit board manufacturers, where one or more cameras scan each circuit to detect critical failures and quality defects. This nomenclature was used by other manufacturers to use optical camera systems and Computer Vision algorithms to increase the product quality. Nowadays, the use of optical inspection using different camera types such as infrared, 3D cameras, and so on depends on the problem requirements, such as measure objects, detect surface effects, and so on; and complex algorithms are used in thousands of industries for different purposes, such as defects detection, recognition, classification, and so on.</p><div><div><div><div><h1 class="title"><a id="ch05lvl1sec38"/>Isolating objects in a scene</h1></div></div></div><p>In this section, we <a id="id175" class="indexterm"/>will introduce you to the first step of any AOI algorithm, that is, isolating different parts or objects in a scene.</p><p>We will take the<a id="id176" class="indexterm"/> example of object detection and classification of three object types: a screw, a packing ring, and a nut and develop these in this chapter and <a class="link" href="ch06.html" title="Chapter 6. Learning Object Classification">Chapter 6</a>, <em>Learning Object Classification</em>.</p><p>Let's say we are in a company that produces these three objects. All of them are in the same carrier tape, and our objective is to detect each object in the carrier tape and classify each one to allow a robot to put each object on the correct shelf:</p><div><img src="img/B04283_05_01.jpg" alt="Isolating objects in a scene"/></div><p>In this chapter, we will isolate each object and detect its position in the image in pixels. In the next chapter, we will classify each isolated object to check whether it is a nut, a screw, or a packing ring.</p><p>In the following<a id="id177" class="indexterm"/> image, we<a id="id178" class="indexterm"/> show our desired result where there are a few objects in the left-hand side image, and in the right-hand side image, we draw each one in different colors. We can show different features such as the area, height, width, countour size, and so on.</p><div><img src="img/B04283_05_02.jpg" alt="Isolating objects in a scene"/></div><p>To achieve this result, we will follow different steps that allow us to better understand and organize our algorithm, as shown in the following diagram:</p><div><img src="img/B04283_05_03.jpg" alt="Isolating objects in a scene"/></div><p>Our <a id="id179" class="indexterm"/>application<a id="id180" class="indexterm"/> is divided into two chapters. In this chapter, we will develop and understand the preprocessing and segmentation steps. In <a class="link" href="ch06.html" title="Chapter 6. Learning Object Classification">Chapter 6</a>, <em>Learning Object Classification,</em> we will extract the characteristics of each segmented object and train our machine learning system/algorithm to identify each object class to allow you to classify our objects.</p><p>Our preprocessing steps are divided into three more substeps, which are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Noise removal</li><li class="listitem" style="list-style-type: disc">Lighting removal</li><li class="listitem" style="list-style-type: disc">Binarization</li></ul></div><p>In the segmentation step, we will use two different algorithms, which are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The contour detection algorithm</li><li class="listitem" style="list-style-type: disc">The connected component extraction (labeling)</li></ul></div><p>We can see <a id="id181" class="indexterm"/>these substeps<a id="id182" class="indexterm"/> in the following diagram along with the application flow:</p><div><img src="img/B04283_05_04.jpg" alt="Isolating objects in a scene"/></div><p>Now, it's time to start the preprocessing step to get the best binarization image by removing the noise and lighting effects in order to minimize the possible detection errors.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch05lvl1sec39"/>Creating an application for AOI</h1></div></div></div><p>To create our <a id="id183" class="indexterm"/>new application, we require a few input parameters when the user executes them; all of them are optional, excluding the input image to be processed:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">An input image to be processed</li><li class="listitem" style="list-style-type: disc">The light image pattern</li><li class="listitem" style="list-style-type: disc">The light operation, where the user can choose between difference or division operations:<div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">If the input value of the user is set to <code class="literal">0</code>, then a difference operation is applied</li><li class="listitem" style="list-style-type: disc">If the input value of the user is set to <code class="literal">1</code>, then a division operation is applied</li></ul></div></li><li class="listitem" style="list-style-type: disc">Segmentation, where<a id="id184" class="indexterm"/> the user can choose between connected components with or without statistics and <code class="literal">findContours</code> methods:<div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">If the input value of the user is set to <code class="literal">1</code>, then the connected <code class="literal">components</code> method for the segment is applied</li><li class="listitem" style="list-style-type: disc">If the input value of the user is set to <code class="literal">2</code>, then the connected components with the statistics area is applied</li><li class="listitem" style="list-style-type: disc">If the input value of the user is set to <code class="literal">3</code>, then the <code class="literal">findContours</code> method is applied to the segmentation</li></ul></div></li></ul></div><p>To enable this user selection, we will use the command line <code class="literal">parser</code> class with these keys:</p><div><pre class="programlisting">// OpenCV command line parser functions
// Keys accecpted by command line parser
const char* keys =
{
   "{help h usage ? | | print this message}"
   "{@image || Image to process}"
   "{@lightPattern || Image light pattern to apply to image input}"
   "{lightMethod | 1 | Method to remove background light, 0 difference, 1 div }"
   "{segMethod | 1 | Method to segment: 1 connected Components, 2 connected components with stats, 3 find Contours }"
};</pre></div><p>We use the command line <code class="literal">parser</code> class that checks the parameters in the <code class="literal">main</code> function:</p><div><pre class="programlisting">int main( int argc, const char** argv )
{
  CommandLineParser parser(argc, argv, keys);
  parser.about("Chapter 5. PhotoTool v1.0.0");
  //If requires help show
  if (parser.has("help"))
  {
      parser.printMessage();
      return 0;
  }

  String img_file= parser.get&lt;String&gt;(0);
  String light_pattern_file= parser.get&lt;String&gt;(1);
  int method_light= parser.get&lt;int&gt;("lightMethod");
  int method_seg= parser.get&lt;int&gt;("segMethod");
  
  // Check if params are correctly parsed in his variables
  if (!parser.check())
  {
      parser.printErrors();
      return 0;
  }</pre></div><p>After <code class="literal">parser</code> class <a id="id185" class="indexterm"/>our command line user data, we check whether the input image is correctly loaded, and then we load the image and check whether it has data:</p><div><pre class="programlisting">// Load image to process
  Mat img= imread(img_file, 0);
  if(img.data==NULL){
    cout &lt;&lt; "Error loading image "&lt;&lt; img_file &lt;&lt; endl;
    return 0;
  }</pre></div><p>Now, we are ready to create our AOI process of segmentation. We will start with the preprocessing task.</p></div>
<div><div><div><div><h1 class="title"><a id="ch05lvl1sec40"/>Preprocessing the input image</h1></div></div></div><p>This <a id="id186" class="indexterm"/>section introduces you to some of the most common techniques that can be applied to preprocess images in the context of object segmentation/detection. The preprocess is the first change that we make in a new image before we start with our work and extract the information that we require from it.</p><p>Normally, in the preprocessing step, we try to minimize the image noise, light conditions, or image deformations due to the camera lens. These steps minimize the errors when you try to detect objects or segment our image.</p><div><div><div><div><h2 class="title"><a id="ch05lvl2sec33"/>Noise removal</h2></div></div></div><p>If we <a id="id187" class="indexterm"/>don't remove the noise, we can detect more objects than we expect because normally noise is represented as a small point in the image and can be segmented as an object. The sensor and scanner circuit normally produce this noise. This variation of brightness or color can be represented in different types, such as Gaussian noise, spike noise, and shot noise. There are different techniques that can be used to remove the noise. We will use a smooth operation, but depending of the type on the noise, we will use some that are better than others. For example, a median filter is normally used to remove the salt-pepper noise:</p><div><img src="img/B04283_05_05.jpg" alt="Noise removal"/></div><p>The left-hand side image is the original input with a salt-pepper noise. If we apply a median blur, we get an awesome result where we lose small details. For example, the borders of a screw for which we maintain the perfect edges. Refer to the top-right figure. If we apply a box filter or a Gaussian filter, the  noise if not removed. It is just smoothed and the details of objects are loosed and smoothed as well. Refer to the bottom-right figure.</p><p>OpenCV provides us with the <code class="literal">medianBlur</code> function that requires the following three parameters:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">An input image with a 1, 3, or 4 channel image. When the kernel size is greater than <code class="literal">5</code>, the image depth can only be <code class="literal">CV_8U</code>.</li><li class="listitem" style="list-style-type: disc">An output image,which is the resulting image, that has the same type and depth as that of the input.</li><li class="listitem" style="list-style-type: disc">The kernel size that has the aperture size greater than <code class="literal">1</code> and an odd value. For example, 3, 5, 7.</li></ul></div><p>This piece of<a id="id188" class="indexterm"/> code used to remove the noise looks like this:</p><div><pre class="programlisting">  Mat img_noise;
  medianBlur(img, img_noise, 3);</pre></div></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec34"/>Removing the background using the light pattern for segmentation</h2></div></div></div><p>In this <a id="id189" class="indexterm"/>section, we will develop a basic algorithm that enables us to remove the background using a light pattern. This preprocessing gives us better segmentation. Refer to the following figures. The top-left figure is the input image without noise, and the top-right figure is the result of applying a thresholding operation; we can see the top artifact. The bottom-left figure is the input image after the removal of the background, the bottom-right figure is the thresholding result where there are no artifacts in it and it's better to segment it.</p><div><img src="img/B04283_05_06.jpg" alt="Removing the background using the light pattern for segmentation"/></div><p>How can we remove<a id="id190" class="indexterm"/> the light from our image? It is very simple; we only need a picture of our scenario without any object that is taken from exactly the same position from where the other images have been taken, and to have the same light conditions. This is a very common technique in AOI because the external conditions are supervised and known. The image result of our case is similar to the following figure:</p><div><img src="img/B04283_05_07.jpg" alt="Removing the background using the light pattern for segmentation"/></div><p>Then, with a simple<a id="id191" class="indexterm"/> mathematical operation, we can remove this light pattern. There are two options to remove it, which are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Difference</li><li class="listitem" style="list-style-type: disc">Division</li></ul></div><p>The difference images are the simplest approach. If we have the light pattern <em>L</em> and the image picture <em>I,</em> the removal <em>R</em> result is the difference between them:</p><div><pre class="programlisting">R= L-I</pre></div><p>This division is a bit more complex but simple at the same time. If we have the light pattern matrix <em>L</em> and the image picture matrix <em>I,</em> the removal <em>R</em> result is as follows:</p><div><pre class="programlisting">R= 255*(1-(I/L))</pre></div><p>In this case, we divide the image by the light pattern. We make the assumption that if our light pattern is white and the objects are darker than the background carrier tape, then the image pixel values will always remain the same or will be lower than the light pixel values. Then, the result that we obtain from <em>I/L</em> is between <code class="literal">0</code> and <code class="literal">1</code>. Finally, we invert the result of this division to get the same color direction range and multiply it by 255 to get values between the <code class="literal">0-255</code> range.</p><p>In our code, we will create a new function called <code class="literal">removeLight</code> with the following parameters:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">An input image to remove the light/background</li><li class="listitem" style="list-style-type: disc">Light pattern mat</li><li class="listitem" style="list-style-type: disc">Method, <code class="literal">0</code> is difference, <code class="literal">1</code> division</li></ul></div><p>The output is a<a id="id192" class="indexterm"/> new image matrix without the light/background.</p><p>The following code implements the background removal using the light pattern:</p><div><pre class="programlisting">Mat removeLight(Mat img, Mat pattern, int method)
{
  Mat aux;
  // if method is normalization
  if(method==1)
  {
    // Require change our image to 32 float for division
    Mat img32, pattern32;
    img.convertTo(img32, CV_32F);
    pattern.convertTo(pattern32, CV_32F);
    // Divide the image by the pattern
    aux= 1-(img32/pattern32);
    // Scale it to convert to 8bit format
    aux=aux*255;
    // Convert 8 bits format
    aux.convertTo(aux, CV_8U);
  }else{
    aux= pattern-img;
  }
  return aux;
}</pre></div><p>Let's try to understand this. After creating the <code class="literal">aux</code> variable, in order to save the result, we select the method that is chosen by the user and passed via a parameter to the function. If the method selected is <code class="literal">1</code>, we apply the <code class="literal">division</code> method.</p><p>The <code class="literal">division</code> method requires a 32-bit float image to allow us to divide the images. The first step is to convert the image and light pattern mat to 32-bit depth:</p><div><pre class="programlisting">// Require change our image to 32 float for division
Mat img32, pattern32;
img.convertTo(img32, CV_32F);
pattern.convertTo(pattern32, CV_32F);</pre></div><p>Now we can<a id="id193" class="indexterm"/> perform the mathematical operations in our matrix, as described, dividing the image by the pattern and inverting the result:</p><div><pre class="programlisting">// Divide the image by the pattern
aux= 1-(img32/pattern32);
// Scale it to convert o 8bit format
aux=aux*255;</pre></div><p>Now, we have the result, but we need to return an 8-bit depth image, and then, use the <code class="literal">convert</code> function, as we did previously, to convert it to a 32-bit float:</p><div><pre class="programlisting">// Convert 8 bits format
aux.convertTo(aux, CV_8U);</pre></div><p>Now we can return the <code class="literal">aux</code> variable with the result. For the difference method, the development is very easy because we don't have to convert our images, we only need to perform the difference and return. If we don't assume that the pattern is equal to or greater than the image, then we will require a few checks and truncate values that can be less than <code class="literal">0</code> or greater than <code class="literal">255</code>:</p><div><pre class="programlisting">aux= pattern-img;</pre></div><p>The following figure is the result of applying the image light pattern to our input image:</p><div><img src="img/B04283_05_08.jpg" alt="Removing the background using the light pattern for segmentation"/></div><p>In the results that we obtain, we can check how the light gradient is removed and the possible artifacts are removed as well.</p><p>However, what happens<a id="id194" class="indexterm"/> when we don't have a light/background pattern? There are a few different techniques to do this, and we are going to present the most basic one. Using a filter, we can create one that can be used, but there are better algorithms from which you can learn the background from a few images, where the pieces appear in different areas. This technique sometimes requires a background estimation image initialization, where our basic approach can play very well. These advanced techniques are explored in the video surveillance chapter.</p><p>To estimate the background image, we will use a blur with a large kernel size that is applied to our input image. This is a common technique used in OCR where the letters are thin and small relative to the whole document, and allows us to perform an approximation of the light patterns in the image. We can see the light/background pattern reconstruction on the left-hand side figure and the ground truth on the right-hand side figure:</p><div><img src="img/B04283_05_09.jpg" alt="Removing the background using the light pattern for segmentation"/></div><p>We can see that there are minor differences in the light patterns, but this result is enough to remove the background, and we can see the result in the following figure using difference images.</p><p>In the following figure, we can see the result of applying the image difference between the original input image<a id="id195" class="indexterm"/> and the estimated background image that are computed with the previous approach:</p><div><img src="img/B04283_05_10.jpg" alt="Removing the background using the light pattern for segmentation"/></div><p>The <code class="literal">calculateLightPattern</code> function creates this light pattern or background approximation:</p><div><pre class="programlisting">Mat calculateLightPattern(Mat img)
{
  Mat pattern;
  // Basic and effective way to calculate the light pattern from one image
  blur(img, pattern, Size(img.cols/3,img.cols/3));
  return pattern;
}</pre></div><p>This <code class="literal">basic</code> function applies a blur to an input image using a big kernel size relative to the image size. From the code, it is one-third of the original width and height.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec35"/>The thresholding operation</h2></div></div></div><p>After<a id="id196" class="indexterm"/> removing the background, we only have to binarize the image for future segmentation. Now, we will apply the <code class="literal">threshold</code> function using two different threshold values: a very low value when we remove the light/background because all non-interest regions are black or very low values, and a medium value <a id="id197" class="indexterm"/>when we do not use a light removal method because we have a white background and the object images have lower values. This last option allows us to check the results with and without the background removal:</p><div><pre class="programlisting">  // Binarize image for segment
  Mat img_thr;
  if(method_light!=2){
    threshold(img_no_light, img_thr, 30, 255, THRESH_BINARY);
  }else{
    threshold(img_no_light, img_thr, 140, 255, THRESH_BINARY_INV);
  }</pre></div><p>Now, we will continue with the most important part of our application: the segmentation. We will use two different approaches or algorithms: connected components and contours.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch05lvl1sec41"/>Segmenting our input image</h1></div></div></div><p>Now, we<a id="id198" class="indexterm"/> will introduce you to the following two techniques used to segment our thresholded image:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The connected components</li><li class="listitem" style="list-style-type: disc">The <code class="literal">findContours</code> function</li></ul></div><p>With these two techniques, we will be allowed to extract each region of interest of our image where our target objects appear; in our case, a nut, screw, and ring.</p><div><div><div><div><h2 class="title"><a id="ch05lvl2sec36"/>The connected component algorithm</h2></div></div></div><p>The <a id="id199" class="indexterm"/>connected component is a <a id="id200" class="indexterm"/>very common algorithm used to segment and identify parts in binary images. A connected component is an iterative algorithm used for the purpose of labeling an image using an 8- or 4-connectivity pixel. Two pixels are connected if they have the same value and are neighbors. In the following figure, each pixel has eight neighbor pixels:</p><div><img src="img/B04283_05_11.jpg" alt="The connected component algorithm"/></div><p>A 4-connectivity <a id="id201" class="indexterm"/>means that only the 2, 4, 5, and 7 neighbors can be connected to the center if they have the <a id="id202" class="indexterm"/>same value. In the case of 8-connectivity, 1, 2, 3, 4, 5, 6, 7, and 8 can be connected if they have the same value.</p><p>In the following example, we can see the difference between an eight and four connectivity algorithm. We will apply each algorithm to the next binarized image. We used a small 9 X 9 image and zoomed it to show how connected components, and the difference between an 4- and 8-connectivity, work:</p><div><img src="img/B04283_05_12.jpg" alt="The connected component algorithm"/></div><p>The 4-connectivity algorithm detects two objects, as shown on the left-hand side image. The 8-connectivity<a id="id203" class="indexterm"/> algorithm detects only one object (the right-hand side image) because two diagonal pixels are connected, whereas <a id="id204" class="indexterm"/>in a 4-connectivity algorithm, only vertical and horizontal pixels are connected. We can see the result in the following figure, where each object has a different gray color value:</p><div><img src="img/B04283_05_13.jpg" alt="The connected component algorithm"/></div><p>OpenCV 3 introduces you to the connected components algorithm with the following two different functions:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">connectedComponents(image, labels, connectivity=8, type=CV_32S)</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">connectedComponentsWithStats(image, labels, stats, centroids, connectivity=8, ltype=CV_32S)</code></li></ul></div><p>Both the <a id="id205" class="indexterm"/>functions return<a id="id206" class="indexterm"/> an integer with the number of detected labels, where the label <code class="literal">0</code> represents the background.</p><p>The difference between these two functions is basically the information that returns each one. Let's check the parameters of each one. The <a id="id207" class="indexterm"/>
<code class="literal">connectedComponents</code> function give us the following parameters:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Image</strong>: This <a id="id208" class="indexterm"/>is the input image to be labeled.</li><li class="listitem" style="list-style-type: disc"><strong>Labels</strong>: This is<a id="id209" class="indexterm"/> a <code class="literal">mat</code> output with the same size of an input image, where each pixel has the value of its label, and all <code class="literal">0</code>'s represent the background, the pixels that have <code class="literal">1</code> as values represent the first connected component object, and so on.</li><li class="listitem" style="list-style-type: disc"><strong>Connectivity</strong>: This<a id="id210" class="indexterm"/> has two possible values: <code class="literal">8</code> or <code class="literal">4</code> that represents the connectivity we want to use.</li><li class="listitem" style="list-style-type: disc"><strong>Type</strong>: This is<a id="id211" class="indexterm"/> the type of the label image that we would want to use: only two types are allowed, <code class="literal">CV32_S</code> or <code class="literal">CV16_U</code>. By default, it is <code class="literal">CV32_S</code>.</li></ul></div><p>The <code class="literal">connectedComponentsWithStats</code> function <a id="id212" class="indexterm"/>has two more parameters that are defined: stats and centroids parameters:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">Stats</code>: This <a id="id213" class="indexterm"/>is an output parameter for each label, including the background label. The following statistics values can be accessed via stats (label, column), where columns are defined as well, as follows:<div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">CC_STAT_LEFT</code>: This is<a id="id214" class="indexterm"/> the leftmost x coordinate of a connected component object</li><li class="listitem" style="list-style-type: disc"><code class="literal">CC_STAT_TOP</code>: This <a id="id215" class="indexterm"/>is the topmost y coordinate of a connected component object</li><li class="listitem" style="list-style-type: disc"><code class="literal">CC_STAT_WIDTH</code>: This <a id="id216" class="indexterm"/>is the width of a connected component object defined by its bounding box</li><li class="listitem" style="list-style-type: disc"><code class="literal">CC_STAT_HEIGHT</code>: This<a id="id217" class="indexterm"/> is the height of a connected component object defined by its bounding box</li><li class="listitem" style="list-style-type: disc"><code class="literal">CC_STAT_AREA</code>: This is<a id="id218" class="indexterm"/> the number of pixels (area) of the connected component object</li></ul></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">Centroids</code>: The<a id="id219" class="indexterm"/> centroid points in <code class="literal">float</code> type for each label inclusive of the background</li></ul></div><p>In our example <a id="id220" class="indexterm"/>application, we <a id="id221" class="indexterm"/>will create two functions that are to be applied to these two OpenCV algorithms and show the user the obtained result in a new image with colored objects in the basic algorithm and draw the area of the stats algorithm for each object.</p><p>Let's define the basic drawing of the connected <code class="literal">component</code> function:</p><div><pre class="programlisting">void ConnectedComponents(Mat img)
{
  // Use connected components to divide our possibles parts of images 
  Mat labels;
  int num_objects= connectedComponents(img, labels);
  // Check the number of objects detected
  if(num_objects &lt; 2 ){
    cout &lt;&lt; "No objects detected" &lt;&lt; endl;
    return;
  }else{
    cout &lt;&lt; "Number of objects detected: " &lt;&lt; num_objects - 1 &lt;&lt; endl;
  }
  // Create output image coloring the objects
  Mat output= Mat::zeros(img.rows,img.cols, CV_8UC3);
  RNG rng( 0xFFFFFFFF );
  for(int i=1; i&lt;num_objects; i++){
    Mat mask= labels==i;
    output.setTo(randomColor(rng), mask);
  }
  imshow("Result", output);
}</pre></div><p>First of all, we <a id="id222" class="indexterm"/>call the OpenCV <code class="literal">connectedComponents</code> function that returns the number of objects detected. If the number of objects is less than two, this means that only the background object is detected, and then, we don't need to draw anything and finish. If the algorithm detects more than one object, then we show the number of objects detected via the terminal:</p><div><pre class="programlisting">Mat labels;
  int num_objects= connectedComponents(img, labels);
  // Check the number of objects detected
  if(num_objects &lt; 2 ){
    cout &lt;&lt; "No objects detected" &lt;&lt; endl;
    return;
  }else{
    cout &lt;&lt; "Number of objects detected: " &lt;&lt; num_objects - 1 &lt;&lt; endl;</pre></div><p>Now, we<a id="id223" class="indexterm"/> will draw all the detected objects in a new image with different colors, and then we need to create a new black image with the same input size and three channels:</p><div><pre class="programlisting">Mat output= Mat::zeros(img.rows,img.cols, CV_8UC3);</pre></div><p>Then, we need to loop over each label, except the <em>0</em> value because it's the background label:</p><div><pre class="programlisting">for(int i=1; i&lt;num_objects; i++){</pre></div><p>To extract each object from the label image, we need to create a mask for each label <code class="literal">i</code> using a comparison, and save it in a new image:</p><div><pre class="programlisting">    Mat mask= labels==i;</pre></div><p>Finally, we set a pseudo-random color to the output image using the mask:</p><div><pre class="programlisting">    output.setTo(randomColor(rng), mask);
  }</pre></div><p>After we loop all images, we have all the objects with different colors in our output image, and we only have to show the output image:</p><div><pre class="programlisting">mshow("Result", output);</pre></div><p>This is the result where each object is painted with a different color or gray value:</p><div><img src="img/B04283_05_14.jpg" alt="The connected component algorithm"/></div><p>Now, we will<a id="id224" class="indexterm"/> explain how to use the<a id="id225" class="indexterm"/> connected components with the stats OpenCV algorithm and show some more information in the output result image. The following function implements this functionality:</p><div><pre class="programlisting">void ConnectedComponentsStats(Mat img)
{
  // Use connected components with stats
  Mat labels, stats, centroids;
  int num_objects= connectedComponentsWithStats(img, labels, stats, centroids);
  // Check the number of objects detected
  if(num_objects &lt; 2 ){
    cout &lt;&lt; "No objects detected" &lt;&lt; endl;
    return;
  }else{
    cout &lt;&lt; "Number of objects detected: " &lt;&lt; num_objects - 1 &lt;&lt; endl;
  }
  // Create output image coloring the objects and show area
  Mat output= Mat::zeros(img.rows,img.cols, CV_8UC3);
  RNG rng( 0xFFFFFFFF );
  for(int i=1; i&lt;num_objects; i++){
    cout &lt;&lt; "Object "&lt;&lt; i &lt;&lt; " with pos: " &lt;&lt; centroids.at&lt;Point2d&gt;(i) &lt;&lt; " with area " &lt;&lt; stats.at&lt;int&gt;(i, CC_STAT_AREA) &lt;&lt; endl;
    Mat mask= labels==i;
    output.setTo(randomColor(rng), mask);
    // draw text with area
    stringstream ss;
    ss &lt;&lt; "area: " &lt;&lt; stats.at&lt;int&gt;(i, CC_STAT_AREA);

    putText(output, 
      ss.str(), 
      centroids.at&lt;Point2d&gt;(i), 
      FONT_HERSHEY_SIMPLEX, 
      0.4, 
      Scalar(255,255,255));
  }
  imshow("Result", output);
}</pre></div><p>Let's <a id="id226" class="indexterm"/>understand the code, as we did in the <a id="id227" class="indexterm"/>non-stats function. We call the connected components algorithm; but, in this case, using the <code class="literal">stats</code> function, we check whether we can detect more than one object:</p><div><pre class="programlisting">Mat labels, stats, centroids;
  int num_objects= connectedComponentsWithStats(img, labels, stats, centroids);
  // Check the number of objects detected
  if(num_objects &lt; 2 ){
    cout &lt;&lt; "No objects detected" &lt;&lt; endl;
    return;
  }else{
    cout &lt;&lt; "Number of objects detected: " &lt;&lt; num_objects - 1 &lt;&lt; endl;
  }</pre></div><p>Now we have two more output results: the <code class="literal">stats</code> and <code class="literal">centroids</code> variables. Then, for each label that we detect, we will show its centroid and area via the command line:</p><div><pre class="programlisting">for(int i=1; i&lt;num_objects; i++){
    cout &lt;&lt; "Object "&lt;&lt; i &lt;&lt; " with pos: " &lt;&lt; centroids.at&lt;Point2d&gt;(i) &lt;&lt; " with area " &lt;&lt; stats.at&lt;int&gt;(i, CC_STAT_AREA) &lt;&lt; endl;</pre></div><p>You can <a id="id228" class="indexterm"/>check the call to the <code class="literal">stats</code> variable in order to extract the area using the <code class="literal">stats.at&lt;int&gt;(I, CC_STAT_AREA)</code> column constant.</p><p>Now, as <a id="id229" class="indexterm"/>mentioned earlier, we paint the output image of the object labeled with the <code class="literal">i</code> number:</p><div><pre class="programlisting">Mat mask= labels==i;
output.setTo(randomColor(rng), mask);</pre></div><p>Finally, we need to add over the image, in the centroid of the object segmented, some info like the area. To do this, we use the <code class="literal">stats</code> and <code class="literal">centroid</code> variables using the <code class="literal">putText</code> function. First, we need to create a stringstream to add the stats area information:</p><div><pre class="programlisting">// draw text with area
    stringstream ss;
    ss &lt;&lt; "area: " &lt;&lt; stats.at&lt;int&gt;(i, CC_STAT_AREA);</pre></div><p>Then, use the <code class="literal">putText</code> using the centroid as the text position:</p><div><pre class="programlisting">    putText(output, 
      ss.str(), 
      centroids.at&lt;Point2d&gt;(i), 
      FONT_HERSHEY_SIMPLEX, 
      0.4, 
      Scalar(255,255,255));</pre></div><p>The result of this function looks like this:</p><div><img src="img/B04283_05_15.jpg" alt="The connected component algorithm"/></div></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec37"/>The findContours algorithm</h2></div></div></div><p>The <a id="id230" class="indexterm"/>
<code class="literal">findContours</code> algorithm is one of the <a id="id231" class="indexterm"/>most frequently used OpenCV algorithms to segment objects. This algorithm has been included in OpenCV since its first version and provides more information and descriptors, such as shapes, topological organizations, and so on, to the developers:</p><div><pre class="programlisting">void findContours(InputOutputArray image, OutputArrayOfArrays contours, OutputArray hierarchy, int mode, int method, Point offset=Point())</pre></div><p>Let's explain each parameter, as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Image</strong>: This is the input binary image.</li><li class="listitem" style="list-style-type: disc"><strong>Contours</strong>: This is the contours output where each detected contour is a vector of points.</li><li class="listitem" style="list-style-type: disc"><strong>Hierarchy</strong>: This is the optional output vector where we store the hierarchy of contours. This is the topology of the image where we can get the relations between each contour.</li><li class="listitem" style="list-style-type: disc"><strong>Mode</strong>: This is the method used to retrieve the contours:<div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">RETR_EXTERNAL</code>: This retrieves only the external contours.</li><li class="listitem" style="list-style-type: disc"><code class="literal">RETR_LIST</code>: This retrieves all the contours without establishing the hierarchy.</li><li class="listitem" style="list-style-type: disc"><code class="literal">RETR_CCOMP</code>: This retrieves all the contours with two levels of hierarchy: external and holes. If another object is inside one hole, then this is put on the top of the hierarchy.</li><li class="listitem" style="list-style-type: disc"><code class="literal">RETR_TREE</code>: This retrieves all the contours that create a full hierarchy between contours.</li></ul></div></li><li class="listitem" style="list-style-type: disc"><strong>Method</strong>: This allows you to perform the approximation method to retrieve the contours' shapes:<div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">CV_CHAIN_APPROX_NONE:</code> This does not apply any approximation to the contours and stores all the contours points.</li><li class="listitem" style="list-style-type: disc"><code class="literal">CV_CHAIN_APPROX_SIMPLE</code>: This compresses all the horizontal, vertical, and diagonal segments that store only the start and end points.</li><li class="listitem" style="list-style-type: disc"><code class="literal">CV_CHAIN_APPROX_TC89_L1,CV_CHAIN_APPROX_TC89_KCOS</code> This applies the Teh-Chin chain approximation algorithm.</li></ul></div></li><li class="listitem" style="list-style-type: disc"><strong>Offset</strong>: This is the optional point value used to shift all the contours. This is very useful when we work in a ROI and is required to retrieve the global positions.</li></ul></div><div><div><h3 class="title"><a id="note21"/>Note</h3><p>
<strong>Note</strong>
</p><p>The input image is modified by the <code class="literal">findContours</code> function. Create a copy of your image before it is sent to this function if you need it.</p></div></div><p>Now <a id="id232" class="indexterm"/>that we know the parameters <a id="id233" class="indexterm"/>of the <code class="literal">findContours</code> function, let's apply them to our example:</p><div><pre class="programlisting">void FindContoursBasic(Mat img)
{
  vector&lt;vector&lt;Point&gt; &gt; contours;
  findContours(img, contours, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE);
  Mat output= Mat::zeros(img.rows,img.cols, CV_8UC3);
  // Check the number of objects detected
  if(contours.size() == 0 ){
    cout &lt;&lt; "No objects detected" &lt;&lt; endl;
    return;
  }else{
    cout &lt;&lt; "Number of objects detected: " &lt;&lt; contours.size() &lt;&lt; endl;
  }
  RNG rng( 0xFFFFFFFF );
  for(int i=0; i&lt;contours.size(); i++)
    drawContours(output, contours, i, randomColor(rng));
  imshow("Result", output);
}</pre></div><p>Let's understand <a id="id234" class="indexterm"/>our implementation line by line.</p><p>In our case, we don't require any hierarchy, so we will retrieve only the external <code class="literal">contours</code> of all possible objects. To do this, we use the <code class="literal">RETR_EXTERNAL</code> mode, and we use the basic contour encoding scheme using the <code class="literal">CHAIN_APPROX_SIMPLE</code> method:</p><div><pre class="programlisting">vector&lt;vector&lt;Point&gt; &gt; contours;
vector&lt;Vec4i&gt; hierarchy;
findContours(img, contours, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE);</pre></div><p>Similar to the connected components examples mentioned earlier, we first check how many <code class="literal">contours</code> we have retrieved. If there are none, then we exit from our function:</p><div><pre class="programlisting">// Check the number of objects detected
  if(contours.size() == 0 ){
    cout &lt;&lt; "No objects detected" &lt;&lt; endl;
    return;
  }else{
    cout &lt;&lt; "Number of objects detected: " &lt;&lt; contours.size() &lt;&lt; endl;
  }</pre></div><p>Finally, we <a id="id235" class="indexterm"/>draw each detected <code class="literal">contour</code> that we detect, and we draw it in our output image with a different color. To do this, OpenCV provides us with a function to draw the result of the find <code class="literal">contours</code> image:</p><div><pre class="programlisting">for(int i=0; i&lt;contours.size(); i++)
    drawContours(output, contours, i, randomColor(rng));
  imshow("Result", output);
}</pre></div><p>The <code class="literal">drawContours</code> function allows the following parameters:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Image</strong>: This is the output image used to draw the contours.</li><li class="listitem" style="list-style-type: disc"><strong>Contours</strong>: This is the vector of contours.</li><li class="listitem" style="list-style-type: disc"><strong>Contour index</strong>: This is a number that indicates the contour to be drawn; if it is negative, all the contours are drawn.</li><li class="listitem" style="list-style-type: disc"><strong>Color</strong>: This is the color used to draw the contour.</li><li class="listitem" style="list-style-type: disc"><strong>Thickness</strong>: If this is negative, then the contour is filled with the color chosen.</li><li class="listitem" style="list-style-type: disc"><strong>Line type</strong>: This is used when we want draw with antialiasing, or other drawing methods.</li><li class="listitem" style="list-style-type: disc"><strong>Hierarchy</strong>: This is an optional parameter and is only needed if you want to draw only some of the contours.</li><li class="listitem" style="list-style-type: disc"><strong>Max level</strong>: This is an optional parameter and taken into account only when the hierarchy parameter is available. If it is set to 0, only the specified contour is drawn, and if it is set to 1, the function draws the current contour and the nested as well. If it is set to 2, then the algorithm draws all the specified contour hierarchies.</li><li class="listitem" style="list-style-type: disc"><strong>Offset</strong>: This is an optional parameter used to shift the contours.</li></ul></div><p>The result of our<a id="id236" class="indexterm"/> example can be shown in the following image:</p><div><img src="img/B04283_05_16.jpg" alt="The findContours algorithm"/></div><p>After a <a id="id237" class="indexterm"/>binarized image, we can see the three different algorithms that are used to divide and separate each object of an image, allowing <a id="id238" class="indexterm"/>us to isolate each object in order to manipulate or extract features.</p><p>We can see the entire process in the following image:</p><div><img src="img/B04283_05_17.jpg" alt="The findContours algorithm"/></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch05lvl1sec42"/>Summary</h1></div></div></div><p>In this chapter, we explored the basics of object segmentation in a controlled situation, where a camera take pictures of different objects.</p><p>We learned how to remove the background and light in order to allow us to binarize our image by minimizing the noise and also three different algorithms used to divide and separate each object of an image, allowing us to isolate each object in order to manipulate or extract features. Finally, we extracted all the objects on an image, where we are going to extract characteristics of each of these objects to train a machine learning system.</p><p>In the next chapter, we are going to predict the class of any of objects in an image, and then call to a robot or any other system to pick any of them, or detect an object that is not in the correct carrier tape, and then notify to a person to pick it up.</p></div></body></html>