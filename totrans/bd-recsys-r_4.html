<html><head></head><body><div class="chapter" title="Chapter&#xA0;4.&#xA0;Evaluating the Recommender Systems"><div class="titlepage"><div><div><h1 class="title"><a id="ch04"/>Chapter 4. Evaluating the Recommender Systems</h1></div></div></div><p>The previous chapter showed you how to build recommender systems. There are a few options, and <a class="indexterm" id="id176"/>some of them can be developed using the <code class="literal">recommenderlab</code> package. In addition, each technique has some parameters. After we build the models, how can we decide which one to use? How can we determine its parameters? We can first test the performance of some models and/or parameter configurations and then choose the one that performs best.</p><p>This chapter will show you how to evaluate recommender models, compare their performances, and choose the most appropriate model. In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Preparing the data to evaluate performance</li><li class="listitem" style="list-style-type: disc">Evaluating the performance of some models</li><li class="listitem" style="list-style-type: disc">Choosing the best performing models</li><li class="listitem" style="list-style-type: disc">Optimizing model parameters</li></ul></div><div class="section" title="Preparing the data to evaluate the models"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec37"/>Preparing the data to evaluate the models</h1></div></div></div><p>To evaluate <a class="indexterm" id="id177"/>models, you need to build them with some data and test them on some other data. This chapter will show you how to prepare the two sets of data. The <code class="literal">recommenderlab</code> package contains prebuilt tools that help in this task.</p><p>The target is to define two datasets, which are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Training set</strong></span>: These <a class="indexterm" id="id178"/>are the models from which users learn</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Testing set</strong></span>: These<a class="indexterm" id="id179"/> are the models that users apply and test</li></ul></div><p>In order to evaluate the models, we need to compare the recommendations with the user preferences. In order to do so, we need to forget about some user preferences in the test set and see whether <a class="indexterm" id="id180"/>the techniques are able to identify them. For each user in the test set, we ignore some purchases and build the recommendations based on the others. Let's load the packages:</p><div class="informalexample"><pre class="programlisting">library(recommenderlab)
library(ggplot2)</pre></div><p>The data-set that we will use is called <code class="literal">MovieLense</code>. Let's define <code class="literal">ratings_movies</code> containing only the most relevant users and movies:</p><div class="informalexample"><pre class="programlisting">data(MovieLense)
ratings_movies &lt;- MovieLense[rowCounts(MovieLense) &gt; 50, colCounts(MovieLense) &gt; 100]ratings_movies
## 560 x 332 rating matrix of class 'realRatingMatrix' with 55298 ratings.</pre></div><p>We are now ready to prepare the data.</p><div class="section" title="Splitting the data"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec37"/>Splitting the data</h2></div></div></div><p>The easiest way to build a training and test set is to split the data in two parts. First, we need to decide<a class="indexterm" id="id181"/> how many users to put into each part. For instance, we can put 80 percent of the users into the training set. We can define <code class="literal">percentage_training</code> by specifying the percentage of the training set:</p><div class="informalexample"><pre class="programlisting">percentage_training &lt;- 0.8</pre></div><p>For each user in the test set, we need to define how many items to use to generate recommendations. The remaining items will be used to test the model accuracy. It's better that this parameter is lower than the minimum number of items purchased by any user so that we don't have users without items to test the models:</p><div class="informalexample"><pre class="programlisting">min(rowCounts(ratings_movies))
## _18_</pre></div><p>For instance, we can keep <code class="literal">15</code> items:</p><div class="informalexample"><pre class="programlisting">items_to_keep &lt;- 15</pre></div><p>Evaluating a model consists of comparing the recommendations with the unknown purchases. The ratings are between 1 and 5, and we need to define what constitutes good and bad items. For this purpose, we will define a threshold with the minimum rating that is considered good:</p><div class="informalexample"><pre class="programlisting">rating_threshold &lt;- 3</pre></div><p>There is an additional parameter defining how many times we want to run the evaluation. For the moment, let's set it to <code class="literal">1</code>:</p><div class="informalexample"><pre class="programlisting">n_eval &lt;- 1</pre></div><p>We are ready to split the data. The <code class="literal">recommenderlab</code> function is <code class="literal">evaluationScheme</code> and its parameters are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">data</code>: This is the initial dataset</li><li class="listitem" style="list-style-type: disc"><code class="literal">method</code>: This is the way to split the data. In this case, it's <code class="literal">split</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">train</code>: This is the percentage of data in the training set</li><li class="listitem" style="list-style-type: disc"><code class="literal">given</code>: This is the number of items to keep</li><li class="listitem" style="list-style-type: disc"><code class="literal">goodRating</code>: This is the rating threshold</li><li class="listitem" style="list-style-type: disc"><code class="literal">k</code>: This is the number of times to run the evaluation</li></ul></div><p>Let's<a class="indexterm" id="id182"/> build <code class="literal">eval_sets</code> containing the sets:</p><div class="informalexample"><pre class="programlisting">eval_sets &lt;- evaluationScheme(data = ratings_movies, method = "split", train = percentage_training, given = items_to_keep, goodRating = rating_threshold, k = n_eval) eval_sets
## Evaluation scheme with 15 items given
## Method: 'split' with 1 run(s).
## Training set proportion: 0.800
## Good ratings: &gt;=3.000000
## Data set: 560 x 332 rating matrix of class 'realRatingMatrix' with 55298 ratings.</pre></div><p>In order to extract the sets, we need to use <code class="literal">getData</code>. There are three sets:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">train</code>: This is the training set</li><li class="listitem" style="list-style-type: disc"><code class="literal">known</code>: This is the test set, with the item used to build the recommendations</li><li class="listitem" style="list-style-type: disc"><code class="literal">unknown</code>: This is the test set, with the item used to test the recommendations</li></ul></div><p>Let's take a look at the training set:</p><div class="informalexample"><pre class="programlisting">getData(eval_sets, "train")
## 448 x 332 rating matrix of class 'realRatingMatrix' with 44472 ratings.</pre></div><p>It's a <code class="literal">realRatingMatrix</code> object, so we can apply methods such as <code class="literal">nrow</code> and <code class="literal">rowCounts</code> to it:</p><div class="informalexample"><pre class="programlisting">nrow(getData(eval_sets, "train")) / nrow(ratings_movies)
## _0.8_</pre></div><p>As expected, about 80 percent of the users are in the training set. Let's take a look at the two test sets:</p><div class="informalexample"><pre class="programlisting">getData(eval_sets, "known")
## 112 x 332 rating matrix of class 'realRatingMatrix' with 1680 ratings.
getData(eval_sets, "unknown")
## 112 x 332 rating matrix of class 'realRatingMatrix' with 9146 ratings.</pre></div><p>They both have the same number of users. There should be about 20 percent of data in the test set:</p><div class="informalexample"><pre class="programlisting">nrow(getData(eval_sets, "known")) / nrow(ratings_movies)
## _0.2_</pre></div><p>Everything is as expected. Let's see how many items we have for each user in the <code class="literal">known</code> set. It should be equal to <code class="literal">items_to_keep</code>, that is, <code class="literal">15</code>:</p><div class="informalexample"><pre class="programlisting">unique(rowCounts(getData(eval_sets, "known")))
## _15_</pre></div><p>The same is<a class="indexterm" id="id183"/> not true for the users in the test set, since the number of remaining items depends on the initial number of purchases:</p><div class="informalexample"><pre class="programlisting">qplot(rowCounts(getData(eval_sets, "unknown"))) + geom_histogram(binwidth = 10) + ggtitle("unknown items by the users")</pre></div><p>The following image displays the unknown items by the users:</p><div class="mediaobject"><img alt="Splitting the data" src="graphics/B03888_04_01.jpg"/></div><p>As <a class="indexterm" id="id184"/>expected, the number of items by users varies a lot.</p></div><div class="section" title="Bootstrapping data"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec38"/>Bootstrapping data</h2></div></div></div><p>In the previous<a class="indexterm" id="id185"/> subsection, we split the data into two parts, and the training set contained 80 percent of the rows. What if, instead, we sample the rows with replacement? The same user can be sampled more than once and, if the training set has the same size as it did earlier, there will be more users in the test set. This approach is called bootstrapping, and it's supported by <code class="literal">recommenderlab</code>. The parameters are the same as the previous approach. The only difference is that we specify <code class="literal">method = "bootstrap"</code> instead of <code class="literal">method = "split"</code>:</p><div class="informalexample"><pre class="programlisting">percentage_training &lt;- 0.8
items_to_keep &lt;- 15
rating_threshold &lt;- 3
n_eval &lt;- 1
eval_sets &lt;- evaluationScheme(data = ratings_movies, method = "bootstrap", train = percentage_training, given = items_to_keep, goodRating = rating_threshold, k = n_eval)</pre></div><p>The number of users in the training set is still equal to 80 percent of the total:</p><div class="informalexample"><pre class="programlisting">nrow(getData(eval_sets, "train")) / nrow(ratings_movies)
## _0.8_</pre></div><p>However, the same is not true for the items in the test set:</p><div class="informalexample"><pre class="programlisting">perc_test &lt;- nrow(getData(eval_sets, "known")) / nrow(ratings_movies)
perc_test
## _0.4393_</pre></div><p>The test set is more than twice as big as the previous set.</p><p>We can extract the unique users in the training set:</p><div class="informalexample"><pre class="programlisting">length(unique(eval_sets@runsTrain[[1]]))
## _314_</pre></div><p>The percentage of unique users in the training set should be complementary to the percentage of users in the test set, which is shown as follows:</p><div class="informalexample"><pre class="programlisting">perc_train &lt;- length(unique(eval_sets@runsTrain[[1]])) / nrow(ratings_movies)
perc_train + perc_test
## _1_</pre></div><p>We can count how many times each user is repeated in the training set:</p><div class="informalexample"><pre class="programlisting">table_train &lt;- table(eval_sets@runsTrain[[1]])
n_repetitions &lt;- factor(as.vector(table_train))
qplot(n_repetitions) + ggtitle("Number of repetitions in the training set")</pre></div><p>The following<a class="indexterm" id="id186"/> image displays the number of repetitions in the training set:</p><div class="mediaobject"><img alt="Bootstrapping data" src="graphics/B03888_04_02.jpg"/></div><p>Most of the users have been sampled fewer than four times.</p></div><div class="section" title="Using k-fold to validate models"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec39"/>Using k-fold to validate models</h2></div></div></div><p>The two previous <a class="indexterm" id="id187"/>approaches tested the recommender <a class="indexterm" id="id188"/>on part of the users. If, instead, we test the recommendation on each user, we could measure the performances much more accurately. We can split the data into some chunks, take a chunk out as the test set, and evaluate the accuracy. Then, we can do the same with each other chunk and compute the average accuracy. This approach is called k-fold and it's supported by <code class="literal">recommenderlab</code>.</p><p>We can use <code class="literal">evaluationScheme</code> and the difference is that, instead of specifying the percentage of<a class="indexterm" id="id189"/> data to put in the training set, we will define how <a class="indexterm" id="id190"/>many chunks we want. The argument is <span class="emphasis"><em>k</em></span>, like the number of repetitions in the previous examples. Clearly, we don't need to specify <code class="literal">train</code> anymore:</p><div class="informalexample"><pre class="programlisting">n_fold &lt;- 4
eval_sets &lt;- evaluationScheme(data = ratings_movies, method = "cross-validation", k = n_fold, given = items_to_keep, goodRating = rating_threshold)</pre></div><p>We can count how many items we have in each set:</p><div class="informalexample"><pre class="programlisting">size_sets &lt;- sapply(eval_sets@runsTrain, length)
size_sets
## _420_, _420_, _420_ and _420_</pre></div><p>As expected, all the sets have the same size.</p><p>This approach is the most accurate one, although it's computationally heavier.</p><p>In this chapter, we've seen different approaches to prepare the training and the test set. In the next chapter, we will start with the evaluation.</p></div></div></div>
<div class="section" title="Evaluating recommender techniques"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec38"/>Evaluating recommender techniques</h1></div></div></div><p>This chapter<a class="indexterm" id="id191"/> will show you two popular approaches to evaluate recommendations. They are both based on the cross-validation framework described in the previous section.</p><p>The first approach is to evaluate the ratings estimated by the algorithm. The other approach is to evaluate the recommendations directly. There is a subsection for each approach.</p><div class="section" title="Evaluating the ratings"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec40"/>Evaluating the ratings</h2></div></div></div><p>In order to<a class="indexterm" id="id192"/> recommend items to new users, collaborative filtering estimates the ratings of items that are not yet purchased. Then, it recommends the top-rated items. At the moment, let's forget about the last step. We can evaluate the model by comparing the estimated ratings with the real ones.</p><p>First, let's prepare the data for validation, as shown in the previous section. Since the <span class="emphasis"><em>k</em></span>-fold is the most accurate approach, we will use it here:</p><div class="informalexample"><pre class="programlisting">n_fold &lt;- 4
items_to_keep &lt;- 15
rating_threshold &lt;- 3
eval_sets &lt;- evaluationScheme(data = ratings_movies, method = "cross-validation", k = n_fold, given = items_to_keep, goodRating = rating_threshold)</pre></div><p>We need to define the model to evaluate. For instance, we can evaluate an item-based collaborative filtering recommender. Let's build it using the Recommender function. We need to <a class="indexterm" id="id193"/>specify the name of the model and the list of its parameters. If we use their defaults, then it's NULL:</p><div class="informalexample"><pre class="programlisting">model_to_evaluate &lt;- "IBCF"
model_parameters &lt;- NULL</pre></div><p>We are now ready to build the model, using the following code:</p><div class="informalexample"><pre class="programlisting">eval_recommender &lt;- Recommender(data = getData(eval_sets, "train"), method = model_to_evaluate, parameter = model_parameters)</pre></div><p>The IBCF can recommend new items and predict their ratings. In order to build the model, we need to specify how many items we want to recommend, for example, <code class="literal">10</code>, even if we don't need to use this parameter in the evaluation:</p><div class="informalexample"><pre class="programlisting">items_to_recommend &lt;- 10</pre></div><p>We can build the matrix with the predicted ratings using the <code class="literal">predict</code> function:</p><div class="informalexample"><pre class="programlisting">eval_prediction &lt;- predict(object = eval_recommender, newdata = getData(eval_sets, "known"), n = items_to_recommend, type = "ratings") class(eval_prediction)
## realRatingMatrix</pre></div><p>The <code class="literal">eval_prediction</code> object is a rating matrix. Let's see how many movies we are recommending to each user. For this purpose, we can visualize the distribution of the number of movies per user:</p><div class="informalexample"><pre class="programlisting">qplot(rowCounts(eval_prediction)) + geom_histogram(binwidth = 10) + ggtitle("Distribution of movies per user")</pre></div><p>The following image displays the distribution of movies per user:</p><div class="mediaobject"><img alt="Evaluating the ratings" src="graphics/B03888_04_03.jpg"/></div><p>The number <a class="indexterm" id="id194"/>of movies per user is roughly between 150 and 300.</p><p>The function to measure the accuracy is <code class="literal">calcPredictionAccuracy</code> and it computes the following aspects:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Root mean square error (RMSE)</strong></span>: This is the standard deviation of the difference<a class="indexterm" id="id195"/> between the real and predicted ratings.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Mean squared error (MSE)</strong></span>: This is the mean of the squared difference between the<a class="indexterm" id="id196"/> real and predicted ratings. It's the square of RMSE, so it contains the same information.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Mean absolute error (MAE)</strong></span>: This is the mean of the absolute difference between<a class="indexterm" id="id197"/> the real and predicted ratings.</li></ul></div><p>We can compute these measures about each user by specifying <code class="literal">byUser = TRUE</code>:</p><div class="informalexample"><pre class="programlisting">eval_accuracy &lt;- calcPredictionAccuracy(
  x = eval_prediction, data = getData(eval_sets, "unknown"), byUser = TRUE)
head(eval_accuracy)</pre></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom"> </th><th style="text-align: left" valign="bottom">
<p>RMSE</p>
</th><th style="text-align: left" valign="bottom">
<p>MSE</p>
</th><th style="text-align: left" valign="bottom">
<p>MAE</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p><span class="strong"><strong>1</strong></span></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">1.217</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">1.481</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.8205</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><span class="strong"><strong>2</strong></span></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.908</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.8244</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.727</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><span class="strong"><strong>6</strong></span></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">1.172</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">1.374</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.903</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><span class="strong"><strong>14</strong></span></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">1.405</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">1.973</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">1.027</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><span class="strong"><strong>15</strong></span></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">1.601</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">2.562</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">1.243</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><span class="strong"><strong>18</strong></span></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.8787</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.7721</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.633</code></p>
</td></tr></tbody></table></div><p>Let's take a <a class="indexterm" id="id198"/>look at the RMSE by a user:</p><div class="informalexample"><pre class="programlisting">qplot(eval_accuracy[, "RMSE"]) + geom_histogram(binwidth = 0.1) + ggtitle("Distribution of the RMSE by user")</pre></div><p>The following image displays the distribution of the RSME by user:</p><div class="mediaobject"><img alt="Evaluating the ratings" src="graphics/B03888_04_04.jpg"/></div><p>Most of the <a class="indexterm" id="id199"/>RMSEs are in the range of 0.8 to 1.4. We evaluated the model for each user. In order to have a performance index of the whole model, we need to compute the average indices, specifying <code class="literal">byUser = FALSE</code>:</p><div class="informalexample"><pre class="programlisting">eval_accuracy &lt;- calcPredictionAccuracy(x = eval_prediction, data = getData(eval_sets, "unknown"), byUser = FALSE) eval_accuracy
## _1.101_, _1.211_ and _0.8124_</pre></div><p>These measures are useful to compare the performance of different models on the same data.</p></div><div class="section" title="Evaluating the recommendations"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec41"/>Evaluating the recommendations</h2></div></div></div><p>Another way to measure accuracies is by comparing the recommendations with the purchases having a positive rating. For this purpose, we can use the prebuilt <code class="literal">evaluate</code> function. Its inputs are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">x</code>: This is the object containing the evaluation scheme.</li><li class="listitem" style="list-style-type: disc"><code class="literal">method</code>: This is the recommendation technique.</li><li class="listitem" style="list-style-type: disc"><code class="literal">n</code>: This is the number of items to recommend to each user. If we can specify a vector of <code class="literal">n</code>, the function will evaluate the recommender performance depending on <code class="literal">n</code>.</li></ul></div><p>We have<a class="indexterm" id="id200"/> already defined a threshold, <code class="literal">rating_threshold &lt;- 3</code>, for positive ratings, and this parameter is already stored inside <code class="literal">eval_sets</code>. The <code class="literal">progress = FALSE</code> argument suppresses a progress report:</p><div class="informalexample"><pre class="programlisting">results &lt;- evaluate(x = eval_sets, method = model_to_evaluate, n = seq(10, 100, 10))
class(results)
## evaluationResults</pre></div><p>The <code class="literal">results</code> object is an <code class="literal">evaluationResults</code> object containing the results of the evaluation. Using <code class="literal">getConfusionMatrix</code>, we can extract a list of confusion matrices. Each element of the list corresponds to a different split of the <span class="emphasis"><em>k</em></span>-fold. Let's take a look at the first element:</p><div class="informalexample"><pre class="programlisting">head(getConfusionMatrix(results)[[1]])</pre></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom"> </th><th style="text-align: left" valign="bottom">
<p>TP</p>
</th><th style="text-align: left" valign="bottom">
<p>FP</p>
</th><th style="text-align: left" valign="bottom">
<p>FN</p>
</th><th style="text-align: left" valign="bottom">
<p>TN</p>
</th><th style="text-align: left" valign="bottom">
<p>precision</p>
</th><th style="text-align: left" valign="bottom">
<p>recall</p>
</th><th style="text-align: left" valign="bottom">
<p>TPR</p>
</th><th style="text-align: left" valign="bottom">
<p>FPR</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p><span class="strong"><strong>10</strong></span></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">3.443</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">6.557</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">70.61</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">236.4</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.3443</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.04642</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.04642</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.02625</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><span class="strong"><strong>20</strong></span></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">6.686</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">13.31</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">67.36</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">229.6</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.3343</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.09175</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.09175</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.05363</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><span class="strong"><strong>30</strong></span></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">10.02</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">19.98</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">64.03</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">223</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.334</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.1393</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.1393</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.08075</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><span class="strong"><strong>40</strong></span></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">13.29</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">26.71</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">60.76</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">216.2</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.3323</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.1849</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.1849</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.1081</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><span class="strong"><strong>50</strong></span></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">16.43</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">33.57</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">57.62</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">209.4</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.3286</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.2308</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.2308</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.1362</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><span class="strong"><strong>60</strong></span></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">19.61</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">40.39</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">54.44</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">202.6</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.3268</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.2759</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.2759</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.164</code></p>
</td></tr></tbody></table></div><p>The first four columns contain the true-false positives/negatives, and they are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>True Positives (TP)</strong></span>: These are recommended items that have been purchased</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>False Positives (FP)</strong></span>: These are recommended items that haven't been purchased</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>False Negatives(FN)</strong></span>: These are not recommended items that have been purchased</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>True Negatives (TN)</strong></span>: These are not recommended items that haven't been purchased</li></ul></div><p>A perfect (or overfitted) model would have only TP and TN.</p><p>If we want<a class="indexterm" id="id201"/> to take account of all the splits at the same time, we can just sum up the indices:</p><div class="informalexample"><pre class="programlisting">columns_to_sum &lt;- c("TP", "FP", "FN", "TN")
indices_summed &lt;- Reduce("+", getConfusionMatrix(results))[, columns_to_sum]
head(indices_summed)</pre></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom"> </th><th style="text-align: left" valign="bottom">
<p>TP</p>
</th><th style="text-align: left" valign="bottom">
<p>FP</p>
</th><th style="text-align: left" valign="bottom">
<p>FN</p>
</th><th style="text-align: left" valign="bottom">
<p>TN</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p><span class="strong"><strong>10</strong></span></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">13.05</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">26.95</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">279.3</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">948.7</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><span class="strong"><strong>20</strong></span></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">25.4</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">54.6</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">267</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">921</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><span class="strong"><strong>30</strong></span></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">37.74</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">82.26</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">254.7</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">893.4</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><span class="strong"><strong>40</strong></span></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">50.58</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">109.4</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">241.8</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">866.2</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><span class="strong"><strong>50</strong></span></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">62.35</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">137.7</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">230</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">838</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><span class="strong"><strong>60</strong></span></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">74.88</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">165.1</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">217.5</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">810.5</code></p>
</td></tr></tbody></table></div><p>Note that we could have used <code class="literal">avg(results)</code> instead.</p><p>The other four columns contain performance indices, and it's harder to summarize them across all the folds. However, we can visualize them by building some charts.</p><p>First, let's build the ROC curve. It displays these factors:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>True Positive Rate (TPR)</strong></span>: This is the percentage of purchased items that have<a class="indexterm" id="id202"/> been recommended. It's the number of TP divided by the number of purchased items (TP + FN).</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>False Positive Rate (FPR)</strong></span>: This is the percentage of not purchased items that have<a class="indexterm" id="id203"/> been recommended. It's the number of FP divided by the number of not purchased items (FP + TN).</li></ul></div><p>The <code class="literal">plot</code> method will build a chart with the <code class="literal">ROC curve</code>. In order to visualize the labels, we add the <code class="literal">annotate = TRUE</code> input:</p><div class="informalexample"><pre class="programlisting">plot(results, annotate = TRUE, main = "ROC curve")</pre></div><p>The following image displays the ROC curve:</p><div class="mediaobject"><img alt="Evaluating the recommendations" src="graphics/B03888_04_05.jpg"/></div><p>Two accuracy metrics <a class="indexterm" id="id204"/>are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Precision</strong></span>: This is<a class="indexterm" id="id205"/> the percentage of recommended items that have been purchased. It's the number of FP divided by the total number of positives (TP + FP).</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Recall</strong></span>: This <a class="indexterm" id="id206"/>is the percentage of purchased items that have been recommended. It's the number of TP divided by the total number of purchases (TP + FN). It's also equal to the True Positive Rate.</li></ul></div><p>If a small percentage of purchased items are recommended, the precision usually decreases. On the other hand, a higher percentage of purchased items will be recommended so that the recall increases:</p><div class="informalexample"><pre class="programlisting">plot(results, "prec/rec", annotate = TRUE, main = "Precision-recall")</pre></div><p>The following image displays the precision-recall:</p><div class="mediaobject"><img alt="Evaluating the recommendations" src="graphics/B03888_04_06.jpg"/></div><p>This chart <a class="indexterm" id="id207"/>reflects the tradeoff between precision and recall. Even if the curve is not perfectly monotonic, the trends are as expected.</p><p>In this section, we've seen how to evaluate a model. In the next section, we will see how to compare two or more models.</p></div></div>
<div class="section" title="Identifying the most suitable model"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec39"/>Identifying the most suitable model</h1></div></div></div><p>The previous <a class="indexterm" id="id208"/>chapter showed you how to evaluate a model. The performance indices are useful to compare different models and/or parameters. Applying different techniques on the same data, we can compare a performance index to pick the most appropriate recommender. Since there are different evaluation metrics, there is no objective way to do it.</p><p>The starting <a class="indexterm" id="id209"/>point is the k-fold evaluation framework that we defined in the previous section. It is stored inside <code class="literal">eval_sets</code>.</p><div class="section" title="Comparing models"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec42"/>Comparing models</h2></div></div></div><p>In order to <a class="indexterm" id="id210"/>compare different models, we first need to define them. Each model is stored in a list with its name and parameters. The components of the list are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">name</code>: This is the model name.</li><li class="listitem" style="list-style-type: disc"><code class="literal">param</code>: This is a list with its parameters. It can be NULL, if all the parameters are left at their defaults.</li></ul></div><p>For instance, that's how we can define an item-based collaborative filtering by setting the <code class="literal">k</code> parameter to <code class="literal">20</code>:</p><div class="informalexample"><pre class="programlisting">list(name = "IBCF", param = list(k = 20))</pre></div><p>In order to evaluate different models, we can define a list with them. We can build the following filtering:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Item-based collaborative filtering, using the Cosine as the distance function</li><li class="listitem" style="list-style-type: disc">Item-based collaborative filtering, using the Pearson correlation as the distance function</li><li class="listitem" style="list-style-type: disc">User-based collaborative filtering, using the Cosine as the distance function</li><li class="listitem" style="list-style-type: disc">User-based collaborative filtering, using the Pearson correlation as the distance function</li><li class="listitem" style="list-style-type: disc">Random recommendations to have a base line</li></ul></div><p>The preceding points are defined in the following code:</p><div class="informalexample"><pre class="programlisting">models_to_evaluate &lt;- list(
  IBCF_cos = list(name = "IBCF", param = list(method = "cosine")),IBCF_cor = list(name = "IBCF", param = list(method = "pearson")),UBCF_cos = list(name = "UBCF", param = list(method = "cosine")),UBCF_cor = list(name = "UBCF", param = list(method = "pearson")),random = list(name = "RANDOM", param=NULL)
)</pre></div><p>In order to evaluate the models properly, we need to test them, varying the number of items. For instance, we might want to recommend up to 100 movies to each user. Since 100 is already a big number of recommendations, we don't need to include higher values:</p><div class="informalexample"><pre class="programlisting">n_recommendations &lt;- c(1, 5, seq(10, 100, 10))</pre></div><p>We are ready<a class="indexterm" id="id211"/> to run and evaluate the models. Like in the previous chapter, the function is <code class="literal">evaluate</code>. The only difference is that now the input method is a list of models:</p><div class="informalexample"><pre class="programlisting">list_results &lt;- evaluate(x = eval_sets, method = models_to_evaluate, n = n_recommendations)
class(list_results)
## evaluationResultList</pre></div><p>The <code class="literal">list_results</code> object is an <code class="literal">evaluationResultList</code> object and it can be treated as a list. Let's take a look at its first element:</p><div class="informalexample"><pre class="programlisting">class(list_results[[1]])
## evaluationResults</pre></div><p>The first element of <code class="literal">list_results</code> is an <code class="literal">evaluationResults</code> object, and this object is the same as the output of evaluate with a single model. We can check whether the same is true for all its elements:</p><div class="informalexample"><pre class="programlisting">sapply(list_results, class) == "evaluationResults"
## TRUE TRUE TRUE TRUE TRUE</pre></div><p>Each element of <code class="literal">list_results</code> is an <code class="literal">evaluationResults</code> object. We can extract the related average confusion matrices using <code class="literal">avg</code>:</p><div class="informalexample"><pre class="programlisting">avg_matrices &lt;- lapply(list_results, avg)</pre></div><p>We can use <code class="literal">avg_matrices</code> to explore the performance evaluation. For instance, let's take a look at the IBCF with Cosine distance:</p><div class="informalexample"><pre class="programlisting">head(avg_matrices$IBCF_cos[, 5:8])</pre></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom"> </th><th style="text-align: left" valign="bottom">
<p>precision</p>
</th><th style="text-align: left" valign="bottom">
<p>recall</p>
</th><th style="text-align: left" valign="bottom">
<p>TPR</p>
</th><th style="text-align: left" valign="bottom">
<p>FPR</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p><span class="strong"><strong>1</strong></span></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.3589</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.004883</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.004883</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.002546</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><span class="strong"><strong>5</strong></span></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.3371</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.02211</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.02211</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.01318</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><span class="strong"><strong>10</strong></span></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.3262</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.0436</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.0436</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.02692</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><span class="strong"><strong>20</strong></span></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.3175</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.08552</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.08552</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.0548</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><span class="strong"><strong>30</strong></span></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.3145</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.1296</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.1296</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.08277</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><span class="strong"><strong>40</strong></span></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.3161</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.1773</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.1773</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0.1103</code></p>
</td></tr></tbody></table></div><p>We have all the metrics of the previous chapter. In the next section, we will explore these metrics to identify the best performing model.</p></div><div class="section" title="Identifying the most suitable model"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec43"/>Identifying the most suitable model</h2></div></div></div><p>We can<a class="indexterm" id="id212"/> compare the models by building a chart displaying their ROC curves. Like the previous section, we can use <code class="literal">plot</code>. The annotate argument specifies which curves will contain the labels. For instance, the first and second curves are labeled by defining <code class="literal">annotate = c(1, 2)</code>. In our case, we will label only the first curve:</p><div class="informalexample"><pre class="programlisting">plot(list_results, annotate = 1, legend = "topleft") title("ROC curve")</pre></div><div class="mediaobject"><img alt="Identifying the most suitable model" src="graphics/B03888_04_07.jpg"/></div><p>A good performance index is the <span class="strong"><strong>area under the curve</strong></span> (<span class="strong"><strong>AUC</strong></span>), that is, the area under the ROC curve. Even <a class="indexterm" id="id213"/>without computing it, we can notice that the highest is UBCF with cosine distance, so it's the best-performing technique.</p><p>Like we did in the previous section, we can build the precision-recall chart:</p><div class="informalexample"><pre class="programlisting">plot(list_results, "prec/rec", annotate = 1, legend = "bottomright") title("Precision-recall")</pre></div><p>The following image shows the precision-recall:</p><div class="mediaobject"><img alt="Identifying the most suitable model" src="graphics/B03888_04_08.jpg"/></div><p>The UBCF <a class="indexterm" id="id214"/>with cosine distance is still the top model. Depending on what we want to achieve, we can set an appropriate number of items to recommend.</p></div><div class="section" title="Optimizing a numeric parameter"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec44"/>Optimizing a numeric parameter</h2></div></div></div><p>Recommendation<a class="indexterm" id="id215"/> models often contain some numeric parameters. For instance, IBCF takes account of the <span class="emphasis"><em>k</em></span>-closest items. How can we optimize <span class="emphasis"><em>k</em></span>?</p><p>In a similar way to categoric parameters, we can test different values of a numeric parameter. In this case, we also need to define which values we want to test.</p><p>So far, we left <span class="emphasis"><em>k</em></span> to its default value: <code class="literal">30</code>. Now, we can explore more values, ranging between <code class="literal">5</code> and <code class="literal">40</code>:</p><div class="informalexample"><pre class="programlisting">vector_k &lt;- c(5, 10, 20, 30, 40)</pre></div><p>Using <code class="literal">lapply</code>, we <a class="indexterm" id="id216"/>can define a list of models to evaluate. The distance metric is the cosine:</p><div class="informalexample"><pre class="programlisting">models_to_evaluate &lt;- lapply(vector_k, function(k){
  list(name = "IBCF", param = list(method = "cosine", k = k))})names(models_to_evaluate) &lt;- paste0("IBCF_k_", vector_k)
Using the same commands as we did earlier, let's build and evaluate the models:
n_recommendations &lt;- c(1, 5, seq(10, 100, 10))
list_results &lt;- evaluate(x = eval_sets, method = models_to_evaluate, n = n_recommendations)</pre></div><p>Building a chart with the ROC curve, we should be able to identify the best-performing <span class="emphasis"><em>k</em></span>:</p><div class="informalexample"><pre class="programlisting">plot(list_results, annotate = 1,      legend = "topleft") title("ROC curve")</pre></div><div class="mediaobject"><img alt="Optimizing a numeric parameter" src="graphics/B03888_04_09.jpg"/></div><p>The <span class="emphasis"><em>k</em></span> having the biggest AUC is 10. Another good candidate is 5, but it can never have a high TPR. This means that, even if we set a very high n value, the algorithm won't be able to recommend<a class="indexterm" id="id217"/> a big percentage of items that the user liked. The IBCF with <code class="literal">k = 5</code> recommends only a few items similar to the purchases. Therefore, it can't be used to recommend many items.</p><p>Let's take a look at the precision-recall chart:</p><div class="informalexample"><pre class="programlisting">plot(list_results, "prec/rec", annotate = 1, legend = "bottomright")
title("Precision-recall")</pre></div><p>The following image displays the precision-recall:</p><div class="mediaobject"><img alt="Optimizing a numeric parameter" src="graphics/B03888_04_10.jpg"/></div><p>To achieve the highest recall, we need to set <code class="literal">k = 10</code>. If we are more interested in the precision, we set <code class="literal">k = 5</code>.</p><p>This section evaluated four techniques using different methods. Then, it optimized a numeric parameter of one of them. Depending on what we want to achieve, the choice of parameters<a class="indexterm" id="id218"/> might be slightly different.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec40"/>Summary</h1></div></div></div><p>This chapter showed you how to evaluate the performance of different models in order to choose the most accurate one. There are different ways to evaluate performances that might potentially lead to different choices. Depending on the business target, the evaluation metric is different. This is an example of how business and data should be combined to achieve the final result.</p><p>The next chapter will explain a complete use case in which we will prepare the data, build different models, and test them.</p></div></body></html>