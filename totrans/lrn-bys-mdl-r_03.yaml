- en: Chapter 3. Introducing Bayesian Inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 1](part0014.xhtml#aid-DB7S2 "Chapter 1. Introducing the Probability
    Theory"), *Introducing the Probability Theory*, we learned about the Bayes theorem
    as the relation between conditional probabilities of two random variables such
    as *A* and *B*. This theorem is the basis for updating beliefs or model parameter
    values in Bayesian inference, given the observations. In this chapter, a more
    formal treatment of Bayesian inference will be given. To begin with, let us try
    to understand how uncertainties in a real-world problem are treated in Bayesian
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian view of uncertainty
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The classical or frequentist statistics typically take the view that any physical
    process-generating data containing noise can be modeled by a stochastic model
    with fixed values of parameters. The parameter values are learned from the observed
    data through procedures such as **maximum likelihood estimate**. The essential
    idea is to search in the parameter space to find the parameter values that maximize
    the probability of observing the data seen so far. Neither the uncertainty in
    the estimation of model parameters from data, nor the uncertainty in the model
    itself that explains the phenomena under study, is dealt with in a formal way.
    *The Bayesian approach, on the other hand, treats all sources of uncertainty using
    probabilities*. Therefore, neither the model to explain an observed dataset nor
    its parameters are fixed, but they are treated as uncertain variables. Bayesian
    inference provides a framework to learn the entire distribution of model parameters,
    not just the values, which maximize the probability of observing the given data.
    The learning can come from both the evidence provided by observed data and domain
    knowledge from experts. There is also a framework to select the best model among
    the family of models suited to explain a given dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have the distribution of model parameters, we can eliminate the effect
    of uncertainty of parameter estimation in the future values of a random variable
    predicted using the learned model. This is done by averaging over the model parameter
    values through marginalization of joint probability distribution, as explained
    in [Chapter 1](part0014.xhtml#aid-DB7S2 "Chapter 1. Introducing the Probability
    Theory"), *Introducing the Probability Theory*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the joint probability distribution of *N* random variables again,
    as discussed in [Chapter 1](part0014.xhtml#aid-DB7S2 "Chapter 1. Introducing the
    Probability Theory"), *Introducing the Probability Theory*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian view of uncertainty](img/image00252.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This time, we have added one more term, *m*, to the argument of the probability
    distribution, in order to indicate explicitly that the parameters ![Bayesian view
    of uncertainty](img/image00253.jpeg) are generated by the model *m*. Then, according
    to Bayes theorem, the probability distribution of model parameters conditioned
    on the observed data ![Bayesian view of uncertainty](img/image00254.jpeg) and
    model *m* is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian view of uncertainty](img/image00255.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Formally, the term on the LHS of the equation ![Bayesian view of uncertainty](img/image00256.jpeg)
    is called **posterior probability distribution**. The second term appearing in
    the numerator of RHS, ![Bayesian view of uncertainty](img/image00257.jpeg), is
    called the **prior probability distribution**. It represents the prior belief
    about the model parameters, before observing any data, say, from the domain knowledge.
    Prior distributions can also have parameters and they are called hyperparameters.
    The term ![Bayesian view of uncertainty](img/image00252.jpeg) is the likelihood
    of model *m* explaining the observed data. Since ![Bayesian view of uncertainty](img/image00258.jpeg),
    it can be considered as a normalization constant ![Bayesian view of uncertainty](img/image00259.jpeg).
    The preceding equation can be rewritten in an iterative form as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian view of uncertainty](img/image00260.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Bayesian view of uncertainty](img/image00261.jpeg) represents values
    of observations that are obtained at time step *n*, ![Bayesian view of uncertainty](img/image00262.jpeg)
    is the marginal parameter distribution updated until time step *n - 1*, and ![Bayesian
    view of uncertainty](img/image00263.jpeg) is the model parameter distribution
    updated after seeing the observations ![Bayesian view of uncertainty](img/image00264.jpeg)
    at time step *n*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Casting Bayes theorem in this iterative form is useful for online learning
    and it suggests the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Model parameters can be learned in an iterative way as more and more data or
    evidence is obtained
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The posterior distribution estimated using the data seen so far can be treated
    as a prior model when the next set of observations is obtained
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even if no data is available, one could make predictions based on prior distribution
    created using the domain knowledge alone
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To make these points clear, let''s take a simple illustrative example. Consider
    the case where one is trying to estimate the distribution of the height of males
    in a given region. The data used for this example is the height measurement in
    centimeters obtained from *M* volunteers sampled randomly from the population.
    We assume that the heights are distributed according to a normal distribution
    with the mean ![Bayesian view of uncertainty](img/image00265.jpeg) and variance
    ![Bayesian view of uncertainty](img/image00266.jpeg):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian view of uncertainty](img/image00267.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'As mentioned earlier, in classical statistics, one tries to estimate the values
    of ![Bayesian view of uncertainty](img/image00265.jpeg) and ![Bayesian view of
    uncertainty](img/image00266.jpeg) from observed data. Apart from the best estimate
    value for each parameter, one could also determine an error term of the estimate.
    In the Bayesian approach, on the other hand, ![Bayesian view of uncertainty](img/image00265.jpeg)
    and ![Bayesian view of uncertainty](img/image00266.jpeg) are also treated as random
    variables. Let''s, for simplicity, assume ![Bayesian view of uncertainty](img/image00266.jpeg)
    is a known constant. Also, let''s assume that the prior distribution for ![Bayesian
    view of uncertainty](img/image00265.jpeg) is a normal distribution with (hyper)
    parameters ![Bayesian view of uncertainty](img/image00268.jpeg) and ![Bayesian
    view of uncertainty](img/image00269.jpeg). In this case, the expression for posterior
    distribution of ![Bayesian view of uncertainty](img/image00265.jpeg) is given
    by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian view of uncertainty](img/image00270.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, for convenience, we have used the notation ![Bayesian view of uncertainty](img/image00271.jpeg)
    for ![Bayesian view of uncertainty](img/image00272.jpeg). It is a simple exercise
    to expand the terms in the product and complete the squares in the exponential.
    This is given as an exercise at the end of the chapter. The resulting expression
    for the posterior distribution ![Bayesian view of uncertainty](img/image00273.jpeg)
    is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian view of uncertainty](img/image00274.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Bayesian view of uncertainty](img/image00275.jpeg) represents the sample
    mean. Though the preceding expression looks complex, it has a very simple interpretation.
    The posterior distribution is also a normal distribution with the following mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian view of uncertainty](img/image00276.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The variance is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian view of uncertainty](img/image00277.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '*The posterior mean is a weighted sum of prior mean* ![Bayesian view of uncertainty](img/image00268.jpeg)
    *and sample mean * ![Bayesian view of uncertainty](img/image00278.jpeg). As the
    sample size *M* increases, the weight of the sample mean increases and that of
    the prior decreases. Similarly, posterior precision (inverse of the variance)
    is the sum of the prior precision ![Bayesian view of uncertainty](img/image00279.jpeg)
    and precision of the sample mean ![Bayesian view of uncertainty](img/image00280.jpeg):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian view of uncertainty](img/image00281.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As *M* increases, the contribution of precision from observations (evidence)
    outweighs that from the prior knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a concrete example where we consider age distribution with the
    population mean 5.5 and population standard deviation 0.5\. We sample 100 people
    from this population by using the following R script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can calculate the posterior distribution using the following R function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![Bayesian view of uncertainty](img/image00282.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: One can see that as the number of samples increases, the estimated mean asymptotically
    approaches the population mean. The initial low value is due to the influence
    of the prior, which is, in this case, 5.0.
  prefs: []
  type: TYPE_NORMAL
- en: This simple and intuitive picture of how the prior knowledge and evidence from
    observations contribute to the overall model parameter estimate holds in any Bayesian
    inference. The precise mathematical expression for how they combine would be different.
    Therefore, one could start using a model for prediction with just prior information,
    either from the domain knowledge or the data collected in the past. Also, as new
    observations arrive, the model can be updated using the Bayesian scheme.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right prior distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the preceding simple example, we saw that if the likelihood function has
    the form of a normal distribution, and when the prior distribution is chosen as
    normal, the posterior also turns out to be a normal distribution. Also, we could
    get a closed-form analytical expression for the posterior mean. Since the posterior
    is obtained by multiplying the prior and likelihood functions and normalizing
    by integration over the parameter variables, the form of the prior distribution
    has a significant influence on the posterior. This section gives some more details
    about the different types of prior distributions and guidelines as to which ones
    to use in a given context.
  prefs: []
  type: TYPE_NORMAL
- en: There are different ways of classifying prior distributions in a formal way.
    One of the approaches is based on how much information a prior provides. In this
    scheme, the prior distributions are classified as *Informative*, *Weakly Informative*,
    *Least Informative*, and *Non-informative*. A detailed discussion of each of these
    classes is beyond the scope of this book, and interested readers should consult
    relevant books (references 1 and 2 in the *References* section of this chapter).
    Here, we take more of a practitioner's approach and illustrate some of the important
    classes of the prior distributions commonly used in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Non-informative priors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s start with the case where we do not have any prior knowledge about the
    model parameters. In this case, we want to express complete ignorance about model
    parameters through a mathematical expression. This is achieved through what are
    called non-informative priors. For example, in the case of a single random variable
    *x* that can take any value between ![Non-informative priors](img/image00283.jpeg)
    and ![Non-informative priors](img/image00284.jpeg), the non-informative prior
    for its mean ![Non-informative priors](img/image00265.jpeg) would be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Non-informative priors](img/image00285.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, the complete ignorance of the parameter value is captured through a uniform
    distribution function in the parameter space. Note that a uniform distribution
    is not a proper distribution function since its integral over the domain is not
    equal to 1; therefore, it is not normalizable. However, one can use an improper
    distribution function for the prior as long as it is multiplied by the likelihood
    function; the resulting posterior can be normalized.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the parameter of interest is variance ![Non-informative priors](img/image00266.jpeg),
    then by definition it can only take non-negative values. In this case, we transform
    the variable so that the transformed variable has a uniform probability in the
    range from ![Non-informative priors](img/image00283.jpeg) to ![Non-informative
    priors](img/image00284.jpeg):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Non-informative priors](img/image00286.jpeg)![Non-informative priors](img/image00287.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'It is easy to show, using simple differential calculus, that the corresponding
    non-informative distribution function in the original variable ![Non-informative
    priors](img/image00266.jpeg) would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Non-informative priors](img/image00288.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Another well-known non-informative prior used in practical applications is
    the Jeffreys prior, which is named after the British statistician Harold Jeffreys.
    This prior is invariant under reparametrization of ![Non-informative priors](img/image00289.jpeg)
    and is defined as proportional to the square root of the determinant of the Fisher
    information matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Non-informative priors](img/image00290.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, it is worth discussing the Fisher information matrix a little bit. If
    *X* is a random variable distributed according to ![Non-informative priors](img/image00291.jpeg),
    we may like to know how much information observations of *X* carry about the unknown
    parameter ![Non-informative priors](img/image00289.jpeg). This is what the Fisher
    Information Matrix provides. It is defined as the second moment of the score (first
    derivative of the logarithm of the likelihood function):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Non-informative priors](img/image00292.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s take a simple two-dimensional problem to understand the Fisher information
    matrix and Jeffreys prior. This example is given by Prof. D. Wittman of the University
    of California (reference 3 in the *References* section of this chapter). Let''s
    consider two types of food item: buns and hot dogs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s assume that generally they are produced in pairs (a hot dog and bun
    pair), but occasionally hot dogs are also produced independently in a separate
    process. There are two observables such as the number of hot dogs (![Non-informative
    priors](img/image00293.jpeg)) and the number of buns (![Non-informative priors](img/image00294.jpeg)),
    and two model parameters such as the production rate of pairs (![Non-informative
    priors](img/image00295.jpeg)) and the production rate of hot dogs alone (![Non-informative
    priors](img/image00296.jpeg)). We assume that the uncertainty in the measurements
    of the counts of these two food products is distributed according to the normal
    distribution, with variance ![Non-informative priors](img/image00297.jpeg) and
    ![Non-informative priors](img/image00298.jpeg), respectively. In this case, the
    Fisher Information matrix for this problem would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Non-informative priors](img/image00299.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, the inverse of the Fisher information matrix would correspond
    to the covariance matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Non-informative priors](img/image00300.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We have included one problem in the *Exercises* section of this chapter to compute
    the Fisher information matrix and Jeffrey's prior. Readers are requested to attempt
    this in order to get a feeling of how to compute Jeffrey's prior from observations.
  prefs: []
  type: TYPE_NORMAL
- en: Subjective priors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the key strengths of Bayesian statistics compared to classical (frequentist)
    statistics is that the framework allows one to capture subjective beliefs about
    any random variables. Usually, people will have intuitive feelings about minimum,
    maximum, mean, and most probable or peak values of a random variable. For example,
    if one is interested in the distribution of hourly temperatures in winter in a
    tropical country, then the people who are familiar with tropical climates or climatology
    experts will have a belief that, in winter, the temperature can go as low as 15°C
    and as high as 27°C with the most probable temperature value being 23°C. This
    can be captured as a prior distribution through the Triangle distribution as shown
    here.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Triangle distribution has three parameters corresponding to a minimum value
    (*a*), the most probable value (*b*), and a maximum value (*c*). The mean and
    variance of this distribution are given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Subjective priors](img/image00301.jpeg)![Subjective priors](img/image00302.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'One can also use a PERT distribution to represent a subjective belief about
    the minimum, maximum, and most probable value of a random variable. The PERT distribution
    is a reparametrized Beta distribution, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Subjective priors](img/image00303.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Subjective priors](img/image00304.jpeg)![Subjective priors](img/image00305.jpeg)![Subjective
    priors](img/image00306.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The PERT distribution is commonly used for project completion time analysis,
    and the name originates from project evaluation and review techniques. Another
    area where Triangle and PERT distributions are commonly used is in **risk modeling**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Often, people also have a belief about the relative probabilities of values
    of a random variable. For example, when studying the distribution of ages in a
    population such as Japan or some European countries, where there are more old
    people than young, an expert could give relative weights for the probability of
    different ages in the populations. This can be captured through a relative distribution
    containing the following details:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Subjective priors](img/image00307.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *min* and *max* represent the minimum and maximum values, *{values}*
    represents the set of possible observed values, and *{weights}* represents their
    relative weights. For example, in the population age distribution problem, these
    could be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Subjective priors](img/image00308.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The weights need not have a sum of 1.
  prefs: []
  type: TYPE_NORMAL
- en: Conjugate priors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If both the prior and posterior distributions are in the same family of distributions,
    then they are called **conjugate distributions** and the corresponding prior is
    called a **conjugate prior for the likelihood function**. Conjugate priors are
    very helpful for getting get analytical closed-form expressions for the posterior
    distribution. In the simple example we considered, we saw that when the noise
    is distributed according to the normal distribution, choosing a normal prior for
    the mean resulted in a normal posterior. The following table gives examples of
    some well-known conjugate pairs that we will use in the later chapters of this
    book:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Likelihood function | Model parameters | Conjugate prior | Hyperparameters
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Binomial | ![Conjugate priors](img/image00309.jpeg)(probability) | Beta |
    ![Conjugate priors](img/image00310.jpeg) |'
  prefs: []
  type: TYPE_TB
- en: '| Poisson | ![Conjugate priors](img/image00311.jpeg)(rate) | Gamma | ![Conjugate
    priors](img/image00312.jpeg) |'
  prefs: []
  type: TYPE_TB
- en: '| Categorical | ![Conjugate priors](img/image00313.jpeg)(probability, number
    of categories) | Dirichlet | ![Conjugate priors](img/image00295.jpeg) |'
  prefs: []
  type: TYPE_TB
- en: '| Univariate normal (known variance ![Conjugate priors](img/image00266.jpeg))
    | ![Conjugate priors](img/image00265.jpeg)(mean) | Normal | ![Conjugate priors](img/image00314.jpeg)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Univariate normal (known mean ![Conjugate priors](img/image00265.jpeg)) |
    ![Conjugate priors](img/image00266.jpeg)(variance) | Inverse Gamma | ![Conjugate
    priors](img/image00310.jpeg) |'
  prefs: []
  type: TYPE_TB
- en: Hierarchical priors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Sometimes, it is useful to define prior distributions for the hyperparameters
    itself. This is consistent with the Bayesian view that all parameters should be
    treated as uncertain by using probabilities. These distributions are called hyper-prior
    distributions. In theory, one can continue this into many levels as a hierarchical
    model. This is one way of eliciting the optimal prior distributions. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hierarchical priors](img/image00315.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '![Hierarchical priors](img/image00316.jpeg) is the prior distribution with
    a hyperparameter ![Hierarchical priors](img/image00295.jpeg). We could define
    a prior distribution for ![Hierarchical priors](img/image00295.jpeg) through a
    second set of equations, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hierarchical priors](img/image00317.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Hierarchical priors](img/image00318.jpeg) is the hyper-prior distribution
    for the hyperparameter ![Hierarchical priors](img/image00295.jpeg), parametrized
    by the hyper-hyper-parameter ![Hierarchical priors](img/image00298.jpeg). One
    can define a prior distribution for ![Hierarchical priors](img/image00298.jpeg)
    in the same way and continue the process forever. The practical reason for formalizing
    such models is that, at some level of hierarchy, one can define a uniform prior
    for the hyper parameters, reflecting complete ignorance about the parameter distribution,
    and effectively truncate the hierarchy. In practical situations, typically, this
    is done at the second level. This corresponds to, in the preceding example, using
    a uniform distribution for ![Hierarchical priors](img/image00318.jpeg).
  prefs: []
  type: TYPE_NORMAL
- en: I want to conclude this section by stressing one important point. Though prior
    distribution has a significant role in Bayesian inference, one need not worry
    about it too much, as long as the prior chosen is reasonable and consistent with
    the domain knowledge and evidence seen so far. The reasons are is that, first
    of all, as we have more evidence, the significance of the prior gets washed out.
    Secondly, when we use Bayesian models for prediction, we will average over the
    uncertainty in the estimation of the parameters using the posterior distribution.
    *This averaging is the key ingredient of Bayesian inference and it removes many
    of the ambiguities in the selection of the right prior*.
  prefs: []
  type: TYPE_NORMAL
- en: Estimation of posterior distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we discussed the essential concept behind Bayesian inference and also
    how to choose a prior distribution. Since one needs to compute the posterior distribution
    of model parameters before one can use the models for prediction, we discuss this
    task in this section. Though the Bayesian rule has a very simple-looking form,
    the computation of posterior distribution in a practically usable way is often
    very challenging. This is primarily because computation of the normalization constant
    ![Estimation of posterior distribution](img/image00259.jpeg) involves *N*-dimensional
    integrals, when there are *N* parameters. Even when one uses a conjugate prior,
    this computation can be very difficult to track analytically or numerically. This
    was one of the main reasons for not using Bayesian inference for multivariate
    modeling until recent decades. In this section, we will look at various approximate
    ways of computing posterior distributions that are used in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Maximum a posteriori estimation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Maximum a posteriori** (**MAP**) estimation is a point estimation that corresponds
    to taking the maximum value or mode of the posterior distribution. Though taking
    a point estimation does not capture the variability in the parameter estimation,
    it does take into account the effect of prior distribution to some extent when
    compared to maximum likelihood estimation. MAP estimation is also called poor
    man''s Bayesian inference.'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the Bayes rule, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Maximum a posteriori estimation](img/image00319.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, for convenience, we have used the notation *X* for the *N*-dimensional
    vector ![Maximum a posteriori estimation](img/image00320.jpeg). The last relation
    follows because the denominator of RHS of Bayes rule is independent of ![Maximum
    a posteriori estimation](img/image00253.jpeg). Compare this with the following
    maximum likelihood estimate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Maximum a posteriori estimation](img/image00321.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The difference between the MAP and ML estimate is that, whereas ML finds the
    mode of the likelihood function, MAP finds the mode of the product of the likelihood
    function and prior.
  prefs: []
  type: TYPE_NORMAL
- en: Laplace approximation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We saw that the MAP estimate just finds the maximum value of the posterior
    distribution. Laplace approximation goes one step further and also computes the
    local curvature around the maximum up to quadratic terms. This is equivalent to
    assuming that the posterior distribution is approximately Gaussian (normal) around
    the maximum. This would be the case if the amount of data were large compared
    to the number of parameters: *M >> N*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Laplace approximation](img/image00322.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *A* is an *N x N* Hessian matrix obtained by taking the derivative of
    the log of the posterior distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Laplace approximation](img/image00323.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'It is straightforward to evaluate the previous expressions at ![Laplace approximation](img/image00324.jpeg),
    using the following definition of conditional probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Laplace approximation](img/image00325.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can get an expression for *P(X|m)* from Laplace approximation that looks
    like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Laplace approximation](img/image00326.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the limit of a large number of samples, one can show that this expression
    simplifies to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Laplace approximation](img/image00327.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The term ![Laplace approximation](img/image00328.jpeg) is called **Bayesian
    information criterion** (**BIC**) and can be used for model selections or model
    comparison. This is one of the **goodness of fit** terms for a statistical model.
    Another similar criterion that is commonly used is **Akaike information criterion**
    (**AIC**), which is defined by ![Laplace approximation](img/image00329.jpeg).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will discuss how BIC can be used to compare different models for model
    selection. In the Bayesian framework, two models such as ![Laplace approximation](img/image00330.jpeg)
    and ![Laplace approximation](img/image00331.jpeg) are compared using the Bayes
    factor. The definition of the Bayes factor ![Laplace approximation](img/image00332.jpeg)
    is the ratio of posterior odds to prior odds that is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Laplace approximation](img/image00333.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, posterior odds is the ratio of posterior probabilities of the two models
    of the given data and prior odds is the ratio of prior probabilities of the two
    models, as given in the preceding equation. If ![Laplace approximation](img/image00334.jpeg),
    model ![Laplace approximation](img/image00330.jpeg) is preferred by the data and
    if ![Laplace approximation](img/image00335.jpeg), model ![Laplace approximation](img/image00331.jpeg)
    is preferred by the data.
  prefs: []
  type: TYPE_NORMAL
- en: In reality, it is difficult to compute the Bayes factor because it is difficult
    to get the precise prior probabilities. It can be shown that, in the large *N*
    limit, ![Laplace approximation](img/image00336.jpeg) can be viewed as a rough
    approximation to ![Laplace approximation](img/image00337.jpeg).
  prefs: []
  type: TYPE_NORMAL
- en: Monte Carlo simulations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The two approximations that we have discussed so far, the MAP and Laplace approximations,
    are useful when the posterior is a very sharply peaked function about the maximum
    value. Often, in real-life situations, the posterior will have long tails. This
    is, for example, the case in e-commerce where the probability of the purchasing
    of a product by a user has a long tail in the space of all products. So, in many
    practical situations, both MAP and Laplace approximations fail to give good results.
    Another approach is to directly sample from the posterior distribution. Monte
    Carlo simulation is a technique used for sampling from the posterior distribution
    and is one of the workhorses of Bayesian inference in practical applications.
    In this section, we will introduce the reader to **Markov Chain Monte Carlo**
    (**MCMC**) simulations and also discuss two common MCMC methods used in practice.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed earlier, let ![Monte Carlo simulations](img/image00338.jpeg) be
    the set of parameters that we are interested in estimating from the data through
    posterior distribution. Consider the case of the parameters being discrete, where
    each parameter has *K* possible values, that is, ![Monte Carlo simulations](img/image00339.jpeg).
    Set up a Markov process with states ![Monte Carlo simulations](img/image00253.jpeg)
    and transition probability matrix ![Monte Carlo simulations](img/image00340.jpeg).
    The essential idea behind MCMC simulations is that one can choose the transition
    probabilities in such a way that the steady state distribution of the Markov chain
    would correspond to the posterior distribution we are interested in. Once this
    is done, sampling from the Markov chain output, after it has reached a steady
    state, will give samples of ![Monte Carlo simulations](img/image00341.jpeg) distributed
    according to the posterior distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Now, the question is how to set up the Markov process in such a way that its
    steady state distribution corresponds to the posterior of interest. There are
    two well-known methods for this. One is the Metropolis-Hastings algorithm and
    the second is Gibbs sampling. We will discuss both in some detail here.
  prefs: []
  type: TYPE_NORMAL
- en: The Metropolis-Hasting algorithm
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The Metropolis-Hasting algorithm was one of the first major algorithms proposed
    for MCMC (reference 4 in the *References* section of this chapter). It has a very
    simple concept—something similar to a hill-climbing algorithm in optimization:'
  prefs: []
  type: TYPE_NORMAL
- en: Let ![The Metropolis-Hasting algorithm](img/image00342.jpeg) be the state of
    the system at time step *t*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To move the system to another state at time step *t + 1*, generate a candidate
    state ![The Metropolis-Hasting algorithm](img/image00343.jpeg) by sampling from
    a proposal distribution ![The Metropolis-Hasting algorithm](img/image00344.jpeg).
    The proposal distribution is chosen in such a way that it is easy to sample from
    it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Accept the proposal move with the following probability:![The Metropolis-Hasting
    algorithm](img/image00345.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If it is accepted, ![The Metropolis-Hasting algorithm](img/image00346.jpeg)
    = ![The Metropolis-Hasting algorithm](img/image00343.jpeg); if not, ![The Metropolis-Hasting
    algorithm](img/image00347.jpeg).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Continue the process until the distribution converges to the steady state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here, ![The Metropolis-Hasting algorithm](img/image00348.jpeg) is the posterior
    distribution that we want to simulate. Under certain conditions, the preceding
    update rule will guarantee that, in the large time limit, the Markov process will
    approach a steady state distributed according to ![The Metropolis-Hasting algorithm](img/image00348.jpeg).
  prefs: []
  type: TYPE_NORMAL
- en: The intuition behind the Metropolis-Hasting algorithm is simple. The proposal
    distribution ![The Metropolis-Hasting algorithm](img/image00344.jpeg) gives the
    conditional probability of proposing state ![The Metropolis-Hasting algorithm](img/image00343.jpeg)
    to make a transition in the next time step from the current state ![The Metropolis-Hasting
    algorithm](img/image00253.jpeg). Therefore, ![The Metropolis-Hasting algorithm](img/image00349.jpeg)
    is the probability that the system is currently in state ![The Metropolis-Hasting
    algorithm](img/image00343.jpeg) and would make a transition to state ![The Metropolis-Hasting
    algorithm](img/image00253.jpeg) in the next time step. Similarly, ![The Metropolis-Hasting
    algorithm](img/image00350.jpeg) is the probability that the system is currently
    in state ![The Metropolis-Hasting algorithm](img/image00253.jpeg) and would make
    a transition to state ![The Metropolis-Hasting algorithm](img/image00343.jpeg)
    in the next time step. If the ratio of these two probabilities is more than 1,
    accept the move. Alternatively, accept the move only with the probability given
    by the ratio. Therefore, the Metropolis-Hasting algorithm is like a hill-climbing
    algorithm where one accepts all the moves that are in the upward direction and
    accepts moves in the downward direction once in a while with a smaller probability.
    The downward moves help the system not to get stuck in local minima.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s revisit the example of estimating the posterior distribution of the
    mean and variance of the height of people in a population discussed in the introductory
    section. This time we will estimate the posterior distribution by using the Metropolis-Hasting
    algorithm. The following lines of R code do this job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To plot the resulting posterior distribution, we use the sm package in R:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting posterior distribution will look like the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Metropolis-Hasting algorithm](img/image00351.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Though the Metropolis-Hasting algorithm is simple to implement for any Bayesian
    inference problem, in practice it may not be very efficient in many cases. The
    main reason for this is that, unless one carefully chooses a proposal distribution
    ![The Metropolis-Hasting algorithm](img/image00344.jpeg), there would be too many
    rejections and it would take a large number of updates to reach the steady state.
    This is particularly the case when the number of parameters are high. There are
    various modifications of the basic Metropolis-Hasting algorithms that try to overcome
    these difficulties. We will briefly describe these when we discuss various R packages
    for the Metropolis-Hasting algorithm in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: R packages for the Metropolis-Hasting algorithm
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: There are several contributed packages in R for MCMC simulation using the Metropolis-Hasting
    algorithm, and here we describe some popular ones.
  prefs: []
  type: TYPE_NORMAL
- en: The **mcmc** package contributed by Charles J. Geyer and Leif T. Johnson is
    one of the popular packages in R for MCMC simulations. It has the `metrop` function
    for running the basic Metropolis-Hasting algorithm. The `metrop` function uses
    a multivariate normal distribution as the proposal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, it is useful to make a variable transformation to improve the speed
    of convergence in MCMC. The mcmc package has a function named `morph` for doing
    this. Combining these two, the function `morph.metrop` first transforms the variable,
    does a Metropolis on the transformed density, and converts the results back to
    the original variable.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from the mcmc package, two other useful packages in R are **MHadaptive**
    contributed by Corey Chivers and the **Evolutionary Monte Carlo** (**EMC**) **algorithm**
    package by Gopi Goswami. Due to lack of space, we will not be discussing these
    two packages in this book. Interested readers are requested to download these
    from the C-RAN project's site and experiment with them.
  prefs: []
  type: TYPE_NORMAL
- en: Gibbs sampling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As mentioned before, the Metropolis-Hasting algorithm suffers from the drawback
    of poor convergence, due to too many rejections, if one does not choose a good
    proposal distribution. To avoid this problem, two physicists Stuart Geman and
    Donald Geman proposed a new algorithm (reference 5 in the *References* section
    of this chapter). This algorithm is called Gibbs sampling and it is named after
    the famous physicist J W Gibbs. Currently, Gibbs sampling is the workhorse of
    MCMC for Bayesian inference.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let ![Gibbs sampling](img/image00352.jpeg) be the set of parameters of the
    model that we wish to estimate:'
  prefs: []
  type: TYPE_NORMAL
- en: Start with an initial state ![Gibbs sampling](img/image00353.jpeg).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At each time step, update the components one by one, by drawing from a distribution
    conditional on the most recent value of rest of the components:![Gibbs sampling](img/image00354.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After *N* steps, all components of the parameter will be updated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Continue with step 2 until the Markov process converges to a steady state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gibbs sampling is a very efficient algorithm since there are no rejections.
    However, to be able to use Gibbs sampling, the form of the conditional distributions
    of the posterior distribution should be known.
  prefs: []
  type: TYPE_NORMAL
- en: R packages for Gibbs sampling
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Unfortunately, there are not many contributed general purpose Gibbs sampling
    packages in R. The **gibbs.met** package provides two generic functions for performing
    MCMC in a Naïve way for user-defined target distribution. The first function is
    `gibbs_met`. This performs Gibbs sampling with each 1-dimensional distribution
    sampled by using the Metropolis algorithm, with normal distribution as the proposal
    distribution. The second function, `met_gaussian`, updates the whole state with
    independent normal distribution centered around the previous state. The gibbs.met
    package is useful for general purpose MCMC on moderate dimensional problems.
  prefs: []
  type: TYPE_NORMAL
- en: In the *Exercises* section of this chapter, we will discuss one problem that
    involves sampling from the two-dimensional normal distribution by using both the
    Metropolis-Hasting algorithm and Gibbs sampling to make these concepts more clear.
    Readers can use these mentioned packages for solving this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from the general purpose MCMC packages, there are several packages in
    R designed to solve a particular type of machine-learning problems. The **GibbsACOV**
    package can be used for one-way mixed-effects ANOVA and ANCOVA models. The **lda**
    package performs collapsed Gibbs sampling methods for topic (LDA) models. The
    **stocc** package fits a spatial occupancy model via Gibbs sampling. The **binomlogit**
    package implements an efficient MCMC for Binomial Logit models. **Bmk** is a package
    for doing diagnostics of MCMC output. **Bayesian Output Analysis Program** (**BOA**)
    is another similar package. **RBugs** is an interface of the well-known **OpenBUGS**
    MCMC package. The **ggmcmc** package is a graphical tool for analyzing MCMC simulation.
    **MCMCglm** is a package for generalized linear mixed models and **BoomSpikeSlab**
    is a package for doing MCMC for Spike and Slab regression. Finally, **SamplerCompare**
    is a package (more of a framework) for comparing the performance of various MCMC
    packages.
  prefs: []
  type: TYPE_NORMAL
- en: Variational approximation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the variational approximation scheme, one assumes that the posterior distribution
    ![Variational approximation](img/image00355.jpeg) can be approximated to a factorized
    form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Variational approximation](img/image00356.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Note that the factorized form is also a conditional distribution, so each ![Variational
    approximation](img/image00357.jpeg) can have dependence on other ![Variational
    approximation](img/image00358.jpeg)s through the conditioned variable *X*. In
    other words, this is not a trivial factorization making each parameter independent.
    The advantage of this factorization is that one can choose more analytically tractable
    forms of distribution functions ![Variational approximation](img/image00359.jpeg).
    In fact, one can vary the functions ![Variational approximation](img/image00360.jpeg)
    in such a way that it is as close to the true posterior ![Variational approximation](img/image00355.jpeg)
    as possible. This is mathematically formulated as a **variational calculus** problem,
    as explained here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use some measures to compute the distance between the two probability
    distributions, such as ![Variational approximation](img/image00361.jpeg) and ![Variational
    approximation](img/image00362.jpeg), where ![Variational approximation](img/image00352.jpeg).
    One of the standard measures of distance between probability distributions is
    the Kullback-Leibler divergence, or KL-divergence for short. It is defined as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Variational approximation](img/image00363.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The reason why it is called a divergence and not distance is that ![Variational
    approximation](img/image00364.jpeg) is not symmetric with respect to *Q* and *P*.
    One can use the relation ![Variational approximation](img/image00365.jpeg) and
    rewrite the preceding expression as an equation for *log P(X)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Variational approximation](img/image00366.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Variational approximation](img/image00367.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Note that, in the equation for *ln P(X)*, there is no dependence on *Q* on the
    LHS. Therefore, maximizing ![Variational approximation](img/image00368.jpeg) with
    respect to *Q* will minimize ![Variational approximation](img/image00364.jpeg),
    since their sum is a term independent of *Q*. By choosing analytically tractable
    functions for *Q*, one can do this maximization in practice. It will result in
    both an approximation for the posterior and a lower bound for *ln P(X)* that is
    the logarithm of evidence or marginal likelihood, since ![Variational approximation](img/image00369.jpeg).
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, variational approximation gives us two quantities in one shot. A
    posterior distribution can be used to make predictions about future observations
    (as explained in the next section) and a lower bound for evidence can be used
    for model selection.
  prefs: []
  type: TYPE_NORMAL
- en: 'How does one implement this minimization of KL-divergence in practice? Without
    going into mathematical details, here we write a final expression for the solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Variational approximation](img/image00370.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Variational approximation](img/image00371.jpeg) implies that the expectation
    of the logarithm of the joint distribution ![Variational approximation](img/image00372.jpeg)
    is taken over all the parameters ![Variational approximation](img/image00373.jpeg)
    except for ![Variational approximation](img/image00358.jpeg). Therefore, the minimization
    of KL-divergence leads to a set of coupled equations; one for each ![Variational
    approximation](img/image00374.jpeg) needs to be solved self-consistently to obtain
    the final solution. Though the variational approximation looks very complex mathematically,
    it has a very simple, intuitive explanation. The posterior distribution of each
    parameter ![Variational approximation](img/image00357.jpeg) is obtained by averaging
    the log of the joint distribution over all the other variables. This is analogous
    to the Mean Field theory in physics where, if there are *N* interacting charged
    particles, the system can be approximated by saying that each particle is in a
    constant external field, which is the average of fields produced by all the other
    particles.
  prefs: []
  type: TYPE_NORMAL
- en: We will end this section by mentioning a few R packages for variational approximation.
    The **VBmix** package can be used for variational approximation in Bayesian mixture
    models. A similar package is **vbdm** used for Bayesian discrete mixture models.
    The package **vbsr** is used for variational inference in Spike Regression Regularized
    Linear Models.
  prefs: []
  type: TYPE_NORMAL
- en: Prediction of future observations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once we have the posterior distribution inferred from data using some of the
    methods described already, it can be used to predict future observations. The
    probability of observing a value *Y*, given observed data *X*, and posterior distribution
    of parameters ![Prediction of future observations](img/image00375.jpeg) is given
    by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Prediction of future observations](img/image00376.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Note that, in this expression, the likelihood function ![Prediction of future
    observations](img/image00377.jpeg) is averaged by using the distribution of the
    parameter given by the posterior ![Prediction of future observations](img/image00375.jpeg).
    This is, in fact, the core strength of the Bayesian inference. This Bayesian averaging
    eliminates the uncertainty in estimating the parameter values and makes the prediction
    more robust.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Derive the equation for the posterior mean by expanding the square in the exponential
    ![Exercises](img/image00378.jpeg) for each *i*, collecting all similar power terms,
    and making a perfect square again. Note that the product of exponentials can be
    written as the exponential of a sum of terms.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For this exercise, we use the dataset corresponding to Smartphone-Based Recognition
    of Human Activities and Postural Transitions, from the UCI Machine Learning repository
    ([https://archive.ics.uci.edu/ml/datasets/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions](https://archive.ics.uci.edu/ml/datasets/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions)).
    It contains values of acceleration taken from an accelerometer on a smartphone.
    The original dataset contains *x*, *y*, and *z* components of the acceleration
    and the corresponding timestamp values. For this exercise, we have used only the
    two horizontal components of the acceleration *x* and *y*. In this exercise, let's
    assume that the acceleration follows a normal distribution. Let's also assume
    a normal prior distribution for the mean values of acceleration with a hyperparameter
    for a mean that is uniformly distributed in the interval (-0.5, 0.5) and a known
    variance equal to 1\. Find the posterior mean value by using the expression given
    in the equation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write an R function to compute the Fisher information matrix. Obtain the Fisher
    information matrix for this problem by using the dataset mentioned in exercise
    1 of this section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up an MCMC simulation for this problem by using the **mcmc** package in
    R. Plot a histogram of the simulated data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up an MCMC simulation using Gibbs sampling. Compare the results with that
    of the Metropolis algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Berger J.O. *Statistical Decision Theory and Bayesian Analysis*. Springer Series
    in Statistics. 1993\. ISBN-10: 0387960988'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Jayes E.T. *Probability Theory: The Logic of Science*. Cambridge University
    Press. 2003\. ISBN-10: 052159271'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wittman D. "Fisher Matrix for Beginners". Physics Department, University of
    California at Davis ([http://www.physics.ucdavis.edu/~dwittman/Fisher-matrix-guide.pdf](http://www.physics.ucdavis.edu/~dwittman/Fisher-matrix-guide.pdf))
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Metropolis N, Rosenbluth A.W., Rosenbluth M.N., Teller A.H., Teller E. "Equations
    of State Calculations by Fast Computing Machines". Journal of Chemical Physics
    21 (6): 1087–1092\. 1953'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Geman S., Geman D. "Stochastic Relaxation, Gibbs Distributions, and the Bayesian
    Restoration of Images". IEEE Transactions on Pattern Analysis and Machine Intelligence
    6 (6): 721-741\. 1984'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered basic principles of Bayesian inference. Starting
    with how uncertainty is treated differently in Bayesian statistics compared to
    classical statistics, we discussed deeply various components of Bayes' rule. Firstly,
    we learned the different types of prior distributions and how to choose the right
    one for your problem. Then we learned the estimation of posterior distribution
    using techniques such as MAP estimation, Laplace approximation, and MCMC simulations.
    Once the readers have comprehended this chapter, they will be in a position to
    apply Bayesian principles in their data analytics problems. Before we start discussing
    specific Bayesian machine learning problems, in the next chapter, we will review
    machine learning in general.
  prefs: []
  type: TYPE_NORMAL
