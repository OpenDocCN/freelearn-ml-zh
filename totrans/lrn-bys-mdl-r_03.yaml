- en: Chapter 3. Introducing Bayesian Inference
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三章。介绍贝叶斯推理
- en: In [Chapter 1](part0014.xhtml#aid-DB7S2 "Chapter 1. Introducing the Probability
    Theory"), *Introducing the Probability Theory*, we learned about the Bayes theorem
    as the relation between conditional probabilities of two random variables such
    as *A* and *B*. This theorem is the basis for updating beliefs or model parameter
    values in Bayesian inference, given the observations. In this chapter, a more
    formal treatment of Bayesian inference will be given. To begin with, let us try
    to understand how uncertainties in a real-world problem are treated in Bayesian
    approach.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第一章](part0014.xhtml#aid-DB7S2 "第一章。介绍概率论")中，*介绍概率论*，我们学习了贝叶斯定理作为两个随机变量，如 *A*
    和 *B* 的条件概率之间的关系。这个定理是贝叶斯推理中根据观察更新信念或模型参数值的基础。在本章中，将给出贝叶斯推理的更正式处理。首先，让我们尝试理解在贝叶斯方法中如何处理现实世界问题中的不确定性。
- en: Bayesian view of uncertainty
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯不确定性视图
- en: The classical or frequentist statistics typically take the view that any physical
    process-generating data containing noise can be modeled by a stochastic model
    with fixed values of parameters. The parameter values are learned from the observed
    data through procedures such as **maximum likelihood estimate**. The essential
    idea is to search in the parameter space to find the parameter values that maximize
    the probability of observing the data seen so far. Neither the uncertainty in
    the estimation of model parameters from data, nor the uncertainty in the model
    itself that explains the phenomena under study, is dealt with in a formal way.
    *The Bayesian approach, on the other hand, treats all sources of uncertainty using
    probabilities*. Therefore, neither the model to explain an observed dataset nor
    its parameters are fixed, but they are treated as uncertain variables. Bayesian
    inference provides a framework to learn the entire distribution of model parameters,
    not just the values, which maximize the probability of observing the given data.
    The learning can come from both the evidence provided by observed data and domain
    knowledge from experts. There is also a framework to select the best model among
    the family of models suited to explain a given dataset.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的或频率派统计学通常认为，任何产生含噪声数据的物理过程都可以用具有固定参数值的随机模型来建模。参数值通过如**最大似然估计**等程序从观察数据中学习。基本思想是在参数空间中搜索以找到最大化观察到的数据概率的参数值。既没有以正式方式处理从数据中估计模型参数的不确定性，也没有处理解释研究现象的模型本身的不确定性。*另一方面，贝叶斯方法使用概率来处理所有不确定性的来源*。因此，解释观察数据集的模型及其参数都不是固定的，而是被视为不确定变量。贝叶斯推理提供了一个框架来学习模型参数的整个分布，而不仅仅是最大化观察数据概率的值。学习可以来自观察数据提供的证据和专家的领域知识。还有一个框架来选择最适合解释给定数据集的模型家族中的最佳模型。
- en: Once we have the distribution of model parameters, we can eliminate the effect
    of uncertainty of parameter estimation in the future values of a random variable
    predicted using the learned model. This is done by averaging over the model parameter
    values through marginalization of joint probability distribution, as explained
    in [Chapter 1](part0014.xhtml#aid-DB7S2 "Chapter 1. Introducing the Probability
    Theory"), *Introducing the Probability Theory*.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了模型参数的分布，我们就可以通过联合概率分布的边缘化来消除使用学习模型预测的随机变量的未来值中参数估计不确定性的影响。这正如在[第一章](part0014.xhtml#aid-DB7S2
    "第一章。介绍概率论")中解释的，*介绍概率论*。
- en: 'Consider the joint probability distribution of *N* random variables again,
    as discussed in [Chapter 1](part0014.xhtml#aid-DB7S2 "Chapter 1. Introducing the
    Probability Theory"), *Introducing the Probability Theory*:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 再次考虑 *N* 个随机变量的联合概率分布，正如在[第一章](part0014.xhtml#aid-DB7S2 "第一章。介绍概率论")中讨论的，*介绍概率论*：
- en: '![Bayesian view of uncertainty](img/image00252.jpeg)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯不确定性视图](img/image00252.jpeg)'
- en: 'This time, we have added one more term, *m*, to the argument of the probability
    distribution, in order to indicate explicitly that the parameters ![Bayesian view
    of uncertainty](img/image00253.jpeg) are generated by the model *m*. Then, according
    to Bayes theorem, the probability distribution of model parameters conditioned
    on the observed data ![Bayesian view of uncertainty](img/image00254.jpeg) and
    model *m* is given by:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们在概率分布的参数中添加了一个额外的项 *m*，以明确表示参数 ![贝叶斯不确定性视图](img/image00253.jpeg) 是由模型 *m*
    生成的。然后，根据贝叶斯定理，给定观察数据 ![贝叶斯不确定性视图](img/image00254.jpeg) 和模型 *m* 的条件下，模型参数的概率分布如下：
- en: '![Bayesian view of uncertainty](img/image00255.jpeg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯不确定性视图](img/image00255.jpeg)'
- en: 'Formally, the term on the LHS of the equation ![Bayesian view of uncertainty](img/image00256.jpeg)
    is called **posterior probability distribution**. The second term appearing in
    the numerator of RHS, ![Bayesian view of uncertainty](img/image00257.jpeg), is
    called the **prior probability distribution**. It represents the prior belief
    about the model parameters, before observing any data, say, from the domain knowledge.
    Prior distributions can also have parameters and they are called hyperparameters.
    The term ![Bayesian view of uncertainty](img/image00252.jpeg) is the likelihood
    of model *m* explaining the observed data. Since ![Bayesian view of uncertainty](img/image00258.jpeg),
    it can be considered as a normalization constant ![Bayesian view of uncertainty](img/image00259.jpeg).
    The preceding equation can be rewritten in an iterative form as follows:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 正式来说，方程 ![贝叶斯不确定性视图](img/image00256.jpeg) 的左侧的项被称为**后验概率分布**。右侧分子中出现的第二个项 ![贝叶斯不确定性视图](img/image00257.jpeg)
    被称为**先验概率分布**。它代表了在观察任何数据之前，例如从领域知识中得到的对模型参数的先验信念。先验分布也可以有参数，这些参数被称为超参数。项 ![贝叶斯不确定性视图](img/image00252.jpeg)
    是模型 *m* 解释观察数据的似然。由于 ![贝叶斯不确定性视图](img/image00258.jpeg)，它可以被视为一个归一化常数 ![贝叶斯不确定性视图](img/image00259.jpeg)。前面的方程可以重写为以下迭代形式：
- en: '![Bayesian view of uncertainty](img/image00260.jpeg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯不确定性视图](img/image00260.jpeg)'
- en: Here, ![Bayesian view of uncertainty](img/image00261.jpeg) represents values
    of observations that are obtained at time step *n*, ![Bayesian view of uncertainty](img/image00262.jpeg)
    is the marginal parameter distribution updated until time step *n - 1*, and ![Bayesian
    view of uncertainty](img/image00263.jpeg) is the model parameter distribution
    updated after seeing the observations ![Bayesian view of uncertainty](img/image00264.jpeg)
    at time step *n*.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![贝叶斯不确定性视图](img/image00261.jpeg) 代表在时间步 *n* 获得的观测值，![贝叶斯不确定性视图](img/image00262.jpeg)
    是直到时间步 *n - 1* 更新的边缘参数分布，而 ![贝叶斯不确定性视图](img/image00263.jpeg) 是在时间步 *n* 观察到观测值
    ![贝叶斯不确定性视图](img/image00264.jpeg) 后更新的模型参数分布。
- en: 'Casting Bayes theorem in this iterative form is useful for online learning
    and it suggests the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 将贝叶斯定理以这种迭代形式表述对于在线学习是有用的，并且它暗示以下内容：
- en: Model parameters can be learned in an iterative way as more and more data or
    evidence is obtained
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型参数可以在获得更多数据和证据的迭代过程中学习。
- en: The posterior distribution estimated using the data seen so far can be treated
    as a prior model when the next set of observations is obtained
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用迄今为止看到的数据估计的后验分布，在获得下一组观测值时可以被视为先验模型。
- en: Even if no data is available, one could make predictions based on prior distribution
    created using the domain knowledge alone
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使没有数据可用，也可以仅基于领域知识创建的先验分布进行预测。
- en: 'To make these points clear, let''s take a simple illustrative example. Consider
    the case where one is trying to estimate the distribution of the height of males
    in a given region. The data used for this example is the height measurement in
    centimeters obtained from *M* volunteers sampled randomly from the population.
    We assume that the heights are distributed according to a normal distribution
    with the mean ![Bayesian view of uncertainty](img/image00265.jpeg) and variance
    ![Bayesian view of uncertainty](img/image00266.jpeg):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这些观点更加清晰，让我们通过一个简单的说明性例子来说明。考虑这样一个案例，即一个人试图估计某个地区男性身高的分布。用于此例的数据是从人口中随机抽取的
    *M* 个志愿者的身高测量值（以厘米为单位）。我们假设身高服从均值为 ![贝叶斯不确定性视图](img/image00265.jpeg) 和方差 ![贝叶斯不确定性视图](img/image00266.jpeg)
    的正态分布：
- en: '![Bayesian view of uncertainty](img/image00267.jpeg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯不确定性视图](img/image00267.jpeg)'
- en: 'As mentioned earlier, in classical statistics, one tries to estimate the values
    of ![Bayesian view of uncertainty](img/image00265.jpeg) and ![Bayesian view of
    uncertainty](img/image00266.jpeg) from observed data. Apart from the best estimate
    value for each parameter, one could also determine an error term of the estimate.
    In the Bayesian approach, on the other hand, ![Bayesian view of uncertainty](img/image00265.jpeg)
    and ![Bayesian view of uncertainty](img/image00266.jpeg) are also treated as random
    variables. Let''s, for simplicity, assume ![Bayesian view of uncertainty](img/image00266.jpeg)
    is a known constant. Also, let''s assume that the prior distribution for ![Bayesian
    view of uncertainty](img/image00265.jpeg) is a normal distribution with (hyper)
    parameters ![Bayesian view of uncertainty](img/image00268.jpeg) and ![Bayesian
    view of uncertainty](img/image00269.jpeg). In this case, the expression for posterior
    distribution of ![Bayesian view of uncertainty](img/image00265.jpeg) is given
    by:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在经典统计学中，人们试图从观测数据中估计![贝叶斯不确定性视图](img/image00265.jpeg)和![贝叶斯不确定性视图](img/image00266.jpeg)的值。除了每个参数的最佳估计值外，还可以确定估计的误差项。另一方面，在贝叶斯方法中，![贝叶斯不确定性视图](img/image00265.jpeg)和![贝叶斯不确定性视图](img/image00266.jpeg)也被视为随机变量。为了简化，我们假设![贝叶斯不确定性视图](img/image00266.jpeg)是一个已知的常数。此外，我们假设![贝叶斯不确定性视图](img/image00265.jpeg)的先验分布是一个具有(超)参数![贝叶斯不确定性视图](img/image00268.jpeg)和![贝叶斯不确定性视图](img/image00269.jpeg)的正态分布。在这种情况下，![贝叶斯不确定性视图](img/image00265.jpeg)的后验分布的表达式如下：
- en: '![Bayesian view of uncertainty](img/image00270.jpeg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯不确定性视图](img/image00270.jpeg)'
- en: 'Here, for convenience, we have used the notation ![Bayesian view of uncertainty](img/image00271.jpeg)
    for ![Bayesian view of uncertainty](img/image00272.jpeg). It is a simple exercise
    to expand the terms in the product and complete the squares in the exponential.
    This is given as an exercise at the end of the chapter. The resulting expression
    for the posterior distribution ![Bayesian view of uncertainty](img/image00273.jpeg)
    is given by:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，我们在这里使用符号![贝叶斯不确定性视图](img/image00271.jpeg)来表示![贝叶斯不确定性视图](img/image00272.jpeg)。将乘积中的项展开并完成指数中的平方是一个简单的练习。这作为本章末尾的练习给出。后验分布![贝叶斯不确定性视图](img/image00273.jpeg)的结果表达式如下：
- en: '![Bayesian view of uncertainty](img/image00274.jpeg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯不确定性视图](img/image00274.jpeg)'
- en: 'Here, ![Bayesian view of uncertainty](img/image00275.jpeg) represents the sample
    mean. Though the preceding expression looks complex, it has a very simple interpretation.
    The posterior distribution is also a normal distribution with the following mean:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![贝叶斯不确定性视图](img/image00275.jpeg)代表样本均值。尽管前面的表达式看起来很复杂，但它有一个非常简单的解释。后验分布也是一个正态分布，具有以下均值：
- en: '![Bayesian view of uncertainty](img/image00276.jpeg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯不确定性视图](img/image00276.jpeg)'
- en: 'The variance is as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 方差如下：
- en: '![Bayesian view of uncertainty](img/image00277.jpeg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯不确定性视图](img/image00277.jpeg)'
- en: '*The posterior mean is a weighted sum of prior mean* ![Bayesian view of uncertainty](img/image00268.jpeg)
    *and sample mean * ![Bayesian view of uncertainty](img/image00278.jpeg). As the
    sample size *M* increases, the weight of the sample mean increases and that of
    the prior decreases. Similarly, posterior precision (inverse of the variance)
    is the sum of the prior precision ![Bayesian view of uncertainty](img/image00279.jpeg)
    and precision of the sample mean ![Bayesian view of uncertainty](img/image00280.jpeg):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*后验均值是先验均值* ![贝叶斯不确定性视图](img/image00268.jpeg) *和样本均值* ![贝叶斯不确定性视图](img/image00278.jpeg)
    *的加权平均值。随着样本大小 *M* 的增加，样本均值的权重增加，而先验的权重减少。同样，后验精度（方差的倒数）是先验精度![贝叶斯不确定性视图](img/image00279.jpeg)和样本均值精度![贝叶斯不确定性视图](img/image00280.jpeg)的和：'
- en: '![Bayesian view of uncertainty](img/image00281.jpeg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯不确定性视图](img/image00281.jpeg)'
- en: As *M* increases, the contribution of precision from observations (evidence)
    outweighs that from the prior knowledge.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 *M* 的增加，来自观测（证据）的精度贡献超过了来自先验知识的贡献。
- en: 'Let''s take a concrete example where we consider age distribution with the
    population mean 5.5 and population standard deviation 0.5\. We sample 100 people
    from this population by using the following R script:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举一个具体的例子，其中我们考虑具有人口均值5.5和人口标准差0.5的年龄分布。我们使用以下R脚本来从这个总体中抽取100人：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can calculate the posterior distribution using the following R function:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下R函数来计算后验分布：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![Bayesian view of uncertainty](img/image00282.jpeg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯不确定性视图](img/image00282.jpeg)'
- en: One can see that as the number of samples increases, the estimated mean asymptotically
    approaches the population mean. The initial low value is due to the influence
    of the prior, which is, in this case, 5.0.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看到，随着样本数量的增加，估计的均值渐近地接近总体均值。初始的低值是由于先验的影响，在这种情况下，先验是 5.0。
- en: This simple and intuitive picture of how the prior knowledge and evidence from
    observations contribute to the overall model parameter estimate holds in any Bayesian
    inference. The precise mathematical expression for how they combine would be different.
    Therefore, one could start using a model for prediction with just prior information,
    either from the domain knowledge or the data collected in the past. Also, as new
    observations arrive, the model can be updated using the Bayesian scheme.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这个关于先验知识和观察证据如何贡献于整体模型参数估计的简单直观图景在任何贝叶斯推理中都成立。它们如何结合的精确数学表达式会有所不同。因此，一个人可以仅使用先验信息，无论是来自领域知识还是过去收集的数据，就开始使用模型进行预测。此外，随着新观察数据的到来，模型可以使用贝叶斯方案进行更新。
- en: Choosing the right prior distribution
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择合适的先验分布
- en: In the preceding simple example, we saw that if the likelihood function has
    the form of a normal distribution, and when the prior distribution is chosen as
    normal, the posterior also turns out to be a normal distribution. Also, we could
    get a closed-form analytical expression for the posterior mean. Since the posterior
    is obtained by multiplying the prior and likelihood functions and normalizing
    by integration over the parameter variables, the form of the prior distribution
    has a significant influence on the posterior. This section gives some more details
    about the different types of prior distributions and guidelines as to which ones
    to use in a given context.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的简单例子中，我们看到了如果似然函数具有正态分布的形式，并且当先验分布被选为正态分布时，后验分布也变成了正态分布。此外，我们还可以得到后验均值的封闭形式解析表达式。由于后验是通过将先验和似然函数相乘，并通过在参数变量上的积分进行归一化得到的，因此先验分布的形式对后验有显著影响。本节将给出有关不同类型先验分布的更多细节，以及在某些特定情境下如何选择先验分布的指导方针。
- en: There are different ways of classifying prior distributions in a formal way.
    One of the approaches is based on how much information a prior provides. In this
    scheme, the prior distributions are classified as *Informative*, *Weakly Informative*,
    *Least Informative*, and *Non-informative*. A detailed discussion of each of these
    classes is beyond the scope of this book, and interested readers should consult
    relevant books (references 1 and 2 in the *References* section of this chapter).
    Here, we take more of a practitioner's approach and illustrate some of the important
    classes of the prior distributions commonly used in practice.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同的方式以正式的方式对先验分布进行分类。其中一种方法是基于先验提供的信息量。在这个方案中，先验分布被分类为 *信息性先验*、*弱信息性先验*、*最少信息性先验*
    和 *非信息性先验*。对每个这些类别的详细讨论超出了本书的范围，感兴趣的读者应参考相关书籍（本章“参考文献”部分的参考1和2）。在这里，我们采取更多实践者的方法，并说明了在实践中最常用的先验分布的一些重要类别。
- en: Non-informative priors
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 非信息先验
- en: 'Let''s start with the case where we do not have any prior knowledge about the
    model parameters. In this case, we want to express complete ignorance about model
    parameters through a mathematical expression. This is achieved through what are
    called non-informative priors. For example, in the case of a single random variable
    *x* that can take any value between ![Non-informative priors](img/image00283.jpeg)
    and ![Non-informative priors](img/image00284.jpeg), the non-informative prior
    for its mean ![Non-informative priors](img/image00265.jpeg) would be the following:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从我们没有关于模型参数的任何先验知识的情况开始。在这种情况下，我们希望通过数学表达式表达对模型参数的完全无知。这是通过所谓的非信息先验实现的。例如，在单个随机变量
    *x* 可以取 ![非信息先验](img/image00283.jpeg) 和 ![非信息先验](img/image00284.jpeg) 之间任何值的情形下，其均值
    ![非信息先验](img/image00265.jpeg) 的非信息先验如下：
- en: '![Non-informative priors](img/image00285.jpeg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![非信息先验](img/image00285.jpeg)'
- en: Here, the complete ignorance of the parameter value is captured through a uniform
    distribution function in the parameter space. Note that a uniform distribution
    is not a proper distribution function since its integral over the domain is not
    equal to 1; therefore, it is not normalizable. However, one can use an improper
    distribution function for the prior as long as it is multiplied by the likelihood
    function; the resulting posterior can be normalized.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，参数值的完全无知通过参数空间中的均匀分布函数来捕捉。请注意，均匀分布不是一个合适的分布函数，因为其在域上的积分不等于1；因此，它不是可归一化的。然而，只要乘以似然函数，就可以使用不适当的先验分布函数；结果的后验可以归一化。
- en: 'If the parameter of interest is variance ![Non-informative priors](img/image00266.jpeg),
    then by definition it can only take non-negative values. In this case, we transform
    the variable so that the transformed variable has a uniform probability in the
    range from ![Non-informative priors](img/image00283.jpeg) to ![Non-informative
    priors](img/image00284.jpeg):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果感兴趣的参数是方差![非信息先验](img/image00266.jpeg)，那么根据定义，它只能取非负值。在这种情况下，我们变换变量，使得变换后的变量在![非信息先验](img/image00283.jpeg)到![非信息先验](img/image00284.jpeg)的范围内具有均匀概率：
- en: '![Non-informative priors](img/image00286.jpeg)![Non-informative priors](img/image00287.jpeg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![非信息先验](img/image00286.jpeg)![非信息先验](img/image00287.jpeg)'
- en: 'It is easy to show, using simple differential calculus, that the corresponding
    non-informative distribution function in the original variable ![Non-informative
    priors](img/image00266.jpeg) would be as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 使用简单的微分微积分很容易证明，在原始变量![非信息先验](img/image00266.jpeg)中对应的非信息分布函数如下：
- en: '![Non-informative priors](img/image00288.jpeg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![非信息先验](img/image00288.jpeg)'
- en: 'Another well-known non-informative prior used in practical applications is
    the Jeffreys prior, which is named after the British statistician Harold Jeffreys.
    This prior is invariant under reparametrization of ![Non-informative priors](img/image00289.jpeg)
    and is defined as proportional to the square root of the determinant of the Fisher
    information matrix:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中另一个著名的非信息先验是杰弗里斯先验，它是以英国统计学家哈罗德·杰弗里斯的名字命名的。这个先验在![非信息先验](img/image00289.jpeg)的重参数化下是不变的，并且定义为与费舍尔信息矩阵的行列式的平方根成比例：
- en: '![Non-informative priors](img/image00290.jpeg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![非信息先验](img/image00290.jpeg)'
- en: 'Here, it is worth discussing the Fisher information matrix a little bit. If
    *X* is a random variable distributed according to ![Non-informative priors](img/image00291.jpeg),
    we may like to know how much information observations of *X* carry about the unknown
    parameter ![Non-informative priors](img/image00289.jpeg). This is what the Fisher
    Information Matrix provides. It is defined as the second moment of the score (first
    derivative of the logarithm of the likelihood function):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，讨论一下费舍尔信息矩阵是值得的。如果*X*是一个按照![非信息先验](img/image00291.jpeg)分布的随机变量，我们可能想知道观察*X*携带了多少关于未知参数![非信息先验](img/image00289.jpeg)的信息。这正是费舍尔信息矩阵提供的内容。它定义为得分（似然函数对数的一阶导数）的第二矩：
- en: '![Non-informative priors](img/image00292.jpeg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![非信息先验](img/image00292.jpeg)'
- en: 'Let''s take a simple two-dimensional problem to understand the Fisher information
    matrix and Jeffreys prior. This example is given by Prof. D. Wittman of the University
    of California (reference 3 in the *References* section of this chapter). Let''s
    consider two types of food item: buns and hot dogs.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的二维问题来理解费舍尔信息矩阵和杰弗里斯先验。这个例子是由加州大学的D. Wittman教授给出的（本章“参考文献”部分的第3个参考文献）。让我们考虑两种食品：面包和热狗。
- en: 'Let''s assume that generally they are produced in pairs (a hot dog and bun
    pair), but occasionally hot dogs are also produced independently in a separate
    process. There are two observables such as the number of hot dogs (![Non-informative
    priors](img/image00293.jpeg)) and the number of buns (![Non-informative priors](img/image00294.jpeg)),
    and two model parameters such as the production rate of pairs (![Non-informative
    priors](img/image00295.jpeg)) and the production rate of hot dogs alone (![Non-informative
    priors](img/image00296.jpeg)). We assume that the uncertainty in the measurements
    of the counts of these two food products is distributed according to the normal
    distribution, with variance ![Non-informative priors](img/image00297.jpeg) and
    ![Non-informative priors](img/image00298.jpeg), respectively. In this case, the
    Fisher Information matrix for this problem would be as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 假设它们通常成对产生（热狗和面包对），但偶尔热狗也会在单独的过程中独立生产。有两个可观测量，如热狗的数量 (![非信息先验](img/image00293.jpeg))
    和面包的数量 (![非信息先验](img/image00294.jpeg))，以及两个模型参数，如成对的产量 (![非信息先验](img/image00295.jpeg))
    和单独热狗的产量 (![非信息先验](img/image00296.jpeg))。我们假设这两种食品产品数量的测量不确定性分别按照正态分布分布，方差分别为
    ![非信息先验](img/image00297.jpeg) 和 ![非信息先验](img/image00298.jpeg)。在这种情况下，该问题的费舍尔信息矩阵如下：
- en: '![Non-informative priors](img/image00299.jpeg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![非信息先验](img/image00299.jpeg)'
- en: 'In this case, the inverse of the Fisher information matrix would correspond
    to the covariance matrix:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，费舍尔信息矩阵的逆对应于协方差矩阵：
- en: '![Non-informative priors](img/image00300.jpeg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![非信息先验](img/image00300.jpeg)'
- en: We have included one problem in the *Exercises* section of this chapter to compute
    the Fisher information matrix and Jeffrey's prior. Readers are requested to attempt
    this in order to get a feeling of how to compute Jeffrey's prior from observations.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章的 *练习* 部分包含了一个问题，用于计算费舍尔信息矩阵和杰弗里先验。请读者尝试解决这个问题，以了解如何从观察结果中计算杰弗里先验。
- en: Subjective priors
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 主观先验
- en: One of the key strengths of Bayesian statistics compared to classical (frequentist)
    statistics is that the framework allows one to capture subjective beliefs about
    any random variables. Usually, people will have intuitive feelings about minimum,
    maximum, mean, and most probable or peak values of a random variable. For example,
    if one is interested in the distribution of hourly temperatures in winter in a
    tropical country, then the people who are familiar with tropical climates or climatology
    experts will have a belief that, in winter, the temperature can go as low as 15°C
    and as high as 27°C with the most probable temperature value being 23°C. This
    can be captured as a prior distribution through the Triangle distribution as shown
    here.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 与经典（频率主义）统计学相比，贝叶斯统计学的关键优势之一是框架允许人们捕捉关于任何随机变量的主观信念。通常，人们会对随机变量的最小值、最大值、平均值以及最可能或峰值有直观的感觉。例如，如果一个人对热带国家冬季每小时温度的分布感兴趣，那么熟悉热带气候或气候学专家将相信，在冬季，温度可以低至15°C，高至27°C，最可能的温度值是23°C。这可以通过三角分布作为先验分布来捕捉，如图所示。
- en: 'The Triangle distribution has three parameters corresponding to a minimum value
    (*a*), the most probable value (*b*), and a maximum value (*c*). The mean and
    variance of this distribution are given by:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 三角分布有三个参数，分别对应最小值 (*a*)、最可能值 (*b*) 和最大值 (*c*)。该分布的均值和方差如下给出：
- en: '![Subjective priors](img/image00301.jpeg)![Subjective priors](img/image00302.jpeg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![主观先验](img/image00301.jpeg)![主观先验](img/image00302.jpeg)'
- en: 'One can also use a PERT distribution to represent a subjective belief about
    the minimum, maximum, and most probable value of a random variable. The PERT distribution
    is a reparametrized Beta distribution, as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一个人还可以使用PERT分布来表示对随机变量的最小值、最大值和最可能值的信念。PERT分布是一个重新参数化的Beta分布，如下所示：
- en: '![Subjective priors](img/image00303.jpeg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![主观先验](img/image00303.jpeg)'
- en: 'Here:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里：
- en: '![Subjective priors](img/image00304.jpeg)![Subjective priors](img/image00305.jpeg)![Subjective
    priors](img/image00306.jpeg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![主观先验](img/image00304.jpeg)![主观先验](img/image00305.jpeg)![主观先验](img/image00306.jpeg)'
- en: The PERT distribution is commonly used for project completion time analysis,
    and the name originates from project evaluation and review techniques. Another
    area where Triangle and PERT distributions are commonly used is in **risk modeling**.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: PERT分布常用于项目完成时间分析，其名称来源于项目评估和审查技术。另一个经常使用三角形和PERT分布的领域是**风险管理**。
- en: 'Often, people also have a belief about the relative probabilities of values
    of a random variable. For example, when studying the distribution of ages in a
    population such as Japan or some European countries, where there are more old
    people than young, an expert could give relative weights for the probability of
    different ages in the populations. This can be captured through a relative distribution
    containing the following details:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，人们也会对随机变量值的相对概率有所信念。例如，当研究日本或某些欧洲国家等人口中老年人比年轻人多的年龄分布时，专家可以为不同年龄在人口中的概率给出相对权重。这可以通过包含以下详细信息的相对分布来捕捉：
- en: '![Subjective priors](img/image00307.jpeg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![主观先验](img/image00307.jpeg)'
- en: 'Here, *min* and *max* represent the minimum and maximum values, *{values}*
    represents the set of possible observed values, and *{weights}* represents their
    relative weights. For example, in the population age distribution problem, these
    could be the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*min* 和 *max* 分别代表最小值和最大值，*{values}* 代表可能观察到的值的集合，而 *{weights}* 代表它们的相对权重。例如，在人口年龄分布问题中，这些可能如下所示：
- en: '![Subjective priors](img/image00308.jpeg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![主观先验](img/image00308.jpeg)'
- en: The weights need not have a sum of 1.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 权重不需要总和为1。
- en: Conjugate priors
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 共轭先验
- en: 'If both the prior and posterior distributions are in the same family of distributions,
    then they are called **conjugate distributions** and the corresponding prior is
    called a **conjugate prior for the likelihood function**. Conjugate priors are
    very helpful for getting get analytical closed-form expressions for the posterior
    distribution. In the simple example we considered, we saw that when the noise
    is distributed according to the normal distribution, choosing a normal prior for
    the mean resulted in a normal posterior. The following table gives examples of
    some well-known conjugate pairs that we will use in the later chapters of this
    book:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果先验分布和后验分布属于同一分布族，那么它们被称为**共轭分布**，相应的先验被称为**似然函数的共轭先验**。共轭先验对于获取后验分布的解析封闭形式表达式非常有帮助。在我们考虑的简单例子中，我们看到了当噪声按照正态分布分布时，选择均值正态先验会导致后验正态分布。以下表格给出了我们将在这本书的后续章节中使用的某些著名共轭对的例子：
- en: '| Likelihood function | Model parameters | Conjugate prior | Hyperparameters
    |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 似然函数 | 模型参数 | 共轭先验 | 超参数 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Binomial | ![Conjugate priors](img/image00309.jpeg)(probability) | Beta |
    ![Conjugate priors](img/image00310.jpeg) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 二项分布 | ![共轭先验](img/image00309.jpeg)(概率) | Beta分布 | ![共轭先验](img/image00310.jpeg)
    |'
- en: '| Poisson | ![Conjugate priors](img/image00311.jpeg)(rate) | Gamma | ![Conjugate
    priors](img/image00312.jpeg) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 泊松分布 | ![共轭先验](img/image00311.jpeg)(速率) |伽马分布 | ![共轭先验](img/image00312.jpeg)
    |'
- en: '| Categorical | ![Conjugate priors](img/image00313.jpeg)(probability, number
    of categories) | Dirichlet | ![Conjugate priors](img/image00295.jpeg) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 分类变量 | ![共轭先验](img/image00313.jpeg)(概率，类别数量) | Dirichlet分布 | ![共轭先验](img/image00295.jpeg)
    |'
- en: '| Univariate normal (known variance ![Conjugate priors](img/image00266.jpeg))
    | ![Conjugate priors](img/image00265.jpeg)(mean) | Normal | ![Conjugate priors](img/image00314.jpeg)
    |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 单变量正态分布（已知方差 ![共轭先验](img/image00266.jpeg)） | ![共轭先验](img/image00265.jpeg)(均值)
    | 正态分布 | ![共轭先验](img/image00314.jpeg) |'
- en: '| Univariate normal (known mean ![Conjugate priors](img/image00265.jpeg)) |
    ![Conjugate priors](img/image00266.jpeg)(variance) | Inverse Gamma | ![Conjugate
    priors](img/image00310.jpeg) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 单变量正态分布（已知均值 ![共轭先验](img/image00265.jpeg)） | ![共轭先验](img/image00266.jpeg)(方差)
    | 逆伽马分布 | ![共轭先验](img/image00310.jpeg) |'
- en: Hierarchical priors
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 层次先验
- en: 'Sometimes, it is useful to define prior distributions for the hyperparameters
    itself. This is consistent with the Bayesian view that all parameters should be
    treated as uncertain by using probabilities. These distributions are called hyper-prior
    distributions. In theory, one can continue this into many levels as a hierarchical
    model. This is one way of eliciting the optimal prior distributions. For example:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，为超参数本身定义先验分布是有用的。这与贝叶斯观点一致，即所有参数都应通过使用概率被视为不确定。这些分布被称为超先验分布。在理论上，人们可以将此扩展到许多层次，作为一个层次模型。这是获取最优先验分布的一种方式。例如：
- en: '![Hierarchical priors](img/image00315.jpeg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![分层先验](img/image00315.jpeg)'
- en: '![Hierarchical priors](img/image00316.jpeg) is the prior distribution with
    a hyperparameter ![Hierarchical priors](img/image00295.jpeg). We could define
    a prior distribution for ![Hierarchical priors](img/image00295.jpeg) through a
    second set of equations, as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '![分层先验](img/image00316.jpeg)是具有超参数![分层先验](img/image00295.jpeg)的先验分布。我们可以通过第二组方程定义![分层先验](img/image00295.jpeg)的先验分布，如下所示：'
- en: '![Hierarchical priors](img/image00317.jpeg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![分层先验](img/image00317.jpeg)'
- en: Here, ![Hierarchical priors](img/image00318.jpeg) is the hyper-prior distribution
    for the hyperparameter ![Hierarchical priors](img/image00295.jpeg), parametrized
    by the hyper-hyper-parameter ![Hierarchical priors](img/image00298.jpeg). One
    can define a prior distribution for ![Hierarchical priors](img/image00298.jpeg)
    in the same way and continue the process forever. The practical reason for formalizing
    such models is that, at some level of hierarchy, one can define a uniform prior
    for the hyper parameters, reflecting complete ignorance about the parameter distribution,
    and effectively truncate the hierarchy. In practical situations, typically, this
    is done at the second level. This corresponds to, in the preceding example, using
    a uniform distribution for ![Hierarchical priors](img/image00318.jpeg).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![分层先验](img/image00318.jpeg)是超参数![分层先验](img/image00295.jpeg)的超先验分布，由超超参数![分层先验](img/image00298.jpeg)参数化。人们可以用相同的方式为![分层先验](img/image00298.jpeg)定义一个先验分布，并永远继续这个过程。将此类模型形式化的实际原因是，在某个层次级别上，人们可以为超参数定义一个均匀先验，反映对参数分布的完全无知，并有效地截断层次结构。在实际情况下，通常在第二级进行此操作。这对应于前一个示例中使用![分层先验](img/image00318.jpeg)的均匀分布。
- en: I want to conclude this section by stressing one important point. Though prior
    distribution has a significant role in Bayesian inference, one need not worry
    about it too much, as long as the prior chosen is reasonable and consistent with
    the domain knowledge and evidence seen so far. The reasons are is that, first
    of all, as we have more evidence, the significance of the prior gets washed out.
    Secondly, when we use Bayesian models for prediction, we will average over the
    uncertainty in the estimation of the parameters using the posterior distribution.
    *This averaging is the key ingredient of Bayesian inference and it removes many
    of the ambiguities in the selection of the right prior*.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我想通过强调一个重要观点来结束本节。尽管先验分布在贝叶斯推理中起着重要作用，但只要选择的先验是合理的，并与迄今为止看到的领域知识和证据一致，就无需过分担心。原因如下：首先，随着我们获得更多证据，先验的重要性会减弱。其次，当我们使用贝叶斯模型进行预测时，我们将使用后验分布对参数估计的不确定性进行平均。*这种平均是贝叶斯推理的关键成分，它消除了选择正确先验时许多的模糊性*。
- en: Estimation of posterior distribution
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 后验分布的估计
- en: So far, we discussed the essential concept behind Bayesian inference and also
    how to choose a prior distribution. Since one needs to compute the posterior distribution
    of model parameters before one can use the models for prediction, we discuss this
    task in this section. Though the Bayesian rule has a very simple-looking form,
    the computation of posterior distribution in a practically usable way is often
    very challenging. This is primarily because computation of the normalization constant
    ![Estimation of posterior distribution](img/image00259.jpeg) involves *N*-dimensional
    integrals, when there are *N* parameters. Even when one uses a conjugate prior,
    this computation can be very difficult to track analytically or numerically. This
    was one of the main reasons for not using Bayesian inference for multivariate
    modeling until recent decades. In this section, we will look at various approximate
    ways of computing posterior distributions that are used in practice.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了贝叶斯推理背后的基本概念以及如何选择先验分布。由于在使用模型进行预测之前，需要计算模型参数的后验分布，因此我们在这节中讨论这个任务。尽管贝叶斯规则看起来非常简单，但在实际可用的方式下计算后验分布通常非常具有挑战性。这主要是因为当有
    *N* 个参数时，计算归一化常数 ![后验分布的估计](img/image00259.jpeg) 涉及到 *N*-维积分。即使使用共轭先验，这种计算也可能非常难以分析或数值跟踪。这就是为什么直到最近几十年，人们不使用贝叶斯推理进行多元建模的主要原因之一。在本节中，我们将探讨在实践中使用的各种计算后验分布的近似方法。
- en: Maximum a posteriori estimation
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最大后验概率估计
- en: '**Maximum a posteriori** (**MAP**) estimation is a point estimation that corresponds
    to taking the maximum value or mode of the posterior distribution. Though taking
    a point estimation does not capture the variability in the parameter estimation,
    it does take into account the effect of prior distribution to some extent when
    compared to maximum likelihood estimation. MAP estimation is also called poor
    man''s Bayesian inference.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**最大后验概率**（**MAP**）估计是一种点估计，对应于取后验分布的最大值或众数。尽管取点估计不能捕捉参数估计中的变异性，但与最大似然估计相比，它在一定程度上考虑了先验分布的影响。MAP估计也被称为穷人的贝叶斯推理。'
- en: 'From the Bayes rule, we have:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 从贝叶斯规则，我们有：
- en: '![Maximum a posteriori estimation](img/image00319.jpeg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![最大后验概率估计](img/image00319.jpeg)'
- en: 'Here, for convenience, we have used the notation *X* for the *N*-dimensional
    vector ![Maximum a posteriori estimation](img/image00320.jpeg). The last relation
    follows because the denominator of RHS of Bayes rule is independent of ![Maximum
    a posteriori estimation](img/image00253.jpeg). Compare this with the following
    maximum likelihood estimate:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，为了方便起见，我们使用了 *X* 表示 *N*-维向量 ![最大后验概率估计](img/image00320.jpeg)。最后一个关系成立是因为贝叶斯规则右侧分母与
    ![最大后验概率估计](img/image00253.jpeg) 无关。将此与以下最大似然估计进行比较：
- en: '![Maximum a posteriori estimation](img/image00321.jpeg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![最大后验概率估计](img/image00321.jpeg)'
- en: The difference between the MAP and ML estimate is that, whereas ML finds the
    mode of the likelihood function, MAP finds the mode of the product of the likelihood
    function and prior.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: MAP估计和ML估计之间的区别在于，ML找到似然函数的众数，而MAP找到似然函数与先验的乘积的众数。
- en: Laplace approximation
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 拉普拉斯近似
- en: 'We saw that the MAP estimate just finds the maximum value of the posterior
    distribution. Laplace approximation goes one step further and also computes the
    local curvature around the maximum up to quadratic terms. This is equivalent to
    assuming that the posterior distribution is approximately Gaussian (normal) around
    the maximum. This would be the case if the amount of data were large compared
    to the number of parameters: *M >> N*.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，最大后验概率估计（MAP）只是找到后验分布的最大值。拉普拉斯近似更进一步，并计算最大值周围的局部曲率，直到二次项。这相当于假设后验分布在大数据量相对于参数数量时近似为高斯（正态）分布：*M
    >> N*。
- en: '![Laplace approximation](img/image00322.jpeg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![拉普拉斯近似](img/image00322.jpeg)'
- en: 'Here, *A* is an *N x N* Hessian matrix obtained by taking the derivative of
    the log of the posterior distribution:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*A* 是通过取后验分布对数的导数得到的 *N x N* 赫essian 矩阵：
- en: '![Laplace approximation](img/image00323.jpeg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![拉普拉斯近似](img/image00323.jpeg)'
- en: 'It is straightforward to evaluate the previous expressions at ![Laplace approximation](img/image00324.jpeg),
    using the following definition of conditional probability:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下条件概率的定义，可以简单地评估前面的表达式在 ![拉普拉斯近似](img/image00324.jpeg)：
- en: '![Laplace approximation](img/image00325.jpeg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![拉普拉斯近似](img/image00325.jpeg)'
- en: 'We can get an expression for *P(X|m)* from Laplace approximation that looks
    like the following:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从拉普拉斯近似中得到一个关于 *P(X|m)* 的表达式，其形式如下：
- en: '![Laplace approximation](img/image00326.jpeg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![拉普拉斯近似](img/image00326.jpeg)'
- en: 'In the limit of a large number of samples, one can show that this expression
    simplifies to the following:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在大量样本的极限情况下，可以证明这个表达式可以简化为以下形式：
- en: '![Laplace approximation](img/image00327.jpeg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![拉普拉斯近似](img/image00327.jpeg)'
- en: The term ![Laplace approximation](img/image00328.jpeg) is called **Bayesian
    information criterion** (**BIC**) and can be used for model selections or model
    comparison. This is one of the **goodness of fit** terms for a statistical model.
    Another similar criterion that is commonly used is **Akaike information criterion**
    (**AIC**), which is defined by ![Laplace approximation](img/image00329.jpeg).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 术语 ![拉普拉斯近似](img/image00328.jpeg) 被称为**贝叶斯信息准则**（**BIC**），可用于模型选择或模型比较。这是统计模型的一个**拟合优度**术语。另一个常用的类似标准是**赤池信息准则**（**AIC**），其定义为
    ![拉普拉斯近似](img/image00329.jpeg)。
- en: 'Now we will discuss how BIC can be used to compare different models for model
    selection. In the Bayesian framework, two models such as ![Laplace approximation](img/image00330.jpeg)
    and ![Laplace approximation](img/image00331.jpeg) are compared using the Bayes
    factor. The definition of the Bayes factor ![Laplace approximation](img/image00332.jpeg)
    is the ratio of posterior odds to prior odds that is given by:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将讨论如何使用BIC来比较不同的模型以进行模型选择。在贝叶斯框架中，使用贝叶斯因子比较两个模型，如 ![拉普拉斯近似](img/image00330.jpeg)
    和 ![拉普拉斯近似](img/image00331.jpeg)。贝叶斯因子的定义 ![拉普拉斯近似](img/image00332.jpeg) 是后验优势与先验优势之比，由以下给出：
- en: '![Laplace approximation](img/image00333.jpeg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![拉普拉斯近似](img/image00333.jpeg)'
- en: Here, posterior odds is the ratio of posterior probabilities of the two models
    of the given data and prior odds is the ratio of prior probabilities of the two
    models, as given in the preceding equation. If ![Laplace approximation](img/image00334.jpeg),
    model ![Laplace approximation](img/image00330.jpeg) is preferred by the data and
    if ![Laplace approximation](img/image00335.jpeg), model ![Laplace approximation](img/image00331.jpeg)
    is preferred by the data.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，后验优势是给定数据的两个模型的后验概率之比，先验优势是两个模型的先验概率之比，如前一个方程所示。如果 ![拉普拉斯近似](img/image00334.jpeg)，数据更倾向于模型
    ![拉普拉斯近似](img/image00330.jpeg)；如果 ![拉普拉斯近似](img/image00335.jpeg)，数据更倾向于模型 ![拉普拉斯近似](img/image00331.jpeg)。
- en: In reality, it is difficult to compute the Bayes factor because it is difficult
    to get the precise prior probabilities. It can be shown that, in the large *N*
    limit, ![Laplace approximation](img/image00336.jpeg) can be viewed as a rough
    approximation to ![Laplace approximation](img/image00337.jpeg).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实中，计算贝叶斯因子是困难的，因为很难得到精确的先验概率。可以证明，在大的 *N* 极限下，![拉普拉斯近似](img/image00336.jpeg)
    可以被视为 ![拉普拉斯近似](img/image00337.jpeg) 的粗略近似。
- en: Monte Carlo simulations
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 蒙特卡洛模拟
- en: The two approximations that we have discussed so far, the MAP and Laplace approximations,
    are useful when the posterior is a very sharply peaked function about the maximum
    value. Often, in real-life situations, the posterior will have long tails. This
    is, for example, the case in e-commerce where the probability of the purchasing
    of a product by a user has a long tail in the space of all products. So, in many
    practical situations, both MAP and Laplace approximations fail to give good results.
    Another approach is to directly sample from the posterior distribution. Monte
    Carlo simulation is a technique used for sampling from the posterior distribution
    and is one of the workhorses of Bayesian inference in practical applications.
    In this section, we will introduce the reader to **Markov Chain Monte Carlo**
    (**MCMC**) simulations and also discuss two common MCMC methods used in practice.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今为止讨论的两个近似，即最大似然估计（MAP）和拉普拉斯近似，当后验概率在最大值附近有一个非常尖锐的峰值时是有用的。在现实生活中的许多情况下，后验概率通常会有长尾。例如，在电子商务中，用户购买产品的概率在所有产品空间中具有长尾。因此，在许多实际情况下，MAP和拉普拉斯近似都无法给出良好的结果。另一种方法是直接从后验分布中进行采样。蒙特卡洛模拟是一种用于从后验分布中采样的技术，是实际应用中贝叶斯推理的得力工具。在本节中，我们将向读者介绍**马尔可夫链蒙特卡洛**（**MCMC**）模拟，并讨论实践中常用的两种MCMC方法。
- en: As discussed earlier, let ![Monte Carlo simulations](img/image00338.jpeg) be
    the set of parameters that we are interested in estimating from the data through
    posterior distribution. Consider the case of the parameters being discrete, where
    each parameter has *K* possible values, that is, ![Monte Carlo simulations](img/image00339.jpeg).
    Set up a Markov process with states ![Monte Carlo simulations](img/image00253.jpeg)
    and transition probability matrix ![Monte Carlo simulations](img/image00340.jpeg).
    The essential idea behind MCMC simulations is that one can choose the transition
    probabilities in such a way that the steady state distribution of the Markov chain
    would correspond to the posterior distribution we are interested in. Once this
    is done, sampling from the Markov chain output, after it has reached a steady
    state, will give samples of ![Monte Carlo simulations](img/image00341.jpeg) distributed
    according to the posterior distribution.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，让![蒙特卡洛模拟](img/image00338.jpeg)是我们通过后验分布从数据中感兴趣估计的参数集。考虑参数是离散的情况，其中每个参数有*K*个可能的值，即![蒙特卡洛模拟](img/image00339.jpeg)。设置一个具有状态![蒙特卡洛模拟](img/image00253.jpeg)和转移概率矩阵![蒙特卡洛模拟](img/image00340.jpeg)的马尔可夫过程。MCMC模拟背后的基本思想是，可以选择转移概率，使得马尔可夫链的稳态分布将对应于我们感兴趣的后续分布。一旦这样做，在马尔可夫链达到稳态后，从马尔可夫链输出中采样将给出![蒙特卡洛模拟](img/image00341.jpeg)的样本，这些样本遵循后验分布。
- en: Now, the question is how to set up the Markov process in such a way that its
    steady state distribution corresponds to the posterior of interest. There are
    two well-known methods for this. One is the Metropolis-Hastings algorithm and
    the second is Gibbs sampling. We will discuss both in some detail here.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，问题是如何设置马尔可夫过程，使其稳态分布对应于感兴趣的后续分布。为此有两种众所周知的方法。一种是Metropolis-Hastings算法，另一种是Gibbs抽样。我们将在下面详细讨论这两种方法。
- en: The Metropolis-Hasting algorithm
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Metropolis-Hasting算法
- en: 'The Metropolis-Hasting algorithm was one of the first major algorithms proposed
    for MCMC (reference 4 in the *References* section of this chapter). It has a very
    simple concept—something similar to a hill-climbing algorithm in optimization:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Metropolis-Hasting算法是第一个被提出的用于MCMC（本章“参考文献”部分的第4个参考文献）的主要算法之一。它有一个非常简单的概念——类似于优化中的爬山算法：
- en: Let ![The Metropolis-Hasting algorithm](img/image00342.jpeg) be the state of
    the system at time step *t*.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让![Metropolis-Hasting算法](img/image00342.jpeg)表示时间步长*t*时系统的状态。
- en: To move the system to another state at time step *t + 1*, generate a candidate
    state ![The Metropolis-Hasting algorithm](img/image00343.jpeg) by sampling from
    a proposal distribution ![The Metropolis-Hasting algorithm](img/image00344.jpeg).
    The proposal distribution is chosen in such a way that it is easy to sample from
    it.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要在时间步长*t + 1*时将系统移动到另一个状态，通过从提议分布![Metropolis-Hasting算法](img/image00344.jpeg)中采样生成一个候选状态![Metropolis-Hasting算法](img/image00343.jpeg)。提议分布被选择得易于从中采样。
- en: Accept the proposal move with the following probability:![The Metropolis-Hasting
    algorithm](img/image00345.jpeg)
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以以下概率接受提议的移动：![Metropolis-Hasting算法](img/image00345.jpeg)
- en: If it is accepted, ![The Metropolis-Hasting algorithm](img/image00346.jpeg)
    = ![The Metropolis-Hasting algorithm](img/image00343.jpeg); if not, ![The Metropolis-Hasting
    algorithm](img/image00347.jpeg).
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果被接受，![Metropolis-Hasting算法](img/image00346.jpeg) = ![Metropolis-Hasting算法](img/image00343.jpeg)；如果不接受，![Metropolis-Hasting算法](img/image00347.jpeg)。
- en: Continue the process until the distribution converges to the steady state.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续这个过程，直到分布收敛到稳态。
- en: Here, ![The Metropolis-Hasting algorithm](img/image00348.jpeg) is the posterior
    distribution that we want to simulate. Under certain conditions, the preceding
    update rule will guarantee that, in the large time limit, the Markov process will
    approach a steady state distributed according to ![The Metropolis-Hasting algorithm](img/image00348.jpeg).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![Metropolis-Hasting算法](img/image00348.jpeg)是我们想要模拟的后验分布。在特定条件下，前面的更新规则将保证，在长时间极限下，马尔可夫过程将趋近于![Metropolis-Hasting算法](img/image00348.jpeg)的稳态分布。
- en: The intuition behind the Metropolis-Hasting algorithm is simple. The proposal
    distribution ![The Metropolis-Hasting algorithm](img/image00344.jpeg) gives the
    conditional probability of proposing state ![The Metropolis-Hasting algorithm](img/image00343.jpeg)
    to make a transition in the next time step from the current state ![The Metropolis-Hasting
    algorithm](img/image00253.jpeg). Therefore, ![The Metropolis-Hasting algorithm](img/image00349.jpeg)
    is the probability that the system is currently in state ![The Metropolis-Hasting
    algorithm](img/image00343.jpeg) and would make a transition to state ![The Metropolis-Hasting
    algorithm](img/image00253.jpeg) in the next time step. Similarly, ![The Metropolis-Hasting
    algorithm](img/image00350.jpeg) is the probability that the system is currently
    in state ![The Metropolis-Hasting algorithm](img/image00253.jpeg) and would make
    a transition to state ![The Metropolis-Hasting algorithm](img/image00343.jpeg)
    in the next time step. If the ratio of these two probabilities is more than 1,
    accept the move. Alternatively, accept the move only with the probability given
    by the ratio. Therefore, the Metropolis-Hasting algorithm is like a hill-climbing
    algorithm where one accepts all the moves that are in the upward direction and
    accepts moves in the downward direction once in a while with a smaller probability.
    The downward moves help the system not to get stuck in local minima.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Metropolis-Hasting算法背后的直觉很简单。提议分布![Metropolis-Hasting算法](img/image00344.jpeg)给出了从当前状态![Metropolis-Hasting算法](img/image00253.jpeg)在下一个时间步过渡到状态![Metropolis-Hasting算法](img/image00343.jpeg)的条件概率。因此，![Metropolis-Hasting算法](img/image00349.jpeg)是系统当前处于状态![Metropolis-Hasting算法](img/image00343.jpeg)并且会在下一个时间步过渡到状态![Metropolis-Hasting算法](img/image00253.jpeg)的概率。同样，![Metropolis-Hasting算法](img/image00350.jpeg)是系统当前处于状态![Metropolis-Hasting算法](img/image00253.jpeg)并且会在下一个时间步过渡到状态![Metropolis-Hasting算法](img/image00343.jpeg)的概率。如果这两个概率的比率大于1，则接受移动。或者，仅以比率给出的概率接受移动。因此，Metropolis-Hasting算法类似于一种爬山算法，其中接受所有向上的移动，并且偶尔以较小的概率接受向下的移动。向下的移动有助于系统不陷入局部最小值。
- en: 'Let''s revisit the example of estimating the posterior distribution of the
    mean and variance of the height of people in a population discussed in the introductory
    section. This time we will estimate the posterior distribution by using the Metropolis-Hasting
    algorithm. The following lines of R code do this job:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新回顾一下在引言部分讨论的估计人群中身高均值和方差后验分布的例子。这次我们将使用Metropolis-Hasting算法来估计后验分布。以下R代码行完成这项工作：
- en: '[PRE2]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To plot the resulting posterior distribution, we use the sm package in R:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 要绘制得到的后验分布，我们使用R中的sm包：
- en: '[PRE3]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The resulting posterior distribution will look like the following figure:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 得到的后验分布将类似于以下图示：
- en: '![The Metropolis-Hasting algorithm](img/image00351.jpeg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![Metropolis-Hasting算法](img/image00351.jpeg)'
- en: Though the Metropolis-Hasting algorithm is simple to implement for any Bayesian
    inference problem, in practice it may not be very efficient in many cases. The
    main reason for this is that, unless one carefully chooses a proposal distribution
    ![The Metropolis-Hasting algorithm](img/image00344.jpeg), there would be too many
    rejections and it would take a large number of updates to reach the steady state.
    This is particularly the case when the number of parameters are high. There are
    various modifications of the basic Metropolis-Hasting algorithms that try to overcome
    these difficulties. We will briefly describe these when we discuss various R packages
    for the Metropolis-Hasting algorithm in the following section.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Metropolis-Hasting算法对于任何贝叶斯推断问题来说实现起来都很简单，但在实践中，它在许多情况下可能并不非常高效。主要原因在于，除非一个人仔细选择一个提议分布![Metropolis-Hasting算法](img/image00344.jpeg)，否则会有太多的拒绝，并且需要大量的更新才能达到稳态。当参数数量较高时，这种情况尤为明显。有各种基本Metropolis-Hasting算法的修改版本，试图克服这些困难。在下一节讨论各种R包的Metropolis-Hasting算法时，我们将简要介绍这些修改。
- en: R packages for the Metropolis-Hasting algorithm
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Metropolis-Hasting算法的R包
- en: There are several contributed packages in R for MCMC simulation using the Metropolis-Hasting
    algorithm, and here we describe some popular ones.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: R中存在多个用于Metropolis-Hasting算法的MCMC模拟的贡献包，在此我们将介绍一些流行的包。
- en: The **mcmc** package contributed by Charles J. Geyer and Leif T. Johnson is
    one of the popular packages in R for MCMC simulations. It has the `metrop` function
    for running the basic Metropolis-Hasting algorithm. The `metrop` function uses
    a multivariate normal distribution as the proposal distribution.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 由 Charles J. Geyer 和 Leif T. Johnson 贡献的 **mcmc** 包是 R 中用于 MCMC 模拟的流行包之一。它包含一个名为
    `metrop` 的函数，用于运行基本的 Metropolis-Hasting 算法。`metrop` 函数使用多元正态分布作为提议分布。
- en: Sometimes, it is useful to make a variable transformation to improve the speed
    of convergence in MCMC. The mcmc package has a function named `morph` for doing
    this. Combining these two, the function `morph.metrop` first transforms the variable,
    does a Metropolis on the transformed density, and converts the results back to
    the original variable.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，进行变量变换以改善 MCMC 收敛速度是有用的。mcmc 包有一个名为 `morph` 的函数用于此目的。结合这两个函数，`morph.metrop`
    函数首先对变量进行变换，然后在变换后的密度上进行 Metropolis 采样，并将结果转换回原始变量。
- en: Apart from the mcmc package, two other useful packages in R are **MHadaptive**
    contributed by Corey Chivers and the **Evolutionary Monte Carlo** (**EMC**) **algorithm**
    package by Gopi Goswami. Due to lack of space, we will not be discussing these
    two packages in this book. Interested readers are requested to download these
    from the C-RAN project's site and experiment with them.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 mcmc 包之外，R 中还有两个有用的包：由 Corey Chivers 贡献的 **MHadaptive** 包和 Gopi Goswami 的
    **进化蒙特卡洛**（**EMC**）**算法**包。由于篇幅限制，我们不会在本书中讨论这两个包。有兴趣的读者请从 C-RAN 项目网站下载这些包并尝试使用它们。
- en: Gibbs sampling
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吉布斯采样
- en: As mentioned before, the Metropolis-Hasting algorithm suffers from the drawback
    of poor convergence, due to too many rejections, if one does not choose a good
    proposal distribution. To avoid this problem, two physicists Stuart Geman and
    Donald Geman proposed a new algorithm (reference 5 in the *References* section
    of this chapter). This algorithm is called Gibbs sampling and it is named after
    the famous physicist J W Gibbs. Currently, Gibbs sampling is the workhorse of
    MCMC for Bayesian inference.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Metropolis-Hasting 算法由于过多的拒绝而存在收敛性差的缺点，如果不选择一个好的提议分布。为了避免这个问题，两位物理学家 Stuart
    Geman 和 Donald Geman 提出了一种新的算法（本章“参考文献”部分的第 5 个参考文献）。这个算法被称为吉布斯采样，并以著名的物理学家 J
    W Gibbs 命名。目前，吉布斯采样是贝叶斯推理中 MCMC 的主要工具。
- en: 'Let ![Gibbs sampling](img/image00352.jpeg) be the set of parameters of the
    model that we wish to estimate:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 设 ![吉布斯采样](img/image00352.jpeg) 为我们希望估计的模型参数集合：
- en: Start with an initial state ![Gibbs sampling](img/image00353.jpeg).
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从一个初始状态 ![吉布斯采样](img/image00353.jpeg) 开始。
- en: At each time step, update the components one by one, by drawing from a distribution
    conditional on the most recent value of rest of the components:![Gibbs sampling](img/image00354.jpeg)
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个时间步，逐个更新组件，通过从基于其他组件最近值的分布中进行抽样：![吉布斯采样](img/image00354.jpeg)
- en: After *N* steps, all components of the parameter will be updated.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 经过 *N* 步后，参数的所有组件都将被更新。
- en: Continue with step 2 until the Markov process converges to a steady state.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续进行步骤 2，直到马尔可夫过程收敛到稳态。
- en: Gibbs sampling is a very efficient algorithm since there are no rejections.
    However, to be able to use Gibbs sampling, the form of the conditional distributions
    of the posterior distribution should be known.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 吉布斯采样是一个非常有效的算法，因为没有拒绝。然而，要使用吉布斯采样，必须知道后验分布的条件分布形式。
- en: R packages for Gibbs sampling
  id: totrans-145
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 吉布斯采样的 R 包
- en: Unfortunately, there are not many contributed general purpose Gibbs sampling
    packages in R. The **gibbs.met** package provides two generic functions for performing
    MCMC in a Naïve way for user-defined target distribution. The first function is
    `gibbs_met`. This performs Gibbs sampling with each 1-dimensional distribution
    sampled by using the Metropolis algorithm, with normal distribution as the proposal
    distribution. The second function, `met_gaussian`, updates the whole state with
    independent normal distribution centered around the previous state. The gibbs.met
    package is useful for general purpose MCMC on moderate dimensional problems.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，在 R 中，贡献的通用吉布斯采样包并不多。**gibbs.met** 包提供了两个通用函数，用于以朴素的方式为用户定义的目标分布执行 MCMC。第一个函数是
    `gibbs_met`。该函数使用 Metropolis 算法对每个一维分布进行采样，提议分布为正态分布。第二个函数 `met_gaussian` 使用独立正态分布更新整个状态，其中心位于前一个状态。gibbs.met
    包对于通用 MCMC 在中等维度问题上是很有用的。
- en: In the *Exercises* section of this chapter, we will discuss one problem that
    involves sampling from the two-dimensional normal distribution by using both the
    Metropolis-Hasting algorithm and Gibbs sampling to make these concepts more clear.
    Readers can use these mentioned packages for solving this exercise.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的*练习*部分，我们将讨论一个涉及通过使用Metropolis-Hasting算法和Gibbs抽样从二维正态分布中采样的问题，以使这些概念更加清晰。读者可以使用这些提到的包来解决这个练习。
- en: Apart from the general purpose MCMC packages, there are several packages in
    R designed to solve a particular type of machine-learning problems. The **GibbsACOV**
    package can be used for one-way mixed-effects ANOVA and ANCOVA models. The **lda**
    package performs collapsed Gibbs sampling methods for topic (LDA) models. The
    **stocc** package fits a spatial occupancy model via Gibbs sampling. The **binomlogit**
    package implements an efficient MCMC for Binomial Logit models. **Bmk** is a package
    for doing diagnostics of MCMC output. **Bayesian Output Analysis Program** (**BOA**)
    is another similar package. **RBugs** is an interface of the well-known **OpenBUGS**
    MCMC package. The **ggmcmc** package is a graphical tool for analyzing MCMC simulation.
    **MCMCglm** is a package for generalized linear mixed models and **BoomSpikeSlab**
    is a package for doing MCMC for Spike and Slab regression. Finally, **SamplerCompare**
    is a package (more of a framework) for comparing the performance of various MCMC
    packages.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通用的MCMC包之外，R中还有几个包专门用于解决特定类型的机器学习问题。**GibbsACOV**包可用于单因素混合效应方差分析（ANOVA）和方差分析协方差（ANCOVA）模型。**lda**包执行主题（LDA）模型的折叠Gibbs抽样方法。**stocc**包通过Gibbs抽样拟合空间占用模型。**binomlogit**包实现了二项Logit模型的效率MCMC。**Bmk**是一个用于进行MCMC输出的诊断的包。**Bayesian
    Output Analysis Program**（**BOA**）是另一个类似的包。**RBugs**是知名**OpenBUGS** MCMC包的接口。**ggmcmc**包是分析MCMC模拟的图形工具。**MCMCglm**是一个用于广义线性混合模型的包，**BoomSpikeSlab**是一个用于进行Spike和Slab回归的MCMC的包。最后，**SamplerCompare**是一个用于比较各种MCMC包性能的包（更多是一个框架）。
- en: Variational approximation
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 变分近似
- en: 'In the variational approximation scheme, one assumes that the posterior distribution
    ![Variational approximation](img/image00355.jpeg) can be approximated to a factorized
    form:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在变分近似方案中，假设后验分布![变分近似](img/image00355.jpeg)可以被近似为因式分解形式：
- en: '![Variational approximation](img/image00356.jpeg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![变分近似](img/image00356.jpeg)'
- en: Note that the factorized form is also a conditional distribution, so each ![Variational
    approximation](img/image00357.jpeg) can have dependence on other ![Variational
    approximation](img/image00358.jpeg)s through the conditioned variable *X*. In
    other words, this is not a trivial factorization making each parameter independent.
    The advantage of this factorization is that one can choose more analytically tractable
    forms of distribution functions ![Variational approximation](img/image00359.jpeg).
    In fact, one can vary the functions ![Variational approximation](img/image00360.jpeg)
    in such a way that it is as close to the true posterior ![Variational approximation](img/image00355.jpeg)
    as possible. This is mathematically formulated as a **variational calculus** problem,
    as explained here.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，因式分解形式也是一种条件分布，因此每个![变分近似](img/image00357.jpeg)都可以通过条件变量*X*依赖于其他![变分近似](img/image00358.jpeg)。换句话说，这不是一个简单的因式分解，使得每个参数相互独立。这种因式分解的优势在于可以选择更易于分析的分布函数形式![变分近似](img/image00359.jpeg)。实际上，可以通过调整函数![变分近似](img/image00360.jpeg)，使其尽可能接近真实的后验![变分近似](img/image00355.jpeg)。这在数学上被表述为一个**变分法**问题，如这里所述。
- en: 'Let''s use some measures to compute the distance between the two probability
    distributions, such as ![Variational approximation](img/image00361.jpeg) and ![Variational
    approximation](img/image00362.jpeg), where ![Variational approximation](img/image00352.jpeg).
    One of the standard measures of distance between probability distributions is
    the Kullback-Leibler divergence, or KL-divergence for short. It is defined as
    follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用一些度量来计算两个概率分布之间的距离，例如![变分近似](img/image00361.jpeg)和![变分近似](img/image00362.jpeg)，其中![变分近似](img/image00352.jpeg)。概率分布之间距离的一个标准度量是Kullback-Leibler散度，或简称KL散度。它被定义为以下：
- en: '![Variational approximation](img/image00363.jpeg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![变分近似](img/image00363.jpeg)'
- en: 'The reason why it is called a divergence and not distance is that ![Variational
    approximation](img/image00364.jpeg) is not symmetric with respect to *Q* and *P*.
    One can use the relation ![Variational approximation](img/image00365.jpeg) and
    rewrite the preceding expression as an equation for *log P(X)*:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 它被称为散度而不是距离的原因是![变分近似](img/image00364.jpeg)在*Q*和*P*上不对称。可以使用关系![变分近似](img/image00365.jpeg)并将前面的表达式重写为*log
    P(X)*的方程：
- en: '![Variational approximation](img/image00366.jpeg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![变分近似](img/image00366.jpeg)'
- en: 'Here:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这里：
- en: '![Variational approximation](img/image00367.jpeg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![变分近似](img/image00367.jpeg)'
- en: Note that, in the equation for *ln P(X)*, there is no dependence on *Q* on the
    LHS. Therefore, maximizing ![Variational approximation](img/image00368.jpeg) with
    respect to *Q* will minimize ![Variational approximation](img/image00364.jpeg),
    since their sum is a term independent of *Q*. By choosing analytically tractable
    functions for *Q*, one can do this maximization in practice. It will result in
    both an approximation for the posterior and a lower bound for *ln P(X)* that is
    the logarithm of evidence or marginal likelihood, since ![Variational approximation](img/image00369.jpeg).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在*ln P(X)*的方程中，左侧对*Q*没有依赖。因此，相对于*Q*最大化![变分近似](img/image00368.jpeg)将最小化![变分近似](img/image00364.jpeg)，因为它们的和是一个与*Q*无关的项。通过选择对*Q*进行解析处理的函数，可以在实践中进行这种最大化。这将导致后验分布的一个近似以及*ln
    P(X)*的下界，即证据的对数或边缘似然，因为![变分近似](img/image00369.jpeg)。
- en: Therefore, variational approximation gives us two quantities in one shot. A
    posterior distribution can be used to make predictions about future observations
    (as explained in the next section) and a lower bound for evidence can be used
    for model selection.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，变分近似一次给出了两个量。后验分布可以用来对未来观测进行预测（如下一节所述），而证据的下界可以用于模型选择。
- en: 'How does one implement this minimization of KL-divergence in practice? Without
    going into mathematical details, here we write a final expression for the solution:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如何在实际中实现KL散度的最小化？不深入数学细节，我们在这里给出解的最终表达式：
- en: '![Variational approximation](img/image00370.jpeg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![变分近似](img/image00370.jpeg)'
- en: Here, ![Variational approximation](img/image00371.jpeg) implies that the expectation
    of the logarithm of the joint distribution ![Variational approximation](img/image00372.jpeg)
    is taken over all the parameters ![Variational approximation](img/image00373.jpeg)
    except for ![Variational approximation](img/image00358.jpeg). Therefore, the minimization
    of KL-divergence leads to a set of coupled equations; one for each ![Variational
    approximation](img/image00374.jpeg) needs to be solved self-consistently to obtain
    the final solution. Though the variational approximation looks very complex mathematically,
    it has a very simple, intuitive explanation. The posterior distribution of each
    parameter ![Variational approximation](img/image00357.jpeg) is obtained by averaging
    the log of the joint distribution over all the other variables. This is analogous
    to the Mean Field theory in physics where, if there are *N* interacting charged
    particles, the system can be approximated by saying that each particle is in a
    constant external field, which is the average of fields produced by all the other
    particles.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![变分近似](img/image00371.jpeg)意味着对联合分布![变分近似](img/image00372.jpeg)的对数期望是在所有参数![变分近似](img/image00373.jpeg)上取的，除了![变分近似](img/image00358.jpeg)。因此，KL散度的最小化导致了一组耦合方程；每个![变分近似](img/image00374.jpeg)都需要自洽地求解以获得最终解。尽管变分近似在数学上看起来非常复杂，但它有一个非常简单、直观的解释。每个参数![变分近似](img/image00357.jpeg)的后验分布是通过对所有其他变量的联合分布的对数进行平均得到的。这在物理学中类似于平均场理论，其中，如果有*N*个相互作用的带电粒子，系统可以通过说每个粒子处于一个由所有其他粒子产生的场的平均值所构成的恒定外部场来近似。
- en: We will end this section by mentioning a few R packages for variational approximation.
    The **VBmix** package can be used for variational approximation in Bayesian mixture
    models. A similar package is **vbdm** used for Bayesian discrete mixture models.
    The package **vbsr** is used for variational inference in Spike Regression Regularized
    Linear Models.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过提及一些用于变分近似的R包来结束本节。**VBmix**包可用于贝叶斯混合模型中的变分近似。一个类似的包是**vbdm**，用于贝叶斯离散混合模型。**vbsr**包用于Spike回归正则化线性模型中的变分推理。
- en: Prediction of future observations
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 未来观测的预测
- en: 'Once we have the posterior distribution inferred from data using some of the
    methods described already, it can be used to predict future observations. The
    probability of observing a value *Y*, given observed data *X*, and posterior distribution
    of parameters ![Prediction of future observations](img/image00375.jpeg) is given
    by:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们使用已经描述的一些方法从数据中推断出后验分布，就可以用它来预测未来的观测值。给定观察到的数据*X*和参数的后验分布![未来观测预测](img/image00375.jpeg)，观察到值*Y*的概率由以下公式给出：
- en: '![Prediction of future observations](img/image00376.jpeg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![未来观测预测](img/image00376.jpeg)'
- en: Note that, in this expression, the likelihood function ![Prediction of future
    observations](img/image00377.jpeg) is averaged by using the distribution of the
    parameter given by the posterior ![Prediction of future observations](img/image00375.jpeg).
    This is, in fact, the core strength of the Bayesian inference. This Bayesian averaging
    eliminates the uncertainty in estimating the parameter values and makes the prediction
    more robust.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这个表达式中，似然函数![未来观测预测](img/image00377.jpeg)是通过后验![未来观测预测](img/image00375.jpeg)给出的参数分布进行平均的。这实际上是贝叶斯推理的核心优势。这种贝叶斯平均消除了估计参数值的不确定性，并使预测更加稳健。
- en: Exercises
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Derive the equation for the posterior mean by expanding the square in the exponential
    ![Exercises](img/image00378.jpeg) for each *i*, collecting all similar power terms,
    and making a perfect square again. Note that the product of exponentials can be
    written as the exponential of a sum of terms.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过展开指数![练习](img/image00378.jpeg)中的平方，对每个*i*收集所有类似的幂次项，并再次使其成为完全平方，推导出后验均值的方程。注意，指数的乘积可以写成求和项的指数。
- en: For this exercise, we use the dataset corresponding to Smartphone-Based Recognition
    of Human Activities and Postural Transitions, from the UCI Machine Learning repository
    ([https://archive.ics.uci.edu/ml/datasets/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions](https://archive.ics.uci.edu/ml/datasets/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions)).
    It contains values of acceleration taken from an accelerometer on a smartphone.
    The original dataset contains *x*, *y*, and *z* components of the acceleration
    and the corresponding timestamp values. For this exercise, we have used only the
    two horizontal components of the acceleration *x* and *y*. In this exercise, let's
    assume that the acceleration follows a normal distribution. Let's also assume
    a normal prior distribution for the mean values of acceleration with a hyperparameter
    for a mean that is uniformly distributed in the interval (-0.5, 0.5) and a known
    variance equal to 1\. Find the posterior mean value by using the expression given
    in the equation.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于这个练习，我们使用的是来自UCI机器学习仓库（[https://archive.ics.uci.edu/ml/datasets/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions](https://archive.ics.uci.edu/ml/datasets/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions)）对应的智能手机基于人类活动和姿势转换识别的数据集。它包含从智能手机加速度计获取的加速度值。原始数据集包含加速度的*x*、*y*和*z*分量以及相应的时间戳值。在这个练习中，我们只使用了加速度的两个水平分量*x*和*y*。在这个练习中，我们假设加速度遵循正态分布。我们还假设加速度均值具有正态先验分布，均值的超参数在区间(-0.5,
    0.5)内均匀分布，且已知方差等于1。通过方程给出的表达式找到后验均值。
- en: Write an R function to compute the Fisher information matrix. Obtain the Fisher
    information matrix for this problem by using the dataset mentioned in exercise
    1 of this section.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个R函数来计算Fisher信息矩阵。使用本节练习1中提到的数据集，获取这个问题的Fisher信息矩阵。
- en: Set up an MCMC simulation for this problem by using the **mcmc** package in
    R. Plot a histogram of the simulated data.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用R中的**mcmc**包设置这个问题的MCMC模拟。绘制模拟数据的直方图。
- en: Set up an MCMC simulation using Gibbs sampling. Compare the results with that
    of the Metropolis algorithm.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Gibbs抽样设置MCMC模拟。将结果与Metropolis算法的结果进行比较。
- en: References
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Berger J.O. *Statistical Decision Theory and Bayesian Analysis*. Springer Series
    in Statistics. 1993\. ISBN-10: 0387960988'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Berger J.O. 《统计决策理论及贝叶斯分析》。Springer统计学系列。1993年。ISBN-10: 0387960988'
- en: 'Jayes E.T. *Probability Theory: The Logic of Science*. Cambridge University
    Press. 2003\. ISBN-10: 052159271'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Jayes E.T. 《概率论：科学的逻辑》。剑桥大学出版社。2003年。ISBN-10: 052159271'
- en: Wittman D. "Fisher Matrix for Beginners". Physics Department, University of
    California at Davis ([http://www.physics.ucdavis.edu/~dwittman/Fisher-matrix-guide.pdf](http://www.physics.ucdavis.edu/~dwittman/Fisher-matrix-guide.pdf))
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Wittman D. "Fisher 矩阵入门". 加州大学戴维斯分校物理系 ([http://www.physics.ucdavis.edu/~dwittman/Fisher-matrix-guide.pdf](http://www.physics.ucdavis.edu/~dwittman/Fisher-matrix-guide.pdf))
- en: 'Metropolis N, Rosenbluth A.W., Rosenbluth M.N., Teller A.H., Teller E. "Equations
    of State Calculations by Fast Computing Machines". Journal of Chemical Physics
    21 (6): 1087–1092\. 1953'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Metropolis N, Rosenbluth A.W., Rosenbluth M.N., Teller A.H., Teller E. "快速计算机器的物态方程计算".
    化学物理杂志 21 (6): 1087–1092\. 1953'
- en: 'Geman S., Geman D. "Stochastic Relaxation, Gibbs Distributions, and the Bayesian
    Restoration of Images". IEEE Transactions on Pattern Analysis and Machine Intelligence
    6 (6): 721-741\. 1984'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Geman S., Geman D. "随机松弛、吉布斯分布以及图像的贝叶斯恢复". IEEE 交易杂志：模式分析与机器智能 6 (6): 721-741\.
    1984'
- en: Summary
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered basic principles of Bayesian inference. Starting
    with how uncertainty is treated differently in Bayesian statistics compared to
    classical statistics, we discussed deeply various components of Bayes' rule. Firstly,
    we learned the different types of prior distributions and how to choose the right
    one for your problem. Then we learned the estimation of posterior distribution
    using techniques such as MAP estimation, Laplace approximation, and MCMC simulations.
    Once the readers have comprehended this chapter, they will be in a position to
    apply Bayesian principles in their data analytics problems. Before we start discussing
    specific Bayesian machine learning problems, in the next chapter, we will review
    machine learning in general.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了贝叶斯推理的基本原理。从贝叶斯统计与经典统计在处理不确定性方面的不同之处开始，我们深入讨论了贝叶斯定理的各个组成部分。首先，我们学习了不同类型的先验分布以及如何为你的问题选择正确的分布。然后，我们学习了使用如
    MAP 估计、拉普拉斯近似和 MCMC 模拟等技术来估计后验分布。一旦读者理解了本章内容，他们就能将贝叶斯原理应用于他们的数据分析问题。在我们开始讨论具体的贝叶斯机器学习问题之前，下一章我们将回顾机器学习的一般知识。
