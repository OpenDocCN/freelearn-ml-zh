<html><head></head><body>
        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Predictive Modeling</h1>
                
            
            <article>
                
<p class="calibre2">Predictive modeling is a process that is using advanced statistics and probability algorithms to predict outcomes, based on a pretrained and built model or function. These algorithms can be groups in a family of algorithms based on the outcome of the predicted variable. The outcome is usually the forecasted value that explains the future behavior. Several variables or input data consist of a mathematical function, also called the model (hence also data modeling), and these input data are trying to explain or predict the outcome. To better understand predictive modeling, the chapter will consist of the following topics:</p>
<ul class="calibre7">
<li class="calibre8">Data modeling</li>
<li class="calibre8">Advanced predictive algorithms</li>
<li class="calibre8">Predictive analytics</li>
<li class="calibre8">Deploying and using predictive solutions</li>
<li class="calibre8">Performing prediction with R Services in SQL Server database</li>
</ul>
<p class="calibre2">The focus in this chapter will be on delivering insight into understanding how predictive modeling can be used in SQL Server 2016/2017, using R on your typical business problem. In the enterprise environment, a business problem can be defined in a very broad aspect. For example, in medicine, a typical problem that predictive modeling can help understand and solve is, will the change of the ingredient A and B for the medicine C, help cure the disease? Furthermore, in the metallurgic industry, can we simulate how an anti-corrosion coating paint will age through time—or in retails, how can a customer select a better product in a store based on their needs or behavior? One can say, our everyday life is intertwined with predictions and forecast. Usually, every logistical problem all of us are facing is a simple question on a potentially very relevant topic: if I leave home for work 5 minutes later, will this affect my driving time if I take one shortcut and so on and so forth. Literally, we can say, our everyday decisions are the sum of all actions we take with a given output.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Data modeling</h1>
                
            
            <article>
                
<p class="calibre2">Data modeling is a process where we try to find a function (or the so-called model) with a set of independent variables or input data. Just like in data warehousing, where modeling is referring to establishing the conceptual framework based on the physical data structure and with the help of ORM or UML (even CRC) diagrams one explores the structures in data the same is seen with exploring the structures when doing predictive analysis. In case of the latter, data modeling is exploring the structures (or relations) between two or more variables. These relations can be presented as a function and are essentially stored as a model.</p>
<p class="calibre2">To start modeling, we will use some of the Microsoft data available at the following GitHub repository:</p>
<p class="calibre2"><a href="https://github.com/Microsoft/sql-server-samples/tree/master/samples/features/machine-learning-services/python/getting-started/rental-prediction" class="calibre10">https://github.com/Microsoft/sql-server-samples/tree/master/samples/features/machine-learning-services/python/getting-started/rental-prediction</a></p>
<p class="calibre2">Do not get confused at this Python example:</p>
<div class="packt_figure"><img class="aligncenter53" src="../images/00087.jpeg"/></div>
<p class="calibre2">Downloading this database will download the <kbd class="calibre11">TutorialDB.bak</kbd> file, which you simply restore to your SQL Server instance, where R in-database is installed. This database is included as part of the accompanying code that comes with this chapter.</p>
<p class="calibre2">Part of modeling data is to set up the understanding of how predictions at a later phase will work. Therefore, in this phase, we will create an understanding of the variables and their relation to each other. Create restore from the downloaded file and run the following restore from the backup T-SQL command:</p>
<pre class="calibre19"><strong class="calibre1">USE [master]</strong><br class="title-page-name"/><strong class="calibre1">BACKUP LOG [TutorialDB] TO DISK = N'C:\Program Files\Microsoft SQL Server\MSSQL14.MSSQLSERVER\MSSQL\Backup\TutorialDB_LogBackup_2018-01-01_23-59-09.bak'</strong></pre>
<pre class="calibre19"><strong class="calibre1">WITH NOFORMAT, NOINIT, NAME = N'TutorialDB_LogBackup_2018-01-01_23-59-09', NOSKIP, NOREWIND, NOUNLOAD, NORECOVERY , STATS = 5</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">RESTORE DATABASE [TutorialDB] FROM DISK = N'C:\Program Files\Microsoft SQL Server\MSSQL14.MSSQLSERVER\MSSQL\Backup\TutorialDB.bak' </strong><br class="title-page-name"/><strong class="calibre1">WITH FILE = 2, MOVE N'TutorialDB' TO N'C:\Program Files\Microsoft SQL Server\MSSQL14.MSSQLSERVER\MSSQL\DATA\TutorialDB.mdf', </strong><br class="title-page-name"/><strong class="calibre1">MOVE N'TutorialDB_log' TO N'C:\Program Files\Microsoft SQL Server\MSSQL14.MSSQLSERVER\MSSQL\DATA\TutorialDB_log.ldf', NOUNLOAD, STATS = 5</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">GO
  </strong></pre>
<p class="calibre2">Alternatively, you can simply use the <kbd class="calibre11">RESTORE</kbd> command in SSMS:</p>
<div class="packt_figure"><img src="../images/00088.gif" class="calibre57"/></div>
<p class="calibre2">You will now have the database restored and the <kbd class="calibre11">dbo.rental_data</kbd> table at your use. For now, this will be enough.</p>
<p class="calibre2">With the dataset ready, we can now start modeling the data by exploring and understanding the variables and the relations among them. This quick exploration can be performed in SQL Operation Studio (link to download: <a href="https://docs.microsoft.com/en-us/sql/sql-operations-studio/download" class="calibre10">https://docs.microsoft.com/en-us/sql/sql-operations-studio/download</a>), where we will use a simple query:</p>
<pre class="calibre19"><strong class="calibre1">SELECT RentalCount,Day,Month, Holiday, Snow FROM rental_data</strong>  </pre>
<p class="calibre2">Besides the standard table view of the results, this will also give a nice chart viewer, where a simple graphical representation of variables will give you better insights into the data:</p>
<div class="packt_figure"><img src="../images/00089.jpeg" class="calibre58"/></div>
<p class="calibre2">But without the general understanding of descriptive statistics, we will not continue. So, using the <kbd class="calibre11">rxSummary</kbd> function from the <kbd class="calibre11">RevoScaleR</kbd> package will give the desired results:</p>
<pre class="calibre19"><strong class="calibre1">EXEC sp_execute_external_Script</strong>
<strong class="calibre1">@LANGUAGE = N'R'</strong>
<strong class="calibre1">,@script = N'</strong>
<strong class="calibre1">    dr_rent &lt;- InputDataSet</strong>
<strong class="calibre1">    dr_rent &lt;- data.frame(dr_rent)</strong>
 <strong class="calibre1">   summary &lt;- rxSummary(~ RentalCount  + Year + Month + Day  + WeekDay + Snow + Holiday , data = dr_rent)</strong>
   <strong class="calibre1"> OutputDataSet &lt;- summary$sDataFrame'</strong>
<strong class="calibre1">,@input_data_1 = N'SELECT  RentalCount, Year, Month, Day, WeekDay, Snow, Holiday FROM rental_data'</strong>
<strong class="calibre1">WITH RESULT SETS ((</strong>
<strong class="calibre1">    [Name]   NVARCHAR(100)</strong>
<strong class="calibre1">    ,Mean   NUMERIC(16,3)</strong>
<strong class="calibre1">    ,StdDev  NUMERIC(16,3)</strong>
<strong class="calibre1">    ,[Min]   INT</strong>
<strong class="calibre1">    ,[Max]  INT</strong>
<strong class="calibre1">    ,ValidObs  INT</strong>
<strong class="calibre1">    ,MissingObs INT</strong>
<strong class="calibre1">));</strong>
<strong class="calibre1">GO</strong>
  </pre>
<p class="calibre2">The following are the results as a simple descriptive statistics table:</p>
<div class="packt_figure"><img src="../images/00090.gif" class="calibre59"/></div>
<p class="calibre2">Exploring the uni- and bi-variate statistics was part of the previous <a target="_blank" href="part0081.html#2D7TI0-e3f81285367248f4bbc6431bcd4f926d" class="calibre10">Chapter 5</a>,<em class="calibre12"> RevoScaleR Package,</em> but here we will focus more on bi- and multi-variate statistics. Before we begin, let's explore the correlations some more. Based on exploring the variable names and descriptive statistics, common sense will tell us that during the holidays, the rental count should be higher. Checking this can be done using the correlation coefficient. The following is a simple example:</p>
<pre class="calibre19"><strong class="calibre1">EXEC sp_execute_external_Script</strong>
<strong class="calibre1">@LANGUAGE = N'R'</strong>
<strong class="calibre1">,@script = N'</strong>
    <strong class="calibre1">  dr_rent &lt;- InputDataSet</strong>
    <strong class="calibre1">  OutputDataSet &lt;- data.frame(cor(dr_rent$Holiday, dr_rent$RentalCount))</strong>
<strong class="calibre1">'</strong>
<strong class="calibre1">,@input_data_1 = N'SELECT  Holiday, RentalCount FROM rental_data'</strong>
<strong class="calibre1">WITH RESULT SETS ((</strong>
<strong class="calibre1">    cor NUMERIC(10,3)</strong>
<strong class="calibre1">    ));</strong>
<strong class="calibre1">GO</strong>
  </pre>
<p class="calibre2">This will give you the idea of the bi-variate relationship of <kbd class="calibre11">0.332</kbd>. This is a weak correlation but a positive one:</p>
<div class="packt_figure"><img src="../images/00091.gif" class="calibre60"/></div>
<p class="calibre2">This simply means that if the <kbd class="calibre11">RentalCount</kbd> variable gets higher, the number of holidays also increases. This indeed makes sense, since if more holidays are coming, more rentals are expected.</p>
<p class="calibre2">Now we can continue exploring and searching for the correlations by combining each of the variables. This is similar to making a CROSS JOIN, but there are easier ways to do this. One is, of course, by using common sense and selecting the meaningful correlations:</p>
<pre class="calibre19"><strong class="calibre1">EXEC sp_execute_external_Script</strong>
<strong class="calibre1">@LANGUAGE = N'R'</strong>
<strong class="calibre1">,@script = N'</strong>
  <strong class="calibre1">  dr_rent &lt;- InputDataSet</strong>
    <strong class="calibre1">dr_rent &lt;- data.frame(dr_rent)</strong>
    <strong class="calibre1">cor_HR &lt;- cor(dr_rent$Holiday, dr_rent$RentalCount)</strong>
    <strong class="calibre1">cor_FR &lt;- cor(as.numeric(dr_rent$FWeekDay), dr_rent$RentalCount)</strong>
    <strong class="calibre1">cor_MR &lt;- cor(dr_rent$Month, dr_rent$RentalCount)</strong>
    <strong class="calibre1">cor_YR &lt;- cor(dr_rent$Year,dr_rent$RentalCount)</strong>
    <strong class="calibre1">d &lt;- data.frame(cbind(cor_HR, cor_FR, cor_MR, cor_YR))</strong>
    <strong class="calibre1">OutputDataSet &lt;- d'</strong>
    <strong class="calibre1">,@input_data_1 = N'SELECT  Holiday, RentalCount,Month,FWeekDay, Year FROM rental_data'</strong>
<strong class="calibre1">WITH RESULT SETS ((</strong>
<strong class="calibre1">    cor_HR NUMERIC(10,3)</strong>
<strong class="calibre1">,cor_FR NUMERIC(10,3)</strong>
<strong class="calibre1">,cor_MR NUMERIC(10,3)</strong>
<strong class="calibre1">,cor_YR NUMERIC(10,3)</strong>
<strong class="calibre1">));</strong>
<strong class="calibre1">GO</strong>
  </pre>
<p class="calibre2">And we get the following results as shown in the figure below. Interpretation and understanding of the results is of high importance. So, the holiday time is by far the most correlative variable with rental count variable. Neither the day of the week, nor the year, play any significant role. There is a very tiny, yet negative correlation of <kbd class="calibre11">-0.110</kbd> between <kbd class="calibre11">Month</kbd> and <kbd class="calibre11">RentalCount</kbd>, which can be understood as higher months might have lower rental counts and vice versa. Since this correlation is so weak, it is meaningless to make a fuss over of this particular correlation (even if it makes or does not make any sense):</p>
<div class="packt_figure"><img src="../images/00092.gif" class="calibre61"/></div>
<p class="calibre2">Similarly, one can explore the distribution of the values within each variable by plotting the boxplots:</p>
<div class="packt_figure"><img src="../images/00093.gif" class="calibre62"/></div>
<p class="calibre2">The second way is to plot the diagram of correlations between the variables. One way to do it is to invoke the <kbd class="calibre11">corrplot</kbd> R library, which gives you a very powerful and useful visualization of correlation. I tend to create the following code:</p>
<pre class="calibre19"><strong class="calibre1">EXEC sp_execute_external_Script</strong>
<strong class="calibre1">@LANGUAGE = N'R'</strong>
<strong class="calibre1">,@script = N'</strong>
<strong class="calibre1">    library(corrplot)  # * footnote</strong>
<strong class="calibre1">    dr_rent &lt;- InputDataSet</strong>
    
<strong class="calibre1">            dr_rent$FWeekDay &lt;- as.numeric(dr_rent$FWeekDay)</strong>
<strong class="calibre1">            dr_rent$FHoliday &lt;- as.numeric(dr_rent$FHoliday)</strong>
            <strong class="calibre1">dr_rent$FSnow &lt;- as.numeric(dr_rent$FSnow)</strong>
    
    <strong class="calibre1">cor.mtest &lt;- function(mat, ...) {</strong>
    <strong class="calibre1">     mat &lt;- as.matrix(mat)</strong>
    <strong class="calibre1">     n &lt;- ncol(mat)</strong>
    <strong class="calibre1">     p.mat&lt;- matrix(NA, n, n)</strong>
    <strong class="calibre1">     diag(p.mat) &lt;- 0</strong>
    <strong class="calibre1">     for (i in 1:(n - 1)) {</strong>
    <strong class="calibre1">         for (j in (i + 1):n) {</strong>
    <strong class="calibre1">         tmp &lt;- cor.test(mat[, i], mat[, j], ...)</strong>
    <strong class="calibre1">         p.mat[i, j] &lt;- p.mat[j, i] &lt;- tmp$p.value</strong>
    <strong class="calibre1">         }</strong>
    <strong class="calibre1">     }</strong>
    <strong class="calibre1">     colnames(p.mat) &lt;- rownames(p.mat) &lt;- colnames(mat)</strong>
    <strong class="calibre1">     p.mat</strong>
    <strong class="calibre1">     }</strong>
    <strong class="calibre1">p.mat &lt;- cor.mtest(dr_rent)</strong>
    
    <strong class="calibre1">R&lt;-cor(dr_rent)</strong>
    
    <strong class="calibre1">col &lt;- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))</strong>
    
    <strong class="calibre1">image_file = tempfile();  </strong>
    <strong class="calibre1">jpeg(filename = image_file);  </strong>
    
<strong class="calibre1">plot_corr &lt;- corrplot(R, method="color", col=col(200),  </strong>
<strong class="calibre1">                type="upper", order="hclust", </strong>
<strong class="calibre1">                addCoef.col = "black", # Add coefficient of correlation</strong>
<strong class="calibre1">                tl.col="black", tl.srt=45, #Text label color and rotation</strong>
<strong class="calibre1">                # Combine with significance</strong>
<strong class="calibre1">                p.mat = p.mat, sig.level = 0.01, insig = "blank", </strong>
<strong class="calibre1">                # hide correlation coefficient on the principal diagonal</strong>
<strong class="calibre1">                diag=FALSE)</strong>
<strong class="calibre1">    dev.off(); </strong>
<strong class="calibre1">OutputDataSet &lt;- data.frame(data=readBin(file(image_file, "rb"), what=raw(), n=1e6));  '</strong>
    
<strong class="calibre1">,@input_data_1 = N'SELECT  *  FROM rental_data'</strong>
<strong class="calibre1">WITH RESULT SETS ((</strong>
<strong class="calibre1">  correlation_plot varbinary(max)</strong></pre>
<pre class="calibre19"><strong class="calibre1">));</strong>
<strong class="calibre1">GO</strong>
  </pre>
<div class="packt_infobox">Code copied and slightly changed from the corrplot lattice documentation.</div>
<p class="calibre2">This procedure can be directly implemented and used in SSRS or in Power BI suit or Excel; the visual is as follows:</p>
<div class="packt_figure"><img src="../images/00094.jpeg" class="calibre63"/></div>
<p class="calibre2">In a single graph, a trained eye will immediately see the correlations and their statistical significance. So, the <kbd class="calibre11">0.33 RentalCount</kbd> and <kbd class="calibre11">Holiday</kbd> is visible here, but also the <kbd class="calibre11">RentalCount</kbd> and <kbd class="calibre11">Snow</kbd> is of <kbd class="calibre11">0.19</kbd> positive correlation. But if we want to explore the behavior of the values dispersion (variance), we can also include the analysis of variance.</p>
<p class="calibre2">If you are working with large datasets or XDF data formats, <kbd class="calibre11">RevoScaleR</kbd> package also comes equipped with functions that compute and calculate correlation matrixes. Here is an R code using <kbd class="calibre11">rxCovCor</kbd> (or, alternatively, one can use <kbd class="calibre11">rxCor</kbd> or <kbd class="calibre11">rxCov</kbd>):</p>
<pre class="calibre19"><strong class="calibre1">Formula_correlation =  ~ RentalCount + Year + Month + Day  + WeekDay + Snow + Holiday </strong>
<strong class="calibre1">allCor &lt;- rxCovCor(Formula_correlation, data = dr_rent, type = "Cor")</strong>
<strong class="calibre1">allCor</strong>
  </pre>
<p class="calibre2">This gives the same results as all the previous calculation of correlations:</p>
<div class="packt_figure"><img src="../images/00095.gif" class="calibre64"/></div>
<p class="calibre2">This output also has the ability to see the standard deviations, means and sum of weights, but the best part is that it stores the results in a data frame, which can be easily imported or used with other T-SQL tables. The results can be invoked using <kbd class="calibre11">allCov$CovCor</kbd> (R language stores the results as an object of lists and each list can be retrieved by using a dollar sign <kbd class="calibre11">$</kbd> and referencing the name of the list—in this case, <kbd class="calibre11">CovCor</kbd>).</p>
<p class="calibre2">When we want to further investigate our so-far highest correlation between the <kbd class="calibre11">RentalCount</kbd> and <kbd class="calibre11">Holiday</kbd>, <strong class="calibre1">Analysis Of Variance </strong>(<strong class="calibre1">ANOVA</strong>) will be the appropriate method. We will compare two groups (or levels) of variable <kbd class="calibre11">Holiday</kbd> (<kbd class="calibre11">0</kbd> is not a holiday while <kbd class="calibre11">1 </kbd> is a holiday) and whether there is a difference between the rental counts. By doing so, calculating F-statistics and its significance will tell us the ratio of between-group variance to within-group variance:</p>
<pre class="calibre19"><strong class="calibre1">EXEC sp_execute_external_Script</strong>
<strong class="calibre1">@LANGUAGE = N'R'</strong>
<strong class="calibre1">,@script = N'</strong>
    <strong class="calibre1">        #ANOVA</strong>
    <strong class="calibre1">        ANOVA &lt;- aov(RentalCount ~ Holiday, data = InputDataSet) </strong>
    <strong class="calibre1">        F_Stat&lt;- unlist(summary(ANOVA))[7]</strong>
    <strong class="calibre1">        F_Stat_Sig &lt;- unlist(summary(ANOVA))[9]</strong>
    <strong class="calibre1">        df &lt;- cbind(F_Stat, F_Stat_Sig)</strong>
<strong class="calibre1">OutputDataSet &lt;- data.frame(df)'</strong>
    
<strong class="calibre1">,@input_data_1 = N'SELECT  RentalCount,Holiday FROM rental_data'</strong>
<strong class="calibre1">WITH RESULT SETS ((</strong>
<strong class="calibre1">    F_Statistic NVARCHAR(200)</strong>
<strong class="calibre1">,Statistical_Significance NUMERIC(16,5)</strong>
<strong class="calibre1">));</strong>
<strong class="calibre1">GO</strong>
  </pre>
<p class="calibre2">After running the T-SQL code with R code for statistical calculation of ANOVA, the output result is created in such manner that it returns the F-Statistic and statistical significance. The following figure shows results returned:</p>
<div class="packt_figure"><img class="aligncenter54" src="../images/00096.gif"/></div>
<p class="calibre2">Results tell us that the F-Statistic is statistically significant—even though it is small—and this means that the means are most likely not equal (and we would be, in this case, rejecting the null hypothesis). To find where the difference lies, <kbd class="calibre11">TukeyHDS</kbd> test would give us further information.</p>
<p class="calibre2">Just to illustrate the difference, since we will not go into the details, we can use the <kbd class="calibre11">stripchart</kbd> visualization of the difference between the holiday distribution of the rentals:</p>
<div class="packt_figure"><img src="../images/00097.gif" class="calibre65"/></div>
<p class="calibre2">With the R code:</p>
<pre class="calibre19"><strong class="calibre1">stripchart(RentalCount ~ Holiday, vertical=TRUE, pch=9, </strong>
    <strong class="calibre1">           data=dr_rent_ANOVA, xlab="Holiday day (Yes/No)", ylab="Rental count", method="jitter", jitter=0.09)</strong>
  </pre>
<p class="calibre2">The distribution of the cases can tell us that on holidays an average of <kbd class="calibre11">400</kbd> or higher rental counts are made, whereas on a normal day, there is a huge density of counts between <kbd class="calibre11">10</kbd> and <kbd class="calibre11">50</kbd>.</p>
<p class="calibre2">When determining which features (variables) are good for further analysis and predictive algorithms, we can use the Decrease Gini mean calculation. One of the Gini mean functions is available in <kbd class="calibre11">randomForest</kbd> package, so, let's call the function and see which variables are to be used:</p>
<pre class="calibre19"><strong class="calibre1">EXEC sp_execute_external_Script</strong>
<strong class="calibre1">@LANGUAGE = N'R'</strong>
<strong class="calibre1">,@script = N'</strong>
<strong class="calibre1">   library(randomForest)</strong>
<strong class="calibre1">    dr_rent  &lt;- InputDataSet</strong>
<strong class="calibre1">    fit_RF &lt;- randomForest(factor(dr_rent$RentalCount)~., data=dr_rent)</strong>
<strong class="calibre1">     vp_rf &lt;- importance(fit_RF)</strong>
    
    <strong class="calibre1">  vp_rf&lt;- data.frame(vp_rf)</strong>
    <strong class="calibre1">  imena &lt;- row.names(vp_rf)</strong>
    <strong class="calibre1">  vp_rf &lt;- data.frame(cbind(imena, vp_rf))</strong>
    <strong class="calibre1">OutputDataSet &lt;- vp_rf'</strong>
    
<strong class="calibre1">,@input_data_1 = N'SELECT  *  FROM rental_data'</strong>
<strong class="calibre1">WITH RESULT SETS ((</strong>
 <strong class="calibre1">   Variable NVARCHAR(200)</strong>
<strong class="calibre1">,MeanDecreaseGini NUMERIC(16,5)</strong>
<strong class="calibre1">));</strong>
<strong class="calibre1">GO</strong>    </pre>
<p class="calibre2">With T-SQL code we are returning the decreased Gini coefficient:</p>
<div class="packt_figure"><img src="../images/00098.gif" class="calibre66"/></div>
<p class="calibre2"><kbd class="calibre11">Gini</kbd> coefficient can also be represented visually as a scatter plot, so that the user can immediately determine which variables contribute most to the model. For the sake of brevity, the code for this graph is included in the code but not in the book.</p>
<div class="packt_figure"><img src="../images/00099.gif" class="calibre67"/></div>
<p class="calibre2">One can now determine, which of the following variables play any role or contribute gain in the model. The <strong class="calibre1">MeanDecreaseGini</strong> was drawn as <kbd class="calibre11">varImpPlot(fit_RF)</kbd>. Technically, this is how one can determine which variables or input parameters have the least or most impact, but each of these techniques will give you some of the aspects—what can be good in the model, and what may not. Comparing the <kbd class="calibre11">Holiday</kbd> <span>variable </span>in the correlation matrix and mean decrease plot, you can see that it gives different methods and different results. Most significant are the ones where particular variables do not play any importance whatsoever through several different methods.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Advanced predictive algorithms and analytics</h1>
                
            
            <article>
                
<p class="calibre2">So far, we have examined the data preparation and data exploration functions available in the <kbd class="calibre11">RevoScaleR</kbd> package. Besides these functions, predicting classification or regression problems can also be done, especially when dealing with large datasets.</p>
<p class="calibre2">I will mention only few of these. The complete list is available online (<a href="https://docs.microsoft.com/en-us/machine-learning-server/r-reference/revoscaler/revoscaler" class="calibre10">https://docs.microsoft.com/en-us/machine-learning-server/r-reference/revoscaler/revoscaler</a>) and some of the points are as follows:</p>
<ul class="calibre7">
<li class="calibre8"><kbd class="calibre11">rxLinMod</kbd>: This is used for building and predicting a linear model</li>
<li class="calibre8"><kbd class="calibre11">rxLogit</kbd>: This is used for building and predicting the logistic regression model</li>
<li class="calibre8"><kbd class="calibre11">rxGlm</kbd>: This is used for creating a generalized linear model</li>
<li class="calibre8"><kbd class="calibre11">rxDTree</kbd>: This is used for creating a classification or regression tree</li>
<li class="calibre8"><kbd class="calibre11">rxBTrees</kbd>: This is used for building a classification or regression decision forest—that is using a stochastic gradient boosting algorithm</li>
<li class="calibre8"><kbd class="calibre11">rxDForest</kbd>: This is used for building a classification or regression decision forest model</li>
<li class="calibre8"><kbd class="calibre11">rxNaiveBayes</kbd>: This is used for building a Naive Bayes classification model</li>
</ul>
<p class="calibre2">All these algorithms are part of a family of supervised algorithms, where the only unsupervised (or undirected) algorithm available in <kbd class="calibre11">RevoScaleR</kbd> package is <kbd class="calibre11">rxKMeans</kbd>, which is used for dealing with clustering.</p>
<p class="calibre2">Using the same dataset as we did earlier, we plug in and start using <kbd class="calibre11">rxLinMod</kbd> and <kbd class="calibre11">rxGlm</kbd> for demonstrating how this can be used within T-SQL:</p>
<pre class="calibre19"><strong class="calibre1">USE RentalDB;
GO

-- rxLinMod

EXEC sp_execute_external_Script
@LANGUAGE = N'R'
,@script = N'
            library(RevoScaleR)
            dr_rent &lt;- InputDataSet
            Formula_supervised =  RentalCount ~ Year + Month + Day  + WeekDay + Snow + Holiday             
            #Create Linear Model 
            rent_lm &lt;- rxLinMod(formula=Formula_supervised, data = dr_rent)

            #PREDICT   
            rent_Pred &lt;- rxPredict(modelObject = rent_lm, data = dr_rent, extraVarsToWrite = c("RentalCount","Year","Month","Day"), writeModelVars = TRUE)
            OutputDataSet &lt;- data.frame(rent_Pred)
'
,@input_data_1 = N'SELECT RentalCount,Year, Month, Day, WeekDay,Snow,Holiday  FROM rental_data'
WITH RESULT SETS ((
 RentalCount_Pred    NUMERIC(16,3)
,RentalCount  NUMERIC(16,3)
,YearINT
,MonthINT
,DayINT
,WeekDayINT
,Snow  INT
,Holiday INT
));
GO



-- rxGlm

EXEC sp_execute_external_Script
@LANGUAGE = N'R'
,@script = N'
            library(RevoScaleR)
            dr_rent &lt;- InputDataSet
            Formula_supervised =  RentalCount ~ Year + Month + Day  + WeekDay + Snow + Holiday             

            #PREDICT   
                rent_glm &lt;- rxGlm(formula = Formula_supervised, family = Gamma, dropFirst = TRUE, data = dr_rent)
                rent_Pred &lt;- rxPredict(modelObject = rent_glm, data = dr_rent, extraVarsToWrite=c("RentalCount","Year","Month","Day"), writeModelVars = TRUE)
            OutputDataSet &lt;- data.frame(rent_Pred)'
,@input_data_1 = N'SELECT RentalCount,Year, Month, Day, WeekDay,Snow,Holiday  FROM rental_data'
WITH RESULT SETS ((
 RentalCount_Pred    NUMERIC(16,3)
,RentalCount  NUMERIC(16,3)
,YearINT
,MonthINT
,DayINT
,WeekDayINT
,Snow  INT
,Holiday INT
));
GO

-- rxDTree

EXEC sp_execute_external_Script
@LANGUAGE = N'R'
,@script = N'
            library(RevoScaleR)
            dr_rent &lt;- InputDataSet
            Formula_supervised =  RentalCount ~ Year + Month + Day  + WeekDay + Snow + Holiday             

            #PREDICT   
                        rent_dt &lt;- rxDTree(formula = Formula_supervised, data = dr_rent)
                        rent_Pred &lt;- rxPredict(modelObject = rent_dt, data = dr_rent, extraVarsToWrite=c("RentalCount","Year","Month","Day"), writeModelVars = TRUE)
            OutputDataSet &lt;- data.frame(rent_Pred)
                        
                '
,@input_data_1 = N'SELECT RentalCount,Year, Month, Day, WeekDay,Snow,Holiday  FROM rental_data'
WITH RESULT SETS ((</strong></pre>
<pre class="calibre19"><strong class="calibre1"> RentalCount_Pred    NUMERIC(16,3)
,RentalCount  NUMERIC(16,3)
,YearINT
,MonthINT
,DayINT
,WeekDayINT
,Snow  INT
,Holiday INT
));
GO</strong>  </pre>
<p class="calibre2">Both will give you the predicted values based on the inputted dataset, along with the newly predicted values:</p>
<div class="packt_figure"><img class="aligncenter55" src="../images/00100.gif"/></div>
<p class="calibre2">A curious eye will tell you that the predicted values are far-off from the original values. So, the prediction formula in both cases was trying to predict variable <kbd class="calibre11">RentalCount</kbd> based on variables: <kbd class="calibre11">Year</kbd>, <kbd class="calibre11">Month</kbd>, <kbd class="calibre11">Day</kbd>, <kbd class="calibre11">WeekDay</kbd>, <kbd class="calibre11">Snow</kbd>, and <kbd class="calibre11">Holiday</kbd>. The formula is set as follows:</p>
<pre class="calibre19"><strong class="calibre1">Formula_supervised =  RentalCount ~ Year + Month + Day  + WeekDay + Snow + Holiday      </strong>
  </pre>
<p class="calibre2">Comparing the variables <kbd class="calibre11">RentalCount_Pred</kbd> and <kbd class="calibre11">RentalCount</kbd> will show the difference/offset from the real and predicted values.</p>
<p class="calibre2">In the preceding sample, you can also compare the results of all the three algorithms. If you run comparison with all three datasets, observation by observation, you can see immediately which algorithms performed best:</p>
<div class="packt_figure"><img src="../images/00101.jpeg" class="calibre68"/></div>
<p class="calibre2">So, the yellow bar represents the original values, and by far the best hit is the decision trees algorithm, given the upper formula and understanding the insights of the data. The graph just represents a randomly taken observation. This can also be achieved by calculating the accuracy or measure that calculates how much deviation had accrued from the original values.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Deploying and using predictive solutions</h1>
                
            
            <article>
                
<p class="calibre2">When developing the in-database solution and creating it for continuous development (and also deployment), several aspects should be taken into consideration. First of all, the environment where data scientists will be working. You might give them a powerful, standalone server or even allocate proper seats in the cloud. They will need it, especially when training the model. This is extremely important, as you don't want to have your highly-paid statisticians and mathematicians wait for the models to compute and generate. So, enabling the route to a highly scalable CPU and RAM powerful computations is a must. Second to this, you have to get the data there. Whether it's on cloud or on premises, getting data there (and later also, back) should not be overlooked, as this might also be the point where you will lose precious time. And, lastly, having the environment set with proper settings, environment variables, packages, and all paths to proprietary software enabled is also of importance.</p>
<p class="calibre2"><kbd class="calibre11">RevoScaleR</kbd> package comes equipped with a function for easily switching the computational environments. We will now invoke a simple command in R:</p>
<pre class="calibre19"><strong class="calibre1">rxSetComputeContext(local)</strong>
<strong class="calibre1">rxSetComputeContext(sql)</strong>  </pre>
<p class="calibre2">By doing this, you can set the local computational environment (that is, the client's machine) or the server side, where—in this case—a standalone R Server would reside. With a simple function call, the computational context (or simple environment) is switched, as course, keeping in mind that all the data resides on both sides (so that you avoid the unneeded data transferring) and that all the server environment variables are set correctly.</p>
<p class="calibre2">For training the model, several good practices can be chosen. Splitting the data for training, testing or training, and testing and validating are several practices. Also, a very good practice is to test the percentage of training/testing/validating datasets. You might get a 50/50 or 60/40 or 70/30 percentage, but usually you carry out and decide this when mining the data. After that, you should also consider validation of the data; several aspects are available, from <strong class="calibre1">leave one out</strong> (<strong class="calibre1">LOO</strong>) or 10-folds or 5-folds for choosing the validation data for validating the results.</p>
<p class="calibre2">Not going into the topic too deep, and to make this demo simpler, we can decide to do a 70/30 percentage on the spot. Since we have the pleasure of the T-SQL database here, we can select and store the training subset in a table, or create a view, or decide which 70% we want to take.</p>
<pre class="calibre19"><strong class="calibre1">-- We can set 70% of the original data</strong>
<strong class="calibre1">-- IN SQL Server</strong>
<strong class="calibre1">SELECT</strong>
<strong class="calibre1">TOP (70)PERCENT</strong>
<strong class="calibre1">*</strong>
<strong class="calibre1">INTO dbo.Train_rental_data</strong>
<strong class="calibre1">FROM rental_data</strong>
<strong class="calibre1">ORDER BY ABS (CAST(BINARY_CHECKSUM(RentalCount,NEWID())asint))ASC</strong>
<strong class="calibre1">-- (318 rows affected) </strong>
    
<strong class="calibre1">-- Or we can set by the year; year 2013 and 2014 for training and 2015 for testing? making it cca 70% for training as well</strong>
<strong class="calibre1">SELECT COUNT(*),YEAR FROM rental_data GROUP BY YEAR</strong>
  </pre>
<p class="calibre2">This also heavily depends on your business model. The first approach simply takes 70% of the data from the original dataset, whereas the second select statement takes roughly 70% of the original data, but makes a split based on the year of the rental. This might have a crucial impact on how the model will behave and also how you want the decision to be affected by this, especially the business model.</p>
<p class="calibre2">Once this is cleared and covered, a best practice is to store the trained model in the tables for faster predictions. We will now create a table as follows:</p>
<pre class="calibre19"><strong class="calibre1">-- or in R
EXEC sp_execute_external_Script
        @language = N'R'
        ,@script = N'
                        library(caTools)
                        
                        set.seed(2910) 
                        dr_rent &lt;- InputDataSet
                        Split &lt;- .70
                        sample = sample.split(dr_rent$RentalCount, SplitRatio = Split)
                        train_dr_rent &lt;- subset(dr_rent, sample == TRUE)
                        test_dr_rent  &lt;- subset(dr_rent, sample == FALSE)
            OutputDataSet &lt;- data.frame(train_dr_rent)
                        
                '
,@input_data_1 = N'SELECT * FROM rental_data'
WITH RESULT SETS ((
         [Year] INT
        ,[Month] INT
        ,[Day] INT
        ,[RentalCount] INT
        ,[WeekDay] INT
        ,[Holiday] INT
        ,[Snow] INT
        ,[FHoliday] INT
        ,[FSnow] INT
        ,[FWeekDay] INT
));
GO  </strong></pre>
<p class="calibre2">Since the <kbd class="calibre11">set.seed</kbd> is defined, you will always get the same subset, wherever you run this code. If you want to get different results, you should comment it out.</p>
<p class="calibre2">Once the sampling is done again, based on the problem you are predicting, you need to define your prediction formula. In this case, I am using a formula converter to create a proper formula:</p>
<pre class="calibre19"><strong class="calibre1">-- Variables to keep
-- and creating formula
EXEC sp_execute_external_Script
        @language = N'R'
        ,@script = N'
        dr_rent &lt;- InputDataSet
            variables_all &lt;- rxGetVarNames(dr_rent)
            variables_to_remove &lt;- c("FSnow", "FWeekDay", "FHoliday")
            traning_variables &lt;- variables_all[!(variables_all %in% c("RentalCount", variables_to_remove))]
            #use as.formula to create an object
        formula &lt;- as.formula(paste("RentalCount ~", paste(traning_variables, collapse = "+")))
                #formula &lt;- paste("RentalCount ~", paste(traning_variables, collapse = "+"))
            OutputDataSet &lt;- data.frame(formula)'
,@input_data_1 = N'SELECT * FROM dbo.Train_rental_data'
WITH RESULT SETS ((
         [Formula_supervised] NVARCHAR(1000)
));
GO </strong></pre>
<p class="calibre2">Creating a formula through a procedure, making it not hard coded, is also a very useful approach, especially in the corporate environment where data scientists will set up the pool of independent variables and later the data engineer would choose which to include, prior to pushing the data to compute the model and deploy it.</p>
<p class="calibre2">The process of bi-variate and multi-variate statistics can also give the data engineers and stewards better insights and understanding of what and how the data is operating and correlating, and that there are no unwanted correlations or variables that just do not function.</p>
<p class="calibre2">With this cleared up, we can set up and build the procedures that will run the model training and have the models stored in the database. Due to the space limits of this chapter, I will only show the creation of one procedure; the rest of the procedures can be found in the accompanying chapter materials:</p>
<p class="calibre2">The procedure for random forest in T-SQL would look like this:</p>
<pre class="calibre19"><strong class="calibre1">-- Random forest

DROP PROCEDURE IF EXISTS dbo.forest_model;
GO


CREATE OR ALTER PROCEDURE dbo.forest_model(
         @trained_model VARBINARY(MAX)OUTPUT
        ,@accuracy FLOATOUTPUT
        )
AS
BEGIN
EXEC sp_execute_external_script
@language = N'R'
,@script = N'
                        library(RevoScaleR)
                        library(caTools)
            library(MLmetrics)

                
                        dr_rent &lt;- InputDataSet
                        set.seed(2910) 
                        Split &lt;- .70
                        sample = sample.split(dr_rent$RentalCount, SplitRatio = Split)
                        train_dr_rent &lt;- subset(dr_rent, sample == TRUE)
                        test_dr_rent  &lt;- subset(dr_rent, sample == FALSE)
                        
                        y_train &lt;- train_dr_rent$RentalCount
                        y_test &lt;- test_dr_rent$RentalCount


            variables_all &lt;- rxGetVarNames(dr_rent)
            variables_to_remove &lt;- c("FSnow", "FWeekDay", "FHoliday")
            traning_variables &lt;- variables_all[!(variables_all %in% c("RentalCount", variables_to_remove))]
            formula &lt;- as.formula(paste("RentalCount ~", paste(traning_variables, collapse = "+")))

                        forest_model &lt;- rxDForest(formula = formula,
                          data = train_dr_rent,
                          nTree = 40,
                          minSplit = 10,
                          minBucket = 5,
                          cp = 0.00005,
                          seed = 5)

                        trained_model &lt;- as.raw(serialize(forest_model, connection=NULL))
                
                        #calculating accuracy
            y_predicted&lt;- rxPredict(forest_model,test_dr_rent)

            predict_forest &lt;-data.frame(actual=y_test,pred=y_predicted)
            #ConfMat &lt;- confusionMatrix(table(predict_forest$actual,predict_forest$RentalCount_Pred))
            #accuracy &lt;- ConfMat$overall[1]
            accu &lt;- LogLoss(y_pred = predict_forest$RentalCount_Pred , y_true =predict_forest$actual)
            accuracy &lt;- accu'


        ,@input_data_1 = N'SELECT * FROM dbo.rental_data'
        ,@params = N'@trained_model VARBINARY(MAX) OUTPUT, @accuracy FLOAT OUTPUT'
        ,@trained_model = @trained_model OUTPUT
        ,@accuracy = @accuracy OUTPUT;
END;
GO
</strong></pre>
<p class="calibre2">I have added something extra to the procedure, such that every time the model is trained, an extra is added. This is accuracy, which will also give the data engineer and stewards in the later phases a good insight into deciding which model outperforms the others.</p>
<p class="calibre2">You can simply run the procedure as follows:</p>
<pre class="calibre19"><strong class="calibre1">DECLARE @model VARBINARY(MAX);</strong>
<strong class="calibre1">DECLARE @accur FLOAT;</strong>
<strong class="calibre1">EXEC dbo.forest_model@model OUTPUT, @accur OUTPUT;</strong>
<strong class="calibre1">INSERT INTO [dbo].[Rental_data_models](model_name, model, accuracy) VALUES ('Random_forest_V1', @model, @accur);</strong>
<strong class="calibre1">GO</strong>  </pre>
<p class="calibre2">This will populate the destination table where the models are kept. The results should be stored in the table <kbd class="calibre11">[dbo].[Rental_data_models]</kbd>:</p>
<div class="packt_figure"><img class="aligncenter56" src="../images/00102.gif"/></div>
<p class="calibre2">Once this is done, you need to have the evaluation procedure set as well that will help determine which model functions the best. However, this part can be done using Power BI, or reporting services, or simply just R.</p>
<p class="calibre2">This is part of R code that can be included in your visualization tool for easier comprehension:</p>
<pre class="calibre19">library(RevoScaleR) 
library(caTools) 
library(MLmetrics) 
 
#evaluate_model function; Source: Microsoft 
evaluate_model &lt;- function(observed, predicted_probability, threshold, model_name) {  
 
  # Given the observed labels and the predicted probability, plot the ROC curve and determine the AUC. 
  data &lt;- data.frame(observed, predicted_probability) 
  data$observed &lt;- as.numeric(as.character(data$observed)) 
  if(model_name =="RF"){ 
    rxRocCurve(actualVarName = "observed", predVarNames = "predicted_probability", data = data, numBreaks = 1000, title = "RF" ) 
  }else{ 
    rxRocCurve(actualVarName = "observed", predVarNames = "predicted_probability", data = data, numBreaks = 1000, title = "GBT" ) 
  } 
  ROC &lt;- rxRoc(actualVarName = "observed", predVarNames = "predicted_probability", data = data, numBreaks = 1000) 
  auc &lt;- rxAuc(ROC) 
 
  # Given the predicted probability and the threshold, determine the binary prediction. 
  predicted &lt;- ifelse(predicted_probability &gt; threshold, 1, 0)  
  predicted &lt;- factor(predicted, levels = c(0, 1))  
 
  # Build the corresponding Confusion Matrix, then compute the Accuracy, Precision, Recall, and F-Score. 
  confusion &lt;- table(observed, predicted) 
  print(model_name) 
  print(confusion)  
  tp &lt;- confusion[1, 1]  
  fn &lt;- confusion[1, 2]  
  fp &lt;- confusion[2, 1]  
  tn &lt;- confusion[2, 2]  
  accuracy &lt;- (tp + tn) / (tp + fn + fp + tn)  
  precision &lt;- tp / (tp + fp)  
  recall &lt;- tp / (tp + fn)  
  fscore &lt;- 2 * (precision * recall) / (precision + recall)  
 
  # Return the computed metrics. 
  metrics &lt;- list("Accuracy" = accuracy,  
"Precision" = precision,  
"Recall" = recall,  
"F-Score" = fscore, 
"AUC" = auc)  
  return(metrics)  
}  
 
 
RF_Scoring &lt;- rxPredict(forest_model, data = train_dr_rent, overwrite = T, type = "response",extraVarsToWrite = c("RentalCount")) 
 
Prediction_RF &lt;- rxImport(inData = RF_Scoring, stringsAsFactors = T, outFile = NULL) 
observed &lt;- Prediction_RF$RentalCount 
 
# Compute the performance metrics of the model. 
Metrics_RF &lt;- evaluate_model(observed = observed, predicted_probability = Prediction_RF$RentalCount_Pred , model_name = "RF", threshold=50) 
 
# Make Predictions, then import them into R. The observed Conversion_Flag is kept through the argument extraVarsToWrite. 
GBT_Scoring &lt;- rxPredict(btree_model,data = train_dr_rent, overwrite = T, type="prob",extraVarsToWrite = c("RentalCount")) 
 
Prediction_GBT &lt;- rxImport(inData = GBT_Scoring, stringsAsFactors = T, outFile = NULL) 
observed &lt;- Prediction_GBT$RentalCount </pre>
<p class="calibre2">The observed values should tell you which model is performing best. Once you have done this, you can choose the model and see how the predictions can be done.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Performing predictions with R Services in the SQL Server database</h1>
                
            
            <article>
                
<p class="calibre2">Calling stored procedures is the easiest way to organize your code and start predicting right away.</p>
<p class="calibre2">Again, only a sample will be shown here of how to create a stored procedure to predict new datasets:</p>
<pre class="calibre19"><strong class="calibre1">CREATE OR ALTER PROCEDURE [dbo].[Predicting_rentalCount] 
(
                 @model VARCHAR(30)
                ,@query NVARCHAR(MAX)
)
AS
BEGIN
        DECLARE @nar_model VARBINARY(MAX) = (SELECT model FROM [dbo].[Rental_data_models] WHERE model_name = @model);

        EXEC sp_execute_external_script
                 @language = N'R'
                ,@script = N'

                                #input from query
                                new_data &lt;- InputDataSet
                                
                                #model from query
                                model &lt;- unserialize(nar_model)                      

                                #prediction
                                prediction &lt;- rxPredict(model,data = new_data, overwrite = TRUE, type="response",extraVarsToWrite = c("RentalCount"))
                                Prediction_New &lt;- rxImport(inData = prediction, stringsAsFactors = T, outFile = NULL)

                                OutputDataSet &lt;- data.frame(Prediction_New)

                                '
                ,@input_data_1 =  @query
                ,@params = N'@nar_model VARBINARY(MAX)'
                ,@nar_model = @nar_model
        WITH RESULT SETS((               
                  Prediction_new NVARCHAR(1000)
                 , OrigPredictecCount NVARCHAR(1000)
        ))
END;
</strong></pre>
<p class="calibre2">Once this is done, you can start predicting using the following code:</p>
<pre class="calibre19"><strong class="calibre1">-- Example of running predictions against selected model
EXEC [dbo].[Predicting_rentalCount]  
         @model = N'Random_forest_V1'
        ,@query = N'SELECT 
                                        2014 AS Year
                                        ,5 AS Month
                                        ,12 AS Day
                                        ,1 AS WeekDay
                                        ,0 AS Holiday
                                        ,0 AS Snow
                                        ,0 AS RentalCount'</strong></pre>
<p class="calibre2">And, as a result, you will get a predicted value for the variables of <kbd class="calibre11">Year</kbd>, <kbd class="calibre11">Month</kbd>, <kbd class="calibre11">Day</kbd>, <kbd class="calibre11">WeekDay</kbd>, <kbd class="calibre11">Holiday</kbd>, and <kbd class="calibre11">Snow</kbd>:</p>
<div class="packt_figure"><img src="../images/00103.gif" class="calibre69"/></div>
<p class="calibre2">Intentionally, the field <kbd class="calibre11">OrigPredictedCount</kbd> was set to <kbd class="calibre11">0</kbd>, but the new predicted value is the value of <kbd class="calibre11">278.996</kbd> and that is based on the input variables. While checking how the model learned, it is best to also check the original value:</p>
<pre class="calibre19"><strong class="calibre1">SELECT</strong>
<strong class="calibre1">*</strong>
<strong class="calibre1">FROM Rental_data</strong>
<strong class="calibre1">WHERE [year] = 2014</strong>
<strong class="calibre1">AND [day] = 12</strong>  </pre>
<p class="calibre2">We see that there is no values in month<kbd class="calibre11">= 5</kbd>, so the model must have it learned from other values:</p>
<div class="packt_figure"><img src="../images/00104.gif" class="calibre70"/></div>
<p class="calibre2">Now that we have covered the supervised predictive algorithms, let's quickly jump into the cluster—part of the functions that <kbd class="calibre11">RevoScaleR</kbd> package supports as the only undirected algorithm.</p>
<p class="calibre2">The following is the example of how to create a simple clustering:</p>
<pre class="calibre19">library("cluster") 
# and remove the Fholidays and Fsnow variables 
DF &lt;- DF[c(1,2,3,4,5,6,7)] 
XDF &lt;- paste(tempfile(), "xdf", sep=".") 
if (file.exists(XDF)) file.remove(XDF) 
rxDataStep(inData = DF, outFile = XDF) 
 
# grab 3 random rows for starting  
centers &lt;- DF[sample.int(NROW(DF), 3, replace = TRUE),]  
 
Formula =  ~ Year + Month + Day + RentalCount + WeekDay + Holiday + Snow  
 
# Example using an XDF file as a data source 
z &lt;- rxKmeans(formula=Formula, data = DF, centers = centers) 
clusplot(DF, z$cluster, color=TRUE, shade=TRUE, labels=4, lines=0, plotchar = TRUE) </pre>
<p class="calibre2">The following is the output, which is the presentation of the clusters:</p>
<div class="packt_figure"><img src="../images/00105.gif" class="calibre71"/></div>
<p class="calibre2">To explore the clustering and play with different number of clusters, one thing for sure would be to use R code directly or to create a report for exploring the characteristics of clusters using Power BI, Excel, or SSRS.</p>
<p class="calibre2">Adding some additional information on cluster centers, statistics as <kbd class="calibre11">withinSS</kbd>, <kbd class="calibre11">betweenSS</kbd>, <kbd class="calibre11">totSS</kbd>, and others will also help us to understand the clusters.</p>
<p class="calibre2">Scree plot is also an additional and very nice presentation of choosing the correct number of clusters. Adding such graphics into a report will also help the user choose the right number of clusters and help them understand what and how clusters are formed.</p>
<p class="calibre2">Scree plot R code is used for determining where elbow is happening and does whether it has the right number of clusters; if so, three clusters would be an optimum number:</p>
<pre class="calibre19">wss &lt;- (nrow(DF) - 1) * sum(apply(DF, 2, var)) 
for (i in 2:20) 
  wss[i] &lt;- sum(kmeans(DF, centers = i)$withinss) 
plot(1:20, wss, type = "b", xlab = "Number of Clusters", ylab = "Within groups sum of squares") </pre>
<p class="calibre2">On this plot we can see where the elbow is being created and we can determine that the best solution is three clusters:</p>
<div class="packt_figure"><img src="../images/00106.gif" class="calibre72"/></div>
<p class="calibre2">Putting everything together into a report (SSRS report) makes exploring even better:</p>
<div class="packt_figure"><img class="aligncenter57" src="../images/00107.jpeg"/></div>
<p class="calibre2">The user can change the number of clusters by selecting the desired number and the report will change accordingly. The report is based on three additional procedures that export the graphs based on the inputted number of clusters:</p>
<pre class="calibre19"><strong class="calibre1">CREATE OR ALTER  PROCEDURE [dbo].[Clustering_rentalCount] 
(
                 @nof_clusters VARCHAR(2)
)
AS
BEGIN

DECLARE @SQLStat NVARCHAR(4000)
SET @SQLStat = 'SELECT  * FROM rental_data'
DECLARE @RStat NVARCHAR(4000)
SET @RStat = 'library(ggplot2)
                library(RevoScaleR)
library(cluster)
              image_file &lt;- tempfile()
              jpeg(filename = image_file, width = 400, height = 400)
 DF &lt;- data.frame(dr_rent)
               DF &lt;- DF[c(1,2,3,4,5,6,7)]
XDF &lt;- paste(tempfile(), "xdf", sep=".")
               if (file.exists(XDF)) file.remove(XDF)
                rxDataStep(inData = DF, outFile = XDF)

                centers &lt;- DF[sample.int(NROW(DF), 3, replace = TRUE),] 
Formula =  ~ Year + Month + Day + RentalCount + WeekDay + Holiday + Snow 
                rxKmeans(formula = Formula, data = XDF, numClusters='+@nof_clusters+')
                z &lt;- rxKmeans(formula=Formula, data = DF, numClusters='+@nof_clusters+')
                clusplot(DF, z$cluster, color=TRUE, shade=TRUE, labels=4, lines=0, plotchar = TRUE)
dev.off()
                    OutputDataSet &lt;- data.frame(data=readBin(file(image_file, "rb"), what=raw(), n=1e6))'

EXECUTE sp_execute_external_script
        @language = N'R'
       ,@script = @RStat
       ,@input_data_1 = @SQLStat
       ,@input_data_1_name = N'dr_rent'
WITH RESULT SETS ((plot varbinary(max)))
END;
GO
</strong></pre>
<p class="calibre2">Running this in SSMS will give you a var binary string, but adding the result of this procedure as an image in SSRS or Power BI/Excel will yield a plot derived from R.</p>
<p class="calibre2">Adding a nice visualization to your exploratory project upon building a predictive analytics system is definitely a very nice wrap-up for business and end users, as well as for the data wranglers and engineers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="calibre2">In this chapter, we have covered the extensible functionalities of the <kbd class="calibre11">RevoScaleR</kbd> package to deliver fast and good predictions based on the explored datasets. In the previous chapter <em class="calibre12">Statistical learning with RevoScaleR package</em>, we have covered data exploration, preparation and simple and bi-variate statistics. This chapter showed how <kbd class="calibre11">RevoScaleR</kbd> package was designed to work with large datasets (that overcome the limitations of RAM and single CPU), enabling spill to disk and multi threading. The same procedures can be used as well in database instances of R, for delivering the predictions to your business and data residing in the database. We have covered this aspect as well, exploring different algorithms and comparing the solutions. Once you have your model selected, you may want to use the <kbd class="calibre11">PREDICT</kbd> clause. which is a new feature in SQL Server 2017 with a slightly altered architecture. Please note that currently (at the time of writing this chapter) the model size can not exceed 100 MB if you want to use <kbd class="calibre11">PREDICT</kbd> clause. Currently, only <kbd class="calibre11">RevoScaleR</kbd> and <kbd class="calibre11">MicrosoftML</kbd> packages are supported to use this clause, and not even all <kbd class="calibre11">RevoScaleR</kbd> (and MicrosoftML) algorithms are supported—currently supported are <kbd class="calibre11">rxLinMod</kbd>, <kbd class="calibre11">rxLogit</kbd>, <kbd class="calibre11">rxBTrees</kbd>, <kbd class="calibre11">rxDtree</kbd>, <kbd class="calibre11">rxdForest</kbd>. However, this real-time scoring with <kbd class="calibre11">PREDICT</kbd> clause will definitely develop and evolve in the next release of SQL Server.</p>
<p class="calibre2">We need to predict a classification or regression problem. The majority of the problems can be supported using <kbd class="calibre11">RevoScaleR</kbd> package and many of these algorithms were also empowered by a new set of additional classifiers available in the <kbd class="calibre11">MicrosoftML</kbd> package. Exploring both packages will give your decision-making a much-needed boost. Also, storing serialized models into the database is an optimal way of storing and calling trained models (functions) that can be retrained by adding a simple logic implementation using SQL Server agents or triggers.</p>
<p class="calibre2">In <a target="_blank" href="part0102.html#318PC0-e3f81285367248f4bbc6431bcd4f926d" class="calibre10">Chapter 07</a>, <em class="calibre12">Operationalizing R Code</em>, you will learn how to operationalize your model and solution and explore different ways how to do it and some good practices.</p>


            </article>

            
        </section>
    </body></html>