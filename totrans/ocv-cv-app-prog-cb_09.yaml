- en: Chapter 9. Describing and Matching Interest Points
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Matching local templates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describing local intensity patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describing keypoints with binary features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned how to detect special points in an image
    with the objective of subsequently performing local image analysis. These keypoints
    are chosen to be distinctive enough such that if a keypoint is detected on the
    image of an object, then the same point is expected to be detected in other images
    depicting the same object. We also described some more sophisticated interest
    point detectors that can assign a representative scale factor and/or an orientation
    to a keypoint. As we will see in this recipe, this additional information can
    be useful to normalize scene representations with respect to viewpoint variations.
  prefs: []
  type: TYPE_NORMAL
- en: In order to perform image analysis based on interest points, we now need to
    build rich representations that uniquely describe each of these keypoints. This
    chapter looks at the different approaches that have been proposed to extract **descriptors**
    from interest points. These descriptors are generally 1D or 2D vectors of binary,
    integer, or floating-point numbers that describe a keypoint and its neighborhood.
    A good descriptor should be distinctive enough to uniquely represent each keypoint
    of an image; it should be robust enough to have the same points represented similarly
    in spite of possible illumination changes or viewpoint variations. Ideally, it
    should also be compact to facilitate processing operations.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most common operations accomplished with keypoints is image matching.
    The objective of performing this task could be, for example, to relate two images
    of the same scene or to detect the occurrence of a target object in an image.
    Here, we will study some basic matching strategies, a subject that will be further
    discussed in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Matching local templates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature point **matching** is the operation by which one can put in correspondence
    points from one image to points from another image (or points from an image set).
    Image points should match when they correspond to the image of the same scene
    element (or the object point) in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: A single pixel is certainly not sufficient to make a decision on the similarity
    of two keypoints. This is why an image **patch** around each keypoint must be
    considered during the matching process. If two patches correspond to the same
    scene element, then one might expect their pixels to exhibit similar values. A
    direct pixel-by-pixel comparison of pixel patches is the solution presented in
    this recipe. This is probably the simplest approach to feature point matching,
    but as we will see, it is not the most reliable one. Nevertheless, in several
    situations, it can give good results.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Most often, patches are defined as squares of odd sizes centered at the keypoint
    position. The similarity between two square patches can then be measured by comparing
    the corresponding pixel intensity values inside the patches. A simple **Sum of
    Squared Differences** (**SSD**) is a popular solution. The feature matching strategy
    then works as follows. First, the keypoints are detected in each image. Here,
    let''s use the FAST detector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We then define a rectangle of the size `11x11` that will be used to define
    patches around each keypoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The keypoints in one image are compared with all the keypoints in the other
    image. For each keypoint of the first image, the most similar patch in the second
    image is identified. This process is implemented using two nested loops, as shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note the use of the `cv::matchTemplate` function, which we will describe in
    the next section, that computes the patch similarity score. When a potential match
    is identified, this match is represented through the use of a `cv::DMatch` object.
    This object stores the index of the two matching keypoints as well as the similarity
    score.
  prefs: []
  type: TYPE_NORMAL
- en: 'The more similar the two image patches are, the higher the probability that
    these patches correspond to the same scene point. This is why it is a good idea
    to sort the resulting match points by their similarity scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: You can then simply retain the matches that pass a given similarity threshold.
    Here, we chose to keep only the `N` best matching points (we use `N=25` to facilitate
    the visualization of the matching results).
  prefs: []
  type: TYPE_NORMAL
- en: 'Interestingly, there is an OpenCV function that can display the matching results
    by concatenating the two images and joining each corresponding point by a line.
    The function is used as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the resulting match result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/00145.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The results obtained are certainly not perfect, but a visual inspection of the
    matched image points shows a number of successful matches. It can also be observed
    that the repetitive structures of the building cause some confusion. Also, since
    we tried to match all the points in the left image with the ones in the right
    image, we obtained cases where a point in the right image was matched with multiple
    points in the left image. This is an asymmetrical matching situation that can
    be corrected by, for example, keeping only the match with the best score for each
    point in the right image.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compare the image patches from each image, here we used a simple criterion,
    that is, a pixel-per-pixel sum of the squared difference specified using the `CV_TM_SQDIFF`
    flag. If we compare the point `(x,y)` of image `I [1]` with a putative match at
    `(x'',y'')` in image `I` `[2]`, then the similarity measure is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00146.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, the sum of the `(i,j)` point provides the offset to cover the square template
    centered at each point. Since the difference between adjacent pixels in similar
    patches should be small, the best-matching patches should be the ones with the
    smallest sum. This is what is done in the main loop of the matching function;
    that is, for each keypoint in one image, we identify the keypoint in the other
    image that gives the lowest sum of the squared difference. We can also reject
    matches for which this sum is over a certain threshold value. In our case, we
    simply sort them from the most similar to the least similar ones.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, the matching was done with square patches of size `11x11`. A
    larger neighborhood creates more distinctive patches, but it also makes them more
    sensitive to local scene variations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparing two image windows from a simple sum of square differences will work
    relatively well as long as the two images show the scene from similar points of
    views and similar viewing conditions. Indeed, a simple lighting change will increase
    or decrease all the pixel intensities of a patch, resulting in a large square
    difference. To make matching more invariant to lighting changes, other formulae
    that could be used to measure the similarity between two image windows exist.
    OpenCV offers a number of these. A very useful formula is the normalized sum of
    square differences (the `CV_TM_SQDIFF_NORMED` flag):'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00147.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Other similarity measures are based on the concept of correlation, defined
    in the signal processing theory as follows (with the `CV_TM_CCORR` flag):'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00148.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This value will be maximal when two patches are similar.
  prefs: []
  type: TYPE_NORMAL
- en: The identified matches are stored in a vector of the `cv::DMatch` instances.
    Essentially, the `cv::DMatch` data structure contains the first index that refers
    to an element in the first vector of keypoints and the second index that refers
    to the matching feature in the second vector of keypoints. It also contains a
    real value that represents the distance between the two matched descriptors. This
    distance value is used in the definition of `operator<` when comparing two `cv::DMatch`
    instances.
  prefs: []
  type: TYPE_NORMAL
- en: When we drew the matches in the previous section, we wanted to limit the number
    of lines to make the results more readable. Therefore, we only displayed the `25`
    matches that had the lowest distance. To do this, we used the `std::nth_element`
    function that positions the Nth element in a sorted order at the Nth position,
    with all the smaller elements placed before this element. Once this is done, the
    vector is simply purged of its remaining elements.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `cv::matchTemplate` function is at the heart of our feature matching method.
    We used it here in a very specific way, which is to compare two image patches.
    However, this function has been designed to be used in a more generic way.
  prefs: []
  type: TYPE_NORMAL
- en: Template matching
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A common task in image analysis is to detect the occurrence of a specific pattern
    or object in an image. This can be done by defining a small image of the object,
    a template, and searching for a similar occurrence in a given image. In general,
    the search is limited to a region of interest inside which we think the object
    can be found. The template is then slid over this region, and a similarity measure
    is computed at each pixel location. This is the operation performed by the `cv::matchTemplate`
    function. The input is a template image of a small size and an image over which
    the search is performed. The result is a `cv::Mat` function of floating-point
    values that correspond to the similarity score at each pixel location. If the
    template is of the size `MxN` and the image is of the size `WxH`, then the resulting
    matrix will have a size of `W-N+1xH-N+1`. In general, you will be interested in
    the location of the highest similarity; so, the typical template matching code
    will look as follows (assuming that the target variable is our template):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Remember that this is a costly operation, so you should limit the search area
    and use a template having a size of only a few pixels.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next recipe, *Describing local intensity patterns*, describes the `cv::BFMatcher`
    class that implements the matching strategy that was used in this recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describing local intensity patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The SURF and SIFT keypoint detection algorithms, discussed in [Chapter 8](part0058_split_000.html#page
    "Chapter 8. Detecting Interest Points"), *Detecting Interest Points,* define a
    location, an orientation, and a scale for each of the detected features. The scale
    factor information is useful to define the size of a window of analysis around
    each feature point. Thus, the defined neighborhood would include the same visual
    information no matter what the scale of the object to which the feature belongs
    has been pictured. This recipe will show you how to describe an interest point's
    neighborhood using **feature descriptors**. In image analysis, the visual information
    included in this neighborhood can be used to characterize each feature point in
    order to make each point distinguishable from the others. Feature descriptors
    are usually N-dimensional vectors that describe a feature point in a way that
    is invariant to change in lighting and to small perspective deformations. Generally,
    descriptors can be compared using simple distance metrics, for example, the Euclidean
    distance. Therefore, they constitute a powerful tool that can be used in feature
    matching applications.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'OpenCV 2 proposes a general interface to compute the descriptors of a list
    of keypoints. It is called `cv::DescriptorExtractor`, and we will use it in a
    way similar to the way we used the `cv::FeatureDetector` interface in the previous
    chapter. In fact, most feature-based methods include both a detector and a descriptor
    component; that''s why classes such as `cv::SURF` and `cv::SIFT` implement both
    these interfaces. This means that you have to create only one object to detect
    and describe keypoints. Here is how you can proceed if you want to match two images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: For SIFT, you will simply create a `cv::SIFT()` object instead. The result is
    a matrix (that is, a `cv::Mat` instance) that will contain as many rows as the
    number of elements in the keypoint vector. Each of these rows is an N-dimensional
    descriptor vector. In the case of the SURF descriptor, it has a default size of
    `64`, and for SIFT, the default dimension is `128`. This vector characterizes
    the intensity pattern surrounding a feature point. The more similar the two feature
    points, the closer their descriptor vectors should be.
  prefs: []
  type: TYPE_NORMAL
- en: 'These descriptors will now be used to match our keypoints. Exactly as we did
    in the previous recipe, each feature descriptor vector in the first image is compared
    to all the feature descriptors in the second image. The pair that obtains the
    best score (that is, the pair with the lowest distance between the two descriptor
    vectors) is then kept as the best match for that feature. This process is repeated
    for all the features in the first image. Very conveniently, this process is implemented
    in OpenCV in the `cv::BFMatcher` class, so we do not need to re-implement the
    double loops that we previously built. This class is used as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This class is a subclass of the `cv::DescriptorMatcher` class that defines the
    common interface for different matching strategies. The result is a vector of
    the `cv::DMatch` instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the current Hessian threshold for SURF, we obtained `90` keypoints for
    the first image and `80` for the second. The brute-force approach will then produce
    `90` matches. Using the `cv::drawMatches` class as in the previous recipe produces
    the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/00149.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'As can be seen, several of these matches correctly link a point on the left-hand
    side with its corresponding point on the right-hand side. You might notice some
    errors; some of these are due to the fact that the observed building has a symmetrical
    facade, which makes some of the local matches ambiguous. For SIFT, with the same
    number of keypoints, we obtained the following match result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/00150.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Good feature descriptors must be invariant to small changes in illumination
    and viewpoint and to the presence of image noise. Therefore, they are often based
    on local intensity differences. This is the case for the SURF descriptors, which
    locally apply the following simple kernels around a keypoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00151.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The first kernel simply measures the local intensity difference in the horizontal
    direction (designated as `dx`), and the second measures this difference in the
    vertical direction (designated as `dy`). The size of the neighborhood used to
    extract the descriptor vector is generally defined as 20 times the scale factor
    of the feature (that is, `20σ`). This square region is then split into `4x4` smaller
    square subregions. For each subregion, the kernel responses (`dx` and `dy`) are
    computed at `5x5` regularly-spaced locations (with the kernel size being `2σ`).
    All of these responses are summed up as follows in order to extract four descriptor
    values for each subregion:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00152.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Since there are `4x4=16` subregions, we have a total of `64` descriptor values.
    Note that in order to give more importance to the neighboring pixels, that is,
    values closer to the keypoint, the kernel responses are weighted by a Gaussian
    centered at the keypoint location (with `σ=3.3`).
  prefs: []
  type: TYPE_NORMAL
- en: The `dx` and `dy` responses are also used to estimate the orientation of the
    feature. These values are computed (with a kernel size of `4σ`) within a circular
    neighborhood of radius `6σ` at locations regularly spaced by intervals of `σ`.
    For a given orientation, the responses inside a certain angular interval (`π/3`)
    are summed, and the orientation giving the longest vector is defined as the dominant
    orientation.
  prefs: []
  type: TYPE_NORMAL
- en: 'SIFT is a richer descriptor that uses an image gradient instead of simple intensity
    differences. It also splits the square neighborhood around each keypoint into
    `4x4` subregions (it is also possible to use `8x8` or `2x2` subregions). Inside
    each of these regions, a histogram of gradient orientations is built. The orientations
    are discretized into 8 bins, and each gradient orientation entry is incremented
    by a value proportional to the gradient magnitude. This is illustrated by the
    following figure, inside which each star-shaped arrow set represents a local histogram
    of a gradient orientation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00153.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: These `16` histograms of 8 bins each concatenated together then produce a descriptor
    of `128` dimensions. Note that as for SURF, the gradient values are weighted by
    a Gaussian filter centered at the keypoint location in order to make the descriptor
    less sensitive to sudden changes in gradient orientations at the perimeter of
    the defined neighborhood. The final descriptor is then normalized to make the
    distance measurement more consistent.
  prefs: []
  type: TYPE_NORMAL
- en: 'With SURF and SIFT features and descriptors, scale-invariant matching can be
    achieved. Here is an example that shows the SURF match result for two images at
    different scales (here, the 50 best matches have been displayed):'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00154.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The match result produced by any matching algorithm always contains a significant
    number of incorrect matches. In order to improve the quality of the match set,
    there exist a number of strategies. Two of them are discussed here.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-checking matches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A simple approach to validate the matches obtained is to repeat the same procedure
    a second time, but this time, each keypoint of the second image is compared with
    all the keypoints of the first image. A match is considered valid only if we obtain
    the same pair of keypoints in both directions (that is, each keypoint is the best
    match of the other). The `cv::BFMatcher` function gives the option to use this
    strategy. It is indeed included as a flag; when set to `true`, it forces the function
    to perform the reciprocal match cross-check:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The improved match results are as shown in the following screenshot (in the
    case of SURF):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cross-checking matches](img/00155.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The ratio test
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have already noted that repetitive elements in scene objects create unreliable
    results because of the ambiguity in matching visually similar structures. What
    happens in such cases is that a keypoint will match well with more than one other
    keypoint. Since the probability of selecting the wrong correspondence is high,
    it might be preferable to reject a match in this case.
  prefs: []
  type: TYPE_NORMAL
- en: To use this strategy, we then need to find the best two matching points of each
    keypoint. This can be done by using the `knnMatch` method of the `cv::DescriptorMatcher`
    class. Since we want only two best matches, we specify `k=2`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to reject all the best matches with a matching distance similar
    to that of their second best match. Since `knnMatch` produces a `std::vector`
    class of `std::vector` (this second vector is of size `k`), we do this by looping
    over each keypoint match and perform a ratio test (this ratio will be one if the
    two best distances are equal). Here is how we can do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The initial match set made up of `90` pairs is now reduced to `23` pairs; a
    good proportion of these are now correct matches:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The ratio test](img/00156.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Distance thresholding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An even simpler strategy consists of rejecting matches for which the distance
    between their descriptors is too high. This is done using the `radiusMatch` method
    of the `cv::DescriptorMatcher` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is again a `std::vector` class of `std::vector` because the method
    will retain all the matches with a distance smaller than the specified threshold.
    This means that a given keypoint might have more than one matching point in the
    other image. Conversely, other keypoints will not have any matches associated
    with them (the corresponding inner `std::vector` class will then have a size of
    `0`). This time, the initial match set of `90` pairs is reduced to `37` pairs
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distance thresholding](img/00157.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Obviously, you can combine all these strategies in order to improve your matching
    results.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Detecting scale-invariant features* recipe in [Chapter 8](part0058_split_000.html#page
    "Chapter 8. Detecting Interest Points"), *Detecting Interest Points,* presents
    the associated SURF and SIFT feature detectors and provides more references on
    the subject
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Matching images using random sample consensus* recipe in [Chapter 10](part0067_split_000.html#page
    "Chapter 10. Estimating Projective Relations in Images"), *Estimating Projective
    Relations in Images*, explains how to use the image and the scene geometry in
    order to obtain a match set of even better quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The *Matching feature points in stereo pairs: A comparative study of some matching
    strategies* article by E. Vincent and R. Laganière in *Machine, Graphics and Vision,
    pp. 237-260, 2001*, describes other simple matching strategies that could be used
    to improve the quality of the match set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describing keypoints with binary features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we learned how to describe a keypoint using rich descriptors
    extracted from the image intensity gradient. These descriptors are floating-point
    vectors that have a dimension of `64`, `128`, or sometimes even longer. This makes
    them costly to manipulate. In order to reduce the memory and computational load
    associated with these descriptors, the idea of using binary descriptors has been
    recently introduced. The challenge here is to make them easy to compute and yet
    keep them robust to scene and viewpoint changes. This recipe describes some of
    these binary descriptors. In particular, we will look at the ORB and BRISK descriptors
    for which we presented their associated feature point detectors in [Chapter 8](part0058_split_000.html#page
    "Chapter 8. Detecting Interest Points"), *Detecting Interest Points*.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Owing to the nice generic interface on top of which the OpenCV detectors and
    the descriptors module are built, using a binary descriptor such as ORB is no
    different from using descriptors such as SURF and SIFT. The complete feature-based
    image matching sequence is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The only difference resides in the use of the **Hamming** norm (the `cv::NORM_HAMMING`
    flag) that measures the distance between two binary descriptors by counting the
    number of bits that are dissimilar. On many processors, this operation is efficiently
    implemented by using an exclusive OR operation, followed by a simple bit count.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the result of the matching:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/00158.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Similar results will be obtained with another popular binary feature detector/descriptor:
    BRISK. In this case, the `cv::DescriptorExtractor` instance is created by the
    new `cv::BRISK(40)` call. As we learned in the previous chapter, its first parameter
    is a threshold that controls the number of detected points.'
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ORB algorithm detects oriented feature points at multiple scales. Based
    on this result, the ORB descriptor extracts a representation of each keypoint
    by using simple intensity comparisons. In fact, ORB builds on a previously proposed
    descriptor called BRIEF. This later creates a binary descriptor by simply selecting
    a random pair of points inside a defined neighborhood around the keypoint. The
    intensity values of the two pixel points are then compared, and if the first point
    has a higher intensity, then the value `1` is assigned to the corresponding descriptor
    bit value. Otherwise, the value `0` is assigned. Repeating this test on a number
    of random pairs generates a descriptor that is made up of several bits; typically,
    `128` to `512` bits (pairwise tests) are used.
  prefs: []
  type: TYPE_NORMAL
- en: This is the scheme used by ORB. Then, the decision to be made is which set of
    point pairs should be used to build the descriptor. Indeed, even if the point
    pairs are randomly chosen, once they have been selected, the same set of binary
    tests must be performed to build the descriptor of all the keypoints in order
    to ensure consistency of the results. To make the descriptor more distinctive,
    intuition tells us that some choices must be better than others. Also, the fact
    that the orientation of each keypoint is known introduces some bias in the intensity
    pattern distribution when this one is normalized with respect to this orientation
    (that is, when the point coordinates are given relative to this keypoint orientation).
    From these considerations and the experimental validation, ORB has identified
    a set of `256` point pairs with high variance and minimal pairwise correlation.
    In other words, the selected binary tests are the ones that have an equal chance
    of being `0` or `1` over a variety of keypoints and also those that are as independent
    from each other as possible.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the parameters that control the feature detection process, the
    `cv::ORB` constructor includes two parameters related to its descriptor. One parameter
    is used to specify the patch size inside which the point pairs are selected (the
    default is `31x31`). The second parameter allows you to perform tests with a triplet
    or quadruplet of points instead of the default point pairs. Note that it is highly
    recommended that you use the default settings.
  prefs: []
  type: TYPE_NORMAL
- en: The descriptor of BRISK is very similar. It is also based on pairwise intensity
    comparisons with two differences. First, instead of randomly selecting the points
    from the `31x31` points of the neighborhood, the chosen points are selected from
    a sampling pattern of a set of concentric circles (made up of `60` points) with
    locations that are equally spaced. Second, the intensity at each of these sample
    points is a Gaussian-smoothed value with a σ value proportional to the distance
    from the central keypoint. From these points, BRISK selects `512` point pairs.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Several other binary descriptors exist, and interested readers should take a
    look at the scientific literature to learn more on this subject. Since it is also
    available in OpenCV, we will describe one additional descriptor here.
  prefs: []
  type: TYPE_NORMAL
- en: FREAK
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: FREAK stands for **Fast Retina Keypoint**. This is also a binary descriptor,
    but it does not have an associated detector. It can be applied on any set of keypoints
    detected, for example, SIFT, SURF, or ORB.
  prefs: []
  type: TYPE_NORMAL
- en: Like BRISK, the FREAK descriptor is also based on a sampling pattern defined
    on concentric circles. However, to design their descriptor, the authors used an
    analogy of the human eye. They observed that on the retina, the density of the
    ganglion cells decreases with the increase in the distance to the fovea. Consequently,
    they built a sampling pattern made of `43` points in which the density of a point
    is much greater near the central point. To obtain its intensity, each point is
    filtered with a Gaussian kernel that has a size that also increases with the distance
    to the center.
  prefs: []
  type: TYPE_NORMAL
- en: In order to identify the pairwise comparisons that should be performed, an experimental
    validation has been performed by following a strategy similar to the one used
    for ORB. By analyzing several thousands of keypoints, the binary tests with the
    highest variance and lowest correlation are retained, resulting in `512` pairs.
  prefs: []
  type: TYPE_NORMAL
- en: FREAK also introduces the idea of performing the descriptor comparisons in cascade.
    That is, the first `128` bits representing coarser information (corresponding
    to the tests performed at the periphery on larger Gaussian kernels) are performed
    first. Only if the compared descriptors pass this initial step will the remaining
    tests be performed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the keypoints detected with ORB, we extract the FREAK descriptors by
    simply creating the `cv::DescriptorExtractor` instance as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The match result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![FREAK](img/00159.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following figure illustrates the sampling pattern used for the three descriptors
    presented in this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '![FREAK](img/00160.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The first square is the ORB descriptor in which point pairs are randomly selected
    on a square grid. Each pair of points linked by a line represent a possible test
    to compare the two pixel intensities. Here, we show only 8 such pairs; the default
    ORB uses 256 pairs. The middle square corresponds to the BRISK sampling pattern.
    Points are uniformly sampled on the shown circles (for clarity, we only identify
    the points on the first circle here). Finally, the third square shows the log-polar
    sampling grid of FREAK. While BRISK has a uniform distribution of points, FREAK
    has a higher density of points closer to the center. For example, in BRISK, you
    find 20 points on the outer circle, while in the case of FREAK, its outer circle
    includes only 6 points.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Detecting FAST features at multiple scales* recipe in [Chapter 8](part0058_split_000.html#page
    "Chapter 8. Detecting Interest Points"), *Detecting Interest Points,* presents
    the associated BRISK and ORB feature detectors and provides more references on
    the subject
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The *BRIEF: Computing a Local Binary Descriptor Very Fast* article by E. M.
    Calonder, V. Lepetit, M. Ozuysal, T. Trzcinski, C. Strecha, and P. Fua in *IEEE
    Transactions on Pattern Analysis and Machine Intelligence, 2012*, describes the
    BRIEF feature descriptor that inspires the presented binary descriptors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The *FREAK: Fast Retina Keypoint* article by A.Alahi, R. Ortiz, and P. Vandergheynst
    in *IEEE Conference on Computer Vision and Pattern Recognition, 2012*, describes
    the FREAK feature descriptor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
