- en: Chapter 9. Describing and Matching Interest Points
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章 描述和匹配兴趣点
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍以下食谱：
- en: Matching local templates
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 匹配局部模板
- en: Describing local intensity patterns
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述局部强度模式
- en: Describing keypoints with binary features
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用二进制特征描述关键点
- en: Introduction
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: In the previous chapter, we learned how to detect special points in an image
    with the objective of subsequently performing local image analysis. These keypoints
    are chosen to be distinctive enough such that if a keypoint is detected on the
    image of an object, then the same point is expected to be detected in other images
    depicting the same object. We also described some more sophisticated interest
    point detectors that can assign a representative scale factor and/or an orientation
    to a keypoint. As we will see in this recipe, this additional information can
    be useful to normalize scene representations with respect to viewpoint variations.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何通过检测图像中的特殊点来进行局部图像分析。这些关键点是选择得足够独特，以至于如果一个关键点在物体的图像中被检测到，那么在描述同一物体的其他图像中，预期也会检测到相同的点。我们还描述了一些更复杂的兴趣点检测器，这些检测器可以为关键点分配一个代表性的尺度因子和/或一个方向。正如我们将在这份食谱中看到的，这些附加信息可以用来根据视点变化对场景表示进行归一化。
- en: In order to perform image analysis based on interest points, we now need to
    build rich representations that uniquely describe each of these keypoints. This
    chapter looks at the different approaches that have been proposed to extract **descriptors**
    from interest points. These descriptors are generally 1D or 2D vectors of binary,
    integer, or floating-point numbers that describe a keypoint and its neighborhood.
    A good descriptor should be distinctive enough to uniquely represent each keypoint
    of an image; it should be robust enough to have the same points represented similarly
    in spite of possible illumination changes or viewpoint variations. Ideally, it
    should also be compact to facilitate processing operations.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 为了基于兴趣点进行图像分析，我们现在需要构建丰富的表示，以独特地描述这些关键点中的每一个。本章探讨了已经提出的从兴趣点中提取**描述符**的不同方法。这些描述符通常是描述关键点和其邻域的1D或2D的二进制、整数或浮点数向量。一个好的描述符应该足够独特，以唯一地表示图像中的每个关键点；它应该足够鲁棒，以便在可能的光照变化或视点变化的情况下，具有相似点的相似表示。理想情况下，它还应该紧凑，以方便处理操作。
- en: One of the most common operations accomplished with keypoints is image matching.
    The objective of performing this task could be, for example, to relate two images
    of the same scene or to detect the occurrence of a target object in an image.
    Here, we will study some basic matching strategies, a subject that will be further
    discussed in the next chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 使用关键点完成的最常见操作之一是图像匹配。执行此任务的目标可能是，例如，将同一场景的两个图像联系起来，或者在图像中检测目标对象的发生。在这里，我们将研究一些基本的匹配策略，这个主题将在下一章中进一步讨论。
- en: Matching local templates
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 匹配局部模板
- en: Feature point **matching** is the operation by which one can put in correspondence
    points from one image to points from another image (or points from an image set).
    Image points should match when they correspond to the image of the same scene
    element (or the object point) in the real world.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 特征点**匹配**是通过将一个图像中的点与另一个图像（或图像集中的点）中的点对应起来的操作。当图像点对应于现实世界中同一场景元素（或物体点）的图像时，它们应该匹配。
- en: A single pixel is certainly not sufficient to make a decision on the similarity
    of two keypoints. This is why an image **patch** around each keypoint must be
    considered during the matching process. If two patches correspond to the same
    scene element, then one might expect their pixels to exhibit similar values. A
    direct pixel-by-pixel comparison of pixel patches is the solution presented in
    this recipe. This is probably the simplest approach to feature point matching,
    but as we will see, it is not the most reliable one. Nevertheless, in several
    situations, it can give good results.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 单个像素当然不足以判断两个关键点的相似性。这就是为什么在匹配过程中必须考虑每个关键点周围的图像**块**。如果两个块对应于同一场景元素，那么可以预期它们的像素将具有相似值。本食谱中提出的解决方案是直接逐像素比较像素块。这可能是特征点匹配的最简单方法，但正如我们将看到的，它并不是最可靠的。尽管如此，在几种情况下，它仍然可以给出良好的结果。
- en: How to do it...
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Most often, patches are defined as squares of odd sizes centered at the keypoint
    position. The similarity between two square patches can then be measured by comparing
    the corresponding pixel intensity values inside the patches. A simple **Sum of
    Squared Differences** (**SSD**) is a popular solution. The feature matching strategy
    then works as follows. First, the keypoints are detected in each image. Here,
    let''s use the FAST detector:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，块被定义为以关键点位置为中心的奇数大小的正方形。然后可以通过比较块内相应的像素强度值来测量两个正方形块之间的相似度。简单的**平方差之和**（**SSD**）是一个流行的解决方案。特征匹配策略的工作方式如下。首先，在每个图像中检测关键点。这里，我们使用FAST检测器：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We then define a rectangle of the size `11x11` that will be used to define
    patches around each keypoint:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义一个大小为`11x11`的矩形，它将用于定义每个关键点周围的块：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The keypoints in one image are compared with all the keypoints in the other
    image. For each keypoint of the first image, the most similar patch in the second
    image is identified. This process is implemented using two nested loops, as shown
    in the following code:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 将一个图像中的关键点与另一个图像中的所有关键点进行比较。对于第一个图像中的每个关键点，在第二个图像中识别最相似的块。这个过程通过以下代码中的两个嵌套循环实现：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note the use of the `cv::matchTemplate` function, which we will describe in
    the next section, that computes the patch similarity score. When a potential match
    is identified, this match is represented through the use of a `cv::DMatch` object.
    This object stores the index of the two matching keypoints as well as the similarity
    score.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 注意使用`cv::matchTemplate`函数，我们将在下一节中描述它，该函数计算块相似度分数。当识别出潜在匹配时，这个匹配通过使用`cv::DMatch`对象来表示。该对象存储两个匹配关键点的索引以及相似度分数。
- en: 'The more similar the two image patches are, the higher the probability that
    these patches correspond to the same scene point. This is why it is a good idea
    to sort the resulting match points by their similarity scores:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 两个图像块越相似，这些块对应相同场景点的概率就越高。这就是为什么按相似度分数对结果匹配点进行排序是一个好主意：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You can then simply retain the matches that pass a given similarity threshold.
    Here, we chose to keep only the `N` best matching points (we use `N=25` to facilitate
    the visualization of the matching results).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以简单地保留通过给定相似度阈值的匹配。在这里，我们选择只保留`N`个最佳匹配点（我们使用`N=25`来便于可视化匹配结果）。
- en: 'Interestingly, there is an OpenCV function that can display the matching results
    by concatenating the two images and joining each corresponding point by a line.
    The function is used as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，有一个OpenCV函数可以通过连接两个图像并使用线连接每个对应点来显示匹配结果。该函数的使用方法如下：
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here is the resulting match result:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是得到的匹配结果：
- en: '![How to do it...](img/00145.jpeg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![如何操作...](img/00145.jpeg)'
- en: How it works...
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The results obtained are certainly not perfect, but a visual inspection of the
    matched image points shows a number of successful matches. It can also be observed
    that the repetitive structures of the building cause some confusion. Also, since
    we tried to match all the points in the left image with the ones in the right
    image, we obtained cases where a point in the right image was matched with multiple
    points in the left image. This is an asymmetrical matching situation that can
    be corrected by, for example, keeping only the match with the best score for each
    point in the right image.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 得到的结果当然不是完美的，但通过视觉检查匹配的图像点显示有许多成功的匹配。也可以观察到建筑的重复结构造成了一些混淆。此外，由于我们试图将左图像中的所有点与右图像中的点匹配，我们得到了右图像中的一个点与左图像中的多个点匹配的情况。这是一种不对称的匹配情况，可以通过例如，只保留每个右图像中每个点的最佳得分匹配来纠正。
- en: 'To compare the image patches from each image, here we used a simple criterion,
    that is, a pixel-per-pixel sum of the squared difference specified using the `CV_TM_SQDIFF`
    flag. If we compare the point `(x,y)` of image `I [1]` with a putative match at
    `(x'',y'')` in image `I` `[2]`, then the similarity measure is given as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较每个图像中的图像块，这里我们使用了一个简单的标准，即使用`CV_TM_SQDIFF`标志指定的像素级平方差之和。如果我们比较图像`I [1]`中的点`(x,y)`与图像`I`
    `[2]`中的潜在匹配点`(x',y')`，那么相似度度量如下：
- en: '![How it works...](img/00146.jpeg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/00146.jpeg)'
- en: Here, the sum of the `(i,j)` point provides the offset to cover the square template
    centered at each point. Since the difference between adjacent pixels in similar
    patches should be small, the best-matching patches should be the ones with the
    smallest sum. This is what is done in the main loop of the matching function;
    that is, for each keypoint in one image, we identify the keypoint in the other
    image that gives the lowest sum of the squared difference. We can also reject
    matches for which this sum is over a certain threshold value. In our case, we
    simply sort them from the most similar to the least similar ones.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`(i,j)`点的和提供了覆盖以每个点为中心的平方模板的偏移量。由于相似块中相邻像素之间的差异应该很小，因此最佳匹配的块应该是具有最小和的块。这正是匹配函数的主循环中所做的；也就是说，对于一张图像中的每个关键点，我们识别另一张图像中的关键点，该关键点给出了最低的平方差和。我们还可以拒绝那些和超过某个阈值值的匹配。在我们的情况下，我们只是按从最相似到最不相似排序。
- en: In our example, the matching was done with square patches of size `11x11`. A
    larger neighborhood creates more distinctive patches, but it also makes them more
    sensitive to local scene variations.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，匹配是通过大小为`11x11`的正方形块完成的。更大的邻域会产生更独特的块，但它也使它们对局部场景变化更敏感。
- en: 'Comparing two image windows from a simple sum of square differences will work
    relatively well as long as the two images show the scene from similar points of
    views and similar viewing conditions. Indeed, a simple lighting change will increase
    or decrease all the pixel intensities of a patch, resulting in a large square
    difference. To make matching more invariant to lighting changes, other formulae
    that could be used to measure the similarity between two image windows exist.
    OpenCV offers a number of these. A very useful formula is the normalized sum of
    square differences (the `CV_TM_SQDIFF_NORMED` flag):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 只要两个图像从相似的角度和相似的观察条件显示场景，从简单的平方差之和比较两个图像窗口就会相对有效。确实，简单的光照变化会增加或减少块中所有像素的强度，从而导致大的平方差。为了使匹配对光照变化更不变，存在可以用来测量两个图像窗口之间相似性的其他公式。OpenCV提供了一系列这些公式。一个非常有用的公式是归一化平方差之和（`CV_TM_SQDIFF_NORMED`标志）：
- en: '![How it works...](img/00147.jpeg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作...](img/00147.jpeg)'
- en: 'Other similarity measures are based on the concept of correlation, defined
    in the signal processing theory as follows (with the `CV_TM_CCORR` flag):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 其他相似度度量基于相关性的概念，在信号处理理论中定义为以下（使用`CV_TM_CCORR`标志）：
- en: '![How it works...](img/00148.jpeg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作...](img/00148.jpeg)'
- en: This value will be maximal when two patches are similar.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当两个块相似时，此值将达到最大。
- en: The identified matches are stored in a vector of the `cv::DMatch` instances.
    Essentially, the `cv::DMatch` data structure contains the first index that refers
    to an element in the first vector of keypoints and the second index that refers
    to the matching feature in the second vector of keypoints. It also contains a
    real value that represents the distance between the two matched descriptors. This
    distance value is used in the definition of `operator<` when comparing two `cv::DMatch`
    instances.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 识别出的匹配项存储在一个`cv::DMatch`实例的向量中。本质上，`cv::DMatch`数据结构包含第一个索引，它指向第一个关键点向量中的一个元素，以及第二个索引，它指向第二个关键点向量中的匹配特征。它还包含一个代表两个匹配描述符之间距离的实数值。这个距离值用于比较两个`cv::DMatch`实例时`operator<`的定义。
- en: When we drew the matches in the previous section, we wanted to limit the number
    of lines to make the results more readable. Therefore, we only displayed the `25`
    matches that had the lowest distance. To do this, we used the `std::nth_element`
    function that positions the Nth element in a sorted order at the Nth position,
    with all the smaller elements placed before this element. Once this is done, the
    vector is simply purged of its remaining elements.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中绘制匹配时，我们希望限制线条的数量以使结果更易于阅读。因此，我们只显示了距离最低的`25`个匹配项。为此，我们使用了`std::nth_element`函数，该函数将排序顺序中的第N个元素放置在第N个位置，所有较小的元素都放置在此元素之前。一旦完成，向量就简单地清除了其剩余的元素。
- en: There's more...
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The `cv::matchTemplate` function is at the heart of our feature matching method.
    We used it here in a very specific way, which is to compare two image patches.
    However, this function has been designed to be used in a more generic way.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`cv::matchTemplate`函数是我们特征匹配方法的核心。我们在这里以非常具体的方式使用它，即比较两个图像块。然而，这个函数被设计成以更通用的方式使用。'
- en: Template matching
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模板匹配
- en: 'A common task in image analysis is to detect the occurrence of a specific pattern
    or object in an image. This can be done by defining a small image of the object,
    a template, and searching for a similar occurrence in a given image. In general,
    the search is limited to a region of interest inside which we think the object
    can be found. The template is then slid over this region, and a similarity measure
    is computed at each pixel location. This is the operation performed by the `cv::matchTemplate`
    function. The input is a template image of a small size and an image over which
    the search is performed. The result is a `cv::Mat` function of floating-point
    values that correspond to the similarity score at each pixel location. If the
    template is of the size `MxN` and the image is of the size `WxH`, then the resulting
    matrix will have a size of `W-N+1xH-N+1`. In general, you will be interested in
    the location of the highest similarity; so, the typical template matching code
    will look as follows (assuming that the target variable is our template):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分析中的常见任务是在图像中检测特定模式或物体的出现。这可以通过定义一个对象的小图像、模板，并在给定图像中搜索相似出现来完成。通常，搜索被限制在感兴趣区域内，我们认为该区域可能包含该对象。然后模板在这个区域上滑动，并在每个像素位置计算相似度度量。这是`cv::matchTemplate`函数执行的操作。输入是一个小尺寸的模板图像和搜索图像。结果是每个像素位置的相似度分数对应的`cv::Mat`浮点值函数。如果模板的大小是`MxN`，图像的大小是`WxH`，那么结果矩阵的大小将是`W-N+1xH-N+1`。通常，你将关注最高相似度的位置；因此，典型的模板匹配代码将如下所示（假设目标变量是我们的模板）：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Remember that this is a costly operation, so you should limit the search area
    and use a template having a size of only a few pixels.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 记住这是一个代价较高的操作，因此你应该限制搜索区域，并使用只有几个像素大小的模板。
- en: See also
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The next recipe, *Describing local intensity patterns*, describes the `cv::BFMatcher`
    class that implements the matching strategy that was used in this recipe
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一个菜谱，*描述局部强度模式*，描述了实现本菜谱中使用的匹配策略的`cv::BFMatcher`类。
- en: Describing local intensity patterns
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 描述局部强度模式
- en: The SURF and SIFT keypoint detection algorithms, discussed in [Chapter 8](part0058_split_000.html#page
    "Chapter 8. Detecting Interest Points"), *Detecting Interest Points,* define a
    location, an orientation, and a scale for each of the detected features. The scale
    factor information is useful to define the size of a window of analysis around
    each feature point. Thus, the defined neighborhood would include the same visual
    information no matter what the scale of the object to which the feature belongs
    has been pictured. This recipe will show you how to describe an interest point's
    neighborhood using **feature descriptors**. In image analysis, the visual information
    included in this neighborhood can be used to characterize each feature point in
    order to make each point distinguishable from the others. Feature descriptors
    are usually N-dimensional vectors that describe a feature point in a way that
    is invariant to change in lighting and to small perspective deformations. Generally,
    descriptors can be compared using simple distance metrics, for example, the Euclidean
    distance. Therefore, they constitute a powerful tool that can be used in feature
    matching applications.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第8章](part0058_split_000.html#page "第8章. 检测兴趣点")中讨论的SURF和SIFT关键点检测算法，*检测兴趣点*，为每个检测到的特征定义了一个位置、一个方向和一个比例。比例因子信息对于定义每个特征点周围的分析窗口的大小是有用的。因此，定义的邻域将包含与对象的比例无关的相同视觉信息。这个菜谱将向你展示如何使用**特征描述符**来描述兴趣点的邻域。在图像分析中，这个邻域包含的视觉信息可以用来表征每个特征点，以便使每个点与其他点区分开来。特征描述符通常是N维向量，以对光照变化和小型透视变形不变的方式描述特征点。通常，描述符可以使用简单的距离度量进行比较，例如欧几里得距离。因此，它们构成了一个强大的工具，可以在特征匹配应用中使用。
- en: How to do it...
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'OpenCV 2 proposes a general interface to compute the descriptors of a list
    of keypoints. It is called `cv::DescriptorExtractor`, and we will use it in a
    way similar to the way we used the `cv::FeatureDetector` interface in the previous
    chapter. In fact, most feature-based methods include both a detector and a descriptor
    component; that''s why classes such as `cv::SURF` and `cv::SIFT` implement both
    these interfaces. This means that you have to create only one object to detect
    and describe keypoints. Here is how you can proceed if you want to match two images:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV 2提出了一种计算关键点列表描述符的通用接口。它被称为`cv::DescriptorExtractor`，我们将以类似于我们在上一章中使用`cv::FeatureDetector`接口的方式使用它。事实上，大多数基于特征的方法都包括检测器和描述符组件；这就是为什么`cv::SURF`和`cv::SIFT`类实现了这两个接口。这意味着你只需要创建一个对象来检测和描述关键点。以下是如何匹配两张图像的步骤：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: For SIFT, you will simply create a `cv::SIFT()` object instead. The result is
    a matrix (that is, a `cv::Mat` instance) that will contain as many rows as the
    number of elements in the keypoint vector. Each of these rows is an N-dimensional
    descriptor vector. In the case of the SURF descriptor, it has a default size of
    `64`, and for SIFT, the default dimension is `128`. This vector characterizes
    the intensity pattern surrounding a feature point. The more similar the two feature
    points, the closer their descriptor vectors should be.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于SIFT，你只需简单地创建一个`cv::SIFT()`对象即可。结果是包含与关键点向量中元素数量一样多的行的矩阵（即`cv::Mat`实例）。这些行中的每一行都是一个N维描述符向量。在SURF描述符的情况下，它有一个默认大小为`64`，而对于SIFT，默认维度是`128`。这个向量表征了特征点周围的强度模式。两个特征点越相似，它们的描述符向量应该越接近。
- en: 'These descriptors will now be used to match our keypoints. Exactly as we did
    in the previous recipe, each feature descriptor vector in the first image is compared
    to all the feature descriptors in the second image. The pair that obtains the
    best score (that is, the pair with the lowest distance between the two descriptor
    vectors) is then kept as the best match for that feature. This process is repeated
    for all the features in the first image. Very conveniently, this process is implemented
    in OpenCV in the `cv::BFMatcher` class, so we do not need to re-implement the
    double loops that we previously built. This class is used as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这些描述符现在将被用来匹配我们的关键点。正如我们在前面的食谱中所做的那样，第一幅图中的每个特征描述符向量与第二幅图中的所有特征描述符进行比较。获得最佳分数的配对（即两个描述符向量之间距离最低的配对）被保留为该特征的最佳匹配。这个过程会重复应用于第一幅图中的所有特征。非常方便的是，这个过程在OpenCV的`cv::BFMatcher`类中得到了实现，因此我们不需要重新实现我们之前构建的双循环。这个类可以这样使用：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This class is a subclass of the `cv::DescriptorMatcher` class that defines the
    common interface for different matching strategies. The result is a vector of
    the `cv::DMatch` instances.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类是`cv::DescriptorMatcher`类的子类，它定义了不同匹配策略的通用接口。结果是`cv::DMatch`实例的向量。
- en: 'With the current Hessian threshold for SURF, we obtained `90` keypoints for
    the first image and `80` for the second. The brute-force approach will then produce
    `90` matches. Using the `cv::drawMatches` class as in the previous recipe produces
    the following image:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用当前SURF的Hessian阈值，我们为第一幅图像获得了`90`个关键点，为第二幅图像获得了`80`个。然后暴力方法将产生`90`个匹配。使用与前面食谱中相同的`cv::drawMatches`类会产生以下图像：
- en: '![How to do it...](img/00149.jpeg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![如何做...](img/00149.jpeg)'
- en: 'As can be seen, several of these matches correctly link a point on the left-hand
    side with its corresponding point on the right-hand side. You might notice some
    errors; some of these are due to the fact that the observed building has a symmetrical
    facade, which makes some of the local matches ambiguous. For SIFT, with the same
    number of keypoints, we obtained the following match result:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如所示，其中一些匹配正确地将左侧的点与其对应的右侧点联系起来。你可能会注意到一些错误；其中一些是由于观察到的建筑有一个对称的立面，这使得一些局部匹配变得模糊。对于SIFT，在相同数量的关键点下，我们得到了以下匹配结果：
- en: '![How to do it...](img/00150.jpeg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![如何做...](img/00150.jpeg)'
- en: How it works...
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Good feature descriptors must be invariant to small changes in illumination
    and viewpoint and to the presence of image noise. Therefore, they are often based
    on local intensity differences. This is the case for the SURF descriptors, which
    locally apply the following simple kernels around a keypoint:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 良好的特征描述符必须对光照和视角的小变化以及图像噪声的存在保持不变。因此，它们通常基于局部强度差异。这是SURF描述符的情况，它们在关键点周围局部应用以下简单的核：
- en: '![How it works...](img/00151.jpeg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![How it works...](img/00151.jpeg)'
- en: 'The first kernel simply measures the local intensity difference in the horizontal
    direction (designated as `dx`), and the second measures this difference in the
    vertical direction (designated as `dy`). The size of the neighborhood used to
    extract the descriptor vector is generally defined as 20 times the scale factor
    of the feature (that is, `20σ`). This square region is then split into `4x4` smaller
    square subregions. For each subregion, the kernel responses (`dx` and `dy`) are
    computed at `5x5` regularly-spaced locations (with the kernel size being `2σ`).
    All of these responses are summed up as follows in order to extract four descriptor
    values for each subregion:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个核简单地测量水平方向上的局部强度差异（指定为`dx`），第二个核测量垂直方向上的这种差异（指定为`dy`）。用于提取描述符向量的邻域大小通常定义为特征缩放因子的20倍（即`20σ`）。然后，将这个正方形区域分割成`4x4`更小的正方形子区域。对于每个子区域，在`5x5`规则间隔的位置（核大小为`2σ`）计算核响应（`dx`和`dy`）。所有这些响应都按以下方式汇总，以便为每个子区域提取四个描述符值：
- en: '![How it works...](img/00152.jpeg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![How it works...](img/00152.jpeg)'
- en: Since there are `4x4=16` subregions, we have a total of `64` descriptor values.
    Note that in order to give more importance to the neighboring pixels, that is,
    values closer to the keypoint, the kernel responses are weighted by a Gaussian
    centered at the keypoint location (with `σ=3.3`).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 由于有`4x4=16`个子区域，我们总共有`64`个描述符值。注意，为了给邻近像素更多的权重，即更接近关键点的值，核响应通过一个以关键点位置为中心的高斯加权（`σ=3.3`）。
- en: The `dx` and `dy` responses are also used to estimate the orientation of the
    feature. These values are computed (with a kernel size of `4σ`) within a circular
    neighborhood of radius `6σ` at locations regularly spaced by intervals of `σ`.
    For a given orientation, the responses inside a certain angular interval (`π/3`)
    are summed, and the orientation giving the longest vector is defined as the dominant
    orientation.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`dx`和`dy`响应也用于估计特征的方向。这些值在半径为`6σ`的圆形邻域内，以`σ`的间隔规则地计算（核大小为`4σ`）。对于给定的方向，在某个角度间隔（`π/3`）内的响应被汇总，给出最长向量的方向被定义为主导方向。'
- en: 'SIFT is a richer descriptor that uses an image gradient instead of simple intensity
    differences. It also splits the square neighborhood around each keypoint into
    `4x4` subregions (it is also possible to use `8x8` or `2x2` subregions). Inside
    each of these regions, a histogram of gradient orientations is built. The orientations
    are discretized into 8 bins, and each gradient orientation entry is incremented
    by a value proportional to the gradient magnitude. This is illustrated by the
    following figure, inside which each star-shaped arrow set represents a local histogram
    of a gradient orientation:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: SIFT是一种更丰富的描述符，它使用图像梯度而不是简单的强度差异。它还将每个关键点周围的正方形邻域分割成`4x4`子区域（也可以使用`8x8`或`2x2`子区域）。在每个这些区域内部，构建一个梯度方向的直方图。方向被离散化到8个箱子中，每个梯度方向条目通过一个与梯度幅度成比例的值增加。这可以通过以下图示来说明，其中每个星形箭头集代表一个梯度方向的局部直方图：
- en: '![How it works...](img/00153.jpeg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![How it works...](img/00153.jpeg)'
- en: These `16` histograms of 8 bins each concatenated together then produce a descriptor
    of `128` dimensions. Note that as for SURF, the gradient values are weighted by
    a Gaussian filter centered at the keypoint location in order to make the descriptor
    less sensitive to sudden changes in gradient orientations at the perimeter of
    the defined neighborhood. The final descriptor is then normalized to make the
    distance measurement more consistent.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 将每个子区域的8个箱子的`16`个直方图连接起来，然后产生一个`128`维度的描述符。注意，对于SURF来说，梯度值通过一个以关键点位置为中心的高斯滤波器加权，以使描述符对定义的邻域边缘梯度方向突然变化的敏感性降低。然后，将最终的描述符归一化，以使距离测量更加一致。
- en: 'With SURF and SIFT features and descriptors, scale-invariant matching can be
    achieved. Here is an example that shows the SURF match result for two images at
    different scales (here, the 50 best matches have been displayed):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SURF和SIFT特征和描述符，可以实现尺度不变匹配。以下是一个示例，显示了不同尺度下两张图像的SURF匹配结果（这里显示了50个最佳匹配）：
- en: '![How it works...](img/00154.jpeg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![How it works...](img/00154.jpeg)'
- en: There's more...
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: The match result produced by any matching algorithm always contains a significant
    number of incorrect matches. In order to improve the quality of the match set,
    there exist a number of strategies. Two of them are discussed here.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 任何匹配算法产生的匹配结果总是包含大量的错误匹配。为了提高匹配集的质量，存在多种策略。这里讨论了其中两种。
- en: Cross-checking matches
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 匹配交叉检查
- en: 'A simple approach to validate the matches obtained is to repeat the same procedure
    a second time, but this time, each keypoint of the second image is compared with
    all the keypoints of the first image. A match is considered valid only if we obtain
    the same pair of keypoints in both directions (that is, each keypoint is the best
    match of the other). The `cv::BFMatcher` function gives the option to use this
    strategy. It is indeed included as a flag; when set to `true`, it forces the function
    to perform the reciprocal match cross-check:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 验证获得的匹配的一个简单方法是在第二次重复相同的程序，但这次，第二幅图像的每个关键点都与第一幅图像的所有关键点进行比较。只有当我们从两个方向获得相同的键点对时，才认为匹配是有效的（也就是说，每个关键点是另一个的最佳匹配）。`cv::BFMatcher`函数提供了使用此策略的选项。这确实是一个标志；当设置为`true`时，它强制函数执行互匹配交叉检查：
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The improved match results are as shown in the following screenshot (in the
    case of SURF):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 改进的匹配结果如下所示（以SURF为例）：
- en: '![Cross-checking matches](img/00155.jpeg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![交叉检查匹配](img/00155.jpeg)'
- en: The ratio test
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 比率测试
- en: We have already noted that repetitive elements in scene objects create unreliable
    results because of the ambiguity in matching visually similar structures. What
    happens in such cases is that a keypoint will match well with more than one other
    keypoint. Since the probability of selecting the wrong correspondence is high,
    it might be preferable to reject a match in this case.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经指出，场景对象中的重复元素由于匹配视觉上相似结构的歧义性，会导致不可靠的结果。在这种情况下发生的情况是，一个关键点会与多个其他关键点很好地匹配。由于选择错误对应关系的概率很高，在这种情况下拒绝匹配可能更可取。
- en: To use this strategy, we then need to find the best two matching points of each
    keypoint. This can be done by using the `knnMatch` method of the `cv::DescriptorMatcher`
    class. Since we want only two best matches, we specify `k=2`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用这种策略，我们接下来需要找到每个关键点的最佳两个匹配点。这可以通过使用`cv::DescriptorMatcher`类的`knnMatch`方法来实现。由于我们只想找到两个最佳匹配，我们指定`k=2`。
- en: '[PRE9]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The next step is to reject all the best matches with a matching distance similar
    to that of their second best match. Since `knnMatch` produces a `std::vector`
    class of `std::vector` (this second vector is of size `k`), we do this by looping
    over each keypoint match and perform a ratio test (this ratio will be one if the
    two best distances are equal). Here is how we can do it:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是拒绝所有与次佳匹配距离相似的最好匹配。由于`knnMatch`生成的是`std::vector`类的`std::vector`（这个第二个向量的大小为`k`），我们通过遍历每个关键点匹配并执行比率测试（如果两个最佳距离相等，这个比率将为1）来实现。以下是我们可以这样做的方法：
- en: '[PRE10]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The initial match set made up of `90` pairs is now reduced to `23` pairs; a
    good proportion of these are now correct matches:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 初始匹配集由`90`对组成，现在已减少到`23`对；其中相当一部分现在是正确匹配：
- en: '![The ratio test](img/00156.jpeg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![比率测试](img/00156.jpeg)'
- en: Distance thresholding
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 距离阈值化
- en: 'An even simpler strategy consists of rejecting matches for which the distance
    between their descriptors is too high. This is done using the `radiusMatch` method
    of the `cv::DescriptorMatcher` class:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更简单的策略是拒绝那些描述符之间距离过高的匹配。这是通过使用`cv::DescriptorMatcher`类的`radiusMatch`方法实现的：
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The result is again a `std::vector` class of `std::vector` because the method
    will retain all the matches with a distance smaller than the specified threshold.
    This means that a given keypoint might have more than one matching point in the
    other image. Conversely, other keypoints will not have any matches associated
    with them (the corresponding inner `std::vector` class will then have a size of
    `0`). This time, the initial match set of `90` pairs is reduced to `37` pairs
    as shown in the following screenshot:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 结果再次是一个`std::vector`类的`std::vector`，因为该方法将保留所有距离小于指定阈值的匹配。这意味着给定关键点可能在另一幅图像中有一个以上的匹配点。相反，其他关键点将没有任何与之关联的匹配（相应的内部`std::vector`类的大小将为`0`）。这次，初始匹配集的`90`对减少到`37`对，如下面的截图所示：
- en: '![Distance thresholding](img/00157.jpeg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![距离阈值化](img/00157.jpeg)'
- en: Obviously, you can combine all these strategies in order to improve your matching
    results.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，你可以将这些策略结合起来以提高你的匹配结果。
- en: See also
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The *Detecting scale-invariant features* recipe in [Chapter 8](part0058_split_000.html#page
    "Chapter 8. Detecting Interest Points"), *Detecting Interest Points,* presents
    the associated SURF and SIFT feature detectors and provides more references on
    the subject
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第 8 章](part0058_split_000.html#page "第 8 章. 检测兴趣点") *检测尺度不变特征* 配方在 *检测兴趣点*
    中介绍了相关的 SURF 和 SIFT 特征检测器，并提供了更多关于该主题的参考文献。'
- en: The *Matching images using random sample consensus* recipe in [Chapter 10](part0067_split_000.html#page
    "Chapter 10. Estimating Projective Relations in Images"), *Estimating Projective
    Relations in Images*, explains how to use the image and the scene geometry in
    order to obtain a match set of even better quality
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第 10 章](part0067_split_000.html#page "第 10 章. 估计图像中的投影关系") *估计图像中的投影关系* 中的
    *使用随机样本一致性匹配图像* 配方解释了如何使用图像和场景几何来获得质量更高的匹配集。'
- en: 'The *Matching feature points in stereo pairs: A comparative study of some matching
    strategies* article by E. Vincent and R. Laganière in *Machine, Graphics and Vision,
    pp. 237-260, 2001*, describes other simple matching strategies that could be used
    to improve the quality of the match set'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'E. Vincent 和 R. Laganière 在 *Machine, Graphics and Vision, pp. 237-260, 2001*
    发表的 *Matching feature points in stereo pairs: A comparative study of some matching
    strategies* 文章，描述了其他一些可以用来提高匹配集质量的简单匹配策略。'
- en: Describing keypoints with binary features
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用二进制特征描述关键点
- en: In the previous recipe, we learned how to describe a keypoint using rich descriptors
    extracted from the image intensity gradient. These descriptors are floating-point
    vectors that have a dimension of `64`, `128`, or sometimes even longer. This makes
    them costly to manipulate. In order to reduce the memory and computational load
    associated with these descriptors, the idea of using binary descriptors has been
    recently introduced. The challenge here is to make them easy to compute and yet
    keep them robust to scene and viewpoint changes. This recipe describes some of
    these binary descriptors. In particular, we will look at the ORB and BRISK descriptors
    for which we presented their associated feature point detectors in [Chapter 8](part0058_split_000.html#page
    "Chapter 8. Detecting Interest Points"), *Detecting Interest Points*.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的配方中，我们学习了如何使用从图像强度梯度中提取的丰富描述符来描述一个关键点。这些描述符是具有 `64`、`128` 或有时甚至更长的维度的浮点向量。这使得它们在操作上成本较高。为了减少与这些描述符相关的内存和计算负载，最近提出了使用二进制描述符的想法。这里的挑战是使它们易于计算，同时保持它们对场景和视点变化的鲁棒性。本配方描述了这些二进制描述符中的一些。特别是，我们将查看
    ORB 和 BRISK 描述符，我们在 [第 8 章](part0058_split_000.html#page "第 8 章. 检测兴趣点") *检测兴趣点*
    中介绍了它们相关的特征点检测器。
- en: How to do it...
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何去做...
- en: 'Owing to the nice generic interface on top of which the OpenCV detectors and
    the descriptors module are built, using a binary descriptor such as ORB is no
    different from using descriptors such as SURF and SIFT. The complete feature-based
    image matching sequence is as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 OpenCV 检测器和描述符模块构建在良好的通用接口之上，使用二进制描述符（如 ORB）与使用描述符（如 SURF 和 SIFT）没有区别。基于特征点的完整图像匹配序列如下：
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The only difference resides in the use of the **Hamming** norm (the `cv::NORM_HAMMING`
    flag) that measures the distance between two binary descriptors by counting the
    number of bits that are dissimilar. On many processors, this operation is efficiently
    implemented by using an exclusive OR operation, followed by a simple bit count.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的区别在于使用了 **Hamming** 范数（`cv::NORM_HAMMING` 标志），它通过计算两个二进制描述符中不同位的数量来衡量它们之间的距离。在许多处理器上，这个操作可以通过使用异或操作，然后进行简单的位计数来高效地实现。
- en: 'The following screenshot shows the result of the matching:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了匹配的结果：
- en: '![How to do it...](img/00158.jpeg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![如何去做...](img/00158.jpeg)'
- en: 'Similar results will be obtained with another popular binary feature detector/descriptor:
    BRISK. In this case, the `cv::DescriptorExtractor` instance is created by the
    new `cv::BRISK(40)` call. As we learned in the previous chapter, its first parameter
    is a threshold that controls the number of detected points.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 使用另一个流行的二进制特征检测器/描述符：BRISK，也会得到类似的结果。在这种情况下，`cv::DescriptorExtractor` 实例是通过新的
    `cv::BRISK(40)` 调用创建的。正如我们在上一章所学，它的第一个参数是一个阈值，用于控制检测到的点的数量。
- en: How it works...
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The ORB algorithm detects oriented feature points at multiple scales. Based
    on this result, the ORB descriptor extracts a representation of each keypoint
    by using simple intensity comparisons. In fact, ORB builds on a previously proposed
    descriptor called BRIEF. This later creates a binary descriptor by simply selecting
    a random pair of points inside a defined neighborhood around the keypoint. The
    intensity values of the two pixel points are then compared, and if the first point
    has a higher intensity, then the value `1` is assigned to the corresponding descriptor
    bit value. Otherwise, the value `0` is assigned. Repeating this test on a number
    of random pairs generates a descriptor that is made up of several bits; typically,
    `128` to `512` bits (pairwise tests) are used.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ORB 算法在多个尺度上检测方向特征点。基于此结果，ORB 描述符通过使用简单的强度比较来提取每个关键点的表示。实际上，ORB 是基于之前提出的描述符
    BRIEF 构建的。随后，它通过简单地选择关键点周围定义的邻域内的随机点对来创建二进制描述符。然后比较这两个像素点的强度值，如果第一个点具有更高的强度，则将值
    `1` 分配给相应的描述符位值。否则，分配值 `0`。在多个随机点对上重复此测试会生成一个由多个位组成的描述符；通常，使用 `128` 到 `512` 位（成对测试）。
- en: This is the scheme used by ORB. Then, the decision to be made is which set of
    point pairs should be used to build the descriptor. Indeed, even if the point
    pairs are randomly chosen, once they have been selected, the same set of binary
    tests must be performed to build the descriptor of all the keypoints in order
    to ensure consistency of the results. To make the descriptor more distinctive,
    intuition tells us that some choices must be better than others. Also, the fact
    that the orientation of each keypoint is known introduces some bias in the intensity
    pattern distribution when this one is normalized with respect to this orientation
    (that is, when the point coordinates are given relative to this keypoint orientation).
    From these considerations and the experimental validation, ORB has identified
    a set of `256` point pairs with high variance and minimal pairwise correlation.
    In other words, the selected binary tests are the ones that have an equal chance
    of being `0` or `1` over a variety of keypoints and also those that are as independent
    from each other as possible.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '这是 ORB 使用的方案。然后，需要做出的决定是使用哪组点对来构建描述符。实际上，即使点对是随机选择的，一旦它们被选中，就必须执行相同的二进制测试集来构建所有关键点的描述符，以确保结果的一致性。为了使描述符更具独特性，直觉告诉我们，某些选择可能比其他选择更好。此外，由于每个关键点的方向是已知的，当将强度模式分布与此方向归一化时（即，当点坐标相对于此关键点方向给出时），这引入了一些偏差。从这些考虑和实验验证中，ORB
    已经确定了一组具有高方差和最小成对相关性的 `256` 个点对。换句话说，所选的二进制测试是那些在各种关键点上具有相等机会为 `0` 或 `1` 的测试，并且尽可能相互独立。 '
- en: In addition to the parameters that control the feature detection process, the
    `cv::ORB` constructor includes two parameters related to its descriptor. One parameter
    is used to specify the patch size inside which the point pairs are selected (the
    default is `31x31`). The second parameter allows you to perform tests with a triplet
    or quadruplet of points instead of the default point pairs. Note that it is highly
    recommended that you use the default settings.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 除了控制特征检测过程的参数外，`cv::ORB` 构造函数还包括两个与其描述符相关的参数。一个参数用于指定点对选择的内部补丁大小（默认为 `31x31`）。第二个参数允许您使用三元组或四元组点而不是默认的点对进行测试。请注意，强烈建议您使用默认设置。
- en: The descriptor of BRISK is very similar. It is also based on pairwise intensity
    comparisons with two differences. First, instead of randomly selecting the points
    from the `31x31` points of the neighborhood, the chosen points are selected from
    a sampling pattern of a set of concentric circles (made up of `60` points) with
    locations that are equally spaced. Second, the intensity at each of these sample
    points is a Gaussian-smoothed value with a σ value proportional to the distance
    from the central keypoint. From these points, BRISK selects `512` point pairs.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: BRISK 的描述符也非常相似。它也是基于成对强度比较，但有两点不同。首先，不是从 `31x31` 邻域点中随机选择点，而是从一组同心圆（由 `60`
    个点组成）的采样模式中选择点，这些圆的点是等间距的。其次，这些采样点的强度是一个与中心关键点距离成比例的高斯平滑值。从这些点中，BRISK 选择 `512`
    个点对。
- en: There's more...
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Several other binary descriptors exist, and interested readers should take a
    look at the scientific literature to learn more on this subject. Since it is also
    available in OpenCV, we will describe one additional descriptor here.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 存在着几个其他的二进制描述符，感兴趣的读者应该查阅科学文献以了解更多关于这个主题的信息。由于它也包含在OpenCV中，我们在这里将描述一个额外的描述符。
- en: FREAK
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: FREAK
- en: FREAK stands for **Fast Retina Keypoint**. This is also a binary descriptor,
    but it does not have an associated detector. It can be applied on any set of keypoints
    detected, for example, SIFT, SURF, or ORB.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: FREAK代表**快速视网膜关键点**。这也是一个二进制描述符，但它没有关联的检测器。它可以应用于任何检测到的关键点集，例如SIFT、SURF或ORB。
- en: Like BRISK, the FREAK descriptor is also based on a sampling pattern defined
    on concentric circles. However, to design their descriptor, the authors used an
    analogy of the human eye. They observed that on the retina, the density of the
    ganglion cells decreases with the increase in the distance to the fovea. Consequently,
    they built a sampling pattern made of `43` points in which the density of a point
    is much greater near the central point. To obtain its intensity, each point is
    filtered with a Gaussian kernel that has a size that also increases with the distance
    to the center.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 与BRISK一样，FREAK描述符也是基于同心圆上定义的采样模式。然而，为了设计他们的描述符，作者使用了人类眼睛的类比。他们观察到在视网膜上，神经节细胞的密度随着距离黄斑的增加而减少。因此，他们构建了一个由`43`个点组成的采样模式，其中点的密度在中心点附近要大得多。为了获得其强度，每个点都通过一个大小也随中心距离增加的高斯核进行过滤。
- en: In order to identify the pairwise comparisons that should be performed, an experimental
    validation has been performed by following a strategy similar to the one used
    for ORB. By analyzing several thousands of keypoints, the binary tests with the
    highest variance and lowest correlation are retained, resulting in `512` pairs.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定应该执行的成对比较，通过遵循与ORB使用的类似策略进行了实验验证。通过分析数千个关键点，保留了具有最高方差和最低相关性的二进制测试，从而得到`512`对。
- en: FREAK also introduces the idea of performing the descriptor comparisons in cascade.
    That is, the first `128` bits representing coarser information (corresponding
    to the tests performed at the periphery on larger Gaussian kernels) are performed
    first. Only if the compared descriptors pass this initial step will the remaining
    tests be performed.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: FREAK还引入了在级联中进行描述符比较的想法。也就是说，首先执行代表较粗信息（对应于在较大高斯核边缘执行的测试）的前`128`位。只有当比较的描述符通过这个初始步骤后，才会执行剩余的测试。
- en: 'Using the keypoints detected with ORB, we extract the FREAK descriptors by
    simply creating the `cv::DescriptorExtractor` instance as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ORB检测到的关键点，我们可以通过简单地创建以下`cv::DescriptorExtractor`实例来提取FREAK描述符：
- en: '[PRE13]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The match result is as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 比赛结果如下：
- en: '![FREAK](img/00159.jpeg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![FREAK](img/00159.jpeg)'
- en: 'The following figure illustrates the sampling pattern used for the three descriptors
    presented in this recipe:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示了本配方中展示的三个描述符所使用的采样模式：
- en: '![FREAK](img/00160.jpeg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![FREAK](img/00160.jpeg)'
- en: The first square is the ORB descriptor in which point pairs are randomly selected
    on a square grid. Each pair of points linked by a line represent a possible test
    to compare the two pixel intensities. Here, we show only 8 such pairs; the default
    ORB uses 256 pairs. The middle square corresponds to the BRISK sampling pattern.
    Points are uniformly sampled on the shown circles (for clarity, we only identify
    the points on the first circle here). Finally, the third square shows the log-polar
    sampling grid of FREAK. While BRISK has a uniform distribution of points, FREAK
    has a higher density of points closer to the center. For example, in BRISK, you
    find 20 points on the outer circle, while in the case of FREAK, its outer circle
    includes only 6 points.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个正方形是ORB描述符，其中在正方形网格上随机选择点对。通过线的连接的点对代表比较两个像素强度的可能测试。在这里，我们只展示了8个这样的对；默认的ORB使用256对。中间的正方形对应于BRISK采样模式。点在显示的圆上均匀采样（为了清晰起见，我们在这里只标识第一圈上的点）。最后，第三个正方形显示了FREAK的对数极坐标采样网格。虽然BRISK的点分布均匀，但FREAK在中心附近的点密度更高。例如，在BRISK中，你会在外圈找到20个点，而在FREAK的情况下，其外圈只包括6个点。
- en: See also
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考信息
- en: The *Detecting FAST features at multiple scales* recipe in [Chapter 8](part0058_split_000.html#page
    "Chapter 8. Detecting Interest Points"), *Detecting Interest Points,* presents
    the associated BRISK and ORB feature detectors and provides more references on
    the subject
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第8章](part0058_split_000.html#page "第8章. 检测兴趣点")中的 *在多个尺度上检测FAST特征* 菜单，*检测兴趣点*，介绍了相关的BRISK和ORB特征检测器，并提供了更多关于该主题的参考资料'
- en: 'The *BRIEF: Computing a Local Binary Descriptor Very Fast* article by E. M.
    Calonder, V. Lepetit, M. Ozuysal, T. Trzcinski, C. Strecha, and P. Fua in *IEEE
    Transactions on Pattern Analysis and Machine Intelligence, 2012*, describes the
    BRIEF feature descriptor that inspires the presented binary descriptors'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'E. M. Calonder、V. Lepetit、M. Ozuysal、T. Trzcinski、C. Strecha和P. Fua在 *IEEE
    Transactions on Pattern Analysis and Machine Intelligence, 2012* 发表的 *BRIEF: 计算局部二值描述符非常快速*
    文章，描述了启发所展示的二值描述符的BRIEF特征描述符'
- en: 'The *FREAK: Fast Retina Keypoint* article by A.Alahi, R. Ortiz, and P. Vandergheynst
    in *IEEE Conference on Computer Vision and Pattern Recognition, 2012*, describes
    the FREAK feature descriptor'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'A.Alahi、R. Ortiz和P. Vandergheynst在 *IEEE Conference on Computer Vision and
    Pattern Recognition, 2012* 发表的 *FREAK: 快速视网膜关键点* 文章，描述了FREAK特征描述符'
