- en: Classification and Regression Trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"The classifiers most likely to be the best are the random forest (RF) versions,
    the best of which (implemented in R and accessed via caret), achieves 94.1 percent
    of the maximum accuracy overcoming 90 percent in the 84.3 percent of the data
    sets."'
  prefs: []
  type: TYPE_NORMAL
- en: '- Fern*á*ndez-Delgado et al. (2014)'
  prefs: []
  type: TYPE_NORMAL
- en: This quote from *Fernández-Delgado et al*. in the *Journal of Machine Learning
    Research* is meant to demonstrate that the techniques in this chapter are quite
    powerful, particularly when used for classification problems. Certainly, they
    don't always offer the best solution but they do provide a good starting point.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapters, we examined the techniques used to predict either
    a quantity or a label classification. Here, we will apply them to both types of
    problems. We will also approach the business problem differently than in the previous
    chapters. Instead of defining a new problem, we will apply the techniques to some
    of the issues that we already tackled, with an eye to see if we can improve our
    predictive power. For all intents and purposes, the business case in this chapter
    is to see if we can improve on the models that we selected before.
  prefs: []
  type: TYPE_NORMAL
- en: The first item of discussion is the basic decision tree, which is both simple
    to build and to understand. However, the single decision tree method does not
    perform as well as the other methods that you learned, for example, the support
    vector machines, or as the ones that we will learn, such as the neural networks.
    Therefore, we will discuss the creation of multiple, sometimes hundreds, of different
    trees with their individual results combined, leading to a single overall prediction.
  prefs: []
  type: TYPE_NORMAL
- en: These methods, as the paper referenced at the beginning of this chapter states,
    perform as well as, or better than, any technique in this book. These methods
    are known as **random forests** and **gradient boosted trees**. Additionally,
    we will take a break from a business case and show how employing the random forest
    method on a dataset can assist in feature elimination/selection.
  prefs: []
  type: TYPE_NORMAL
- en: An overview of the techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now get to an overview of the techniques, covering the regression and
    classification trees, random forests, and gradient boosting. This will set the
    stage for the practical business cases.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the regression trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To establish an understanding of tree-based methods, it is probably easier to
    start with a quantitative outcome and then move on to how it works in a classification
    problem. The essence of a tree is that the features are partitioned, starting
    with the first split that improves the RSS the most. These binary splits continue
    until the termination of the tree. Each subsequent split/partition is not done
    on the entire dataset but only on the portion of the prior split that it falls
    under. This top-down process is referred to as recursive partitioning. It is also
    a process that is **greedy**, a term you may stumble upon in reading about the
    machine learning methods. Greedy means that, during each split in the process,
    the algorithm looks for the greatest reduction in the RSS without any regard to
    how well it will perform on the latter partitions. The result is that you may
    end up with a full tree of unnecessary branches leading to a low bias but a high
    variance. To control this effect, you need to appropriately prune the tree to
    an optimal size after building a full tree.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6.1*, provides a visual of this technique in action. The data is hypothetical
    with 30 observations, a response ranging from 1 to 10, and two predictor features,
    both ranging in value from 0 to 10 named **X1** and **X2**. The tree has three
    splits leading to four terminal nodes. Each split is basically an `if...then`
    statement or uses an R syntax `ifelse()`. The first split is--if **X1** is less
    than **3.5**, then the response is split into four observations with an average
    value of **2.4** and the remaining 26 observations. This left branch of four observations
    is a terminal node as any further splits would not substantially improve the RSS.
    The predicted value for these four observations in that partition of the tree
    becomes the average. The next split is at **X2 < 4** and finally, **X1 < 7.5**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An advantage of this method is that it can handle highly nonlinear relationships;
    however, can you see a couple of potential problems? The first issue is that an
    observation is given the average of the terminal node under which it falls. This
    can hurt the overall predictive performance (high bias). Conversely, if you keep
    partitioning the data further and further so as to achieve a low bias, a high
    variance can become an issue. As with the other methods, you can use cross-validation
    to select the appropriate tree depth size:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06473_06_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1: Regression Tree with 3 splits and 4 terminal nodes and the corresponding
    node average and number of observations'
  prefs: []
  type: TYPE_NORMAL
- en: Classification trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classification trees operate under the same principle as regression trees, except
    that the splits are not determined by the RSS but an error rate. The error rate
    used is not what you would expect where the calculation is simply the misclassified
    observations divided by the total observations. As it turns out, when it comes
    to tree-splitting, a misclassification rate, by itself, may lead to a situation
    where you can gain information with a further split but not improve the misclassification
    rate. Let's look at an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have a node, let''s call it **N0**, where you have seven observations
    labeled `No` and three observations labeled `Yes`, and we can say that the misclassified
    rate is 30 percent. With this in mind, let''s calculate a common alternative error
    measure called the **Gini index**. The formula for a single node Gini index is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_06_01b.png)'
  prefs: []
  type: TYPE_IMG
- en: Then, for `N0`, the Gini is *1 - (.7)² - (.3)²*, which is equal to *0.42*, versus
    the misclassification rate of 30 per cent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Taking this example further, we will now create node *N1* with three observations
    from `Class 1` and none from `Class 2`, along with *N2*, which has four observations
    from `Class 1` and three from `Class 2`. Now, the overall misclassification rate
    for this branch of the tree is still 30 per cent, but look at how the overall
    Gini index has improved:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Gini(N1) = 1 - (3/3)² - (0/3)² = 0*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Gini(N2) = 1 - (4/7)² - (3/7)² = 0.49*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*New Gini index = (proportion of N1 x Gini(N1)) + (proportion of N2 x Gini(N2))*,
    which is equal to *(.3 x 0) + (.7 x 0.49)* or *0.343*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By doing a split on a surrogate error rate, we actually improved our model impurity,
    reducing it from *0.42* to *0.343*, whereas the misclassification rate did not
    change. This is the methodology that is used by the `rpart()` package, which we
    will be using in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Random forest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To greatly improve our model's predictive ability, we can produce numerous trees
    and combine the results. The random forest technique does this by applying two
    different tricks in model development. The first is the use of **bootstrap aggregation**
    or **bagging**, as it is called.
  prefs: []
  type: TYPE_NORMAL
- en: In bagging, an individual tree is built on a random sample of the dataset, roughly
    two-thirds of the total observations (note that the remaining one-third is referred
    to as **out-of-bag** (**oob**)). This is repeated dozens or hundreds of times
    and the results are averaged. Each of these trees is grown and not pruned based
    on any error measure, and this means that the variance of each of these individual
    trees is high. However, by averaging the results, you can reduce the variance
    without increasing the bias.
  prefs: []
  type: TYPE_NORMAL
- en: The next thing that random forest brings to the table is that concurrently with
    the random sample of the data--that is, bagging--it also takes a random sample
    of the input features at each split. In the `randomForest` package, we will use
    the default random number of the predictors that are sampled, which, for classification
    problems, is the square root of the total predictors and for regression, it is
    the total number of the predictors divided by three. The number of predictors
    the algorithm randomly chooses at each split can be changed via the model tuning
    process.
  prefs: []
  type: TYPE_NORMAL
- en: By doing this random sample of the features at each split and incorporating
    it into the methodology, you can mitigate the effect of a highly correlated predictor
    becoming the main driver in all of your bootstrapped trees, preventing you from
    reducing the variance that you hoped to achieve with bagging. The subsequent averaging
    of the trees that are less correlated to each other is more generalizable and
    robust to outliers than if you only performed bagging.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Boosting methods can become extremely complicated to learn and understand, but
    you should keep in mind what is fundamentally happening behind the curtain. The
    main idea is to build an initial model of some kind (linear, spline, tree, and
    so on) called the base learner, examine the residuals, and fit a model based on
    these residuals around the so-called **loss function**. A loss function is merely
    the function that measures the discrepancy between the model and desired prediction,
    for example, a squared error for regression or the logistic function for classification.
    The process continues until it reaches some specified stopping criterion. This
    is sort of like the student who takes a practice exam and gets 30 out of 100 questions
    wrong and, as a result, studies only these 30 questions that were missed. In the
    next practice exam, they get 10 out of those 30 wrong and so only focus on those
    10 questions, and so on. If you would like to explore the theory behind this further,
    a great resource for you is available in Frontiers in Neurorobotics, *Gradient
    boosting machines, a tutorial*, Natekin A., Knoll A. (2013), at [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/).
  prefs: []
  type: TYPE_NORMAL
- en: As just mentioned, boosting can be applied to many different base learners,
    but here we will only focus on the specifics of **tree-based learning**. Each
    tree iteration is small and we will determine how small with one of the tuning
    parameters referred to as interaction depth. In fact, it may be as small as one
    split, which is referred to as a stump.
  prefs: []
  type: TYPE_NORMAL
- en: Trees are sequentially fit to the residuals, according to the loss function,
    up to the number of trees that we specified (our stopping criterion).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a number of parameters that require tuning in the model-building
    process using the `Xgboost` package, which stands for **eXtreme Gradient Boosting**.
    This package has become quite popular for online data contests because of its
    winning performance. There is excellent background material on boosting trees
    and on Xgboost on the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://xgboost.readthedocs.io/en/latest/model.html](http://xgboost.readthedocs.io/en/latest/model.html)'
  prefs: []
  type: TYPE_NORMAL
- en: What we will do in the business case is show how to begin to optimize the hyperparameters
    and produce meaningful output and predictions. These parameters can interact with
    each other, and if you just tinker with one without considering the other, your
    model may worsen the performance. The `caret` package will help us in the tuning
    endeavor.
  prefs: []
  type: TYPE_NORMAL
- en: Business case
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The overall business objective in this situation is to see whether we can improve
    the predictive ability for some of the cases that we already worked on in the
    previous chapters. For regression, we will revisit the prostate cancer dataset
    from [Chapter 4](04f803d3-791a-4505-80a7-905dfd6fdcc3.xhtml), *Advanced Feature
    Selection in Linear Models*. The baseline mean squared error to improve on is
    0.444.
  prefs: []
  type: TYPE_NORMAL
- en: For classification purposes, we will utilize both the breast cancer biopsy data
    from [Chapter 3](d5d39222-b2f8-4c80-9348-34e075893e47.xhtml), *Logistic Regression
    and Discriminant Analysis* and the Pima Indian Diabetes data from [Chapter 5](a7511867-5362-4215-a7dd-bbdc162740d1.xhtml),
    *More Classification Techniques - K-Nearest Neighbors and Support Vector Machines*.
    In the breast cancer data, we achieved 97.6 per cent predictive accuracy. For
    the diabetes data, we are seeking to improve on the 79.6 per cent accuracy rate.
  prefs: []
  type: TYPE_NORMAL
- en: Both random forests and boosting will be applied to all three datasets. The
    simple tree method will only be used on the breast and prostate cancer sets from
    [Chapter 4](04f803d3-791a-4505-80a7-905dfd6fdcc3.xhtml), *Advanced Feature Selection
    in Linear Models*.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling and evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To perform the modeling process, we will need to load seven different R packages.
    Then, we will go through each of the techniques and compare how well they perform
    on the data analyzed with the prior methods in the previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Regression tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will jump right into the `prostate` dataset, but let''s first load the necessary
    R packages. As always, please ensure that you have the libraries installed prior
    to loading the packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We will first do regression with the `prostate` data and prepare it, as we
    did in [Chapter 4](04f803d3-791a-4505-80a7-905dfd6fdcc3.xhtml), *Advanced Feature
    Selection in Linear Models*. This involves calling the dataset, coding the `gleason`
    score as an indicator variable using the `ifelse()` function, and creating the
    `test` and `train` sets. The `train` set will be `pros.train` and the `test` set
    will be `pros.test`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To build a regression tree on the `train` data, we will use the `rpart()` function
    from R''s `party` package. The syntax is quite similar to what we used in the
    other modeling techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can call this object and examine the error per number of splits in order
    to determine the optimal number of splits in the tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a very important table to analyze. The first column labeled `CP` is
    the cost complexity parameter. The second column, `nsplit`, is the number of splits
    in the tree. The `rel error` column stands for relative error and is the RSS for
    the number of splits divided by the RSS for no splits *RSS(k)/RSS(0)*. Both `xerror`
    and `xstd` are based on the ten-fold cross-validation with `xerror` being the
    average error and `xstd` the standard deviation of the cross-validation process.
    We can see that while five splits produced the lowest error on the `full` dataset,
    four splits produced a slightly less error using cross-validation. You can examine
    this using `plotcp()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_06_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The plot shows us the relative error by the tree size with the corresponding
    error bars. The horizontal line on the plot is the upper limit of the lowest standard
    error. Selecting a tree size, `5`, which is four splits, we can build a new tree
    object where `xerror` is minimized by pruning our tree by first creating an object
    for `cp` associated with the pruned tree from the table. Then the `prune()` function
    handles the rest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'With this done, you can plot and compare the full and pruned trees. The tree
    plots produced by the `partykit` package are much better than those produced by
    the `party` package. You can simply use the `as.party()` function as a wrapper
    in `plot()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_06_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we will use the `as.party()` function for the pruned tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_06_04.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the splits are exactly the same in the two trees with the exception
    of the last split, which includes the variable `age` for the full tree. Interestingly,
    both the first and second splits in the tree are related to the log of cancer
    volume (`lcavol`). These plots are quite informative as they show the splits,
    nodes, observations per node, and boxplots of the outcome that we are trying to
    predict.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how well the pruned tree performs on the `test` data. What we will
    do is create an object of the predicted values using the `predict()` function
    and incorporate the `test` data. Then, calculate the errors (the predicted values
    minus the actual values) and finally, the mean of the squared errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We have not improved on the predictive value from our work in [Chapter 4](04f803d3-791a-4505-80a7-905dfd6fdcc3.xhtml),
    *Advanced Feature Selection in Linear Models* where the baseline MSE was `0.44`.
    However, the technique is not without value. One can look at the tree plots that
    we produced and easily explain what the primary drivers behind the response are.
    As mentioned in the introduction, the trees are easy to interpret and explain,
    which may be more important than accuracy in many cases.
  prefs: []
  type: TYPE_NORMAL
- en: Classification tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the classification problem, we will prepare the breast cancer data in the
    same fashion as we did in [Chapter 3](d5d39222-b2f8-4c80-9348-34e075893e47.xhtml),
    *Logistic Regression and Discriminant Analysis*. After loading the data, you will
    delete the patient ID, rename the features, eliminate the few missing values,
    and then create the `train`/`test` datasets in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'With the data set up appropriately, we will use the same syntax style for a
    classification problem as we did previously for a regression problem, but before
    creating a classification tree, we will need to ensure that the outcome is `Factor`,
    which can be done using the `str()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'First, create the tree and then examine the table for the optimal number of
    splits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The cross-validation error is at a minimum with only two splits (row `3`).
    We can now prune the tree, plot the pruned tree, and see how it performs on the
    `test` set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_06_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'An examination of the tree plot shows that the uniformity of the cell size
    is the first split, then nuclei. The full tree had an additional split at the
    cell thickness. We can predict the `test` observations using `type="class"` in
    the `predict()` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The basic tree with just two splits gets us almost 96 percent accuracy. This
    still falls short of 97.6 percent with logistic regression but should encourage
    us to believe that we can improve on this with the upcoming methods, starting
    with random forests.
  prefs: []
  type: TYPE_NORMAL
- en: Random forest regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will start by focusing on the `prostate` data again. Before
    moving on to the breast cancer and Pima Indian sets. We will use the `randomForest`
    package. The general syntax to create a `random forest` object is to use the `randomForest()`
    function and specify the formula and dataset as the two primary arguments. Recall
    that for regression, the default variable sample per tree iteration is p/3, and
    for classification, it is the square root of p, where p is equal to the number
    of predictor variables in the data frame. For larger datasets, in terms of p,
    you can tune the `mtry` parameter, which will determine the number of p sampled
    at each iteration. If p is less than 10 in these examples, we will forgo this
    procedure. When you want to optimize `mtry` for larger p datasets, you can utilize
    the `caret` package or use the `tuneRF()` function in `randomForest`. With this,
    let''s build our forest and examine the results, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The call of the `rf.pros` object shows us that the random forest generated
    `500` different trees (the default) and sampled two variables at each split. The
    result is an MSE of `0.68` and nearly 53 percent of the variance explained. Let''s
    see if we can improve on the default number of trees. Too many trees can lead
    to overfitting; naturally, how many is too many depends on the data. Two things
    can help out, the first one is a plot of `rf.pros` and the other is to ask for
    the minimum MSE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_06_06.png)'
  prefs: []
  type: TYPE_IMG
- en: This plot shows the MSE by the number of trees in the model. You can see that
    as the trees are added, significant improvement in MSE occurs early on and then
    flatlines just before `100` trees are built in the forest.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can identify the specific and optimal tree with the `which.min()` function,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We can try `75` trees in the random forest by just specifying `ntree=75` in
    the model syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that the MSE and variance explained have both improved slightly.
    Let''s see another plot before testing the model. If we are combining the results
    of `75` different trees that are built using bootstrapped samples and only two
    random predictors, we will need a way to determine the drivers of the outcome.
    One tree alone cannot be used to paint this picture, but you can produce a variable
    importance plot and corresponding list. The y-axis is a list of variables in descending
    order of importance and the x-axis is the percentage of improvement in MSE. Note
    that for the classification problems, this will be an improvement in the Gini
    index. The function is `varImpPlot()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_06_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Consistent with the single tree, `lcavol` is the most important variable and
    `lweight` is the second-most important variable. If you want to examine the raw
    numbers, use the `importance()` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, it is time to see how it did on the `test` data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The MSE is still higher than our `0.44` that we achieved in [Chapter 4](04f803d3-791a-4505-80a7-905dfd6fdcc3.xhtml),
    *Advanced Feature Selection in Linear Models* with LASSO and no better than just
    a single tree.
  prefs: []
  type: TYPE_NORMAL
- en: Random forest classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Perhaps you are disappointed with the performance of the random forest regression
    model, but the true power of the technique is in the classification problems.
    Let''s get started with the breast cancer diagnosis data. The procedure is nearly
    the same as we did with the regression problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The `OOB` error rate is `3.16%`. Again, this is with all the **500** trees
    factored into the analysis. Let''s plot the **Error** by **trees**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_06_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The plot shows that the minimum error and standard error is the lowest with
    quite a few trees. Let''s now pull the exact number using `which.min()` again.
    The one difference from before is that we need to specify column `1` to get the
    error rate. This is the overall error rate and there will be additional columns
    for each error rate by the class label. We will not need them in this example.
    Also, `mse` is no longer available but rather `err.rate` is used instead, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Only 19 trees are needed to optimize the model accuracy. Let''s try this and
    see how it performs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Well, how about that? The `train` set error is below 3 percent, and the model
    even performs better on the `test` set where we had only three observations misclassified
    out of `209` and none were false positives. Recall that the best so far was with
    logistic regression with 97.6 percent accuracy. So this seems to be our best performer
    yet on the breast cancer data. Before moving on, let''s have a look at the variable
    importance plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_06_09.png)'
  prefs: []
  type: TYPE_IMG
- en: The importance in the preceding plot is in each variable's contribution to the
    mean decrease in the Gini index. This is rather different from the splits of the
    single tree. Remember that the full tree had splits at the size (consistent with
    random forest), then nuclei, and then thickness. This shows how potentially powerful
    a technique building random forests can be, not only in the predictive ability,
    but also in feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moving on to the tougher challenge of the Pima Indian diabetes model, we will
    first need to prepare the data in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will move on to the building of the model, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We get a `20` per cent misclassification rate error, which is no better than
    what we''ve done before on the `train` set. Let''s see if optimizing the tree
    size can improve things dramatically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'At `80` trees in the forest, there is minimal improvement in the `OOB` error.
    Can random forest live up to the hype on the `test` data? We will see in the following
    way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Well, we get only 73 percent accuracy on the `test` data, which is inferior
    to what we achieved using the SVM.
  prefs: []
  type: TYPE_NORMAL
- en: While random forest disappointed on the diabetes data, it proved to be the best
    classifier so far for the breast cancer diagnosis. Finally, we will move on to
    gradient boosting.
  prefs: []
  type: TYPE_NORMAL
- en: Extreme gradient boosting - classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned previously, we will be using the `xgboost` package in this section,
    which we have already loaded. Given the method's well-earned reputation, let's
    try it on the diabetes data.
  prefs: []
  type: TYPE_NORMAL
- en: 'As stated in the boosting overview, we will be tuning a number of parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`nrounds`: The maximum number of iterations (number of trees in final model).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`colsample_bytree`: The number of features, expressed as a ratio, to sample
    when building a tree. Default is 1 (100% of the features).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_child_weight`: The minimum weight in the trees being boosted. Default
    is 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eta`: Learning rate, which is the contribution of each tree to the solution.
    Default is 0.3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gamma`: Minimum loss reduction required to make another leaf partition in
    a tree.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subsample`: Ratio of data observations. Default is 1 (100%).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_depth`: Maximum depth of the individual trees.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the `expand.grid()` function, we will build our experimental grid to run
    through the training process of the `caret` package. If you do not specify values
    for all of the preceding parameters, even if it is just a default, you will receive
    an error message when you execute the function. The following values are based
    on a number of training iterations I've done previously. I encourage you to try
    your own tuning values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s build the grid as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This creates a grid of 24 different models that the `caret` package will run
    so as to determine the best tuning parameters. A note of caution is in order.
    On a dataset of the size that we will be working with, this process takes only
    a few seconds. However, in large datasets, this can take hours. As such, you must
    apply your judgment and experiment with smaller samples of the data in order to
    identify the tuning parameters, in case the time is of the essence, or you are
    constrained by the size of your hard drive.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before using the `train()` function from the `caret` package, I would like
    to specify the `trainControl` argument by creating an object called `control`.
    This object will store the method that we want so as to train the tuning parameters.
    We will use the `5` fold cross-validation, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'To utilize the `train.xgb()` function, just specify the formula as we did with
    the other models: the `train` dataset inputs, labels, method, train control, and
    experimental grid. Remember to set the random seed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Since in `trControl` I set `verboseIter` to `TRUE`, you should have seen each
    training iteration within each k-fold.
  prefs: []
  type: TYPE_NORMAL
- en: 'Calling the object gives us the optimal parameters and the results of each
    of the parameter settings, as follows (abbreviated for simplicity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the best combination of parameters to build a model. The accuracy
    in the training data was 81% with a Kappa of 0.55\. Now it gets a little tricky,
    but this is what I''ve seen as best practice. First, create a list of parameters
    that will be used by the `xgboost` training function, `xgb.train()`. Then, turn
    the dataframe into a matrix of input features and a list of labeled numeric outcomes
    (0s and 1s). Then further, turn the features and labels into the input required,
    as `xgb.Dmatrix`. Try this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'With all of that prepared, just create the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Before seeing how it does on the test set, let''s check the variable importance
    and also plot it. You can examine three items: **gain**, **cover**, and** frequency**. **Gain** is
    the improvement in accuracy that feature brings to the branches it is on. **Cover**
    is the relative number of total observations related to this feature. **Frequency**
    is the per cent of times that feature occurs in all of the trees. The following
    code produces the desired output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_06_10.png)'
  prefs: []
  type: TYPE_IMG
- en: How does the feature importance compare to other methods?
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how we see it performed on the test set, which like the training data
    must be in a matrix. Let''s also bring in the tools from the `InformationValue`
    package to help our efforts. This code loads the library and produces some output
    to analyze model performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Did you notice what I did there with `optimalCutoff()`? Well, that function
    from `InformationValue` provides the optimal probability threshold to minimize
    error. By the way, the model error is around 25%. It''s still not superior to
    our SVM model. As an aside, we see the ROC curve and the achievement of an AUC
    above 0.8\. The following code produces the ROC curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_06_11.png)'
  prefs: []
  type: TYPE_IMG
- en: Model selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recall that our primary objective in this chapter was to use the tree-based
    methods to improve the predictive ability of the work done in the prior chapters.
    What did we learn? First, on the `prostate` data with a quantitative response,
    we were not able to improve on the linear models that we produced in [Chapter
    4](04f803d3-791a-4505-80a7-905dfd6fdcc3.xhtml), *Advanced Feature Selection in
    Linear Models*. Second, the random forest outperformed logistic regression on
    the Wisconsin Breast Cancer data of [Chapter 3](d5d39222-b2f8-4c80-9348-34e075893e47.xhtml),
    *Logistic Regression and Discriminant Analysis*. Finally, and I must say disappointingly,
    we were not able to improve on the SVM model on the Pima Indian diabetes data
    with boosted trees.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, we can feel comfortable that we have good models for the prostate
    and breast cancer problems. We will try one more time to improve the model for
    diabetes in [Chapter 7](73c55dff-b99c-4ce7-ab3d-9a47ff2f6f2f.xhtml), *Neural Networks and
    Deep Learning*. Before we bring this chapter to a close, I want to introduce the
    powerful method of feature elimination using random forest techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Selection with random forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we''ve looked at several feature selection techniques, such as regularization,
    best subsets, and recursive feature elimination. I now want to introduce an effective
    feature selection method for classification problems with Random Forests using
    the `Boruta` package. A paper is available that provides details on how it works
    in providing all relevant features:'
  prefs: []
  type: TYPE_NORMAL
- en: Kursa M., Rudnicki W. (2010), *Feature Selection with the Boruta Package*, *Journal
    of Statistical Software,* 36(11), 1 - 13
  prefs: []
  type: TYPE_NORMAL
- en: What I will do here is provide an overview of the algorithm and then apply it
    to a wide dataset. This will not serve as a separate business case but as a template
    to apply the methodology. I have found it to be highly effective, but be advised
    it can be computationally intensive. That may seem to defeat the purpose, but
    it effectively eliminates unimportant features, allowing you to focus on building
    a simpler, more efficient, and more insightful model. It is time well spent.
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, the algorithm creates **shadow** **attributes** by copying
    all the inputs and shuffling the order of their observations to decorrelate them.
    Then, a random forest model is built on all the inputs and a Z-score of the mean
    accuracy loss for each feature, including the shadow ones. Features with significantly
    higher Z-scores or significantly lower Z-scores than the shadow attributes are
    deemed **important** and **unimportant** respectively. The shadow attributes and
    those features with known importance are removed and the process repeats itself
    until all features are assigned an importance value. You can also specify the
    maximum number of random forest iterations. After completion of the algorithm,
    each of the original features will be labeled as **confirmed**, **tentative**,
    or r**ejected**. You must decide on whether or not to include the tentative features
    for further modeling. Depending on your situation, you have some options:'
  prefs: []
  type: TYPE_NORMAL
- en: Change the random seed and rerun the methodology multiple (k) times and select
    only those features that are confirmed in all the k runs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Divide your data (training data) into k folds, run separate iterations on each
    fold, and select those features which are confirmed for all the k folds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note that all of this can be done with just a few lines of code. Let''s have
    a look at the code, applying it to the `Sonar` data from the `mlbench` package.
    It consists of 208 observations, 60 numerical input features, and one vector of
    labels for classification. The labels are factors where, `R` if the `sonar` object
    is a rock and `M` if it is a mine. The first thing to do is load the data and
    do a quick, very quick, data exploration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'To run the algorithm, you just need to load the `Boruta` package and create
    a formula in the `boruta()` function. Keep in mind that the labels must be and
    as a factor, or the algorithm will not work. If you want to track the progress
    of the algorithm, specify `doTrace = 1`. Also, don''t forget to set the random
    seed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned in the previous section, this can be computationally intensive.
    Here is how long it took on my old-fashioned laptop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'A simple table will provide the count of the final importance decision. We
    see that we could safely eliminate half of the features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Using these results, it is simple to create a new dataframe with our selected
    features. We start out using the `getSelectedAttributes()` function to capture
    the feature names. In this example, let''s only select those that are confirmed.
    If we wanted to include confirmed and tentative, we just specify `withTentative
    = TRUE` in the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the feature names, we create our subset of the Sonar data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: There you have it! The `Sonar.features` dataframe includes all the confirmed
    features from the boruta algorithm. It can now be subjected to further meaningful
    data exploration and analysis. A few lines of code and some patience as the algorithm
    does its job can significantly improve your modeling efforts and insight generation.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned both the power and limitations of tree-based learning
    methods for both classification and regression problems. Single trees, while easy
    to build and interpret, may not have the necessary predictive power for many of
    the problems that we are trying to solve. To improve on the predictive ability,
    we have the tools of random forest and gradient-boosted trees at our disposal.
    With random forest, hundreds or even thousands of trees are built and the results
    aggregated for an overall prediction. Each tree of the random forest is built
    using a sample of the data called bootstrapping as well as a sample of the predictive
    variables. As for gradient boosting, an initial, and a relatively small, tree
    is produced. After this initial tree is built, subsequent trees are produced based
    on the residuals/misclassifications. The intended result of such a technique is
    to build a series of trees that can improve on the weakness of the prior tree
    in the process, resulting in decreased bias and variance. We also saw that in
    R, one can utilize random forests as a feature selection method.
  prefs: []
  type: TYPE_NORMAL
- en: While these methods are extremely powerful, they are not some sort of nostrum
    in the world of machine learning. Different datasets require judgment on the part
    of the analyst as to which techniques are applicable. The techniques to be applied
    to the analysis and the selection of the tuning parameters are equally important.
    This fine tuning can make all the difference between a good predictive model and
    a great predictive model.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we turn our attention to using R to build neural networks
    and deep learning models.
  prefs: []
  type: TYPE_NORMAL
