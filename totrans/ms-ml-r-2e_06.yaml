- en: Classification and Regression Trees
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类和回归树
- en: '"The classifiers most likely to be the best are the random forest (RF) versions,
    the best of which (implemented in R and accessed via caret), achieves 94.1 percent
    of the maximum accuracy overcoming 90 percent in the 84.3 percent of the data
    sets."'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: “最有可能成为最佳分类器的是随机森林（RF）版本，其中最好的（在R中实现并通过caret访问）达到了94.1%的最大准确率，在84.3%的数据集中超过了90%。”
- en: '- Fern*á*ndez-Delgado et al. (2014)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- Fern*á*ndez-Delgado等人（2014年）'
- en: This quote from *Fernández-Delgado et al*. in the *Journal of Machine Learning
    Research* is meant to demonstrate that the techniques in this chapter are quite
    powerful, particularly when used for classification problems. Certainly, they
    don't always offer the best solution but they do provide a good starting point.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这段来自*Fernández-Delgado等人*在*机器学习研究杂志*中的引言，旨在表明本章中的技术非常强大，尤其是在用于分类问题时。当然，它们并不总是提供最佳解决方案，但它们确实提供了一个良好的起点。
- en: In the previous chapters, we examined the techniques used to predict either
    a quantity or a label classification. Here, we will apply them to both types of
    problems. We will also approach the business problem differently than in the previous
    chapters. Instead of defining a new problem, we will apply the techniques to some
    of the issues that we already tackled, with an eye to see if we can improve our
    predictive power. For all intents and purposes, the business case in this chapter
    is to see if we can improve on the models that we selected before.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们探讨了用于预测数量或标签分类的技术。在这里，我们将将这些技术应用于这两种类型的问题。我们还将与之前章节不同的方式来处理商业问题。而不是定义一个新的问题，我们将应用这些技术到我们已经解决的问题上，目的是看看我们是否可以提高我们的预测能力。从所有目的和意图来看，本章的商业案例是看看我们是否可以改进之前选定的模型。
- en: The first item of discussion is the basic decision tree, which is both simple
    to build and to understand. However, the single decision tree method does not
    perform as well as the other methods that you learned, for example, the support
    vector machines, or as the ones that we will learn, such as the neural networks.
    Therefore, we will discuss the creation of multiple, sometimes hundreds, of different
    trees with their individual results combined, leading to a single overall prediction.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论的第一项是基本的决策树，它既容易构建也容易理解。然而，单个决策树方法的表现不如你学到的其他方法，例如支持向量机，或者我们将要学习的方法，例如神经网络。因此，我们将讨论创建多个、有时是数百个不同的树，并将它们的个别结果结合起来，以得出一个单一的总体预测。
- en: These methods, as the paper referenced at the beginning of this chapter states,
    perform as well as, or better than, any technique in this book. These methods
    are known as **random forests** and **gradient boosted trees**. Additionally,
    we will take a break from a business case and show how employing the random forest
    method on a dataset can assist in feature elimination/selection.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法，正如本章开头引用的论文所述，其表现与本书中的任何技术一样好，甚至更好。这些方法被称为**随机森林**和**梯度提升树**。此外，我们将从商业案例中暂时休息一下，展示如何在数据集上应用随机森林方法来帮助特征消除/选择。
- en: An overview of the techniques
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术概述
- en: We will now get to an overview of the techniques, covering the regression and
    classification trees, random forests, and gradient boosting. This will set the
    stage for the practical business cases.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将对技术进行概述，涵盖回归和分类树、随机森林和梯度提升。这将为实际商业案例奠定基础。
- en: Understanding the regression trees
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解回归树
- en: To establish an understanding of tree-based methods, it is probably easier to
    start with a quantitative outcome and then move on to how it works in a classification
    problem. The essence of a tree is that the features are partitioned, starting
    with the first split that improves the RSS the most. These binary splits continue
    until the termination of the tree. Each subsequent split/partition is not done
    on the entire dataset but only on the portion of the prior split that it falls
    under. This top-down process is referred to as recursive partitioning. It is also
    a process that is **greedy**, a term you may stumble upon in reading about the
    machine learning methods. Greedy means that, during each split in the process,
    the algorithm looks for the greatest reduction in the RSS without any regard to
    how well it will perform on the latter partitions. The result is that you may
    end up with a full tree of unnecessary branches leading to a low bias but a high
    variance. To control this effect, you need to appropriately prune the tree to
    an optimal size after building a full tree.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 要建立对基于树的方法的了解，可能从定量结果开始，然后转向它在分类问题中的应用要容易一些。树的本质在于特征被分区，从第一个能最大程度提高RSS的分支开始。这些二进制分支一直持续到树的终止。每个后续的分支/分区不是在整个数据集上进行的，而是在先前的分支下它所属的部分上进行的。这种自上而下的过程被称为递归分区。它也是一个**贪婪**的过程，你可能在阅读有关机器学习方法的资料时遇到这个术语。贪婪意味着在过程中的每个分支中，算法寻找RSS的最大减少，而不考虑它对后续分区性能的影响。结果是，你可能会得到一个包含许多不必要的分支的完整树，导致低偏差但高方差。为了控制这种影响，在构建完整树之后，你需要适当地修剪树到最佳大小。
- en: '*Figure 6.1*, provides a visual of this technique in action. The data is hypothetical
    with 30 observations, a response ranging from 1 to 10, and two predictor features,
    both ranging in value from 0 to 10 named **X1** and **X2**. The tree has three
    splits leading to four terminal nodes. Each split is basically an `if...then`
    statement or uses an R syntax `ifelse()`. The first split is--if **X1** is less
    than **3.5**, then the response is split into four observations with an average
    value of **2.4** and the remaining 26 observations. This left branch of four observations
    is a terminal node as any further splits would not substantially improve the RSS.
    The predicted value for these four observations in that partition of the tree
    becomes the average. The next split is at **X2 < 4** and finally, **X1 < 7.5**.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.1* 展示了该技术在实际应用中的视觉效果。数据是假设的，包含30个观测值，响应值从1到10不等，以及两个预测特征，这两个特征的价值范围均为0到10，分别命名为**X1**和**X2**。该树有三个分支，导致四个终端节点。每个分支基本上是一个`if...then`语句或使用R语言的`ifelse()`语法。第一个分支是：如果**X1**小于**3.5**，则响应值被分为四个观测值，平均值为**2.4**，剩余的26个观测值。这个包含四个观测值的左分支是一个终端节点，因为任何进一步的分支都不会显著提高RSS。该树中该分区中这四个观测值的预测值成为平均值。下一个分支是在**X2
    < 4**，最后是**X1 < 7.5**。'
- en: 'An advantage of this method is that it can handle highly nonlinear relationships;
    however, can you see a couple of potential problems? The first issue is that an
    observation is given the average of the terminal node under which it falls. This
    can hurt the overall predictive performance (high bias). Conversely, if you keep
    partitioning the data further and further so as to achieve a low bias, a high
    variance can become an issue. As with the other methods, you can use cross-validation
    to select the appropriate tree depth size:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的优点是它可以处理高度非线性关系；然而，你能看到几个潜在的问题吗？第一个问题是，一个观测值被赋予其所属终端节点的平均值。这可能会损害整体预测性能（高偏差）。相反，如果你继续进一步分区数据，以实现低偏差，那么高方差可能成为一个问题。与其他方法一样，你可以使用交叉验证来选择合适的树深度大小：
- en: '![](img/B06473_06_01.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06473_06_01.png)'
- en: 'Figure 6.1: Regression Tree with 3 splits and 4 terminal nodes and the corresponding
    node average and number of observations'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：具有3个分支和4个终端节点的回归树以及相应的节点平均数和观测值数量
- en: Classification trees
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类树
- en: Classification trees operate under the same principle as regression trees, except
    that the splits are not determined by the RSS but an error rate. The error rate
    used is not what you would expect where the calculation is simply the misclassified
    observations divided by the total observations. As it turns out, when it comes
    to tree-splitting, a misclassification rate, by itself, may lead to a situation
    where you can gain information with a further split but not improve the misclassification
    rate. Let's look at an example.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 分类树遵循与回归树相同的原理，只是分割不是由 RSS 决定的，而是由错误率决定的。使用的错误率不是你所期望的，计算只是将误分类的观测值除以总观测值。实际上，当涉及到树分割时，一个误分类率本身可能会导致一种情况，即进一步的分割可以获取信息，但不会提高误分类率。让我们看一个例子。
- en: 'Suppose we have a node, let''s call it **N0**, where you have seven observations
    labeled `No` and three observations labeled `Yes`, and we can say that the misclassified
    rate is 30 percent. With this in mind, let''s calculate a common alternative error
    measure called the **Gini index**. The formula for a single node Gini index is
    as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个节点，让我们称它为 **N0**，其中你有七个标记为 `No` 的观测值和三个标记为 `Yes` 的观测值，我们可以说误分类率是 30%。考虑到这一点，让我们计算一个常见的替代错误度量，称为**基尼指数**。单个节点的基尼指数公式如下：
- en: '![](img/image_06_01b.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_06_01b.png)'
- en: Then, for `N0`, the Gini is *1 - (.7)² - (.3)²*, which is equal to *0.42*, versus
    the misclassification rate of 30 per cent.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，对于 `N0`，基尼指数是 *1 - (.7)² - (.3)²*，等于 *0.42*，与 30% 的误分类率相比。
- en: 'Taking this example further, we will now create node *N1* with three observations
    from `Class 1` and none from `Class 2`, along with *N2*, which has four observations
    from `Class 1` and three from `Class 2`. Now, the overall misclassification rate
    for this branch of the tree is still 30 per cent, but look at how the overall
    Gini index has improved:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以此为例，我们现在将创建一个名为 *N1* 的节点，它有三个来自 `Class 1` 的观测值，没有来自 `Class 2` 的观测值，以及一个名为 *N2*
    的节点，它有四个来自 `Class 1` 的观测值和三个来自 `Class 2` 的观测值。现在，这个树分支的整体误分类率仍然是 30%，但看看整体基尼指数是如何提高的：
- en: '*Gini(N1) = 1 - (3/3)² - (0/3)² = 0*'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Gini(N1) = 1 - (3/3)² - (0/3)² = 0*'
- en: '*Gini(N2) = 1 - (4/7)² - (3/7)² = 0.49*'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Gini(N2) = 1 - (4/7)² - (3/7)² = 0.49*'
- en: '*New Gini index = (proportion of N1 x Gini(N1)) + (proportion of N2 x Gini(N2))*,
    which is equal to *(.3 x 0) + (.7 x 0.49)* or *0.343*'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*新的基尼指数 = (N1的比例 x Gini(N1)) + (N2的比例 x Gini(N2))*，等于 *(0.3 x 0) + (0.7 x 0.49)*
    或 *0.343*'
- en: By doing a split on a surrogate error rate, we actually improved our model impurity,
    reducing it from *0.42* to *0.343*, whereas the misclassification rate did not
    change. This is the methodology that is used by the `rpart()` package, which we
    will be using in this chapter.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在代理错误率上进行分割，我们实际上提高了模型的不纯度，将其从 *0.42* 降低到 *0.343*，而误分类率没有变化。这是 `rpart()` 包所使用的方法，我们将在本章中使用。
- en: Random forest
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林
- en: To greatly improve our model's predictive ability, we can produce numerous trees
    and combine the results. The random forest technique does this by applying two
    different tricks in model development. The first is the use of **bootstrap aggregation**
    or **bagging**, as it is called.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了极大地提高我们模型的预测能力，我们可以生成许多树并合并结果。随机森林技术通过在模型开发中应用两种不同的技巧来实现这一点。第一种是使用**自助聚合**或**袋装法**，正如其名称所示。
- en: In bagging, an individual tree is built on a random sample of the dataset, roughly
    two-thirds of the total observations (note that the remaining one-third is referred
    to as **out-of-bag** (**oob**)). This is repeated dozens or hundreds of times
    and the results are averaged. Each of these trees is grown and not pruned based
    on any error measure, and this means that the variance of each of these individual
    trees is high. However, by averaging the results, you can reduce the variance
    without increasing the bias.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在袋装法中，单个树是基于数据集的随机样本构建的，大约占总观测值的二分之一（注意，剩余的三分之一被称为**袋外**（**oob**））。这会重复数十次或数百次，然后取平均值。这些树中的每一棵都是基于任何错误度量进行生长而不进行剪枝的，这意味着这些单个树的方差很高。然而，通过平均结果，你可以降低方差而不增加偏差。
- en: The next thing that random forest brings to the table is that concurrently with
    the random sample of the data--that is, bagging--it also takes a random sample
    of the input features at each split. In the `randomForest` package, we will use
    the default random number of the predictors that are sampled, which, for classification
    problems, is the square root of the total predictors and for regression, it is
    the total number of the predictors divided by three. The number of predictors
    the algorithm randomly chooses at each split can be changed via the model tuning
    process.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林带来的下一件事是，在随机抽取数据的同时——即袋装——它还随机抽取每个分割点的输入特征。在`randomForest`包中，我们将使用默认的预测因子抽样随机数，对于分类问题，这是总预测因子的平方根，对于回归，则是总预测因子数除以三。算法在每个分割点随机选择的预测因子数量可以通过模型调优过程进行更改。
- en: By doing this random sample of the features at each split and incorporating
    it into the methodology, you can mitigate the effect of a highly correlated predictor
    becoming the main driver in all of your bootstrapped trees, preventing you from
    reducing the variance that you hoped to achieve with bagging. The subsequent averaging
    of the trees that are less correlated to each other is more generalizable and
    robust to outliers than if you only performed bagging.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在每个分割点进行特征随机抽样并将其纳入方法论，你可以减轻高度相关预测因子成为所有你的自助树的主要驱动因素的影响，防止你降低你希望通过袋装实现的方差。随后对彼此相关性较低的树的平均化更具一般性和对异常值更稳健，如果你只执行袋装的话。
- en: Gradient boosting
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度提升
- en: Boosting methods can become extremely complicated to learn and understand, but
    you should keep in mind what is fundamentally happening behind the curtain. The
    main idea is to build an initial model of some kind (linear, spline, tree, and
    so on) called the base learner, examine the residuals, and fit a model based on
    these residuals around the so-called **loss function**. A loss function is merely
    the function that measures the discrepancy between the model and desired prediction,
    for example, a squared error for regression or the logistic function for classification.
    The process continues until it reaches some specified stopping criterion. This
    is sort of like the student who takes a practice exam and gets 30 out of 100 questions
    wrong and, as a result, studies only these 30 questions that were missed. In the
    next practice exam, they get 10 out of those 30 wrong and so only focus on those
    10 questions, and so on. If you would like to explore the theory behind this further,
    a great resource for you is available in Frontiers in Neurorobotics, *Gradient
    boosting machines, a tutorial*, Natekin A., Knoll A. (2013), at [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 提升方法可能变得极其复杂，难以学习和理解，但你应该牢记幕后的基本发生情况。主要思想是构建某种类型的初始模型（线性、样条、树等）称为基学习器，检查残差，并在所谓的**损失函数**周围拟合模型。损失函数仅仅是衡量模型与期望预测之间差异的函数，例如，回归中的平方误差或分类中的逻辑函数。这个过程会持续进行，直到达到某个指定的停止标准。这有点像那个参加模拟考试并且答错100题中的30题的学生，因此只复习这些答错的30题。在下一场模拟考试中，他们又答错了那30题中的10题，因此只关注这10题，以此类推。如果你想进一步探索这一理论的背后，Frontiers
    in Neurorobotics上的一个很好的资源是*Natekin A., Knoll A. (2013)*的*梯度提升机，教程*，可在[http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/)找到。
- en: As just mentioned, boosting can be applied to many different base learners,
    but here we will only focus on the specifics of **tree-based learning**. Each
    tree iteration is small and we will determine how small with one of the tuning
    parameters referred to as interaction depth. In fact, it may be as small as one
    split, which is referred to as a stump.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 正如刚才提到的，提升可以应用于许多不同的基学习器，但在这里我们只会关注**基于树的学习的具体细节**。每个树迭代都很小，我们将通过一个称为交互深度的调优参数来确定它有多小。实际上，它可能小到只有一个分割，这被称为树桩。
- en: Trees are sequentially fit to the residuals, according to the loss function,
    up to the number of trees that we specified (our stopping criterion).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 树按损失函数的顺序拟合到残差，直到我们指定的树的数量（我们的停止标准）。
- en: 'There are a number of parameters that require tuning in the model-building
    process using the `Xgboost` package, which stands for **eXtreme Gradient Boosting**.
    This package has become quite popular for online data contests because of its
    winning performance. There is excellent background material on boosting trees
    and on Xgboost on the following website:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用`Xgboost`包进行模型构建过程中，有许多参数需要调整，该包代表**极端梯度提升**。由于其在在线数据竞赛中的获胜表现，这个包已经变得非常流行。以下网站上提供了关于提升树和Xgboost的优秀背景资料：
- en: '[http://xgboost.readthedocs.io/en/latest/model.html](http://xgboost.readthedocs.io/en/latest/model.html)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://xgboost.readthedocs.io/en/latest/model.html](http://xgboost.readthedocs.io/en/latest/model.html)'
- en: What we will do in the business case is show how to begin to optimize the hyperparameters
    and produce meaningful output and predictions. These parameters can interact with
    each other, and if you just tinker with one without considering the other, your
    model may worsen the performance. The `caret` package will help us in the tuning
    endeavor.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在商业案例中，我们将展示如何开始优化超参数并产生有意义的输出和预测。这些参数可以相互影响，如果你只调整一个而不考虑其他，你的模型可能会降低性能。`caret`包将帮助我们进行调优。
- en: Business case
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 商业案例
- en: The overall business objective in this situation is to see whether we can improve
    the predictive ability for some of the cases that we already worked on in the
    previous chapters. For regression, we will revisit the prostate cancer dataset
    from [Chapter 4](04f803d3-791a-4505-80a7-905dfd6fdcc3.xhtml), *Advanced Feature
    Selection in Linear Models*. The baseline mean squared error to improve on is
    0.444.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，整体业务目标是看看我们是否可以提高之前章节中已经处理的一些案例的预测能力。对于回归，我们将重新审视来自[第4章](04f803d3-791a-4505-80a7-905dfd6fdcc3.xhtml)，*线性模型中的高级特征选择*的前列腺癌数据集。我们需要改进的基线均方误差是0.444。
- en: For classification purposes, we will utilize both the breast cancer biopsy data
    from [Chapter 3](d5d39222-b2f8-4c80-9348-34e075893e47.xhtml), *Logistic Regression
    and Discriminant Analysis* and the Pima Indian Diabetes data from [Chapter 5](a7511867-5362-4215-a7dd-bbdc162740d1.xhtml),
    *More Classification Techniques - K-Nearest Neighbors and Support Vector Machines*.
    In the breast cancer data, we achieved 97.6 per cent predictive accuracy. For
    the diabetes data, we are seeking to improve on the 79.6 per cent accuracy rate.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类目的，我们将利用来自[第3章](d5d39222-b2f8-4c80-9348-34e075893e47.xhtml)，*逻辑回归和判别分析*的乳腺癌活检数据和来自[第5章](a7511867-5362-4215-a7dd-bbdc162740d1.xhtml)，*更多分类技术
    - K最近邻和支持向量机*的皮马印第安人糖尿病数据。在乳腺癌数据中，我们达到了97.6%的预测准确率。对于糖尿病数据，我们希望提高79.6%的准确率。
- en: Both random forests and boosting will be applied to all three datasets. The
    simple tree method will only be used on the breast and prostate cancer sets from
    [Chapter 4](04f803d3-791a-4505-80a7-905dfd6fdcc3.xhtml), *Advanced Feature Selection
    in Linear Models*.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林和提升将应用于所有三个数据集。简单树方法仅用于来自[第4章](04f803d3-791a-4505-80a7-905dfd6fdcc3.xhtml)，*线性模型中的高级特征选择*的乳腺癌和前列腺癌数据集。
- en: Modeling and evaluation
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建模和评估
- en: To perform the modeling process, we will need to load seven different R packages.
    Then, we will go through each of the techniques and compare how well they perform
    on the data analyzed with the prior methods in the previous chapters.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行建模过程，我们需要加载七个不同的R包。然后，我们将逐一介绍这些技术，并比较它们在之前章节中使用先前方法分析的数据上的表现。
- en: Regression tree
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归树
- en: 'We will jump right into the `prostate` dataset, but let''s first load the necessary
    R packages. As always, please ensure that you have the libraries installed prior
    to loading the packages:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将直接跳到`prostate`数据集，但首先让我们加载必要的R包。和往常一样，请在加载包之前确保你已经安装了这些库：
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We will first do regression with the `prostate` data and prepare it, as we
    did in [Chapter 4](04f803d3-791a-4505-80a7-905dfd6fdcc3.xhtml), *Advanced Feature
    Selection in Linear Models*. This involves calling the dataset, coding the `gleason`
    score as an indicator variable using the `ifelse()` function, and creating the
    `test` and `train` sets. The `train` set will be `pros.train` and the `test` set
    will be `pros.test`, as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先使用`prostate`数据进行回归，并像在[第4章](04f803d3-791a-4505-80a7-905dfd6fdcc3.xhtml)，*线性模型中的高级特征选择*中做的那样准备数据。这包括调用数据集，使用`ifelse()`函数将`gleason`评分编码为指示变量，并创建`test`和`train`集。`train`集将是`pros.train`，而`test`集将是`pros.test`，如下所示：
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To build a regression tree on the `train` data, we will use the `rpart()` function
    from R''s `party` package. The syntax is quite similar to what we used in the
    other modeling techniques:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 要在`train`数据上构建回归树，我们将使用R的`party`包中的`rpart()`函数。语法与我们使用的其他建模技术非常相似：
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can call this object and examine the error per number of splits in order
    to determine the optimal number of splits in the tree:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以调用这个对象并检查每个分割的误差，以确定树的最佳分割数：
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This is a very important table to analyze. The first column labeled `CP` is
    the cost complexity parameter. The second column, `nsplit`, is the number of splits
    in the tree. The `rel error` column stands for relative error and is the RSS for
    the number of splits divided by the RSS for no splits *RSS(k)/RSS(0)*. Both `xerror`
    and `xstd` are based on the ten-fold cross-validation with `xerror` being the
    average error and `xstd` the standard deviation of the cross-validation process.
    We can see that while five splits produced the lowest error on the `full` dataset,
    four splits produced a slightly less error using cross-validation. You can examine
    this using `plotcp()`:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常重要的表格，需要分析。第一列标记为`CP`的是成本复杂度参数。第二列`nsplit`是树中的分割数。`rel error`列代表相对误差，是分割数的RSS除以无分割的RSS（*RSS(k)/RSS(0)*）。`xerror`和`xstd`都是基于十折交叉验证的，其中`xerror`是平均误差，`xstd`是交叉验证过程的标准差。我们可以看到，虽然五个分割在`full`数据集上产生了最低的误差，但四个分割使用交叉验证产生了略低的误差。你可以使用`plotcp()`来检查这一点：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令的输出如下：
- en: '![](img/image_06_02.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_06_02.png)'
- en: 'The plot shows us the relative error by the tree size with the corresponding
    error bars. The horizontal line on the plot is the upper limit of the lowest standard
    error. Selecting a tree size, `5`, which is four splits, we can build a new tree
    object where `xerror` is minimized by pruning our tree by first creating an object
    for `cp` associated with the pruned tree from the table. Then the `prune()` function
    handles the rest:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图表展示了树的大小与相应误差条对应的相对误差。图表上的水平线是最低标准误差的上限。选择一个树的大小，`5`，即四个分割，我们可以通过首先从表中创建一个与修剪后的树关联的`cp`对象来修剪我们的树，从而最小化`xerror`。然后`prune()`函数处理剩余的部分：
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'With this done, you can plot and compare the full and pruned trees. The tree
    plots produced by the `partykit` package are much better than those produced by
    the `party` package. You can simply use the `as.party()` function as a wrapper
    in `plot()`:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些后，你可以绘制并比较完整树和修剪后的树。`partykit`包生成的树图比`party`包生成的要好得多。你可以简单地使用`as.party()`函数作为`plot()`的包装器：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令的输出如下：
- en: '![](img/image_06_03.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_06_03.png)'
- en: 'Now we will use the `as.party()` function for the pruned tree:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用`as.party()`函数对修剪后的树进行处理：
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令的输出如下：
- en: '![](img/image_06_04.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_06_04.png)'
- en: Note that the splits are exactly the same in the two trees with the exception
    of the last split, which includes the variable `age` for the full tree. Interestingly,
    both the first and second splits in the tree are related to the log of cancer
    volume (`lcavol`). These plots are quite informative as they show the splits,
    nodes, observations per node, and boxplots of the outcome that we are trying to
    predict.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，两个树中的分割完全相同，除了最后一个分割，它包含了完整树的变量`age`。有趣的是，树中的第一个和第二个分割都与癌症体积的对数(`lcavol`)相关。这些图表非常有信息量，因为它们显示了分割、节点、每个节点的观测值以及我们试图预测的结果的箱线图。
- en: 'Let''s see how well the pruned tree performs on the `test` data. What we will
    do is create an object of the predicted values using the `predict()` function
    and incorporate the `test` data. Then, calculate the errors (the predicted values
    minus the actual values) and finally, the mean of the squared errors:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看修剪后的树在`test`数据上的表现如何。我们将使用`predict()`函数创建预测值的对象，并包含`test`数据。然后，计算误差（预测值减去实际值），最后，计算平方误差的平均值：
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We have not improved on the predictive value from our work in [Chapter 4](04f803d3-791a-4505-80a7-905dfd6fdcc3.xhtml),
    *Advanced Feature Selection in Linear Models* where the baseline MSE was `0.44`.
    However, the technique is not without value. One can look at the tree plots that
    we produced and easily explain what the primary drivers behind the response are.
    As mentioned in the introduction, the trees are easy to interpret and explain,
    which may be more important than accuracy in many cases.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第4章](04f803d3-791a-4505-80a7-905dfd6fdcc3.xhtml)，*线性模型中的高级特征选择*中并没有提高预测值，那里的基线均方误差（MSE）是`0.44`。然而，这项技术并非没有价值。人们可以查看我们生成的树图，并轻松解释响应背后的主要驱动因素。正如引言中提到的，树很容易解释和说明，这在许多情况下可能比准确性更重要。
- en: Classification tree
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类树
- en: 'For the classification problem, we will prepare the breast cancer data in the
    same fashion as we did in [Chapter 3](d5d39222-b2f8-4c80-9348-34e075893e47.xhtml),
    *Logistic Regression and Discriminant Analysis*. After loading the data, you will
    delete the patient ID, rename the features, eliminate the few missing values,
    and then create the `train`/`test` datasets in the following way:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类问题，我们将以与[第3章](d5d39222-b2f8-4c80-9348-34e075893e47.xhtml)，*逻辑回归和判别分析*中相同的方式准备乳腺癌数据。在加载数据后，您将删除患者ID，重命名特征，消除少量缺失值，然后按照以下方式创建`train`/`test`数据集：
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'With the data set up appropriately, we will use the same syntax style for a
    classification problem as we did previously for a regression problem, but before
    creating a classification tree, we will need to ensure that the outcome is `Factor`,
    which can be done using the `str()` function:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据设置适当的情况下，我们将使用与之前回归问题相同的语法风格来处理分类问题，但在创建分类树之前，我们需要确保结果是`因子`类型，这可以通过使用`str()`函数来完成：
- en: '[PRE10]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'First, create the tree and then examine the table for the optimal number of
    splits:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，创建树，然后检查表以确定最佳分割数：
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The cross-validation error is at a minimum with only two splits (row `3`).
    We can now prune the tree, plot the pruned tree, and see how it performs on the
    `test` set:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证误差在只有两个分割（行`3`）时达到最小。现在我们可以剪枝树，绘制剪枝后的树，并查看它在`test`集上的表现：
- en: '[PRE12]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令的输出如下：
- en: '![](img/image_06_05.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_06_05.png)'
- en: 'An examination of the tree plot shows that the uniformity of the cell size
    is the first split, then nuclei. The full tree had an additional split at the
    cell thickness. We can predict the `test` observations using `type="class"` in
    the `predict()` function, as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 检查树图显示，细胞大小的均匀性是第一个分割，然后是核。完整的树在细胞厚度处还有一个额外的分割。我们可以使用`predict()`函数中的`type="class"`来预测`test`观测值，如下所示：
- en: '[PRE13]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The basic tree with just two splits gets us almost 96 percent accuracy. This
    still falls short of 97.6 percent with logistic regression but should encourage
    us to believe that we can improve on this with the upcoming methods, starting
    with random forests.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 只有两个分割的基本树使我们几乎达到96%的准确率。这仍然低于逻辑回归的97.6%，但应该鼓励我们相信我们可以通过即将到来的方法来提高这一点，从随机森林开始。
- en: Random forest regression
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林回归
- en: 'In this section, we will start by focusing on the `prostate` data again. Before
    moving on to the breast cancer and Pima Indian sets. We will use the `randomForest`
    package. The general syntax to create a `random forest` object is to use the `randomForest()`
    function and specify the formula and dataset as the two primary arguments. Recall
    that for regression, the default variable sample per tree iteration is p/3, and
    for classification, it is the square root of p, where p is equal to the number
    of predictor variables in the data frame. For larger datasets, in terms of p,
    you can tune the `mtry` parameter, which will determine the number of p sampled
    at each iteration. If p is less than 10 in these examples, we will forgo this
    procedure. When you want to optimize `mtry` for larger p datasets, you can utilize
    the `caret` package or use the `tuneRF()` function in `randomForest`. With this,
    let''s build our forest and examine the results, as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将再次关注`prostate`数据。在继续到乳腺癌和皮马印第安人集之前。我们将使用`randomForest`包。创建`random forest`对象的一般语法是使用`randomForest()`函数，并指定公式和数据集作为两个主要参数。回想一下，对于回归，默认的每棵树迭代变量样本是p/3，对于分类，它是p的平方根，其中p等于数据框中预测变量的数量。对于较大的数据集，在p的术语中，您可以调整`mtry`参数，这将确定每次迭代中采样的p的数量。如果这些示例中的p小于10，我们将省略此过程。当您想要为较大的p数据集优化`mtry`时，您可以使用`caret`包或使用`randomForest`中的`tuneRF()`函数。有了这个，让我们构建我们的森林并检查结果，如下所示：
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The call of the `rf.pros` object shows us that the random forest generated
    `500` different trees (the default) and sampled two variables at each split. The
    result is an MSE of `0.68` and nearly 53 percent of the variance explained. Let''s
    see if we can improve on the default number of trees. Too many trees can lead
    to overfitting; naturally, how many is too many depends on the data. Two things
    can help out, the first one is a plot of `rf.pros` and the other is to ask for
    the minimum MSE:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`rf.pros`对象的调用显示随机森林生成了`500`棵不同的树（默认值）并在每次分割中采样两个变量。结果是MSE为`0.68`和几乎53%的方差解释。让我们看看我们是否能改进默认的树的数量。过多的树可能导致过拟合；自然地，多少是过多的取决于数据。有两件事可以帮助，第一件事是`rf.pros`的图表，另一件事是要求最小MSE：'
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令的输出如下：
- en: '![](img/image_06_06.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_06_06.png)'
- en: This plot shows the MSE by the number of trees in the model. You can see that
    as the trees are added, significant improvement in MSE occurs early on and then
    flatlines just before `100` trees are built in the forest.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图表显示了模型中树的数目与MSE的关系。您可以看到，随着树的增加，MSE的显著改进发生在早期，然后在森林中构建了`100`棵树之前趋于平稳。
- en: 'We can identify the specific and optimal tree with the `which.min()` function,
    as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`which.min()`函数来识别特定的和最优的树，如下所示：
- en: '[PRE16]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can try `75` trees in the random forest by just specifying `ntree=75` in
    the model syntax:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在模型语法中指定`ntree=75`来尝试随机森林中的`75`棵树：
- en: '[PRE17]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You can see that the MSE and variance explained have both improved slightly.
    Let''s see another plot before testing the model. If we are combining the results
    of `75` different trees that are built using bootstrapped samples and only two
    random predictors, we will need a way to determine the drivers of the outcome.
    One tree alone cannot be used to paint this picture, but you can produce a variable
    importance plot and corresponding list. The y-axis is a list of variables in descending
    order of importance and the x-axis is the percentage of improvement in MSE. Note
    that for the classification problems, this will be an improvement in the Gini
    index. The function is `varImpPlot()`:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到均方误差（MSE）和解释的方差都略有提高。在测试模型之前，让我们看看另一个图表。如果我们正在结合使用自举样本构建的`75`棵不同树的结果，并且只有两个随机预测因子，我们需要一种方法来确定结果的原因。单独一棵树不能描绘这幅图，但您可以生成一个变量重要性图和相应的列表。y轴是按重要性降序排列的变量列表，x轴是MSE改进的百分比。注意，对于分类问题，这将是在基尼指数上的改进。该函数是`varImpPlot()`：
- en: '[PRE18]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令的输出如下：
- en: '![](img/image_06_07.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_06_07.png)'
- en: 'Consistent with the single tree, `lcavol` is the most important variable and
    `lweight` is the second-most important variable. If you want to examine the raw
    numbers, use the `importance()` function, as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 与单棵树一致，`lcavol`是最重要的变量，`lweight`是第二重要的变量。如果您想检查原始数字，请使用`importance()`函数，如下所示：
- en: '[PRE19]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, it is time to see how it did on the `test` data:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候看看它在`test`数据上的表现了：
- en: '[PRE20]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The MSE is still higher than our `0.44` that we achieved in [Chapter 4](04f803d3-791a-4505-80a7-905dfd6fdcc3.xhtml),
    *Advanced Feature Selection in Linear Models* with LASSO and no better than just
    a single tree.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Random forest classification
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Perhaps you are disappointed with the performance of the random forest regression
    model, but the true power of the technique is in the classification problems.
    Let''s get started with the breast cancer diagnosis data. The procedure is nearly
    the same as we did with the regression problem:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The `OOB` error rate is `3.16%`. Again, this is with all the **500** trees
    factored into the analysis. Let''s plot the **Error** by **trees**:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_06_08.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
- en: 'The plot shows that the minimum error and standard error is the lowest with
    quite a few trees. Let''s now pull the exact number using `which.min()` again.
    The one difference from before is that we need to specify column `1` to get the
    error rate. This is the overall error rate and there will be additional columns
    for each error rate by the class label. We will not need them in this example.
    Also, `mse` is no longer available but rather `err.rate` is used instead, as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Only 19 trees are needed to optimize the model accuracy. Let''s try this and
    see how it performs:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Well, how about that? The `train` set error is below 3 percent, and the model
    even performs better on the `test` set where we had only three observations misclassified
    out of `209` and none were false positives. Recall that the best so far was with
    logistic regression with 97.6 percent accuracy. So this seems to be our best performer
    yet on the breast cancer data. Before moving on, let''s have a look at the variable
    importance plot:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_06_09.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
- en: The importance in the preceding plot is in each variable's contribution to the
    mean decrease in the Gini index. This is rather different from the splits of the
    single tree. Remember that the full tree had splits at the size (consistent with
    random forest), then nuclei, and then thickness. This shows how potentially powerful
    a technique building random forests can be, not only in the predictive ability,
    but also in feature selection.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'Moving on to the tougher challenge of the Pima Indian diabetes model, we will
    first need to prepare the data in the following way:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now we will move on to the building of the model, as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We get a `20` per cent misclassification rate error, which is no better than
    what we''ve done before on the `train` set. Let''s see if optimizing the tree
    size can improve things dramatically:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'At `80` trees in the forest, there is minimal improvement in the `OOB` error.
    Can random forest live up to the hype on the `test` data? We will see in the following
    way:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Well, we get only 73 percent accuracy on the `test` data, which is inferior
    to what we achieved using the SVM.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: While random forest disappointed on the diabetes data, it proved to be the best
    classifier so far for the breast cancer diagnosis. Finally, we will move on to
    gradient boosting.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Extreme gradient boosting - classification
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned previously, we will be using the `xgboost` package in this section,
    which we have already loaded. Given the method's well-earned reputation, let's
    try it on the diabetes data.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'As stated in the boosting overview, we will be tuning a number of parameters:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '`nrounds`: The maximum number of iterations (number of trees in final model).'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`colsample_bytree`: The number of features, expressed as a ratio, to sample
    when building a tree. Default is 1 (100% of the features).'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_child_weight`: The minimum weight in the trees being boosted. Default
    is 1.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eta`: Learning rate, which is the contribution of each tree to the solution.
    Default is 0.3.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gamma`: Minimum loss reduction required to make another leaf partition in
    a tree.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subsample`: Ratio of data observations. Default is 1 (100%).'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_depth`: Maximum depth of the individual trees.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the `expand.grid()` function, we will build our experimental grid to run
    through the training process of the `caret` package. If you do not specify values
    for all of the preceding parameters, even if it is just a default, you will receive
    an error message when you execute the function. The following values are based
    on a number of training iterations I've done previously. I encourage you to try
    your own tuning values.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s build the grid as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This creates a grid of 24 different models that the `caret` package will run
    so as to determine the best tuning parameters. A note of caution is in order.
    On a dataset of the size that we will be working with, this process takes only
    a few seconds. However, in large datasets, this can take hours. As such, you must
    apply your judgment and experiment with smaller samples of the data in order to
    identify the tuning parameters, in case the time is of the essence, or you are
    constrained by the size of your hard drive.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'Before using the `train()` function from the `caret` package, I would like
    to specify the `trainControl` argument by creating an object called `control`.
    This object will store the method that we want so as to train the tuning parameters.
    We will use the `5` fold cross-validation, as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'To utilize the `train.xgb()` function, just specify the formula as we did with
    the other models: the `train` dataset inputs, labels, method, train control, and
    experimental grid. Remember to set the random seed:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Since in `trControl` I set `verboseIter` to `TRUE`, you should have seen each
    training iteration within each k-fold.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'Calling the object gives us the optimal parameters and the results of each
    of the parameter settings, as follows (abbreviated for simplicity):'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This gives us the best combination of parameters to build a model. The accuracy
    in the training data was 81% with a Kappa of 0.55\. Now it gets a little tricky,
    but this is what I''ve seen as best practice. First, create a list of parameters
    that will be used by the `xgboost` training function, `xgb.train()`. Then, turn
    the dataframe into a matrix of input features and a list of labeled numeric outcomes
    (0s and 1s). Then further, turn the features and labels into the input required,
    as `xgb.Dmatrix`. Try this:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'With all of that prepared, just create the model:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Before seeing how it does on the test set, let''s check the variable importance
    and also plot it. You can examine three items: **gain**, **cover**, and** frequency**. **Gain** is
    the improvement in accuracy that feature brings to the branches it is on. **Cover**
    is the relative number of total observations related to this feature. **Frequency**
    is the per cent of times that feature occurs in all of the trees. The following
    code produces the desired output:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_06_10.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
- en: How does the feature importance compare to other methods?
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how we see it performed on the test set, which like the training data
    must be in a matrix. Let''s also bring in the tools from the `InformationValue`
    package to help our efforts. This code loads the library and produces some output
    to analyze model performance:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Did you notice what I did there with `optimalCutoff()`? Well, that function
    from `InformationValue` provides the optimal probability threshold to minimize
    error. By the way, the model error is around 25%. It''s still not superior to
    our SVM model. As an aside, we see the ROC curve and the achievement of an AUC
    above 0.8\. The following code produces the ROC curve:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output of the code is as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_06_11.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
- en: Model selection
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recall that our primary objective in this chapter was to use the tree-based
    methods to improve the predictive ability of the work done in the prior chapters.
    What did we learn? First, on the `prostate` data with a quantitative response,
    we were not able to improve on the linear models that we produced in [Chapter
    4](04f803d3-791a-4505-80a7-905dfd6fdcc3.xhtml), *Advanced Feature Selection in
    Linear Models*. Second, the random forest outperformed logistic regression on
    the Wisconsin Breast Cancer data of [Chapter 3](d5d39222-b2f8-4c80-9348-34e075893e47.xhtml),
    *Logistic Regression and Discriminant Analysis*. Finally, and I must say disappointingly,
    we were not able to improve on the SVM model on the Pima Indian diabetes data
    with boosted trees.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: As a result, we can feel comfortable that we have good models for the prostate
    and breast cancer problems. We will try one more time to improve the model for
    diabetes in [Chapter 7](73c55dff-b99c-4ce7-ab3d-9a47ff2f6f2f.xhtml), *Neural Networks and
    Deep Learning*. Before we bring this chapter to a close, I want to introduce the
    powerful method of feature elimination using random forest techniques.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Feature Selection with random forests
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we''ve looked at several feature selection techniques, such as regularization,
    best subsets, and recursive feature elimination. I now want to introduce an effective
    feature selection method for classification problems with Random Forests using
    the `Boruta` package. A paper is available that provides details on how it works
    in providing all relevant features:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Kursa M., Rudnicki W. (2010), *Feature Selection with the Boruta Package*, *Journal
    of Statistical Software,* 36(11), 1 - 13
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: What I will do here is provide an overview of the algorithm and then apply it
    to a wide dataset. This will not serve as a separate business case but as a template
    to apply the methodology. I have found it to be highly effective, but be advised
    it can be computationally intensive. That may seem to defeat the purpose, but
    it effectively eliminates unimportant features, allowing you to focus on building
    a simpler, more efficient, and more insightful model. It is time well spent.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, the algorithm creates **shadow** **attributes** by copying
    all the inputs and shuffling the order of their observations to decorrelate them.
    Then, a random forest model is built on all the inputs and a Z-score of the mean
    accuracy loss for each feature, including the shadow ones. Features with significantly
    higher Z-scores or significantly lower Z-scores than the shadow attributes are
    deemed **important** and **unimportant** respectively. The shadow attributes and
    those features with known importance are removed and the process repeats itself
    until all features are assigned an importance value. You can also specify the
    maximum number of random forest iterations. After completion of the algorithm,
    each of the original features will be labeled as **confirmed**, **tentative**,
    or r**ejected**. You must decide on whether or not to include the tentative features
    for further modeling. Depending on your situation, you have some options:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Change the random seed and rerun the methodology multiple (k) times and select
    only those features that are confirmed in all the k runs
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Divide your data (training data) into k folds, run separate iterations on each
    fold, and select those features which are confirmed for all the k folds
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note that all of this can be done with just a few lines of code. Let''s have
    a look at the code, applying it to the `Sonar` data from the `mlbench` package.
    It consists of 208 observations, 60 numerical input features, and one vector of
    labels for classification. The labels are factors where, `R` if the `sonar` object
    is a rock and `M` if it is a mine. The first thing to do is load the data and
    do a quick, very quick, data exploration:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'To run the algorithm, you just need to load the `Boruta` package and create
    a formula in the `boruta()` function. Keep in mind that the labels must be and
    as a factor, or the algorithm will not work. If you want to track the progress
    of the algorithm, specify `doTrace = 1`. Also, don''t forget to set the random
    seed:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'As mentioned in the previous section, this can be computationally intensive.
    Here is how long it took on my old-fashioned laptop:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'A simple table will provide the count of the final importance decision. We
    see that we could safely eliminate half of the features:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Using these results, it is simple to create a new dataframe with our selected
    features. We start out using the `getSelectedAttributes()` function to capture
    the feature names. In this example, let''s only select those that are confirmed.
    If we wanted to include confirmed and tentative, we just specify `withTentative
    = TRUE` in the function:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Using the feature names, we create our subset of the Sonar data:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: There you have it! The `Sonar.features` dataframe includes all the confirmed
    features from the boruta algorithm. It can now be subjected to further meaningful
    data exploration and analysis. A few lines of code and some patience as the algorithm
    does its job can significantly improve your modeling efforts and insight generation.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned both the power and limitations of tree-based learning
    methods for both classification and regression problems. Single trees, while easy
    to build and interpret, may not have the necessary predictive power for many of
    the problems that we are trying to solve. To improve on the predictive ability,
    we have the tools of random forest and gradient-boosted trees at our disposal.
    With random forest, hundreds or even thousands of trees are built and the results
    aggregated for an overall prediction. Each tree of the random forest is built
    using a sample of the data called bootstrapping as well as a sample of the predictive
    variables. As for gradient boosting, an initial, and a relatively small, tree
    is produced. After this initial tree is built, subsequent trees are produced based
    on the residuals/misclassifications. The intended result of such a technique is
    to build a series of trees that can improve on the weakness of the prior tree
    in the process, resulting in decreased bias and variance. We also saw that in
    R, one can utilize random forests as a feature selection method.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: While these methods are extremely powerful, they are not some sort of nostrum
    in the world of machine learning. Different datasets require judgment on the part
    of the analyst as to which techniques are applicable. The techniques to be applied
    to the analysis and the selection of the tuning parameters are equally important.
    This fine tuning can make all the difference between a good predictive model and
    a great predictive model.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些方法非常强大，但在机器学习的世界里，它们并不是某种灵丹妙药。不同的数据集需要分析师的判断，以确定哪些技术适用。应用于分析和调整参数选择的技术同样重要。这种精细调整可能对良好预测模型和优秀预测模型之间的差异产生重大影响。
- en: In the next chapter, we turn our attention to using R to build neural networks
    and deep learning models.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将注意力转向使用 R 语言构建神经网络和深度学习模型。
