<html><head></head><body>
        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Video Analysis &amp;#x2013; Motion Detection and Tracking</h1>
                
            
            <article>
                
<p class="calibre2">As a computer vision developer, there is absolutely no way you can avoid dealing with video feeds from stored video files or cameras and other such sources. Treating video frames as individual images is one way to process videos, which surprisingly doesn't require much more effort or knowledge of the algorithms than what you have learned so far. For instance, you can apply a smoothening filter on a video, or in other words, a set of video frames, the same way as you would when you apply it on an individual image. The only trick here is that you must extract each frame from a video, as described in <span class="calibre12"><a target="_blank" href="part0030.html#SJGS0-15c05657f8254d318ea883ef10fc67f4" class="calibre9">Chapter 2</a>, <em class="calibre7">Getting Started with OpenCV</em></span>. However, in computer vision, there are certain algorithms that are meant to work with consecutive video frames and the result on their operation depends not just on an individual image but also on the result of the same operation on the previous frames. Both of the algorithm types we just mentioned will be the main topics covered in this chapter.</p>
<p class="calibre2">After learning about histograms and back-projection images in the previous chapter, we are ready to take on computer vision algorithms that are used to detect and track objects in real-time. These algorithms highly rely on a firm understanding on all the topics we learned in <span class="calibre12"><a target="_blank" href="part0102.html#318PC0-15c05657f8254d318ea883ef10fc67f4" class="calibre9">Chapter 5</a>, <em class="calibre7">Back-Projection and Histograms</em></span>. Based on this, we'll start this chapter with a couple of simple examples about how to use the computer vision algorithms we've learned so far, to process frames from a video file or camera, and then we'll move on to learn about two of the most famous object detection and tracking algorithms, the Mean Shift and CAM Shift algorithms. Then, we'll learn how to use the Kalman filter to correct the result of our object detection and tracking algorithms and how to remove noise from the results to get a better tracking result. We'll end this chapter by learning about motion analysis and background/foreground extraction.</p>
<p class="calibre2">In this chapter, we'll cover the following:</p>
<ul class="calibre10">
<li class="calibre11">How to apply filters and perform such operations on videos</li>
<li class="calibre11">Using the Mean Shift algorithm to detect and track objects</li>
<li class="calibre11">Using the CAM Shift algorithm to detect and track objects</li>
<li class="calibre11">Using the Kalman filter to improve tracking results and remove noise</li>
<li class="calibre11">Using the background and foreground extraction algorithms</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Technical requirements</h1>
                
            
            <article>
                
<ul class="calibre10">
<li class="calibre11">IDE to develop C++ or Python applications</li>
<li class="calibre11">OpenCV library</li>
</ul>
<p class="calibre2">Refer to <span class="calibre12"><a target="_blank" href="part0030.html#SJGS0-15c05657f8254d318ea883ef10fc67f4" class="calibre9">Chapter 2</a>, <em class="calibre7">Getting Started with OpenCV</em></span>, for more information about how to set up a personal computer and make it ready for developing computer vision applications using the OpenCV library.</p>
<p class="calibre2">You can use the following URL to download the source codes and examples for this chapter: <a href="https://github.com/PacktPublishing/Hands-On-Algorithms-for-Computer-Vision/tree/master/Chapter06" class="calibre9">https://github.com/PacktPublishing/Hands-On-Algorithms-for-Computer-Vision/tree/master/Chapter06</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Processing videos</h1>
                
            
            <article>
                
<p class="calibre2">To be able to use any of the algorithms that we've learned so far on videos, we need to be able to read video frames and store them in <kbd class="calibre13">Mat</kbd> objects. We have already learned about how to deal with video files, cameras, and RTSP feeds in the initial chapters of this book. So, extending that, using what we learned in the previous chapters, we can use a code similar to the following, in order to apply colormaps to the video feed from the default camera on a computer:</p>
<pre class="calibre15">VideoCapture cam(0); 
// check if camera was opened correctly 
if(!cam.isOpened()) 
    return -1; 
// infinite loop 
while(true) 
{ 
    Mat frame; 
    cam &gt;&gt; frame; 
    if(frame.empty()) 
        break; 
 
    applyColorMap(frame, frame, COLORMAP_JET); 
 
    // display the frame 
    imshow("Camera", frame); 
 
    // stop camera if space is pressed 
    if(waitKey(10) == ' ') 
        break; 
} 
 
cam.release(); </pre>
<p class="calibre2">Just as we learned in <span class="calibre12"><a target="_blank" href="part0030.html#SJGS0-15c05657f8254d318ea883ef10fc67f4" class="calibre9">Chapter 2</a>, <em class="calibre7">Getting Started with OpenCV</em></span>, we simply need to create a <kbd class="calibre13">VideoCapture</kbd> object and read the video frames from the default camera (which has an index of zero). In the preceding example, we have added a single line of code that is responsible for applying a colormap on the extracted video frames. Try the preceding example code and you'll see the <kbd class="calibre13">COLORMAP_JET</kbd> colormap applied to every frame of the camera, much like what we learned in <span class="calibre12"><a target="_blank" href="part0085.html#2H1VQ0-15c05657f8254d318ea883ef10fc67f4" class="calibre9">Chapter 4</a>, <em class="calibre7">Drawing, Filtering, and Transformation</em></span>, and finally the results are displayed in real-time. Pressing the spacebar should stop the video processing altogether.</p>
<p class="calibre2">Similarly, we can perform different video-processing algorithms in real-time, based on a specific key being pressed. Here's an example, replacing only the <kbd class="calibre13">for</kbd> loop in the preceding code, which results in the original video being displayed unless either the <em class="calibre7">J</em> or <em class="calibre7">H</em> key is pressed:</p>
<pre class="calibre15">int key = -1; 
while(key != ' ') 
{ 
    Mat frame; 
    cam &gt;&gt; frame; 
    if(frame.empty()) 
        break; 
    switch (key) 
    { 
    case 'j': applyColorMap(frame, frame, COLORMAP_JET); 
        break; 
    case 'h': applyColorMap(frame, frame, COLORMAP_HOT); 
        break; 
    } 
    imshow("Camera", frame); 
    int k = waitKey(10); 
    if(k &gt; 0) 
        key = k; 
} </pre>
<p class="calibre2">The original output video of the camera will be displayed unless any of the mentioned keys are pressed. Pressing <em class="calibre7">J</em> will trigger <kbd class="calibre13">COLORMAP_JET</kbd>, while pressing <em class="calibre7">H</em> will trigger the <kbd class="calibre13">COLORMAP_HOT</kbd> colormap being applied to the camera frames. Similar to the previous example, pressing the spacebar key will stop the process. Also, pressing any keys other than space, <em class="calibre7">J</em>, or <em class="calibre7">H</em> will result in the original video being displayed.</p>
<div class="packt_infobox">The <kbd class="calibre29">applyColorMap</kbd> function in the preceding examples is just a random algorithm that is used to describe the technique used to process videos in real-time. You can use any of the algorithms you have learned in this book that perform on a single image. You can, for instance, write a program that performs a smoothening filter on the video, or Fourier transformation, or even a program that displays the Hue channel histogram in real-time. The use cases are infinite, however the method used is almost identical for all algorithms that perform a single complete operation on any individual image.</div>
<p class="calibre2">Besides performing an operation on individual video frames, one can also perform operations that depend on any number of consecutive frames. Let's see how this is done using a very simple, but extremely important, use case.</p>
<p class="calibre2">Let's assume we want to find the average brightness of the last 60 frames read from a camera at any given moment. Such a value is quite useful when we want to automatically adjust the brightness of the video when the content of the frames is extremely dark or extremely bright. In fact, a similar operation is usually performed by the internal processor of most digital cameras, and even the smartphone in your pocket. You can give it a try by turning on the camera on your smartphone and pointing it toward a light, or the sun, or by entering a very dark environment. The following code demonstrates how the average of the brightness of the last <kbd class="calibre13">60</kbd> frames is calculated and displayed in the corner of the video:</p>
<pre class="calibre15">VideoCapture cam(0); 
if(!cam.isOpened()) 
    return -1; 
vector&lt;Scalar&gt; avgs; 
int key = -1; 
while(key != ' ') 
{ 
    Mat frame; 
    cam &gt;&gt; frame; 
    if(frame.empty()) 
        break; 
    if(avgs.size() &gt; 60) // remove the first item if more than 60 
        avgs.erase(avgs.begin()); 
    Mat frameGray; 
    cvtColor(frame, frameGray, CV_BGR2GRAY); 
    avgs.push_back( mean(frameGray) ); 
    Scalar allAvg = mean(avgs); 
    putText(frame, 
            to_string(allAvg[0]), 
            Point(0,frame.rows-1), 
            FONT_HERSHEY_PLAIN, 
            1.0, 
            Scalar(0,255,0)); 
    imshow("Camera", frame); 
    int k = waitKey(10); 
    if(k &gt; 0) 
        key = k; 
} 
cam.release(); </pre>
<p class="calibre2">For the most part, this example code is quite similar to the examples we saw earlier in this chapter. The main difference here is that we are storing the average of the last <kbd class="calibre13">60</kbd> frames that are calculated using the OpenCV mean function in a <kbd class="calibre13">vector</kbd> of <kbd class="calibre13">Scalar</kbd> objects, and then we calculate the average of all averages. The calculated value is then drawn on the input frame using the <kbd class="calibre13">putText</kbd> function. The following image depicts a single frame that is displayed when the preceding example code is executed:</p>
<div class="cdpaligncenter"><img src="../images/00069.jpeg" class="calibre82"/></div>
<p class="calibre2">Notice the value displayed in the bottom-left corner of the image, which will start to decrease when the content of the video frames becomes darker and increase when they become brighter. Based on this result, you can, for instance, change the brightness value or warn the user of your application that the content is too dark or bright, and so on.</p>
<p class="calibre2">The examples in this initial section of the chapter were meant to teach you the idea of processing individual frames using the algorithms you've learned in the previous chapters, and a few simple programming techniques used to calculate a value based on consecutive frames. In the following sections of this chapter, we'll be learning about some of the most important video-processing algorithms, namely object detection and tracking algorithms, which depend on the concepts and techniques we learned in this section and the previous chapters of this book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Understanding the Mean Shift algorithm</h1>
                
            
            <article>
                
<p class="calibre2">The Mean Shift algorithm is an iterative algorithm that can be used to find the maxima of a density function. A very rough translation of the preceding sentence to computer vision terminology would be the following—the Mean Shift algorithm can be used to find an object in an image using a back-projection image. But how is it achieved in practice? Let's walk through this step by step. Here are the individual operations that are performed to find an object using the Mean Shift algorithm, in order:</p>
<ol class="calibre14">
<li value="1" class="calibre11">The back-projection of an image is created using a modified histogram to find the pixels that are most likely to contain our object of interest. (It is also common to filter the back-projection image to get rid of unwanted noise, but this is an optional operation to improve the results.)</li>
<li value="2" class="calibre11">An initial search window is needed. This search window will contain our object of interest after a number of iterations, which we'll get to in the next step. After each iteration, the search window is updated by the algorithm. The updating of the search window happens by calculating the mass center of the search window in the back-projection image, and then shifting the current center point of the search window to the mass center of the window. The following picture demonstrates the concept of the mass center in a search window and how the shifting happens:</li>
</ol>
<div class="cdpaligncenter"><img src="../images/00070.gif" class="calibre20"/></div>
<p class="calibre31">The two points at the two ends of the arrow in the preceding picture correspond to the search-window center and mass center.</p>
<ol start="3" class="calibre14">
<li value="3" class="calibre11">Just like any iterative algorithm, some termination criteria are required by the Mean Shift algorithm to stop the algorithm when the results are as expected or when reaching an accepted result does not happen as fast as needed. So, the number of iterations and an epsilon value are used as termination criteria. Either by reaching the number of iterations in the algorithm or by finding a shift distance that is smaller than the given epsilon value (convergence), the algorithm will stop.</li>
</ol>
<p class="calibre2">Now, let's see a hands-on example of how this algorithm is used in practice by using the OpenCV library. The <kbd class="calibre13">meanShift</kbd> function in OpenCV implements the Mean Shift algorithm almost exactly as it was described in the preceding steps. This function requires a back-projection image, a search window, and the termination criteria, and it is used as seen in the following example:</p>
<pre class="calibre15">Rect srchWnd(0, 0, 100, 100); 
TermCriteria criteria(TermCriteria::MAX_ITER 
                      + TermCriteria::EPS, 
                      20, // number of iterations 
                      1.0 // epsilon value 
                      ); 
// Calculate back-projection image 
meanShift(backProject, 
          srchWnd, 
          criteria);</pre>
<p class="calibre2"><kbd class="calibre13">srchWnd</kbd> is a <kbd class="calibre13">Rect</kbd> object, which is simply a rectangle that must contain an initial value that is used and then updated by the <kbd class="calibre13">meanShift</kbd> function. <kbd class="calibre13">backProjection</kbd> must contain a proper back-projection image that is calculated with any of the methods that we learned in <span class="calibre12"><a target="_blank" href="part0102.html#318PC0-15c05657f8254d318ea883ef10fc67f4" class="calibre9">Chapter 5</a>, <em class="calibre7">Back-Projection and Histograms</em></span>. The <kbd class="calibre13">TermCriteria</kbd> class is an OpenCV class that is used by iterative algorithms that require similar termination criteria. The first parameter defines the type of the termination criteria, which can be <kbd class="calibre13">MAX_ITER</kbd> (same as <kbd class="calibre13">COUNT</kbd>), <kbd class="calibre13">EPS</kbd>, or both. In the preceding example, we have used the termination criteria of <kbd class="calibre13">20</kbd> iterations and an epsilon value of <kbd class="calibre13">1.0</kbd>, which of course can be changed depending on the environment and application. The most important thing to note here is that a higher number of iterations and a lower epsilon can yield more accurate results, but it can also lead to slower performance, and vice versa.</p>
<p class="calibre2">The preceding example is just a demonstration of how the <kbd class="calibre13">meanShift</kbd> function is called. Now, let's walk through a complete hands-on example to learn our first real-time object-tracking algorithm:</p>
<ol class="calibre14">
<li value="1" class="calibre11">The structure of the tracking example we'll create is quite similar to the previous examples in this chapter. We need to open a video, or a camera, on the computer using the <kbd class="calibre13">VideoCapture</kbd> class and then start reading the frames, as seen here:</li>
</ol>
<pre class="calibre30">VideoCapture cam(0); 
if(!cam.isOpened()) 
    return -1; 
 
int key = -1; 
while(key != ' ') 
{ 
    Mat frame; 
    cam &gt;&gt; frame; 
    if(frame.empty()) 
        break; 
 
    int k = waitKey(10); 
    if(k &gt; 0) 
        key = k; 
} 
 
cam.release(); </pre>
<p class="calibre31">Again, we have used the <kbd class="calibre13">waitKey</kbd> function to stop the loop if the spacebar key is pressed.</p>
<ol start="2" class="calibre14">
<li value="2" class="calibre11">We're going to assume that our object of interest has a green color. So, we're going to form a hue histogram that contains only the green colors, as seen here:</li>
</ol>
<pre class="calibre30">int bins = 360; 
int grnHue = 120; // green color hue value 
int hueOffset = 50; // the accepted threshold 
Mat histogram(bins, 1, CV_32FC1); 
for(int i=0; i&lt;bins; i++) 
{ 
    histogram.at&lt;float&gt;(i, 0) = 
            (i &gt; grnHue - hueOffset) 
            &amp;&amp; 
            (i &lt; grnHue + hueOffset) 
            ? 
                255.0 : 0.0; 
} </pre>
<p class="calibre31">This needs to happen before entering the process loop, since our histogram is going to stay constant throughout the whole process.</p>
<ol start="3" class="calibre14">
<li value="3" class="calibre11">One last thing to take care of before entering the actual process loop and the tracking code is the termination criteria, which will stay constant throughout the whole process. Here's how we'll create the required termination criteria:</li>
</ol>
<pre class="calibre30">Rect srchWnd(0,0, 100, 100); 
TermCriteria criteria(TermCriteria::MAX_ITER 
                      + TermCriteria::EPS, 
                      20, 
                      1.0); </pre>
<div class="packt_infobox">The initial value of the search window is quite important when using the Mean Shift algorithm to track objects, since this algorithm always makes an assumption about the initial position of the object to be tracked. This is an obvious downside of the Mean Shift algorithm, which we'll learn how to deal with later on in this chapter when we discuss the CAM Shift algorithm and its implementation in the OpenCV library.</div>
<ol start="4" class="calibre14">
<li value="4" class="calibre11">After each frame is read in the <kbd class="calibre13">while</kbd> loop we're using for the tracking code, we must calculate the back-projection image of the input frame using the green hue histogram that we created. Here's how it's done:</li>
</ol>
<pre class="calibre30">Mat frmHsv, hue; 
vector&lt;Mat&gt; hsvChannels; 
cvtColor(frame, frmHsv, COLOR_BGR2HSV); 
split(frmHsv, hsvChannels); 
hue = hsvChannels[0]; 
 
int nimages = 1; 
int channels[] = {0}; 
Mat backProject; 
float rangeHue[] = {0, 180}; 
const float* ranges[] = {rangeHue}; 
double scale = 1.0; 
bool uniform = true; 
calcBackProject(&amp;hue, 
                nimages, 
                channels, 
                histogram, 
                backProject, 
                ranges, 
                scale, 
                uniform); </pre>
<p class="calibre31">You can refer to <span class="calibre12"><a target="_blank" href="part0102.html#318PC0-15c05657f8254d318ea883ef10fc67f4" class="calibre9">Chapter 5</a>, <em class="calibre7">Back-Projection and Histograms</em>,</span> for more detailed instructions about calculating the back-projection image.</p>
<ol start="5" class="calibre14">
<li value="5" class="calibre11">Call the <kbd class="calibre13">meanShift</kbd> function to update the search window using the back-projection image and provided termination criteria, as seen here:</li>
</ol>
<pre class="calibre30">meanShift(backProject, 
          srchWnd, 
          criteria); </pre>
<ol start="6" class="calibre14">
<li value="6" class="calibre11">To visualize the search window, or in other words the tracked object, we need to draw the search-window rectangle on the input frame. Here's how you can do this by using the rectangle function:</li>
</ol>
<pre class="calibre30">rectangle(frame, 
          srchWnd, // search window rectangle 
          Scalar(0,0,255), // red color 
          2 // thickness 
          );</pre>
<p class="calibre31">We can do the same on back-projection image result, however, first we need to convert the back-projection image to BGR color space. Remember that the result of the back-projection image contained a single channel image with the same depth as the input image. Here's how we can draw a red rectangle at the search-window position on the back-projection image:</p>
<pre class="calibre30">cvtColor(backProject, backProject, COLOR_GRAY2BGR); 
rectangle(backProject, 
          srchWnd, 
          Scalar(0,0,255), 
          2); </pre>
<ol start="7" class="calibre14">
<li value="7" class="calibre11">Add the means to switch between the back-projection and original video frame using the <em class="calibre26">B</em> and <em class="calibre26">V</em> keys. Here's how it's done:</li>
</ol>
<pre class="calibre30">switch(key) 
{ 
case 'b': imshow("Camera", backProject); 
    break; 
case 'v': default: imshow("Camera", frame); 
    break; 
} </pre>
<p class="calibre2">Let's give our program a try and see how it performs when executed in a slightly controlled environment. The following picture demonstrates the initial position of the search window and our green object of interest, both in the original frame view and the back-projection view:</p>
<div class="cdpaligncenter"><img src="../images/00071.jpeg" class="calibre83"/></div>
<p class="calibre2">Moving the object<span class="calibre12"> </span><span class="calibre12">around</span><span class="calibre12"> will cause the</span> <kbd class="calibre13">meanShift</kbd> <span class="calibre12">function to update the search window and consequently track the object. Here's another result, depicting the object tracked to the bottom-right corner of the view:</span></p>
<div class="cdpaligncenter"><img src="../images/00072.jpeg" class="calibre84"/></div>
<p class="calibre2">Notice the small amount of noise that can be seen in the corner, which would be taken care of by the <kbd class="calibre13">meanShift</kbd> function since the mass center is not affected too much by it. However, as mentioned previously, it is a good idea to perform some sort of filtering on the back-projection image to get rid of noise. For instance, and in case of noise similar to what we have in the back-projection image, we can use the <kbd class="calibre13">GaussianBlur</kbd> function, or even better, the <kbd class="calibre13">erode</kbd> function, to get rid of unwanted pixels in the back-projection image. For more information on how to use filtering functions, you can refer to <span class="calibre12"><a target="_blank" href="part0085.html#2H1VQ0-15c05657f8254d318ea883ef10fc67f4" class="calibre9">Chapter 4</a>, <em class="calibre7">Drawing, Filtering, and Transformation</em></span>.</p>
<p class="calibre2">In such tracking applications, we usually need to observe, record, or in any way process the route that the object of interest has taken before any given moment and for a desired period of time. This can be simply achieved by using the center point of the search window, as seen in the following example:</p>
<pre class="calibre15">Point p(srchWnd.x + srchWnd.width/2, 
        srchWnd.y + srchWnd.height/2); 
route.push_back(p); 
if(route.size() &gt; 60) // last 60 frames 
    route.erase(route.begin()); // remove first element</pre>
<p class="calibre2"> </p>
<p class="calibre2">Obviously, the <kbd class="calibre13">route</kbd> is a <kbd class="calibre13">vector</kbd> of <kbd class="calibre13">Point</kbd> objects. <kbd class="calibre13">route</kbd> needs to be updated after the <kbd class="calibre13">meanShift</kbd> function call, and then we can use the following call to the <kbd class="calibre13">polylines</kbd> function in order to draw the <kbd class="calibre13">route</kbd> over the original video frame:</p>
<pre class="calibre15">polylines(frame, 
          route, // the vector of Point objects 
          false, // not a closed polyline 
          Scalar(0,255,0), // green color 
          2 // thickness 
          ); </pre>
<p class="calibre2">The following picture depicts the result of displaying the tracking route (for the last 60 frames) on the original video frames read from the camera:</p>
<div class="cdpaligncenter"><img src="../images/00073.jpeg" class="calibre85"/></div>
<p class="calibre2">Now, let's address some issues that we observed while working with the <kbd class="calibre13">meanShift</kbd> function. First of all, it is not convenient to create the hue histogram manually. A flexible program should allow the user to choose the object they want to track, or at least allow the user to choose the color of the object of interest conveniently. The same can be said about the search window size and its initial position. There are a number of ways to deal with such issues and we're going to address them with a hands-on example.</p>
<p class="calibre2">When using the OpenCV library, you can use the <kbd class="calibre13">setMouseCallback</kbd> function to customize the behavior of mouse clicks on an output window. This can be used in combination with a few simple methods, such as <kbd class="calibre13">bitwise_not</kbd> to mimic an easy-to-use object selection for the users. <kbd class="calibre13">setMouseCallback</kbd>, as it can be guessed from its name, sets a callback function to handle the mouse clicks on a given window.</p>
<p class="calibre2">The following callback function in conjunction with the variables defined here can be used to create a convenient object selector:</p>
<pre class="calibre15">bool selecting = false; 
Rect selection; 
Point spo; // selection point origin 
 
void onMouse(int event, int x, int y, int flags, void*) 
{ 
    switch(event) 
    { 
    case EVENT_LBUTTONDOWN: 
    { 
        spo.x = x; 
        spo.y = y; 
        selection.x = spo.x; 
        selection.y = spo.y; 
        selection.width = 0; 
        selection.height = 0; 
        selecting = true; 
 
    } break; 
    case EVENT_LBUTTONUP: 
    { 
        selecting = false; 
    } break; 
    default: 
    { 
        selection.x = min(x, spo.x); 
        selection.y = min(y, spo.y); 
        selection.width = abs(x - spo.x); 
        selection.height = abs(y - spo.y); 
    } break; 
    } 
} </pre>
<p class="calibre2">The <kbd class="calibre13">event</kbd> contains an entry from the <kbd class="calibre13">MouseEventTypes</kbd> enum, which describes whether a mouse button was pressed or released. Based on such a simple event, we can decide when the user is actually selecting an object that's visible on the screen. This is depicted as follows:</p>
<pre class="calibre15">if(selecting) 
{ 
    Mat sel(frame, selection); 
    bitwise_not(sel, sel); // invert the selected area 
 
    srchWnd = selection; // set the search window 
 
    // create the histogram using the hue of the selection 
} </pre>
<p class="calibre2">This allows a huge amount of flexibility for our applications, and the code is bound to work with objects of any color. Make sure to check out the example codes for this chapter from the online Git repository for a complete example project that uses all the topics we've learned so far in this chapter.</p>
<p class="calibre2">Another method of selecting an object or a region on an image is by using the <kbd class="calibre13">selectROI</kbd> and <kbd class="calibre13">selectROIs</kbd> functions in the OpenCV library. These functions allow the user to select a rectangle (or rectangles) on an image using simple mouse clicks and drags. Note that the <kbd class="calibre13">selectROI</kbd> and <kbd class="calibre13">selectROIs</kbd> functions are easier to use than handling mouse clicks using callback functions, however they do not offer the same amount of power, flexibility, and customization.</p>
<p class="calibre2">Before moving on to the next section, let's recall that <kbd class="calibre13">meanShift</kbd> does not handle an increase or decrease in the size of the object that is being tracked, nor does it take care of the orientation of the object. These are probably the main issues that have led to the development of a more sophisticated version of the Mean Shift algorithm, which is the next topic we're going to learn about in this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Using the Continuously Adaptive Mean (CAM) Shift</h1>
                
            
            <article>
                
<p class="calibre2">To overcome the limitations of the Mean Shift algorithm, we can use an improved version of it, which is called the <strong class="calibre4">Continuously Adaptive Mean</strong> Shift, or simply the <strong class="calibre4">CAM</strong> Shift algorithm. OpenCV contains the implementation for the CAM Shift algorithm in a function named <kbd class="calibre13">CamShift</kbd>, which is used almost in an identical manner to the <kbd class="calibre13">meanShift</kbd> function. The input parameters of the <kbd class="calibre13">CamShift</kbd> function are the same as <kbd class="calibre13">meanShift</kbd>, since it also uses a back-projection image to update a search window using a given set of termination criteria. In addition, <kbd class="calibre13">CamShift</kbd> also returns a <kbd class="calibre13">RotatedRect</kbd> object, which contains both the search window and its angle.</p>
<p class="calibre2">Without using the returned <kbd class="calibre13">RotatedRect</kbd> object, you can simply replace any call to the <kbd class="calibre13">meanShift</kbd> function with <kbd class="calibre13">CamShift</kbd>, and the only difference would be that the results will be scale-invariant, meaning the search window will become bigger if the object is nearer (or bigger) and vice versa. For instance, we can replace the call to the <kbd class="calibre13">meanShift</kbd> function in the preceding example code for the Mean Shift algorithm with the following:</p>
<pre class="calibre15">CamShift(backProject, 
         srchWnd, 
         criteria); </pre>
<p class="calibre2">The following images depict the result of replacing the <kbd class="calibre13">meanShift</kbd> function with <kbd class="calibre13">CamShift</kbd> in the example from the previous section:</p>
<div class="cdpaligncenter"><img src="../images/00074.jpeg" class="calibre86"/></div>
<p class="calibre2">Notice that the results are now scale-invariant, even though we didn't change anything except replace the mentioned function. As the object moves farther away from the camera, or becomes smaller, the same Mean Shift algorithm is used to calculate its position, however, this time the search window is resized to fit the exact size of the object, and the rotation is calculated, which we didn't use. To be able to use the rotation value of the object, we need to store the result of the <kbd class="calibre13">CamShift</kbd> function in a <kbd class="calibre13">RotatedRect</kbd> object first, as seen in the following example:</p>
<pre class="calibre15">RotatedRect rotRect = CamShift(backProject, 
                               srchWnd, 
                               criteria); </pre>
<p class="calibre2">To draw a <kbd class="calibre13">RotatedRect</kbd> object, or in other words a rotated rectangle, you must use the <kbd class="calibre13">points</kbd> method of <kbd class="calibre13">RotatedRect</kbd> to extract the consisting <kbd class="calibre13">4</kbd> points of the rotated rectangle first, and then draw them all using the line function, as seen in the following example:</p>
<pre class="calibre15">Point2f rps[4]; 
rotRect.points(rps); 
for(int i=0; i&lt;4; i++) 
    line(frame, 
         rps[i], 
         rps[(i+1)%4], 
         Scalar(255,0,0),// blue color 
         2); </pre>
<p class="calibre2">You can also use a <kbd class="calibre13">RotatedRect</kbd> object to draw a rotated ellipse that is covered by the rotated rectangle. Here's how:</p>
<pre class="calibre15">ellipse(frame, 
        rotRect, 
        Scalar(255,0,0), 
        2); </pre>
<p class="calibre2">The following image displays the result of using the <kbd class="calibre13">RotatedRect</kbd> object to draw a rotated rectangle and ellipse at the same time, over the tracked object:</p>
<div class="cdpaligncenter"><img src="../images/00075.jpeg" class="calibre87"/></div>
<p class="calibre2">In the preceding image, the red rectangle is the search window, the blue rectangle is the resulting rotated rectangle, and the green ellipse is drawn by using the resulting rotated rectangle.</p>
<p class="calibre2">To summarize, we can say that <kbd class="calibre13">CamShift</kbd> is far better suited to dealing with objects of varying size and rotation than <kbd class="calibre13">meanShift</kbd>, however, there are still a couple of possible enhancements that can be done when using the <kbd class="calibre13">CamShift</kbd> algorithm. First things first, the initial window size still needs to be set, but since <kbd class="calibre13">CamShift</kbd> is taking care of the size changes, then we can simply set the initial window size to be the same as the whole image size. This would help us avoid having to deal with the initial position and size of the search window. If we can also create the histogram of the object of interest using a previously saved file on disk or any similar method, then we will have an object detector and tracker that works out of the box, at least for all the cases where our object of interest has a visibly different color than the environment.</p>
<p class="calibre2">Another huge improvement to such a color-based detection and tracking algorithm can be achieved by using the <kbd class="calibre13">inRange</kbd> function to enforce a threshold on the S and V channels of the HSV image that we are using to calculate the histogram. The reason is that in our example, we simply used the <strong class="calibre4">hue</strong> (or the <strong class="calibre4">H</strong>, or the first) channel, and we didn't take into account the high possibility of having extremely dark or bright pixels that might have the same hue as our object of interest. This can be done by using the following code when calculating the histogram of the object to be tracked:</p>
<pre class="calibre15">int lbHue = 00 , hbHue = 180; 
int lbSat = 30 , hbSat = 256; 
int lbVal = 30 , hbVal = 230; 
 
Mat mask; 
inRange(objImgHsv, 
        Scalar(lbHue, lbSat, lbVal), 
        Scalar(hbHue, hbSat, hbVal), 
        mask); 
 
calcHist(&amp;objImgHue, 
         nimages, 
         channels, 
         mask, 
         histogram, 
         dims, 
         histSize, 
         ranges, 
         uniform); </pre>
<p class="calibre2">In the preceding example code, the variables starting with <kbd class="calibre13">lb</kbd> and <kbd class="calibre13">hb</kbd> refer to the lower bound and higher bound of the values that are allowed to pass the <kbd class="calibre13">inRange</kbd> function. <kbd class="calibre13">objImgHsv</kbd> is obviously a <kbd class="calibre13">Mat</kbd> object containing our object of interest, or a ROI that contains our object of interest. <kbd class="calibre13">objImgHue</kbd> is the first channel of <kbd class="calibre13">objImgHsv</kbd>, which is extracted using a previous call to the <kbd class="calibre13">split</kbd> function. The rest of the parameters are nothing new, and you've already used them in previous calls to the functions used in this example.</p>
<p class="calibre2">Combining all of the algorithms and techniques described in this section can help you create an object-detector, or even a face-detector and tracker that can work in realtime and with stunning speed. However, you might still need to account for the noise that will interfere, especially with the tracking, which is almost inevitable because of the nature of color-based or histogram-based trackers. One of the most widely used solutions to these issues is the subject of the next section in this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Using the Kalman filter for tracking and noise reduction</h1>
                
            
            <article>
                
<p class="calibre2">The Kalman filter is a popular algorithm that is used for reducing the noise of a signal, such as the result of the tracking algorithm that we used in the preceding section. To be precise, the Kalman filter is an estimation algorithm that is used to predict the next state of a signal based on previous observations. Digging deep into the definition and details of the Kalman filter would require a chapter of its own, but we'll try to walk through this simple, yet extremely powerful, algorithm with a couple of hands-on examples to learn how it is used in practice.</p>
<p class="calibre2">For the first example, we're going to write a program that tracks the mouse cursor while it is moved on a canvas, or the OpenCV window. The Kalman filter is implemented using the <kbd class="calibre13">KalmanFilter</kbd> class in OpenCV and it includes all (and many more) of the Kalman filter implementation details, which we'll discuss in this section.</p>
<p class="calibre2">First of all, <kbd class="calibre13">KalmanFilter</kbd> must be initialized with a number of dynamic parameters, measurement parameters, and control parameters, in addition to the type of the underlying data used in the Kalman filter itself. We're going to ignore control parameters, since they are outside the scope of our examples, so we'll set them simply to zero. As for the data type, we'll go for the default 32-bit float, or <kbd class="calibre13">CV_32F</kbd> in terms of OpenCV types. Dynamic parameters in a 2D movement, which is the case with our example, correspond to the following:</p>
<ul class="calibre10">
<li class="calibre11"><em class="calibre26">X</em>, or position in <em class="calibre26">x</em> direction</li>
<li class="calibre11"><em class="calibre26">Y</em>, or position in <em class="calibre26">y</em> direction</li>
<li class="calibre11"><em class="calibre26">X</em>', or velocity in <em class="calibre26">x</em> direction</li>
<li class="calibre11"><em class="calibre26">Y</em>', or velocity in <em class="calibre26">y</em> direction</li>
</ul>
<div class="packt_infobox">A higher dimensionality of the parameters can also be used, which would then cause the preceding list to be followed by <em class="calibre50">X</em>'' (acceleration in <em class="calibre50">x</em> direction) and so on.</div>
<p class="calibre2">As for the measurement parameters, we'll simply have <em class="calibre7">X</em> and <em class="calibre7">Y</em>, which correspond to the mouse position in our first example. Keeping in mind what was said about dynamic and measurement parameters, here's how we can initialize a <kbd class="calibre13">KalmanFilter</kbd> class instance (object) that fits for tracking a point on a 2D space:</p>
<pre class="calibre15">KalmanFilter kalman(4, // dynamic parameters: X,Y,X',Y' 
                    2  // measurement parameters: X,Y 
                    ); </pre>
<p class="calibre2">Note that in this example, control parameters and type parameters are simply ignored and set to their default values, otherwise we could have written the same code as seen here:</p>
<pre class="calibre15">KalmanFilter kalman(4, 2, 0, CV_32F); </pre>
<p class="calibre2">The <kbd class="calibre13">KalmanFilter</kbd> class requires a transition matrix to be set before it is correctly usable. This transition matrix is used for calculating (and updating) the estimated, or next, state of the parameters. We'll be using the following transition matrix in our example for tracking the mouse position:</p>
<pre class="calibre15">Mat_&lt;float&gt; tm(4, 4); // transition matrix 
tm &lt;&lt; 1,0,1,0, // next x = 1X + 0Y + 1X' + 0Y' 
      0,1,0,1, // next y = 0X + 1Y + 0X' + 1Y' 
      0,0,1,0, // next x'= 0X + 0Y + 1X' + 0Y 
      0,0,0,1; // next y'= 0X + 0Y + 0X' + 1Y' 
kalman.transitionMatrix =  tm; </pre>
<p class="calibre2">After completing the steps required for this example, it would be wise to return here and update the values in the transition matrix and observe the behavior of the Kalman filter. For instance, try to update the matrix row that corresponds to the estimated <em class="calibre7">Y</em> (marked as <kbd class="calibre13">next y</kbd> in the comments) and you'll notice that the tracked position <em class="calibre7">Y</em> value is affected by it. Try experimenting with all of the values in the transition matrix for a better understanding of its effect.</p>
<p class="calibre2">Besides the transition matrix, we also need to take care of the initialization of the dynamic parameters' state and measurements, which are the initial mouse positions in our example. Here's how we initialize the mentioned values:</p>
<pre class="calibre15">Mat_&lt;float&gt; pos(2,1); 
pos.at&lt;float&gt;(0) = 0; 
pos.at&lt;float&gt;(1) = 0; 
 
kalman.statePre.at&lt;float&gt;(0) = 0; // init x 
kalman.statePre.at&lt;float&gt;(1) = 0; // init y 
kalman.statePre.at&lt;float&gt;(2) = 0; // init x' 
kalman.statePre.at&lt;float&gt;(3) = 0; // init y'</pre>
<p class="calibre2">As you'll see later on, the <kbd class="calibre13">KalmanFilter</kbd> class requires a vector instead of a <kbd class="calibre13">Point</kbd> object, since it is designed to work with higher dimensionalities too. For this reason, we'll update the <kbd class="calibre13">pos</kbd> vector in the preceding code snippet with the last mouse positions before performing any calculations. Other than the initializations that we just mentioned, we also need to initialize the measurement matrix of the Kalman filter. This is done as seen here:</p>
<pre class="calibre15">setIdentity(kalman.measurementMatrix); </pre>
<p class="calibre2">The <kbd class="calibre13">setIdentity</kbd> function in OpenCV is simply used to initialize matrices with a scaled identity matrix. If only a single matrix is provided as a parameter to the <kbd class="calibre13">setIdentity</kbd> function, it will set to the identity matrix, however if a second <kbd class="calibre13">Scalar</kbd> is provided in addition, then all elements of the identity matrix will be multiplied (or scaled) using the given <kbd class="calibre13">Scalar</kbd> value.</p>
<p class="calibre2">One last initialization is the process noise covariance. We'll use a very small value for this, which causes a tracking with a natural-movement feeling, although with a little bit of overshoot when tracking. Here's how we initialize the process-noise covariance matrix:</p>
<pre class="calibre15">setIdentity(kalman.processNoiseCov, 
            Scalar::all(0.000001)); </pre>
<p class="calibre2">It is also common to initialize the following matrices before using the <kbd class="calibre13">KalmanFilter</kbd> class:</p>
<ul class="calibre10">
<li class="calibre11"><kbd class="calibre13">controlMatrix</kbd> (not used if the control parameter count is zero)</li>
<li class="calibre11"><kbd class="calibre13">errorCovPost</kbd></li>
<li class="calibre11"><kbd class="calibre13">errorCovPre</kbd></li>
<li class="calibre11"><kbd class="calibre13">gain</kbd></li>
<li class="calibre11"><kbd class="calibre13">measurementNoiseCov</kbd></li>
</ul>
<p class="calibre2">Using all of the matrices mentioned in the preceding list will provide a huge amount of customization to the <kbd class="calibre13">KalmanFilter</kbd> class, but it also requires a great amount of knowledge about the type of noise-filtering and tracking that is needed and the environment that the filter will be implemented in. These matrices and their usage have deep roots in control theory, and control science in general, which is a topic for another book. Note that in our example, we'll simply use the default values of the mentioned matrices, thus we have ignored them altogether.</p>
<p class="calibre2">The next thing we need in our tracking example using the Kalman filter is to set up a window on which we can track mouse movements. We are assuming that the position of the mouse on the window is the position of a detected and tracked object and we'll use our <kbd class="calibre13">KalmanFilter</kbd> object to predict and de-noise these detections, or in Kalman-filter-algorithm terminology, we are going to correct these measurements. We can use the <kbd class="calibre13">namedWindow</kbd> function to create a window using OpenCV. Consequently, the <kbd class="calibre13">setMouseCallback</kbd> function can be used to assign a callback function for mouse interactions with that specific window. Here's how we can do it:</p>
<pre class="calibre15">string window = "Canvas"; 
namedWindow(window); 
setMouseCallback(window, onMouse); </pre>
<p class="calibre2">We've used the word <kbd class="calibre13">Canvas</kbd> for the window, but obviously you can use any other name you like. <kbd class="calibre13">onMouse</kbd> is the callback function that will be assigned to react on mouse interactions with this window. It is defined like this:</p>
<pre class="calibre15">void onMouse(int, int x, int y, int, void*) 
{ 
    objectPos.x = x; 
    objectPos.y = y; 
} </pre>
<p class="calibre2"><kbd class="calibre13">objectPos</kbd> in the preceding code snippet is the <kbd class="calibre13">Point</kbd> object that is used to store the last position of the mouse on the window. It needs to be defined globally in order to be accessible by both <kbd class="calibre13">onMouse</kbd> and the main function in which we'll use the <kbd class="calibre13">KalmanFilter</kbd> class. Here's the definition:</p>
<pre class="calibre15">Point objectPos; </pre>
<p class="calibre2">Now, for the actual tracking, or to use the more correct terminology, correcting the measurements that contain noise, we need to use the following code, which is followed by the required explanations:</p>
<pre class="calibre15">vector&lt;Point&gt; trackRoute; 
while(waitKey(10) &lt; 0) 
{ 
    // empty canvas 
    Mat canvas(500, 1000, CV_8UC3, Scalar(255, 255, 255)); 
 
    pos(0) = objectPos.x; 
    pos(1) = objectPos.y; 
 
    Mat estimation = kalman.correct(pos); 
 
    Point estPt(estimation.at&lt;float&gt;(0), 
                estimation.at&lt;float&gt;(1)); 
 
    trackRoute.push_back(estPt); 
    if(trackRoute.size() &gt; 100) 
        trackRoute.erase(trackRoute.begin()); 
 
    polylines(canvas, 
              trackRoute, 
              false, 
              Scalar(0,0,255), 
              5); 
 
    imshow(window, canvas); 
 
    kalman.predict(); 
} </pre>
<p class="calibre2">In the preceding code, we are using the <kbd class="calibre13">trackRoute</kbd> vector to record the estimations over the last <kbd class="calibre13">100</kbd> frames. Pressing any key will cause the <kbd class="calibre13">while</kbd> loop, and consequently the program, to return. Inside the loop, and where we actually use the <kbd class="calibre13">KalmanFilter</kbd> class, we simply perform the following operations in order:</p>
<ol class="calibre14">
<li value="1" class="calibre11">Create an empty <kbd class="calibre13">Mat</kbd> object to be used as a canvas to draw, and for the content of the window on which the tracking will happen</li>
<li value="2" class="calibre11">Read the <kbd class="calibre13">objectPos</kbd>, which contains the last position of the mouse on the window and store it in the <kbd class="calibre13">pos</kbd> vector, which is usable with the <kbd class="calibre13">KalmanFilter</kbd> class</li>
<li value="3" class="calibre11">Read an estimation using the correct method of the <kbd class="calibre13">KalmanFilter</kbd> class</li>
<li value="4" class="calibre11">Convert the result of the estimation back to a <kbd class="calibre13">Point</kbd> object that can be used for drawing</li>
<li value="5" class="calibre11">Store the estimated point (or the tracked point) in the <kbd class="calibre13">trackRoute</kbd> vector, and make sure the number of items in the <kbd class="calibre13">trackRoute</kbd> vector doesn't exceed <kbd class="calibre13">100</kbd>, since that is the number of frames for which we want to keep a record of estimated points</li>
<li value="6" class="calibre11">Use the polylines function to draw the route, stored as <kbd class="calibre13">Point</kbd> objects in <kbd class="calibre13">trackRoute</kbd></li>
<li value="7" class="calibre11">Display the results using the <kbd class="calibre13">imshow</kbd> function</li>
<li value="8" class="calibre11">Update the internal matrices of the <kbd class="calibre13">KalmanFilter</kbd> class using the predict function</li>
</ol>
<p class="calibre2">Try executing the tracking program and move your mouse cursor around the window that is shown. You'll notice a smooth tracking result, which is drawn using the thick red line that's visible in the following screenshot:</p>
<div class="cdpaligncenter"><img src="../images/00076.gif" class="calibre88"/></div>
<p class="calibre2">Note that the position of the mouse cursor is ahead of the tracking, and the noise in mouse movement is almost completely removed. It is a good idea to try to visualize the mouse movement for a better comparison between the <kbd class="calibre13">KalmanFilter</kbd> results and actual measurements. Simply add the following code to the loop after the point where <kbd class="calibre13">trackRoute</kbd> was drawn in the preceding code:</p>
<pre class="calibre15">mouseRoute.push_back(objectPos); 
if(mouseRoute.size() &gt; 100) 
    mouseRoute.erase(mouseRoute.begin()); 
polylines(canvas, 
          mouseRoute, 
          false, 
          Scalar(0,0,0), 
          2); </pre>
<p class="calibre2">And obviously, you need to define the <kbd class="calibre13">mouseRoute</kbd> vector before entering the <kbd class="calibre13">while</kbd> loop, as seen here:</p>
<pre class="calibre15">vector&lt;Point&gt; mouseRoute; </pre>
<p class="calibre2">Let's try the same application with this minor update and see how the results compare to each other. Here's another screenshot depicting the results of actual mouse movements and corrected movements (or tracked, or filtered, depending on the terminology) drawn in the same window:</p>
<div class="cdpaligncenter"><img src="../images/00077.jpeg" class="calibre89"/></div>
<p class="calibre2">In the preceding result, the arrow is simply used to depict the overall direction of an extremely noisy measurement (mouse movement or detected object position, depending on the application), which is drawn using a thin black color, and the corrected results using the Kalman filter algorithm, which are drawn with a thick red line in the image. Try moving you mouse around and comparing the results visually. Remember what we mentioned about the <kbd class="calibre13">KalmanFilter</kbd> internal matrices and how you need to set their values according to the use case and application? For instance, a bigger process-noise covariance would have resulted in less de-noising and consequently less filtering. Let's try setting the process noise covariance value to <kbd class="calibre13">0.001</kbd>, instead of the previous 0.000001 value, try the same program once again, and compare the results. Here's how you can set the process-noise covariance:</p>
<pre class="calibre15">setIdentity(kalman.processNoiseCov, 
            Scalar::all(0.001)); </pre>
<p class="calibre2">Now, try running the program once again and you can easily notice that less de-noising is happening as you move the mouse cursor around the window:</p>
<div class="cdpaligncenter"><img src="../images/00078.gif" class="calibre90"/></div>
<p class="calibre2">By now, you can probably guess that the result of setting an extremely high value for process-noise covariance would be almost the same as using no filter at all. This is the reason why setting correct values for the Kalman filter algorithm is extremely important and also the reason why it depends so much on the application. However, there are methods for setting most of those parameters programmatically, and even dynamically while the tracking is being done to achieve the best results. For instance, using a function that is able to determine the possible amount of noise at any given moment, we can dynamically set the process noise covariance to high or low values for less and more de-noising of the measurements.</p>
<p class="calibre2">Now, let's use <kbd class="calibre13">KalmanFilter</kbd> to perform a real-life tracking correction on objects using the <kbd class="calibre13">CamShift</kbd> function, instead of mouse movements. It's important to note that the applied logic is exactly the same. We need to initialize a <kbd class="calibre13">KalmanFilter</kbd> object and set its parameters according to the amount of noise and so on. For simplicity, you can start off with the exact same set of parameters that we set for tracking the mouse cursor in the previous example, and then try to adjust them. We need to start by creating the same tracking program that we wrote in the previous sections. However, right after calling <kbd class="calibre13">CamShift</kbd> (or <kbd class="calibre13">meanShift</kbd> function) to update the search window, instead of displaying the results, we'll perform a correction using the <kbd class="calibre13">KalmanFilter</kbd> class to de-noise the results. Here's how it is done with a similar example code:</p>
<pre class="calibre15">CamShift(backProject, 
         srchWnd, 
         criteria); 
 
Point objectPos(srchWnd.x + srchWnd.width/2, 
                srchWnd.y + srchWnd.height/2); 
 
pos(0) = objectPos.x; 
pos(1) = objectPos.y; 
 
Mat estimation = kalman.correct(pos); 
 
Point estPt(estimation.at&lt;float&gt;(0), 
            estimation.at&lt;float&gt;(1)); 
 
drawMarker(frame, 
           estPt, 
           Scalar(0,255,0), 
           MARKER_CROSS, 
           30, 
           2); 
 
kalman.predict();</pre>
<p class="calibre2">You can refer to the online source code repository of this chapter for the complete example project containing the preceding code, which is almost identical to what you saw in the previous sections, except for the simple fact that <kbd class="calibre13">KalmanFilter</kbd> is used to correct the detected and tracked object position. As you can see, <kbd class="calibre13">objectPos</kbd>, which was previously read from the mouse movement position, is now set to the central point of the search window. After that, the <kbd class="calibre13">correct</kbd> function is called to perform an estimation and the results are displayed by drawing a green cross mark for the corrected tracking result. Besides the main advantage of using the Kalman filter algorithm, which is useful for getting rid of noise in detection and tracking results, it can also help with cases where detection is momentarily lost or impossible. Although losing detection is, technically speaking, an extreme case of noise, we're trying to point out the difference in terms of computer vision.</p>
<p class="calibre2">By going through a few examples and also experimenting with your own projects where the Kalman filter can be of help and trying different set of parameters for it, you'll instantly understand its long-standing popularity for whenever a practical algorithm for correcting a measurement (that contains noise) is required. What we learned in this section was a fairly simple case of how the Kalman filter is used (which was enough for our use case), but it is important to note that the same algorithm can be used to de-noise measurements of higher dimensionalities and with much more complexity.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to extract the background/foreground</h1>
                
            
            <article>
                
<p class="calibre2">The segmentation of background and foreground content in images is one of the most important video and motion analysis topics, and there has been a huge amount of research done in this area to provide some very practical and easy-to-use algorithms, which we're going to learn in the final section of this chapter. Current versions of OpenCV include the implementation of two background segmentation algorithms.</p>
<div class="packt_infobox">To use a terminology that is shorter, clearer, and more compatible with OpenCV functions and classes, we'll refer to background/foreground extraction and background/foreground segmentation simply as background segmentation.</div>
<p class="calibre2">The following two algorithms are available by default to be used for background segmentation using OpenCV:</p>
<ul class="calibre10">
<li class="calibre11"><kbd class="calibre13">BackgroundSubtractorKNN</kbd></li>
<li class="calibre11"><kbd class="calibre13">BackgroundSubtractorMOG2</kbd></li>
</ul>
<p class="calibre2">Both of these classes are subclasses of <kbd class="calibre13">BackgroundSubtractor</kbd>, which contains all of the required interfaces that one can expect from a proper background segmentation algorithm, which we'll get to later on. This simply allows us to use polymorphism to switch between algorithms that produce the same results and can be used in a very similar fashion for the exact same reason. The <kbd class="calibre13">BackgroundSubtractorKNN</kbd> class implements the K-nearest neighbors background segmentation algorithm, which is used in the case of a low foreground pixel count. <kbd class="calibre13">BackgroundSubtractorMOG2</kbd>, on the other hand, implements the Gaussian mixture-based background-segmentation algorithm. You can refer to the OpenCV online documentation for detailed information about the internal behavior and implementation of these algorithms. It's also a good idea to go through the referred articles for both of these algorithms, especially if you are looking for a custom background segmentation algorithm of your own.</p>
<div class="packt_infobox">Besides the algorithms we already mentioned, there are many more algorithms that can be used for background segmentation using OpenCV, which are included in the extra module, <kbd class="calibre29">bgsegm</kbd>. We'll omit those algorithms, since their usage is quite similar to the algorithms we'll be talking about in this section, and also because they do not exist in OpenCV by default.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">An example of background segmentation</h1>
                
            
            <article>
                
<p class="calibre2">Let's start with the <kbd class="calibre13">BackgroundSubtractorKNN</kbd> class and a hands-on example to see how background segmentation algorithms are used. You can use the <kbd class="calibre13">createBackgroundSubtractorKNN</kbd> function to create an object of the <kbd class="calibre13">BackgroundSubtractorKNN</kbd> type. Here's how:</p>
<pre class="calibre15">int history = 500; 
double dist2Threshold = 400.0; 
bool detectShadows = true; 
Ptr&lt;BackgroundSubtractorKNN&gt; bgs = 
        createBackgroundSubtractorKNN(history, 
                                      dist2Threshold, 
                                      detectShadows);</pre>
<p class="calibre2">To understand the parameters used in the <kbd class="calibre13">BackgroundSubtractorKNN</kbd> class, it is important to first note that this algorithm uses a sampling technique over the history of pixels to create a sampled background image. With that being said, the <kbd class="calibre13">history</kbd> parameter is used to define the number of previous frames that are used for sampling the background image, and the <kbd class="calibre13">dist2Threshold</kbd> parameter is the threshold of squared distance between a pixel's current value and its corresponding pixel value in the sampled background image. <kbd class="calibre13">detectShadows</kbd> is a self-explanatory parameter that is used to determine whether the shadows are going to be detected during background segmentation or not.</p>
<p class="calibre2">Now, we can simply use <kbd class="calibre13">bgs</kbd> to extract foreground masks from a video and use it to detect movements or an object entering the scene. Here's how:</p>
<pre class="calibre15">VideoCapture cam(0); 
if(!cam.isOpened()) 
    return -1; 
 
while(true) 
{ 
    Mat frame; 
    cam &gt;&gt; frame; 
    if(frame.empty()) 
        break; 
 
    Mat fgMask; // foreground mask 
    bgs-&gt;apply(frame, 
               fgMask); 
 
    Mat fg; // foreground image 
    bitwise_and(frame, frame, fg, fgMask); 
 
    Mat bg; // background image 
    bgs-&gt;getBackgroundImage(bg); 
 
    imshow("Input Image", frame); 
    imshow("Background Image", bg); 
    imshow("Foreground Mask", fgMask); 
    imshow("Foreground Image", fg); 
 
    int key = waitKey(10); 
    if(key == 27) // escape key 
        break; 
} 
 
cam.release();</pre>
<p class="calibre2">Let's quickly review the parts of the previous code that are new and maybe not that obvious. First things first, we use the <kbd class="calibre13">apply</kbd> function of the <kbd class="calibre13">BackgroundSubtractorKNN</kbd> class to perform a background/foreground segmentation operation. This function also updates the internal sampled background image for us. After that, we use the <kbd class="calibre13">bitwise_and</kbd> function with the foreground mask to extract the foreground image's content. To retrieve the sampled background image itself, we simply use the <kbd class="calibre13">getBackgroundImage</kbd> function. Finally, we display all of the results. Here are some example results that depict a scene (top-left), the extracted background image (top-right), the foreground mask (bottom-left), and the foreground image (bottom-right):</p>
<div class="cdpaligncenter"><img src="../images/00079.jpeg" class="calibre20"/></div>
<p class="calibre2">Notice that the shadow of the hand that moved into the scene is also captured by the background segmentation algorithm. In our example, we omitted the <kbd class="calibre13">learningRate</kbd> parameter when using the <kbd class="calibre13">apply</kbd> function. This parameter can be used to set the rate at which the learned background model is updated. A value of <kbd class="calibre13">0</kbd> means the model will not be updated at all, which can be quite useful if you are sure that the background will stay the same for any known period. A value of 1.0 means an extremely quick update of the model. As in the case of our example, we skipped this parameter, which causes it to use -1.0, and it means that the algorithm itself will decide on the learning rate. Another important thing to note is that the result of the apply function can yield an extremely noisy mask, which can be smoothed out by using a simple blur function, such as <kbd class="calibre13">medianBlur</kbd>, as seen here:</p>
<pre class="calibre15">medianBlur(fgMask,fgMask,3); </pre>
<p class="calibre2">Using the <kbd class="calibre13">BackgroundSubtractorMOG2</kbd> class is quite similar to <kbd class="calibre13">BackgroundSubtractorKNN</kbd>. Here's an example:</p>
<pre class="calibre15">int history = 500; 
double varThreshold = 16.0; 
bool detectShadows = true; 
Ptr&lt;BackgroundSubtractorMOG2&gt; bgs = 
        createBackgroundSubtractorMOG2(history, 
                                       varThreshold, 
                                       detectShadows); </pre>
<p class="calibre2">Note that the <kbd class="calibre13">createBackgroundSubtractorMOG2</kbd> function is used quite similarly to what we saw before to create an instance of the <kbd class="calibre13">BackgroundSubtractorMOG2</kbd> class. The only parameter that differs here is <kbd class="calibre13">varThreshold</kbd>, which corresponds to the variance threshold used for matching the pixels value and the background model. Using the <kbd class="calibre13">apply</kbd> and <kbd class="calibre13">getBackgroundImage</kbd> functions is identical in both background segmentation classes. Try modifying the threshold values in both algorithms to learn more about the visual effects of the parameters.</p>
<p class="calibre2">Background-segmentation algorithms have great potential for video editing software or even detecting and tracking objects in an environment with backgrounds that do not change too much. Try to use them in conjunction with the algorithms that you learned previously in this chapter to build tracking algorithms that make use of multiple algorithms to improve the results.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="calibre2">The video analysis module in OpenCV is a collection of extremely powerful algorithms, functions, and classes that we have learned about in this chapter. Starting from the whole idea of video processing and simple calculations based on the content of consecutive video frames, we moved on to learn about the Mean Shift algorithm and how it is used to track objects with known colors and specifications using a back-projection image. We also learned about the more sophisticated version of the Mean Shift algorithm, which is called the Continuously Adaptive Mean Shift, or simply CAM Shift. We learned that this algorithm is also capable of handling objects of different sizes and determining their orientation. Moving on with the tracking algorithms, we learned about the powerful Kalman filter and how it is used for de-noising and correcting the tracking results. We used the Kalman filter to track mouse movements and to correct the tracking results of the Mean Shift and CAM Shift algorithms. Finally, we learned about OpenCV classes that implement background-segmentation algorithms. We wrote a simple program to use background-segmentation algorithms and output the calculated background and foreground images. By now, we are familiar with some of the most popular and widely used computer vision algorithms that allow real-time detection and tracking of objects.</p>
<p class="calibre2">In the next chapter, we'll be learning about many feature extraction algorithms, functions, and classes, and how to use features to detect objects or extract useful information from images based on their key points and descriptors.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Questions</h1>
                
            
            <article>
                
<ol class="calibre14">
<li value="1" class="calibre11">All the examples in this chapter that deal with cameras return when there is a single failed or corrupted frame that leads to the detection of an empty frame. What type of modification is needed to allow a predefined number of retries before stopping the process?</li>
<li value="2" class="calibre11">How can we call the <kbd class="calibre13">meanShift</kbd> function to perform the Mean Shift algorithm with 10 iterations and an epsilon value of 0.5?</li>
<li value="3" class="calibre11">How can we visualize the hue histogram of the tracked object? Assume <kbd class="calibre13">CamShift</kbd> is used for tracking.</li>
<li value="4" class="calibre11">Set the process-noise covariance in the <kbd class="calibre13">KalmanFilter</kbd> class so that the filtered and measured values overlap. Assume only the process-noise covariance is set, of all the available matrices for <kbd class="calibre13">KalmanFilter</kbd> class-behavior control.</li>
</ol>
<p class="calibre2"> </p>
<ol start="5" class="calibre14">
<li value="5" class="calibre11">Let's assume that the <em class="calibre26">Y</em> position of the mouse on a window is used to describe the height of a filled rectangle that starts from the top-left corner of the window and has a width that equals the window width. Write a Kalman filter that can be used to correct the height of the rectangle (single value) and remove noise in the mouse movement that will cause a visually smooth resizing of the filled rectangle.</li>
<li value="6" class="calibre11">Create a <kbd class="calibre13">BackgroundSubtractorMOG2</kbd> object to extract the foreground image contents while avoiding shadow changes.</li>
<li value="7" class="calibre11">Write a program to display the <em class="calibre26">current</em> (as opposed to sampled) background image using a background-segmentation algorithm.</li>
</ol>


            </article>

            
        </section>
    </body></html>