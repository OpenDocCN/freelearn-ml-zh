<html><head></head><body>
		<div id="_idContainer137">
			<h1 id="_idParaDest-136"><em class="italic"><a id="_idTextAnchor137"/>Chapter 8</em>: Putting It All Together</h1>
			<p>In this chapter, we will revisit the <em class="italic">Lending Club Loan Application</em> data that we first introduced in <a href="B16721_03_Final_SK_ePub.xhtml#_idTextAnchor042"><em class="italic">Chapter 3</em></a>, <em class="italic">Fundamental Workflow <a id="_idTextAnchor138"/>– Data to Deployable Model</em>. This time, we begin the way most data science projects do, that is, with a raw data file and a general objective or question. Along the way, we will refine both the data and the problem statements so that they are relevant to the business and can be answered by the available data. Data scientists rarely begin with modeling-ready data; therefore, the treatment in this chapter more accurately reflects the job of a data scientist in the enterprise. We will then model the data and evaluate various candidate models, updating them as required, until we arrive at a final model. We will evaluate the final model and illustrate the preparation steps required for model deployment. This reinforces what we introduced in <a href="B16721_05_Final_SK_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 5</em></a> through <a href="B16721_07_Final_SK_ePub.xhtml#_idTextAnchor127"><em class="italic">Chapter 7</em></a>.</p>
			<p>By the end of this chapter, you will be able to take an unstructured problem with a raw data source and create a deployable model to answer a refined predictive question. For completeness, we will include all the code required to do each step of data preparation, feature engineering, model building, and evaluation. In general, any code that has already been covered in <a href="B16721_05_Final_SK_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 5</em></a> through <a href="B16721_07_Final_SK_ePub.xhtml#_idTextAnchor127"><em class="italic">Chapter 7</em></a> will be left uncommented.</p>
			<p>This chapter is divided into four sections, each of which has individual steps. The sections are listed as follows:</p>
			<ul>
				<li>Data wrangling</li>
				<li>Feature engineering</li>
				<li>Model building and evaluation</li>
				<li>Preparation for model pipeline deployment</li>
			</ul>
			<h1 id="_idParaDest-137"><a id="_idTextAnchor139"/>Technical requirements</h1>
			<p>If you still have not set up your H2O environment at this stage, to do so, please see <a href="B16721_Appendix_Final_SK_ePub.xhtml#_idTextAnchor268"><em class="italic">Appendix</em></a><em class="italic"> – Alternative</em><em class="italic"> Methods to Launch H2O Clusters</em>. </p>
			<h1 id="_idParaDest-138"><a id="_idTextAnchor140"/>Data wrangling</h1>
			<p>It is frequently said that 80–90% of a data scientist's job is dealing with data. At a minimum, you should understand the data granularity (that is, what the rows represent) and know what each column in the dataset means. Presented with a raw data source, there are multiple steps required to clean, organize, and transform your data into a modeling-ready dataset format. </p>
			<p>The dataset used for the <em class="italic">Lending Club</em> example in <em class="italic">Chapters 3</em>, <em class="italic">5</em>, and <em class="italic">7</em> was derived from a raw data<a id="_idIndexMarker574"/> file that we begin with here. In this section, we will illustrate the following steps: </p>
			<ol>
				<li>Import the raw data and determine which columns to keep.</li>
				<li>Define the problem, and create a response variable.</li>
				<li>Convert the implied numeric data from strings into numeric values.</li>
				<li>Clean up any messy categorical columns.</li>
			</ol>
			<p>Let's begin with the first step: importing the data.</p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor141"/>Importing the raw data</h2>
			<p>We import the raw data<a id="_idIndexMarker575"/> file using the following code:</p>
			<pre class="source-code">input_csv = "rawloans.csv"</pre>
			<pre class="source-code">loans = h2o.import_file(input_csv,</pre>
			<pre class="source-code">             col_types = {"int_rate": "string",</pre>
			<pre class="source-code">                          "revol_util": "string",</pre>
			<pre class="source-code">                          "emp_length": "string",</pre>
			<pre class="source-code">                          "verification_status": "string"})</pre>
			<p>A dictionary in the <strong class="source-inline">h2o.import_file</strong> code specifies the input column type of <strong class="source-inline">string</strong> for four of the input variables: <strong class="source-inline">int_rate</strong>, <strong class="source-inline">revol_util</strong>, <strong class="source-inline">emp_length</strong>, and <strong class="source-inline">verification_status</strong>. Specifying the column type explicitly ensures that the column is read in as the modeler intended. Without this code, these string variables might have been read as categorical columns with multiple levels.</p>
			<p>The dataset dimensions are obtained by the following command:</p>
			<pre class="source-code">loans.dim</pre>
			<p>This returns 42,536 rows (corresponding to 42,536 customer credit applications) and 52 columns. Next, we specify <a id="_idIndexMarker576"/>the 22 columns we wish to keep for our analysis:</p>
			<pre class="source-code">keep = ['addr_state', 'annual_inc', 'delinq_2yrs',</pre>
			<pre class="source-code">        'dti', 'earliest_cr_line', 'emp_length', 'grade',</pre>
			<pre class="source-code">        'home_ownership', 'inq_last_6mths', 'installment',</pre>
			<pre class="source-code">        'issue_d', 'loan_amnt', 'loan_status',</pre>
			<pre class="source-code">        'mths_since_last_delinq', 'open_acc', 'pub_rec',</pre>
			<pre class="source-code">        'purpose', 'revol_bal', 'revol_util', 'term',</pre>
			<pre class="source-code">        'total_acc', 'verification_status']</pre>
			<p>And we want to remove the remaining columns using the <strong class="source-inline">drop</strong> method:</p>
			<pre class="source-code">remove = list(set(loans.columns) - set(keep))</pre>
			<pre class="source-code">loans = loans.drop(remove)</pre>
			<p>But what about the 30 columns that we removed? They contained things such as text descriptions of the purpose of the loan, additional customer information such as the address or zip code, columns with almost completely missing information or other data quality issues, and more. Selecting the appropriate columns from a raw data source is an important task that takes much time and effort on the part of the data scientist.</p>
			<p>The columns we keep are those we believe are most likely to be predictive. Explanations for each column are listed as follows:</p>
			<ul>
				<li><strong class="source-inline">addr_state</strong>: This is the US state where the borrower resides.</li>
				<li><strong class="source-inline">annual_inc</strong>: This is the self-reported annual income of the borrower.</li>
				<li><strong class="source-inline">delinq_2yrs</strong>: This is the number of times the borrower has been more than 30 days late in payments during the last 2 years. </li>
				<li><strong class="source-inline">dti</strong>: This is the debt-to-income ratio (current debt divided by income).</li>
				<li><strong class="source-inline">earliest_cr_line</strong>: This is the date of the earliest credit line (generally, longer credit histories correlate with better credit risk).</li>
				<li><strong class="source-inline">emp_length</strong>: This is the length of employment.</li>
				<li><strong class="source-inline">grade</strong>: This is a risk rating from A to G assigned to the loan by the lender. </li>
				<li><strong class="source-inline">home_ownership</strong>: Does the borrower own a home or rent?</li>
				<li><strong class="source-inline">inq_last_6mths</strong>: This is the number of credit inquiries in the last 6 months.</li>
				<li><strong class="source-inline">installment</strong>: This is the monthly amount owed by the borrower.</li>
				<li><strong class="source-inline">issue_d</strong>: This is the date the loan was issued.</li>
				<li><strong class="source-inline">loan_amnt</strong>: This is the total <a id="_idIndexMarker577"/>amount lent to the borrower.</li>
				<li><strong class="source-inline">loan_status</strong>: This is a category. </li>
				<li><strong class="source-inline">mths_since_last_delinq</strong>: This is the number of months since the last delinquency.</li>
				<li><strong class="source-inline">open_acc</strong>: This is the number of open credit lines.</li>
				<li><strong class="source-inline">pub_rec</strong>: This is the number of derogatory public records (bankruptcies, tax liens, and judgments).</li>
				<li><strong class="source-inline">purpose</strong>: This is the borrower's stated purpose for the loan.</li>
				<li><strong class="source-inline">revol_bal</strong>: This is the revolving balance (that is, the amount owed on credit cards at the end of the billing cycle).</li>
				<li><strong class="source-inline">revol_util</strong>: This is the revolving utilization (that is, the amount of credit used divided by the total credit available to the borrower).</li>
				<li><strong class="source-inline">term</strong>: This is the number of payments on the loan in months (either 36 or 60).</li>
				<li><strong class="source-inline">total_acct</strong>: This is the borrower's total number of credit lines. </li>
				<li><strong class="source-inline">verification_status</strong>: This tells us whether the income was verified or not.</li>
			</ul>
			<p>Assuming our data columns have <a id="_idIndexMarker578"/>been properly selected, we can move on to the next step: creating the response variable.</p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor142"/>Defining the problem and creating the response variable</h2>
			<p>The creation of the<a id="_idIndexMarker579"/> response variable depends on the problem definition. The goal of this use case is to predict which customers will default on a loan. A model that <a id="_idIndexMarker580"/>predicts a loan default needs a<a id="_idIndexMarker581"/> response variable that differentiates between good and bad loans. Let's start by investigating the <strong class="source-inline">loan_status</strong> variable using the following code:</p>
			<pre class="source-code">loans["loan_status"].table().head(20)</pre>
			<p>This produces a table with all possible values of the loan status stored in our data:</p>
			<div>
				<div id="_idContainer117" class="IMG---Figure">
					<img src="image/B16721_08_001.jpg" alt="Figure 8.1 – Loan status categories from the raw Lending Club loan default dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.1 – Loan status categories from the raw Lending Club loan default dataset</p>
			<p>As you can see in <em class="italic">Figure 8.1</em>, the <strong class="source-inline">loan_status</strong> variable is relatively complex, containing 11 categories that are somewhat redundant or overlapping. For instance, <strong class="source-inline">Charged Off</strong> indicates that 5,435 loans were bad. <strong class="source-inline">Default</strong> contains another 7. <strong class="source-inline">Fully Paid</strong> shows that 30,843 loans were good. Some loans, for example, those indicated by the <strong class="source-inline">Current</strong> or <strong class="source-inline">Late</strong> categories, are still ongoing and so are not yet good or bad.</p>
			<p>Multiple loans were provided that did not meet the credit policy. Why this was allowed is unclear and worth <a id="_idIndexMarker582"/>checking with the data source. Did the credit policy change so that these loans are of an earlier vintage? Are these formal overrides or<a id="_idIndexMarker583"/> were they accidental? Whatever the case might be, these categories hint at a different underlying population that might require our<a id="_idIndexMarker584"/> attention. Should we remove these loans altogether, ignore the issue by collapsing them into their corresponding categories, or create a <strong class="source-inline">Meets Credit Policy</strong> indicator variable and model them directly? A better understanding of the data would allow the data scientist to make an informed decision.</p>
			<p>In the end, we need a binary response variable based on a population of loans that have either been paid off or defaulted. First, filter out any ongoing loans.</p>
			<h3>Removing ongoing loans</h3>
			<p>We need to build our model <a id="_idIndexMarker585"/>with only those loans that have either defaulted or been fully paid off. Ongoing loans have <strong class="source-inline">loan_status</strong> such as <strong class="source-inline">Current</strong> or <strong class="source-inline">In Grace Period</strong>. The following code captures the rows whose statuses indicate ongoing loans:</p>
			<pre class="source-code">ongoing_status = [</pre>
			<pre class="source-code">    "Current",</pre>
			<pre class="source-code">    "In Grace Period",</pre>
			<pre class="source-code">    "Late (16-30 days)",</pre>
			<pre class="source-code">    "Late (31-120 days)",</pre>
			<pre class="source-code">    "Does not meet the credit policy.  Status:Current",</pre>
			<pre class="source-code">    "Does not meet the credit policy.  Status:In Grace Period"</pre>
			<pre class="source-code">]</pre>
			<p>We use the following code to remove those ongoing loans and display the status for the remaining loans:</p>
			<pre class="source-code">loans = loans[~loans["loan_status"].isin(ongoing_status)]</pre>
			<pre class="source-code">loans["loan_status"].table()</pre>
			<p>The resulting status categories are shown in <em class="italic">Figure 8.2</em>:</p>
			<div>
				<div id="_idContainer118" class="IMG---Figure">
					<img src="image/B16721_08_002.jpg" alt="Figure 8.2 – Loan status categories after filtering the ongoing loans&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.2 – Loan status categories after filtering the ongoing loans</p>
			<p>Note that, in <em class="italic">Figure 8.2</em>, five <a id="_idIndexMarker586"/>categories of loan status now need to be summarized in a binary response variable. This is detailed in the next step.</p>
			<h3>Defining the binary response variable</h3>
			<p>We start by <a id="_idIndexMarker587"/>forming a <strong class="source-inline">fully_paid</strong> list to summarize the<a id="_idIndexMarker588"/> loan status categories:</p>
			<pre class="source-code">fully_paid = [</pre>
			<pre class="source-code">    "Fully Paid",</pre>
			<pre class="source-code">    "Does not meet the credit policy.  Status:Fully Paid"</pre>
			<pre class="source-code">]</pre>
			<p>Next, let's create a binary response column, <strong class="source-inline">bad_loan</strong>, as an indicator for any loans that were not completely paid off:</p>
			<pre class="source-code">response = "bad_loan"</pre>
			<pre class="source-code">loans[response] = ~(loans["loan_status"].isin(fully_paid))</pre>
			<pre class="source-code">loans[response] = loans[response].asfactor()</pre>
			<p>Finally, remove the original loan status column:</p>
			<pre class="source-code">loans = loans.drop("loan_status")</pre>
			<p>We remove the original loan<a id="_idIndexMarker589"/> status column because the information we need for building our predictive model is now contained in the <strong class="source-inline">bad_loan</strong> response variable.</p>
			<p>Next, we will convert string<a id="_idIndexMarker590"/> data into numeric values.</p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor143"/>Converting implied numeric data from strings into numeric values</h2>
			<p>There are various ways that <a id="_idIndexMarker591"/>data can be messy. In the preceding step, we saw how variables can sometimes contain redundant categories that might benefit from summarization. The format in which data values are displayed and stored can also cause problems. Therefore, the 28% that we naturally interpret as a number is, typically, input as a character string by a computer. Converting implied numeric data into actual numeric data is a very typical data quality task.</p>
			<p>Consider the <strong class="source-inline">revol_util</strong> and <strong class="source-inline">emp_length</strong> columns:</p>
			<pre class="source-code">loans[["revol_util", "emp_length"]].head()</pre>
			<p>The output is shown in the following screenshot:</p>
			<div>
				<div id="_idContainer119" class="IMG---Figure">
					<img src="image/B16721_08_003.jpg" alt="Figure 8.3 – Variables stored as strings to be converted into numeric values&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.3 – Variables stored as strings to be converted into numeric values</p>
			<p>The <strong class="source-inline">revol_util</strong> variable, as shown in <em class="italic">Figure 8.3</em>, is inherently numeric but has a trailing percent sign. In this case, the solution is simple: strip the <strong class="bold">%</strong> sign and convert the strings into numeric values. This <a id="_idIndexMarker592"/>can be done with the following code:</p>
			<pre class="source-code">x = "revol_util"</pre>
			<pre class="source-code">loans[x] = loans[x].gsub(pattern="%", replacement="")</pre>
			<pre class="source-code">loans[x] = loans[x].trim()</pre>
			<pre class="source-code">loans[x] = loans[x].asnumeric()</pre>
			<p>The <strong class="source-inline">gsub</strong> method substitutes <strong class="source-inline">%</strong> with a blank space. The <strong class="source-inline">trim</strong> method removes any whitespace in the string. The <strong class="source-inline">asnumeric</strong> method converts the string value into a number. </p>
			<p>The <strong class="source-inline">emp_length</strong> column is only slightly more complex. First, we need to strip out the <strong class="source-inline">year</strong> or <strong class="source-inline">years</strong> term. Also, we must deal with the <strong class="source-inline">&lt;</strong> and <strong class="source-inline">+</strong> signs. If we define <strong class="source-inline">&lt; 1</strong> as <strong class="source-inline">0</strong> and <strong class="source-inline">10+</strong> as <strong class="source-inline">10</strong>, then <strong class="source-inline">emp_length</strong> can also be cast as numeric. This can be done using the following code:</p>
			<pre class="source-code">x = "emp_length"</pre>
			<pre class="source-code">loans[x] = loans[x].gsub(pattern="([ ]*+[a-zA-Z].*)|(n/a)", </pre>
			<pre class="source-code">                         replacement="") </pre>
			<pre class="source-code">loans[x] = loans[x].trim()</pre>
			<pre class="source-code">loans[x] = loans[x].gsub(pattern="&lt; 1", replacement="0")</pre>
			<pre class="source-code">loans[x] = loans[x].gsub(pattern="10\\+", replacement="10") </pre>
			<pre class="source-code">loans[x] = loans[x].asnumeric()</pre>
			<p>Next, we will complete our data<a id="_idIndexMarker593"/> wrangling steps by cleaning up any messy categorical columns.</p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor144"/>Cleaning up messy categorical columns</h2>
			<p>The last step<a id="_idIndexMarker594"/> in preparation for feature engineering and modeling is clarifying the options or levels in often messy categorical columns. This standardization task is illustrated by the <strong class="source-inline">verification_status</strong> variable. Use the following code to find the levels of <strong class="source-inline">verification_status</strong>:</p>
			<pre class="source-code">loans["verification_status"].head()</pre>
			<p>The results are displayed in <em class="italic">Figure 8.4</em>:</p>
			<div>
				<div id="_idContainer120" class="IMG---Figure">
					<img src="image/B16721_08_004.jpg" alt="Figure 8.4 – The categories of the verification status from the raw data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.4 – The categories of the verification status from the raw data</p>
			<p>Because there are multiple values in <em class="italic">Figure 8.4</em> that mean verified (<strong class="source-inline">VERIFIED - income</strong> and <strong class="source-inline">VERIFIED - income source</strong>), we simply replace them with <strong class="source-inline">verified</strong>. The following <a id="_idIndexMarker595"/>code uses the <strong class="source-inline">sub</strong> method for easy replacement:</p>
			<pre class="source-code">x = "verification_status"</pre>
			<pre class="source-code">loans[x] = loans[x].sub(pattern = "VERIFIED - income source",</pre>
			<pre class="source-code">                        replacement = "verified")</pre>
			<pre class="source-code">loans[x] = loans[x].sub(pattern = "VERIFIED - income",</pre>
			<pre class="source-code">                        replacement = "verified")</pre>
			<pre class="source-code">loans[x] = loans[x].asfactor()</pre>
			<p>After completing all our data wrangling steps, we will move on to feature engineering.</p>
			<h1 id="_idParaDest-143"><a id="_idTextAnchor145"/>Feature engineering</h1>
			<p>In <a href="B16721_05_Final_SK_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 5</em></a>, <em class="italic">Advanced Model Building – Part I</em>, we introduced some feature engineering concepts and discussed<a id="_idIndexMarker596"/> target encoding at length. In this section, we will delve into feature engineering in a bit more depth. We can organize feature engineering as follows:</p>
			<ul>
				<li>Algebraic transformations</li>
				<li>Features engineered from dates</li>
				<li>Simplifying categorical variables by combining categories</li>
				<li>Missing value indicator functions</li>
				<li>Target encoding categorical columns</li>
			</ul>
			<p>The ordering of these transformations is not important except for the last one. Target encoding is the only transformation that requires data to be split into train and test sets. By saving it for the end, we can apply the other transformations to the entire dataset at once rather than separately to the training and test splits. Also, we introduce stratified sampling for splitting data in H2O-3. This has very little impact on our current use case but is important when data is highly imbalanced, such as in fraud modeling. </p>
			<p>In the following sections, we<a id="_idIndexMarker597"/> include all our feature engineering code for completeness. Code that has been introduced earlier will be merely referenced, while new feature engineering tasks will merit discussion. Let's begin with algebraic transformations.</p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor146"/>Algebraic transformations</h2>
			<p>The most straightforward form <a id="_idIndexMarker598"/>of feature engineering entails taking simple transformations of the raw data columns: the log, the square, the square root, the differences in columns, the ratios of the columns, and more. Often, the inspiration for these transformations comes from an underlying theory or is based on subject-matter expertise.</p>
			<p>The <strong class="source-inline">credit_length</strong> variable, as defined in <a href="B16721_05_Final_SK_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 5</em></a>, <em class="italic">Advanced Model Building – Part I</em>, is one such transformation. Recall that this is created with the following code:</p>
			<pre class="source-code">loans["credit_length"] = loans["issue_d"].year() - \</pre>
			<pre class="source-code">    loans["earliest_cr_line"].year()</pre>
			<p>The justification for this variable is based on a business observation: customers with longer credit histories tend to be at lower risk of defaulting. Also, we drop the <strong class="source-inline">earliest_cr_line</strong> variable, which is no longer needed:</p>
			<pre class="source-code">loans = loans.drop(["earliest_cr_line"])</pre>
			<p>Another simple feature we could try is <em class="italic">(annual income)/(number of credit lines)</em>, taking the log for distributional and numerical stability. Let's name it <strong class="source-inline">log_inc_per_acct</strong>. This ratio makes intuitive sense: larger incomes should be able to support a greater number of credit lines. This is similar to the debt-to-income ratio in intent but captures slightly different information. We can code it as follows:</p>
			<pre class="source-code">x = "log_inc_per_acct"</pre>
			<pre class="source-code">loans[x] = loans['annual_inc'].log() - \</pre>
			<pre class="source-code">    loans['total_acc'].log()</pre>
			<p>Next, we will consider the second feature engineering task: encoding information from dates.</p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor147"/>Features engineered from dates</h2>
			<p>As noted in <a href="B16721_05_Final_SK_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 5</em></a>, <em class="italic">Advanced Model Building – Part I</em>, there is a wealth of information contained in <a id="_idIndexMarker599"/>date values that are potentially predictive. To the <strong class="source-inline">issue_d_year</strong> and <strong class="source-inline">issue_d_month</strong> features that we created earlier, we add <strong class="source-inline">issue_d_dayOfWeek</strong> and <strong class="source-inline">issue_d_weekend</strong> as new factors. The code to do this is as follows:</p>
			<pre class="source-code">x = "issue_d"</pre>
			<pre class="source-code">loans[x + "_year"] = loans[x].year()</pre>
			<pre class="source-code">loans[x + "_month"] = loans[x].month().asfactor()</pre>
			<pre class="source-code">loans[x + "_dayOfWeek"] = loans[x].dayOfWeek().asfactor()</pre>
			<pre class="source-code">weekend = ["Sat", "Sun"]</pre>
			<pre class="source-code">loans[x + "_weekend"] = loans[x + "_dayOfWeek"].isin(weekend)</pre>
			<pre class="source-code">loans[x + "_weekend"] = loans[x + "_weekend"].asfactor()</pre>
			<p>At the end, we drop the original date variable:</p>
			<pre class="source-code">loans = loans.drop(x)</pre>
			<p>Next, we will address how to simplify categorical variables at the feature engineering stage.</p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor148"/>Simplifying categorical variables by combining categories </h2>
			<p>In the data wrangling stage, we cleaned up the messy categorical levels for the <strong class="source-inline">verification_status</strong> column, removing redundant or overlapping level definitions and making<a id="_idIndexMarker600"/> categories mutually exclusive. On the other hand, during this feature engineering stage, the category levels are already non-overlapping and carefully defined. The data values themselves, for instance, small counts for certain categories, might suggest some engineering approaches to improve predictive modeling.</p>
			<p>Summarize the <strong class="source-inline">home_ownership</strong> categorical variable using the following code:</p>
			<pre class="source-code">x = "home_ownership"</pre>
			<pre class="source-code">loans[x].table()</pre>
			<p>The tabled results are shown in the following screenshot:</p>
			<div>
				<div id="_idContainer121" class="IMG---Figure">
					<img src="image/B16721_08_005.jpg" alt="Figure 8.5 – Levels of the raw home ownership variable&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption"> </p>
			<p class="figure-caption">Figure 8.5 – Levels of the raw home ownership variable</p>
			<p>In <em class="italic">Figure 8.5</em>, although there are<a id="_idIndexMarker601"/> five recorded categories within home ownership, the largest three have thousands of observations: <strong class="source-inline">MORTGAGE</strong>, <strong class="source-inline">OWN</strong>, and <strong class="source-inline">RENT</strong>. The remaining two, <strong class="source-inline">NONE</strong> and <strong class="source-inline">OTHER</strong>, are so infrequent (8 and 135, respectively) that we will combine them with <strong class="source-inline">OWN</strong> to create an expanded <strong class="source-inline">OTHER</strong> category. </p>
			<p class="callout-heading">Collapsing Data Categories</p>
			<p class="callout">Depending on the inference we want to make, or our understanding of the problem, it might make more sense to collapse <strong class="source-inline">NONE</strong> and <strong class="source-inline">OTHER</strong> into the <strong class="source-inline">RENT</strong> or <strong class="source-inline">MORTGAGE</strong> categories.</p>
			<p>The procedure for combining the categorical levels is shown by the following command:</p>
			<pre class="source-code">loans[x].levels()</pre>
			<p>This is given by replacing the <strong class="source-inline">NONE</strong> and <strong class="source-inline">OWN</strong> level descriptions with <strong class="source-inline">OTHER</strong> and assigning it to a new variable, <strong class="source-inline">home_3cat</strong>, as shown in the following code:</p>
			<pre class="source-code">lvls = ["MORTGAGE", "OTHER", "OTHER", "OTHER", "RENT"]</pre>
			<pre class="source-code">loans["home_3cat"] = \</pre>
			<pre class="source-code">    loans[x].set_levels(lvls).ascharacter().asfactor()</pre>
			<p>Then, we drop the original <strong class="source-inline">home_ownership</strong> column:</p>
			<pre class="source-code">loans = loans.drop(x)</pre>
			<p>Next, we will visit how to <a id="_idIndexMarker602"/>create useful indicator functions for missing data.</p>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor149"/>Missing value indicator functions</h2>
			<p>When data is not missing at random, the pattern of missingness might be a source of predictive information. In other words, sometimes, the fact that a value is missing is as, or more, important than<a id="_idIndexMarker603"/> the actual value itself. Especially in cases where missing values are abundant, creating a missing value indicator function can prove helpful.</p>
			<p>The most interesting characteristic of employment length, <strong class="source-inline">emp_length</strong>, is whether the value for a customer is missing. Simple pivot tables show that the proportion of bad loans is 26.3% for customers with missing <strong class="source-inline">emp_length</strong> values and 18.0% for non-missing values. That disparity in default rates suggests using a missing value indicator function as a predictor.</p>
			<p>The code for creating a missing indicator function for the <strong class="source-inline">emp_length</strong> variable is simple: </p>
			<pre class="source-code">loans["emp_length_missing"] = loans["emp_length"] == None</pre>
			<p>Here, the new <strong class="source-inline">emp_length_missing</strong> column contains the indicator function. Unlike the other features that we engineered earlier, the original <strong class="source-inline">emp_length</strong> column does not need to be dropped as a possible predictor.</p>
			<p>Next, we will turn to target encoding categorical columns.</p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor150"/>Target encoding categorical columns</h2>
			<p>In <a href="B16721_05_Final_SK_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 5</em></a>, <em class="italic">Advanced Model Building – Part I</em>, we introduced<a id="_idIndexMarker604"/> target encoding in H2O-3 in some detail. As a prerequisite to target encoding, recall that a train and test set was required. We split the data using the <strong class="source-inline">split_frame</strong> method with code similar to the following:</p>
			<pre class="source-code">train, test = loans.split_frame(ratios = [0.8], seed = 12345)</pre>
			<p>The <strong class="source-inline">split_frame</strong> method <a id="_idIndexMarker605"/>creates a completely random sample split. This approach is required for all regression models and works well for relatively balanced classification problems. However, when binary classification is highly imbalanced, stratified sampling should be used instead. </p>
			<h3>Stratified sampling for binary classification data splits</h3>
			<p>Stratified sampling for binary <a id="_idIndexMarker606"/>classification works by separately sampling the good and bad loans. In other words, recall that 16% of the loans in our Lending Club dataset are bad. We wish to split the data into 80% train and 20% test datasets. If we separately sample 20% of the bad loans and 20% of the good loans and then combine them, we have a test dataset that preserves the 16% bad loan percentage. Combining the remaining data results in a 16% bad loan percentage in our training data. Therefore, stratified sampling preserves the original category ratios.</p>
			<p>We use the <strong class="source-inline">stratified_split</strong> method on the response column to create a new variable named <strong class="source-inline">split</strong>, which contains the <strong class="source-inline">train</strong> and <strong class="source-inline">test</strong> values, as shown in the following code:</p>
			<pre class="source-code">loans["split"] = loans[response].stratified_split(\</pre>
			<pre class="source-code">    test_frac = 0.2, seed = 12345)</pre>
			<pre class="source-code">loans[[response,"split"]].table()</pre>
			<p>The results of the stratified split are shown in the following screenshot: </p>
			<div>
				<div id="_idContainer122" class="IMG---Figure">
					<img src="image/B16721_08_006.jpg" alt="Figure 8.6 – The stratified split of loan data into train and test&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.6 – The stratified split of loan data into train and test</p>
			<p>We use the <strong class="source-inline">split</strong> column to <a id="_idIndexMarker607"/>create a Boolean mask for deriving the <strong class="source-inline">train</strong> and <strong class="source-inline">test</strong> datasets, as shown in the following code:</p>
			<pre class="source-code">mask = loans["split"] == "train"</pre>
			<pre class="source-code">train = loans[mask, :].drop("split")</pre>
			<pre class="source-code">test = loans[~mask, :].drop("split")</pre>
			<p>Note that we drop the <strong class="source-inline">split</strong> column from both datasets after their creation. Now we are ready to target encode using these train and test splits.</p>
			<h3>Target encoding the Lending Club data </h3>
			<p>The following<a id="_idIndexMarker608"/> code for target encoding the <strong class="source-inline">purpose</strong> and <strong class="source-inline">addr_state</strong> variables is similar to the code from <a href="B16721_05_Final_SK_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 5</em></a>, <em class="italic">Advanced Model Building – Part I</em>, which we have included here without discussion:</p>
			<pre class="source-code">from h2o.estimators import H2OTargetEncoderEstimator</pre>
			<pre class="source-code">encoded_columns = ["purpose", "addr_state"]</pre>
			<pre class="source-code">train["fold"] = train.kfold_column(n_folds = 5, seed = 25)</pre>
			<pre class="source-code">te = H2OTargetEncoderEstimator(</pre>
			<pre class="source-code">    data_leakage_handling = "k_fold",</pre>
			<pre class="source-code">    fold_column = "fold",</pre>
			<pre class="source-code">    noise = 0.05,</pre>
			<pre class="source-code">    blending = True,</pre>
			<pre class="source-code">    inflection_point = 10,</pre>
			<pre class="source-code">    smoothing = 20)</pre>
			<pre class="source-code">te.train(x = encoded_columns,</pre>
			<pre class="source-code">         y = response,</pre>
			<pre class="source-code">         training_frame = train)</pre>
			<pre class="source-code">train_te = te.transform(frame = train)</pre>
			<pre class="source-code">test_te = te.transform(frame = test, noise = 0.0)</pre>
			<p>Next, we redefine the <strong class="source-inline">train</strong> and <strong class="source-inline">test</strong> datasets, dropping<a id="_idIndexMarker609"/> the encoded columns from the target-encoded <strong class="source-inline">train_te</strong> and <strong class="source-inline">test_te</strong> splits. Also, we also drop the <strong class="source-inline">fold</strong> column from the <strong class="source-inline">train_te</strong> dataset (note that it does not exist in the <strong class="source-inline">test_te</strong> dataset). The code is as follows: </p>
			<pre class="source-code">train = train_te.drop(encoded_columns).drop("fold")</pre>
			<pre class="source-code">test = test_te.drop(encoded_columns)</pre>
			<p>With our updated <strong class="source-inline">train</strong> and <strong class="source-inline">test</strong> datasets, we are ready to tackle the model building and evaluation processes.</p>
			<h1 id="_idParaDest-149"><a id="_idTextAnchor151"/>Model building and evaluation</h1>
			<p>Our approach to model <a id="_idIndexMarker610"/>building starts with AutoML. Global explainability applied to the AutoML leaderboard either results in picking a candidate model or yields insights that we feed back into a new round of modified AutoML models. This process can be repeated if improvements in modeling or explainability are apparent. If a single model rather than a stacked ensemble is chosen, we can show how an additional random grid search could produce better models. Then, the final candidate model is evaluated. </p>
			<p>The beauty of this approach in H2O-3 is that the modeling heavy lifting is done for us automatically with AutoML. Iterating through this process is straightforward, and the improvement cycle can be repeated, as needed, until we have arrived at a satisfactory final model. </p>
			<p>We organize the modeling steps as follows:</p>
			<ol>
				<li value="1">Model search and optimization with AutoML.</li>
				<li>Investigate global explainability with the AutoML leaderboard models.</li>
				<li>Select a model from the AutoML candidates, with an optional additional grid search.</li>
				<li>Final model evaluation.</li>
			</ol>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor152"/>Model search and optimization with AutoML </h2>
			<p>The model build process using H2O-3 AutoML <a id="_idIndexMarker611"/>was extensively introduced in <a href="B16721_05_Final_SK_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 5</em></a>, <em class="italic">Advanced Model Building – Part I</em>. Here, we will follow a virtually identical process to create a leaderboard of models fit by AutoML. For clarity, we redefine our <strong class="source-inline">response</strong> column and <strong class="source-inline">predictors</strong> before removing the <strong class="source-inline">bad_loan</strong> response from the set of <strong class="source-inline">predictors</strong>:</p>
			<pre class="source-code">response = "bad_loan"</pre>
			<pre class="source-code">predictors = train.columns</pre>
			<pre class="source-code">predictors.remove(response)</pre>
			<p>Our AutoML parameters only exclude deep learning models, allowing the process to run for up to 30 minutes, as shown in the following code snippet:</p>
			<pre class="source-code">from h2o.automl import H2OAutoML</pre>
			<pre class="source-code">aml = H2OAutoML(max_runtime_secs = 1800,</pre>
			<pre class="source-code">                exclude_algos = ['DeepLearning'],</pre>
			<pre class="source-code">                seed = 12345)</pre>
			<pre class="source-code">aml.train(x = predictors, </pre>
			<pre class="source-code">          y = response, </pre>
			<pre class="source-code">          training_frame = train)</pre>
			<p>As demonstrated in <a href="B16721_05_Final_SK_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 5</em></a>, <em class="italic">Advanced Model Building – Part I</em>, we can access H2O Flow to monitor the model build process in more detail. Once the training on the <strong class="source-inline">aml</strong> object finishes, we<a id="_idIndexMarker612"/> proceed to investigate the resulting models with global explainability.</p>
			<h2 id="_idParaDest-151"><a id="_idTextAnchor153"/>Investigating global explainability with AutoML models</h2>
			<p>In <a href="B16721_07_Final_SK_ePub.xhtml#_idTextAnchor127"><em class="italic">Chapter 7</em></a>, <em class="italic">Understanding ML Models</em>, we outlined the <a id="_idIndexMarker613"/>use of global explainability for a series of models produced by AutoML. Here, we will follow the same procedure by calling the <strong class="source-inline">explain</strong> method with the <strong class="source-inline">test</strong> data split:</p>
			<pre class="source-code">aml.explain(test)</pre>
			<p>The resulting AutoML leaderboard is shown in the following screenshot:</p>
			<div>
				<div id="_idContainer123" class="IMG---Figure">
					<img src="image/B16721_08_007.jpg" alt="Figure 8.7 – The top 10 models of the AutoML leaderboard&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.7 – The top 10 models of the AutoML leaderboard</p>
			<p>The stacked ensemble <strong class="source-inline">AllModels</strong> and <strong class="source-inline">BestOfFamily</strong> models claim the top two positions on the leaderboard in <em class="italic">Figure 8.7</em>. The best single model is enclosed by a green box and labeled <strong class="source-inline">model_6</strong> from <strong class="source-inline">XGBoost_grid__1</strong>. We will investigate this model a bit further as a possible candidate model.</p>
			<p>The <strong class="bold">Model Correlation</strong> plot is shown in <em class="italic">Figure 8.8</em>. The green box indicates the correlation between our <a id="_idIndexMarker614"/>candidate XGBoost model and the two stacked ensembles. It confirms that the candidate model has among the highest correlation with the ensembles: </p>
			<div>
				<div id="_idContainer124" class="IMG---Figure">
					<img src="image/B16721_08_008.jpg" alt=" Figure 8.8 – Model Correlation plot for the AutoML leaderboard models&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption"> Figure 8.8 – Model Correlation plot for the AutoML leaderboard models</p>
			<p>The <strong class="bold">Variable Importance Heatmap</strong> diagram in <em class="italic">Figure 8.9</em> tells us more about the stability of the individual features than about the relationship between the models. The GBM grid models of 1, 2, 3, and 7 <a id="_idIndexMarker615"/>cluster together, and the XGBoost grid models of 6, 7, and 9 appear very similar in terms of how important variables are in these models:</p>
			<div>
				<div id="_idContainer125" class="IMG---Figure">
					<img src="image/B16721_08_009.jpg" alt="Figure 8.9 – Variable Importance Heatmap for AutoML models&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.9 – Variable Importance Heatmap for AutoML models</p>
			<p>The multiple model <strong class="bold">Partial Dependence Plots</strong> (<strong class="bold">PDPs</strong>), in conjunction with the variable importance heatmap, yield some <a id="_idIndexMarker616"/>valuable insights. <em class="italic">Figure 8.10</em> shows the PDP for <strong class="source-inline">grade</strong>, a feature with values<a id="_idIndexMarker617"/> from A to G that appear to be increasing at default risk. In other words, the average response for A is less than that for B, which is itself less than that for C, and so forth. This diagnostic appears to be confirming a business rating practice:</p>
			<div>
				<div id="_idContainer126" class="IMG---Figure">
					<img src="image/B16721_08_010.jpg" alt=" Figure 8.10 – The multiple model PDP for grade&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption"> Figure 8.10 – The multiple model PDP for grade</p>
			<p>In <em class="italic">Figure 8.11</em>, the PDP for annual income acts as a diagnostic. Intuitively, an increase in annual income should correspond to a decrease in bad loan rates. We can formally enforce (rather than just hope for) a monotonic decreasing relationship between the annual income and the default rate by adding monotonicity constraints to our model build code:</p>
			<div>
				<div id="_idContainer127" class="IMG---Figure">
					<img src="image/B16721_08_011.jpg" alt="Figure 8.11 – The multiple model PDP for annual income&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.11 – The multiple model PDP for annual income</p>
			<p>Monotonicity constraints can be <a id="_idIndexMarker618"/>applied to one or more numeric variables in the GBM, XGBoost, and AutoML models in H2O-3. To do so, supply a <strong class="source-inline">monotone_constraints</strong> parameter with a dictionary of variable names and the direction of the monotonicity: <strong class="source-inline">1</strong> for a monotonic increasing relationship and <strong class="source-inline">-1</strong> for monotonic decreasing. The following code shows how we add a monotonic decreasing <strong class="source-inline">annual_inc</strong> constraint:</p>
			<pre class="source-code">maml = H2OAutoML(</pre>
			<pre class="source-code">         max_runtime_secs = 1800,</pre>
			<pre class="source-code">         exclude_algos = ['DeepLearning'],</pre>
			<pre class="source-code">         monotone_constraints = {"annual_inc": -1}, </pre>
			<pre class="source-code">         seed = 12345)</pre>
			<p class="callout-heading">Monotonic Increasing and Decreasing Constraints</p>
			<p class="callout">Formally, the monotonic increasing constraint is a monotonic non-decreasing constraint, meaning that the function must either be increasing or flat. Likewise, the monotonic decreasing constraint is more correctly termed a monotonic non-increasing constraint. </p>
			<p>Fitting a constrained model <a id="_idIndexMarker619"/>proceeds as usual:</p>
			<pre class="source-code">maml.train(x = predictors, </pre>
			<pre class="source-code">           y = response, </pre>
			<pre class="source-code">           training_frame = train)</pre>
			<p>Here is the <strong class="source-inline">explain</strong> method:</p>
			<pre class="source-code">maml.explain(test)</pre>
			<p>This produces the leaderboard, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<img src="image/B16721_08_012.jpg" alt="Figure 8.12 – The leaderboard for AutoML with monotonic constraints&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption"> </p>
			<p class="figure-caption">Figure 8.12 – The leaderboard for AutoML with monotonic constraints</p>
			<p>The first 10 models of the updated AutoML leaderboard are shown in <em class="italic">Figure 8.12</em>. Note that a new model has been added, the monotonic stacked ensemble (boxed in red). This stacked ensemble uses, as constituent models, only those that are monotonic. In our case, this means that any DRF and XRT random forest models fit by AutoML would be excluded. Also note that the monotonic version of XGBoost model 6 is once more the leading single model, boxed in green.</p>
			<p><em class="italic">Figure 8.13</em> shows the<a id="_idIndexMarker620"/> monotonic multiple model PDP for annual income:  </p>
			<div>
				<div id="_idContainer129" class="IMG---Figure">
					<img src="image/B16721_08_013.jpg" alt="Figure 8.13 – The multiple model PDP for annual income&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.13 – The multiple model PDP for annual income</p>
			<p>Note that only two of the models included in the PDP of <em class="italic">Figure 8.13</em> are not monotonic: the DRF and XRT models. They are both versions of random forest that do not have monotonic options. This plot confirms that the monotonic constraints on annual income worked as intended. (Note that the PDP in <em class="italic">Figure 8.11</em> is very similar. The models there might have displayed monotonicity, but it was not enforced.)</p>
			<p>Next, we will consider how to choose a model from the AutoML leaderboard.</p>
			<h2 id="_idParaDest-152"><a id="_idTextAnchor154"/>Selecting a model from the AutoML candidates</h2>
			<p>Once AutoML has created a<a id="_idIndexMarker621"/> class of models, it is left to the<a id="_idIndexMarker622"/> data scientist to determine which model to put into production. If pure predictive accuracy is the only requirement, then the choice is rather simple: select the top model in the leaderboard (usually, this is the <strong class="bold">All Models</strong> stacked ensemble). In the case where monotonic constraints are required, the monotonic stacked ensemble is usually the most predictive.</p>
			<p>If business or regulatory constraints only allow a single model to be deployed, then we can select one based on<a id="_idIndexMarker623"/> a combination of predictive performance and other considerations, such as the modeling type. Let's select XGBoost model 6 as our candidate model:</p>
			<pre class="source-code">candidate = h2o.get_model(maml.leaderboard[3, 'model_id'])</pre>
			<p>H2O-3 AutoML does a tremendous job at building and tuning models across multiple modeling types. For<a id="_idIndexMarker624"/> individual models, it is sometimes possible to get an improvement in performance via an additional random grid search. We will explore this in the next section.</p>
			<h3>Random grid search to improve the selected model (optional)</h3>
			<p>We use the parameters of the candidate <a id="_idIndexMarker625"/>model as starting points for our random grid search. The idea is to search within the neighborhood of the candidate model for models that perform slightly better, noting that any improvements found will likely be minor. The stacked ensemble models give us a ceiling for how well an individual model can perform. The data scientist must judge whether the difference between candidate model performance and stacked ensemble performance warrants the extra effort in searching for possibly better models.</p>
			<p>We can list the model parameters using the following code:</p>
			<pre class="source-code">candidate.actual_params</pre>
			<p>Start by importing <strong class="source-inline">H2OGridSearch</strong> and the candidate model estimator; in our case, that is <strong class="source-inline">H2OXGBoostEstimator</strong>:</p>
			<pre class="source-code">from h2o.grid.grid_search import H2OGridSearch</pre>
			<pre class="source-code">from h2o.estimators import H2OXGBoostEstimator</pre>
			<p>The hyperparameters are selected by looking at the candidate model's actual parameters and searching within the neighborhood of those values. For instance, the sample rate for the candidate model was reported as 80%, and in our hyperparameter tuning, we select a range between 60% and 100%. Likewise, a 60% column sample rate leads us to implement a <a id="_idIndexMarker626"/>range between 40% and 80% for the grid search. The hyperparameter tuning code is as follows:  </p>
			<pre class="source-code">hyperparams_tune = {</pre>
			<pre class="source-code">    'max_depth' : list(range(2, 6, 1)),</pre>
			<pre class="source-code">    'sample_rate' : [x/100. for x in range(60,101)],</pre>
			<pre class="source-code">    'col_sample_rate' : [x/100. for x in range(40,80)],</pre>
			<pre class="source-code">    'col_sample_rate_per_tree': [x/100. for x in</pre>
			<pre class="source-code">         range(80,101)],</pre>
			<pre class="source-code">    'learn_rate' : [x/100. for x in range(5,31)]</pre>
			<pre class="source-code">}</pre>
			<p>We limit the overall runtime of the random grid search to 30 minutes, as follows: </p>
			<pre class="source-code">search_criteria_tune = {</pre>
			<pre class="source-code">    'strategy' : "RandomDiscrete",</pre>
			<pre class="source-code">    'max_runtime_secs' : 1800,</pre>
			<pre class="source-code">    'stopping_rounds' : 5,</pre>
			<pre class="source-code">    'stopping_metric' : "AUC",</pre>
			<pre class="source-code">    'stopping_tolerance': 5e-4</pre>
			<pre class="source-code">}</pre>
			<p>We add the monotonic constraints to the model and define our grid search:</p>
			<pre class="source-code">monotone_xgb_grid = H2OXGBoostEstimator(</pre>
			<pre class="source-code">    ntrees = 90,</pre>
			<pre class="source-code">    nfolds = 5,</pre>
			<pre class="source-code">    score_tree_interval = 10,</pre>
			<pre class="source-code">    monotone_constraints = {"annual_inc": -1},</pre>
			<pre class="source-code">    seed = 25)</pre>
			<pre class="source-code">monotone_grid = H2OGridSearch(</pre>
			<pre class="source-code">    monotone_xgb_grid,</pre>
			<pre class="source-code">    hyper_params = hyperparams_tune,</pre>
			<pre class="source-code">    grid_id = 'monotone_grid',</pre>
			<pre class="source-code">    search_criteria = search_criteria_tune)</pre>
			<p>Then, we train the model:</p>
			<pre class="source-code">monotone_grid.train(</pre>
			<pre class="source-code">    x = predictors,</pre>
			<pre class="source-code">    y = response,</pre>
			<pre class="source-code">    training_frame = train)</pre>
			<p>Returning to<a id="_idIndexMarker627"/> our results after this long training period, we extract the top two models to compare them with our initial candidate model. Note that we order by <strong class="source-inline">logloss</strong>:</p>
			<pre class="source-code">monotone_sorted = monotone_grid.get_grid(sort_by = 'logloss',</pre>
			<pre class="source-code">                                         decreasing = False)</pre>
			<pre class="source-code">best1 = monotone_sorted.models[0]</pre>
			<pre class="source-code">best2 = monotone_sorted.models[1]</pre>
			<p>Determine the performance of each of these models on the test data split:</p>
			<pre class="source-code">candidate.model_performance(test).logloss()</pre>
			<pre class="source-code">best1.model_performance(test).logloss()</pre>
			<pre class="source-code">best2.model_performance(test).logloss()</pre>
			<p>On the test sample, the logloss for the <strong class="source-inline">candidate</strong> model is 0.3951, <strong class="source-inline">best1</strong> is 0.3945, and <strong class="source-inline">best2</strong> is 0.3937. Based on this criterion alone, the <strong class="source-inline">best2</strong> model is our updated candidate model. The <a id="_idIndexMarker628"/>next step is the evaluation of this final model.</p>
			<h2 id="_idParaDest-153"><a id="_idTextAnchor155"/>Final model evaluation</h2>
			<p>Having selected <strong class="source-inline">best2</strong> as our final candidate, next, we evaluate this individual model using the <strong class="source-inline">explain</strong> method:</p>
			<pre class="source-code">final = best2</pre>
			<pre class="source-code">final.explain(test)</pre>
			<p>We will use the variable<a id="_idIndexMarker629"/> importance plot in <em class="italic">Figure 8.14</em> in conjunction with individual PDPs to understand the impact of the input variables on this model:</p>
			<div>
				<div id="_idContainer130" class="IMG---Figure">
					<img src="image/B16721_08_014.jpg" alt="Figure 8.14 – Variable importance for the final model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption"> </p>
			<p class="figure-caption">Figure 8.14 – Variable importance for the final model</p>
			<p>The <strong class="source-inline">term</strong> variable is by far the most important variable in the final model. Inspecting the PDP for "term" in <em class="italic">Figure 8.15</em> explains why.</p>
			<div>
				<div id="_idContainer131" class="IMG---Figure">
					<img src="image/B16721_08_015.jpg" alt="Figure 8.15 – PDP for term&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.15 – PDP for term</p>
			<p>Loans with a term <a id="_idIndexMarker630"/>of 36 months have a default rate of around 12%, while 60-month loans have a default rate that jumps to over 25%. Note that because this is an XGBoost model, <strong class="source-inline">term</strong> was parameterized as <strong class="source-inline">term 36 months</strong>. </p>
			<p>The next variable in importance is <strong class="source-inline">grade A</strong>. This is an indicator function for one level of the categorical <strong class="source-inline">grade</strong> variable. Looking at the PDP for <strong class="source-inline">grade</strong> in <em class="italic">Figure 8.16</em>, loans with a level of A only have a 10% default rate with an approximate 5% jump for the next lowest risk grade, B: </p>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<img src="image/B16721_08_016.jpg" alt=" Figure 8.16 – PDP for grade&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption"> Figure 8.16 – PDP for grade</p>
			<p>The next two variables<a id="_idIndexMarker631"/> are numeric and roughly equivalent in importance: credit inquiries in the last 6 months (<strong class="source-inline">inq_last_6mths</strong>) and annual income. Their PDPs are shown in <em class="italic">Figures 8.17</em> and <em class="italic">Figure 8.18</em>, respectively. The credit inquiries PDP appears to be monotonic except for the right-hand tail. This is likely due to thin data in this upper region of high numbers of inquiries. It would probably make sense to add a monotonic constraint to this variable as we did for annual income in <em class="italic">Figure 8.18</em>:</p>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<img src="image/B16721_08_017.jpg" alt="Figure 8.17 – PDP for the number of inquiries in the last 6 months&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.17 – PDP for the number of inquiries in the last 6 months</p>
			<div>
				<div id="_idContainer134" class="IMG---Figure">
					<img src="image/B16721_08_018.jpg" alt="Figure 8.18 – PDP for monotonic annual income&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.18 – PDP for monotonic annual income</p>
			<p><em class="italic">Figure 8.19</em> shows the PDP for revolving credit utilization. Unlike earlier numeric plots, the <strong class="source-inline">revol_util</strong> variable is not visibly monotonic. In general, the higher the utilization, the greater the default rate. However, there is a relatively high default rate at the utilization of zero. Sometimes, effects such as this are caused by mixtures of disparate populations. For<a id="_idIndexMarker632"/> example, this could be a combination of customers who have credit lines but carry no balances (generally good risks) with customers who have no credit lines at all (generally poorer risks). Without reparameterization, <strong class="source-inline">revol_util</strong> should not be constrained to be monotonic:</p>
			<div>
				<div id="_idContainer135" class="IMG---Figure">
					<img src="image/B16721_08_019.jpg" alt="Figure 8.19 – PDP for revolving utilization&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.19 – PDP for revolving utilization</p>
			<p>Finally, <em class="italic">Figure 8.20</em> shows the SHAP summary for the final model. The relative importance in terms of SHAP values is slightly different than that of our feature importance and PDP views: </p>
			<div>
				<div id="_idContainer136" class="IMG---Figure">
					<img src="image/B16721_08_020.jpg" alt="Figure 8.20 – The SHAP Summary plot for the final model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.20 – The SHAP Summary plot for the final model</p>
			<p>This has been a<a id="_idIndexMarker633"/> taster of what a final model review or whitepaper would show. Some of these are multiple pages in length. </p>
			<h1 id="_idParaDest-154"><a id="_idTextAnchor156"/>Preparation for model pipeline deployment </h1>
			<p>Exporting a model as <a id="_idIndexMarker634"/>a MOJO for final model deployment is trivial, for instance, consider the following:</p>
			<pre class="source-code">final.download_MOJO("final_MOJO.zip")</pre>
			<p>Deployment of the MOJO in various architectures via multiple recipes is covered, in detail, in <a href="B16721_09_Final_SK_ePub.xhtml#_idTextAnchor159"><em class="italic">Chapter 9</em></a>, <em class="italic">Production Scoring and the H2O MOJO</em>. In general, there is a significant amount of effort that must be assigned to productionizing data for model scoring. The key is that data used in production must have a schema identical to that of the training data used in modeling. In our case, that means all the data wrangling and feature engineering tasks must be productionized before scoring in production can occur. In other words, the process is simply as follows:</p>
			<ol>
				<li value="1">Transform raw data into the training data format.</li>
				<li>Score the model using the MOJO on the transformed data.</li>
			</ol>
			<p>It is a best <a id="_idIndexMarker635"/>practice to work with your DevOps or equivalent production team well in advance of model delivery to understand the data requirements for deployment. This includes specifying roles and responsibilities such as who is responsible for producing the data transformation code, how is the code to be tested, who is responsible for implementation, and more. Usually, the delivery of a MOJO is not the end of the effort for a data science leader. We will discuss the importance of this partnership, in more detail, in <a href="B16721_09_Final_SK_ePub.xhtml#_idTextAnchor159"><em class="italic">Chapter 9</em></a>, <em class="italic">Production Scoring and the H2O MOJO</em>.</p>
			<h1 id="_idParaDest-155"><a id="_idTextAnchor157"/>Summary</h1>
			<p>In this chapter, we reviewed the entire data science model-building process. We started with raw data and a somewhat vaguely defined use case. Further inspection of the data allowed us to refine the problem statement to one that was relevant to the business and that could be addressed with the data at hand. We performed extensive feature engineering in the hopes that some features might be important predictors in our model. We introduced an efficient and powerful method of model building using H2O AutoML to build an array of different models using multiple algorithms. Selecting one of those models, we demonstrated how to further refine the model with additional hyperparameter tuning using grid search. Throughout the model-building process, we used the diagnostics and model explanations introduced in <a href="B16721_07_Final_SK_ePub.xhtml#_idTextAnchor127"><em class="italic">Chapter 7</em></a>, <em class="italic">Understanding ML Models</em>, to evaluate our ML model. After arriving at a suitable model, we showed the simple steps required to prepare for the enterprise deployment of a model pipeline built in H2O. </p>
			<p>The next chapter introduces us to the process of deploying these models into production using the H2O MOJO for scoring. </p>
		</div>
	</body></html>