<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Introduction to Machine Learning with Scala</h1>
                </header>
            
            <article>
                
<p class="mce-root">In <span>this chapter, we will explain some basic concepts of</span> <strong>machine learning</strong> <span>(</span><strong>ML</strong><span>) that will be used in all subsequent chapters. We will start with a brief introduction to ML including basic learning workflow, ML rule of thumb, and different learning tasks. Then we will gradually cover most important ML tasks. </span></p>
<p class="mce-root">Also, we will discuss getting started with Scala and Scala-based ML libraries for getting a quick start for the next chapter. Finally, we get started with ML with Scala and Spark ML by solving a real-life problem. The chapter will briefly cover the following topics:</p>
<ul>
<li class="mce-root">Overview of ML</li>
<li class="mce-root">ML tasks</li>
<li class="mce-root">Introduction to Scala</li>
<li class="mce-root">Scala ML libraries</li>
<li class="mce-root">Getting started with ML with Spark ML</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You'll be required to have basic knowledge of Scala and Java. Since Scala is also a JVM-based language, make sure both Java JRE and JDK are installed and configured on your machine. To be more specific, you'll need Scala 2.11.x and Java 1.8.x version installed. Also, you need an IDE, such as Eclipse, IntelliJ IDEA, or Scala IDE, with the necessary plugins. However, if you're using IntelliJ IDEA, Scala will already <span>be </span>integrated.</p>
<p>The code files of this chapter can be found on GitHub:</p>
<p><a href="https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter01" target="_blank">https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter01</a></p>
<p>Check out the following video to see the Code in Action:<br/>
<a href="http://bit.ly/2V3Id08" target="_blank">http://bit.ly/2V3Id08</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Overview of ML</h1>
                </header>
            
            <article>
                
<p>ML approaches are based on a set of statistical and mathematical algorithms in order to carry out tasks such as classification, regression analysis, concept learning, predictive modeling, clustering, and mining of useful patterns. Using ML, we aim to improve the whole learning process automatically such that we may not need complete human interactions, or we can <span>at least</span> reduce the level of such interactions as much as possible.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working principles of a learning algorithm</h1>
                </header>
            
            <article>
                
<p>Tom M. Mitchell explained what learning really means from a computer science perspective:</p>
<div class="packt_quote">"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E."</div>
<p>Based on this definition, we can conclude that a computer program or machine can do the following:</p>
<ul>
<li>Learn from data and histories</li>
<li>Improve with experience</li>
<li>Iteratively enhance a model that can be used to predict outcomes of questions</li>
</ul>
<p>Since the preceding points are at the core of predictive analytics, almost every ML algorithm we use can be treated as an optimization problem. This is about finding parameters that minimize an objective function, for example, a weighted sum of two terms such as a cost function and regularization. Typically, an objective function has two components:</p>
<ul>
<li>A regularizer, which controls the complexity of the model</li>
<li>The loss, which measures the error of the model on the training data</li>
</ul>
<p>On the other hand, the regularization parameter defines the trade-off between minimizing the training error and the model's complexity, in an effort to avoid overfitting problems. Now, if both of these components are convex, then their sum is also convex. So, when using an ML algorithm, the goal is to obtain the best hyperparameters of a function that return the minimum error when making predictions. Therefore, by using a convex optimization technique, we can minimize the function until it converges toward the minimum error.</p>
<p>Given that a problem is convex, it is usually easier to analyze the asymptotic behavior of the algorithm, which shows how fast it converges as the model observes more and more training data. The task of ML is to train a model so that it can recognize complex patterns from the given input data and can make decisions in an automated way.</p>
<p><span class="fontstyle0">Thus, inferencing is all about testing the model against new (that is, unobserved) data and evaluating the performance of the model itself. However, in the whole process and for making the predictive model a successful one, data acts as the first-class citizen in all ML tasks. In reality, the data that we feed to our machine learning systems must be made up of mathematical objects, such as vectors, so that they can consume such data. For example, in the following diagram, raw images are embedded into numeric values called feature vectors before feeding in to the learning algorithm: <br/></span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-518 image-border" src="assets/6b259b7b-104b-42f6-b03f-e6e686e14938.png" style="width:44.67em;height:26.50em;"/></p>
<p><span class="fontstyle0">Depending on the available data and feature types, the performance of your predictive model can vacillate dramatically. Therefore, selecting the right features is one of the most important steps before the inferencing takes place. This is called feature engineering,</span> where the domain knowledge about the data is used to create only selective or useful features that help prepare the feature vectors to be used so that a machine learning algorithm works.</p>
<p>For example, comparing hotels is quite difficult unless we already have a personal experience of staying in multiple hotels. However, with the help of an ML model, which is already trained with quality features out of thousands of reviews and features (for example, how many stars does a hotel have, size of the room, location, room service, and so on), it is pretty feasible now. We'll see several examples throughout the chapters. However, before developing such an ML model, knowing some ML concepts is also important.<span class="fontstyle2"> </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">General machine learning rule of thumb</h1>
                </header>
            
            <article>
                
<p><span class="fontstyle2">The general machine learning rule of thumb is that the more data there is, the better the predictive model. However, having more features often creates a mess, to the extent that the performance degrades drastically, especially if the dataset is high-dimensional</span><span class="fontstyle0">.</span> The entire learning process requires input datasets that can be split into three types <span>(or are already provided as such)</span>:</p>
<ul>
<li>A <strong>training set</strong> is the knowledge base coming from historical or live data that is used to fit the parameters of the ML algorithm. During the training phase, the ML model utilizes the training set to find optimal weights of the network and reach the objective function by minimizing the training error. Here, the back-prop rule or an optimization algorithm is used to train the model, but all the hyperparameters are needed to be set before the learning process starts.</li>
<li>A <strong>validation set</strong> is a set of examples used to tune the parameters of an ML model. It ensures that the model is trained well and generalizes toward avoiding overfitting. Some ML practitioners refer to it as a development set or dev set as well.</li>
<li>A <strong>test set</strong> is used for evaluating the performance of the trained model on unseen data. This step is also referred to as model inferencing. After assessing the final model on the test set (that is, when we're fully satisfied with the model's performance), we do not have to tune the model any further, but the trained model can be deployed in a production-ready environment.</li>
</ul>
<p class="mce-root"/>
<p>A common practice is splitting the input data (after necessary pre-processing and feature engineering) into 60% for training, 10% for validation, and 20% for testing, but it really depends on use cases. Sometimes, we also need to perform up-sampling or down-sampling on the data based on the availability and quality of the datasets.</p>
<p>This rule of thumb of learning on different types of training sets can differ across machine learning tasks, as we will cover in the next section. However, before that, let's take a quick look at a few common phenomena in machine learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">General issues in machine learning models</h1>
                </header>
            
            <article>
                
<p><span><span class="ember-view"><span>When we use this input data for the training, validation, and testing, usually the learning algorithms cannot learn 100% accurately, which involves training, validation, and test error (or loss).</span></span></span> There are two types of error that one can encounter in a machine learning model:</p>
<ul>
<li>Irreducible error</li>
<li>Reducible error</li>
</ul>
<p>The irreducible error cannot be reduced even with the most robust and sophisticated model. However, the reducible error, which has two components, called bias and variance, can be reduced<strong>.</strong> Therefore, to understand the model (that is, prediction errors), we need to focus on bias and variance only:</p>
<ul>
<li class="graf graf--p graf-after--p">Bias means how far the predicted value are from the actual values. Usually, if the average predicted values are very different from the actual values (labels), then the bias is higher.</li>
<li class="graf graf--p graf-after--p">An ML model will have a high bias because it can't model the relationship between input and output variables (can't capture the complexity of data well) and becomes very simple. Thus, a too-simple model with high variance causes underfitting of the data.</li>
</ul>
<p class="graf graf--p graf-after--p">The following diagram gives some high-level insights and also shows what a just-right fit model should look like:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-709 image-border" src="assets/643e8299-1ebf-4072-a764-c87d8d90aab3.png" style="width:113.50em;height:21.92em;"/></p>
<p class="graf graf--p graf-after--h3">Variance signifies the variability between the predicted values and the actual values<span> (how scattered they are)</span>.</p>
<div class="packt_tip"><strong>Identifying high bias and high variance</strong>: If the model has a high training error as well as the validation error or test error is the same as the training error, the model has high bias. On the other hand, if the model has low training error but has high validation or high test error, the model has a high variance.</div>
<p class="graf graf--p graf-after--h3">An ML model usually performs very well on the training set but doesn't work well on the test set (because of high error rates). Ultimately, it results in an underfit model. <span><span class="ember-view">We can recap the overfitting and underfitting once more:</span></span></p>
<ul>
<li><span><span class="ember-view"><strong>Underfitting</strong></span></span>: I<span><span class="ember-view">f your training and validation error are both relatively equal and very high, then your model is most likely underfitting your training data.</span></span></li>
<li><span><span class="ember-view"><strong>Overfitting</strong></span></span>: I<span><span class="ember-view">f your training error is low and your validation error is high, then your model is most likely overfitting your training data.</span></span> The just-rightfit model learns very well and performs better on unseen data too.</li>
</ul>
<div class="packt_tip"><strong>Bias-variance trade-off</strong>: The high bias and high variance issue is often called bias-variance trade-off, because a model cannot be too complex or too simple at the same time. Ideally, we would strive for the best model that has both low bias and low variance.</div>
<p class="mce-root">Now we know the basic working principle of an ML algorithm. However, based on problem type and the method used to solve a problem, ML tasks can be different, for example, supervised learning, unsupervised learning, and reinforcement learning. We'll discuss these learning tasks in more detail in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ML tasks</h1>
                </header>
            
            <article>
                
<p>Although every ML problem is more or less an optimization problem, the way they are solved can vary. In fact, learning tasks can be categorized into three types: supervised learning, unsupervised learning, and reinforcement learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Supervised learning</h1>
                </header>
            
            <article>
                
<p>Supervised learning is the simplest and most well-known automatic learning task. It is based on a number of predefined examples, in which the category to which each of the inputs should belong is already known, as shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-710 image-border" src="assets/a5c163b5-9617-43ef-916f-edd20c1a9bc1.png" style="width:124.75em;height:54.08em;"/></p>
<p><span><span>The preceding diagram </span></span>shows a typical workflow of supervised learning. An actor (for example, a data scientist or data engineer) performs <strong>Extraction Transformation Load</strong> (<strong>ETL</strong>) and the necessary feature engineering (including feature extraction, selection, and so on) to get the appropriate data with features and labels so that they can be fed in to the model. Then he would split the data into training, development, and test sets. The training set is used to train an ML model, the validation set is used to validate the training against the overfitting problem and regularization, and then the actor would evaluate the model's performance on the test set (that is, unseen data).</p>
<p>However, if the performance is not satisfactory, he can perform additional tuning to get the best model based on hyperparameter optimization. Finally, he would deploy the best model in a production-ready environment. The following diagram summarizes these steps in a nutshell:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-521 image-border" src="assets/dfa111da-e783-452e-a17f-32ccb53902de.png" style="width:165.00em;height:54.42em;"/></div>
<p>In the overall life cycle, there might be many actors involved (for example, a data engineer, data scientist, or an ML engineer) to perform each step independently or collaboratively. The supervised learning context includes classification and regression tasks; classification is used to predict which class a data point is a part of (discrete value). It is also used for predicting the label of the class attribute. On the other hand, regression is used for predicting continuous values and making a numeric prediction of the class attribute.</p>
<p>In the context of supervised learning, the learning process required for the input dataset is split randomly into three sets, for example, 60% for the training set, 10% for the validation set, and the remaining 30% for the testing set.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Unsupervised learning</h1>
                </header>
            
            <article>
                
<p class="mce-root">How would you summarize and group a dataset if the labels were not given? Probably, you'll try to answer this question by finding the underlying structure of a dataset and measuring the statistical properties such as frequency distribution, mean, standard deviation, and so on. If the question is <em>how would you effectively represent data in a compressed format?</em> You'll probably reply saying that you'll use some software for doing the compression, although you might have no idea how that software would do <span><span>it</span></span>. The following diagram shows the typical workflow of an unsupervised learning task:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-522 image-border" src="assets/69e15120-3013-419e-ac6d-66248ab3c9a9.png" style="width:144.17em;height:45.75em;"/></p>
<p class="graf graf--p graf-after--p">These are exactly two of the main goals of unsupervised learning, which <span>is largely a data-driven process</span>. We call this type of learning <em>unsupervised</em> because you will have to deal with unlabeled data. The following quote comes from <span>Yann LeCun, director of AI research (source:</span> Predictive Learning, NIPS 2016, Yann LeCun, Facebook Research<span>):</span></p>
<div class="graf graf--p graf-after--p packt_quote"><span><em>"Most of human and animal learning is unsupervised learning. If intelligence was a cake, unsupervised learning would be the cake, supervised learning would be the icing on the cake, and reinforcement learning would be the cherry on the cake. We know how to make the icing and the cherry, but we don't know how to make the cake. We need to solve the unsupervised learning problem before we can even think of getting to true AI".</em></span></div>
<p class="graf graf--p graf-after--p">The two most widely used unsupervised learning tasks include the following:</p>
<ul>
<li class="graf graf--p graf-after--p"><strong>Clustering</strong>: Grouping data points based on similarity (or statistical properties). For example, a company such as Airbnb often groups its apartments and houses into neighborhoods so that customers can navigate the listed ones more easily.</li>
<li class="graf graf--p graf-after--p"><strong>Dimensionality</strong> <strong>reduction</strong>: Compressing the data with the structure and statistical properties preserved as much as possible. For example, often the number of dimensions of the dataset needs to be reduced for the modeling and visualization.</li>
<li class="graf graf--p graf-after--p"><strong>Anomaly detection</strong>: Useful in several applications such as identification of credit card <span>fraud detection, identifying faulty pieces of hardware in an industrial engineering process, and identifying outliers in large-scale datasets.</span></li>
<li class="graf graf--p graf-after--p"><strong>Association rule mining</strong>: Often <span>used in market basket analysis, for example, asking which items are brought together and frequently.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reinforcement learning</h1>
                </header>
            
            <article>
                
<p>Reinforcement learning is an artificial intelligence approach that focuses on the learning of the system through its interactions with the environment. In reinforcement learning, the system's parameters are adapted based on the feedback obtained from the environment, which in turn provides feedback on the decisions made by the system. The following diagram shows a person making decisions in order to arrive at their destination. Let's take an example of the route you take from home to work:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-523 image-border" src="assets/529a2d39-87ec-450b-8fb1-23e795d276f0.png" style="width:32.33em;height:20.58em;"/></div>
<p>In this case, you take the same route to work every day. However, out of the blue, one day you get curious and decide to try a different route with a view to finding the shortest path. Similarly, based on your experience and the time taken with the different route, you'd decide whether you should take a specific route more often. We can take a look at one more example in terms of a system modeling a chess player. In order to improve its performance, the system utilizes the result of its previous moves; such a system is said to be a system learning with reinforcement.</p>
<p>So far, we have learned the basic working principles of ML and different learning tasks. However, a summarized view of each learning task with some example use cases is a mandate, which we will see in the next subsection.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summarizing learning types with applications</h1>
                </header>
            
            <article>
                
<p>We have seen the basic working principles of ML algorithms. Then we have seen what the basic ML tasks are and how they formulate domain-specific problems. However, each of these learning tasks can be solved using different algorithms. The following diagram provides a glimpse into this:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img class="alignnone size-full wp-image-524 image-border" src="assets/69724fda-a4e8-4d3f-b8ac-522d883c906f.png" style="width:65.42em;height:32.67em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Types of learning and related problems</div>
<p>The following diagram summarizes the previously mentioned ML tasks and some applications:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img class="alignnone size-full wp-image-525 image-border" src="assets/8a2cf2df-a148-4609-8eb0-af0f975bdda8.png" style="width:114.42em;height:80.50em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">ML tasks and some use cases from different application domains</div>
<p>However, the preceding diagram lists only a few use cases and applications using different ML tasks. In practice, ML is used in numerous use cases and applications. We will try to cover a few of those throughout this book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Overview of Scala</h1>
                </header>
            
            <article>
                
<p><span class="fontstyle0"><span>Scala is a scalable, functional, and object-oriented programming language that is most closely related to Java. However, Scala is designed to be more concise and have features of functional programming languages.</span></span> <span class="fontstyle0">For example, Apache Spark, which is written in Scala, is a fast and general engine for large-scale data processing.</span></p>
<p><span class="fontstyle0">Scala's success is due to many factors: it has <span>many tools that enable succinct expression, it is very concise because you need less typing, and it therefore requires less reading, and it offers very good performance as well</span>. This is why Spark has more support for Scala in the sense that more APIs are available that are written in Scala compared to R, Python, and Java.</span> <span class="fontstyle2">Scala's symbolic operators are easy to read and, compared to Java, most of the Scala codes are comparatively concise and easy to read; Java is too verbose.</span> Functional programming concepts such as <span class="fontstyle0">pattern matching and higher-order functions are also available in Scala.<br/></span></p>
<p>The best way to get started with Scala is either using Scala through the <strong>Scala build tool</strong> (<strong>SBT</strong>) or to use Scala through an <strong>integrated development environment</strong> (<strong>IDE</strong>). Either way, the first important step is downloading, installing, and configuring Scala. However, since Scala runs on <strong>Java Virtual Machine</strong> (<strong>JVM</strong>), having Java installed and configured on your machine is a prerequisite. Therefore, I'm not going to cover how to do that. Instead, I will provide some useful links (<a href="https://en.wikipedia.org/wiki/Integrated_development_environment" target="_blank">https://en.wikipedia.org/wiki/Integrated_development_environment</a>).</p>
<div class="packt_infobox">Just follow the instructions on how to set up both Java and an IDE (for example, IntelliJ IDEA) or build tool (for example, SBT) at <a href="https://www.scala-lang.org/download/">https://www.scala-lang.org/download/</a>. If you're using Windows (for example, Windows 10) or Linux (for example, Ubuntu), visit <a href="https://www.journaldev.com/7456/download-install-scala-linux-unix-windows">https://www.journaldev.com/7456/download-install-scala-linux-unix-windows</a>. Finally, here are some macOS instructions: <a href="http://sourabhbajaj.com/mac-setup/Scala/README.html">http://sourabhbajaj.com/mac-setup/Scala/README.html</a>.</div>
<p class="mce-root">Java programmers normally prefer Scala when they need to add some functional programming flavor to their codes as Scala runs on JVM. <span class="fontstyle0">There are various other options when it comes to editors. The following are some options to choose from:</span></p>
<ul>
<li><span class="fontstyle0">Scala IDE</span></li>
<li><span class="fontstyle0">Scala plugin for Eclipse</span></li>
<li><span class="fontstyle0">IntelliJ IDEA</span></li>
<li><span class="fontstyle0">Emacs</span></li>
<li><span class="fontstyle0">Vim</span></li>
</ul>
<p><span class="fontstyle0">Eclipse has several advantages using numerous beta plugins and local, remote, and high-level debugging facilities with semantic highlighting and code completion for Scala.</span><span class="fontstyle0"> </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ML libraries in Scala</h1>
                </header>
            
            <article>
                
<p>Although Scala is a relatively new programming language compared to Java and Python, the <span><span class="ember-view"><span>question will arise as to why we need to consider learning it while we have Python and R. Well, Python and R are two leading programming languages for rapid prototyping and data analytics including building, exploring, and manipulating powerful models.</span></span></span></p>
<p><span><span class="ember-view"><span>But Scala is becoming the key language too in the development of functional products, which are well suited for big data analytics.</span></span></span> <span><span class="ember-view"><span>Big data applications often require stability, flexibility, high speed, scalability, and concurrency. All of these requirements can be fulfilled with Scala because Scala is not only a general-purpose language but also a powerful choice for data science (for example, Spark MLlib/ML).</span></span></span> I've been using Scala for the last couple of years and I found that more and more Scala ML libraries are in development. Up next, we will discuss available and widely used Scala libraries that can be used for developing ML applications.<br/></p>
<div class="packt_tip">Interested readers can take a quick look at this, which lists the 15 most popular Scala libraries for ML and data science:<br/>
<a href="https://www.datasciencecentral.com/profiles/blogs/top-15-scala-libraries-for-data-science-in-2018-1" target="_blank">https://www.datasciencecentral.com/profiles/blogs/top-15-scala-libraries-for-data-science-in-2018-1</a></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Spark MLlib and ML</h1>
                </header>
            
            <article>
                
<p>MLlib is a library that provides user-friendly ML algorithms that are implemented using Scala. The same API is then exposed to provide support for other languages such as Java, Python, and R. Spark MLlib provides support for local vectors and matrix data types stored on a single machine, as well as distributed matrices backed by one or multiple <strong>resilient distributed datasets</strong> (<strong>RDDs</strong>).</p>
<p>RDD is the primary data abstraction of Apache Spark, often called Spark Core, that represents an immutable, partitioned collection of elements that can be operated on in parallel. The resiliency makes RDD fault-tolerant (based on RDD lineage graph). RDD can help in distributed computing even when data is stored on multiple nodes in a Spark cluster. Also, RDD can be converted into a dataset as a collection of partitioned data with primitive values such as tuples or other objects.</p>
<p>Spark ML is a new set of ML APIs that allows users to quickly assemble and configure practical machine learning pipelines on top of datasets, which makes it easier to combine multiple algorithms into a single pipeline. For example, an ML algorithm (called estimator) and a set of transformers (for example, a <kbd>StringIndexer</kbd>, a <kbd>StandardScalar</kbd>, and a <kbd>VectorAssembler</kbd>) can be chained together to perform the ML task as stages without needing to run them sequentially.</p>
<div class="packt_infobox">Interested readers can take a look at the Spark MLlib and ML guide at <a href="https://spark.apache.org/docs/latest/ml-guide.html">https://spark.apache.org/docs/latest/ml-guide.html</a>.<a href="https://spark.apache.org/docs/latest/ml-guide.html"/></div>
<p>At this point, I have to inform you of something very useful. Since we will be using Spark MLlib and ML APIs in upcoming chapters too. Therefore, it would be worth fixing some issues in advance. If you're a Windows user, then let me tell you about a very weird issue that you will experience while working with Spark. The thing is that Spark works on Windows, macOS, and Linux. While using Eclipse or IntelliJ IDEA to develop your Spark applications on Windows, you might face an I/O exception error and, consequently, your application might not compile successfully or may be interrupted.</p>
<p>Spark needs a runtime environment for Hadoop on Windows too. Unfortunately, the binary distribution of Spark (v2.4.0, for example) does not contain Windows-native components such as <kbd>winutils.exe</kbd> or <kbd>hadoop.dll</kbd>. However, these are required (not optional) to run Hadoop on Windows if you cannot ensure the runtime environment, an I/O exception saying the following will appear:</p>
<pre class="mce-root"><strong>03/02/2019 11:11:10 ERROR util.Shell: Failed to locate the winutils binary in the hadoop binary path</strong><br/><strong> java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.</strong></pre>
<p>There are two ways to tackle this issue on Windows and from IDEs such as Eclipse and IntelliJ IDEA:</p>
<ol>
<li>Download <kbd>winutls.exe</kbd> from <a href="https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin/">https://github.com/steveloughran/ winutils/tree/ master/hadoop-2. 7. 1/bin/</a>.</li>
<li>Download and copy it inside the <kbd>bin</kbd> folder in the Spark distribution—for example, <kbd>spark-2.2.0-bin-hadoop2.7/bin/</kbd>.</li>
<li>Select <span class="packt_screen">Project</span> | <span class="packt_screen">Run Configurations...</span> | <span class="packt_screen">Environment</span> | <span class="packt_screen">New</span> | and create a variable named <kbd>HADOOP_HOME</kbd>, then put the path in the <span class="packt_screen">Value</span> field. Here is an example: <kbd>c:/spark-2.2.0-bin-hadoop2.7/bin/</kbd> | <span class="packt_screen">OK</span> | <span class="packt_screen">Apply</span> | <span class="packt_screen">Run</span>.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ScalNet and DynaML</h1>
                </header>
            
            <article>
                
<p>ScalNet is a wrapper around Deeplearning4J intended to emulate a Keras-like API for developing deep learning applications. If you're already familiar with neural network architectures and are coming from a JVM background, it would be worth exploring the Scala-based Sc<span class="text_exposed_show">alNet</span> <span class="text_exposed_show">library:</span></p>
<div class="text_exposed_show">
<ul>
<li>GitHub (<a href="https://github.com/deeplearning4j/deeplearning4j/tree/master/scalnet?fbclid=IwAR01enpe_dySCpU1aPkMorznm6k31cDmQ49wE52_jAGQzcr-3CZs9NNSVas" target="_blank">https://github.com/deeplear…/deeplearning4j/…/master/scalnet</a>)<a href="https://github.com/deeplearning4j/deeplearning4j/tree/master/scalnet?fbclid=IwAR01enpe_dySCpU1aPkMorznm6k31cDmQ49wE52_jAGQzcr-3CZs9NNSVas" target="_blank"/></li>
<li>Example (<a href="https://github.com/deeplearning4j/ScalNet/tree/master/src/test/scala/org/deeplearning4j/scalnet/examples?fbclid=IwAR2uMjTESm9KHAIZ_mZCHckZhRuZJByhmAbQDoUAn1vCVC1SoE0KmKDmQ9M" target="_blank">https://github.com/…/sc…/org/deeplearning4j/scalnet/examples</a>)<a href="https://github.com/deeplearning4j/ScalNet/tree/master/src/test/scala/org/deeplearning4j/scalnet/examples?fbclid=IwAR2uMjTESm9KHAIZ_mZCHckZhRuZJByhmAbQDoUAn1vCVC1SoE0KmKDmQ9M" target="_blank"/></li>
</ul>
<p>DynaML is a Scala and JVM ML toolbox for research, education, and industry. This library provides an interactive, end-to-end, and enterprise-friendly way of developing ML applications. If you're interested, see more at <a href="https://transcendent-ai-labs.github.io/DynaML/">https://transcendent-ai-labs.github.io/DynaML/</a>.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ScalaNLP, Vegas, and Breeze</h1>
                </header>
            
            <article>
                
<p><span>Breeze is one of the primary scientific computing libraries for Scala, which provides a fast and efficient way of data manipulation operations such as matrix and vector operations for creating, transposing, filling with numbers, conducting element-wise operations, and calculating determinants. </span></p>
<p><span>Breeze enables b</span><span>asic operations based on the <kbd>netlib-java</kbd> library, which enables extremely fast algebraic computations. In addition, Breeze provides a way to perform signal-processing operations<strong>,</strong> necessary for working with digital signals.</span></p>
<div class="mce-root packt_infobox">The following are the GitHub links:
<ul>
<li class="mce-root">Breeze (<a href="https://github.com/scalanlp/breeze/">https://github.com/scalanlp/breeze/</a>)<a href="https://github.com/scalanlp/breeze/"/></li>
<li class="mce-root">Breeze examples (<a href="https://github.com/scalanlp/breeze-examples">https://github.com/scalanlp/breeze-examples</a>)<a href="https://github.com/scalanlp/breeze-examples"/></li>
<li class="mce-root">Breeze quickstart (<a href="https://github.com/scalanlp/breeze/wiki/Quickstart">https://github.com/scalanlp/breeze/wiki/Quickstart</a>)<a href="https://github.com/scalanlp/breeze/wiki/Quickstart"/></li>
</ul>
</div>
<p>On the other hand, ScalaNLP is a suite of <span class="site-description">scientific computing, ML, and natural language processing</span>, which also acts as an umbrella project for several libraries, including Breeze and Epic. <span>Vegas is another Scala library for data visualization, which allows plotting specifications such as filtering, transformations, and aggregations. Vegas is more functional than the other numerical <span class="text-gray-dark mr-2">processing library,</span> Breeze.<br/></span></p>
<div class="packt_tip">For more information and examples of using Vegas and Breeze, refer to GitHub:<br/>
<ul>
<li>Vegas (<a href="https://github.com/vegas-viz/Vegas">https://github.com/vegas-viz/Vegas</a>)<a href="https://github.com/vegas-viz/Vegas"/></li>
<li>Breeze (<a href="https://github.com/scalanlp/breeze">https://github.com/scalanlp/breeze</a>)<a href="https://github.com/scalanlp/breeze"/></li>
</ul>
</div>
<p>Whereas the visualization library of Breeze is backed by Breeze and JFreeChart, Vegas can be considered a <span class="text-gray-dark mr-2">missing Matplotlib for Scala <span><span>and</span></span> Spark, because it</span> provides several options for rendering plots through and within interactive notebook environments, such as Jupyter and Zeppelin.</p>
<div class="packt_infobox">Refer to Zeppelin notebook solutions of each chapter in the GitHub repository of this book.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting started learning</h1>
                </header>
            
            <article>
                
<p>In this section, we'll see a real-life example of a classification problem. The idea is to develop a classifier that, given the values for sex, age, time, number of warts, type, and area, will predict whether a patient has to go through the cryotherapy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Description of the dataset</h1>
                </header>
            
            <article>
                
<p>We will use a recently added cryotherapy dataset from the UCI machine learning repository. The dataset can be downloaded from <a href="http://archive.ics.uci.edu/ml/datasets/Cryotherapy+Dataset+#">http://archive.ics.uci.edu/ml/datasets/Cryotherapy+Dataset+#</a>.<a href="http://archive.ics.uci.edu/ml/datasets/Cryotherapy+Dataset+#"/></p>
<p>This dataset contains information about wart treatment results of 90 patients using cryotherapy. In case you don't know, a wart is a kind of skin problem caused by infection with a type of human papillomavirus. Warts are typically small, rough, and hard growths that are similar in color to the rest of the skin.</p>
<p>There are two available treatments for this problem:</p>
<ul>
<li><strong>Salicylic acid</strong>: A type of gel containing salicylic acid used in medicated band-aids.</li>
<li><strong>Cryotherapy</strong>: A freezing liquid (usually nitrogen) is sprayed onto the wart. It will destroy the cells in the affected area. After the cryotherapy, usually, a blister develops, which eventually turns into a scab and falls off after a week or so.</li>
</ul>
<p>There are 90 samples or instances that were either recommended to go through cryotherapy or be discharged without cryotherapy. There are seven attributes in the dataset:</p>
<ul>
<li><kbd>sex</kbd>: Patient gender, characterized by <kbd>1</kbd> (male) or <kbd>0</kbd> (female).</li>
<li><kbd>age</kbd>: Patient age.</li>
<li><kbd>Time</kbd>: Observation and treatment time in hours.</li>
<li><kbd>Number_of_Warts</kbd>: Number of warts.</li>
<li><kbd>Type</kbd>: Types of warts.</li>
<li><kbd>Area</kbd>: The amount of affected area.</li>
<li><kbd>Result_of_Treatment</kbd>: The recommended result of the treatment, characterized by either <kbd>1</kbd> (yes) or <kbd>0</kbd> (no). It is also the target column.</li>
</ul>
<p>As you can understand, it is a classification problem because we will have to predict discrete labels. More specifically, it is a binary classification problem. Since this is a small dataset with only six features, we can start with a very basic classification algorithm called logistic regression, where the logistic function is applied to the regression to get the probabilities of it belonging in either class. We will learn more details about logistic regression and other classification algorithms in <a href="51712107-c5bc-4d7d-a84a-3039aafc8c0a.xhtml" target="_blank">Chapter 3</a>, <em>Scala for Learning Classification</em>. For this, we use the Spark ML-based implementation of logistic regression in Scala.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring the programming environment</h1>
                </header>
            
            <article>
                
<p>I am assuming that Java is already installed on your machine and <kbd>JAVA_HOME</kbd> is set too. Also, I'm assuming that your IDE has the Maven plugin installed. If so, then just create a Maven project and add the project properties as follows:</p>
<pre>&lt;properties&gt;<br/>     &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;<br/>     &lt;java.version&gt;1.8&lt;/java.version&gt;<br/>     &lt;jdk.version&gt;1.8&lt;/jdk.version&gt;<br/>     &lt;spark.version&gt;2.3.0&lt;/spark.version&gt;<br/> &lt;/properties&gt;</pre>
<p>In the preceding <kbd>properties</kbd> tag, I specified the Spark version (that is, <kbd>2.3.0</kbd>), but you can adjust it. Then add the following dependencies in the <kbd>pom.xml</kbd> file:</p>
<pre>&lt;dependencies&gt;<br/>     &lt;dependency&gt;<br/>         &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;<br/>         &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;<br/>         &lt;version&gt;${spark.version}&lt;/version&gt;<br/>     &lt;/dependency&gt;<br/>     &lt;dependency&gt;<br/>         &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;<br/>         &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;<br/>         &lt;version&gt;${spark.version}&lt;/version&gt;<br/>         &lt;/dependency&gt;<br/>     &lt;dependency&gt;<br/>         &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;<br/>         &lt;artifactId&gt;spark-mllib_2.11&lt;/artifactId&gt;<br/>         &lt;version&gt;${spark.version}&lt;/version&gt;<br/>         &lt;/dependency&gt;<br/>     &lt;dependency&gt;<br/>         &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;<br/>         &lt;artifactId&gt;spark-graphx_2.11&lt;/artifactId&gt;<br/>         &lt;version&gt;${spark.version}&lt;/version&gt;<br/>     &lt;/dependency&gt;<br/>     &lt;dependency&gt;<br/>         &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;<br/>         &lt;artifactId&gt;spark-yarn_2.11&lt;/artifactId&gt;<br/>         &lt;version&gt;${spark.version}&lt;/version&gt;<br/>         &lt;/dependency&gt;<br/>     &lt;dependency&gt;<br/>         &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;<br/>         &lt;artifactId&gt;spark-network-shuffle_2.11&lt;/artifactId&gt;<br/>         &lt;version&gt;${spark.version}&lt;/version&gt;<br/>         &lt;/dependency&gt;<br/>    &lt;dependency&gt;<br/>         &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;<br/>         &lt;artifactId&gt;spark-streaming-flume_2.11&lt;/artifactId&gt;<br/>         &lt;version&gt;${spark.version}&lt;/version&gt;<br/>     &lt;/dependency&gt;<br/>     &lt;dependency&gt;<br/>         &lt;groupId&gt;com.databricks&lt;/groupId&gt;<br/>         &lt;artifactId&gt;spark-csv_2.11&lt;/artifactId&gt;<br/>         &lt;version&gt;1.3.0&lt;/version&gt;<br/>         &lt;/dependency&gt;<br/> &lt;/dependencies&gt;</pre>
<p>Then, if everything goes smoothly, all the JAR files will be downloaded in the project home as Maven dependencies. Alright! Then we can start writing the code.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting started with Apache Spark</h1>
                </header>
            
            <article>
                
<p>Since you're here to learn how to solve a real-life problem in Scala, exploring available Scala libraries would be worthwhile. Unfortunately, we don't have many options except for the Spark MLlib and ML, which can be used for the regression analysis very easily and comfortably. Importantly, it has every regression analysis algorithm implemented as high-level interfaces. I assume that Scala, Java, and your favorite IDE such as Eclipse or IntelliJ IDEA are already configured on your machine. We will introduce some concepts of Spark without providing much detail, but we will continue learning in upcoming chapters too.</p>
<p>First, I'll introduce <kbd>SparkSession</kbd>, which is a unified entry point of a Spark application introduced from Spark 2.0. Technically, <kbd>SparkSession</kbd> is the gateway to interact with some of Spark's functionality with a few constructs such as <kbd>SparkContext</kbd>, <kbd>HiveContext</kbd>, and <kbd>SQLContext</kbd>, which are all encapsulated in a <kbd>SparkSession</kbd>. Previously, you have seen how to create such a session, probably without knowing it. Well, a <kbd>SparkSession</kbd> can be created as a builder pattern as follows:</p>
<pre><strong>import</strong> org.apache.spark.sql.SparkSession<br/><strong>val</strong> spark = <strong>SparkSession</strong><br/>      .builder // the builder itself<br/>      .master("local[4]") // number of cores (i.e. 4, use * for all cores) <br/>      .config("spark.sql.warehouse.dir", "/temp") // Spark SQL Hive Warehouse location<br/>      .appName("SparkSessionExample") // name of the Spark application<br/>      .getOrCreate() // get the existing session or create a new one</pre>
<p>The preceding builder will try to get an existing <kbd>SparkSession</kbd> or create a new one. Then the newly created <kbd>SparkSession</kbd> will be assigned as the global default.</p>
<div class="packt_tip">By the way, when using <kbd>spark-shell</kbd>, you don't need to create a <kbd>SparkSession</kbd> explicitly, because it's already created and accessible with the <kbd>spark</kbd> <span>variable</span>.</div>
<p>Creating a DataFrame is probably the most important task in every data analytics task. Spark provides a <kbd>read()</kbd> method that can be used to read data from numerous sources in various formats such as CSV, JSON, Avro, and JDBC. For example, the following code snippet shows how to read a CSV file and create a Spark DataFrame:</p>
<pre><strong>val</strong> dataDF = spark.read<br/>      .option("header", "true") // we read the header to know the column and structure<br/>      .option("inferSchema", "true") // we infer the schema preserved in the CSV<br/>      .format("com.databricks.spark.csv") // we're using the CSV reader from DataBricks<br/>      .load("data/inputData.csv") // Path of the CSV file<br/>      .cache // [Optional] cache if necessary </pre>
<p>Once a DataFrame is created, we can see a few samples (that is, rows) by invoking the <kbd>show()</kbd> method, as well as print the schema using the <kbd>printSchema()</kbd> method. Invoking <kbd>describe().show()</kbd> will show the statistics about the DataFrame:</p>
<pre>dataDF.show() // show first 10 rows <br/>dataDF.printSchema() // shows the schema (including column name and type)<br/>dataDF.describe().show() // shows descriptive statistics</pre>
<p>In many cases, we have to use the <kbd>spark.implicits._</kbd> package<em>,</em> which is one of the most useful imports. It is handy, with a lot of implicit methods for converting Scala objects to datasets and vice versa. Once we have created a DataFrame, we can create a view (temporary or global) for performing SQL using either the <kbd>ceateOrReplaceTempView()</kbd> method or the <kbd>createGlobalTempView()</kbd> method, respectively:</p>
<pre>dataDF.createOrReplaceTempView("myTempDataFrame") // create or replace a local temporary view with dataDF<br/>dataDF.createGlobalTempView("myGloDataFrame") // create a global temporary view with dataframe dataDF</pre>
<p>Now a SQL query can be issued to see the data in tabular format:</p>
<pre>spark.sql("SELECT * FROM myTempDataFrame")// will show all the records</pre>
<p>To drop these views, <kbd>spark.catalog.dropTempView("myTempDataFrame")</kbd> or <kbd>spark.catalog.dropGlobalTempView("myGloDataFrame")</kbd>, respectively, can be invoked. By the way, once you're done simply invoking the <kbd>spark.stop()</kbd> method, it will destroy the <kbd>SparkSession</kbd> and all the resources allocated by the Spark application. Interested readers can read detailed API documentation at <a href="https://spark.apache.org/">https://</a><a href="https://spark.apache.org/">spark.apache.org/</a> to get more information.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reading the training dataset</h1>
                </header>
            
            <article>
                
<p>There is a <span><kbd>Cryotherapy.xlsx</kbd> </span>Excel file, which contains data as well as data usage agreement texts. So, I just copied the data and saved it in a CSV file named <kbd>Cryotherapy.csv</kbd>. Let's start by creating <kbd>SparkSession</kbd>—the gateway to access Spark:</p>
<pre><strong>val</strong> spark = SparkSession<br/>      .builder<br/>      .master("local[*]")<br/>      .config("spark.sql.warehouse.dir", "/temp")<br/>      .appName("CryotherapyPrediction")<br/>      .getOrCreate()<br/><br/><strong>import</strong> spark.implicits._</pre>
<p>Then let's read the training set and see a glimpse of it:</p>
<pre><strong>var</strong> CryotherapyDF = spark.read.option("header", "true")<br/>              .option("inferSchema", "true")<br/>              .csv("data/Cryotherapy.csv")</pre>
<p>Let's take a look to see if the preceding CSV reader managed to read the data properly, including header and types:</p>
<pre>CryotherapyDF.printSchema()</pre>
<p>As seen from the following screenshot, the schema of the Spark DataFrame has been correctly identified. Also, as expected, <span>all the features of</span> my ML algorithms are numeric (in other words, in integer or double format):</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-526 image-border" src="assets/6a21026d-dda3-4de9-b779-610209849628.png" style="width:28.25em;height:10.00em;"/></p>
<p>A snapshot of the dataset can be seen using the <kbd>show()</kbd> method. We can limit the number of rows; here, let's say <kbd>5</kbd>:</p>
<pre>CryotherapyDF.show(5)</pre>
<p>The output of the preceding line of code shows the first five samples of the DataFrame:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-527 image-border" src="assets/ca5ade58-f80c-4354-b9ad-a630145e1087.png" style="width:34.75em;height:12.42em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preprocessing and feature engineering</h1>
                </header>
            
            <article>
                
<p>As per the dataset description on the UCI machine learning repository, there are no null values. Also, the Spark ML-based classifiers expect numeric values to model them. The good thing is that, as seen in the schema, all the required fields are numeric (that is, either integers or floating point values). Also, the Spark ML algorithms expect a <kbd>label</kbd> column, which in our case is <kbd>Result_of_Treatment</kbd>. Let's rename it to <kbd>label</kbd> using the Spark-provided <kbd>withColumnRenamed()</kbd> method:</p>
<pre>//Spark ML algorithm expect a 'label' column, which is in our case 'Survived". Let's rename it to 'label'<br/>CryotherapyDF = CryotherapyDF.withColumnRenamed("Result_of_Treatment", "label")<br/>CryotherapyDF.printSchema()</pre>
<p>All the Spark ML-based classifiers expect training data containing two objects called <kbd>label</kbd> (which we already have) and <kbd>features</kbd>. We have seen that we have six features. However, those features have to be assembled to create a feature vector. This can be done using the <kbd>VectorAssembler()</kbd> method. It is one kind of transformer from the Spark ML library. But first we need to select all the columns except the <kbd>label</kbd> column:</p>
<pre>val selectedCols = Array("sex", "age", "Time", "Number_of_Warts", "Type", "Area")</pre>
<p>Then we instantiate a <kbd>VectorAssembler()</kbd> transformer and transform as follows:</p>
<pre>val vectorAssembler = new VectorAssembler()<br/>          .setInputCols(selectedCols)<br/>          .setOutputCol("features")<br/>val numericDF = vectorAssembler.transform(CryotherapyDF)<br/>                    .select("label", "features")<br/>numericDF.show()</pre>
<p>As expected, the last line of the preceding code segment shows the assembled DataFrame having <kbd>label</kbd> and <kbd>features</kbd>, which are needed to train an ML algorithm:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-528 image-border" src="assets/144b0868-c2b9-4a88-b5f4-9f29ed0b43ed.png" style="width:15.67em;height:17.92em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing training data and training a classifier</h1>
                </header>
            
            <article>
                
<p>Next, we separate the training set and test sets. Let's say that 80% of the training set will be used for the training and the other 20% will be used to evaluate the trained model:</p>
<pre><strong>val</strong> splits = numericDF.randomSplit(Array(0.8, 0.2))<br/><strong>val</strong> trainDF = splits(0)<br/><strong>val</strong> testDF = splits(1)</pre>
<p>Instantiate a decision tree classifier by specifying impurity, max bins, and the max depth of the trees. Additionally, we set the <kbd>label</kbd> and <kbd>feature</kbd> columns:</p>
<pre><strong>val</strong> dt = new DecisionTreeClassifier()<br/>      .setImpurity("gini")<br/>      .setMaxBins(10)<br/>      .setMaxDepth(30)<br/>      .setLabelCol("label")<br/>      .setFeaturesCol("features")</pre>
<p>Now that the data and the classifier are ready, we can perform the training:</p>
<pre><strong>val</strong> dtModel = dt.fit(trainDF)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating the model</h1>
                </header>
            
            <article>
                
<p>Since it's a binary classification problem, we need the <kbd>BinaryClassificationEvaluator()</kbd> estimator to evaluate the model's performance on the test set:</p>
<pre><strong>val</strong> evaluator = new BinaryClassificationEvaluator()<br/>      .setLabelCol("label")</pre>
<p>Now that the training is completed and we have a trained decision tree model, we can evaluate the trained model on the test set:</p>
<pre><strong>val</strong> predictionDF = dtModel.transform(testDF)</pre>
<p>Finally, we compute the classification accuracy:</p>
<pre><strong>val</strong> accuracy = evaluator.evaluate(predictionDF)<br/>println("Accuracy =  " + accuracy)    </pre>
<p>You should experience about 96% classification accuracy:</p>
<pre><strong>Accuracy =  0.9675436785432</strong></pre>
<p>Finally, we stop the <kbd>SparkSession</kbd> by invoking the <kbd>stop()</kbd> method:</p>
<pre>spark.stop()</pre>
<p>We have managed to achieve about 96% accuracy with minimum effort. However, there are other performance metrics such as precision, recall, and F1 measure. We will discuss them in upcoming chapters. Also, if you're a newbie to ML and haven't understood all the steps in this example, don't worry. We'll recap all of these steps in other chapters with various other examples.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we have learned some basic concepts of ML, which is used to solve a real-life problem. We started with a brief introduction to ML including a basic learning workflow, the ML rule of thumb, and different learning tasks, and then we gradually covered important ML tasks such as supervised learning, unsupervised learning, and reinforcement learning. Additionally, we discussed Scala-based ML libraries. Finally, we have seen how to get started with machine learning with Scala and Spark ML by solving a simple classification problem.</p>
<p>Now that we know basic <span>ML</span> and Scala-based ML libraries, we can start learning in a more structured way. In the next chapter, we will learn about regression analysis techniques. Then we will develop a predictive analytics application for predicting slowness in traffic using linear regression and generalized linear regression algorithms.</p>


            </article>

            
        </section>
    </body></html>