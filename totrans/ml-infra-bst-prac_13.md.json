["```py\n# read the file with data using openpyxl\nimport pandas as pd\n# we read the data from the excel file,\n# which is the defect data from the ant 1.3 system\ndfDataAnt13 = pd.read_excel('./chapter_6_dataset_numerical.xlsx',\n                            sheet_name='ant_1_3',\n                            index_col=0)\n# prepare the dataset\nimport sklearn.model_selection\nX = dfDataAnt13.drop(['Defect'], axis=1)\ny = dfDataAnt13.Defect\nX_train, X_test, y_train, y_test = \\\n        sklearn.model_selection.train_test_split(X, y, random_state=42, train_size=0.9)\n```", "```py\n# now that we have the data prepared\n# we import the decision tree classifier and train it\nfrom sklearn.tree import DecisionTreeClassifier\n# first we create an empty classifier\ndecisionTreeModel = DecisionTreeClassifier()\n# then we train the classifier\ndecisionTreeModel.fit(X_train, y_train)\n# and we test it for the test set\ny_pred_cart = decisionTreeModel.predict(X_test)\n```", "```py\n# now, let's evaluate the code\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nprint(f'Accuracy: {accuracy_score(y_test, y_pred_cart):.2f}')\nprint(f'Precision: {precision_score(y_test, y_pred_cart, average=\"weighted\"):.2f}, Recall: {recall_score(y_test, y_pred_cart, average=\"weighted\"):.2f}')\n```", "```py\nAccuracy: 0.83\nPrecision: 0.94, Recall: 0.83\n```", "```py\nfrom sklearn.tree import export_text\ntree_rules = export_text(decisionTreeModel, feature_names=list(X_train.columns))\nprint(tree_rules)\n```", "```py\n|--- WMC <= 36.00\n|   |--- ExportCoupling <= 1.50\n|   |   |--- NOM <= 2.50\n|   |   |   |--- NOM <= 1.50\n|   |   |   |   |--- class: 0\n|   |   |   |--- NOM >  1.50\n|   |   |   |   |--- WMC <= 5.50\n|   |   |   |   |   |--- class: 0\n|   |   |   |   |--- WMC >  5.50\n|   |   |   |   |   |--- CBO <= 4.50\n|   |   |   |   |   |   |--- class: 1\n|   |   |   |   |   |--- CBO >  4.50\n|   |   |   |   |   |   |--- class: 0\n|   |   |--- NOM >  2.50\n|   |   |   |--- class: 0\n```", "```py\n# read the file with data using openpyxl\nimport pandas as pd\n# we read the data from the excel file,\n# which is the defect data from the ant 1.3 system\ndfDataCamel12 = pd.read_excel('./chapter_6_dataset_numerical.xlsx',\n                            sheet_name='camel_1_2',\n                            index_col=0)\n# prepare the dataset\nimport sklearn.model_selection\nX = dfDataCamel12.drop(['Defect'], axis=1)\ny = dfDataCamel12.Defect\nX_train, X_test, y_train, y_test = \\\n        sklearn.model_selection.train_test_split(X, y, random_state=42, train_size=0.9)\n```", "```py\n# now that we have the data prepared\n# we import the decision tree classifier and train it\nfrom sklearn.tree import DecisionTreeClassifier\n# first we create an empty classifier\ndecisionTreeModelCamel = DecisionTreeClassifier()\n# then we train the classifier\ndecisionTreeModelCamel.fit(X_train, y_train)\n# and we test it for the test set\ny_pred_cart_camel = decisionTreeModel.predict(X_test)\n```", "```py\n# now, let's evaluate the code\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nprint(f'Accuracy: {accuracy_score(y_test, y_pred_cart_camel):.2f}')\nprint(f'Precision: {precision_score(y_test, y_pred_cart_camel, average=\"weighted\"):.2f}, Recall: {recall_score(y_test, y_pred_cart_camel, average=\"weighted\"):.2f}')\n```", "```py\nAccuracy: 0.65\nPrecision: 0.71, Recall: 0.65\n```", "```py\nfrom sklearn.tree import export_text\ntree_rules = export_text(decisionTreeModel, feature_names=list(X_train.columns))\nprint(tree_rules)\n```", "```py\n|--- WMC >  36.00\n|   |--- DCC <= 3.50\n|   |   |--- WMC <= 64.50\n|   |   |   |--- NOM <= 17.50\n|   |   |   |   |--- ImportCoupling <= 7.00\n|   |   |   |   |   |--- NOM <= 6.50\n|   |   |   |   |   |   |--- class: 0\n|   |   |   |   |   |--- NOM >  6.50\n|   |   |   |   |   |   |--- CBO <= 4.50\n|   |   |   |   |   |   |   |--- class: 0\n|   |   |   |   |   |   |--- CBO >  4.50\n|   |   |   |   |   |   |   |--- ExportCoupling <= 13.00\n|   |   |   |   |   |   |   |   |--- NOM <= 16.50\n|   |   |   |   |   |   |   |   |   |--- class: 1\n|   |   |   |   |   |   |   |   |--- NOM >  16.50\n|   |   |   |   |   |   |   |   |   |--- class: 0\n|   |   |   |   |   |   |   |--- ExportCoupling >  13.00\n|   |   |   |   |   |   |   |   |--- class: 0\n|   |   |   |   |--- ImportCoupling >  7.00\n|   |   |   |   |   |--- class: 0\n|   |   |   |--- NOM >  17.50\n|   |   |   |   |--- class: 1\n|   |   |--- WMC >  64.50\n|   |   |   |--- class: 0\n```", "```py\nfrom sklearn.ensemble import RandomForestClassifier\nrandomForestModel = RandomForestClassifier()\nrandomForestModel.fit(X_train, y_train)\ny_pred_rf = randomForestModel.predict(X_test)\n```", "```py\nAccuracy: 0.62\nPrecision: 0.63, Recall: 0.62\n```", "```py\n# now, let's check which of the features are the most important ones\n# first we create a dataframe from this list\n# then we sort it descending\n# and then filter the ones that are not imporatnt\ndfImportantFeatures = pd.DataFrame(randomForestModel.feature_importances_, index=X.columns, columns=['importance'])\n# sorting values according to their importance\ndfImportantFeatures.sort_values(by=['importance'],\n                                ascending=False,\n                                inplace=True)\n# choosing only the ones that are important, skipping\n# the features which have importance of 0\ndfOnlyImportant = dfImportantFeatures[dfImportantFeatures['importance'] != 0]\n# print the results\nprint(f'All features: {dfImportantFeatures.shape[0]}, but only {dfOnlyImportant.shape[0]} are used in predictions. ')\n```", "```py\n# we use matplotlib and seaborn to make the plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Define size of bar plot\n# We make the x axis quite much larger than the y-axis since\n# there is a lot of features to visualize\nplt.figure(figsize=(40,10))\n# plot Searborn bar chart\n# we just use the blue color\nsns.barplot(y=dfOnlyImportant['importance'],\n            x=dfOnlyImportant.index,\n            color='steelblue')\n# we make the x-labels rotated so that we can fit\n# all the features\nplt.xticks(rotation=90)\nsns.set(font_scale=6)\n# add chart labels\nplt.title('Importance of features, in descending order')\nplt.xlabel('Feature importance')\nplt.ylabel('Feature names')\n```", "```py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n# Define the neural network architecture\nclass NeuralNetwork(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(NeuralNetwork, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        return out\n```", "```py\n# Define the hyperparameters\ninput_size = X_train.shape[1]  # Number of input features\nhidden_size = 64              # Number of neurons in the hidden layer\nnum_classes = 2               # Number of output classes\n# Create an instance of the neural network\nmodel = NeuralNetwork(input_size, hidden_size, num_classes)\n# Define the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n# Convert the data to PyTorch tensors\nX_train_tensor = torch.Tensor(X_train.values)\ny_train_tensor = torch.LongTensor(y_train.values)\nX_test_tensor = torch.Tensor(X_test.values)\n# Training the neural network\nnum_epochs = 10000\nbatch_size = 32\n```", "```py\nfor epoch in range(num_epochs):\n    for I in range(0, len(X_train_tensor), batch_size):\n        batch_X = X_train_tensor[i:i+batch_size]\n        batch_y = y_train_tensor[i:i+batch_size]\n        # Forward pass\n        outputs = model(batch_X)\n        loss = criterion(outputs, batch_y)\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    # Print the loss at the end of each epoch\n    if (epoch % 100 == 0):\n      print(\"\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.3f\"\")\n```", "```py\nwith torch.no_grad():\n    model.eval()  # Set the model to evaluation mode\n    X_test_tensor = torch.Tensor(X_test.values)\n    outputs = model(X_test_tensor)\n    _, predicted = torch.max(outputs.data, 1)\n    y_pred_nn = predicted.numpy()\n# now, let's evaluate the code\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nprint(f'Accuracy: {accuracy_score(y_test, y_pred_nn):.2f}')\nprint(f'Precision: {precision_score(y_test, y_pred_nn, average=\"weighted\"):.2f}, Recall: {recall_score(y_test, y_pred_nn, average=\"weighted\"):.2f}')\n```", "```py\nAccuracy: 0.73\nPrecision: 0.79, Recall: 0.73\n```", "```py\nX_trainL, X_testL, y_trainL, y_testL = \\\n        sklearn.model_selection.train_test_split(X, y, random_state=42, train_size=0.8)\n```", "```py\n# now, let's evaluate the model on this new data\nwith torch.no_grad():\n    model.eval()  # Set the model to evaluation mode\n    X_test_tensor = torch.Tensor(X_testL.values)\n    outputs = model(X_test_tensor)\n    _, predicted = torch.max(outputs.data, 1)\n    y_pred_nn = predicted.numpy()\nprint(f'Accuracy: {accuracy_score(y_testL, y_pred_nn):.2f}')\nprint(f'Precision: {precision_score(y_testL, y_pred_nn, average=\"weighted\"):.2f}, Recall: {recall_score(y_testL, y_pred_nn, average=\"weighted\"):.2f}')\n```", "```py\nAccuracy: 0.85\nPrecision: 0.86, Recall: 0.85\n```"]