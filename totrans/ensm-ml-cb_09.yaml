- en: Homogeneous Ensembles Using Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: An ensemble of homogeneous models for energy prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An ensemble of homogeneous models for handwritten digit classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the case of ensemble models, each base classifier must have some degree
    of diversity within itself. This diversity can be obtained in one of the following
    manners:'
  prefs: []
  type: TYPE_NORMAL
- en: By using different subsets of training data through various resampling methods
    or randomization of the training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By using different learning hyperparameters for different base learners
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By using different learning algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case of ensemble models, where different algorithms are used for the
    base learners, the ensemble is called a **heterogeneous ensemble method**. If
    the same algorithm is used for all the base learners on different distributions
    of the training set, the ensemble is called a **homogeneous ensemble**.
  prefs: []
  type: TYPE_NORMAL
- en: An ensemble of homogeneous models for energy prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following example, we will use the Keras API. Keras is an open source
    high-level framework for building deep neural networks. It's written on top of
    TensorFlow or Theano and uses them for its calculations behind the scenes. Keras
    can run on both CPU and GPU. The default settings of Keras are designed to deliver
    good results in most cases.
  prefs: []
  type: TYPE_NORMAL
- en: The focus of Keras is the idea of a model. Keras supports two types of models.
    The main type of model is a sequence of layers, called **sequential**. The other
    type of model in Keras is the non-sequential model, called **model**.
  prefs: []
  type: TYPE_NORMAL
- en: 'To build a sequential model, carry out the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Instantiate a sequential model using `Sequential()`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add layers to it one by one using the `Dense` class
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compile the model with the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A mandatory loss function
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A mandatory optimizer
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Optional evaluation parameters
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use data to fit the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here''s a diagrammatic flow of the preceding steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f89f4516-767a-4737-a6a6-dd1e6b30bd1d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following code block, we can see a short code example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll start by installing Keras. In order to install Keras, you will need to
    have Theano or TensorFlow installed in your system. In this example, we'll go
    with TensorFlow as the backend for Keras.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two variants of TensorFlow: a CPU version and a GPU version.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To install the current CPU-only version, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If you have to install the GPU package, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you''ve installed TensorFlow, you''ll need to install Keras using the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to upgrade your already-installed Keras library, use the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we''re done with installing the libraries, let''s import the required
    libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We set our working directory according to our requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We read our `energydata.csv` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We check whether there are any null values in our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll now build our `test` subset and train our neural network models:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Separate the `test` subset to apply the models in order to make predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the shape of the `train `and `test` subsets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Take the `test` subset and split it into target and feature variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Validate the preceding split by checking the shape of `X_test` and `Y_test`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create multiple neural network models using Keras. We use `For...Loop`
    to build multiple models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Take the summation of the predicted values and divide them by the number of iterations
    to get the average predicted values. We use the average predicted values to calculate
    the **mean-squared error** (**MSE**) for our ensemble:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here''s a diagrammatic representation of the ensemble homogeneous model workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e78d32ca-81b3-482a-9bd6-5ece7e724693.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, we assume that we have 100 train samples. We train
    100 models on our 100 train samples and apply them to our test sample. We get
    100 sets of predictions, which we ensemble by averaging whether the target variable
    is a numeric variable or whether we are calculating probabilities for a classification
    problem. In the case of class predictions, we would opt for max voting.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 1*, we separated our train and test samples. This is the same test
    sample that we used for our predictions with all the models we built in this recipe.
    In *Step 2*, we checked the shape of the `train` and `test` subsets. In *Step
    3*, we split our test subset into target and predictor variables, and then checked
    the shape again in *Step 4* to ensure we got the right split.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 5*, we used the Keras library to build our neural network models. We
    initialized two variables, `ensemble` and `frac`. We used the `ensemble` variable
    to run a `for` loop for a certain number of iterations (in our case, we set it
    to `200`). We then used the `frac` variable to assign the proportion of data we
    took for our bootstrap samples from the training subset. In our example, we set
    `frac` to `0.8`.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 5*, within the `for...loop` iteration, we built multiple neural network
    models and applied the models to our test subset to get the predictions. We created
    sequential models by passing a list of layers using the `add()` method. In the
    first layer, we specified the input dimensions using the `input_dim` argument.
    Because we have 24 input dimensions, we set `input_dim` to `24`. We also mentioned
    the `Activation` function to use in each layer by setting the `Activation` argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also set the `Activation` function through an `Activation` layer, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In this step, before we build our model, we configure the learning process using
    the `compile` method. The `compile` method takes the mandatory `loss function`,
    the mandatory `optimizer`, and the optional `metrics` as an argument.
  prefs: []
  type: TYPE_NORMAL
- en: The `optimizer` argument can take values such as **Stochastic Gradient Descent **(**SGD**), `RMSprop`,
    `Adagrad`, `Adadelta`, `Adam`, `Adamax`, or `Nadam`.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss function` can take values such as `mean_squared_error`, `mean_absolute_error`, `mean_absolute_percentage_error`, `mean_squared_logarithmic_error`, `squared_hinge`, `categorical_hinge`,
    or `binary_crossentropy`. More details are available at [https://keras.io/losses/](https://keras.io/losses/).'
  prefs: []
  type: TYPE_NORMAL
- en: We also keep adding the predictions array to an array variable, called `predictions_total`,
    using the `np.add()` method.
  prefs: []
  type: TYPE_NORMAL
- en: Once we finished all the iterations in the `for` loop in *Step 5*, we divided
    the summation of predictions by the number of iterations, which is held in the
    `ensemble` variable and set to `200`, to get the average predictions. We used
    the average predictions to calculate the MSE of the ensemble result.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have high computational requirements, you can use Google Colaboratory. Colaboratory
    is a free Jupyter notebook environment that requires no setup and runs entirely
    in the cloud. It's a free cloud service that supports free GPU. You can use Google
    Colab to build your deep learning applications using TensorFlow, Keras, PyTorch,
    and OpenCV.
  prefs: []
  type: TYPE_NORMAL
- en: Once you create your account with [https://colab.research.google.com/](https://colab.research.google.com/),
    you can log in using your credentials.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you''re logged in, you can move straight to the `File` menu to create
    your Python notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/931b17de-964b-4ef9-a5ca-fbefd119a528.png)'
  prefs: []
  type: TYPE_IMG
- en: Once you click on the File tab, you'll see New Python 3 notebook; a new notebook
    is created that supports Python 3.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can click on Untitled0.ipynb in the top-left corner to rename the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b6b14a5c-e286-4f02-93f5-b9c58805fef2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Go to Edit and then Notebook settings. A window pops up to indicate the different
    settings that you can have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c957538-b972-4980-a900-d69544ed9fda.png)'
  prefs: []
  type: TYPE_IMG
- en: Choose the **Graphics Processing Unit** (**GPU**) option as the Hardware accelerator,
    as shown in the preceding screenshot, in order to use the free GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'One neat thing about Google Colab is it can work on your own Google Drive. You
    can choose to create your own folder in your Google Drive or use the default Colab
    Notebooks folder. In order to use the default Google Colab Notebooks folder, follow
    the steps shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b21d3679-750f-4335-9a7f-74af17b93341.png)'
  prefs: []
  type: TYPE_IMG
- en: To start reading your datasets, you can store them in folders in Google Drive.
  prefs: []
  type: TYPE_NORMAL
- en: 'After you have logged in to Google Colab and created a new notebook, you will
    have to mount the drive by executing the following code in your notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'When the preceding code is run, it will ask for the authorization code to be
    entered, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4cad1459-aa85-4a26-92a2-e1ba88e9d95e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on the preceding URL to get an authorization code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e49f2e8c-79d1-4314-abf4-74b6df76ed6e.png)'
  prefs: []
  type: TYPE_IMG
- en: Paste the authorization code into the text box. You'll get a different authorization
    code each time. Upon authorization, the drive is mounted.
  prefs: []
  type: TYPE_NORMAL
- en: Once the drive is mounted, you can read `.csv` file using `pandas`, as we showed
    earlier in the chapter. The rest of the code, as shown in the *How to do it* section,
    runs as it is. If you use the GPU, you'll notice that there is a substantial increase
    in the speed of your computational performance.
  prefs: []
  type: TYPE_NORMAL
- en: In order to install additional libraries in Google Colab, you'll need to run
    the `pip install` command with a ! sign before it. For example, you can run `!pip
    install utils` to install utils in the Google Colab instance.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are various activation functions available for use with the Keras library:'
  prefs: []
  type: TYPE_NORMAL
- en: Softmax activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exponential linear unit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaled exponential linear unit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Softplus activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rectified linear unit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperbolic tangent activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sigmoid activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exponential activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information about the preceding activation functions, visit [https://keras.io/activations/](https://keras.io/activations/).
  prefs: []
  type: TYPE_NORMAL
- en: An ensemble of homogeneous models for handwritten digit classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we will use a dataset called The **Street View House Numbers**
    (**SVHN**) dataset from [http://ufldl.stanford.edu/housenumbers/](http://ufldl.stanford.edu/housenumbers/).
    The dataset is also provided in the GitHub in `.hd5f` format.
  prefs: []
  type: TYPE_NORMAL
- en: This dataset is a real-world dataset and is obtained from house numbers in Google
    Street View images.
  prefs: []
  type: TYPE_NORMAL
- en: We use Google Colab to train our models. In the first phase, we build a single
    model using Keras. In the second phase, we ensemble multiple homogeneous models
    and ensemble the results.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dataset has 60,000 house number images. Each image is labeled between 1
    and 10\. Digit 1 is labelled as 1, digit 9 is labelled as 9, and digit 0 is labelled
    as 10\. The images are 32 x 32 images centered around a single character. In some
    cases, we can see the images are visually indistinct.
  prefs: []
  type: TYPE_NORMAL
- en: 'We import the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We mount the Google Drive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we import a library called `h5py` to read the HDF5 format file and our data
    file, which is called `SVHN_single_grey.h5`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We load the training and test subsets and close the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We reshape our train and test subsets. We also change the datatype to float:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We now normalize our data by dividing it by 255.0\. This also converts the
    data type of the values to float:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We check the shape of the train and test subsets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We see that the shape of the `train` and `test` features and our target subsets
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cfdbb7d5-3001-4dc6-b046-01dde8bf8b48.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We visualize some of the images. We also print labels on top of the images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The first 10 images are shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ecbd6145-9449-460e-9e18-1cd953487adc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We now perform one-hot encoding on our target variable. We also store our `y_test`
    labels in another variable, called `y_test_actuals`, for later use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The shapes before and after one-hot encoding are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/81fa3c37-968c-4a24-8d35-a2a672133a44.png)'
  prefs: []
  type: TYPE_IMG
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll now build a single model with the Keras library:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Build a linear stack of layers with the sequential model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Compile the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Fit the model to the `train` data and validate it with the `test` data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the model''s accuracy at every epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We see the following model accuracy plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a5b170b0-ef5f-41cc-9915-340b5d345445.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Plot the loss at every epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We see the following model loss plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/480f23ae-eb8f-43f0-ab1a-8f0f7347fa55.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Reuse the code from the scikit-learn website to plot the confusion matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the confusion matrix both numerically and graphically:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The confusion matrix appears as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0744e47b-b9c7-464e-8753-4eed29c9d736.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We''ll now look at how to ensemble the results of multiple homogeneous models.
    Define a function to fit the model to the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Write a function to ensemble the predictions of all the models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '`numpy.argmax`returns indices of the max element of the array in a particular
    axis.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Write a function to evaluate the models and get the accuracy score of each
    model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Fit all the models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the accuracy score against each epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Step 1* to *Step 7*, we built a single neural network model to see how to
    use a labelled image dataset to train our model and predict the actual label for
    an unseen image.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Step 1*, we built a linear stack of layers with the sequential model using
    Keras. We defined the three layers: one input layer, one hidden layer, and one
    output layer. We provided `input_shape=1024` to the input layer since we have
    32 x 32 images. We used the relu activation function in the first and second layers.
    Because ours is a multi-class classification problem, we used softmax as the activation
    function for our output layer.'
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 2*, we compiled the model with `loss='categorical_crossentropy'` and
    `optimizer='adam'`. In *Step 3*, we fitted our model to our train data and validated
    it on our test data.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 4* and* Step 5*, we plotted the model accuracy and the loss metric
    for every epoch.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 6* and *Step 7*, we reused a `plot_confusion_matrix()` function from
    the scikit-learn website to plot our confusion matrix both numerically and visually.
  prefs: []
  type: TYPE_NORMAL
- en: 'From *Step 8* onward, we ensembled multiple models. We wrote three custom functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`train_models()`: To train and compile our model using sequential layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ensemble_predictions()`: To ensemble the predictions and find the maximum
    value across classes for all observations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`evaluate_models()`: To calculate the accuracy score for every model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In *Step 11*, we fitted all the models. We set the `no_of_models` variable to
    `50`. We trained our models in a loop by calling the `train_models()` function.
    We then passed `x_train` and `y_train` to the `train_models()` function for every
    model built at every iteration. We also called `evaluate_models()`, which returned
    the accuracy scores of each model built. We then appended all the accuracy scores.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 12*, we plotted the accuracy scores for all the models.
  prefs: []
  type: TYPE_NORMAL
