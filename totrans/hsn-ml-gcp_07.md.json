["```py\ndatalab create --no-create-repository <instance name>\n```", "```py\n%bash\npip install google-cloud\n```", "```py\nimport os\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/datalab/google-\napi.json\"\n```", "```py\nfrom google.cloud import vision\n```", "```py\nclient = vision.ImageAnnotatorClient()\n```", "```py\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\nimg=mpimg.imread('/content/datalab/11.jpg')\nplt.axis('off')\nplt.imshow(img)\n```", "```py\nresponse = client.face_detection({'source' : {'image_uri': \"gs://kish-\nbucket/11.jpg\"},})\n```", "```py\nresponse\n```", "```py\nimport matplotlib.patches as patches\nimport numpy as np\nfig,ax = plt.subplots(1)\n\n# Display the image\nax.imshow(img)\n\n# Create a Rectangle patch\nx_width = np.abs(response.face_annotations[0].bounding_poly.vertices[1].x-\n  response.face_annotations[0].bounding_poly.vertices[0].x)\ny_height = np.abs(response.face_annotations[0].bounding_poly.vertices[1].y-\n  response.face_annotations[0].bounding_poly.vertices[3].y)\n\nrect =\n patches.Rectangle((response.face_annotations[0].bounding_poly.vertices[0].x,\n response.face_annotations[0].bounding_poly.vertices[0].y),\n                         x_width,y_height,linewidth=5,edgecolor='y',facecolor='none')\n\n# Add the patch to the Axes\nax.add_patch(rect)\nplt.axis('off')\nplt.show()\n```", "```py\nresponse_label = client.label_detection({'source' : {'image_uri': \"gs://kish-\nbucket/11.jpg\"},})\n```", "```py\nresponse_text = client.text_detection({'source' : {'image_uri': \"gs://kish-\nbucket/11.jpg\"},})\n```", "```py\nresponse = client.logo_detection({'source' : {'image_uri':\n\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b3/Wikipedia-logo-v2-\nen.svg/135px-Wikipedia-logo-v2-en.svg.png\"},})\n```", "```py\nresponse = client.landmark_detection({'source' : {'image_uri': \n \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1d/\n  Taj_Mahal_%28Edited%29.jpeg/250px-Taj_Mahal_%28Edited%29.jpeg\"},})\n```", "```py\nimport os\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/datalab/google-api.json\"\n```", "```py\nfrom google.cloud import translate\n```", "```py\nclient = translate.Client()\n```", "```py\nfrom google.cloud import language\n```", "```py\nclient = language.LanguageServiceClient()\n```", "```py\ntext=\"this is a good text\"\nfrom google.cloud.language_v1 import types\ndocument = types.Document(\n        content=text,\n        type='PLAIN_TEXT')\nsentiment = client.analyze_sentiment(document).document_sentiment\nsentiment.score\n```", "```py\ndocument = language.types.Document(content='Michelangelo Caravaggio, Italian    painter, is known for \"The Calling of Saint Matthew\".'\n                                   ,type='PLAIN_TEXT') \nresponse = client.analyze_entities(document=document)\n\nfor entity in response.entities:\n  print('name: {0}'.format(entity.name)) \n```", "```py\ntokens = client.analyze_syntax(document).tokens\ntokens[0].text.content\n# The preceding output is u'Michelangelo'\n```", "```py\npos_tag = ('UNKNOWN', 'ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM','PRON', 'PRT', 'PUNCT', 'VERB', 'X', 'AFFIX')\nfor token in tokens:print(u'{}: {}'.format(pos_tag[token.part_of_speech.tag],\n                               token.text.content))\n```", "```py\nfrom google.cloud import speech\nimport os\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/datalab/google-api.json\"\nfrom google.cloud.speech import enums\nfrom google.cloud.speech import types\n```", "```py\nclient = speech.SpeechClient()\n```", "```py\naudio = types.RecognitionAudio(uri='gs://kish-bucket/how_are_you.flac')\n```", "```py\nconfig = types.RecognitionConfig(\nencoding=enums.RecognitionConfig.AudioEncoding.FLAC,\nsample_rate_hertz=16000,\nlanguage_code='en-US')\n```", "```py\nresponse = client.recognize(config, audio)\n```", "```py\nfor result in response.results: \n  print(result)\n```", "```py\noperation = client.long_running_recognize(config, audio)\n```", "```py\nresponse = operation.result(timeout=90)\n```", "```py\nfrom google.cloud import videointelligence\nimport os\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/datalab/google-api.json\"\nfrom google.cloud.speech import enums\nfrom google.cloud.speech import types\n```", "```py\nfeatures = [videointelligence.enums.Feature.LABEL_DETECTION]\n```", "```py\nmode = videointelligence.enums.LabelDetectionMode.SHOT_AND_FRAME_MODE\nconfig = videointelligence.types.LabelDetectionConfig(\n    label_detection_mode=mode)\ncontext = videointelligence.types.VideoContext(\n    label_detection_config=config)\n```", "```py\npath=\"gs://kish-bucket/Hemanvi_video.mp4\"\noperation = video_client.annotate_video(\n        path, features=features, video_context=context)\n```", "```py\nresult = operation.result(timeout=90)\n```", "```py\nsegment_labels = result.annotation_results[0].segment_label_annotations\nfor i, segment_label in enumerate(segment_labels):\n    print('Video label description: {}'.format(\n        segment_label.entity.description))\n    for category_entity in segment_label.category_entities:\n        print('\\tLabel category description: {}'.format(\n            category_entity.description))\n\n    for i, segment in enumerate(segment_label.segments):\n        start_time = (segment.segment.start_time_offset.seconds +\n                      segment.segment.start_time_offset.nanos / 1e9)\n        end_time = (segment.segment.end_time_offset.seconds +\n                    segment.segment.end_time_offset.nanos / 1e9)\n        positions = '{}s to {}s'.format(start_time, end_time)\n        confidence = segment.confidence\n        print('\\tSegment {}: {}'.format(i, positions))\n        print('\\tConfidence: {}'.format(confidence))\n    print('\\n')\n```", "```py\nshot_labels = result.annotation_results[0].shot_label_annotations\nfor i, shot_label in enumerate(shot_labels):\n    print('Shot label description: {}'.format(\n        shot_label.entity.description))\n    for category_entity in shot_label.category_entities:\n        print('\\tLabel category description: {}'.format(\n            category_entity.description))\n\n    for i, shot in enumerate(shot_label.segments):\n        start_time = (shot.segment.start_time_offset.seconds +\n                      shot.segment.start_time_offset.nanos / 1e9)\n        end_time = (shot.segment.end_time_offset.seconds +\n                    shot.segment.end_time_offset.nanos / 1e9)\n        positions = '{}s to {}s'.format(start_time, end_time)\n        confidence = shot.confidence\n        print('\\tSegment {}: {}'.format(i, positions))\n        print('\\tConfidence: {}'.format(confidence))\n    print('\\n')\n```", "```py\nframe_labels = result.annotation_results[0].frame_label_annotations\nfor i, frame_label in enumerate(frame_labels):\n    print('Frame label description: {}'.format(\n        frame_label.entity.description))\n    for category_entity in frame_label.category_entities:\n        print('\\tLabel category description: {}'.format(\n            category_entity.description))\n\n    # Each frame_label_annotation has many frames,\n    # here we print information only about the first frame.\n    frame = frame_label.frames[0]\n    time_offset = (frame.time_offset.seconds +\n                   frame.time_offset.nanos / 1e9)\n    print('\\tFirst frame time offset: {}s'.format(time_offset))\n    print('\\tFirst frame confidence: {}'.format(frame.confidence))\n    print('\\n')\n```"]