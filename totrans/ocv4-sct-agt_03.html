<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Searching for Luxury Accommodations Worldwide</h1>
                </header>
            
            <article>
                
<p>Today the bridal suite, tomorrow a prison. A secret agent's sleeping arrangements are horribly unpredictable.</p>
<p>Each day, someone in MI6 gets the job of booking a stellar hotel room and, conversely, some evil henchman has to pick a warehouse or dilapidated apartment, plus a lamp, a chair, and implements of bondage. For mini missions or brief beatings, it is tolerable to leave the choice of venue to a fallible human being. However, for long-term rentals or acquisitions, would it not be wiser to develop a specialized search engine that takes the legwork and the guesswork out of the equation?</p>
<p>With this motivation, we are going to develop a desktop app called <kbd>Luxocator: The Luxury Locator</kbd>. It is a search engine that finds images on the web by keyword search and classifies each image as a luxury, interior scene; luxury, exterior scene; Stalinist, interior scene; or Stalinist, exterior scene, according to certain visual cues in the image.</p>
<p>Particularly, our classifier relies on comparing statistical distributions of color in different images or sets of images. This topic is called <strong>color histogram analysis</strong>. We will learn how to efficiently store and process our statistical model and how to redistribute it, along with our code, in an application bundle. Specifically, this chapter covers the following programming topics:</p>
<ul>
<li>Using OpenCV's Python bindings, along with the NumPy and SciPy libraries, to classify images based on color histogram analysis</li>
<li>Using the Bing Image Search API to acquire images from a web search</li>
<li>Building a GUI application with <kbd>wxPython</kbd></li>
<li>Using PyInstaller to bundle a Python application as an executable that can run on other systems</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>This chapter's project has the following software dependencies:</p>
<ul>
<li><strong>A Python environment with the following modules</strong>: OpenCV, NumPy, SciPy, Requests, wxPython, and optionally PyInstaller</li>
</ul>
<p>Setup instructions are covered in <a href="e3ac8266-975b-43ca-8221-482a15eb0e05.xhtml">Chapter 1</a>, <em>Preparing for the Mission</em>. Refer to the setup instructions for any version requirements. Basic instructions for running Python code are covered in <a href="c44b1aaa-fe12-4054-85fb-37d584f15d3b.xhtml">Appendix C</a>, <em>Running with Snakes (or, First Steps with Python)</em>.</p>
<p class="mce-root"><span>The completed project for this chapter can be found in this book's GitHub repository, </span><a href="https://github.com/PacktPublishing/OpenCV-4-for-Secret-Agents-Second-Edition">https://github.com/PacktPublishing/OpenCV-4-for-Secret-Agents-Second-Edition</a><span>, in the </span><kbd>Chapter002</kbd><span> folder</span><span>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Planning the Luxocator app</h1>
                </header>
            
            <article>
                
<p>This chapter uses Python. Being a high-level, interpreted language with great third-party libraries for numeric and scientific computing, Python lets us focus on the functionality of the system rather than implementing subsystem details. For our first project, such a high-level perspective is precisely what we need.</p>
<p>Let's look at an overview of Luxocator's functionality and our choice of Python libraries that support this functionality. Like many computer vision applications, Luxocator has six basic steps:</p>
<ol>
<li><strong>Acquire a static set of reference images</strong>: For Luxocator, we (the developers) choose certain images that we deem to be luxury indoor scenes, other images that we consider Stalinist indoor scenes, and so on. We load these images into memory.</li>
<li><strong>Train a model based on the reference images</strong>: For Luxocator, our model describes each image in terms of its normalized color histogram—that is, the distribution of colors across the image's pixels. We use OpenCV and NumPy to perform the calculations.</li>
<li><strong>Store the results of the training</strong>: For Luxocator, we use SciPy to compress the reference histograms and write/read them to/from disk.</li>
<li><strong>Acquire a dynamic set of query images</strong>: For Luxocator, we acquire query images using the Bing Search API through a Python wrapper. We also use the Requests library to download the full-resolution images.</li>
</ol>
<p class="mce-root"/>
<ol start="5">
<li><strong>Compare the query images to the reference images</strong>: For Luxocator, we compare each query image and each reference image based on the intersection of their histograms. We then make a classification based on the average results of these comparisons. We use NumPy to perform the calculations.</li>
<li><strong>Present the results of the comparison</strong>: For Luxocator, we provide a GUI for initiating a search and navigating the results. <span>This cross-platform GUI is developed in <kbd>wxPython</kbd>. </span>A classification label, such as <span class="packt_screen">Stalinist</span>, <span class="packt_screen">exterior</span>, is shown below each image. See the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1b03f4f2-86f3-4b97-845a-1d148dba3fe2.png"/></p>
<p>Optionally, we use PyInstaller to build Luxocator so that it can be deployed to users who do not have Python or the aforementioned libraries. <span>However, remember that you might need to do extra troubleshooting of your own t</span>o make PyInstaller <span>work in some environments, including Raspberry Pi or other ARM devices.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating, comparing, and storing histograms</h1>
                </header>
            
            <article>
                
<div class="packt_quote CDPAlignLeft CDPAlign">"A grey-green color that often finds itself on the walls of public institutions-e.g., hospitals, schools, government buildings—and, where appropriated, on sundry supplies and equipment."<br/>
<span>                                                          – "institutional green", Segen's Medical Dictionary (2012)</span></div>
<p>I hesitate to make sweeping statements about the ideal color of paint on a wall. It depends. I have found solace in many walls of many colors. My mother is a painter and I like paint in general.</p>
<p>But not all color is paint. Some color is dirt. Some color is concrete or marble, plywood or mahogany. Some color is the sky through big windows, the ocean, the golf course, the swimming pool, or Jacuzzi. Some color is discarded plastics and beer bottles, baked food on the stove, or perished vermin. Some color is unknown. Maybe the paint camouflages the dirt.</p>
<p>A typical camera can capture at least 16.7 million (<em>256 * 256 * 256</em>) distinct colors. For any given image, we can count the number of pixels of each color. This set of counts is called the <strong>color histogram</strong> of the image. Typically, most entries in the histogram will be zero because most scenes are not polychromatic (many-colored).</p>
<p>We can normalize the histogram by dividing the color counts by the total number of pixels. Since the number of pixels is factored out, normalized histograms are comparable even if the original images have different resolutions.</p>
<p>Given a pair of normalized histograms, we can measure the histograms' similarity on a scale of zero to one. One measure of similarity is called the <strong>intersection</strong> of the histograms. It is computed as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/b373e041-8828-452c-9593-b94fe12d709a.png" style="width:18.25em;height:2.75em;"/></p>
<p>Here is the equivalent Python code (which we will optimize later):</p>
<pre>def intersection(hist0, hist1):<br/>    assert len(hist0) == len(hist1),<br/>            'Histogram lengths are mismatched'<br/>    result = 0<br/>    for i in range(len(hist0)):<br/>        result += min(hist0[i], hist1[i])<br/>    return result </pre>
<p>For example, suppose that in one image, <kbd>50%</kbd> of the pixels are black and <kbd>50%</kbd> are white. In another image, <kbd>100%</kbd> of the pixels are black. The similarity is as follows:</p>
<pre>min(50%, 100%) + min(50%, 0%) = 50% = 0.5</pre>
<p><span>Here, a similarity of one does not mean that the images are identical; it means that their normalized histograms are identical. Relative to the first image, the second image could be a different size, could be flipped, or could even contain the same pixel values in a randomly different order.</span></p>
<p><span>Conversely, a similarity of zero does not mean that the images look completely different to a layperson; it just means that they have no color values in common. For example, an image that is all black and another image that is all charcoal-gray have histograms with a similarity of zero by our definition.</span></p>
<p>For the purpose of classifying images, we want to find the average similarity between a query histogram and a set of multiple reference histograms. A single reference histogram (and a single reference image) would be far too specific for a broad classification such as <strong>Lu</strong><strong>xury, indoor</strong>.</p>
<div class="packt_infobox">Although we focus on one approach to comparing histograms, there are many alternatives. For a discussion of several algorithms and their implementations in Python, see this blog post by Adrian Rosebrock at <a href="http://www.pyimagesearch.com/2014/07/14/3-ways-compare-histograms-using-opencv-python/"><span class="URLPACKT">http://www.pyimagesearch.com/2014/07/14/3-ways-compare-histograms-using-opencv-python/</span></a>.</div>
<p>Let's write a class called <kbd>HistogramClassifier</kbd>, which creates and stores sets of references histograms and finds the average similarity between a query histogram and each set of reference histograms. To support this functionality, we will use OpenCV, NumPy, and SciPy. Create a file called <kbd>HistogramClassifier.py</kbd> and add the following shebang line (path to the Python interpreter) and <kbd>import</kbd> statements at the top:</p>
<pre>#!/usr/bin/env python<br/><br/><br/>import numpy # Hint to PyInstaller<br/>import cv2<br/>import scipy.io<br/>import scipy.sparse</pre>
<div class="packt_infobox">Some versions of PyInstaller work better if we explicitly import the <kbd>numpy</kbd> module before the <kbd>cv2</kbd> module. Remember that OpenCV's Python bindings depend on NumPy. Also, note that the OpenCV Python module is called <kbd>cv2</kbd>, even though we are using OpenCV 4. The name <kbd>cv2</kbd> comes from a historical distinction between the parts of OpenCV that had an underlying C++ implementation (called <kbd>cv2</kbd>) and the parts that an older, underlying C implementation (called <kbd>cv</kbd>). As of OpenCV 4, everything is <kbd>cv2</kbd>.</div>
<p>An instance of <kbd>HistogramClassifier</kbd> stores several variables. A public Boolean called <kbd>verbose</kbd> controls the level of logging. A public float called <kbd>minimumSimilarityForPositiveLabel</kbd> defines a similarity threshold—if all the average similarities fall below this value, then the query image is given an <kbd>'Unknown'</kbd> classification. Several variables store values related to the color model. We assume that our images have three color channels with 8 bits (256 possible values) per channel. Finally, and most importantly, a dictionary called <kbd>_references</kbd> maps string keys such as <kbd>'Luxury, interior'</kbd> to lists of reference histograms. Let's declare the variables in the <kbd>HistogramClassifier</kbd> class's <kbd>__init__</kbd> method, as follows:</p>
<pre>class HistogramClassifier(object):<br/><br/>    def __init__(self):<br/><br/>        self.verbose = False<br/>        self.minimumSimilarityForPositiveLabel = 0.075<br/><br/>        self._channels = range(3)<br/>        self._histSize = [256] * 3<br/>        self._ranges = [0, 255] * 3<br/>        self._references = {}</pre>
<div class="packt_infobox">By convention, in a Python class, a variable or method name is prefixed with an underscore if the variable or method is meant to be protected (accessed only within the class and its subclasseses). However, this level of protection is not actually enforced. Most of our member variables and methods in this book are marked as protected, but a few are public. Python supports private variables and methods (denoted by a double-underscore prefix) that are meant to be inaccessible, even to subclasses. However, we avoid private variables and methods in this book because Python classes should typically be highly extensible.</div>
<p><kbd>HistogramClassifier</kbd> has a method, <kbd>_createNormalizedHist</kbd>, which takes two arguments—an image and a Boolean indicating whether to store the resulting histogram in a <strong>sparse</strong> (compressed) format. The histogram is computed using an OpenCV function, <kbd>cv2.calcHist</kbd>. As arguments, it takes the image, the number of channels, the histogram size (that is, the dimensions of the color model), and the range of each color channel. We flatten the resulting histogram into a one-dimensional format that uses memory more efficiently. Then, optionally, we convert the histogram into a sparse format using a SciPy function called <kbd>scipy.sparse.csc_matrix</kbd>.</p>
<div class="packt_infobox">
<p><span>A sparse matrix uses a form of compression that relies on a default value, normally <kbd>0</kbd>. That is to say, we do not bother storing all the zeroes individually; instead, we note the ranges that are full of zeroes. For histograms, this is an important optimization because in a typical image, most of the possible colors are absent. So, most of the histogram values are <kbd>0</kbd>.</span></p>
<p><span>Compared to an uncompressed format, a sparse format offers better memory efficiency but worse computational efficiency. The same trade-off applies to compressed formats in general.</span></p>
</div>
<p>Here is the implementation of <kbd>_createNormalizedHist</kbd>:</p>
<pre>    def _createNormalizedHist(self, image, sparse):<br/>        # Create the histogram.<br/>        hist = cv2.calcHist([image], self._channels, None,<br/>                            self._histSize, self._ranges)<br/>        # Normalize the histogram.<br/>        hist[:] = hist * (1.0 / numpy.sum(hist))<br/>        # Convert the histogram to one column for efficient storage.<br/>        hist = hist.reshape(16777216, 1)  # 16777216 == pow(2, 24)<br/>        if sparse:<br/>            # Convert the histogram to a sparse matrix.<br/>            hist = scipy.sparse.csc_matrix(hist)<br/>        return hist</pre>
<p>A public method, <kbd>addReference</kbd>, accepts two arguments—an image and a label (the label is a string describing the classification). We pass the image to <kbd>_createNormalizedHist</kbd> in order to create a normalized histogram in a sparse format. For a reference histogram, the sparse format is more appropriate because we want to keep many reference histograms in memory for the entire duration of a classification session. After creating the histogram, we add it to a list in <kbd>_references</kbd>, using the label as the key. Here is the implementation of <kbd>addReference</kbd>:</p>
<pre>    def addReference(self, image, label):<br/>        hist = self._createNormalizedHist(image, True)<br/>        if label not in self._references:<br/>            self._references[label] = [hist]<br/>        else:<br/>            self._references[label] += [hist]</pre>
<p>For the purposes of Luxocator, reference images come from files on disk. Let's give <kbd>HistogramClassifier</kbd> a public method, <kbd>addReferenceFromFile</kbd>, which accepts a file path instead of directly accepting an image. It also accepts a label. We load the image from file using an OpenCV method called <kbd>cv2.imread</kbd>, which accepts a path and a color format. Based on our earlier assumption about having three color channels, we always want to load images in color, not grayscale. This option is represented by the <kbd>cv2.IMREAD_COLOR</kbd> value. Having loaded the image, we pass it and the label to <kbd>addReference</kbd>. The implementation of <kbd>addReferenceFromFile</kbd> is as follows:</p>
<pre>    def addReferenceFromFile(self, path, label):<br/>        image = cv2.imread(path, cv2.IMREAD_COLOR)<br/>        self.addReference(image, label)</pre>
<p>Now, we arrive at the crux of the matter—the <kbd>classify</kbd> public method, which accepts a query image, as well as an optional string to identify the image in log output. For each set of reference histograms, we compute the average similarity to the query histogram. If all similarity values fall below <kbd>minimumSimilarityForPositiveLabel</kbd>, we return the <kbd>'Unknown'</kbd> <span>label.</span> Otherwise, we return the label of the most similar set of reference histograms. If <kbd>verbose</kbd> is <kbd>True</kbd>, we also log all the labels and their respective average similarities. Here is the method's implementation:</p>
<pre>    def classify(self, queryImage, queryImageName=None):<br/>        queryHist = self._createNormalizedHist(queryImage, False)<br/>        bestLabel = 'Unknown'<br/>        bestSimilarity = self.minimumSimilarityForPositiveLabel<br/>        if self.verbose:<br/>            print('================================================')<br/>            if queryImageName is not None:<br/>                print('Query image:')<br/>                print(' %s' % queryImageName)<br/>            print('Mean similarity to reference images by label:')<br/>        for label, referenceHists in self._references.items():<br/>            similarity = 0.0<br/>            for referenceHist in referenceHists:<br/>                similarity += cv2.compareHist(<br/>                        referenceHist.todense(), queryHist,<br/>                        cv2.HISTCMP_INTERSECT)<br/>            similarity /= len(referenceHists)<br/>            if self.verbose:<br/>                print(' %8f %s' % (similarity, label))<br/>            if similarity &gt; bestSimilarity:<br/>                bestLabel = label<br/>                bestSimilarity = similarity<br/>        if self.verbose:<br/>            print('================================================')<br/>        return bestLabel</pre>
<p>Note the use of the <kbd>todense</kbd> method to decompress a sparse matrix.</p>
<p>We also provide a public method, <kbd>classifyFromFile</kbd>, which accepts a file path instead of directly accepting an image. Here is the implementation:</p>
<pre>    def classifyFromFile(self, path, queryImageName=None):<br/>        if queryImageName is None:<br/>            queryImageName = path<br/>        queryImage = cv2.imread(path, cv2.IMREAD_COLOR)<br/>        return self.classify(queryImage, queryImageName)</pre>
<p>Computing all our reference histograms will take a bit of time. We do not want to recompute them every time we run Luxocator. So, we need to serialize and deserialize (save and load) the histograms to/from disk. For this purpose, SciPy provides two functions, <kbd>scipy.io.savemat</kbd> and <kbd>scipy.io.loadmat</kbd>. They accept a file and various optional arguments.</p>
<p>We can implement a <kbd>serialize</kbd> method with optional compression, as follows:</p>
<pre>    def serialize(self, path, compressed=False):<br/>        file = open(path, 'wb')<br/>        scipy.io.savemat(<br/>            file, self._references, do_compression=compressed)</pre>
<p>When deserializing, we get a dictionary from <kbd>scipy.io.loadmat</kbd>. However, this dictionary contains more than our original <kbd>_references</kbd> dictionary. It also contains some serialization metadata and some serialization metadata, and some additional arrays that wrap the lists that were originally in <kbd>_references</kbd>. We strip out these unwanted, added contents and store the result back in <kbd>_references</kbd>. The implementation is as follows:</p>
<pre>    def deserialize(self, path):<br/>        file = open(path, 'rb')<br/>        self._references = scipy.io.loadmat(file)<br/>        for key in list(self._references.keys()):<br/>            value = self._references[key]<br/>            if not isinstance(value, numpy.ndarray):<br/>                # This entry is serialization metadata so delete it.<br/>                del self._references[key]<br/>                continue<br/>            # The serializer wraps the data in an extra array.<br/>            # Unwrap the data.<br/>            self._references[key] = value[0]</pre>
<p>That is our classifier. Next, we will test our classifier by feeding it some reference images and a query image.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the classifier with reference images</h1>
                </header>
            
            <article>
                
<div class="packt_quote">"Can you identify this coastline? Given time, yes."<br/>
<span>                                      – P</span>hoto caption, Dante Stella (<a href="http://www.dantestella.com/technical/hex352.html"><span class="URLPACKT">http://www.dantestella.com/technical/hex352.html</span></a>)</div>
<p>A small selection of reference images is included in this book's GitHub repository in a folder called <kbd>Chapter002/images</kbd>. Feel free to experiment with the classifier by adding more reference images, since a larger set may yield more reliable results. Bear in mind that our classifier relies on average similarity, so the more times you include a given color scheme in the reference images, the more heavily you are weighting the classifier in favor of that color scheme.</p>
<p>At the end of <kbd>HistogramClassifier.py</kbd>, let's add a <kbd>main</kbd> method to train and serialize a classifier using our reference images. We will also run the classifier on a couple of the images as a test. Here is a partial implementation:</p>
<pre>def main():<br/>    classifier = HistogramClassifier()<br/>    classifier.verbose = True<br/>    <br/>    # 'Stalinist, interior' reference images<br/>    classifier.addReferenceFromFile(<br/>            'images/communal_apartments_01.jpg',<br/>            'Stalinist, interior')<br/>    # ...<br/>    # Other reference images are omitted for brevity.<br/>    # See the GitHub repository for the full implementation.<br/>    # ...<br/>    <br/>    classifier.serialize('classifier.mat')<br/>    classifier.deserialize('classifier.mat')<br/>    classifier.classifyFromFile('images/dubai_damac_heights.jpg')<br/>    classifier.classifyFromFile('images/communal_apartments_01.jpg')<br/><br/>if __name__ == '__main__':<br/>    main()</pre>
<p>Depending on the number of reference images, this method may take several minutes (or even longer) to run. Fortunately, since we are serializing the trained classifier, we will not have to run such a method every time we open our main application. Instead, we will simply deserialize the trained classifier from file, as we will see later in this chapter in the <em>Integrating everything into the GUI</em> section.</p>
<div class="packt_tip">For a large number of training images, you might wish to modify the <kbd>main</kbd> function of <kbd>HistogramClassifier.py</kbd> to use all images in a specified folder. (F<span>or examples of iteration over all images in a folder, </span>refer to the <kbd>describe.py</kbd> file in the code for <a href="49c9a5fb-89a3-4c0d-bbee-021d2618168c.xhtml">Chapter 3</a><em>, Training a Smart Alarm to Recognize the Villain and His Cat</em>.) However, for a small number of training images, I find it more convenient to specify a list of images in code so that we can comment and uncomment individual images to see the effect on training.</div>
<p>Next, let's consider how our main application will acquire query images.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Acquiring images from the web</h1>
                </header>
            
            <article>
                
<p>Our query images will come from a web search. Before we start implementing the search functionality, let's write some helper functions that let us fetch images through the <kbd>Requests</kbd> library and convert them into an OpenCV-compatible format. Because this functionality is highly reusable, we will put it in a module of static utility functions. Let's create a file called <kbd>RequestsUtils.py</kbd> and import OpenCV, NumPy, and Requests, as follows:</p>
<pre>#!/usr/bin/env python<br/><br/><br/>import numpy # Hint to PyInstaller<br/>import cv2<br/>import requests<br/>import sys</pre>
<p>As a global variable, let's store <kbd>HEADERS</kbd>, a dictionary of headers that we will use when making web requests. Some servers reject requests that appear to come from a bot. To improve the chance of our requests being accepted, let's set the <kbd>'User-Agent'</kbd> header to a value that mimics a web browser, as follows:</p>
<pre># Spoof a browser's User-Agent string.<br/># Otherwise, some sites will reject us as a bot.<br/>HEADERS = {<br/>    'User-Agent': 'Mozilla/5.0 ' \<br/>                  '(Macintosh; Intel Mac OS X 10.9; rv:25.0) ' \<br/>                  'Gecko/20100101 Firefox/25.0'<br/>}</pre>
<p>Whenever we receive a response to a web request, we want to check whether the status code is <kbd>200</kbd> OK. This is only a cursory test of whether the response is valid, but it is a good enough test for our purposes. We implement this test in the following method, <kbd>validateResponse</kbd>, which returns <kbd>True</kbd> if the response is deemed valid; otherwise, it logs an error message and returns <kbd>False</kbd>:</p>
<pre>def validateResponse(response):<br/>    statusCode = response.status_code<br/>    if statusCode == 200:<br/>        return True<br/>    url = response.request.url<br/>    sys.stderr.write(<br/>            'Received unexpected status code (%d) when requesting %s\n' % \<br/>            (statusCode, url))<br/>    return False</pre>
<p>With the help of <kbd>HEADERS</kbd> and <kbd>validateResponse</kbd>, we can try to get an image from a URL and return that image in an OpenCV-compatible format (failing that, we return <kbd>None</kbd>). As an intermediate step, we read raw data from a web response into a NumPy array using a function called <kbd>numpy.fromstring</kbd>. We then interpret this data as an image using a function called <kbd>cv2.imdecode</kbd>. Here is our implementation, a function called <kbd>cvImageFromUrl</kbd> that accepts a URL as an argument:</p>
<pre>def cvImageFromUrl(url):<br/>    response = requests.get(url, headers=HEADERS)<br/>    if not validateResponse(response):<br/>        return None<br/>    imageData = numpy.fromstring(response.content, numpy.uint8)<br/>    image = cv2.imdecode(imageData, cv2.IMREAD_COLOR)<br/>    if image is None:<br/>        sys.stderr.write(<br/>                'Failed to decode image from content of %s\n' % url)<br/>    return image</pre>
<p>To test these two functions, let's give <kbd>RequestsUtils.py</kbd> a <kbd>main</kbd> function that downloads an image from the web, converts it into an OpenCV-compatible format, and writes it to disk using an OpenCV function called <kbd>imwrite</kbd>. Here is our implementation:</p>
<pre>def main():<br/>    image = cvImageFromUrl('http://nummist.com/images/ceiling.gaze.jpg')<br/>    if image is not None:<br/>        cv2.imwrite('image.png', image)<br/><br/>if __name__ == '__main__':<br/>    main()</pre>
<p>To confirm that everything worked, open <kbd>image.png</kbd> (which should be in the same directory as <kbd>RequestsUtils.py</kbd>) and compare it to the online image, which you can view in a web browser at <a href="http://nummist.com/images/ceiling.gaze.jpg"><span class="URLPACKT">http://nummist.com/images/ceiling.gaze.jpg</span></a>.</p>
<div class="packt_infobox">Although we are putting a simple test of our <kbd>RequestUtils</kbd> module in a <kbd>main</kbd> function, a more sophisticated and maintainable approach to writing tests in Python is to use the classes in the <kbd>unittest</kbd> module of the standard library. For more information, refer to the official tutorial at <a href="https://docs.python.org/3/library/unittest.html"><span class="URLPACKT">https://docs.python.org/3/library/unittest.html</span></a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Acquiring images from Bing Image Search</h1>
                </header>
            
            <article>
                
<p>Microsoft's search engine, Bing, has an API that enables us to send queries and receive results in our own application. For a limited number of queries per month, the Bing Search API is free to use (currently, the limit is three thousand queries per month and three queries per second). However, we must register for it by performing the following steps:</p>
<ol>
<li>Go to <a href="https://azure.microsoft.com/"><span class="URLPACKT">https://azure.microsoft.com/</span></a> and log in. You will need to create a Microsoft account if you do not already have one.</li>
<li>Go to <a href="https://azure.microsoft.com/en-us/services/cognitive-services/bing-image-search-api/">https://azure.microsoft.com/en-us/services/cognitive-services/bing-image-search-api/</a>. <span>Click the</span> <span class="packt_screen">Try Bing Image Search</span><span> button.</span></li>
<li>Next to the <span class="packt_screen">Guest</span> option, click the <span class="packt_screen">Get started</span> button to start a free seven-day trial. After you have started your trial, g<span>o to <a href="https://azure.microsoft.com/en-us/try/cognitive-services/">https://azure.microsoft.com/en-us/try/cognitive-services/</a></span>. <span>Select the <span class="packt_screen">Search APIs</span> tab. Under the <span class="packt_screen">Bing Image Search APIs v7</span> section, find the 32-character API key (you might find two keys labeled <span class="packt_screen">Key 1</span> and <span class="packt_screen">Key 2</span> . Either of these is fine). Copy the key and save it in a safe place. We will need to use it later to associate our Bing session with our Microsoft Account.</span></li>
<li><span>As an alternative to step three, or after your seven-day trial expires, you can create a free account. Next to the</span><span> </span><span class="packt_screen">Free Azure account</span><span> option</span><span>, click the </span><span class="packt_screen">Sign up</span><span> button</span><span> to register for a free account with limited uses per month (of course, if you decide to use Luxocator obsessively, to the exclusion of normal activities, you can always upgrade to a paid account later). Even though the account is free, the registration process requires you to provide a phone number and a credit card in order to verify your identity. </span>Once you have completed the registration process, click the <span class="packt_screen">Portal</span> tab to go the Microsoft Azure control panel. Click <span class="packt_screen">Cognitive Services</span>, then <span class="packt_screen">Add</span>, then <span class="packt_screen">Bing Search v7</span>, and then <span class="packt_screen">Create</span>. Fill out the <strong><span class="packt_screen">Create</span></strong> dialog by following the example in the following screenshot. Click the dialog's <span class="packt_screen">Create</span> button. Click the <span class="packt_screen">Go to resource</span> button. Click <span class="packt_screen">Keys</span>. Find the 32-character API key (y<span>ou might see two keys labeled </span><span class="packt_screen">Key 1</span><span> and </span><span class="packt_screen">Key 2</span><span>. Either of these is fine).</span> <span>Copy the key and save it in a safe place:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/19793bb0-a3ea-4938-bd28-9b2e98337f52.png" style="width:23.17em;height:56.50em;"/></p>
<ol start="5">
<li>Create an environment variable named<span> </span><kbd>BING_SEARCH_KEY</kbd>. Set its value equal to the API key that we created in step three or four (l<span>ater, in our code, we will access the value of this environment variable in order to associate our Bing search session with our API key). Depending on your operating system, there are many different ways to create an environment variable. On Windows, you may want to use the Control Panel to add a user environment variable. On Unix-like systems, you may want to add a definition of the environment variable by editing the user's login script, which is called <kbd>~/.profile</kbd> on Mac, Ubuntu, and many other systems. After you have created the environment variable, reboot (or log out and log back in).</span></li>
</ol>
<p>The Bing Search API and several other Microsoft APIs have a third-party Python wrapper called <kbd>py-ms-cognitive</kbd>. We can install it using Python's package manager, <kbd>pip</kbd>. Open a Terminal (on Unix-like systems) or <span>Command Prompt (on Windows)</span> and run the following command:</p>
<pre><strong>$</strong> <strong>pip install --user py-ms-cognitive</strong></pre>
<p>Building atop <kbd>py-ms-cognitive</kbd>, we want a high-level interface for submitting a query string and navigating through a resulting list of images, which should be in an OpenCV-compatible format. We will make a class, <kbd>ImageSearchSession</kbd>, which offers such an interface. First, let's create a file, <kbd>ImageSearchSession.py</kbd>, and add the following <kbd>import</kbd> statements at the top:</p>
<pre>#!/usr/bin/env python<br/><br/><br/>from py_ms_cognitive import PyMsCognitiveImageSearch<br/>PyMsCognitiveImageSearch.SEARCH_IMAGE_BASE = \<br/>        'https://api.cognitive.microsoft.com/bing/v7.0/images/search'<br/><br/>import numpy # Hint to PyInstaller<br/>import cv2<br/>import os<br/>import pprint<br/>import sys<br/><br/>import RequestsUtils</pre>
<p>Note that we are modifying one of the <kbd>py_ms_cognitive</kbd> Python wrapper's static variables, <kbd>PyMsCognitiveImageSearch.SEARCH_IMAGE_BASE</kbd>. We do this because, by default, <kbd>py_ms_cognitive</kbd> uses an outdated base URL for the Bing Search API endpoint.</p>
<p>For <kbd>py_ms_cognitive</kbd>, we are using OpenCV, pretty-print (for logging JSON results from the search), system libraries, and our networking utility functions.</p>
<p><span>Like <kbd>HistogramClassifier</kbd>, <kbd>ImageSearchSession</kbd> has a public Boolean called <kbd>verbose</kbd> to control the level of logging. Moreover,</span> <kbd>ImageSearchSession</kbd> has member variables to store the current query, metadata about the current image results, and metadata to help us navigate to the previous and next results. We can initialize these variables like so:</p>
<pre>class ImageSearchSession(object):<br/><br/>    def __init__(self):<br/>        self.verbose = False<br/>        <br/>        self._query = ''<br/>        self._results = []<br/>        self._offset = 0<br/>        self._numResultsRequested = 0<br/>        self._numResultsReceived = 0<br/>        self._numResultsAvailable = 0</pre>
<p>We provide getters for many of the member variables, as follows:</p>
<pre>    @property<br/>    def query(self):<br/>        return self._query<br/><br/>    @property<br/>    def offset(self):<br/>        return self._offset<br/><br/>    @property<br/>    def numResultsRequested(self):<br/>        return self._numResultsRequested<br/><br/>    @property<br/>    def numResultsReceived(self):<br/>        return self._numResultsReceived<br/><br/>    @property<br/>    def numResultsAvailable(self):<br/>        return self._numResultsAvailable</pre>
<p>Given these variables, we can navigate through a large set of results by fetching only a few at a time; that is, by looking through a window into the results. We can move our window to earlier or later results, as needed, by simply adjusting the offset by the number of requested results and clamping the offset to the valid range. Here are some implementations of the <kbd>searchPrev</kbd> and <kbd>searchNext</kbd> methods, which rely on a more general search method that we will implement afterwards:</p>
<pre>    def searchPrev(self):<br/>        if self._offset == 0:<br/>            return<br/>        offset = max(0, self._offset - self._numResultsRequested)<br/>        self.search(self._query, self._numResultsRequested, offset)<br/><br/>    def searchNext(self):<br/>        if self._offset + self._numResultsRequested &gt;= \<br/>                self._numResultsAvailable:<br/>            return<br/>        offset = self._offset + self._numResultsRequested<br/>        self.search(self._query, self._numResultsRequested, offset)</pre>
<p>The more general-purpose <kbd>search</kbd> method accepts a query string, a maximum number of results, and an offset relative to the first available result. We store these arguments in member variables for reuse in the <kbd>searchPrev</kbd> and <kbd>searchNext</kbd> methods. The search method also uses the <kbd>BING_SEARCH_KEY</kbd> environment variable that we defined earlier. Here is this first part of the method's implementation:</p>
<pre>    def search(self, query, numResultsRequested=50, offset=0):<br/>        if 'BING_SEARCH_KEY' in os.environ:<br/>            bingKey = os.environ['BING_SEARCH_KEY']<br/>        else:<br/>            sys.stderr.write(<br/>                    'Environment variable BING_SEARCH_KEY is undefined. '<br/>                    'Please define it, equal to your Bing Search API '<br/>                    'key.\n')<br/>            return<br/><br/>        self._query = query<br/>        self._numResultsRequested = numResultsRequested<br/>        self._offset = offset</pre>
<p>Then, we set up our search parameters, specifying that the results should be in <kbd>JSON</kbd> format and should include color photos only:</p>
<pre>        params = {'color':'ColorOnly', 'imageType':'Photo'}</pre>
<p>We set up a search and request the results in <kbd>JSON</kbd> format. We handle any exceptions by printing an error message, setting the number of search results to <kbd>0</kbd>, and returning prematurely:</p>
<pre>        searchService = PyMsCognitiveImageSearch(<br/>                bingKey, query, custom_params=params)<br/>        searchService.current_offset = offset<br/><br/>        try:<br/>            self._results = searchService.search(numResultsRequested,<br/>                                                 'json')<br/>        except Exception as e:<br/>            sys.stderr.write(<br/>                    'Error when requesting Bing image search for '<br/>                    '"%s":\n' % query)<br/>            sys.stderr.write('%s\n' % str(e))<br/>            self._offset = 0<br/>            self._numResultsReceived = 0<br/>            return</pre>
<p><span>If the request succeeded, we proceed to parse the <kbd>JSON</kbd>. </span>We store metadata about the actual number of results received and number of results available:</p>
<pre class="mce-root">        json = searchService.most_recent_json<br/>        self._numResultsReceived = len(self._results)<br/>        if self._numResultsRequested &lt; self._numResultsReceived:<br/>            # py_ms_cognitive modified the request to get more results.<br/>            self._numResultsRequested = self._numResultsReceived<br/>        self._numResultsAvailable = int(json[u'totalEstimatedMatches'])</pre>
<p>If the <kbd>verbose</kbd> public variable is <kbd>True</kbd>, we print the JSON results. Here is the end of the method's implementation:</p>
<pre class="mce-root">        if self.verbose:<br/>            print('Received results of Bing image search for ' <br/>                  '"%s":' % query)<br/>            pprint.pprint(json)</pre>
<p>Although the <kbd>search</kbd> method fetches a textual description of results, including image URLs, it does not actually fetch any full-sized images. This is good, because the full-sized images may be large and we do not need them all at once. Instead, we provide another method, <kbd>getCvImageAndUrl</kbd>, to retrieve the image and image URL that have a specified index in the current results. The index is given as an argument. As an optional second argument, this method accepts a Boolean indicating whether a thumbnail should be used instead of the full-sized image. We use <kbd>cvImageFromUrl</kbd> to fetch and convert the thumbnail or full-sized image. Here is our implementation:</p>
<pre>    def getCvImageAndUrl(self, index, useThumbnail = False):<br/>        if index &gt;= self._numResultsReceived:<br/>            return None, None<br/>        result = self._results[index]<br/>        if useThumbnail:<br/>            url = result.thumbnail_url<br/>        else:<br/>            url = result.content_url<br/>        return RequestsUtils.cvImageFromUrl(url), url</pre>
<p>The caller of <kbd>getCvImageAndUrl</kbd> is responsible for dealing gracefully with image downloads that are slow or that fail. Recall that our <kbd>cvImageFromUrl</kbd> function just logs an error and returns <kbd>None</kbd> if the download fails.</p>
<p>To test <kbd>ImageSearchSession</kbd>, let's write a main function that instantiates the class, sets <kbd>verbose</kbd> to <kbd>True</kbd>, searches for <kbd>'luxury condo sales'</kbd>, and writes the first resulting image to disk. Here is the implementation:</p>
<pre>def main():<br/>    session = ImageSearchSession()<br/>    session.verbose = True<br/>    session.search('luxury condo sales')<br/>    image, url = session.getCvImageAndUrl(0)<br/>    cv2.imwrite('image.png', image)<br/><br/>if __name__ == '__main__':<br/>    main()</pre>
<p>Now that we have a classifier and a search session, we are almost ready to proceed to the frontend of Luxocator. We just need a few more utility functions to help us prepare data and images for bundling and display.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing images and resources for the app</h1>
                </header>
            
            <article>
                
<p>Alongside <kbd>RequestsUtils.py</kbd> and <kbd>ImageSearchSession.py</kbd>, let's create another file called <kbd>ResizeUtils.py</kbd> with the following <kbd>import</kbd> statements:</p>
<pre>import numpy # Hint to PyInstaller<br/>import cv2</pre>
<p>For display in a GUI, images usually have to be resized. One popular mode of resizing is called <strong>aspect fill</strong>. Here, we want to preserve the image's aspect ratio while changing its larger dimension (width for a landscape image or height for a portrait image) to a certain value. OpenCV does not directly provide this resizing mode, but it does provide a function, <kbd>cv2.resize</kbd>, which accepts an image, target dimensions, and optional arguments, including an interpolation method. We can write our own function, <kbd>cvResizeAspectFill</kbd>, which accepts an image, maximum size, and preferred interpolation methods for upsizing and downsizing. It determines the appropriate arguments for <kbd>cv2.resize</kbd> and passes them along. Here is the implementation:</p>
<pre>def cvResizeAspectFill(src, maxSize,<br/>                       upInterpolation=cv2.INTER_LANCZOS4,<br/>                       downInterpolation=cv2.INTER_AREA):<br/>    h, w = src.shape[:2]<br/>    if w &gt; h:<br/>        if w &gt; maxSize:<br/>            interpolation=downInterpolation<br/>        else:<br/>            interpolation=upInterpolation<br/>        h = int(maxSize * h / float(w))<br/>        w = maxSize<br/>    else:<br/>        if h &gt; maxSize:<br/>            interpolation=downInterpolation<br/>        else:<br/>            interpolation=upInterpolation<br/>        w = int(maxSize * w / float(h))<br/>        h = maxSize<br/>    dst = cv2.resize(src, (w, h), interpolation=interpolation)<br/>    return dst</pre>
<div class="packt_infobox">For a description of the interpolation methods that OpenCV supports, see the official documentation at <a href="https://docs.opencv.org/master/da/d54/group__imgproc__transform.html#ga47a974309e9102f5f08231edc7e7529d">https://docs.opencv.org/master/da/d54/group__imgproc__transform.html#ga47a974309e9102f5f08231edc7e7529d</a>. For upsizing, we default to <kbd>cv2.INTER_LANCZOS4</kbd>, which produces sharp results. For downsizing, we default to <kbd>cv2.INTER_AREA</kbd>, which produces moiré-free results (moiré is an artifact that makes parallel lines or concentric curves look like crosshatching when they are sharpened at certain magnifications).</div>
<p>Now, let's create another file called <kbd>WxUtils.py</kbd> with the following <kbd>import</kbd> statements:</p>
<pre>import numpy # Hint to PyInstaller<br/>import cv2<br/>import wx</pre>
<p>Due to API changes between <kbd><span>wxPython</span> 3</kbd> and <kbd>wxPython 4</kbd>, it is important for us to check which version has been imported. We use the following code to get a version string, such as <kbd>'4.0.3'</kbd>, and to parse the major version number, such as <kbd>4</kbd>:</p>
<pre>WX_MAJOR_VERSION = int(wx.__version__.split('.')[0])</pre>
<p>OpenCV and wxPython use different image formats, so we will implement a conversion function, <kbd>wxBitmapFromCvImage</kbd>. While OpenCV stores color channels in BGR order, wxPython expects RGB order. We can use an OpenCV function, <kbd>cv2.cvtColor</kbd>, to reformat the image data accordingly. Then, we can use a wxPython function, <kbd>wx.BitmapFromBuffer</kbd>, to read the reformatted data into a wxPython bitmap, which we return. Here is the implementation:</p>
<pre>def wxBitmapFromCvImage(image):<br/>    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)<br/>    h, w = image.shape[:2]<br/>    # The following conversion fails on Raspberry Pi.<br/>    if WX_MAJOR_VERSION &lt; 4:<br/>        bitmap = wx.BitmapFromBuffer(w, h, image)<br/>    else:<br/>        bitmap = wx.Bitmap.FromBuffer(w, h, image)<br/>    return bitmap</pre>
<div class="packt_tip">On some versions of Raspberry Pi and Raspbian, <kbd>wx.BitmapFromBuffer</kbd> suffers from a platform-specific bug that causes it to fail. For a workaround, see <a href="ddc68808-2fe3-4064-b333-632e68bb4ddf.xhtml">Appendix A</a>,<em> Making WxUtils.py Compatible with Raspberry Pi</em>, at the end of this book.</div>
<p>We have one more utility module to make. Let's create a file, <kbd>PyInstallerUtils.py</kbd>, with <kbd>import</kbd> statements for the <kbd>os</kbd> and <kbd>sys</kbd> modules from Python's standard library:</p>
<pre><strong>import os</strong><br/><strong>import sys</strong></pre>
<p>When we bundle our application using PyInstaller, the paths to resources will change. So, we need a function that correctly resolves paths, regardless of whether our application has been bundled or not. Let's add a function, <kbd>pyInstallerResourcePath</kbd>, which resolves a given path relative to the app directory (the <kbd>'_MEIPASS'</kbd> attribute) or, failing that, the current working directory (<kbd>'.'</kbd>). It is implemented as follows:</p>
<pre>def resourcePath(relativePath):<br/>    basePath = getattr(sys, '_MEIPASS', os.path.abspath('.'))<br/>    return os.path.join(basePath, relativePath)</pre>
<p>Our utilities modules are done now and we can move on to implementing the frontend of Luxocator.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Integrating everything into the GUI</h1>
                </header>
            
            <article>
                
<p>For Luxocator's front end, let's create a file called <kbd>Luxocator.py</kbd>. This module depends on OpenCV, wxPython, and some of Python's standard OS and threading functionality. It also depends on all the other modules that we have written in this chapter. Add the following shebang line and <kbd>import</kbd> statements at the top of the file:</p>
<pre>#!/usr/bin/env python<br/><br/><br/>import numpy # Hint to PyInstaller<br/>import cv2<br/>import os<br/>import threading<br/>import wx<br/><br/>from HistogramClassifier import HistogramClassifier<br/>from ImageSearchSession import ImageSearchSession<br/>import PyInstallerUtils<br/>import ResizeUtils<br/>import WxUtils</pre>
<p>Now, let's implement the <kbd>Luxocator</kbd> class as a subclass of <kbd>wx.Frame</kbd>, which represents a GUI frame such as the contents of a window. Most of our GUI code is in the <kbd>Luxocator</kbd> class's <kbd>__init__</kbd> method, which is, therefore, a big method but not very complicated. Our GUI elements include a search control, previous and next buttons, a bitmap, and a label to show the classification result. All of these GUI elements are stored in member variables. The bitmap is confined to a certain maximum size (by default, 768 pixels in the larger dimension), and the other elements are laid out below it. Several methods are registered as callbacks to handle events such as the window closing, the <em>Esc</em> key being pressed, a search string being entered, or the next or previous button being clicked. Besides the GUI elements, other member variables include instances of our <kbd>HistogramClassifier</kbd> and <kbd>ImageSearchSession</kbd> classes. Here is the implementation of the initializer, interspersed with some remarks on the GUI elements that we are using:</p>
<pre>class Luxocator(wx.Frame):<br/><br/>    def __init__(self, classifierPath, maxImageSize=768,<br/>                 verboseSearchSession=False,<br/>                 verboseClassifier=False):<br/><br/>        style = wx.CLOSE_BOX | wx.MINIMIZE_BOX | wx.CAPTION | \<br/>            wx.SYSTEM_MENU | wx.CLIP_CHILDREN<br/>        wx.Frame.__init__(self, None, title='Luxocator', style=style)<br/>        self.SetBackgroundColour(wx.Colour(232, 232, 232))<br/><br/>        self._maxImageSize = maxImageSize<br/>        border = 12<br/>        defaultQuery = 'luxury condo sales'<br/><br/>        self._index = 0<br/>        self._session = ImageSearchSession()<br/>        self._session.verbose = verboseSearchSession<br/>        self._session.search(defaultQuery)<br/><br/>        self._classifier = HistogramClassifier()<br/>        self._classifier.verbose = verboseClassifier<br/>        self._classifier.deserialize(classifierPath)<br/><br/>        self.Bind(wx.EVT_CLOSE, self._onCloseWindow)<br/><br/>        quitCommandID = wx.NewId()<br/>        self.Bind(wx.EVT_MENU, self._onQuitCommand,<br/>                  id=quitCommandID)<br/>        acceleratorTable = wx.AcceleratorTable([<br/>            (wx.ACCEL_NORMAL, wx.WXK_ESCAPE, quitCommandID)<br/>        ])<br/>        self.SetAcceleratorTable(acceleratorTable)</pre>
<div class="packt_infobox"><span>For more information about using bitmaps, controls, and layouts in wxPython, refer to the official wiki at </span><a href="http://wiki.wxpython.org/"><span class="URLPACKT">http://wiki.wxpython.org/</span></a><span>.</span></div>
<p>The search control (coming up next) deserves special attention because it contains multiple controls within it, and its behavior differs slightly across operating systems. It may have up to three sub-controls—a text field, a search button, and a cancel button. There may be a callback for the <em>Enter</em> key being pressed while the text field is active. If the search and cancel buttons are present, they have callbacks for being clicked. We can set up the search control and its callbacks as follows:</p>
<pre>        self._searchCtrl = wx.SearchCtrl(<br/>                self, size=(self._maxImageSize / 3, -1),<br/>                style=wx.TE_PROCESS_ENTER)<br/>        self._searchCtrl.SetValue(defaultQuery)<br/>        self._searchCtrl.Bind(wx.EVT_TEXT_ENTER,<br/>                              self._onSearchEntered)<br/>        self._searchCtrl.Bind(wx.EVT_SEARCHCTRL_SEARCH_BTN,<br/>                              self._onSearchEntered)<br/>        self._searchCtrl.Bind(wx.EVT_SEARCHCTRL_CANCEL_BTN,<br/>                              self._onSearchCanceled)</pre>
<p>By contrast, the label, previous and next buttons, and bitmap do not have any sub-controls that concern us. We can set them up as follows:</p>
<pre>        self._labelStaticText = wx.StaticText(self)<br/><br/>        self._prevButton = wx.Button(self, label='Prev')<br/>        self._prevButton.Bind(wx.EVT_BUTTON,<br/>                              self._onPrevButtonClicked)<br/><br/>        self._nextButton = wx.Button(self, label='Next')<br/>        self._nextButton.Bind(wx.EVT_BUTTON,<br/>                              self._onNextButtonClicked)<br/><br/>        self._staticBitmap = wx.StaticBitmap(self)</pre>
<p>Our controls are lined up horizontally, with the search control on the left edge of the window, the previous and next buttons on the right edge, and the label halfway in-between the search control and previous button. We use an instance of <kbd>wx.BoxSizer</kbd> to define this horizontal layout:</p>
<pre>        controlsSizer = wx.BoxSizer(wx.HORIZONTAL)<br/>        controlsSizer.Add(self._searchCtrl, 0,<br/>                          wx.ALIGN_CENTER_VERTICAL | wx.RIGHT,<br/>                          border)<br/>        controlsSizer.Add((0, 0), 1) # Spacer<br/>        controlsSizer.Add(<br/>                self._labelStaticText, 0, wx.ALIGN_CENTER_VERTICAL)<br/>        controlsSizer.Add((0, 0), 1) # Spacer<br/>        controlsSizer.Add(<br/>                self._prevButton, 0,<br/>                wx.ALIGN_CENTER_VERTICAL | wx.LEFT | wx.RIGHT,<br/>                border)<br/>        controlsSizer.Add(<br/>                self._nextButton, 0, wx.ALIGN_CENTER_VERTICAL)</pre>
<p>The best thing about layouts (and Russian dolls) is that they can be nested, one inside another. Our horizontal layout of controls needs to appear below the bitmap. This relationship is a vertical layout, which we define using another <kbd>wx.BoxSizer</kbd> instance:</p>
<pre>        self._rootSizer = wx.BoxSizer(wx.VERTICAL)<br/>        self._rootSizer.Add(self._staticBitmap, 0,<br/>                            wx.TOP | wx.LEFT | wx.RIGHT, border)<br/>        self._rootSizer.Add(controlsSizer, 0, wx.EXPAND | wx.ALL,<br/>                            border)<br/><br/>        self.SetSizerAndFit(self._rootSizer)<br/><br/>        self._updateImageAndControls()</pre>
<p>That is the end of the <kbd>__init__</kbd> method.</p>
<p>As we can see in the following code, we provide getters and setters for the <kbd>verbose</kbd> property of our <kbd>ImageSearchSession</kbd> instance and our <kbd>HistogramClassifier</kbd> instance:</p>
<pre>    @property<br/>    def verboseSearchSession(self):<br/>        return self._session.verbose<br/><br/>    @verboseSearchSession.setter<br/>    def verboseSearchSession(self, value):<br/>        self._session.verbose = value<br/><br/>    @property<br/>    def verboseClassifier(self):<br/>        return self._classifier.verbose<br/><br/>    @verboseClassifier.setter<br/>    def verboseClassifier(self, value):<br/>        self._classifier.verbose = value</pre>
<p>Our <kbd>_onCloseWindow</kbd> callback just cleans up the application by calling the <kbd>Destroy</kbd> method of the superclass. Here is its implementation:</p>
<pre>    def _onCloseWindow(self, event):<br/>        self.Destroy()</pre>
<p>Similarly, we have connected the <em>Esc</em> key to the <kbd>_onQuitCommand</kbd> callback, which closes the window. This, in turn, will result in <kbd>_onCloseWindow</kbd> being called. Here is the implementation of <span><kbd>_onQuitCommand</kbd>:</span></p>
<pre>    def _onQuitCommand(self, event):<br/>        self.Close()</pre>
<p>Our <kbd>_onSearchEntered</kbd> callback submits the query string through the search method of <kbd>ImageSearchSession</kbd>. Then, it calls a helper method, <kbd>_updateImageAndControls</kbd>, which asynchronously fetches images and updates the GUI, as we will see later. Here is the implementation of <kbd>_onSearchEntered</kbd>:</p>
<pre>    def _onSearchEntered(self, event):<br/>        query = event.GetString()<br/>        if len(query) &lt; 1:<br/>            return<br/>        self._session.search(query)<br/>        self._index = 0<br/>        self._updateImageAndControls()</pre>
<p>Our <kbd>_onSearchCanceled</kbd> callback simply clears the search control's text field, as seen in the following code:</p>
<pre>    def _onSearchCanceled(self, event):<br/>        self._searchCtrl.Clear()</pre>
<p>Our remaining GUI event callbacks, <kbd>_onNextButtonClicked</kbd> and <kbd>_onPrevButtonClicked</kbd>, check whether more results are available and, if so, uses the <kbd>searchNext</kbd> or <kbd>searchPrev</kbd> method of <kbd>ImageSearchSession</kbd>. Then, using the <kbd>_updateImageAndControls</kbd> helper method, images are fetched asynchronously and the GUI is updated. Here are the implementations of the callbacks:</p>
<pre>    def _onNextButtonClicked(self, event):<br/>        self._index += 1<br/>        if self._index &gt;= self._session.offset + \<br/>                self._session.numResultsReceived - 1:<br/>            self._session.searchNext()<br/>        self._updateImageAndControls()<br/><br/>    def _onPrevButtonClicked(self, event):<br/>        self._index -= 1<br/>        if self._index &lt; self._session.offset:<br/>            self._session.searchPrev()<br/>        self._updateImageAndControls()</pre>
<p>The <kbd>_disableControls</kbd> method disables the search control and the previous and next buttons, as follows:</p>
<pre>    def _disableControls(self):<br/>        self._searchCtrl.Disable()<br/>        self._prevButton.Disable()<br/>        self._nextButton.Disable()</pre>
<p>Conversely, the <kbd>_enableControls</kbd> method enables the search control, the previous button (if we are not already at the first available search result), and the next button (if we are not already at the last available search result). Here is the implementation:</p>
<pre>    def _enableControls(self):<br/>        self._searchCtrl.Enable()<br/>        if self._index &gt; 0:<br/>            self._prevButton.Enable()<br/>        if self._index &lt; self._session.numResultsAvailable - 1:<br/>            self._nextButton.Enable()</pre>
<p>The <kbd>_updateImageAndControls</kbd> method first disables the controls because we do not want to handle any new queries until the current query is handled. Then, a busy cursor is shown and another helper method, <kbd>_updateImageAndControlsAsync</kbd>, is started on a background thread. Here is the implementation:</p>
<pre>    def _updateImageAndControls(self):<br/>        # Disable the controls.<br/>        self._disableControls()<br/>        # Show the busy cursor.<br/>        wx.BeginBusyCursor()<br/>        # Get the image in a background thread.<br/>        threading.Thread(<br/>                target=self._updateImageAndControlsAsync).start()</pre>
<p>The background method, <kbd>_updateImageAndControlsAsync</kbd>, starts by fetching an image and converting it into OpenCV format. If the image cannot be fetched and converted, an error message is used as the label. Otherwise, the image is classified and resized to an appropriate size for display. Then, the resized image and the classification label are passed to a third and final helper method, <kbd>_updateImageAndControlsResync</kbd>, which updates the GUI on the main thread. Here is the implementation of <kbd>_updateImageAndControlsAsync</kbd>:</p>
<pre>    def _updateImageAndControlsAsync(self):<br/>        if self._session.numResultsRequested == 0:<br/>            image = None<br/>            label = 'Search had no results'<br/>        else:<br/>            # Get the current image.<br/>            image, url = self._session.getCvImageAndUrl(<br/>                self._index % self._session.numResultsRequested)<br/>            if image is None:<br/>                # Provide an error message.<br/>                label = 'Failed to decode image'<br/>            else:<br/>                # Classify the image.<br/>                label = self._classifier.classify(image, url)<br/>                # Resize the image while maintaining its aspect ratio.<br/>                image = ResizeUtils.cvResizeAspectFill(<br/>                    image, self._maxImageSize)<br/>        # Update the GUI on the main thread.<br/>        wx.CallAfter(self._updateImageAndControlsResync, image,<br/>                     label)</pre>
<p>The synchronous callback, <kbd>_updateImageAndControlsResync</kbd>, hides the busy cursor, creates a wxPython bitmap from the fetched image (or just a black bitmap if no image was successfully fetched and converted), shows the image and its classification label, resizes GUI elements, re-enables controls, and refreshes the window. Here is its implementation:</p>
<pre>    def _updateImageAndControlsResync(self, image, label):<br/>        # Hide the busy cursor.<br/>        wx.EndBusyCursor()<br/>        if image is None:<br/>            # Provide a black bitmap.<br/>            bitmap = wx.Bitmap(self._maxImageSize,<br/>                               self._maxImageSize / 2)<br/>        else:<br/>            # Convert the image to bitmap format.<br/>            bitmap = WxUtils.wxBitmapFromCvImage(image)<br/>        # Show the bitmap.<br/>        self._staticBitmap.SetBitmap(bitmap)<br/>        # Show the label.<br/>        self._labelStaticText.SetLabel(label)<br/>        # Resize the sizer and frame.<br/>        self._rootSizer.Fit(self)<br/>        # Re-enable the controls.<br/>        self._enableControls()<br/>        # Refresh.<br/>        self.Refresh()</pre>
<p>When the image cannot be successfully fetched and converted, the user sees something like the following screenshot, containing a black placeholder image:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7f60d4b5-d0cf-4e4a-9e10-f393b476c78d.png"/></p>
<p>Conversely, when an image is successfully fetched and converted, the users sees the classification result, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/38150fba-5da2-485f-bfb9-32b5adcd3007.png"/></p>
<p>That completes the implementation of the <kbd>Luxocator</kbd> class. Now, let's write a <kbd>main</kbd> method to set resource paths and launch an instance of <kbd>Luxocator</kbd>:</p>
<pre>def main():<br/>    os.environ['REQUESTS_CA_BUNDLE'] = \<br/>            PyInstallerUtils.resourcePath('cacert.pem')<br/>    app = wx.App()<br/>    luxocator = Luxocator(<br/>            PyInstallerUtils.resourcePath('classifier.mat'),<br/>            verboseSearchSession=False, verboseClassifier=False)<br/>    luxocator.Show()<br/>    app.MainLoop()<br/><br/>if __name__ == '__main__':<br/>    main()</pre>
<p>Note that one of the resources is a certificate bundle called <kbd>cacert.pem</kbd>. It is required by Requests in order to make an SSL connection, which is, in turn, required by Bing. You can find a copy of it inside this chapter's code bundle, which is downloadable from my website at <a href="http://nummist.com/opencv/7376_02.zip"><span class="URLPACKT">http://nummist.com/opencv/7376_02.zip</span></a>. Place <kbd>cacert.pem</kbd> in the same folder as <kbd>Luxocator.py</kbd>. Note that our code sets an environment variable, <kbd>REQUESTS_CA_BUNDLE</kbd>, which is used by Requests to locate the certificate bundle.</p>
<div class="packt_infobox">Depending on how it is installed or how it is bundled with an app, Requests may or may not have an internal version of the certificate bundle. For predictability, it is better to provide this external version.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running Luxocator and troubleshooting SSL problems</h1>
                </header>
            
            <article>
                
<p>At this point, you can run <kbd>Luxocator.py</kbd>, enter search keywords, and navigate through the results. Watch for any errors that Luxocator might print to the Terminal. On some systems, notably Ubuntu 14.04 and its derivatives, such as Linux Mint 17, you might run into a bug in the Requests library when Luxocator attempts to access an HTTPS URL. The symptom of this bug is an error message similar to the following:</p>
<pre>requests.exceptions.SSLError: [Errno 1] _ssl.c:510: error:14077410:SSL routines:SSL23_GET_SERVER_HELLO:sslv3 alert handshake failure</pre>
<p>If you encounter this problem, you can try to resolve it by installing additional SSL-related packages and downgrading Requests to an earlier version. Some users of Ubuntu 14.04 and its derivatives report that they resolved the problem by running the following commands:</p>
<pre><strong>$ sudo apt-get install python-dev libssl-dev libffi-dev</strong><br/><strong>$ pip install --user pyopenssl==0.13.1 pyasn1 ndg-httpsclient</strong></pre>
<p>Alternatively, some users of Ubuntu 14.04 and its derivatives report that they resolved the problem by upgrading to a newer version of the operating system. Note that the problem is not specific to Luxocator, but rather it affects any software that uses Requests, so it is potentially an issue of system-wide importance.</p>
<p>When you are satisfied with your results from testing Luxocator, let's proceed to build a Luxocator package that we can more easily distribute to other users' systems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building Luxocator for distribution</h1>
                </header>
            
            <article>
                
<p>To tell PyInstaller how to build Luxocator, we must create a specification file, which we will call <kbd>Luxocator.spec</kbd>. Actually, the specification file is a Python script that uses a PyInstaller class called <kbd>Analysis</kbd> and the PyInstaller functions called <kbd>PYZ</kbd>, <kbd>EXE</kbd>, and <kbd>BUNDLE</kbd>. The <kbd>Analysis</kbd> class is responsible for analyzing one or more Python scripts (in our case, just <kbd>Luxocator.py</kbd>) and tracing all the dependencies that must be bundled with these scripts in order to make a redistributable application. Sometimes, <kbd>Analysis</kbd> makes mistakes or omissions, so we modify the list of dependencies after it is initialized. Then, we zip the scripts, make an executable, and (for Mac) make an app bundle using <kbd>PYZ</kbd>, <kbd>EXE</kbd>, and <kbd>BUNDLE</kbd>, respectively. Here is the implementation:</p>
<pre>a = Analysis(['Luxocator.py'],<br/>             pathex=['.'],<br/>             hiddenimports=[],<br/>             hookspath=None,<br/>             runtime_hooks=None)<br/><br/><br/># Include SSL certificates for the sake of the 'requests' module.<br/>a.datas.append(('cacert.pem', 'cacert.pem', 'DATA'))<br/><br/># Include our app's classifier data.<br/>a.datas.append(('classifier.mat', 'classifier.mat', 'DATA'))<br/><br/><br/>pyz = PYZ(a.pure)<br/><br/>exe = EXE(pyz,<br/>          a.scripts,<br/>          a.binaries,<br/>          a.zipfiles,<br/>          a.datas,<br/>          name='Luxocator',<br/>          icon='win\icon-windowed.ico',<br/>          debug=False,<br/>          strip=None,<br/>          upx=True,<br/>          console=False)<br/><br/>app = BUNDLE(exe,<br/>             name='Luxocator.app',<br/>             icon=None)</pre>
<p>Note that this script specifies three resource files that must be bundled with the app: <kbd>cacert.pem</kbd>, <kbd>classifier.mat</kbd>, and <kbd>winicon-windowed.ico</kbd>. We have already discussed <kbd>cacert.pem</kbd> in the previous section, and <kbd>classifier.mat</kbd> is the output of our main function in <kbd>HistogramClassifier.py</kbd>. The Windows icon file, <kbd>winicon-windowed.ico</kbd>, is included in this book's GitHub repository in the <kbd>Chapter002/win</kbd> folder. Alternatively, you may provide your own icon file if you prefer.</p>
<div class="packt_infobox">For more information about PyInstaller's <kbd>Analysis</kbd> class, specification files, and other functionality, see the official documentation at <a href="https://pyinstaller.readthedocs.io/">https://pyinstaller.readthedocs.io/</a>.</div>
<p>Now, let's write a platform-specific shell script to clean any old builds, train our classifier, and then bundle the app using PyInstaller. On Windows, create a script called <kbd>build.bat</kbd>, containing the following commands:</p>
<pre>set PYINSTALLER=pyinstaller<br/><br/>REM Remove any previous build of the app.<br/>rmdir build /s /q<br/>rmdir dist /s /q<br/><br/>REM Train the classifier.<br/>python HistogramClassifier.py<br/><br/>REM Build the app.<br/>"%PYINSTALLER%" --onefile --windowed Luxocator.spec<br/><br/>REM Make the app an executable.<br/>rename dist\Luxocator Luxocator.exe</pre>
<div class="packt_tip">If <kbd>pyinstaller.exe</kbd> is not in your system's <kbd>Path</kbd>, you will need to change the <kbd>build.bat</kbd> script's definition of the <kbd>PYINSTALLER</kbd> variable in order to provide a full path to <span><kbd>pyinstaller.exe</kbd></span>.</div>
<p>Similarly, on Mac or Linux, create a script called <kbd>build.sh</kbd>. Make it executable (for example, by running <kbd>$ chmod +x build.sh</kbd> in the Terminal). The file should contain the following commands:</p>
<pre>#!/usr/bin/env sh<br/><br/><br/># Search for common names of PyInstaller in $PATH.<br/>if [ -x "$(command -v "pyinstaller")" ]; then<br/>    PYINSTALLER=pyinstaller<br/>elif [ -x "$(command -v "pyinstaller-3.6")" ]; then<br/>    PYINSTALLER=pyinstaller-3.6<br/>elif [ -x "$(command -v "pyinstaller-3.5")" ]; then<br/>    PYINSTALLER=pyinstaller-3.5<br/>elif [ -x "$(command -v "pyinstaller-3.4")" ]; then<br/>    PYINSTALLER=pyinstaller-3.4<br/>elif [ -x "$(command -v "pyinstaller-2.7")" ]; then<br/>    PYINSTALLER=pyinstaller-2.7<br/>else<br/>    echo "Failed to find PyInstaller in \$PATH"<br/>    exit 1<br/>fi<br/>echo "Found PyInstaller in \$PATH with name \"$PYINSTALLER\""<br/><br/># Remove any previous build of the app.<br/>rm -rf build<br/>rm -rf dist<br/><br/># Train the classifier.<br/>python HistogramClassifier.py<br/><br/># Build the app.<br/>"$PYINSTALLER" --onefile --windowed Luxocator.spec<br/><br/># Determine the platform.<br/>platform=`uname -s`<br/><br/>if [ "$platform" = 'Darwin' ]; then<br/>    # We are on Mac.<br/>    # Copy custom metadata and resources into the app bundle.<br/>    cp -r mac/Contents dist/Luxocator.app<br/>fi</pre>
<div class="packt_tip"><span>I</span><span>f the </span><kbd>pyinstaller</kbd><span> executable (or a similar executable, such as <kbd>pyinstaller-3.6</kbd> for Python 3.6)  is not in your system's </span><kbd>PATH</kbd><span>, you will need to change the </span><kbd>build.sh</kbd><span> script's definition of the </span><kbd>PYINSTALLER</kbd><span> variable in order to provide a full path to </span><span><kbd>pyinstaller</kbd></span><span>.</span></div>
<p>Note that on Mac (the Darwin platform), we are manually modifying the app bundle's contents as a post-build step. We do this in order to overwrite the default app icon and default properties file that PyInstaller puts in all Mac apps (notably, in some versions of PyInstaller, the default properties do not include support for Retina mode, so they make the app look pixelated on recent Mac hardware. Our customizations fix this issue). This book's GitHub repository includes the custom Mac app contents in a folder called <kbd>Chapter002/mac/Contents</kbd>. You may modify its files to provide any icon and properties you want.</p>
<p>After running the platform-specific build script, we should have a redistributable build of Luxocator at <kbd>dist/Luxocator.exe</kbd> (Windows), <kbd>dist/Luxocator.app</kbd> (Mac), or <kbd>dist/Luxocator</kbd> (Linux). If we are using 64-bit Python libraries on our development machine, this build will only work on 64-bit systems. Otherwise, it should work on both 32-bit and 64-bit systems. The best way to test the build is to run it on another machine that doesn't have any of the relevant libraries (such as OpenCV) installed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>So much can happen in a single mission! We trained an OpenCV/NumPy/SciPy histogram classifier, performed Bing Image Searches, built a <kbd>wxPython</kbd> app, and used PyInstaller to bundle it all for redistribution to Russia with love (or, indeed, to any destination with any sentiment). At this point, you are well-primed to create other Python applications that combine computer vision, web requests, and a GUI.</p>
<p>For our next mission, we will dig our claws deeper into OpenCV and computer vision by building a fully functional cat recognizer!</p>


            </article>

            
        </section>
    </body></html>