<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Support Vector Machines</h1>
                </header>
            
            <article>
                
<p>In this chapter, we're going to introduce another approach to classification using a family of algorithms called support vector machines. They can work with both linear and non-linear scenarios, allowing high performance in many different contexts. Together with neural networks, SVMs probably represent the best choice for many tasks where it's not easy to find out a good separating hyperplane. For example, for a long time, SVMs were the best choice for MNIST dataset classification, thanks to the fact that they can capture very high non-linear dynamics using a mathematical trick, without complex modifications in the algorithm. In the first part, we're going to discuss the basics of linear SVM, which then will be used for their non-linear extensions. We'll also discuss some techniques to control the number of parameters and, at the end, the application of support vector algorithms to regression problems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Linear support vector machines</h1>
                </header>
            
            <article>
                
<p>Let's consider a dataset of feature vectors we want to classify:</p>
<div class="CDPAlignCenter CDPAlign"><img height="52" width="291" src="assets/a8ebf2e9-c219-44a4-bbfd-190e1364aaef.png"/></div>
<p class="CDPAlignLeft CDPAlign">For simplicity, we assume it as a binary classification (in all the other cases, it's possible to use automatically the one-versus-all strategy) and we set our class labels as -1 and 1:</p>
<div class="CDPAlignCenter CDPAlign"><img height="42" width="263" src="assets/56b6c321-1019-4144-84fc-e5bd104e136a.png"/></div>
<p class="CDPAlignLeft CDPAlign">Our goal is to find the best separating hyperplane, for which the equation is:</p>
<div class="CDPAlignCenter CDPAlign"><img height="58" width="332" src="assets/d48b7499-e5f1-4593-8180-bf8e3793c2fe.png"/></div>
<p class="CDPAlignLeft CDPAlign">In the following figure, there's a bidimensional representation of such a hyperplane:</p>
<div class="CDPAlignCenter CDPAlign"><img height="363" width="518" class="image-border" src="assets/d2582ae4-446f-426d-9a37-3edd703ee10b.png"/></div>
<p>In this way, our classifier can be written as:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img height="43" width="192" src="assets/01bbc949-57f8-4dd0-928e-de89da5ea280.png"/></div>
<p>In a realistic scenario, the two classes are normally separated by a margin with two boundaries where a few elements lie. Those elements are called <strong>support vectors</strong>. For a more generic mathematical expression, it's preferable to renormalize our dataset so that the support vectors will lie on two hyperplanes with equations:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img height="48" width="116" src="assets/ed90dc11-fc80-474f-aace-973758eb6ecb.png"/></div>
<p>In the following figure, there's an example with two support vectors. The dashed line is the original separating hyperplane:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="image-border" src="assets/93c4e356-cf65-4925-9c48-6591dcfaf927.png"/></div>
<p>Our goal is to maximize the distance between these two boundary hyperplanes so as to reduce the probability of misclassification (which is higher when the distance is short, and there aren't two well-defined blobs as in the previous figure). </p>
<p>Considering that the boundaries are parallel, the distance between them is defined by the length of the segment perpendicular to both and connecting two points:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="image-border" src="assets/37e1322e-e6d5-4fbd-83d5-54ba7c0982d1.png"/></div>
<p>Considering the points as vectors, therefore, we have:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img height="31" width="98" src="assets/bde1f510-5fab-4167-8e97-cd3164a5b07f.png"/></div>
<p>Now, considering the boundary hyperplane equations, we get:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img height="35" width="398" src="assets/d1a13501-f5c0-41cc-a4c6-ebe3cedbefcd.png"/></div>
<p>The first term of the last part is equal to -1, so we solve for <em>t</em>:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img height="54" width="83" src="assets/1d0a8ff9-e358-429c-a4a2-fb6e69eac926.png"/></div>
<p>The distance between <em>x<sub>1</sub></em> and <em>x<sub>2</sub></em> is the length of the segment <em>t</em>; hence we get:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img height="50" width="172" src="assets/2e5930f6-b8c6-40a4-a67a-89d7ecf6d919.png"/></div>
<p>Now, considering all points of our dataset, we can impose the following constraint:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img height="42" width="212" src="assets/0637c2a1-8d11-4d8b-ac8d-0b2bff9bec9f.png"/></div>
<p>This is guaranteed by using -1, 1 as class labels and boundary margins. The equality is true only for the support vectors, while for all the other points it will greater than 1. It's important to consider that the model doesn't take into account vectors beyond this margin. In many cases, this can yield a very robust model, but in many datasets this can also be a strong limitation. In the next paragraph, we're going to use a trick to avoid this rigidness while keeping the same optimization technique.<br/>
At this point, we can define the function to minimize in order to train a support vector machine:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img height="75" width="139" src="assets/f1ea27ad-76a9-476f-8710-3cea0273df5d.png"/></div>
<p>This can be further simplified (by removing the square root from the norm) in the following quadratic programming problem:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img height="77" width="134" src="assets/0a21ed94-6589-4cd0-996f-0ef0dd4b6e76.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">scikit-learn implementation</h1>
                </header>
            
            <article>
                
<p>In order to allow the model to have a more flexible separating hyperplane, all scikit-learn implementations are based on a simple variant that includes so-called <strong>slack variables</strong> in the function to minimize:</p>
<div class="CDPAlignCenter CDPAlign"><img height="59" width="163" src="assets/7c0cd89f-8dc3-4a0e-8649-ef36e6c0df0a.png"/></div>
<p class="CDPAlignLeft CDPAlign">In this case, the constraints become:</p>
<div class="CDPAlignCenter CDPAlign"><img height="37" width="153" src="assets/b4198582-8d33-4738-9d96-8133c14f06e7.png"/></div>
<p class="CDPAlignLeft CDPAlign">The introduction of the slack variables allows us to create a flexible margin so that some vectors belonging to a class can also be found in the opposite part of the hyperspace and can be included in the model training. The strength of this flexibility can be set using the parameter <em>C</em>. Small values (close to zero) bring about very hard margins, while values greater than or equal to 1 allow more and more flexibility (<span>also</span><span> </span><span>increasing the misclassification rate). The right choice of</span> <em>C</em> <span>is not immediate, but the best value can be found automatically by using a grid search as seen in the previous chapters. In our examples, we keep the default value of 1.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Linear classification</h1>
                </header>
            
            <article>
                
<p>Our first example is based on a linear SVM, as described in the previous section. We start by creating a dummy dataset with 500 vectors subdivided into two classes:</p>
<pre><strong>from sklearn.datasets import make_classification</strong><br/><br/><strong>&gt;&gt;&gt; nb_samples = 500</strong><br/><strong>&gt;&gt;&gt; X, Y = make_classification(n_samples=nb_samples, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1)</strong></pre>
<p class="CDPAlignLeft CDPAlign">In the following figure, there's a plot of our dataset. Notice that some points overlap the two main blobs. For this reason, a positive <em>C</em> value is needed to allow the model to capture a more complex dynamic.</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/1707af43-728e-43d6-acd8-924b1cb35601.png"/></div>
<p class="CDPAlignLeft CDPAlign">scikit-learn provides the <kbd>SVC</kbd> <span>class,</span><span> </span><span>which is a very efficient implementation that can be used in most cases. We're going to use it together with cross-validation to validate performance:</span></p>
<pre><strong>from sklearn.svm import SVC</strong><br/><strong>from sklearn.model_selection import cross_val_score</strong><br/><br/><strong>&gt;&gt;&gt; svc = SVC(kernel='linear')</strong><br/><strong>&gt;&gt;&gt; cross_val_score(svc, X, Y, scoring='accuracy', cv=10).mean()</strong><br/><strong>0.93191356542617032</strong></pre>
<p class="CDPAlignLeft CDPAlign">The <kbd>kernel</kbd> <span>parameter</span><span> </span><span>must be set to</span> <kbd>'linear'</kbd> <span>in this example. In the next section, we're going to discuss how it works and how it can improve the SVM's performance dramatically in non-linear scenarios. As expected, the accuracy is comparable to a logistic regression, as this model tries to find an optimal linear separator. After training a model, it's possible to get an array of support vectors, through the instance variable called </span><kbd>support_vectors_</kbd><span>. A plot of them, for our example, is shown in the following figure:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/31b9af15-46ea-4aa2-aab7-89bc88beea07.png"/></div>
<p>As it's possible to see, they are placed in a strip along the separating line. The effect of <kbd>C</kbd> and the slack variables determined a movable margin that partially captured the existing overlap. Of course, it's impossible to separate the sets in a perfect way with a linear classifier and, on the other hand, most real-life problems are non-linear; for this reason, it's a <span>necessary</span><span> </span><span>further step.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kernel-based classification</h1>
                </header>
            
            <article>
                
<p>When working with non-linear problems, it's useful to transform the original vectors by projecting them into a higher dimensional space where they can be linearly separated. We saw a similar approach when we discussed polynomial regression. SVMs also adopt the same approach, even if there's <span>now</span><span> </span><span>a complexity problem that we need to overcome. Our mathematical formulation becomes:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="83" width="191" src="assets/13acd1b1-0e4a-44b7-b546-76a7174f2e5d.png"/></div>
<p class="CDPAlignLeft CDPAlign">Every feature vector is now filtered by a non-linear function that can completely reshape the scenario. However, the introduction of such a function increased the computational complexity in a way that could apparently discourage this approach. To understand what has happened, it's necessary to express the quadratic problem using Lagrange multipliers. The entire procedure is beyond the scope of this book (in Nocedal J., Wright S. J., <em>Numerical Optimization</em>, Springer, you can find a complete and formal description of quadratic programming problems); however, the final formulation is:</p>
<div class="CDPAlignCenter CDPAlign"><img height="103" width="294" src="assets/bd4e49ed-3a01-497c-b755-2fdf06753109.png"/></div>
<p>Therefore it's necessary to compute the following <span>for every couple of vectors</span><span>:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="33" width="92" src="assets/82139cea-c5ca-4297-a4eb-3b519e4733cc.png"/></div>
<p>And this procedure can be a bottleneck, unacceptable for large problems. However, it's now that the so-called <strong>kernel trick</strong> takes place. There are particular functions (called kernels) that have the following property:</p>
<div class="CDPAlignCenter CDPAlign"><img height="40" width="183" src="assets/ec32e87d-dc9b-44b5-bec1-d4bc04037db4.png"/></div>
<p>In other words, the value of the kernel for two feature vectors is the product of the two projected vectors. With this trick, the computational complexity remains almost the same, but we can benefit from the power of non-linear projections even in a very large number of dimensions.</p>
<p>Excluding the linear kernel, which is a simple product, scikit-learn supports three different kernels that can solve many real-life problems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Radial Basis Function</h1>
                </header>
            
            <article>
                
<p>The RBF kernel is the default value for SVC and is based on the function:</p>
<div class="CDPAlignCenter CDPAlign"><img height="43" width="179" src="assets/2268fe32-9d0f-43c7-882c-9c3c51c37479.png"/></div>
<p class="CDPAlignLeft CDPAlign">The gamma <span>parameter</span><span> </span><span>determines the amplitude of the function, which is not influenced by the direction but only by the distance.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Polynomial kernel</h1>
                </header>
            
            <article>
                
<p>The polynomial kernel is based on the function:</p>
<div class="CDPAlignCenter CDPAlign"><img height="45" width="219" src="assets/54a30a9a-c292-4d46-9561-9f3d54313be7.png"/></div>
<p>The exponent <em>c</em> is specified through the parameter degree, while the constant term <em>r</em> is called <kbd>coef0</kbd>. This function can easily expand the dimensionality with a large number of support variables and overcome very non-linear problems; however, the requirements in terms of resources are normally higher. Considering that a non-linear function can often be approximated quite well for a bounded area (by adopting polynomials), it's not surprising that many complex problems become easily solvable using this kernel.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sigmoid kernel</h1>
                </header>
            
            <article>
                
<p>The sigmoid kernel is based on this function:</p>
<div class="CDPAlignCenter CDPAlign"><img height="67" width="239" src="assets/48cff76f-30b8-402d-b170-81fa7035e466.png"/></div>
<p class="CDPAlignLeft CDPAlign">The constant term <em>r</em> is specified through the parameter <kbd>coef0</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Custom kernels</h1>
                </header>
            
            <article>
                
<p>Normally, built-in kernels can efficiently <span>solve</span><span> </span><span>most real-life problems; however scikit-learn allows us to create custom kernels as normal Python functions:</span></p>
<pre><strong>import numpy as np </strong><br/> <br/><strong>&gt;&gt;&gt; def custom_kernel(x1, x2): </strong><br/><strong>       return np.square(np.dot(x1, x2) + 1)</strong></pre>
<p>The function can be passed to SVC through the <kbd>kernel</kbd> parameter, which can assume fixed string values (<kbd>'linear'</kbd>, <kbd>'rbf'</kbd>, <kbd>'poly'</kbd> and <kbd>'sigmoid'</kbd>) or a callable (such as <kbd>kernel=custom_kernel</kbd>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Non-linear examples</h1>
                </header>
            
            <article>
                
<p>To show the power of kernel SVMs, we're going to solve two problems. The first one is simpler but purely non-linear and the dataset is generated through the <kbd>make_circles()</kbd> built-in function:</p>
<pre><strong>from sklearn.datasets import make_circles </strong><br/> <br/><strong>&gt;&gt;&gt; nb_samples = 500 </strong><br/><strong>&gt;&gt;&gt; X, Y = make_circles(n_samples=nb_samples, noise=0.1)</strong></pre>
<p>A plot of this dataset is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/6cdb11e2-c2f8-4311-9b95-31f15d3014b1.png"/></div>
<p>As it's possible to see, a linear classifier can never separate the two sets and every approximation will contain on average 50% misclassifications. A logistic regression example is shown here:</p>
<pre><strong>from sklearn.linear_model import LogisticRegression </strong><br/> <br/><strong>&gt;&gt;&gt; lr = LogisticRegression() </strong><br/><strong>&gt;&gt;&gt; cross_val_score(lr, X, Y, scoring='accuracy', cv=10).mean() </strong><br/><strong>0.438</strong></pre>
<p>As expected, the accuracy is below 50% and no other optimizations can increase it dramatically. Let's consider, instead, a grid search with an SVM and different kernels (keeping the default values of each one):</p>
<pre><strong>import multiprocessing </strong><br/><strong>from sklearn.model_selection import GridSearchCV </strong><br/> <br/><strong>&gt;&gt;&gt; param_grid = [ </strong><br/><strong>    {  </strong><br/><strong>        'kernel': ['linear', 'rbf', 'poly', 'sigmoid'], </strong><br/><strong>        'C': [ 0.1, 0.2, 0.4, 0.5, 1.0, 1.5, 1.8, 2.0, 2.5, 3.0 ] </strong><br/><strong>    } </strong><br/><strong>] </strong><br/> <br/><strong>&gt;&gt;&gt; gs = GridSearchCV(estimator=SVC(), param_grid=param_grid, </strong><br/><strong>                  scoring='accuracy', cv=10, n_jobs=multiprocessing.cpu_count()) </strong><br/> <br/><strong>&gt;&gt;&gt; gs.fit(X, Y) </strong><br/><strong>GridSearchCV(cv=10, error_score='raise', </strong><br/><strong>       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, </strong><br/><strong>  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf', </strong><br/><strong>  max_iter=-1, probability=False, random_state=None, shrinking=True, </strong><br/><strong>  tol=0.001, verbose=False), </strong><br/><strong>       fit_params={}, iid=True, n_jobs=8, </strong><br/><strong>       param_grid=[{'kernel': ['linear', 'rbf', 'poly', 'sigmoid'], 'C': [0.1, 0.2, 0.4, 0.5, 1.0, 1.5, 1.8, 2.0, 2.5, 3.0]}], </strong><br/><strong>       pre_dispatch='2*n_jobs', refit=True, return_train_score=True, </strong><br/><strong>       scoring='accuracy', verbose=0) </strong><br/> <br/><strong>&gt;&gt;&gt; gs.best_estimator_ </strong><br/><strong>SVC(C=2.0, cache_size=200, class_weight=None, coef0=0.0, </strong><br/><strong>  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf', </strong><br/><strong>  max_iter=-1, probability=False, random_state=None, shrinking=True, </strong><br/><strong>  tol=0.001, verbose=False) </strong><br/> <br/><strong>&gt;&gt;&gt; gs.best_score_ </strong><br/><strong>0.87</strong></pre>
<p>As expected from the geometry of our dataset, the best kernel is a radial basis function, which yields 87% accuracy. Further refinements on <kbd>gamma</kbd> could slightly increase this value, but as there is a partial overlap between the two subsets, it's very difficult to achieve an accuracy close to 100%. However, our goal is not to overfit our model; it is to guarantee an appropriate level of generalization. So, considering the shape, a limited number of misclassifications is acceptable to ensure that the model captures sub-oscillations in the boundary surface.</p>
<p>Another interesting example is provided by the MNIST handwritten digit dataset. We have already seen it and classified it using linear models. Now we can try to find the best kernel with an SVM:</p>
<pre><strong>from sklearn.datasets import load_digits </strong><br/> <br/><strong>&gt;&gt;&gt; digits = load_digits() </strong><br/> <br/><strong>&gt;&gt;&gt; param_grid = [ </strong><br/><strong>    {  </strong><br/><strong>        'kernel': ['linear', 'rbf', 'poly', 'sigmoid'], </strong><br/><strong>        'C': [ 0.1, 0.2, 0.4, 0.5, 1.0, 1.5, 1.8, 2.0, 2.5, 3.0 ] </strong><br/><strong>    } </strong><br/><strong>] </strong><br/> <br/><strong>&gt;&gt;&gt; gs = GridSearchCV(estimator=SVC(), param_grid=param_grid, </strong><br/><strong>                  scoring='accuracy', cv=10, n_jobs=multiprocessing.cpu_count()) </strong><br/> <br/><strong>&gt;&gt;&gt; gs.fit(digits.data, digits.target) </strong><br/><strong>GridSearchCV(cv=10, error_score='raise', </strong><br/><strong>       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, </strong><br/><strong>  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf', </strong><br/><strong>  max_iter=-1, probability=False, random_state=None, shrinking=True, </strong><br/><strong>  tol=0.001, verbose=False), </strong><br/><strong>       fit_params={}, iid=True, n_jobs=8, </strong><br/><strong>       param_grid=[{'kernel': ['linear', 'rbf', 'poly', 'sigmoid'], 'C': [0.1, 0.2, 0.4, 0.5, 1.0, 1.5, 1.8, 2.0, 2.5, 3.0]}], </strong><br/><strong>       pre_dispatch='2*n_jobs', refit=True, return_train_score=True, </strong><br/><strong>       scoring='accuracy', verbose=0) </strong><br/> <br/><strong>&gt;&gt;&gt; gs.best_estimator_ </strong><br/><strong>SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0, </strong><br/><strong>  decision_function_shape=None, degree=3, gamma='auto', kernel='poly', </strong><br/><strong>  max_iter=-1, probability=False, random_state=None, shrinking=True, </strong><br/><strong>  tol=0.001, verbose=False) </strong><br/> <br/><strong>&gt;&gt;&gt; gs.best_score_ </strong><br/><strong>0.97885364496382865</strong></pre>
<p>Hence the best classifier (with almost 98% accuracy) is based on a polynomial kernel and a very low <kbd>C</kbd> value. This means that a non-linear transformation with very hard margins can easily capture the dynamics of all digits. Indeed, SVMs (with various internal alternatives) have always shown excellent performance with this dataset and their usage can easily <span>be</span><span> </span><span>extended to similar problems.</span></p>
<p>Another interesting example is based on the Olivetti face dataset, which is not part of scikit-learn but can be automatically downloaded and set up using a built-in function called <kbd>fetch_olivetti_faces()</kbd>:</p>
<pre><strong>from sklearn.datasets import fetch_olivetti_faces</strong><br/><br/><strong>&gt;&gt;&gt; faces = fetch_olivetti_faces(data_home='/ML/faces/')</strong></pre>
<p>Through the <kbd>data_home</kbd> parameter, it is possible to specify in which local folder the dataset must be placed. A subset of samples is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="492" width="496" class="image-border" src="assets/29c1cfe5-9c08-44c8-b5f2-19bac1d60f67.png"/></div>
<p>There are 40 different people and each of them is represented with 10 pictures of 64 x 64 pixels. The number of classes (40) is not high, but considering the similarity of many photos, a good classifier should be able to capture some specific anatomical details. Performing a grid search with non-linear kernels, we get:</p>
<pre><strong>&gt;&gt;&gt; param_grid = [</strong><br/><strong> { </strong><br/><strong>   'kernel': ['rbf', 'poly'],</strong><br/><strong>   'C': [ 0.1, 0.5, 1.0, 1.5 ],</strong><br/><strong>   'degree': [2, 3, 4, 5],</strong><br/><strong>   'gamma': [0.001, 0.01, 0.1, 0.5]</strong><br/><strong> }</strong><br/><strong>]</strong><br/><br/><strong>&gt;&gt;&gt; gs = GridSearchCV(estimator=SVC(), param_grid=param_grid, scoring='accuracy', cv=8,  n_jobs=multiprocessing.cpu_count())</strong><br/><strong>&gt;&gt;&gt; gs.fit(faces.data, faces.target)</strong><br/><strong>GridSearchCV(cv=8, error_score='raise',</strong><br/><strong>       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,</strong><br/><strong>  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',</strong><br/><strong>  max_iter=-1, probability=False, random_state=None, shrinking=True,</strong><br/><strong>  tol=0.001, verbose=False),</strong><br/><strong>       fit_params={}, iid=True, n_jobs=8,</strong><br/><strong>       param_grid=[{'kernel': ['rbf', 'poly'], 'C': [0.1, 0.5, 1.0, 1.5], 'gamma': [0.001, 0.01, 0.1, 0.5], 'degree': [2, 3, 4, 5]}],</strong><br/><strong>       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,</strong><br/><strong>       scoring='accuracy', verbose=0)</strong><br/><br/><strong>&gt;&gt;&gt; gs.best_estimator_</strong><br/><strong>SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,</strong><br/><strong>  decision_function_shape=None, degree=2, gamma=0.1, kernel='poly',</strong><br/><strong>  max_iter=-1, probability=False, random_state=None, shrinking=True,</strong><br/><strong>  tol=0.001, verbose=False)</strong></pre>
<p>So the best estimator is polynomial-based with <kbd>degree=2</kbd>, and the corresponding accuracy is:</p>
<pre><strong>&gt;&gt;&gt; gs.best_score_</strong><br/><strong>0.96999999999999997</strong></pre>
<p>This confirms the ability of SVM to capture non-linear dynamics even with simple kernels that can be computed in a very limited amount of time. It would be interesting for the reader to try different parameter combinations or preprocess the data and apply principal component analysis to reduce its dimensionality.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Controlled support vector machines</h1>
                </header>
            
            <article>
                
<p>With real datasets, SVM can extract a very large number of support vectors <span>to increase accuracy,</span><span> and</span><span> that can slow down the whole process. To allow finding out a trade-off between precision and number of support vectors, scikit-learn provides an implementation called</span> <kbd>NuSVC</kbd><span>, where the parameter</span> <kbd>nu</kbd> <span>(bounded between 0—not included—and 1) can be used to control at the same time the number of support vectors (greater values will increase their number) and training errors (lower values reduce the fraction of errors). Let's consider an example with a linear kernel and a simple dataset. In the following figure, there's a scatter plot of our set:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/96bb39ac-c5b9-4100-bc07-46b56e06bff0.png"/></div>
<p>Let's start checking the number of support vectors for a standard SVM:</p>
<pre><strong>&gt;&gt;&gt; svc = SVC(kernel='linear') </strong><br/><strong>&gt;&gt;&gt; svc.fit(X, Y) </strong><br/><strong>&gt;&gt;&gt; svc.support_vectors_.shape </strong><br/><strong>(242L, 2L)</strong></pre>
<p>So the model has found 242 support vectors. Let's now try to optimize this number using cross-validation. The default value is 0.5, which is an acceptable trade-off:</p>
<pre><strong>from sklearn.svm import NuSVC </strong><br/> <br/><strong>&gt;&gt;&gt; nusvc = NuSVC(kernel='linear', nu=0.5) </strong><br/><strong>&gt;&gt;&gt; nusvc.fit(X, Y) </strong><br/><strong>&gt;&gt;&gt; nusvc.support_vectors_.shape </strong><br/><strong>(251L, 2L) </strong><br/> <br/><strong>&gt;&gt;&gt; cross_val_score(nusvc, X, Y, scoring='accuracy', cv=10).mean() </strong><br/><strong>0.80633213285314143</strong></pre>
<p>As expected, the behavior is similar to a standard SVC. Let's now reduce the value of <kbd>nu</kbd>:</p>
<pre><strong>&gt;&gt;&gt; nusvc = NuSVC(kernel='linear', nu=0.15) </strong><br/><strong>&gt;&gt;&gt; nusvc.fit(X, Y) </strong><br/><strong>&gt;&gt;&gt; nusvc.support_vectors_.shape </strong><br/><strong>(78L, 2L) </strong><br/> <br/><strong>&gt;&gt;&gt; cross_val_score(nusvc, X, Y, scoring='accuracy', cv=10).mean() </strong><br/><strong>0.67584393757503003</strong></pre>
<p>In this case, the number of support vectors is less than before and also the accuracy has been affected by this choice. Instead of trying different values, we can look for the best choice with a grid search:</p>
<pre><strong>import numpy as np </strong><br/> <br/><strong>&gt;&gt;&gt; param_grid = [ </strong><br/><strong>    {  </strong><br/><strong>        'nu': np.arange(0.05, 1.0, 0.05) </strong><br/><strong>    } </strong><br/><strong>] </strong><br/> <br/><strong>&gt;&gt;&gt; gs = GridSearchCV(estimator=NuSVC(kernel='linear'), param_grid=param_grid, </strong><br/><strong>                  scoring='accuracy', cv=10, n_jobs=multiprocessing.cpu_count()) </strong><br/><strong>&gt;&gt;&gt; gs.fit(X, Y) </strong><br/><strong>GridSearchCV(cv=10, error_score='raise', </strong><br/><strong>       estimator=NuSVC(cache_size=200, class_weight=None, coef0=0.0, </strong><br/><strong>   decision_function_shape=None, degree=3, gamma='auto', kernel='linear', </strong><br/><strong>   max_iter=-1, nu=0.5, probability=False, random_state=None, </strong><br/><strong>   shrinking=True, tol=0.001, verbose=False), </strong><br/><strong>       fit_params={}, iid=True, n_jobs=8, </strong><br/><strong>       param_grid=[{'nu': array([ 0.05,  0.1 ,  0.15,  0.2 ,  0.25,  0.3 ,  0.35,  0.4 ,  0.45, </strong><br/><strong>        0.5 ,  0.55,  0.6 ,  0.65,  0.7 ,  0.75,  0.8 ,  0.85,  0.9 ,  0.95])}], </strong><br/><strong>       pre_dispatch='2*n_jobs', refit=True, return_train_score=True, </strong><br/><strong>       scoring='accuracy', verbose=0) </strong><br/> <br/><strong>&gt;&gt;&gt; gs.best_estimator_ </strong><br/><strong>NuSVC(cache_size=200, class_weight=None, coef0=0.0, </strong><br/><strong>   decision_function_shape=None, degree=3, gamma='auto', kernel='linear', </strong><br/><strong>   max_iter=-1, nu=0.5, probability=False, random_state=None, </strong><br/><strong>   shrinking=True, tol=0.001, verbose=False) </strong><br/> <br/><strong>&gt;&gt;&gt; gs.best_score_ </strong><br/><strong>0.80600000000000005 </strong><br/> <br/><strong>&gt;&gt;&gt; gs.best_estimator_.support_vectors_.shape </strong><br/><strong>(251L, 2L)</strong></pre>
<p>Therefore, in this case as well, the default value of 0.5 yielded the most accurate results. Normally, this approach works quite well, but when it's necessary to reduce the number of support vectors, it can be a good starting point for progressively reducing the value of <kbd>nu</kbd> until the result is acceptable.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Support vector regression</h1>
                </header>
            
            <article>
                
<p>scikit-learn provides a support vector regressor based on a very simple variant of the algorithm already described (see the original documentation for further information). The real power of this approach resides in the usage of non-linear kernels (in particular, polynomials); however, the user is advised to evaluate the degree progressively because the complexity can grow rapidly, together with the training time.</p>
<p>For our example, I've created a dummy dataset based on a second-order noisy function:</p>
<pre><strong>&gt;&gt;&gt; nb_samples = 50 </strong><br/> <br/><strong>&gt;&gt;&gt; X = np.arange(-nb_samples, nb_samples, 1) </strong><br/><strong>&gt;&gt;&gt; Y = np.zeros(shape=(2 * nb_samples,)) </strong><br/> <br/><strong>&gt;&gt;&gt; for x in X: </strong><br/><strong>       Y[int(x)+nb_samples] = np.power(x*6, 2.0) / 1e4 + np.random.uniform(-2, 2)</strong></pre>
<p>The dataset in plotted in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/27ae755b-e335-45e8-a74d-bc694fac9ef6.png"/></div>
<p>In order to avoid a very long training process, the model is evaluated with <kbd>degree</kbd> set to <kbd>2</kbd>. The epsilon parameter allows us to specify a soft margin for predictions; if a predicted value is contained in the ball centered on the target value and the radius is equal to epsilon, no penalty is applied to the function to be minimized. The default value is 0.1:</p>
<pre><strong>from sklearn.svm import SVR </strong><br/> <br/><strong>&gt;&gt;&gt; svr = SVR(kernel='poly', degree=2, C=1.5, epsilon=0.5) </strong><br/><strong>&gt;&gt;&gt; cross_val_score(svr, X.reshape((nb_samples*2, 1)), Y, scoring='neg_mean_squared_error', cv=10).mean() </strong><br/><strong>-1.4641683636397234</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<p>Nocedal J., Wright S. J., <em>Numerical Optimization</em>, Springer</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we discussed how a support vector machine works <span>in</span><span> </span><span>both linear and non-linear scenarios, starting from the basic mathematical formulation. The main concept is to find the hyperplane that maximizes the distance between the classes by using a limited number of samples (called support vectors) that are closest to the separation margin.</span></p>
<p>We saw how to transform a non-linear problem using kernel functions, which allow remapping of the original space to a another high-dimensional one where the problem becomes linearly separable. We also saw how to control the number of support vectors and how to use SVMs for regression problems.</p>
<p>In the next chapter, we're going to introduce another classification method called decision trees, which is the last one explained in this book.</p>


            </article>

            
        </section>
    </body></html>