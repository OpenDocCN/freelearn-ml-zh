<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Hierarchical Clustering</h1>
                </header>
            
            <article>
                
<p>In this chapter, we're going to discuss a particular clustering technique called hierarchical clustering. Instead of working with the relationships existing in the whole dataset, this approach starts with a single entity containing all elements (divisive) or N separate elements (agglomerative), and proceeds by splitting or merging the clusters according to some specific criteria, which we're going to analyze and compare.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hierarchical strategies</h1>
                </header>
            
            <article>
                
<p>Hierarchical clustering is based on the general concept of finding a hierarchy of partial clusters, built using either a bottom-up or a top-down approach. More formally, they are called:</p>
<ul>
<li><strong>Agglomerative clustering</strong>:  The process starts from the bottom (each initial cluster is made up of a single element) and proceeds by merging the clusters until a stop criterion is reached. In general, the target has a sufficiently small number of clusters at the end of the process.</li>
<li><strong>Divisive clustering</strong>:<strong> </strong>In this case, the initial state is a single cluster with all samples and the process proceeds by splitting the intermediate cluster until all elements are separated. At this point, the process continues with an aggregation criterion based on the dissimilarity between elements. A famous approach (which is beyond the scope of this book) called <strong>DIANA</strong> is described in Kaufman L., Roussew P.J., <em><span class="a-size-large">Finding Groups In Data: An Introduction To Cluster Analysis</span></em><span class="a-size-large">, Wiley</span>.</li>
</ul>
<p>scikit-learn implements only the agglomerative clustering. However, this is not a real limitation because the complexity of divisive clustering is higher and the performances of agglomerative clustering are quite similar to the ones achieved by the divisive approach.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Agglomerative clustering</h1>
                </header>
            
            <article>
                
<p>Let's consider the following dataset:</p>
<div class="CDPAlignCenter CDPAlign"><img height="45" width="252" src="assets/403e8d5c-2021-48a5-b640-1512d5f61bf3.png"/></div>
<p class="CDPAlignLeft CDPAlign">We define <strong>affinity</strong>, a metric function of two arguments with the same dimensionality <em>m</em>. The most common metrics (also supported by scikit-learn) are:</p>
<ul>
<li><strong>Euclidean</strong> or <em>L2</em>:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="70" width="351" src="assets/6068dda2-c6f1-4f82-93c4-280ca75d55b5.png"/></div>
<ul>
<li><strong>Manhattan</strong> (also known as City Block) or <em>L1</em>:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="59" width="329" src="assets/3c2e2e77-d772-4f7b-8661-4403eaa4e496.png"/></div>
<ul>
<li><strong>Cosine distance</strong>:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="50" width="251" src="assets/d325da43-5b82-4426-8617-269e9039515c.png"/></div>
<p>The Euclidean distance is normally a good choice, but sometimes it's useful to a have a metric whose difference with the Euclidean one gets larger and larger. The Manhattan metric has this property; to show it, in the following figure there's a plot representing the distances from the origin of points belonging to the line <em>y = x</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/99347e35-cd84-405f-9c60-9d3b5e92e451.png"/></div>
<p class="mce-root CDPAlignLeft CDPAlign">The cosine distance, instead, is useful when we need a distance proportional to the angle between two vectors. If the direction is the same, the distance is null, while it is maximum when the angle is equal to 180° (meaning opposite directions). This distance can be employed when the clustering must not consider the <em>L2</em> norm of each point. For example, a dataset could contain bidimensional points with different scales and we need to group them into clusters corresponding to circular sectors. Alternatively, we could be interested in their position according to the four quadrants because we have assigned a specific meaning (invariant to the distance between a point and the origin) to each of them. </p>
<p class="CDPAlignLeft CDPAlign">Once a metric has been chosen (let's simply call it <em>d(x,y)</em>), the next step is defining a strategy (called <strong>linkage</strong>) to aggregate different clusters. There are many possible methods, but scikit-learn supports the three most common ones:</p>
<ul>
<li><strong>Complete linkage</strong>: For each pair of clusters, the algorithm computes and merges them to minimize the maximum distance between the clusters (in other words, the distance of the farthest elements):</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="48" width="390" src="assets/f6e971dc-fe8a-4bc9-b8ca-42be837dcb97.png"/></div>
<ul>
<li><strong>Average linkage</strong>: It's similar to complete linkage, but in this case the algorithm uses the average distance between the pairs of clusters:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="61" width="311" src="assets/0dfa57ed-60c7-4d4c-996c-3d8be1184fd2.png"/></div>
<ul>
<li><strong>Ward's linkage</strong>: In this method, all clusters are considered and the algorithm computes the sum of squared distances within the clusters and merges them to minimize it. From a statistical viewpoint, the process of agglomeration leads to a reduction in the variance of each resulting cluster. The measure is:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="69" width="290" src="assets/b5095ee1-5ea0-40a3-b453-617cc1cda4b1.png"/></div>
<ul>
<li class="CDPAlignCenter CDPAlign">
<div class="CDPAlignLeft CDPAlign">The Ward's linkage supports only the Euclidean distance.</div>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dendrograms</h1>
                </header>
            
            <article>
                
<p>To better understand the agglomeration process, it's useful to introduce a graphical method called a <strong>dendrogram</strong>, which shows in a static way how the aggregations are performed, starting from the bottom (where all samples are separated) till the top (where the linkage is complete). Unfortunately, scikit-learn doesn't support them. However, SciPy (which is a mandatory requirement for it) provides some useful built-in functions.</p>
<p>Let's start by creating a dummy dataset:</p>
<pre><strong>from sklearn.datasets import make_blobs</strong><br/><br/><strong>&gt;&gt;&gt; nb_samples = 25</strong><br/><strong>&gt;&gt;&gt; X, Y = make_blobs(n_samples=nb_samples, n_features=2, centers=3, cluster_std=1.5)</strong></pre>
<p>To avoid excessive complexity in the resulting plot, the number of samples has been kept very low. In the following figure, there's a representation of the dataset:</p>
<div class="CDPAlignCenter CDPAlign"><img height="346" width="428" class="image-border" src="assets/9fc2356b-469f-4608-9b89-8ddb84aaa110.png"/></div>
<p>Now we can compute the dendrogram. The first step is computing a distance matrix:</p>
<pre><strong>from scipy.spatial.distance import pdist</strong><br/><br/><strong>&gt;&gt;&gt; Xdist = pdist(X, metric='euclidean')</strong></pre>
<p>We have chosen a Euclidean metric, which is the most suitable in this case. At this point, it's necessary to decide which linkage we want. Let's take Ward; however, all known methods are supported:</p>
<pre><strong>from scipy.cluster.hierarchy import linkage</strong><br/><br/><strong>&gt;&gt;&gt; Xl = linkage(Xdist, method='ward')</strong></pre>
<p>Now, it's possible to create and visualize a dendrogram:</p>
<pre><strong>from scipy.cluster.hierarchy import dendrogram</strong><br/><br/><strong>&gt;&gt;&gt; Xd = dendrogram(Xl)</strong></pre>
<p>The resulting plot is shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img height="347" width="430" class="image-border" src="assets/aeb9496e-6b92-4693-9997-536fa9fa4e81.png"/></div>
<p>In the <em>x</em> axis, there are the samples (numbered progressively), while the <em>y</em> axis represents the distance. Every arch connects two clusters that are merged together by the algorithm. For example, 23 and 24 are single elements merged together. The element 13 is then aggregated to the resulting cluster, and so the process continues.</p>
<p>As you can see, if we decide to cut the graph at the distance of 10, we get two separate clusters: the first one from 15 to 24 and the other one from 0 to 20. Looking at the previous dataset plot, all the points with <em>Y</em> &lt; 10 are considered to be part of the first cluster, while the others belong to the second cluster. If we increase the distance, the linkage becomes very aggressive (particularly in this example with only a few samples) and with values greater than 27, only one cluster is generated (even if the internal variance is quite high!).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Agglomerative clustering in scikit-learn</h1>
                </header>
            
            <article>
                
<p>Let's consider a more complex dummy dataset with 8 centers:</p>
<pre><strong>&gt;&gt;&gt; nb_samples = 3000</strong><br/><strong>&gt;&gt;&gt; X, _ = make_blobs(n_samples=nb_samples, n_features=2, centers=8, cluster_std=2.0)</strong></pre>
<p>A graphical representation is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="303" width="367" class="image-border" src="assets/f8717c2a-bc36-4558-91e9-a159466d9e6f.png"/></div>
<p>We can now perform an agglomerative clustering with different linkages (always keeping the Euclidean distance) and compare the results. Let's start with a complete linkage (<kbd>AgglomerativeClustering</kbd> uses the method <kbd>fit_predict()</kbd> to train the model and transform the original dataset):</p>
<pre><strong>from sklearn.cluster import AgglomerativeClustering</strong><br/><br/><strong>&gt;&gt;&gt; ac = AgglomerativeClustering(n_clusters=8, linkage='complete')</strong><br/><strong>&gt;&gt;&gt; Y = ac.fit_predict(X)</strong></pre>
<p>A plot of the result (using both different markers and colors) is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="400" width="507" class="image-border" src="assets/abd002fc-e335-49f4-90b6-b3164b37f179.png"/></div>
<p>The result is totally bad. This approach penalizes the inter-variance and merges cluster, which in most cases should be different. In the previous plot, the three clusters in the middle are quite fuzzy, and the probability of wrong placement is very high considering the variance of the cluster represented by dots. Let's now consider the average linkage:</p>
<pre><strong>&gt;&gt;&gt; ac = AgglomerativeClustering(n_clusters=8, linkage='average')</strong><br/><strong>&gt;&gt;&gt; Y = ac.fit_predict(X)</strong></pre>
<p>The result is shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img height="416" width="520" class="image-border" src="assets/3e522106-8595-4b55-abd8-65a7588910ba.png"/></div>
<p>In this case, the clusters are better defined, even if some of them could have become really small. It can also be useful to try other metrics (in particular <em>L1</em>) and compare the results. The last method, which is often the best (it's the default one), is Ward's linkage, that can be used only with a Euclidean metric (also the default one):</p>
<pre><strong>&gt;&gt;&gt; ac = AgglomerativeClustering(n_clusters=8)</strong><br/><strong>&gt;&gt;&gt; Y = ac.fit_predict(X)</strong></pre>
<p>The resulting plot is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="386" width="483" class="image-border" src="assets/e4badc80-5485-42c6-8e06-fa09fc61ce90.png"/></div>
<p>In this case, it's impossible to modify the metric so, as also suggested in the official scikit-learn documentation, a valid alternative could be the average linkage, which can be used with any affinity.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Connectivity constraints</h1>
                </header>
            
            <article>
                
<p>scikit-learn also allows specifying a connectivity matrix, which can be used as a constraint when finding the clusters to merge. In this way, clusters which are far from each other (non-adjacent in the connectivity matrix) are skipped. A very common method for creating such a matrix involves using the k-nearest neighbors graph function (implemented as <kbd>kneighbors_graph()</kbd>), that is based on the number of neighbors a sample has (according to a specific metric). In the following example, we consider a circular dummy dataset (often used in the official documentation also):</p>
<pre><strong>from sklearn.datasets import make_circles</strong><br/><br/><strong>&gt;&gt;&gt; nb_samples = 3000</strong><br/><strong>&gt;&gt;&gt; X, _ = make_circles(n_samples=nb_samples, noise=0.05)</strong></pre>
<p>A graphical representation is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="385" width="472" class="image-border" src="assets/d90add68-2585-4421-b9ab-1554e941611d.png"/></div>
<p>We start with unstructured agglomerative clustering based on average linkage and impose 20 clusters:</p>
<pre><strong>&gt;&gt;&gt; ac = AgglomerativeClustering(n_clusters=20, linkage='average')</strong><br/><strong>&gt;&gt;&gt; ac.fit(X)</strong></pre>
<p>In this case, we have used the method <kbd>fit()</kbd> because the class <kbd>AgglomerativeClustering</kbd>, after being trained, exposes the labels (cluster number) through the instance variable <kbd>labels_</kbd><strong> </strong>and it's easier to use this variable when the number of clusters is very high. A graphical plot of the result is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="408" width="484" class="image-border" src="assets/a133383d-5e1a-4660-9264-31b8bbc09e14.png"/></div>
<p>Now we can try to impose a constraint with different values for <em>k</em>:</p>
<pre><strong>from sklearn.neighbors import kneighbors_graph</strong><br/><br/><strong>&gt;&gt;&gt; acc = []</strong><br/><strong>&gt;&gt;&gt; k = [50, 100, 200, 500]</strong><br/><br/><strong>&gt;&gt;&gt; for i in range(4):</strong><br/><strong>&gt;&gt;&gt;    kng = kneighbors_graph(X, k[i])</strong><br/><strong>&gt;&gt;&gt;    ac1 = AgglomerativeClustering(n_clusters=20, connectivity=kng, linkage='average')</strong><br/><strong>&gt;&gt;&gt;    ac1.fit(X)</strong><br/><strong>&gt;&gt;&gt;    acc.append(ac1)</strong></pre>
<p>The resulting plots are shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/5f59b111-a461-4a2d-8118-6f62d97d9864.png"/></div>
<p>As you can see, imposing a constraint (in this case, based on k-nearest neighbors) allows controlling how the agglomeration creates new clusters and can be a powerful tool for tuning the models, or for avoiding elements whose distance is large in the original space could be taken into account during the merging phase (this is particularly useful when clustering images).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<p><span>Kaufman L., Roussew P.J., </span><span class="a-size-large"><em>Finding Groups In Data: An Introduction To Cluster Analysis</em>, Wiley</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have presented hierarchical clustering, focusing our attention on the agglomerative version, which is the only one supported by scikit-learn. We discussed the philosophy, which is rather different to the one adopted by many other methods. In agglomerative clustering, the process begins by considering each sample as a single cluster and proceeds by merging the blocks until the number of desired clusters is reached. In order to perform this task, two elements are needed: a metric function (also called affinity) and a linkage criterion. The former is used to determine the distance between the elements, while the latter is a target function that is used to determine which clusters must be merged.</p>
<p>We also saw how to visualize this process through dendrograms using SciPy. This technique is quite useful when it's necessary to maintain a complete control of the process and the final number of clusters is initially unknown (it's easier to decide where to cut the graph). We showed how to use scikit-learn to perform agglomerative clustering with different metrics and linkages and, at the end of the chapter, we also introduced the connectivity constraints that are useful when it's necessary to force the process to avoid merging clusters which are too far apart.</p>
<p>In the next chapter, we're going to introduce the recommendation systems, that are employed daily by many different systems to automatically suggest items to a user, according to his/her similarity to other users and <span>their</span> preferences.</p>
<p> </p>


            </article>

            
        </section>
    </body></html>