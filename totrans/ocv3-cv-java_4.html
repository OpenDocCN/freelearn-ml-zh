<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch04"/>Chapter 4. Image Transforms</h1></div></div></div><p>This chapter covers the methods to change an image into an alternate representation of data in order to cover important problems of computer vision and image processing. Some examples of these methods are artifacts that are used to find image edges as well as transforms that help us find lines and circles in an image. In this chapter, we have covered stretch, shrink, warp, and rotate operations. A very useful and famous transform is Fourier, which transforms signals between the time domain and frequency domain. In OpenCV, you can <a id="id140" class="indexterm"/>find the <strong>Discrete Fourier Transform</strong> (<strong>DFT</strong>) and <strong>Discrete Cosine Transform</strong> (<strong>DCT</strong>). Another transform that we've covered in <a id="id141" class="indexterm"/>this chapter is related to integral images that allow rapid summing of sub regions, which is a very useful step in tracking faces algorithm. Besides this, you will also get to see distance transform and histogram equalization in this chapter.</p><p>We will cover the following topics:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Gradients and sobel derivatives</li><li class="listitem" style="list-style-type: disc">The Laplace and canny transforms</li><li class="listitem" style="list-style-type: disc">The line and circle Hough transforms</li><li class="listitem" style="list-style-type: disc">Geometric transforms: stretch, shrink, warp, and rotate</li><li class="listitem" style="list-style-type: disc">Discrete Fourier Transform (DFT) and Discrete Cosine Transform (DCT)</li><li class="listitem" style="list-style-type: disc">Integral images</li><li class="listitem" style="list-style-type: disc">Distance transforms</li><li class="listitem" style="list-style-type: disc">Histogram equalization</li></ul></div><p>By the end of this chapter, you will have learned a handful of transforms that will enable you to find edges, lines, and circles in images. Besides, you will be able to stretch, shrink, warp, and rotate images as well as you will be able to change the domain from the spatial domain to the frequency domain. Other important transforms used for face tracking will be covered in this chapter as well. Finally, distance transforms and histogram equalization will also be explored in detail.</p><div><div><div><div><h1 class="title"><a id="ch04lvl1sec30"/>The Gradient and Sobel derivatives</h1></div></div></div><p>A key <a id="id142" class="indexterm"/>building block in computer vision is finding edges and this is closely related to finding an approximation to derivatives in an image. From <a id="id143" class="indexterm"/>basic calculus, it is known that a derivative shows the variation of a given function or an input signal with some dimension. When we find the local maximum of the derivative, this will yield regions where the signal varies the most, which for an image might mean an edge. Hopefully, there's an easy way to approximate a derivative for discrete signals through a kernel convolution. A convolution basically means applying some transforms to every part of the image. The most used transform for differentiation is the Sobel filter [1], which works for horizontal, vertical, and even mixed partial derivatives of any order.</p><p>In order to approximate the value for the horizontal derivative, the following sobel kernel matrix is convoluted with an input image:</p><div><img src="img/3972OS_04_12.jpg" alt="The Gradient and Sobel derivatives"/></div><p>This means that, for each input pixel, the calculated value of its upper-right neighbor plus twice its right neighbor, plus its bottom-right neighbor, minus its upper-left neighbor, minus its left neighbor, minus its left-bottom neighbor will be calculated, yielding a resulting image. In order to use this operator in OpenCV, you can call Imgproc's <code class="literal">Sobel</code> function according to the following signature:</p><div><pre class="programlisting">public static void Sobel(Mat src, Mat dst, int ddepth, int dx,int dy)</pre></div><p>The <code class="literal">src</code> parameter is the input image and <code class="literal">dst</code> is the output. <code class="literal">Ddepth</code> is the output image's depth and when this is assigned as <code class="literal">-1</code>, this has the same depth as the source. The <code class="literal">dx</code> and <code class="literal">dy</code> parameters will inform us about the order in each of these directions. When setting <code class="literal">dy</code> to <code class="literal">0</code> and <code class="literal">dx</code> to <code class="literal">1</code>, the kernel that we've used is the one mentioned in the preceding matrix. The example project <code class="literal">kernels</code> from this chapter shows a customizable look of these operators, as shown in the following screenshot:</p><div><img src="img/3972OS_04_01.jpg" alt="The Gradient and Sobel derivatives"/></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec31"/>The Laplace and Canny transforms</h1></div></div></div><p>Another <a id="id144" class="indexterm"/>quite useful operator to find edges is the Laplacian <a id="id145" class="indexterm"/>transformation. Instead of relying on the first order derivatives, OpenCV's Laplacian transformation implements the discrete operator for the following function:</p><div><img src="img/3972OS_04_13.jpg" alt="The Laplace and Canny transforms"/></div><p>The matrix can be approximated to the convolution with the following kernel when using finite difference methods and a 3x3 aperture:</p><div><img src="img/3972OS_04_14.jpg" alt="The Laplace and Canny transforms"/></div><p>The signature for the preceding function is as follows:</p><div><pre class="programlisting">Laplacian(Mat source, Mat destination, int ddepth)</pre></div><p>While source and destination matrices are simple parameters, <code class="literal">ddepth</code> is the depth of the destination matrix. When you set this parameter to <code class="literal">-1</code>, it will have the same depth as the source image, although you might want more depth when you apply this operator. Besides this, there are overloaded versions of this method that receive an aperture size, a scale factor, and an adding scalar.</p><p>Besides using the Laplacian method, you can also use the Canny algorithm, which is an excellent approach that was proposed by computer scientist John F. Canny, who optimized edge detection for low error rate, single identification, and correct localization. In order to fulfill it, the Canny algorithm applies a Gaussian to filter the noise, calculates intensity gradients through sobel, suppresses spurious responses, and applies double thresholds followed by a hysteresis that suppresses the weak and unconnected edges. For more information, check this paper [2]. The method's signature is as follows:</p><div><pre class="programlisting">Canny(Mat image, Mat edges, double threshold1, double threshold2, int apertureSize, boolean L2gradient)</pre></div><p>The <code class="literal">image</code> parameter <a id="id146" class="indexterm"/>is the input matrix, <code class="literal">edges</code> is the output image, <code class="literal">threshold1</code> is the first threshold for the hysteresis procedure (values smaller than this will be ignored), and <code class="literal">threshold2</code> is the high threshold for hysteresis (values higher than <a id="id147" class="indexterm"/>this will be considered as strong edges, while the smaller values and the ones higher than the low threshold will be checked for connection with strong edges). The aperture size is used for the Sobel operator when calculating the gradient and the <code class="literal">boolean</code> informs us which norm to use for the gradient. You can also check out the source code to use this operator in the kernel's project sample in this chapter.</p></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec32"/>The line and circle Hough transforms</h1></div></div></div><p>In case you need to <a id="id148" class="indexterm"/>find straight lines or circles in an image, you can use Hough transforms, as they are very useful. In this section, we will cover OpenCV methods to extract them from your image.</p><p>The idea behind <a id="id149" class="indexterm"/>the original Hough line transform is that any point in a binary image could be part of a set of lines. Suppose each straight line could be parameterized by the <em>y =</em> <em>mx + b</em> line equation, where <em>m</em> is the line slope and <em>b</em> is the <em>y</em> axis intercept of this line. Now, we could iterate the whole binary image, storing each of the <em>m</em> and <em>b</em> parameters and checking their accumulation. The local maximum points of the <em>m</em> and <em>b</em> parameters would yield equations of straight lines that mostly appeared in the image. Actually, instead of using the slope and <em>y</em> axis interception point, we use the polar straight line representation.</p><p>Since OpenCV not only supports the standard Hough transform, but also the progressive probabilistic Hough transform for which the two functions are <code class="literal">Imgproc.HoughLines</code> and <code class="literal">Imgproc.HoughLinesP</code>, respectively. For detailed information, refer to [3]. These functions' signatures are explained as follows:</p><div><pre class="programlisting">HoughLines(Mat image, Mat lines, double rho, double theta, int threshold)
HoughLinesP(Mat image, Mat lines, double rho, double theta, int threshold)</pre></div><p>The <code class="literal">hough</code> project <a id="id150" class="indexterm"/>from this chapter shows an example of the usage of them. The following is the code to retrieve lines from <code class="literal">Imgproc.HoughLines</code>:</p><div><pre class="programlisting">Mat canny = new Mat();
Imgproc.Canny(originalImage, canny, 10, 50, aperture, false);
image = originalImage.clone();
Mat lines = new Mat();
Imgproc.HoughLines(canny, lines, 1, Math.PI/180, lowThreshold);</pre></div><p>Note that we need <a id="id151" class="indexterm"/>to apply the Hough transform over an edge image; therefore, the first two lines of the preceding code will take care of this. Then, the original image is cloned for display and a <code class="literal">Mat</code> object is created in the fourth line in order to keep the lines. In the last line, we can see the application of <code class="literal">HoughLines</code>.</p><p>The third parameter in  <code class="literal">Imgproc.HoughLines</code> refers to the distance resolution of the accumulator in pixels, while the fourth parameter is the angle resolution of the accumulator in radians. The fifth parameter is the accumulator threshold, which means that only the lines with more than the specified amount of votes will be returned. The <code class="literal">lowThreshold</code> variable is tied to the scale slider in the example application for the user to experiment with it. It is important to observe that the lines are returned in the <code class="literal">lines</code> matrix, which has two columns in which each line returns the <code class="literal">rho</code> and <code class="literal">theta</code> parameters of the polar coordinates. These coordinates refer to the distance between the top-left corner of the image and the line rotation in radians, respectively. Following this example, you will find out how to draw the lines from the returned matrix. You can see the working of the Hough transform in the following screenshot:</p><div><img src="img/3972OS_04_02.jpg" alt="The line and circle Hough transforms"/></div><p>Besides having the <a id="id152" class="indexterm"/>standard Hough transform, OpenCV also offers a probabilistic Hough line transform as well as a circular version. Both the <a id="id153" class="indexterm"/>implementations are explored in the same <code class="literal">Hough</code> sample project, and the following screenshot shows the working of the circular version:</p><div><img src="img/3972OS_04_03.jpg" alt="The line and circle Hough transforms"/></div></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec33"/>Geometric transforms – stretch, shrink, warp, and rotate</h1></div></div></div><p>While working with images and computer vision, it is very common that you will require the ability to preprocess an image using known geometric transforms, such as stretching, shrinking, rotation, and warping. The latter is the same as nonuniform resizing. These transforms can be realized through the multiplication of source points with a 2 x 3 matrix and they get the name of <a id="id154" class="indexterm"/>
<strong>affine transformations</strong> while turning rectangles in parallelograms. Hence, they have the limitation of requiring the destination to have parallel sides. On the other hand, a 3 x 3 matrix multiplication represents perspective transforms. They <a id="id155" class="indexterm"/>offer more flexibility since they can map a 2D quadrilateral to another. The following screenshot shows a very useful application of this concept.</p><p>Here, we will find out which is the perspective transform that maps the side of a building in a perspective view to its frontal view:</p><div><img src="img/3972OS_04_04.jpg" alt="Geometric transforms – stretch, shrink, warp, and rotate"/></div><p>Note that the input to this problem is the perspective photograph of the building, which is seen on the left-hand side of the preceding image, as well as the four corner points of the highlighted quadrilateral shape. The output is to the right and shows what a viewer would see if he/she looks at the side of the building.</p><p>Since affine transforms are a subset of perspective transformations, we will focus on the latter ones here. The code available for this example is in the <code class="literal">warps</code> project of this chapter. The main method used here is <code class="literal">warpPerspective</code> from <code class="literal">Imgproc</code>. It applies a perspective transformation to an input image. Here is the method signature for the <code class="literal">warpPerspective</code> method:</p><div><pre class="programlisting">public static void warpPerspective(Mat src, Mat dst, Mat M, Size dsize)</pre></div><p>The<code class="literal"> Mat src</code> parameter is, naturally, the input image, which is the left-hand side image in the preceding screenshot, while <code class="literal">dst Mat</code> is the image on the right-hand side; make sure you initialize this parameter before using the method. The not-so-straightforward parameter here is <code class="literal">Mat M</code>, which <a id="id156" class="indexterm"/>is the warping matrix. In order to calculate it, you can use the <code class="literal">getPerspectiveTransform</code> method from <code class="literal">Imgproc</code> as well. This method will calculate the perspective matrix from two sets of the four correlated 2D points, the source and destination points. In our example, the source points are the ones that are highlighted on the left-hand side of the screenshot, while the destination points are the four corner points of the image to the right. These points can be stored through the <code class="literal">MatOfPoint2f</code> class, which stores the <code class="literal">Point</code> objects. The <code class="literal">getPerspectiveTransform</code> method's signature is as follows:</p><div><pre class="programlisting">public static Mat getPerspectiveTransform(Mat src, Mat dst)</pre></div><p><code class="literal">Mat src</code> and <code class="literal">Mat dst</code> are the same as the <code class="literal">MatOfPoint2f</code> class mentioned previously, which is a subclass of <code class="literal">Mat</code>.</p><p>In our example, we added a mouse listener to retrieve points clicked by the user. A detail to be kept in mind is that these points are stored in the order: top-left, top-right, bottom-left, and bottom-right. In the example application, the currently modified point can be chosen through four radio <a id="id157" class="indexterm"/>buttons above the images. The act of clicking and dragging listeners has been added to the code, so both approaches work.</p></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec34"/>Discrete Fourier Transform and Discrete Cosine Transform</h1></div></div></div><p>When <a id="id158" class="indexterm"/>dealing with image analysis, it would be very useful if you could change an image from the spatial domain, which is the image in terms of its <em>x</em> and <em>y</em> coordinates, to the frequency domain—the image decomposed in its high and low frequency components—so that you would be able to see and manipulate frequency parameters. This could come in handy in image compression because it is known that human vision is not much sensitive to high frequency signals as it is to low frequency signals. In this way, you could transform an image from the spatial domain to the frequency domain and remove high frequency components, reducing the required memory to represent the image and hence compressing it. An image frequency can be pictured in a better way by the next image.</p><p>In order to change an image from the spatial domain to the frequency domain, the Discrete Fourier Transform can be used. As we might need to bring it back from the frequency domain to the spatial domain, another transform, which is the Inverse Discrete Fourier Transform, can be applied.</p><p>The formal <a id="id159" class="indexterm"/>definition of DFT is as follows:</p><div><img src="img/3972OS_04_15.jpg" alt="Discrete Fourier Transform and Discrete Cosine Transform"/></div><p>The <code class="literal">f(i,j)</code> value is the image in the spatial domain and <code class="literal">F(k,l)</code> is the image in the frequency domain. Note that <code class="literal">F(k,l)</code> is a complex function, which means that it has a real and an imaginary part. This way, it will be represented by two OpenCV <code class="literal">Mat</code> objects or by <code class="literal">Mat</code> with two channels. The easiest way to analyze a DFT is by plotting its magnitude and taking its logarithm, since values for the DFT can be in different orders of magnitude.</p><p>For instance, this is a pulse pattern, which is a signal that can come from zero, represented as black, to the top, represented as white, on its left, and its Fourier transform magnitude with the applied logarithm to its right:</p><div><img src="img/3972OS_04_05.jpg" alt="Discrete Fourier Transform and Discrete Cosine Transform"/></div><p>Looking back at the preceding DFT transform, we can think of <code class="literal">F(k,l)</code> as the value that would be yielded by multiplying each point of the spatial image with a base function, which is related to the frequency domain, and by summing the products. Remember that base functions are sinusoidal and they have increasing frequencies. This way, if some of the base functions oscillate at the same rate as the signal, it will be able to sum up to a big number, which <a id="id160" class="indexterm"/>will be seen as a white dot on the Fourier Transform image. On the other hand, if the given frequency is not present in the image, the oscillation and multiplication with the image will result in a small number, which won't be noticed in the Fourier Transform image.</p><p>Another thing to observe from the equation is that <code class="literal">F(0,0)</code> will yield a base function that is always <code class="literal">1</code>. This way, <code class="literal">F(0,0)</code> will simply refer to the sum of all the pixels of the spatial image. We can also check whether <code class="literal">F(N-1, N-1)</code> corresponds to the base function related to the highest frequency in the image. Note that the previous image basically has a DC component, which would be the image mean and it could be checked from the white dot in the middle of the Discrete Fourier transform image. Besides, the image to the left could be seen as a series of pulses and hence it would have a frequency in the <em>x</em> axis, which can be noticed by the two dots near the central point in the Fourier Transform image to the right. Nonetheless, we will need to use multiple frequencies to approximate the pulse shape. In this way, more dots can be seen in the <em>x</em>-axis of the image to the right. The following screenshot gives more insight and helps you understand the Fourier analysis:</p><div><img src="img/3972OS_04_06.jpg" alt="Discrete Fourier Transform and Discrete Cosine Transform"/></div><p>Now, we will again check the DC level at the center of the DFT image, to the right, as a bright central dot. Besides, we can also check multiple frequencies in a diagonal pattern. An important piece of information that can be retrieved is the direction of spatial variation, which is clearly seen as bright dots in the DFT image.</p><p>It is time to work on some code now. The following code shows you how to make room to apply the <a id="id161" class="indexterm"/>DFT. Remember, from the preceding screenshot, that the result of a DFT is complex. Besides, we need them stored as floating point values. This way, we first convert our 3-channel image to gray and then to a float. After this, we put the converted image and an empty <code class="literal">Mat</code> object into a list of mats, combining them into a single <code class="literal">Mat</code> object through the use of the <code class="literal">Core.merge</code> function, shown as follows:</p><div><pre class="programlisting">Mat gray = new Mat();
Imgproc.cvtColor(originalImage, gray, Imgproc.COLOR_RGB2GRAY);
Mat floatGray = new Mat();
gray.convertTo(floatGray, CvType.CV_32FC1);

List&lt;Mat&gt; matList = new ArrayList&lt;Mat&gt;();
matList.add(floatGray);
Mat zeroMat = Mat.zeros(floatGray.size(), CvType.CV_32F);
matList.add(zeroMat);
Mat complexImage = new Mat();
Core.merge(matList, complexImage);</pre></div><p>Now, it's easy to apply an in-place Discrete Fourier Transform:</p><div><pre class="programlisting">Core.dft(complexImage,complexImage);</pre></div><p>In order to get some meaningful information, we will print the image, but first, we have to obtain its magnitude. In order to get it, we will use the standard way that we learned in school, which is getting the square root of the sum of the squares of the real and complex parts of numbers.</p><p>Again, OpenCV has a function for this, which is <code class="literal">Core.magnitude</code>, whose signature is <code class="literal">magnitude(Mat x, Mat y, Mat magnitude)</code>, as shown in the following code:</p><div><pre class="programlisting">List&lt;Mat&gt; splitted = new ArrayList&lt;Mat&gt;();
Core.split(complexImage,splitted);
Mat magnitude = new Mat();
Core.magnitude(splitted.get(0), splitted.get(1), magnitude);</pre></div><p>Before using <code class="literal">Core.magnitude</code>, just pay attention to the process of unpacking a DFT in the splitted mats using <code class="literal">Core.split</code>.</p><p>Since the values can be in different orders of magnitude, it is important to get the values in a logarithmic <a id="id162" class="indexterm"/>scale. Before doing this, it is important to add <code class="literal">1</code> to all the values in the matrix just to make sure we won't get negative values when applying the <code class="literal">log</code> function. Besides this, there's already an OpenCV function to deal with logarithms, which is <code class="literal">Core.log</code>:</p><div><pre class="programlisting">Core.add(Mat.ones(magnitude.size(), CvType.CV_32F), magnitude, magnitude);
Core.log(magnitude, magnitude);</pre></div><p>Now, it is time to shift the image to the center, so that it's easier to analyze its spectrum. The code to do this is simple and goes like this:</p><div><pre class="programlisting">int cx = magnitude.cols()/2;
int cy = magnitude.rows()/2;
Mat q0 = new Mat(magnitude,new Rect(0, 0, cx, cy));   
Mat q1 = new Mat(magnitude,new Rect(cx, 0, cx, cy));  
Mat q2 = new Mat(magnitude,new Rect(0, cy, cx, cy));  
Mat q3 = new Mat(magnitude ,new Rect(cx, cy, cx, cy));
Mat tmp = new Mat();
q0.copyTo(tmp);
q3.copyTo(q0);
tmp.copyTo(q3);

q1.copyTo(tmp);
q2.copyTo(q1);
tmp.copyTo(q2);</pre></div><p>As a last step, it's important to normalize the image, so that it can be seen in a better way. Before we normalize it, it should be converted to CV_8UC1:</p><div><pre class="programlisting">magnitude.convertTo(magnitude, CvType.CV_8UC1);
Core.normalize(magnitude, magnitude,0,255, Core.NORM_MINMAX, CvType.CV_8UC1);</pre></div><p>When using the DFT, it's often enough to calculate only half of the DFT when you deal with real-valued data, as is the case with images. This way, an analog concept called the Discrete Cosine <a id="id163" class="indexterm"/>Transform can be used. In case you want it, it can be invoked through <code class="literal">Core.dct</code>.</p></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec35"/>Integral images</h1></div></div></div><p>Some face recognition <a id="id164" class="indexterm"/>algorithms, such as OpenCV's face detection algorithm make heavy use of features like the ones shown in the following image:</p><div><img src="img/3972OS_04_07.jpg" alt="Integral images"/></div><p>These are the so-called <a id="id165" class="indexterm"/>Haar-like features and they are calculated as the sum of pixels in the white area minus the sum of pixels in the black area. You might find <a id="id166" class="indexterm"/>this type of a feature kind of odd, but when training it for face detection, it can be built to be an extremely powerful classifier using only two of these features, as depicted in the following image:</p><div><img src="img/3972OS_04_08.jpg" alt="Integral images"/></div><p>In fact, a classifier that uses only the two preceding features can be adjusted to detect 100 percent of a given face training database with only 40 percent of false positives. Taking out the sum of all pixels in an image as well as calculating the sum of each area can be a long process. However, this process must be tested for each frame in a given input image, hence calculating these features fast is a requirement that we need to fulfill.</p><p>First, let's define an <a id="id167" class="indexterm"/>integral image sum as the following expression:</p><div><img src="img/3972OS_04_16.jpg" alt="Integral images"/></div><p>For instance, if the following matrix represents our image:</p><div><img src="img/3972OS_04_17.jpg" alt="Integral images"/></div><p>An integral image would be like the following:</p><div><img src="img/3972OS_04_18.jpg" alt="Integral images"/></div><p>The trick here follows from the following property:</p><div><img src="img/3972OS_04_19.jpg" alt="Integral images"/></div><p>This means that in order to <a id="id168" class="indexterm"/>find the sum of a given rectangle bounded by the points <code class="literal">(x1,y1)</code>, <code class="literal">(x2,y1)</code>, <code class="literal">(x2,y2)</code>, and <code class="literal">(x1,y2)</code>, you just need to use the integral image at the point <code class="literal">(x2,y2)</code>, but you also need to subtract the points <code class="literal">(x1-1,y2)</code> from <code class="literal">(x2,y1-1)</code>. Also, since the integral image at <code class="literal">(x1-1, y1-1)</code> has been subtracted twice, we just need to add it once.</p><p>The following code will generate the preceding matrix and make use of <code class="literal">Imgproc.integral</code> to create the integral images:</p><div><pre class="programlisting">Mat image = new Mat(3,3 ,CvType.CV_8UC1);
Mat sum = new Mat();
byte[] buffer = {0,2,4,6,8,10,12,14,16};
image.put(0,0,buffer);
System.out.println(image.dump());
Imgproc.integral(image, sum);
System.out.println(sum.dump());</pre></div><p>The output of this program is like the one shown in the preceding matrices for A and Sum A.</p><p>It is important to verify that the output is a 4 x 4 matrix because of the initial row and column of zeroes, which are used to make the computation efficient.</p></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec36"/>Distance transforms</h1></div></div></div><p>Simply put, a distance <a id="id169" class="indexterm"/>transform applied to an image will generate an output image whose pixel values will be the closest distance to a zero-valued pixel in the input image. Basically, they will have the closest distance to the background, given a specified distance measure. The following screenshot gives you an idea of what happens to the silhouette of a human body:</p><div><img src="img/3972OS_04_09.jpg" alt="Distance transforms"/><div><p>Human silhouette by J E Theriot</p></div></div><p>This transform can be very useful in the process of getting the topological skeleton of a given segmented image as well as to produce blurring effects. Another interesting application of this transform is in the segmentation of overlapping objects, along with a watershed.</p><p>Generally, the distance <a id="id170" class="indexterm"/>transform is applied to an edge image, which results from a Canny filter. We are going to make use of Imgproc's<code class="literal"> distanceTransform</code> method, which can be seen in action in the <code class="literal">distance</code> project, which you can find in this chapter's source code. Here are the most important lines of this example program:</p><div><pre class="programlisting">protected void processOperation() {
  Imgproc.Canny(originalImage, image, 220, 255, 3, false);
  Imgproc.threshold(image, image, 100, 255, Imgproc.THRESH_BINARY_INV );
  Imgproc.distanceTransform(image, image, Imgproc.CV_DIST_L2, 3);
  image.convertTo(image, CvType.CV_8UC1);
  Core.multiply(image, new Scalar(20), image);

  updateView();
}</pre></div><p>Firstly, a Canny edge detector filter is applied to the input image. Then, a threshold with <code class="literal">THRESH_BINARY_INV</code> converts the edges to black and beans to white. Only then, the distance transform is applied. The first argument is the input image, the second one is the output matrix, and the third argument specifies how distances are calculated. In our example, <code class="literal">CVDIST_L2</code> means Euclidean, while other distances, such as <code class="literal">CVDIST_L1</code> or <code class="literal">CVDIST_L12</code>, among others exist. Since the output of <code class="literal">distanceTtransform</code> is a single channel 32 bit Float image, a conversion is required. Finally, we apply <code class="literal">Core.multiply</code> to increase the contrast.</p><p>The following screenshot <a id="id171" class="indexterm"/>gives you a good idea of the whole process:</p><div><img src="img/3972OS_04_10.jpg" alt="Distance transforms"/></div></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec37"/>Histogram equalization</h1></div></div></div><p>The human visual system is very sensitive to contrast in images, which is the difference in the color and brightness of different objects. Besides, the human eye is a miraculous system that can feel intensities at the 10<sub>16</sub> light levels [4]. No wonder some sensors could mess up the image data.</p><p>When analyzing <a id="id172" class="indexterm"/>images, it is very useful to draw their histograms. They simply show you the lightness distribution of a digital image. In order to do that, you need to count the number of pixels with the exact lightness and plot that as a distribution graph. This gives us a great insight into the dynamic range of an image.</p><p>When a camera picture has been captured with a very narrow light range, it gets difficult to see the details in the shadowed areas or other areas with poor local contrast. Fortunately, there's a technique to spread frequencies for uniform intensity distribution, which is called <a id="id173" class="indexterm"/>
<strong>histogram equalization</strong>. The following image shows the same picture with their respective histograms before and after the histogram equalization technique is applied:</p><div><img src="img/3972OS_04_11.jpg" alt="Histogram equalization"/></div><p>Note that the light values, located at the rightmost part of the upper histogram, are rarely used, while the middle range values are too tied. Spreading the values along the full range yields better contrast and details can be more easily perceived by this. The histogram equalized image makes better use of intensities that generate better contrast. In order to accomplish this task, a cumulative distribution can be used to remap the histogram to something that resembles a uniform distribution. Then, it's just a matter of checking where the points from the original <a id="id174" class="indexterm"/>histogram would be mapped to the uniform distribution through the use of a cumulative Gaussian distribution, for instance.</p><p>Now, the good part is that all these details have been wrapped in a simple call to OpenCV's <code class="literal">equalizeHist</code> function. Here is the sample from the <code class="literal">histogram</code> project in this chapter:</p><div><pre class="programlisting">protected void processOperation() {
  Imgproc.cvtColor(originalImage, grayImage, Imgproc.COLOR_RGB2GRAY);
  Imgproc.equalizeHist(grayImage, image);
  updateView();
}</pre></div><p>This piece of code simply converts the image to a single channel image; however, you can use <code class="literal">equalizeHist</code> on a color <a id="id175" class="indexterm"/>image as long as you treat each channel separately. The <code class="literal">Imgproc.equalizeHist</code> method outputs the corrected image following the previously mentioned concept.</p></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec38"/>References</h1></div></div></div><div><ol class="orderedlist arabic"><li class="listitem"><em>A 3x3 Isotropic Gradient Operator for Image Processing</em> presented at a talk at the Stanford Artificial Project in 1968, by I. Sobel and G. Feldman.</li><li class="listitem"><em>A Computational Approach To Edge Detection</em>, IEEE Trans. Pattern Analysis and Machine Intelligence, by Canny, J.</li><li class="listitem"><em>Robust Detection of Lines Using the Progressive Probabilistic Hough Transform</em>, CVIU 78 1, by Matas, J. and Galambos, C., and Kittler, J.V. pp 119-137 (2000).</li><li class="listitem"><em>Advanced High Dynamic Range Imaging: Theory and Practice</em>, CRC Press, by Banterle, Francesco; Artusi, Alessandro; Debattista, Kurt; Chalmers, Alan.</li></ol></div></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec39"/>Summary</h1></div></div></div><p>This chapter covered the key aspects of computer vision's daily use. We started with the important edge detectors, where you gained the experience of how to find them through the Sobel, Laplacian, and Canny edge detectors. Then, we saw how to use the Hough transforms to find straight lines and circles. After that, the geometric transforms stretch, shrink, warp, and rotate were explored with an interactive sample. We then explored how to transform images from the spatial domain to the frequency domain using the Discrete Fourier analysis. After that, we showed you a trick to calculate Haar-like features fast in an image through the use of integral images. We then explored the important distance transforms and finished the chapter by explaining histogram equalization to you.</p><p>Now, be ready to dive into machine learning algorithms, as we will cover how to detect faces in the next chapter. Also, you will learn how to create your own object detector and understand how supervised learning works in order to better train your classification trees.</p></div></body></html>