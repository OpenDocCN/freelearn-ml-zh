["```py\nimport twitter\nconsumer_key = \"<Your Consumer Key Here>\"\nconsumer_secret = \"<Your Consumer Secret Here>\"\naccess_token = \"<Your Access Token Here>\"\naccess_token_secret = \"<Your Access Token Secret Here>\"\nauthorization = twitter.OAuth(access_token, \naccess_token_secret, consumer_key, consumer_secret)\nt = twitter.Twitter(auth=authorization, retry=True)\n\n```", "```py\nimport os \ndata_folder = os.path.join(os.path.expanduser(\"~\"), \"Data\", \"twitter\")\noutput_filename = os.path.join(data_folder, \"python_tweets.json\")\n\n```", "```py\noriginal_users = [] \ntweets = []\nuser_ids = {}\n\n```", "```py\nsearch_results = t.search.tweets(q=\"python\", count=100)['statuses']\nfor tweet in search_results:\n    if 'text' in tweet:\n        original_users.append(tweet['user']['screen_name']) \n        user_ids[tweet['user']['screen_name']] = tweet['user']['id']\n        tweets.append(tweet['text'])\n\n```", "```py\nfrom sklearn.externals import joblib\noutput_filename = os.path.join(os.path.expanduser(\"~\"), \"Models\", \"twitter\", \"python_context.pkl\")\n\n```", "```py\njoblib.dump(model, output_filename)\n\n```", "```py\nmodel_filename = os.path.join(os.path.expanduser(\"~\"), \"Models\", \"twitter\", \"python_context.pkl\")\n\n```", "```py\nimport spacy\nfrom sklearn.base import TransformerMixin\n\n# Create a spaCy parser\nnlp = spacy.load('en')\n\nclass BagOfWords(TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        results = []\n        for document in X:\n            row = {}\n            for word in list(nlp(document, tag=False, parse=False, entity=False)):\n                if len(word.text.strip()): # Ignore words that are just whitespace\n                    row[word.text] = True\n                    results.append(row)\n        return results\n\n```", "```py\nfrom sklearn.externals import joblib\ncontext_classifier = joblib.load(model_filename)\n\n```", "```py\ny_pred = context_classifier.predict(tweets)\n\n```", "```py\nrelevant_tweets = [tweets[i] for i in range(len(tweets)) if y_pred[i] == 1]\nrelevant_users = [original_users[i] for i in range(len(tweets)) if y_pred[i] == 1]\n\n```", "```py\nimport time\n\ndef get_friends(t, user_id):\n    friends = []\n    cursor = -1\n    while cursor != 0: \n        try:\n            results = t.friends.ids(user_id= user_id, cursor=cursor, count=5000)\n            friends.extend([friend for friend in results['ids']])\n            cursor = results['next_cursor'] \n            if len(friends) >= 10000:\n                break\n        except TypeError as e:\n            if results is None:\n                print(\"You probably reached your API limit, waiting for 5 minutes\")\n                sys.stdout.flush() \n                time.sleep(5*60) # 5 minute wait \n            else: \n                # Some other error happened, so raise the error as normal\n                raise e\n        except twitter.TwitterHTTPError as e:\n            print(e)\n            break\n        finally:\n            # Break regardless -- this stops us going over our API limit\n            time.sleep(60)\n\n```", "```py\nfriends = {} \nfor screen_name in relevant_users:\n    user_id = user_ids[screen_name]\n    friends[user_id] = get_friends(t, user_id)\n\n```", "```py\nfriends = {user_id:friends[user_id] \n           for user_id in friends\n           if len(friends[user_id]) > 0}\n\n```", "```py\nfrom collections import defaultdict\ndef count_friends(friends): \n    friend_count = defaultdict(int)\n    for friend_list in friends.values(): \n        for friend in friend_list:\n            friend_count[friend] += 1 \n    return friend_count\n\n```", "```py\nfriend_count = count_friends(friends)\nfrom operator import itemgetter\nbest_friends = sorted(friend_count, key=friend_count.get, reverse=True)\n\n```", "```py\nwhile len(friends) < 150:\n    for user_id, count in best_friends:\n        if user_id in friends:\n            # Already have this user, move to next one\n            continue\n        friends[user_id] = get_friends(t, user_id) \n        for friend in friends[user_id]: \n            friend_count[friend] += 1\n        best_friends = sorted(friend_count.items(), key=itemgetter(1), reverse=True)\n        break\n\n```", "```py\nimport json\nfriends_filename = os.path.join(data_folder, \"python_friends.json\")\nwith open(friends_filename, 'w') as outf: \n    json.dump(friends, outf)\n\n```", "```py\nwith open(friends_filename) as inf:\n    friends = json.load(inf)\n\n```", "```py\nimport networkx as nx \nG = nx.DiGraph()\n\n```", "```py\nmain_users = friends.keys() \nG.add_nodes_from(main_users)\n\n```", "```py\nfor user_id in friends:\n    for friend in friends[user_id]:\n        if str(friend) in main_users: \n            G.add_edge(user_id, friend) \n\n```", "```py\n %matplotlib inline \n nx.draw(G)\n\n```", "```py\nfrom matplotlib import pyplot as plt\nplt.figure(3,figsize=(20,20))\nnx.draw(G, alpha=0.1, edge_color='b')\n\n```", "```py\nfriends = {user: set(friends[user]) for user in friends}\n\n```", "```py\ndef compute_similarity(friends1, friends2):\n    return len(friends1 & friends2) / (len(friends1 | friends2)  + 1e-6)\n\n```", "```py\ndef create_graph(followers, threshold=0): \n    G = nx.Graph()\n    for user1 in friends.keys(): \n        for user2 in friends.keys(): \n            if user1 == user2:\n                continue\n            weight = compute_similarity(friends[user1], friends[user2])\n            if weight >= threshold:\n                G.add_node(user1) \n                G.add_node(user2)\n                G.add_edge(user1, user2, weight=weight)\n    return G\n\n```", "```py\nG = create_graph(friends)\n\n```", "```py\nplt.figure(figsize=(10,10))\n\n```", "```py\npos = nx.spring_layout(G)\n\n```", "```py\nnx.draw_networkx_nodes(G, pos)\n\n```", "```py\nedgewidth = [ d['weight'] for (u,v,d) in G.edges(data=True)]\n\n```", "```py\nnx.draw_networkx_edges(G, pos, width=edgewidth)\n\n```", "```py\nG = create_graph(friends, 0.1)\n\n```", "```py\nsub_graphs = nx.connected_component_subgraphs(G)\n\n```", "```py\nfor i, sub_graph in enumerate(sub_graphs):\n    n_nodes = len(sub_graph.nodes()) \n    print(\"Subgraph {0} has {1} nodes\".format(i, n_nodes))\n\n```", "```py\nG = create_graph(friends, 0.25) \nsub_graphs = nx.connected_component_subgraphs(G) \nfor i, sub_graph in enumerate(sub_graphs): \n    n_nodes = len(sub_graph.nodes()) \n    print(\"Subgraph {0} has {1} nodes\".format(i, n_nodes))\n\n```", "```py\nsub_graphs = nx.connected_component_subgraphs(G) \nn_subgraphs = nx.number_connected_components(G)\n\n```", "```py\nfig = plt.figure(figsize=(20, (n_subgraphs * 3)))\nfor i, sub_graph in enumerate(sub_graphs): \n    ax = fig.add_subplot(int(n_subgraphs / 3) + 1, 3, i + 1)\n    ax.get_xaxis().set_visible(False) \n    ax.get_yaxis().set_visible(False)\n    pos = nx.spring_layout(G) \n    nx.draw_networkx_nodes(G, pos, sub_graph.nodes(), ax=ax, node_size=500) \n    nx.draw_networkx_edges(G, pos, sub_graph.edges(), ax=ax)\n\n```", "```py\nimport numpy as np\nfrom sklearn.metrics import silhouette_score\n\ndef compute_silhouette(threshold, friends):\n    G = create_graph(friends, threshold=threshold) \n    if len(G.nodes()) < 2:\n        return -99\n    sub_graphs = nx.connected_component_subgraphs(G)\n\n    if not (2 <= nx.number_connected_components() < len(G.nodes()) - 1): \n        return -99\n\n    label_dict = {}\n    for i, sub_graph in enumerate(sub_graphs): \n        for node in sub_graph.nodes(): \n            label_dict[node] = i\n\n    labels = np.array([label_dict[node] for node in G.nodes()])\n    X = nx.to_scipy_sparse_matrix(G).todense()\n    X = 1 - X\n    return silhouette_score(X, labels, metric='precomputed')\n\n```", "```py\ndef inverted_silhouette(threshold, friends):\n    return -compute_silhouette(threshold, friends)\n\n```", "```py\nfrom scipy.optimize import minimize\nresult = minimize(inverted_silhouette, 0.1, args=(friends,))\n\n```"]