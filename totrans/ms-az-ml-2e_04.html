<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer094">
			<h1 id="_idParaDest-55"><em class="italic"><a id="_idTextAnchor054"/>Chapter 3</em>: Preparing the Azure Machine Learning Workspace</h1>
			<p>In the previous chapter, we learned how to navigate different Azure services for implementing ML solutions in the cloud. We realized that the best service for training custom ML models programmatically and automating infrastructure and deployments is the Azure Machine Learning service. In this chapter, we will set up and explore the Azure Machine Learning workspace, create a cloud training cluster, and perform data experimentation locally and on cloud compute, while collecting all the artifacts of the ML runs in Azure Machine Learning.</p>
			<p>In the first section, we will learn how to manage Azure resources using different tools such as the Azure <strong class="bold">Command-Line Interface (CLI)</strong>, the Azure SDKs, and <strong class="bold">Azure Resource Manager (ARM)</strong> templates. We will set up and explore the Azure CLI, as well as Azure Machine Learning extensions, and subsequently deploy an Azure Machine Learning workspace.</p>
			<p>We will then look under the hood of Azure Machine Learning by exploring the resources that were deployed as part of Azure Machine Learning, such as the storage account, Azure Key Vault, Azure Application Insights, and Azure Container Registry. Following that, we will dive into Azure Machine Learning and explore the workspace to better understand the individual components.</p>
			<p>Finally, in the last section, we will put all this knowledge into practice and run our first experiment with Azure Machine Learning. After setting up our environment, we will enhance a simple ML Keras training script to log metrics, logs, models, and code snapshots into Azure Machine Learning. We will then progress to schedule training runs on our local machine as well as on a training cluster in Azure. </p>
			<p>By the end of this chapter, you will see all your successful training runs, metrics, and tracked models in your Azure Machine Learning workspace, and you will have a good understanding of Azure Machine Learning to start your ML journey.</p>
			<p>The following are the topics that will be covered in this chapter:</p>
			<ul>
				<li>Deploying an Azure Machine Learning workspace</li>
				<li>Exploring the Azure Machine Learning service</li>
				<li>Running ML experiments with Azure Machine Learning</li>
			</ul>
			<h1 id="_idParaDest-56"><a id="_idTextAnchor055"/>Technical requirements</h1>
			<p>In this chapter, we will use the following Python libraries and versions to perform and manage experiment runs on Azure Machine Learning:</p>
			<ul>
				<li><strong class="source-inline">azureml-core 1.34.0 </strong></li>
				<li><strong class="source-inline">azureml-sdk 1.34.0 </strong></li>
				<li><strong class="source-inline">azureml-widgets 1.34.0 </strong></li>
				<li><strong class="source-inline">tensorflow 2.6.0 </strong></li>
			</ul>
			<p>You can run this code using either a local Python interpreter or a notebook environment hosted in Azure Machine Learning. However, some scripts need to be scheduled to execute in Azure. </p>
			<p>All code examples in this chapter can be found in the GitHub repository for this book: <a href="https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter03">https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter03</a>. </p>
			<h1 id="_idParaDest-57"><a id="_idTextAnchor056"/>Deploying an Azure Machine Learning workspace </h1>
			<p>Before we can start delving deep into ML on Azure itself, we need to understand how to deploy an Azure Machine Learning workspace <a id="_idIndexMarker294"/>or Azure services in general, what tooling is supported, and which one of those we will use to work with throughout the book.</p>
			<p>As a first step, we will require an Azure subscription. </p>
			<p>If you are working in an organization and want to use your work account, you can go to portal.azure.com and log in with your work account. If the login works, you will land on the portal itself, and your work <a id="_idIndexMarker295"/>account is shown at the top right. This means that your company already has an <strong class="bold">Azure Active Directory</strong> (<strong class="bold">AAD</strong>) instance set up. In this case, talk to your Azure Global Administrator, if you haven't already, to discuss which Azure subscription to use for your purpose. </p>
			<p>If you are new to Azure and want to use your private account, go to azure.com and click on <strong class="bold">Free Account</strong> to create an AAD for yourself with a free trial subscription. This trial gives you a certain amount of money to spend for 30 days on Azure services.</p>
			<p>In any case, in the end, you should have the capability to log in to the Azure portal with your identity, and you <a id="_idIndexMarker296"/>should know which Azure subscription (name and/or subscription ID) you want to deploy your ML services to.</p>
			<p>With this all done, we will now have a look at how to deploy and manage our Azure environment in general and what options and tooling there are to choose from.</p>
			<h2 id="_idParaDest-58"><a id="_idTextAnchor057"/>Understanding the available tooling for Azure deployments</h2>
			<p>In Azure, any action that <a id="_idIndexMarker297"/>deploys or changes an Azure service goes <a id="_idIndexMarker298"/>through <a id="_idIndexMarker299"/>the so-called ARM. As shown in <em class="italic">Figure 3.1</em>, ARM accepts <a id="_idIndexMarker300"/>requests <a id="_idIndexMarker301"/>from either the <strong class="bold">Azure portal</strong>, <strong class="bold">Azure PowerShell</strong> (a PowerShell extension), the <strong class="bold">Azure CLI</strong>, or the <strong class="bold">Azure REST API</strong>:</p>
			<div>
				<div id="_idContainer079" class="IMG---Figure">
					<img src="image/B17928_03_001.jpg" alt="Figure 3.1 – Azure Resource Manager " width="1650" height="489"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.1 – Azure Resource Manager</p>
			<p>In the Azure portal, you can select <strong class="bold">Create a resource</strong> from the left-hand menu to deploy any service or Marketplace image to your subscription. If you search for <strong class="source-inline">machine learning</strong>, the set of results set will show a service called <strong class="bold">Machine Learning</strong> from Microsoft. Clicking on this card and then <strong class="bold">Create</strong> will open the deployment wizard for this service. This will give you a sense of what is required to deploy this service.</p>
			<p>But we will not go any further on the portal itself, as we want to facilitate a more programmatic approach in this book. Using this approach will greatly enable the reproducibility and automation of all the tasks performed in Azure. Therefore, we will concentrate on the latter solutions – let's take a look at them:</p>
			<ul>
				<li><strong class="bold">Azure CLI</strong>: This is a fully fledged command-line environment that you can install on every <a id="_idIndexMarker302"/>major operating system. The latest version can be downloaded from <a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli">https://docs.microsoft.com/en-us/cli/azure/install-azure-cli</a>. </li>
				<li><strong class="bold">Azure Power Shell</strong>: As the name suggests, this is a library of PowerShell modules, which can be added to a PowerShell environment. Previously, PowerShell was <a id="_idIndexMarker303"/>only available on Windows, but the new PowerShell Core 7.x now officially supports the major Linux releases and macOS. The following description shows how to install it on your system: https://docs.microsoft.com/en-us/powershell/azure/install-az-ps.</li>
				<li><strong class="bold">Azure REST API</strong>: This is available to call ARM through REST, which allows you to manage Azure r<a id="_idIndexMarker304"/>esources through <strong class="source-inline">curl</strong> or the popular Python <strong class="source-inline">requests</strong> library. The following article describes the given syntax: https://docs.microsoft.com/en-us/rest/api/resources/.</li>
			</ul>
			<p>All of these <a id="_idIndexMarker305"/>options allow the use of so-called <strong class="bold">ARM templates</strong> (https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/overview), Azure's version of <strong class="bold">Infrastructure as Code</strong> (<strong class="bold">IaC</strong>). It gives you the <a id="_idIndexMarker306"/>ability to save and version-control infrastructure <a id="_idIndexMarker307"/>definitions in files. This way is highly recommended <a id="_idIndexMarker308"/>when dealing with complex infrastructure deployment, but we will not dive any further into this topic. The only additional point to <a id="_idIndexMarker309"/>make here is that there are other tools on the market <a id="_idIndexMarker310"/>for IaC management. The most prominent tool is called <strong class="bold">Terraform</strong> (https://www.terraform.io/), which allows infrastructure management of any cloud vendor or on-premises environment, including Azure. To achieve this, Terraform utilizes the Azure CLI under the hood.</p>
			<p>In summary, you can choose any of the aforementioned options for the tasks at hand, especially if you have a strong preference for one of them. </p>
			<p>As we will not manage complex infrastructure and want to avoid any unnecessary additional levels of complexity, we will utilize the Azure CLI throughout the rest of the book. Furthermore, the <a id="_idIndexMarker311"/>new ML CLI extension offers a couple of neat features for Azure Machine Learning, which we will discover throughout the chapter:</p>
			<div>
				<div id="_idContainer080" class="IMG---Figure">
					<img src="image/B17928_03_002.jpg" alt="Figure 3.2 – The Azure CLI " width="761" height="483"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.2 – The Azure CLI</p>
			<p>If you haven't already, please feel free to download and install or update the CLI with the latest version. When you are ready, open your favorite command line or terminal and type <strong class="source-inline">az</strong> into the console. You should be greeted by the screen shown in <em class="italic">Figure 3.2</em>.</p>
			<h2 id="_idParaDest-59"><a id="_idTextAnchor058"/>Deploying the workspace</h2>
			<p>After this short introduction to ARM, let's deploy our first ML workspace. We will deploy a workspace using <a id="_idIndexMarker312"/>the Azure CLI. If you would like to rather deploy it <a id="_idIndexMarker313"/>via the Azure portal, you can follow this tutorial: https://docs.microsoft.com/en-us/azure/machine-learning/quickstart-create-resources. </p>
			<p>If you had a short look through the list of commands in the CLI, you might have noticed that there seems to be no command referencing ML. Let's rectify this and set up our first Azure Machine Learning workspace via the CLI following these steps:</p>
			<ol>
				<li>Log in to your Azure environment through the CLI:<p class="source-code"><strong class="bold">$ az login</strong></p></li>
			</ol>
			<p>This command will open a website with an AAD login screen. After you have done this, return to the console. The screen will now show you some information about your AAD tenant (<strong class="source-inline">homeTenantId</strong>), your subscriptions (<strong class="source-inline">id</strong>, <strong class="source-inline">name</strong>), and your user. </p>
			<ol>
				<li value="2">If you have more than one subscription shown to you and need to check which subscription is active, use the following command:<p class="source-code"><strong class="bold">$ az account show --output table</strong></p></li>
			</ol>
			<p>In the output, check whether the <strong class="source-inline">IsDefault</strong> column shows <strong class="source-inline">True</strong> for your preferred subscription. If not, use the following command to set it to your chosen one by typing in the name of it – <strong class="source-inline">&lt;yoursub&gt;</strong> – and checking again:</p>
			<p class="source-code"><strong class="bold">$ az account set --subscription "&lt;yoursub&gt;"</strong></p>
			<ol>
				<li value="3">Now that we are <a id="_idIndexMarker314"/>deploying to the correct subscription in the <a id="_idIndexMarker315"/>correct tenant, let's check the situation with the installed extension. Type in the following command in your terminal:<p class="source-code"><strong class="bold">$ az extension list</strong></p></li>
			</ol>
			<p>If neither <strong class="source-inline">azure-cli-ml</strong> nor <strong class="source-inline">ml</strong> is shown in the list, you are missing an extension for using Azure Machine Learning via the CLI. The first of them denotes <strong class="source-inline">Azure ML CLI 1.0</strong>, the second one <strong class="source-inline">Azure ML CLI 2.0</strong>. Version 2 of the ML CLI was announced at Microsoft Build 2021 (<a href="https://techcommunity.microsoft.com/t5/azure-ai/announcing-the-new-cli-and-arm-rest-apis-for-azure-machine/ba-p/2393447">https://techcommunity.microsoft.com/t5/azure-ai/announcing-the-new-cli-and-arm-rest-apis-for-azure-machine/ba-p/2393447</a>), offering fine-grained control of the ML workspace. Therefore, we will be using the new version of the CLI extension. </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Azure ML CLI 2.0 offers new abilities to directly control the jobs, clusters, and pipelines of the ML workspace from the command line. It also offers support for YAML configuration files, which are crucial for MLOps.</p>
			<ol>
				<li value="4">If you are running the old version, you should remove that version, but be aware that, as some commands are slightly different, you might break a script you are already using. To <a id="_idIndexMarker316"/>clean up the namespace and remove the <a id="_idIndexMarker317"/>previous version, you can use the following commands:<p class="source-code"><strong class="bold">$ az extension remove -n azure-cli-ml</strong></p><p class="source-code"><strong class="bold">$ az extension remove -n ml</strong></p></li>
				<li>Let's install the ML extension using the following command:<p class="source-code"><strong class="bold">$ az extension add -n ml</strong></p></li>
			</ol>
			<p>After that, feel free to check the installed extensions again.</p>
			<ol>
				<li value="6">Now, we will be able to use it. First off, we will have a look at the help page for the extension:<p class="source-code"><strong class="bold">$ az ml -h</strong></p></li>
			</ol>
			<p>This will show you the following subgroups:</p>
			<p class="source-code"><strong class="bold">code: Manage Azure ML code assets.</strong></p>
			<p class="source-code"><strong class="bold">compute: Manage Azure ML compute resources.</strong></p>
			<p class="source-code"><strong class="bold">data: Manage Azure ML data assets.</strong></p>
			<p class="source-code"><strong class="bold">datastore: Manage Azure ML datastores.</strong></p>
			<p class="source-code"><strong class="bold">endpoint: Manage Azure ML endpoints.</strong></p>
			<p class="source-code"><strong class="bold">environment: Manage Azure ML environments.</strong></p>
			<p class="source-code"><strong class="bold">job: Manage Azure ML jobs.</strong></p>
			<p class="source-code"><strong class="bold">model: Manage Azure ML models.</strong></p>
			<p class="source-code"><strong class="bold">workspace: Manage Azure ML workspaces.</strong></p>
			<p>As you can see, we have a lot of options to control our workspace from the CLI. We will come back to many of them later in the book. For now, we are interested in managing our workspace. </p>
			<ol>
				<li value="7">If you type the following command, we will have a look to see whether we are still missing requirements for the creation of the ML workspace:<p class="source-code"><strong class="bold">$ az ml workspace create -h</strong></p></li>
			</ol>
			<p>Going through the arguments, you will see that a <strong class="bold">resource group</strong> is required. A resource group in Azure is a logical construct where resources need to be deployed to. It is one vital part of the <strong class="bold">Azure management hierarchy</strong>. For further reading, have a look at access management in Azure: https://docs.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-setup-guide/organize-resources.</p>
			<p>Furthermore, if you <a id="_idIndexMarker318"/>scroll down to the examples in the console <a id="_idIndexMarker319"/>output, you will also see that the new version of the CLI has a neat property that lets us deploy the workspace from a <strong class="bold">Yet Another Markup Language</strong> (<strong class="bold">YAML</strong>) file. We will not do this now, but it is something to keep in mind.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">The Azure Machine Learning service can be completely operated using the Azure ML CLI 2.0 extension, YAML configuration files, and a training or inference script.</p>
			<ol>
				<li value="8">A resource group in Azure also requires a location. Therefore, let's have a look at the available data center locations for the Azure cloud by running this command:<p class="source-code"><strong class="bold">$ az account list-locations -o table</strong></p></li>
			</ol>
			<p>Have a look at the name of your preferred region and use it in the following command to create the resource group. Our example here will create a resource group in West US 2 with the name <strong class="source-inline">mldemo</strong>:</p>
			<p class="source-code"><strong class="bold">$ az group create -n mldemo -l westus2</strong></p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Even though we define the resource group to be in West US 2, resources inside a resource group can be in different regions. It is just best practice to define a group in a specific region and let the resources inside that group be in the same region.</p>
			<ol>
				<li value="9">Now, we can create the workspace itself by using the following command:<p class="source-code"><strong class="bold">$ az ml workspace create -w mldemows -g mldemo -l westus2</strong></p></li>
			</ol>
			<p>This will create a <a id="_idIndexMarker320"/>workspace named <strong class="source-inline">mldemows</strong> in the <strong class="source-inline">mldemo</strong> resource <a id="_idIndexMarker321"/>group. If we remove the location setting, it will take the location of the resource group. </p>
			<p>This command can take a bit of time. When it is done, you will see output like this:</p>
			<p class="source-code"><strong class="bold">AppInsights  Done (7s)</strong></p>
			<p class="source-code"><strong class="bold">StorageAccount ...  Done (31s)</strong></p>
			<p class="source-code"><strong class="bold">KeyVault  Done (23s)</strong></p>
			<p class="source-code"><strong class="bold">Workspace ................  Done (1m 49s)</strong></p>
			<p class="source-code"><strong class="bold">Total time : 2m 26s</strong></p>
			<p class="source-code"><strong class="bold">{</strong></p>
			<p class="source-code"><strong class="bold">"application_insights": "/subscriptions/... ",</strong></p>
			<p class="source-code"><strong class="bold">"description": "mldemows",</strong></p>
			<p class="source-code"><strong class="bold">"discovery_url":"https://westus2.api.azureml.ms/discovery",</strong></p>
			<p class="source-code"><strong class="bold">"friendly_name": "mldemows",</strong></p>
			<p class="source-code"><strong class="bold">"hbi_workspace": false,</strong></p>
			<p class="source-code"><strong class="bold">"key_vault": "/subscriptions/... ",</strong></p>
			<p class="source-code"><strong class="bold">"location": "westus2",</strong></p>
			<p class="source-code"><strong class="bold">"mlflow_tracking_uri": "azureml://westus2.api.azureml.ms/mlflow/v1.0/subscriptions/... ",</strong></p>
			<p class="source-code"><strong class="bold">"name": "mldemows",</strong></p>
			<p class="source-code"><strong class="bold">"storage_account": "/subscriptions/... ",</strong></p>
			<p class="source-code"><strong class="bold">"tags": {}</strong></p>
			<p class="source-code"><strong class="bold">}</strong></p>
			<p>As you can see, the preceding command created multiple resources, together with the Azure Machine Learning <a id="_idIndexMarker322"/>workspace, that are required for <a id="_idIndexMarker323"/>running ML experiments. We will come back to the reasons in the next section.</p>
			<ol>
				<li value="10">Finally, to have a look at the deployment at any point, you can run the following command:<p class="source-code"><strong class="bold">$ az ml workspace show -g mldemo -w mldemows</strong></p></li>
			</ol>
			<p>We have created our first Azure Machine Learning workspace. Good work! In the next section, we will have a look at what this entails.</p>
			<h1 id="_idParaDest-60"><a id="_idTextAnchor059"/>Exploring the Azure Machine Learning service</h1>
			<p>Before we continue to set up our own development environment and do some ML, we will have a look at <a id="_idIndexMarker324"/>what was just deployed besides the main workspace, get a base understanding of all features available in the service, which we will utilize throughout the book, and have a first short look at <strong class="bold">Azure Machine Learning</strong><strong class="bold"> Studio</strong>.</p>
			<h2 id="_idParaDest-61"><a id="_idTextAnchor060"/>Analyzing the deployed services</h2>
			<p>We will start by navigating <a id="_idIndexMarker325"/>to the Azure portal again. There, type the name of the workspace as <strong class="source-inline">mldemows</strong> in the top search bar. You should see something like the result shown in <em class="italic">Figure 3.3</em>:</p>
			<div>
				<div id="_idContainer081" class="IMG---Figure">
					<img src="image/B17928_03_003.jpg" alt="Figure 3.3 – An Azure portal search for an ML workspace " width="586" height="261"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.3 – An Azure portal search for an ML workspace</p>
			<p>As you can see, besides the main <strong class="source-inline">mldemows</strong> workspace, three other services were deployed, namely <strong class="bold">Storage account</strong>, <strong class="bold">Key vault</strong>, and <strong class="bold">Application Insights</strong>. As most of them require unique names, you will see a random alphanumeric code at the end of each name. For each one of these additional services, we can provide our own already existing service when we deploy the workspace. </p>
			<p>In addition, an <strong class="bold">Azure container registry</strong> will be required at a later stage but does not need to be there during <a id="_idIndexMarker326"/>the initial deployment of the workspace.</p>
			<p>Knowing now what additional services were deployed, let's discuss why they are there.</p>
			<h3>The storage account for an ML workspace</h3>
			<p>The storage account, typically referred to as the <strong class="bold">default storage account</strong>, is the main datastore for the <a id="_idIndexMarker327"/>workspace. This storage is vital for the operation of the <a id="_idIndexMarker328"/>service. It stores among other things experiment runs, models, snapshots, and even source files, such as Jupyter notebooks. We will have <a id="_idIndexMarker329"/>a more in-depth look at default workspace storage, many other datastores in and around Azure, and how they can be integrated in <a href="B17928_04_ePub.xhtml#_idTextAnchor071"><em class="italic">Chapter 4</em></a>, <em class="italic">Ingesting Data and Managing Datasets</em>. </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Be aware that if you would want to use your own storage account as default storage when deploying the workspace, it cannot have a hierarchical namespace (Azure Data Lake) and it cannot be premium storage (high-performant SSDs).</p>
			<h3>Azure Key Vault for an ML workspace</h3>
			<p>Key Vault is a cloud-managed service that can store <em class="italic">secrets</em> such as passwords, API keys, certificates, and <a id="_idIndexMarker330"/>cryptographic keys. Secrets in the service are held either in <a id="_idIndexMarker331"/>a software vault or a managed <strong class="bold">Hardware Security Module </strong>(<strong class="bold">HSM</strong>). For the ML workspace, and any other service for that <a id="_idIndexMarker332"/>matter, it is crucial to store your access keys in a secure environment.</p>
			<p>So far, we have only handled relatively unimportant information such as a subscription ID, but if we want, for example, to pull data from external storage, we will either need a key to access it or call a function to another service, where this information is stored securely. You can be the judge of what is the better choice.</p>
			<p>The developers of the ML workspace chose the latter options. Due to that, an Azure key vault is required to store the internal secrets for the workspace and give you the possibility to store any secret necessary to read out datasets, perform ML training on compute targets, and deploy your final models to internal or external targets.</p>
			<p>Now, the question might <a id="_idIndexMarker333"/>arise of how to get secure access to Key Vault itself. This is done through a so-called <strong class="bold">managed identity</strong>, which gives the workspace (the app) itself an identity to assign rights to.</p>
			<p class="callout-heading">Managed Identities on Azure</p>
			<p class="callout">A managed identity is an identity given <a id="_idIndexMarker334"/>to an application that behaves the same way as a user identity. </p>
			<p>As with the other services, you could have linked an already existing key vault during deployment without any restrictions.</p>
			<h3>Application Insights for an ML workspace</h3>
			<p><strong class="bold">Applications Insights</strong> is a module of <strong class="bold">Azure Monitor</strong>, which in turn is a suite in Azure to monitor <a id="_idIndexMarker335"/>infrastructure and applications, which stores and <a id="_idIndexMarker336"/>surfaces infrastructure metrics such as CPU usage and log files of applications. </p>
			<p>The Azure Machine Learning workspace uses Application Insights to store compute infrastructure logs, ML script logs, and defined metrics of the ML model runs and is therefore required for the operation of the workspace.</p>
			<h3>Azure Container Registry for an ML workspace</h3>
			<p><strong class="bold">Azure Container Registry</strong> (<strong class="bold">ACR</strong>) is a service based on the <strong class="bold">Docker Registry</strong>. It is used to store and <a id="_idIndexMarker337"/>manage Docker container images <a id="_idIndexMarker338"/>and artifacts. For the workspace, the registry is required at the point <a id="_idIndexMarker339"/>when we start running training on or deploying models to a compute that is not our local machine. In this process, a container is packed and registered to ACR, which then can be tracked and utilized in ML scripts or by deployment pipelines.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Please be aware that the ML service by default deploys ACR in the basic service tier. To reduce the time for building and deploying an image to a compute target, you might want to change the Container Registry service level to Standard or Premium. </p>
			<h2 id="_idParaDest-62"><a id="_idTextAnchor061"/>Understanding the workspace interior</h2>
			<p>Now that we understand <a id="_idIndexMarker340"/>the additional deployed service, we will have a look at the interior of the workspace itself. <em class="italic">Figure 3.4</em> shows nearly every aspect of note of an Azure Machine Learning workspace:</p>
			<div>
				<div id="_idContainer082" class="IMG---Figure">
					<img src="image/B17928_03_04.jpg" alt="Figure 3.4 – A structural view of an Azure Machine Learning workspace " width="1602" height="800"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.4 – A structural view of an Azure Machine Learning workspace</p>
			<p>Let's get an understanding <a id="_idIndexMarker341"/>of each of these aspects, except for <strong class="bold">Associated Azure resources</strong>, as we already discussed that in the <em class="italic">Analyzing the deployed services</em> section.</p>
			<h3>User roles</h3>
			<p>As with any other <a id="_idIndexMarker342"/>service in Azure, user authentication <a id="_idIndexMarker343"/>and authorization are performed through AAD and so-called <strong class="bold">Azure Role-Based Access Control</strong> (<strong class="bold">Azure RBAC</strong>). </p>
			<p class="callout-heading">Role-based Access Control on Azure</p>
			<p class="callout">Azure RBAC is used to assign to an identity from AAD (a user, a service principal, or a managed identity) a specific role <a id="_idIndexMarker344"/>on a resource, which defines the level of access to the resource and the type of granular action that can be performed. </p>
			<p>In the case of the ML workspace, we can assign an identity the Azure predefined base roles (<strong class="bold">Owner</strong>, <strong class="bold">Contributor</strong>, or <strong class="bold">Reader</strong>) and two custom roles named <strong class="bold">AzureML Data Scientist</strong> and <strong class="bold">AzureML Metrics Writer</strong>. Here are their details:</p>
			<ul>
				<li><strong class="bold">Reader</strong>: This role is allowed to look at everything but cannot change any data or action anything that would change the state of the resource (for example, deploying a compute or changing a network configuration).</li>
				<li><strong class="bold">Contributor</strong>: This role is allowed to look at and change everything but is not allowed to change <a id="_idIndexMarker345"/>the user roles and rights on the resource.</li>
				<li><strong class="bold">Owner</strong>: This role is allowed to do any action on a specific resource. </li>
				<li><strong class="bold">AzureML Data Scientist</strong>: This role is not allowed any action in the workspace except creating or deleting compute resources or modifying the workspace settings.</li>
				<li><strong class="bold">AzureML Metrics Writer</strong>: This role is only allowed to write metrics to the workspace.</li>
			</ul>
			<p>Besides these, the ML workspace does not offer additional custom roles. </p>
			<p>To give you more fine-grained control in this matter, RBAC lets you build your own custom roles, as a lot of actions a user <a id="_idIndexMarker346"/>can perform in the ML workspace are defined as so-called <strong class="bold">actions</strong> in RBAC. All available actions for the Azure Machine Learning service can be found in this <a id="_idIndexMarker347"/>list of resource providers, <a href="https://docs.microsoft.com/en-us/azure/role-based-access-control/resource-provider-operations">https://docs.microsoft.com/en-us/azure/role-based-access-control/resource-provider-operations</a>, under the operation group named <strong class="bold">Microsoft.MachineLearningServices</strong>. </p>
			<p>To get some inspiration for different roles, have a look at common scenarios and custom roles suggested by Microsoft: <a href="https://docs.microsoft.com/en-us/azure/machine-learning/how-to-assign-roles#common-scenarios">https://docs.microsoft.com/en-us/azure/machine-learning/how-to-assign-roles#common-scenarios</a>. We will have a look in the next section where you can define and assign them.</p>
			<h3>Experiments</h3>
			<p>The goal of ML – in a <a id="_idIndexMarker348"/>nutshell – is to find a mathematical function, which would be hard to find algorithmically, that when given specific input results in as many cases as possible in the expected output. This function is typically referred to as an <strong class="bold">ML model</strong>. A model we train might be a function that assigns voices in a <a id="_idIndexMarker349"/>sound file to specific speakers or that recommends products for customers on a web shop based on the buying behavior of similar buyers (see <a href="B17928_13_ePub.xhtml#_idTextAnchor202"><em class="italic">Chapter 13</em></a>, <em class="italic">Building a Recommendation Engine in Azure</em>).</p>
			<p>To achieve this, we need to train ML models utilizing already existing ML algorithms, with the goal to lower <a id="_idIndexMarker350"/>the output of the so-called <strong class="bold">loss function</strong> of said model. This requires tweaking <a id="_idIndexMarker351"/>the settings of our models and, mathematically speaking, in the best case, finding the global minimum of the loss function on the <em class="italic">n</em>-dimensional room of all possible functions. Depending on the complexity of our model, this requires a lot of reiterations.</p>
			<p>Therefore, to keep track of the iterations of our model training, we define them as <strong class="bold">runs</strong> and align them to a construct called an <strong class="bold">experiment</strong>, which collects all information concerning a specific model <a id="_idIndexMarker352"/>we want to train. To do this, we will connect any training script run we perform to a specific experiment.</p>
			<h3>Datasets and datastores</h3>
			<p>Any ML model requires data to operate with, either for training or for testing purposes. Instead of <a id="_idIndexMarker353"/>linking data sources and different data files directly in <a id="_idIndexMarker354"/>our scripts, we can reference <strong class="bold">datasets</strong>, which we can define <a id="_idIndexMarker355"/>inside the workspace. Datasets, in turn, curate data from <strong class="bold">datastores</strong>, which we can define and attach in the workspace. We will go into more <a id="_idIndexMarker356"/>detail on how to handle data, datasets, and datastores in <a href="B17928_04_ePub.xhtml#_idTextAnchor071"><em class="italic">Chapter 4</em></a>, <em class="italic">Ingesting Data and Managing Datasets</em>.</p>
			<h3>Compute targets</h3>
			<p>In order to run experiments <a id="_idIndexMarker357"/>and, later on, host <a id="_idIndexMarker358"/>models for inferencing, we require a <strong class="bold">compute target</strong>. The ML service comes with two options in this area, namely the following:</p>
			<ul>
				<li><strong class="bold">Compute instance</strong>: A single virtual machine typically used for development, as a notebook server, or as <a id="_idIndexMarker359"/>a target for training and inference</li>
				<li><strong class="bold">Compute cluster</strong>: A multi-node cluster of machines typically used for complex training and <a id="_idIndexMarker360"/>production environments for inference</li>
			</ul>
			<p>You can find a list <a id="_idIndexMarker361"/>of supported compute targets (virtual machines) here: <a href="https://docs.microsoft.com/en-us/azure/machine-learning/concept-compute-target#supported-vm-series-and-sizes">https://docs.microsoft.com/en-us/azure/machine-learning/concept-compute-target#supported-vm-series-and-sizes</a>. There are more details concerning <a id="_idIndexMarker362"/>their pricing in the following overview: <a href="https://azure.microsoft.com/en-us/pricing/details/virtual-machines/linux/">https://azure.microsoft.com/en-us/pricing/details/virtual-machines/linux/</a>.</p>
			<p>Besides these two <a id="_idIndexMarker363"/>options, the workspace <a id="_idIndexMarker364"/>offers a bunch <a id="_idIndexMarker365"/>of other possible <a id="_idIndexMarker366"/>targets for both training and inferencing. Popular compute options are your own local <a id="_idIndexMarker367"/>computer, any type of Spark engine (<strong class="bold">Apache Spark</strong>, <strong class="bold">Azure Databricks</strong>, or <strong class="bold">Synapse</strong>) for training, and <strong class="bold">Azure Kubernetes Service</strong> (<strong class="bold">AKS</strong>) for inferencing. For a full updated list of options, refer to <a href="https://docs.microsoft.com/en-us/azure/machine-learning/concept-compute-target">https://docs.microsoft.com/en-us/azure/machine-learning/concept-compute-target</a>. </p>
			<h3>Environments</h3>
			<p>When you write a simple Python <a id="_idIndexMarker368"/>script and run it in the <a id="_idIndexMarker369"/>Python interpreter, you run it in a so-called <strong class="bold">environment</strong>. In this example, your environment would be defined by the Python version (for example, Python 3.8.10), specific library extensions you might have installed (for example, <strong class="source-inline">numpy</strong>), and certainly the operating system you are running it on. This is also true for any ML script that we run. </p>
			<p>For our purpose, we operate in an environment that requires a specific Python version and certain libraries <a id="_idIndexMarker370"/>such as the Azure Machine Learning Python SDK and libraries containing ML algorithms and tooling, such as <strong class="bold">TensorFlow</strong>. For our own local machine, and especially if we want to run our script on a much faster compute cluster in the workspace, we need a good way to define the environment for the compute target.</p>
			<p>To facilitate this, the workspace gives us the ability to define and register ML environments. These are typically <strong class="bold">Docker containers</strong> encompassing the OS and every runtime, library, and <a id="_idIndexMarker371"/>dependency required. For defining libraries and dependencies for Python inside the container, the package manager <strong class="bold">Conda</strong> (<a href="https://conda.io/">https://conda.io/</a>) is used in <a id="_idIndexMarker372"/>most cases under the hood. Speaking of that, let's classify the types of environments we can work with or create:</p>
			<ul>
				<li><strong class="bold">Curated environments</strong> use predefined <a id="_idIndexMarker373"/>environments containing typical runtimes and ML frameworks. </li>
				<li><strong class="bold">System-managed environments</strong> (using default behavior) build environments <a id="_idIndexMarker374"/>starting from a base image with dependency management through Conda. </li>
				<li><strong class="bold">User-managed environments</strong> build environments by either starting from a base image <a id="_idIndexMarker375"/>but allowing you to handle all libraries and dependencies yourself through Docker steps, or by creating a complete custom Docker image.</li>
			</ul>
			<p>When we start our <a id="_idIndexMarker376"/>first experiments at the end of this chapter, we will see how to use environments in our ML runs. </p>
			<p class="callout-heading">Azure Machine Learning Environments</p>
			<p class="callout">An environment in Azure Machine Learning is a Docker container encompassing an OS and any runtimes, libraries, and additional dependencies required.</p>
			<p>We can conclude that we require a defined environment to run experiments on compute clusters in the workspace. For our local computer, on the other hand, we could just run on the <em class="italic">environment</em> we curated on the machine and ignore the ML workspace environments. But if we were to use the environment methods of the Azure Machine Learning Python SDK in our ML scripts, the run would require some type of defined environment. This can either be the given environment our machine exists in, a local Docker runtime, or a runtime powered by a Conda environment definition.</p>
			<h3>Runs</h3>
			<p>A <strong class="bold">run</strong> is the actual execution <a id="_idIndexMarker377"/>of a model training on a <a id="_idIndexMarker378"/>compute target. Before executing a run, it <a id="_idIndexMarker379"/>requires (in most cases) a so-called <strong class="bold">run configuration</strong>. This configuration is composed of the following:</p>
			<ul>
				<li><strong class="bold">A training script</strong>: The training script that performs the actual ML training (which basically takes <a id="_idIndexMarker380"/>your source folder with all source files, zips it, and sends it to the compute target)</li>
				<li><strong class="bold">An environment</strong>: The ML environment <a id="_idIndexMarker381"/>described previously</li>
				<li><strong class="bold">A compute target</strong>: The target compute <a id="_idIndexMarker382"/>instance or cluster that the run will be executed in</li>
			</ul>
			<p>We will see later in the <a id="_idIndexMarker383"/>chapter when we do our first experiments that there is a <strong class="source-inline">RunConfiguration</strong> class in the Azure Machine Learning Python library that needs to be used to execute the run.</p>
			<p class="callout-heading">Azure Machine Learning Experiment Runs</p>
			<p class="callout">A run is the execution of a <a id="_idIndexMarker384"/>training script in a given environment on a specified compute target.</p>
			<p>On top of that, during and after the execution of the run, it tracks and collects the following information:</p>
			<ul>
				<li><strong class="bold">Log files</strong>: Includes the log files generated during the execution and any statement we add to the logging</li>
				<li><strong class="bold">Metrics</strong>: Includes standard run metrics and any type of object (values, images, and tables) that we want to track specifically during the run</li>
				<li><strong class="bold">Snapshots</strong>: Includes a copy of the source directory containing our training scripts (using the ZIP file that we already required for the run configuration)</li>
				<li><strong class="bold">Output files</strong>: Includes the files generated by the algorithm (the model) and any file we additionally want to attach to the run</li>
			</ul>
			<p>We will see later that we can utilize the <strong class="source-inline">Run</strong> class in the Azure Machine Learning Python library to influence what is tracked.</p>
			<h3>Registered models</h3>
			<p>As said before, the output of our experiment runs is an ML model. This model is basically a mathematical function or, to be more precise, a piece of code implementing a function. Depending <a id="_idIndexMarker385"/>on the ML framework we utilize, the function is stored in binary format in one or multiple output files found in the identically <a id="_idIndexMarker386"/>named folder. Popular formats for serialized ML <a id="_idIndexMarker387"/>models are <strong class="bold">pickle</strong> (Python), <strong class="bold">H5</strong> (Keras), <strong class="bold">Protobuf</strong> (TensorFlow and Caffe), and <a id="_idIndexMarker388"/>other custom formats.</p>
			<p>As all models from different runs would <em class="italic">just</em> be stored in the output files of the run itself, the workspace offers the ability to register a model to the <em class="italic">model registry</em>. In the registry, the models are stored with a name and a version. Each time you add a model with the same name, the registry adds a new version of the existing model with a new version number. In addition, you can tag each model with metainformation, such as the framework utilized.</p>
			<p class="callout-heading">Azure Machine Learning Model Registry</p>
			<p class="callout">The model registry in Azure Machine Learning stores <a id="_idIndexMarker389"/>names and versions of registered models for tracking and deployment.</p>
			<p>In the end, the model registry helps you to keep track of the different results you achieved through training and allows you to deploy different versions of the model for production, development, and test environments.</p>
			<h3>Deployments and deployment endpoints</h3>
			<p>Once a model is trained and registered, it can be packaged as a service – by defining an entry script and <a id="_idIndexMarker390"/>environment – and deployed to a <a id="_idIndexMarker391"/>compute target. The entry script's job is to load the model during initialization, as well as parse user inputs, evaluate the <a id="_idIndexMarker392"/>model, and return the results for a user request. This process is called <strong class="bold">deployment</strong> in Azure Machine Learning. Compute targets for deployments can be <a id="_idIndexMarker393"/>either managed services such as <strong class="bold">Azure Container Instances</strong> (<strong class="bold">ACI</strong>) or <strong class="bold">Azure Kubernetes Service</strong> (<strong class="bold">AKS</strong>), or a completely <a id="_idIndexMarker394"/>custom user-managed AKS cluster. Every deployment typically serves a single model. </p>
			<p>If you want to abstract multiple model deployments behind a common endpoint, you can define an <strong class="bold">endpoint service</strong>. This is a common requirement for rolling out multiple model <a id="_idIndexMarker395"/>versions, performing <strong class="bold">blue-green deployments</strong>, or <strong class="bold">A/B testing</strong>. An endpoint is a <a id="_idIndexMarker396"/>separate service in Azure Machine Learning that provides a common <a id="_idIndexMarker397"/>domain for multiple model deployments, performs <strong class="bold">Secure Socket Layer (SSL)</strong>/<strong class="bold">Transport Layer Security (TLS)</strong> termination, <a id="_idIndexMarker398"/>and <a id="_idIndexMarker399"/>allows traffic <a id="_idIndexMarker400"/>allocation <a id="_idIndexMarker401"/>between deployments. Endpoints can also be deployed to multiple compute targets, including ACI and AKS.</p>
			<p class="callout-heading">Azure Machine Learning Endpoints</p>
			<p class="callout">A deployment endpoint in Azure Machine Learning is a service offering a common domain for accessing and testing <a id="_idIndexMarker402"/>multiple versions of a model.</p>
			<p>For both deployments and endpoints, we differentiate between <strong class="bold">online scoring</strong> and <strong class="bold">batch scoring</strong>: </p>
			<ul>
				<li><strong class="bold">Online scoring</strong>: A model is evaluated synchronously for a single input record (or small batch of input records) where <a id="_idIndexMarker403"/>the input data, as well as the scoring results, are passed directly in the request and response. </li>
				<li><strong class="bold">Batch scoring</strong>: A user typically passes a location to the input data instead of sending input data <a id="_idIndexMarker404"/>with the request. In this case, the model is evaluated asynchronously and provides the results in an output location.</li>
			</ul>
			<p>We will discuss the deployment of models and endpoints in more detail in <a href="B17928_14_ePub.xhtml#_idTextAnchor217"><em class="italic">Chapter 14</em></a>, <em class="italic">Model Deployments, Endpoints, and Operations</em>.</p>
			<h3>Pipelines</h3>
			<p>The final part to mention is <strong class="bold">ML pipelines</strong>. Everything we have discussed so far might be enough to do <a id="_idIndexMarker405"/>some data preparation, model training, model deployment, and inferencing for ourselves. But even that would entail multiple manual steps. Certainly, we can automate most parts of this using the Azure CLI through some scripting and be quite happy with our setup. </p>
			<p>Now, imagine that we want to work with a team and build automated retraining and deployment of our model whenever there is new data to train on. We would have to run similar steps again, such as preprocessing, training, and optimization – just this time with new training <a id="_idIndexMarker406"/>data. This process is typically repeated whenever there is significant data drift between the training data and the inferencing data. This is the point where we need to think about bringing in ideas and proven solutions from DevOps, as in the end, we will also write code and deploy infrastructure into a production environment. </p>
			<p>Therefore, pipelines are used to facilitate workflows and bring automation to every step of the ML chain; we will take a closer look at them in <a href="B17928_08_ePub.xhtml#_idTextAnchor135"><em class="italic">Chapter 8</em></a>, <em class="italic">Azure Machine Learning Pipelines</em>. Pipelines are also one of the integral parts of MLOps, and we will see them in action in <a href="B17928_16_ePub.xhtml#_idTextAnchor252"><em class="italic">Chapter 16</em></a>, <em class="italic">Bringing Models into Production with MLOps</em>. </p>
			<h2 id="_idParaDest-63"><a id="_idTextAnchor062"/>Surveying Azure Machine Learning Studio</h2>
			<p>Now that we have a good understanding of the features of the workspace, let's continue where we left off before and have a look into the Azure portal and <strong class="bold">Azure Machine Learning Studio</strong>, the web service to <a id="_idIndexMarker407"/>operate every aspect of the ML process. This time, search again for our workspace name and click on <strong class="bold">mldemows</strong>, the ML workspace. You will be shown the typical menu structure for an Azure resource on the left and the <strong class="bold">Overview</strong> page of the service on the right, as shown in <em class="italic">Figure 3.5</em>:</p>
			<div>
				<div id="_idContainer083" class="IMG---Figure">
					<img src="image/B17928_03_005.jpg" alt="Figure 3.5 – The Azure resource view " width="1154" height="655"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.5 – The Azure resource view</p>
			<p>This is the administration view from an infrastructure perspective. The major points of interest for you to keep in mind are the following:</p>
			<ul>
				<li><strong class="bold">Overview</strong>: The panel showing the names and attached services of the workspace and the button to launch the ML studio.</li>
				<li><strong class="bold">Access control (IAM)</strong>: The <a id="_idIndexMarker408"/>panel to set user access rights on every aspect of the workspace, as discussed in the last section.</li>
				<li><strong class="bold">Networking</strong>: The panel to integrate the service into a private virtual network by activating a <strong class="bold">private endpoint</strong> for the workspace.</li>
				<li><strong class="bold">Identity</strong>: The panel showing the already created managed identity of the workspace, which can be used to give the workspace access to external Azure services, such as a storage account using RBAC.</li>
				<li><strong class="bold">Usage + quotas</strong>: The panel to access the available quota on the subscription, which defines how many cores of which type of virtual machine the user is allowed to deploy within the subscription.</li>
			</ul>
			<p>By clicking on the <strong class="bold">Launch studio</strong> button on the overview page, the actual Azure Machine Learning Studio will open in a new tab, greeting you with the view shown in <em class="italic">Figure 3.6</em>.</p>
			<div>
				<div id="_idContainer084" class="IMG---Figure">
					<img src="image/B17928_03_006.jpg" alt="Figure 3.6 – The Azure Machine Learning Studio home page " width="1030" height="703"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.6 – The Azure Machine Learning Studio home page</p>
			<p>You can theoretically do everything we will do in this book through this web application, but in certain areas, this can be cumbersome. We will discuss in detail how we set up and operate our <a id="_idIndexMarker409"/>development environment in the next section, but it is a good idea to get an understanding of this web service, as we will come back to it throughout the book.</p>
			<p>Looking at the menu to the left, there are three major categories, namely <strong class="bold">Author</strong>, <strong class="bold">Assets</strong>, and <strong class="bold">Manage</strong>. Let's match what we already know about the workspace to what is shown to us in the web service.</p>
			<h3>Author</h3>
			<p>The first section of the menu <a id="_idIndexMarker410"/>shows you the options for authoring your ML experiments. They are as follows:</p>
			<ul>
				<li><strong class="bold">Notebooks</strong>: Create and <a id="_idIndexMarker411"/>author Jupyter notebooks utilizing a notebook <strong class="bold">virtual machine (VM)</strong> (compute instance) in the cloud.</li>
				<li><strong class="bold">Automated ML</strong>: Create ML models through a wizard, offering insights and suggestions based on your given dataset and problem to solve.</li>
				<li><strong class="bold">Designer</strong>: Build ML models through a GUI interface using logical building blocks.</li>
			</ul>
			<p>We have already discussed why we prefer using code and notebooks in <a href="B17928_02_ePub.xhtml#_idTextAnchor034"><em class="italic">Chapter 2</em></a>,<em class="italic"> Choosing the Right Machine Learning Service in Azure</em>. We will come back to automated ML later in this book in <a href="B17928_11_ePub.xhtml#_idTextAnchor178"><em class="italic">Chapter 11</em></a>, <em class="italic">Hyperparameter Tuning and Automated Machine Learning</em>. </p>
			<p>For now, the options to author our notebooks are to either work in the web service environment and utilize a Jupyter server on a compute instance in the cloud, or to work from our local computer with a local Jupyter server.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">We will stay in our own local environment for most of the book, but be aware that in a bigger team, it might be of value to have a notebook server in the cloud. </p>
			<h3>Assets</h3>
			<p>The second section <a id="_idIndexMarker412"/>of the menu shows you the assets available to utilize in your scripts. They are as follows:</p>
			<ul>
				<li><strong class="bold">Datasets</strong>: View and create datasets in the workspace and configure dataset monitoring for understanding data drift between your training data and the inference data from a deployed model (imaging a sensor that is placed differently in production than when gathering test data or that is suddenly broken).</li>
				<li><strong class="bold">Experiments</strong>: View all experiments and all runs that have been tracked, including their detailed run statistics (metrics, snapshots, logs, and outputs) and infrastructure monitoring logs of the compute target.</li>
				<li><strong class="bold">Pipelines</strong>: Create pipelines, view pipeline runs, and define endpoints for pipelines.</li>
				<li><strong class="bold">Models</strong>: Register models <a id="_idIndexMarker413"/>and view their properties, including their version, the datasets they are using, the artifacts they are made of, and the endpoints they are actively deployed to.</li>
				<li><strong class="bold">Endpoints</strong>: View and create web service endpoints.</li>
			</ul>
			<p>Going through these pages, we can see a lot of the workspace items we already discussed, from datasets to model training through experiments and their runs, registering models, and surfacing service endpoints for our deployments, up to managing all of this through ML pipelines.</p>
			<p>You might have seen some other additional features, such as <strong class="bold">Dataset Monitoring</strong>, which we will come back to in <a href="B17928_04_ePub.xhtml#_idTextAnchor071"><em class="italic">Chapter 4</em></a>, <em class="italic">Ingestion Data and Managing Datasets</em>. </p>
			<p>We will have a closer look at the experiment and run statistics at the end of this chapter when we have an experiment and a run has been shown in Azure Machine Learning Studio.</p>
			<h3>Manage</h3>
			<p>The final section of the <a id="_idIndexMarker414"/>menu shows us the machines and services that we can manage in our workspace. They are as follows:</p>
			<ul>
				<li><strong class="bold">Compute</strong>: Create, view, and manage compute instances, compute clusters, inference clusters, and other attached computes (for example, external VMs or Databricks clusters), including performed runs, distribution of runs on nodes (if existing), and monitoring of the infrastructure itself (for example, CPU usage).</li>
				<li><strong class="bold">Environments</strong>: View available curated environments and create your own custom environments from a Python virtual environment, a Conda YAML configuration, a Docker image stored in the container registry, or from your own Docker file.</li>
				<li><strong class="bold">Datastores</strong>: View, manage, and browse the workspace datastores (<strong class="source-inline">workspacefilestore</strong> and <strong class="source-inline">workspaceblobstore</strong>), the global Azure Machine Learning dataset repository (<strong class="source-inline">azureml_globaldatasets</strong>), and any already attached external storage or attach new ones, including Azure Data Lake, Azure Blob storage, Azure file shares, and Azure SQL, MySQL, and PostgreSQL databases.</li>
				<li><strong class="bold">Data Labeling</strong>: Create <a id="_idIndexMarker415"/>labeling projects for image classification and object detection.</li>
				<li><strong class="bold">Linked Services</strong>: Link an Azure Synapse Spark pool to the workspace.</li>
			</ul>
			<p>In these views, we find the final missing pieces, the compute targets in the workspace, the environments, and our available datastores, from which we source our datasets for modeling. Furthermore, we find a service to help us with data labeling of source files (typically images) and the possibility to link Azure Synapse to our workspace.</p>
			<p>We will go into more detail on the datastores in the next chapter and on data labeling in <a href="B17928_06_ePub.xhtml#_idTextAnchor102"><em class="italic">Chapter 6</em></a>, <em class="italic">Feature Engineering and Labeling</em>. We will not cover the Azure Synapse integration in detail in this book.</p>
			<p>Now that we have a good overview of the features and tooling of the Azure Machine Learning service, we can now return to our local machine and start our first experiments with Azure Machine Learning.</p>
			<h1 id="_idParaDest-64"><a id="_idTextAnchor063"/>Running ML experiments with Azure Machine Learning</h1>
			<p>So far, we have installed the Azure CLI locally, deployed our ML workspace to our Azure subscription, and <a id="_idIndexMarker416"/>had a look through the features <a id="_idIndexMarker417"/>and functionalities of the Azure Machine Learning workspace.</p>
			<p>In this final section of the chapter, we will set up our local environment, including Python, the Azure Machine Learning Python SDK, and optionally Visual Studio Code, and embark on our first experiments locally and with compute targets in the cloud.</p>
			<h2 id="_idParaDest-65"><a id="_idTextAnchor064"/>Setting up a local environment</h2>
			<p>In the beginning, we discussed briefly the tooling available for deploying Azure resources through <a id="_idIndexMarker418"/>Azure Resource Manager. In the same vein, let's have a look at the options for authoring and orchestrating the workspace from our local environment. The options are as follows: </p>
			<ul>
				<li>Using Python 3, the Azure Machine Learning Python SDK, a Jupyter Python extension, and the Azure ML CLI (1.0/2.0) extension (and an editor of choice)</li>
				<li>Using Python3, the Azure Machine Learning Python SDK, an Azure ML CLI (1.0/2.0) extension, <strong class="bold">Visual Studio Code (VS Code)</strong>, <a id="_idIndexMarker419"/>and VS Code extensions (Azure, Azure Machine Learning, Jupyter, and so on)</li>
				<li>Using Python3, an Azure ML CLI 2.0 extension, YAML, and VS Code (or an editor of choice)</li>
				<li>Using R, an Azure ML CLI 2.0 extension, YAML, and VS Code (or an editor of choice)</li>
			</ul>
			<p>The first two options are the de facto standard at the time of writing and the ones we will focus on primarily in this book. We will use the Azure Machine Learning Python SDK with Python 3 and leave it to you if you prefer to work mostly from the console with source files and optionally an editor <a id="_idIndexMarker420"/>of choice, or if you want to use an <strong class="bold">integrated development environment (IDE)</strong> such as VS Code, which comes with a feature-rich editor and helpful extensions for Azure, Azure Machine Learning, and Jupyter. </p>
			<p>In both cases, we will author a Jupyter notebook to orchestrate our ML experiments on the workspace and one or more Python source files to implement the training procedures. </p>
			<p>The latter two options were introduced with the more extensive <strong class="bold">Azure ML CLI 2.0</strong>. Instead of writing a <a id="_idIndexMarker421"/>Jupyter notebook, we completely detach the orchestration of the workspace (run configuration, environments, deployments, and endpoints) from the training and inference source code. This is done through YAML configuration files. An example of an ML experiment run looks like this:</p>
			<p class="source-code">$schema: https://.../commandJob.schema.json</p>
			<p class="source-code"><strong class="bold">code</strong>:</p>
			<p class="source-code">  local_path: &lt;path-to-python-scripts&gt;</p>
			<p class="source-code"><strong class="bold">command</strong>: python &lt;script-name&gt; --data {inputs.trainingData1}</p>
			<p class="source-code"><strong class="bold">environment</strong>:</p>
			<p class="source-code">  docker:</p>
			<p class="source-code">    image: docker.io/python</p>
			<p class="source-code"><strong class="bold">compute</strong>:</p>
			<p class="source-code">  target: azureml:goazurego</p>
			<p class="source-code"><strong class="bold">inputs</strong>:</p>
			<p class="source-code">  trainingData1:</p>
			<p class="source-code">    mode: mount</p>
			<p class="source-code">    data:</p>
			<p class="source-code">      local_path: &lt;path-to-training-data&gt;</p>
			<p>As you can see, this YAML <a id="_idIndexMarker422"/>structure references the actual code to be executed (<strong class="source-inline">code</strong>), the runtime to use (<strong class="source-inline">command</strong>), and defines every part (<strong class="source-inline">environment</strong>, <strong class="source-inline">compute</strong>, and <strong class="source-inline">data</strong>) necessary for the training run in a descriptive manner.</p>
			<p class="callout-heading">YAML Configurations</p>
			<p class="callout">YAML configuration files are a descriptive way to run experiments, create compute services and endpoints, and <a id="_idIndexMarker423"/>deploy models in Azure Machine Learning.</p>
			<p>This is a more structural way of thinking about the task we will perform and will come in handy when we talk about production systems and MLOps in <a href="B17928_16_ePub.xhtml#_idTextAnchor252"><em class="italic">Chapter 16</em></a>, <em class="italic">Bringing Models into Production with MLOps</em>. Finally, this option is the only one allowing source files to be written in <strong class="bold">R</strong>, the domain-specific language for data science, and is highly supported in VS Code through the Azure Machine Learning VS Code extension.</p>
			<h3>Setting up the Python environment</h3>
			<p>Now that we have a good idea about the possible local development environments we can work with, let's <a id="_idIndexMarker424"/>set up our Python environment:</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">The following actions only have to be done if you run your experiments on your own local machine and not if you <a id="_idIndexMarker425"/>are using a notebook compute instance in the Azure Machine Learning Studio authoring environment or a <strong class="bold">Data Science Virtual Machine</strong> (<strong class="bold">DSVM</strong>) in Azure.</p>
			<ol>
				<li value="1">First, check whether there is already a Python version installed on your system by running the following command:<p class="source-code"><strong class="bold">$ python --version</strong></p></li>
				<li>Next, please check the metadata of the Azure Machine Learning Python extension on https://pypi.org/project/azureml-sdk/. There are certain times when the extension is behind <a id="_idIndexMarker426"/>the most recent Python release. If you already have an unsupported Python version on your system,  either uninstall that version or read up on how to operate multiple Python environments on the same machine.</li>
				<li>After you have verified the supported Python release, either go to <a href="https://www.python.org/">https://www.python.org/</a> and find the supported version for Windows and macOS or use the Terminal and the <strong class="source-inline">apt-get</strong> command under your Linux distribution. An example for Python 3.8 would look like this:<p class="source-code"><strong class="bold">$ sudo apt-get install python3.8</strong></p></li>
				<li>If you have installed Python for the first time or reinstalled it again, please check that Python is correctly integrated into the path environment variable by checking for the Python version (see <em class="italic">step 1</em>). If all is good, we can move forward and install the SDK by running the following command:<p class="source-code"><strong class="bold">$ python -m pip install azureml-sdk</strong></p></li>
			</ol>
			<p>If this command is trying to resolve a lot of dependencies, you might still be operating with an unsupported version of Python or the package installer <strong class="bold">PIP</strong>.</p>
			<ol>
				<li value="5">If you want to work with VS Code, you can jump to the next paragraph now. If you prefer to work <a id="_idIndexMarker427"/>primarily with the command line, please install either a local JupyterLab or a local Jupyter notebook server (<a href="https://jupyter.org/index.html">https://jupyter.org/index.html</a>) with one of the following commands:<p class="source-code"><strong class="bold">$ python -m pip install jupyterlab </strong></p><p class="source-code"><strong class="bold">$ python -m pip install notebook</strong></p></li>
			</ol>
			<p>After that, you can start either environment from the command line, like this: </p>
			<p class="source-code"><strong class="bold">$ jupyter-lab</strong></p>
			<p class="source-code"><strong class="bold">$ jupyter notebook</strong></p>
			<p>With this version of the setup, you can now proceed to the <em class="italic">Running a simple experiment with Azure Machine Learning</em> section.</p>
			<h3>Setting up Visual Studio Code</h3>
			<p>VS Code is a lightweight but very <a id="_idIndexMarker428"/>powerful IDE. It is highly integrated with Azure, Azure Machine Learning, and Git, and has a very good editor, an integrated terminal, and <a id="_idIndexMarker429"/>a long list of useful extensions to choose from. </p>
			<p>Let's have a look at it:</p>
			<ol>
				<li value="1">Download the tool <a id="_idIndexMarker430"/>either from <a href="https://code.visualstudio.com/">https://code.visualstudio.com/</a> or through Azure Marketplace and install it.</li>
				<li>After you open it, you will be greeted by the view shown in <em class="italic">Figure 3.7</em> (probably with a darker theme):</li>
			</ol>
			<div>
				<div id="_idContainer085" class="IMG---Figure">
					<img src="image/B17928_03_007.jpg" alt="Figure 3.7 – The VS Code interface " width="1040" height="756"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.7 – The VS Code interface</p>
			<ol>
				<li value="3">If you click on the top menu on <strong class="bold">View</strong> | <strong class="bold">Command Palette</strong> (or hit <em class="italic">Ctrl</em> + <em class="italic">Shift  </em>+ <em class="italic">P</em>), you will see the first highlight of the IDE – you can search for, and issue commands to, the tool itself. Any extension we add will bring its own options to <a id="_idIndexMarker431"/>this palette. It helps us to quickly navigate through the environment. For example, if you want to change the theme of the UI, simply type <strong class="source-inline">&gt;Theme</strong> and look for <strong class="source-inline">&gt;Preferences: Color Themes</strong>. </li>
			</ol>
			<p>Clicking on it will give you a quick way to set the theme of the UI.</p>
			<ol>
				<li value="4">Now, to open the terminal, you can click on the top menu on <strong class="bold">View</strong> | <strong class="bold">Terminal</strong>. You can enter <strong class="source-inline">az</strong> again to see the same as shown in <em class="italic">Figure 3.7</em>. </li>
				<li>Looking at the left menu, you will find an <strong class="bold">EXPLORER</strong> tab, where you can add your source folders and files, a <strong class="bold">Source Control</strong> tab to connect to Git, a <strong class="bold">Run and Debug</strong> tab that lets you handle the debugging of your code, and an <strong class="bold">Extensions</strong> tab where you can search for VS Code extensions. </li>
			</ol>
			<p>Go to the <strong class="bold">Extensions</strong> tab and search and install the following extensions, if they are not already installed: <strong class="bold">Azure Tools</strong>, <strong class="bold">Azure Machine Learning</strong>, <strong class="bold">Python</strong>, <strong class="bold">Pylance</strong>, <strong class="bold">YAML</strong>, and <strong class="bold">Jupyter</strong>.   </p>
			<ol>
				<li value="6">After the installation, you will find a new tab in the left menu called <strong class="bold">Azure</strong>. Have a look around here. If you now either click on the option to sign <a id="_idIndexMarker432"/>in or if you open the command palette again and search for something such as <strong class="source-inline">sign in azure</strong>, you will find a way to sign in.   </li>
			</ol>
			<p>After you are through with signing in to Azure, the <strong class="bold">Azure</strong> tab will populate with your subscription names, resource groups, and any resource you might have. If you look under the <strong class="bold">MACHINE LEARNING</strong> headline, you will also find your previously deployed workspace, as shown in <em class="italic">Figure 3.8</em>:</p>
			<div>
				<div id="_idContainer086" class="IMG---Figure">
					<img src="image/B17928_03_008.jpg" alt="Figure 3.8 – The VS Code Azure Machine Learning extension " width="696" height="480"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.8 – The VS Code Azure Machine Learning extension</p>
			<ol>
				<li value="7">In the next section, download the files for this chapter to work with. Just open the folder <a id="_idIndexMarker433"/>via <strong class="bold">File</strong> | <strong class="bold">Open Folder…</strong>, which will add them to the <strong class="bold">Explorer</strong> tab, from where you can start the journey.</li>
			</ol>
			<p>VS Code has much more to offer, but we will concentrate primarily on understanding ML and the Azure Machine Learning workspace from now on, not on operating every aspect of this editor. If you need <a id="_idIndexMarker434"/>more help using VS Code, please feel free to visit <a href="https://code.visualstudio.com/docs/introvideos/basics">https://code.visualstudio.com/docs/introvideos/basics</a> or any other resource that can help you with it.</p>
			<h2 id="_idParaDest-66"><a id="_idTextAnchor065"/>Enhancing a simple experiment</h2>
			<p>One great use case for starting with Azure Machine Learning is to add advanced logging, tracking, and monitoring capabilities to your existing ML scripts and pipelines. Imagine you have a central place to track <a id="_idIndexMarker435"/>all ML experiments from all your data scientists, monitor training, and validation metrics, upload your trained models and other output files, and save a snapshot of the current environment every time a new training run is executed. You can achieve this with Azure Machine Learning by simply adding a few lines of code to your training scripts.</p>
			<p>We will start by adding Azure Machine Learning workspace functionality to a <strong class="bold">Keras</strong> (<a href="https://keras.io">https://keras.io</a>) ML training script. Keras is one <a id="_idIndexMarker436"/>of many ML libraries we can choose from, depending on the ML algorithms we require.</p>
			<h3>A working directory and preparation</h3>
			<p>Before we begin, please download the code files for this chapter from the repository and extract them to your preferred working directory. After that, either switch to this directory in the <a id="_idIndexMarker437"/>console or open it as a folder in VS Code.</p>
			<p>In either case, you will find the following files in the directory:</p>
			<ul>
				<li><strong class="source-inline">.azureml/config.json</strong>: The Azure Machine Learning workspace configuration file</li>
				<li><strong class="source-inline">.azureml/requirements.txt</strong>: The Python PIP environment requirements</li>
				<li><strong class="source-inline">00_setup_env.sh</strong>: A shell script to set up the Azure CLI and Python environment from scratch (as we already did)</li>
				<li><strong class="source-inline">01_setup_azure_ml_ws.sh</strong>: A shell script to set up the Azure Machine Learning workspace (as we did already)</li>
				<li><strong class="source-inline">0x_run_experiment_*.ipynb</strong>: Multiple Jupyter notebooks for the upcoming experiments</li>
				<li><strong class="source-inline">04_setup_azure_ml_compute.sh</strong>: A shell script to create a workspace compute instance from a YAML configuration</li>
				<li><strong class="source-inline">compute.yml</strong>: A YAML configuration file for a workspace compute instance</li>
				<li><strong class="source-inline">code/*.py</strong>: A folder containing <a id="_idIndexMarker438"/>the Python model training scripts we will use </li>
				<li><strong class="source-inline">.amlignore</strong>: A file denoting everything that should be ignored by the run snapshot</li>
			</ul>
			<p>Let's start with our first experiment:</p>
			<ol>
				<li value="1">First, we need to install the missing Python package we will need for the following experiments. Run the following command, which will install the packages defined in the PIP requirements file:<p class="source-code"><strong class="bold">$ python -m pip install -r .azureml/requirements.txt</strong></p></li>
			</ol>
			<p>PIP will point out that the Azure Machine Learning SDK is already installed.</p>
			<ol>
				<li value="2">Next, open the <strong class="source-inline">config.json</strong> file and enter your subscription ID after the <strong class="source-inline">subscription_id</strong> key. This is necessary, as we will load this configuration in all notebooks using the following code:<p class="source-code"><strong class="bold">from azureml.core import Workspace</strong></p><p class="source-code"><strong class="bold">ws = Workspace.from_config()</strong></p></li>
			</ol>
			<p>The <strong class="source-inline">from_config()</strong> method looks for a file called <strong class="source-inline">config.json</strong> either in the current working directory or in a directory called <strong class="source-inline">.azureml</strong>. We will choose to add it to the folder, as it is part of the <strong class="source-inline">.amlignore</strong> file.</p>
			<ol>
				<li value="3">Open the <strong class="source-inline">02_run_experiment_keras_base.ipynb</strong> notebook.</li>
			</ol>
			<p>In the following, we will have a look through the notebook in order to understand the actual model training script, how we can add snapshots, outputs, and logs to the Azure Machine Learning experiment, and how we can catalog the best model in the model registry.</p>
			<h3>A training script for Keras</h3>
			<p>Navigate to the second block in the notebook. Imagine this part to be your original ML training file (plus the <strong class="source-inline">model.fit()</strong> function that you will find in the final block). </p>
			<p>Let's understand <a id="_idIndexMarker439"/>the actual training code.</p>
			<p>First, we import the classes we require for the training from the <strong class="source-inline">tensorflow</strong> library (Keras is a part of TensorFlow):</p>
			<p class="source-code">import tensorflow</p>
			<p class="source-code">from tensorflow.keras.datasets import cifar10</p>
			<p class="source-code">…</p>
			<p>We then proceed to get our training and test data from the CIFAR-10 dataset and change it into a useful format. The <strong class="source-inline">cifar10.load_data()</strong> function will fill the training set with 50,000 datapoints and the test set with 10,000 data points:</p>
			<p class="source-code">(x_train, y_train), (x_test, y_test) = <strong class="bold">cifar10.load_data()</strong></p>
			<p class="source-code">…</p>
			<p class="source-code">y_train = tensorflow.keras.utils.to_categorical</p>
			<p class="source-code">                         (y_train, num_classes)</p>
			<p class="source-code">…</p>
			<p class="callout-heading">Test and Training Datasets</p>
			<p class="callout">The training dataset is made up of the data points we train our model on; the test dataset is made up <a id="_idIndexMarker440"/>of the data points we will evaluate our model against after it has been trained. These should be completely distinct from each other.</p>
			<p>After that, we start <a id="_idIndexMarker441"/>defining our model – in this case, a <strong class="source-inline">Sequential</strong> model (<a href="https://keras.io/guides/sequential_model/">https://keras.io/guides/sequential_model/</a>) – and we set the name of the model and the <a id="_idIndexMarker442"/>location for the output. We will use the <strong class="bold">HDF5</strong> file format (or H5 for short) for Keras, as mentioned before:</p>
			<p class="source-code"><strong class="bold">model</strong> = Sequential()</p>
			<p class="source-code">…</p>
			<p class="source-code">model_name       = 'keras_cifar10_trained_model.h5'</p>
			<p class="source-code">model_output_dir = os.path.join(os.getcwd(), 'outputs')</p>
			<p>After that, we define an optimizer (<strong class="source-inline">RMSProp</strong> in this case), a checkpoint <strong class="bold">callback</strong>, which we will <a id="_idIndexMarker443"/>discuss later; and finally, we <em class="italic">compile</em> the model by setting a <strong class="source-inline">loss</strong> function, <strong class="source-inline">optimizer</strong>, and additional <strong class="source-inline">metrics</strong> to track during the training run:</p>
			<p class="source-code">opt = RMSprop(learning_rate=0.0001, decay=1e-6)</p>
			<p class="source-code">…</p>
			<p class="source-code">checkpoint_cb = ModelCheckpoint(model_path, </p>
			<p class="source-code">                                monitor='val_loss',</p>
			<p class="source-code">                                save_best_only=True)</p>
			<p class="source-code">…</p>
			<p class="source-code"><strong class="bold">model.compile</strong>(<strong class="bold">loss</strong>='categorical_crossentropy', </p>
			<p class="source-code">              <strong class="bold">optimizer</strong>=opt, </p>
			<p class="source-code">              <strong class="bold">metrics</strong>=['accuracy'])</p>
			<p>The part that would otherwise complete our original script is the one found in the last block of the notebook, which we will discuss in a moment:</p>
			<p class="source-code"><strong class="bold">model.fit</strong>(x_train, y_train,</p>
			<p class="source-code">          <strong class="bold">batch_size</strong>=batch_size,</p>
			<p class="source-code">          <strong class="bold">epochs</strong>=epochs,</p>
			<p class="source-code">          <strong class="bold">validation_data</strong>=(x_test, y_test),</p>
			<p class="source-code">          <strong class="bold">shuffle</strong>=True,</p>
			<p class="source-code">          <strong class="bold">callbacks</strong>=[azureml_cb, checkpoint_cb])</p>
			<p>As you can see, this is most of the notebook code. The rest of the code you can see is what you need to add to your script to enable tracking of your experiment runs, which we will analyze next.</p>
			<h3>Tracking snapshots, output, and logs</h3>
			<p>We will now have <a id="_idIndexMarker444"/>a look at the code we have ignored so far. First, return <a id="_idIndexMarker445"/>to the first block <a id="_idIndexMarker446"/>of the notebook we skipped before:</p>
			<p class="source-code">from azureml.core import Workspace, Experiment</p>
			<p class="source-code">ws  = Workspace.from_config()</p>
			<p class="source-code">exp = <strong class="bold">Experiment</strong>(workspace=ws, name="cifar10_cnn_local")</p>
			<p>In this snippet, we define a workspace object called <strong class="source-inline">ws</strong> using our config file, and as a second step, we define an experiment object, <strong class="source-inline">exp</strong>, to be tracked in the defined workspace under a chosen name. As you can see, we name it <strong class="source-inline">cifar10_cnn_local</strong> because we will <a id="_idIndexMarker447"/>utilize the CIFAR-10 dataset (<a href="https://www.kaggle.com/c/cifar-10">https://www.kaggle.com/c/cifar-10</a>), we will run a <strong class="bold">Convolutional Neural Network</strong> (<strong class="bold">CNN</strong>), and we will do so on a local machine. If an experiment with the same name already exists, this <a id="_idIndexMarker448"/>invocation returns the existing experiment as a handle; otherwise, a new experiment will be created. Through the given name, all the runs in this experiment are now grouped together and can be displayed and analyzed on a single dashboard. </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Running this code block might open a website to log in to your Azure account. This is called interactive authentication. Please do this to grant your current execution environment <a id="_idIndexMarker449"/>access to your Azure Machine Learning workspace. If you run a non-interactive Python script rather than a notebook environment, you can <a id="_idIndexMarker450"/>provide the Azure CLI credentials through other means described here: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication#use-interactive-authentication.</p>
			<p>Once you have successfully linked the workspace into the <strong class="source-inline">ws</strong> object, you can continue adding tracking capabilities to your ML experiments. We will use this object to create experiments, runs, and log metrics, and register models in our Azure Machine Learning workspace.</p>
			<p>Now, let's jump to the final block, where we will perform a run of the experiment. As described before, a run is a single execution of your experiment (your training script), with different settings, models, code, and data but the same comparable metric. You use runs to test multiple hypotheses for a given experiment and track all the results within the same experiments. </p>
			<p>Typically, we can create a <strong class="source-inline">run</strong> object and start logging this run here by invoking the following function:</p>
			<p class="source-code"># Create and start an interactive run</p>
			<p class="source-code">run = <strong class="bold">exp.start_logging</strong>(snapshot_directory='.')</p>
			<p>The preceding <a id="_idIndexMarker451"/>code not only creates and initializes a new run; it also takes <a id="_idIndexMarker452"/>a snapshot of <a id="_idIndexMarker453"/>the current environment, defined through the <strong class="source-inline">snapshot_directory</strong> argument, and uploads it to the Azure Machine Learning workspace. To disable this feature, you need to explicitly pass <strong class="source-inline">snapshot_directory=None</strong> to the <strong class="source-inline">start_logging()</strong> function. </p>
			<p>In this case, the snapshot will take every file and folder existing in the current directory. To restrict this, we can specify the files and folders to ignore using a <strong class="source-inline">.amlignore</strong> file.</p>
			<p>Looking at the code itself in the last notebook block, you can see that this is not the same line of code shown previously. </p>
			<p>This is because it is good practice to wrap your training code in a <strong class="source-inline">try</strong> and <strong class="source-inline">except</strong> block in order to propagate the status of your run in Azure. If the training run fails, then the run will be reported as a failed run in Azure. You can achieve this by using the following code snippet:</p>
			<p class="source-code">run = exp.start_logging(snapshot_directory='.')</p>
			<p class="source-code"><strong class="bold">try</strong>:</p>
			<p class="source-code">  # train your model here</p>
			<p class="source-code">  run.complete()</p>
			<p class="source-code"><strong class="bold">except</strong>:</p>
			<p class="source-code">  run.cancel()</p>
			<p class="source-code">  raise</p>
			<p>We included the <strong class="source-inline">raise</strong> statement in order to fail the script when an error occurs. This would normally not happen, as all exceptions are caught. You can simplify the preceding code by using the <strong class="source-inline">with</strong> statement in Python. This will yield the same result and is much easier to read:</p>
			<p class="source-code"><strong class="bold">with</strong> exp.start_logging(snapshot_directory='.') as run:</p>
			<p class="source-code">  # train your model here</p>
			<p class="source-code">  pass</p>
			<p>By using only this single <a id="_idIndexMarker454"/>line of code, you can track a snapshot for each execution of <a id="_idIndexMarker455"/>your experimentation runs automatically and, hence, never lose code or configurations and always come <a id="_idIndexMarker456"/>back to specific code, parameters, or models used for one of your ML runs. This is not very impressive yet, but we are just getting started using the features of Azure Machine Learning.</p>
			<p>Now, execute every code block in this notebook and wait for completion.</p>
			<p>Once executed, go back to Azure Machine Learning Studio and navigate to the <strong class="bold">Experiments</strong> view. You should find the name of our experiment, <strong class="source-inline">cifar10_cnn_local</strong>. When you click on it, you will see some metrics in a graph and a list of runs associated with the experiment. Click on the most recent run and then on <strong class="bold">Snapshot</strong>. You should now see that the notebook attached everything in our working directory to the snapshot, except for the folders we ignored (for example, <strong class="source-inline">.azureml</strong>). </p>
			<p><em class="italic">Figure 3.9</em> shows the uploaded snapshot files of a run in our experiment:</p>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<img src="image/B17928_03_009.jpg" alt="Figure 3.9 – A snapshot view of an experiment run " width="1298" height="696"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.9 – A snapshot view of an experiment run</p>
			<p>Besides the <strong class="source-inline">snapshot</strong> directory, which is uploaded before the run starts, we also end up with two additional directories after the run created by the ML script, namely <strong class="source-inline">outputs</strong> and <strong class="source-inline">logs</strong>.</p>
			<p>Once a run is <a id="_idIndexMarker457"/>completed using <strong class="source-inline">run.complete()</strong>, all content of the <strong class="source-inline">outputs</strong> directory <a id="_idIndexMarker458"/>is automatically uploaded to the Azure Machine Learning <a id="_idIndexMarker459"/>workspace. In our simple example using Keras, we can use a checkpoint callback to only store the <em class="italic">best model</em> of all epochs to the <strong class="source-inline">outputs</strong> directory, which then is tracked with our run. Have a look at this sample code:</p>
			<p class="source-code">import os</p>
			<p class="source-code">from keras.calbacks import ModelCheckpoint</p>
			<p class="source-code">model_output_dir = os.path.join(os.getcwd(), 'outputs')</p>
			<p class="source-code">model_name       = 'keras_cifar10_trained_model.h5'</p>
			<p class="source-code">model_path       = os.path.join(model_output_dir, model_name)</p>
			<p class="source-code"># define a checkpoint callback</p>
			<p class="source-code">checkpoint_cb = ModelCheckpoint(model_path,</p>
			<p class="source-code">                                <strong class="bold">monitor='val_loss'</strong>,</p>
			<p class="source-code">                                <strong class="bold">save_best_only=True</strong>)</p>
			<p class="source-code"># train the model</p>
			<p class="source-code">model.fit(x_train, y_train,</p>
			<p class="source-code">          batch_size=batch_size,</p>
			<p class="source-code">          epochs=epochs,</p>
			<p class="source-code">          <strong class="bold">validation_split=0.2</strong>,</p>
			<p class="source-code">          shuffle=True,</p>
			<p class="source-code">          callbacks=[<strong class="bold">checkpoint_cb</strong>])</p>
			<p>In the preceding code, we <a id="_idIndexMarker460"/>trained a Keras model for five epochs. The process <a id="_idIndexMarker461"/>sets apart 20% (<strong class="source-inline">validation_split</strong>) of the training <a id="_idIndexMarker462"/>data as a so-called validation set.</p>
			<p class="callout-heading">Validation Datasets</p>
			<p class="callout">The validation set is the third set of datapoints, which the model is evaluated against during model training. It should <a id="_idIndexMarker463"/>neither be a subset of the training data nor the test data. </p>
			<p>After that, the function runs through every epoch with a shuffled (<strong class="source-inline">shuffle=True</strong>) training dataset. In every epoch, it takes and overwrites the model file in the defined <strong class="source-inline">output</strong> folder if the model of this epoch is performing better on the validation set, which we defined by having a lower validation loss (<strong class="source-inline">monitor='val_loss'</strong>). Therefore, we will <a id="_idIndexMarker464"/>only have the best model stored in the <strong class="source-inline">output</strong> folder <a id="_idIndexMarker465"/>at the end. Hence, whenever we run the training with the previous <a id="_idIndexMarker466"/>experiment tracking, the model gets uploaded automatically once the run is completed.</p>
			<p>If you go back to the second code block in the notebook, you will see that we already added the checkpoint callback in our code. Let's check what we got then.</p>
			<p>In Azure Machine Learning Studio, navigate to <strong class="bold">Outputs + logs</strong> in the run overview. You can see here that the best model, named <strong class="source-inline">keras_cifar10_trained_model.h5</strong>, was uploaded to the Azure Machine Learning workspace. </p>
			<p>This is also very convenient, as you won't lose track of your trained models anymore. On top of that, all artifacts you see here are stored in the workspace Blob storage, which is highly scalable and inexpensive.</p>
			<p><em class="italic">Figure 3.10</em> shows the additional output and log information of a run in our experiment:</p>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="image/B17928_03_010.jpg" alt="Figure 3.10 – Outputs and logs of an experiment run " width="1028" height="580"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.10 – Outputs and logs of an experiment run</p>
			<p>The <strong class="source-inline">logs</strong> directory <a id="_idIndexMarker467"/>contains the log output from Keras, which you also <a id="_idIndexMarker468"/>saw in the Jupyter notebook when executing the last block. In the <a id="_idIndexMarker469"/>current run, this was uploaded after the run, together with the <strong class="source-inline">output</strong> folder and the model. </p>
			<p class="callout-heading">Azure Machine Learning Log Streaming</p>
			<p class="callout">Log streaming in Azure Machine Learning allows you to see logs in Azure Machine Learning Studio while a run is being executed.</p>
			<p>We will see later that if the training script run is invoked through <strong class="source-inline">ScriptRunConfig</strong> rather than being executed directly, the logging will <strong class="bold">stream</strong> to the workspace (see also the <strong class="bold">Enable log streaming</strong> button). This will allow you to see the logs here while the run is still going on.</p>
			<h3>Cataloging models to the model registry</h3>
			<p>As a final step, we want <a id="_idIndexMarker470"/>to register our best model, which we have stored in the <strong class="source-inline">output</strong> folder, to the model registry in the Azure Machine Learning workspace.</p>
			<p>If we navigate to the final block of the notebook again, we can see that the last lines read like this:</p>
			<p class="source-code"># Upload the best model</p>
			<p class="source-code">run.upload_file(model_name, model_path)</p>
			<p class="source-code"># Register the best model</p>
			<p class="source-code">run.register_model(model_name, model_path=model_name, </p>
			<p class="source-code">    model_framework='TfKeras')</p>
			<p>Here, we first force the upload of the model. This is needed because all output resources are only uploaded when the run is completed and not immediately. Hence, after uploading the model, we can simply register it in the model registry by invoking the <strong class="source-inline">run.register_model()</strong> method.</p>
			<p>If you navigate in Azure Machine Learning Studio to <strong class="bold">Models</strong>, you should find a model registered under the name <strong class="source-inline">keras_cifar10_trained_model.h5</strong> from the <strong class="source-inline">cifar10_cnn_local</strong> experiment. If you click on it, you will find details about the model under <strong class="bold">Details</strong>, including the version number, and you will find the actual model file we created under <strong class="bold">Artifacts</strong>. </p>
			<p><em class="italic">Figure 3.11</em> shows the model details of the registered model:</p>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="image/B17928_03_011.jpg" alt="Figure 3.11 – A registered model in the Azure Machine Learning model registry " width="838" height="563"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.11 – A registered model in the Azure Machine Learning model registry</p>
			<p>The model can then be <a id="_idIndexMarker471"/>used for automatic deployments from the Azure Machine Learning service. We will look at this in a lot more detail in <a href="B17928_14_ePub.xhtml#_idTextAnchor217"><em class="italic">Chapter 14</em></a>, <em class="italic">Model Deployments, Endpoints, and Operations</em>, and <a href="B17928_11_ePub.xhtml#_idTextAnchor178"><em class="italic">Chapter 11</em></a>, <em class="italic">Hyperparameter Tuning and Automated Machine Learning</em>.</p>
			<p>Now that we know how to run a simple experiment, let's learn how to log metrics and track results in the next section.</p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor066"/>Logging metrics and tracking results</h2>
			<p>We already saw three useful features to track snapshot code, upload output artifacts, and register trained model files in our Azure Machine Learning workspace. As we saw, these features can be added to any <a id="_idIndexMarker472"/>existing experimentation and training Python script or <a id="_idIndexMarker473"/>notebook with a few lines of code. In a similar way, we can extend the experimentation script to also track all kinds of variables, such as training accuracy and validation loss per epoch, as well as the test set accuracy of the best model.</p>
			<p>Using the <strong class="source-inline">run.log()</strong> method, you can track any parameter during training and experimentation. You simply supply a name and a value, and Azure will do the rest for you. The backend automatically detects whether you send a list of values – hence multiple values with the same key when you log the same value multiple times in the same run – or a single value per run, such as the test performance. In Azure Machine Learning Studio, these values will be used automatically to visualize your overall training performance. </p>
			<p>Our Keras model so far is tracking the <em class="italic">loss</em> as a metric by default and the <em class="italic">accuracy</em> of the model through our model compilation. We just don't log them to the workspace. </p>
			<p>We previously talked about the different datasets we are using in the script, namely the training dataset, the <a id="_idIndexMarker474"/>validation dataset, and the test dataset. Remember that the <a id="_idIndexMarker475"/>validation dataset is evaluated at the end of each epoch, which also means we can get the <strong class="bold">validation loss</strong> and the <strong class="bold">validation accuracy</strong> at the end of each epoch. Further, after we have found the best model of all epochs, we want to evaluate this model against the test data, which we have not done yet. This then results in the <em class="italic">test loss</em> and <em class="italic">test accuracy</em> of the model. </p>
			<p>In the following, we will first add the test metrics to our run, then the validation metrics, and then have a look at them in Azure Machine Learning Studio. Finally, we will enhance the code so that we only register a model if it is better than all of the models from previous runs. Feel free to have the <strong class="source-inline">02_run_experiment_keras_enhanced.ipynb</strong> notebook open to follow along.</p>
			<h3>Evaluation of the best model </h3>
			<p>The goal is to evaluate the best training model of all epochs against the test dataset to get the overall test <a id="_idIndexMarker476"/>metrics. In order to do this, we need to load it back into our model object. Luckily, we already only stored the best model of the whole run in our <strong class="source-inline">output</strong> folder using the checkpoint callback that we defined before. Let's look at the code:</p>
			<p class="source-code"># load the overall best model into the model object</p>
			<p class="source-code">model = load_model(model_path)</p>
			<p class="source-code"># evaluate the best model against the test dataset</p>
			<p class="source-code">scores = <strong class="bold">model.evaluate</strong>(x_test, y_test, verbose=1)</p>
			<p class="source-code">print('Test loss of best model:', scores[0])</p>
			<p class="source-code"><strong class="bold">run.log</strong>('Test loss', scores[0])</p>
			<p class="source-code">print('Test accuracy of best model:', scores[1])</p>
			<p class="source-code"><strong class="bold">run.log</strong>('Test accuracy', scores[1])</p>
			<p>As you can see, we get <a id="_idIndexMarker477"/>back the best model and then evaluate it, extracting the loss (<strong class="source-inline">scores[0]</strong>) and the accuracy (<strong class="source-inline">scores[1]</strong>). Having done this part, let's have a look at the validation metrics.</p>
			<h3>A Keras callback for validation metrics </h3>
			<p>The goal is to evaluate the model created in each epoch against the validation dataset to get the <a id="_idIndexMarker478"/>validation metrics for each epoch. We already used an existing callback to check for the best model in each epoch, so it might be a good idea to write one ourselves to track the metrics in each epoch.</p>
			<p>Open the <strong class="source-inline">keras_azure_ml_cb.py</strong> file in the <strong class="source-inline">code</strong> directory. You will be greeted by the following:</p>
			<p class="source-code">from keras.callbacks import Callback</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">class AzureMlKerasCallback(Callback):</p>
			<p class="source-code">    def __init__(self, run):</p>
			<p class="source-code">        super(AzureMlKerasCallback, self).__init__()</p>
			<p class="source-code">        self.run = run</p>
			<p class="source-code">    def <strong class="bold">on_epoch_end</strong>(self, epoch, <strong class="bold">logs=None</strong>):</p>
			<p class="source-code">        # logs is filled by Keras at the end of an epoch</p>
			<p class="source-code">        logs = logs or {}</p>
			<p class="source-code">        for metric_name, metric_val in logs.items():</p>
			<p class="source-code">          if isinstance(metric_val, (np.ndarray, np.generic)):</p>
			<p class="source-code">           self.run.log_list(metric_name, metric_val.tolist())</p>
			<p class="source-code">          else:</p>
			<p class="source-code">           <strong class="bold">self.run.log(metric_name, metric_val)</strong></p>
			<p>The preceding code implements a simple Keras callback function. When the callback is executed, Keras passes <a id="_idIndexMarker479"/>the current epoch as well as all training and validation metrics as a dictionary (<strong class="source-inline">logs</strong>). </p>
			<p>What then happens is that for all dictionary entries, we pull out the name and the value to log them to the experiment run with the <strong class="source-inline">run.log(metric_name,metric_val)</strong> function. We only have to check whether the value is a single value or an array type, as the Azure Machine Learning SDK has a different function called <strong class="source-inline">run.log_list()</strong> for multi-value entries.</p>
			<p>We can now use this callback in our model training the same way as we did with the previous callback, by adding it to the <strong class="source-inline">model.fit()</strong> function:</p>
			<p class="source-code"># create an Azure Machine Learning monitor callback</p>
			<p class="source-code"><strong class="bold">azureml_cb = AzureMlKerasCallback(run)</strong></p>
			<p class="source-code">model.fit(x_train, y_train,</p>
			<p class="source-code">  batch_size=batch_size,</p>
			<p class="source-code">  epochs=epochs,</p>
			<p class="source-code">  validation_data=(x_test, y_test),</p>
			<p class="source-code">  callbacks=[<strong class="bold">azureml_cb</strong>, checkpoint_cb])</p>
			<p>This extends Keras naturally using a callback function to track the training and validation loss and accuracy in the <a id="_idIndexMarker480"/>Azure Machine Learning service. Any metric defined on the model itself will now be tracked automatically in the experiment run.</p>
			<h3>Running metric visualization in Azure Machine Learning Studio</h3>
			<p>After we have <a id="_idIndexMarker481"/>added a bunch of metrics to <a id="_idIndexMarker482"/>the experiment run, let's run the notebook as is and have a look at the run statistics in Azure Machine Learning Studio.</p>
			<p>When you open the run, the <strong class="bold">Metrics</strong> list of types, as with both validation metrics, are automatically converted into line charts and plotted, as shown in <em class="italic">Figure 3.12</em>:</p>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="image/B17928_03_012.jpg" alt="Figure 3.12 – The metrics view of an experiment run " width="1053" height="829"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.12 – The metrics view of an experiment run</p>
			<p>We can see that the test metrics and validation metrics are all accounted for. In addition, we can see <strong class="bold">Test loss</strong> and <strong class="bold">Test accuracy</strong> as metrics, which are also provided by Keras for each epoch as the evaluation of the model against the training dataset.</p>
			<p>Another nifty feature is that the ML workspace experiment gives you an overview of all your runs. It <a id="_idIndexMarker483"/>automatically uses both the scalar values <a id="_idIndexMarker484"/>and training and validation metrics that were logged per run and displays them on a dashboard. You can modify the displayed values and the aggregation method used to aggregate those values over the individual runs.</p>
			<p><em class="italic">Figure 3.13</em> shows the accuracy and the validation accuracy of all experiment runs:</p>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<img src="image/B17928_03_013.jpg" alt="Figure 3.13 – The visualized metrics of all experiment runs " width="1297" height="700"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.13 – The visualized metrics of all experiment runs</p>
			<p>This is the simplest method of tracking values from the runs and displaying them with the corresponding experiments. Adding a few lines of code to your existing ML training scripts – independent of which framework you are using – automatically tracks your model scores and displays all experiments in a dashboard.</p>
			<h3>Enhancing the registration of models</h3>
			<p>Now that we have <a id="_idIndexMarker485"/>metrics to read out and work with, we can, as a final step, enhance the way we save the best model to the model registry.</p>
			<p>So far, we always update the model with a new version as soon as a new model is available. However, this doesn't automatically mean that the new model has a better performance than the last model we registered in the workspace. As we want a new <strong class="bold">version</strong> of the model to actually be better than the last version, we need to check for that.</p>
			<p>Therefore, a common approach is to register the new model only if the specified metric is better than the <a id="_idIndexMarker486"/>highest previously stored metric for the experiment. Let's implement this functionality. </p>
			<p>We can define a function that returns a generator of metrics from an experiment, like this:</p>
			<p class="source-code">from azureml.core import Run</p>
			<p class="source-code">def <strong class="bold">get_metrics_from_exp</strong>(exp, metric, status='Completed'):</p>
			<p class="source-code">  <strong class="bold">for run in Run.list</strong>(exp, status=status):</p>
			<p class="source-code">    yield run.get_metrics().get(metric)</p>
			<p>The preceding generator function yields the specified tracked metric for each run that is completed. We can use this function to return the best metric from all previous experiment runs to compare the evaluated score from the current model and decide whether we should register a new version of the model. We should do this only if the current model performs better than the previous recorded model. For that, we need to compare a metric. Using the <strong class="bold">test accuracy</strong> is a good idea, as it is the model tested against unknown data:</p>
			<p class="source-code"># get the highest test accuracy</p>
			<p class="source-code">best_test_acc = max(get_metrics_from_exp(</p>
			<p class="source-code">                    exp,'Test accuracy')</p>
			<p class="source-code">                    default = 0)</p>
			<p class="source-code"># upload the model</p>
			<p class="source-code">run.upload_file(model_name, model_path)</p>
			<p class="source-code">if scores[1] &gt; best_test_acc:</p>
			<p class="source-code">  # register the best model as a new version</p>
			<p class="source-code">  run.register_model(model_name, model_path=model_name)</p>
			<p>As you can see, we get the result for the test accuracy metric of all previously runs tracked in this experiment <a id="_idIndexMarker487"/>and select the largest. We then register the model only if the test accuracy of the new model is higher than the previously stored best score. Nevertheless, we still upload and track the model binaries with the experiment run. </p>
			<p>We now have an enhanced version of our notebook, including metrics tracking and a better version to register a model in the model registry.</p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor067"/>Scheduling the script execution</h2>
			<p>In the previous section, we saw how you can annotate your existing ML experimentation and training code with a few lines of code in order to track relevant metrics and run artifacts in your <a id="_idIndexMarker488"/>workspace. In this section, we move from invoking the training script directly to scheduling the training script on the local machine. You might ask why this extra step is useful because there are not many differences between invoking the training script directly and scheduling the training script to run locally.</p>
			<p>The main motivation behind this exercise is that in the subsequent step, we can change the execution target to a remote compute target and run the training code on a compute cluster in the cloud instead of the local machine. This will be a huge benefit, as we can now easily test code locally and later deploy the same code to a highly scalable compute environment in the cloud.</p>
			<p>One more thing to note is that when scheduling the training script instead of invoking it, the standard output and error streams, as well as all files in the <strong class="bold">logs</strong> directory, will be streamed directly to the Azure Machine Learning workspace run. This has the benefit of tracking the script output in real time in your ML workspace, even if your code is running on the remote compute cluster.</p>
			<p>Let's implement this in a so-called <strong class="bold">authoring script</strong>. We call it an authoring script (or authoring environment) when the <a id="_idIndexMarker489"/>script or environment's job is to schedule another training or experimentation script. In addition, we will now refer to the script that runs and <a id="_idIndexMarker490"/>executes the training as the <strong class="bold">execution script</strong> (or execution environment).</p>
			<p>We need to define two things in the authoring script – an environment we will run on and a run configuration, to which we will hand over the execution script, the environment, and a possible compute target.</p>
			<p>Open the <strong class="source-inline">03_run_experiment_local.ipynb</strong> notebook file. Compared to our previous notebooks, you can see that this is a very short file, as the actual Keras training is happening now <a id="_idIndexMarker491"/>in the execution script, which you can find in the <strong class="source-inline">cifar10_cnn_remote.py</strong> file in the <strong class="source-inline">code</strong> folder.</p>
			<p>First, we need to define an <a id="_idIndexMarker492"/>environment. As we are still running locally, we create an environment with <strong class="bold">user-managed dependencies</strong> called <strong class="source-inline">user-managed-env</strong>. This will just take our environment as is from our local machine:</p>
			<p class="source-code">from azureml.core.environment import Environment</p>
			<p class="source-code">myenv = Environment(name = "user-managed-env")</p>
			<p class="source-code"><strong class="bold">myenv.python.user_managed_dependencies = True</strong></p>
			<p>In the next block, we define the location and name of the execution script we want to run locally: </p>
			<p class="source-code">import os</p>
			<p class="source-code">script = 'cifar10_cnn_remote.py'</p>
			<p class="source-code">script_folder = os.path.join(os.getcwd(), 'code')</p>
			<p>Finally, we define a run configuration using a <strong class="source-inline">ScriptRunConfig</strong> object and attach to it the source directory, the script name, and our previously defined local environment:</p>
			<p class="source-code">from azureml.core import ScriptRunConfig</p>
			<p class="source-code">runconfig = ScriptRunConfig(<strong class="bold">source_directory</strong>=script_folder,</p>
			<p class="source-code">                            <strong class="bold">script</strong>=script,</p>
			<p class="source-code">                            <strong class="bold">environment</strong> = myenv)</p>
			<p class="source-code">run = exp.submit(runconfig)</p>
			<p class="source-code">run.wait_for_completion(show_output=True)</p>
			<p>Now, execute the whole notebook, and while doing so, navigate to Azure Machine Learning Studio and look for the current run for our experiment called <strong class="source-inline">cifar10_cnn_remote</strong>. When it is visible, go to the <strong class="bold">Outputs + logs</strong> tab of the new run. You will see that the <strong class="source-inline">azureml-logs</strong> and <strong class="source-inline">logs/azureml</strong> folders will now be populated with the logging output during the run.</p>
			<p><em class="italic">Figure 3.14</em> shows an example of the ingested streaming logs:</p>
			<div>
				<div id="_idContainer092" class="IMG---Figure">
					<img src="image/B17928_03_014.jpg" alt="Figure 3.14  – The streaming logs of an Azure Machine Learning experiment run " width="1185" height="666"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.14  – The streaming logs of an Azure Machine Learning experiment run</p>
			<p>This is very handy, as now we don't really need to know where the code is ultimately executed. All we care <a id="_idIndexMarker493"/>about is seeing the output, the progress of the run while tracking all metrics, generated models, and all other artifacts. The link to the current run can be retrieved by calling the <strong class="source-inline">print(run.get_portal_url())</strong> method. </p>
			<p>However, instead of navigating to the Azure portal every time we run a training script, we can embed a widget in our notebook environment to give us the same (and more) functionality, directly within Jupyter, JupyterLab, or VS Code. To do so, we need to replace the <strong class="source-inline">run.wait_for_completion()</strong> line with the following snippet:</p>
			<p class="source-code">from <strong class="bold">azureml.widgets</strong> import RunDetails</p>
			<p class="source-code">RunDetails(run).show()</p>
			<p>Please be aware that you need to add the <strong class="bold">Azure Widgets Python extension</strong> to your environment. Please refer to this <a id="_idIndexMarker494"/>installation guide for the extension: <a href="https://docs.microsoft.com/en-us/python/api/azureml-widgets/azureml.widgets.rundetails?view=azure-ml-py">https://docs.microsoft.com/en-us/python/api/azureml-widgets/azureml.widgets.rundetails?view=azure-ml-py</a>.</p>
			<p>Finally, let's have a look at the execution script we are using. Open the file named <strong class="source-inline">cifar10_cnn_remote.py</strong> in the <strong class="source-inline">code</strong> directory. Scanning through this, you should find two additional parts that we added to the original model training code.</p>
			<p>The first one is the <a id="_idIndexMarker495"/>part where we write debug logs into the <strong class="source-inline">logs</strong> folder:</p>
			<p class="source-code"># log output of the script </p>
			<p class="source-code">logging.basicConfig(filename='logs/debug.log', </p>
			<p class="source-code">                    filemode='w',</p>
			<p class="source-code">                    level=logging.DEBUG)</p>
			<p class="source-code">logger_cb = CSVLogger('logs/training.log')</p>
			<p>The second part looks like this:</p>
			<p class="source-code">from azureml.core import Run</p>
			<p class="source-code"># load the current run</p>
			<p class="source-code">run = Run.get_context()</p>
			<p>The reason for this call is that when we want to move to a remote execution environment, we need to infer the run context. Therefore, we need to load the <strong class="source-inline">run</strong> object from the current execution context instead of creating a new run, as shown in the previous sections, where we used the <strong class="source-inline">exp.start_logging()</strong> call.</p>
			<p>The <strong class="source-inline">run</strong> object will be automatically linked with the experiment when it was scheduled through the authoring script. This is handy for remote execution, as we don't need to explicitly specify the <strong class="source-inline">run</strong> object in the execution script anymore. Using this inferred <strong class="source-inline">run</strong> object, we can log values, upload files and folders, and register models exactly as in the previous sections. </p>
			<h2 id="_idParaDest-69"><a id="_idTextAnchor068"/>Running experiments on a cloud compute</h2>
			<p>After running our experiments so far on our local machine, let's proceed now as a final step in this chapter <a id="_idIndexMarker496"/>to run the <a id="_idIndexMarker497"/>same ML model on a compute target in the ML workspace.</p>
			<p>The recommended compute target for training ML models in Azure is the managed Azure Machine Learning compute cluster, an auto-scaling compute cluster that is directly managed within your Azure subscription. If you have already used Azure for batch workloads, you will find it similar to Azure Batch and Azure Batch AI, with less configuration and tightly embedded in the Azure Machine Learning service. </p>
			<p>There are three options to deploy a cluster, either through the Azure CLI and YAML, through the Python SDK, or through Azure Machine Learning Studio. In the following steps, we will use the first options, as they are becoming more prevalent, especially with MLOps. After that, we will see how with Python code the second option works as well. </p>
			<p>Open the <strong class="source-inline">compute.yml</strong> file in the working directory. You will see the following:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">compute.yml</p>
			<p class="source-code">$schema: https://azuremlschemas.azureedge.net/latest/compute.schema.json</p>
			<p class="source-code">name: mldemocompute</p>
			<p class="source-code">type: amlcompute</p>
			<p class="source-code">size: STANDARD_D2_V2</p>
			<p class="source-code">location: westus2</p>
			<p class="source-code">min_instances: 0</p>
			<p class="source-code">max_instances: 2</p>
			<p class="source-code">idle_time_before_scale_down: 900</p>
			<p>This describes a compute cluster named <strong class="source-inline">mldemocompute</strong> that we want to deploy. This configuration defines a compute type (<strong class="source-inline">amlcompute</strong>) in the ML workspace with 0–2 nodes with a VM size of <strong class="bold">Standard D2v2</strong> (2 CPUs, 7 GB of RAM, and 100 GB HDD) in the West US 2 Azure region. In addition, we define the idle time before the cluster scales down (shuts off) to be 15 minutes (which equals 900 seconds).</p>
			<p>There are many other settings for compute clusters, including diverse network and load balancing settings. You can also define VM types with GPUs as your worker nodes – for example, <strong class="bold">Standard_NC6</strong> (6 CPUs, 56 GB of RAM, 340 GB SSD, 1 GPU, and 12 GB GPU memory) – by simply changing the configuration. </p>
			<p>In contrast to other managed clusters, such as Azure Databricks, you don't pay for a head or master node, just for worker nodes. We will go into a lot more detail about VM types for deep learning in <a href="B17928_10_ePub.xhtml#_idTextAnchor165"><em class="italic">Chapter 10</em></a>, <em class="italic">Training Deep Neural Networks on Azure</em>, and run distributed training on GPU clusters in <a href="B17928_12_ePub.xhtml#_idTextAnchor189"><em class="italic">Chapter 12</em></a>, <em class="italic">Distributed Machine Learning on Azure</em>.  </p>
			<p>If you are working with VS Code, the <strong class="bold">Azure ML</strong> extension (reachable in the Azure tab on the left) can show <a id="_idIndexMarker498"/>you YAML templates. Just go to <a id="_idIndexMarker499"/>your ML workspace, and under <strong class="bold">mldemows</strong> | <strong class="bold">Compute</strong> | <strong class="bold">Compute clusters</strong>, click on the <strong class="bold">+</strong> sign on the right. It will generate a template file, which looks like a bare version of the preceding one. In addition, if you have installed the YAML extension, it will understand the schema link in the file and will autocomplete your typing:</p>
			<ol>
				<li value="1">Open the console and run the following CLI command to create the compute instance from the YAML file:<p class="source-code"><strong class="bold">$ az ml compute create -f compute.yml -g mldemo -w mldemows</strong></p></li>
			</ol>
			<p>You can also call the shell script in the working directory called <strong class="source-inline">04_setup_azure_ml_compute.sh</strong>.</p>
			<p>After a short while, it will give you an output showing the properties of the created compute cluster.</p>
			<ol>
				<li value="2">Open the notebook called <strong class="source-inline">05_run_experiment_remote.ipynb</strong>.</li>
			</ol>
			<p>The second block in that notebook shows you the following code:</p>
			<p class="source-code">from azureml.core.compute import ComputeTarget, AmlCompute</p>
			<p class="source-code">from azureml.core.compute_target import ComputeTargetException</p>
			<p class="source-code">cluster_name = "mldemocompute"</p>
			<p class="source-code">min_nodes = 0</p>
			<p class="source-code">max_nodes = 2</p>
			<p class="source-code">vm_size = "STANDARD_D2_V2"</p>
			<p class="source-code"><strong class="bold">try</strong>:   </p>
			<p class="source-code">  aml_cluster = ComputeTarget</p>
			<p class="source-code">                (workspace=ws, name=cluster_name)</p>
			<p class="source-code"><strong class="bold">except ComputeTargetException</strong>:</p>
			<p class="source-code">  print('Cluster not '%s' not found, creating one now.' </p>
			<p class="source-code">         % cluster_name)</p>
			<p class="source-code">  config = AmlCompute.provisioning_configuration</p>
			<p class="source-code">           (vm_size=vm_size, </p>
			<p class="source-code">            min_nodes=min_nodes, </p>
			<p class="source-code">            max_nodes=max_nodes)</p>
			<p class="source-code">  aml_cluster = ComputeTarget.create</p>
			<p class="source-code">                (workspace=ws, </p>
			<p class="source-code">                 name=cluster_name,</p>
			<p class="source-code">                 provisioning_configuration=config)</p>
			<p class="source-code">aml_cluster.<strong class="bold">wait_for_completion</strong>(show_output=True)</p>
			<p>The <strong class="source-inline">except</strong> clause of the <strong class="source-inline">try</strong> construct shows you the way you can create a compute cluster through <a id="_idIndexMarker500"/>the Python SDK. As the <a id="_idIndexMarker501"/>name of the cluster is the same as the one we already deployed via the CLI, when executing this block, it will just link our compute to the <strong class="source-inline">aml_cluster</strong> object through the <strong class="source-inline">try</strong> clause. </p>
			<p>Either way, this <strong class="source-inline">try..except</strong> clause is very handy, as it either gives us back the already existing cluster or creates a new one for us. The final line of code is necessary if the compute target does not already exist, as we need to wait for the compute target to be ready to receive the run configuration in the next steps.</p>
			<p>If we now have a look at the environment definition and the run configuration, we will see some minor changes to the code from the <strong class="source-inline">03_run_experiment_local.ipynb</strong> notebook. Our environment definition now looks like this:</p>
			<p class="source-code">myenv = Environment.from_pip_requirements</p>
			<p class="source-code">        (name = "remote_env", <strong class="bold">file_path = pipreq_path</strong>)</p>
			<p>As you can see, we attach to the environment our PIP configuration file we worked with locally. In the backend, the SDK will convert this to a <strong class="bold">Conda properties file</strong> and create a container from a <a id="_idIndexMarker502"/>Docker base image. If <a id="_idIndexMarker503"/>you run the cells up to this one, you will see which base image and configuration Azure Machine Learning builds based on this input. A small excerpt of this is shown here:</p>
			<p class="source-code">"docker": {</p>
			<p class="source-code">"baseImage": "mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210714.v1",</p>
			<p class="source-code">"platform": {</p>
			<p class="source-code">    "architecture": "amd64",</p>
			<p class="source-code">    "os": "Linux"</p>
			<p class="source-code">  }</p>
			<p class="source-code">}</p>
			<p>Having a look at the final block in the notebook, we can see that the only difference is that we now define the compute target to be our <strong class="source-inline">aml_cluster</strong> in the run configuration and pass the new environment.</p>
			<p>Finally, we now run the whole notebook.</p>
			<p>The training script is now executed in the remote compute target on Azure. In the experiment run in Azure Machine Learning Studio, the snapshot, outputs, and logs look very similar to the local run. However, we can now also see the logs of the Docker environment build process for the compute target, as shown in <em class="italic">Figure 3.15</em>:</p>
			<div>
				<div id="_idContainer093" class="IMG---Figure">
					<img src="image/B17928_03_015.jpg" alt="Figure 3.15 – The Docker build phase for a remote experiment run " width="1285" height="690"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.15 – The Docker build phase for a remote experiment run</p>
			<p>As a final exercise, let's <a id="_idIndexMarker504"/>understand the <a id="_idIndexMarker505"/>steps that are performed when we submit this run to the Azure Machine Learning workspace:</p>
			<ol>
				<li value="1">The Azure Machine Learning service builds a Docker container from the defined environment if it doesn't exist already.</li>
				<li>The Azure Machine Learning service registers your environment in the private container registry so that it can be reused for other scripts and deployments.</li>
				<li>The Azure Machine Learning service queues your script execution.</li>
				<li>The Azure Machine Learning compute initializes and scales up a compute node using the defined container. </li>
				<li>The Azure Machine Learning compute executes the script.</li>
				<li>The Azure Machine Learning compute captures logs, artifacts, and metrics and streams them to the Azure Machine Learning service, and inlines the logs in the Jupyter notebook through the widget.</li>
				<li>The Azure Machine Learning service stores all artifacts in the workspace storage and your metrics in Application Insights.</li>
				<li>The Azure Machine Learning service provides you with all the information about the run through <a id="_idIndexMarker506"/>Azure Machine Learning Studio <a id="_idIndexMarker507"/>or the Python SDK.</li>
				<li>The Azure Machine Learning compute automatically scales itself down after 15 minutes (in our case) of inactivity.</li>
			</ol>
			<p>Congratulations on following along with this exercise. Given that it took us maybe 5 minutes to set up the Azure Machine Learning workspace, we get a fully fledged batch compute scheduling and execution environment for all our ML workloads. Many bits and pieces of this environment can be tuned and configured to our liking, and best of all, everything can be automated through the Azure CLI or the Azure Python SDK. Throughout the book, we will use these tools to configure, start, scale, and delete clusters for training and scoring.</p>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor069"/>Summary</h1>
			<p>This concludes the first part of this book. By now, you should have a good idea of what ML in general entails, what services and options are available in Azure, and how to utilize the Azure Machine Learning service to do ML experimentation and enhance your existing ML modeling scripts.</p>
			<p>In the next part of the book, we will concentrate on one of the aspects of ML often overlooked, the data itself. It is extremely vital to get this right. You might have heard the phrase <em class="italic">garbage in, garbage out</em> before, which holds true. Therefore, we will be working on removing as many pitfalls as possible by running automated data ingestion, cleaning and preparing data, extracting features, and performing labeling. In the end, we will bring all our knowledge together to discuss how to set up an ingestion and training ML pipeline. </p>
			<p>As the first step of this process, we need to understand different data sources and formats and bring our data to the Azure Machine Learning workspace, which we will discuss in the next chapter.</p>
		</div>
	</div>
</div>
</body></html>