<html><head></head><body>
		<div id="_idContainer181">
			<h1 id="_idParaDest-191"><a id="_idTextAnchor222"/>Chapter 12: Model Serving and Monitoring</h1>
			<p><a id="_idTextAnchor223"/>In this chapter, we will reflect on the need to serve and monitor <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) models in production and explore different means of serving ML models for users or consumers of the model. Then, we will revisit the <strong class="bold">Explainable Monitoring framework</strong> from <a href="B16572_11_Final_JM_ePub.xhtml#_idTextAnchor206"><em class="italic">Chapter 11</em></a>, <em class="italic">Key Principles for Monitoring Your ML System</em>, and implement it for the business use case we have been solving using MLOps to predict the weather. The implementation of an Explainable Monitoring framework is hands-on. We will infer the deployed API and monitor and analyze the inference data using <strong class="bold">drifts</strong> (such as data drift, feature drift, and model drift) to measure the performance of an ML system. Finally, we will look at several concepts to govern ML systems for the robust performance of ML systems to drive continuous learning and delivery.</p>
			<p>Let's start by reflecting on the need to monitor ML in production. Then, we will move on to explore the following topics in this chapter:</p>
			<ul>
				<li>Serving, monitoring, and maintaining models in production</li>
				<li>Exploring different modes of serving ML models</li>
				<li>Implementing the Explainable Monitoring framework </li>
				<li>Governing your ML system</li>
			</ul>
			<h1 id="_idParaDest-192"><a id="_idTextAnchor224"/>Serving, monitoring, and maintaining models in production</h1>
			<p>There is no <a id="_idIndexMarker798"/>point in deploying a model or an ML<a id="_idIndexMarker799"/> system<a id="_idIndexMarker800"/> and not monitoring it. Monitoring performance is one of the most important aspects of an ML system. Monitoring enables us to analyze and map out the business impact an ML system offers to stakeholders in a qualitative and quantitative manner. In order to achieve maximum business impact, users of ML systems need to be served in the most convenient manner. After that, they can consume the ML system and generate value. In previous chapters, we developed and deployed an ML model to predict the weather conditions at a port as part of the business use case that we had been solving for practical implementation. In this chapter, we will revisit the Explainable Monitoring framework that we discussed in <a href="B16572_11_Final_JM_ePub.xhtml#_idTextAnchor206"><em class="italic">Chapter 11</em></a>, <em class="italic">Key Principles for Monitoring Your ML System</em>, and implement it within our business use case. In <em class="italic">Figure 12.1</em>, we can see the <strong class="bold">Explainable Monitoring</strong> framework and some of its components, as highlighted in green: </p>
			<div>
				<div id="_idContainer154" class="IMG---Figure">
					<img src="image/image0011.jpg" alt="Figure 12.1 – Components of the Explainable Monitoring framework to be implemented&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.1 – Components of the Explainable Monitoring framework to be implemented</p>
			<p>We will implement Explainable Monitoring for these areas: <em class="italic">Data Integrity</em>, <em class="italic">Model Drift</em>, <em class="italic">Application Performance</em>, <em class="italic">Bias and Threat Detection</em>, <em class="italic">Local and Global Explanations</em>, <em class="italic">Alerts and Actions</em>, <em class="italic">Model QA and Control</em>, and <em class="italic">Model Auditing and Reports</em>. These components are the most significant, in our use case, to understand the implementation of Explainable Monitoring. We will leave out <em class="italic">Data Slicing</em> because we do not have much variety in terms of the demographics or samples within the data (for example, sex, age groups, and more). By using information from other components, we can assess the model's performance and its fairness. In this chapter, we will implement components of the <strong class="bold">Monitor</strong> and <strong class="bold">Analyze</strong> modules: <em class="italic">Data Integrity</em>, <em class="italic">Model Drift</em>, <em class="italic">Application Performance</em>, <em class="italic">Bias and Threat Detection</em>, and <em class="italic">Local and Global Explanations</em>. The remaining component implementations will be covered in <a href="B16572_13_Final_JM_ePub.xhtml#_idTextAnchor234"><em class="italic">Chapter 13</em></a>, <em class="italic">Governing the ML System for Continual Learning</em>. Before we move on to the implementation<a id="_idIndexMarker801"/> process, let's take a look at how models can<a id="_idIndexMarker802"/> be <a id="_idIndexMarker803"/>served for users to consume. </p>
			<h1 id="_idParaDest-193"><a id="_idTextAnchor225"/>Exploring different modes of serving ML models </h1>
			<p>In this section, we<a id="_idIndexMarker804"/> will consider how a model can be served for users (both humans and machines) to consume the ML service efficiently. Model serving is a critical area, which an ML system needs to succeed at to fulfill its business impact, as any lag or bug in this area can be costly in terms of serving users. Robustness, availability, and convenience are key factors to keep in mind while serving ML models. Let's take a look at some ways in which ML models can be served: this can be via batch service or on-demand mode (for instance, when a query is made on demand in order to get a prediction). A model can be served to either a machine or a human user in on-demand mode. Here is an example of serving a model to a user:</p>
			<div>
				<div id="_idContainer155" class="IMG---Figure">
					<img src="image/image002.jpg" alt="Figure 12.2 – Serving a model to users&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.2 – Serving a model to users</p>
			<p>In a typical scenario (in on-demand mode), a model is served as a service for users to consume, as shown in <em class="italic">Figure 12.2</em>. Then, an external application on a machine or a human makes a query to the prediction or ML service using their data. The ML service, upon receiving a request, uses a load balancer to route the request to an available resource (such as a container or an application) within the ML application. The load balancer also manages resources within the ML service to orchestrate and generate new containers or resources on demand. The load balance redirects the query from the user to the model running in a container within the ML application to get the prediction. On getting the prediction, the load balance reverts back to the external application on a machine, or to a human who is making the request, or to the query within the model prediction. In this <a id="_idIndexMarker805"/>way, the ML service is able to serve its users. The ML system orchestrates with the model store or registry to keep itself updated with either the latest or best-performing models in order to serve the users in the best manner. In comparison to this typical scenario where users make a query, there is another use case where the model is served as a batch service.</p>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor226"/>Serving the model as a batch service</h2>
			<p>Batch processing<a id="_idIndexMarker806"/> or serving is applied to large quantities or batches of input data (that is, not single observations but bunches of observations together). In cases where there is a large bunch of data to be inferred, a model is normally served in batch mode. One example of this is when the model is used to process the data of all consumers or users of a product or service in one go. Alternatively, a batch of data from a factory for a fixed timeline might need to be processed to detect anomalies in the machines. Compared to on-demand mode, batch mode is more resource-efficient and is usually employed when some latency can be afforded: </p>
			<div>
				<div id="_idContainer156" class="IMG---Figure">
					<img src="image/image0031.jpg" alt="Figure 12.3 – Batch Inference &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.3 – Batch inference </p>
			<p>One of the key advantages of batch processing is that unlike a REST API-based service, a batch service might require lighter or less infrastructure. Writing a batch job is easier for a data scientist compared to deploying an online REST service. This is because the data scientist just <a id="_idIndexMarker807"/>needs to train a model or deserialize a trained model on a machine and perform batch inference on a batch of data. The results of batch inference can be stored in a database as opposed to sending responses to users or consumers. However, one major disadvantage is the high latency and it not being in real time. Typically, a batch service can process hundreds or thousands of features at once. A series of tests can be used to determine the optimal batch size to arrive at an acceptable latency for the use case. Typical batch sizes can be 32, 64, 128, or 518 to the power of 2. Batch inference can be scheduled periodically and can serve many use cases where latency is not an issue. One such example is discussed next.</p>
			<p class="callout-heading">A real-world example </p>
			<p class="callout">One real-world example is a bank extracting information from batches of text documents. A bank receives thousands of documents a day from its partner institutions. It is not possible for a human agent to read through all of them and highlight any red flags in the operations listed in the documents. Batch inferencing is used to extract name entities and red flags from all the documents received by the bank in one go. The results of the batch inference or serving are then stored in a database.</p>
			<h2 id="_idParaDest-195"><a id="_idTextAnchor227"/>Serving the model to a human user</h2>
			<p>Before processing a<a id="_idIndexMarker808"/> request from human users, it is essential to check whether the user has adequate permissions to use the model. Additionally, in most cases, it is helpful to know the context in which the request was made. Gathering the context of the request will enable the model to produce better predictions. After gathering the context, we can transform it into model-readable input and infer the model to get a prediction. </p>
			<p>In practice, here are the key steps in serving an on-demand model to human users:</p>
			<ol>
				<li>Validate or authorize the request.</li>
				<li>Analyze and gather contextual information (for example, historic data, user experience data, or any other personal data of the user).</li>
				<li>Transform any contextual information into a model-readable input or schema.</li>
				<li>Infer the model with input data (with the request and contextual information) to make a prediction or get an output.</li>
				<li>Interpret the output as per the context.</li>
				<li>Relay the output to the user.<p class="callout-heading">A real-world example </p><p class="callout">Consider a chatbot serving human customers to book flight tickets. It performs contextual inference to serve human users.</p></li>
			</ol>
			<h2 id="_idParaDest-196"><a id="_idTextAnchor228"/>Serving the model to a machine </h2>
			<p>We can serve a<a id="_idIndexMarker809"/> machine or an external application using a <strong class="source-inline">REST</strong> API or a streaming service based on the use case. Typically, machine inference data requirements are either predetermined or within a standard schema. A well-defined topology and data schema in the form of a REST API or streaming service will work. Serving on demand to a machine or human varies from case to case, as, in some scenarios, demand may vary (for example, at a particular time of day when the demand for serving the user might be high, such as in the afternoon). To handle a high demand from the service, autoscaling (on the cloud) can help spawn more resources on demand and kill any idle resources to free up more resources. However, autoscaling is not a one-stop solution for scaling, as it cannot handle sudden or peculiar spikes in demand on its own:</p>
			<div>
				<div id="_idContainer157" class="IMG---Figure">
					<img src="image/image004.jpg" alt="Figure 12.4 – Message Broker for on-demand serving&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.4 – Message broker for on-demand serving</p>
			<p>The approach <a id="_idIndexMarker810"/>shown in <em class="italic">Figure 12.4</em> is resource-efficient to handle high volume demand spikes. To handle sudden spikes, message brokers such as Apache Kafka or Spark can be useful. A message broker runs processes to write and read to a queue: one process to write messages in a queue and another process to read from that queue. The served model is periodically connected to the message broker to process batches of input data from the queue to make predictions for each element in the batch. After processing the input data batches and generating predictions, the predictions are written to the output queue, which is then pushed to the users as per their requests. </p>
			<p class="callout-heading">A real-world example</p>
			<p class="callout">Consider a social media company that has millions of users. The company uses a single or common ML model for the recommender system to recommend newsfeed articles or posts to users. As the volume of requests is high in order to serve many users, it cannot depend on a REST API-based ML system (as it is synchronous). A streaming solution is better as it provides asynchronous inference for the company to serve its users. When a user logs into their application or account hosted on a machine (such as a social media company server), the application running on their machine infers the ML model (that is, the recommender system) via a streaming service for recommendations for the user newsfeed. Likewise, thousands of other users log in at the same time. The streaming service can serve all of these users seamlessly. Note that this wouldn't have been possible with the<a id="_idIndexMarker811"/> REST API service. By using a streaming service for the recommender system model, the social media company is able to serve its high volume of users in real time, avoiding significant lags.</p>
			<h1 id="_idParaDest-197"><a id="_idTextAnchor229"/>Implementing the Explainable Monitoring framework </h1>
			<p>To implement the <a id="_idIndexMarker812"/>Explainable Monitoring framework, it is worth doing a recap of what has been discussed so far, in terms of implementing hypothetical use cases. Here is a recap of what we did for our use case implementation, including the problem and solution:</p>
			<ul>
				<li><strong class="bold">Problem context</strong>: You work as a data scientist in a small team with three other data scientists<a id="_idIndexMarker813"/> for a cargo shipping company based in the port of Turku in Finland. 90% of the goods imported into Finland arrive via cargo shipping at various ports across the country. For cargo shipping, weather conditions and logistics can be challenging at times. Rainy conditions can distort operations and logistics at the ports, which can affect supply chain operations. Forecasting rainy conditions in advance allows us to optimize resources such as human resources, logistics, and transport resources for efficient supply chain operations at ports. Business-wise, forecasting rainy conditions in advance enables ports to reduce their operational costs by up to approximately 20% by enabling the efficient planning and scheduling of human resources, logistics, and transport resources for supply chain operations.</li>
				<li><strong class="bold">Task or solution</strong>: You, as a data scientist, are tasked to develop an ML-driven solution to forecast weather conditions 4 hours in advance at the port of Turku in Finland. This will enable the port to optimize its resources, thereby enabling cost savings of up to 20%. To get started, you are provided with a historic weather dataset<a id="_idIndexMarker814"/> with a 10-year-timeline from the port of Turku (the dataset can be accessed in the Git repository of this book). Your task is to build a continuous learning-driven ML solution to optimize operations at the port of Turku.</li>
			</ul>
			<p>So far, we have developed ML models and deployed them as REST API endpoints inside a Kubernetes cluster at <a href="http://20.82.202.164:80/api/v1/service/weather-prod-service/score">http://20.82.202.164:80/api/v1/service/weather-prod-service/score</a> (the address of your endpoint will be different).</p>
			<p>Next, we will replicate a real-life inference scenario for this endpoint. To do this, we will use the test dataset we had split and registered in <a href="B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074"><em class="italic">Chapter 4</em></a>, <em class="italic">Machine Learning Pipelines</em>, in the <em class="italic">Data ingestion and feature engineering</em> section. Go to your Azure ML workspace and download the <strong class="source-inline">test_data.csv</strong> dataset (which was registered as <strong class="source-inline">test_dataset</strong>) from the <strong class="bold">Datasets</strong> section or the Blob storage that is connected to your workspace, as<a id="_idIndexMarker815"/> shown in <em class="italic">Figure 12.5</em>:</p>
			<div>
				<div id="_idContainer158" class="IMG---Figure">
					<img src="image/image0051.jpg" alt="Figure 12.5 – Downloading the validation dataset (which was previously split and registered)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.5 – Downloading the validation dataset (which was previously split and registered)</p>
			<p>Get ready to infer the <strong class="source-inline">test_data.csv</strong> data with the REST API endpoint or ML service. Go to the <strong class="source-inline">12_Model_Serving_Monitoring</strong> folder and place the downloaded dataset (<strong class="source-inline">test_data.csv</strong>) inside the folder. Next, access the <strong class="source-inline">inference.</strong> <strong class="source-inline">py</strong> file:</p>
			<p class="source-code">import json</p>
			<p class="source-code">import requests</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">data = pd.read_csv('test_data.csv')</p>
			<p class="source-code">data = data.drop(columns=['Timestamp', 'Location', 'Future_weather_condition'])</p>
			<p class="source-code">                        </p>
			<p class="source-code">url = 'http://20.82.202.164:80/api/v1/service/weather-prod-service/score'</p>
			<p class="source-code">headers = {'Content-Type':'application/json'}</p>
			<p class="source-code">                     </p>
			<p class="source-code">for I in range(len(data)):</p>
			<p class="source-code">            inference_data = data.values[i].tolist()</p>
			<p class="source-code">            inference_data = json.dumps(""dat"": [inference_data]})</p>
			<p class="source-code">            r = requests.post(url, data=inference_data, headers=headers)</p>
			<p class="source-code">            print(str(i)+str(r.content))</p>
			<p>In the preceding<a id="_idIndexMarker816"/> code, we perform the following steps:</p>
			<ol>
				<li value="1">In the <strong class="source-inline">inference.py</strong> file, begin by importing the necessary libraries, such as <strong class="source-inline">json</strong>, <strong class="source-inline">requests</strong>, and <strong class="source-inline">pandas</strong>. </li>
				<li>Next, import the dataset (<strong class="source-inline">test_data.csv</strong>) to use to infer with the endpoint. </li>
				<li>Drop the unnecessary columns for inference, such as <strong class="source-inline">Timestamp</strong>, <strong class="source-inline">Location</strong>, and <strong class="source-inline">Future_weather_condition</strong> (we will predict this final column by querying the endpoint). </li>
				<li>Next, point to the URL of the endpoint (you can find this by navigating to <strong class="bold">Azure ML Workspace</strong> | <strong class="bold">Endpoints</strong> | <strong class="bold">Weather-prod-service</strong> | <strong class="bold">Consume</strong>). For simplicity, since we did not have authentication or keys set up for the service, we have the header application/JSON with no keys or authentication. </li>
				<li>Finally, we will loop through the data array by inferring each element in the array with the endpoint. To run the script, simply replace <strong class="source-inline">'url'</strong> with your endpoint and<a id="_idIndexMarker817"/> run the following command in the Terminal (from the folder location) to execute the script:<p class="source-code"><strong class="bold">&gt;&gt; python3 inference.py</strong></p></li>
			</ol>
			<p>The running script will take around 10–15 minutes to infer all of the elements of the inference data. After this, we can monitor the inference and analyze the results of the inferring data. Let's monitor and analyze this starting with data integrity. </p>
			<h2 id="_idParaDest-198"><a id="_idTextAnchor230"/>Monitoring your ML system</h2>
			<p>The <strong class="bold">Monitor</strong> module <a id="_idIndexMarker818"/>is dedicated to monitoring the application in production (that is, serving the ML model). The action monitor module has the following three functionalities: </p>
			<ul>
				<li>Data integrity:<p>-To register the target dataset </p><p>-To create a data drift monitor </p><p>-To perform data drift analysis</p><p>-To perform feature drift analysis</p></li>
				<li>Model drift</li>
				<li>Application performance</li>
			</ul>
			<p>Let's take a look <a id="_idIndexMarker819"/>at each of these functionalities in more detail next.</p>
			<h3>Data integrity</h3>
			<p>To monitor data<a id="_idIndexMarker820"/> integrity for inference data, we need to monitor data drift and feature drift to see whether there are any anomalous changes in the incoming data or any new patterns: </p>
			<ul>
				<li><strong class="bold">Data drift</strong>: This is when <a id="_idIndexMarker821"/>the properties of the independent variables change. For example, data changes can occur due to seasonality or the addition of new products or changes in consumer desires or habits, as it did during the COVID-19 pandemic.</li>
				<li><strong class="bold">Feature drift</strong>: This is <a id="_idIndexMarker822"/>when properties of the feature(s) change over time. For example, the temperature is changing due to changing seasons or seasonality, that is, in summer, the temperature is warmer compared to temperatures during winter or autumn.</li>
			</ul>
			<p>To monitor drifts, we will measure the difference for the baseline dataset versus the target dataset. The first step is to define the baseline dataset and the target dataset. This depends on use case to use case; we will use the following datasets as the baseline and target datasets: </p>
			<ul>
				<li><strong class="bold">Baseline dataset</strong>: This<a id="_idIndexMarker823"/> is the training dataset.</li>
				<li><strong class="bold">Target dataset</strong>: This is the<a id="_idIndexMarker824"/> inference dataset.</li>
			</ul>
			<p>We will use the training dataset that we previously used to train our models as the baseline dataset. This is because the model used in inference knows the patterns in the training dataset very well. The training dataset is ideal for comparing how inference data changes over time. We will compile all the inference data collected during inference into the inference dataset and compare these two datasets (that is, the baseline dataset and the target dataset) to gauge data and feature drifts for the target dataset.</p>
			<h3>Registering the target dataset</h3>
			<p>The training <a id="_idIndexMarker825"/>dataset was registered in <a href="B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074"><em class="italic">Chapter 4</em></a>, <em class="italic">Machine Learning Pipelines</em>, in the <em class="italic">Data ingestion and feature engineering</em> section. We need to register the inference dataset within the <strong class="bold">Datasets</strong> section of the Azure ML workspace. </p>
			<p>Inference data is collected as a result of using the <strong class="source-inline">azureml.monitoring</strong> SDK (the <strong class="source-inline">modelDataCollector</strong> function). By enabling monitoring functions using the <strong class="source-inline">modelDataCollector</strong> function in your scoring file (in <strong class="source-inline">score.py</strong>, as we did in <a href="B16572_06_Final_JM_ePub.xhtml#_idTextAnchor124"><em class="italic">Chapter 6</em></a>, <em class="italic">Key Principles of Deploying Your ML System</em>), we store inference data in the form of a time-series dataset in the Blob storage. In the Blob storage connected to your Azure ML workspace, inference data is stored in the <strong class="source-inline">modeldata</strong> container. In the <strong class="source-inline">modeldata</strong> container, the inference data (including both inputs and outputs) is stored the form of CSV files that are partitioned inside folders. These are structured as per year, per month, and per day (when the inference data was recorded in the production). Inside the partitioned folders, inference data is stored in CSV files that are named <strong class="source-inline">inputs.csv</strong> and <strong class="source-inline">outputs.csv</strong>. We need to register these <strong class="source-inline">input.csv</strong> files to monitor data drift and feature drift. Follow these steps to register the <strong class="source-inline">input.csv</strong> files:</p>
			<ol>
				<li value="1">Go to the <strong class="bold">Datasets</strong> section and click on <strong class="bold">Create dataset</strong>. Then, select the <strong class="bold">From datastore</strong> option, as shown in <em class="italic">Figure 12.6</em>:<div id="_idContainer159" class="IMG---Figure"><img src="image/image006.jpg" alt="Figure 12.6 – Registering the inference dataset &#13;&#10;"/></div><p class="figure-caption">Figure 12.6 – Registering the inference dataset </p></li>
				<li>Name the dataset (for example, <strong class="source-inline">Inputs-Inference-Dataset</strong>), select the dataset type as <strong class="bold">Tabular</strong>, and write an appropriate description in the <strong class="bold">Description</strong> field name by describing the purpose of your dataset. Click on <strong class="bold">Next</strong> to specify the datastore<a id="_idIndexMarker826"/> selection. Select the <strong class="bold">modeldata</strong> datastore, as shown in <em class="italic">Figure 12.7</em>:<div id="_idContainer160" class="IMG---Figure"><img src="image/image0071.jpg" alt="Figure 12.7 – Datastore selection (the Inputs-Inference-data registration)&#13;&#10;"/></div><p class="figure-caption">Figure 12.7 – Datastore selection (the Inputs-Inference-data registration)</p></li>
				<li>After selecting the <strong class="bold">modeldata</strong> datastore, you will be prompted to mention the path of the file(s). Click on the <strong class="bold">Browse</strong> button to specify the path. You will be presented with a list of files in your <strong class="bold">modeldata</strong> datastore. Go to the location where you can spot an <strong class="source-inline">input.csv</strong> file. You can find this in the folder of your <strong class="source-inline">support vectorclassifier model</strong>, which is inside the folder with your service name (for example, <strong class="source-inline">prod-webservice</strong>). Then, go into the subfolders (the default, inputs, and folders structured with dates), and go to the folder of your current date to find the <strong class="source-inline">input.csv</strong> file. Select the <strong class="screen-inline">input.csv</strong> file, as shown in <em class="italic">Figure 12.8</em>:<div id="_idContainer161" class="IMG---Figure"><img src="image/image008.jpg" alt="Figure 12.8 – Selecting path of the input.csv file (the Inputs-Inference-data registration)&#13;&#10;"/></div><p class="figure-caption">Figure 12.8 – Selecting path of the input.csv file (the Inputs-Inference-data registration)</p></li>
				<li>After <a id="_idIndexMarker827"/>selecting the <strong class="source-inline">input.csv</strong> file, click on the <strong class="bold">Save</strong> button and change the last part to include <strong class="source-inline">/**/inputs*.csv</strong> (as shown in <em class="italic">Figure 12.9</em>). This is an important step that will refer to all of the <strong class="source-inline">input.csv</strong> files in the <strong class="source-inline">inputs</strong> folder dynamically. Without referencing all of the <strong class="source-inline">input.csv</strong> files, we will confine the path to only one <strong class="source-inline">input.csv</strong> file (which was selected previously in <em class="italic">Figure 12.8</em>). By referring to all of the <strong class="source-inline">input.csv</strong> files, we will compile all of the input data (the <strong class="source-inline">inputs.csv</strong> files) into the target dataset (for example, <strong class="source-inline">Inputs-Inference-Data</strong>):<div id="_idContainer162" class="IMG---Figure"><img src="image/image0091.jpg" alt="Figure 12.9 – Referencing the path to dynamically access all the input.csv files &#13;&#10;"/></div><p class="figure-caption">Figure 12.9 – Referencing the path to dynamically access all the input.csv files </p></li>
				<li>Click on the <strong class="bold">Next</strong> button<a id="_idIndexMarker828"/> to advance to <strong class="bold">Settings and preview</strong>:<div id="_idContainer163" class="IMG---Figure"><img src="image/image010.jpg" alt="Figure 12.10 – Settings and preview (the inference dataset registration)&#13;&#10;"/></div><p class="figure-caption">Figure 12.10 – Settings and preview (the inference dataset registration)</p><p>As shown in <em class="italic">Figure 12.10</em>, we can configure the settings and preview the dataset. Point to the correct column names by selecting the <strong class="bold">Column headers</strong> dropdown and then selecting <strong class="bold">Combine headers from all files</strong>. Check for the correct column names (for example, <strong class="bold">Temperature_C</strong> and <strong class="bold">Humidity</strong>). After selecting the appropriate column names, click<a id="_idIndexMarker829"/> on the <strong class="bold">Next</strong> button to advance to the next window. Select the right schema by selecting all the columns you would like to monitor, along with their data types, as shown in <em class="italic">Figure 12.11</em>:</p><div id="_idContainer164" class="IMG---Figure"><img src="image/image0111.jpg" alt="Figure 12.11 – Schema selection (the inference dataset registration)&#13;&#10;"/></div><p class="figure-caption">Figure 12.11 – Schema selection (the inference dataset registration)</p><p>Make sure that you select the <strong class="bold">Timestamp</strong> and <strong class="bold">Date</strong> properties in the <strong class="bold">$aml_dc_scoring_timestamp</strong> column as these contain the timestamps of the inference. This step is important. Only a time-series format dataset can be used to compute drift (by the Azure drift model); otherwise, we cannot compute drift. After selecting the right schema by selecting all of the columns, click on <strong class="bold">Next</strong> to confirm all of the necessary details (such as the name of the dataset, the dataset version, its path, and more). </p></li>
				<li>Click on the <strong class="bold">Create</strong> button to create the dataset. When your dataset has been created successfully, you can view the dataset from the <strong class="bold">Dataset</strong> section in your Azure ML workspace. Go to the <strong class="bold">Datasets</strong> section to confirm your dataset has been created. Identify and click on your created dataset. Upon clicking, you will be able to view the details of your registered inference dataset, as shown in <em class="italic">Figure 12.12</em>: </li>
			</ol>
			<div>
				<div id="_idContainer165" class="IMG---Figure">
					<img src="image/image012.jpg" alt="Figure 12.12 – Viewing the registered inference dataset &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.12 – Viewing the registered inference dataset </p>
			<p>You can see all the <a id="_idIndexMarker830"/>essential attributes of your registered dataset in <em class="italic">Figure 12.12</em>. It is important to note that the relative path is dynamic and it points to the referencing all of the <strong class="source-inline">input.csv</strong> files. The result of referencing all of the input files is shown in <strong class="bold">Files in dataset</strong>. This will show multiple files in the long run (that is, it shows 6 files after 6 days of registering the dataset). It might be 1 file for you, as you have only just registered the dataset. With the passing of days or time, the number of <strong class="source-inline">input.csv</strong> files will keep increasing as a new <strong class="source-inline">input.csv</strong> file is created in the datastore in Blob storage each day. Congratulations on registering the inference data. Next, we will configure the data drift monitor.</p>
			<h3>Creating the data drift monitor </h3>
			<p>To monitor data <a id="_idIndexMarker831"/>drift and feature drift, we will use built-in drift monitoring features from the Azure ML workspace as part of the <strong class="bold">Datasets</strong> section. To monitor drifts, let's set up a <strong class="source-inline">Data Drift Monitor</strong> feature on our Azure ML workspace:</p>
			<ol>
				<li value="1">Go to your workspace and access the <strong class="bold">Datasets</strong> section. Then, select <strong class="bold">Dataset Monitors</strong> (it is in preview mode at the moment, as this feature is still being tested). Click on <strong class="bold">Create</strong>, as shown in <em class="italic">Figure 12.13</em>:<div id="_idContainer166" class="IMG---Figure"><img src="image/image0131.jpg" alt="Figure 12.13 – Creating the data drift monitor&#13;&#10;"/></div><p class="figure-caption">Figure 12.13 – Creating the data drift monitor</p></li>
				<li>Upon selecting<a id="_idIndexMarker832"/> the <strong class="bold">Create</strong> button, you will be prompted to create a new data drift monitor. Select the target dataset of your choice.</li>
				<li>In the <em class="italic">Registering the target dataset</em> section, we registered the <strong class="source-inline">inputs.csv</strong> files as <strong class="source-inline">Input-InferenceData</strong>. Select your inference dataset as the target dataset, as shown in <em class="italic">Figure 12.14</em>:<div id="_idContainer167" class="IMG---Figure"><img src="image/image014.jpg" alt="Figure 12.14 – Select target dataset &#13;&#10;"/></div><p class="figure-caption">Figure 12.14 – Select target dataset </p></li>
				<li>After selecting <a id="_idIndexMarker833"/>your target dataset, you will be prompted to point to your baseline dataset, which should be your training dataset (it was used to train your deployed ML model). Select your baseline dataset, as shown in <em class="italic">Figure 12.15</em>:<div id="_idContainer168" class="IMG---Figure"><img src="image/image0151.jpg" alt="Figure 12.15 – Select base dataset and configuring the monitor settings &#13;&#10;"/></div><p class="figure-caption">Figure 12.15 – Select the baseline dataset and configure the monitor settings </p></li>
				<li>After selecting<a id="_idIndexMarker834"/> the baseline dataset, you will be prompted to set up monitor settings, such as the name of data drift monitor (for example, <strong class="source-inline">weather-Data-Drift</strong>), the compute target to run data drift jobs, the frequency of data drift jobs (for example, once a day), and the threshold for monitoring drift (for example, 60). You will also be asked to give an email of your choice to receive notifications when the data drift surpasses a set threshold. </li>
				<li>After configuring the settings, create a data drift monitor. Go to your newly created data drift (in the <strong class="bold">Datasets</strong> section, click on <strong class="bold">Dataset Monitors</strong> to view your drift monitors), as shown in <em class="italic">Figure 12.16</em>:<div id="_idContainer169" class="IMG---Figure"><img src="image/image016.jpg" alt="Figure 12.16 – Data drift overview (it is currently empty)&#13;&#10;"/></div><p class="figure-caption">Figure 12.16 – Data drift overview (it is currently empty)</p><p>When you <a id="_idIndexMarker835"/>access your data drift monitor, you will see that there is no data. This is because we haven't computed any drift yet. In order to compute drift, we need a compute resource. </p></li>
				<li>Go to the <strong class="bold">Compute</strong> section, access the <strong class="bold">Compute clusters</strong> tab, and create a new compute resource (for example, <strong class="bold">drift-compute – Standard_DS_V2 machine</strong>), as shown in <em class="italic">Figure 12.17</em>:<div id="_idContainer170" class="IMG---Figure"><img src="image/image0171.jpg" alt="Figure 12.17 – Creating a compute cluster to compute data drift&#13;&#10;"/></div><p class="figure-caption">Figure 12.17 – Creating a compute cluster to compute data drift</p></li>
				<li>After creating <a id="_idIndexMarker836"/>the compute cluster, go back to your data drift monitor (for example, <strong class="bold">Weather-Data-Drift</strong>). Next, we will compute the data drift. </li>
				<li>Click on <strong class="bold">Analyze existing data</strong> and submit a run to analyze any existing inference data, as shown in <em class="italic">Figure 12.18</em>:<div id="_idContainer171" class="IMG---Figure"><img src="image/image018.jpg" alt="Figure 12.18 – Submitting run to analyze any data drift&#13;&#10;"/></div><p class="figure-caption">Figure 12.18 – Submitting run to analyze any data drift</p></li>
				<li>Select the start and end dates and the compute target (which was created previously, that is, <strong class="bold">drift-compute</strong>). Then, click on <strong class="bold">Submit</strong> to run drift computation. It will usually take around 10 minutes to analyze and compute data drift. You can track the progress of your runs in the <strong class="bold">Experiments</strong> section of your Azure ML workspace. <p><strong class="bold">Data drift analysis</strong>: After successfully finishing the run, data drift has been computed. Using the drift overview, as shown in <em class="italic">Figure 12.19</em>, we can monitor and analyze your ML model performance in production. We can view the data drift magnitude <a id="_idIndexMarker837"/>and drift distribution by features:</p></li>
			</ol>
			<div>
				<div id="_idContainer172" class="IMG---Figure">
					<img src="image/image019.jpg" alt="Figure 12.19 – Data Drift magnitude trend&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.19 – Data Drift magnitude trend</p>
			<p>The way that model drift is measured by the Azure ML service is that it uses a separate drift model (maintained by Azure), which looks at the baseline and compares inference data. This comparison results in a simple statistical percentage or degree of change in data.</p>
			<p>In <em class="italic">Figure 12.19</em>, the <strong class="bold">Drift magnitude trend</strong> suggests that we have had inferences made to the model on 3 days (that is, <strong class="bold">03/23/21</strong>, <strong class="bold">04/03/21</strong>, and <strong class="bold">04/04/21</strong>). </p>
			<p>The analysis shows that the data drift on these three occasions is below the threshold of 70% (this is the red line, which indicates the threshold). The data drift on <strong class="bold">03/23/21</strong> is around 50%; on <strong class="bold">04/03/21</strong>, it is around 44%; and on <strong class="bold">04/04/21</strong>, it is 40%. This gives us an idea of the changing trend in the incoming inference data to<a id="_idIndexMarker838"/> the model. Likewise, we can monitor feature drift. </p>
			<ul>
				<li><strong class="bold">Feature drift analysis</strong>: You can assess individual features and their drift by scrolling down to the <strong class="bold">Feature details</strong> section and selecting a feature of your choice. For example, we can see the <strong class="bold">Temperature_C</strong> distribution over time feature, as shown in <em class="italic">Figure 12.20</em>:</li>
			</ul>
			<div>
				<div id="_idContainer173" class="IMG---Figure">
					<img src="image/image020.jpg" alt="Figure 12.20 – Feature drift trend (Temperature_C)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.20 – Feature drift trend (Temperature_C)</p>
			<p>To monitor the feature change over time, we can select some metrics of our choice for the feature. Metrics such as <strong class="bold">Mean value</strong>, <strong class="bold">Min value</strong>, <strong class="bold">Max value</strong>, <strong class="bold">Euclidean distance</strong>, or <strong class="bold">Wasserstein distance</strong> are available to analyze feature drift. Select a metric of your choice (for example, <strong class="bold">Mean value</strong>). We have selected the <strong class="bold">Mean value</strong> metric to assess the temperature drift, as shown in <em class="italic">Figure 12.20</em>. The <strong class="bold">Mean value</strong> metric has changed from 14 to 8 as time has progressed; this shows the change of drift in the <strong class="bold">Temperature_C</strong> feature. Such a change is expected as seasonal changes give rise to changes in temperature. We can also monitor feature distribution change, as shown in <em class="italic">Figure 12.21</em>: </p>
			<div>
				<div id="_idContainer174" class="IMG---Figure">
					<img src="image/image021.jpg" alt="Figure 12.21 – The Feature distribution trend &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption"> </p>
			<p class="figure-caption">Figure 12.21 – The Feature distribution trend </p>
			<p>If the drift is<a id="_idIndexMarker839"/> drastic or anomalous, we need to check for the quality of input data being inferred into the system. Insights into feature drift enable us to understand the changing data and world around us. Likewise, we can monitor model drift to understand the model performance in accordance with the changing data and world. </p>
			<h3>Model drift</h3>
			<p>Monitoring model <a id="_idIndexMarker840"/>drift enables us to keep a check on our model performance in production. Model drift is where the properties of dependent variables change. For example, in our case, this is the classification results of the weather (that is, rain or no rain). Just as we set up data drift in the <em class="italic">Creating the data drift monitor</em> section, we can also set up a model drift monitor to monitor model outputs. Here are the high-level steps to set up model drift:</p>
			<ol>
				<li value="1">Register a new dataset (for example, <strong class="bold">Output-Inference-Data</strong>) by referencing all of the <strong class="source-inline">Outputs.csv</strong> files. The <strong class="bold">Outputs</strong> dataset can be created from the <strong class="bold">Datasets</strong> section. When creating an outputs inference dataset, select the important columns (for example, <strong class="bold">Future_Weather_Condition</strong>) and change the dataset into tabular and time-series format (drift can only be computed in time-series data) by selecting a column with <strong class="bold">Timestamp</strong>. </li>
				<li>Create a new monitor (for example, the model drift monitor) from the <strong class="bold">Dataset</strong> section, and click on <strong class="bold">Dataset Monitor</strong>. Select the feature to monitor (for example, <strong class="bold">future weather condition</strong>) and set a threshold that you want to monitor. </li>
				<li>Analyze <a id="_idIndexMarker841"/>the model drift in the overview (as shown in <em class="italic">Figure 12.22</em>):</li>
			</ol>
			<div>
				<div id="_idContainer175" class="IMG---Figure">
					<img src="image/image022.jpg" alt=" Figure 12.22 – Submitting run to analyze the data drift&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption"> Figure 12.22 – Submitting a run to analyze the data drift</p>
			<p>If your model drift has surpassed a set threshold, then that may be an indication that you should retrain or train your model comparison results in a simple statistical percentage or a degree of change in data. When the data drift has gone past the threshold (for example, 70%), we can notify the administrator or product owner via email or take actions such as deploying another model or retraining the existing model. Using smart actions, we can govern ML systems to produce maximum value. We will explore ways to govern ML systems in the next chapter (<a href="B16572_13_Final_JM_ePub.xhtml#_idTextAnchor234"><em class="italic">Chapter 13</em></a>, <em class="italic">Governing the ML System for Continual Learning</em>). So far, we have implemented the setting up data drift, feature drift, and model drift. Next, let's monitor the ML system's application <a id="_idIndexMarker842"/>performance.</p>
			<h3>Application performance </h3>
			<p>You have deployed the<a id="_idIndexMarker843"/> ML service in the form of REST API endpoints, which can be consumed by users. We can monitor these endpoints using Azure Application Insights (enabled by Azure Monitor). To monitor our application performance, access the Application Insights dashboard, as shown in <em class="italic">Figure 12.23</em>. Go to the <strong class="bold">Endpoints</strong> section in your Azure ML service workspace and select the REST API endpoint your ML model is deployed on. Click on <strong class="bold">Application Insights url</strong> to access the Application Insights endpoint connected to your REST API endpoint:</p>
			<div>
				<div id="_idContainer176" class="IMG---Figure">
					<img src="image/image023.jpg" alt="Figure 12.23 – Application Insights Overview&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.23 – Application Insights Overview</p>
			<p>From the <strong class="bold">Application Insights Overview</strong> section, we can monitor and analyze critical application performance information for your ML service. Additionally, we can monitor information such as failed requests, server response time, server requests, and availability from the <strong class="bold">Overview</strong> section, as shown in <em class="italic">Figure 12.24</em>:</p>
			<div>
				<div id="_idContainer177" class="IMG---Figure">
					<img src="image/image024.jpg" alt="Figure 12.24 – Application Insights Overview &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.24 – Application Insights Overview </p>
			<p>Based on these <a id="_idIndexMarker844"/>metrics and this information, we can monitor the application performance. Ideally, we should not have any failed requests or long server response times. To get deeper insights into the application performance, we can access the application dashboard (by clicking on the button at the top of the screen), as shown in <em class="italic">Figure 12.25</em>:</p>
			<div>
				<div id="_idContainer178" class="IMG---Figure">
					<img src="image/image025.jpg" alt="Figure 12.25 – Application dashboard with a more detailed performance assessment &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.25 – Application dashboard with a more detailed performance assessment </p>
			<p>From the <a id="_idIndexMarker845"/>application dashboard, we can monitor the application performance in more detail. For instance, we can monitor application usage, reliability, and other information. In terms of usage, <strong class="bold">Unique sessions and users</strong> is critical information to monitor the number of unique users the application is able to serve. Additionally, the <strong class="bold">Average availability</strong> information is useful to assess the availability of the service for our users. With this information, we can make scaling decisions if more resources are needed to serve users. </p>
			<p>We can monitor application reliability by assessing information such as the number of failed requests, server exceptions, and dependency failures. We can monitor responsiveness using information such as the average server response time and CPU utilization. Ideally, the application should not have any failures, and if there are any failures, we can take a deeper look into the application logs, as shown in <em class="italic">Figure 12.26</em>, by accessing <strong class="bold">Transaction search</strong> or logs:</p>
			<div>
				<div id="_idContainer179" class="IMG---Figure">
					<img src="image/image026.jpg" alt="Figure 12.26 – Accessing the logs to understand any errors or failures&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">  </p>
			<p class="figure-caption">Figure 12.26 – Accessing the logs to understand any errors or failures</p>
			<p>We can take a closer look into the logs of applications to understand any failures or errors in orde<a id="_idIndexMarker846"/>r to debug the application and maintain the healthy functioning of the application. A functional ML application results in satisfied users and maximum business impact. Therefore, monitoring applications can be rewarding to reveal potential failures and maintain the application in order to serve users in the most efficient way. </p>
			<h2 id="_idParaDest-199"><a id="_idTextAnchor231"/>Analyzing your ML system</h2>
			<p>Monitoring and <a id="_idIndexMarker847"/>analyzing your ML system in production in real time is key to understanding the performance of your ML system and ensuring its robustness to produce maximized business value. Humans play a key role in analyzing model performance and detecting subtle anomalies and threats. We can analyze model performance to detect any biases or threats and to understand why the model makes decisions in a certain pattern. We can do this by applying advanced techniques, such as data slicing, adversarial attack prevention techniques, or by understanding local and global explanations. </p>
			<h3>Data slicing </h3>
			<p>For our use case, we will leave out data slicing as we do not have much variety in terms of <a id="_idIndexMarker848"/>demographics or samples within the data (for example, sex, age groups, and more). To measure the fairness of the model, we will focus on bias detection. </p>
			<h3>Bias and threat detection</h3>
			<p>To determine the<a id="_idIndexMarker849"/> model bias in production, we can use a bias-variance trade-off method. This makes it simple to monitor and analyze the model bias or any possible threat. It goes without saying that there might be better methods to monitor bias, but the idea here is to keep it simple, as, sometimes, simplicity is better and more efficient. </p>
			<p>The disparity between our model's average prediction and the right value we are attempting to predict is the bias. Variance is the variability of the estimation of the model for a given data point or a value that informs us of our data spread. Analyzing bias and variance for inference data for the deployed model reveals the bias to be 20.1 and variance to be 1.23 (you can <a id="_idIndexMarker850"/>read more on analyzing bias and variance at <a href="https://machinelearningmastery.com/calculate-the-bias-variance-trade-off/">https://machinelearningmastery.com/calculate-the-bias-variance-trade-off/</a>). This means our model has high bias and low variance; therefore, it might be a good idea to train or retrain our model with inference data to balance the bias-variance.</p>
			<h3>Local and global explanations</h3>
			<p>Local and<a id="_idIndexMarker851"/> global explanations offer different perspectives on <a id="_idIndexMarker852"/>model performance. Local explanation offers a justification for model prediction for a specific or individual input, whereas global explanation provides insights into the model's predictive process, independent of any particular input. We previously looked at global explanations while exploring monitoring drifts in <em class="italic">Figure 12.19</em>. We can further investigate feature distribution, as shown in <em class="italic">Figure 12.21</em>, to understand local explanations in detail. </p>
			<p>Analyzing your ML system for fairness, bias, and local and global explanations gives us key insights into model performance, and we can use this information to govern our ML system.</p>
			<h1 id="_idParaDest-200"><a id="_idTextAnchor232"/>Governing your ML system </h1>
			<p>A great part of <a id="_idIndexMarker853"/>system governance involves quality assurance and control, model auditing, and reporting to have end-to-end trackability and compliance with regulations. The ML systems' efficacy (that is, its ability to produce a desired or intended result) is dependent on the way it is governed to achieve maximum business value. So far, we have monitored and analyzed our deployed model for inference data:</p>
			<div>
				<div id="_idContainer180" class="IMG---Figure">
					<img src="image/image027.jpg" alt="Figure 12.27 – Components of governing your ML system &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.27 – Components of governing your ML system </p>
			<p>The efficacy of an ML system can be determined by using smart actions that are taken based on monitoring and alerting. In the next chapter, we will explore ML system governance in terms of alerts and actions, model QA and control, and model auditing and reports. </p>
			<h1 id="_idParaDest-201"><a id="_idTextAnchor233"/>Summary</h1>
			<p>In this chapter, we learned about the key principles of serving ML models to our users and monitoring them to achieve maximized business value. We explored the different means of serving ML models for users or consumers of the model and implemented the Explainable Monitoring framework for a hypothetical business use case and deployed a model. We carried out this hands-on implementation of an Explainable Monitoring framework to measure the performance of ML systems. Finally, we discussed the need for governing ML systems to ensure the robust performance of ML systems. </p>
			<p>We will further explore the governance of ML systems and continual learning concepts in the next and final chapter!</p>
		</div>
	</body></html>