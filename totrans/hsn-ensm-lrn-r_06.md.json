["```py\n> data(\"spam\")\n> set.seed(12345)\n> Train_Test <- sample(c(\"Train\",\"Test\"),nrow(spam),replace = TRUE,prob = c(0.7,0.3))\n> spam_Train <- spam[Train_Test==\"Train\",]\n> spam_Formula <- as.formula(\"type~.\")\n> spam_b0 <- boosting(spam_Formula,data=spam_Train,mfinal=1)\n> sum(predict(spam_b0,newdata=spam_Train)$class==\n+       spam_Train$type)/nrow(spam_Train)\n[1] 0.905\n> mb0 <- margins(spam_b0,newdata=spam_Train)$margins\n> mb0[1:20]\n [1]  1 -1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n> summary(mb0)\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n -1.000   1.000   1.000   0.809   1.000   1.000 \n```", "```py\n> spam_b1 <- boosting(spam_Formula,data=spam_Train,mfinal=5)\n> sum(predict(spam_b1,newdata=spam_Train)$class==\n+       spam_Train$type)/nrow(spam_Train)\n[1] 0.948\n> mb1 <- margins(spam_b1,newdata=spam_Train)$margins\n> mb1[1:20]\n [1]  1.0000 -0.2375  0.7479 -0.2375  0.1771  0.5702  0.6069  0.7479  1.0000\n[10]  0.7479  1.0000  1.0000 -0.7479  1.0000  0.7479  1.0000  0.7479  1.0000\n[19] -0.0146  1.0000\n> summary(mb1)\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n -1.000   0.631   1.000   0.783   1.000   1.000 \n> spam_b2 <- boosting(spam_Formula,data=spam_Train,mfinal=10)\n> sum(predict(spam_b2,newdata=spam_Train)$class==\n+       spam_Train$type)/nrow(spam_Train)\n[1] 0.969\t\t\n> mb2 <- margins(spam_b2,newdata=spam_Train)$margins\n> mb2[1:20]\n [1]  0.852  0.304  0.245  0.304  0.288  0.629  0.478  0.678  0.827  0.678\n[11]  1.000  1.000 -0.272  0.517  0.700  0.517  0.700  0.478  0.529  0.852\n> summary(mb2)\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n -0.517   0.529   0.807   0.708   1.000   1.000 \n> spam_b3 <- boosting(spam_Formula,data=spam_Train,mfinal=20)\n> sum(predict(spam_b3,newdata=spam_Train)$class==\n+       spam_Train$type)/nrow(spam_Train)\n[1] 0.996\n> mb3 <- margins(spam_b3,newdata=spam_Train)$margins\n> mb3[1:20]\n [1] 0.5702 0.3419 0.3212 0.3419 0.3612 0.6665 0.4549 0.7926 0.7687 0.6814\n[11] 0.8958 0.5916 0.0729 0.6694 0.6828 0.6694 0.6828 0.6130 0.6813 0.7467\n> summary(mb3)\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n -0.178   0.537   0.719   0.676   0.869   1.000 \n> spam_b4<- boosting(spam_Formula,data=spam_Train,mfinal=50)\n> sum(predict(spam_b4,newdata=spam_Train)$class==\n+       spam_Train$type)/nrow(spam_Train)\n[1] 1\n> mb4 <- margins(spam_b4,newdata=spam_Train)$margins\n> mb4[1:20]\n [1] 0.407 0.333 0.386 0.333 0.379 0.518 0.486 0.536 0.579 0.647 0.695 0.544\n[13] 0.261 0.586 0.426 0.586 0.426 0.488 0.572 0.677\n> summary(mb4)\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.098   0.444   0.590   0.586   0.729   1.000 \n> spam_b5<- boosting(spam_Formula,data=spam_Train,mfinal=200)\n> sum(predict(spam_b5,newdata=spam_Train)$class==\n+       spam_Train$type)/nrow(spam_Train)\n[1] 1\n> mb5 <- margins(spam_b5,newdata=spam_Train)$margins\n> mb5[1:20]\n [1] 0.386 0.400 0.362 0.368 0.355 0.396 0.368 0.462 0.489 0.491 0.700 0.486\n[13] 0.317 0.426 0.393 0.426 0.393 0.385 0.624 0.581\n> summary(mb5)\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.272   0.387   0.482   0.510   0.607   0.916 \n```", "```py\n> View(cbind(mb1,mb2,mb3,mb4,mb5)[mb1<0,])\n```", "```py\n> spam_Train2 <- spam_Train\n> spam_Train2$type <- as.numeric(spam_Train2$type)-1\n> spam_gbm <- gbm(spam_Formula,distribution=\"bernoulli\",\n+       data=spam_Train2, n.trees=500,bag.fraction = 0.8,\n+       shrinkage = 0.1)\n> plot(spam_gbm) # output suppressed\n> summary(spam_gbm)\n                                var      rel.inf\ncharExclamation     charExclamation 21.740302311\ncharDollar               charDollar 18.505561199\nremove                       remove 11.722965305\nyour                           your  8.282553567\nfree                           free  8.142952834\nhp                               hp  7.399617456\n\nnum415                       num415  0.000000000\ndirect                       direct  0.000000000\ncs                               cs  0.000000000\noriginal                   original  0.000000000\ntable                         table  0.000000000\ncharSquarebracket charSquarebracket  0.000000000\n```", "```py\n> spam_gbm\ngbm(formula = spam_Formula, distribution = \"bernoulli\", data = spam_Train2, \n    n.trees = 500, shrinkage = 0.1, bag.fraction = 0.8)\nA gradient boosted model with bernoulli loss function.\n500 iterations were performed.\nThere were 57 predictors, of which 43 had nonzero influence.\n\nHere, the choice of shrinkage = 0.1 leads to 43 nonzero influential variables. We can now reduce the shrinkage factor drastically and observe the impact: \n\n> spam_gbm2 <- gbm(spam_Formula,distribution=\"bernoulli\",\n+       data=spam_Train2,n.trees=500,bag.fraction = 0.8,\n+       shrinkage = 0.0001)\n> spam_gbm2\ngbm(formula = spam_Formula, distribution = \"bernoulli\", data = spam_Train2, \n    n.trees = 500, shrinkage = 1e-04, bag.fraction = 0.8)\nA gradient boosted model with Bernoulli loss function.\n500 iterations were performed.\nThere were 57 predictors of which 2 had nonzero influence.\n```", "```py\n> windows(height=100,width=200)\n> par(mfrow=c(1,2))\n> gbm.perf(spam_gbm,plot.it=TRUE)\nUsing OOB method...\n[1] 151\nWarning message:\nIn gbm.perf(spam_gbm, plot.it = TRUE) :\n  OOB generally underestimates the optimal number of iterations although predictive performance is reasonably competitive. Using cv.folds>0 when calling gbm usually results in improved predictive performance.\n> gbm.perf(spam_gbm2,plot.it=TRUE)\nUsing OOB method...\n[1] 500\nWarning message:\nIn gbm.perf(spam_gbm2, plot.it = TRUE) :\n  OOB generally underestimates the optimal number of iterations although predictive performance is reasonably competitive. Using cv.folds>0 when calling gbm usually results in improved predictive performance.\n```", "```py\n> # Poisson regression and boosting\n> # https://stats.idre.ucla.edu/r/dae/poisson-regression/\n> pregnancy <- read.csv(\"https://stats.idre.ucla.edu/stat/data/poisson_sim.csv\")\n> pregnancy <- within(pregnancy, {\n+   prog <- factor(prog, levels=1:3, \n+   labels=c(\"General\", \"Academic\",\"Vocational\"))\n+   id <- factor(id)\n+ })\n> summary(pregnancy)\n       id        num_awards           prog          math     \n 1      :  1   Min.   :0.00   General   : 45   Min.   :33.0  \n 2      :  1   1st Qu.:0.00   Academic  :105   1st Qu.:45.0  \n 3      :  1   Median :0.00   Vocational: 50   Median :52.0  \n 4      :  1   Mean   :0.63                    Mean   :52.6  \n 5      :  1   3rd Qu.:1.00                    3rd Qu.:59.0  \n 6      :  1   Max.   :6.00                    Max.   :75.0  \n (Other):194                                                 \n> pregnancy_Poisson <- glm(num_awards ~ prog + math, \n+                     family=\"poisson\", data=pregnancy)\n> summary(pregnancy_Poisson)\n\nCall:\nglm(formula = num_awards ~ prog + math, family = \"poisson\", data = pregnancy)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-2.204  -0.844  -0.511   0.256   2.680  \n\nCoefficients:\n               Estimate Std. Error z value Pr(>|z|)    \n(Intercept)     -5.2471     0.6585   -7.97  1.6e-15 ***\nprogAcademic     1.0839     0.3583    3.03   0.0025 ** \nprogVocational   0.3698     0.4411    0.84   0.4018    \nmath             0.0702     0.0106    6.62  3.6e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 287.67  on 199  degrees of freedom\nResidual deviance: 189.45  on 196  degrees of freedom\nAIC: 373.5\n\nNumber of Fisher Scoring iterations: 6\n\n> pregnancy_boost <- gbm(num_awards ~ prog+math,dist=\"poisson\",\n+ n.trees=100,interaction.depth = 2,shrinkage=0.1,data=pregnancy)\n> cbind(pregnancy$num_awards,predict(m1,type=\"response\"),\n+       predict(pboost,n.trees=100,type=\"response\"))\n    [,1]   [,2]   [,3]\n1      0 0.1352 0.1240\n2      0 0.0934 0.1072\n3      0 0.1669 0.3375\n4      0 0.1450 0.0850\n5      0 0.1260 0.0257\n6      0 0.1002 0.0735\n\n195    1 1.0469 1.4832\n196    2 2.2650 2.0241\n197    2 1.4683 0.4047\n198    1 2.2650 2.0241\n199    0 2.4296 2.0241\n200    3 2.6061 2.0241\n> sum((pregnancy$num_awards-predict(m1,type=\"response\"))^2)\n[1] 151\n> sum((pregnancy$num_awards-predict(pboost,n.trees=100,\n+    type=\"response\"))^2)\n[1] 141\n> summary(pregnancy_boost)\n      var rel.inf\nmath math    89.7\nprog prog    10.3\n```", "```py\n> pretty.gbm.tree(pregnancy_boost,i.tree=18)\n  SplitVar SplitCodePred LeftNode RightNode MissingNode ErrorReduction Weight\n0        1      64.50000        1         5           6          10.41    100\n1        1      57.50000        2         3           4           1.14     90\n2       -1      -0.01146       -1        -1          -1           0.00     71\n3       -1       0.02450       -1        -1          -1           0.00     19\n4       -1      -0.00387       -1        -1          -1           0.00     90\n5       -1       0.05485       -1        -1          -1           0.00     10\n6       -1       0.00200       -1        -1          -1           0.00    100\n  Prediction\n0    0.00200\n1   -0.00387\n2   -0.01146\n3    0.02450\n4   -0.00387\n5    0.05485\n6    0.00200\n> pretty.gbm.tree(pregnancy_boost,i.tree=63)\n  SplitVar SplitCodePred LeftNode RightNode MissingNode ErrorReduction Weight\n0        1      60.50000        1         5           6          3.837    100\n1        0      20.00000        2         3           4          0.407     79\n2       -1      -0.00803       -1        -1          -1          0.000     40\n3       -1       0.05499       -1        -1          -1          0.000     39\n4       -1       0.02308       -1        -1          -1          0.000     79\n5       -1       0.02999       -1        -1          -1          0.000     21\n6       -1       0.02453       -1        -1          -1          0.000    100\n  Prediction\n0    0.02453\n1    0.02308\n2   -0.00803\n3    0.05499\n4    0.02308\n5    0.02999\n6    0.02453\n```", "```py\n> # Survival data\n> pbc_boost <- gbm(Surv(time,status==2)~trt + age + sex+ascites +\n+                    hepato + spiders + edema + bili + chol + \n+                    albumin + copper + alk.phos + ast + trig + \n+                    platelet + protime + stage,\n+                  n.trees=100,interaction.depth = 2,\n+                  shrinkage=0.01,dist=\"coxph\",data=pbc)\n> summary(pbc_boost)\n              var rel.inf\nbili         bili  54.220\nage           age  10.318\nprotime   protime   9.780\nstage       stage   7.364\nalbumin   albumin   6.648\ncopper     copper   5.899\nascites   ascites   2.361\nedema       edema   2.111\nast           ast   0.674\nplatelet platelet   0.246\nalk.phos alk.phos   0.203\ntrig         trig   0.178\ntrt           trt   0.000\nsex           sex   0.000\nhepato     hepato   0.000\nspiders   spiders   0.000\nchol         chol   0.000\n> pretty.gbm.tree(pbc_boost,i.tree=2)  # output suppressed\n> pretty.gbm.tree(pbc_boost,i.tree=72) # output suppressed\n```", "```py\n> ## The xgboost Package\n> data(\"spam\")\n> spam2 <- spam\n> spam2$type <- as.numeric(spam2$type)-1\n> head(data.frame(spam2$type,spam$type))\n  spam2.type spam.type\n1          1      spam\n2          1      spam\n3          1      spam\n4          1      spam\n5          1      spam\n6          1      spam\n> # 1 denotes spam, and 0 - nonspam\n> set.seed(12345)\n> Train_Test <- sample(c(\"Train\",\"Test\"),nrow(spam2),replace = TRUE,\n+                      prob = c(0.7,0.3))\n> spam2_Train <- spam2[Train_Test==\"Train\",]\n> spam_Formula <- as.formula(\"type~.\")\n> spam_train <- list()\n```", "```py\n> spam_train$data <- as(spam_train$data,\"dgCMatrix\")\n> spam_train$label <- spam2_Train$type\n> class(spam_train$data)\n[1] \"dgCMatrix\"\nattr(,\"package\")\n[1] \"Matrix\"\n```", "```py\n> # Simple XGBoosting\n> spam_xgb<- xgboost(data=spam_train$data,label=spam_train$label,+                     nrounds = 100,objective=\"binary:logistic\")\n[1]\ttrain-error:0.064062 \n[2]\ttrain-error:0.063437 \n[3]\ttrain-error:0.053438 \n[4]\ttrain-error:0.050313 \n[5]\ttrain-error:0.047812 \n[6]\ttrain-error:0.045313 \n\n[95]\ttrain-error:0.002188 \n[96]\ttrain-error:0.001875 \n[97]\ttrain-error:0.001875 \n[98]\ttrain-error:0.001875 \n[99]\ttrain-error:0.000937 \n[100]\ttrain-error:0.000937 \n```", "```py\n> xgb_predict <- predict(spam_xgb,spam_train$data)\n> sum(xgb_predict>0.5)\n[1] 1226\n> sum(spam_train$label)\n[1] 1229\n> table(spam_train$label,c(xgb_predict>0.5))\n\n    FALSE TRUE\n  0  1971    0\n  1     3 1226\n```", "```py\n> # XGBoosting with cross-validation\n> spam_xgb_cv <- xgb.cv(data=spam_train$data,\n+           label=spam_train$label,nfold=10,nrounds = 100,\n+           objective=\"binary:logistic\",prediction = TRUE)\n[1]\ttrain-error:0.064410+0.001426\ttest-error:0.091246+0.01697\n[2]\ttrain-error:0.058715+0.001862\ttest-error:0.082805+0.01421\n[3]\ttrain-error:0.052986+0.003389\ttest-error:0.077186+0.01472\n[4]\ttrain-error:0.049826+0.002210\ttest-error:0.073123+0.01544\n[5]\ttrain-error:0.046910+0.001412\ttest-error:0.070937+0.01340\n[6]\ttrain-error:0.043958+0.001841\ttest-error:0.066249+0.01346\n\n[95]\ttrain-error:0.001667+0.000340\ttest-error:0.048119+0.00926\n[96]\ttrain-error:0.001528+0.000318\ttest-error:0.047181+0.01008\n[97]\ttrain-error:0.001458+0.000260\ttest-error:0.046868+0.00974\n[98]\ttrain-error:0.001389+0.000269\ttest-error:0.047181+0.00979\n[99]\ttrain-error:0.001215+0.000233\ttest-error:0.047182+0.00969\n[100]\ttrain-error:0.001111+0.000260\ttest-error:0.045932+0.01115\n> xgb_cv_predict <- spam_xgb_cv$pred\n> sum(xgb_cv_predict>0.5)\n[1] 1206\n> table(spam_train$label,c(xgb_cv_predict>0.5))\n\n    FALSE TRUE\n  0  1909   62\n  1    85 1144\n```", "```py\n> # Stop early\n> spam_xgb_cv2 <- xgb.cv(data=spam_train$data,label=\n+             spam_train$label, early_stopping_rounds = 5,\n+             nfold=10,nrounds = 100,objective=\"binary:logistic\",\n+             prediction = TRUE)\n[1]\ttrain-error:0.064271+0.002371\ttest-error:0.090294+0.02304\nMultiple eval metrics are present. Will use test_error for early stopping.\nWill train until test_error hasn't improved in 5 rounds.\n\n[2]\ttrain-error:0.059028+0.003370\ttest-error:0.085614+0.01808\n[3]\ttrain-error:0.052048+0.002049\ttest-error:0.075930+0.01388\n[4]\ttrain-error:0.049236+0.002544\ttest-error:0.072811+0.01333\n[5]\ttrain-error:0.046007+0.002775\ttest-error:0.070622+0.01419\n[6]\ttrain-error:0.042882+0.003065\ttest-error:0.066559+0.01670\n\n[38]\ttrain-error:0.010382+0.001237\ttest-error:0.048121+0.01153[39]\ttrain-error:0.010069+0.001432\ttest-error:0.048434+0.01162\n[40]\ttrain-error:0.009653+0.001387\ttest-error:0.048435+0.01154\n[41]\ttrain-error:0.009236+0.001283\ttest-error:0.048435+0.01179\n[42]\ttrain-error:0.008924+0.001173\ttest-error:0.048748+0.01154\nStopping. Best iteration:\n[37]\ttrain-error:0.010625+0.001391\ttest-error:0.048121+0.01162\n```", "```py\n> # Continue training\n> xgboost(xgb_model=spam_xgb,\n+         data=spam_train$data,label=spam_train$label,\n+         nrounds = 10)\n[101]\ttrain-error:0.000937 \n[102]\ttrain-error:0.000937 \n[103]\ttrain-error:0.000937 \n[104]\ttrain-error:0.000937 \n[105]\ttrain-error:0.000937 \n[106]\ttrain-error:0.000937 \n[107]\ttrain-error:0.000937 \n[108]\ttrain-error:0.000937 \n[109]\ttrain-error:0.000937 \n[110]\ttrain-error:0.000937 \n##### xgb.Booster\nraw: 136 Kb \ncall:\n  xgb.train(params = params, data = dtrain, nrounds = nrounds, \n    watchlist = watchlist, verbose = verbose, print_every_n = print_every_n, early_stopping_rounds = early_stopping_rounds, maximize = maximize, save_period = save_period, save_name = save_name, xgb_model = xgb_model, callbacks = callbacks)\nparams (as set within xgb.train):\n  silent = \"1\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.print.evaluation(period = print_every_n)\n  cb.evaluation.log()\n  cb.save.model(save_period = save_period, save_name = save_name)\nniter: 110\nevaluation_log:\n    iter train_error\n       1    0.064062\n       2    0.063437\n---                 \n     109    0.000937\n     110    0.000937\n```", "```py\n> # Variable Importance\n> xgb.plot.importance(xgb.importance(names(spam_train$data),\n+                                    spam_xgb)[1:10,])\n```", "```py\n> library(h2o)\n\n----------------------------------------------------------------------\n\nYour next step is to start H2O:\n    > h2o.init()\n\nFor H2O package documentation, ask for help:\n    > ??h2o\n\nAfter starting H2O, you can use the Web UI at http://localhost:54321\nFor more information visit http://docs.h2o.ai\n\n----------------------------------------------------------------------\n\nAttaching package: 'h2o'\n\nThe following objects are masked from 'package:stats':\n\n    cor, sd, var\n\nThe following objects are masked from 'package:base':\n\n    %*%, %in%, &&, ||, apply, as.factor, as.numeric, colnames,\n    colnames<-, ifelse, is.character, is.factor, is.numeric, log,\n    log10, log1p, log2, round, signif, trunc\n\nWarning message:\npackage 'h2o' was built under R version 3.4.4 \n> h2o.init()\n\nH2O is not running yet, starting it now...\n\nNote:  In case of errors look at the following log files:\n    C:\\Users\\tprabhan\\AppData\\Local\\Temp\\Rtmpu6f0fO/h2o_TPRABHAN_started_from_r.out\n    C:\\Users\\tprabhan\\AppData\\Local\\Temp\\Rtmpu6f0fO/h2o_TPRABHAN_started_from_r.err\n\njava version \"1.7.0_67\"\nJava(TM) SE Runtime Environment (build 1.7.0_67-b01)\nJava HotSpot(TM) 64-Bit Server VM (build 24.65-b04, mixed mode)\n\nStarting H2O JVM and connecting: .. Connection successful!\n\nR is connected to the H2O cluster: \n    H2O cluster uptime:         4 seconds 449 milliseconds \n    H2O cluster version:        3.16.0.2 \n    H2O cluster version age:    7 months and 7 days !!! \n    H2O cluster name:           H2O_started_from_R_TPRABHAN_saz680 \n    H2O cluster total nodes:    1 \n    H2O cluster total memory:   1.76 GB \n    H2O cluster total cores:    4 \n    H2O cluster allowed cores:  4 \n    H2O cluster healthy:        TRUE \n    H2O Connection ip:          localhost \n    H2O Connection port:        54321 \n    H2O Connection proxy:       NA \n    H2O Internal Security:      FALSE \n    H2O API Extensions:         AutoML, Algos, Core V3, Core V4 \n    R Version:                  R version 3.4.0 (2017-04-21) \n\nWarning message:\nIn h2o.clusterInfo() : \nYour H2O cluster version is too old (7 months and 7 days)!\nPlease download and install the latest version from http://h2o.ai/download/\n```"]