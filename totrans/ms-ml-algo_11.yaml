- en: Autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to look at an unsupervised model family whose
    performance has been boosted by modern deep learning techniques. Autoencoders
    offer a different approach to classic problems such as dimensionality reduction
    or dictionary learning, but unlike many other algorithms, they don't suffer the
    capacity limitations that affect many famous models. Moreover, they can exploit
    specific neural layers (such as convolutions) to extract pieces of information
    based on specialized criteria. In this way, the internal representations can be
    more robust to different kinds of distortions and much more efficient in terms
    of the amount of information they can process.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, we are going to discuss the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Standard autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Denoising autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sparse autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variational autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we discussed how real datasets are very often high-dimensional
    representations of samples that lie on low-dimensional manifolds (this is one
    of the semi-supervised pattern's assumptions, but it's generally true). As the
    complexity of a model is proportional to the dimensionality of the input data,
    many techniques have been analyzed and optimized in order to reduce the actual
    number of *valid components*. For example, PCA selects the features according
    to the relative explained variance, while ICA and generic dictionary learning
    techniques look for basic atoms that can be combined to rebuild the original samples.
    In this chapter, we are going to analyze a family of models based on a slightly
    different approach, but whose capabilities are dramatically increased by the employment
    of deep learning methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'A generic **autoencoder** is a model that is split into two separate (but not
    completely autonomous) components called an **Encoder** and a **Decoder**. The
    task of the encoder is to transform an input sample into an encoded feature vector,
    while the task of the decoder is the opposite: rebuilding the original sample
    using the feature vector as input. The following diagram shows a schematic representation
    of a generic model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d193d496-7b5e-494f-bd76-4dd061a611ec.png)'
  prefs: []
  type: TYPE_IMG
- en: Schema of a generic autoencoder
  prefs: []
  type: TYPE_NORMAL
- en: 'More formally, we can describe the encoder as a parametrized function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1119ad07-4885-4d70-b284-6a7605a9e978.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The output *z[i]* is a vectorial code whose dimensionality is normally quite
    lower than the inputs. Analogously, the decoder is described as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a57ab307-c63a-4395-9098-440a507acab1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The goal of a standard algorithm is to minimize a cost function that is proportional
    to the reconstruction error. A classic method is based on the mean squared error
    (working on a dataset with *M* samples):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b1579613-65da-42f3-b2ec-7c8a3acc242f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This function depends only on the input samples (which are constant) and the
    parameter vectors; therefore, this is *de facto* an unsupervised method where
    we can control the internal structure and the constraints imposed on the *z[i ]*code.
    From a probabilistic viewpoint, if the input *x**[i ]*samples are drawn from a *p(X) *data-generating
    process, our goal is to find a *q(•) *parametric distribution that minimizes the
    Kullback–Leibler divergence with *p(X)*. Considering the previous definitions,
    we can define *q(•**)* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1901744e-a487-4f9d-95eb-2e26367c1a69.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, the Kullback–Leibler divergence becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8ccc7428-9f9d-4003-8477-430246f12cdf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The first term represents the negative entropy of the original distribution,
    which is constant and isn''t involved in the optimization process. The other term
    is the cross-entropy between the *p* and *q*. If we assume Gaussian distributions
    for *p* and *q*, the mean squared error is proportional to the cross-entropy (for
    optimization purposes, it''s equivalent to it), and therefore this cost function
    is still valid under a probabilistic approach. Alternatively, it''s possible to
    consider Bernoulli distributions for *p* and *q*, and the cross-entropy becomes
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/57185459-91e2-4b61-9fee-75335a8af9c2.png)'
  prefs: []
  type: TYPE_IMG
- en: The main difference between the two approaches is that while a mean squared
    error can be applied to *x[i] ∈ ℜ^q* (or multidimensional matrices), Bernoulli
    distributions need *x[i]* *∈ [0, 1]*^(*q* )(formally, this condition should be
    *x[i] ∈ {0, 1}^q*; however, the optimization can also be successfully performed
    when the values are not binary). The same constraint is necessary for the reconstructions;
    therefore, when using neural networks, the most common choice is to employ sigmoid
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: An example of a deep convolutional autoencoder with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This example (like all the others in this and the following chapters) is based
    on TensorFlow (for information about the installation of TensorFlow, please refer
    to the information box at the end of the section), because this framework allows
    a greater flexibility that is sometimes much more problematic with Keras. We will
    approach this example pragmatically, and so we are not going to explore all the
    features because they are beyond the scope of this book; however, interested readers
    can refer to *Deep Learning with TensorFlow - Second Edition, Zaccone G., Karim
    R., Packt*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we are going to create a deep convolutional autoencoder and
    train it using the Fashion MNIST dataset. The first step is loading the data (using
    the Keras helper function), normalizing, and in order to speed up the computation,
    limiting the training set to 1,000 samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we can create the `Graph`, setting up the whole architecture,
    which is made up of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The encoder (all layers have padding "same" and ReLU activation):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolution with 32 filters, kernel size equal to (3 × 3), and strides (2 ×
    2)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolution with 64 filters, kernel size equal to (3 × 3), and strides (1× 1)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolution with 128 filters, kernel size equal to (3 × 3), and strides (1 ×
    1)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The decoder:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transpose convolution with 128 filters, kernel size equal to (3 × 3), and strides
    (2 × 2)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Transpose convolution with 64 filters, kernel size equal to (3 × 3), and strides
    (1× 1)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Transpose convolution with 32 filters, kernel size equal to (3 × 3), and strides
    (1 × 1)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Transpose convolution with 1 filter, kernel size equal to (3 × 3), strides (1 ×
    1), and sigmoid activation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As the images are (28 × 28), we prefer to resize each batch to the dimensions
    of (32 × 32) to easily manage all the subsequent operations that are based on
    sizes which are a power of 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The loss function is a standard L2 without any other constraint. I invite the
    reader to test different optimizers and learning rates to employ a solution that
    guarantees the minimum loss value. After defining the `Graph`, it''s possible
    to set up an `InteractiveSession` (or a standard one), initialize all variables,
    and begin the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the training process is finished, we can check the average code length
    for the whole dataset (this information is useful to compare this result with
    the one achieved by imposing a sparsity constraint):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This value is very small, indicating that the representations are already rather
    sparse; however, we are going to compare it with the mean obtained by a sparse
    autoencoder. We can now process a few images (10) by encoding and decoding them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/107520ee-94a1-4dc3-ba5b-f6416cf0d4f5.png)'
  prefs: []
  type: TYPE_IMG
- en: Original images (upper row); decoded images (lower row)
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the reconstructions are rather lossy, but the autoencoder successfully
    learned how to reduce the dimensionality of the input samples. As an exercise,
    I invite the reader to split the code into two separate sections (encoder and
    decoder) and to optimize the architecture in order to achieve better accuracy
    on the whole Fashion MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow is available for Linux, Windows, and OS X with both CPU and CUDA
    GPU support. In many cases, it's possible to install it using the `pip install
    -U tensorflow `command; however, I suggest that you read the updated instructions
    for each platform at [https://www.tensorflow.org/install/](https://www.tensorflow.org/install/).
  prefs: []
  type: TYPE_NORMAL
- en: Denoising autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Autoencoders can be used to determine under-complete representations of a dataset;
    however, Bengio et al. (in P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and
    P. Manzagol''s book *Stacked Denoising Autoencoders: Learning Useful Representations
    in a Deep Network with a Local Denoising Criterion,* from the *Journal* *of Machine
    Learning Research 11/2010*) proposed to use them not to learn the exact representation
    of a sample in order to rebuild it from a low-dimensional code, but rather to
    denoise input samples. This is not a brand new idea, because, for example, Hopfield
    networks (proposed a few decades ago) had the same purpose, but its limitations
    in terms of capacity led researchers to look for different methods. Nowadays,
    deep autoencoders can easily manage high-dimensional data (such as images) with a
    consequent space requirement, that''s why many people are now reconsidering the
    idea of teaching a network how to rebuild a sample image starting from a corrupted
    one.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, there are not many differences between denoising autoencoders and
    standard autoencoders. However, in this case, the encoder must work with noisy
    samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b1ba7e8-705a-4b9a-a017-59bd6da0cf05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The decoder''s cost function remains the same. If the noise is sampled for
    each batch, repeating the process for a sufficiently large number of iterations
    allows the autoencoder to learn how to rebuild the original image when some fragments
    are missing or corrupted. To achieve this goal, the authors suggested different
    possible kinds of noise. The most common choice is to sample Gaussian noise, which
    has some helpful features and is coherent with many real noisy processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/59e89e7a-90c5-44d8-90db-4500f1499453.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Another possibility is to employ an input dropout layer, zeroing some random
    elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/495a9795-f1a1-4574-a0ac-4a0deda8a2c4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This choice is clearly more drastic, and the rate must be properly tuned. A
    very large number of dropped pixels can irreversibly delete many pieces of information
    and the reconstruction can become more difficult and *rigid* (our purpose is to
    extend the autoencoder''s ability to other samples drawn from the same distribution).
    Alternatively, it''s possible to mix up Gaussian noise and the dropout''s, switching
    between them with a fixed probability. Clearly, the models must be more complex
    than standard autoencoders because now they have to cope with missing information;
    the same concept applies to the code length: very under-complete code wouldn''t
    be able to provide all the elements needed to reconstruct the original image in
    the most accurate way. I suggest testing all the possibilities, in particular
    when the noise is constrained by external conditions (for example, old photos
    or messages transmitted through channels affected by precise noise processes).
    If the model must also be employed for never-before-seen samples, it''s extremely
    important to select samples that represent the true distribution, using data augmentation
    techniques (limited to operations compatible with the specific problem) whenever
    the number of elements is not enough to reach the desired level of accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: An example of a denoising autoencoder with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example (based on the previous one), we are going to employ a very
    similar architecture, but as the goal is denoising the images, we will impose
    a code length equal to (width × height), setting all the strides to (1 × 1), and
    therefore we won''t need to resize the images anymore:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we need to pass both the noisy images (through the `placeholder
    input_noisy_images`) and the original ones (which are used to compute the final
    L2 loss function). For our example, we have decided to employ Gaussian noise with
    a standard deviation of `σ = 0.2` (clipping the final values so that they are
    always constrained between 0 and 1):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The result after 200 epochs is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/430f3d36-e54f-4420-9fd4-622f797a49ac.png)'
  prefs: []
  type: TYPE_IMG
- en: Noisy samples (upper row); denoised samples (lower row)
  prefs: []
  type: TYPE_NORMAL
- en: The denoising autoencoder has successfully learned to rebuild the original images
    in the presence of Gaussian noise. I invite the reader to test other methods (such
    as using an initial dropout) and increase the noise level to understand what the
    maximum corruption is that this model can effectively remove.
  prefs: []
  type: TYPE_NORMAL
- en: Sparse autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In general, standard autoencoders produce dense internal representations. This
    means that most of the values are different from zero. In some cases, however,
    it''s more useful to have sparse codes that can better represent the atoms belonging
    to a dictionary. In this case, if *z[i]* = (0, 0, *z[i]^n*, ..., 0, *z[i]^m*,
    ...), we can consider each sample as the overlap of specific atoms weighted accordingly.
    To achieve this objective, we can simply apply an L1 penalty to the code layer,
    as explained in [Chapter 1](acff0775-f21c-4b6d-8ef2-c78713e21364.xhtml), *Machine
    Learning* *Models* *Fundamentals*. The loss function for a single sample therefore
    becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29b6cc9e-af0c-4533-b5a7-f578a866da9d.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, we need to consider the extra hyperparameter α, which must be
    tuned to increase the sparsity without a negative impact on the accuracy. As a
    general rule of thumb, I suggest starting with a value equal to 0.01 and reducing
    it until the desired result has been achieved. In most cases, higher values yield
    very poor performance, and therefore they are generally avoided.
  prefs: []
  type: TYPE_NORMAL
- en: 'A different approach has been proposed by Andrew Ng (in his book *Sparse Autoencoder,
    CS294A, Stanford University*). If we consider the code layer as a set of independent
    Bernoulli random variables, we can enforce sparsity by considering a generic reference
    Bernoulli variable with a very low mean (for example, *p[r]* = 0.01) and adding
    the Kullback–Leibler divergence between the generic element *z[i]^((j))* and *p[r]*
    to the cost function. For a single sample, the extra term is as follows (*p* is
    the code length):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b39a7eec-78fc-4f40-9149-a3f3a69ef8c7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The resulting loss function becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f84c138c-77bb-4cfa-8f0e-0b072d9360dc.png)'
  prefs: []
  type: TYPE_IMG
- en: The effect of this penalty is similar to L1 (with the same considerations about
    the α hyperparameter), but many experiments have confirmed that the resulting
    cost function is easier to optimize, and it's possible to achieve the same level
    of sparsity that reaches higher reconstruction accuracies. When working with sparse
    autoencoders, the code length is often larger because of the assumption that a
    single element is made up of a small number of atoms (compared to the dictionary
    size). As a result, I suggest that you evaluate the level of sparsity with different
    code lengths and select the combination that maximizes the former and minimizes
    the latter.
  prefs: []
  type: TYPE_NORMAL
- en: Adding sparseness to the Fashion MNIST deep convolutional autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we are going to add an L1 regularization term to the cost
    function that was defined in the first exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The training process is exactly the same, and therefore we can directly show
    the final code mean after 200 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the mean is now lower, indicating that more code values are
    close to 0\. I invite the reader to implement the other strategy, considering
    that it's easier to create a constant vector filled with small values (for example,
    0.01) and exploit the vectorization properties offered by TensorFlow. I also suggest
    simplifying the Kullback–Leibler divergence by splitting it into an entropy term
    *H(p[r])* (which is constant) and a cross-entropy *H(z, p[r])* term.
  prefs: []
  type: TYPE_NORMAL
- en: Variational autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **variational autoencoder** (**VAE**) is a generative model proposed by Kingma
    and Wellin (in their work *Auto-Encoding Variational Bayes, arXiv:1312.6114 [stat.ML]*)
    that partially resembles a standard autoencoder, but it has some fundamental internal
    differences. The goal, in fact, is not finding an encoded representation of a
    dataset, but determining the parameters of a generative process that is able to
    yield all possible outputs given an input data-generating process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take the example of a model based on a learnable parameter vector *θ*
    and a set of latent variables *z* that have a probability density function *p(z;θ)*.
    Our goal can therefore be expressed as the research of the *θ* parameters that
    maximize the likelihood of the marginalized distribution *p(x;**θ)* (obtained
    through the integration of the joint probability *p(x,z;θ*)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/95315e07-cf03-4353-bb7a-4a7128d08681.png)'
  prefs: []
  type: TYPE_IMG
- en: If this problem could be easily solved in closed form, a large set of samples
    drawn from the *p(x)* data generating process would be enough to find a *p(x;θ)*
    good approximation. Unfortunately, the previous expression is intractable in the
    majority of cases because the true prior *p(z)* is unknown (this is a secondary
    issue, as we can easily make some helpful assumptions) and the posterior distribution
    *p(x|z;θ)* is almost always close to zero. The first problem can be solved by
    selecting a simple prior (the most common choice is *z ∼ N(0, I)*), but the second
    one is still very hard because only a few *z* values can lead to the generation
    of acceptable samples. This is particularly true when the dataset is very high
    dimensional and complex (for example, images). Even if there are millions of combinations,
    only a small number of them can yield realistic samples (if the images are photos
    of cars, we expect four wheels in the lower part, but it's still possible to generate
    samples where the wheels are on the top). For this reason, we need to exploit
    a method to reduce the sample space. Variational Bayesian methods (read *C.* Fox
    and S. Roberts's work *A Tutorial on Variational Bayesian Inference* from *Orchid*
    for further information) are based on the idea of employing *proxy* distributions,
    which are easy to sample and, in this case, whose density is very high (that is,
    the probability of generating a reasonable output is much higher than the true
    posterior).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we define an approximate posterior, considering the architecture
    of a standard autoencoder. In particular, we can introduce a *q(z|x;θ[q])* distribution
    that acts as an encoder (that doesn''t behave determinastically anymore), which
    can be easily modeled with a neural network. Our goal, of course, is to find the
    best *θ[q]* parameter set to maximize the similarity between *q* and the true
    posterior distribution *p(z|x;θ)*. This result can be achieved by minimizing the
    Kullback–Leibler divergence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b18cfc5e-0f3f-46ae-851c-b4794a60d700.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the last formula, the term *log p(x;**θ)* doesn''t depend on *z*, and therefore
    it can be extracted from the expected value operator and the expression can be
    manipulated to simplify it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/96a6d5c0-f5c8-4920-9ca5-8736ae50a881.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The equation can be also rewritten as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56866363-4d7f-4463-8c41-e41f7ae56d30.png)'
  prefs: []
  type: TYPE_IMG
- en: On the right-hand side, we now have the term **ELBO** (short for **evidence
    lower bound**) and the Kullback–Leibler divergence between the probabilistic encoder
    *q(z|x;θ[q])* and the true posterior distribution *p(z|x;**θ)*. As we want to
    maximize the log-probability of a sample under the *θ* parametrization, and considering
    that the KL divergence is always non-negative, we can only work with the ELBO
    (which is a lot easier to manage than the other term). Indeed, the loss function
    that we are going to optimize is the negative ELBO. To achieve this goal, we need
    two more important steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first one is choosing an appropriate structure for *q(z|x;θ[q])*. As *p(z;θ)*
    is assumed to be normal, we can supposedly model *q(z|x;θ[q])* as a multivariate
    Gaussian distribution, splitting the probabilistic encoder into two blocks fed
    with the same lower layers:'
  prefs: []
  type: TYPE_NORMAL
- en: A mean *μ(z|x;θ[q])* generator that outputs a *μ[i] ∈ ℜ^p* vector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A *Σ(z|x;θ[q]**)* covariance generator (assuming a diagonal matrix) that outputs
    a *σ[i] ∈ ℜ^p* vector so that *Σ[i]=*diag*(σ[i])*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this way, *q(z|x;θ[q]**) = N(μ(z|x;θ[q]), Σ(z|x;θ[q]))*, and therefore the
    second term on the right-hand side is the Kullback*-*Leibler divergence between
    two Gaussian distributions that can be easily expressed as follows (*p* is the
    dimension of both the mean and covariance vector):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f7f2056-f177-4232-85d8-b8000a39adcb.png)'
  prefs: []
  type: TYPE_IMG
- en: This operation is simpler than expected because, as *Σ* is diagonal, the trace
    corresponds to the sum of the elements *Σ[1]* + *Σ[2]* + [...] + *Σ[p]* and log(|*Σ*|)
    = log(*Σ[1]Σ[2]...Σ[p]*) = log *Σ[1]* + log *Σ[2]* + ... + log *Σ[p]*.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, maximizing the right-hand side of the previous expression is
    equivalent to maximizing the expected value of the log probability to generate
    acceptable samples and minimizing the discrepancy between the normal prior and
    the Gaussian distribution synthesized by the encoder. Everything seems much simpler
    now, but there is still a problem to solve. We want to use neural networks and
    the stochastic gradient descent algorithm, and therefore we need differentiable
    functions. As the Kullback*-*Leibler divergence can be computed only using minibatches
    with *n* elements (the approximation becomes close to the true value after a sufficient
    number of iterations), it''s necessary to sample *n* values from the distribution
    *N(μ(z|x;θ[q]), Σ(z|x;θ[q]**)*) and, unfortunately, this operation is not differentiable.
    To solve this problem, the authors suggested a reparameterization trick: instead
    of sampling from *q(z|x;θ[q])*, we can sample from a normal distribution, *ε ∼
    N(0, I)*, and build the actual samples as *μ(z|x;θ[q])* + *ε · Σ(z|x;θ[q])*².
    Considering that *ε* is a constant vector during a batch (both the forward and
    backward phases), it''s easy to compute the gradient with respect to the previous
    expression and optimize both the decoder and the encoder.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The last element to consider is the first term on the right-hand side of the
    expression that we want to maximize:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f5497892-3ca4-4615-becd-3f428a38df72.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This term represents the negative cross-entropy between the actual distribution
    and the reconstructed one. As discussed in the first section, there are two feasible
    choices: Gaussian or Bernoulli distributions. In general, variational autoencoders
    employ a Bernoulli distribution with input samples and reconstruction values constrained
    between 0 and 1\. However, many experiments have confirmed that the mean squared
    error can speed up the training process, and therefore I suggest that the reader
    test both methods and pick the one that guarantees the best performance (both
    in terms of accuracy and training speed).'
  prefs: []
  type: TYPE_NORMAL
- en: An example of a variational autoencoder with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s continue working with the Fashion MNIST dataset to build a variational
    autoencoder. As explained, the output of the encoder is now split into two components:
    the mean and covariance vectors (both with dimensions equal to *(width · height)*)
    and the decoder input is obtained by sampling from a normal distribution and projecting
    the code components. The complete `Graph` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the only differences are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The generation of the encoder input is `(normal_samples * code_std) + code_mean`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The use of sigmoid cross-entropy as reconstruction loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The presence of the Kullback*-*Leibler divergence as a regularization term
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The training process is identical to the first example in this chapter, as
    the sampling operations are performed directly by TensorFlow. The result after
    200 epochs is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ab30ddbf-f422-4504-95ea-388e16d1561c.png)'
  prefs: []
  type: TYPE_IMG
- en: Variational autoencoder output
  prefs: []
  type: TYPE_NORMAL
- en: As an exercise, I invite the reader to use RGB datasets (such as Cifar-10, which
    is found at [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html))
    to test the generation ability of the VAE by comparing the output samples with
    the one drawn from the original distribution.
  prefs: []
  type: TYPE_NORMAL
- en: In these kinds of experiments, where the random numbers are generated by both
    NumPy and TensorFlow, the random seeds are always set equal to 1,000 (`np.random.seed(1000)`
    and `tf.set_random_seed(1000)`). Other values or subsequent tests without resetting
    the seeds can yield slightly different results.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we presented autoencoders as unsupervised models that can
    learn to represent high-dimensional datasets with lower-dimensional codes. They
    are structured into two separate blocks (which, however, are trained together):
    an encoder, responsible for mapping the input sample to an internal representation,
    and a decoder, which must perform the inverse operation, rebuilding the original
    image starting from the code.'
  prefs: []
  type: TYPE_NORMAL
- en: We have also discussed how autoencoders can be used to denoise samples and how
    it's possible to impose a sparsity constraint on the code layer to resemble the
    concept of standard dictionary learning. The last topic was about a slightly different
    pattern called a variational autoencoder. The idea is to build a generative model
    that is able to reproduce all the possible samples belonging to a training distribution.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to briefly introduce a very important model
    family called **generative adversarial networks** (**GANs**), which are not very
    different from the purposes of a variational autoencoder, but which have a much
    more flexible approach.
  prefs: []
  type: TYPE_NORMAL
