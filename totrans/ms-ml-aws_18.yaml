- en: Creating Clusters on AWS
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在AWS上创建集群
- en: One of the key problems in machine learning is understanding how to scale and
    parallelize the learning across multiple machines. Whether you are training deep
    learning models, which are very heavy on hardware usage, or just launching machines
    for creating predictions it is essential that we select the appropriate hardware
    configuration, both for const considerations and runtime performance reasons.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的一个关键问题是如何在多台机器上扩展和并行化学习。无论你是训练深度学习模型，这些模型对硬件的使用非常重，还是仅仅为了创建预测而启动机器，选择合适的硬件配置对于成本考虑和运行时性能都是至关重要的。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Choosing your instance types
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择您的实例类型
- en: Distributed deep learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式深度学习
- en: Choosing your instance types
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择您的实例类型
- en: In the [Chapter 4](af506fc8-f482-453e-8162-93a676b2e737.xhtml), *Predicting
    User Behavior with Tree-Based Methods,* and other chapters, we had to launch EMR
    clusters and SageMaker instances (servers) for learning and model serving. In
    this section, we discuss the characteristics of the different instance types.
    In this chapter, you can find all supported instance types AWS provides at [https://aws.amazon.com/ec2/instance-types/](.).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](af506fc8-f482-453e-8162-93a676b2e737.xhtml)“使用基于树的算法预测用户行为”和其他章节中，我们不得不启动EMR集群和SageMaker实例（服务器）来进行学习和模型服务。在本节中，我们讨论了不同实例类型的特性。在本章中，你可以找到AWS提供的所有支持的实例类型，请参阅[https://aws.amazon.com/ec2/instance-types/](.)。
- en: Depending on the task at hand, we should use different instance types. For example,
    we may require an instance type with GPUs rather than CPUs for deep learning.
    When launching a large iterative **Extract, Transform, and Load** (**ETL**) job
    (that is, a data transformation job) on Apache Spark, we might need large amounts
    of memory. To make it easier for the users, AWS has classified the instances into
    families that are catered for different use cases. Additionally, AWS constantly
    provides newer hardware configurations for each family. These are called **generations**.
    Typically, a new generation provides improved performance over the previous generation.
    However, older generations are usually still available. In turn, each family has
    machines of different sizes in terms of compute and memory capabilities.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 根据手头的任务，我们应该使用不同的实例类型。例如，我们可能需要一个带有GPU而不是CPU的实例类型来进行深度学习。当在Apache Spark上启动大型迭代**提取、转换和加载**（**ETL**）（即数据转换作业）时，我们可能需要大量的内存。为了使用户更容易操作，AWS已经将实例分类为针对不同用例的系列。此外，AWS为每个系列不断提供新的硬件配置。这些被称为**代**。通常，新一代提供了比上一代更好的性能。然而，旧一代通常仍然可用。反过来，每个系列在计算和内存能力方面都有不同大小的机器。
- en: 'The most commonly used families are as follows:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的系列如下：
- en: Compute optimized (C-family)
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算优化型（C系列）
- en: Memory optimized (M-family)
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存优化型（M系列）
- en: Accelerated computing (P-family)
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加速计算型（P系列）
- en: Storage optimized (I-family)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储优化型（I系列）
- en: General purpose (R-family)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通用型（R系列）
- en: 'There are other families for each optimization objective, but in the previous
    list, we list the most commonly used family for each. Each family may have a different
    configuration. The following table shows a few configurations for the C and M
    families. Each configuration has a different price. For example, the fifth generation,
    xlarge, and C-family machine costs $0.085 at the time of this writing on the us-east-1
    region of AWS. As you can see, at a given price level, the user can choose to
    pay for a configuration that has more memory power and less compute power or vice
    versa. The **Memory (GB)** column in the following table shows values in gigabytes
    and the vCPUs are units of processing power in virtual machines, as measured by
    AWS. The prices shown in the table are just reference prices that correspond to
    the Virginia data center region on AWS as priced in March, 2019\. Currently, AWS
    charges for the use of the instances for each second the machine is up (that is, even
    though the price is shown as an hourly amount, a machine can be launched for 120
    seconds and the user would only need to pay the corresponding fraction of the
    hourly price):'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 每个优化目标都有其他系列，但在之前的列表中，我们列出了每个系列中最常用的一个。每个系列可能具有不同的配置。下表显示了C系列和M系列的一些配置。每个配置都有不同的价格。例如，在撰写本文时，AWS
    us-east-1区域第五代xlarge和C系列机器的价格为每小时0.085美元。正如您所看到的，在给定的价格水平上，用户可以选择支付具有更多内存和较少计算能力的配置，或者相反。下表中的**内存（GB）**列显示的是千兆字节的值，而vCPUs是虚拟机中的处理能力单位，由AWS测量。表中的价格只是参考价格，对应于2019年3月AWS弗吉尼亚数据中心区域的价格。目前，AWS按机器开启的每秒钟收费（即，尽管价格显示为每小时金额，但机器可以运行120秒，用户只需支付相应的小时价格的一部分）：
- en: '| **Model** | **vCPU** | **Memory (GB)** | **On-demand price (us-east-1 region)**
    |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **vCPU** | **内存（GB）** | **On-demand价格（us-east-1区域）** |'
- en: '| c5.large | 2 | 4 | $0.085 per hour |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| c5.large | 2 | 4 | $0.085 per hour |'
- en: '| c5.xlarge | 4 | 8 | $0.17 per hour |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| c5.xlarge | 4 | 8 | $0.17 per hour |'
- en: '| c5.2xlarge | 8 | 16 | $0.34 per hour |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| c5.2xlarge | 8 | 16 | $0.34 per hour |'
- en: '| c5.4xlarge | 16 | 32 | $0.68 per hour |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| c5.4xlarge | 16 | 32 | $0.68 per hour |'
- en: '| m5.large | 2 | 8 | $0.096 per hour |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| m5.large | 2 | 8 | $0.096 per hour |'
- en: '| m5.xlarge | 4 | 16 | $0.192 per hour |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| m5.xlarge | 4 | 16 | $0.192 per hour |'
- en: '| m5.2xlarge | 8 | 32 | $0.384 per hour |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| m5.2xlarge | 8 | 32 | $0.384 per hour |'
- en: 'The price for a given configuration can change due to a number of factors,
    namely, the following:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 给定配置的价格可能会因多种因素而变化，具体如下：
- en: The region (data center) of the machine
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器的区域（数据中心）
- en: Whether the instance is requested as spot or on-demand
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实例是否请求为spot或on-demand
- en: The use of reserved pricing
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预留定价
- en: On-demand versus spot instance pricing
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: On-demand与spot实例定价
- en: On-demand is the most flexible way to request machines from the cloud. Prices
    for on-demand instances are fixed and once you launch the machine, it is guaranteed
    to remain up (unless an error occurs or AWS is experimenting capacity issues,
    which is extremely rare). On the other hand, spot pricing is based on auctions.
    AWS has a set of excess capacity machines that are auctioned, typically at a lower
    price than on-demand. To obtain such machines, at launch time, the user needs
    to specify how much he or she is willing to spend on such an instance. If the
    current market price is below the bid value, the machine is successfully provisioned.
    As soon as the market price exceeds the bid, the machine can be taken away from
    the user. So, if you use spot pricing, you need to know that the machine can go
    down at any moment. That said, based on our experience, spot pricing can be reliably
    for large scale (thousands of machines) production workloads successfully. It
    is important to choose the bid price and machine configuration adequately and
    be ready to change these every so often upon the changes in the spot market prices.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: On-demand是请求云中机器最灵活的方式。on-demand实例的价格是固定的，一旦启动机器，就保证其持续运行（除非发生错误或AWS正在实验容量问题，这极为罕见）。另一方面，spot定价基于拍卖。AWS有一系列过剩容量机器，通常以低于on-demand的价格拍卖。为了获得这样的机器，在启动时，用户需要指定他或她愿意为这样的实例支付多少。如果当前市场价格低于出价价值，机器将成功配置。一旦市场价格超过出价，机器就可以从用户那里收回。因此，如果您使用spot定价，您需要知道机器可以在任何时候关闭。话虽如此，根据我们的经验，spot定价对于大规模（数千台机器）生产工作负载可以可靠地成功。重要的是要适当地选择出价价格和机器配置，并准备好根据spot市场价格的变化不时地更改这些配置。
- en: 'In the following link, you can inspect the market value of each instance type
    in different regions and availability zones (these are distinct isolated data
    centers within a region) at [https://console.aws.amazon.com/ec2sp/v1/spot/home?region=us-east-1#](https://console.aws.amazon.com/ec2sp/v1/spot/home?region=us-east-1#):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下链接中，您可以检查不同地区和可用区域（这些是区域内的独立隔离数据中心）中每种实例类型的市值：[https://console.aws.amazon.com/ec2sp/v1/spot/home?region=us-east-1#](https://console.aws.amazon.com/ec2sp/v1/spot/home?region=us-east-1#)
- en: '![](img/d122e1e5-53bc-4ae6-b538-87ff52d1dac2.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d122e1e5-53bc-4ae6-b538-87ff52d1dac2.png)'
- en: The preceding diagram shows the market price for **c5.4xlarge** machines between
    March and February, 2019\. The reader might observe that the region **us-east-1d**
    seems to have a lower market price than the rest of the regions. This means that
    whenever possible, you could request spot instances on that region at a lower
    bid price.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表显示了 2019 年 3 月至 2 月期间 **c5.4xlarge** 机器的市场价格。读者可能会注意到，区域 **us-east-1d**
    的市场价格似乎比其他地区低。这意味着，只要可能，您可以在该区域以较低出价请求 spot 实例。
- en: 'Currently, SageMaker does not support spot pricing, and only on-demand instances
    are allowed. Additionally, there is a different price chart for SageMaker-supported
    instances, which can be found via the following link: [https://aws.amazon.com/sagemaker/pricing/](https://aws.amazon.com/sagemaker/pricing/).
     There are different prices for the different things you can do with SakeMaker
    (notebooks, training jobs, batch transform jobs, endpoints, and so on.).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，SageMaker 不支持 spot 定价，只允许按需实例。此外，还有针对 SageMaker 支持的实例的不同价格表，您可以通过以下链接找到：[https://aws.amazon.com/sagemaker/pricing/](https://aws.amazon.com/sagemaker/pricing/)。SageMaker
    可以用于不同的事情（笔记、训练作业、批量转换作业、端点等），因此有不同的价格。
- en: As for **Elastic MapReduce** (**EMR**), it does support spot instances. However,
    there is a minor additional cost added to the raw instance type cost when launched
    through EMR.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 **弹性映射减少**（**EMR**），它确实支持 spot 实例。然而，当通过 EMR 启动时，会额外增加原始实例类型成本的一小部分。
- en: Reserved pricing
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预留定价
- en: Costs can be reduced if you have an accurate estimate of you compute needs ahead.
    In that case, you can pay AWS upfront and get significant discounts for on-demand
    instances. For example, if you plan to spend USD 1,000 on m5.xlarge machines over
    the course of a year, you can opt to pay upfront the USD 1,000 amount and obtain
    a 40% saving. The more you pay upfront, the larger the savings rate.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您提前准确估计了您的计算需求，可以降低成本。在这种情况下，您可以预先支付 AWS 并获得按需实例的显著折扣。例如，如果您计划在一年内为 m5.xlarge
    机器花费 1,000 美元，您可以选择预先支付 1,000 美元并获得 40% 的节省。您预付的越多，节省率就越高。
- en: Details can be found in the following link: [https://aws.amazon.com/ec2/pricing/reserved-instances/pricing/](https://aws.amazon.com/ec2/pricing/reserved-instances/pricing/).[ 
     ](https://aws.amazon.com/ec2/pricing/reserved-instances/pricing/)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 详细信息可以在以下链接中找到：[https://aws.amazon.com/ec2/pricing/reserved-instances/pricing/](https://aws.amazon.com/ec2/pricing/reserved-instances/pricing/)[ 
     ](https://aws.amazon.com/ec2/pricing/reserved-instances/pricing/)
- en: Amazon Machine Images (AMIs)
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 亚马逊机器镜像（AMIs）
- en: 'Machines can be launched outside EMR or SageMaker directly via the **Elastic
    Compute** service ([https://aws.amazon.com/ec2](https://aws.amazon.com/ec2)).
    This is useful when you want to handle the deployment of your own application
    on the AWS cloud or want to custom-configure the packages that you have available
    on the instance. When you launch an instance through EC2, you can select an AMI
    and the machine will come up with all the libraries and packages necessary for
    your application. You can create your own AMI from a running instance for re-use
    at a later time or through Docker specs. However, AWS provides several pre-backed
    AMIs that are very useful for deep learning. We highly encourage you to take a
    look at the available AMIs via this link: [https://aws.amazon.com/machine-learning/amis/](https://aws.amazon.com/machine-learning/amis/).
    These AMIs include the most common machine learning packages (such as TensorFlow,
    Anaconda, and scikit-learn) installed in a way that ensures compatibility between
    the different library versions (typically, a tricky task). These **Deep Learning
    AMIs** are typically referred to as **DLAMIs**.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 机器可以通过 **弹性计算** 服务（[https://aws.amazon.com/ec2](https://aws.amazon.com/ec2)）直接在
    EMR 或 SageMaker 外启动。当你想在 AWS 云上部署自己的应用程序或想自定义配置实例上可用的软件包时，这非常有用。通过 EC2 启动实例时，你可以选择一个
    AMI，机器将带有为你的应用程序所需的所有库和软件包。你可以从一个正在运行的实例创建自己的 AMI，以便稍后重用或通过 Docker 规范。然而，AWS 提供了几个预配置的
    AMI，这些 AMI 对于深度学习非常有用。我们强烈建议您通过此链接查看可用的 AMI：[https://aws.amazon.com/machine-learning/amis/](https://aws.amazon.com/machine-learning/amis/)。这些
    AMI 包括最常见的机器学习软件包（如 TensorFlow、Anaconda 和 scikit-learn），以确保不同库版本之间的兼容性（通常是一个棘手的问题）。这些
    **深度学习 AMI** 通常被称为 **DLAMIs**。
- en: Deep learning hardware
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习硬件
- en: Most of the instance types in AWS are based on CPUs. CPU instances are typically
    optimal for performing various sequential tasks. However, the accelerated computing
    instance types (for example, the P or G families) are based on **graphical processing
    units** (**GPUs**). These kinds of instances, which were originally popular on
    gaming consoles, turned out to be ideal for deep learning. GPUs are characterized
    by having more cores than CPUs, but with less processing power. Thus, GPUs are
    capable of fast parallel processing of simpler instructions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 中的大多数实例类型都是基于 CPU 的。CPU 实例通常适用于执行各种顺序任务。然而，加速计算实例类型（例如，P 或 G 系列）是基于 **图形处理单元**（**GPU**）。这些最初在游戏机上流行的实例类型，最终被证明非常适合深度学习。GPU
    的特点是拥有比 CPU 更多的核心，但处理能力较低。因此，GPU 能够快速并行处理简单的指令。
- en: In particular, GPUs allow for the very fast and parallel multiplication of matrices.
    Recall from [Chapter 7](c832a5c1-d877-4c90-bfb5-e3a0fe99d19a.xhtml), *Implementing
    Deep Learning Algorithms*, that deep learning involves multiplying the weights
    by the signals on different layer inputs, much like a vector dot-product. In fact,
    matrix multiplications involve doing several dot products between several columns
    and rows simultaneously. Matrix multiplication is usually the main bottleneck
    in deep learning, and GPUs are extremely good at performing such operations as
    there is an opportunity to perform tons of calculations in parallel.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是GPU允许进行非常快速和并行的矩阵乘法。回想一下 [第 7 章](c832a5c1-d877-4c90-bfb5-e3a0fe99d19a.xhtml)，*实现深度学习算法*，深度学习涉及将权重与不同层输入的信号相乘，就像向量点积一样。实际上，矩阵乘法涉及同时进行多个列和行的点积。矩阵乘法通常是深度学习中的主要瓶颈，而
    GPU 极擅长执行此类操作，因为有机会并行进行大量计算。
- en: 'In the following table, we can see typical machine configurations used for
    deep learning and their relevant characteristics. The number of GPUs and networking
    performance are especially important when it comes to distributing the deep learning
    workloads, as we will discuss in the following sections:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下表格中，我们可以看到用于深度学习的典型机器配置及其相关特性。当涉及到分配深度学习工作负载时，GPU 数量和网络性能尤其重要，我们将在以下章节中讨论：
- en: '| **Model** | **GPUs** | **vCPU** | **Mem (GiB)** | **GPU Mem (GiB)** | **Networking
    performance** |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **GPU** | **vCPU** | **Mem (GiB)** | **GPU Mem (GiB)** | **网络性能**
    |'
- en: '| p3.2xlarge | 1 | 8 | 61 | 16 | Up to 10 gigabits |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| p3.2xlarge | 1 | 8 | 61 | 16 | 最高 10 吉比特 |'
- en: '| p3.8xlarge | 4 | 32 | 244 | 64 | 10 gigabits |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| p3.8xlarge | 4 | 32 | 244 | 64 | 10吉比特 |'
- en: '| p3.16xlarge | 8 | 64 | 488 | 128 | 25 gigabits |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| p3.16xlarge | 8 | 64 | 488 | 128 | 25吉比特 |'
- en: Elastic Inference Acceleration
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 弹性推理加速
- en: In 2018, AWS announced a new feature that allows us to combine regular instances
    attached through GPU-based accelerator devices via a network at a fraction of
    the having a GPU instance. Details can be found at [https://docs.aws.amazon.com/sagemaker/latest/dg/ei.htm](https://docs.aws.amazon.com/sagemaker/latest/dg/ei.htm).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 2018 年，AWS 宣布了一个新功能，允许我们通过网络以 GPU 实例的一小部分成本将通过基于 GPU 的加速设备附加的常规实例组合起来。详细信息可以在
    [https://docs.aws.amazon.com/sagemaker/latest/dg/ei.htm](https://docs.aws.amazon.com/sagemaker/latest/dg/ei.htm)
    找到。
- en: Distributed deep learning
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式深度学习
- en: Let's explore the **distributed deep learning** concept next.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们接下来探索 **分布式深度学习** 概念。
- en: Model versus data parallelization
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型与数据并行化
- en: When are training large amounts of data, or when the network structure is huge,
    we usually need to distribute the training across different machines/threads so
    that learning can be performed in parallel. This parallelization may happen within
    a single machine with several GPUs or across several machines synchronizing through
    a network. The two main strategies for distributing deep learning workloads are
    data parallelization and model parallelization.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练大量数据或网络结构巨大时，我们通常需要在不同的机器/线程之间分发训练，以便可以并行执行学习。这种并行化可能发生在具有多个 GPU 的单台机器内，或者通过网络在不同机器之间同步。分发深度学习工作负载的两种主要策略是数据并行化和模型并行化。
- en: In data parallelization, we run a number of mini-batches in parallel using the
    same weights (that is, the same model). This implies synchronizing the weights
    of the different mini-batches upon a series of runs. One strategy for combining
    the weights of the different parallel runs is to average the weights resulting
    of each parallel mini-batch. An efficient way to average out the gradients of
    each machine or thread is to use algorithms such as **AllReduce** that allow combining
    the gradients in a distributed fashion without the need of a central combiner.
    Other alternatives involve hosting a parameter server that acts as a central location
    for synchronizing weights.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据并行化中，我们使用相同的权重（即相同的模型）并行运行多个小批量。这意味着在一系列运行中同步不同小批量的权重。合并不同并行运行权重的策略之一是将每个并行小批量产生的权重进行平均。使用允许以分布式方式合并梯度而不需要中央合并器的算法（如
    **AllReduce**）来平均每个机器或线程的梯度是一种有效的方法。其他替代方案包括托管一个参数服务器，该服务器充当同步权重的中央位置。
- en: Model parallelism, on the other hand, involves having different threads or machines
    processing the same mini-batch in parallel while distributing the actual processing.
    The algorithm being run needs to be able to distribute the work in different threads.
    This typically works well on machines with multiple GPUs that share a high-speed
    bus, because model parallelization typically only requires synchronizing the outputs
    of each layer after each forward pass. However, this synchronization might involve
    more or less data than the weights synchronization in data parallelism, depending
    on the structure of the network.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 模型并行化，另一方面，涉及不同的线程或机器在并行处理相同的 mini-batch 的同时，分发实际的处理。正在运行的算法需要能够在不同的线程中分发工作。这种方法通常在具有多个
    GPU 且共享高速总线的机器上运行良好，因为模型并行化通常只需要在每个前向传递后同步每个层的输出。然而，这种同步可能涉及的数据量可能比数据并行化中的权重同步多或少，这取决于网络的结构。
- en: Distributed TensorFlow
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式 TensorFlow
- en: '**TensorFlow** natively supports data parallelization on a single machine with
    more than one GPU, using **AllReduce**. The algorithms for distributing the learning
    through TensorFlow is an active area of development within TensorFlow.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**TensorFlow** 本地支持在具有多个 GPU 的单台机器上使用 **AllReduce** 进行数据并行化。TensorFlow 中通过
    TensorFlow 分发学习的算法是一个活跃的开发领域。'
- en: 'For example, we can launch a notebook instance with more than one GPU:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以启动一个具有多个 GPU 的笔记本实例：
- en: '![](img/4eb27f1a-8cd7-40a0-840d-5d9d68369e73.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4eb27f1a-8cd7-40a0-840d-5d9d68369e73.png)'
- en: In this example, we have a four-GPU machine. Let's examine how we would change
    the code to our regressor estimator that we considered in [Chapter 8](a05fc52e-bb4c-4200-b0c5-154dccaad739.xhtml),
    *Implementing Deep Learning with TensorFlow on AWS*. Recall we used `LinearRegressor`
    for solving our house value estimation. To enable the distributed learning across
    GPUs, we need to define a distribution strategy.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们有一个四 GPU 机器。让我们看看我们如何修改代码来适应我们在 [第 8 章](a05fc52e-bb4c-4200-b0c5-154dccaad739.xhtml)
    中考虑的回归估计器，*在 AWS 上使用 TensorFlow 实现深度学习*。回想一下，我们使用了 `LinearRegressor` 来解决我们的房屋价值估计问题。为了在
    GPU 之间启用分布式学习，我们需要定义一个分布策略。
- en: 'The simplest is `MirroredStrategy`, which uses the AllReduce technique. This
    strategy is instantiated and submitted to the regressor as an input, as we show
    in the following code block:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的是`MirroredStrategy`，它使用AllReduce技术。这种策略被实例化并作为输入提交给回归器，正如我们在下面的代码块中所示：
- en: '[PRE0]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Currently, the distribution strategy supports `GradientDescentOptimizer` that
    accepts a learning rate as input. Also, the way to provide the input functions
    needs to change slightly compared to what we did in [Chapter 8](a05fc52e-bb4c-4200-b0c5-154dccaad739.xhtml), *Implementing
    Deep Learning with TensorFlow on AWS*. In distributed processing, the input function
    needs to return `tf.Dataset` that we create from tensors obtained through the `pandas`
    `as_matrix()` function:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，分布策略支持接受学习率作为输入的`GradientDescentOptimizer`。此外，与我们在[第8章](a05fc52e-bb4c-4200-b0c5-154dccaad739.xhtml)，*在AWS上使用TensorFlow实现深度学习*中所做的方法相比，提供输入函数的方式需要稍作改变。在分布式处理中，输入函数需要返回我们通过`pandas`的`as_matrix()`函数获得的张量创建的`tf.Dataset`：
- en: '[PRE1]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The training is done in the same way as we did in [Chapter 8](a05fc52e-bb4c-4200-b0c5-154dccaad739.xhtml),
    *Implementing Deep Learning with TensorFlow on AWS*:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 训练方式与我们在[第8章](a05fc52e-bb4c-4200-b0c5-154dccaad739.xhtml)，*在AWS上使用TensorFlow实现深度学习*中做的一样：
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the `train_distributed_tensorflow.ipynb` notebook, you can see the full example.
    In this particular toy example, the distributed learning is not justifiable. However,
    it should serve the reader as a reference, as there is currently not much documentation
    and or many examples available regarding how to successfully perform the training
    on a multi-CPU environment.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在`train_distributed_tensorflow.ipynb`笔记本中，你可以看到完整的示例。在这个特定的玩具示例中，分布式学习并不合理。然而，它应该为读者提供一个参考，因为目前关于如何在多CPU环境中成功进行训练的文档和示例并不多。
- en: Distributed learning through Apache Spark
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过Apache Spark进行分布式学习
- en: In previous chapters, we showed how to use Apache Spark for distributed machine
    learning though the Spark ML library. However, if you want to combine Apache Spark
    with deep learning libraries such as TensorFlow, it is possible to obtain significant
    benefits.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们展示了如何通过Spark ML库使用Apache Spark进行分布式机器学习。然而，如果你想要将Apache Spark与TensorFlow等深度学习库结合使用，则可以获得显著的好处。
- en: Data parallelization
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据并行化
- en: In this scheme, the same mini-batches run in parallel throughout the Spark executors
    (in a map-like transformation) and the weights are averaged (in a reduce-like
    operation). Tools such as SparkFlow ([https://github.com/lifeomic/sparkflow](https://github.com/lifeomic/sparkflow))
    allow us to define a simple TensorFlow model (such as the one we developed in
    [Chapter 8](a05fc52e-bb4c-4200-b0c5-154dccaad739.xhtml), *Implementing Deep Learning
    with TensorFlow on AWS*) and perform parallel training by making the Spark driver
    act as a parameter server. Through this library, we can work with pipeline abstractions
    (estimators and transformers) that work as smart wrappers of TensorFlow graphs.
    Similarly, BigDL ([https://bigdl-project.github.io](https://bigdl-project.github.io/)) allows
    us to distribute deep learning training using `allreduce` **stochastic gradient
    descent** (**SGD**) implementations.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方案中，相同的迷你批次在Spark执行器中并行运行（类似于映射转换），权重被平均（类似于归约操作）。例如，SparkFlow ([https://github.com/lifeomic/sparkflow](https://github.com/lifeomic/sparkflow))这样的工具允许我们定义一个简单的TensorFlow模型（例如我们在[第8章](a05fc52e-bb4c-4200-b0c5-154dccaad739.xhtml)，*在AWS上使用TensorFlow实现深度学习*)中开发的模型）并通过让Spark驱动器充当参数服务器来执行并行训练。通过这个库，我们可以使用作为TensorFlow图智能包装器的管道抽象（估计器和转换器）。同样，BigDL
    ([https://bigdl-project.github.io](https://bigdl-project.github.io/))允许我们使用`allreduce`
    **随机梯度下降**（**SGD**）实现来分布深度学习训练。
- en: Model parallelization
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型并行化
- en: At the time of this chapter writing, there is no native library that allows
    us to do model parallelization with TensorFlow through Apache Spark. However,
    Apache Spark does come with an implementation of a **multilayer perceptron classifier**
    (**MLPC**) ( [https://spark.apache.org/docs/latest/ml-classification-regression.html#multilayer-perceptron-classifier](https://spark.apache.org/docs/latest/ml-classification-regression.html#multilayer-perceptron-classifier))
    that implements model parallelization through Apache Spark. This implementation
    is relatively simplistic compared to the power of libraries such as TensorFlow.
    For example, the network structure and the activation functions are fixed. You
    can only define the number of layers and a few other parameters. That said, it
    is a good way to get started with distributed deep learning, as your data pipelines
    are already in Spark.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本章时，没有本机库允许我们通过Apache Spark使用TensorFlow进行模型并行化。然而，Apache Spark确实提供了一个**多层感知器分类器**（**MLPC**）（[https://spark.apache.org/docs/latest/ml-classification-regression.html#multilayer-perceptron-classifier](https://spark.apache.org/docs/latest/ml-classification-regression.html#multilayer-perceptron-classifier)）的实现，该实现通过Apache
    Spark实现模型并行化。与TensorFlow等库的强大功能相比，这种实现相对简单。例如，网络结构和激活函数是固定的。你只能定义层数和少数其他参数。尽管如此，由于你的数据管道已经在Spark中，这是一个开始分布式深度学习的良好方式。
- en: Distributed hyperparameter tuning
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式超参数调整
- en: By having a Spark cluster, it is possible to train variants of the same neural
    network on different machines. Each of these variants could be different hyperparameters,
    or even slightly different network structures. For example, you might want to
    switch the activation functions on a particular layer. If we can predefine all
    these combinations of neural networks beforehand, a simple `map()` transformation
    can be performed through Spark. Each parallel training job can return the generated
    model, as well as the loss metric. Libraries such as `sparkdl` ([https://github.com/databricks/spark-deep-learning](https://github.com/databricks/spark-deep-learning))
    come with good tools for performing such tasks (especially if you're working with
    images). We'll cover hyperparameter tuning in more detail in [Chapter 15](691fc3d8-e4b8-4e3f-a8d9-e13f53f058c4.xhtml),
    *Tuning Clusters for Machine Learning*.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 通过拥有一个Spark集群，可以在不同的机器上训练同一神经网络的变体。这些变体可能具有不同的超参数，甚至稍微不同的网络结构。例如，你可能想要切换特定层的激活函数。如果我们事先预定义所有这些神经网络的组合，Spark可以通过简单的`map()`转换来执行。每个并行训练作业都可以返回生成的模型以及损失指标。例如，`sparkdl`库（[https://github.com/databricks/spark-deep-learning](https://github.com/databricks/spark-deep-learning)）提供了执行此类任务的良好工具（特别是如果你正在处理图像）。我们将在第15章[调整集群以进行机器学习](691fc3d8-e4b8-4e3f-a8d9-e13f53f058c4.xhtml)中更详细地介绍超参数调整。
- en: Distributed predictions at scale
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式大规模预测
- en: Once we have a serialized model, it is possible to make predictions in parallel
    by sending the model to the different executors and applying it to the data distributed
    by Spark. The `sparkdl` library, for example, implements a Keras transformer that
    makes distributed predictions, given a Keras model such as the one we developed
    in [Chapter 8](a05fc52e-bb4c-4200-b0c5-154dccaad739.xhtml), *Implementing Deep
    Learning with TensorFlow on AWS*.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了序列化的模型，就可以通过将模型发送到不同的执行器并将它应用于Spark分布式数据来并行地进行预测。例如，`sparkdl`库实现了Keras转换器，它可以在给定Keras模型（如我们在第8章[在AWS上使用TensorFlow实现深度学习](a05fc52e-bb4c-4200-b0c5-154dccaad739.xhtml)中开发的模型）的情况下进行分布式预测。
- en: Parallelization in SageMaker
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SageMaker中的并行化
- en: Many of the use cases identified in the previous section can also easily be
    addressed just by using **SageMaker**. With SageMaker, we can launch several instances
    performing parallel training variants of different models. Many of SageMaker's
    built-in algorithms are designed to perform model parallelization, which is why
    we usually specify the number (and type) of machines to be used for training.
    Additionally, it comes with advanced parameter-tuning capabilities that we'll
    explore in [Chapter 15](691fc3d8-e4b8-4e3f-a8d9-e13f53f058c4.xhtml), *Tuning Clusters
    for Machine Learning*. Lastly, the distributed predictions are done through batch
    transform jobs such as the ones we showed in [Chapter 4](af506fc8-f482-453e-8162-93a676b2e737.xhtml),
    *Predicting User Behavior with Tree-Based Methods*.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中确定的大多数用例也可以仅通过使用**SageMaker**轻松解决。使用SageMaker，我们可以启动多个实例，执行不同模型的并行训练变体。SageMaker的许多内置算法都是设计用来执行模型并行化的，这就是为什么我们通常指定用于训练的机器数量（和类型）。此外，它还提供了高级参数调整功能，我们将在[第15章](691fc3d8-e4b8-4e3f-a8d9-e13f53f058c4.xhtml)“调整集群以适应机器学习”中探讨。最后，分布式预测是通过批量转换作业完成的，例如我们在[第4章](af506fc8-f482-453e-8162-93a676b2e737.xhtml)“使用基于树的预测用户行为”中展示的。
- en: Summary
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered the basic considerations regarding how to choose
    the kinds of machines for the training clusters. These involve making tradeoffs
    between costs, memory sizes, compute power, and provisioning limitations. As for
    deep learning, we provided a concrete example on how to run distributed TensorFlow
    on SageMaker notebooks and some guidelines on how to further distribute your deep
    learning pipelines through Apache Spark on EMR. In the next chapter, *Optimizing
    Models in Spark and SageMaker*, we will dive into the problem of tuning our models
    for optimal performance from the standpoint of model accuracy.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了关于如何选择训练集群机器类型的基本考虑因素。这些考虑因素包括在成本、内存大小、计算能力和供应限制之间进行权衡。至于深度学习，我们提供了一个如何在SageMaker笔记本上运行分布式TensorFlow的具体示例，以及一些如何在EMR上通过Apache
    Spark进一步分布式你的深度学习管道的指南。在下一章“在Spark和SageMaker中优化模型”中，我们将深入探讨从模型准确性的角度调整我们的模型以实现最佳性能的问题。
