- en: Creating Clusters on AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the key problems in machine learning is understanding how to scale and
    parallelize the learning across multiple machines. Whether you are training deep
    learning models, which are very heavy on hardware usage, or just launching machines
    for creating predictions it is essential that we select the appropriate hardware
    configuration, both for const considerations and runtime performance reasons.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing your instance types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing your instance types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the [Chapter 4](af506fc8-f482-453e-8162-93a676b2e737.xhtml), *Predicting
    User Behavior with Tree-Based Methods,* and other chapters, we had to launch EMR
    clusters and SageMaker instances (servers) for learning and model serving. In
    this section, we discuss the characteristics of the different instance types.
    In this chapter, you can find all supported instance types AWS provides at [https://aws.amazon.com/ec2/instance-types/](.).
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the task at hand, we should use different instance types. For example,
    we may require an instance type with GPUs rather than CPUs for deep learning.
    When launching a large iterative **Extract, Transform, and Load** (**ETL**) job
    (that is, a data transformation job) on Apache Spark, we might need large amounts
    of memory. To make it easier for the users, AWS has classified the instances into
    families that are catered for different use cases. Additionally, AWS constantly
    provides newer hardware configurations for each family. These are called **generations**.
    Typically, a new generation provides improved performance over the previous generation.
    However, older generations are usually still available. In turn, each family has
    machines of different sizes in terms of compute and memory capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most commonly used families are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute optimized (C-family)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory optimized (M-family)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accelerated computing (P-family)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage optimized (I-family)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: General purpose (R-family)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are other families for each optimization objective, but in the previous
    list, we list the most commonly used family for each. Each family may have a different
    configuration. The following table shows a few configurations for the C and M
    families. Each configuration has a different price. For example, the fifth generation,
    xlarge, and C-family machine costs $0.085 at the time of this writing on the us-east-1
    region of AWS. As you can see, at a given price level, the user can choose to
    pay for a configuration that has more memory power and less compute power or vice
    versa. The **Memory (GB)** column in the following table shows values in gigabytes
    and the vCPUs are units of processing power in virtual machines, as measured by
    AWS. The prices shown in the table are just reference prices that correspond to
    the Virginia data center region on AWS as priced in March, 2019\. Currently, AWS
    charges for the use of the instances for each second the machine is up (that is, even
    though the price is shown as an hourly amount, a machine can be launched for 120
    seconds and the user would only need to pay the corresponding fraction of the
    hourly price):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **vCPU** | **Memory (GB)** | **On-demand price (us-east-1 region)**
    |'
  prefs: []
  type: TYPE_TB
- en: '| c5.large | 2 | 4 | $0.085 per hour |'
  prefs: []
  type: TYPE_TB
- en: '| c5.xlarge | 4 | 8 | $0.17 per hour |'
  prefs: []
  type: TYPE_TB
- en: '| c5.2xlarge | 8 | 16 | $0.34 per hour |'
  prefs: []
  type: TYPE_TB
- en: '| c5.4xlarge | 16 | 32 | $0.68 per hour |'
  prefs: []
  type: TYPE_TB
- en: '| m5.large | 2 | 8 | $0.096 per hour |'
  prefs: []
  type: TYPE_TB
- en: '| m5.xlarge | 4 | 16 | $0.192 per hour |'
  prefs: []
  type: TYPE_TB
- en: '| m5.2xlarge | 8 | 32 | $0.384 per hour |'
  prefs: []
  type: TYPE_TB
- en: 'The price for a given configuration can change due to a number of factors,
    namely, the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The region (data center) of the machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether the instance is requested as spot or on-demand
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The use of reserved pricing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On-demand versus spot instance pricing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: On-demand is the most flexible way to request machines from the cloud. Prices
    for on-demand instances are fixed and once you launch the machine, it is guaranteed
    to remain up (unless an error occurs or AWS is experimenting capacity issues,
    which is extremely rare). On the other hand, spot pricing is based on auctions.
    AWS has a set of excess capacity machines that are auctioned, typically at a lower
    price than on-demand. To obtain such machines, at launch time, the user needs
    to specify how much he or she is willing to spend on such an instance. If the
    current market price is below the bid value, the machine is successfully provisioned.
    As soon as the market price exceeds the bid, the machine can be taken away from
    the user. So, if you use spot pricing, you need to know that the machine can go
    down at any moment. That said, based on our experience, spot pricing can be reliably
    for large scale (thousands of machines) production workloads successfully. It
    is important to choose the bid price and machine configuration adequately and
    be ready to change these every so often upon the changes in the spot market prices.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following link, you can inspect the market value of each instance type
    in different regions and availability zones (these are distinct isolated data
    centers within a region) at [https://console.aws.amazon.com/ec2sp/v1/spot/home?region=us-east-1#](https://console.aws.amazon.com/ec2sp/v1/spot/home?region=us-east-1#):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d122e1e5-53bc-4ae6-b538-87ff52d1dac2.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram shows the market price for **c5.4xlarge** machines between
    March and February, 2019\. The reader might observe that the region **us-east-1d**
    seems to have a lower market price than the rest of the regions. This means that
    whenever possible, you could request spot instances on that region at a lower
    bid price.
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, SageMaker does not support spot pricing, and only on-demand instances
    are allowed. Additionally, there is a different price chart for SageMaker-supported
    instances, which can be found via the following link: [https://aws.amazon.com/sagemaker/pricing/](https://aws.amazon.com/sagemaker/pricing/).
     There are different prices for the different things you can do with SakeMaker
    (notebooks, training jobs, batch transform jobs, endpoints, and so on.).'
  prefs: []
  type: TYPE_NORMAL
- en: As for **Elastic MapReduce** (**EMR**), it does support spot instances. However,
    there is a minor additional cost added to the raw instance type cost when launched
    through EMR.
  prefs: []
  type: TYPE_NORMAL
- en: Reserved pricing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Costs can be reduced if you have an accurate estimate of you compute needs ahead.
    In that case, you can pay AWS upfront and get significant discounts for on-demand
    instances. For example, if you plan to spend USD 1,000 on m5.xlarge machines over
    the course of a year, you can opt to pay upfront the USD 1,000 amount and obtain
    a 40% saving. The more you pay upfront, the larger the savings rate.
  prefs: []
  type: TYPE_NORMAL
- en: Details can be found in the following link: [https://aws.amazon.com/ec2/pricing/reserved-instances/pricing/](https://aws.amazon.com/ec2/pricing/reserved-instances/pricing/).[ 
     ](https://aws.amazon.com/ec2/pricing/reserved-instances/pricing/)
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Machine Images (AMIs)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Machines can be launched outside EMR or SageMaker directly via the **Elastic
    Compute** service ([https://aws.amazon.com/ec2](https://aws.amazon.com/ec2)).
    This is useful when you want to handle the deployment of your own application
    on the AWS cloud or want to custom-configure the packages that you have available
    on the instance. When you launch an instance through EC2, you can select an AMI
    and the machine will come up with all the libraries and packages necessary for
    your application. You can create your own AMI from a running instance for re-use
    at a later time or through Docker specs. However, AWS provides several pre-backed
    AMIs that are very useful for deep learning. We highly encourage you to take a
    look at the available AMIs via this link: [https://aws.amazon.com/machine-learning/amis/](https://aws.amazon.com/machine-learning/amis/).
    These AMIs include the most common machine learning packages (such as TensorFlow,
    Anaconda, and scikit-learn) installed in a way that ensures compatibility between
    the different library versions (typically, a tricky task). These **Deep Learning
    AMIs** are typically referred to as **DLAMIs**.'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning hardware
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the instance types in AWS are based on CPUs. CPU instances are typically
    optimal for performing various sequential tasks. However, the accelerated computing
    instance types (for example, the P or G families) are based on **graphical processing
    units** (**GPUs**). These kinds of instances, which were originally popular on
    gaming consoles, turned out to be ideal for deep learning. GPUs are characterized
    by having more cores than CPUs, but with less processing power. Thus, GPUs are
    capable of fast parallel processing of simpler instructions.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, GPUs allow for the very fast and parallel multiplication of matrices.
    Recall from [Chapter 7](c832a5c1-d877-4c90-bfb5-e3a0fe99d19a.xhtml), *Implementing
    Deep Learning Algorithms*, that deep learning involves multiplying the weights
    by the signals on different layer inputs, much like a vector dot-product. In fact,
    matrix multiplications involve doing several dot products between several columns
    and rows simultaneously. Matrix multiplication is usually the main bottleneck
    in deep learning, and GPUs are extremely good at performing such operations as
    there is an opportunity to perform tons of calculations in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following table, we can see typical machine configurations used for
    deep learning and their relevant characteristics. The number of GPUs and networking
    performance are especially important when it comes to distributing the deep learning
    workloads, as we will discuss in the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **GPUs** | **vCPU** | **Mem (GiB)** | **GPU Mem (GiB)** | **Networking
    performance** |'
  prefs: []
  type: TYPE_TB
- en: '| p3.2xlarge | 1 | 8 | 61 | 16 | Up to 10 gigabits |'
  prefs: []
  type: TYPE_TB
- en: '| p3.8xlarge | 4 | 32 | 244 | 64 | 10 gigabits |'
  prefs: []
  type: TYPE_TB
- en: '| p3.16xlarge | 8 | 64 | 488 | 128 | 25 gigabits |'
  prefs: []
  type: TYPE_TB
- en: Elastic Inference Acceleration
  prefs: []
  type: TYPE_NORMAL
- en: In 2018, AWS announced a new feature that allows us to combine regular instances
    attached through GPU-based accelerator devices via a network at a fraction of
    the having a GPU instance. Details can be found at [https://docs.aws.amazon.com/sagemaker/latest/dg/ei.htm](https://docs.aws.amazon.com/sagemaker/latest/dg/ei.htm).
  prefs: []
  type: TYPE_NORMAL
- en: Distributed deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's explore the **distributed deep learning** concept next.
  prefs: []
  type: TYPE_NORMAL
- en: Model versus data parallelization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When are training large amounts of data, or when the network structure is huge,
    we usually need to distribute the training across different machines/threads so
    that learning can be performed in parallel. This parallelization may happen within
    a single machine with several GPUs or across several machines synchronizing through
    a network. The two main strategies for distributing deep learning workloads are
    data parallelization and model parallelization.
  prefs: []
  type: TYPE_NORMAL
- en: In data parallelization, we run a number of mini-batches in parallel using the
    same weights (that is, the same model). This implies synchronizing the weights
    of the different mini-batches upon a series of runs. One strategy for combining
    the weights of the different parallel runs is to average the weights resulting
    of each parallel mini-batch. An efficient way to average out the gradients of
    each machine or thread is to use algorithms such as **AllReduce** that allow combining
    the gradients in a distributed fashion without the need of a central combiner.
    Other alternatives involve hosting a parameter server that acts as a central location
    for synchronizing weights.
  prefs: []
  type: TYPE_NORMAL
- en: Model parallelism, on the other hand, involves having different threads or machines
    processing the same mini-batch in parallel while distributing the actual processing.
    The algorithm being run needs to be able to distribute the work in different threads.
    This typically works well on machines with multiple GPUs that share a high-speed
    bus, because model parallelization typically only requires synchronizing the outputs
    of each layer after each forward pass. However, this synchronization might involve
    more or less data than the weights synchronization in data parallelism, depending
    on the structure of the network.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**TensorFlow** natively supports data parallelization on a single machine with
    more than one GPU, using **AllReduce**. The algorithms for distributing the learning
    through TensorFlow is an active area of development within TensorFlow.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we can launch a notebook instance with more than one GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4eb27f1a-8cd7-40a0-840d-5d9d68369e73.png)'
  prefs: []
  type: TYPE_IMG
- en: In this example, we have a four-GPU machine. Let's examine how we would change
    the code to our regressor estimator that we considered in [Chapter 8](a05fc52e-bb4c-4200-b0c5-154dccaad739.xhtml),
    *Implementing Deep Learning with TensorFlow on AWS*. Recall we used `LinearRegressor`
    for solving our house value estimation. To enable the distributed learning across
    GPUs, we need to define a distribution strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest is `MirroredStrategy`, which uses the AllReduce technique. This
    strategy is instantiated and submitted to the regressor as an input, as we show
    in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Currently, the distribution strategy supports `GradientDescentOptimizer` that
    accepts a learning rate as input. Also, the way to provide the input functions
    needs to change slightly compared to what we did in [Chapter 8](a05fc52e-bb4c-4200-b0c5-154dccaad739.xhtml), *Implementing
    Deep Learning with TensorFlow on AWS*. In distributed processing, the input function
    needs to return `tf.Dataset` that we create from tensors obtained through the `pandas`
    `as_matrix()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The training is done in the same way as we did in [Chapter 8](a05fc52e-bb4c-4200-b0c5-154dccaad739.xhtml),
    *Implementing Deep Learning with TensorFlow on AWS*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In the `train_distributed_tensorflow.ipynb` notebook, you can see the full example.
    In this particular toy example, the distributed learning is not justifiable. However,
    it should serve the reader as a reference, as there is currently not much documentation
    and or many examples available regarding how to successfully perform the training
    on a multi-CPU environment.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed learning through Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, we showed how to use Apache Spark for distributed machine
    learning though the Spark ML library. However, if you want to combine Apache Spark
    with deep learning libraries such as TensorFlow, it is possible to obtain significant
    benefits.
  prefs: []
  type: TYPE_NORMAL
- en: Data parallelization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this scheme, the same mini-batches run in parallel throughout the Spark executors
    (in a map-like transformation) and the weights are averaged (in a reduce-like
    operation). Tools such as SparkFlow ([https://github.com/lifeomic/sparkflow](https://github.com/lifeomic/sparkflow))
    allow us to define a simple TensorFlow model (such as the one we developed in
    [Chapter 8](a05fc52e-bb4c-4200-b0c5-154dccaad739.xhtml), *Implementing Deep Learning
    with TensorFlow on AWS*) and perform parallel training by making the Spark driver
    act as a parameter server. Through this library, we can work with pipeline abstractions
    (estimators and transformers) that work as smart wrappers of TensorFlow graphs.
    Similarly, BigDL ([https://bigdl-project.github.io](https://bigdl-project.github.io/)) allows
    us to distribute deep learning training using `allreduce` **stochastic gradient
    descent** (**SGD**) implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Model parallelization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the time of this chapter writing, there is no native library that allows
    us to do model parallelization with TensorFlow through Apache Spark. However,
    Apache Spark does come with an implementation of a **multilayer perceptron classifier**
    (**MLPC**) ( [https://spark.apache.org/docs/latest/ml-classification-regression.html#multilayer-perceptron-classifier](https://spark.apache.org/docs/latest/ml-classification-regression.html#multilayer-perceptron-classifier))
    that implements model parallelization through Apache Spark. This implementation
    is relatively simplistic compared to the power of libraries such as TensorFlow.
    For example, the network structure and the activation functions are fixed. You
    can only define the number of layers and a few other parameters. That said, it
    is a good way to get started with distributed deep learning, as your data pipelines
    are already in Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed hyperparameter tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By having a Spark cluster, it is possible to train variants of the same neural
    network on different machines. Each of these variants could be different hyperparameters,
    or even slightly different network structures. For example, you might want to
    switch the activation functions on a particular layer. If we can predefine all
    these combinations of neural networks beforehand, a simple `map()` transformation
    can be performed through Spark. Each parallel training job can return the generated
    model, as well as the loss metric. Libraries such as `sparkdl` ([https://github.com/databricks/spark-deep-learning](https://github.com/databricks/spark-deep-learning))
    come with good tools for performing such tasks (especially if you're working with
    images). We'll cover hyperparameter tuning in more detail in [Chapter 15](691fc3d8-e4b8-4e3f-a8d9-e13f53f058c4.xhtml),
    *Tuning Clusters for Machine Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed predictions at scale
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we have a serialized model, it is possible to make predictions in parallel
    by sending the model to the different executors and applying it to the data distributed
    by Spark. The `sparkdl` library, for example, implements a Keras transformer that
    makes distributed predictions, given a Keras model such as the one we developed
    in [Chapter 8](a05fc52e-bb4c-4200-b0c5-154dccaad739.xhtml), *Implementing Deep
    Learning with TensorFlow on AWS*.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelization in SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many of the use cases identified in the previous section can also easily be
    addressed just by using **SageMaker**. With SageMaker, we can launch several instances
    performing parallel training variants of different models. Many of SageMaker's
    built-in algorithms are designed to perform model parallelization, which is why
    we usually specify the number (and type) of machines to be used for training.
    Additionally, it comes with advanced parameter-tuning capabilities that we'll
    explore in [Chapter 15](691fc3d8-e4b8-4e3f-a8d9-e13f53f058c4.xhtml), *Tuning Clusters
    for Machine Learning*. Lastly, the distributed predictions are done through batch
    transform jobs such as the ones we showed in [Chapter 4](af506fc8-f482-453e-8162-93a676b2e737.xhtml),
    *Predicting User Behavior with Tree-Based Methods*.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the basic considerations regarding how to choose
    the kinds of machines for the training clusters. These involve making tradeoffs
    between costs, memory sizes, compute power, and provisioning limitations. As for
    deep learning, we provided a concrete example on how to run distributed TensorFlow
    on SageMaker notebooks and some guidelines on how to further distribute your deep
    learning pipelines through Apache Spark on EMR. In the next chapter, *Optimizing
    Models in Spark and SageMaker*, we will dive into the problem of tuning our models
    for optimal performance from the standpoint of model accuracy.
  prefs: []
  type: TYPE_NORMAL
