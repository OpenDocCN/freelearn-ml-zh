<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch13" class="calibre6"/>Chapter 13. Tracking Visual Motion</h1></div></div></div><p class="calibre8">In this chapter, we will cover the following recipes:</p><div><ul class="itemizedlist"><li class="listitem">Tracing feature points in a video</li><li class="listitem">Estimating the optical flow</li><li class="listitem">Tracking an object in a video</li></ul></div><div><div><div><div><h1 class="title1"><a id="ch13lvl1sec76" class="calibre6"/>Introduction</h1></div></div></div><p class="calibre8">Video sequences are interesting because they show scenes and objects in motion. The preceding chapter introduced the tools for reading, processing, and saving videos. In this chapter, we will look at different algorithms that track the visible motion in a sequence of images. This visible or <strong class="calibre15">apparent motion</strong> can be caused by objects that move in different directions and at various speeds or by the motion of the camera (or a combination of both).</p><p class="calibre8">Tracking apparent motion is of utmost importance in many applications. It allows you to follow specific objects while they are moving in order to estimate their speed and determine where they are going. It also permits you to stabilize videos taken from handheld cameras by removing or reducing the amplitude of camera jitters. Motion estimation is also used in video coding to compress a video sequence in order to facilitate its transmission or storage. This chapter will present a few algorithms that track the motion in an image sequence, and as we will see, this tracking can be achieved either sparsely (that is, at few image locations, this is <strong class="calibre15">sparse motion</strong>) or densely (at every pixel of an image, this is <strong class="calibre15">dense motion</strong>).</p></div></div>
<div><div><div><div><h1 class="title1"><a id="ch13lvl1sec77" class="calibre6"/>Tracing feature points in a video</h1></div></div></div><p class="calibre8">We learned in previous chapters that analyzing an image through some of its most distinctive points can lead to effective and efficient computer vision algorithms. This is also true for image sequences in which the motion of some interest points can be used to understand how the different elements of a captured scene move. In this recipe, you will learn how to perform a temporal analysis of a sequence by tracking feature points as they move from frame to frame.</p><div><div><div><div><h2 class="title2"><a id="ch13lvl2sec231" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">To start the tracking process, the first thing to do is to detect the feature points in an initial frame. You then try to track these points in the next frame. Obviously, since we are dealing with a video sequence, there is a good chance that the object, on which the feature points are found, has moved (this motion can also be due to camera movement). Therefore, you must search around a point's previous location in order to find its new location in the next frame. This is what accomplishes the <code class="literal">cv::calcOpticalFlowPyrLK</code> function. You input two consecutive frames and a vector of feature points in the first image; the function then returns a vector of new point locations. To track the points over a complete sequence, you repeat this process from frame to frame. Note that as you follow the points across the sequence, you will unavoidably lose track of some of them such that the number of tracked feature points will gradually reduce. Therefore, it could be a good idea to detect new features from time to time.</p><p class="calibre8">We will now take advantage of the video-processing framework we defined in <a href="ch12.html" title="Chapter 12. Processing Video Sequences">
Chapter 12
</a>, <em class="calibre16">Processing Video Sequences</em>, and we will define a class that implements the <code class="literal">FrameProcessor</code> interface introduced in the <em class="calibre16">Processing the video frames</em> recipe of this chapter. The data attributes of this class include the variables that are required to perform both the detection of feature points and their tracking:</p><pre class="programlisting">    class FeatureTracker : public FrameProcessor { 
 
      cv::Mat gray;      // current gray-level image 
      cv::Mat gray_prev; // previous gray-level image 
      // tracked features from 0-&gt;1 
      std::vector&lt;cv::Point2f&gt; points[2]; 
      // initial position of tracked points 
      std::vector&lt;cv::Point2f&gt; initial; 
      std::vector&lt;cv::Point2f&gt; features;  // detected features 
      int max_count;               // maximum number of features to detect 
      double qlevel;               // quality level for feature detection 
      double minDist;              // min distance between two points 
      std::vector&lt;uchar&gt; status;   // status of tracked features 
      std::vector&lt;float&gt; err;      // error in tracking 
 
      public: 
 
        FeatureTracker() : max_count(500), qlevel(0.01), minDist(10.) {} 
</pre><p class="calibre8">Next, we define the <code class="literal">process</code> method that will be called for each frame of the sequence. Basically, we need to proceed as follows. First, the feature points are detected if necessary. Next, these points are tracked. You reject the points that you cannot track or you no longer want to track. You are now ready to handle the successfully tracked points. Finally, the current frame and its points become the previous frame and points for the next iteration. Here is how to do this:</p><pre class="programlisting">    void process(cv:: Mat &amp;frame, cv:: Mat &amp;output) { 
 
      // convert to gray-level image 
      cv::cvtColor(frame, gray, CV_BGR2GRAY);  
      frame.copyTo(output); 
 
      // 1. if new feature points must be added 
      if(addNewPoints()){ 
        // detect feature points 
        detectFeaturePoints(); 
        // add the detected features to  
        // the currently tracked features 
        points[0].insert(points[0].end(),  
                         features.begin(), features.end()); 
        initial.insert(initial.end(),  
                       features.begin(), features.end()); 
      } 
 
      // for first image of the sequence 
      if(gray_prev.empty()) 
        gray.copyTo(gray_prev); 
 
      // 2. track features 
      cv::calcOpticalFlowPyrLK( 
                gray_prev, gray, // 2 consecutive images 
                points[0],       // input point positions in first image 
                points[1],       // output point positions in the 2nd image 
                status,          // tracking success 
                err);            // tracking error 
 
      // 3. loop over the tracked points to reject some 
      int k=0; 
      for( int i= 0; i &lt; points[1].size(); i++ ) { 
 
        // do we keep this point? 
        if (acceptTrackedPoint(i)) { 
          // keep this point in vector 
          initial[k]= initial[i]; 
          points[1][k++] = points[1][i]; 
        } 
      } 
 
      // eliminate unsuccesful points 
      points[1].resize(k); 
      initial.resize(k); 
 
      // 4. handle the accepted tracked points 
      handleTrackedPoints(frame, output); 
 
      // 5. current points and image become previous ones 
      std::swap(points[1], points[0]); 
      cv::swap(gray_prev, gray); 
    } 
</pre><p class="calibre8">This method makes use of four utility methods. It should be easy for you to change any of these methods in order to define a new behavior for your own tracker. The first of these methods detects the feature points. Note that we have already discussed the <code class="literal">cv::goodFeatureToTrack</code> function in the first recipe of <a href="ch08.html" title="Chapter 8. Detecting Interest Points">
Chapter 8
</a>, <em class="calibre16">Detecting Interest Points</em>:</p><pre class="programlisting">    // feature point detection 
    void detectFeaturePoints() { 
 
      // detect the features 
      cv::goodFeaturesToTrack(gray,  // the image 
                        features,    // the output detected features 
                        max_count,   // the maximum number of features  
                        qlevel,      // quality level 
                        minDist);    // min distance between two features 
    } 
</pre><p class="calibre8">The second method determines whether new feature points should be detected. This will happen when a negligible number of tracked points remain:</p><pre class="programlisting">    // determine if new points should be added 
    bool addNewPoints() { 
 
      // if too few points 
      return points[0].size()&lt;=10; 
    } 
</pre><p class="calibre8">The third method rejects some of the tracked points based on a criteria defined by the application. Here, we decided to reject the points that do not move (in addition to those that cannot be tracked by the <code class="literal">cv::calcOpticalFlowPyrLK</code> function). We consider that non-moving points belong to the background scene and are therefore uninteresting:</p><pre class="programlisting">    //determine which tracked point should be accepted 
    bool acceptTrackedPoint(int i) { 
 
      return status[i] &amp;&amp;  //status is false if unable to track point i 
        // if point has moved 
        (abs(points[0][i].x-points[1][i].x)+ 
            (abs(points[0][i].y-points[1][i].y))&gt;2); 
    } 
</pre><p class="calibre8">Finally, the fourth method handles the tracked feature points by drawing all the tracked points with a line that joins them to their initial position (that is, the position where they were detected the first time) on the current frame:</p><pre class="programlisting">    // handle the currently tracked points 
    void handleTrackedPoints(cv:: Mat &amp;frame, cv:: Mat &amp;output) { 
 
      // for all tracked points 
      for (int i= 0; i &lt; points[1].size(); i++ ) { 
 
        // draw line and circle 
        cv::line(output, initial[i],  // initial position  
                 points[1][i],        // new position  
                 cv::Scalar(255,255,255)); 
        cv::circle(output, points[1][i], 3,       
                   cv::Scalar(255,255,255),-1); 
      } 
    } 
</pre><p class="calibre8">A simple main function to track the feature points in a video sequence would then be written as follows:</p><pre class="programlisting">    int main() { 
      // Create video procesor instance 
      VideoProcessor processor; 
  
      // Create feature tracker instance 
      FeatureTracker tracker; 
      // Open video file 
      processor.setInput("bike.avi"); 
 
      // set frame processor 
      processor.setFrameProcessor(&amp;tracker); 
 
      // Declare a window to display the video 
      processor.displayOutput("Tracked Features"); 
 
      // Play the video at the original frame rate 
      processor.setDelay(1000./processor.getFrameRate()); 
 
      // Start the process 
      processor.run(); 
    } 
</pre><p class="calibre8">The resulting program will show you the evolution of the moving tracked features over time. Here are, for example, two such frames at two different instants. In this video, the camera is fixed. The young cyclist is therefore the only moving object. Here is the result that is obtained after a few frames have been processed:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_13_001.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">A few seconds later, we obtain the following frame:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_13_002.jpg" class="calibre17"/></div><p class="calibre8">
</p></div><div><div><div><div><h2 class="title2"><a id="ch13lvl2sec232" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">To track the feature points from frame to frame, we must locate the new position of a feature point in the subsequent frame. If we assume that the intensity of the feature point does not change from one frame to the next one, we are looking for a displacement <code class="literal">(u,v)</code> as follows:</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/B05388_13_12.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Here, <code class="literal">I<sub class="calibre21">t</sub></code> and <code class="literal">I<sub class="calibre21">t+1</sub></code> are the current frame and the one at the next instant, respectively. This constant intensity assumption generally holds for small displacement in images that are taken at two nearby instants. We can then use the Taylor expansion in order to approximate this equation by an equation that involves the image derivatives:</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/B05388_13_13.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">This latter equation leads us to another equation (as a consequence of the constant intensity assumption that cancels the two intensity terms):</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/B05388_13_14.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">This constraint is the fundamental <strong class="calibre15">optical flow</strong> constraint equation and is known as the <strong class="calibre15">brightness constancy equation</strong>.</p><p class="calibre8">This constraint is exploited by the so-called <strong class="calibre15">Lukas-Kanade feature tracking</strong> algorithm. In addition to using this constraint, the Lukas-Kanade algorithm also makes an assumption that the displacement of all the points in the neighborhood of the feature point is the same. We can therefore impose the optical flow constraint on all these points with a unique <code class="literal">(u,v)</code> unknown displacement. This gives us more equations than the number of unknowns (two), and therefore, we can solve this system of equations in a mean-square sense. In practice, it is solved iteratively, and the OpenCV implementation also offers us the possibility to perform this estimation at a different resolution in order to make the search more efficient and more tolerant to a larger displacement. By default, the number of image levels is <code class="literal">3</code> and the window size is <code class="literal">15</code>. These parameters can obviously be changed. You can also specify the termination criteria, which define the conditions that stop the iterative search. The sixth parameter of <code class="literal">cv::calcOpticalFlowPyrLK</code> contains the residual mean-square error that can be used to assess the quality of the tracking. The fifth parameter contains binary flags that tell us whether tracking the corresponding point was considered successful or not.</p><p class="calibre8">The preceding description represents the basic principles behind the Lukas-Kanade tracker. The current implementation contains other optimizations and improvements that make the algorithm more efficient in the computation of the displacement of a large number of feature points.</p></div><div><div><div><div><h2 class="title2"><a id="ch13lvl2sec233" class="calibre6"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem"><a href="ch08.html" title="Chapter 8. Detecting Interest Points">
Chapter 8
</a>, <em class="calibre16">Detecting Interest Points</em>, where there is a discussion on feature point detection</li><li class="listitem">The <em class="calibre16">Tracking an object in a video</em> recipe of this chapter uses feature point tracking in order to track objects</li><li class="listitem">The classic article by <em class="calibre16">B. Lucas</em> and <em class="calibre16">T. Kanade</em>, <em class="calibre16">An Iterative Image Registration Technique with an Application to Stereo Vision</em>, at the <em class="calibre16">Int. Joint Conference in </em><em class="calibre16">Artificial Intelligence</em>, pp. 674-679, 1981, describes the original feature point tracking algorithm</li><li class="listitem">The article by J. Shi and C. Tomasi, <em class="calibre16">Good Features to Track</em>, at the <em class="calibre16">IEEE Conference on Computer Vision and Pattern Recognition</em>, pp. 593-600, 1994, describes an improved version of the original feature point tracking algorithm</li></ul></div></div></div>
<div><div><div><div><h1 class="title1"><a id="ch13lvl1sec78" class="calibre6"/>Estimating the optical flow</h1></div></div></div><p class="calibre8">When a scene is observed by a camera, the observed brightness pattern is projected on the image sensor and thus forms an image. In a video sequence, we are often interested in capturing the motion pattern, that is the projection of the 3D motion of the different scene elements on an image plane. This image of projected 3D motion vectors is called the <strong class="calibre15">motion field</strong>. However, it is not possible to directly measure the 3D motion of scene points from a camera sensor. All we observe is a brightness pattern that is in motion from frame to frame. This apparent motion of the brightness pattern is called the <strong class="calibre15">optical flow</strong>. One might think that the motion field and optical flow should be equal, but this is not always true. An obvious case would be the observation of a uniform object; for example, if a camera moves in front of a white wall, then no optical flow is generated. </p><p class="calibre8">Another classical example is the illusion produced by a rotating barber pole:</p><p class="calibre8">
</p><div><img alt="Estimating the optical flow" src="img/image_13_006.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">In this case, the motion field should show motion vectors in the horizontal direction as the vertical cylinder rotates around its main axis. However, observers perceive this motion as red and blue strips moving up and this is what the optical flow will show. In spite of these differences, the optical flow is considered to be a valid approximation of the motion field. This recipe will explain how the optical flow of an image sequence can be estimated.</p><div><div><div><div><h2 class="title2"><a id="ch13lvl2sec234" class="calibre6"/>Getting ready</h2></div></div></div><p class="calibre8">Estimating the optical flow means quantifying the apparent motion of the brightness pattern in an image sequence. So let's consider one frame of the video at one given instant. If we look at one particular pixel <code class="literal">(x,y)</code> on the current frame, we would like to know where this point is moving in the subsequent frames. That is to say that the coordinates of this point are moving over time-a fact that can be expressed as <code class="literal">(x(t),y(t))</code>-and our goal is to estimate the velocity of this point <code class="literal">(dx/dt,dy/dt)</code>. The brightness of this particular point at time <code class="literal">t</code> can be obtained by looking at the corresponding frame of the sequence, that is, <code class="literal">I(x(t),y(t),t)</code>.</p><p class="calibre8">From our <strong class="calibre15">image brightness constancy</strong> assumption, we can write that the brightness of this point does not vary with respect to time:</p><p class="calibre8">
</p><div><img alt="Getting ready" src="img/B05388_13_15.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The chain rule allows us to write the following:</p><p class="calibre8">
</p><div><img alt="Getting ready" src="img/B05388_13_16.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">This equation is known as the <strong class="calibre15">brightness constancy equation</strong> and it relates the optical flow components (the derivatives of <code class="literal">x</code> and <code class="literal">y</code> with respect to time) with the image derivatives. This is exactly the equation we derived in the previous recipe; we simply demonstrated it differently.</p><p class="calibre8">This single equation (composed of two unknowns) is however insufficient to compute the optical flow at a pixel location. We therefore need to add an additional constraint. A common choice is to assume the smoothness of the optical flow, which means that the neighboring optical flow vectors should be similar. Any departure from this assumption should therefore be penalized. One particular formulation for this constraint is based on the Laplacian of the optical flow:</p><p class="calibre8">
</p><div><img alt="Getting ready" src="img/equation.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The objective is therefore to find the optical flow field that minimizes both the deviations from the brightness constancy equation and the Laplacian of the flow vectors.</p></div><div><div><div><div><h2 class="title2"><a id="ch13lvl2sec235" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">Several approaches have been proposed to solve the dense optical flow estimation problem, and OpenCV implements a few of them. Let's use the <code class="literal">cv::DualTVL1OpticalFlow</code> class that is built as a subclass of the generic <code class="literal">cv::Algorithm</code> base class. Following the implemented pattern, the first thing to do is to create an instance of this class and obtain a pointer to it:</p><pre class="programlisting">    //Create the optical flow algorithm 
    cv::Ptr&lt;cv::DualTVL1OpticalFlow&gt; tvl1 = cv::createOptFlow_DualTVL1(); 
</pre><p class="calibre8">Since the object we just created is in a ready-to-use state, we simply call the method that calculates an optical flow field between the two frames:</p><pre class="programlisting">    cv::Mat oflow;   // image of 2D flow vectors 
    //compute optical flow between frame1 and frame2 
    tvl1-&gt;calc(frame1, frame2, oflow); 
</pre><p class="calibre8">The result is an image of 2D vectors (<code class="literal">cv::Point</code>) that represents the displacement of each pixel between the two frames. In order to display the result, we must therefore show these vectors. This is why we created a function that generates an image map for an optical flow field. To control the visibility of the vectors, we used two parameters. The first one is a stride value that is defined such that only one vector over a certain number of pixels will be displayed. This stride makes space for the display of the vectors. The second parameter is a scale factor that extends the vector length to make it more apparent. Each drawn optical flow vector is then a simple line that ends with a plain circle to symbolize the tip of an arrow. Our mapping function is therefore as follows:</p><pre class="programlisting">    // Drawing optical flow vectors on an image 
    void drawOpticalFlow(const cv::Mat&amp; oflow,  // the optical flow 
          cv::Mat&amp; flowImage,      // the produced image 
          int stride,              // the stride for displaying the vectors 
          float scale,             // multiplying factor for the vectors 
          const cv::Scalar&amp; color) // the color of the vectors 
    { 
      // create the image if required 
      if (flowImage.size() != oflow.size()) { 
        flowImage.create(oflow.size(), CV_8UC3); 
        flowImage = cv::Vec3i(255,255,255); 
      } 
 
      //for all vectors using stride as a step 
      for (int y = 0; y &lt; oflow.rows; y += stride) 
        for (int x = 0; x &lt; oflow.cols; x += stride) { 
          //gets the vector 
          cv::Point2f vector = oflow.at&lt; cv::Point2f&gt;(y, x); 
          // draw the line      
          cv::line(flowImage, cv::Point(x,y), 
                   cv::Point(static_cast&lt;int&gt;(x + scale*vector.x + 0.5),             
                             static_cast&lt;int&gt;(y + scale*vector.y + 0.5)),
                   color); 
          // draw the arrow tip 
          cv::circle(flowImage, 
                     cv::Point(static_cast&lt;int&gt;(x + scale*vector.x + 0.5),  
                               static_cast&lt;int&gt;(y + scale*vector.y + 0.5)),
                     1, color, -1); 
        } 
    } 
</pre><p class="calibre8">Consider the following two frames:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_13_010.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">If these frames are used, then the estimated optical flow field can be visualized by calling our drawing function:</p><pre class="programlisting">    // Draw the optical flow image 
    cv::Mat flowImage; 
    drawOpticalFlow(oflow,                // input flow vectors 
                    flowImage,            // image to be generated 
                    8,                    // display vectors every 8 pixels 
                    2,                    // multiply size of vectors by 2 
                    cv::Scalar(0, 0, 0)); // vector color 
</pre><p class="calibre8">The result is as follows:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_13_011.jpg" class="calibre17"/></div><p class="calibre8">
</p></div><div><div><div><div><h2 class="title2"><a id="ch13lvl2sec236" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">We explained in the first section of this recipe that an optical flow field can be estimated by minimizing a function that combines the brightness constancy constraint and a smoothness function. The equations we presented then constitute the classical formulation of the problem, and this one has been improved in many ways.</p><p class="calibre8">The method we used in the previous section is known as the <strong class="calibre15">Dual TV L1</strong> method. It has two main ingredients. The first one is the use of a smoothing constraint that aims at minimizing the absolute value of the optical flow gradient (instead of the square of it). This choice reduces the impact of the smoothing term, especially at regions of discontinuity where, for example, the optical flow vectors of a moving object are quite different from the ones of its background. The second ingredient is the use of a <strong class="calibre15">first-order Taylor approximation</strong>; this linearizes the formulation of the brightness constancy constraint. We will not enter into the details of this formulation here; it is suffice to say that this linearization facilitates the iterative estimation of the optical flow field. However, since the linear approximation is only valid for small displacements, the method requires a coarse-to-fine estimation scheme.</p><p class="calibre8">In this recipe, we used this method with its default parameters. A number of setters and getters methods allow you to modify the ones which can have an impact on the quality of the solution and on the speed of the computation. For example, one can modify the number of scales used in the pyramidal estimation or specify a more or less strict stopping criterion to be adopted during each iterative estimation step. Another important parameter is the weight associated with the brightness constancy constraint versus the smoothness constraint. For example, if we reduce the importance given to brightness constancy by two, we then obtain a smoother optical flow field:</p><pre class="programlisting">    // compute a smoother optical flow between 2 frames 
    tvl1-&gt;setLambda(0.075); 
    tvl1-&gt;calc(frame1, frame2, oflow); 
</pre><p class="calibre8">
</p><div><img alt="How it works..." src="img/image_13_012.jpg" class="calibre17"/></div><p class="calibre8">
</p></div><div><div><div><div><h2 class="title2"><a id="ch13lvl2sec237" class="calibre6"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem">The article by <em class="calibre16">B.K.P. Horn</em> and <em class="calibre16">B.G. Schunck</em>, <em class="calibre16">Determining optical flow, in Artificial Intelligence</em>, 1981, is the classical reference on optical flow estimation</li><li class="listitem">The article by <em class="calibre16">C. Zach</em>, <em class="calibre16">T. Pock</em>, and <em class="calibre16">H. Bischof</em>, <em class="calibre16">A duality based approach for real time tv-l 1 optical flow</em>, at <em class="calibre16">IEEE conference on Computer Vision and Pattern Recognition</em> 2007, describes the details of the <code class="literal">Dual TV-L1</code> method</li></ul></div></div></div>
<div><div><div><div><h1 class="title1"><a id="ch13lvl1sec79" class="calibre6"/>Tracking an object in a video</h1></div></div></div><p class="calibre8">In the previous two recipes, we learned how to track the motion of points and pixels in an image sequence. In many applications, however, the requirement is rather to track a specific moving object in a video. An object of interest is first identified and then it must be followed over a long sequence. This is challenging because as it evolves in the scene, the image of this object will undergo many changes in appearance due to viewpoint and illumination variations, non-rigid motion, occlusion, and so on.</p><p class="calibre8">This recipe presents some of the object-tracking algorithms implemented in the OpenCV library. These implementations are based on a common framework, which facilitates the substitution of one method by another. Contributors have also made available a number of new methods. Note that, we have already presented a solution to the object-tracking problem in the <em class="calibre16">Counting pixels with integral images</em> recipe in <a href="ch04.html" title="Chapter 4. Counting the Pixels with Histograms">
Chapter 4
</a>, <em class="calibre16">Counting the Pixels with Histograms</em>; this one was based on the use of histograms computed through integral images.</p><div><div><div><div><h2 class="title2"><a id="ch13lvl2sec238" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">The visual object-tracking problem generally assumes that no prior knowledge about the objects to be tracked is available. Tracking is therefore initiated by identifying the object in a frame, and tracking must start at this point. The initial identification of the object is achieved by specifying a bounding box inside which the target is inscribed. The objective of the tracker module is then to reidentify this object in a subsequent frame.</p><p class="calibre8">The <code class="literal">cv::Tracker</code> class of OpenCV that defines the object-tracking framework has therefore, two main methods. The first one is the <code class="literal">init</code> method used to define the initial target bounding box. The second one is the <code class="literal">update</code> method that outputs a new bounding box, given a new frame. Both the methods accept a frame (a <code class="literal">cv::Mat</code> instance) and a bounding box (a <code class="literal">cv::Rect2D</code> instance) as arguments; in one case, the bounding box is an input, while for the second method, the bounding box is an output parameter.</p><p class="calibre8">In order to test one of the proposed object tracker algorithms, we use the video-processing framework that has been presented in the previous chapter. In particular, we define a frame-processing subclass that will be called by our <code class="literal">VideoProcessor</code> class when each frame of the image sequence is received. This subclass has the following attributes:</p><pre class="programlisting">    class VisualTracker : public FrameProcessor { 
 
      cv::Ptr&lt;cv::Tracker&gt; tracker; 
      cv::Rect2d box; 
      bool reset; 
 
      public: 
      // constructor specifying the tracker to be used 
      VisualTracker(cv::Ptr&lt;cv::Tracker&gt; tracker) :   
                    reset(true), tracker(tracker) {} 
</pre><p class="calibre8">The <code class="literal">reset</code> attribute is set to <code class="literal">true</code> whenever the tracker has been reinitiated through the specification of a new target's bounding box. It is the <code class="literal">setBoundingBox</code> method that is used to store a new object position:</p><pre class="programlisting">   // set the bounding box to initiate tracking 
   void setBoundingBox(const cv::Rect2d&amp; bb) { 
      box = bb; 
      reset = true; 
   } 
</pre><p class="calibre8">The callback method used to process each frame then simply calls the appropriate method of the tracker and draws the new computed bounding box on the frame to be displayed:</p><pre class="programlisting">    // callback processing method 
    void process(cv:: Mat &amp;frame, cv:: Mat &amp;output) { 
 
      if (reset) { // new tracking session 
        reset = false; 
        tracker-&gt;init(frame, box); 
 
      } else { 
        // update the target's position 
        tracker-&gt;update(frame, box); 
      } 
 
      // draw bounding box on current frame 
      frame.copyTo(output); 
      cv::rectangle(output, box, cv::Scalar(255, 255, 255), 2); 
    } 
</pre><p class="calibre8">To demonstrate how an object can be tracked using the <code class="literal">VideoProcessor</code> and <code class="literal">FrameProcessor</code> instances, we use the <strong class="calibre15">Median Flow tracker</strong> defined in OpenCV:</p><pre class="programlisting">    int main(){ 
      // Create video procesor instance 
      VideoProcessor processor; 
 
      // generate the filename 
      std::vector&lt;std::string&gt; imgs; 
      std::string prefix = "goose/goose"; 
      std::string ext = ".bmp"; 
 
      // Add the image names to be used for tracking 
      for (long i = 130; i &lt; 317; i++) { 
 
        std::string name(prefix); 
        std::ostringstream ss; ss &lt;&lt; std::setfill('0') &lt;&lt;  
                 std::setw(3) &lt;&lt; i; name += ss.str(); 
        name += ext; 
        imgs.push_back(name); 
      } 
 
      // Create feature tracker instance 
      VisualTracker tracker(cv::TrackerMedianFlow::createTracker()); 
 
      // Open video file 
      processor.setInput(imgs); 
 
      // set frame processor 
      processor.setFrameProcessor(&amp;tracker); 
 
      // Declare a window to display the video 
      processor.displayOutput("Tracked object"); 
 
      // Define the frame rate for display 
      processor.setDelay(50); 
 
      // Specify the original target position 
      tracker.setBoundingBox(cv::Rect(290,100,65,40)); 
 
      // Start the tracking 
      processor.run(); 
    } 
</pre><p class="calibre8">The first bounding box identifies one goose in our test image sequence. This one is then automatically tracked in the subsequent frames:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_13_013.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Unfortunately, as the sequence progresses, the tracker will unavoidably make errors. The accumulation of these small errors will cause the tracker to slowly drift from the real target position. Here is, for example, the estimated position of our target after <code class="literal">130</code> frames have been processed:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_13_014.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Eventually, the tracker will lose track of the object. The ability of a tracker to follow an object over a long period of time is the most important criteria that characterizes the performance of an object tracker.</p></div><div><div><div><div><h2 class="title2"><a id="ch13lvl2sec239" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">In this recipe, we showed how the generic <code class="literal">cv::Tracker</code> class can be used to track an object in an image sequence. We selected the Median Flow tracker algorithm to illustrate the tracking result. This is a simple but effective method to track a textured object as long as its motion is not too rapid and it is not too severely occluded.</p><p class="calibre8">The Median Flow tracker is based on feature point tracking. It first starts by defining a grid of points over the object to be tracked. One could have instead detected interest points on the object using, for instance, the <code class="literal">FAST</code> operator presented in <a href="ch08.html" title="Chapter 8. Detecting Interest Points">
Chapter 8
</a>, <em class="calibre16">Detecting Interest Points</em>. However, using points at predefined locations presents a number of advantages. It saves time by avoiding the computation of interest points. It guarantees that a sufficient number of points will be available for tracking. It also makes sure that these points will be well distributed over the whole object. The Median Flow implementation uses, by default, a grid of <code class="literal">10x10</code> points:</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/image_13_015.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The next step is to use the Lukas-Kanade feature-tracking algorithm presented in the first recipe of this chapter, <em class="calibre16">Tracing feature points in a video</em>. Each point of the grid is then tracked over the next frame:</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/image_13_016.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The Median Flow algorithm then estimates the errors made when tracking these points. These errors can be estimated, for example, by computing the sum of absolute pixel difference in a window around the point at its initial and tracked position. This is the type of error that is conveniently computed and returned by the <code class="literal">cv::calcOpticalFlowPyrLK</code> function. Another error measure proposed by the Median Flow algorithm is to use the so-called forward-backward error. After the points have been tracked between a frame and the next one, these points at their new position are backward-tracked to check whether they will return to their original position in the initial image. The difference between the thus obtained forward-backward position and the initial one is the error in tracking.</p><p class="calibre8">Once the tracking error of each point has been computed, only 50 percent of the points having the smallest error are considered. This group is used to compute the new position of the bounding box in the next image. Each of these points votes for a displacement value, and the median of these possible displacements is retained. For the change in scale, the points are considered in pairs. The ratio of the distance between the two points in the initial frame and the next one is estimated. Again, it is the median of these scales that is finally applied.</p><p class="calibre8">The Median Tracker is one of many other visual object trackers based on feature point tracking. Another family of solutions is the one that is based on template matching, a concept we discussed in the <em class="calibre16">Matching local templates</em> recipe in <a href="ch09.html" title="Chapter 9. Describing and Matching Interest Points">
Chapter 9
</a>, <em class="calibre16">Describing and Matching Interest Points</em>. A good representative of these kinds of approaches is the <strong class="calibre15">Kernelized Correlation Filter </strong>(<strong class="calibre15">KCF</strong>) algorithm, implemented as the <code class="literal">cv::TrackerKCF</code> class in OpenCV:</p><pre class="programlisting">     VisualTracker tracker(cv::TrackerKCF::createTracker()); 
</pre><p class="calibre8">Basically, this one uses the target's bounding box as a template to search for the new object position in the next view. This is normally computed through a simple correlation, but KCF uses a special trick based on the Fourier transform that we briefly mentioned in the introduction of <a href="ch06.html" title="Chapter 6. Filtering the Images">
Chapter 6
</a>, <em class="calibre16">Filtering the Images</em>. Without entering into any details, the signal-processing theory tells us that correlating a template over an image corresponds to simple image multiplication in the frequency domain. This considerably speeds up the identification of the matching window in the next frame and makes KCF one of the fastest and robust trackers. As an example, here is the position of the bounding box after a tracking of <code class="literal">130</code> frames using KCF:</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/image_13_017.jpg" class="calibre17"/></div><p class="calibre8">
</p></div><div><div><div><div><h2 class="title2"><a id="ch13lvl2sec240" class="calibre6"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem">The article by <em class="calibre16">Z. Kalal</em>, <em class="calibre16">K. Mikolajczyk</em>, and <em class="calibre16">J. Matas</em>, <em class="calibre16">Forward-backward error: Automatic detection of tracking failures</em>, in <em class="calibre16">Int. Conf. on Pattern Recognition</em>, 2010, describes the Median Flow algorithm</li><li class="listitem">The article by <em class="calibre16">Z. Kalal</em>, <em class="calibre16">K. Mikolajczyk</em>, and <em class="calibre16">J. Matas</em>, <em class="calibre16">Tracking-learning-detection</em>, in <em class="calibre16">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, vol 34, no 7, 2012, is an advanced tracking method that uses the Median Flow algorithm</li><li class="listitem">The article by <em class="calibre16">J.F. Henriques</em>, <em class="calibre16">R. Caseiro</em>, <em class="calibre16">P. Martins</em>, <em class="calibre16">J. Batist</em>a, <em class="calibre16">High-Speed Tracking with Kernelized Correlation Filters</em>, in <em class="calibre16">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, vol 37, no 3, 2014, describes the KCF tracker algorithm</li></ul></div></div></div></body></html>