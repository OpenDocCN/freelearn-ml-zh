<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer065">
<h1 id="_idParaDest-44"><em class="italic"><a id="_idTextAnchor043"/>Chapter 4</em>: Encoding, Transforming, and Scaling Features</h1>
<p>The first three chapters of this book focused on data cleaning, exploration, and how to identify missing values and outliers. The next few chapters will delve heavily into feature engineering, starting, in this chapter, with techniques to encode, transform, and scale data to improve the performance of machine learning models.</p>
<p>Typically, machine learning algorithms require some form of encoding of variables. Additionally, our models often perform better with scaling so that features with higher variability do not overwhelm the optimization. We will show you how to use different scaling techniques when your features have dramatically different ranges. </p>
<p>Specifically, in this chapter, we will explore the following main topics:</p>
<ul>
<li>Creating training datasets and avoiding data leakage</li>
<li>Identifying irrelevant or redundant observations to be removed</li>
<li>Encoding categorical features</li>
<li>Encoding features with medium or high cardinality</li>
<li>Transforming features</li>
<li>Binning features</li>
<li>Scaling features</li>
</ul>
<h1 id="_idParaDest-45"><a id="_idTextAnchor044"/>Technical requirements</h1>
<p>In this chapter, we will work extensively with the <strong class="source-inline">feature-engine</strong> and <strong class="source-inline">category_encoders</strong> packages alongside the <strong class="source-inline">sklearn</strong> library. You can use <strong class="source-inline">pip</strong> to install these packages with <strong class="source-inline">pip install feature-engine</strong>, <strong class="source-inline">pip install</strong> <strong class="source-inline">category_encoders</strong>, and <strong class="source-inline">pip install scikit-learn</strong>. The code in this chapter uses version 0.24.2 of <strong class="source-inline">sklearn</strong>, version 1.1.2 of <strong class="source-inline">feature-engine</strong>, and version 2.2.2 of <strong class="source-inline">category_encoders</strong>. Note that either <strong class="source-inline">pip install feature-engine</strong> or <strong class="source-inline">pip install feature_engine</strong> will work.</p>
<p>All of the code for this chapter can be found on GitHub at <a href="https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning/tree/main/4.%20PruningEncodingandRescalingFeatures">https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning/tree/main/4.%20PruningEncodingandRescalingFeatures</a>.</p>
<h1 id="_idParaDest-46"><a id="_idTextAnchor045"/>Creating training datasets and avoiding data leakage</h1>
<p>One of the biggest threats to the performance of our models is data leakage. <strong class="bold">Data leakage</strong> occurs whenever our models are informed by data that is not in the training dataset. Sometimes, we<a id="_idIndexMarker310"/> inadvertently assist our model training with information that cannot be gleaned from the training data alone and end up with an overly rosy assessment of our model's accuracy.</p>
<p>Data scientists do not really intend for this to happen, hence the term <em class="italic">leakage</em>. This is not a <em class="italic">don't do it</em> kind of discussion. We all know not to do it. This is more of a <em class="italic">which steps should I take to avoid the problem?</em> discussion. It is actually quite easy to have some data leakage unless we develop routines to prevent it.</p>
<p>For example, if we have missing values for a feature, we might impute the mean across the whole dataset for those values. However, in order to validate our model, we subsequently split our data into training and testing datasets. We would then have accidentally introduced data leakage into our training dataset since the information from the full dataset (that is, the global mean) would have been used.</p>
<p>One of the practices that data scientists have adopted to avoid this is to establish separate training and<a id="_idIndexMarker311"/> testing datasets as close to the beginning of the analysis as possible. This can become a little more complicated with validation techniques such as cross-validation, but in the following chapters, we will go over how to avoid data leakage in a variety of situations.</p>
<p>We can use scikit-learn to create training and testing DataFrames for the National Longitudinal Survey of Youth data.</p>
<p class="callout-heading">Note</p>
<p class="callout">The <strong class="bold">National Longitudinal Survey </strong>(<strong class="bold">NLS</strong>) of <a id="_idIndexMarker312"/>Youth is conducted by the United States Bureau of Labor Statistics. This survey started with a cohort of individuals in 1997 who were born between 1980 and 1985, with annual follow-ups each year through to 2017. For this section, I pulled 89 variables on grades, employment, income, and attitudes toward the government from the hundreds of data items within the survey. Separate files for SPSS, Stata, and SAS can be downloaded from the repository. The NLS data can be downloaded for public use from <a href="https://www.nlsinfo.org/investigator/pages/search">https://www.nlsinfo.org/investigator/pages/search</a>.</p>
<p>Let's start<a id="_idIndexMarker313"/> creating the DataFrame:</p>
<ol>
<li>First, we import the <strong class="source-inline">train_test_split</strong> module from <strong class="source-inline">sklearn</strong> and load the NLS data:<p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">nls97 = pd.read_csv("data/nls97b.csv")</p><p class="source-code">nls97.set_index("personid", inplace=True)</p></li>
<li>Then, we can create<a id="_idIndexMarker314"/> training and testing DataFrames for the features (<strong class="source-inline">X_train</strong> and <strong class="source-inline">X_test</strong>) and the targets (<strong class="source-inline">y_train</strong> and <strong class="source-inline">y_test</strong>). In this example, <strong class="source-inline">wageincome</strong> is the target variable. We set the <strong class="source-inline">test_size</strong> parameter to <strong class="source-inline">0.3</strong> to leave 30% of the <a id="_idIndexMarker315"/>observations for testing. Note that we will only work with the <strong class="bold">Scholastic Assessment Test</strong> (<strong class="bold">SAT</strong>) and <strong class="bold">grade point average</strong> (<strong class="bold">GPA</strong>) data<a id="_idIndexMarker316"/> from the NLS:<p class="source-code">feature_cols = ['satverbal','satmath','gpascience',</p><p class="source-code">  'gpaenglish','gpamath','gpaoverall']</p><p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(nls97[feature_cols],\</p><p class="source-code">  nls97[['wageincome']], test_size=0.3, \</p><p class="source-code">  random_state=0)</p></li>
<li>Let's take a look at the <a id="_idIndexMarker317"/>training DataFrames created with <strong class="source-inline">train_test_split</strong>. We get the expected number of observations, 6,288, which is 70% of the total number of observations in the NLS DataFrame<a id="_idIndexMarker318"/> of 8,984:<p class="source-code">nls97.shape[0]</p><p class="source-code"><strong class="bold">8984</strong></p><p class="source-code">X_train.info()</p><p class="source-code"><strong class="bold">&lt;class 'pandas.core.frame.DataFrame'&gt;</strong></p><p class="source-code"><strong class="bold">Int64Index: 6288 entries, 574974 to 370933</strong></p><p class="source-code"><strong class="bold">Data columns (total 6 columns):</strong></p><p class="source-code"><strong class="bold"> #   Column        Non-Null Count     Dtype</strong></p><p class="source-code"><strong class="bold">---  ------        --------------   -------</strong></p><p class="source-code"><strong class="bold"> 0   satverbal      1001 non-null   float64</strong></p><p class="source-code"><strong class="bold"> 1   satmath      1001 non-null   float64</strong></p><p class="source-code"><strong class="bold"> 2   gpascience     3998 non-null   float64</strong></p><p class="source-code"><strong class="bold"> 3   gpaenglish     4078 non-null   float64</strong></p><p class="source-code"><strong class="bold"> 4   gpamath        4056 non-null   float64</strong></p><p class="source-code"><strong class="bold"> 5   gpaoverall     4223 non-null   float64</strong></p><p class="source-code"><strong class="bold">dtypes: float64(6)</strong></p><p class="source-code"><strong class="bold">memory usage: 343.9 KB</strong></p><p class="source-code">y_train.info()</p><p class="source-code"><strong class="bold">&lt;class 'pandas.core.frame.DataFrame'&gt;</strong></p><p class="source-code"><strong class="bold">Int64Index: 6288 entries, 574974 to 370933</strong></p><p class="source-code"><strong class="bold">Data columns (total 1 columns):</strong></p><p class="source-code"><strong class="bold"> #   Column        Non-Null Count    Dtype</strong></p><p class="source-code"><strong class="bold">---  ------        --------------  -------</strong></p><p class="source-code"><strong class="bold"> 0   wageincome    3599 non-null   float64</strong></p><p class="source-code"><strong class="bold">dtypes: float64(1)</strong></p><p class="source-code"><strong class="bold">memory usage: 98.2 KB</strong></p></li>
<li>Additionally, let's look at the<a id="_idIndexMarker319"/> testing DataFrames. We get 30% of the total<a id="_idIndexMarker320"/> number of observations, as expected:<p class="source-code">X_test.info()</p><p class="source-code"><strong class="bold">&lt;class 'pandas.core.frame.DataFrame'&gt;</strong></p><p class="source-code"><strong class="bold">Int64Index: 2696 entries, 363170 to 629736</strong></p><p class="source-code"><strong class="bold">Data columns (total 6 columns):</strong></p><p class="source-code"><strong class="bold"> #   Column        Non-Null Count    Dtype  </strong></p><p class="source-code"><strong class="bold">---  ------        --------------  -------  </strong></p><p class="source-code"><strong class="bold"> 0   satverbal      405 non-null   float64</strong></p><p class="source-code"><strong class="bold"> 1   satmath        406 non-null   float64</strong></p><p class="source-code"><strong class="bold"> 2   gpascience    1686 non-null   float64</strong></p><p class="source-code"><strong class="bold"> 3   gpaenglish    1720 non-null   float64</strong></p><p class="source-code"><strong class="bold"> 4   gpamath       1710 non-null   float64</strong></p><p class="source-code"><strong class="bold"> 5   gpaoverall    1781 non-null   float64</strong></p><p class="source-code"><strong class="bold">dtypes: float64(6)</strong></p><p class="source-code"><strong class="bold">memory usage: 147.4 KB</strong></p><p class="source-code">y_test.info()</p><p class="source-code"><strong class="bold">&lt;class 'pandas.core.frame.DataFrame'&gt;</strong></p><p class="source-code"><strong class="bold">Int64Index: 2696 entries, 363170 to 629736</strong></p><p class="source-code"><strong class="bold">Data columns (total 1 columns):</strong></p><p class="source-code"><strong class="bold"> #   Column          Non-Null Count    Dtype  </strong></p><p class="source-code"><strong class="bold">---  ------          --------------  -------  </strong></p><p class="source-code"><strong class="bold"> 0   wageincome      1492 non-null   float64</strong></p><p class="source-code"><strong class="bold">dtypes: float64(1)</strong></p><p class="source-code"><strong class="bold">memory usage: 42.1 KB</strong></p></li>
</ol>
<p>We will use scikit-learn's <strong class="source-inline">test_train_split</strong> to create separate training and testing DataFrames in the rest of this chapter. We will introduce more complicated strategies for constructing testing datasets for validation in <a href="B17978_06_ePub.xhtml#_idTextAnchor078"><em class="italic">Chapter 6</em></a>, <em class="italic">Preparing for Model Evaluation</em>.</p>
<p>Next, we begin our feature engineering work by removing features that are obviously unhelpful. This is because they have the same data as another feature or there is no variation in the responses.</p>
<h1 id="_idParaDest-47"><a id="_idTextAnchor046"/>Removing redundant or unhelpful features</h1>
<p>During the <a id="_idIndexMarker321"/>process of data cleaning and manipulation, we often end up with data that is no longer meaningful. Perhaps we subsetted data based on a single feature value, and we have retained that feature even though it now has the same value for all observations. Or, for the subset of the data that we are using, two features have the same value. Ideally, we catch those redundancies during our data cleaning. However, if we do not catch them during that process, we can use the open source <strong class="source-inline">feature-engine</strong> package to help us.</p>
<p>Additionally, there might be features that are so highly correlated that it is very unlikely that we could build a model that could use all of them effectively. <strong class="source-inline">feature-engine</strong> has a method, <strong class="source-inline">DropCorrelatedFeatures</strong>, that makes it easy to remove a feature when it is highly correlated with another feature.</p>
<p>In this section, we will work with land temperature data, along with the NLS data. Note that we will only load temperature data for Poland here.</p>
<p class="callout-heading">Data Note</p>
<p class="callout">The land temperature dataset contains the average temperature readings (in Celsius) in 2019 from over 12,000 stations across the world, though the majority of the stations are in the United States. The raw data was retrieved from the Global Historical Climatology Network integrated database. It has been made available for public use by the United States National Oceanic and Atmospheric Administration at <a href="https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-monthly-version-4">https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-monthly-version-4</a>.</p>
<p>Let's start<a id="_idIndexMarker322"/> removing redundant and unhelpful features:</p>
<ol>
<li value="1">Let's import the modules we need from <strong class="source-inline">feature_engine</strong> and <strong class="source-inline">sklearn</strong>, and load the NLS data and temperature data for Poland. The data from Poland was pulled from a larger dataset of 12,000 weather stations across the world. We use <strong class="source-inline">dropna</strong> to drop observations with any missing data:<p class="source-code">import pandas as pd</p><p class="source-code">import feature_engine.selection as fesel</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">nls97 = pd.read_csv("data/nls97b.csv")</p><p class="source-code">nls97.set_index("personid", inplace=True)</p><p class="source-code">ltpoland = pd.read_csv("data/ltpoland.csv")</p><p class="source-code">ltpoland.set_index("station", inplace=True)</p><p class="source-code">ltpoland.dropna(inplace=True)</p></li>
<li>Next, we create training and testing DataFrames, as we did in the previous section:<p class="source-code">feature_cols = ['satverbal','satmath','gpascience',</p><p class="source-code">  'gpaenglish','gpamath','gpaoverall']</p><p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(nls97[feature_cols],\</p><p class="source-code">  nls97[['wageincome']], test_size=0.3, \</p><p class="source-code">  random_state=0)</p></li>
<li>We can use the <a id="_idIndexMarker323"/>pandas <strong class="source-inline">corr</strong> method to see how these features are correlated:<p class="source-code">X_train.corr()</p><p class="source-code"><strong class="bold">          satverbal  satmath  gpascience  gpaenglish \</strong></p><p class="source-code"><strong class="bold">satverbal     1.000    0.729       0.439      0.444  </strong></p><p class="source-code"><strong class="bold">satmath       0.729    1.000       0.480      0.430</strong></p><p class="source-code"><strong class="bold">gpascience    0.439    0.480       1.000      0.672</strong></p><p class="source-code"><strong class="bold">gpaenglish    0.444    0.430       0.672      1.000</strong></p><p class="source-code"><strong class="bold">gpamath       0.375    0.518       0.606      0.600</strong></p><p class="source-code"><strong class="bold">gpaoverall    0.421    0.485       0.793      0.844</strong></p><p class="source-code"><strong class="bold">            gpamath   gpaoverall  </strong></p><p class="source-code"><strong class="bold">satverbal     0.375        0.421  </strong></p><p class="source-code"><strong class="bold">satmath       0.518        0.485  </strong></p><p class="source-code"><strong class="bold">gpascience    0.606        0.793  </strong></p><p class="source-code"><strong class="bold">gpaenglish    0.600        0.844  </strong></p><p class="source-code"><strong class="bold">gpamath       1.000        0.750  </strong></p><p class="source-code"><strong class="bold">gpaoverall    0.750        1.000  </strong></p></li>
</ol>
<p>Here, <strong class="source-inline">gpaoverall</strong> is highly correlated with <strong class="source-inline">gpascience</strong>, <strong class="source-inline">gpaenglish</strong>, and <strong class="source-inline">gpamath</strong>. The <strong class="source-inline">corr</strong> method returns the Pearson coefficients by default. This is fine when we can assume a linear relationship between the features. However, when this assumption does not make sense, we should consider requesting Spearman coefficients instead. We can do that by passing <strong class="source-inline">spearman</strong> to the method parameter of <strong class="source-inline">corr</strong>.</p>
<ol>
<li value="4">Let's drop features that have a correlation higher than 0.75 with another feature. We pass 0.75 to the <strong class="source-inline">threshold</strong> parameter of <strong class="source-inline">DropCorrelatedFeatures</strong>, indicating that we want to use Pearson coefficients and that we want to evaluate all the features by setting the variables to <strong class="source-inline">None</strong>. We use the <strong class="source-inline">fit</strong> method on the training data and then transform both the<a id="_idIndexMarker324"/> training and testing data. The <strong class="source-inline">info</strong> method shows that the resulting training DataFrame (<strong class="source-inline">X_train_tr</strong>) has all of the features except <strong class="source-inline">gpaoverall</strong>, which has correlations of 0.793 and 0.844 with <strong class="source-inline">gpascience</strong> and <strong class="source-inline">gpaenglish</strong>, respectively (<strong class="source-inline">DropCorrelatedFeatures</strong> will evaluate from left to right, so if <strong class="source-inline">gpamath</strong> and <strong class="source-inline">gpaoverall</strong> are highly correlated, it will drop <strong class="source-inline">gpaoverall</strong>. If <strong class="source-inline">gpaoverall</strong> had been to the left of <strong class="source-inline">gpamath</strong>, it would have dropped <strong class="source-inline">gpamath</strong>):<p class="source-code">tr = fesel.DropCorrelatedFeatures(variables=None, method='pearson', threshold=0.75)</p><p class="source-code">tr.fit(X_train)</p><p class="source-code">X_train_tr = tr.transform(X_train)</p><p class="source-code">X_test_tr = tr.transform(X_test)</p><p class="source-code">X_train_tr.info()</p><p class="source-code"><strong class="bold">&lt;class 'pandas.core.frame.DataFrame'&gt;</strong></p><p class="source-code"><strong class="bold">Int64Index: 6288 entries, 574974 to 370933</strong></p><p class="source-code"><strong class="bold">Data columns (total 5 columns):</strong></p><p class="source-code"><strong class="bold"> #   Column       Non-Null Count     Dtype  </strong></p><p class="source-code"><strong class="bold">---  ------        --------------  -------  </strong></p><p class="source-code"><strong class="bold"> 0   satverbal     1001 non-null   float64</strong></p><p class="source-code"><strong class="bold"> 1   satmath       1001 non-null   float64</strong></p><p class="source-code"><strong class="bold"> 2   gpascience    3998 non-null   float64</strong></p><p class="source-code"><strong class="bold"> 3   gpaenglish    4078 non-null   float64</strong></p><p class="source-code"><strong class="bold"> 4   gpamath       4056 non-null   float64</strong></p><p class="source-code"><strong class="bold">dtypes: float64(5)</strong></p><p class="source-code"><strong class="bold">memory usage: 294.8 KB</strong></p></li>
</ol>
<p>Typically, we would evaluate a feature more carefully before deciding to drop it. However, there are times when feature selection is part of a pipeline, and we need to automate the process. This can be done with <strong class="source-inline">DropCorrelatedFeatures</strong> since all of the <strong class="source-inline">feature_engine</strong> methods can be brought into a scikit-learn pipeline.</p>
<ol>
<li value="5">Now, let's create <a id="_idIndexMarker325"/>training and testing DataFrames from the land temperature data for Poland. The value of <strong class="source-inline">year</strong> is the same for all observations, as is the value for <strong class="source-inline">country</strong>. Additionally, the value for <strong class="source-inline">latabs</strong> is the same as it is for <strong class="source-inline">latitude</strong> for each observation:<p class="source-code">feature_cols = ['year','month','latabs', </p><p class="source-code">  'latitude','elevation', 'longitude','country']</p><p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(ltpoland[feature_cols],\</p><p class="source-code">  ltpoland[['temperature']], test_size=0.3, \</p><p class="source-code">  random_state=0)</p><p class="source-code"><strong class="bold">X_train.sample(5, random_state=99)</strong></p><p class="source-code"><strong class="bold">         year  month  latabs  latitude  elevation  longitude country</strong></p><p class="source-code"><strong class="bold">station  </strong></p><p class="source-code"><strong class="bold">SIEDLCE   2019   11    52    52    152    22    Poland</strong></p><p class="source-code"><strong class="bold">OKECIE    2019    6    52    52    110    21    Poland</strong></p><p class="source-code"><strong class="bold">BALICE    2019    1    50    50    241    20    Poland</strong></p><p class="source-code"><strong class="bold">BALICE    2019    7    50    50    241    20    Poland</strong></p><p class="source-code"><strong class="bold">BIALYSTOK 2019   11    53    53    151    23    Poland</strong></p><p class="source-code">X_train.year.value_counts()</p><p class="source-code"><strong class="bold">2019    84</strong></p><p class="source-code"><strong class="bold">Name: year, dtype: int64</strong></p><p class="source-code">X_train.country.value_counts()</p><p class="source-code"><strong class="bold">Poland    84</strong></p><p class="source-code"><strong class="bold">Name: country, dtype: int64</strong></p><p class="source-code">(X_train.latitude!=X_train.latabs).sum()</p><p class="source-code"><strong class="bold">0</strong></p></li>
<li>Let's drop features<a id="_idIndexMarker326"/> with the same values throughout the training dataset. Notice that <strong class="source-inline">year</strong> and <strong class="source-inline">country</strong> are removed after the transform:<p class="source-code">tr = fesel.DropConstantFeatures()</p><p class="source-code">tr.fit(X_train)</p><p class="source-code">X_train_tr = tr.transform(X_train)</p><p class="source-code">X_test_tr = tr.transform(X_test)</p><p class="source-code">X_train_tr.head()</p><p class="source-code"> </p><p class="source-code"><strong class="bold">         month  latabs  latitude  elevation  longitude</strong></p><p class="source-code"><strong class="bold">station                                                 </strong></p><p class="source-code"><strong class="bold">OKECIE       1      52        52        110         21</strong></p><p class="source-code"><strong class="bold">LAWICA       8      52        52         94         17</strong></p><p class="source-code"><strong class="bold">LEBA        11      55        55          2         18</strong></p><p class="source-code"><strong class="bold">SIEDLCE     10      52        52        152         22</strong></p><p class="source-code"><strong class="bold">BIALYSTOK   11      53        53        151         23</strong></p></li>
<li>Let's drop features<a id="_idIndexMarker327"/> that have the same values as other features. In this case, the transform drops <strong class="source-inline">latitude</strong>, which has the same values as <strong class="source-inline">latabs</strong>:<p class="source-code">tr = fesel.DropDuplicateFeatures()</p><p class="source-code">tr.fit(X_train_tr)</p><p class="source-code">X_train_tr = tr.transform(X_train_tr)</p><p class="source-code">X_train_tr.head()</p><p class="source-code"><strong class="bold">           month   latabs   elevation   longitude</strong></p><p class="source-code"><strong class="bold">station                                       </strong></p><p class="source-code"><strong class="bold">OKECIE         1       52         110          21</strong></p><p class="source-code"><strong class="bold">LAWICA         8       52          94          17</strong></p><p class="source-code"><strong class="bold">LEBA          11       55           2          18</strong></p><p class="source-code"><strong class="bold">SIEDLCE       10       52         152          22</strong></p><p class="source-code"><strong class="bold">BIALYSTOK     11       53         151          23</strong></p></li>
</ol>
<p>This fixes some obvious problems with our features in the NLS data and the land temperature data for Poland. We dropped <strong class="source-inline">gpaoverall</strong> from a DataFrame that has the other GPA features because it is highly correlated with them. Additionally, we removed redundant data, dropping features with the same value throughout the DataFrame and features that duplicate the values of another feature.</p>
<p>The rest of this chapter explores somewhat messier feature engineering challenges: encoding, transforming, binning, and scaling.</p>
<h1 id="_idParaDest-48"><a id="_idTextAnchor047"/>Encoding categorical features</h1>
<p>There are several reasons<a id="_idIndexMarker328"/> why we might need to encode features before using them in most machine learning algorithms. First, these algorithms typically require numeric data. Second, when a categorical feature <em class="italic">is</em> represented with numbers, for example, 1 for female and 2 for male, we need to encode the values so that they are recognized as categorical. Third, the feature might actually be ordinal, with a discrete number of values that represent some meaningful ranking. Our models need to capture that ranking. Finally, a categorical feature might have a large number of values (known as high cardinality), and we might want our encoding to collapse categories. </p>
<p>We can handle the encoding of features with a limited number of values, say 15 or less, with one-hot encoding. In this section, we will, first, go over one-hot encoding and then discuss ordinal encoding. We will look at strategies for handling categorical features with high cardinality in the next section.</p>
<h2 id="_idParaDest-49"><a id="_idTextAnchor048"/>One-hot encoding</h2>
<p>One-hot encoding<a id="_idIndexMarker329"/> a feature creates a binary vector for each<a id="_idIndexMarker330"/> value of that feature. So, if a feature, called <em class="italic">letter</em>, has three unique values, <em class="italic">A</em>, <em class="italic">B</em>, and <em class="italic">C</em>, one-hot encoding creates three binary vectors to represent those values. The first binary vector, which we can call <em class="italic">letter_A,</em> has 1 whenever <em class="italic">letter</em> has a value of <em class="italic">A</em>, and 0 when it is <em class="italic">B</em> or <em class="italic">C</em>. <em class="italic">letter_B</em> and <em class="italic">letter_C</em> would be coded similarly. The transformed features, <em class="italic">letter_A</em>, <em class="italic">letter_B</em>, and <em class="italic">letter_C</em>, are often referred <a id="_idIndexMarker331"/>to as <strong class="bold">dummy variables</strong>. <em class="italic">Figure 4.1</em> illustrates one-hot encoding:</p>
<div>
<div class="IMG---Figure" id="_idContainer035">
<img alt="Figure 4.1 – The one-hot encoding of a categorical feature " height="232" src="image/Figure_1.1_B17978.jpg" width="810"/>
</div>
</div>
<p class="figure-caption">Figure 4.1 – The one-hot encoding of a categorical feature</p>
<p>A number of features from the NLS data are appropriate for one-hot encoding. In the following code blocks, we <a id="_idIndexMarker332"/>encode some of those <a id="_idIndexMarker333"/>features:</p>
<ol>
<li value="1">Let's start by importing the <strong class="source-inline">OneHotEncoder</strong> module from <strong class="source-inline">feature_engine</strong> and loading the data. Additionally, we import the <strong class="source-inline">OrdinalEncoder</strong> module from scikit-learn since we will use it later:<p class="source-code">import pandas as pd</p><p class="source-code">from feature_engine.encoding import OneHotEncoder</p><p class="source-code">from sklearn.preprocessing import OrdinalEncoder</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">nls97 = pd.read_csv("data/nls97b.csv")</p><p class="source-code">nls97.set_index("personid", inplace=True)</p></li>
<li>Next, we create training and testing DataFrames for the NLS data:<p class="source-code">feature_cols =['gender','maritalstatus','colenroct99']</p><p class="source-code">nls97demo = nls97[['wageincome'] + feature_cols].dropna()</p><p class="source-code">X_demo_train, X_demo_test, y_demo_train, y_demo_test=\</p><p class="source-code">  train_test_split(nls97demo[feature_cols],\</p><p class="source-code">  nls97demo[['wageincome']], test_size=0.3, \</p><p class="source-code">  random_state=0)</p></li>
<li>One option we have for the encoding is the pandas <strong class="source-inline">get_dummies</strong> method. We can use it to indicate that we want to convert the <strong class="source-inline">gender</strong> and <strong class="source-inline">maritalstatus</strong> features. <strong class="source-inline">get_dummies</strong> gives us a dummy variable for each value of <strong class="source-inline">gender</strong> and <strong class="source-inline">maritalstatus</strong>. For example, <strong class="source-inline">gender</strong> has the values of <strong class="source-inline">Female</strong> and <strong class="source-inline">Male</strong>. <strong class="source-inline">get_dummies</strong> creates a feature, <strong class="source-inline">gender_Female</strong>, which is 1 when <strong class="source-inline">gender</strong> is <strong class="source-inline">Female</strong> and 0 when <strong class="source-inline">gender</strong> is <strong class="source-inline">Male</strong>. When <strong class="source-inline">gender</strong> is <strong class="source-inline">Male</strong>, <strong class="source-inline">gender_Male</strong> is 1 and <strong class="source-inline">gender_Female</strong> is 0. This is a tried-and-true method of doing this type of encoding and has served statisticians <a id="_idIndexMarker334"/>well for <a id="_idIndexMarker335"/>many years:<p class="source-code">pd.get_dummies(X_demo_train, \</p><p class="source-code">  columns=['gender','maritalstatus']).head(2).T</p><p class="source-code"><strong class="bold">personid                    736081          832734</strong></p><p class="source-code"><strong class="bold">colenroct99                 1.Not enrolled  1.Not enrolled</strong></p><p class="source-code"><strong class="bold">gender_Female               1               0</strong></p><p class="source-code"><strong class="bold">gender_Male                 0               1</strong></p><p class="source-code"><strong class="bold">maritalstatus_Divorced      0               0</strong></p><p class="source-code"><strong class="bold">maritalstatus_Married       1               0</strong></p><p class="source-code"><strong class="bold">maritalstatus_Never-married 0               1</strong></p><p class="source-code"><strong class="bold">maritalstatus_Separated     0               0</strong></p><p class="source-code"><strong class="bold">maritalstatus_Widowed       0               0</strong></p></li>
</ol>
<p>We are not saving the DataFrame created by <strong class="source-inline">get_dummies</strong> because, later in this section, we will be using a different technique to do the encoding.</p>
<p>Typically, we create <em class="italic">k-1</em> dummy variables for <em class="italic">k</em> unique values for a feature. So, if <strong class="source-inline">gender</strong> has two values in our data, we only need to create one dummy variable. If we know the value for <strong class="source-inline">gender_Female</strong>, we also know the value of <strong class="source-inline">gender_Male</strong>; therefore, the latter variable is redundant. Similarly, we know the value of <strong class="source-inline">maritalstatus_Divorced</strong> if we know the values of the other <strong class="source-inline">maritalstatus</strong> dummies. Creating a redundancy in this way is inelegantly referred to as <a id="_idIndexMarker336"/>the <strong class="bold">dummy variable trap</strong>. To avoid this problem, we drop one dummy from each group.</p>
<p class="callout-heading">Note</p>
<p class="callout">For some machine learning algorithms, such as linear regression, dropping one dummy variable is actually required. In estimating the parameters of a linear model, the matrix is inverted. If our model has an intercept, and all dummy variables are included, the matrix cannot be inverted.</p>
<ol>
<li value="4">We can set <a id="_idIndexMarker337"/>the <strong class="source-inline">get_dummies</strong> <strong class="source-inline">drop_first</strong> parameter <a id="_idIndexMarker338"/>to <strong class="source-inline">True</strong> to drop the first dummy from each group:<p class="source-code">pd.get_dummies(X_demo_train, \</p><p class="source-code">  columns=['gender','maritalstatus'],</p><p class="source-code">  drop_first=True).head(2).T</p><p class="source-code"><strong class="bold">personid                      736081          832734</strong></p><p class="source-code"><strong class="bold">colenroct99                  1. Not enrolled  1. Not enrolled</strong></p><p class="source-code"><strong class="bold">gender_Male                  0                1</strong></p><p class="source-code"><strong class="bold">maritalstatus_Married        1                0</strong></p><p class="source-code"><strong class="bold">maritalstatus_Never-married  0                1</strong></p><p class="source-code"><strong class="bold">maritalstatus_Separated      0                0</strong></p><p class="source-code"><strong class="bold">maritalstatus_Widowed        0                0</strong></p></li>
</ol>
<p>An alternative to <strong class="source-inline">get_dummies</strong> is the one-hot encoder in either <strong class="source-inline">sklearn</strong> or <strong class="source-inline">feature_engine</strong>. These one-hot encoders have the advantage that they can be easily brought into a machine learning pipeline, and they can persist information gathered from the training dataset to the testing dataset.</p>
<ol>
<li value="5">Let's use the <strong class="source-inline">OneHotEncoder</strong> module from <strong class="source-inline">feature_engine</strong> to do the encoding. We set <strong class="source-inline">drop_last</strong> to <strong class="source-inline">True</strong> to drop one of the dummies from each group. We fit the encoding to the training data and then transform both the training and testing data:<p class="source-code">ohe = OneHotEncoder(drop_last=True,</p><p class="source-code">  variables=['gender','maritalstatus'])</p><p class="source-code">ohe.fit(X_demo_train)</p><p class="source-code">X_demo_train_ohe = ohe.transform(X_demo_train)</p><p class="source-code">X_demo_test_ohe = ohe.transform(X_demo_test)</p><p class="source-code">X_demo_train_ohe.filter(regex='gen|mar', axis="columns").head(2).T</p><p class="source-code"><strong class="bold">personid                     736081          832734</strong></p><p class="source-code"><strong class="bold">gender_Female                1               0</strong></p><p class="source-code"><strong class="bold">maritalstatus_Married        1               0</strong></p><p class="source-code"><strong class="bold">maritalstatus_Never-married  0               1</strong></p><p class="source-code"><strong class="bold">maritalstatus_Divorced       0               0</strong></p><p class="source-code"><strong class="bold">maritalstatus_Separated      0               0</strong></p></li>
</ol>
<p>This demonstrates<a id="_idIndexMarker339"/> that one-hot encoding is a fairly <a id="_idIndexMarker340"/>straightforward way to prepare nominal data for a machine learning algorithm. But what if our categorical features are ordinal, rather than nominal? In that case, we need to use ordinal encoding.</p>
<h2 id="_idParaDest-50"><a id="_idTextAnchor049"/>Ordinal encoding </h2>
<p>Categorical features can be either <a id="_idIndexMarker341"/>nominal or ordinal, as discussed in <a href="B17978_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Examining the Distribution of Features and Targets</em>. Gender <a id="_idIndexMarker342"/>and marital status are nominal. Their values do not imply order. For example, "never married" is not a higher value than "divorced." </p>
<p>However, when a categorical feature is ordinal, we want the encoding to capture the ranking of the values. For example, if we have a feature that has the values of low, medium, and high, one-hot encoding would lose this ordering. Instead, a transformed feature with the values of 1, 2, and 3 for low, medium, and high, respectively, would be better. We can accomplish this with ordinal encoding.</p>
<p>The college enrollment feature on the NLS dataset can be considered an ordinal feature. The values range from <em class="italic">1. Not enrolled</em> to <em class="italic">3. 4-year college</em>. We should use ordinal encoding to prepare it for modeling. We will do that next:</p>
<ol>
<li value="1">We can use the <strong class="source-inline">OrdinalEncoder</strong> module of <strong class="source-inline">sklearn</strong> to encode the college enrollment for 1999 feature. First, let's take a look at the values of <strong class="source-inline">colenroct99</strong> prior <a id="_idIndexMarker343"/>to encoding. The values<a id="_idIndexMarker344"/> are strings, but there is an implied order:<p class="source-code">X_demo_train.colenroct99.unique()</p><p class="source-code"><strong class="bold">array(['1. Not enrolled', '2. 2-year college ', </strong></p><p class="source-code"><strong class="bold">       '3. 4-year college'], dtype=object)</strong></p><p class="source-code">X_demo_train.head()</p><p class="source-code"><strong class="bold">            gender   maritalstatus   colenroct99</strong></p><p class="source-code"><strong class="bold">personid                                           </strong></p><p class="source-code"><strong class="bold">736081      Female   Married         1. Not enrolled</strong></p><p class="source-code"><strong class="bold">832734      Male     Never-married   1. Not enrolled</strong></p><p class="source-code"><strong class="bold">453537      Male     Married         1. Not enrolled</strong></p><p class="source-code"><strong class="bold">322059      Female   Divorced        1. Not enrolled</strong></p><p class="source-code"><strong class="bold">324323      Female   Married         2. 2-year college</strong></p></li>
<li>We can tell the <strong class="source-inline">OrdinalEncoder</strong> module to rank the values in the same order by passing the preceding array into the <strong class="source-inline">categories</strong> parameter. Then, we can use <strong class="source-inline">fit_transform</strong> to transform the college enrollment field, <strong class="source-inline">colenroct99</strong>. (The <strong class="source-inline">fit_transform</strong> method of the <strong class="source-inline">sklearn</strong> <strong class="source-inline">OrdinalEncoder</strong> module returns a NumPy array, so we need to use the pandas DataFrame method to create a DataFrame.) Finally, we join the encoded features with the other features from the training data:<p class="source-code">oe = OrdinalEncoder(categories=\</p><p class="source-code">  [X_demo_train.colenroct99.unique()])</p><p class="source-code">colenr_enc = \</p><p class="source-code">  pd.DataFrame(oe.fit_transform(X_demo_train[['colenroct99']]),</p><p class="source-code">    columns=['colenroct99'], index=X_demo_train.index)</p><p class="source-code">X_demo_train_enc = \</p><p class="source-code">  X_demo_train[['gender','maritalstatus']].\</p><p class="source-code">  join(colenr_enc)</p></li>
<li>Let's take a look at a few observations of the resulting DataFrame. Additionally, we should compare the <a id="_idIndexMarker345"/>counts of the original college <a id="_idIndexMarker346"/>enrollment feature to the transformed feature:<p class="source-code">X_demo_train_enc.head()</p><p class="source-code"><strong class="bold">             gender       maritalstatus    colenroct99</strong></p><p class="source-code"><strong class="bold">personid                                    </strong></p><p class="source-code"><strong class="bold">736081       Female       Married          0</strong></p><p class="source-code"><strong class="bold">832734       Male         Never-married    0</strong></p><p class="source-code"><strong class="bold">453537       Male         Married          0</strong></p><p class="source-code"><strong class="bold">322059       Female       Divorced         0</strong></p><p class="source-code"><strong class="bold">324323       Female       Married          1</strong></p><p class="source-code">X_demo_train.colenroct99.value_counts().sort_index()</p><p class="source-code"><strong class="bold">1. Not enrolled        3050</strong></p><p class="source-code"><strong class="bold">2. 2-year college       142</strong></p><p class="source-code"><strong class="bold">3. 4-year college       350</strong></p><p class="source-code"><strong class="bold">Name: colenroct99, dtype: int64</strong></p><p class="source-code">X_demo_train_enc.colenroct99.value_counts().sort_index()</p><p class="source-code"><strong class="bold">0       3050</strong></p><p class="source-code"><strong class="bold">1       142</strong></p><p class="source-code"><strong class="bold">2       350</strong></p><p class="source-code"><strong class="bold">Name: colenroct99, dtype: int64</strong></p></li>
</ol>
<p>The ordinal encoding<a id="_idIndexMarker347"/> replaces the initial values for <strong class="source-inline">colenroct99</strong> with numbers from 0 to 2. It is now in a form that is consumable<a id="_idIndexMarker348"/> by many machine learning models, and we have retained the meaningful ranking information.</p>
<p class="callout-heading">Note</p>
<p class="callout">Ordinal encoding is appropriate for non-linear models such as decision trees. It might not make sense in a linear regression model because that would assume that the distance between values was equally meaningful across the whole distribution. In this example, that would assume that the increase from 0 to 1 (that is, from no enrollment to 2-year enrollment) is the same thing as the increase from 1 to 2 (that is, from 2-year enrollment to 4-year enrollment).</p>
<p>One-hot encoding and ordinal encoding are relatively straightforward approaches to engineering categorical features. It can be more complicated to deal with categorical features when there are many more unique values. In the next section, we will go over a couple of techniques for handling those features.</p>
<h1 id="_idParaDest-51"><a id="_idTextAnchor050"/>Encoding categorical features with medium or high cardinality</h1>
<p>When <a id="_idIndexMarker349"/>we are working with a categorical feature that has many unique values, say 10 or more, it can be impractical to create a dummy variable for each value. When there is high cardinality, that is, a very large number of unique values, there might be too few observations with certain values to provide much information for our models. At the extreme, with an ID variable, there is just one observation for each value.</p>
<p>There are a couple of ways in which to handle medium or high cardinality. One way is to create dummies for the top <em class="italic">k</em> categories and group the remaining values into an <em class="italic">other</em> category. Another way is to use feature hashing, also known as the hashing trick. In this section, we will explore both strategies. We will be using the COVID-19 dataset for this example:</p>
<ol>
<li value="1">Let's create training and testing DataFrames from COVID-19 data, and import the <strong class="source-inline">feature_engine</strong> and <strong class="source-inline">category_encoders</strong> libraries:<p class="source-code">import pandas as pd</p><p class="source-code">from feature_engine.encoding import OneHotEncoder</p><p class="source-code">from category_encoders.hashing import HashingEncoder</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">covidtotals = pd.read_csv("data/covidtotals.csv")</p><p class="source-code">feature_cols = ['location','population',</p><p class="source-code">    'aged_65_older','diabetes_prevalence','region']</p><p class="source-code">covidtotals = covidtotals[['total_cases'] + feature_cols].dropna()</p><p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(covidtotals[feature_cols],\</p><p class="source-code">  covidtotals[['total_cases']], test_size=0.3, </p><p class="source-code">  random_state=0)</p></li>
</ol>
<p>The feature <a id="_idIndexMarker350"/>region has 16 unique values, the first 6 of which have counts of 10 or more:</p>
<p class="source-code">X_train.region.value_counts()</p>
<p class="source-code"><strong class="bold">Eastern Europe  16</strong></p>
<p class="source-code"><strong class="bold">East Asia  12</strong></p>
<p class="source-code"><strong class="bold">Western Europe  12</strong></p>
<p class="source-code"><strong class="bold">West Africa  11</strong></p>
<p class="source-code"><strong class="bold">West Asia  10</strong></p>
<p class="source-code"><strong class="bold">East Africa  10</strong></p>
<p class="source-code"><strong class="bold">South America  7</strong></p>
<p class="source-code"><strong class="bold">South Asia  7</strong></p>
<p class="source-code"><strong class="bold">Central Africa  7</strong></p>
<p class="source-code"><strong class="bold">Southern Africa  7</strong></p>
<p class="source-code"><strong class="bold">Oceania / Aus  6</strong></p>
<p class="source-code"><strong class="bold">Caribbean  6</strong></p>
<p class="source-code"><strong class="bold">Central Asia  5</strong></p>
<p class="source-code"><strong class="bold">North Africa  4</strong></p>
<p class="source-code"><strong class="bold">North America  3</strong></p>
<p class="source-code"><strong class="bold">Central America  3</strong></p>
<p class="source-code"><strong class="bold">Name: region, dtype: int64</strong></p>
<ol>
<li value="2">We can use the <strong class="source-inline">OneHotEncoder</strong> module from <strong class="source-inline">feature_engine</strong> again to encode the <strong class="source-inline">region</strong> feature. This time, we use the <strong class="source-inline">top_categories</strong> parameter to indicate that we only want to create dummies for the top six category values. Any values that do not fall into the top six will have a 0 for all of the dummies:<p class="source-code">ohe = OneHotEncoder(top_categories=6, variables=['region'])</p><p class="source-code">covidtotals_ohe = ohe.fit_transform(covidtotals)</p><p class="source-code">covidtotals_ohe.filter(regex='location|region',</p><p class="source-code">  axis="columns").sample(5, random_state=99).T</p><p class="source-code"><strong class="bold">          97      173      92         187        104</strong></p><p class="source-code"><strong class="bold">Location  Israel  Senegal  Indonesia  Sri Lanka  Kenya</strong></p><p class="source-code"><strong class="bold">region_Eastern Europe  0      0      0      0      0</strong></p><p class="source-code"><strong class="bold">region_Western Europe  0      0      0      0      0</strong></p><p class="source-code"><strong class="bold">region_West Africa     0      1      0      0      0</strong></p><p class="source-code"><strong class="bold">region_East Asia       0      0      1      0      0</strong></p><p class="source-code"><strong class="bold">region_West Asia       1      0      0      0      0</strong></p><p class="source-code"><strong class="bold">region_East Africa     0      0      0      0      1</strong></p></li>
</ol>
<p>An alternative<a id="_idIndexMarker351"/> approach to one-hot encoding, when a categorical feature has many unique values, is to use <strong class="bold">feature hashing</strong>. </p>
<h2 id="_idParaDest-52"><a id="_idTextAnchor051"/>Feature hashing</h2>
<p>Feature hashing<a id="_idIndexMarker352"/> maps a large number of unique feature values to a smaller number of dummy variables. We can specify the number of dummy variables to create. However, collisions are possible; that is, some feature values might map to the same dummy variable combination. The number of collisions increases as we decrease the number of requested dummy variables.</p>
<p>We can use <strong class="source-inline">HashingEncoder</strong> from <strong class="source-inline">category_encoders</strong> to do feature hashing. We use <strong class="source-inline">n_components</strong> to indicate that we want six dummy variables (we copy the <strong class="source-inline">region</strong> feature before we<a id="_idIndexMarker353"/> do the transform so that we can compare the original values to the new dummies):</p>
<pre class="source-code">X_train['region2'] = X_train.region</pre>
<pre class="source-code">he = HashingEncoder(cols=['region'], n_components=6)</pre>
<pre class="source-code">X_train_enc = he.fit_transform(X_train)</pre>
<pre class="source-code">X_train_enc.\</pre>
<pre class="source-code"> groupby(['col_0','col_1','col_2','col_3','col_4',</pre>
<pre class="source-code">   'col_5','region2']).\</pre>
<pre class="source-code">    size().reset_index().rename(columns={0:'count'})</pre>
<pre class="source-code"><strong class="bold">  col_0 col_1 col_2 col_3 col_4 col_5 region2         count</strong></pre>
<pre class="source-code"><strong class="bold">0   0     0     0     0     0     1   Caribbean       6</strong></pre>
<pre class="source-code"><strong class="bold">1   0     0     0     0     0     1   Central Africa  7</strong></pre>
<pre class="source-code"><strong class="bold">2   0     0     0     0     0     1   East Africa     10</strong></pre>
<pre class="source-code"><strong class="bold">3   0     0     0     0     0     1   North Africa    4</strong></pre>
<pre class="source-code"><strong class="bold">4   0     0     0     0     1     0   Central America 3</strong></pre>
<pre class="source-code"><strong class="bold">5   0     0     0     0     1     0   Eastern Europe  16</strong></pre>
<pre class="source-code"><strong class="bold">6   0     0     0     0     1     0   North America   3</strong></pre>
<pre class="source-code"><strong class="bold">7   0     0     0     0     1     0   Oceania / Aus   6</strong></pre>
<pre class="source-code"><strong class="bold">8   0     0     0     0     1     0   Southern Africa 7</strong></pre>
<pre class="source-code"><strong class="bold">9   0     0     0     0     1     0   West Asia       10</strong></pre>
<pre class="source-code"><strong class="bold">10  0     0     0     0     1     0   Western Europe  12</strong></pre>
<pre class="source-code"><strong class="bold">11  0     0     0     1     0     0   Central Asia    5</strong></pre>
<pre class="source-code"><strong class="bold">12  0     0     0     1     0     0   East Asia       12</strong></pre>
<pre class="source-code"><strong class="bold">13  0     0     0     1     0     0   South Asia      7</strong></pre>
<pre class="source-code"><strong class="bold">14  0     0     1     0     0     0   West Africa     11</strong></pre>
<pre class="source-code"><strong class="bold">15  1     0     0     0     0     0   South America   7</strong></pre>
<p>Unfortunately, this gives us a large number of collisions. For example, Caribbean, Central Africa, East Africa, and North Africa all get the same dummy variable values. In this case at least, using<a id="_idIndexMarker354"/> one-hot encoding and specifying the number of categories, as we did in the last section, was a better solution.</p>
<p>In the previous two sections, we covered common encoding strategies: one-hot encoding, ordinal encoding, and feature hashing. Almost all of our categorical features will require some kind of encoding before we can use them in a model. However, sometimes, we need to alter our features in other ways, including with transformations, binning, and scaling. In the next three sections, we will consider the reasons why we might need to alter our features in these ways and explore tools for doing that.</p>
<h1 id="_idParaDest-53"><a id="_idTextAnchor052"/>Using mathematical transformations</h1>
<p>Sometimes, we want to use<a id="_idIndexMarker355"/> features that do not have a Gaussian distribution with a machine learning algorithm that assumes our features are distributed in that way. When that happens, we either need to change our minds about which algorithm to use (for example, we could choose KNN rather than linear regression) or transform our features so that they approximate a Gaussian distribution. In this section, we will go over a couple of strategies for doing the latter:</p>
<ol>
<li value="1">We start by importing the transformation module from <strong class="source-inline">feature_engine</strong>, <strong class="source-inline">train_test_split</strong> from <strong class="source-inline">sklearn</strong>, and <strong class="source-inline">stats</strong> from <strong class="source-inline">scipy</strong>. Additionally, we create training and testing DataFrames with the COVID-19 data:<p class="source-code">import pandas as pd</p><p class="source-code">from feature_engine import transformation as vt</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">from scipy import stats</p><p class="source-code">covidtotals = pd.read_csv("data/covidtotals.csv")</p><p class="source-code">feature_cols = ['location','population',</p><p class="source-code">    'aged_65_older','diabetes_prevalence','region']</p><p class="source-code">covidtotals = covidtotals[['total_cases'] + feature_cols].dropna()</p><p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(covidtotals[feature_cols],\</p><p class="source-code">  covidtotals[['total_cases']], test_size=0.3, \</p><p class="source-code">  random_state=0)</p></li>
<li>Let's take a look at<a id="_idIndexMarker356"/> how the total number of cases by country is distributed. We should also calculate the skew:<p class="source-code">y_train.total_cases.skew()</p><p class="source-code"><strong class="bold">6.313169268923333</strong></p><p class="source-code">plt.hist(y_train.total_cases)</p><p class="source-code">plt.title("Total COVID Cases  (in millions)")</p><p class="source-code">plt.xlabel('Cases')</p><p class="source-code">plt.ylabel("Number of Countries")</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This produces the following histogram:</p>
<div>
<div class="IMG---Figure" id="_idContainer036">
<img alt="Figure 4.2 – A histogram of the total number of COVID cases " height="278" src="image/Figure_1.2_B17978.jpg" width="391"/>
</div>
</div>
<p class="figure-caption">Figure 4.2 – A histogram of the total number of COVID cases</p>
<p>This illustrates <a id="_idIndexMarker357"/>the very high skew for the total number of cases. In fact, it looks log-normal, which is not surprising given the large number of very low values and several very high values.</p>
<p class="callout-heading">Note</p>
<p class="callout">For more information about the measures of skew and kurtosis, please refer to <a href="B17978_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Examining the Distribution of Features and Targets</em>.</p>
<ol>
<li value="3">Let's try a log transformation. All we need to do to get <strong class="source-inline">feature_engine</strong> to do the transformation is call <strong class="source-inline">LogTranformer</strong> and pass the feature or features that we would <a id="_idIndexMarker358"/>like to transform:<p class="source-code">tf = vt.LogTransformer(variables = ['total_cases'])</p><p class="source-code">y_train_tf = tf.fit_transform(y_train)</p><p class="source-code">y_train_tf.total_cases.skew()</p><p class="source-code">-1.3872728024141519</p><p class="source-code">plt.hist(y_train_tf.total_cases)</p><p class="source-code">plt.title("Total COVID Cases (log transformation)")</p><p class="source-code">plt.xlabel('Cases')</p><p class="source-code">plt.ylabel("Number of Countries")</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This produces the following histogram:</p>
<div>
<div class="IMG---Figure" id="_idContainer037">
<img alt="Figure 4.3 – A histogram of the total number of COVID cases with log transformation " height="278" src="image/Figure_1.3_B17978.jpg" width="382"/>
</div>
</div>
<p class="figure-caption">Figure 4.3 – A histogram of the total number of COVID cases with log transformation</p>
<p>Effectively, log <a id="_idIndexMarker359"/>transformations increase variability at the lower end of the distribution and decrease variability at the upper end. This produces a more symmetrical distribution. This is because the slope of the logarithmic function is steeper for smaller values than for larger ones.</p>
<ol>
<li value="4">This is definitely a big improvement, but there is now some negative skew. Perhaps a Box-Cox transformation will yield better results. Let's try that:<p class="source-code">tf = vt.BoxCoxTransformer(variables = ['total_cases'])</p><p class="source-code">y_train_tf = tf.fit_transform(y_train)</p><p class="source-code">y_train_tf.total_cases.skew()</p><p class="source-code"><strong class="bold">0.07333475786753735</strong></p><p class="source-code">plt.hist(y_train_tf.total_cases)</p><p class="source-code">plt.title("Total COVID Cases (Box-Cox transformation)")</p><p class="source-code">plt.xlabel('Cases')</p><p class="source-code">plt.ylabel("Number of Countries")</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This produces<a id="_idIndexMarker360"/> the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer038">
<img alt="Figure 4.4 – A histogram of the total number of COVID cases with a Box-Cox transformation " height="278" src="image/Figure_1.4_B17978.jpg" width="382"/>
</div>
</div>
<p class="figure-caption">Figure 4.4 – A histogram of the total number of COVID cases with a Box-Cox transformation</p>
<p>Box-Cox transformations identify a value for lambda between -5 and 5 that generates a distribution that is closest to normal. It uses the following equation for the transformation:</p>
<p><img alt="" height="49" src="image/B17978_04_001.png" width="613"/></p>
<p>or</p>
<p><img alt="" height="49" src="image/B17978_04_002.png" width="470"/></p>
<p>Here, <img alt="" height="48" src="image/B17978_04_003.png" width="90"/> is our <a id="_idIndexMarker361"/>transformed feature. Just for fun, let's see the value of the lambda that was used to transform <strong class="source-inline">total_cases</strong>:</p>
<pre class="source-code">stats.boxcox(y_train.total_cases)[1]</pre>
<pre class="source-code"><strong class="bold">0.10435377585681517</strong></pre>
<p>The lambda for the Box-Cox transformation is <strong class="source-inline">0.104</strong>. For comparison, the lambda for a feature with a Gaussian distribution would be 1.000, meaning that no transformation would be necessary.</p>
<p>Now that our transformed total cases feature looks good, we can build a model with it as the target. Additionally, we can set up our pipeline to restore values to their original scaling when we make predictions. <strong class="source-inline">feature_engine</strong> has a number of other transformations that are implemented similarly to the log and Box-Cox transformations.</p>
<h1 id="_idParaDest-54"><a id="_idTextAnchor053"/>Feature binning</h1>
<p>Sometimes, we will want to convert a <a id="_idIndexMarker362"/>continuous feature into a categorical feature. The process of creating <em class="italic">k</em> equally spaced intervals from the minimum to the maximum value of a distribution is<a id="_idIndexMarker363"/> called <strong class="bold">binning</strong> or, the somewhat less-friendly<a id="_idIndexMarker364"/> term, <strong class="bold">discretization</strong>. Binning can address several important issues with a feature: skew, excessive kurtosis, and the presence of outliers.</p>
<h2 id="_idParaDest-55"><a id="_idTextAnchor054"/>Equal-width and equal-frequency binning</h2>
<p>Binning might be a good choice with the<a id="_idIndexMarker365"/> COVID case data. Let's<a id="_idIndexMarker366"/> try that (this might also be useful with other variables in the dataset, including total deaths and population, but we will only work with total cases for now. <strong class="source-inline">total_cases</strong> is the target variable in the following code, so it is a column – the only column – on the <strong class="source-inline">y_train</strong> DataFrame):</p>
<ol>
<li value="1">First, we need to import <strong class="source-inline">EqualFrequencyDiscretiser</strong> and <strong class="source-inline">EqualWidthDiscretiser</strong> from <strong class="source-inline">feature_engine</strong>. Additionally, we need to create training and testing DataFrames from the COVID data:<p class="source-code">import pandas as pd</p><p class="source-code">from feature_engine.discretisation import EqualFrequencyDiscretiser as efd</p><p class="source-code">from feature_engine.discretisation import EqualWidthDiscretiser as ewd</p><p class="source-code">from sklearn.preprocessing import KBinsDiscretizer</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">covidtotals = pd.read_csv("data/covidtotals.csv")</p><p class="source-code">feature_cols = ['location','population',</p><p class="source-code">    'aged_65_older','diabetes_prevalence','region']</p><p class="source-code">covidtotals = covidtotals[['total_cases'] + feature_cols].dropna()</p><p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(covidtotals[feature_cols],\</p><p class="source-code">  covidtotals[['total_cases']], test_size=0.3, random_state=0)</p></li>
<li>We can use the<a id="_idIndexMarker367"/> pandas <strong class="source-inline">qcut</strong> method, and its <strong class="source-inline">q</strong> parameter, to <a id="_idIndexMarker368"/>create 10 bins of relatively equal frequency:<p class="source-code">y_train['total_cases_group'] = pd.qcut(y_train.total_cases, q=10, labels=[0,1,2,3,4,5,6,7,8,9])</p><p class="source-code"><strong class="bold">y_train.total_cases_group.value_counts().sort_index()</strong></p><p class="source-code"><strong class="bold">0   13</strong></p><p class="source-code"><strong class="bold">1   13</strong></p><p class="source-code"><strong class="bold">2   12</strong></p><p class="source-code"><strong class="bold">3   13</strong></p><p class="source-code"><strong class="bold">4   12</strong></p><p class="source-code"><strong class="bold">5   13</strong></p><p class="source-code"><strong class="bold">6   12</strong></p><p class="source-code"><strong class="bold">7   13</strong></p><p class="source-code"><strong class="bold">8   12</strong></p><p class="source-code"><strong class="bold">9   13</strong></p><p class="source-code"><strong class="bold">Name: total_cases_group, dtype: int64</strong></p></li>
<li>We can accomplish the same thing with <strong class="source-inline">EqualFrequencyDiscretiser</strong>. First, we define a function to run the transformation. The function takes a <strong class="source-inline">feature_engine</strong> transformation and the training and testing DataFrames. It returns the transformed DataFrames (it is not necessary to define a function, but it makes sense here since we will repeat these steps later):<p class="source-code">def runtransform(bt, dftrain, dftest):</p><p class="source-code">  bt.fit(dftrain)</p><p class="source-code">  train_bins = bt.transform(dftrain)</p><p class="source-code">  test_bins = bt.transform(dftest)</p><p class="source-code">  return train_bins, test_bins</p></li>
<li>Next, we<a id="_idIndexMarker369"/> create an <strong class="source-inline">EqualFrequencyDiscretiser</strong> transformer and call the <strong class="source-inline">runtransform</strong> function <a id="_idIndexMarker370"/>that we just created:<p class="source-code">y_train.drop(['total_cases_group'], axis=1, inplace=True)</p><p class="source-code">bintransformer = efd(q=10, variables=['total_cases'])</p><p class="source-code">y_train_bins, y_test_bins = runtransform(bintransformer, y_train, y_test)</p><p class="source-code">y_train_bins.total_cases.value_counts().sort_index()</p><p class="source-code"><strong class="bold">0  13</strong></p><p class="source-code"><strong class="bold">1  13</strong></p><p class="source-code"><strong class="bold">2  12</strong></p><p class="source-code"><strong class="bold">3  13</strong></p><p class="source-code"><strong class="bold">4  12</strong></p><p class="source-code"><strong class="bold">5  13</strong></p><p class="source-code"><strong class="bold">6  12</strong></p><p class="source-code"><strong class="bold">7  13</strong></p><p class="source-code"><strong class="bold">8  12</strong></p><p class="source-code"><strong class="bold">9  13</strong></p><p class="source-code"><strong class="bold">Name: total_cases, dtype: int64</strong></p></li>
</ol>
<p>This gives us the same results as <strong class="source-inline">qcut</strong>, but it has the advantage of being easier to bring into a machine learning pipeline since we are using <strong class="source-inline">feature_engine</strong> to produce it. The equal-frequency binning addresses both the skew and outlier problems.</p>
<p class="callout-heading">Note</p>
<p class="callout">We will explore machine learning pipelines in detail in this book, starting with <a href="B17978_06_ePub.xhtml#_idTextAnchor078"><em class="italic">Chapter 6</em></a>, <em class="italic">Preparing for Model Evaluation</em>. Here, the key point is that feature engine transformers can be a part of a pipeline that includes other <strong class="source-inline">sklearn</strong>-compatible transformers, even ones we construct ourselves.</p>
<ol>
<li value="5"><strong class="source-inline">EqualWidthDiscretiser</strong> works <a id="_idIndexMarker371"/>similarly:<p class="source-code">bintransformer = ewd(bins=10, variables=['total_cases'])</p><p class="source-code">y_train_bins, y_test_bins = runtransform(bintransformer, y_train, y_test)</p><p class="source-code">y_train_bins.total_cases.value_counts().sort_index()</p><p class="source-code"><strong class="bold">0  119</strong></p><p class="source-code"><strong class="bold">1  4</strong></p><p class="source-code"><strong class="bold">5  1</strong></p><p class="source-code"><strong class="bold">9  2</strong></p><p class="source-code"><strong class="bold">Name: total_cases, dtype: int64</strong></p></li>
</ol>
<p>This is a far <a id="_idIndexMarker372"/>less successful transformation. Almost all of the values are at the bottom of the distribution in the data prior to the binning, so it is not surprising that equal-width binning would have the same problem. It results in only 4 bins, even though we requested 10.</p>
<ol>
<li value="6">Let's examine the range of each bin. Here, we can see that the equal-width binner is not even able to construct equal-width bins because of the small number of observations at the top of the distribution:<p class="source-code">pd.options.display.float_format = '{:,.0f}'.format</p><p class="source-code">y_train_bins = y_train_bins.\</p><p class="source-code">  rename(columns={'total_cases':'total_cases_group'}).\</p><p class="source-code">  join(y_train)</p><p class="source-code">y_train_bins.groupby("total_cases_group")["total_cases"].agg(['min','max'])</p><p class="source-code">  min  max</p><p class="source-code"><strong class="bold">total_cases_group       </strong></p><p class="source-code"><strong class="bold">0  1           3,304,135</strong></p><p class="source-code"><strong class="bold">1  3,740,567   5,856,682</strong></p><p class="source-code"><strong class="bold">5  18,909,037  18,909,037</strong></p><p class="source-code"><strong class="bold">9  30,709,557  33,770,444</strong></p></li>
</ol>
<p>Although in this case, equal-width binning <a id="_idIndexMarker373"/>was a bad choice, there<a id="_idIndexMarker374"/> are many times when it makes sense. It can be useful when data is more uniformly distributed or when the equal widths make sense substantively.</p>
<h2 id="_idParaDest-56"><a id="_idTextAnchor055"/>K-means binning</h2>
<p>Another option is to<a id="_idIndexMarker375"/> use <em class="italic">k</em>-means clustering to determine the bins. The <em class="italic">k</em>-means algorithm randomly selects <em class="italic">k</em> data points as centers of clusters and then assigns the other data points to the closest cluster. The mean of each cluster is computed, and the data points are reassigned to the nearest new cluster. This process is repeated until the optimal centers are found.</p>
<p>When <em class="italic">k</em>-means is used for binning, all data points in the same cluster will have the same ordinal value:</p>
<ol>
<li value="1">We can <a id="_idIndexMarker376"/>use scikit-learn's <strong class="source-inline">KBinsDiscretizer</strong> to create bins with the COVID cases data: <p class="source-code">kbins = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='kmeans')</p><p class="source-code">y_train_bins = \</p><p class="source-code">  pd.DataFrame(kbins.fit_transform(y_train),</p><p class="source-code">  columns=['total_cases'])</p><p class="source-code">y_train_bins.total_cases.value_counts().sort_index()</p><p class="source-code"><strong class="bold">0  49</strong></p><p class="source-code"><strong class="bold">1  24</strong></p><p class="source-code"><strong class="bold">2  23</strong></p><p class="source-code"><strong class="bold">3  11</strong></p><p class="source-code"><strong class="bold">4  6</strong></p><p class="source-code"><strong class="bold">5  6</strong></p><p class="source-code"><strong class="bold">6  4</strong></p><p class="source-code"><strong class="bold">7  1</strong></p><p class="source-code"><strong class="bold">8  1</strong></p><p class="source-code"><strong class="bold">9  1</strong></p><p class="source-code"><strong class="bold">Name: total_cases, dtype: int64</strong></p></li>
<li>Let's compare the skew and kurtosis of the original total cases variable to that of the binned variable. Recall that we would expect a skew of 0 and a kurtosis near 3 for a variable with a Gaussian distribution. The distribution of the binned variable is much closer to Gaussian:<p class="source-code">y_train.total_cases.agg(['skew','kurtosis'])</p><p class="source-code"><strong class="bold">skew          6.313</strong></p><p class="source-code"><strong class="bold">kurtosis     41.553</strong></p><p class="source-code"><strong class="bold">Name: total_cases, dtype: float64</strong></p><p class="source-code">y_train_bins.total_cases.agg(['skew','kurtosis'])</p><p class="source-code"><strong class="bold">skew            1.439</strong></p><p class="source-code"><strong class="bold">kurtosis        1.923</strong></p><p class="source-code"><strong class="bold">Name: total_cases, dtype: float64</strong></p></li>
</ol>
<p>Binning can help us to<a id="_idIndexMarker377"/> address skew, kurtosis, and outliers in our data. However, it does mask much of the variation in the feature and reduces its explanatory potential. Often, some form of scaling, such as min-max or z-score, is a better option. Let's examine feature scaling next.</p>
<h1 id="_idParaDest-57"><a id="_idTextAnchor056"/>Feature scaling</h1>
<p>Often, the features we want to <a id="_idIndexMarker378"/>use in our model are on very different scales. Put simply, the distance between the minimum and maximum values, or the range, varies substantially across possible features. For example, in the COVID-19 data, the total cases feature goes from 1 to almost 34 million, while aged 65 or older goes from 9 to 27 (the number represents the percentage of the population).</p>
<p>Having features on very different scales impacts many machine learning algorithms. For example, KNN models often use Euclidean distance, and features with greater ranges will have a greater influence on the model. Scaling can address this problem.</p>
<p>In this section, we will go over two <a id="_idIndexMarker379"/>popular approaches to <a id="_idIndexMarker380"/>scaling: <strong class="bold">min-max scaling</strong> and <strong class="bold">standard</strong> (or <strong class="bold">z-score</strong>) scaling. Min-max scaling <a id="_idIndexMarker381"/>replaces each value with its location in the range. More precisely, the following happens:</p>
<p class="figure"><img alt="" height="38" src="image/B17978_04_004.png" width="48"/> = <img alt="" height="58" src="image/B17978_04_005.png" width="530"/></p>
<p>Here, <img alt="" height="37" src="image/B17978_04_006.png" width="46"/> is the min-max score, <img alt="" height="38" src="image/B17978_04_007.png" width="48"/> is the value for the <img alt="" height="40" src="image/B17978_04_008.png" width="48"/> observation of the <img alt="" height="49" src="image/B17978_04_009.png" width="56"/> feature, and <img alt="" height="47" src="image/B17978_04_010.png" width="87"/> and <img alt="" height="39" src="image/B17978_04_011.png" width="96"/> are the minimum and maximum values of the <img alt="" height="49" src="image/B17978_04_012.png" width="57"/> feature.</p>
<p>Standard scaling normalizes the feature values around a mean of 0. Those who studied undergraduate statistics will recognize it as the z-score. Specifically, it is as follows: </p>
<p><img alt="" height="59" src="image/B17978_04_013.png" width="353"/></p>
<p>Here, <img alt="" height="39" src="image/B17978_04_014.png" width="49"/> is the value for the <img alt="" height="40" src="image/B17978_04_015.png" width="48"/> observation of the <img alt="" height="49" src="image/B17978_04_009.png" width="56"/> feature, <img alt="" height="38" src="image/B17978_04_017.png" width="36"/> is the mean for feature <img alt="" height="41" src="image/B17978_04_018.png" width="20"/>, and <img alt="" height="38" src="image/B17978_04_019.png" width="30"/> is the standard <a id="_idIndexMarker382"/>deviation for that feature.</p>
<p>We can use scikit-learn's preprocessing module to get the min-max and standard scalers:</p>
<ol>
<li value="1">We start by importing the preprocessing module and creating training and testing DataFrames from the COVID-19 data:<p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler</p><p class="source-code">covidtotals = pd.read_csv("data/covidtotals.csv")</p><p class="source-code">feature_cols = ['population','total_deaths',</p><p class="source-code">    'aged_65_older','diabetes_prevalence']</p><p class="source-code">covidtotals = covidtotals[['total_cases'] + feature_cols].dropna()</p><p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(covidtotals[feature_cols],\</p><p class="source-code">  covidtotals[['total_cases']], test_size=0.3, random_state=0)</p></li>
<li>Now, we can run the <a id="_idIndexMarker383"/>min-max scaler. The <strong class="source-inline">fit_transform</strong> method for <strong class="source-inline">sklearn</strong> will return a <strong class="source-inline">numpy</strong> array. We convert it into a pandas DataFrame using the columns and index from the training DataFrame. Notice how all features now have values between 0 and 1:<p class="source-code">scaler = MinMaxScaler()</p><p class="source-code">X_train_mms = pd.DataFrame(scaler.fit_transform(X_train),</p><p class="source-code">  columns=X_train.columns, index=X_train.index)</p><p class="source-code">X_train_mms.describe()</p><p class="source-code"><strong class="bold">     population total_deaths aged_65_older diabetes_prevalence</strong></p><p class="source-code"><strong class="bold">count  123.00      123.00        123.00        123.00</strong></p><p class="source-code"><strong class="bold">mean   0.04        0.04          0.30          0.41</strong></p><p class="source-code"><strong class="bold">std    0.13        0.14          0.24          0.23</strong></p><p class="source-code"><strong class="bold">min    0.00        0.00          0.00          0.00</strong></p><p class="source-code"><strong class="bold">25%    0.00        0.00          0.10          0.26</strong></p><p class="source-code"><strong class="bold">50%    0.01        0.00          0.22          0.37</strong></p><p class="source-code"><strong class="bold">75%    0.02        0.02          0.51          0.54</strong></p><p class="source-code"><strong class="bold">max    1.00        1.00          1.00          1.00</strong></p></li>
<li>We run the standard scaler in the same manner:<p class="source-code">scaler = StandardScaler()</p><p class="source-code">X_train_ss = pd.DataFrame(scaler.fit_transform(X_train),</p><p class="source-code">  columns=X_train.columns, index=X_train.index)</p><p class="source-code">X_train_ss.describe()</p><p class="source-code"><strong class="bold">       population  total_deaths  aged_65_older  diabetes_prevalence</strong></p><p class="source-code"><strong class="bold">count  123.00      123.00        123.00       123.00</strong></p><p class="source-code"><strong class="bold">mean  -0.00       -0.00         -0.00        -0.00</strong></p><p class="source-code"><strong class="bold">std    1.00        1.00          1.00         1.00</strong></p><p class="source-code"><strong class="bold">min   -0.29       -0.32         -1.24        -1.84</strong></p><p class="source-code"><strong class="bold">25%   -0.27       -0.31         -0.84        -0.69</strong></p><p class="source-code"><strong class="bold">50%   -0.24       -0.29         -0.34        -0.18</strong></p><p class="source-code"><strong class="bold">75%   -0.11       -0.18          0.87         0.59</strong></p><p class="source-code"><strong class="bold">max    7.58        6.75          2.93         2.63</strong></p></li>
</ol>
<p>If we have outliers in our<a id="_idIndexMarker384"/> data, robust scaling might be a good option. Robust scaling subtracts the median from each value of a variable and divides that value by the interquartile range. So, each value is as follows:</p>
<p><img alt="" height="61" src="image/B17978_04_020.png" width="1045"/></p>
<p>Here, <img alt="" height="38" src="image/B17978_04_021.png" width="49"/> is the value of the <img alt="" height="48" src="image/B17978_04_022.png" width="55"/> feature, and <img alt="" height="48" src="image/B17978_04_023.png" width="159"/>, <img alt="" height="49" src="image/B17978_04_024.png" width="259"/>,and <img alt="" height="48" src="image/B17978_04_025.png" width="249"/> are the median, third, and first quantiles of the <img alt="" height="49" src="image/B17978_04_026.png" width="55"/> feature. Robust scaling is less sensitive to extreme values since it does not use the mean or variance.</p>
<ol>
<li value="4">We can use scikit-learn's <strong class="source-inline">RobustScaler</strong> module to do robust scaling:<p class="source-code">scaler = RobustScaler()</p><p class="source-code">X_train_rs = pd.DataFrame(</p><p class="source-code">  scaler.fit_transform(X_train),</p><p class="source-code">  columns=X_train.columns, index=X_train.index)</p><p class="source-code">X_train_rs.describe()</p><p class="source-code"><strong class="bold">     population total_deaths aged_65_older diabetes_prevalence</strong></p><p class="source-code"><strong class="bold">count  123.00      123.00      123.00      123.00</strong></p><p class="source-code"><strong class="bold">mean   1.47        2.22        0.20        0.14</strong></p><p class="source-code"><strong class="bold">std    6.24        7.65        0.59        0.79</strong></p><p class="source-code"><strong class="bold">min   -0.35       -0.19       -0.53       -1.30</strong></p><p class="source-code"><strong class="bold">25%   -0.24       -0.15       -0.30       -0.40</strong></p><p class="source-code"><strong class="bold">50%    0.00        0.00        0.00        0.00</strong></p><p class="source-code"><strong class="bold">75%    0.76        0.85        0.70        0.60</strong></p><p class="source-code"><strong class="bold">max    48.59       53.64       1.91        2.20</strong></p></li>
</ol>
<p>We use<a id="_idIndexMarker385"/> feature scaling with most machine learning algorithms. Although it is not often required, it yields noticeably better results. Min-max scaling and standard scaling are popular scaling techniques, but there are times when robust scaling might be the better option.</p>
<h1 id="_idParaDest-58"><a id="_idTextAnchor057"/>Summary</h1>
<p>In this chapter, we covered a wide range of feature engineering techniques. We used tools to drop redundant or highly correlated features. We explored the most common kinds of encoding – one-hot encoding, ordinal encoding, and hashing encoding. Following this, we used transformations to improve the distribution of our features. Finally, we used common binning and scaling approaches to address skew, kurtosis, and outliers, and to adjust for features with widely different ranges.</p>
<p>Some of the techniques we discussed in this chapter are required for most machine learning models. We almost always need to encode our features for algorithms in order to understand them correctly. For example, most algorithms cannot make sense of <em class="italic">female</em> or <em class="italic">male</em> values or know not to treat ZIP codes as ordinal. Although not typically necessary, scaling is often a very good idea when we have features with vastly different ranges. When we are using algorithms that assume a Gaussian distribution of our features, some form of transformation might be required for our features to be consistent with that assumption.</p>
<p>We now have a good sense of how our features are distributed, have imputed missing values, and have done some feature engineering where necessary. We are now prepared to begin perhaps the most interesting and meaningful part of the model building process – feature selection. </p>
<p>In the next chapter, we will examine key feature selection tasks, building on the feature cleaning, exploration, and engineering work that we have done so far.</p>
</div>
</div>
</body></html>