<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Setting Up the Development Environment</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span><span>Just like</span></span> traditional software development, ML application development requires the mastery of specialist boilerplate code and a development environment that allows the developer to proceed at a pace that has the lowest amount of friction and distraction. Software developers typically waste a lot of time with basic setup and data wrangling tasks. Being a productive and professional ML developer requires the ability to quickly prototype solutions; this means expending as little effort as possible on trivial tasks.</p>
<p>In the previous chapter, we outlined the main ML problems and a development process that you can follow to obtain a commercial solution. We also explained the advantages offered by Go as a programming language when creating ML applications.</p>
<p class="mce-root">In this chapter, we will guide you through the steps that are required to set up a development environment for Go that is optimized for ML applications. Specifically, we will cover the following topics:</p>
<ul>
<li>How to install Go</li>
<li>Running Go interactively using Jupyter and gophernotes</li>
<li>Data wrangling with Gota</li>
<li>Data visualization with gonum/plot and gophernotes</li>
<li>Data preprocessing (formatting, cleaning, and sampling)</li>
<li>Data transformation (normalization and encoding of categorical variables)</li>
</ul>
<div class="packt_tip packt_infobox">The code examples that accompany this book are optimized for Debian-based Linux distributions. However, they can be adapted for other distributions (for example, by changing <kbd>apt</kbd> to <kbd>yum</kbd>) and Windows with Cygwin.</div>
<p>Once you have completed this chapter, you will be able to quickly explore, visualize, and process any dataset for subsequent use by an ML algorithm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Go</h1>
                </header>
            
            <article>
                
<p><span>Development environments are personal. Most developers will prefer one code editor or toolset over another. While we recommend the use of interactive tools such as Jupyter via gophernotes, the only prerequisite to running the code examples in this book is a working installation of Go 1.10 or higher. That is, the <kbd>go</kbd> command should be available and the <kbd>GOPATH</kbd> environment variable should be set up correctly. </span></p>
<p>To install Go, download a binary release for your system from <a href="https://golang.org/dl/">https://golang.org/dl/</a>. Then, refer to the one of the following subsections that matches your operating system<sup>[2]</sup>.</p>
<div class="packt_tip">If you only want to use gophernotes to run Go code and you intend to use Docker as the installation method, then you can skip this section and go straight to the <em>Running Go interactively with gophernotes</em> section.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Linux, macOS, and FreeBSD</h1>
                </header>
            
            <article>
                
<p>The binary releases are packaged as tarballs. Extract the binaries and add them to your <kbd>PATH</kbd>. Here's an example:</p>
<pre><strong>tar -C /usr/local -xzf go$VERSION.$OS-$ARCH.tar.gz &amp;&amp; \</strong><br/><strong> export PATH=$PATH:/usr/local/go/bin</strong></pre>
<p>To configure your <kbd>GOPATH</kbd> environment variable, you will need to decide where you will want your Go files, including any personal repositories, to live. One possible location is <kbd>$HOME/go</kbd>. Once you have decided on this, set the environment variable, for example as follows:</p>
<pre><strong>export GOPATH=$HOME/go</strong></pre>
<p class="mce-root">To make this instruction permanent, you will need to add this line to <kbd>.bashrc</kbd>. For instructions if you're using other shells (such as <kbd>.zsh</kbd>), please refer to the official Go installation instructions at <a href="https://github.com/golang/go/wiki/SettingGOPATH">https://github.com/golang/go/wiki/SettingGOPATH</a>. </p>
<div class="packt_tip">Make sure that your <kbd>GOPATH</kbd> is not in the same directory as your Go installation, otherwise this can cause issues. </div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Windows</h1>
                </header>
            
            <article>
                
<p>The binary releases are packaged either as a ZIP file or an MSI installer that automatically configures your environment variables. We recommend using the MSI installer. However, if you do not, then after extracting the contents of the ZIP file to a suitable location (such as <kbd>C:\Program Files\Go</kbd>), make sure that you add the <kbd>subdirectory</kbd> bin to your <kbd>PATH</kbd> environment variable using the control panel. </p>
<p>Once the binaries have been installed to a suitable location, you will need to configure your <kbd>GOPATH</kbd>. First, decide where you want your Go files, including any personal repositories, to live. One possible location is <kbd>C:\go</kbd>. Once you have decided, set the <kbd>GOPATH</kbd> environment variable to the path of this directory.</p>
<p>If you are unsure how to set environment variables, refer to the official Go installation instructions at <a href="https://github.com/golang/go/wiki/SettingGOPATH">https://github.com/golang/go/wiki/SettingGOPATH</a>.</p>
<div class="packt_tip"><span>Make sure that your <kbd>GOPATH</kbd> is not in the same directory as your Go installation, otherwise this can cause issues.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running Go interactively with gophernotes</h1>
                </header>
            
            <article>
                
<p>Project Jupyter is a not-for-profit organization that was created to develop language-agnostic interactive computing for data science<sup>[3]</sup>. The result is a mature, well-supported environment to explore, visualize, and process data that can significantly accelerate development by providing immediate feedback and integrations with plotting libraries such as <kbd>gonum</kbd>/<kbd>plot</kbd>. </p>
<p>While its first iteration, called iPython, only supported Python-based handlers (called <em>kernels</em>) at first, the latest version of Jupyter has over 50 kernels that support dozens of languages, including three kernels for the Go language<sup>[4]</sup>. GitHub has support for rendering Jupyter files (called <em>notebooks</em>)<sup>[5]</sup>, and there are various specialized hubs for sharing notebooks online, including Google Research Colabs<sup>[6]</sup>, Jupyter's community hub called NBViewer<sup>[7]</sup>, and its enterprise offering, JupyterHub<sup>[8]</sup>. Notebooks for presentation purposes can be converted into other file formats such as HTML using the nbconvert utility<sup>[9]</sup>. </p>
<p class="mce-root">In this book, we will be using Jupyter together with the gophernotes kernel for Go. The simplest way to get started with gophernotes on Linux and Windows is to use its Docker<sup>[10]</sup> image.</p>
<div class="mce-root packt_infobox">For alternative installation methods, we recommend checking the README page of the gophernotes GitHub repository at:<br/>
 <a href="https://github.com/gopherdata/gophernotes">https://github.com/gopherdata/gophernotes</a>. </div>
<p class="mce-root">The steps to begin a new gophernotes-based project are as follows:</p>
<ol>
<li>Create a new directory to hold the project files (this does not need to be in your <kbd>GOPATH</kbd>).</li>
<li>(Optional) Initialize a new git repository by running <kbd>git init</kbd> in the new directory.</li>
<li>Run the following command from the new directory (you may need to prefix it with <kbd>sudo</kbd>, depending on how you installed Docker):<br/>
<kbd>docker run -it -p 8888:8888 -v $(pwd):/usr/share/notebooks gopherdata/gophernotes:latest-ds</kbd></li>
<li>In the terminal, there will be a URL ending in <kbd>?token=[some combination of letters and numbers]</kbd>. Navigate to this URL in a modern web browser. The new directory you created will be mapped to <kbd>/usr/share/notebooks</kbd>, so navigate to this directory in the tree that presents itself. </li>
</ol>
<div class="packt_infobox">On Windows, you may need to modify the preceding command by replacing <kbd>$(pwd)</kbd> with <kbd>%CD%</kbd>.</div>
<p>Now that we have learned how to install Go and set up a basic development environment with gophernotes, it's time to learn about data preprocessing. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example – the most common phrases in positive and negative reviews</h1>
                </header>
            
            <article>
                
<p>In our first code example, we will use the multi-domain sentiment dataset (version 2.0)<sup>[11]</sup>. This dataset contains Amazon reviews from four different product categories. We will download it, preprocess it, and load it into Gota, a data wrangling library, to find the most common phrases in positive and negative reviews that do not co-occur in both. This is a basic example that involves no ML algorithms, but will serve as a hands-on introduction to Go, gophernotes, and Gota. </p>
<div class="packt_infobox">You can find the full code example in the companion repository to this book at <a href="https://github.com/PacktPublishing/Machine-Learning-with-Go-Quick-Start-Guide" target="_blank">https://github.com/PacktPublishing/Machine-Learning-with-Go-Quick-Start-Guide</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Initializing the example directory and downloading the dataset</h1>
                </header>
            
            <article>
                
<p>Following the process we implemented previously, create an empty directory to hold the code files. Before opening gophernotes, download the dataset from <a href="http://www.cs.jhu.edu/~mdredze/datasets/sentiment/processed_acl.tar.gz">http://www.cs.jhu.edu/~mdredze/datasets/sentiment/processed_acl.tar.gz</a> and extract it to <kbd>datasets/words</kbd>. On most Linux distributions, you can do this with the following script:</p>
<pre><strong>mkdir -p datasets/words &amp;&amp; \</strong><br/><strong>wget http://www.cs.jhu.edu/~mdredze/datasets/sentiment/processed_acl.tar.gz -O datasets/words-temp.tar.gz &amp;&amp; \</strong><br/><strong>tar xzvf datasets/words-temp.tar.gz -C datasets/words &amp;&amp; \</strong><br/><strong>rm datasets/words-temp.tar.gz</strong></pre>
<p>Now, start gophernotes and navigate the tree to <kbd>/usr/share/notebooks</kbd>. Create a new Notebook by clicking on <em><span class="packt_screen">New</span></em> | <em><span class="packt_screen">Go</span></em>. You will see a blank Jupyter Notebook:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-382 image-border" src="assets/d688d626-95d1-4552-a14b-8d7287e0f1df.png" style="width:43.17em;height:9.50em;"/></p>
<p class="mce-root"/>
<p>Input cells in Jupyter are marked with the <kbd>In</kbd> label. When you run the code in an input cell (<em>Shift</em> + <em>Enter</em>), a new output cell will be created with the result, marked as <kbd>Out</kbd>. Each cell is numbered with its execution order. For example, the <kbd>In [1]</kbd> cell is the first cell you ran within a given session. </p>
<p>Try running some Go statements, like the following snippet:</p>
<pre>a := 1<br/>import "fmt"<br/>fmt.Println("Hello, world")<br/>a</pre>
<p>In particular, note that the <kbd>a</kbd> variable is displayed in the output cell, even though there was no call to <kbd>fmt.Println()</kbd>.</p>
<div class="packt_tip">All the imports, variables, and funcs you define within a session remain in memory, even if you delete the input cells. To clear the current scope, go to <span class="packt_screen">Kernel | Restart</span>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loading the dataset files</h1>
                </header>
            
            <article>
                
<p>One of the basic tasks of data processing is to read the input file and load its contents. A simple way to do this is to use the <kbd>io/ioutil</kbd> utility func <kbd>ReadFile</kbd>. Unlike in a <kbd>.go</kbd> file, where you would need to place this code inside your <kbd>main</kbd> func, with gophernotes, you can run the following code without declaring any func at all:</p>
<pre>import "io/ioutil"<br/><br/>const kitchenReviews = "../datasets/words/processed_acl/kitchen"<br/><br/>positives, err := ioutil.ReadFile(kitchenReviews + "/positive.review")<br/>negatives, err2 := ioutil.ReadFile(kitchenReviews + "/negative.review")<br/>if err != nil || err2 != nil {<br/> fmt.Println("Error(s)", err, err2)<br/>}</pre>
<p>The preceding code will load the contents of reviews of kitchen products with positive sentiments into a byte slice called <kbd>positives</kbd> and the ones with negative sentiments into the byte slice called <kbd>negatives</kbd>. If you have correctly downloaded the datasets and you run this code, it should not output anything because there are no errors. If any errors appear, check that the dataset files have been extracted to the correct folder. </p>
<p>If you have opened the <kbd>positive.review</kbd> or <kbd>negative.review</kbd> file in a text editor, you may have noticed that they are formatted as a space or newline separated list of pairs, that is, <kbd>phrase:frequency</kbd>. For example, the start of the positive review is as follows:</p>
<pre>them_it:1 hovering:1 and_occasional:1 cousin_the:2 fictional_baudelaire:1 their_struggles:1</pre>
<p>In the next subsection, we will parse these pairs into a Go struct.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Parsing contents into a Struct</h1>
                </header>
            
            <article>
                
<p>We will use the <kbd>strings</kbd> package to parse the contents of the data files into slices of pairs. Each item in the slice of strings will contain a single pair, such as <kbd>them_it:1</kbd>. We will then further split this pair by the colon symbol and use the <kbd>strconv</kbd> package to parse the integer frequency into an <kbd>int</kbd>. Each <kbd>Pair</kbd> will be of the following type:</p>
<pre>type Pair struct {<br/>  Phrase string<br/>  Frequency int<br/>}</pre>
<p>We will do this as follows:</p>
<ol>
<li>First, observe that the separation between the pairs can be either a new line (<kbd>\n</kbd>) or a space. We will use the <kbd>strings.Fields</kbd> func of the strings package, which will split the string by any consecutive whitespace characters: </li>
</ol>
<pre style="padding-left: 60px">pairsPositive := strings.Fields(string(positives))<br/>pairsNegative := strings.Fields(string(negatives))</pre>
<ol start="2">
<li>Now, we will iterate each pair, splitting by the colon separator and using the <kbd>strconv</kbd> package to parse the frequency to an integer:</li>
</ol>
<pre style="padding-left: 60px">// pairsAndFilters returns a slice of Pair, split by : to obtain the phrase and frequency,<br/>// as well as a map of the phrases that can be used as a lookup table later.<br/>func pairsAndFilters(splitPairs []string) ([]Pair, map[string]bool) {<br/>  var (<br/>    pairs []Pair<br/>    m map[string]bool<br/>  )<br/>  m = make(map[string]bool)<br/>  for _, pair := range splitPairs {<br/>    p := strings.Split(pair, ":")<br/>    phrase := p[0]<br/>    m[phrase] = true<br/>    if len(p) &lt; 2 {<br/>      continue<br/>    }<br/>    freq, err := strconv.Atoi(p[1])<br/>    if err != nil {<br/>      continue<br/>    }<br/>    pairs = append(pairs, Pair{<br/>      Phrase: phrase,<br/>      Frequency: freq,<br/>    })<br/>  }<br/>  return pairs, m<br/>}</pre>
<ol start="3">
<li>We will also return a map of phrases so that we can later exclude phrases that are in the intersection between positive and negative reviews. The reason for doing this is that words that are common to both positive and negative reviews are less likely to be indicative of the positive or negative sentiment. This is done with the following function:</li>
</ol>
<pre style="padding-left: 60px">// exclude returns a slice of Pair that does not contain the phrases in the exclusion map<br/>func exclude(pairs []Pair, exclusions map[string]bool) []Pair {<br/>  var ret []Pair<br/>  for i := range pairs {<br/>    if !exclusions[pairs[i].Phrase] {<br/>      ret = append(ret, pairs[i])<br/>    }<br/>  }<br/>  return ret<br/>}<br/><br/></pre>
<ol start="4">
<li>Finally, we will apply this to our slices of pairs:</li>
</ol>
<pre style="padding-left: 60px">parsedPositives, posPhrases := pairsAndFilters(pairsPositive)<br/>parsedNegatives, negPhrases := pairsAndFilters(pairsNegative)<br/>parsedPositives = exclude(parsedPositives, negPhrases)<br/>parsedNegatives = exclude(parsedNegatives, posPhrases)</pre>
<p>The next step is to load the parsed pairs into Gota, the data wrangling library for Go.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loading the data into a Gota dataframe</h1>
                </header>
            
            <article>
                
<p>The Gota library contains implementation of dataframes, series, and some general data wrangling algorithms<sup>[12]</sup>. The concept of a dataframe is integral to a number of popular data science libraries and languages such as Python's pandas, R, and Julia. In a nutshell, a <strong>dataframe</strong> is a list of lists (called a <strong>column</strong> or <strong>series</strong>) that each have the same length. Every list has a name—the column name or series name, depending on the nomenclature favored by the library. This abstraction mimics a database table and makes an easy fundamental building block for mathematical and statistical tools. </p>
<p>The Gota library has two packages: the <kbd>dataframe</kbd> and the <kbd>series</kbd> packages. The series package contains functions and structures to represent individual lists, whereas the <kbd>dataframe</kbd> package deals with the entire dataframe—that is, the table—as a whole. A Go developer may wish to use Gota to quickly sort, filter, aggregate, or perform relational operations, such as inner joins between two tables, saving on boilerplate code such as implementing a <kbd>sort</kbd> interface<sup>[13]</sup>. </p>
<p>There are several ways to create a new dataframe with Gota:</p>
<ul>
<li><kbd>dataframe.New(se ...series.Series)</kbd>: Accepts a slice of series (which can be created via the <kbd>series.New</kbd> func).</li>
<li><kbd>dataframe.LoadRecords(records [][]string, options ...LoadOption)</kbd>: Accepts a slice of slices. The first slice will be a slice of strings representing the column names.</li>
<li><kbd>dataframe.LoadStructs(i interface{}, options ...LoadOption)</kbd>: Accepts a slice of structs. Gota will use reflection to determine the column names based on the struct field names. </li>
<li><kbd>dataframe.LoadMaps(maps []map[string][]interface{})</kbd>: Accepts a slice of maps of column names to slices. </li>
<li><kbd>dataframe.LoadMatrix(mat Matrix)</kbd>: Accepts a slice that is compatible with the mat64 matrix interface. </li>
</ul>
<p>In our case, because we have parsed the data into structs, we will use the <kbd>LoadStructs</kbd> function, making one dataframe for positive reviews and one for negative reviews:</p>
<pre>dfPos := dataframe.LoadStructs(parsedPositives)<br/>dfNeg := dataframe.LoadStructs(parsedNegatives)</pre>
<div class="packt_tip">If you want to inspect the content of a dataframe, that is, <kbd>df</kbd>, just use <kbd>fmt.Println(df)</kbd>. This will show you the first 10 rows of the dataframe, along with its column names and some useful metadata, such as the total number of rows. </div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Finding the most common phrases</h1>
                </header>
            
            <article>
                
<p>Now that the data has been parsed, the co-occurring phrases have been filtered out, and the resulting phrase/frequency pairs have been loaded into dataframes, all that is remaining is to find the most common phrases for the positive and negative reviews and display them. One way of doing this without dataframes would be to create a <kbd>type ByFrequency []Pair</kbd> type that implements the <kbd>sort</kbd> interface, and then compose <kbd>sort.Reverse</kbd> and <kbd>sort.Sort</kbd> to order positive pairs and negative pairs by descending frequency. However, by using Gota, we can achieve this with one line per dataframe:</p>
<pre>dfPos = dfPos.Arrange(dataframe.RevSort("Frequency"))<br/>dfNeg = dfNeg.Arrange(dataframe.RevSort("Frequency"))</pre>
<p>Printing the dataframes now gives the top 10 most common phrases for positive and negative reviews of kitchen items, respectively. For positive reviews, we have the following <span>output</span>:</p>
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>[46383x2] DataFrame

    Phrase       Frequency<br/><br/> 0: tic-tac-toe  10       
<br/> 1: wusthoff     7        
<br/> 2: emperor      7        
<br/> 3: shot_glasses 6        
<br/> 4: pulp         6        
<br/> 5: games        6        
<br/> 6: sentry       6        
<br/> 7: gravel       6        
<br/> 8: the_emperor  5        
<br/> 9: aebleskivers 5        
<br/>    ...          ...      
<br/>    &lt;string&gt;     &lt;int&gt;    </pre></div>
</div>
<p>For negative reviews, we have the following output:</p>
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>[45760x2] DataFrame

    Phrase          Frequency<br/> 0: seeds           9        
<br/> 1: perculator      7        
<br/> 2: probes          7        
<br/> 3: cork            7        
<br/> 4: coffee_tank     5        
<br/> 5: brookstone      5        
<br/> 6: convection_oven 5        
<br/> 7: black_goo       5        
<br/> 8: waring_pro      5        
<br/> 9: packs           5        <br/><br/>    ...             ...      
<br/>    &lt;string&gt;        &lt;int&gt;    </pre></div>
</div>
<p>This completes this example. In the following section, we will cover the other transformation and processing features of Gota in more detail. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example – exploring body mass index data with gonum/plot</h1>
                </header>
            
            <article>
                
<p>In the previous section, we introduced gophernotes and Gota. In this section, we will explore a dataset containing 500 samples of gender, height, and BMI index. We will do this using the <kbd>gonum/plot</kbd> library. This library, which was originally a fork of the 2012 Plotinum library<sup>[15]</sup>, contains several packages that make creating data visualizations in Go much easier<sup>[16]</sup>:</p>
<ul>
<li>The <kbd>plot</kbd> package contains a layout and formatting interface.</li>
<li>The <kbd>plotter</kbd> package abstracts the layout and formatting for common plot types, such as bar charts, scatter plots, and so on.</li>
<li>The <kbd>plotutil</kbd> package contains utility funcs for common plot types.</li>
<li>The <kbd>vg</kbd> package exposes an API for vector graphics and is particularly useful when exporting plots to other software. We will not be covering this package. </li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing gonum and gonum/plot</h1>
                </header>
            
            <article>
                
<p>Regardless of whether you are using the Docker image to run gophernotes as suggested previously or a different method, you will need to use <kbd>gonum/plot</kbd>. To do this, run the <kbd>go get gonum.org/v1/plot/...</kbd> command. If you do not have the <kbd>gonum</kbd> library installed, and you are not using the gophernotes Docker image, you will need to install this separately using the <kbd>go get github.com/gonum/...</kbd> command. </p>
<div class="packt_tip">To open a terminal from Jupyter, open up the web UI to the tree view (the default view) and then click on <strong>New</strong> | <strong>Terminal</strong>. </div>
<p>Note that, despite their names, gonum and gonum/plot are not part of the same repository, so you need to install both separately. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loading the data</h1>
                </header>
            
            <article>
                
<p class="mce-root">If you have cloned the project repository, it will already contain the 500-person BMI dataset in the <kbd>datasets/bmi</kbd> folder. You can also download the dataset yourself from Kaggle<sup>[14]</sup>. The dataset is a single CSV file with the following first few rows:</p>
<pre>Gender,Height,Weight,Index<br/>Male,174,96,4<br/>Male,189,87,2<br/>Female,185,110,4<br/>Female,195,104,3<br/>Male,149,61,3<br/>...</pre>
<p>Like in the previous section, we will use <kbd>io</kbd>/<kbd>ioutil</kbd> to read the file into a byte slice, but this time, we will take advantage of Gota's ReadCSV method (which takes an <kbd>io.Reader</kbd> as an argument) to directly load the data into a dataframe with no preprocessing:</p>
<pre>b, err := ioutil.ReadFile(path)<br/>if err != nil {<br/>  fmt.Println("Error!", err)<br/>}<br/>df := dataframe.ReadCSV(bytes.NewReader(b))</pre>
<p class="mce-root">Inspect the dataframe to make sure that the data has been loaded correctly:</p>
<pre>[500x4] DataFrame

    Gender   Height Weight Index<br/> 0: Male     174    96     4    
<br/> 1: Male     189    87     2    
<br/> 2: Female   185    110    4    
<br/> 3: Female   195    104    3    
<br/> 4: Male     149    61     3    
<br/> 5: Male     189    104    3    
<br/> 6: Male     147    92     5    
<br/> 7: Male     154    111    5    
<br/> 8: Male     174    90     3    
<br/> 9: Female   169    103    4    
<br/>    ...      ...    ...    ...  
<br/>    &lt;string&gt; &lt;int&gt;  &lt;int&gt;  &lt;int&gt;</pre>
<p>Note that the data types of the series have been inferred automatically.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the distributions of the data series</h1>
                </header>
            
            <article>
                
<p><span>A good way to understand each series is to plot a histogram. This will give you an impression of how each series is distributed. Using <kbd>gonum</kbd>/<kbd>plot</kbd>, we will plot histograms for each series. However, before we plot anything, </span>we can quickly access some summary statistics via Gota to gain a rudimentary understanding of the dataset:</p>
<pre>fmt.Println("Minimum", df.Col("Height").Min())<br/>fmt.Println("Maximum", df.Col("Height").Max())<br/>fmt.Println("Mean", df.Col("Height").Mean())<br/>fmt.Println("Median", df.Col("Height").Quantile(0.5))</pre>
<p>This tells us that the heights of the sampled individuals lie between 140 cm and 199 cm, that their mean and median are 169 cm and 170 cm, respectively, and the fact that the mean and the median are so close suggests low skewness—that is, a symmetric distribution. </p>
<p class="mce-root">An even quicker way to achieve this for all columns simultaneously is to use the <kbd>dataframe.Describe</kbd> function. This produces another dataframe that contains summary statistics of each column:</p>
<pre>[7x5] DataFrame

    column   Gender   Height     Weight     Index   <br/> 0: mean     -        169.944000 106.000000 3.748000<br/><br/> 1: stddev   -        16.375261  32.382607  1.355053
<br/> 2: min      Female   140.000000 50.000000  0.000000
<br/> 3: 25%      -        156.000000 80.000000  3.000000
<br/> 4: 50%      -        170.000000 106.000000 4.000000
<br/> 5: 75%      -        184.000000 136.000000 5.000000</pre>
<pre> 6: max      Male     199.000000 160.000000 5.000000
<br/>    &lt;string&gt; &lt;string&gt; &lt;float&gt;    &lt;float&gt;    &lt;float&gt; </pre>
<p>Now, we will visualize the distributions using histograms. First, we will need to convert a column of a Gota dataframe into a plot-friendly <kbd>plotter.Values</kbd> slice. This can be accomplished with the following utility function:</p>
<pre>// SeriesToPlotValues takes a column of a Dataframe and converts it to a gonum/plot/plotter.Values slice.<br/>// Panics if the column does not exist.<br/>func SeriesToPlotValues(df dataframe.DataFrame, col string) plotter.Values {<br/>  rows, _ := df.Dims()<br/>  v := make(plotter.Values, rows)<br/>  s := df.Col(col)<br/>  for i := 0; i &lt; rows; i++ {<br/>    v[i] = s.Elem(i).Float()<br/>  }<br/>  return v<br/>}</pre>
<p>The <kbd>dataframe.Col</kbd> func extracts just the required column from the given dataframe—in our case, a single column. You can also use <kbd>dataframe.Select</kbd>, which takes a slice of strings of column names to return a dataframe containing only the required columns. This can be useful for discarding unnecessary data.</p>
<p>Now, we can use gonum/plot to create a JPEG image of a histogram of a given column with a chosen title:</p>
<pre>// HistogramData returns a byte slice of JPEG data for a histogram of the column with name col in the dataframe df.<br/>func HistogramData(v plotter.Values, title string) []byte {<br/>  // Make a plot and set its title.<br/>  p, err := plot.New()<br/>  if err != nil {<br/>    panic(err)<br/>  }<br/>  p.Title.Text = title<br/>  h, err := plotter.NewHist(v, 10)<br/>  if err != nil {<br/>    panic(err)<br/>  }<br/>  //h.Normalize(1) // Uncomment to normalize the area under the histogram to 1<br/>  p.Add(h)<br/>  w, err := p.WriterTo(5*vg.Inch, 4*vg.Inch, "jpg")<br/>  if err != nil {<br/>    panic(err)<br/>  }<br/>  var b bytes.Buffer<br/>  writer := bufio.NewWriter(&amp;b)<br/>  w.WriteTo(writer)<br/>  return b.Bytes()<br/>}</pre>
<p>To display the resulting plot using gophernotes, use the appropriate method of the display object. In this case, we are producing a JPEG image, so calling <kbd>display.JPEG</kbd> with the byte slice that was produced by the preceding code will display the plot in the output cell. The full code input cell would be as follows:</p>
<pre>Display.JPEG(HistogramData(SeriesToPlotValues(df, "Age"), "Age Histogram"))</pre>
<p>In general, the steps to create a new plot from one of gonum's built-in plotters are as follows:</p>
<ol>
<li>Create a new plot with <kbd>plot.New()</kbd> <span>– </span>this is like a canvas that the plot will live on.</li>
<li>Set any plot attributes, such as its title.</li>
<li>Create a new plotter based on one of the available types (<kbd>BarChart</kbd>, <kbd>BoxPlot</kbd>, <kbd>ColorBar</kbd>, <kbd>Contour</kbd>, <kbd>HeatMap</kbd>, <kbd>Histogram</kbd>, <kbd>Line</kbd>, <kbd>QuartPlot</kbd>, <kbd>Sankey</kbd>, or <kbd>Scatter</kbd>).</li>
<li>Set any plotter attributes and add the plotter to the plot by calling its <kbd>Add</kbd> method.</li>
<li>If you wish to display the plot via gophernotes, use the <kbd>WriterTo</kbd> method and a byte buffer to output the plot data as a slice of bytes that can be passed to the built-in display object. Otherwise, use <kbd>p.Save</kbd> to save the image to a file.</li>
</ol>
<div class="packt_infobox">If, instead of displaying the image in gophernotes, you wish to save it, you can do this with the plot's <kbd>Save</kbd> method. For example, <kbd>p.Save(5*vg.Inch, 4*vg.Inch, title + ".png")</kbd> will save the plot to a 5" x 4" PNG file. </div>
<p>The resulting histograms for the 500-person weight/height/BMI dataset are as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-433 image-border" src="assets/28d2f746-280a-4d9c-aaa8-c16831aaae93.png" style="width:124.83em;height:34.67em;"/></p>
<p>In the following example, we will not just load and visualize data, but also transform it to make it more suitable for use with an ML algorithm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example – preprocessing data with Gota</h1>
                </header>
            
            <article>
                
<p class="mce-root">The quality and speed of the ML algorithm training process depends on the quality of the input data. While many algorithms are robust to irrelevant columns and data that is not normalized, some are not. For example, many models requires data inputs to be normalized to lie between 0 and 1. In this section, we will look at some quick and easy ways to preprocess data with Gota. For these examples, we will be using a dataset containing 1,035 records of the height (inch) and weight (lbs) of major league baseball players<sup>[17]</sup>. The dataset, as described on the UCLA website, consists of the following features:</p>
<ul>
<li><kbd>Name</kbd>: Player name</li>
<li><kbd>Team</kbd>: The baseball team that the player was a member of</li>
<li><kbd>Position</kbd>: The player's position</li>
<li><kbd>Height (inches)</kbd>: Player height</li>
<li><kbd>Weight (pounds)</kbd>: Player weight in pounds</li>
<li><kbd>Age</kbd>: Player age at the time of recording</li>
</ul>
<p>For the purposes of this exercise, we will preprocess the data in the following manner:</p>
<ul>
<li>Remove the name and team column</li>
<li>Convert the height and weight columns into the float type</li>
<li>Filter out players with a weight greater than or equal to 260 pounds</li>
</ul>
<ul>
<li>Normalize the height and weight columns</li>
<li>Divide the data into training and validation subsets with approximately 70% of rows in the training subset and 30% in the validation subset</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loading the data into Gota</h1>
                </header>
            
            <article>
                
<p>The dataset is supplied as an HTML table on the UCLA website<sup>[17]</sup>. In the companion repository to this book, you will find a CSV version. To quickly convert the HTML table yourself into CSV format without needing to write any code, first highlight the table and copy and paste this into a spreadsheet program such as Microsoft Excel. Then, save the spreadsheet as a CSV file. Open this file in a text editor to ensure there are no artefacts or extraneous rows in the file.</p>
<p>Loading the dataset is done using the <kbd>dataframe.ReadCSV</kbd> method. Inspecting the dataframe produces the following output:</p>
<pre>[1034x6] DataFrame

    Name            Team     Position       Height(inches) Weight(pounds) ...<br/><br/> 0: Adam_Donachie   BAL      Catcher        74             180            ...
<br/> 1: Paul_Bako       BAL      Catcher        74             215            ...
<br/> 2: Ramon_Hernandez BAL      Catcher        72             210            ...
<br/> 3: Kevin_Millar    BAL      First_Baseman  72             210            ...
<br/> 4: Chris_Gomez     BAL      First_Baseman  73             188            ...
<br/> 5: Brian_Roberts   BAL      Second_Baseman 69             176            ...
<br/> 6: Miguel_Tejada   BAL      Shortstop      69             209            ...
<br/> 7: Melvin_Mora     BAL      Third_Baseman  71             200            ...
<br/> 8: Aubrey_Huff     BAL      Third_Baseman  76             231            ...
<br/> 9: Adam_Stern      BAL      Outfielder     71             180            ...
<br/>    ...             ...      ...            ...            ...            ...
<br/>    &lt;string&gt;        &lt;string&gt; &lt;string&gt;       &lt;int&gt;          &lt;int&gt;          ...
<br/>Not Showing: Age &lt;float&gt;</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Removing and renaming columns</h1>
                </header>
            
            <article>
                
<p>For this exercise, we have decided that we do not need the <kbd>Name</kbd> or the <kbd>Team</kbd> columns. We can use the dataframe's <kbd>Select</kbd> method to specify a slice of strings of column names that we wish to keep:</p>
<pre>df = df.Select([]string{"Position", "Height(inches)", "Weight(pounds)", "Age"})</pre>
<p>While we are at it, the <kbd>Height</kbd> and <kbd>Weight</kbd> columns should be renamed to remove the units from the column names. This can be achieved with the <kbd>Rename</kbd> method:</p>
<pre>df = df.Rename("Height", "Height(inches)")<br/>df = df.Rename("Weight", "Weight(pounds)")</pre>
<p>The resulting dataset is as follows:</p>
<pre>[1034x4] DataFrame

    Position       Height Weight Age      <br/><br/> 0: Catcher        74     180    22.990000
<br/> 1: Catcher        74     215    34.690000
<br/> 2: Catcher        72     210    30.780000
<br/> 3: First_Baseman  72     210    35.430000
<br/> 4: First_Baseman  73     188    35.710000
<br/> 5: Second_Baseman 69     176    29.390000
<br/> 6: Shortstop      69     209    30.770000
<br/> 7: Third_Baseman  71     200    35.070000
<br/> 8: Third_Baseman  76     231    30.190000
<br/> 9: Outfielder     71     180    27.050000
<br/>    ...            ...    ...    ...      
<br/>    &lt;string&gt;       &lt;int&gt;  &lt;int&gt;  &lt;float&gt;  </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Converting a column into a different type</h1>
                </header>
            
            <article>
                
<p>Our dataframe now has the correct columns with more concise names. However, the height and weight columns are of the <kbd>int</kbd> type, whereas we need them to be of the <kbd>float</kbd> type so that we can correctly normalize their values. The easiest way to do this is to add this as a <kbd>LoadOption</kbd> when first loading the data into a dataframe. Namely, <kbd>func WithTypes(coltypes map[string]series.Type) LoadOption</kbd> accepts a map of column names to series types, and we can use this to perform the conversion at load time. </p>
<p>However, suppose that we have not done this. In that case, we convert the column type by replacing the column with a new series that has the correct type. To generate this series, we can use the <kbd>series.New</kbd> method, together with <kbd>df.Col</kbd> to isolate the column of interest. For example, to produce a series of floats from the current height series, we can use the following code:</p>
<pre>heightFloat := series.New(df.Col("Height"), series.Float, "Height")</pre>
<p>To replace the column, we can use the <kbd>Mutate</kbd> method:</p>
<pre>df.Mutate(heightFloat)</pre>
<p>Doing this for both the <kbd>Height</kbd> and the <kbd>Weight</kbd> columns now produces the following output:</p>
<pre>[1034x4] DataFrame

    Position       Height Weight Age      <br/> 0: Catcher        74.00000     180.00000    22.990000<br/><br/> 1: Catcher        74.00000     215.00000    34.690000
<br/> 2: Catcher        72.00000     210.00000    30.780000
<br/> 3: First_Baseman  72.00000     210.00000    35.430000
<br/> 4: First_Baseman  73.00000     188.00000    35.710000
<br/> 5: Second_Baseman 69.00000     176.00000    29.390000
<br/> 6: Shortstop      69.00000     209.00000    30.770000
<br/> 7: Third_Baseman  71.00000     200.00000    35.070000
<br/> 8: Third_Baseman  76.00000     231.00000    30.190000
<br/> 9: Outfielder     71.00000     180.00000    27.050000
<br/>    ...            ...    ...    ...      
<br/>    &lt;string&gt;       &lt;float&gt;  &lt;float&gt;  &lt;float&gt; </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Filtering out unwanted data</h1>
                </header>
            
            <article>
                
<p>Suppose that, after exploring the data, we do not wish to keep samples where the player weight is greater than or equal to 260 pounds. This could be because there are not enough samples of heavier players, and so any analysis would not be representative of the player population as a whole. Such players could be called <strong>outliers</strong> in regards to the current dataset. </p>
<div class="packt_tip">You can find the reference (Godocs) for the Gota library at <a href="https://godoc.org/github.com/kniren/gota">https://godoc.org/github.com/kniren/gota</a>. </div>
<p>Gota dataframes can be filtered using the <kbd>Filter</kbd> func. This accepts a <kbd>dataframe.F struct</kbd>, which consists of the target column, a comparator, and a value, such as <kbd>{"Column", series.Eq, 1}</kbd>, which would match only rows where <kbd>Column</kbd> was equal to <kbd>1</kbd>. The available comparators are as follows:</p>
<ul>
<li><kbd>series.Eq</kbd>: Keeps only rows that are equal to the given value</li>
<li><kbd>series.Neq</kbd>: Keeps only rows that are not equal to the given value</li>
<li><kbd>series.Greater</kbd>: Keeps only rows that are greater than the given value</li>
</ul>
<ul>
<li><kbd>series.GreaterEq</kbd>: Keeps only rows that are greater than or equal to the given value</li>
<li><kbd>series.Less</kbd>: Keeps only rows that are less than the given value</li>
<li><kbd>series.LessEq</kbd>: Keeps only rows that are less than or equal to the given value</li>
</ul>
<div class="packt_tip">The <kbd>series.Comparator</kbd> type is an alias for a string. These strings are the same as the ones that are used in the Go language itself. For example, <kbd>series.Neq</kbd> is equivalent to <kbd>"!="</kbd>.</div>
<p>For this exercise, we will apply the series. We will use the <kbd>less</kbd> filter in order to remove rows where the weight is greater than or equal to 260 pounds:</p>
<pre>df = df.Filter(dataframe.F{"Weight", "&lt;", 260})</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Normalizing the Height, Weight, and Age columns</h1>
                </header>
            
            <article>
                
<p class="mce-root">Data normalization, also known as feature scaling, is the process of transforming a group of independent variables to map them onto the same range. There are several methods to achieve this:</p>
<ul>
<li><strong>Rescaling</strong> <strong>(min/max normalization)</strong>: This will linearly map the variable range onto the [0,1] range, where the minimum value of the series will map to 0 and its maximum will map to 1. This is achieved by applying the following formula:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f30229f7-b4a4-48b2-b81a-2e4b65e43a02.png" style="width:10.08em;height:3.00em;"/></p>
<ul>
<li><strong>Mean normalization</strong>: This will map the variable range if we apply the following formula:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/21ff6a5a-b923-4483-8996-bc613230a6f8.png" style="width:10.50em;height:3.08em;"/></p>
<ul>
<li><strong>Standardization</strong> <strong>(z-score normalization)</strong>: This very common method of normalization for ML applications uses the mean and standard deviation to transform the series of values into their z-scores, that is, how many standard deviations from the mean the data point lies. This is done by computing the mean and standard deviation of the series and then applying the following formula:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/6e6e18c2-e0bd-427e-853a-3a88497825f3.png" style="width:6.50em;height:2.75em;"/></p>
<div class="packt_infobox">Note that this is not guaranteed to map the variable onto a closed range. </div>
<p>Rescaling can be implemented with the following utility func:</p>
<pre>// rescale maps the given column values onto the range [0,1]<br/>func rescale(df dataframe.DataFrame, col string) dataframe.DataFrame {<br/>  s := df.Col(col)<br/>  min := s.Min()<br/>  max := s.Max()<br/>  v := make([]float64, s.Len(), s.Len())<br/>  for i := 0; i &lt; s.Len(); i++ {<br/>    v[i] = (s.Elem(i).Float() - min) / (max - min)<br/>  }<br/>  rs := series.Floats(v)<br/>  rs.Name = col<br/>  return df.Mutate(rs)<br/>}</pre>
<p>Mean normalization can be implemented with the following utility function:</p>
<pre>// meanNormalise maps the given column values onto the range [-1,1] by subtracting mean and dividing by max - min<br/>func meanNormalise(df dataframe.DataFrame, col string) dataframe.DataFrame {<br/>  s := df.Col(col)<br/>  min := s.Min()<br/>  max := s.Max()<br/>  mean := s.Mean()<br/>  v := make([]float64, s.Len(), s.Len())<br/>  for i := 0; i &lt; s.Len(); i++ {<br/>    v[i] = (s.Elem(i).Float() - mean) / (max - min)<br/>  }<br/>  rs := series.Floats(v)<br/>  rs.Name = col<br/>  return df.Mutate(rs)<br/>}</pre>
<p>Standardization can be implemented with the following utility func:</p>
<pre>// meanNormalise maps the given column values onto the range [-1,1] by subtracting mean and dividing by max - min<br/>func standardise(df dataframe.DataFrame, col string) dataframe.DataFrame {<br/>  s := df.Col(col)<br/>  std := s.StdDev()<br/>  mean := s.Mean()<br/>  v := make([]float64, s.Len(), s.Len())<br/>  for i := 0; i &lt; s.Len(); i++ {<br/>    v[i] = (s.Elem(i).Float() - mean) / std<br/>  }<br/>  rs := series.Floats(v)<br/>  rs.Name = col<br/>  return df.Mutate(rs)<br/>}</pre>
<p>For this example, we will apply rescaling to the <kbd>Height</kbd> and <kbd>Weight</kbd> columns with the following code:</p>
<pre>df = rescale(df, "Height")<br/>df = rescale(df, "Weight")</pre>
<p>The result is as follows. Note that the values of the <kbd>Height</kbd> and <kbd>Weight</kbd> columns now lie between 0 and 1, as intended:</p>
<pre>[1034x4] DataFrame

    Position       Height   Weight   Age      <br/><br/> 0: Catcher        0.437500 0.214286 22.990000
<br/> 1: Catcher        0.437500 0.464286 34.690000
<br/> 2: Catcher        0.312500 0.428571 30.780000
<br/> 3: First_Baseman  0.312500 0.428571 35.430000
<br/> 4: First_Baseman  0.375000 0.271429 35.710000
<br/> 5: Second_Baseman 0.125000 0.185714 29.390000
<br/> 6: Shortstop      0.125000 0.421429 30.770000
<br/> 7: Third_Baseman  0.250000 0.357143 35.070000
<br/> 8: Third_Baseman  0.562500 0.578571 30.190000
<br/> 9: Outfielder     0.250000 0.214286 27.050000
<br/>    ...            ...      ...      ...      
<br/>    &lt;string&gt;       &lt;float&gt;  &lt;float&gt;  &lt;float&gt; </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sampling to obtain training/validation subsets</h1>
                </header>
            
            <article>
                
<p>When training an ML algorithm, it is useful to reserve a portion of the dataset for validation. This is used to test the generalization of the model to previously unseen data and thus to ensure its usefulness when presented with real-life data that isn't part of the training set. Without the validation step, it is not possible to say whether a model will have good predictive power.</p>
<p>While there are no accepted conventions regarding how much of the dataset to reserve for validation, a fraction between 10% and 30% is common. Research that has been conducted into how much of the dataset to reserve for validation concluded that the more adjustable parameters a model has, the less the fraction of the data needs to be reserved for validation<sup>[18]</sup>. For this exercise, we will divide our MLB dataset into two subsets: a training subset containing approximately 70% of samples, and a validation subset containing 30% of samples. There are two ways of doing this:</p>
<ul>
<li>Select the first 70% of rows to form part of the training subset and the second 30% to form part of the validation subset</li>
<li>Select a random 70% of samples to form part of the training subset and use the remainder for the validation subset</li>
</ul>
<p>In general, it is better to avoid deterministic sampling to ensure that both subsets are representative of the overall population. To implement random sampling, we will use the <kbd>math/rand</kbd> package to produce random indices and combine this with Gota's <kbd>dataframe.Subset</kbd> method. The first step is to generate a random permutation of the indices of the dataframe:</p>
<pre>rand.Perm(df.Nrow())</pre>
<p>Now, we will take the first 70% of this slice for training and the remaining elements for validation, resulting in the following utility:</p>
<pre>// split splits the dataframe into training and validation subsets. valFraction (0 &lt;= valFraction &lt;= 1) of the samples<br/>// are reserved for validation and the rest are for training.<br/>func Split(df dataframe.DataFrame, valFraction float64) (training dataframe.DataFrame, validation dataframe.DataFrame) {<br/>  perm := rand.Perm(df.Nrow())<br/>  cutoff := int(valFraction * float64(len(perm)))<br/>  training = df.Subset(perm[:cutoff])<br/>  validation = df.Subset(perm[cutoff:len(perm)])<br/>  return training, validation<br/>}</pre>
<p>Applying this to our dataframe with <kbd>split(df, 0.7)</kbd> produces the following output. The first dataframe is the training subset, while the second is the validation subset:</p>
<pre>[723x4] DataFrame

    Position         Height   Weight   Age      <br/> 0: Relief_Pitcher   0.500000 0.285714 25.640000<br/><br/> 1: Starting_Pitcher 0.500000 0.500000 33.410000<br/><br/> 2: Second_Baseman   0.375000 0.235714 28.200000
<br/> 3: Relief_Pitcher   0.562500 0.392857 33.310000
<br/> 4: Outfielder       0.187500 0.250000 27.450000
<br/> 5: Relief_Pitcher   0.500000 0.042857 27.320000
<br/> 6: Relief_Pitcher   0.562500 0.428571 40.970000
<br/> 7: Second_Baseman   0.250000 0.357143 33.150000
<br/> 8: Outfielder       0.312500 0.071429 25.180000
<br/> 9: Relief_Pitcher   0.562500 0.321429 29.990000
<br/>    ...              ...      ...      ...      
<br/>    &lt;string&gt;         &lt;float&gt;  &lt;float&gt;  &lt;float&gt;  
<br/> [310x4] DataFrame
<br/>    Position         Height   Weight   Age      
<br/> 0: Relief_Pitcher   0.375000 0.285714 25.080000
<br/> 1: Relief_Pitcher   0.437500 0.285714 28.310000
<br/> 2: Outfielder       0.437500 0.357143 34.140000
<br/> 3: Shortstop        0.187500 0.285714 25.080000
<br/> 4: Starting_Pitcher 0.500000 0.428571 32.550000
<br/> 5: Outfielder       0.250000 0.250000 30.550000
<br/> 6: Starting_Pitcher 0.500000 0.357143 28.480000
<br/> 7: Third_Baseman    0.250000 0.285714 30.960000
<br/> 8: Catcher          0.250000 0.421429 30.670000
<br/> 9: Third_Baseman    0.500000 0.428571 25.480000
<br/>    ...              ...      ...      ...      
<br/>    &lt;string&gt;         &lt;float&gt;  &lt;float&gt;  &lt;float&gt;  </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Encoding data with categorical variables</h1>
                </header>
            
            <article>
                
<p>In the preceding dataframe, the <kbd>Position</kbd> column is a string. Suppose we want an ML algorithm to use this input, because, say, we are attempting to predict the weight of the player and players in certain positions tend to have different body composition. In this case, we need to <strong>encode</strong> the string to a numerical value that can be used by the algorithm. </p>
<p>The naive solution is to determine the set of all player positions and assign an increasing integer to each member of the set. For example, we might end up with the <kbd>{Relief_Pitcher, Starting_Pitcher, Shortstop, Outfielder,...}</kbd> set, whereupon we would assign <kbd>0</kbd> to <kbd>Relief_Pitcher</kbd>, <kbd>1</kbd> to <kbd>Starting_Pitcher</kbd>, <kbd>2</kbd> to <kbd>Shortstop</kbd>, and so on. However, the flaw of this approach is in how the numbers are assigned, because it gives importance to the order of the categories where none exist. Suppose that a step of the ML algorithm computes a mean across categories. Therefore, it might conclude that <kbd>Starting_Pitcher</kbd> is the mean of <kbd>Relief_Pitcher</kbd> and <kbd>Shortstop</kbd>! Other types of algorithms might infer correlations where none exist. </p>
<p> </p>
<p><span>To solve this issue, we can use </span><strong>one-hot encoding</strong><span>. This type of encoding will split a categorical column with N possible values into N columns. Each of the columns, which correspond to one of the categories, will have the value <kbd>1</kbd>, where that input belongs to the given column, and <kbd>0</kbd> otherwise. This also allows for the scenario where an input sample may belong to multiple categories. </span></p>
<p>The steps to generate a one-hot encoding for a given column with Gota are as follows:</p>
<ol>
<li>Enumerate the unique values of the categorical column</li>
<li>Create a new series for each unique value, mapping each row to <kbd>1</kbd> if it belongs to this category and <kbd>0</kbd> otherwise</li>
<li>Mutate the original dataframe by adding the series created in <em>step 2</em> and removing the original column</li>
</ol>
<p>Enumerating the unique values can be done easily using a map:</p>
<pre>func UniqueValues(df dataframe.DataFrame, col string) []string {<br/>  var ret []string<br/>  m := make(map[string]bool)<br/>  for _, val := range df.Col(col).Records() {<br/>    m[val] = true<br/>  }<br/>  for key := range m {<br/>    ret = append(ret, key)<br/>  }<br/>  return ret<br/>}</pre>
<p>Note that this makes use of the <kbd>series.Records</kbd> method to return the values of a given column as a slice of strings. Also, note that the order in which the values are returned will not necessarily be the same every time. Running this func on our dataframe with <kbd>UniqueValues(df, "Position")</kbd> yields the following unique values:</p>
<pre>[Shortstop Outfielder Starting_Pitcher Relief_Pitcher Second_Baseman First_Baseman Third_Baseman Designated_Hitter Catcher]</pre>
<p>The second step is to iterate over the dataframe, creating new series as we go along:</p>
<pre>func OneHotSeries(df dataframe.DataFrame, col string, vals []string) []series.Series {<br/>  m := make(map[string]int)<br/>  s := make([]series.Series, len(vals), len(vals))<br/>  //cache the mapping for performance reasons<br/>  for i := range vals {<br/>    m[vals[i]] = i<br/>  }<br/>  for i := range s {<br/>    vals := make([]int, df.Col(col).Len(), df.Col(col).Len())<br/>    for j, val := range df.Col(col).Records() {<br/>      if i == m[val] {<br/>        vals[j] = 1<br/>      }<br/>    }<br/>    s[i] = series.Ints(vals)<br/>  }<br/>  for i := range vals {<br/>    s[i].Name = vals[i]<br/>  }<br/>  return s<br/>}</pre>
<p>This func will return one series for each unique value of the categorical variable. These series will have the names of the categories. In our case, we can call it with <kbd>OneHotSeries(df, "Position", UniqueValues(df, "Position"))</kbd>. Now, we will mutate our original dataframe and drop the <kbd>Position</kbd> column:</p>
<pre>ohSeries := OneHotSeries(df, "Position", UniqueValues(df, "Position"))<br/>for i := range ohSeries {<br/>  df = df.Mutate(ohSeries[i])<br/>}</pre>
<p>Printing <kbd>df</kbd> yields the following result:</p>
<pre>[1034x13] DataFrame

    Position       Height   Weight   Age       Shortstop Catcher ...<br/><br/> 0: Catcher        0.437500 0.214286 22.990000 0         1       ...
<br/> 1: Catcher        0.437500 0.464286 34.690000 0         1       ...
<br/> 2: Catcher        0.312500 0.428571 30.780000 0         1       ...
<br/> 3: First_Baseman  0.312500 0.428571 35.430000 0         0       ...
<br/> 4: First_Baseman  0.375000 0.271429 35.710000 0         0       ...
<br/> 5: Second_Baseman 0.125000 0.185714 29.390000 0         0       ...
<br/> 6: Shortstop      0.125000 0.421429 30.770000 1         0       ...
<br/> 7: Third_Baseman  0.250000 0.357143 35.070000 0         0       ...
<br/> 8: Third_Baseman  0.562500 0.578571 30.190000 0         0       ...
<br/> 9: Outfielder     0.250000 0.214286 27.050000 0         0       ...
<br/>    ...            ...      ...      ...       ...       ...     ...
<br/>    &lt;string&gt;       &lt;float&gt;  &lt;float&gt;  &lt;float&gt;   &lt;int&gt;     &lt;int&gt;   ...
<br/><br/>Not Showing: Second_Baseman &lt;int&gt;, Outfielder &lt;int&gt;, Designated_Hitter &lt;int&gt;,
<br/>Starting_Pitcher &lt;int&gt;, Relief_Pitcher &lt;int&gt;, First_Baseman &lt;int&gt;, Third_Baseman &lt;int&gt;</pre>
<p>To conclude, just drop the <kbd>Position</kbd> column using <kbd>df = df.Drop("Position")</kbd>. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we covered how to set up a development environment for Go that is optimized for ML applications. We explained how to install an interactive environment, Jupyter, to accelerate data exploration and visualization using libraries such as Gota and gonum/plot.</p>
<p class="mce-root">We also introduced some basic data processing steps, such as filtering outliers, removing unnecessary columns, and normalization. Finally, we covered sampling. This chapter took the first few steps in the ML life cycle: data acquisition, exploration, and preparation. Now that you have read this chapter, you have learned how to load data into a Gota dataframe, how to use the dataframe and series packages to process and prepare the data into a format that is required by your chosen algorithm, and how to visualize it with gonum's plot package. You have also learned about different ways of normalizing the data, which is an important step for improving the accuracy and speed of many ML algorithms. </p>
<p>In the next chapter, we will introduce supervised learning algorithms and exemplify how to choose an ML algorithm, train it, and validate its predictive power on previously unseen data. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further readings</h1>
                </header>
            
            <article>
                
<ol>
<li><em>Software Development Waste</em>. Todd Sedano and Paul Ralph. ICSE '17 Proceedings of the 39th International Conference on Software Engineering. Pages 130-140. </li>
<li>See the official Go installation instructions at <a href="https://golang.org/doc/install">https://golang.org/doc/install</a>. Retrieved February 19th, 2019.</li>
<li><a href="https://jupyter.org/about">https://jupyter.org/about</a>. Retrieved February 19th, 2019.</li>
<li><a href="https://github.com/jupyter/jupyter/wiki/Jupyter-kernels">https://github.com/jupyter/jupyter/wiki/Jupyter-kernels</a>. Retrieved February 19th, 2019.</li>
<li>For further instructions, see <a href="https://help.github.com/articles/working-with-jupyter-notebook-files-on-github/">https://help.github.com/articles/working-with-jupyter-notebook-files-on-github/</a>. Retrieved February 19th, 2019.</li>
<li><a href="https://colab.research.google.com">https://colab.research.google.com</a>. Retrieved February 19th, 2019.</li>
<li><a href="https://nbviewer.jupyter.org/">https://nbviewer.jupyter.org/</a>. Retrieved February 19th, 2019. </li>
<li><a href="https://jupyter.org/hub">https://jupyter.org/hub</a>. Retrieved February 19th, 2019.</li>
<li><a href="https://github.com/jupyter/nbconvert">https://github.com/jupyter/nbconvert</a>. Retrieved February 19th, 2019.</li>
<li>For Docker installation instructions, see <a href="https://docs.docker.com/install/">https://docs.docker.com/install/</a> for Linux and <a href="https://docs.docker.com/docker-for-windows/install/">https://docs.docker.com/docker-for-windows/install/</a> for Windows. Retrieved February 19th, 2019.</li>
<li><span>John Blitzer, Mark Dredze, Fernando Pereira. Biographies, Bollywood, <em>Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification.</em> Association of Computational Linguistics (ACL), 2007.</span></li>
<li><a href="https://github.com/go-gota/gota">https://github.com/go-gota/gota</a>. Retrieved February 19th, 2019.</li>
<li><a href="https://godoc.org/sort#Interface">https://godoc.org/sort#Interface</a>. Retrieved February 19th, 2019. </li>
<li><a href="https://www.kaggle.com/yersever/500-person-gender-height-weight-bodymassindex/version/2">https://www.kaggle.com/yersever/500-person-gender-height-weight-bodymassindex/version/2</a>. Retrieved February 20th, 2019.</li>
<li><a href="https://code.google.com/archive/p/plotinum/">https://code.google.com/archive/p/plotinum/</a>. Retrieved February 20th, 2019.</li>
<li><a href="https://github.com/gonum/plot">https://github.com/gonum/plot</a>. Retrieved February 20th, 2019.</li>
<li><a href="http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_MLB_HeightsWeights">http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_MLB_HeightsWeights</a>. Retrieved February 20th, 2019.</li>
<li>Guyon, Isabelle. 1996. <em>A Scaling Law for the Validation-Set Training-Set Size Ratio</em>. AT&amp;T Bell Lab. 1. </li>
</ol>


            </article>

            
        </section>
    </body></html>