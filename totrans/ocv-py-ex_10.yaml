- en: Chapter 10. Object Recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to learn about object recognition and how we can
    use it to build a visual search engine. We will discuss feature detection, building
    feature vectors, and using machine learning to build a classifier. We will learn
    how to use these different blocks to build an object recognition system.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will know:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the difference between object detection and object recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is a dense feature detector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is a visual dictionary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to build a feature vector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is supervised and unsupervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are Support Vector Machines and how to use them to build a classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to recognize an object in an unknown image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object detection versus object recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we proceed, we need to understand what we are going to discuss in this
    chapter. You must have frequently heard the terms "object detection" and "object
    recognition", and they are often mistaken to be the same thing. There is a very
    distinct difference between the two.
  prefs: []
  type: TYPE_NORMAL
- en: Object detection refers to detecting the presence of a particular object in
    a given scene. We don't know what the object might be. For instance, we discussed
    face detection in [Chapter 4](ch04.html "Chapter 4. Detecting and Tracking Different
    Body Parts"), *Detecting and Tracking Different Body Parts*. During the discussion,
    we only detected whether or not a face is present in the given image. We didn't
    recognize the person! The reason we didn't recognize the person is because we
    didn't care about that in our discussion. Our goal was to find the location of
    the face in the given image. Commercial face recognition systems employ both face
    detection and face recognition to identify a person. First, we need to locate
    the face, and then, run the face recognizer on the cropped face.
  prefs: []
  type: TYPE_NORMAL
- en: Object recognition is the process of identifying an object in a given image.
    For instance, an object recognition system can tell you if a given image contains
    a dress or a pair of shoes. In fact, we can train an object recognition system
    to identify many different objects. The problem is that object recognition is
    a really difficult problem to solve. It has eluded computer vision researchers
    for decades now, and has become the holy grail of computer vision. Humans can
    identify a wide variety of objects very easily. We do it everyday and we do it
    effortlessly, but computers are unable to do it with that kind of accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider the following image of a latte cup:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Object detection versus object recognition](img/B04554_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'An object detector will give you the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Object detection versus object recognition](img/B04554_10_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, consider the following image of a teacup:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Object detection versus object recognition](img/B04554_10_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If you run it through an object detector, you will see the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Object detection versus object recognition](img/B04554_10_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, the object detector detects the presence of the teacup, but
    nothing more than that. If you train an object recognizer, it will give you the
    following information, as shown in the image below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Object detection versus object recognition](img/B04554_10_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If you consider the second image, it will give you the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Object detection versus object recognition](img/B04554_10_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, a perfect object recognizer would give you all the information
    associated with that object. An object recognizer functions more accurately if
    it knows where the object is located. If you have a big image and the cup is a
    small part of it, then the object recognizer might not be able to recognize it.
    Hence, the first step is to detect the object and get the bounding box. Once we
    have that, we can run an object recognizer to extract more information.
  prefs: []
  type: TYPE_NORMAL
- en: What is a dense feature detector?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to extract a meaningful amount of information from the images, we
    need to make sure our feature extractor extracts features from all the parts of
    a given image. Consider the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is a dense feature detector?](img/B04554_10_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If you extract features using a feature extractor, it will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is a dense feature detector?](img/B04554_10_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If you use `Dense` detector, it will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is a dense feature detector?](img/B04554_10_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can control the density as well. Let''s make it sparse:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is a dense feature detector?](img/B04554_10_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'By doing this, we can make sure that every single part in the image is processed.
    Here is the code to do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This gives us close control over the amount of information that gets extracted.
    When we use a SIFT detector, some parts of the image are neglected. This works
    well when we are dealing with the detection of prominent features, but when we
    are building an object recognizer, we need to evaluate all parts of the image.
    Hence, we use a dense detector and then extract features from those keypoints.
  prefs: []
  type: TYPE_NORMAL
- en: What is a visual dictionary?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be using the **Bag of Words** model to build our object recognizer.
    Each image is represented as a histogram of visual words. These visual words are
    basically the **N** centroids built using all the keypoints extracted from training
    images. The pipeline is as shown in the image that follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is a visual dictionary?](img/B04554_10_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: From each training image, we detect a set of keypoints and extract features
    for each of those keypoints. Every image will give rise to a different number
    of keypoints. In order to train a classifier, each image must be represented using
    a fixed length feature vector. This feature vector is nothing but a histogram,
    where each bin corresponds to a visual word.
  prefs: []
  type: TYPE_NORMAL
- en: When we extract all the features from all the keypoints in the training images,
    we perform K-Means clustering and extract N centroids. This N is the length of
    the feature vector of a given image. Each image will now be represented as a histogram,
    where each bin corresponds to one of the 'N' centroids. For simplicity, let's
    say that N is set to 4\. Now, in a given image, we extract **K** keypoints. Out
    of these K keypoints, some of them will be closest to the first centroid, some
    of them will be closest to the second centroid, and so on. So, we build a histogram
    based on the closest centroid to each keypoint. This histogram becomes our feature
    vector. This process is called **vector quantization**.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand vector quantization, let''s consider an example. Assume we have
    an image and we''ve extracted a certain number of feature points from it. Now
    our goal is to represent this image in the form of a feature vector. Consider
    the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is a visual dictionary?](img/B04554_10_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, we have 4 centroids. Bear in mind that the points shown in
    the figures represent the feature space and not the actual geometric locations
    of those feature points in the image. It is shown this way in the preceding figure
    so that it''s easy to visualize. Points from many different geometric locations
    in an image can be close to each other in the feature space. Our goal is to represent
    this image as a histogram, where each bin corresponds to one of these centroids.
    This way, no matter how many feature points we extract from an image, it will
    always be converted to a fixed length feature vector. So, we "round off" each
    feature point to its nearest centroid, as shown in the next image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is a visual dictionary?](img/B04554_10_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If you build a histogram for this image, it will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is a visual dictionary?](img/B04554_10_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, if you consider a different image with a different distribution of feature
    points, it will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is a visual dictionary?](img/B04554_10_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The clusters would look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is a visual dictionary?](img/B04554_10_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The histogram would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is a visual dictionary?](img/B04554_10_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the histograms are very different for the two images even though
    the points seem to be randomly distributed. This is a very powerful technique
    and it's widely used in computer vision and signal processing. There are many
    different ways to do this and the accuracy depends on how fine-grained you want
    it to be. If you increase the number of centroids, you will be able to represent
    the image better, thereby increasing the uniqueness of your feature vector. Having
    said that, it's important to mention that you cannot just keep increasing the
    number of centroids indefinitely. If you do that, it will become too noisy and
    lose its power.
  prefs: []
  type: TYPE_NORMAL
- en: What is supervised and unsupervised learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are familiar with the basics of machine learning, you will certainly
    know what supervised and unsupervised learning is all about. To give a quick refresher,
    supervised learning refers to building a function based on labeled samples. For
    example, if we are building a system to separate dress images from footwear images,
    we first need to build a database and label it. We need to tell our algorithm
    what images correspond to dresses and what images correspond to footwear. Based
    on this data, the algorithm will learn how to identify dresses and footwear so
    that when an unknown image comes in, it can recognize what's inside that image.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning is the opposite of what we just discussed. There is no
    labeled data available here. Let's say we have a bunch of images, and we just
    want to separate them into three groups. We don't know what the criteria will
    be. So, an unsupervised learning algorithm will try to separate the given set
    of data into 3 groups in the best possible way. The reason we are discussing this
    is because we will be using a combination of supervised and unsupervised learning
    to build our object recognition system.
  prefs: []
  type: TYPE_NORMAL
- en: What are Support Vector Machines?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Support Vector Machines** (**SVM**) are supervised learning models that are
    very popular in the realm of machine learning. SVMs are really good at analyzing
    labeled data and detecting patterns. Given a bunch of data points and the associated
    labels, SVMs will build the separating hyperplanes in the best possible way.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Wait a minute, what are "hyperplanes"? To understand that, let''s consider
    the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What are Support Vector Machines?](img/B04554_10_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the points are being separated by line boundaries that are equidistant
    from the points. This is easy to visualize in 2 dimensions. If it were in 3 dimensions,
    the separators would be planes. When we build features for images, the length
    of the feature vectors is usually in the six-digit range. So, when we go to such
    a high dimensional space, the equivalent of "lines" would be hyperplanes. Once
    the hyperplanes are formulated, we use this mathematical model to classify unknown
    data, based on where it falls on this map.
  prefs: []
  type: TYPE_NORMAL
- en: What if we cannot separate the data with simple straight lines?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There is something called the **kernel trick** that we use in SVMs. Consider
    the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What if we cannot separate the data with simple straight lines?](img/B04554_10_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As we can see, we cannot draw a simple straight line to separate the red points
    from the blue points. Coming up with a nice curvy boundary that will satisfy all
    the points is prohibitively expensive. SVMs are really good at drawing "straight
    lines". So, what''s our answer here? The good thing about SVMs is that they can
    draw these "straight lines" in any number of dimensions. So technically, if you
    project these points into a high dimensional space, where they can separated by
    a simple hyperplane, SVMs will come up with an exact boundary. Once we have that
    boundary, we can project it back to the original space. The projection of this
    hyperplane on our original lower dimensional space looks curvy, as we can see
    in the next figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What if we cannot separate the data with simple straight lines?](img/B04554_10_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The topic of SVMs is really deep and we will not be able to discuss it in detail
    here. If you are really interested, there is a ton of material available online.
    You can go through a simple tutorial to understand it better.
  prefs: []
  type: TYPE_NORMAL
- en: How do we actually implement this?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have now arrived at the core. The discussion up until now was necessary because
    it gives you the background required to build an object recognition system. Now,
    let's build an object recognizer that can recognize whether the given image contains
    a dress, a pair of shoes, or a bag. We can easily extend this system to detect
    any number of items. We are starting with three distinct items so that you can
    start experimenting with it later.
  prefs: []
  type: TYPE_NORMAL
- en: Before we start, we need to make sure that we have a set of training images.
    There are many databases available online where the images are already arranged
    into groups. Caltech256 is perhaps one of the most popular databases for object
    recognition. You can download it from [http://www.vision.caltech.edu/Image_Datasets/Caltech256](http://www.vision.caltech.edu/Image_Datasets/Caltech256).
    Create a folder called `images` and create three subfolders inside it, that is,
    `dress`, `footwear`, and `bag`. Inside each of those subfolders, add 20 images
    corresponding to that item. You can just download these images from the internet,
    but make sure those images have a clean background.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, a dress image would like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How do we actually implement this?](img/B04554_10_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A footwear image would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How do we actually implement this?](img/B04554_10_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A bag image would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How do we actually implement this?](img/B04554_10_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have 60 training images, we are ready to start. As a side note,
    object recognition systems actually need tens of thousands of training images
    in order to perform well in the real world. Since we are building an object recognizer
    to detect 3 types of objects, we will take only 20 training images per object.
    Adding more training images will increase the accuracy and robustness of our system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step here is to extract feature vectors from all the training images
    and build the visual dictionary (also known as codebook). Here is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: What happened inside the code?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first thing we need to do is extract the centroids. This is how we are going
    to build our visual dictionary. The `get_centroids` method in the `FeatureExtractor`
    class is designed to do this. We keep collecting the image features extracted
    from keypoints until we have a sufficient number of them. Since we are using a
    dense detector, 10 images should be sufficient. The reason we are just taking
    10 images is because they will give rise to a large number of features. The centroids
    will not change much even if you add more feature points.
  prefs: []
  type: TYPE_NORMAL
- en: Once we've extracted the centroids, we are ready to move on to the next step
    of feature extraction. The set of centroids is our visual dictionary. The function,
    `extract_feature_map`, will extract a feature vector from each image and associate
    it with the corresponding label. The reason we do this is because we need this
    mapping to train our classifier. We need a set of datapoints, and each datapoint
    should be associated with a label. So, we start from an image, extract the feature
    vector, and then associate it with the corresponding label (like bag, dress, or
    footwear).
  prefs: []
  type: TYPE_NORMAL
- en: The `Quantizer` class is designed to achieve vector quantization and build the
    feature vector. For each keypoint extracted from the image, the `get_feature_vector`
    method finds the closest visual word in our dictionary. By doing this, we end
    up building a histogram based on our visual dictionary. Each image is now represented
    as a combination from a set of visual words. Hence the name, **Bag of Words**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to train the classifier using these features. Here is the
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: How did we build the trainer?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We use the `scikit-learn` package to build the SVM model. You can install it,
    as shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We start with labeled data and feed it to the `OneVsOneClassifier` method. We
    have a `classify` method that classifies an input image and associates a label
    with it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s give this a trial run, shall we? Make sure you have a folder called
    `images`, where you have the training images for the three classes. Create a folder
    called `models`, where the learning models will be stored. Run the following commands
    on your terminal to create the features and train the classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the classifier has been trained, we just need a module to classify
    the input image and detect the object inside. Here is the code to do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We are all set! We just extract the `feature` vector from the input image and
    use it as the input argument to the classifier. Let''s go ahead and see if this
    works. Download a random footwear image from the internet and make sure it has
    a clean background. Run the following command by replacing `new_image.jpg` with
    the right filename:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We can use the same technique to build a visual search engine. A visual search
    engine looks at the input image and shows a bunch of images that are similar to
    it. We can reuse the object recognition framework to build this. Extract the feature
    vector from the input image, and compare it with all the feature vectors in the
    training dataset. Pick out the top matches and display the results. This is a
    simple way of doing things!
  prefs: []
  type: TYPE_NORMAL
- en: In the real world, we have to deal with billions of images. So, you cannot afford
    to search through every single image before you display the output. There are
    a lot of algorithms that are used to make sure that this is efficient and fast
    in the real world. Deep Learning is being used extensively in this field and it
    has shown a lot of promise in recent years. It is a branch of machine learning
    that focuses on learning optimal representation of data, so that it becomes easier
    for the machines to *learn* new tasks. You can learn more about it at [http://deeplearning.net](http://deeplearning.net).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to build an object recognition system. The differences
    between object detection and object recognition were discussed in detail. We learned
    about the dense feature detector, visual dictionary, vector quantization, and
    how to use these concepts to build a feature vector. The concepts of supervised
    and unsupervised learning were discussed. We talked about Support Vector Machines
    and how we can use them to build a classifier. We learned how to recognize an
    object in an unknown image, and how we can extend that concept to build a visual
    search engine.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to discuss stereo imaging and 3D reconstruction.
    We will talk about how we can build a depth map and extract the 3D information
    from a given scene.
  prefs: []
  type: TYPE_NORMAL
