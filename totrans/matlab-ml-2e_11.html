<html><head></head><body>
<div id="_idContainer104">
<h1 class="chapter-number" id="_idParaDest-216"><a id="_idTextAnchor223"/><span class="koboSpan" id="kobo.1.1">11</span></h1>
<h1 id="_idParaDest-217"><a id="_idTextAnchor224"/><span class="koboSpan" id="kobo.2.1">Anomaly Detection in MATLAB</span></h1>
<p><span class="koboSpan" id="kobo.3.1">Throughout the life cycle of a physical system, the occurrence of failures or malfunctions poses a potential threat to its normal functioning. </span><span class="koboSpan" id="kobo.3.2">To safeguard against critical interruptions, it becomes imperative to implement an anomaly detection system within the facility. </span><span class="koboSpan" id="kobo.3.3">Termed as a </span><strong class="bold"><span class="koboSpan" id="kobo.4.1">fault diagnosis system</span></strong><span class="koboSpan" id="kobo.5.1">, this mechanism is designed to identify potential</span><a id="_idIndexMarker1182"/><span class="koboSpan" id="kobo.6.1"> malfunctions within the monitored system. </span><span class="koboSpan" id="kobo.6.2">The pursuit of fault detection stands as a pivotal and defining phase in maintenance interventions, demanding a systematic and deterministic approach to comprehensively analyze all conceivable causes that might have led to </span><span class="No-Break"><span class="koboSpan" id="kobo.7.1">the malfunction.</span></span></p>
<p><span class="koboSpan" id="kobo.8.1">In this chapter, we will learn the basic concepts of anomaly detection systems and how to implement an anomaly detection system </span><span class="No-Break"><span class="koboSpan" id="kobo.9.1">in MATLAB.</span></span></p>
<p><span class="koboSpan" id="kobo.10.1">We’re going to cover the following </span><span class="No-Break"><span class="koboSpan" id="kobo.11.1">main topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.12.1">Introducing anomaly detection and fault </span><span class="No-Break"><span class="koboSpan" id="kobo.13.1">diagnosis systems</span></span></li>
<li><span class="koboSpan" id="kobo.14.1">Using </span><strong class="bold"><span class="koboSpan" id="kobo.15.1">machine learning</span></strong><span class="koboSpan" id="kobo.16.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.17.1">ML</span></strong><span class="koboSpan" id="kobo.18.1">) to identify </span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">anomalous</span></span><span class="No-Break"><span class="koboSpan" id="kobo.20.1"> functioning</span></span></li>
<li><span class="koboSpan" id="kobo.21.1">Building a fault diagnosis system </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">using MATLAB</span></span></li>
<li><span class="koboSpan" id="kobo.23.1">Understanding advanced </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">regularization techniques</span></span></li>
</ul>
<h1 id="_idParaDest-218"><a id="_idTextAnchor225"/><span class="koboSpan" id="kobo.25.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.26.1">In this chapter, we will introduce basic ML concepts. </span><span class="koboSpan" id="kobo.26.2">To understand these topics, a basic knowledge of algebra and mathematical modeling is needed. </span><span class="koboSpan" id="kobo.26.3">A working knowledge of the MATLAB environment is </span><span class="No-Break"><span class="koboSpan" id="kobo.27.1">also required.</span></span></p>
<p><span class="koboSpan" id="kobo.28.1">To work with the MATLAB code in this chapter, you need the following files (available on GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">at </span></span><a href="https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition"><span class="No-Break"><span class="koboSpan" id="kobo.30.1">https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.31.1">):</span></span></p>
<ul>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.32.1">GearboxAccData.xlsx</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.33.1">AnomalyDetectGearBox.m</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.34.1">DroneFaultDiagnosis.xlsx</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.35.1">UAVFaultDiagnosis.m</span></strong></span></li>
</ul>
<h1 id="_idParaDest-219"><a id="_idTextAnchor226"/><span class="koboSpan" id="kobo.36.1">Introducing anomaly detection and fault diagnosis systems</span><a id="_idTextAnchor227"/></h1>
<p><span class="koboSpan" id="kobo.37.1">Anomaly detection and</span><a id="_idIndexMarker1183"/><span class="koboSpan" id="kobo.38.1"> fault diagnosis systems are crucial components </span><a id="_idIndexMarker1184"/><span class="koboSpan" id="kobo.39.1">of various industries, particularly in areas where safety, reliability, and efficiency are of utmost importance, such as manufacturing, healthcare, finance, and cybersecurity. </span><span class="koboSpan" id="kobo.39.2">These systems aim to identify unusual or unexpected patterns, behaviors, or conditions in data, processes, or systems that may indicate the presence of faults, defects, </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">or anomalies.</span></span></p>
<p><span class="koboSpan" id="kobo.41.1">Delving into the realm of anomaly detection, this section provides a comprehensive overview, unraveling the key principles and methodologies employed in identifying deviations from the norm within diverse systems </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">and datasets.</span></span></p>
<h2 id="_idParaDest-220"><a id="_idTextAnchor228"/><span class="koboSpan" id="kobo.43.1">Anomaly detection overview</span></h2>
<p><span class="koboSpan" id="kobo.44.1">Anomaly detection is </span><a id="_idIndexMarker1185"/><span class="koboSpan" id="kobo.45.1">a technique used in data analysis and ML to identify data points or patterns that deviate significantly from the expected or normal behavior within a dataset. </span><span class="koboSpan" id="kobo.45.2">Anomalies, also known as outliers, are data points that do not conform to most of the data and may indicate errors, fraud, unusual events, or other important information. </span><span class="koboSpan" id="kobo.45.3">Anomaly detection has various applications across different domains, such as cybersecurity, industrial </span><strong class="bold"><span class="koboSpan" id="kobo.46.1">quality control</span></strong><span class="koboSpan" id="kobo.47.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.48.1">QC</span></strong><span class="koboSpan" id="kobo.49.1">), finance, healthcare, </span><span class="No-Break"><span class="koboSpan" id="kobo.50.1">and more.</span></span></p>
<p><span class="koboSpan" id="kobo.51.1">We can start to get an overview of different types of anomalies to understand what is intended with this term, we will list some types </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">of anomalies:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.53.1">Point anomalies</span></strong><span class="koboSpan" id="kobo.54.1">: These are individual data points that are considered anomalies, such as a single fraudulent transaction in a credit </span><span class="No-Break"><span class="koboSpan" id="kobo.55.1">card dataset.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.56.1">Contextual anomalies</span></strong><span class="koboSpan" id="kobo.57.1">: These are anomalies that are context-dependent. </span><span class="koboSpan" id="kobo.57.2">A data point might not be an anomaly on its own but is unusual in a particular context or time, such as a sudden spike in web traffic during a </span><span class="No-Break"><span class="koboSpan" id="kobo.58.1">holiday sale.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.59.1">Collective anomalies</span></strong><span class="koboSpan" id="kobo.60.1">: These are anomalies that are identified by examining a group of data points collectively. </span><span class="koboSpan" id="kobo.60.2">These anomalies involve patterns or relationships between </span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">data points.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.62.1">There are several methods for addressing anomaly detection problems, ranging from simple statistical techniques to complex ML algorithms. </span><span class="koboSpan" id="kobo.62.2">The choice of method depends on the nature of the</span><a id="_idIndexMarker1186"/><span class="koboSpan" id="kobo.63.1"> data and the specific problem you are trying to solve. </span><span class="koboSpan" id="kobo.63.2">Here, we are listing the most </span><span class="No-Break"><span class="koboSpan" id="kobo.64.1">used ones:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.65.1">Statistical methods</span></strong><span class="koboSpan" id="kobo.66.1">: Statistical techniques such as z-scores, percentiles, and boxplots can be used to identify anomalies based on deviations from the mean or median of the </span><span class="No-Break"><span class="koboSpan" id="kobo.67.1">data distribution.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.68.1">ML</span></strong><span class="koboSpan" id="kobo.69.1">: Supervised, unsupervised, and semi-supervised ML algorithms can be used for anomaly </span><a id="_idIndexMarker1187"/><span class="koboSpan" id="kobo.70.1">detection. </span><span class="koboSpan" id="kobo.70.2">Some </span><a id="_idIndexMarker1188"/><span class="koboSpan" id="kobo.71.1">popular methods include Isolation Forest, </span><strong class="bold"><span class="koboSpan" id="kobo.72.1">One-Class Support Vector Machine</span></strong><span class="koboSpan" id="kobo.73.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.74.1">One-Class SVM</span></strong><span class="koboSpan" id="kobo.75.1">), </span><strong class="bold"><span class="koboSpan" id="kobo.76.1">autoencoders</span></strong><span class="koboSpan" id="kobo.77.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.78.1">AEs</span></strong><span class="koboSpan" id="kobo.79.1">), and </span><span class="No-Break"><span class="koboSpan" id="kobo.80.1">k-means clustering.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.81.1">Time series analysis</span></strong><span class="koboSpan" id="kobo.82.1">: Specialized techniques are used for detecting anomalies in time series data, such </span><a id="_idIndexMarker1189"/><span class="koboSpan" id="kobo.83.1">as </span><strong class="bold"><span class="koboSpan" id="kobo.84.1">autoregressive</span></strong><span class="koboSpan" id="kobo.85.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.86.1">AR</span></strong><span class="koboSpan" id="kobo.87.1">) models, exponential</span><a id="_idIndexMarker1190"/><span class="koboSpan" id="kobo.88.1"> smoothing, and </span><strong class="bold"><span class="koboSpan" id="kobo.89.1">moving </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.90.1">averages</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.91.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.92.1">MAs</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.93.1">).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.94.1">Density estimation</span></strong><span class="koboSpan" id="kobo.95.1">: Methods </span><a id="_idIndexMarker1191"/><span class="koboSpan" id="kobo.96.1">such as </span><strong class="bold"><span class="koboSpan" id="kobo.97.1">kernel density estimation</span></strong><span class="koboSpan" id="kobo.98.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.99.1">KDE</span></strong><span class="koboSpan" id="kobo.100.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.101.1">Gaussian Mixture Models</span></strong><span class="koboSpan" id="kobo.102.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.103.1">GMMs</span></strong><span class="koboSpan" id="kobo.104.1">) are </span><a id="_idIndexMarker1192"/><span class="koboSpan" id="kobo.105.1">used to estimate the probability density function of the data and identify anomalies as </span><span class="No-Break"><span class="koboSpan" id="kobo.106.1">low-density regions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.107.1">Deep learning (DL)</span></strong><span class="koboSpan" id="kobo.108.1">: </span><strong class="bold"><span class="koboSpan" id="kobo.109.1">Neural networks</span></strong><span class="koboSpan" id="kobo.110.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.111.1">NNs</span></strong><span class="koboSpan" id="kobo.112.1">), especially </span><strong class="bold"><span class="koboSpan" id="kobo.113.1">deep AEs</span></strong><span class="koboSpan" id="kobo.114.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.115.1">DAEs</span></strong><span class="koboSpan" id="kobo.116.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.117.1">recurrent NNs</span></strong><span class="koboSpan" id="kobo.118.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.119.1">RNNs</span></strong><span class="koboSpan" id="kobo.120.1">), are</span><a id="_idIndexMarker1193"/><span class="koboSpan" id="kobo.121.1"> used</span><a id="_idIndexMarker1194"/><span class="koboSpan" id="kobo.122.1"> for</span><a id="_idIndexMarker1195"/><span class="koboSpan" id="kobo.123.1"> anomaly</span><a id="_idIndexMarker1196"/><span class="koboSpan" id="kobo.124.1"> detection in high-dimensional data </span><span class="No-Break"><span class="koboSpan" id="kobo.125.1">or sequences.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.126.1">Ensemble methods</span></strong><span class="koboSpan" id="kobo.127.1">: Combining multiple anomaly detection models can improve overall</span><a id="_idIndexMarker1197"/><span class="koboSpan" id="kobo.128.1"> performance </span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">and robustness.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.130.1">In addressing anomaly detection problems, we have to face some challenges. </span><span class="koboSpan" id="kobo.130.2">For example, determining an appropriate threshold for defining anomalies can be challenging. </span><span class="koboSpan" id="kobo.130.3">Imbalanced datasets, where anomalies are rare, can make model training and evaluation tricky. </span><span class="koboSpan" id="kobo.130.4">Handling high-dimensional data and noisy datasets can also </span><span class="No-Break"><span class="koboSpan" id="kobo.131.1">be challenging.</span></span></p>
<p><span class="koboSpan" id="kobo.132.1">Anomaly detection is a valuable tool for identifying rare but potentially important events or patterns in</span><a id="_idIndexMarker1198"/><span class="koboSpan" id="kobo.133.1"> large datasets. </span><span class="koboSpan" id="kobo.133.2">The choice of method depends on the specific domain, data characteristics, and the nature of anomalies that need to </span><span class="No-Break"><span class="koboSpan" id="kobo.134.1">be detected.</span></span></p>
<h2 id="_idParaDest-221"><a id="_idTextAnchor229"/><span class="koboSpan" id="kobo.135.1">Fault diagnosis systems explained</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.136.1">Diagnostics</span></strong><span class="koboSpan" id="kobo.137.1"> refers </span><a id="_idIndexMarker1199"/><span class="koboSpan" id="kobo.138.1">to </span><a id="_idIndexMarker1200"/><span class="koboSpan" id="kobo.139.1">a procedure that translates information obtained from measuring parameters and collecting data about a machine into insights about its current or potential failures. </span><span class="koboSpan" id="kobo.139.2">This process encompasses a combination of analytical and synthetic activities, utilizing physical measurements and machine-specific characteristics to derive valuable information regarding the machine’s condition and its future trends. </span><span class="koboSpan" id="kobo.139.3">This information is crucial for assessing both short-term and </span><span class="No-Break"><span class="koboSpan" id="kobo.140.1">long-term reliability.</span></span></p>
<p><span class="koboSpan" id="kobo.141.1">The adoption of fault diagnosis techniques is becoming increasingly important to ensure high levels of safety and dependability in automated and autonomous systems. </span><span class="koboSpan" id="kobo.141.2">Indeed, over recent years, the global scientific community has invested significant efforts in developing systematic approaches for diagnosing failures in a wide array of systems. </span><span class="koboSpan" id="kobo.141.3">The primary objective of a fault diagnosis system is to continuously monitor a system during its operation to achieve three key goals: detecting the presence of faults (fault detection), pinpointing the specific location of these faults (fault isolation), and understanding how these faults evolve over time (</span><span class="No-Break"><span class="koboSpan" id="kobo.142.1">fault identification).</span></span></p>
<p><span class="koboSpan" id="kobo.143.1">Usually, the output of a fault diagnosis system provides a group of sensitive variables that are influenced by the type of fault, with some modifications when the system experiences a failure. </span><span class="koboSpan" id="kobo.143.2">Subsequently, the system extracts and processes information embedded within these fault occurrences to carry out the tasks of detecting, isolating, and identifying faults. </span><span class="koboSpan" id="kobo.143.3">The techniques employed for fault diagnostics can be categorized into three </span><span class="No-Break"><span class="koboSpan" id="kobo.144.1">fundamental groups:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.145.1">Model-based</span></strong><span class="koboSpan" id="kobo.146.1">: This approach </span><a id="_idIndexMarker1201"/><span class="koboSpan" id="kobo.147.1">relies on precise mathematical models, enabling efficient fault detection and diagnosis. </span><span class="koboSpan" id="kobo.147.2">These models are constructed to depict the actual degradation processes of the components under scrutiny. </span><span class="koboSpan" id="kobo.147.3">This involves describing, in terms of the laws of physics, how operational conditions influence the performance and lifespan of assets. </span><span class="koboSpan" id="kobo.147.4">Key variables encompass various thermal, mechanical, chemical, and electrical parameters. </span><span class="koboSpan" id="kobo.147.5">Expressing their impact on machinery’s health is a challenging endeavor, demanding a high degree of domain expertise and modeling skills from those creating such solutions. </span><span class="koboSpan" id="kobo.147.6">Once the model is established, it becomes essential to have sensors that can provide data regarding the relevant quantities identified during the analysis and modeling phase. </span><span class="koboSpan" id="kobo.147.7">This data serves as input for the model. </span><span class="koboSpan" id="kobo.147.8">The primary advantage of this approach lies in its descriptiveness. </span><span class="koboSpan" id="kobo.147.9">It allows for a detailed analysis of the causes behind each output it generates because it’s rooted in a physical representation of the process. </span><span class="koboSpan" id="kobo.147.10">This, in turn, enables validation and certification. </span><span class="koboSpan" id="kobo.147.11">However, its accuracy heavily relies on the quality of analysis and modeling conducted by domain experts. </span><span class="koboSpan" id="kobo.147.12">Conversely, drawbacks include its complexity and high implementation costs, along with its high specificity to the</span><a id="_idIndexMarker1202"/><span class="koboSpan" id="kobo.148.1"> system, limiting possibilities for reuse </span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">and expansion.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.150.1">Knowledge-based</span></strong><span class="koboSpan" id="kobo.151.1">: This approach also leans on domain experts, aiming to model the </span><a id="_idIndexMarker1203"/><span class="koboSpan" id="kobo.152.1">expertise and behavior of these experts directly. </span><span class="koboSpan" id="kobo.152.2">The objective is to formalize the knowledge they possess, enabling its automated replication and application. </span><span class="koboSpan" id="kobo.152.3">Expert systems are essentially software programs that leverage knowledge repositories collected from proficient individuals in various fields. </span><span class="koboSpan" id="kobo.152.4">These systems then employ inference and reasoning mechanisms to emulate human thought processes, offering support and solutions for practical issues. </span><span class="koboSpan" id="kobo.152.5">Two prevalent techniques for implementing this type of model are rule-based mechanisms and fuzzy logic. </span><span class="koboSpan" id="kobo.152.6">Rule-based approaches are valued for their simplicity of implementation and interpretability. </span><span class="koboSpan" id="kobo.152.7">However, they may fall short in expressing complex conditions and may suffer from a combinatorial explosion when dealing with many rules. </span><span class="koboSpan" id="kobo.152.8">On the other hand, the use of fuzzy logic permits describing the system’s state through more imprecise and less rigid inputs, simplifying the formalization and model description process and making it more intuitive. </span><span class="koboSpan" id="kobo.152.9">As with model-based methods, the effectiveness of expert systems heavily hinges on the quality and level of detail achieved by the model, resulting in highly </span><span class="No-Break"><span class="koboSpan" id="kobo.153.1">specific solutions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.154.1">Data-based methods</span></strong><span class="koboSpan" id="kobo.155.1">: The data-based approach applies statistical and ML techniques to the data gathered from machines, with the goal of discerning the condition of components. </span><span class="koboSpan" id="kobo.155.2">The aim is to acquire comprehensive real-time information about the machinery, typically through sensors and production and maintenance activity logs, and to correlate this information with the individual components’ degradation levels or the system’s performance. </span><span class="koboSpan" id="kobo.155.3">Presently, this approach is most prevalent in practical applications. </span><span class="koboSpan" id="kobo.155.4">This is attributed to several advantages it offers over the other two methods. </span><span class="koboSpan" id="kobo.155.5">Data-driven methods require substantial data volumes for effectiveness. </span><span class="koboSpan" id="kobo.155.6">However, given the accessibility of modern interconnected sensors, this requirement is generally easy to fulfill. </span><span class="koboSpan" id="kobo.155.7">In contrast to the other approaches, they have the significant advantage of not mandating in-depth domain-specific expertise, thereby reducing experts’ impact on the model’s final performance. </span><span class="koboSpan" id="kobo.155.8">While expert insights can facilitate the selection of input variables, their influence is less pronounced compared to other methods. </span><span class="koboSpan" id="kobo.155.9">Additionally, ML and data mining techniques may uncover relationships </span><a id="_idIndexMarker1204"/><span class="koboSpan" id="kobo.156.1">between input parameters and system states that even experts might not know beforehand. </span><span class="koboSpan" id="kobo.156.2">ML-based algorithms can be harnessed to build predictive models that yield </span><span class="No-Break"><span class="koboSpan" id="kobo.157.1">domain-specific knowledge.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.158.1">Embarking on the intersection of machinery and intelligence, this section explores a cutting-edge approach to fault diagnosis by harnessing the power of ML. </span><span class="koboSpan" id="kobo.158.2">Unraveling the potential of algorithms </span><a id="_idIndexMarker1205"/><span class="koboSpan" id="kobo.159.1">and models, we delve into how these technological advancements enable a more precise and efficient identification of faults within </span><span class="No-Break"><span class="koboSpan" id="kobo.160.1">complex systems.</span></span></p>
<h2 id="_idParaDest-222"><a id="_idTextAnchor230"/><span class="koboSpan" id="kobo.161.1">Approaching fault diagnosis using ML</span></h2>
<p><span class="koboSpan" id="kobo.162.1">ML algorithms </span><a id="_idIndexMarker1206"/><span class="koboSpan" id="kobo.163.1">autonomously derive knowledge</span><a id="_idIndexMarker1207"/><span class="koboSpan" id="kobo.164.1"> from data through the inputs they receive, eliminating the need for explicit developer instructions. </span><span class="koboSpan" id="kobo.164.2">In such models, the machine independently identifies the patterns required to achieve the desired outcome, a hallmark </span><a id="_idIndexMarker1208"/><span class="koboSpan" id="kobo.165.1">of </span><strong class="bold"><span class="koboSpan" id="kobo.166.1">artificial intelligence</span></strong><span class="koboSpan" id="kobo.167.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.168.1">AI</span></strong><span class="koboSpan" id="kobo.169.1">). </span><span class="koboSpan" id="kobo.169.2">During the learning process that characterizes these algorithms, the system is supplied with a training dataset, enabling it to estimate the relationships between input and output data. </span><span class="koboSpan" id="kobo.169.3">These relationships constitute the parameters of the model that the </span><span class="No-Break"><span class="koboSpan" id="kobo.170.1">system infers.</span></span></p>
<p><span class="koboSpan" id="kobo.171.1">The selection of a particular ML model hinges heavily on the intended goal of the system. </span><span class="koboSpan" id="kobo.171.2">The nature of the problem is modeled differently based on the objective, and two primary approaches can be identified: diagnostics and prognostics. </span><span class="koboSpan" id="kobo.171.3">A diagnostic system is designed to pinpoint and identify faults when they occur. </span><span class="koboSpan" id="kobo.171.4">This involves continuous monitoring of a system, alerting when deviations from the expected behavior are detected, specifying which component is affected by the anomaly, and categorizing the type </span><span class="No-Break"><span class="koboSpan" id="kobo.172.1">of anomaly.</span></span></p>
<p><span class="koboSpan" id="kobo.173.1">In contrast, prognostics seeks to predict whether a fault is imminent or to estimate the probability of its occurrence. </span><span class="koboSpan" id="kobo.173.2">As a proactive analysis, prognostics can contribute significantly to cost reduction in terms of interventions, but it represents a more intricate objective to accomplish. </span><span class="koboSpan" id="kobo.173.3">Another viable option is the simultaneous application of diagnostic and prognostic solutions to the same system. </span><span class="koboSpan" id="kobo.173.4">This combination offers two valuable advantages: diagnostics can provide support in cases where prognostics falls short. </span><span class="koboSpan" id="kobo.173.5">This situation is inevitable because some failures do not follow predictable patterns, and even when certain failures are predictable with a high degree of accuracy, they may not be identifiable in every instance. </span><span class="koboSpan" id="kobo.173.6">The information gleaned from diagnostic applications </span><a id="_idIndexMarker1209"/><span class="koboSpan" id="kobo.174.1">can</span><a id="_idIndexMarker1210"/><span class="koboSpan" id="kobo.175.1"> serve as supplementary input for predictive systems, enabling the creation of more sophisticated and </span><span class="No-Break"><span class="koboSpan" id="kobo.176.1">precise models.</span></span></p>
<p><span class="koboSpan" id="kobo.177.1">Fault diagnosis problems can be addressed using the following two types of approaches depending on the type </span><span class="No-Break"><span class="koboSpan" id="kobo.178.1">of data:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.179.1">Regression</span></strong><span class="koboSpan" id="kobo.180.1">: Regression</span><a id="_idIndexMarker1211"/><span class="koboSpan" id="kobo.181.1"> can serve </span><a id="_idIndexMarker1212"/><span class="koboSpan" id="kobo.182.1">as a modeling technique for prognostic challenges. </span><span class="koboSpan" id="kobo.182.2">This entails estimating the remaining useful life of a component in continuous time units. </span><span class="koboSpan" id="kobo.182.3">In this scenario, the training dataset should exclusively encompass data from components that have experienced failures, facilitating the labeling of inputs starting from the failure event backward in time. </span><span class="koboSpan" id="kobo.182.4">This approach to the problem still adheres</span><a id="_idIndexMarker1213"/><span class="koboSpan" id="kobo.183.1"> to the </span><strong class="bold"><span class="koboSpan" id="kobo.184.1">supervised learning</span></strong><span class="koboSpan" id="kobo.185.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.186.1">SL</span></strong><span class="koboSpan" id="kobo.187.1">) framework, where input data is linked to continuous output values in </span><span class="No-Break"><span class="koboSpan" id="kobo.188.1">this context.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.189.1">Binary classification</span></strong><span class="koboSpan" id="kobo.190.1">: The simplest approach is to frame fault detection as a binary </span><a id="_idIndexMarker1214"/><span class="koboSpan" id="kobo.191.1">classification problem, where each input representing the system’s state must be categorized into one of two exclusive values. </span><span class="koboSpan" id="kobo.191.2">In the context of diagnostic problem-solving, this entails determining whether the machine is operating correctly or incorrectly, categorizing all possible states into these two classes. </span><span class="koboSpan" id="kobo.191.3">This process aligns with SL, as the input data is paired with labels that represent the model’s output. </span><span class="koboSpan" id="kobo.191.4">In such cases, the system learns to discern relationships between the input data and the corresponding labels. </span><span class="koboSpan" id="kobo.191.5">In the case of prognostics, the task shifts toward assessing whether the machine could potentially fail within a specified time frame. </span><span class="koboSpan" id="kobo.191.6">The distinction between these two meanings arises solely from the differing interpretations of the labels. </span><span class="koboSpan" id="kobo.191.7">This implies that the same model can be employed to address both issues. </span><span class="koboSpan" id="kobo.191.8">The differentiation lies in how the dataset is labeled during the model </span><span class="No-Break"><span class="koboSpan" id="kobo.192.1">training phase.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.193.1">Multiclass classification</span></strong><span class="koboSpan" id="kobo.194.1">: The multiclass variant represents an extension of binary </span><a id="_idIndexMarker1215"/><span class="koboSpan" id="kobo.195.1">classification, introducing a broader range of available labels for selection. </span><span class="koboSpan" id="kobo.195.2">Nevertheless, each input is still associated with only one label. </span><span class="koboSpan" id="kobo.195.3">In the diagnostic scenario, the extension follows a straightforward approach, involving the determination of whether the machine is operating correctly or not. </span><span class="koboSpan" id="kobo.195.4">In the latter case, it’s about pinpointing the specific anomaly state from various possibilities. </span><span class="koboSpan" id="kobo.195.5">When considering prognostics applications, the problem shifts toward determining </span><a id="_idIndexMarker1216"/><span class="koboSpan" id="kobo.196.1">the time interval before the machine’s potential failure. </span><span class="koboSpan" id="kobo.196.2">Here, the labels denote different intervals representing proximity to a </span><span class="No-Break"><span class="koboSpan" id="kobo.197.1">failure event.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.198.1">For a precise diagnosis </span><a id="_idIndexMarker1217"/><span class="koboSpan" id="kobo.199.1">of </span><a id="_idIndexMarker1218"/><span class="koboSpan" id="kobo.200.1">faults in intricate machinery, it is imperative to gather information by collecting data, employing advanced signal processing algorithms for data analysis, and subsequently extracting the appropriate features to efficiently identify and classify faults. </span><span class="koboSpan" id="kobo.200.2">Data can be acquired through various methods, primarily via measurements of physical attributes that describe the machine’s state during operation. </span><span class="koboSpan" id="kobo.200.3">These measurements are obtained by specialized sensors that convert physical properties into electrical values, often referred to as </span><span class="No-Break"><span class="koboSpan" id="kobo.201.1">sensor data.</span></span></p>
<p><span class="koboSpan" id="kobo.202.1">It’s worth noting that the choice of parameters depends greatly on the specific system under scrutiny, with examples including noise, vibrations, pressure, temperature, and humidity. </span><span class="koboSpan" id="kobo.202.2">Typically, in modern automated industrial systems, the necessary diagnostic data is readily available. </span><span class="koboSpan" id="kobo.202.3">In cases where it’s not, adding extra sensors becomes the initial step in establishing an effective fault identification strategy. </span><span class="koboSpan" id="kobo.202.4">Additionally, data can also be gathered by linking the machine’s static operating conditions to specific time points, incorporating factors such as material codes, machine production speed, or the type of product being manufactured. </span><span class="koboSpan" id="kobo.202.5">This type of data is categorized as </span><span class="No-Break"><span class="koboSpan" id="kobo.203.1">statistical data.</span></span></p>
<p><span class="koboSpan" id="kobo.204.1">Finally, data may encompass the history of significant events and actions related to a machine and</span><a id="_idIndexMarker1219"/><span class="koboSpan" id="kobo.205.1"> its </span><a id="_idIndexMarker1220"/><span class="koboSpan" id="kobo.206.1">components, which is often referred to as log data. </span><span class="koboSpan" id="kobo.206.2">This can include records of faults, repairs, replacements, and other </span><span class="No-Break"><span class="koboSpan" id="kobo.207.1">pertinent activities.</span></span></p>
<p><span class="koboSpan" id="kobo.208.1">With the foundational concepts of fault diagnosis explored, it’s now time to practically address a </span><span class="No-Break"><span class="koboSpan" id="kobo.209.1">simulation problem.</span></span></p>
<h1 id="_idParaDest-223"><a id="_idTextAnchor231"/><span class="koboSpan" id="kobo.210.1">Using ML to identify anomalous functioning</span></h1>
<p><span class="koboSpan" id="kobo.211.1">A gearbox, also</span><a id="_idIndexMarker1221"/><span class="koboSpan" id="kobo.212.1"> known as a gear</span><a id="_idIndexMarker1222"/><span class="koboSpan" id="kobo.213.1"> mechanism or transmission, is a mechanical device designed to transmit mechanical power from one component to another while altering the speed, torque, and direction of rotation. </span><span class="koboSpan" id="kobo.213.2">Gearboxes consist of a set of gears with different sizes and configurations that mesh to provide specific mechanical advantages based on the </span><span class="No-Break"><span class="koboSpan" id="kobo.214.1">desired output.</span></span></p>
<p><span class="koboSpan" id="kobo.215.1">In an automotive context, a gearbox, often referred to as a transmission or gearshift, is a critical component that plays a central role in controlling the power output of the engine and the vehicle’s speed. </span><span class="koboSpan" id="kobo.215.2">Anomaly detection in a gearbox involves identifying irregularities or deviations from the normal behavior of the gearbox components and their associated systems. </span><span class="koboSpan" id="kobo.215.3">Detecting anomalies in a car’s gearbox is crucial for ensuring the vehicle’s safety, reliability, and </span><span class="No-Break"><span class="koboSpan" id="kobo.216.1">overall performance.</span></span></p>
<p><span class="koboSpan" id="kobo.217.1">To promptly identify anomalies in a gearbox, we can use sensors such as those measuring temperature, pressure, speed, and fluid levels to continuously collect data. </span><span class="koboSpan" id="kobo.217.2">These sensors provide valuable information about the gearbox’s condition. </span><span class="koboSpan" id="kobo.217.3">Raw sensor data often contains noise and outliers. </span><span class="koboSpan" id="kobo.217.4">Data preprocessing techniques, such as filtering and data cleaning, are applied to ensure the quality and reliability of the data. </span><span class="koboSpan" id="kobo.217.5">Relevant features are extracted from the preprocessed data. </span><span class="koboSpan" id="kobo.217.6">These features may include gear engagement patterns, rotational speeds, and temperature fluctuations. </span><span class="koboSpan" id="kobo.217.7">Anomaly detection typically begins by establishing a baseline model of the normal behavior of the gearbox. </span><span class="koboSpan" id="kobo.217.8">This is done by analyzing historical data to understand typical operating conditions </span><span class="No-Break"><span class="koboSpan" id="kobo.218.1">and performance.</span></span></p>
<p><span class="koboSpan" id="kobo.219.1">Various</span><a id="_idIndexMarker1223"/><span class="koboSpan" id="kobo.220.1"> anomaly </span><a id="_idIndexMarker1224"/><span class="koboSpan" id="kobo.221.1">detection techniques are applied to identify deviations from the established normal behavior. </span><span class="koboSpan" id="kobo.221.2">Some common techniques include </span><span class="No-Break"><span class="koboSpan" id="kobo.222.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.223.1">Statistical methods</span></strong><span class="koboSpan" id="kobo.224.1">: These methods flag data points that significantly deviate from the mean or follow a </span><span class="No-Break"><span class="koboSpan" id="kobo.225.1">different distribution</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.226.1">ML algorithms</span></strong><span class="koboSpan" id="kobo.227.1">: These models can learn to recognize patterns in the data and </span><span class="No-Break"><span class="koboSpan" id="kobo.228.1">detect anomalies</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.229.1">Time series analysis</span></strong><span class="koboSpan" id="kobo.230.1">: For gearbox data collected over time, time series methods </span><a id="_idIndexMarker1225"/><span class="koboSpan" id="kobo.231.1">such</span><a id="_idIndexMarker1226"/><span class="koboSpan" id="kobo.232.1"> as </span><strong class="bold"><span class="koboSpan" id="kobo.233.1">Autoregressive Integrated Moving Average</span></strong><span class="koboSpan" id="kobo.234.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.235.1">ARIMA</span></strong><span class="koboSpan" id="kobo.236.1">) or </span><strong class="bold"><span class="koboSpan" id="kobo.237.1">long short-term memory</span></strong><span class="koboSpan" id="kobo.238.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.239.1">LSTM</span></strong><span class="koboSpan" id="kobo.240.1">) NNs can be used to </span><span class="No-Break"><span class="koboSpan" id="kobo.241.1">detect anomalies</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.242.1">Anomalies are detected based on predefined thresholds or through statistical analysis of the data. </span><span class="koboSpan" id="kobo.242.2">These thresholds are determined during the model training phase and may be adjusted over time to optimize detection. </span><span class="koboSpan" id="kobo.242.3">When an anomaly is detected, the system can generate alerts for the vehicle operator or service technician. </span><span class="koboSpan" id="kobo.242.4">These alerts may include information about the specific anomaly, its potential impact, and recommended actions for maintenance </span><span class="No-Break"><span class="koboSpan" id="kobo.243.1">or repair.</span></span></p>
<p><span class="koboSpan" id="kobo.244.1">Anomaly detection is an ongoing process. </span><span class="koboSpan" id="kobo.244.2">The gearbox’s condition is continuously monitored, and data is analyzed in real time to ensure prompt detection of any emerging issues. </span><span class="koboSpan" id="kobo.244.3">Anomaly detection in car gearboxes is essential for predictive maintenance, preventing unexpected failures, and optimizing the lifespan and performance of the vehicle. </span><span class="koboSpan" id="kobo.244.4">It is a critical component of modern automotive diagnostics and maintenance systems, especially in commercial and industrial settings where vehicle reliability is of </span><span class="No-Break"><span class="koboSpan" id="kobo.245.1">utmost importance.</span></span></p>
<p><span class="koboSpan" id="kobo.246.1">Navigating the landscape of anomaly detection, this section focuses on the application of logistic regression as a powerful tool. </span><span class="koboSpan" id="kobo.246.2">By delving into the intricacies of this statistical model, we uncover</span><a id="_idIndexMarker1227"/><span class="koboSpan" id="kobo.247.1"> its</span><a id="_idIndexMarker1228"/><span class="koboSpan" id="kobo.248.1"> efficacy in identifying anomalies within datasets, shedding light on its nuances with a </span><span class="No-Break"><span class="koboSpan" id="kobo.249.1">practical implementation.</span></span></p>
<h2 id="_idParaDest-224"><a id="_idTextAnchor232"/><span class="koboSpan" id="kobo.250.1">Anomaly detection using logistic regression</span></h2>
<p><span class="koboSpan" id="kobo.251.1">In this instance, we</span><a id="_idIndexMarker1229"/><span class="koboSpan" id="kobo.252.1"> will employ data </span><a id="_idIndexMarker1230"/><span class="koboSpan" id="kobo.253.1">captured by accelerometers that recorded vibrations from a gearbox under two distinct operating conditions: one representing a healthy state and the other indicating a broken state. </span><span class="koboSpan" id="kobo.253.2">To ensure comprehensive coverage, the sensors were positioned in opposing directions to capture any operational variances. </span><span class="koboSpan" id="kobo.253.3">These datasets will serve as the foundation for training various algorithms aimed at classifying the gearbox’s </span><span class="No-Break"><span class="koboSpan" id="kobo.254.1">operational conditions.</span></span></p>
<p><span class="koboSpan" id="kobo.255.1">Let’s go in deep to describe </span><span class="No-Break"><span class="koboSpan" id="kobo.256.1">the algorithm:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.257.1">We start by importing the dataset into the MATLAB environment. </span><span class="koboSpan" id="kobo.257.2">The dataset provides measurements of vibrations recorded by two sensors, along with the associated classification of engine operation: </span><strong class="source-inline"><span class="koboSpan" id="kobo.258.1">0</span></strong><span class="koboSpan" id="kobo.259.1"> denotes a broken state, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.260.1">1</span></strong><span class="koboSpan" id="kobo.261.1"> indicates a </span><span class="No-Break"><span class="koboSpan" id="kobo.262.1">healthy state:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.263.1">
data = readtable('GearboxAccData.xlsx');</span></pre></li> <li><span class="koboSpan" id="kobo.264.1">A table with </span><strong class="source-inline"><span class="koboSpan" id="kobo.265.1">20000</span></strong><span class="koboSpan" id="kobo.266.1"> rows and </span><strong class="source-inline"><span class="koboSpan" id="kobo.267.1">3</span></strong><span class="koboSpan" id="kobo.268.1"> columns </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">was imported.</span></span></li>
<li><span class="koboSpan" id="kobo.270.1">To perform a first visual examination of the data, let’s utilize graphs for assistance. </span><span class="koboSpan" id="kobo.270.2">For instance, we can create boxplots illustrating the data distribution as detected by the </span><span class="No-Break"><span class="koboSpan" id="kobo.271.1">two sensors:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.272.1">
subplot(1,2,1)
boxchart(data.state,data.Vib1)
xlabel('state')
ylabel('Vib1')
subplot(1,2,2)
boxchart(data.state,data.Vib2)
xlabel('state')
ylabel('Vib2')</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.273.1">The chart seen in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.274.1">Figure 11</span></em></span><em class="italic"><span class="koboSpan" id="kobo.275.1">.1</span></em><span class="koboSpan" id="kobo.276.1"> will </span><span class="No-Break"><span class="koboSpan" id="kobo.277.1">be drawn:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer102">
<span class="koboSpan" id="kobo.278.1"><img alt="Figure 11.1 – Boxplot of the data" src="image/B21156_11_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.279.1">Figure 11.1 – Boxplot of the data</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.280.1">A boxplot serves as </span><a id="_idIndexMarker1231"/><span class="koboSpan" id="kobo.281.1">a </span><a id="_idIndexMarker1232"/><span class="koboSpan" id="kobo.282.1">visual representation employed to characterize the distribution of a sample through basic dispersion and positional measures. </span><span class="koboSpan" id="kobo.282.2">Upon scrutinizing </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.283.1">Figure 11</span></em></span><em class="italic"><span class="koboSpan" id="kobo.284.1">.1</span></em><span class="koboSpan" id="kobo.285.1">, it becomes evident that the data distributions from the two sensors vary. </span><span class="koboSpan" id="kobo.285.2">Sensor </span><strong class="source-inline"><span class="koboSpan" id="kobo.286.1">Vib1</span></strong><span class="koboSpan" id="kobo.287.1"> appears to exhibit a greater divergence in vibration values between malfunctioning and normal conditions. </span><span class="koboSpan" id="kobo.287.2">This implies that faults are likely manifesting at the location of </span><span class="No-Break"><span class="koboSpan" id="kobo.288.1">this sensor.</span></span></p>
<ol>
<li value="4"><span class="koboSpan" id="kobo.289.1">After visually analyzing the data at our disposal, we need to split the data so that we can use it to train an anomaly detection model. </span><span class="koboSpan" id="kobo.289.2">Data splitting is a fundamental step in the process of building and evaluating ML models. </span><span class="koboSpan" id="kobo.289.3">It involves dividing your dataset into multiple subsets, typically including a training set, a validation set, and a test set. </span><span class="koboSpan" id="kobo.289.4">Each of these subsets serves a specific purpose in the ML workflow. </span><span class="koboSpan" id="kobo.289.5">We will divide the data into two subsets: a training set and a test set. </span><span class="koboSpan" id="kobo.289.6">The training set is the largest portion of your data and is used to train the ML model. </span><span class="koboSpan" id="kobo.289.7">It’s the dataset the model learns from and uses to update its parameters. </span><span class="koboSpan" id="kobo.289.8">The test set is used to evaluate the final, trained model’s performance. </span><span class="koboSpan" id="kobo.289.9">It provides an unbiased estimate of how well the model will perform on unseen data. </span><span class="koboSpan" id="kobo.289.10">The test set should not be used during model development or </span><a id="_idIndexMarker1233"/><span class="koboSpan" id="kobo.290.1">training</span><a id="_idIndexMarker1234"/><span class="koboSpan" id="kobo.291.1"> to avoid data leakage. </span><span class="koboSpan" id="kobo.291.2">Let’s see how to make a </span><span class="No-Break"><span class="koboSpan" id="kobo.292.1">data split:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.293.1">
n = length(data.state);
SplitData = cvpartition(n,'Holdout',0.3);
TrainIndex = training(SplitData);
TrainData = data(TrainIndex,:);
TestIndex = test(SplitData);
TestData = data(TestIndex,:);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.294.1">We first extracted the number of observations present in our dataset using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.295.1">length()</span></strong><span class="koboSpan" id="kobo.296.1"> function: this function provides the size of the most extensive dimension in the </span><em class="italic"><span class="koboSpan" id="kobo.297.1">X</span></em><span class="koboSpan" id="kobo.298.1"> array. </span><span class="koboSpan" id="kobo.298.2">When dealing with vectors, this size corresponds to the total number of elements. </span><span class="koboSpan" id="kobo.298.3">After that, we used the </span><strong class="source-inline"><span class="koboSpan" id="kobo.299.1">cvpartition()</span></strong><span class="koboSpan" id="kobo.300.1"> function to establish a random partition for a dataset. </span><span class="koboSpan" id="kobo.300.2">We can utilize this partition to create training and test subsets, which are crucial for validating a statistical model. </span><span class="koboSpan" id="kobo.300.3">We used the two object functions, </span><strong class="source-inline"><span class="koboSpan" id="kobo.301.1">training</span></strong><span class="koboSpan" id="kobo.302.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.303.1">test</span></strong><span class="koboSpan" id="kobo.304.1">, to extract the training data index and the test data index from the initial dataset. </span><span class="koboSpan" id="kobo.304.2">After that, we applied these indices to extract </span><span class="No-Break"><span class="koboSpan" id="kobo.305.1">the data.</span></span></p></li> <li><span class="koboSpan" id="kobo.306.1">Now, we can build the logistic regression model. </span><span class="koboSpan" id="kobo.306.2">Logistic regression is a statistical method used for binary classification, but it can be extended for multiclass classification too. </span><span class="koboSpan" id="kobo.306.3">It’s a type of regression analysis that models the probability of a binary outcome, often denoted as </span><strong class="source-inline"><span class="koboSpan" id="kobo.307.1">0</span></strong><span class="koboSpan" id="kobo.308.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.309.1">1</span></strong><span class="koboSpan" id="kobo.310.1">. </span><span class="koboSpan" id="kobo.310.2">Logistic regression is widely used in various fields, including medicine, social sciences, economics, and ML. </span><span class="koboSpan" id="kobo.310.3">Logistic regression is used to predict the probability of an observation belonging to one of two classes or categories. </span><span class="koboSpan" id="kobo.310.4">It’s not suitable for problems with more than two classes; for such cases, multinomial logistic regression or other techniques are used. </span><span class="koboSpan" id="kobo.310.5">The logistic regression model uses a </span><strong class="source-inline"><span class="koboSpan" id="kobo.311.1">sigmoid</span></strong> <em class="italic"><span class="koboSpan" id="kobo.312.1">(logistic)</span></em><span class="koboSpan" id="kobo.313.1"> function to map input features to a probability between </span><strong class="source-inline"><span class="koboSpan" id="kobo.314.1">0</span></strong><span class="koboSpan" id="kobo.315.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.316.1">1</span></strong><span class="koboSpan" id="kobo.317.1">. </span><span class="koboSpan" id="kobo.317.2">Logistic regression is a powerful and interpretable method for binary classification tasks. </span><span class="koboSpan" id="kobo.317.3">It serves as a foundational technique in ML and statistics and is especially useful when you need to understand the relationship between input features and</span><a id="_idIndexMarker1235"/><span class="koboSpan" id="kobo.318.1"> the</span><a id="_idIndexMarker1236"/><span class="koboSpan" id="kobo.319.1"> probability of a binary outcome. </span><span class="koboSpan" id="kobo.319.2">Let’s go to train </span><span class="No-Break"><span class="koboSpan" id="kobo.320.1">the model:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.321.1">
LogRegModel = fitglm(TrainData{:,1:2},TrainData{:,3},
'Distribution','binomial','link', 'logit')</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.322.1">We used the </span><strong class="source-inline"><span class="koboSpan" id="kobo.323.1">fitglm()</span></strong><span class="koboSpan" id="kobo.324.1"> function for creating a logistic regression model: this function provides a generalized linear model fitted to the variables in the table or dataset array. </span><span class="koboSpan" id="kobo.324.2">By default, it designates the last variable as the response variable. </span><span class="koboSpan" id="kobo.324.3">We set a binomial distribution of the response variable and </span><strong class="source-inline"><span class="koboSpan" id="kobo.325.1">logit</span></strong><span class="koboSpan" id="kobo.326.1"> as a link function. </span><span class="koboSpan" id="kobo.326.2">The link function </span><em class="italic"><span class="koboSpan" id="kobo.327.1">f(μ) = log(μ/(1–μ))</span></em><span class="koboSpan" id="kobo.328.1"> is associated with logistic regression. </span><span class="koboSpan" id="kobo.328.2">It transforms the linear combination of predictors into a probability range (</span><strong class="source-inline"><span class="koboSpan" id="kobo.329.1">0</span></strong><span class="koboSpan" id="kobo.330.1"> to </span><strong class="source-inline"><span class="koboSpan" id="kobo.331.1">1</span></strong><span class="koboSpan" id="kobo.332.1">). </span><span class="koboSpan" id="kobo.332.2">In this formula, </span><em class="italic"><span class="koboSpan" id="kobo.333.1">\(\mu\)</span></em><span class="koboSpan" id="kobo.334.1"> represents the probability of the dependent variable being </span><strong class="source-inline"><span class="koboSpan" id="kobo.335.1">1</span></strong><span class="koboSpan" id="kobo.336.1">. </span><span class="koboSpan" id="kobo.336.2">The logarithmic transformation maps the odds of an event occurring in a linear space, facilitating modeling in the context of binary outcomes. </span><span class="koboSpan" id="kobo.336.3">As the logistic function’s range is (</span><strong class="source-inline"><span class="koboSpan" id="kobo.337.1">0</span></strong><span class="koboSpan" id="kobo.338.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.339.1">1</span></strong><span class="koboSpan" id="kobo.340.1">), the log-odds ratio spans the entire real number line. </span><span class="koboSpan" id="kobo.340.2">This link function is crucial in logistic regression, aiding in the estimation of probabilities and prediction of </span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">binary outcomes.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.342.1">The distribution refers to the probability distribution that describes the variability of the response variable (dependent variable) in the model. </span><span class="koboSpan" id="kobo.342.2">In other words, it specifies the form of the likelihood function that the model assumes for the response variable. </span><span class="koboSpan" id="kobo.342.3">The choice of distribution is based on the nature of the response variable and the assumptions of the model. </span><span class="koboSpan" id="kobo.342.4">The link function defines the relationship between the linear predictor and the expected value of the response variable. </span><span class="koboSpan" id="kobo.342.5">We model the linear predictor as a linear combination of the predictors’ coefficients. </span><span class="koboSpan" id="kobo.342.6">The link function transforms this linear predictor into the </span><a id="_idIndexMarker1237"/><span class="koboSpan" id="kobo.343.1">expected </span><a id="_idIndexMarker1238"/><span class="koboSpan" id="kobo.344.1">value of the </span><span class="No-Break"><span class="koboSpan" id="kobo.345.1">response variable.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.346.1">The following model was returned from the </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">training process:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.348.1">LogRegModel =
Generalized linear regression model:
    logit(y) ~ 1 + x1 + x2
    Distribution = Binomial
Estimated Coefficients:
          Estimate        SE         tStat      pValue
        __________    _________    ________    _______
(Intercept)0.00028422   0.016903   0.016814    0.98658
x1         0.00041526  0.0028803    0.14417    0.88537
x2         0.00086682  0.004003     0.21654    0.82857
14000 observations, 13997 error degrees of freedom
Dispersion: 1
Chi^2-statistic vs. </span><span class="koboSpan" id="kobo.348.2">constant model: 0.0608, p-value = 0.97</span></pre></li> <li><span class="koboSpan" id="kobo.349.1">After training the model, we can use it to identify anomalies, starting from data that the algorithm has never seen before. </span><span class="koboSpan" id="kobo.349.2">To do this, we will use the </span><span class="No-Break"><span class="koboSpan" id="kobo.350.1">test dataset:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.351.1">
PredY = predict(LogRegModel, TestData{:,1:2});</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.352.1">We used the </span><strong class="source-inline"><span class="koboSpan" id="kobo.353.1">predict()</span></strong><span class="koboSpan" id="kobo.354.1"> function; this function provides the forecasted response values generated by the generalized linear regression model. </span><span class="koboSpan" id="kobo.354.2">After that, we have to also obtain binary predictions based on </span><span class="No-Break"><span class="koboSpan" id="kobo.355.1">a threshold:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.356.1">threshold = 0.5;
PredBin = PredY &gt;= threshold;</span></pre></li> <li><span class="koboSpan" id="kobo.357.1">In this way, all the </span><strong class="source-inline"><span class="koboSpan" id="kobo.358.1">&gt;=</span></strong><span class="koboSpan" id="kobo.359.1"> values of the threshold are transformed into 1 and all the </span><strong class="source-inline"><span class="koboSpan" id="kobo.360.1">&gt;</span></strong><span class="koboSpan" id="kobo.361.1"> values of the threshold are transformed </span><span class="No-Break"><span class="koboSpan" id="kobo.362.1">into 0.</span></span></li>
<li><span class="koboSpan" id="kobo.363.1">We now can</span><a id="_idIndexMarker1239"/><span class="koboSpan" id="kobo.364.1"> calculate </span><a id="_idIndexMarker1240"/><span class="koboSpan" id="kobo.365.1">the accuracy of the model. </span><span class="koboSpan" id="kobo.365.2">This term refers to the degree to which a predictive model’s predictions align with the actual or observed values. </span><span class="koboSpan" id="kobo.365.3">It is a measure of how well the model performs in making </span><span class="No-Break"><span class="koboSpan" id="kobo.366.1">correct predictions:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.367.1">
accuracy = sum(PredBin == TestData{:,3}) / length(TestData{:,3});
fprintf('Accuracy: %.2f%%\n', accuracy * 100);</span></pre></li> <li><span class="koboSpan" id="kobo.368.1">The following result </span><span class="No-Break"><span class="koboSpan" id="kobo.369.1">is printed:</span></span><pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.370.1">Accuracy: 48.82%</span></strong></pre></li> <li><span class="koboSpan" id="kobo.371.1">Certainly not a high value of accuracy, which tells us that half of the predictions are wrong. </span><span class="koboSpan" id="kobo.371.2">A non-linear model to handle non-linearity in the data is needed. </span><span class="koboSpan" id="kobo.371.3">Logistic regression being a linear model fails to capture the non-linear relationship between dependent and independent variables. </span><span class="koboSpan" id="kobo.371.4">Surely, we can increase the accuracy by adopting another algorithm, as we will see in the </span><span class="No-Break"><span class="koboSpan" id="kobo.372.1">next section.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.373.1">Let’s move on</span><a id="_idIndexMarker1241"/><span class="koboSpan" id="kobo.374.1"> to</span><a id="_idIndexMarker1242"/><span class="koboSpan" id="kobo.375.1"> improving accuracy using the Random Forest </span><span class="No-Break"><span class="koboSpan" id="kobo.376.1">algorithm next.</span></span></p>
<h2 id="_idParaDest-225"><a id="_idTextAnchor233"/><span class="koboSpan" id="kobo.377.1">Improving accuracy using the Random Forest algorithm</span></h2>
<p><span class="koboSpan" id="kobo.378.1">Random Forest is </span><a id="_idIndexMarker1243"/><span class="koboSpan" id="kobo.379.1">a powerful</span><a id="_idIndexMarker1244"/><span class="koboSpan" id="kobo.380.1"> ensemble learning algorithm used for both classification and regression tasks. </span><span class="koboSpan" id="kobo.380.2">It is an extension of the </span><strong class="bold"><span class="koboSpan" id="kobo.381.1">Bootstrap Aggregating</span></strong><span class="koboSpan" id="kobo.382.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.383.1">Bagging</span></strong><span class="koboSpan" id="kobo.384.1">) algorithm that builds multiple decision trees during training</span><a id="_idIndexMarker1245"/><span class="koboSpan" id="kobo.385.1"> and combines their predictions to improve accuracy and reduce overfitting. </span><span class="koboSpan" id="kobo.385.2">Random Forest begins by creating multiple random samples (with replacements) from the original dataset. </span><span class="koboSpan" id="kobo.385.3">Each of these samples is </span><a id="_idIndexMarker1246"/><span class="koboSpan" id="kobo.386.1">called a </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.387.1">bootstrap sample</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.388.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.389.1">For each bootstrap sample, a decision tree is constructed. </span><span class="koboSpan" id="kobo.389.2">These trees are often referred to as </span><strong class="bold"><span class="koboSpan" id="kobo.390.1">base learners</span></strong><span class="koboSpan" id="kobo.391.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.392.1">weak learners</span></strong><span class="koboSpan" id="kobo.393.1">. </span><span class="koboSpan" id="kobo.393.2">Decision</span><a id="_idIndexMarker1247"/><span class="koboSpan" id="kobo.394.1"> trees are grown with the </span><span class="No-Break"><span class="koboSpan" id="kobo.395.1">following </span></span><span class="No-Break"><a id="_idIndexMarker1248"/></span><span class="No-Break"><span class="koboSpan" id="kobo.396.1">characteristics:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.397.1">Random feature selection</span></strong><span class="koboSpan" id="kobo.398.1">: At each node of the tree, a random subset of features is considered for splitting, reducing the correlation </span><span class="No-Break"><span class="koboSpan" id="kobo.399.1">between trees</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.400.1">Limited depth</span></strong><span class="koboSpan" id="kobo.401.1">: Trees are often grown to a certain depth to </span><span class="No-Break"><span class="koboSpan" id="kobo.402.1">prevent overfitting</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.403.1">No pruning</span></strong><span class="koboSpan" id="kobo.404.1">: The Random Forest algorithm typically does not </span><span class="No-Break"><span class="koboSpan" id="kobo.405.1">prune trees</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.406.1">For classification tasks, each tree in the Random Forest algorithmvotes for the class it predicts. </span><span class="koboSpan" id="kobo.406.2">The class that receives the most votes is the final prediction. </span><span class="koboSpan" id="kobo.406.3">For regression tasks, each tree in the Random Forest predicts a numeric value. </span><span class="koboSpan" id="kobo.406.4">The final prediction is the average of these individual tree predictions. </span><span class="koboSpan" id="kobo.406.5">The majority vote (for classification) or averaging (for regression) results in the </span><span class="No-Break"><span class="koboSpan" id="kobo.407.1">ensemble decision.</span></span></p>
<p><span class="koboSpan" id="kobo.408.1">Random Forest includes a mechanism to estimate the model’s accuracy without the need for a separate test set. </span><span class="koboSpan" id="kobo.408.2">This is known as </span><a id="_idIndexMarker1249"/><span class="koboSpan" id="kobo.409.1">the </span><strong class="bold"><span class="koboSpan" id="kobo.410.1">out-of-bag</span></strong><span class="koboSpan" id="kobo.411.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.412.1">OOB</span></strong><span class="koboSpan" id="kobo.413.1">) error. </span><span class="koboSpan" id="kobo.413.2">For each data point, it estimates the error rate using only the trees that did not use this point during training. </span><span class="koboSpan" id="kobo.413.3">Random Forest can provide an estimate of the importance of each feature in the dataset. </span><span class="koboSpan" id="kobo.413.4">This is done by measuring the decrease in prediction accuracy when the values of a particular feature are </span><span class="No-Break"><span class="koboSpan" id="kobo.414.1">randomly shuffled.</span></span></p>
<p><span class="koboSpan" id="kobo.415.1">The following list provides key characteristics and advantages of using </span><span class="No-Break"><span class="koboSpan" id="kobo.416.1">Random Forest:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.417.1">Random Forest is known for its strong generalization performance and resistance </span><span class="No-Break"><span class="koboSpan" id="kobo.418.1">to overfitting</span></span></li>
<li><span class="koboSpan" id="kobo.419.1">It can handle both categorical and </span><span class="No-Break"><span class="koboSpan" id="kobo.420.1">numerical data</span></span></li>
<li><span class="koboSpan" id="kobo.421.1">The random feature selection and ensemble nature make it robust to noisy data </span><span class="No-Break"><span class="koboSpan" id="kobo.422.1">and outliers</span></span></li>
<li><span class="koboSpan" id="kobo.423.1">Random Forest can be used for feature selection, as it provides a feature </span><span class="No-Break"><span class="koboSpan" id="kobo.424.1">importance ranking</span></span></li>
<li><span class="koboSpan" id="kobo.425.1">It works well “out of the box” without much tuning </span><span class="No-Break"><span class="koboSpan" id="kobo.426.1">of hyperparameters</span></span></li>
<li><span class="koboSpan" id="kobo.427.1">It can be used for both classification and </span><span class="No-Break"><span class="koboSpan" id="kobo.428.1">regression tasks</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.429.1">Overall, Random Forest is a popular and effective ML algorithm used in a wide range of applications, from image classification and text analysis to financial modeling </span><span class="No-Break"><span class="koboSpan" id="kobo.430.1">and healthcare.</span></span></p>
<p><span class="koboSpan" id="kobo.431.1">To implement an automated system to detect anomalies in gearbox data, we will use the same data </span><a id="_idIndexMarker1250"/><span class="koboSpan" id="kobo.432.1">already used in </span><a id="_idIndexMarker1251"/><span class="koboSpan" id="kobo.433.1">the </span><em class="italic"><span class="koboSpan" id="kobo.434.1">Anomaly detection using logistic regression</span></em><span class="koboSpan" id="kobo.435.1"> section. </span><span class="koboSpan" id="kobo.435.2">Refer to the </span><span class="No-Break"><span class="koboSpan" id="kobo.436.1">following steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.437.1">Random Forest involves bootstrapping, where we create multiple random samples (with replacement) from the dataset. </span><span class="koboSpan" id="kobo.437.2">We can do this to create different subsets of your data. </span><span class="koboSpan" id="kobo.437.3">We start by defining some parameters </span><span class="No-Break"><span class="koboSpan" id="kobo.438.1">as follows:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.439.1">
numTrees = 100;
numSamples = size(TrainData, 1);
sampleSize = round(0.6 * numSamples);  % Adjust as needed</span></pre></li> <li><span class="koboSpan" id="kobo.440.1">We defined the number of trees in the forest (</span><strong class="source-inline"><span class="koboSpan" id="kobo.441.1">numTrees</span></strong><span class="koboSpan" id="kobo.442.1">), the number of observations that we will use in the training (</span><strong class="source-inline"><span class="koboSpan" id="kobo.443.1">numSamples</span></strong><span class="koboSpan" id="kobo.444.1">), and the sample </span><span class="No-Break"><span class="koboSpan" id="kobo.445.1">size (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.446.1">sampleSize</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.447.1">).</span></span></li>
<li><span class="koboSpan" id="kobo.448.1">We will pass </span><strong class="source-inline"><span class="koboSpan" id="kobo.449.1">forest</span></strong><span class="koboSpan" id="kobo.450.1"> to initialize the forest, setting the data as a </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.451.1">cell</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.452.1"> structure:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.453.1">
forest = cell(numTrees, 1);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.454.1">Now, we will use a </span><strong class="source-inline"><span class="koboSpan" id="kobo.455.1">for</span></strong><span class="koboSpan" id="kobo.456.1"> loop to repeatedly execute a block of code a specific number of times or iterate over a sequence. </span><span class="koboSpan" id="kobo.456.2">This will be necessary to create a random sample with a replacement first and to then train a decision tree on </span><span class="No-Break"><span class="koboSpan" id="kobo.457.1">the sample:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.458.1">for i = 1:numTrees
    sampleIndices = randi(numSamples, sampleSize, 1);
    X_sample = TrainData{:,1:2}(sampleIndices, :);
    y_sample = TrainData{:,3}(sampleIndices);
    tree = fitctree(X_sample, y_sample);
    forest{i} = tree;
end</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.459.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.460.1">fitctree()</span></strong><span class="koboSpan" id="kobo.461.1"> function trains a binary decision tree for multiclass classification. </span><span class="koboSpan" id="kobo.461.2">It </span><a id="_idIndexMarker1252"/><span class="koboSpan" id="kobo.462.1">returns a trained </span><a id="_idIndexMarker1253"/><span class="koboSpan" id="kobo.463.1">binary classification decision tree using the input variables found in the table and the output responses. </span><span class="koboSpan" id="kobo.463.2">The binary tree forms a split at branching nodes using values from a column in </span><span class="No-Break"><span class="koboSpan" id="kobo.464.1">the table.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.465.1">The process involves randomly selecting samples to simulate bootstrap sampling, yet the Random Forest classifier typically chooses </span><strong class="source-inline"><span class="koboSpan" id="kobo.466.1">\( \sqrt{p} \)</span></strong><span class="koboSpan" id="kobo.467.1"> features for each decision tree. </span><span class="koboSpan" id="kobo.467.2">In the context of two features, every decision tree would ideally operate with a single feature when introduced into the </span><strong class="source-inline"><span class="koboSpan" id="kobo.468.1">for</span></strong><span class="koboSpan" id="kobo.469.1"> loop. </span><span class="koboSpan" id="kobo.469.2">Consequently, substantial performance enhancement isn’t anticipated in </span><span class="No-Break"><span class="koboSpan" id="kobo.470.1">this scenario.</span></span></p></li> <li><span class="koboSpan" id="kobo.471.1">Now, we can use the model to test the </span><span class="No-Break"><span class="koboSpan" id="kobo.472.1">model performance:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.473.1">
numTestSamples = size(TestData, 1);
RPredY = zeros(numTestSamples, numTrees);</span></pre></li> <li><span class="koboSpan" id="kobo.474.1">We calculated the number of observations in the test data and we initialized the prediction using a matrix </span><span class="No-Break"><span class="koboSpan" id="kobo.475.1">of zeros.</span></span></li>
<li><span class="koboSpan" id="kobo.476.1">We can now make </span><span class="No-Break"><span class="koboSpan" id="kobo.477.1">the predictions:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.478.1">
for i = 1:numTrees
    tree = forest{i};
    RPredY(:, i) = predict(tree, TestData{:,1:2});
end</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.479.1">Once again, we used the </span><strong class="source-inline"><span class="koboSpan" id="kobo.480.1">predict()</span></strong><span class="koboSpan" id="kobo.481.1"> function to make predictions on the dataset never seen before by </span><span class="No-Break"><span class="koboSpan" id="kobo.482.1">the algorithm.</span></span></p></li> <li><span class="koboSpan" id="kobo.483.1">Finally, we will calculate the performance of the model using </span><span class="No-Break"><span class="koboSpan" id="kobo.484.1">the accuracy:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.485.1">
FinalRPredY = mode(RPredY, 2);
accuracy = sum(FinalRPredY == TestData{:,3}) / length(TestData{:,3});</span></pre></li> <li><span class="koboSpan" id="kobo.486.1">We used the </span><strong class="source-inline"><span class="koboSpan" id="kobo.487.1">mode()</span></strong><span class="koboSpan" id="kobo.488.1"> function, which evaluates the most frequent values in an array. </span><span class="koboSpan" id="kobo.488.2">This function provides the mode of elements along a specified dimension. </span><span class="koboSpan" id="kobo.488.3">In our case, this function returns results in a column vector that holds the most common value for </span><span class="No-Break"><span class="koboSpan" id="kobo.489.1">each row.</span></span></li>
<li><span class="koboSpan" id="kobo.490.1">The following accuracy </span><span class="No-Break"><span class="koboSpan" id="kobo.491.1">is returned:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.492.1">
accuracy
accuracy =
    0.5765</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.493.1">We </span><a id="_idIndexMarker1254"/><span class="koboSpan" id="kobo.494.1">obtained the </span><span class="No-Break"><span class="koboSpan" id="kobo.495.1">improvement</span></span><span class="No-Break"><a id="_idIndexMarker1255"/></span><span class="No-Break"><span class="koboSpan" id="kobo.496.1"> requested.</span></span></p></li> </ol>
<p><span class="koboSpan" id="kobo.497.1">In the next section, we will see how to implement a fault diagnosis system using the acoustic emission</span><a id="_idIndexMarker1256"/><span class="koboSpan" id="kobo.498.1"> data of an </span><strong class="bold"><span class="koboSpan" id="kobo.499.1">unmanned aerial </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.500.1">vehicle</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.501.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.502.1">UAV</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.503.1">).</span></span></p>
<h1 id="_idParaDest-226"><a id="_idTextAnchor234"/><span class="koboSpan" id="kobo.504.1">Building a fault diagnosis system using MATLAB</span></h1>
<p><span class="koboSpan" id="kobo.505.1">Technological </span><a id="_idIndexMarker1257"/><span class="koboSpan" id="kobo.506.1">advancements have given rise to</span><a id="_idIndexMarker1258"/><span class="koboSpan" id="kobo.507.1"> highly capable aircraft that can autonomously manage flight operations. </span><span class="koboSpan" id="kobo.507.2">These aircraft belong to the category known as UAVs, meaning they can fly without human pilots. </span><span class="koboSpan" id="kobo.507.3">UAVs offer numerous advantages, including significantly reduced operating costs compared to traditional piloted aircraft, the ability to operate in environments unsuitable for human presence, and the capacity for timely aerial surveillance, such as during </span><span class="No-Break"><span class="koboSpan" id="kobo.508.1">natural disasters.</span></span></p>
<p><span class="koboSpan" id="kobo.509.1">Initially employed primarily for military purposes, UAVs were used in dull missions involving monotonous and lengthy surveillance and reconnaissance, as well as in dirty missions that posed risks to human pilots’ safety. </span><span class="koboSpan" id="kobo.509.2">They also played a crucial role in dangerous missions where human lives were at risk. </span><span class="koboSpan" id="kobo.509.3">However, today, they are considered the future of modern aeronautics. </span><span class="koboSpan" id="kobo.509.4">The vast potential of UAV technology, its successes in military operations, and advancements in micro- and nanotechnologies have spurred both industries and universities to develop increasingly modern and reliable UAV systems. </span><span class="koboSpan" id="kobo.509.5">These systems can be utilized across a wide spectrum of civilian and </span><span class="No-Break"><span class="koboSpan" id="kobo.510.1">military missions.</span></span></p>
<p><span class="koboSpan" id="kobo.511.1">The widespread proliferation of UAVs worldwide has brought to light uncharted issues. </span><span class="koboSpan" id="kobo.511.2">While the convenience of using these devices is evident, they also introduce a host of challenges and potential hazards associated with this technology. </span><span class="koboSpan" id="kobo.511.3">Among concerns that have arisen is the safety of UAV flights—what would be the consequences if a UAV were to crash to the ground? </span><span class="koboSpan" id="kobo.511.4">An automatic system for identifying a defect on the propeller of a UAV, therefore, represents a tool that can guarantee us an acceptable level </span><span class="No-Break"><span class="koboSpan" id="kobo.512.1">of safety.</span></span></p>
<p><span class="koboSpan" id="kobo.513.1">In this example, we will develop an algorithm for fault diagnosis of a UAV propeller from acoustic </span><span class="No-Break"><span class="koboSpan" id="kobo.514.1">emission measurements:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.515.1">We start importing the dataset into the </span><span class="No-Break"><span class="koboSpan" id="kobo.516.1">MATLAB workspace:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.517.1">
UAVData = readtable('DroneFaultDiagnosis.xlsx');</span></pre></li> <li><span class="koboSpan" id="kobo.518.1">There are 847 observations, 32 features, 31 predictors, and 1 binary response. </span><span class="koboSpan" id="kobo.518.2">We can explore the data using </span><span class="No-Break"><span class="koboSpan" id="kobo.519.1">a boxplot:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.520.1">
boxplot(UAVData{:,1:31})
xlabel('Features')
ylabel('Leq (dBA)')</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.521.1">The following plot </span><span class="No-Break"><span class="koboSpan" id="kobo.522.1">will display:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer103">
<span class="koboSpan" id="kobo.523.1"><img alt="Figure 11.2 – Boxplot of the predictors" src="image/B21156_11_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.524.1">Figure 11.2 – Boxplot of the predictors</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.525.1">We can see that</span><a id="_idIndexMarker1259"/><span class="koboSpan" id="kobo.526.1"> more </span><a id="_idIndexMarker1260"/><span class="koboSpan" id="kobo.527.1">variability in the data is shown for the low frequencies on the left of the graph (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.528.1">Figure 11</span></em></span><em class="italic"><span class="koboSpan" id="kobo.529.1">.2</span></em><span class="koboSpan" id="kobo.530.1">). </span><span class="koboSpan" id="kobo.530.2">This tells us that it is the low frequencies that will allow us to identify damage to the UAV’s propeller. </span><span class="koboSpan" id="kobo.530.3">Low frequencies are valuable for identifying damage to a UAV’s propeller due to their unique acoustic signatures. </span><span class="koboSpan" id="kobo.530.4">When a propeller is damaged, it often produces distinctive low-frequency vibrations or sounds that are not easily discernible in higher frequency ranges. </span><span class="koboSpan" id="kobo.530.5">These low-frequency signals can be indicative of structural issues, such as cracks, imbalances, or deformities, which might not be as prominent in higher frequency ranges. </span><span class="koboSpan" id="kobo.530.6">By focusing on low frequencies, you enhance sensitivity to subtle but crucial vibrations associated with propeller damage, providing a more effective means of early detection and preventive maintenance </span><span class="No-Break"><span class="koboSpan" id="kobo.531.1">for UAVs.</span></span></p>
<ol>
<li value="3"><span class="koboSpan" id="kobo.532.1">Now, we have to partition data for </span><span class="No-Break"><span class="koboSpan" id="kobo.533.1">the model:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.534.1">
n= height(UAVData)
RandSplitInd=randperm(n);
DivNum=round(0.7*n);
TrainData=UAVData(RandSplitInd(1:DivNum),:);
TestData=UAVData(RandSplitInd(DivNum+1:end),:);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.535.1">We first extracted the number of observations of the data using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.536.1">height()</span></strong><span class="koboSpan" id="kobo.537.1"> function, which returns the number of table rows. </span><span class="koboSpan" id="kobo.537.2">Then, we used </span><strong class="source-inline"><span class="koboSpan" id="kobo.538.1">randperm()</span></strong><span class="koboSpan" id="kobo.539.1">; this function generates a row vector that represents a random permutation of integers from 1 to </span><em class="italic"><span class="koboSpan" id="kobo.540.1">n</span></em><span class="koboSpan" id="kobo.541.1">, ensuring that no elements are repeated. </span><span class="koboSpan" id="kobo.541.2">After that, we decided to divide the data into 70% for the training set and the remaining 30% for the test set. </span><span class="koboSpan" id="kobo.541.3">Finally, we used the indices obtained to split the dataset into two subsets to train and test </span><span class="No-Break"><span class="koboSpan" id="kobo.542.1">the model.</span></span></p></li> <li><span class="koboSpan" id="kobo.543.1">To detect faults in</span><a id="_idIndexMarker1261"/><span class="koboSpan" id="kobo.544.1"> the</span><a id="_idIndexMarker1262"/><span class="koboSpan" id="kobo.545.1"> UAV propeller, we will use SVMs for </span><em class="italic"><span class="koboSpan" id="kobo.546.1">binary classification</span></em><span class="koboSpan" id="kobo.547.1">. </span><span class="koboSpan" id="kobo.547.2">SVMs represent a category of </span><strong class="bold"><span class="koboSpan" id="kobo.548.1">supervised ML</span></strong><span class="koboSpan" id="kobo.549.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.550.1">SML</span></strong><span class="koboSpan" id="kobo.551.1">) models </span><a id="_idIndexMarker1263"/><span class="koboSpan" id="kobo.552.1">applied in tasks such as classification and regression analysis. </span><span class="koboSpan" id="kobo.552.2">They are powerful and versatile algorithms that excel in various applications, including image classification, text classification, and outlier detection. </span><span class="koboSpan" id="kobo.552.3">SVMs are primarily designed for binary classification problems, where the goal is to separate data points into two distinct classes. </span><span class="koboSpan" id="kobo.552.4">The classifier finds a hyperplane that best separates the data, maximizing the margin between the classes. </span><span class="koboSpan" id="kobo.552.5">The margin is the distance between the hyperplane and the nearest data points from each class. </span><span class="koboSpan" id="kobo.552.6">SVMs aim to maximize this margin because a larger margin implies better generalization of unseen data and </span><span class="No-Break"><span class="koboSpan" id="kobo.553.1">improved robustness.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.554.1">Support Vector Classification</span></strong><span class="koboSpan" id="kobo.555.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.556.1">SVC</span></strong><span class="koboSpan" id="kobo.557.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.558.1">Support Vector Regression</span></strong><span class="koboSpan" id="kobo.559.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.560.1">SVR</span></strong><span class="koboSpan" id="kobo.561.1">) are both</span><a id="_idIndexMarker1264"/><span class="koboSpan" id="kobo.562.1"> types of SVMs. </span><span class="koboSpan" id="kobo.562.2">SVC is employed for</span><a id="_idIndexMarker1265"/><span class="koboSpan" id="kobo.563.1"> classification tasks, aiming to separate data into distinct categories, while SVR is used for regression, predicting continuous outcomes. </span><span class="koboSpan" id="kobo.563.2">In both cases, the SVMs work by finding the optimal hyperplane that maximizes the margin between different classes or fits the data points for regression. </span><span class="koboSpan" id="kobo.563.3">They are effective in high-dimensional spaces and versatile in handling complex relationships in data, making them valuable tools in various </span><span class="No-Break"><span class="koboSpan" id="kobo.564.1">ML applications.</span></span></li>
<li><span class="koboSpan" id="kobo.565.1">Support vectors are data points closest to the decision boundary (hyperplane). </span><span class="koboSpan" id="kobo.565.2">These points are critical in defining the margin and the position of the hyperplane. </span><span class="koboSpan" id="kobo.565.3">SVMs derive their name from these support vectors. </span><span class="koboSpan" id="kobo.565.4">In a two-dimensional space, the hyperplane is a straight line that separates the data into two classes. </span><span class="koboSpan" id="kobo.565.5">In higher dimensions, it’s a hyperplane. </span><span class="koboSpan" id="kobo.565.6">The goal is to find the hyperplane that maximizes the margin while correctly classifying as many data points as possible. </span><span class="koboSpan" id="kobo.565.7">SVMs can be applied to non-linear problems by transforming the input data into a higher-dimensional space, where a linear separation is possible. </span><span class="koboSpan" id="kobo.565.8">Various kernel functions, such as the polynomial kernel, </span><strong class="bold"><span class="koboSpan" id="kobo.566.1">radial basis function</span></strong><span class="koboSpan" id="kobo.567.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.568.1">RBF</span></strong><span class="koboSpan" id="kobo.569.1">) kernel, and </span><a id="_idIndexMarker1266"/><span class="koboSpan" id="kobo.570.1">sigmoid kernel, are used to achieve this. </span><span class="koboSpan" id="kobo.570.2">SVMs are known for their ability </span><a id="_idIndexMarker1267"/><span class="koboSpan" id="kobo.571.1">to</span><a id="_idIndexMarker1268"/><span class="koboSpan" id="kobo.572.1"> provide robust models with strong generalization capabilities, particularly when dealing with high-dimensional data. </span><span class="koboSpan" id="kobo.572.2">SVMs are widely used in ML and have shown effectiveness in a variety of applications. </span><span class="koboSpan" id="kobo.572.3">While they excel in many scenarios, selecting the appropriate kernel and tuning hyperparameters can be critical for their success. </span><span class="koboSpan" id="kobo.572.4">Let's apply SVM model to identify a fault in </span><span class="No-Break"><span class="koboSpan" id="kobo.573.1">UAV blades.</span></span><pre class="source-code"><span class="koboSpan" id="kobo.574.1">
SVMModel = fitcsvm(TrainData{:,1:31}, TrainData{:,32}, 'KernelFunction', 'linear', 'BoxConstraint', 1);</span></pre></li> <li><span class="koboSpan" id="kobo.575.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.576.1">fitcsvm()</span></strong><span class="koboSpan" id="kobo.577.1"> function trains an SVM model for classification, which can be used for one-class or two-class (binary) problems, on a predictor dataset with low or </span><span class="No-Break"><span class="koboSpan" id="kobo.578.1">moderate dimensions.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.579.1">The following parameters </span><span class="No-Break"><span class="koboSpan" id="kobo.580.1">are passed:</span></span></p><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.581.1">TrainData{:,1:31}</span></strong><span class="koboSpan" id="kobo.582.1">: The training data for the model is represented as a table, where each row represents an observation, and each column represents a predictor variable. </span><span class="koboSpan" id="kobo.582.2">Multicolumn variables and non-character cell arrays are </span><span class="No-Break"><span class="koboSpan" id="kobo.583.1">not supported.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.584.1">TrainData{:,32}</span></strong><span class="koboSpan" id="kobo.585.1">: The name of the response variable should be provided as a character vector or </span><span class="No-Break"><span class="koboSpan" id="kobo.586.1">string scalar.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.587.1">KernelFunction</span></strong><span class="koboSpan" id="kobo.588.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.589.1">linear</span></strong><span class="koboSpan" id="kobo.590.1">: The choice of kernel function used to calculate the elements of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.591.1">Gram</span></strong><span class="koboSpan" id="kobo.592.1"> matrix is specified as a pair using </span><strong class="source-inline"><span class="koboSpan" id="kobo.593.1">KernelFunction</span></strong><span class="koboSpan" id="kobo.594.1"> followed by the name of the kernel function, provided as a comma-separated pair. </span><span class="koboSpan" id="kobo.594.2">The following kernel functions are available: </span><strong class="source-inline"><span class="koboSpan" id="kobo.595.1">gaussian, linear, </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.596.1">and polynomial</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.597.1">.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.598.1">BoxConstraint</span></strong><span class="koboSpan" id="kobo.599.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.600.1">1</span></strong><span class="koboSpan" id="kobo.601.1">: This parameter controls the regularization strength. </span><span class="koboSpan" id="kobo.601.2">It determines how much error the SVM model is willing to tolerate to achieve a wider margin between the classes. </span><span class="koboSpan" id="kobo.601.3">A larger </span><strong class="source-inline"><span class="koboSpan" id="kobo.602.1">BoxConstraint</span></strong><span class="koboSpan" id="kobo.603.1"> value allows more tolerance for misclassifications, while a smaller value enforces a stricter fit to the </span><span class="No-Break"><span class="koboSpan" id="kobo.604.1">training data.</span></span></li></ul></li>
<li><span class="koboSpan" id="kobo.605.1">Now, we can use the </span><a id="_idIndexMarker1269"/><span class="koboSpan" id="kobo.606.1">SVM </span><a id="_idIndexMarker1270"/><span class="koboSpan" id="kobo.607.1">model to perform a fault diagnosis on the acoustic emission of a UAV. </span><span class="koboSpan" id="kobo.607.2">We will use the test dataset, which contains data that has not yet been seen by </span><span class="No-Break"><span class="koboSpan" id="kobo.608.1">the algorithm:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.609.1">
PredY = predict(SVMModel, TestData{:,1:31});
accuracy = sum(PredY == TestData{:,32}) / length(PredY);</span></pre></li> <li><span class="koboSpan" id="kobo.610.1">First, we used the </span><strong class="source-inline"><span class="koboSpan" id="kobo.611.1">predict()</span></strong><span class="koboSpan" id="kobo.612.1"> function to detect a fault in the UAV propeller, and then we calculated the accuracy of </span><span class="No-Break"><span class="koboSpan" id="kobo.613.1">the model.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.614.1">The following result </span><span class="No-Break"><span class="koboSpan" id="kobo.615.1">is returned:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.616.1">
accuracy
accuracy =
    0.9055</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.617.1">91% accuracy is a great result. </span><span class="koboSpan" id="kobo.617.2">This is evidence that an SVM is a good choice to predict a possible fault in the </span><span class="No-Break"><span class="koboSpan" id="kobo.618.1">UAV propeller.</span></span></p></li> </ol>
<p><span class="koboSpan" id="kobo.619.1">In the next part of</span><a id="_idIndexMarker1271"/><span class="koboSpan" id="kobo.620.1"> this </span><a id="_idIndexMarker1272"/><span class="koboSpan" id="kobo.621.1">chapter, we will introduce some methods to prevent overfitting and improve the performance </span><span class="No-Break"><span class="koboSpan" id="kobo.622.1">of algorithms.</span></span></p>
<h1 id="_idParaDest-227"><a id="_idTextAnchor235"/><span class="koboSpan" id="kobo.623.1">Understanding advanced regularization techniques</span></h1>
<p><span class="koboSpan" id="kobo.624.1">Advanced regularization</span><a id="_idIndexMarker1273"/><span class="koboSpan" id="kobo.625.1"> techniques are methods used in ML and statistical modeling to prevent overfitting and improve the generalization performance of models. </span><span class="koboSpan" id="kobo.625.2">Overfitting occurs when a model fits the training data too closely, capturing noise and irrelevant patterns, which leads to poor performance on unseen data. </span><span class="koboSpan" id="kobo.625.3">Regularization techniques introduce constraints or penalties to the model’s parameters during training to encourage simpler, more </span><span class="No-Break"><span class="koboSpan" id="kobo.626.1">generalized models.</span></span></p>
<h2 id="_idParaDest-228"><a id="_idTextAnchor236"/><span class="koboSpan" id="kobo.627.1">Understanding dropout</span></h2>
<p><span class="koboSpan" id="kobo.628.1">Dropout is a</span><a id="_idIndexMarker1274"/><span class="koboSpan" id="kobo.629.1"> regularization technique used in </span><a id="_idIndexMarker1275"/><span class="koboSpan" id="kobo.630.1">NNs, particularly </span><strong class="bold"><span class="koboSpan" id="kobo.631.1">deep NNs</span></strong><span class="koboSpan" id="kobo.632.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.633.1">DNNs</span></strong><span class="koboSpan" id="kobo.634.1">), to prevent overfitting. </span><span class="koboSpan" id="kobo.634.2">Overfitting occurs when an NN learns </span><a id="_idIndexMarker1276"/><span class="koboSpan" id="kobo.635.1">to fit the training data too closely, capturing noise and memorizing specific examples rather than generalizing from the data. </span><span class="koboSpan" id="kobo.635.2">Dropout is a simple yet effective method for improving a model’s </span><span class="No-Break"><span class="koboSpan" id="kobo.636.1">generalization performance.</span></span></p>
<p><span class="koboSpan" id="kobo.637.1">During the training phase, at each forward and backward pass, dropout randomly “drops out” (deactivates) a portion of the neurons (units) in the NN. </span><span class="koboSpan" id="kobo.637.2">This means that during the forward pass, the output of some neurons is set to zero with a certain probability, effectively excluding them from the computation. </span><span class="koboSpan" id="kobo.637.3">The dropout probability is a hyperparameter and determines the probability that a neuron will be dropped out. </span><span class="koboSpan" id="kobo.637.4">A common value is around 0.5, which means that each neuron has a 50% chance of being dropped out </span><span class="No-Break"><span class="koboSpan" id="kobo.638.1">during training.</span></span></p>
<p><span class="koboSpan" id="kobo.639.1">The dropout process introduces variability during training because, for each forward and backward pass, a different set of neurons is active, and the network effectively trains on different subnetworks. </span><span class="koboSpan" id="kobo.639.2">This variability forces the network to learn more robust features and prevents it from relying too heavily on any neuron or feature. </span><span class="koboSpan" id="kobo.639.3">During the inference or testing phase (when you use the trained model for predictions), dropout is typically turned off. </span><span class="koboSpan" id="kobo.639.4">However, the output of each neuron is scaled by a factor of </span><em class="italic"><span class="koboSpan" id="kobo.640.1">1/(1 - p)</span></em><span class="koboSpan" id="kobo.641.1">, where </span><em class="italic"><span class="koboSpan" id="kobo.642.1">p</span></em><span class="koboSpan" id="kobo.643.1"> is the dropout probability used during training. </span><span class="koboSpan" id="kobo.643.2">This scaling helps maintain the expected values of neuron activations, ensuring that the model’s behavior is consistent with what it learned </span><span class="No-Break"><span class="koboSpan" id="kobo.644.1">during training.</span></span></p>
<p><span class="koboSpan" id="kobo.645.1">The key benefits of dropout are </span><span class="No-Break"><span class="koboSpan" id="kobo.646.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.647.1">Regularization</span></strong><span class="koboSpan" id="kobo.648.1">: Dropout </span><a id="_idIndexMarker1277"/><span class="koboSpan" id="kobo.649.1">acts as a regularization technique by preventing the NN from overfitting the training data. </span><span class="koboSpan" id="kobo.649.2">Training on various subnetworks encourages the network to generalize better to </span><span class="No-Break"><span class="koboSpan" id="kobo.650.1">unseen data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.651.1">Reducing co-adaptation</span></strong><span class="koboSpan" id="kobo.652.1">: Dropout discourages neurons from relying too heavily on their neighboring neurons. </span><span class="koboSpan" id="kobo.652.2">This helps prevent the co-adaptation of neurons, where certain neurons become specialized to predict the output of other neurons. </span><span class="koboSpan" id="kobo.652.3">As a result, dropout can lead to more independent and </span><span class="No-Break"><span class="koboSpan" id="kobo.653.1">informative features.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.654.1">Ensembling effect</span></strong><span class="koboSpan" id="kobo.655.1">: The dropout process is akin to training multiple models with different architectures, which is similar to an ensemble of models. </span><span class="koboSpan" id="kobo.655.2">This ensembling effect can lead to improved </span><span class="No-Break"><span class="koboSpan" id="kobo.656.1">model performance.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.657.1">In practice, dropout is </span><a id="_idIndexMarker1278"/><span class="koboSpan" id="kobo.658.1">widely used in DL and is especially</span><a id="_idIndexMarker1279"/><span class="koboSpan" id="kobo.659.1"> effective when dealing with large and complex NNs. </span><span class="koboSpan" id="kobo.659.2">It’s a simple yet powerful technique for improving model generalization and reducing overfitting, making it an essential tool for training robust </span><span class="No-Break"><span class="koboSpan" id="kobo.660.1">DL models.</span></span></p>
<p><span class="koboSpan" id="kobo.661.1">Venturing into the intricacies of model regularization, the next section delves into the distinct realms of L1 and L2 regularization. </span><span class="koboSpan" id="kobo.661.2">Unraveling the nuances of these techniques, we explore how they contribute to enhancing the robustness and generalization capabilities of </span><span class="No-Break"><span class="koboSpan" id="kobo.662.1">ML models.</span></span></p>
<h2 id="_idParaDest-229"><a id="_idTextAnchor237"/><span class="koboSpan" id="kobo.663.1">Exploring L1 and L2 regularization</span></h2>
<p><span class="koboSpan" id="kobo.664.1">L1 and L2 regularization </span><a id="_idIndexMarker1280"/><span class="koboSpan" id="kobo.665.1">are two </span><a id="_idIndexMarker1281"/><span class="koboSpan" id="kobo.666.1">common techniques used in ML and statistical modeling to prevent overfitting by adding penalty terms to the loss function. </span><span class="koboSpan" id="kobo.666.2">They encourage the model to have smaller weights and, in the case of L1, can lead to </span><span class="No-Break"><span class="koboSpan" id="kobo.667.1">feature selection.</span></span></p>
<p><span class="koboSpan" id="kobo.668.1">Let’s introduce two </span><span class="No-Break"><span class="koboSpan" id="kobo.669.1">typical methods:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.670.1">L1 regularization (Lasso)</span></strong><span class="koboSpan" id="kobo.671.1">: L1 regularization, also </span><a id="_idIndexMarker1282"/><span class="koboSpan" id="kobo.672.1">known as</span><a id="_idIndexMarker1283"/><span class="koboSpan" id="kobo.673.1"> Lasso regularization, introduces a penalty term to the loss function, proportionate to the absolute values of the model’s coefficients (weights). </span><span class="koboSpan" id="kobo.673.2">Mathematically, it can be represented </span><span class="No-Break"><span class="koboSpan" id="kobo.674.1">as follows:</span></span><p class="list-inset"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.675.1">L</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.676.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.677.1">s</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.678.1">s</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.679.1">w</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.680.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.681.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.682.1">h</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.683.1">L</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.684.1">1</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.685.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.686.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.687.1">g</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.688.1">u</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.689.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.690.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.691.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.692.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.693.1">z</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.694.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.695.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.696.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.697.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.698.1">n</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.699.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.700.1">O</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.701.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.702.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.703.1">g</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.704.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.705.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.706.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.707.1">l</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.708.1">L</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.709.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.710.1">s</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.711.1">s</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.712.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.713.1">λ</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Symbol_v-normal"><span class="koboSpan" id="kobo.714.1">*</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.715.1">Σ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.716.1">|</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.717.1">w</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.718.1">_</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.719.1">i</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.720.1">|</span></span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.721.1">Here, </span><em class="italic"><span class="koboSpan" id="kobo.722.1">λ</span></em><span class="koboSpan" id="kobo.723.1"> (lambda) is a hyperparameter that controls the strength </span><span class="No-Break"><span class="koboSpan" id="kobo.724.1">of regularization.</span></span></p><p class="list-inset"><em class="italic"><span class="koboSpan" id="kobo.725.1">L1</span></em><span class="koboSpan" id="kobo.726.1"> regularization has the effect of shrinking some of the model’s weights to exactly zero. </span><span class="koboSpan" id="kobo.726.2">As a result, it performs feature selection by effectively removing less important features from the model. </span><em class="italic"><span class="koboSpan" id="kobo.727.1">L1</span></em><span class="koboSpan" id="kobo.728.1"> regularization is especially useful when you suspect that only a subset of your features is relevant for making predictions. </span><span class="koboSpan" id="kobo.728.2">It helps you create simpler and more interpretable models by eliminating </span><span class="No-Break"><span class="koboSpan" id="kobo.729.1">irrelevant features.</span></span></p></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.730.1">L2 regularization (Ridge)</span></strong><span class="koboSpan" id="kobo.731.1">: L2 regularization, also known as Ridge regularization, adds </span><a id="_idIndexMarker1284"/><span class="koboSpan" id="kobo.732.1">a penalty term to the loss function that is proportional to the square of the model’s coefficients (weights). </span><span class="koboSpan" id="kobo.732.2">Mathematically, it can be represented </span><span class="No-Break"><span class="koboSpan" id="kobo.733.1">as follows:</span></span><p class="list-inset"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.734.1">L</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.735.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.736.1">s</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.737.1">s</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.738.1">w</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.739.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.740.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.741.1">h</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.742.1">L</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.743.1">2</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.744.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.745.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.746.1">g</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.747.1">u</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.748.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.749.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.750.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.751.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.752.1">z</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.753.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.754.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.755.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.756.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.757.1">n</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.758.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.759.1">O</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.760.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.761.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.762.1">g</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.763.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.764.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.765.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.766.1">l</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.767.1">L</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.768.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.769.1">s</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.770.1">s</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.771.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.772.1">λ</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Symbol_v-normal"><span class="koboSpan" id="kobo.773.1">*</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.774.1">Σ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.775.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.776.1">w</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.777.1">_</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.778.1">i</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.779.1">^</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.780.1">2</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.781.1">)</span></span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.782.1">Again, </span><em class="italic"><span class="koboSpan" id="kobo.783.1">λ</span></em><span class="koboSpan" id="kobo.784.1"> is the hyperparameter controlling the strength </span><span class="No-Break"><span class="koboSpan" id="kobo.785.1">of regularization.</span></span></p><p class="list-inset"><em class="italic"><span class="koboSpan" id="kobo.786.1">L2</span></em><span class="koboSpan" id="kobo.787.1"> regularization encourages all the model’s weights to be small but rarely forces them to be exactly zero. </span><span class="koboSpan" id="kobo.787.2">It prevents any one feature from having an overwhelmingly large weight and has the effect of making the model more stable and numerically well conditioned. </span><em class="italic"><span class="koboSpan" id="kobo.788.1">L2</span></em><span class="koboSpan" id="kobo.789.1"> regularization is often used when you want to prevent multicollinearity (high correlation between features) and reduce the impact of outliers. </span><span class="koboSpan" id="kobo.789.2">It’s suitable when you believe that all features are relevant but should not be </span><span class="No-Break"><span class="koboSpan" id="kobo.790.1">overly influential.</span></span></p></li>
</ul>
<p><span class="koboSpan" id="kobo.791.1">In the following list, we summarize the key differences between </span><em class="italic"><span class="koboSpan" id="kobo.792.1">L1</span></em><span class="koboSpan" id="kobo.793.1"> and </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.794.1">L2</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.795.1"> regularization:</span></span></p>
<ul>
<li><em class="italic"><span class="koboSpan" id="kobo.796.1">L1</span></em><span class="koboSpan" id="kobo.797.1"> encourages sparsity by setting some weights to exactly zero, leading to feature selection, while </span><em class="italic"><span class="koboSpan" id="kobo.798.1">L2</span></em><span class="koboSpan" id="kobo.799.1"> mainly shrinks weights but rarely eliminates </span><span class="No-Break"><span class="koboSpan" id="kobo.800.1">them entirely</span></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.801.1">L1</span></em><span class="koboSpan" id="kobo.802.1"> is suitable for situations where you suspect that only a subset of features is relevant, whereas </span><em class="italic"><span class="koboSpan" id="kobo.803.1">L2</span></em><span class="koboSpan" id="kobo.804.1"> is more appropriate when you want to prevent multicollinearity and ensure all features contribute to </span><span class="No-Break"><span class="koboSpan" id="kobo.805.1">the model</span></span></li>
<li><span class="koboSpan" id="kobo.806.1">A combination </span><a id="_idIndexMarker1285"/><span class="koboSpan" id="kobo.807.1">of both </span><em class="italic"><span class="koboSpan" id="kobo.808.1">L1</span></em><span class="koboSpan" id="kobo.809.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.810.1">L2</span></em><span class="koboSpan" id="kobo.811.1"> regularization is known as elastic net, which </span><a id="_idIndexMarker1286"/><span class="koboSpan" id="kobo.812.1">balances </span><strong class="bold"><span class="koboSpan" id="kobo.813.1">feature selection</span></strong><span class="koboSpan" id="kobo.814.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.815.1">L1</span></strong><span class="koboSpan" id="kobo.816.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.817.1">feature </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.818.1">grouping</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.819.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.820.1">L2</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.821.1">)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.822.1">The choice between </span><em class="italic"><span class="koboSpan" id="kobo.823.1">L1</span></em><span class="koboSpan" id="kobo.824.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.825.1">L2</span></em><span class="koboSpan" id="kobo.826.1"> regularization depends on the specific problem, the nature of the data, and your understanding of the underlying relationships between features. </span><span class="koboSpan" id="kobo.826.2">In some</span><a id="_idIndexMarker1287"/><span class="koboSpan" id="kobo.827.1"> cases, a </span><a id="_idIndexMarker1288"/><span class="koboSpan" id="kobo.828.1">combination of both </span><em class="italic"><span class="koboSpan" id="kobo.829.1">L1</span></em><span class="koboSpan" id="kobo.830.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.831.1">L2</span></em><span class="koboSpan" id="kobo.832.1"> regularization may provide the best results by offering a balance between sparsity </span><span class="No-Break"><span class="koboSpan" id="kobo.833.1">and stability.</span></span></p>
<h2 id="_idParaDest-230"><a id="_idTextAnchor238"/><span class="koboSpan" id="kobo.834.1">Introducing early stopping</span></h2>
<p><span class="koboSpan" id="kobo.835.1">Early stopping is a </span><a id="_idIndexMarker1289"/><span class="koboSpan" id="kobo.836.1">regularization </span><a id="_idIndexMarker1290"/><span class="koboSpan" id="kobo.837.1">technique commonly used in ML, particularly in training DNNs, to prevent overfitting and optimize model performance. </span><span class="koboSpan" id="kobo.837.2">It involves monitoring the model’s performance on a validation dataset during the training process and stopping the training when a certain criterion </span><span class="No-Break"><span class="koboSpan" id="kobo.838.1">is met.</span></span></p>
<p><span class="koboSpan" id="kobo.839.1">During the training process, your dataset is typically divided into two parts: the training dataset and the validation dataset. </span><span class="koboSpan" id="kobo.839.2">The training dataset is used to update the model’s parameters, while the validation dataset is used to monitor the model’s performance. </span><span class="koboSpan" id="kobo.839.3">To implement early stopping, you need to define a performance criterion. </span><span class="koboSpan" id="kobo.839.4">Common choices for the criterion include validation loss or validation accuracy, depending on the specific ML task (for example, regression or classification). </span><span class="koboSpan" id="kobo.839.5">You aim to minimize the validation loss or maximize the </span><span class="No-Break"><span class="koboSpan" id="kobo.840.1">validation accuracy.</span></span></p>
<p><span class="koboSpan" id="kobo.841.1">As the model trains, you periodically evaluate its performance on the validation dataset by calculating the validation loss </span><span class="No-Break"><span class="koboSpan" id="kobo.842.1">or accuracy.</span></span></p>
<p><span class="koboSpan" id="kobo.843.1">You typically monitor the validation performance at regular intervals or after a fixed number of training epochs. </span><span class="koboSpan" id="kobo.843.2">You set a threshold, or a rule based on the validation performance, to decide when to stop the training. </span><span class="koboSpan" id="kobo.843.3">This could involve tracking whether the validation loss is decreasing or if the validation accuracy has reached a plateau. </span><span class="koboSpan" id="kobo.843.4">The most common condition is to stop training when validation performance starts to degrade or stagnate. </span><span class="koboSpan" id="kobo.843.5">In other words, you look for the point where the validation loss begins to increase or the validation accuracy starts to decrease. </span><span class="koboSpan" id="kobo.843.6">When the early stopping condition is met, training is halted. </span><span class="koboSpan" id="kobo.843.7">The model’s parameters at the point when training stops are</span><a id="_idIndexMarker1291"/><span class="koboSpan" id="kobo.844.1"> typically considered the </span><span class="No-Break"><span class="koboSpan" id="kobo.845.1">final model.</span></span></p>
<p><span class="koboSpan" id="kobo.846.1">What are the benefits of using early stopping? </span><span class="koboSpan" id="kobo.846.2">There are </span><span class="No-Break"><span class="koboSpan" id="kobo.847.1">a few:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.848.1">Prevents overfitting</span></strong><span class="koboSpan" id="kobo.849.1">: Early stopping </span><a id="_idIndexMarker1292"/><span class="koboSpan" id="kobo.850.1">prevents the model from overfitting the training data because it terminates the training process when the model starts to fit the noise in </span><span class="No-Break"><span class="koboSpan" id="kobo.851.1">the data</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.852.1">Saves training time</span></strong><span class="koboSpan" id="kobo.853.1">: It can significantly reduce training time because the model doesn’t continue to train unnecessarily once optimal or near-optimal performance </span><span class="No-Break"><span class="koboSpan" id="kobo.854.1">is achieved</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.855.1">Enables better generalization</span></strong><span class="koboSpan" id="kobo.856.1">: The model’s performance is more likely to generalize well to </span><a id="_idIndexMarker1293"/><span class="koboSpan" id="kobo.857.1">new, unseen data since it’s not overfitting the </span><span class="No-Break"><span class="koboSpan" id="kobo.858.1">training data</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.859.1">However, it’s essential to be cautious when using early stopping: choosing the right hyperparameters, such as the early stopping threshold or the frequency of validation checks, is critical. </span><span class="koboSpan" id="kobo.859.2">The wrong choices can lead to premature stopping or excessive training. </span><span class="koboSpan" id="kobo.859.3">It’s also important to use a separate validation dataset that the model has not seen during training. </span><span class="koboSpan" id="kobo.859.4">Data leakage can occur if the same data is used both for training </span><span class="No-Break"><span class="koboSpan" id="kobo.860.1">and validation.</span></span></p>
<p><span class="koboSpan" id="kobo.861.1">Early stopping is a valuable tool for training ML models, but it should be used with careful </span><a id="_idIndexMarker1294"/><span class="koboSpan" id="kobo.862.1">consideration of the problem and the </span><a id="_idIndexMarker1295"/><span class="koboSpan" id="kobo.863.1">specifics of the data. </span><span class="koboSpan" id="kobo.863.2">It helps to achieve better model generalization and reduces the risk </span><span class="No-Break"><span class="koboSpan" id="kobo.864.1">of overfitting.</span></span></p>
<h1 id="_idParaDest-231"><a id="_idTextAnchor239"/><span class="koboSpan" id="kobo.865.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.866.1">In this chapter, we saw how to implement an automatic fault diagnosis system in MATLAB. </span><span class="koboSpan" id="kobo.866.2">We started by introducing the essential concepts of anomaly detection and fault diagnosis. </span><span class="koboSpan" id="kobo.866.3">Then, we saw how to implement a system for identifying anomalous operations in MATLAB. </span><span class="koboSpan" id="kobo.866.4">We used vibrational data from a gearbox to train a model based on logistic regression. </span><span class="koboSpan" id="kobo.866.5">Subsequently, we used the same data, but this time using a model based on Random Forest to improve the performance of the </span><span class="No-Break"><span class="koboSpan" id="kobo.867.1">predictive model.</span></span></p>
<p><span class="koboSpan" id="kobo.868.1">In the next section, we implemented a model for identifying a fault in UAV propellers based on acoustic emission. </span><span class="koboSpan" id="kobo.868.2">We used a classification model based on </span><span class="No-Break"><span class="koboSpan" id="kobo.869.1">an SVM.</span></span></p>
<p><span class="koboSpan" id="kobo.870.1">In the final section, we introduced the most popular methods for regularizing algorithms to improve </span><span class="No-Break"><span class="koboSpan" id="kobo.871.1">model performance.</span></span></p>
<p><span class="koboSpan" id="kobo.872.1">In conclusion, this book serves as a comprehensive guide and invaluable resource for both beginners and seasoned practitioners navigating the dynamic landscape of machine learning. </span><span class="koboSpan" id="kobo.872.2">The book not only equips readers with a solid foundation in key concepts and methodologies but also empowers them with practical skills to implement and deploy machine learning models using MATLAB. </span><span class="koboSpan" id="kobo.872.3">As we embark on the ever-evolving journey of technological advancement, this book stands as a beacon, illuminating the path to understanding and mastering the intricate realm of machine learning. </span><span class="koboSpan" id="kobo.872.4">Whether you are a student, researcher, or industry professional, the knowledge gained from these pages will undoubtedly propel you towards harnessing the full potential of machine learning in your endeavors. </span><span class="koboSpan" id="kobo.872.5">May this book be a catalyst for innovation and discovery, inspiring readers to contribute to the exciting and transformative field of </span><span class="No-Break"><span class="koboSpan" id="kobo.873.1">machine learning.</span></span></p>
</div>
</body></html>