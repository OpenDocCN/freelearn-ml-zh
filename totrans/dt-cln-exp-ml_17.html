<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer231">
<h1 id="_idParaDest-152"><em class="italic"><a id="_idTextAnchor152"/>Chapter 13</em>: Support Vector Machine Classification</h1>
<p>There are some similarities between support vector classification models and k-nearest neighbors models. They are both intuitive and flexible. However, support vector classification, due to the nature of the algorithm, scales better than k-nearest neighbor. Unlike logistic regression, it can handle nonlinear models rather easily. The strategies and issues with using support vector machines for classification are similar to those we discussed in <a href="B17978_08_ePub.xhtml#_idTextAnchor106"><em class="italic">Chapter 8</em></a>, <em class="italic">Support Vector Regression</em>, when we used support vector machines for regression.</p>
<p>One of the key advantages<a id="_idIndexMarker1006"/> of <strong class="bold">support vector classification</strong> (<strong class="bold">SVC</strong>) is the ability it gives us to reduce model complexity without increasing our feature space. But it also provides multiple levers we can adjust to limit the possibility of overfitting. We can choose a linear model or select from several nonlinear kernels. We can use a regularization parameter, much as we did for logistic regression. With extensions, we can also use these same techniques to construct multiclass models.</p>
<p>We will explore the following topics in this chapter:</p>
<ul>
<li>Key concepts for SVC</li>
<li>Linear SVC models</li>
<li>Nonlinear SVM classification models<a id="_idTextAnchor153"/></li>
<li>SVMs for multiclass classification</li>
</ul>
<h1 id="_idParaDest-153"><a id="_idTextAnchor154"/>Technical requirements</h1>
<p>We will stick to the pandas, NumPy, and scikit-learn libraries in this chapter. All code in this chapter was tested with scikit-learn versions 0.24.2 and 1.0.2. The code that displays the decision boundaries needs scikit-learn version 1.1.1 or later.</p>
<h1 id="_idParaDest-154"><a id="_idTextAnchor155"/>Key concepts for SVC</h1>
<p>We can<a id="_idIndexMarker1007"/> use <strong class="bold">support vector machines</strong> (<strong class="bold">SVMs</strong>) to find a line or curve to separate instances by class. When classes can be discriminated by a line, they are said to <a id="_idIndexMarker1008"/>be <strong class="bold">linearly separable</strong>.</p>
<p>There may, however, be <a id="_idIndexMarker1009"/>many possible linear classifiers, as we can see in <em class="italic">Figure 13.1</em>. Each line successfully discriminates between the two classes, represented by dots and squares, using the two features x1 and x2. The key difference is in how the lines would classify new instances, represented by the transparent rectangle. Using the line closest to the squares would cause the transparent rectanglez to be classified as a dot. Using either of the other two lines would classify it as a square.</p>
<div>
<div class="IMG---Figure" id="_idContainer213">
<img alt="Figure 13.1 – Three possible linear classifiers " height="587" src="image/B17978_13_001.jpg" width="846"/>
</div>
</div>
<p class="figure-caption">Figure 13.1 – Three possible linear classifiers</p>
<p>When a linear discriminant is very close to training instances, as is the case with two of the lines in <em class="italic">Figure 13.2</em>, there is a greater risk of misclassifying new instances. We want a line that <a id="_idIndexMarker1010"/>gives us the maximum margin between classes; one that is furthest away from border data points for each class. That is the middle line in <em class="italic">Figure 13.1</em>, but it can be seen more clearly in <em class="italic">Figure 13.2</em>: </p>
<div>
<div class="IMG---Figure" id="_idContainer214">
<img alt="Figure 13.2 – SVM classification and maximum margin " height="580" src="image/B17978_13_002.jpg" width="850"/>
</div>
</div>
<p class="figure-caption">Figure 13.2 – SVM classification and maximum margin</p>
<p>The bold line splits the maximum margin and is referred to as the decision boundary. The border data points for each class are known as the support vectors.</p>
<p>We use SVM to find the linear discriminant with the maximum margin between classes.  It does this by finding an equation representing a margin that can be maximized, where the margin is the distance between a data point and the separating hyperplane. With two features, as in <em class="italic">Figure 13.2</em>, that hyperplane is just a line. However, this can be generalized to feature spaces with more dimensions.</p>
<p>With data points such as those in <em class="italic">Figure 13.2</em>, we can use what is known as <strong class="bold">hard margin classification</strong> without <a id="_idIndexMarker1011"/>problems; that is, we can be strict about all observations for each class being on the correct side of the decision boundary. But what if our data points look like those in <em class="italic">Figure 13.3</em>? Here, there is a square very close to the dots. The hard margin classifier is the left line, giving us quite<a id="_idIndexMarker1012"/> tiny margins. </p>
<div>
<div class="IMG---Figure" id="_idContainer215">
<img alt="Figure 13.3 – SVMs with hard and soft margins " height="590" src="image/B17978_13_003.jpg" width="847"/>
</div>
</div>
<p class="figure-caption">Figure 13.3 – SVMs with hard and soft margins</p>
<p>If we <a id="_idIndexMarker1013"/>use <strong class="bold">soft margin classification</strong> instead, we<a id="_idIndexMarker1014"/> get the line to the right. Soft margin classification relaxes the constraint that all instances have to be correctly separated. As is the case with the data in <em class="italic">Figure 13.3</em>, allowing for a small number of misclassifications in the training data can give us a larger margin. We ignore the wayward square and get a decision boundary represented by the soft margin line.</p>
<p>The amount of relaxation of the constraint is determined by the <em class="italic">C</em> hyperparameter. The larger the value of <em class="italic">C</em>, the greater the penalty for margin violations. Not surprisingly, models with larger <em class="italic">C</em> values are more prone to overfitting. <em class="italic">Figure 13.4</em> illustrates how the margin changes with values of <em class="italic">C</em>. At <em class="italic">C = 1</em>, the penalty for misclassification is low, giving us a much greater margin than when <em class="italic">C</em> is 100. Even at a <em class="italic">C</em> of 100, however, some margin violation still happens.</p>
<div>
<div class="IMG---Figure" id="_idContainer216">
<img alt="Figure 13.4 – Soft margins at different C values " height="654" src="image/B17978_13_004.jpg" width="1637"/>
</div>
</div>
<p class="figure-caption">Figure 13.4 – Soft margins at different C values</p>
<p>As a <a id="_idIndexMarker1015"/>practical matter, we almost always build our SVC models<a id="_idIndexMarker1016"/> with soft margins. The default value for <em class="italic">C</em> in scikit-learn is 1.</p>
<h2 id="_idParaDest-155"><a id="_idTextAnchor156"/>Nonlinear SVM and the kernel trick</h2>
<p>We have not<a id="_idIndexMarker1017"/> yet fully addressed the issue of linear separability with SVC. For simplicity, it is helpful to return to a classification problem involving two features. Let’s say a plot of two features against a categorical target looks like the illustration in <em class="italic">Figure 13.5</em>. The target has two possible values, represented by the dots and squares. x1 and x2 are numeric and have negative values.</p>
<div>
<div class="IMG---Figure" id="_idContainer217">
<img alt="Figure 13.5 – Class labels not linearly separable with two features " height="546" src="image/B17978_13_005.jpg" width="558"/>
</div>
</div>
<p class="figure-caption">Figure 13.5 – Class labels not linearly separable with two features</p>
<p>What <a id="_idIndexMarker1018"/>can we do in a case like this to identify a margin between the classes? It is often the case that a margin can be identified at a higher dimension. In this example, we can use a polynomial transformation, as illustrated in <em class="italic">Figure 13.6</em>:</p>
<div>
<div class="IMG---Figure" id="_idContainer218">
<img alt="Figure 13.6 – Using polynomial transformation to establish the margin " height="677" src="image/B17978_13_006.jpg" width="916"/>
</div>
</div>
<p class="figure-caption">Figure 13.6 – Using polynomial transformation to establish the margin</p>
<p>There is now a third dimension, which is the sum of the squares of x1 and x2. The dots are all higher than the squares. This is similar to how we used polynomial transformation with linear regression.</p>
<p>One<a id="_idIndexMarker1019"/> drawback of this approach is that we can quickly end up with too many features for our model to perform well. This is where the <strong class="bold">kernel trick</strong> comes <a id="_idIndexMarker1020"/>in very handy. SVC can use a kernel function to expand the feature space implicitly without actually creating more features. This is done by creating a vector of values that can be used to fit a nonlinear margin.</p>
<p>While this allows us to fit a polynomial transformation like the hypothetical one illustrated in <em class="italic">Figure 13.6</em>, the most frequently used kernel function with SVC is the <strong class="bold">radial basis function</strong> (<strong class="bold">RBF</strong>). RBF <a id="_idIndexMarker1021"/>is popular because it is faster than other common kernel functions and because it can be used with the gamma hyperparameter for additional flexibility. The equation for the RBF kernel is as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer219">
<img alt="" height="90" src="image/B17978_13_0011.jpg" width="648"/>
</div>
</div>
<p>Here, <img alt="" height="35" src="image/B17978_13_002.png" width="37"/> and <img alt="" height="40" src="image/B17978_13_003.png" width="35"/> are data points. Gamma, <img alt="" height="36" src="image/B17978_13_004.png" width="28"/>, determines the amount of influence of each point. With high values of gamma, points have to be very close to each other to be grouped together. At very high values of gamma, we start to see islands of points.</p>
<p>Of course, what is a high value for gamma, or of <em class="italic">C</em>, depends partly on our data. A good approach is to create visualizations of decision boundaries at different values for gamma and <em class="italic">C</em> before doing much modeling. This will give us a sense of whether or not we are underfitting or overfitting at different hyperparameter values. We will plot decision boundaries at different values of gamma and <em class="italic">C</em> in this chapter.</p>
<h2 id="_idParaDest-156"><a id="_idTextAnchor157"/>Multiclass classification with SVC</h2>
<p>All of our discussion <a id="_idIndexMarker1022"/>about SVC so far has centered on binary classification. Fortunately, all of the key concepts that <a id="_idIndexMarker1023"/>apply to SVMs for binary classification also apply to classification when our target has more than two possible values. We transform the multiclass problem into a binary classification problem by modeling it as a <strong class="bold">one-versus-one</strong>, or a <strong class="bold">one-versus-rest</strong> problem.</p>
<div>
<div class="IMG---Figure" id="_idContainer223">
<img alt="Figure 13.7 – Multiclass SVC options " height="656" src="image/B17978_13_007.jpg" width="1648"/>
</div>
</div>
<p class="figure-caption">Figure 13.7 – Multiclass SVC options</p>
<p>One-versus-one classification is<a id="_idIndexMarker1024"/> easy<a id="_idIndexMarker1025"/> to illustrate in a three-class example, as shown on the left side of <em class="italic">Figure 13.7</em>. A decision boundary is estimated between each class and each of the other classes. For example, the dotted line is the decision boundary for the dot class versus the square class. The solid line is the decision boundary between the dots and the ovals.</p>
<p>With <a id="_idIndexMarker1026"/>one-versus-rest classification, a <a id="_idIndexMarker1027"/>decision boundary is constructed between each class and those instances that are not of that class. This is illustrated on the right side of <em class="italic">Figure 13.7</em>. The solid line is the decision boundary between the dots and the instances that are not dots (those that are squares or ovals). The dotted and double lines are the decision boundaries for the squares versus the rest and the ovals versus the rest of the instances, respectively.</p>
<p>We can construct both linear and nonlinear SVC models using either one-versus-one or one-versus-rest classification. We can also specify values for <em class="italic">C</em> to construct soft margins. However, the construction of more decision boundaries with each of these techniques claims greater computational resources than SVC for binary classification. If we have a large number of <a id="_idIndexMarker1028"/>observations, many features, and more than a couple of parameters to tune, we will likely need very good system resources to get timely results.</p>
<p>The <a id="_idIndexMarker1029"/>three-class example hides one thing that is different about one-versus-one and one-versus-rest classifiers. With three classes, they use the same number of classifiers (three), but the number of classifiers increases relatively rapidly with one-versus-one. The number of classifiers will always be equal to the number of class values with one-versus-rest, whereas, with one-versus-one, it is equal to the following:</p>
<div>
<div class="IMG---Figure" id="_idContainer224">
<img alt="" height="114" src="image/B17978_13_0051.jpg" width="316"/>
</div>
</div>
<p>Here, <em class="italic">S</em> is the number of classifiers and <em class="italic">N</em> is the cardinality (the number of class values) of the target. So, with a cardinality of 4, one-versus-rest needs 4 classifiers, and one-versus-one uses 6.</p>
<p>We explore multiclass SVC models in the last section of this chapter, but let’s start with a relatively straightforward linear model to see SVC in action. There are two things to keep in mind when doing the preprocessing for an SVC model. First, SVC is sensitive to the scale of features, so we will need to address that before fitting our model. Second, if we are using hard margins or high values for <em class="italic">C</em>, outliers might have a large effect on our model.</p>
<h1 id="_idParaDest-157"><a id="_idTextAnchor158"/>Linear SVC models</h1>
<p>We can <a id="_idIndexMarker1030"/>often get good results by using a linear SVC model. When we have more than two features, there is no easy way to visualize whether our data is linearly separable or not. We often decide on linear or nonlinear based on hyperparameter tuning. For this section, we will assume we can get good performance with a linear model and soft margins.</p>
<p>We will work with data on <strong class="bold">National Basketball Association</strong> (<strong class="bold">NBA</strong>) games in this section. The dataset has statistics from each NBA game from the 2017/2018 season through the 2020/2021 season. This includes the home team, whether the home team won, the <a id="_idIndexMarker1031"/>visiting team, shooting percentages for visiting and home teams, turnovers, rebounds, and assists by both teams, and a number of other measures.</p>
<p class="callout-heading">Note</p>
<p class="callout">NBA game data is available for download for the public at <a href="https://www.kaggle.com/datasets/wyattowalsh/basketball">https://www.kaggle.com/datasets/wyattowalsh/basketball</a>. This dataset has game data starting with the 1946/1947 NBA season. It uses <strong class="source-inline">nba_api</strong> to pull stats from <a href="http://nba.com">nba.com</a>. That API is available at <a href="https://github.com/swar/nba_api">https://github.com/swar/nba_api</a>.</p>
<p>Let’s build a <a id="_idIndexMarker1032"/>linear SVC model:</p>
<ol>
<li>We start by loading the familiar libraries. The only new modules are <strong class="source-inline">LinearSVC</strong> and <strong class="source-inline">DecisionBoundaryDisplay</strong>. We will use <strong class="source-inline">DecisionBoundaryDisplay</strong> to show the boundaries of a linear model:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import OneHotEncoder, StandardScaler</p><p class="source-code">from sklearn.svm import LinearSVC</p><p class="source-code">from scipy.stats import uniform</p><p class="source-code">from sklearn.impute import SimpleImputer</p><p class="source-code">from sklearn.pipeline import make_pipeline</p><p class="source-code">from sklearn.compose import ColumnTransformer</p><p class="source-code">from sklearn.feature_selection import RFECV</p><p class="source-code">from sklearn.inspection import DecisionBoundaryDisplay</p><p class="source-code">from sklearn.model_selection import cross_validate, \</p><p class="source-code">  RandomizedSearchCV, RepeatedStratifiedKFold</p><p class="source-code">import sklearn.metrics as skmet</p><p class="source-code">import seaborn as sns</p><p class="source-code">import os</p><p class="source-code">import sys</p><p class="source-code">sys.path.append(os.getcwd() + "/helperfunctions")</p><p class="source-code">from preprocfunc import OutlierTrans</p></li>
<li>We are <a id="_idIndexMarker1033"/>ready to load the NBA game data. We just have a little cleaning to do. A small number of observations have missing values for our target, <strong class="source-inline">WL_HOME</strong>, whether the home team won. We remove those observations. We convert the <strong class="source-inline">WL_HOME</strong> feature to a <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong> feature.</li>
</ol>
<p>There is not much of a problem with a class imbalance here. This will save us some time later:</p>
<p class="source-code">nbagames = pd.read_csv("data/nbagames2017plus.csv", parse_dates=['GAME_DATE'])</p>
<p class="source-code">nbagames = \</p>
<p class="source-code">  nbagames.loc[nbagames.WL_HOME.isin(['W','L'])]</p>
<p class="source-code">nbagames.shape</p>
<p class="source-code"><strong class="bold">(4568, 149)</strong></p>
<p class="source-code">nbagames['WL_HOME'] = \</p>
<p class="source-code">  np.where(nbagames.WL_HOME=='L',0,1).astype('int')</p>
<p class="source-code">nbagames.WL_HOME.value_counts(dropna=False)</p>
<p class="source-code"><strong class="bold">1    2586</strong></p>
<p class="source-code"><strong class="bold">0    1982</strong></p>
<p class="source-code"><strong class="bold">Name: WL_HOME, dtype: int64</strong></p>
<ol>
<li value="3">Let’s<a id="_idIndexMarker1034"/> organize our features by data type:<p class="source-code">num_cols = ['FG_PCT_HOME','FTA_HOME','FG3_PCT_HOME',</p><p class="source-code">  'FTM_HOME','FT_PCT_HOME','OREB_HOME','DREB_HOME',</p><p class="source-code">  'REB_HOME','AST_HOME','STL_HOME','BLK_HOME',</p><p class="source-code">  'TOV_HOME','FG_PCT_AWAY','FTA_AWAY','FG3_PCT_AWAY',</p><p class="source-code">  'FT_PCT_AWAY','OREB_AWAY','DREB_AWAY','REB_AWAY',</p><p class="source-code">  'AST_AWAY','STL_AWAY','BLK_AWAY','TOV_AWAY']</p><p class="source-code">cat_cols = ['SEASON']</p></li>
<li>Let’s look at some descriptive statistics. (I have omitted some features from the printout to save space.) We will need to scale these features since they have very different ranges. There are no missing values but we will generate some when we assign missings to extreme values:<p class="source-code">nbagames[['WL_HOME'] + num_cols].agg(['count','min','median','max']).T</p><p class="source-code"><strong class="bold">                  count     min     median  max</strong></p><p class="source-code"><strong class="bold">WL_HOME           4,568     0.00    1.00    1.00</strong></p><p class="source-code"><strong class="bold">FG_PCT_HOME       4,568</strong><strong class="bold">     0.27    0.47    0.65</strong></p><p class="source-code"><strong class="bold">FTA_HOME          4,568     1.00    22.00   64.00</strong></p><p class="source-code"><strong class="bold">FG3_PCT_HOME      4,568     0.06    0.36</strong><strong class="bold">    0.84</strong></p><p class="source-code"><strong class="bold">FTM_HOME          4,568     1.00    17.00   44.00</strong></p><p class="source-code"><strong class="bold">FT_PCT_HOME       4,568     0.14    0.78    1.00</strong></p><p class="source-code"><strong class="bold">OREB_HOME         4,568</strong><strong class="bold">     1.00    10.00   25.00</strong></p><p class="source-code"><strong class="bold">DREB_HOME         4,568     18.00   35.00   55.00</strong></p><p class="source-code"><strong class="bold">REB_HOME          4,568     22.00   45.00</strong><strong class="bold">   70.00</strong></p><p class="source-code"><strong class="bold">AST_HOME          4,568     10.00   24.00   50.00</strong></p><p class="source-code"><strong class="bold">.........</strong></p><p class="source-code"><strong class="bold">FT_PCT_AWAY       4,568     0.26    0.78    1.00</strong></p><p class="source-code"><strong class="bold">OREB_AWAY</strong><strong class="bold">         4,568     0.00    10.00   26.00</strong></p><p class="source-code"><strong class="bold">DREB_AWAY         4,568     18.00   34.00   56.00</strong></p><p class="source-code"><strong class="bold">REB_AWAY          4,568     22.00</strong><strong class="bold">   44.00   71.00</strong></p><p class="source-code"><strong class="bold">AST_AWAY          4,568     9.00    24.00   46.00</strong></p><p class="source-code"><strong class="bold">STL_AWAY          4,568     0.00    8.00    19.00</strong></p><p class="source-code"><strong class="bold">BLK_AWAY</strong><strong class="bold">          4,568     0.00    5.00    15.00</strong></p><p class="source-code"><strong class="bold">TOV_AWAY          4,568     3.00    14.00   30.00</strong></p></li>
<li>We should <a id="_idIndexMarker1035"/>also review the correlations of the features:<p class="source-code">corrmatrix = nbagames[['WL_HOME'] + \</p><p class="source-code">  num_cols].corr(method="pearson")</p><p class="source-code">sns.heatmap(corrmatrix, </p><p class="source-code">  xticklabels=corrmatrix.columns,</p><p class="source-code">  yticklabels=corrmatrix.columns, cmap="coolwarm")</p><p class="source-code">plt.title('Heat Map of Correlation Matrix')</p><p class="source-code">plt.tight_layout()</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer225">
<img alt="Figure 13.8 – Heat map of NBA game statistics correlations " height="456" src="image/B17978_13_008.jpg" width="601"/>
</div>
</div>
<p class="figure-caption">Figure 13.8 – Heat map of NBA game statistics correlations</p>
<p>Several features <a id="_idIndexMarker1036"/>are correlated with the target, including the field goal percentage of the home team (<strong class="source-inline">FG_PCT_HOME</strong>) and defensive rebounds of the home team (<strong class="source-inline">DREB_HOME</strong>).</p>
<p>There is also correlation among the features. For example, the field goal percentage of the home team (<strong class="source-inline">FG_PCT_HOME</strong>) and the 3-point field goal percentage of the home team (<strong class="source-inline">FG3_PCT_HOME</strong>) are positively correlated, not surprisingly. Also, rebounds of the home team (<strong class="source-inline">REB_HOME</strong>) and defensive rebounds of the home team (<strong class="source-inline">DREB_HOME</strong>) are likely too closely correlated for any model to disentangle their impact.</p>
<ol>
<li value="6">Next, we create training and testing DataFrames:<p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(nbagames[num_cols + cat_cols],\</p><p class="source-code">  nbagames[['WL_HOME']], test_size=0.2, random_state=0)</p></li>
<li>We need to<a id="_idIndexMarker1037"/> set up our column transformations. For the numeric columns, we check for outliers and scale the data. We one-hot encode the one categorical feature, <strong class="source-inline">SEASON</strong>. We will use these transformations later with the pipeline for our grid search:<p class="source-code">ohe = OneHotEncoder(drop='first', sparse=False)</p><p class="source-code">cattrans = make_pipeline(ohe)</p><p class="source-code">standtrans = make_pipeline(OutlierTrans(2),</p><p class="source-code">  SimpleImputer(strategy="median"), StandardScaler())</p><p class="source-code">coltrans = ColumnTransformer(</p><p class="source-code">  transformers=[</p><p class="source-code">    ("cat", cattrans, cat_cols),</p><p class="source-code">    ("stand", standtrans, num_cols)</p><p class="source-code">  ]</p><p class="source-code">)</p></li>
<li>Before constructing our model, let’s look at a decision boundary from a linear SVC model. We base the boundary on two features correlated with the target: the field goal percentage of the home team (<strong class="source-inline">FG_PCT_HOME</strong>) and defensive rebounds of the home team (<strong class="source-inline">DREB_HOME</strong>).</li>
</ol>
<p>We create a function, <strong class="source-inline">dispbound</strong>, which will use the <strong class="source-inline">DecisionBoundaryDisplay</strong> module to show the boundary. This module is available with scikit-learn versions 1.1.1 or later. <strong class="source-inline">DecisionBoundaryDisplay</strong> needs a model to fit, two features, and target values:</p>
<p class="source-code">pipe0 = make_pipeline(OutlierTrans(2),</p>
<p class="source-code">  SimpleImputer(strategy="median"), StandardScaler())</p>
<p class="source-code">X_train_enc = pipe0.\</p>
<p class="source-code">  fit_transform(X_train[['FG_PCT_HOME','DREB_HOME']])</p>
<p class="source-code">def dispbound(model, X, xvarnames, y, title):</p>
<p class="source-code">  dispfit = model.fit(X,y)</p>
<p class="source-code">  disp = DecisionBoundaryDisplay.from_estimator(</p>
<p class="source-code">    dispfit, X, response_method="predict",</p>
<p class="source-code">    xlabel=xvarnames[0], ylabel=xvarnames[1],</p>
<p class="source-code">    alpha=0.5,</p>
<p class="source-code">  )</p>
<p class="source-code">  scatter = disp.ax_.scatter(X[:,0], X[:,1],</p>
<p class="source-code">    c=y, edgecolor="k")</p>
<p class="source-code">  </p>
<p class="source-code">  disp.ax_.set_title(title)</p>
<p class="source-code">  legend1 = disp.ax_.legend(*scatter.legend_elements(),</p>
<p class="source-code">    loc="lower left", title="Home Win")</p>
<p class="source-code">  disp.ax_.add_artist(legend1)</p>
<p class="source-code">dispbound(LinearSVC(max_iter=1000000,loss='hinge'),</p>
<p class="source-code">  X_train_enc, ['FG_PCT_HOME','DREB_HOME'],</p>
<p class="source-code">  y_train.values.ravel(),</p>
<p class="source-code">  'Linear SVC Decision Boundary')</p>
<p>This produces the<a id="_idIndexMarker1038"/> following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer226">
<img alt="Figure 13.9 – Decision boundary for a two-feature linear SVC model " height="446" src="image/B17978_13_009.jpg" width="557"/>
</div>
</div>
<p class="figure-caption">Figure 13.9 – Decision boundary for a two-feature linear SVC model</p>
<p>We get a pretty decent linear boundary with just the two features. That is great news, but let’s do a more carefully constructed model.</p>
<ol>
<li value="9">To build our model, we first instantiate a linear SVC object and set up recursive feature elimination. We then add the column transformation, the feature selection, and the linear SVC to a pipeline and fit it:<p class="source-code">svc = LinearSVC(max_iter=1000000, loss='hinge',</p><p class="source-code">   random_state=0)</p><p class="source-code">rfecv = RFECV(estimator=svc, cv=5)</p><p class="source-code">pipe1 = make_pipeline(coltrans, rfecv, svc)</p><p class="source-code">pipe1.fit(X_train, y_train.values.ravel())</p></li>
<li>Let’s see what <a id="_idIndexMarker1039"/>features were selected from our recursive feature elimination. We need to first get the column names after the one-hot encoding. We can then use the <strong class="source-inline">get_support</strong> method of the <strong class="source-inline">rfecv</strong> object to get the features that were selected. (You will get a deprecated warning regarding <strong class="source-inline">get_feature_names</strong> if you are using scikit-learn versions 1 or later. You can use <strong class="source-inline">get_feature_names_out</strong> instead, though that will not work with earlier versions of scikit-learn.):<p class="source-code">new_cat_cols = \</p><p class="source-code">  pipe1.named_steps['columntransformer'].\</p><p class="source-code">  named_transformers_['cat'].\</p><p class="source-code">  named_steps['onehotencoder'].\</p><p class="source-code">  get_feature_names(cat_cols)</p><p class="source-code">new_cols = np.concatenate((new_cat_cols, np.array(num_cols)))</p><p class="source-code">sel_cols = new_cols[pipe1['rfecv'].get_support()]</p><p class="source-code">np.set_printoptions(linewidth=55)</p><p class="source-code">sel_cols</p><p class="source-code"><strong class="bold">array(['SEASON_2018', 'SEASON_2019', 'SEASON_2020',</strong></p><p class="source-code"><strong class="bold">       'FG_PCT_HOME', 'FTA_HOME', 'FG3_PCT_HOME',</strong></p><p class="source-code"><strong class="bold">       'FTM_HOME', 'FT_PCT_HOME', 'OREB_HOME',</strong></p><p class="source-code"><strong class="bold">       'DREB_HOME', 'REB_HOME', 'AST_HOME',</strong></p><p class="source-code"><strong class="bold">       'TOV_HOME', 'FG_PCT_AWAY', 'FTA_AWAY',</strong></p><p class="source-code"><strong class="bold">       'FG3_PCT_AWAY', 'FT_PCT_AWAY', 'OREB_AWAY',</strong></p><p class="source-code"><strong class="bold">       'DREB_AWAY', 'REB_AWAY', 'AST_AWAY',</strong></p><p class="source-code"><strong class="bold">       'BLK_AWAY', 'TOV_AWAY'], dtype=object)</strong></p></li>
<li>We should <a id="_idIndexMarker1040"/>look at the coefficients. Coefficients for each of the selected columns can be accessed with the <strong class="source-inline">coef_</strong> attribute of the <strong class="source-inline">linearsvc</strong> object. Perhaps not surprisingly, the shooting percentages of the home team (<strong class="source-inline">FG_PCT_HOME</strong>) and the away team (<strong class="source-inline">FG_PCT_AWAY</strong>) are the most important positive and negative predictors of the home team winning. The next most important features are the number of turnovers of the away and home teams:<p class="source-code">pd.Series(pipe1['linearsvc'].\</p><p class="source-code">  coef_[0], index=sel_cols).\</p><p class="source-code">  sort_values(ascending=False)</p><p class="source-code"><strong class="bold">FG_PCT_HOME     2.21</strong></p><p class="source-code"><strong class="bold">TOV_AWAY        1.20</strong></p><p class="source-code"><strong class="bold">REB_HOME        1.19</strong></p><p class="source-code"><strong class="bold">FTM_HOME        0.95</strong></p><p class="source-code"><strong class="bold">FG3_PCT_HOME    0.94</strong></p><p class="source-code"><strong class="bold">FT_PCT_HOME     0.31</strong></p><p class="source-code"><strong class="bold">AST_HOME        0.25</strong></p><p class="source-code"><strong class="bold">OREB_HOME       0.18</strong></p><p class="source-code"><strong class="bold">DREB_AWAY       0.11</strong></p><p class="source-code"><strong class="bold">SEASON_2018     0.10</strong></p><p class="source-code"><strong class="bold">FTA_HOME       -0.05</strong></p><p class="source-code"><strong class="bold">BLK_AWAY       -0.07</strong></p><p class="source-code"><strong class="bold">SEASON_2019    -0.11</strong></p><p class="source-code"><strong class="bold">SEASON_2020    -0.19</strong></p><p class="source-code"><strong class="bold">AST_AWAY       -0.44</strong></p><p class="source-code"><strong class="bold">OREB_AWAY      -0.47</strong></p><p class="source-code"><strong class="bold">DREB_HOME      -0.49</strong></p><p class="source-code"><strong class="bold">FT_PCT_AWAY    -0.53</strong></p><p class="source-code"><strong class="bold">REB_AWAY       -0.63</strong></p><p class="source-code"><strong class="bold">FG3_PCT_AWAY   -0.80</strong></p><p class="source-code"><strong class="bold">FTA_AWAY       -0.81</strong></p><p class="source-code"><strong class="bold">TOV_HOME       -1.19</strong></p><p class="source-code"><strong class="bold">FG_PCT_AWAY    -1.91</strong></p><p class="source-code"><strong class="bold">dtype: float64</strong></p></li>
<li>Let’s take a look <a id="_idIndexMarker1041"/>at the predictions. Our model predicts the home team winning very well:<p class="source-code">pred = pipe1.predict(X_test)</p><p class="source-code">print("accuracy: %.2f, sensitivity: %.2f, specificity: %.2f, precision: %.2f"  %</p><p class="source-code">  (skmet.accuracy_score(y_test.values.ravel(), pred),</p><p class="source-code">  skmet.recall_score(y_test.values.ravel(), pred),</p><p class="source-code">  skmet.recall_score(y_test.values.ravel(), pred, pos_label=0),</p><p class="source-code">  skmet.precision_score(y_test.values.ravel(), pred)))</p><p class="source-code"><strong class="bold">accuracy: 0.93, sensitivity: 0.95, specificity: 0.92, precision: 0.93</strong></p></li>
<li>We should confirm that these metrics are not a fluke by doing some cross-validation. We use repeated stratified k folds for our validation, indicating that we want seven folds and 10 iterations. We get pretty much the same results as we did during the previous step:<p class="source-code">kf = RepeatedStratifiedKFold(n_splits=7,n_repeats=10,\</p><p class="source-code">  random_state=0)</p><p class="source-code">scores = cross_validate(pipe1, X_train, \</p><p class="source-code">  y_train.values.ravel(), \</p><p class="source-code">  scoring=['accuracy','precision','recall','f1'], \</p><p class="source-code">  cv=kf, n_jobs=-1)</p><p class="source-code">print("accuracy: %.2f, precision: %.2f, sensitivity: %.2f, f1: %.2f"  %</p><p class="source-code">  (np.mean(scores['test_accuracy']),\</p><p class="source-code">  np.mean(scores['test_precision']),\</p><p class="source-code">  np.mean(scores['test_recall']),\</p><p class="source-code">  np.mean(scores['test_f1'])))</p><p class="source-code"><strong class="bold">accuracy: 0.93, precision: 0.93, sensitivity: 0.95, f1: 0.94</strong></p></li>
<li>We have <a id="_idIndexMarker1042"/>been using the default value of <strong class="source-inline">C</strong> of <strong class="source-inline">1</strong> so far. We can try to identify a better value for <strong class="source-inline">C</strong> with a randomized grid search:<p class="source-code">svc_params = {</p><p class="source-code"> 'linearsvc__C': uniform(loc=0, scale=100)</p><p class="source-code">}</p><p class="source-code">rs = RandomizedSearchCV(pipe1, svc_params, cv=10, </p><p class="source-code">  scoring='accuracy', n_iter=20, random_state=0)</p><p class="source-code">rs.fit(X_train, y_train.values.ravel())</p><p class="source-code">rs.best_params_</p><p class="source-code"><strong class="bold">{'linearsvc__C': 54.88135039273247}</strong></p><p class="source-code">rs.best_score_</p><p class="source-code"><strong class="bold">0.9315809566584325</strong></p></li>
</ol>
<p>The best <strong class="source-inline">C</strong> value is 2.02 and the best accuracy score is 0.9316.</p>
<ol>
<li value="15">Let’s take a <a id="_idIndexMarker1043"/>closer look at the scores for each of the 20 times we ran the grid search. Each score is the average accuracy score across 10 folds. We actually get pretty much the same score regardless of the <strong class="source-inline">C</strong> value: <p class="source-code">results = \</p><p class="source-code">  pd.DataFrame(rs.cv_results_['mean_test_score'], \</p><p class="source-code">    columns=['meanscore']).\</p><p class="source-code">  join(pd.DataFrame(rs.cv_results_['params'])).\</p><p class="source-code">  sort_values(['meanscore'], ascending=False)</p><p class="source-code">results</p><p class="source-code">              <strong class="bold">meanscore</strong>        <strong class="bold">linearsvc__C</strong></p><p class="source-code"><strong class="bold">0</strong>             <strong class="bold">0.93</strong>             <strong class="bold">54.88</strong></p><p class="source-code"><strong class="bold">8</strong>             <strong class="bold">0.93</strong>             <strong class="bold">96.37</strong></p><p class="source-code"><strong class="bold">18</strong>            <strong class="bold">0.93</strong>             <strong class="bold">77.82</strong></p><p class="source-code"><strong class="bold">17</strong>            <strong class="bold">0.93</strong>             <strong class="bold">83.26</strong></p><p class="source-code"><strong class="bold">13</strong>            <strong class="bold">0.93</strong>             <strong class="bold">92.56</strong></p><p class="source-code"><strong class="bold">12</strong>            <strong class="bold">0.93</strong>             <strong class="bold">56.80</strong></p><p class="source-code"><strong class="bold">11</strong>            <strong class="bold">0.93</strong>             <strong class="bold">52.89</strong></p><p class="source-code"><strong class="bold">1</strong>             <strong class="bold">0.93</strong>             <strong class="bold">71.52</strong></p><p class="source-code"><strong class="bold">10</strong>            <strong class="bold">0.93</strong>             <strong class="bold">79.17</strong></p><p class="source-code"><strong class="bold">7</strong>             <strong class="bold">0.93</strong>             <strong class="bold">89.18</strong></p><p class="source-code"><strong class="bold">6</strong>             <strong class="bold">0.93</strong>             <strong class="bold">43.76</strong></p><p class="source-code"><strong class="bold">5</strong>             <strong class="bold">0.93</strong>             <strong class="bold">64.59</strong></p><p class="source-code"><strong class="bold">3</strong>             <strong class="bold">0.93</strong>             <strong class="bold">54.49</strong></p><p class="source-code"><strong class="bold">2</strong>             <strong class="bold">0.93</strong>             <strong class="bold">60.28</strong></p><p class="source-code"><strong class="bold">19</strong>            <strong class="bold">0.93</strong>             <strong class="bold">87.00</strong></p><p class="source-code"><strong class="bold">9</strong>             <strong class="bold">0.93</strong>             <strong class="bold">38.34</strong></p><p class="source-code"><strong class="bold">4</strong>             <strong class="bold">0.93</strong>             <strong class="bold">42.37</strong></p><p class="source-code"><strong class="bold">14</strong>            <strong class="bold">0.93</strong>             <strong class="bold">7.10</strong></p><p class="source-code"><strong class="bold">15</strong>            <strong class="bold">0.93</strong>             <strong class="bold">8.71</strong></p><p class="source-code"><strong class="bold">16</strong>            <strong class="bold">0.93</strong>             <strong class="bold">2.02</strong></p></li>
<li>Let’s now<a id="_idIndexMarker1044"/> look at some of the predictions. Our model does well across the board, but not any better than the initial model:<p class="source-code">pred = rs.predict(X_test)</p><p class="source-code">print("accuracy: %.2f, sensitivity: %.2f, specificity: %.2f, precision: %.2f"  %</p><p class="source-code">  (skmet.accuracy_score(y_test.values.ravel(), pred),</p><p class="source-code">  skmet.recall_score(y_test.values.ravel(), pred),</p><p class="source-code">  skmet.recall_score(y_test.values.ravel(), pred, pos_label=0),</p><p class="source-code">  skmet.precision_score(y_test.values.ravel(), pred)))</p><p class="source-code"><strong class="bold">accuracy: 0.93, sensitivity: 0.95, specificity: 0.92, precision: 0.93</strong></p></li>
<li>Let’s also<a id="_idIndexMarker1045"/> look at a confusion matrix:<p class="source-code">cm = skmet.confusion_matrix(y_test, pred)</p><p class="source-code">cmplot = \</p><p class="source-code">  skmet.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Loss', 'Won'])</p><p class="source-code">cmplot.plot()</p><p class="source-code">cmplot.ax_.set(title='Home Team Win Confusion Matrix', </p><p class="source-code">  xlabel='Predicted Value', ylabel='Actual Value')</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer227">
<img alt="Figure 13.10 – Confusion matrix for wins by the home team " height="444" src="image/B17978_13_010.jpg" width="523"/>
</div>
</div>
<p class="figure-caption">Figure 13.10 – Confusion matrix for wins by the home team</p>
<p>Our model <a id="_idIndexMarker1046"/>largely predicts home team wins and losses correctly. Tuning the value of <strong class="source-inline">C</strong> did not make much of a difference, as we get pretty much the same accuracy regardless of the <strong class="source-inline">C</strong> value.</p>
<p class="callout-heading">Note</p>
<p class="callout">You may have noticed that we are using the accuracy metric more often with the NBA games data than with the heart disease and machine failure data that we have worked with in previous chapters. We focused more on sensitivity with that data. There are two reasons for that. First, accuracy is a more compelling measure when classes are closer to being balanced for reasons we discussed in detail in <a href="B17978_06_ePub.xhtml#_idTextAnchor078"><em class="italic">Chapter 6</em></a>, <em class="italic">Preparing for Model Evaluation</em>. Second, in predicting heart disease and machine power failure, we are biased towards sensitivity, as the cost of a false negative is higher than that of a false positive in those domains. For predicting NBA games, there is no such bias.</p>
<p>One advantage of linear SVC models is how easy they are to interpret. We are able to look at coefficients, which <a id="_idIndexMarker1047"/>helps us make sense of the model and communicate the basis of our predictions to others. It nonetheless can be helpful to confirm that we do not get better results with a nonlinear model. We will do that in the next section.</p>
<h1 id="_idParaDest-158"><a id="_idTextAnchor159"/>Nonlinear SVM classification models</h1>
<p>Although <a id="_idIndexMarker1048"/>nonlinear SVC is more complicated conceptually than linear SVC, as we saw in the first section of this chapter, running a nonlinear model with scikit-learn is relatively straightforward. The main difference from a linear model is that we need to do a fair bit more hyperparameter tuning. We have to specify values for <strong class="source-inline">C</strong>, for <strong class="source-inline">gamma</strong>, and for the kernel we want to use.</p>
<p>While there are theoretical reasons for hypothesizing that some hyperparameter values might work better than others for a given modeling challenge, we usually resolve those values empirically, that is, with hyperparameter tuning. We try that in this section with the same NBA games data that we used in the previous section:</p>
<ol>
<li value="1">We load the same libraries that we used in the previous section. We also import the <strong class="source-inline">LogisticRegression</strong> module. We will use that with a feature selection wrapper method later:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.preprocessing import MinMaxScaler</p><p class="source-code">from sklearn.pipeline import make_pipeline</p><p class="source-code">from sklearn.svm import SVC</p><p class="source-code">from sklearn.linear_model import LogisticRegression</p><p class="source-code">from scipy.stats import uniform</p><p class="source-code">from sklearn.feature_selection import RFECV</p><p class="source-code">from sklearn.impute import SimpleImputer</p><p class="source-code">from scipy.stats import randint</p><p class="source-code">from sklearn.model_selection import RandomizedSearchCV</p><p class="source-code">import sklearn.metrics as skmet</p><p class="source-code">import os</p><p class="source-code">import sys</p><p class="source-code">sys.path.append(os.getcwd() + "/helperfunctions")</p><p class="source-code">from preprocfunc import OutlierTrans</p></li>
<li>We<a id="_idIndexMarker1049"/> import the <strong class="source-inline">nbagames</strong> module, which has the code that loads and preprocesses the NBA games data. This is just a copy of the code that we ran in the previous section to prepare the data for modeling. There is no need to repeat those steps here.</li>
</ol>
<p>We also import the <strong class="source-inline">dispbound</strong> function we used in the previous section to display decision boundaries. We copied that code into a file called <strong class="source-inline">displayfunc.py</strong> in the <strong class="source-inline">helperfunctions</strong> subfolder of the current directory:</p>
<p class="source-code">import nbagames as ng</p>
<p class="source-code">from displayfunc import dispbound</p>
<ol>
<li value="3">We use the <strong class="source-inline">nbagames</strong> module to get the training and testing data:<p class="source-code">X_train = ng.X_train</p><p class="source-code">X_test = ng.X_test</p><p class="source-code">y_train = ng.y_train</p><p class="source-code">y_test = ng.y_test</p></li>
<li>Before constructing a model, let’s look at the decision boundaries for a couple of different kernels with two features: the field goal percentage of the home team (<strong class="source-inline">FG_PCT_HOME</strong>) and defensive rebounds of the home team (<strong class="source-inline">DREB_HOME</strong>). We start with the <strong class="source-inline">rbf</strong> kernel, using different values for <strong class="source-inline">gamma</strong> and <strong class="source-inline">C</strong>:<p class="source-code">pipe0 = make_pipeline(OutlierTrans(2),</p><p class="source-code">  SimpleImputer(strategy="median"),</p><p class="source-code">  StandardScaler())</p><p class="source-code">X_train_enc = \</p><p class="source-code">  pipe0.fit_transform(X_train[['FG_PCT_HOME',</p><p class="source-code">   'DREB_HOME']])</p><p class="source-code">dispbound(SVC(kernel='rbf', gamma=30, C=1),</p><p class="source-code">  X_train_enc,['FG_PCT_HOME','DREB_HOME'],</p><p class="source-code">  y_train.values.ravel(),</p><p class="source-code">  "SVC with rbf kernel-gamma=30, C=1")</p></li>
</ol>
<p>Running <a id="_idIndexMarker1050"/>this a few different ways produces the following plots:</p>
<div>
<div class="IMG---Figure" id="_idContainer228">
<img alt="Figure 13.11 – Decision boundaries with the rbf kernel and different gamma and C values " height="1337" src="image/B17978_13_011.jpg" width="571"/>
</div>
</div>
<p class="figure-caption">Figure 13.11 – Decision boundaries with the rbf kernel and different gamma and C values</p>
<p>At values for <strong class="source-inline">gamma</strong> and <strong class="source-inline">C</strong> near the default, we see some bending of the decision boundary to accommodate a few wayward points in the loss class. These<a id="_idIndexMarker1051"/> are instances where the home team lost despite having very high defensive rebound totals. With the <strong class="source-inline">rbf</strong> kernel, two of these instances are now correctly classified. There are also a couple of instances with a high home team field goal percentage but low home team defensive rebounds, which are now correctly classified. However, there is not much change overall in our predictions compared with the linear model from the previous section.</p>
<p>But this changes significantly if we increase values for <strong class="source-inline">C</strong> or <strong class="source-inline">gamma</strong>. Recall that higher values of <strong class="source-inline">C</strong> increase the penalty for misclassification. This leads to boundaries that wind around instances more.</p>
<p>Increasing <strong class="source-inline">gamma</strong> to <strong class="source-inline">30</strong> causes substantial overfitting. High values of <strong class="source-inline">gamma</strong> mean that data points have to be very close to each other to be grouped together. This results in decision boundaries closely tied to small numbers of instances, sometimes just one instance.</p>
<ol>
<li value="5">We can also show the boundaries for a polynomial kernel. We will keep the <strong class="source-inline">C</strong> value at the default to focus on the effect of changing the number of degrees:<p class="source-code">dispbound(SVC(kernel='poly', degree=7),</p><p class="source-code">  X_train_enc, ['FG_PCT_HOME','DREB_HOME'],</p><p class="source-code">  y_train.values.ravel(),</p><p class="source-code">  "SVC with polynomial kernel - degree=7")</p></li>
</ol>
<p>Running this a couple of different ways produces the following plots:</p>
<div>
<div class="IMG---Figure" id="_idContainer229">
<img alt="Figure 13.12 – Decision boundaries with polynomial kernel and different degrees " height="897" src="image/B17978_13_012.jpg" width="570"/>
</div>
</div>
<p class="figure-caption">Figure 13.12 – Decision boundaries with polynomial kernel and different degrees</p>
<p>We can see some bending of the decision boundary at higher degree levels to handle a couple of unusual instances. There is not much overfitting here, but not really much improvement in our predictions either.</p>
<p>This at<a id="_idIndexMarker1052"/> least hints at what to expect when we construct the model. We should try some nonlinear models but there is a good chance that they will not lead to much improvement over the linear model we used in the previous section.</p>
<ol>
<li value="6">Now, we are ready to set up the pipeline that we will use for our nonlinear SVC. Our pipeline will do the column transformation and a recursive feature elimination. We use logistic regression for the feature selection: <p class="source-code">rfecv = RFECV(estimator=LogisticRegression())</p><p class="source-code">svc = SVC()</p><p class="source-code">pipe1 = make_pipeline(ng.coltrans, rfecv, svc)</p></li>
<li>We create a dictionary to use for our hyperparameter tuning. This dictionary is structured somewhat differently from other dictionaries we have used for this purpose. That is because certain hyperparameters only work with certain other hyperparameters. For example, <strong class="source-inline">gamma</strong> does not work with a linear kernel:<p class="source-code">svc_params = [</p><p class="source-code">  {</p><p class="source-code">    'svc__kernel': ['rbf'],</p><p class="source-code">    'svc__C': uniform(loc=0, scale=20),</p><p class="source-code">    'svc__gamma': uniform(loc=0, scale=100)</p><p class="source-code">  },</p><p class="source-code">  {</p><p class="source-code">    'svc__kernel': ['poly'],</p><p class="source-code">    'svc__degree': randint(1, 5),</p><p class="source-code">    'svc__C': uniform(loc=0, scale=20),</p><p class="source-code">    'svc__gamma': uniform(loc=0, scale=100)</p><p class="source-code">  },</p><p class="source-code">  {</p><p class="source-code">    'svc__kernel': ['linear','sigmoid'],</p><p class="source-code">    'svc__C': uniform(loc=0, scale=20)</p><p class="source-code">  }</p><p class="source-code">]</p><p class="callout-heading">Note</p><p class="callout">You may have noticed that one of the kernels we will be using is linear, and wonder how this is different from the linear SVC module we used in the previous section. <strong class="source-inline">LinearSVC</strong> will often converge faster, particularly with large datasets. It does not use the kernel trick. We will also likely get different results as the optimization is different in several ways.</p></li>
<li>Now<a id="_idIndexMarker1053"/> we are ready to fit an SVC model. The best model is actually one with a linear kernel:<p class="source-code">rs = RandomizedSearchCV(pipe1, svc_params, cv=5, </p><p class="source-code">  scoring='accuracy', n_iter=10, n_jobs=-1,</p><p class="source-code">  verbose=5, random_state=0)</p><p class="source-code">rs.fit(X_train, y_train.values.ravel())</p><p class="source-code">rs.best_params_</p><p class="source-code"><strong class="bold">{'svc__C': 1.1342595463488636, 'svc__kernel': 'linear'}</strong></p><p class="source-code">rs.best_score_</p><p class="source-code"><strong class="bold">0.9299405955437289</strong></p></li>
<li>Let’s<a id="_idIndexMarker1054"/> take a closer look at the hyperparameters selected and the associated accuracy scores. We can get the 20 randomly chosen hyperparameter combinations from the <strong class="source-inline">params</strong> list from the grid object’s <strong class="source-inline">cv_results_</strong> dictionary. We can get the mean test score from that same dictionary.</li>
</ol>
<p>We sort by accuracy score in descending order. Linear kernels outperform polynomial and <strong class="source-inline">rbf</strong> kernels, though not substantially better than polynomial at <strong class="source-inline">3</strong>, <strong class="source-inline">4</strong>, and <strong class="source-inline">5</strong> degrees. <strong class="source-inline">rbf</strong> kernels perform particularly poorly:</p>
<p class="source-code">results = \</p>
<p class="source-code">  pd.DataFrame(rs.cv_results_['mean_test_score'], \</p>
<p class="source-code">    columns=['meanscore']).\</p>
<p class="source-code">  join(pd.json_normalize(rs.cv_results_['params'])).\</p>
<p class="source-code">  sort_values(['meanscore'], ascending=False)</p>
<p class="source-code">results</p>
<p class="source-code"><strong class="bold">            C          gamma     kernel     degree</strong></p>
<p class="source-code"><strong class="bold">meanscore                              </strong></p>
<p class="source-code"><strong class="bold">0.93        1.13       NaN       linear     NaN</strong></p>
<p class="source-code"><strong class="bold">0.89        1.42       64.82     poly       3.00</strong></p>
<p class="source-code"><strong class="bold">0.89        9.55       NaN       sigmoid    NaN</strong></p>
<p class="source-code"><strong class="bold">0.89        11.36      NaN       sigmoid    NaN</strong></p>
<p class="source-code"><strong class="bold">0.89        2.87       75.86     poly       5.00</strong></p>
<p class="source-code"><strong class="bold">0.64        12.47      43.76     poly       4.00</strong></p>
<p class="source-code"><strong class="bold">0.64        15.61      72.06     poly       4.00</strong></p>
<p class="source-code"><strong class="bold">0.57        11.86      84.43     rbf        NaN</strong></p>
<p class="source-code"><strong class="bold">0.57        16.65      77.82     rbf        NaN</strong></p>
<p class="source-code"><strong class="bold">0.57        19.57      79.92     rbf        NaN</strong></p>
<p class="callout-heading">Note</p>
<p class="callout">We use the pandas <strong class="source-inline">json_nomalize</strong> method to handle the somewhat messy hyperparameter combinations we pull from the <strong class="source-inline">params</strong> list. It is messy because different hyperparameters are available depending on the kernel used. This means that the 20 dictionaries in the <strong class="source-inline">params</strong> list will have different keys. For example, the polynomial kernels will have values for degrees. The linear and <strong class="source-inline">rbf</strong> kernels will not.</p>
<ol>
<li value="10">We<a id="_idIndexMarker1055"/> can access the support vectors via the <strong class="source-inline">best_estimator_</strong> attribute. There are 625 support vectors <em class="italic">holding up</em> the decision boundary:<p class="source-code">rs.best_estimator_['svc'].\</p><p class="source-code">  support_vectors_.shape</p><p class="source-code"><strong class="bold">(625, 18)</strong></p></li>
<li>Finally, we can take a look at the predictions. Not surprisingly, we do not get better results than we got with the linear SVC model that we ran in the last section. I say not surprisingly because the best model was found to be a model with a linear kernel:<p class="source-code">pred = rs.predict(X_test)</p><p class="source-code">print("accuracy: %.2f, sensitivity: %.2f, specificity: %.2f, precision: %.2f"  %</p><p class="source-code">  (skmet.accuracy_score(y_test.values.ravel(), pred),</p><p class="source-code">  skmet.recall_score(y_test.values.ravel(), pred),</p><p class="source-code">  skmet.recall_score(y_test.values.ravel(), pred, </p><p class="source-code">    pos_label=0),</p><p class="source-code">  skmet.precision_score(y_test.values.ravel(), pred)))</p><p class="source-code"><strong class="bold">accuracy: 0.93, sensitivity: 0.94, specificity: 0.91, precision: 0.93</strong></p></li>
</ol>
<p>Although<a id="_idIndexMarker1056"/> we have not improved upon our model from the previous section, it was still a worthwhile exercise to experiment with some nonlinear models. Indeed, this is often how we discover whether we have data that can be successfully separated linearly. This is typically difficult to visualize and so we rely on hyperparameter tuning to tell us which kernel classifies our data best.</p>
<p>This section and the previous one demonstrate the key techniques for using SVMs for binary classification. Much of what we have done so far applies to multiclass classification as well. We will take a look at SVC modeling strategies when our target has more than two values in the next section.</p>
<h1 id="_idParaDest-159"><a id="_idTextAnchor160"/>SVMs for multiclass classification</h1>
<p>All of the <a id="_idIndexMarker1057"/>same concerns that we had when we used SVC for binary classification apply when we are doing multiclass classification. We need to determine whether the classes are linearly separable, and if not, which kernel will yield the best results. As discussed in the first section of this chapter, we also need to decide whether that classification is best modeled as one-versus-one or one-versus-rest. One-versus-one finds decision boundaries that separate each class from each of the other classes. One-versus-rest finds decision boundaries that distinguish each class from all other instances. We try both approaches in this section.</p>
<p>We will work with the machine failure data that we worked with in previous chapters.</p>
<p class="callout-heading">Note</p>
<p class="callout">This dataset on machine failure is available for public use at <a href="https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification">https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification</a>. There are 10,000 observations, 12 features, and two possible targets. One is binary: the machine failed or did not. The other has types of failure. The instances in this dataset are synthetic, generated by a process designed to mimic machine failure rates and causes.</p>
<p>Let’s build a <a id="_idIndexMarker1058"/>multiclass SVC model:</p>
<ol>
<li value="1">We start by loading the same libraries that we have been using in this chapter:<p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import OneHotEncoder, MinMaxScaler</p><p class="source-code">from sklearn.pipeline import make_pipeline</p><p class="source-code">from sklearn.svm import SVC</p><p class="source-code">from scipy.stats import uniform</p><p class="source-code">from sklearn.impute import SimpleImputer</p><p class="source-code">from sklearn.compose import ColumnTransformer</p><p class="source-code">from sklearn.model_selection import RandomizedSearchCV</p><p class="source-code">import sklearn.metrics as skmet</p><p class="source-code">import os</p><p class="source-code">import sys</p><p class="source-code">sys.path.append(os.getcwd() + "/helperfunctions")</p><p class="source-code">from preprocfunc import OutlierTrans</p></li>
<li>We will load the machine failure type dataset and take a look at its structure. There is a mixture of character and numeric data. There are no missing values:<p class="source-code">machinefailuretype = pd.read_csv("data/machinefailuretype.csv")</p><p class="source-code">machinefailuretype.info()</p><p class="source-code"><strong class="bold">&lt;class 'pandas.core.frame.DataFrame'&gt;</strong></p><p class="source-code"><strong class="bold">RangeIndex: 10000 entries, 0 to 9999</strong></p><p class="source-code"><strong class="bold">Data columns (total 10 columns):</strong></p><p class="source-code"><strong class="bold"> #   Column                 Non-Null Count     Dtype  </strong></p><p class="source-code"><strong class="bold">---  ------                 --------------     -----  </strong></p><p class="source-code"><strong class="bold"> 0   udi                    10000 non-null     int64  </strong></p><p class="source-code"><strong class="bold"> 1   product                10000 non-null     object </strong></p><p class="source-code"><strong class="bold"> 2   machinetype            10000 non-null     object </strong></p><p class="source-code"><strong class="bold"> 3   airtemp                10000 non-null     float64</strong></p><p class="source-code"><strong class="bold"> 4   processtemperature     10000 non-null     float64</strong></p><p class="source-code"><strong class="bold"> 5   rotationalspeed        10000 non-null     int64  </strong></p><p class="source-code"><strong class="bold"> 6   torque                 10000 non-null     float64</strong></p><p class="source-code"><strong class="bold"> 7   toolwear               10000 non-null     int64  </strong></p><p class="source-code"><strong class="bold"> 8   fail                   10000 non-null     int64  </strong></p><p class="source-code"><strong class="bold"> 9   failtype               10000 non-null     object </strong></p><p class="source-code"><strong class="bold">dtypes: float64(3), int64(4), object(3)</strong></p><p class="source-code"><strong class="bold">memory usage: 781.4+ KB</strong></p></li>
<li>Let’s look <a id="_idIndexMarker1059"/>at a few observations:<p class="source-code">machinefailuretype.head()</p><p class="source-code"><strong class="bold">   udi product machinetype airtemp processtemperature\</strong></p><p class="source-code"><strong class="bold">0  1   M14860       M         298        309 </strong></p><p class="source-code"><strong class="bold">1  2   L47181       L         298        309 </strong></p><p class="source-code"><strong class="bold">2  3   L47182       L         298        308 </strong></p><p class="source-code"><strong class="bold">3  4   L47183       L         298        309 </strong></p><p class="source-code"><strong class="bold">4  5   L47184       L         298        309 </strong></p><p class="source-code"><strong class="bold">   rotationalspeed  torque  toolwear  fail  failtype  </strong></p><p class="source-code"><strong class="bold">0        1551         43        0       0   No Failure  </strong></p><p class="source-code"><strong class="bold">1        1408         46        3       0   No Failure  </strong></p><p class="source-code"><strong class="bold">2        1498         49        5       0   No Failure  </strong></p><p class="source-code"><strong class="bold">3        1433         40        7       0   No Failure  </strong></p><p class="source-code"><strong class="bold">4        1408         40        9       0   No Failure</strong></p></li>
<li>Let’s<a id="_idIndexMarker1060"/> also look at the distribution of the target. We have a significant class imbalance, so we will need to deal with that in some way:<p class="source-code">machinefailuretype.failtype.\</p><p class="source-code">  value_counts(dropna=False).sort_index()</p><p class="source-code"><strong class="bold">Heat Dissipation Failure     112</strong></p><p class="source-code"><strong class="bold">No Failure                   9652</strong></p><p class="source-code"><strong class="bold">Overstrain Failure           78</strong></p><p class="source-code"><strong class="bold">Power Failure                95</strong></p><p class="source-code"><strong class="bold">Random Failures              18</strong></p><p class="source-code"><strong class="bold">Tool Wear Failure            45</strong></p><p class="source-code"><strong class="bold">Name: failtype, dtype: int64</strong></p></li>
<li>We can save ourselves some trouble later by creating a numeric code for failure type, which we will use rather than the character value. We do not need to put this into<a id="_idIndexMarker1061"/> a pipeline since we are not introducing any data leakage in the conversion:<p class="source-code">def setcode(typetext):</p><p class="source-code">  if (typetext=="No Failure"):</p><p class="source-code">    typecode = 1</p><p class="source-code">  elif (typetext=="Heat Dissipation Failure"):</p><p class="source-code">    typecode = 2</p><p class="source-code">  elif (typetext=="Power Failure"):</p><p class="source-code">    typecode = 3</p><p class="source-code">  elif (typetext=="Overstrain Failure"):</p><p class="source-code">    typecode = 4</p><p class="source-code">  else:</p><p class="source-code">    typecode = 5</p><p class="source-code">  return typecode</p><p class="source-code">machinefailuretype["failtypecode"] = \</p><p class="source-code">  machinefailuretype.apply(lambda x: setcode(x.failtype), axis=1)</p></li>
<li>We should also look at some descriptive statistics. We will need to scale the features:<p class="source-code">num_cols = ['airtemp','processtemperature',</p><p class="source-code">  'rotationalspeed','torque','toolwear']</p><p class="source-code">cat_cols = ['machinetype']</p><p class="source-code">machinefailuretype[num_cols].agg(['min','median','max']).T</p><p class="source-code"><strong class="bold">                       min        median     max</strong></p><p class="source-code"><strong class="bold">airtemp                295.30     300.10     304.50</strong></p><p class="source-code"><strong class="bold">processtemperature     305.70     310.10     313.80</strong></p><p class="source-code"><strong class="bold">rotationalspeed        1,168.00   1,503.00   2,886.00</strong></p><p class="source-code"><strong class="bold">torque                 3.80       40.10      76.60</strong></p><p class="source-code"><strong class="bold">toolwear               0.00       108.00     253.00</strong></p></li>
<li>Let’s now create<a id="_idIndexMarker1062"/> training and testing DataFrames. We should also use the <strong class="source-inline">stratify</strong> parameter to ensure an equal distribution of target values in our training and testing data:<p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(machinefailuretype[num_cols + cat_cols],\</p><p class="source-code">  machinefailuretype[['failtypecode']],\</p><p class="source-code">  stratify=machinefailuretype[['failtypecode']], \</p><p class="source-code">  test_size=0.2, random_state=0)</p></li>
<li>We set up the column transformations we need to run. For the numeric columns, we set outliers to the median and then scale the values. We do one-hot-encoding of the one categorical feature, <strong class="source-inline">machinetype</strong>. It has <strong class="source-inline">H</strong>, <strong class="source-inline">M</strong>, and <strong class="source-inline">L</strong> values for high, medium, and low quality:<p class="source-code">ohe = OneHotEncoder(drop='first', sparse=False)</p><p class="source-code">cattrans = make_pipeline(ohe)</p><p class="source-code">standtrans = make_pipeline(OutlierTrans(3),</p><p class="source-code">  SimpleImputer(strategy="median"),</p><p class="source-code">  MinMaxScaler())</p><p class="source-code">coltrans = ColumnTransformer(</p><p class="source-code">  transformers=[</p><p class="source-code">    ("cat", cattrans, cat_cols),</p><p class="source-code">    ("stand", standtrans, num_cols),</p><p class="source-code">  ]</p><p class="source-code">)</p></li>
<li>Next, we <a id="_idIndexMarker1063"/>set up a pipeline with the column transformation and the SVC instance. We set the <strong class="source-inline">class_weight</strong> parameter to <strong class="source-inline">balanced</strong> to deal with class imbalance. This applies a weight that is inversely related to the frequency of the target class:<p class="source-code">svc = SVC(class_weight='balanced', probability=True)</p><p class="source-code">pipe1 = make_pipeline(coltrans, svc)</p></li>
</ol>
<p>We only have a handful of features in this case so we will not worry about feature selection. (We might still be concerned about features that are highly correlated, but that is not an issue with this dataset.)</p>
<ol>
<li value="10">We create a dictionary with the hyperparameter combinations to use with a grid search. This is largely the same as the dictionary we used in the previous section, except we have added a decision function shape key. This will cause the grid search to try both one-versus-one (<strong class="source-inline">ovo</strong>) and one-versus-rest (<strong class="source-inline">ovr</strong>) classification:<p class="source-code">svc_params = [</p><p class="source-code">  {</p><p class="source-code">    'svc__kernel': ['rbf'],</p><p class="source-code">    'svc__C': uniform(loc=0, scale=20),</p><p class="source-code">    'svc__gamma': uniform(loc=0, scale=100),</p><p class="source-code">    'svc__decision_function_shape': ['ovr','ovo']</p><p class="source-code">  },</p><p class="source-code">  {</p><p class="source-code">    'svc__kernel': ['poly'],</p><p class="source-code">    'svc__degree': np.arange(0,6),</p><p class="source-code">    'svc__C': uniform(loc=0, scale=20),</p><p class="source-code">    'svc__gamma': uniform(loc=0, scale=100),</p><p class="source-code">    'svc__decision_function_shape': ['ovr','ovo']</p><p class="source-code">  },</p><p class="source-code">  {</p><p class="source-code">    'svc__kernel': ['linear','sigmoid'],</p><p class="source-code">    'svc__C': uniform(loc=0, scale=20),</p><p class="source-code">    'svc__decision_function_shape': ['ovr','ovo']</p><p class="source-code">  }</p><p class="source-code">]</p></li>
<li>Now we are<a id="_idIndexMarker1064"/> ready to run the randomized grid search. We will base our scoring on the area under the ROC curve. The best hyperparameters include the one-versus-one decision function and the <strong class="source-inline">rbf</strong> kernel:<p class="source-code">rs = RandomizedSearchCV(pipe1, svc_params, cv=7, scoring="roc_auc_ovr", n_iter=10)</p><p class="source-code">rs.fit(X_train, y_train.values.ravel())</p><p class="source-code">rs.best_params_</p><p class="source-code"><strong class="bold">{'svc__C': 5.609789456747942,</strong></p><p class="source-code"><strong class="bold"> 'svc__decision_function_shape': 'ovo',</strong></p><p class="source-code"><strong class="bold"> 'svc__gamma': 27.73459801111866,</strong></p><p class="source-code"><strong class="bold"> 'svc__kernel': 'rbf'}</strong></p><p class="source-code">rs.best_score_</p><p class="source-code"><strong class="bold">0.9187636814475847</strong></p></li>
<li>Let’s see the score for each iteration. In addition to the best model that we saw in the previous step, there are several other hyperparameter combinations that have scores that <a id="_idIndexMarker1065"/>are nearly as high. One-versus-rest with a linear kernel does nearly as well as the best-performing model:<p class="source-code">results = \</p><p class="source-code">  pd.DataFrame(rs.cv_results_['mean_test_score'], \</p><p class="source-code">    columns=['meanscore']).\</p><p class="source-code">  join(pd.json_normalize(rs.cv_results_['params'])).\</p><p class="source-code">  sort_values(['meanscore'], ascending=False)</p><p class="source-code">results</p><p class="source-code"><strong class="bold">meanscore  svc__C svc__decision_function_shape  svc__gamma svc__kernel</strong></p><p class="source-code"><strong class="bold">7     0.92     5.61     ovo     27.73     rbf</strong></p><p class="source-code"><strong class="bold">5     0.91     9.43     ovr     NaN       linear</strong></p><p class="source-code"><strong class="bold">3     0.91     5.40     ovr     NaN       linear</strong></p><p class="source-code"><strong class="bold">0     0.90     19.84    ovr     28.70     rbf</strong></p><p class="source-code"><strong class="bold">8     0.87     5.34     ovo     93.87     rbf</strong></p><p class="source-code"><strong class="bold">6     0.86     8.05     ovr     80.57     rbf</strong></p><p class="source-code"><strong class="bold">9     0.86     4.41     ovo     66.66     rbf</strong></p><p class="source-code"><strong class="bold">1     0.86     3.21     ovr     85.35     rbf</strong></p><p class="source-code"><strong class="bold">4     0.85     0.01     ovo     38.24     rbf</strong></p><p class="source-code"><strong class="bold">2     0.66     7.61     ovr     NaN       sigmoid</strong></p></li>
<li>We should take a look at the confusion matrix:<p class="source-code">pred = rs.predict(X_test)</p><p class="source-code">cm = skmet.confusion_matrix(y_test, pred)</p><p class="source-code">cmplot = skmet.ConfusionMatrixDisplay(confusion_matrix=cm,</p><p class="source-code">   display_labels=['None', 'Heat','Power','Overstrain','Other'])</p><p class="source-code">cmplot.plot()</p><p class="source-code">cmplot.ax_.set(title='Machine Failure Type Confusion Matrix', </p><p class="source-code">  xlabel='Predicted Value', ylabel='Actual Value')</p></li>
</ol>
<p>This produces<a id="_idIndexMarker1066"/> the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer230">
<img alt="Figure 13.13 – Confusion matrix for machine failure type prediction " height="441" src="image/B17978_13_013.jpg" width="571"/>
</div>
</div>
<p class="figure-caption">Figure 13.13 – Confusion matrix for machine failure type prediction</p>
<ol>
<li value="14">Let’s also do a classification report. We do not get great scores for sensitivity for most classes, though <a id="_idIndexMarker1067"/>our model does predict heat and overstrain failures pretty well:<p class="source-code">print(skmet.classification_report(y_test, pred,</p><p class="source-code">  target_names=['None', 'Heat','Power', 'Overstrain', 'Other']))</p><p class="source-code">  <strong class="bold">            precision    recall  f1-score   support</strong></p><p class="source-code"><strong class="bold">        None       0.99      0.97      0.98      1930</strong></p><p class="source-code"><strong class="bold">        Heat       0.50      0.91      0.65        22</strong></p><p class="source-code"><strong class="bold">       Power       0.60      0.47      0.53        19</strong></p><p class="source-code"><strong class="bold">  Overstrain       0.65      0.81      0.72        16</strong></p><p class="source-code"><strong class="bold">       Other       0.06      0.15      0.09        13</strong></p><p class="source-code"><strong class="bold">    accuracy                           0.96      2000</strong></p><p class="source-code"><strong class="bold">   macro avg       0.56      0.66      0.59      2000</strong></p><p class="source-code"><strong class="bold">weighted avg       0.97      0.96      0.96      2000</strong></p></li>
</ol>
<p>When modeling targets such as machine failure types that have a high class imbalance, we are often more concerned with metrics other than accuracy. This is partly determined by our domain knowledge. Avoiding false negatives may be more important than avoiding false positives. Doing a thorough check on a machine too early is definitely preferable to doing it too late. </p>
<p>The 96 to 97 percent weighted precision, recall (sensitivity), and f1 scores do not provide a good sense of the performance of our model. They mainly reflect the large class imbalance and the fact that it is very easy to predict no machine failure. The much lower macro averages (which are just simple averages across classes) indicate that our model struggles to predict some types of machine failure.</p>
<p>This example <a id="_idIndexMarker1068"/>illustrates that it is relatively easy to extend SVC to models that have targets with more than two values. We can specify whether we want to use one-versus-one or one-versus-rest classification. The one-versus-rest approach can be faster when the number of classes is above three since there will be fewer classifiers trained.</p>
<h1 id="_idParaDest-160"><a id="_idTextAnchor161"/>Summary</h1>
<p>In this chapter, we explored the different strategies for implementing SVC. We used linear SVC (which does not use kernels), which can perform very well when our classes are linearly separable. We then examined how to use the kernel trick to extend SVC to cases where the classes are not linearly separable. Finally, we used one-versus-one and one-versus-rest classification to handle targets with more than two values.</p>
<p>SVC is an exceptionally useful technique for binary and multiclass classification. It can handle both straightforward and complicated relationships between features and the target. There are few supervised learning problems for which SVMs should not at least be considered. However, it is not very efficient with very large datasets.</p>
<p>In the next chapter, we will explore another popular and flexible classification algorithm, Naive Bayes.</p>
</div>
</div>
</body></html>