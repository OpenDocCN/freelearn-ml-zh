<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer307">
    <h1 class="chapterNumber">6</h1>
    <h1 class="chapterTitle" id="_idParaDest-134">Forecasting Numeric Data – Regression Methods</h1>
    <p class="normal">Mathematical relationships help us to make sense of many aspects of everyday life. For example, body weight is a function of one’s calorie intake; income is often related to education and job experience; and poll numbers help to estimate a presidential candidate’s odds of being re-elected.</p>
    <p class="normal">When such patterns are formulated with numbers, we gain additional clarity. For example, an additional 250 kilocalories consumed daily may result in nearly a kilogram of weight gain per month; each year of job experience may be worth an additional $1,000 in yearly salary; and a president is more likely to be re-elected when the economy is strong. Obviously, these equations do not perfectly fit every situation, but we expect that they are reasonably correct most of the time.</p>
    <p class="normal">This chapter extends our machine learning toolkit by going beyond the classification methods covered previously and introducing techniques for estimating relationships within numeric data. While examining several real-world numeric prediction tasks, you will learn:</p>
    <ul>
      <li class="bulletList">The basic statistical principles used in regression, a technique that models the size and strength of numeric relationships</li>
      <li class="bulletList">How to prepare data for regression analysis, estimate and interpret regression models, and apply regression variants such as generalized linear models</li>
      <li class="bulletList">A pair of hybrid techniques known as regression trees and model trees, which adapt decision tree classifiers for numeric prediction tasks</li>
    </ul>
    <p class="normal">Based on a large body of work in the field of statistics, the methods used in this chapter are a bit heavier on math than those covered previously, but don’t worry! Even if your algebra skills are a bit rusty, R takes care of the heavy lifting.</p>
    <h1 class="heading-1" id="_idParaDest-135">Understanding regression</h1>
    <p class="normal">Regression involves <a id="_idIndexMarker619"/>specifying the relationship <a id="_idIndexMarker620"/>between a single numeric <strong class="keyWord">dependent variable</strong> (the value to be predicted) and one or more <a id="_idIndexMarker621"/>numeric <strong class="keyWord">independent variables</strong> (the predictors). As the name implies, the dependent variable depends upon the value of the independent variable or variables. The simplest forms of regression assume that the relationship between the independent and dependent variables follows a straight line.</p>
    <div class="note">
      <p class="normal">The origin of the term “regression” to describe the process of fitting lines to data is rooted in a study of genetics by Sir Francis Galton in the late 19th century. He discovered that fathers who were extremely short or tall tended to have sons whose heights were closer to the average height. He called this phenomenon “regression to the mean.”</p>
    </div>
    <p class="normal">You might recall from basic algebra that lines can be <a id="_idIndexMarker622"/>defined in a <strong class="keyWord">slope-intercept form</strong> similar to <em class="italic">y</em> = <em class="italic">a</em> + <em class="italic">bx</em>. In this form, the letter <em class="italic">y</em> indicates the dependent variable and <em class="italic">x</em> indicates the independent variable. The <strong class="keyWord">slope</strong> term <em class="italic">b</em> specifies how much the line rises for each increase in <em class="italic">x</em>. Positive <a id="_idIndexMarker623"/>values define lines that slope upward while negative values define lines that slope downward. The term <em class="italic">a</em> is known as<a id="_idIndexMarker624"/> the <strong class="keyWord">intercept</strong> because it specifies the point where the line crosses, or intercepts, the vertical <em class="italic">y</em> axis. It indicates the value of <em class="italic">y</em> when <em class="italic">x</em> = 0.</p>
    <figure class="mediaobject"><img alt="" src="../Images/B17290_06_01.png"/></figure>
    <p class="packt_figref">Figure 6.1: Examples of lines with various slopes and intercepts</p>
    <p class="normal">Regression equations model data using a similar slope-intercept format. The machine’s job is to identify values of <em class="italic">a</em> and <em class="italic">b</em> such that the specified line is best able to relate the supplied <em class="italic">x</em> values to the values of <em class="italic">y</em>. </p>
    <p class="normal">There may not always be a single set of <em class="italic">a</em> and <em class="italic">b</em> parameters that perfectly relates the values, so the machine must also have some way to quantify the margin of error and choose the best fit. We’ll discuss this in depth shortly.</p>
    <p class="normal">Regression analysis is <a id="_idIndexMarker625"/>used for a huge variety of tasks—it is almost surely the most widely used machine learning method. It can be used both for explaining the past and extrapolating into the future <a id="_idIndexMarker626"/>and can be applied to nearly any task. Some specific use cases include:</p>
    <ul>
      <li class="bulletList">Examining how populations and individuals vary by their measured characteristics, in scientific studies in the fields of economics, sociology, psychology, physics, and ecology</li>
      <li class="bulletList">Quantifying the causal relationship between an event and its response, in cases such as clinical drug trials, engineering safety tests, or market research</li>
      <li class="bulletList">Identifying patterns that can be used to forecast future behavior given known criteria, such as for predicting insurance claims, natural disaster damage, election results, and crime rates</li>
    </ul>
    <p class="normal">Regression methods are also <a id="_idIndexMarker627"/>used for <strong class="keyWord">statistical hypothesis testing</strong>, which determines whether a premise is likely to be true or false in light of observed data. The regression model’s estimates of the strength and consistency of a relationship provide information that can be used to assess whether the observations are due to chance alone.</p>
    <div class="note">
      <p class="normal">Hypothesis testing is <a id="_idIndexMarker628"/>extremely nuanced and falls outside the scope of machine learning. If you are interested in this topic, an introductory statistics textbook is a good place to get started, for instance, <em class="italic">Intuitive Introductory Statistics, Wolfe, D. A. and Schneider, G., Springer, 2017</em>.</p>
    </div>
    <p class="normal">Regression analysis is not synonymous with a single algorithm. Rather, it is an umbrella term for many methods, which can be adapted to nearly any machine learning task. If you were limited to choosing only a single machine learning method to study, regression would be a good choice. One could devote an entire career to nothing else and perhaps still have much to learn.</p>
    <p class="normal">In this chapter, we’ll start with the most <a id="_idIndexMarker629"/>basic <strong class="keyWord">linear regression</strong> models—those that use straight lines. The case when there is only a single independent variable is known<a id="_idIndexMarker630"/> as <strong class="keyWord">simple linear regression</strong>. In the case of two <a id="_idIndexMarker631"/>or more independent variables, it is known as <strong class="keyWord">multiple linear regression</strong>, or <a id="_idIndexMarker632"/>simply <strong class="keyWord">multiple regression</strong>. Both techniques assume a single dependent variable, which is measured on a continuous scale.</p>
    <p class="normal">Regression <a id="_idIndexMarker633"/>can also be used for other types of dependent variables and even for some classification tasks. For instance, <strong class="keyWord">logistic regression</strong> is<a id="_idIndexMarker634"/> used to model a binary categorical outcome, while <strong class="keyWord">Poisson regression</strong>—named after the <a id="_idIndexMarker635"/>French mathematician Siméon Poisson—models integer count data. The method known<a id="_idIndexMarker636"/> as <strong class="keyWord">multinomial logistic regression</strong> models a categorical outcome and can therefore be used for classification.</p>
    <p class="normal">These specialized<a id="_idIndexMarker637"/> regression methods fall into the class of <strong class="keyWord">generalized linear models</strong> (<strong class="keyWord">GLMs</strong>), which adapt the straight lines of traditional regression models to allow the modeling of other forms of data. These will be described later in this chapter.</p>
    <p class="normal">Because similar statistical principles apply across all regression methods, once you understand the linear case, learning about the other variants is straightforward. We’ll begin with the basic case of simple linear regression. Despite the name, this method is not too simple to address complex problems. In the next section, we’ll see how the use of a simple linear regression model might have averted a tragic engineering disaster.</p>
    <h2 class="heading-2" id="_idParaDest-136">Simple linear regression</h2>
    <p class="normal">On<a id="_idIndexMarker638"/> January 28th, 1986, seven crew members of the United States <a id="_idIndexMarker639"/>space shuttle <em class="italic">Challenger</em> were killed when a rocket booster failed, causing a catastrophic disintegration. In the aftermath, experts quickly focused on the launch temperature as a potential culprit. The rubber O-rings responsible for sealing the rocket joints had never been tested below 40<img alt="" src="../Images/B17290_06_001.png"/>F (4<img alt="" src="../Images/B17290_06_001.png"/>C), and the weather on launch day was unusually cold and below freezing.</p>
    <p class="normal">With the benefit of hindsight, the accident has been a case study for the importance of data analysis and <a id="_idIndexMarker640"/>visualization. Although it is unclear what information was available to the rocket engineers and decision-makers leading up to the launch, it is undeniable that better data, utilized carefully, might very well have averted this disaster.</p>
    <div class="note">
      <p class="normal">This section’s analysis is based on data presented in <em class="italic">Risk Analysis of the Space Shuttle: Pre-Challenger Prediction of Failure, Dalal, S. R., Fowlkes, E. B., and Hoadley, B., Journal of the American Statistical Association, 1989, Vol. 84, pp. 945-957</em>. For one perspective on how data may have changed the result, see <em class="italic">Visual Explanations: Images And Quantities, Evidence And Narrative, Tufte, E. R., Cheshire, C. T.: Graphics Press, 1997</em>. For a counterpoint, see <em class="italic">Representation and misrepresentation: Tufte and the Morton Thiokol engineers on the Challenger, Robison, W., Boisjoly, R., Hoeker, D., and Young, S., Science and Engineering Ethics, 2002, Vol. 8, pp. 59-81</em>.</p>
    </div>
    <p class="normal">The <a id="_idIndexMarker641"/>rocket engineers almost certainly knew that cold temperatures could make the components more brittle and less able to seal properly, which would result in a higher chance of a dangerous fuel leak. However, given the political pressure to continue with the launch, they needed data to support this hypothesis. A regression model that demonstrated a link between temperature and O-ring failures, and could forecast the chance of failure given the expected temperature at launch, might have been very helpful.</p>
    <p class="normal">To build the regression model, the scientists might have used the data on launch temperature and component distress recorded during the 23 previous successful shuttle launches. Component distress indicates one of two types of problems. The first problem, called erosion, occurs when excessive heat burns up the O-ring. The second problem, called blow-by, occurs when hot gasses leak through or “blow by” a poorly sealed O-ring. Since the shuttle had a total of six primary O-rings, up to six distresses could occur per flight. Though the rocket could survive one or more distress events or be destroyed with as few as one, each additional distress increased the probability of a catastrophic failure. The following scatterplot shows a plot of primary O-ring distresses detected for the previous 23 launches, as compared to the temperature at launch:</p>
    <figure class="mediaobject"><img alt="Chart, scatter chart  Description automatically generated" src="../Images/B17290_06_02.png"/></figure>
    <p class="packt_figref">Figure 6.2: A visualization of space shuttle O-ring distresses versus launch temperature</p>
    <p class="normal">Examining <a id="_idIndexMarker642"/>the plot, there is an apparent trend: launches <a id="_idIndexMarker643"/>occurring at higher temperatures tend to have fewer O-ring distress events. Additionally, the coldest launch (53<img alt="" src="../Images/B17290_06_001.png"/>F) had two distress events, a level that had only been reached in one other launch. With this information in mind, the fact that the Challenger was scheduled to launch in conditions more than 20 degrees colder seems concerning. But exactly how concerned should they have been? To answer this question, we can turn to simple linear regression.</p>
    <p class="normal">A simple linear regression model defines the relationship between a dependent variable and a single independent predictor variable using a line defined by an equation in the following form:</p>
    <p class="center"><img alt="" src="../Images/B17290_06_004.png"/></p>
    <p class="normal">Aside from the Greek characters, this equation is virtually identical to the slope-intercept form described previously. The intercept, <img alt="" src="../Images/B17290_06_005.png"/> (alpha), describes where the line crosses the <em class="italic">y</em> axis, while the slope, <img alt="" src="../Images/B17290_06_006.png"/> (beta), describes the change in <em class="italic">y</em> given an increase of <em class="italic">x</em>. For the shuttle launch data, the slope would tell us <a id="_idIndexMarker644"/>the expected change in O-ring failures for each <a id="_idIndexMarker645"/>degree the launch temperature increases.</p>
    <div class="packt_tip">
      <p class="normal">Greek characters are often used in the field of statistics to indicate variables that are parameters of a statistical function. Therefore, performing a regression analysis involves finding <strong class="keyWord">parameter estimates</strong> for <img alt="" src="../Images/B17290_06_007.png"/> and <img alt="" src="../Images/B17290_06_008.png"/>. The parameter estimates for alpha and beta are typically denoted using <em class="italic">a</em> and <em class="italic">b</em>, although you may find that some of this terminology and notation is used interchangeably.</p>
    </div>
    <p class="normal">Suppose we know that the estimated regression parameters in the equation for the shuttle launch data are <em class="italic">a</em> = 3.70 and <em class="italic">b</em> = -0.048. Consequently, the full linear equation is <em class="italic">y</em> = 3.70 – 0.048x. Ignoring for a moment how these numbers were obtained, we can plot the line on the scatterplot like this:</p>
    <figure class="mediaobject"><img alt="Chart, line chart, scatter chart  Description automatically generated" src="../Images/B17290_06_03.png"/></figure>
    <p class="packt_figref">Figure 6.3: A regression line modeling the relationship between distress events and launch temperature</p>
    <p class="normal">As the line shows, at 60 degrees Fahrenheit, we predict less than one O-ring distress event. At 50 degrees <a id="_idIndexMarker646"/>Fahrenheit, we expect around 1.3 failures. If we use the <a id="_idIndexMarker647"/>model to extrapolate all the way out to 31 degrees—the forecasted temperature for the Challenger launch—we would expect about <em class="italic">3.70 - 0.048 * 31 = 2.21</em> O-ring distress events.</p>
    <p class="normal">Assuming that each O-ring failure is equally likely to cause a catastrophic fuel leak, this means that the Challenger launch at 31 degrees was nearly three times riskier than the typical launch at 60 degrees, and over eight times riskier than a launch at 70 degrees.</p>
    <p class="normal">Notice that the line doesn’t pass through each data point exactly. Instead, it cuts through the data somewhat evenly, with some predictions lower or higher than the line. In the next section, we will learn why this particular line was chosen.</p>
    <h2 class="heading-2" id="_idParaDest-137">Ordinary least squares estimation</h2>
    <p class="normal">To <a id="_idIndexMarker648"/>determine the optimal estimates of <img alt="" src="../Images/B17290_06_005.png"/> and <img alt="" src="../Images/B17290_06_006.png"/> an estimation method known as <strong class="keyWord">ordinary least squares</strong> (<strong class="keyWord">OLS</strong>) is <a id="_idIndexMarker649"/>used. In OLS regression, the slope and <a id="_idIndexMarker650"/>intercept are chosen such that they minimize<a id="_idIndexMarker651"/> the <strong class="keyWord">sum of the squared errors</strong> (<strong class="keyWord">SSE</strong>). The errors, also known<a id="_idIndexMarker652"/> as <strong class="keyWord">residuals</strong>, are the vertical distance between the predicted <em class="italic">y</em> value and the actual <em class="italic">y</em> value. Because the errors can be over-estimates or under-estimates, they can be positive or negative values; squaring them makes the errors positive regardless of direction. The residuals are illustrated for several points in the following diagram:</p>
    <figure class="mediaobject"><img alt="Chart, line chart  Description automatically generated" src="../Images/B17290_06_04.png"/></figure>
    <p class="packt_figref">Figure 6.4: The regression line predictions differ from the actual values by a residual amount</p>
    <p class="normal">In <a id="_idIndexMarker653"/>mathematical terms, the goal of OLS regression can be expressed as the task of minimizing the following equation:</p>
    <p class="center"><img alt="" src="../Images/B17290_06_011.png"/></p>
    <p class="normal">In plain<a id="_idIndexMarker654"/> language, this equation defines <em class="italic">e</em> (the error) as the difference between the actual <em class="italic">y</em> value and the predicted <em class="italic">y</em> value. The error values are squared to eliminate the negative values and summed across all points in the data.</p>
    <div class="packt_tip">
      <p class="normal">The caret character (^) above the <em class="italic">y</em> term is a commonly used feature of statistical notation. It indicates that the term is an estimate for the true <em class="italic">y</em> value. This is referred to as the <em class="italic">y hat</em>.</p>
    </div>
    <p class="normal">The solution for <em class="italic">a</em> depends on the value of <em class="italic">b</em>. It can be obtained using the following formula:</p>
    <p class="center"><img alt="" src="../Images/B17290_06_012.png"/></p>
    <div class="packt_tip">
      <p class="normal">To understand these equations, you’ll need to know another bit of statistical notation. The horizontal bar appearing over the <em class="italic">x</em> and <em class="italic">y</em> terms indicates the mean value of <em class="italic">x</em> or <em class="italic">y</em>. This is referred to as the <em class="italic">x</em> bar or <em class="italic">y</em> bar.</p>
    </div>
    <p class="normal">Though the proof<a id="_idIndexMarker655"/> is beyond the scope of this book, it can be shown using calculus that the value of <em class="italic">b</em> that results in the minimum squared error is:</p>
    <p class="center"><img alt="" src="../Images/B17290_06_013.png"/></p>
    <p class="normal">If we <a id="_idIndexMarker656"/>break this equation apart into its component pieces, we can simplify it somewhat. The denominator for <em class="italic">b</em> should look familiar; it is very similar to the variance of <em class="italic">x</em>, which is denoted as Var(<em class="italic">x</em>). As we learned in <em class="chapterRef">Chapter 2</em>, <em class="italic">Managing and Understanding Data</em>, the variance involves finding the average squared deviation from the mean of <em class="italic">x</em>. This can be expressed as:</p>
    <p class="center"><img alt="" src="../Images/B17290_06_014.png"/></p>
    <p class="normal">The numerator involves taking the sum of each data point’s deviation from the mean <em class="italic">x</em> value multiplied by that point’s deviation away from the mean <em class="italic">y</em> value. This is similar to the covariance function for <em class="italic">x</em> and <em class="italic">y</em>, denoted as Cov(<em class="italic">x</em>, <em class="italic">y</em>). The covariance formula is:</p>
    <p class="center"><img alt="" src="../Images/B17290_06_015.png"/></p>
    <p class="normal">If we divide the covariance function by the variance function, the <em class="italic">n</em> terms in the numerator and denominator cancel each other out and we can rewrite the formula for <em class="italic">b</em> as:</p>
    <p class="center"><img alt="" src="../Images/B17290_06_016.png"/></p>
    <p class="normal">Given this restatement, it is easy to calculate the value of <em class="italic">b</em> using built-in R functions. Let’s apply them to the shuttle launch data to estimate the regression line.</p>
    <div class="packt_tip">
      <p class="normal">If you would like to follow along with these examples, download the <code class="inlineCode">challenger.csv</code> file from the Packt Publishing website and load it to a data frame using the <code class="inlineCode">launch &lt;- read.csv("challenger.csv")</code> command.</p>
    </div>
    <p class="normal">If the <a id="_idIndexMarker657"/>shuttle launch data is stored in a data<a id="_idIndexMarker658"/> frame named <code class="inlineCode">launch</code>, the independent variable <em class="italic">x</em> is named <code class="inlineCode">temperature</code>, and the dependent variable <em class="italic">y</em> is named <code class="inlineCode">distress_ct</code>, we can then use the R functions <code class="inlineCode">cov()</code> and <code class="inlineCode">var()</code> to estimate <code class="inlineCode">b</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> b <span class="hljs-operator">&lt;-</span> cov<span class="hljs-punctuation">(</span>launch<span class="hljs-operator">$</span>temperature<span class="hljs-punctuation">,</span> launch<span class="hljs-operator">$</span>distress_ct<span class="hljs-punctuation">)</span> <span class="hljs-operator">/</span>
         var<span class="hljs-punctuation">(</span>launch<span class="hljs-operator">$</span>temperature<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> b
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] -0.04753968
</code></pre>
    <p class="normal">We can then estimate <code class="inlineCode">a</code> by using the computed <code class="inlineCode">b</code> value and applying the <code class="inlineCode">mean()</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> a <span class="hljs-operator">&lt;-</span> mean<span class="hljs-punctuation">(</span>launch<span class="hljs-operator">$</span>distress_ct<span class="hljs-punctuation">)</span> <span class="hljs-operator">-</span> b <span class="hljs-operator">*</span> mean<span class="hljs-punctuation">(</span>launch<span class="hljs-operator">$</span>temperature<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> a
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 3.698413
</code></pre>
    <p class="normal">Estimating the regression equation by hand is obviously less than ideal, so R predictably provides a function for fitting regression models automatically. We will use this function shortly. Before that, it is important to expand your understanding of the regression model’s fit by first learning a method for measuring the strength of a linear relationship. Additionally, you will soon learn how to apply multiple linear regression to problems with more than one independent variable.</p>
    <h2 class="heading-2" id="_idParaDest-138">Correlations</h2>
    <p class="normal">The <strong class="keyWord">correlation</strong> between<a id="_idIndexMarker659"/> two variables is a number that indicates how <a id="_idIndexMarker660"/>closely their relationship follows a straight line. Without additional qualification, correlation typically refers to <a id="_idIndexMarker661"/>the <strong class="keyWord">Pearson correlation coefficient</strong>, which was developed by the 20th-century mathematician Karl Pearson. A correlation falls in the range between -1 to +1. The maximum and minimum values indicate a perfectly linear relationship, while a correlation close to zero indicates the absence of a linear relationship.</p>
    <p class="normal">The following formula defines Pearson’s correlation:</p>
    <p class="center"><img alt="" src="../Images/B17290_06_017.png"/></p>
    <div class="packt_tip">
      <p class="normal">More Greek notation has been introduced here: the first symbol (which looks like a lowercase <em class="italic">p</em>) is <em class="italic">rho</em>, and it is used to denote the Pearson correlation statistic. The symbols that look like <em class="italic">q</em> characters rotated counterclockwise are the lowercase Greek letter <em class="italic">sigma</em>, and they indicate the standard deviation of <em class="italic">x</em> or <em class="italic">y</em>.</p>
    </div>
    <p class="normal">Using this <a id="_idIndexMarker662"/>formula, we can calculate the correlation between <a id="_idIndexMarker663"/>the launch temperature and the number of O-ring distress events. Recall that the covariance function is <code class="inlineCode">cov()</code> and the standard deviation function is <code class="inlineCode">sd()</code>. We’ll store the result in <code class="inlineCode">r</code>, a letter that is commonly used to indicate the estimated correlation:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> r <span class="hljs-operator">&lt;-</span> cov<span class="hljs-punctuation">(</span>launch<span class="hljs-operator">$</span>temperature<span class="hljs-punctuation">,</span> launch<span class="hljs-operator">$</span>distress_ct<span class="hljs-punctuation">)</span> <span class="hljs-operator">/</span>
         <span class="hljs-punctuation">(</span>sd<span class="hljs-punctuation">(</span>launch<span class="hljs-operator">$</span>temperature<span class="hljs-punctuation">)</span> <span class="hljs-operator">*</span> sd<span class="hljs-punctuation">(</span>launch<span class="hljs-operator">$</span>distress_ct<span class="hljs-punctuation">))</span>
<span class="hljs-operator">&gt;</span> r
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] -0.5111264
</code></pre>
    <p class="normal">Alternatively, we can obtain the same result with the <code class="inlineCode">cor()</code> correlation function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> cor<span class="hljs-punctuation">(</span>launch<span class="hljs-operator">$</span>temperature<span class="hljs-punctuation">,</span> launch<span class="hljs-operator">$</span>distress_ct<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] -0.5111264
</code></pre>
    <p class="normal">The correlation between the temperature and the number of distressed O-rings is -0.51. The negative correlation implies that increases in temperature are related to decreases in the number of distressed O-rings. To the NASA engineers studying the O-ring data, this would have been a very clear indicator that a low-temperature launch could be problematic. The correlation also tells us about the relative strength of the relationship between temperature and O-ring distress. Because -0.51 is halfway to the maximum negative correlation of -1, this implies that there is a moderately strong negative linear association.</p>
    <p class="normal">There are various rules of thumb used to interpret correlation strength. One method assigns a status of “weak” to values between 0.1 and 0.3; “moderate” to the range of 0.3 to 0.5; and “strong” to values above 0.5 (these also apply to similar ranges of negative correlations). However, these thresholds may be too strict or too lax for certain purposes. Often, the correlation must be interpreted in context. </p>
    <p class="normal">For data involving human beings, a correlation of 0.5 may be considered very high; for data generated by mechanical processes, a correlation of 0.5 may be very weak.</p>
    <div class="packt_tip">
      <p class="normal">You have probably heard the expression “correlation does not imply causation.” This is rooted in the fact that a correlation only describes the association between a pair of variables, yet there could be other explanations that are unaccounted for and responsible for the observed relationship. For example, there may be a strong correlation between life expectancy and time per day spent watching movies, but before doctors recommend that we all watch more movies, we need to rule out another explanation: younger people watch more movies and younger people are (in general) less likely to die.</p>
    </div>
    <p class="normal">Measuring <a id="_idIndexMarker664"/>the <a id="_idIndexMarker665"/>correlation between two variables gives us a way to quickly check for linear relationships between independent variables and the dependent variable. This will be increasingly important as we start defining regression models with a larger number of predictors.</p>
    <h2 class="heading-2" id="_idParaDest-139">Multiple linear regression</h2>
    <p class="normal">Most <a id="_idIndexMarker666"/>real-world analyses have more than one<a id="_idIndexMarker667"/> independent variable. Therefore, it is likely that you will be using multiple linear regression for most numeric prediction tasks. The strengths and<a id="_idIndexMarker668"/> weaknesses of multiple linear regression are <a id="_idIndexMarker669"/>shown in the following table:</p>
    <table class="table-container" id="table001-4">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Strengths</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Weaknesses</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <ul>
              <li class="bulletList">By far the most common approach to modeling numeric data</li>
              <li class="bulletList">Can be adapted to model almost any modeling task</li>
              <li class="bulletList">Provides estimates of both the size and strength of the relationships among features and the outcome</li>
            </ul>
          </td>
          <td class="table-cell">
            <ul>
              <li class="bulletList">Makes strong assumptions about the data</li>
              <li class="bulletList">The model’s form must be specified by the user in advance</li>
              <li class="bulletList">Does not handle missing data</li>
              <li class="bulletList">Only works with numeric features, so categorical data requires additional preparation</li>
              <li class="bulletList">Requires some knowledge of statistics to understand the model</li>
            </ul>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">We can understand multiple regression as an extension of simple linear regression. The goal in both cases is similar—to find values of slope coefficients that minimize the prediction error of a linear equation. The key difference is that there are additional terms for the additional independent variables.</p>
    <p class="normal">Multiple regression models<a id="_idIndexMarker670"/> take the form of the following<a id="_idIndexMarker671"/> equation. The dependent variable <em class="italic">y</em> is specified as the sum of an intercept term <img alt="" src="../Images/B17290_06_005.png"/> plus the product of the estimated <img alt="" src="../Images/B17290_06_006.png"/> value and the <em class="italic">x</em> variable for each of <em class="italic">i</em> features. An error term <img alt="" src="../Images/B17290_06_020.png"/> (denoted by the Greek letter <em class="italic">epsilon</em>) has been added here as a reminder that the predictions are not perfect. This represents the residual term noted previously:</p>
    <p class="center"><img alt="" src="../Images/B17290_06_021.png"/></p>
    <p class="normal">Let’s consider for a moment the interpretation of the estimated regression parameters. You will note that in the preceding equation, a coefficient is provided for each feature. This allows each feature to have a separate estimated effect on the value of <em class="italic">y</em>. In other words, <em class="italic">y</em> changes by the amount <img alt="" src="../Images/B17290_06_022.png"/> for each unit increase in feature <img alt="" src="../Images/B17290_06_023.png"/>. The intercept <img alt="" src="../Images/B17290_06_005.png"/> is then the expected value of <em class="italic">y</em> when the independent variables are all zero.</p>
    <p class="normal">Since the intercept term <img alt="" src="../Images/B17290_06_005.png"/> is really no different than any other regression parameter, it is also sometimes denoted as <img alt="" src="../Images/B17290_06_024.png"/> (pronounced <em class="italic">beta naught</em>) as shown in the following equation:</p>
    <p class="center"><img alt="" src="../Images/B17290_06_025.png"/></p>
    <p class="normal">Just like before, the intercept is unrelated to any of the independent <em class="italic">x</em> variables. However, for reasons that will become clear shortly, it helps to imagine <img alt="" src="../Images/B17290_06_024.png"/> as if it were being multiplied by a term <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">0</sub>. We assign <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">0</sub> to be a constant with the value of 1:</p>
    <p class="center"><img alt="" src="../Images/B17290_06_027.png"/></p>
    <p class="normal">To<a id="_idIndexMarker672"/> estimate the regression parameters, each observed value of the dependent variable <em class="italic">y</em> must be related to observed values of the independent <em class="italic">x</em> variables using the regression equation in the previous form. The following figure is a graphical representation of the setup of a multiple regression task:</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_06_05.png"/></figure>
    <p class="packt_figref">Figure 6.5: Multiple regression seeks to find the <img alt="" src="../Images/B17290_06_006.png"/> values that relate the X values to Y while minimizing <img alt="" src="../Images/B17290_06_020.png"/></p>
    <p class="normal">The many rows and columns of data illustrated in the preceding figure can be described in a condensed <a id="_idIndexMarker673"/>formulation <a id="_idIndexMarker674"/>using <strong class="keyWord">matrix notation</strong> in bold font to indicate that each of the terms represents multiple values. Simplified in this way, the formula is as follows:</p>
    <p class="center"><img alt="" src="../Images/B17290_06_030.png"/></p>
    <p class="normal">In matrix <a id="_idIndexMarker675"/>notation, the dependent variable is a vector, <strong class="keyWord">Y</strong>, with a row for every example. The independent variables have been combined into a matrix, <strong class="keyWord">X</strong>, with a column for each feature plus an additional column of 1 values for the intercept. Each column has a row for every example. The regression coefficients <img alt="" src="../Images/B17290_06_031.png"/> and residual errors <strong class="keyWord"><img alt="" src="../Images/_eqn_032.png"/></strong> are also now vectors.</p>
    <p class="normal">The goal now is to solve for <img alt="" src="../Images/B17290_06_031.png"/>, the vector of regression coefficients that minimizes the sum of the squared errors between the predicted and actual <strong class="keyWord">Y</strong> values. Finding the optimal solution requires the use of matrix algebra; therefore, the derivation deserves more careful attention than can be provided in this text. However, if you’re willing to trust the work of others, the best estimate of the vector <img alt="" src="../Images/B17290_06_031.png"/> can be computed as:</p>
    <p class="center"><img alt="" src="../Images/B17290_06_034.png"/></p>
    <p class="normal">This solution uses a pair of matrix operations: the <strong class="keyWord">T</strong> indicates the transpose of matrix <strong class="keyWord">X</strong>, while the negative exponent indicates<a id="_idIndexMarker676"/> the <strong class="keyWord">matrix inverse</strong>. Using R’s built-in matrix operations, we can thus implement a simple multiple regression learner. Let’s apply this formula to the Challenger launch data.</p>
    <div class="packt_tip">
      <p class="normal">If you are unfamiliar with the preceding matrix operations, the Wolfram MathWorld pages for transpose (<a href="http://mathworld.wolfram.com/Transpose.html"><span class="url">http://mathworld.wolfram.com/Transpose.html</span></a>) and matrix inverse (<a href="http://mathworld.wolfram.com/MatrixInverse.html"><span class="url">http://mathworld.wolfram.com/MatrixInverse.html</span></a>) provide a thorough introduction and are understandable even without an advanced mathematics background.</p>
    </div>
    <p class="normal">Using the<a id="_idIndexMarker677"/> following code, we can create a basic regression<a id="_idIndexMarker678"/> function named <code class="inlineCode">reg()</code>, which takes a parameter <code class="inlineCode">y</code> and a parameter <code class="inlineCode">x</code>, and returns a vector of estimated beta coefficients:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> reg <span class="hljs-operator">&lt;-</span> <span class="hljs-keyword">function</span><span class="hljs-punctuation">(</span>y<span class="hljs-punctuation">,</span> x<span class="hljs-punctuation">)</span> <span class="hljs-punctuation">{</span>
    x <span class="hljs-operator">&lt;-</span> as.matrix<span class="hljs-punctuation">(</span>x<span class="hljs-punctuation">)</span>
    x <span class="hljs-operator">&lt;-</span> cbind<span class="hljs-punctuation">(</span>Intercept <span class="hljs-operator">=</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> x<span class="hljs-punctuation">)</span>
    b <span class="hljs-operator">&lt;-</span> solve<span class="hljs-punctuation">(</span>t<span class="hljs-punctuation">(</span>x<span class="hljs-punctuation">)</span> <span class="hljs-operator">%*%</span> x<span class="hljs-punctuation">)</span> <span class="hljs-operator">%*%</span> t<span class="hljs-punctuation">(</span>x<span class="hljs-punctuation">)</span> <span class="hljs-operator">%*%</span> y
    colnames<span class="hljs-punctuation">(</span>b<span class="hljs-punctuation">)</span> <span class="hljs-operator">&lt;-</span> <span class="hljs-string">"estimate"</span>
    print<span class="hljs-punctuation">(</span>b<span class="hljs-punctuation">)</span>
  <span class="hljs-punctuation">}</span>
</code></pre>
    <p class="normal">The <code class="inlineCode">reg()</code> function created here uses several R commands that we have not used previously. First, since we will be using the function with sets of columns from a data frame, the <code class="inlineCode">as.matrix()</code> function converts the data frame into matrix form. </p>
    <p class="normal">Next, the <code class="inlineCode">cbind()</code> function binds an additional column onto the <code class="inlineCode">x</code> matrix; the command <code class="inlineCode">Intercept = 1</code> instructs R to name the new column <code class="inlineCode">Intercept</code> and to fill the column with repeating <code class="inlineCode">1</code> values. Then, a series of matrix operations are performed on the <code class="inlineCode">x</code> and <code class="inlineCode">y</code> objects:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">solve()</code> takes the inverse of a matrix</li>
      <li class="bulletList"><code class="inlineCode">t()</code> is used to transpose a matrix</li>
      <li class="bulletList"><code class="inlineCode">%*%</code> multiplies two matrices</li>
    </ul>
    <p class="normal">By combining these R functions as shown, our function will return a vector <code class="inlineCode">b</code>, which contains estimated parameters for the linear model relating <code class="inlineCode">x</code> to <code class="inlineCode">y</code>. The final two lines in the function give the <code class="inlineCode">b</code> vector a name and print the result on screen.</p>
    <p class="normal">Let’s apply this function to the shuttle launch data. As shown in the following code, the dataset includes three features and the distress count (<code class="inlineCode">distress_ct</code>), which is the outcome of interest:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> str<span class="hljs-punctuation">(</span>launch<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">'data.frame':    23 obs. of  4 variables:
 $ distress_ct         : int  0 1 0 0 0 0 0 0 1 1 ...
 $ temperature         : int  66 70 69 68 67 72 73 70 57 63 ...
 $ field_check_pressure: int  50 50 50 50 50 50 100 100 200 200 ...
 $ flight_num          : int  1 2 3 4 5 6 7 8 9 10 ...
</code></pre>
    <p class="normal">We can confirm<a id="_idIndexMarker679"/> that our function is working correctly by comparing its result for the simple linear regression model of O-ring failures versus temperature, which we found earlier to have parameters <em class="italic">a</em> = 3.70 and <em class="italic">b</em> = -0.048. Since temperature is in the second column of the launch data, we can run the <code class="inlineCode">reg()</code> function as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> reg<span class="hljs-punctuation">(</span>y <span class="hljs-operator">=</span> launch<span class="hljs-operator">$</span>distress_ct<span class="hljs-punctuation">,</span> x <span class="hljs-operator">=</span> launch<span class="hljs-punctuation">[</span><span class="hljs-number">2</span><span class="hljs-punctuation">])</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">               estimate
Intercept    3.69841270
temperature -0.04753968
</code></pre>
    <p class="normal">These values<a id="_idIndexMarker680"/> exactly match our prior result, so let’s use the function to build a multiple regression model. We’ll apply it just as before, but this time we will specify columns two through four for the <code class="inlineCode">x</code> parameter to add two additional predictors:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> reg<span class="hljs-punctuation">(</span>y <span class="hljs-operator">=</span> launch<span class="hljs-operator">$</span>distress_ct<span class="hljs-punctuation">,</span> x <span class="hljs-operator">=</span> launch<span class="hljs-punctuation">[</span><span class="hljs-number">2</span><span class="hljs-operator">:</span><span class="hljs-number">4</span><span class="hljs-punctuation">])</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">                         estimate
Intercept             3.527093383
temperature          -0.051385940
field_check_pressure  0.001757009
flight_num            0.014292843
</code></pre>
    <p class="normal">This model predicts the number of O-ring distress events using temperature, field check pressure, and the launch ID number. Notably, the inclusion of the two new predictors did not change our finding from the simple linear regression model. Just as before, the coefficient for the temperature variable is negative, which suggests that as temperature increases, the number of expected O-ring events decreases. The magnitude of the effect is also approximately the same: roughly 0.05 fewer distress events are expected for each degree increase in launch temperature.</p>
    <p class="normal">The two new predictors also contribute to the predicted distress events. The field check pressure refers to the amount of pressure applied to the O-ring during pre-launch testing. Although the check pressure was originally 50 psi, it was raised to 100 and 200 psi for some launches, which led some to believe that it may be responsible for O-ring erosion. The coefficient is positive, but small, providing at least a little evidence for this hypothesis. The flight number accounts for the shuttle’s age. With each flight, it gets older, and parts may be more brittle or prone to fail. The small positive association between flight number and distress count may reflect this fact.</p>
    <p class="normal">Overall, our retrospective analysis of the space shuttle data suggests that there was reason to believe that the Challenger launch was highly risky given the weather conditions. Perhaps if the engineers had applied linear regression beforehand, a disaster could have been averted. Of course, the reality of the situation, and the political implications involved, were surely not as simple then as they now appear in hindsight.</p>
    <h2 class="heading-2" id="_idParaDest-140">Generalized linear models and logistic regression</h2>
    <p class="normal">As demonstrated<a id="_idIndexMarker681"/> in the analysis of the Challenger space shuttle launch data, standard linear regression is a useful method for modeling the relationship between a numeric outcome and one or more predictors. It is no wonder that regression has stood the test of time. Even after over a hundred years, it remains one of the most important techniques in our toolkit, even though it is no more sophisticated than finding the best straight line to fit the data.</p>
    <p class="normal">However, not every problem is well suited to being modeled by a line, and moreover, the statistical assumptions made by regression models are violated in many real-world tasks. Even the Challenger data is less than ideal for linear regression as it violates the regression assumption that the target variable is measured on a continuous scale. As the number of O-ring failures can only take countable values, it makes no sense that the model might predict exactly 2.21 distress events rather than either two or three. </p>
    <p class="normal">For modeling counting values, for categorical or binary outcomes, as well as other cases where the target is not a normally distributed continuous variable, standard linear regression is not the best tool for the job—even though many still apply it to these types of problems, and it often performs surprisingly well.</p>
    <p class="normal">To address these <a id="_idIndexMarker682"/>shortcomings, linear regression can be adapted to other use cases using the aptly named GLM, which was first described in 1972 by statisticians John Nelder and Robert Wedderburn. The GLM loosens two assumptions of traditional regression modeling. First, it allows the target to be a non-normally distributed, non-continuous variable. Second, it allows the variance of the target variable to be related to its mean. The former property opens the door for modeling categorical data or counting data, or even cases where there is a limited range of values to predict, such as probability values falling on a range between 0 to 1. The latter property allows the model to better fit cases where the predictors relate to the predictions in a nonlinear way, such as exponential growth in which an increase of one unit of time leads to greater and greater increases in the outcome.</p>
    <div class="note">
      <p class="normal">To read the original publication about GLMs, see <em class="italic">Generalized linear models, Nelder, J. A. and Wedderburn, T. W. M., Journal of the Royal Statistical Society, 1972, Vol. 135, pp. 370-384</em>. For a gentler, non-mathematical introduction, see <em class="italic">An introduction to generalized linear models, Dunteman, G. H. and Ho, M. H. R., Quantitative Applications in the Social Sciences, 2006, Vol. 145</em>.</p>
    </div>
    <p class="normal">These two generalizations of linear regression are<a id="_idIndexMarker683"/> reflected in the two key components of any GLM:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">The <strong class="keyWord">family</strong> refers<a id="_idIndexMarker684"/> to the distribution of the target feature, which must be chosen from members of the <strong class="keyWord">exponential family</strong> of <a id="_idIndexMarker685"/>distributions, which includes the normal Gaussian distribution as well as others like Poisson, Binomial, and Gamma. The chosen distribution may be discrete or continuous, and it may span different ranges of values, such as only positive values or only values between zero and one.</li>
      <li class="numberedList">The <strong class="keyWord">link function</strong> transforms<a id="_idIndexMarker686"/> the relationship between the predictors and the target such that it can be modeled using a linear equation, despite the original relationship being nonlinear. There is <a id="_idIndexMarker687"/>always a <strong class="keyWord">canonical link function</strong>, which is determined by the chosen family and used by default, but in some cases, one may choose a different link to vary how the model is interpreted or to obtain a better model fit.</li>
    </ol>
    <p class="normal">Varying <a id="_idIndexMarker688"/>the family and link functions gives the GLM approach tremendous flexibility to adapt to many different real-world use cases and to conform to the natural distribution of the target variable. Knowing which combination to use depends on both how the model will be applied as well as the theoretical distribution of the target. Understanding these factors in detail requires knowledge of the various distributions in the exponential family and a background in statistical theory. Thankfully, most GLM use cases conform to a few common combinations of family and link, which are listed in the table that follows:</p>
    <table class="table-container" id="table002-4">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Family</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Canonical Link Function</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Target Range</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Notes and Applications</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Gaussian (normal)</p>
          </td>
          <td class="table-cell">
            <p class="normal">Identity</p>
          </td>
          <td class="table-cell">
            <p class="normal">-<img alt="" src="../Images/B17290_06_036.png"/> to <img alt="" src="../Images/B17290_06_036.png"/></p>
          </td>
          <td class="table-cell">
            <p class="normal">Used for linear response modeling; reduces the GLM to standard linear regression.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Poisson</p>
          </td>
          <td class="table-cell">
            <p class="normal">Log</p>
          </td>
          <td class="table-cell">
            <p class="normal">Integers 0 to <img alt="" src="../Images/B17290_06_036.png"/></p>
          </td>
          <td class="table-cell">
            <p class="normal">Known as Poisson regression; models the count of an event occurring (such as the total number of O-ring failures) by estimating the frequency at which the event happens.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Binomial</p>
          </td>
          <td class="table-cell">
            <p class="normal">Logit</p>
          </td>
          <td class="table-cell">
            <p class="normal">0 to 1</p>
          </td>
          <td class="table-cell">
            <p class="normal">Known as logistic regression; used for modeling a binary outcome (such as whether any O-ring failed) by estimating the probability that the outcome occurs.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Gamma</p>
          </td>
          <td class="table-cell">
            <p class="normal">Negative Inverse</p>
          </td>
          <td class="table-cell">
            <p class="normal">0 to <img alt="" src="../Images/B17290_06_036.png"/></p>
          </td>
          <td class="table-cell">
            <p class="normal">One of many possibilities for modeling right-skewed data; could be used for modeling the time to an event (such as seconds to an O-ring failure) or cost data (such as insurance claims costs for a car accident).</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Multinomial</p>
          </td>
          <td class="table-cell">
            <p class="normal">Logit</p>
          </td>
          <td class="table-cell">
            <p class="normal">1 of <em class="italic">K</em> categories</p>
          </td>
          <td class="table-cell">
            <p class="normal">Known as multinomial logistic regression; used for modeling a categorical outcome (such as a successful, unsuccessful, or aborted shuttle launch) by estimating the probability the example falls into each of the categories. Generally uses specialized packages rather than a GLM function to aid interpretation.</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">Due to the <a id="_idIndexMarker689"/>nuances of interpreting GLMs, it<a id="_idIndexMarker690"/> takes much practice and careful study to be adept at applying just one, and few people can claim to be experts at using all of them. Entire textbooks are devoted to each GLM variant. Fortunately, in the domain of machine learning, interpreting and understanding are less important than being able to apply the correct GLM form to practical problems and produce useful predictions. While this chapter cannot cover each of the listed methods, an introduction to the key details will allow you to later pursue the GLM variants most relevant to your own work.</p>
    <p class="normal">Beginning with the simplest variant listed in the table, standard linear regression can be thought of as a special type of GLM that uses the Gaussian family and the identity link function. The <strong class="keyWord">identity link</strong> implies<a id="_idIndexMarker691"/> that the relationship between the target <em class="italic">y</em> and a predictor <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">i</sub> is not transformed in any way. Thus, as with standard regression, an estimated regression parameter <img alt="" src="../Images/B17290_06_022.png"/> can be interpreted quite simply as the increase in <em class="italic">y</em> given a one-unit increase in <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">i</sub>, assuming all other factors are held equal.</p>
    <p class="normal">GLM forms that use other link functions are not as simple to interpret and fully understanding the impact of individual predictors requires much more careful analysis. This is because the regression parameters must be interpreted as the additive increase in <em class="italic">y</em> for a one-unit increase in <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">i</sub>, but only after being transformed through the link function. </p>
    <p class="normal">For <a id="_idIndexMarker692"/>instance, the Poisson family that uses<a id="_idIndexMarker693"/> the <strong class="keyWord">log link</strong> function<a id="_idIndexMarker694"/> to model the expected count of events relates <em class="italic">y</em> to the predictors <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">i</sub> through the natural logarithm; consequently, the additive effect of <img alt="" src="../Images/B17290_06_040.png"/> on <em class="italic">y</em> becomes multiplicative on the original scale of the response variable. This is because using the properties of logarithms, we know <img alt="" src="../Images/B17290_06_041.png"/>, and this becomes <img alt="" src="../Images/B17290_06_042.png"/> after exponentiating to remove the logarithm. </p>
    <p class="normal">Due to this multiplicative impact, the parameter estimates are understood as relative rates of increase rather than the absolute increase in <em class="italic">y</em> as with linear regression.</p>
    <p class="normal">To see this in practice, suppose we built a Poisson regression model of the count of O-ring failures versus launch temperature. If <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">1</sub> is temperature and the estimated <img alt="" src="../Images/B17290_06_043.png"/> = -0.103, then we can determine that there are about 9.8 percent fewer O-ring failures on average per each additional degree of temperature at launch. This is because <em class="italic">exp(-0.103) = 0.902</em>, or 90.2 percent of the failures per degree, which implies that we would expect 9.8 percent fewer failures per degree increase. Applying this to the Challenger shuttle launch temperature at 36 degrees Fahrenheit, we can extrapolate that a launch 17 degrees warmer (53 degrees Fahrenheit was the previous coldest launch) would have had about <em class="italic">(0.902)^17 = 17.2</em> percent of the expected number of failures, equivalent to a drop of 82.8 percent.</p>
    <p class="normal">The GLM variant that <a id="_idIndexMarker695"/>uses a binomial family distribution with a <a id="_idIndexMarker696"/>logit link function is known as <strong class="keyWord">logistic regression</strong>, and is perhaps the single most important form as it allows regression to be adapted to binary classification tasks. The <strong class="keyWord">logit link</strong> is <a id="_idIndexMarker697"/>a function in the form <em class="italic">log(p / (1 - p))</em>, where <em class="italic">p</em> is a probability; the inner portion (<em class="italic">p / (1 - p)</em>) expresses the probability as <strong class="keyWord">odds</strong>, exactly like the odds used in gambling and sports betting in phrases like “the team has a 2:1 chance of winning.” After taking the natural logarithm, the estimated regression coefficients are interpreted as log odds. Because we understand odds more intuitively than log odds, we usually exponentiate <a id="_idIndexMarker698"/>the estimated logistic regression coefficients to convert log odds into odds for interpretation. However, because logistic regression coefficients indicate the difference in the odds of <em class="italic">y</em> due to a one-unit increase in <em class="italic">x</em>, the exponentiated odds become <strong class="keyWord">odds ratios</strong>, which express the relative increase or decrease in the chances that <em class="italic">y</em> happens.</p>
    <p class="normal">In the <a id="_idIndexMarker699"/>context of the space shuttle data, suppose we built a<a id="_idIndexMarker700"/> logistic regression model for the binary classification task of predicting whether or not a launch would have one or more O-ring failures. A factor that does not change the probability of an O-ring failure would keep the odds balanced at 1:1 (50-50 probability), which translates to log odds of <em class="italic">log(0.5 / (1 - 0.5)) = 0</em> and the estimated regression coefficient <img alt="" src="../Images/B17290_06_006.png"/> = 0 for this feature. Finding the odds ratio as <em class="italic">exp(0) = 1</em> shows that the odds remain unchanged regardless of the value of this factor. Now, suppose a factor like temperature reduces the chance that the outcome occurs, and that in the logistic regression model with <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">1</sub> as temperature, then the estimated <img alt="" src="../Images/B17290_06_043.png"/> = -0.232. By exponentiating this, we find the odds ratio <em class="italic">exp(-0.232) = 0.793</em>, which means that the odds of a failure drop by about 20 percent for a one-degree increase in temperature, assuming all else is held equal. It is very important to note that this does not imply that the <em class="italic">probability</em> of a failure drops by 20 percent for each degree increase. </p>
    <p class="normal">Because the relationship between odds and probability is nonlinear, the impact of a temperature change on the failure probability depends on the context in which the temperature change is occurring!</p>
    <p class="normal">Odds and probabilities are related via the inverse connection between the logit and logistic functions. The logistic function has the convenient property that for any input <em class="italic">x</em> value, the output is a value in the range between 0 to 1—exactly the same range as a probability. Additionally, the logistic function creates an S-shaped curve when graphed, as illustrated in <em class="italic">Figure 6.6</em>, which shows a hypothetical logistic regression model of O-ring failure probability versus launch temperature. The probability of failure on the <em class="italic">y</em> axis is changed most strongly in the middle of the temperature range; at temperature extremes, the predicted failure probability changes very little for each additional degree of temperature added or removed.</p>
    <figure class="mediaobject"><img alt="Chart, line chart  Description automatically generated" src="../Images/B17290_06_06.png"/></figure>
    <p class="packt_figref">Figure 6.6: Hypothetical logistic regression curve representing space shuttle launch data</p>
    <p class="normal">The <a id="_idIndexMarker701"/>fitted logistic regression model creates a curve representing a <a id="_idIndexMarker702"/>probability estimation on a continuous scale anywhere in the range between 0 to 1, despite the target outcome (represented by circles in the figure) only taking the value <em class="italic">y</em> = 0 or <em class="italic">y</em> = 1. To obtain the binary prediction, simply define the probability threshold within which the target outcome is to be predicted. For example, if the predicted probability of an O-ring failure is greater than 0.50, then predict “failure,” and otherwise, predict “no failure.” Using a 50 percent threshold is common, but a higher or lower threshold can be used to adjust the cost sensitivity of the model.</p>
    <p class="normal">Examining the logistic curve in <em class="italic">Figure 6.6</em> leads to another question: how does the modeling algorithm determine the curve that best fits the data? After all, given that this is not a straight line, the OLS algorithm used in standard linear regression no longer seems to apply. </p>
    <p class="normal">Indeed, generalized linear models use a different technique <a id="_idIndexMarker703"/>called <strong class="keyWord">maximum likelihood estimation</strong> (<strong class="keyWord">MLE</strong>), which finds the parameter values for the specified distribution that are most likely to have generated the observed data.</p>
    <p class="normal">Because OLS estimation is a special case of maximum likelihood estimation, using OLS or MLE for a linear model makes no difference given that the assumptions of OLS modeling are met. For applications outside of linear modeling, the MLE technique will produce different results and must be used instead of OLS. The MLE technique is built into GLM modeling software, and usually applies analytical techniques to identify the optimal model parameters by iterating repeatedly over the data rather than finding the correct solution directly. Fortunately, as we will see shortly, building a GLM in R is hardly more challenging than training a simpler linear model.</p>
    <p class="normal">This introduction only scratched the surface of what is possible with linear regression and GLM. Although theory and simple examples like the Challenger dataset are helpful for understanding how regression models work, there is more involved in <a id="_idIndexMarker704"/>building a useful model than what we’ve seen so far. R’s built-in regression functions include the additional functionality needed to fit more sophisticated <a id="_idIndexMarker705"/>models while providing additional diagnostic output to aid model interpretation and assess the fit. Let’s apply these functions and expand our knowledge of regression by attempting a real-world learning task.</p>
    <h1 class="heading-1" id="_idParaDest-141">Example – predicting auto insurance claims costs using linear regression</h1>
    <p class="normal">For an automobile<a id="_idIndexMarker706"/> insurance company to make money, it needs to collect more in membership premiums than it spends on claims paid to its beneficiaries in case of vehicle theft, damages, or loss of life in accidents. Consequently, insurers invest time and money to develop models that accurately forecast claims costs for the insured population. This is the field<a id="_idIndexMarker707"/> known as <strong class="keyWord">actuarial science</strong>, which uses sophisticated statistical techniques to estimate risk across insured populations.</p>
    <p class="normal">Insurance expenses are difficult to predict accurately for individuals because accidents, and especially fatal accidents, are thankfully relatively rare—a bit over one fatality per 100 million vehicle miles traveled in the United States—yet, when they do happen, they are extremely costly. Moreover, the specific conditions leading to any given accident are based on factors that are so hard to measure that they appear to be random. An excellent driver with a clean driving record could have bad luck and be hit by a drunk driver, while another person can drive distracted by their cellular phone and, due to good fortune, never cause an accident.</p>
    <p class="normal">Because of the <a id="_idIndexMarker708"/>near impossibility of predicting expenses for a single person, insurance companies apply the law of averages and compute the average cost to insure segments of people with similar risk profiles. If the expense estimates for each risk segment are correct, the insurance company can price the insurance premiums lower for segments with less risk, and potentially attract new, low-risk customers from competing insurance companies. We will simulate this scenario in the analysis that follows.</p>
    <h2 class="heading-2" id="_idParaDest-142">Step 1 – collecting data</h2>
    <p class="normal">The dataset <a id="_idIndexMarker709"/>for this example is a simulation created for this book based on demographics and traffic statistics from the United States government. It is intended to approximate the real-world conditions of automobile insurance companies in the U.S. state of Michigan, which is home to about ten million residents and seven million licensed drivers.</p>
    <div class="packt_tip">
      <p class="normal">If you would like to follow along interactively, download the <code class="inlineCode">autoinsurance.csv</code> file from the Packt Publishing GitHub repository for this book and save it to your R working folder.</p>
    </div>
    <p class="normal">The insurance dataset includes 20,000 examples of beneficiaries enrolled in the hypothetical automobile vehicle insurance plan. This is much smaller than the typical datasets used by practicing actuaries, especially for very rare outcomes, but the size has been reduced to allow analysis even on computers with limited memory. Each example represents an insured individual’s characteristics and total insurance claims costs (<code class="inlineCode">expenses</code>) charged to the plan for the calendar year. The features available at the time of enrollment are:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">age</code>: The age of the driver, from 16 to 89</li>
      <li class="bulletList"><code class="inlineCode">geo_area</code>: The geographic area of the vehicle owner’s primary residence, and where the vehicle will be used most often; zip codes were bucketed into urban, suburban, and rural categories</li>
      <li class="bulletList"><code class="inlineCode">est_value</code>: The estimated market value of the vehicle(s), based on age and depreciation; capped at $125,000—the maximum allowed insured value</li>
      <li class="bulletList"><code class="inlineCode">vehicle_type</code>: The type of passenger vehicle, either a car, truck, minivan, or sport utility vehicle (SUV)</li>
      <li class="bulletList"><code class="inlineCode">miles_driven</code>: The distance driven (in miles) in the calendar year</li>
      <li class="bulletList"><code class="inlineCode">college_grad_ind</code>: A binary indicator set to <code class="inlineCode">1</code> if the beneficiary has a college education or higher</li>
      <li class="bulletList"><code class="inlineCode">speeding_ticket_ind</code>: A binary indicator set to <code class="inlineCode">1</code> if a speeding ticket or infraction was received during the past five years</li>
      <li class="bulletList"><code class="inlineCode">clean_driving_ind</code>: A binary indicator set to <code class="inlineCode">1</code> if no at-fault insurance claims were paid during the past five years</li>
    </ul>
    <p class="normal">In this <a id="_idIndexMarker710"/>example scenario, each of the 20,000 beneficiaries enrolled in a “safe driving discount” program, which required the use of a device or mobile phone application that uses location tracking to monitor safe driving conditions throughout the year. This helped validate the accuracy of <code class="inlineCode">miles_driven</code> and created the following two additional predictors, which are intended to reflect more risky driving behaviors:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">hard_braking_ind</code>: A binary indicator set to <code class="inlineCode">1</code> if the vehicle frequently applies “hard brakes” (as in the case of stopping suddenly)</li>
      <li class="bulletList"><code class="inlineCode">late_driving_ind</code>: A binary indicator set to <code class="inlineCode">1</code> if the vehicle is driven regularly after midnight</li>
    </ul>
    <p class="normal">It is important to give some thought to how these variables may relate to billed insurance expenses—some in more obvious ways than others. For instance, we would clearly expect cars driven more often to be at a higher risk of an accident than those that stay home in the garage. On the other hand, it isn’t as clear whether urban, rural, or suburban drivers would be riskier; rural drivers may drive farther, but urban driving involves more traffic and may be more at risk of vehicle theft. A regression model will help us disentangle these relationships, but requires us to specify the connections between the features ourselves rather than detecting them automatically, which is unlike many other machine learning methods. We’ll explore some of the potential relationships in the next section.</p>
    <div class="packt_tip">
      <p class="normal">It may also be interesting to consider which potentially useful predictors are not included in the training dataset. Gender is often used for automobile insurance pricing (and it varies whether males or females are more costly!) but the state of Michigan banned the use of gender and credit scoring for this purpose in 2020. Such features may be highly predictive, but may lead to systematic biases against protected groups, as discussed in <em class="chapterRef">Chapter 1</em>, <em class="italic">Introducing Machine Learning</em>.</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-143">Step 2 – exploring and preparing the data</h2>
    <p class="normal">As we <a id="_idIndexMarker711"/>have <a id="_idIndexMarker712"/>done before, we will use the <code class="inlineCode">read.csv()</code> function to load the data for analysis. We can safely use <code class="inlineCode">stringsAsFactors = TRUE</code> because it is appropriate to convert the three nominal variables into factors:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> insurance <span class="hljs-operator">&lt;-</span> read.csv<span class="hljs-punctuation">(</span><span class="hljs-string">"insurance.csv"</span><span class="hljs-punctuation">,</span> stringsAsFactors <span class="hljs-operator">=</span> <span class="hljs-literal">TRUE</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">The <code class="inlineCode">str()</code> function<a id="_idIndexMarker713"/> confirms that the data is formatted as we expected:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> str<span class="hljs-punctuation">(</span>insurance<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">'data.frame':    20000 obs. of  11 variables:
 $ age                : int  19 30 39 64 33 27 62 39 67 38 ...
 $ geo_area           : Factor w/ 3 levels "rural","suburban", ...
 $ vehicle_type       : Factor w/ 4 levels "car","minivan", ...
 $ est_value          : int  28811 52603 113870 35228 ...
 $ miles_driven       : int  11700 12811 9784 17400 ...
 $ college_grad_ind   : int  0 1 1 0 0 1 1 0 1 1 ...
 $ speeding_ticket_ind: int  1 0 0 0 0 0 0 0 0 0 ...
 $ hard_braking_ind   : int  1 0 0 0 0 0 0 0 0 0 ...
 $ late_driving_ind   : int  0 0 0 0 0 0 0 0 0 0 ...
 $ clean_driving_ind  : int  0 1 0 1 1 0 1 1 0 1 ...
 $ expenses           : num  0 6311 49684 0 0 ...
</code></pre>
    <p class="normal">Our model’s<a id="_idIndexMarker714"/> dependent variable is <code class="inlineCode">expenses</code>, which measures the loss or damages each person claimed under the insurance plan for the year. Prior to building a linear regression model, it is often helpful to check for normality. Although linear regression will not fail without a normally distributed dependent variable, the model often fits better when this is true. Let’s look at the summary statistics:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> summary<span class="hljs-punctuation">(</span>insurance<span class="hljs-operator">$</span>expenses<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
      0       0       0    1709       0  232797
</code></pre>
    <p class="normal">The minimum, first quartile, median, and third quartile are all zero, which implies that at least 75% of the beneficiaries had no expenses in the calendar year. The fact that the mean value is greater than the median gives us the sense that the distribution of insurance expenses is right-skewed, but the skew is likely to be quite extreme as the average expense was $1,709 while the maximum was $232,797. We can confirm this visually using a histogram:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> hist<span class="hljs-punctuation">(</span>insurance<span class="hljs-operator">$</span>expenses<span class="hljs-punctuation">)</span>
</code></pre>
    <figure class="mediaobject"><img alt="Text  Description automatically generated" src="../Images/B17290_06_07.png"/></figure>
    <p class="packt_figref">Figure 6.7: The distribution of annual insurance claims costs</p>
    <p class="normal">As expected, the<a id="_idIndexMarker715"/> figure shows a<a id="_idIndexMarker716"/> right-skewed distribution with a huge spike at zero, reflecting the fact that only a small portion (about 8.6%) made an insurance claim. Among those that did claim a vehicle loss or damages, the tail end of the distribution extends far to the right, beyond $200,000 worth of expenses for the costliest injuries. Although this distribution is not ideal for linear regression, knowing this weakness ahead of time may help us design a better-fitting model later. For now, using only the distribution of <code class="inlineCode">expenses</code>, we can say that the average beneficiary should be charged an annual premium of $1,709 for the insurer to break even, or about $150 a month per subscriber for a slight profit. Of course, this assumes that the risk and costs are shared evenly. An improved insurance model will push greater costs onto riskier drivers and provide safe drivers with financial savings.</p>
    <p class="normal">Before we add additional predictors, it is important to note that regression models require that every feature is numeric, yet we have two factor-type features in our data frame. For instance, the <code class="inlineCode">geo_area</code> variable is divided into <code class="inlineCode">urban</code>, <code class="inlineCode">suburban</code>, and <code class="inlineCode">rural</code> levels, while <code class="inlineCode">vehicle_type</code> has categories for <code class="inlineCode">car</code>, <code class="inlineCode">truck</code>, <code class="inlineCode">suv</code>, and <code class="inlineCode">minivan</code>. </p>
    <p class="normal">Let’s take a closer look to see how they are distributed:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> table<span class="hljs-punctuation">(</span>insurance<span class="hljs-operator">$</span>geo_area<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">   rural suburban    urban 
    3622     8727     7651 
</code></pre>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> table<span class="hljs-punctuation">(</span>insurance<span class="hljs-operator">$</span>vehicle_type<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">    car minivan     suv   truck 
   5801     726    9838    3635
</code></pre>
    <p class="normal">Here, we <a id="_idIndexMarker717"/>see that the data has <a id="_idIndexMarker718"/>been divided nearly evenly between urban and suburban areas, but rural is a much smaller portion of the data. Additionally, SUVs are the most popular vehicle type, followed by cars and trucks, with minivans in a distant fourth place. We will see how R’s linear regression function handles these factor variables shortly.</p>
    <h3 class="heading-3" id="_idParaDest-144">Exploring relationships between features – the correlation matrix</h3>
    <p class="normal">Before<a id="_idIndexMarker719"/> fitting a regression model to data, it can be useful to determine how the independent variables are related to the dependent variable and each other. A <strong class="keyWord">correlation matrix</strong> provides a quick overview of these relationships. Given a set of variables, it provides a correlation for each pairwise relationship.</p>
    <p class="normal">To create a correlation matrix for the four numeric, non-binary variables in the insurance data frame, use the <code class="inlineCode">cor()</code> command:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> cor<span class="hljs-punctuation">(</span>insurance<span class="hljs-punctuation">[</span><span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-string">"age"</span><span class="hljs-punctuation">,</span> <span class="hljs-string">"est_value"</span><span class="hljs-punctuation">,</span> <span class="hljs-string">"miles_driven"</span><span class="hljs-punctuation">,</span> <span class="hljs-string">"expenses"</span><span class="hljs-punctuation">)])</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">                      age   est_value miles_driven     expenses
age           1.000000000 -0.05990552   0.04812638 -0.009121269
est_value    -0.059905524  1.00000000  -0.01804807  0.088100468
miles_driven  0.048126376 -0.01804807   1.00000000  0.062146507
expenses     -0.009121269  0.08810047   0.06214651  1.000000000
</code></pre>
    <p class="normal">At the intersection of each row and column pair, the correlation is listed for the variables indicated by that row and column. The diagonal is always <code class="inlineCode">1.0000000</code> since there is always a perfect correlation between a variable and itself. The values above and below the diagonal are identical since correlations are symmetrical. In other words, <code class="inlineCode">cor(x, y)</code> is equal to <code class="inlineCode">cor(y, x)</code>.</p>
    <p class="normal">None of the correlations in the matrix are very strong, but the associations do match with common sense. For instance, <code class="inlineCode">age</code> and <code class="inlineCode">expenses</code> appear to have a weak negative correlation, meaning that as someone ages, their expected insurance cost goes down slightly—probably reflecting the greater driving experience. There are also positive correlations between <code class="inlineCode">est_value</code> and <code class="inlineCode">expenses</code> and <code class="inlineCode">miles_driven</code> and <code class="inlineCode">expenses</code>, which indicate that more valuable cars and more extensive driving lead to greater expenses. We’ll try to tease out these types of relationships more clearly when we build our final regression model.</p>
    <h3 class="heading-3" id="_idParaDest-145">Visualizing relationships between features – the scatterplot matrix</h3>
    <p class="normal">It can also be <a id="_idIndexMarker720"/>helpful to visualize the relationships between numeric features with scatterplots. Although we could create a scatterplot for each possible relationship, doing so for a large set of features quickly becomes tedious.</p>
    <p class="normal">An alternative is to create a <strong class="keyWord">scatterplot matrix</strong> (sometimes abbreviated as <strong class="keyWord">SPLOM</strong>), which is simply a collection of scatterplots arranged in a grid. It is used to detect patterns among three or more variables. The scatterplot matrix is not a true multi-dimensional visualization because only two features are examined at a time. Still, it provides a general sense of how the data may be interrelated.</p>
    <p class="normal">We can use R’s graphical capabilities to create a scatterplot matrix for the four non-binary numeric features: <code class="inlineCode">age</code>, <code class="inlineCode">est_value</code>, <code class="inlineCode">miles_driven</code>, and <code class="inlineCode">expenses</code>. The <code class="inlineCode">pairs()</code> function is provided in the default R installation and provides basic functionality for producing scatterplot matrices. To invoke the function, simply provide it with a subset of the data frame to plot. Given the relatively large size of our <code class="inlineCode">insurance</code> dataset, we’ll set the plot character parameter <code class="inlineCode">pch = "."</code> to a dot to make the visualization easier to read, then limit the columns to the four variables of interest:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> pairs<span class="hljs-punctuation">(</span>insurance<span class="hljs-punctuation">[</span><span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-string">"age"</span><span class="hljs-punctuation">,</span> <span class="hljs-string">"est_value"</span><span class="hljs-punctuation">,</span> <span class="hljs-string">"miles_driven"</span><span class="hljs-punctuation">,</span>
                  <span class="hljs-string">"expenses"</span><span class="hljs-punctuation">)],</span> pch <span class="hljs-operator">=</span> <span class="hljs-string">"."</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">This produces the following scatterplot matrix:</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_06_08.png"/></figure>
    <p class="packt_figref">Figure 6.8: A scatterplot matrix of the numeric features in the insurance dataset</p>
    <p class="normal">In the <a id="_idIndexMarker721"/>scatterplot matrix, the intersection of each row and column holds the scatterplot of the variables indicated by the row and column pair. The diagrams above and below the diagonal are transpositions because the <em class="italic">x</em> axis and <em class="italic">y</em> axis have been swapped. Do you notice any patterns in these plots? Although they mostly look like random clouds of points, a couple seem to display some subtle trends. The relationships of both <code class="inlineCode">est_value</code> and <code class="inlineCode">miles_driven</code> with <code class="inlineCode">expenses</code> seem to display a slight upward trend, which confirms visually what we already learned from the correlation matrix.</p>
    <p class="normal">By adding more information to the plot, it can be made even more useful. An enhanced scatterplot matrix can be created with the <code class="inlineCode">pairs.panels()</code> function in the <code class="inlineCode">psych</code> package. If you do not have this package installed, type <code class="inlineCode">install.packages("psych")</code> to install it on your system and load it using the <code class="inlineCode">library(psych)</code> command. Then, we can create a scatterplot matrix using the <code class="inlineCode">pch</code> parameter to set the plotting character as we did previously:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>psych<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> pairs.panels<span class="hljs-punctuation">(</span>insurance<span class="hljs-punctuation">[</span><span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-string">"age"</span><span class="hljs-punctuation">,</span> <span class="hljs-string">"est_value"</span><span class="hljs-punctuation">,</span> <span class="hljs-string">"miles_driven"</span><span class="hljs-punctuation">,</span>
                           <span class="hljs-string">"expenses"</span><span class="hljs-punctuation">)],</span> pch <span class="hljs-operator">=</span> <span class="hljs-string">"."</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">This produces a more informative scatterplot matrix, as follows:</p>
    <figure class="mediaobject"><img alt="A picture containing chart  Description automatically generated" src="../Images/B17290_06_09.png"/></figure>
    <p class="packt_figref">Figure 6.9: The pairs.panels() function adds detail to the scatterplot matrix</p>
    <p class="normal">In the <code class="inlineCode">pairs.panels()</code> output, the scatterplots above the diagonal are replaced with a correlation matrix. The <a id="_idIndexMarker722"/>diagonal now contains histograms depicting the distribution of values for each feature. Finally, the scatterplots below the diagonal are presented with additional visual information.</p>
    <p class="normal">The oval-shaped object on each scatterplot (which may be difficult to see in print due to the mass of black points but can be seen more easily on a computer screen) is a <strong class="keyWord">correlation ellipse</strong>. It provides a simple visual indicator of correlation strength. In this dataset, there are no strong correlations, so the ovals are mostly flat; with stronger correlations, the ovals would be tilted upward or downward to indicate a positive or negative correlation. The dot at the center of the ellipse is a point reflecting the means of the <em class="italic">x</em>- and <em class="italic">y</em>-axis variables.</p>
    <p class="normal">The line <a id="_idIndexMarker723"/>superimposed across the scatterplot (which appears in red on a computer screen) is called a <strong class="keyWord">loess curve</strong>. It indicates the general relationship between the <em class="italic">x</em>-axis and <em class="italic">y</em>-axis variables. It is best understood by example. Although the small size of the plot makes this trend difficult to see, the curve for <code class="inlineCode">age</code> and <code class="inlineCode">miles_driven</code> slopes upward very slightly until reaching middle age, then levels off. This means that driving tends to increase with age up to the point at which it remains roughly constant over time. </p>
    <p class="normal">Although not observed here, the loess curve can sometimes be quite dramatic with V- or U-shaped curves as well as stairstep patterns. Recognizing such patterns can assist later with developing a better-fitting regression model.</p>
    <h2 class="heading-2" id="_idParaDest-146">Step 3 – training a model on the data</h2>
    <p class="normal">To fit a linear <a id="_idIndexMarker724"/>regression model to data with R, the <code class="inlineCode">lm()</code> function can be used. This is part of the <code class="inlineCode">stats</code> package, which should be included and loaded by default with your R installation. The <code class="inlineCode">lm()</code> syntax is as follows:</p>
    <figure class="mediaobject"><img alt="Text  Description automatically generated" src="../Images/B17290_06_10.png"/></figure>
    <p class="packt_figref">Figure 6.10: Multiple regression syntax</p>
    <p class="normal">The<a id="_idIndexMarker725"/> following command fits a linear regression model, which relates the ten independent variables to the total insurance expenses. The R formula syntax uses the tilde character (<code class="inlineCode">~</code>) to describe the model; the dependent variable <code class="inlineCode">expenses</code> is written to the left of the tilde while the independent variables go to the right, separated by <code class="inlineCode">+</code> signs. </p>
    <p class="normal">There is no need to specify the regression model’s intercept term, as it is included by default:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> ins_model <span class="hljs-operator">&lt;-</span> lm<span class="hljs-punctuation">(</span>expenses <span class="hljs-operator">~</span> age <span class="hljs-operator">+</span> geo_area <span class="hljs-operator">+</span> vehicle_type <span class="hljs-operator">+</span>
                    est_value <span class="hljs-operator">+</span> miles_driven <span class="hljs-operator">+</span>
                    college_grad_ind <span class="hljs-operator">+</span> speeding_ticket_ind <span class="hljs-operator">+</span>
                    hard_braking_ind <span class="hljs-operator">+</span> late_driving_ind <span class="hljs-operator">+</span>
                    clean_driving_ind<span class="hljs-punctuation">,</span>
                  data <span class="hljs-operator">=</span> insurance<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">Because the period character (<code class="inlineCode">.</code>) can be used to specify all features (excluding those already specified in the formula), the following command is equivalent to the prior command:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> ins_model <span class="hljs-operator">&lt;-</span> lm<span class="hljs-punctuation">(</span>expenses <span class="hljs-operator">~</span> .<span class="hljs-punctuation">,</span> data <span class="hljs-operator">=</span> insurance<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">After <a id="_idIndexMarker726"/>building the model, simply type the name of the model object to see the estimated beta coefficients. Note that the <code class="inlineCode">options(scipen = 999)</code> command turns off scientific notation to make the output easier to read:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> options<span class="hljs-punctuation">(</span>scipen <span class="hljs-operator">=</span> <span class="hljs-number">999</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> ins_model
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Call:
lm(formula = expenses ~ ., data = insurance)
Coefficients:
        (Intercept)                  age     geo_areasuburban  
        -1154.91486             -1.88603            191.07895  
      geo_areaurban  vehicle_typeminivan      vehicle_typesuv  
          169.11426            115.27862            -19.69500  
  vehicle_typetruck            est_value         miles_driven  
           21.56836              0.03115              0.11899  
   college_grad_ind  speeding_ticket_ind     hard_braking_ind  
          -25.04030            155.82410             11.84522  
   late_driving_ind    clean_driving_ind  
          362.48550           -239.04740  
</code></pre>
    <p class="normal">Understanding the regression coefficients for a linear regression model is fairly straightforward. The intercept is the predicted value of <code class="inlineCode">expenses</code> when the independent variables are equal to zero. However, in many cases, the intercept is of little explanatory value by itself, as it is often impossible to have values of zero for all features. </p>
    <p class="normal">This is the case here, where no insured person can exist with zero age or no miles driven, and consequently, the intercept has no real-world interpretation. For this reason, in practice, the intercept is often ignored.</p>
    <p class="normal">The beta coefficients indicate the estimated increase in insurance claims costs for an increase of one unit in each feature, assuming all other values are held constant. For instance, for each additional year of age, we would expect $1.89 lower insurance claims costs on average, assuming everything else is held equal. Similarly, we would expect $0.12 higher claims for each additional mile driven and $0.03 higher per dollar of insured value, all else equal.</p>
    <p class="normal">You might <a id="_idIndexMarker727"/>notice that although we only specified 10 features in our model formula, there are 13 coefficients reported in addition to the intercept. This happened because the <code class="inlineCode">lm()</code> function automatically applies dummy coding to each of the factor-type variables included in the model.</p>
    <p class="normal">As explained in <em class="chapterRef">Chapter 3</em>, <em class="italic">Lazy Learning – Classification Using Nearest Neighbors</em>, dummy coding allows a nominal feature to be treated as numeric by creating a binary variable for each category of the feature except one, which serves as the reference category. Each dummy variable is set to <code class="inlineCode">1</code> if the observation falls into the specified category or <code class="inlineCode">0</code> otherwise. For example, the <code class="inlineCode">geo_area</code> feature has three categories: <code class="inlineCode">urban</code>, <code class="inlineCode">suburban</code>, and <code class="inlineCode">rural</code>. Thus, two dummy variables were used, which are named <code class="inlineCode">geo_areaurban</code> and <code class="inlineCode">geo_areasuburban</code>. For the observations where the <code class="inlineCode">geo_area = "rural"</code>, <code class="inlineCode">geo_areaurban</code> and <code class="inlineCode">geo_areasuburban</code> will both be set to zero. Similarly, for the four-category <code class="inlineCode">vehicle_type</code> feature, R created three dummy variables named <code class="inlineCode">vehicle_typeminivan</code>, <code class="inlineCode">vehicle_typesuv</code>, and <code class="inlineCode">vehicle_typetruck</code>. This left <code class="inlineCode">vehicle_type = "car" </code>to serve as the reference category when the three dummy variables are all zero.</p>
    <p class="normal">When dummy coded features are used in a regression model, the regression coefficients are interpreted relative to the categories that were omitted. In our model, R automatically held out the <code class="inlineCode">geo_arearural</code> and <code class="inlineCode">vehicle_typecar</code> variables, making rural car owners the reference group. Thus, urban dwellers have $169.11 more claims costs each year relative to rural areas and trucks cost the insurer an average of $21.57 more than cars per year. To be clear, these differences assume all other features are held equal, so they are independent of the fact that rural drivers may drive more miles or less expensive vehicles. We would expect two people who are otherwise identical, except that one lives in a rural area and one lives in an urban area, to differ by about $170, on average.</p>
    <div class="packt_tip">
      <p class="normal">By default, R uses the first level of the factor variable as the reference. If you would prefer to use another level, the <code class="inlineCode">relevel()</code> function can be used to specify the reference group manually. Use the <code class="inlineCode">?relevel</code> command in R for more information.</p>
    </div>
    <p class="normal">In general, the results of the linear regression model make logical sense; however, we currently have no sense of how well the model is fitting the data. We’ll answer this question in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-147">Step 4 – evaluating model performance</h2>
    <p class="normal">The <a id="_idIndexMarker728"/>parameter estimates we obtained by typing <code class="inlineCode">ins_model</code> tell us about how the independent variables are related to the dependent variable, but they tell us nothing about how well the model fits our data. To evaluate the model performance, we can use the <code class="inlineCode">summary()</code> command on the stored model:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> summary<span class="hljs-punctuation">(</span>ins_model<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">This produces the following output, which has been annotated for illustrative purposes:</p>
    <figure class="mediaobject"><img alt="A picture containing text  Description automatically generated" src="../Images/B17290_06_11.png"/></figure>
    <p class="packt_figref">Figure 6.11: The summary output from a regression model can be divided into three main components, which are annotated in this figure</p>
    <p class="normal">The <code class="inlineCode">summary()</code> output <a id="_idIndexMarker729"/>may seem overwhelming at first, but the basics are easy to pick up. As indicated by the numbered labels in the preceding output, there are three main ways to evaluate the performance, or fit, of our model:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">The <strong class="screenText">Residuals</strong> section provides summary statistics for the prediction errors, some of which are apparently quite substantial. Since a residual is equal to the true value minus the predicted value, the maximum error of <code class="inlineCode">231252</code> suggests that the model under-predicted expenses by more than $230,000 for at least one observation. On the other hand, the majority of errors are relatively small negative values, which means that we are over-estimating expenses for most enrollees. This is exactly how the insurance company can afford to cover the expenses for costly accidents.</li>
      <li class="numberedList">For each estimated regression coefficient, the <strong class="keyWord">p-value</strong>, denoted by <code class="inlineCode">Pr(&gt;|t|)</code>, provides an estimate of the probability that the true coefficient is zero given the value of the estimate. Small p-values suggest that the true coefficient is very unlikely to be zero, which means that the feature is extremely unlikely to have no relationship with the dependent variable. Note that some of the p-values have stars (<code class="inlineCode">***</code>), which correspond to the footnotes that specify the <strong class="keyWord">significance level</strong> met by the estimate. This level is a threshold, chosen prior to building the model, which will be used to indicate “real” findings, as opposed to those due to chance alone; p-values less than the significance level are considered <strong class="keyWord">statistically significant</strong>. If the model had few such terms, it may be a cause for concern, since this would indicate that the features used are not very predictive of the outcome. Here, our model has a few highly significant variables, and they seem to be related to the outcome in expected ways.</li>
      <li class="numberedList">The <strong class="keyWord">Multiple R-squared</strong> value (also called the coefficient of determination) provides a measure of how well our model as a whole explains the values of the dependent variable. It is similar to the correlation coefficient in that the closer the value is to 1.0, the better the model perfectly explains the data. Since the R-squared value is 0.01241, we know that the model explains about 1.2 percent of the variation in the dependent variable. Because models with more features always explain more variation, the <strong class="keyWord">Adjusted R-squared</strong> value corrects R-squared by penalizing models with a large number of independent variables. This is useful for comparing the performance of models with different numbers of explanatory variables.</li>
    </ol>
    <p class="normal">Given the <a id="_idIndexMarker730"/>preceding three performance indicators, our model is performing well enough. The size of some of the errors is a bit concerning, but not surprising given the nature of insurance expense data. </p>
    <p class="normal">Additionally, it is not uncommon for regression models of real-world data to have low R-squared values. Although a value of 0.01241 is especially low, it reflects the fact that we have no proximate predictors of automobile accidents; to truly predict accidents, we would need real-time driving data, or at least some measure of true driving skill. This being said, as we will see in the next section, we still may be able to improve the model’s performance by specifying the model in a slightly different way.</p>
    <h2 class="heading-2" id="_idParaDest-148">Step 5 – improving model performance</h2>
    <p class="normal">As <a id="_idIndexMarker731"/>mentioned previously, a key difference between regression modeling and other machine learning approaches is that regression typically leaves feature selection and model specification to the user. Consequently, if we have subject-matter knowledge about how a feature is related to the outcome, we can use this information to inform the model specification and potentially improve the model’s performance.</p>
    <h3 class="heading-3" id="_idParaDest-149">Model specification – adding nonlinear relationships</h3>
    <p class="normal">In<a id="_idIndexMarker732"/> linear regression, the relationship between an independent variable and the dependent variable is assumed to be linear, yet this may not necessarily be true. For example, the effect of age on insurance expenditures may not be constant across all age values; the treatment may become disproportionately expensive for the youngest and oldest populations—a U-shaped curve, if expenses were plotted against age.</p>
    <p class="normal">If you recall, a typical regression equation follows a form similar to this:</p>
    <p class="center"><img alt="" src="../Images/B17290_06_046.png"/></p>
    <p class="normal">To account for a nonlinear relationship, we can add a higher-order term to the regression equation, treating the model as a polynomial. In effect, we will be modeling a relationship like this:</p>
    <p class="center"><img alt="" src="../Images/B17290_06_047.png"/></p>
    <p class="normal">The<a id="_idIndexMarker733"/> difference between these two models is that an additional regression parameter will be estimated, which is intended to capture the effect of the <em class="italic">x</em><sup class="superscript">2</sup> term. This allows the impact of age to be measured as a function of age and age squared.</p>
    <p class="normal">To add the nonlinear age to the model, we simply need to create a new variable:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> insurance<span class="hljs-operator">$</span>age2 <span class="hljs-operator">&lt;-</span> insurance<span class="hljs-operator">$</span>age<span class="hljs-operator">^</span><span class="hljs-number">2</span>
</code></pre>
    <p class="normal">Then, when we produce our improved model, we’ll add both <code class="inlineCode">age</code> and <code class="inlineCode">age2</code> to the <code class="inlineCode">lm()</code> formula using the form <code class="inlineCode">expenses ~ age + age2</code>. This will allow the model to separate the linear and nonlinear impact of age on medical expenses.</p>
    <h3 class="heading-3" id="_idParaDest-150">Model specification – adding interaction effects</h3>
    <p class="normal">So far, we <a id="_idIndexMarker734"/>have only considered each feature’s individual contribution to the outcome. What if certain features have a combined impact on the dependent variable? For instance, a habit of hard braking and late driving may have harmful effects separately, but it is reasonable to assume that their combined effect may be worse than the sum of each one alone.</p>
    <p class="normal">When two features have a combined <a id="_idIndexMarker735"/>effect, this is known as an <strong class="keyWord">interaction</strong>. If we suspect that two variables interact, we can test this hypothesis by adding their interaction to the model. Interaction effects are specified using the R formula syntax. To interact the hard braking indicator (<code class="inlineCode">hard_braking_ind</code>) with the late driving indicator (<code class="inlineCode">late_driving_ind</code>), we would write a formula in the form <code class="inlineCode">expenses ~ hard_braking_ind*late_driving_ind</code>.</p>
    <p class="normal">The <code class="inlineCode">*</code> operator is a shorthand that instructs R to model <code class="inlineCode">expenses ~ hard_braking_ind + </code><code class="inlineCode">late_driving_ind</code><code class="inlineCode"> + hard_braking_ind:late_driving_ind</code>. The colon operator (<code class="inlineCode">:</code>) in the expanded form indicates that <code class="inlineCode">hard_braking_ind:late_driving_ind</code> is the interaction between the two variables. Note that the expanded form automatically also included the individual <code class="inlineCode">hard_braking_ind</code> and <code class="inlineCode">late_driving_ind</code> variables as well as their interaction.</p>
    <div class="packt_tip">
      <p class="normal">If you have trouble deciding whether to include a variable, a common practice is to include it and examine the p-value. If the variable is not statistically significant, you have a plausible justification for excluding it from the model.</p>
    </div>
    <h3 class="heading-3" id="_idParaDest-151">Putting it all together – an improved regression model</h3>
    <p class="normal">Based on a bit <a id="_idIndexMarker736"/>of subject-matter knowledge of how insurance costs may be related to enrollee characteristics, we developed what we think is a more accurately specified regression formula. To summarize the improvements, we:</p>
    <ul>
      <li class="bulletList">Added a nonlinear term for age</li>
      <li class="bulletList">Specified an interaction between hard braking and late driving</li>
    </ul>
    <p class="normal">We’ll train the model using the <code class="inlineCode">lm()</code> function as before, but this time we’ll add the interaction term in addition to <code class="inlineCode">age2</code>, which will be included automatically:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> ins_model2 <span class="hljs-operator">&lt;-</span> lm<span class="hljs-punctuation">(</span>expenses <span class="hljs-operator">~</span> . <span class="hljs-operator">+</span> hard_braking_ind<span class="hljs-operator">:</span>late_driving_ind<span class="hljs-punctuation">,</span>
                 data <span class="hljs-operator">=</span> insurance<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">Next, we summarize the results:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> summary<span class="hljs-punctuation">(</span>ins_model2<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">The output is as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">Call:
lm(formula = expenses ~ . hard_barking_ind:late_driving_ind, 
    data = insurance)
Residuals: 
    Min       1Q   Median       3Q      Max  
  -6618    -1996    -1491    -1044   231358  
Coefficients:
                      Estimate   Std. Error  t value  Pr(&gt;|z|)   
(Intercept)        -535.038171   457.146614   -1.170   0.2419 
age                 -33.142400   15.366892    -2.157   0.0310 *
geo_areasuburban    178.825158   143.305863    1.248   0.2121 
geo_areaurban       132.463265   158.726709    0.835   0.4040 
vehicle_typeminivan 178.825158   143.305863    1.248   0.2121 
vehicle_typesuv      -8.006108   118.116633   -0.068   0.9460 
vehicle_typetruck    26.426396   153.650455    0.172   0.8634 
est_value             0.031179   0.002496    12.489   &lt;0.000000002 ***
miles_driven          0.118748   0.014327     8.289   &lt;0.000000002 ***
college_grad_ind     17.248581   117.398583    0.147   0.8832 
speeding_ticket_ind 155.061583   140.143658    1.107   0.2658 
hard_braking_ind    -12.442358   109.794208   -0.113   0.9098 
late_driving_ind    183.329848   284.218859    0.645   0.5189 
clean driving_ind    -232.843170   111.106714    -2.096   0.0361 
age2                    0.343165     0.165340     2.076   0.0380 
hard_braking_ind:     469.079140   461.685886     1.016   0.3096 
 late_driving_ind
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 6995 on 19984 degrees of freedom
Multiple R-squared: 0.01267,  Adjusted R-squared: 0.01193
F-statistic: 17.1 on 15 and 19984 DF,
p-value: &lt;0.00000000000000022
</code></pre>
    <p class="normal">Although <a id="_idIndexMarker737"/>the R-squared and adjusted R-squared values didn’t change much compared to the previous model, the new features present some interesting insights. In particular, the estimate for <code class="inlineCode">age</code> is relatively large and negative (lower expenses) but <code class="inlineCode">age2</code> is relatively small and positive (higher expenses). However, because age squared grows faster than age, expenses will begin to rise for very high age groups. The overall effect is a U-shaped expense curve, where the youngest and oldest enrollees are predicted to have higher expenses. The interaction of <code class="inlineCode">hard_braking_ind</code> and <code class="inlineCode">late_driving_ind</code> is also interesting, as it is a relatively large positive. Although the interaction is not statistically significant, the direction of the effect implies that driving late at night is especially dangerous if you are the type of driver that already drives dangerously.</p>
    <div class="note">
      <p class="normal">Strictly speaking, regression modeling makes some strong assumptions about the data. These assumptions are not as important for numeric forecasting, as the model’s worth is not based upon whether it truly captures the underlying process—we simply care about the accuracy of its predictions. However, if you would like to make firm inferences from the regression model coefficients, it is necessary to run diagnostic tests to ensure that the regression assumptions have not been violated. For an excellent introduction to this topic, see <em class="italic">Multiple Regression: A Primer, Allison, P. D., Pine Forge Press, 1998</em>.</p>
    </div>
    <h3 class="heading-3" id="_idParaDest-152">Making predictions with a regression model</h3>
    <p class="normal">After<a id="_idIndexMarker738"/> examining the estimated regression coefficients and fit statistics, we can also use the model to predict the expenses of future enrollees on the insurance plan. To illustrate the process of making predictions, let’s first apply the model to the original training data using the <code class="inlineCode">predict()</code> function as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> insurance<span class="hljs-operator">$</span>pred <span class="hljs-operator">&lt;-</span> predict<span class="hljs-punctuation">(</span>ins_model2<span class="hljs-punctuation">,</span> insurance<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">This saves the predictions as a new vector named <code class="inlineCode">pred </code>in the insurance data frame. We can then compute the correlation between the predicted and actual costs of insurance:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> cor<span class="hljs-punctuation">(</span>insurance<span class="hljs-operator">$</span>pred<span class="hljs-punctuation">,</span> insurance<span class="hljs-operator">$</span>expenses<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.1125714
</code></pre>
    <p class="normal">The correlation of 0.11 suggests a relatively weak linear relationship between the predicted and actual values, which is disappointing but not too surprising given the seemingly random nature of motor vehicle accidents. It can also be useful to examine this finding as a scatterplot. The following R commands plot the relationship and then add an identity line with an intercept equal to zero and a slope equal to one. The <code class="inlineCode">col</code>, <code class="inlineCode">lwd</code>, and <code class="inlineCode">lty</code> parameters affect the line color, width, and type, respectively:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> plot<span class="hljs-punctuation">(</span>insurance<span class="hljs-operator">$</span>pred<span class="hljs-punctuation">,</span> insurance<span class="hljs-operator">$</span>expenses<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> abline<span class="hljs-punctuation">(</span>a <span class="hljs-operator">=</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> b <span class="hljs-operator">=</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> col <span class="hljs-operator">=</span> <span class="hljs-string">"red"</span><span class="hljs-punctuation">,</span> lwd <span class="hljs-operator">=</span> <span class="hljs-number">3</span><span class="hljs-punctuation">,</span> lty <span class="hljs-operator">=</span> <span class="hljs-number">2</span><span class="hljs-punctuation">)</span>
</code></pre>
    <figure class="mediaobject"><img alt="Chart, scatter chart  Description automatically generated" src="../Images/B17290_06_12.png"/></figure>
    <p class="packt_figref">Figure 6.12: In this scatterplot, points falling on or near the diagonal dashed line where <em class="italic">y = x</em> indicate the predictions that were very close to the actual values</p>
    <p class="normal">The <a id="_idIndexMarker739"/>off-diagonal points falling above the line are cases where the actual expenses are greater than expected, while cases falling below the line are those less than expected. We can see here that a small number of people with much larger-than-expected expenses are balanced by a huge number of people with slightly smaller-than-expected expenses.</p>
    <p class="normal">Now, suppose you would like to forecast the expenses for potential new enrollees on the insurance plan. To do this, you must provide the <code class="inlineCode">predict()</code> function a data frame with the prospective drivers’ data. In the case of many drivers, you may consider creating a CSV spreadsheet file to load in R, or for a smaller number, you may simply create a data frame within the <code class="inlineCode">predict()</code> function itself. For example, to estimate the insurance expenses for a 30-year-old, rural driver of a truck valued at $25,000 driven for about 14,000 miles annually with a clean driving record:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> predict<span class="hljs-punctuation">(</span>ins_model2<span class="hljs-punctuation">,</span>
          data.frame<span class="hljs-punctuation">(</span>age <span class="hljs-operator">=</span> <span class="hljs-number">30</span><span class="hljs-punctuation">,</span> age2 <span class="hljs-operator">=</span> <span class="hljs-number">30</span><span class="hljs-operator">^</span><span class="hljs-number">2</span><span class="hljs-punctuation">,</span> geo_area <span class="hljs-operator">=</span> <span class="hljs-string">"</span><span class="hljs-string">rural"</span><span class="hljs-punctuation">,</span> 
                     vehicle_type <span class="hljs-operator">=</span> <span class="hljs-string">"truck"</span><span class="hljs-punctuation">,</span> est_value <span class="hljs-operator">=</span> <span class="hljs-number">25000</span><span class="hljs-punctuation">,</span>
                     miles_driven <span class="hljs-operator">=</span> <span class="hljs-number">14000</span><span class="hljs-punctuation">,</span> college_grad_ind <span class="hljs-operator">=</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span>
                     speeding_ticket_ind <span class="hljs-operator">=</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> hard_braking_ind <span class="hljs-operator">=</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span>
                     late_driving_ind <span class="hljs-operator">=</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> clean_driving_ind <span class="hljs-operator">=</span> <span class="hljs-number">1</span><span class="hljs-punctuation">))</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">       1 
1015.059
</code></pre>
    <p class="normal">Using <a id="_idIndexMarker740"/>this value, the insurance company would need to charge about $1,015 annually to break even for this demographic group. To compare the rate for someone who is otherwise similar except for a history of a recent accident, use the <code class="inlineCode">predict()</code> function in much the same way:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> predict<span class="hljs-punctuation">(</span>ins_model2<span class="hljs-punctuation">,</span>
          data.frame<span class="hljs-punctuation">(</span>age <span class="hljs-operator">=</span> <span class="hljs-number">30</span><span class="hljs-punctuation">,</span> age2 <span class="hljs-operator">=</span> <span class="hljs-number">30</span><span class="hljs-operator">^</span><span class="hljs-number">2</span><span class="hljs-punctuation">,</span> geo_area <span class="hljs-operator">=</span> <span class="hljs-string">"rural"</span><span class="hljs-punctuation">,</span> 
                     vehicle_type <span class="hljs-operator">=</span> <span class="hljs-string">"truck"</span><span class="hljs-punctuation">,</span> est_value <span class="hljs-operator">=</span> <span class="hljs-number">25000</span><span class="hljs-punctuation">,</span>
                     miles_driven <span class="hljs-operator">=</span> <span class="hljs-number">14000</span><span class="hljs-punctuation">,</span> college_grad_ind <span class="hljs-operator">=</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span>
                     speeding_ticket_ind <span class="hljs-operator">=</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> hard_braking_ind <span class="hljs-operator">=</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span>
                     late_driving_ind <span class="hljs-operator">=</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> clean_driving_ind <span class="hljs-operator">=</span> <span class="hljs-number">0</span><span class="hljs-punctuation">))</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">       1 
1247.903
</code></pre>
    <p class="normal">Note that the difference between these two values, <em class="italic">1015.059 – 1247.903 = -232.844</em>, is the same as the estimated regression model coefficient for <code class="inlineCode">clean_driving_ind</code>. On average, drivers with a clean history are estimated to have about $232.84 less in expenses for the plan per year, all else being equal.</p>
    <p class="normal">This illustrates the more general fact that the predicted expenses are a sum of each of the regression coefficients times their corresponding value in the prediction data frame. For instance, using the model’s regression coefficient of 0.118748 for the miles driven, we can predict that adding 10,000 additional miles will result in an increase in expenses of <em class="italic">10,000 * 0.118748 = 1187.48</em>, which can be confirmed as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> predict<span class="hljs-punctuation">(</span>ins_model2<span class="hljs-punctuation">,</span>
          data.frame<span class="hljs-punctuation">(</span>age <span class="hljs-operator">=</span> <span class="hljs-number">30</span><span class="hljs-punctuation">,</span> age2 <span class="hljs-operator">=</span> <span class="hljs-number">30</span><span class="hljs-operator">^</span><span class="hljs-number">2</span><span class="hljs-punctuation">,</span> geo_area <span class="hljs-operator">=</span> <span class="hljs-string">"rural"</span><span class="hljs-punctuation">,</span> 
                     vehicle_type <span class="hljs-operator">=</span> <span class="hljs-string">"truck"</span><span class="hljs-punctuation">,</span> est_value <span class="hljs-operator">=</span> <span class="hljs-number">25000</span><span class="hljs-punctuation">,</span>
                     miles_driven <span class="hljs-operator">=</span> <span class="hljs-number">14000</span><span class="hljs-punctuation">,</span> college_grad_ind <span class="hljs-operator">=</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span>
                     speeding_ticket_ind <span class="hljs-operator">=</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> hard_braking_ind <span class="hljs-operator">=</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span>
                     late_driving_ind <span class="hljs-operator">=</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> clean_driving_ind <span class="hljs-operator">=</span> <span class="hljs-number">0</span><span class="hljs-punctuation">))</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">       1 
1247.903 
</code></pre>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> <span class="hljs-number">2435.384</span> <span class="hljs-operator">-</span> <span class="hljs-number">1247.903</span> 
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 1187.481
</code></pre>
    <p class="normal">Following similar steps for a number of additional customer risk segments, the insurance<a id="_idIndexMarker741"/> company would be able to develop a pricing structure that fairly sets costs according to drivers’ estimated risk level while also maintaining a consistent profit across all segments.</p>
    <div class="packt_tip">
      <p class="normal">Exporting the model’s regression coefficients allows you to build your own forecasting function. One potential use case for doing so would be to implement the regression model in a customer database for real-time prediction.</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-153">Going further – predicting insurance policyholder churn with logistic regression</h2>
    <p class="normal">Actuarial <a id="_idIndexMarker742"/>estimates of claims costs are not the only potential application of machine learning inside an insurance company. Marketing and customer retention teams are likely to be very interested in predicting <strong class="keyWord">churn</strong>, or the customers that leave the company after choosing not to renew their insurance plan. In many businesses, preventing churn is highly valued, as churned customers not only reduce the income stream for one business but also often increase the revenue stream of a direct competitor. Additionally, marketing teams know that the costs of acquiring new customers are generally much higher than the costs of retaining an existing customer. </p>
    <p class="normal">Therefore, knowing ahead of time which customers are most likely to churn can help direct retention resources to intervene and prevent churn before it happens.</p>
    <p class="normal">Historically, marketing teams have used a simple model called <strong class="keyWord">recency, frequency, monetary value</strong> (<strong class="keyWord">RFM</strong>) to identify highly valuable customers as well as those most likely to churn. The RFM analysis <a id="_idIndexMarker743"/>considers three characteristics of each customer:</p>
    <ul>
      <li class="bulletList">How recently have they purchased? Customers that haven’t purchased in a while may be less valuable and more likely to never return.</li>
      <li class="bulletList">How frequently do they purchase? Do they come back year after year, or are there irregular gaps in their purchasing behavior? Customers that exhibit loyalty may be more valuable and more likely to return.</li>
      <li class="bulletList">How much money do they spend when they purchase? Do they spend more than the average customer or upgrade to premium products? These customers are more valuable financially, but also demonstrate their love of the brand.</li>
    </ul>
    <p class="normal">Historical customer <a id="_idIndexMarker744"/>purchase data is collected with the intention of developing measures of each of these three factors. The measures are then converted into a standard scale (such as a scale from zero to ten) for each of the three areas and summed to create a final RFM score for each customer. A very recent and frequent purchaser that spends an average amount may have a combined score of <em class="italic">10 + 10 + 5 = 25</em> while a customer that purchased once a long time ago may have a score of <em class="italic">2 + 1 + 4 = 7</em>, which places them much lower on the RFM scale.</p>
    <p class="normal">This <a id="_idIndexMarker745"/>type of analysis is a crude but useful tool for understanding a set of customers and helping identify the types of data that may be useful for predicting churn. However, an RFM analysis is not particularly scientific and provides no formal estimation of the probability of churn or the factors that increase its likelihood. In contrast, a logistic regression model predicting a binary churn outcome provides both an estimated probability of churn for each customer as well as the impact of each predictor.</p>
    <p class="normal">As with the insurance claims cost example, we’ll build a churn model using a simulated dataset created for this book, which is intended to approximate the behavior of customers of the automobile insurance company.</p>
    <div class="packt_tip">
      <p class="normal">If you would like to follow along interactively, download the <code class="inlineCode">insurance_churn.csv</code> file from the Packt Publishing GitHub repository for this book and save it to your R working folder.</p>
    </div>
    <p class="normal">The churn dataset includes 5,000 examples of current and former beneficiaries enrolled in the hypothetical automobile insurance plan. Each example includes features measuring customer behaviors in the plan year, as well as a binary indicator (<code class="inlineCode">churn</code>) of whether they churned out of the plan by not renewing at the end of the year. The available features include:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">member_id</code>: A randomly assigned customer identification number</li>
      <li class="bulletList"><code class="inlineCode">loyalty_years</code>: The number of consecutive years enrolled in the insurance plan</li>
      <li class="bulletList"><code class="inlineCode">vehicles_covered</code>: The number of vehicles covered by the insurance plan</li>
      <li class="bulletList"><code class="inlineCode">premium_plan_ind</code>: A binary indicator that the member paid for a premium, high-cost version of the plan with additional benefits</li>
      <li class="bulletList"><code class="inlineCode">mobile_app_user</code>: A binary indicator that the member uses the mobile phone companion application</li>
      <li class="bulletList"><code class="inlineCode">home_auto_bundle</code>: A binary indicator that the member also holds a homeowner’s insurance plan offered by the same company</li>
      <li class="bulletList"><code class="inlineCode">auto_pay_ind</code>: A binary indicator that the member has automatic payments turned on</li>
      <li class="bulletList"><code class="inlineCode">recent_rate_increase</code>: A binary indicator that the member’s price was raised recently</li>
    </ul>
    <div class="packt_tip">
      <p class="normal">Notice that many of these factors relate to the three components of RFM in that they are measures of loyalty and monetary value. Thus, even if a more sophisticated model is built later, it can still be helpful to perform an RFM analysis as an earlier step in the process.</p>
    </div>
    <p class="normal">To<a id="_idIndexMarker746"/> read this dataset into R, type:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> churn_data <span class="hljs-operator">&lt;-</span> read.csv<span class="hljs-punctuation">(</span><span class="hljs-string">"</span><span class="hljs-string">insurance_churn.csv"</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">Using the <code class="inlineCode">table()</code> and <code class="inlineCode">prop.table()</code> functions, we can see the overall churn rate is just over 15 percent:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> prop.table<span class="hljs-punctuation">(</span>table<span class="hljs-punctuation">(</span>churn_data<span class="hljs-operator">$</span>churn<span class="hljs-punctuation">))</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">     0      1 
0.8492 0.1508
</code></pre>
    <p class="normal">In a more formal analysis, it would be wise to perform more data exploration before going further. Here, we will jump ahead to creating the logistic regression model to predict these churned customers.</p>
    <p class="normal">The <code class="inlineCode">glm()</code> function, which is part of R’s built-in <code class="inlineCode">stats</code> package, is used to fit a GLM model such as logistic regression as well as the other variants like Poisson regression described earlier in the chapter. The syntax for logistic regression is shown in the following figure:</p>
    <figure class="mediaobject"><img alt="Text, letter  Description automatically generated" src="../Images/B17290_06_13.png"/></figure>
    <p class="packt_figref">Figure 6.13: Logistic regression syntax</p>
    <p class="normal">Note <a id="_idIndexMarker747"/>the many similarities between the <code class="inlineCode">glm()</code> function syntax and the <code class="inlineCode">lm()</code> function used earlier for standard linear regression. Aside from specifying the family and link function, fitting the model is no more difficult. The key differences are primarily in how the resulting model is interpreted.</p>
    <div class="packt_tip">
      <p class="normal">Beware that R’s <code class="inlineCode">glm()</code> function defaults to Gaussian with an identity link, so it is easy to accidentally perform standard linear regression when another GLM form is desired! For this reason, it is wise to form a habit of always specifying the family and link when building a GLM in R.</p>
    </div>
    <p class="normal">To fit<a id="_idIndexMarker748"/> the logistic regression churn model, we specify the <code class="inlineCode">binomial</code> family with the <code class="inlineCode">logit</code> link function. Here, we model churn as a function of all other features in the dataset, minus <code class="inlineCode">member_id</code>, which is unique to each member and therefore useless for prediction:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> churn_model <span class="hljs-operator">&lt;-</span> glm<span class="hljs-punctuation">(</span>churn <span class="hljs-operator">~</span> . <span class="hljs-operator">-</span>member_id<span class="hljs-punctuation">,</span> data <span class="hljs-operator">=</span> churn_data<span class="hljs-punctuation">,</span>
                     family <span class="hljs-operator">=</span> binomial<span class="hljs-punctuation">(</span>link <span class="hljs-operator">=</span> <span class="hljs-string">"logit"</span><span class="hljs-punctuation">))</span>
</code></pre>
    <p class="normal">Using <code class="inlineCode">summary()</code> on the resulting <code class="inlineCode">churn_model</code> object shows the estimated regression parameters:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> summary<span class="hljs-punctuation">(</span>churn_model<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Call:
glm(formula = churn ~ . - member_id,
      family = binomial(link = "logit"), data = ins_churn)
Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.1244  -0.6152  -0.5033  -0.3950   2.4995  
Coefficients:
                      Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)          -0.488893   0.141666  -3.451 0.000558 ***
loyalty_years        -0.072284   0.007193 -10.050  &lt; 2e-16 ***
vehicles_covered     -0.212980   0.055237  -3.856 0.000115 ***
premium_plan_ind     -0.370574   0.148937  -2.488 0.012842 *  
mobile_app_user      -0.292273   0.080651  -3.624 0.000290 ***
home_auto_bundle     -0.267032   0.093932  -2.843 0.004472 ** 
auto_pay_ind         -0.075698   0.106130  -0.713 0.475687    
recent_rate_increase  0.648100   0.102596   6.317 2.67e-10 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
(Dispersion parameter for binomial family taken to be 1)
    Null deviance: 4240.9  on 4999  degrees of freedom
Residual deviance: 4059.2  on 4992  degrees of freedom
AIC: 4075.2
Number of Fisher Scoring iterations: 5
</code></pre>
    <p class="normal">At a high <a id="_idIndexMarker749"/>level, the logistic regression output is fairly similar to a linear regression output. The p-values (labeled <code class="inlineCode">Pr(&gt;|z|)</code>) and significance codes (denoted by <code class="inlineCode">*</code> characters) indicate whether the variables are statistically significant. All features aside from the <code class="inlineCode">auto_pay_ind</code> are significant at the 0.05 level or better. The direction of the relationship between the predictors and the target outcome can also be understood simply by looking at the sign (positive or negative) before the <code class="inlineCode">Estimate</code> value. Nearly all estimates are negative, which implies that these features reduce churn, except for <code class="inlineCode">recent_rate_increase</code>, which is positive and therefore increases churn. These connections make sense; an increase in the price of the insurance plan would be expected to increase churn, while members that have been loyal for years or pay for premium plan features are less likely to leave.</p>
    <p class="normal">Interpreting the impact of a specific feature on churn is where logistic regression is trickier than linear regression, as the estimates are shown in log odds. Suppose we want to know how much more likely churn is after a recent increase in the price of the insurance plan. Since the estimate for <code class="inlineCode">recent_rate_increase</code> is 0.6481, this means that the log odds of churn increase by 0.6481 when the rate increase indicator is <code class="inlineCode">1</code> versus when it is <code class="inlineCode">0</code>. Exponentiating this to remove the logarithm and find the odds ratio, we find that <em class="italic">exp(0.6481) = 1.911905</em>, which implies that churn is almost twice as likely (or 91.2 percent more likely) after a rate increase.</p>
    <p class="normal">In the opposite direction, members that use the mobile app (<code class="inlineCode">mobile_app_user</code>) have an estimated difference in log odds of -0.292273 versus those that do not. Finding the odds ratio as <em class="italic">exp(-0.292273) = 0.7465647</em> suggests that the churn of app users is about 75 percent of those that do not use the app, or a decrease of about 25 percent for app users. Similarly, we can find that churn is reduced by about seven percent for each additional year of loyalty, as <em class="italic">exp(-0.072284) = 0.9302667</em>. Similar calculations can be performed for all other predictors in the model, including the intercept, which represents the odds of churn when all predictors are zero.</p>
    <p class="normal">To use this model to prevent churn, we can make predictions on a database of current plan members. Let’s begin by loading a dataset containing 1000 subscribers, using the <code class="inlineCode">test</code> dataset available for this chapter:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> churn_test <span class="hljs-operator">&lt;-</span> read.csv<span class="hljs-punctuation">(</span><span class="hljs-string">"insurance_churn_test.csv"</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">We’ll then use the logistic regression model object with the <code class="inlineCode">predict()</code> function to add a new column to this data frame, which contains the predictions for each member:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> churn_test<span class="hljs-operator">$</span>churn_prob <span class="hljs-operator">&lt;-</span> predict<span class="hljs-punctuation">(</span>churn_model<span class="hljs-punctuation">,</span> churn_test<span class="hljs-punctuation">,</span>
                                   type <span class="hljs-operator">=</span> <span class="hljs-string">"response"</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">Note <a id="_idIndexMarker750"/>that the <code class="inlineCode">type = "response"</code> parameter is set so that the predictions are in probabilities rather than the default <code class="inlineCode">type = "link"</code> setting, which produces predictions as log odds values.</p>
    <p class="normal">Summarizing these predicted probabilities, we see that the average churn probability is about 15 percent, but some users are predicted to have very low churn, while others have a churn probability as high as 41 percent:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> summary<span class="hljs-punctuation">(</span>churn_test<span class="hljs-operator">$</span>churn_prob<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
0.02922 0.09349 0.13489 0.14767 0.18452 0.41604
</code></pre>
    <p class="normal">Suppose the customer retention team has the resources to intervene in a limited number of cases. By sorting the members to identify those with the highest predicted churn likelihood, we can provide the team with the direction most likely to make the greatest impact.</p>
    <p class="normal">First, use the <code class="inlineCode">order()</code> function to obtain a vector with the row numbers sorted in decreasing order according to their churn probability:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> churn_order <span class="hljs-operator">&lt;-</span> order<span class="hljs-punctuation">(</span>churn_test<span class="hljs-operator">$</span>churn_prob<span class="hljs-punctuation">,</span> decreasing <span class="hljs-operator">=</span> <span class="hljs-literal">TRUE</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">Next, after ordering the <code class="inlineCode">churn_test</code> data frame according to the <code class="inlineCode">churn_order</code> vector and taking the two columns of interest, use the <code class="inlineCode">head()</code> function to take the top <code class="inlineCode">n</code> rows; in this case, we’ll set <code class="inlineCode">n = 5</code> to limit to the five members most likely to churn:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> head<span class="hljs-punctuation">(</span>churn_test<span class="hljs-punctuation">[</span>churn_order<span class="hljs-punctuation">,</span>
        <span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-string">"member_id"</span><span class="hljs-punctuation">,</span> <span class="hljs-string">"churn_prob"</span><span class="hljs-punctuation">)],</span> n <span class="hljs-operator">=</span> <span class="hljs-number">5</span><span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">    member_id churn_prob
406  29603520  0.4160438
742  12588881  0.4160438
390  23228258  0.3985958
541  86406649  0.3985958
614  49806111  0.3985958
</code></pre>
    <p class="normal">After <a id="_idIndexMarker751"/>saving the result to a spreadsheet with <code class="inlineCode">n</code> set to a higher number, it would be possible to provide the customer retention team with a list of the insurance plan members that are most likely to churn. Focusing retention efforts on these members is likely to be a more fruitful use of the marketing budget than targeting members at random, as most members have a very low churn probability. In this way, machine learning can provide a substantial return on minimal investment, with an impact that is easily quantifiable by comparing the churn rates before and after this intervention.</p>
    <p class="normal">Estimates of revenue retained as a result of churn prevention can be obtained using simple assumptions about the proportion of customers that will respond to retention efforts. For example, if we assume <em class="italic">N</em> of members targeted by the churn model will be retained, this will result in <em class="italic">N</em> times <em class="italic">$X</em> retained revenue, where <em class="italic">$X</em> is the average customer spend. Bringing this number to stakeholders helps provide justification for implementing the machine learning project.</p>
    <p class="normal">The example is only the tip of the iceberg, as churn modeling can become much more sophisticated with additional efforts. For instance, rather than targeting the customers with the highest churn probability, it is also possible to consider the revenue lost if the customer churns; it may be worth prioritizing high-value customers even if they have a lower churn probability than a low-value customer. Additionally, because some customers are assured to churn regardless of intervention while others may be more flexible about staying, it is possible to model retention likelihood in addition to modeling churn probability. In any case, even in a simple form, churn modeling is low-hanging fruit for most businesses and a great first place to implement machine learning.</p>
    <h1 class="heading-1" id="_idParaDest-154">Understanding regression trees and model trees</h1>
    <p class="normal">If you recall from <em class="chapterRef">Chapter 5</em>, <em class="italic">Divide and Conquer – Classification Using Decision Trees and Rules</em>, a decision <a id="_idIndexMarker752"/>tree builds a model, much like a flowchart, in which decision nodes, leaf nodes, and branches define a series of decisions, which are used to classify examples. Such trees can also be used for numeric prediction by making only small adjustments to the tree-growing algorithm. In this section, we will consider the ways in which trees for numeric prediction differ from trees used for classification.</p>
    <p class="normal">Trees for numeric prediction fall into two categories. The first, known as <strong class="keyWord">regression trees</strong>, were<a id="_idIndexMarker753"/> introduced in the<a id="_idIndexMarker754"/> 1980s as part of the seminal <strong class="keyWord">classification and regression tree</strong> (<strong class="keyWord">CART</strong>) algorithm. Despite the name, regression trees do not use linear regression methods as described earlier in this chapter; rather, they make predictions based on the average value of examples that reach a leaf.</p>
    <div class="note">
      <p class="normal">The CART algorithm is described in detail in <em class="italic">Classification and Regression Trees, Breiman, L., Friedman, J. H., Stone, C. J., Olshen, R. A., Chapman and Hall, 1984</em>.</p>
    </div>
    <p class="normal">The second type of tree for numeric prediction is known<a id="_idIndexMarker755"/> as a <strong class="keyWord">model tree</strong>. Introduced several years later than regression trees, they are less widely known, but perhaps more powerful. Model trees are grown in much the same way as regression trees, but at each leaf, a multiple linear regression model is built from the examples reaching that node. Depending on the number of leaf nodes, a model tree may build tens or even hundreds of such models. </p>
    <p class="normal">This makes model trees more difficult to understand than the equivalent regression tree, with the benefit that they may result in a more accurate model.</p>
    <div class="note">
      <p class="normal">The earliest <a id="_idIndexMarker756"/>model tree algorithm, <strong class="keyWord">M5</strong>, is described in <em class="italic">Learning with continuous classes, Quinlan, J. R., Proceedings of the 5th Australian Joint Conference on Artificial Intelligence, 1992, pp. 343-348</em>.</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-155">Adding regression to trees</h2>
    <p class="normal">Trees that can<a id="_idIndexMarker757"/> perform numeric prediction offer a compelling, yet often overlooked, alternative to regression modeling. The strengths and weaknesses of regression trees and model trees relative to the more common regression methods are listed<a id="_idIndexMarker758"/> in the<a id="_idIndexMarker759"/> following table:</p>
    <table class="table-container" id="table003-3">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Strengths</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Weaknesses</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <ul>
              <li class="bulletList">Combines the strengths of decision trees with the ability to model numeric data</li>
              <li class="bulletList">Does not require the user to specify the model in advance</li>
              <li class="bulletList">Uses automatic feature selection, which allows the approach to be used with a very large number of features</li>
              <li class="bulletList">May fit some types of data much better than linear regression</li>
              <li class="bulletList">Does not require knowledge of statistics to interpret the model</li>
            </ul>
          </td>
          <td class="table-cell">
            <ul>
              <li class="bulletList">Not as well known as linear regression</li>
              <li class="bulletList">Requires a large amount of training data</li>
              <li class="bulletList">Difficult to determine the overall net effect of individual features on the outcome</li>
              <li class="bulletList">Large trees can become more difficult to interpret than a regression model</li>
            </ul>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">Though<a id="_idIndexMarker760"/> traditional regression methods are typically the<a id="_idIndexMarker761"/> first choice for numeric prediction tasks, in some cases, numeric decision trees offer distinct advantages. For instance, decision trees may be better suited for tasks with many features or many complex, nonlinear relationships between features and the outcome; these situations present challenges for regression. Regression modeling also makes assumptions about the data that are often violated in real-world data; this is not the case for trees.</p>
    <p class="normal">Trees for numeric prediction are built in much the same way as they are for classification. Beginning at the root node, the data is partitioned using a divide-and-conquer strategy according to the feature that will result in the greatest increase in homogeneity in the outcome after a split is performed. </p>
    <p class="normal">In classification trees, you will recall that homogeneity is measured by entropy. This is undefined for numeric data. Instead, for numeric decision trees, homogeneity is measured by statistics such as variance, standard deviation, or absolute deviation from the mean.</p>
    <p class="normal">One common splitting criterion is <a id="_idIndexMarker762"/>called the <strong class="keyWord">standard deviation reduction</strong> (<strong class="keyWord">SDR</strong>). It is defined by the following formula:</p>
    <p class="center"><img alt="" src="../Images/B17290_06_048.png"/></p>
    <p class="normal">In this formula, the <em class="italic">sd</em>(<em class="italic">T</em>) function refers to the standard deviation of the values in set <em class="italic">T</em>, while <em class="italic">T</em><sub class="subscript-italic" style="font-style: italic;">1</sub>, <em class="italic">T</em><sub class="subscript-italic" style="font-style: italic;">2</sub>, ..., <em class="italic">T</em><sub class="subscript-italic" style="font-style: italic;">n</sub> are sets of values resulting from a split on a feature. The <em class="italic">|T|</em> term refers to the number of observations in set <em class="italic">T</em>. Essentially, the formula measures the reduction in standard deviation by comparing the standard deviation pre-split to the weighted standard deviation post-split.</p>
    <p class="normal">For example, consider <a id="_idIndexMarker763"/>the following case in which a tree is deciding whether to perform a split on binary feature A or a split on binary feature B:</p>
    <figure class="mediaobject"><img alt="Table  Description automatically generated" src="../Images/B17290_06_14.png"/></figure>
    <p class="packt_figref">Figure 6.14: The algorithm considers splits on features A and B, which creates different T<sub class="subscript-italic" style="font-style: italic;">1</sub> and T<sub class="subscript-italic" style="font-style: italic;">2</sub> groups</p>
    <p class="normal">Using the groups that would result from the proposed splits, we can compute the SDR for A and B as follows. The <code class="inlineCode">length()</code> function used here returns the number of elements in a vector. Note that the overall group T is named <code class="inlineCode">tee</code> to avoid overwriting R’s built-in <code class="inlineCode">T()</code> and <code class="inlineCode">t()</code> functions.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> tee <span class="hljs-operator">&lt;-</span> <span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">2</span><span class="hljs-punctuation">,</span> <span class="hljs-number">2</span><span class="hljs-punctuation">,</span> <span class="hljs-number">3</span><span class="hljs-punctuation">,</span> <span class="hljs-number">4</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span><span class="hljs-punctuation">,</span> <span class="hljs-number">6</span><span class="hljs-punctuation">,</span> <span class="hljs-number">6</span><span class="hljs-punctuation">,</span> <span class="hljs-number">7</span><span class="hljs-punctuation">,</span> <span class="hljs-number">7</span><span class="hljs-punctuation">,</span> <span class="hljs-number">7</span><span class="hljs-punctuation">,</span> <span class="hljs-number">7</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> at1 <span class="hljs-operator">&lt;-</span> <span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">2</span><span class="hljs-punctuation">,</span> <span class="hljs-number">2</span><span class="hljs-punctuation">,</span> <span class="hljs-number">3</span><span class="hljs-punctuation">,</span> <span class="hljs-number">4</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> at2 <span class="hljs-operator">&lt;-</span> <span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-number">6</span><span class="hljs-punctuation">,</span> <span class="hljs-number">6</span><span class="hljs-punctuation">,</span> <span class="hljs-number">7</span><span class="hljs-punctuation">,</span> <span class="hljs-number">7</span><span class="hljs-punctuation">,</span> <span class="hljs-number">7</span><span class="hljs-punctuation">,</span> <span class="hljs-number">7</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> bt1 <span class="hljs-operator">&lt;-</span> <span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">2</span><span class="hljs-punctuation">,</span> <span class="hljs-number">2</span><span class="hljs-punctuation">,</span> <span class="hljs-number">3</span><span class="hljs-punctuation">,</span> <span class="hljs-number">4</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> bt2 <span class="hljs-operator">&lt;-</span> <span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-number">5</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span><span class="hljs-punctuation">,</span> <span class="hljs-number">6</span><span class="hljs-punctuation">,</span> <span class="hljs-number">6</span><span class="hljs-punctuation">,</span> <span class="hljs-number">7</span><span class="hljs-punctuation">,</span> <span class="hljs-number">7</span><span class="hljs-punctuation">,</span> <span class="hljs-number">7</span><span class="hljs-punctuation">,</span> <span class="hljs-number">7</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> sdr_a <span class="hljs-operator">&lt;-</span> sd<span class="hljs-punctuation">(</span>tee<span class="hljs-punctuation">)</span> <span class="hljs-operator">-</span> <span class="hljs-punctuation">(</span><span class="hljs-built_in">length</span><span class="hljs-punctuation">(</span>at1<span class="hljs-punctuation">)</span> <span class="hljs-operator">/</span> <span class="hljs-built_in">length</span><span class="hljs-punctuation">(</span>tee<span class="hljs-punctuation">)</span> <span class="hljs-operator">*</span> sd<span class="hljs-punctuation">(</span>at1<span class="hljs-punctuation">)</span> <span class="hljs-operator">+</span>
             <span class="hljs-built_in">length</span><span class="hljs-punctuation">(</span>at2<span class="hljs-punctuation">)</span> <span class="hljs-operator">/</span> <span class="hljs-built_in">length</span><span class="hljs-punctuation">(</span>tee<span class="hljs-punctuation">)</span> <span class="hljs-operator">*</span> sd<span class="hljs-punctuation">(</span>at2<span class="hljs-punctuation">))</span>
<span class="hljs-operator">&gt;</span> sdr_b <span class="hljs-operator">&lt;-</span> sd<span class="hljs-punctuation">(</span>tee<span class="hljs-punctuation">)</span> <span class="hljs-operator">-</span> <span class="hljs-punctuation">(</span><span class="hljs-built_in">length</span><span class="hljs-punctuation">(</span>bt1<span class="hljs-punctuation">)</span> <span class="hljs-operator">/</span> <span class="hljs-built_in">length</span><span class="hljs-punctuation">(</span>tee<span class="hljs-punctuation">)</span> <span class="hljs-operator">*</span> sd<span class="hljs-punctuation">(</span>bt1<span class="hljs-punctuation">)</span> <span class="hljs-operator">+</span>
             <span class="hljs-built_in">length</span><span class="hljs-punctuation">(</span>bt2<span class="hljs-punctuation">)</span> <span class="hljs-operator">/</span> <span class="hljs-built_in">length</span><span class="hljs-punctuation">(</span>tee<span class="hljs-punctuation">)</span> <span class="hljs-operator">*</span> sd<span class="hljs-punctuation">(</span>bt2<span class="hljs-punctuation">))</span>
</code></pre>
    <p class="normal">Let’s compare the SDR of A against the SDR of B:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> sdr_a
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 1.202815
</code></pre>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> sdr_b
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 1.392751
</code></pre>
    <p class="normal">The SDR for the split on feature A was about 1.2 versus 1.4 for the split on feature B. Since the standard deviation was reduced more for the split on B, the decision tree would use B first. It results in slightly more homogeneous sets than does A.</p>
    <p class="normal">Suppose that the tree stopped growing here using this one and only split. A regression tree’s work is done. It can make predictions for new examples depending on whether the example’s value on feature B places the example into group <em class="italic">T</em><sub class="subscript-italic" style="font-style: italic;">1</sub> or <em class="italic">T</em><sub class="subscript-italic" style="font-style: italic;">2</sub>. If the example ends up in <em class="italic">T</em><sub class="subscript-italic" style="font-style: italic;">1</sub>, the model would predict <em class="italic">mean</em>(<em class="italic">bt1</em>) =<em class="italic"> 2</em>, otherwise it would predict <em class="italic">mean</em>(<em class="italic">bt2</em>) = <em class="italic">6.25</em>.</p>
    <p class="normal">In contrast, a <a id="_idIndexMarker764"/>model tree would go one step further. Using the seven training examples falling in group <em class="italic">T</em><sub class="subscript-italic" style="font-style: italic;">1</sub> and the eight in <em class="italic">T</em><sub class="subscript-italic" style="font-style: italic;">2</sub>, the model tree could build a linear regression model of the outcome versus feature A. Note that feature B is of no help in building the regression model because all examples at the leaf have the same value of B—they were placed into <em class="italic">T</em><sub class="subscript-italic" style="font-style: italic;">1</sub> or <em class="italic">T</em><sub class="subscript-italic" style="font-style: italic;">2</sub> according to their value of B. The model tree can then make predictions for new examples using either of the two linear models.</p>
    <p class="normal">To further illustrate the differences between these two approaches, let’s work through a real-world example.</p>
    <h1 class="heading-1" id="_idParaDest-156">Example – estimating the quality of wines with regression trees and model trees</h1>
    <p class="normal">Winemaking<a id="_idIndexMarker765"/> is a challenging and competitive <a id="_idIndexMarker766"/>business, which offers the potential for great profit. However, there are numerous factors that <a id="_idIndexMarker767"/>contribute to the profitability of a winery. As an agricultural product, variables as diverse as the weather and the growing environment impact the quality of a varietal. The bottling and manufacturing can also affect the flavor for better or worse. Even the way the product is marketed, from the bottle design to the price point, can affect the customer’s perception of the taste.</p>
    <p class="normal">Consequently, the winemaking industry has invested heavily in data collection and machine learning methods that may assist with the decision science of winemaking. For example, machine learning has been used to discover key differences in the chemical composition of wines from different regions, and to identify the chemical factors that lead a wine to taste sweeter.</p>
    <p class="normal">More recently, machine learning has been employed to assist with rating the quality of wine—a notoriously difficult task. A review written by a renowned wine critic often determines whether the product ends up on the top or bottom shelf, in spite of the fact that even expert judges are inconsistent when rating a wine in a blinded test.</p>
    <p class="normal">In this case study, we will use regression trees and model trees to create a system capable of mimicking expert ratings of wine. Because trees result in a model that is readily understood, this could allow winemakers to identify key factors that contribute to better-rated wines. Perhaps more importantly, the system does not suffer from the human elements of tasting, such as the rater’s mood or palate fatigue. Computer-aided wine testing may therefore result in a better product, as well as more objective, consistent, and fair ratings.</p>
    <h2 class="heading-2" id="_idParaDest-157">Step 1 – collecting data</h2>
    <p class="normal">To develop<a id="_idIndexMarker768"/> the wine rating model, we will use data donated to the UCI Machine Learning Repository (<a href="http://archive.ics.uci.edu/ml"><span class="url">http://archive.ics.uci.edu/ml</span></a>) by P. Cortez, A. Cerdeira, F. Almeida, T. Matos, and J. Reis. Their dataset includes examples of red and white Vinho Verde wines from Portugal—one of the world’s leading wine-producing countries. Because the factors that contribute to a highly rated wine may differ between the red and white varieties, for this analysis we will examine only the more popular white wines.</p>
    <div class="packt_tip">
      <p class="normal">To follow along with this example, download the <code class="inlineCode">whitewines.csv</code> file from the Packt Publishing GitHub repository for this book and save it to your R working directory. The <code class="inlineCode">redwines.csv</code> file is also available in case you would like to explore this data on your own.</p>
    </div>
    <p class="normal">The white wine data includes information on 11 chemical properties of 4,898 wine samples. For each wine, a laboratory analysis measured characteristics such as acidity, sugar content, chlorides, sulfur, alcohol, pH, and density. The samples were then rated in a blind tasting by panels of no less than three judges on a quality scale ranging from 0 (very bad) to 10 (excellent). In the case that the judges disagreed on the rating, the median value was used.</p>
    <p class="normal">The study by Cortez evaluated the ability of three machine learning approaches to model the wine data: multiple regression, artificial neural networks, and support vector machines. We covered multiple regression earlier in this chapter, and we will learn about neural networks and support vector machines in <em class="chapterRef">Chapter 7</em>, <em class="italic">Black-Box Methods – Neural Networks and Support Vector Machines</em>. The study found that the support vector machine offered significantly better results than the linear regression model. However, unlike regression, the support vector machine model is difficult to interpret. Using regression trees and model trees, we may be able to improve the regression results while still having a model that is relatively easy to understand.</p>
    <div class="note">
      <p class="normal">To read more about the wine study described here, refer to <em class="italic">Modeling wine preferences by data mining from physicochemical properties, Cortez, P., Cerdeira, A., Almeida, F., Matos, T., and Reis, J., Decision Support Systems, 2009, Vol. 47, pp. 547-553</em>.</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-158">Step 2 – exploring and preparing the data</h2>
    <p class="normal">As usual, we <a id="_idIndexMarker769"/>will use the <code class="inlineCode">read.csv()</code> function<a id="_idIndexMarker770"/> to load the data into R. Since all features are numeric, we can safely ignore the <code class="inlineCode">stringsAsFactors</code> parameter:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> wine <span class="hljs-operator">&lt;-</span> read.csv<span class="hljs-punctuation">(</span><span class="hljs-string">"whitewines.csv"</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">The <code class="inlineCode">wine</code> data includes 11 features and the quality outcome, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> str<span class="hljs-punctuation">(</span>wine<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">'data.frame':	4898 obs. of  12 variables:
 $ fixed.acidity       : num  6.7 5.7 5.9 5.3 6.4 7 7.9 ...
 $ volatile.acidity    : num  0.62 0.22 0.19 0.47 0.29 ...
 $ citric.acid         : num  0.24 0.2 0.26 0.1 0.21 0.41 ...
 $ residual.sugar      : num  1.1 16 7.4 1.3 9.65 0.9 ...
 $ chlorides           : num  0.039 0.044 0.034 0.036 0.041 ...
 $ free.sulfur.dioxide : num  6 41 33 11 36 22 33 17 34 40 ...
 $ total.sulfur.dioxide: num  62 113 123 74 119 95 152 ...
 $ density             : num  0.993 0.999 0.995 0.991 0.993 ...
 $ pH                  : num  3.41 3.22 3.49 3.48 2.99 3.25 ...
 $ sulphates           : num  0.32 0.46 0.42 0.54 0.34 0.43 ...
 $ alcohol             : num  10.4 8.9 10.1 11.2 10.9 ...
 $ quality             : int  5 6 6 4 6 6 6 6 6 7 ...
</code></pre>
    <p class="normal">Compared with other types of machine learning models, one of the advantages of trees is that they can handle many types of data without preprocessing. This means we do not need to normalize or standardize the features.</p>
    <p class="normal">However, a bit of effort to examine the distribution of the outcome variable is needed to inform our evaluation of the model’s performance. For instance, suppose that there was very little variation in quality from wine to wine, or that wines fell into a bimodal distribution: either very good or very bad. This may impact the way we design the model. To check for such extremes, we can examine the distribution of wine quality using a histogram:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> hist<span class="hljs-punctuation">(</span>wine<span class="hljs-operator">$</span>quality<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">This produces the following figure:</p>
    <figure class="mediaobject"><img alt="Chart, bar chart  Description automatically generated" src="../Images/B17290_06_15.png"/></figure>
    <p class="packt_figref">Figure 6.15: The distribution of the quality ratings of white wines</p>
    <p class="normal">The <a id="_idIndexMarker771"/>wine quality values appear to follow<a id="_idIndexMarker772"/> a roughly normal, bell-shaped distribution, centered around a value of six. This makes sense intuitively, because most wines are of average quality; few are particularly bad or good. Although the results are not shown here, it is also useful to examine the <code class="inlineCode">summary(wine)</code> output for outliers or other potential data problems. Even though trees are fairly robust to messy data, it is always prudent to check for severe problems. For now, we’ll assume that the data is reliable.</p>
    <p class="normal">Our last step, then, is to divide the dataset into training and testing sets. Since the <code class="inlineCode">wine</code> dataset was already sorted randomly, we can partition it into two sets of contiguous rows as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> wine_train <span class="hljs-operator">&lt;-</span> wine<span class="hljs-punctuation">[</span><span class="hljs-number">1</span><span class="hljs-operator">:</span><span class="hljs-number">3750</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">]</span>
<span class="hljs-operator">&gt;</span> wine_test <span class="hljs-operator">&lt;-</span> wine<span class="hljs-punctuation">[</span><span class="hljs-number">3751</span><span class="hljs-operator">:</span><span class="hljs-number">4898</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">]</span>
</code></pre>
    <p class="normal">In order to mirror the conditions used by Cortez, we used sets of 75 percent and 25 percent for training and testing, respectively. We’ll evaluate the performance of our tree-based models on the testing data to see if we can obtain results comparable to the prior research study.</p>
    <h2 class="heading-2" id="_idParaDest-159">Step 3 – training a model on the data</h2>
    <p class="normal">We will begin by<a id="_idIndexMarker773"/> training a regression tree model. Although almost any implementation of decision trees can be used to perform regression tree modeling, the <code class="inlineCode">rpart</code> (recursive partitioning) package offers the most faithful implementation of regression trees as they were described by the CART team. As the classic R implementation of CART, the <code class="inlineCode">rpart</code> package is also well documented and supported with functions for visualizing and evaluating the <code class="inlineCode">rpart</code> models.</p>
    <p class="normal">Install the <code class="inlineCode">rpart</code> package using the <code class="inlineCode">install.packages("rpart")</code> command. It can then be loaded into your R session using the <code class="inlineCode">library(rpart)</code> statement. The following syntax will train a tree using the default settings, which work well most of the time, but not always. If you need more fine-tuned settings, refer to the documentation for the control parameters using the <code class="inlineCode">?rpart.control</code> command.</p>
    <figure class="mediaobject"><img alt="Text  Description automatically generated" src="../Images/B17290_06_16.png"/></figure>
    <p class="packt_figref">Figure 6.16: Regression tree syntax</p>
    <p class="normal">Using the <a id="_idIndexMarker774"/>R formula interface, we can specify <code class="inlineCode">quality</code> as the outcome variable and use the dot notation to allow all other columns in the <code class="inlineCode">wine_train</code> data frame to be used as predictors. The resulting regression tree model object is named <code class="inlineCode">m.rpart</code> to distinguish it from the model tree we will train later:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> m.rpart <span class="hljs-operator">&lt;-</span> rpart<span class="hljs-punctuation">(</span>quality <span class="hljs-operator">~</span> .<span class="hljs-punctuation">,</span> data <span class="hljs-operator">=</span> wine_train<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">For basic information about the tree, simply type the name of the model object:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> m.rpart
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">n= 3750 
node), split, n, deviance, yval
      * denotes terminal node
 1) root 3750 2945.53200 5.870933  
   2) alcohol&lt; 10.85 2372 1418.86100 5.604975  
     4) volatile.acidity&gt;=0.2275 1611  821.30730 5.432030  
       8) volatile.acidity&gt;=0.3025 688  278.97670 5.255814 *
       9) volatile.acidity&lt; 0.3025 923  505.04230 5.563380 *
     5) volatile.acidity&lt; 0.2275 761  447.36400 5.971091 *
   3) alcohol&gt;=10.85 1378 1070.08200 6.328737  
     6) free.sulfur.dioxide&lt; 10.5 84   95.55952 5.369048 *
     7) free.sulfur.dioxide&gt;=10.5 1294  892.13600 6.391036  
      14) alcohol&lt; 11.76667 629  430.11130 6.173291  
        28) volatile.acidity&gt;=0.465 11   10.72727 4.545455 *
        29) volatile.acidity&lt; 0.465 618  389.71680 6.202265 *
      15) alcohol&gt;=11.76667 665  403.99400 6.596992 *
</code></pre>
    <p class="normal">For <a id="_idIndexMarker775"/>each node in the tree, the number of examples reaching the decision point is listed. For instance, all 3,750 examples begin at the root node, of which 2,372 have <code class="inlineCode">alcohol &lt; 10.85</code> and 1,378 have <code class="inlineCode">alcohol &gt;= 10.85</code>. Because <code class="inlineCode">alcohol</code> was used first in the tree, it is the single most important predictor of wine quality.</p>
    <p class="normal">Nodes indicated by <code class="inlineCode">*</code> are terminal or leaf nodes, which means that they result in a prediction (listed here as <code class="inlineCode">yval</code>). For example, node 5 has a <code class="inlineCode">yval</code> of 5.971091. When the tree is used for predictions, any wine samples with <code class="inlineCode">alcohol &lt; 10.85</code> and <code class="inlineCode">volatile.acidity &lt; 0.2275</code> would therefore be predicted to have a quality value of 5.97.</p>
    <p class="normal">A more detailed summary of the tree’s fit, including the mean squared error for each of the nodes and an overall measure of feature importance, can be obtained using the <code class="inlineCode">summary(m.rpart)</code> command.</p>
    <h3 class="heading-3" id="_idParaDest-160">Visualizing decision trees</h3>
    <p class="normal">Although<a id="_idIndexMarker776"/> the tree can be understood using only the preceding output, it is often more readily understood using visualization. The <code class="inlineCode">rpart.plot</code> package by Stephen Milborrow provides an easy-to-use function that produces publication-quality decision trees.</p>
    <div class="note">
      <p class="normal">For more information on <code class="inlineCode">rpart.plot</code>, including additional examples of the types of decision tree diagrams the function can produce, refer to the author’s website at <a href="http://www.milbo.org/rpart-plot/"><span class="url">http://www.milbo.org/rpart-plot/</span></a>.</p>
    </div>
    <p class="normal">After<a id="_idIndexMarker777"/> installing the package using the <code class="inlineCode">install.packages("rpart.plot")</code> command, the <code class="inlineCode">rpart.plot()</code> function produces a tree diagram from any <code class="inlineCode">rpart</code> model object. The following commands plot the regression tree we built earlier:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>rpart.plot<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> rpart.plot<span class="hljs-punctuation">(</span>m.rpart<span class="hljs-punctuation">,</span> digits <span class="hljs-operator">=</span> <span class="hljs-number">3</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">The resulting tree diagram is as follows:</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_06_17.png"/></figure>
    <p class="packt_figref">Figure 6.17: A visualization of the wine quality regression tree model</p>
    <p class="normal">In addition to the <code class="inlineCode">digits</code> parameter, which controls the number of numeric digits to include in the diagram, many other aspects of the visualization can be adjusted. The following command shows just a few of the useful options:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> rpart.plot<span class="hljs-punctuation">(</span>m.rpart<span class="hljs-punctuation">,</span> digits <span class="hljs-operator">=</span> <span class="hljs-number">4</span><span class="hljs-punctuation">,</span> fallen.leaves <span class="hljs-operator">=</span> <span class="hljs-literal">TRUE</span><span class="hljs-punctuation">,</span>
               type <span class="hljs-operator">=</span> <span class="hljs-number">3</span><span class="hljs-punctuation">,</span> extra <span class="hljs-operator">=</span> <span class="hljs-number">101</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">The <code class="inlineCode">fallen.leaves</code> parameter forces the leaf nodes to be aligned at the bottom of the plot, while the <code class="inlineCode">type</code> and <code class="inlineCode">extra</code> parameters affect the way the decisions and nodes are labeled. The numbers <code class="inlineCode">3</code> and <code class="inlineCode">101</code> refer to specific style formats, which can be found in the command’s documentation or via experimentation with various numbers.</p>
    <p class="normal">The <a id="_idIndexMarker778"/>result of these changes is a very different-looking tree diagram:</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_06_18.png"/></figure>
    <p class="packt_figref">Figure 6.18: Changing the plot function parameters allows customization of the tree visualization</p>
    <p class="normal">Visualizations like these may assist with the dissemination of regression tree results, as they are readily understood even without a mathematics background. In both cases, the numbers shown in the leaf nodes are the predicted values for the examples reaching that node. Showing the diagram to the wine producers may thus help to identify the key factors involved in predicting higher-rated wines.</p>
    <h2 class="heading-2" id="_idParaDest-161">Step 4 – evaluating model performance</h2>
    <p class="normal">To use the<a id="_idIndexMarker779"/> regression tree model to make predictions on the test data, we use the <code class="inlineCode">predict()</code> function. By default, this returns the estimated numeric value for the outcome variable, which we’ll save in a vector named <code class="inlineCode">p.rpart</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> p.rpart <span class="hljs-operator">&lt;-</span> predict<span class="hljs-punctuation">(</span>m.rpart<span class="hljs-punctuation">,</span> wine_test<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">A quick look at the summary statistics of our predictions suggests a potential problem: the predictions fall into a much narrower range than the true values:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> summary<span class="hljs-punctuation">(</span>p.rpart<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  4.545   5.563   5.971   5.893   6.202   6.597
</code></pre>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> summary<span class="hljs-punctuation">(</span>wine_test<span class="hljs-operator">$</span>quality<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  3.000   5.000   6.000   5.901   6.000   9.000
</code></pre>
    <p class="normal">This<a id="_idIndexMarker780"/> finding suggests that the model is not correctly identifying the extreme cases, and in particular, the best and worst wines. On the other hand, between the first and third quartile, we may be doing well.</p>
    <p class="normal">The correlation between the predicted and actual quality values provides a simple way to gauge the model’s performance. Recall that the <code class="inlineCode">cor()</code> function can be used to measure the relationship between two equal-length vectors. We’ll use this to compare how well the predicted values correspond to the true values:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> cor<span class="hljs-punctuation">(</span>p.rpart<span class="hljs-punctuation">,</span> wine_test<span class="hljs-operator">$</span>quality<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.5369525
</code></pre>
    <p class="normal">A correlation of <code class="inlineCode">0.54</code> is certainly acceptable. However, the correlation only measures how strongly the predictions are related to the true value; it is not a measure of how far off the predictions were from the true values.</p>
    <h3 class="heading-3" id="_idParaDest-162">Measuring performance with the mean absolute error</h3>
    <p class="normal">Another <a id="_idIndexMarker781"/>way to think about the model’s performance is to consider how far, on average, its prediction was from the true value. This measurement is called the <strong class="keyWord">mean absolute error</strong> (<strong class="keyWord">MAE</strong>). </p>
    <p class="normal">The <a id="_idIndexMarker782"/>equation for MAE is as follows, where <em class="italic">n</em> indicates the number of predictions and <em class="italic">e</em><sub class="subscript-italic" style="font-style: italic;">i</sub> indicates the error for prediction <em class="italic">i</em>:</p>
    <p class="center"><img alt="" src="../Images/B17290_06_049.png"/></p>
    <p class="normal">As the name implies, this equation takes the mean of the absolute value of the errors. Since the error is just the difference between the predicted and actual values, we can create a simple <code class="inlineCode">MAE()</code> function as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> MAE <span class="hljs-operator">&lt;-</span> <span class="hljs-keyword">function</span><span class="hljs-punctuation">(</span>actual<span class="hljs-punctuation">,</span> predicted<span class="hljs-punctuation">)</span> <span class="hljs-punctuation">{</span>
    mean<span class="hljs-punctuation">(</span><span class="hljs-built_in">abs</span><span class="hljs-punctuation">(</span>actual <span class="hljs-operator">-</span> predicted<span class="hljs-punctuation">))</span>
<span class="hljs-punctuation">}</span>
</code></pre>
    <p class="normal">The MAE for our predictions is then:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> MAE<span class="hljs-punctuation">(</span>p.rpart<span class="hljs-punctuation">,</span> wine_test<span class="hljs-operator">$</span>quality<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.5872652
</code></pre>
    <p class="normal">This<a id="_idIndexMarker783"/> implies that, on average, the difference between our model’s predictions and the true quality score was about <code class="inlineCode">0.59</code>. On a quality scale from 0 to 10, this seems to suggest that our model is doing fairly well.</p>
    <p class="normal">On the other hand, recall that most wines were neither very good nor very bad; the typical quality score was around 5 to 6. Therefore, a classifier that did nothing but predict the mean value may also do fairly well according to this metric.</p>
    <p class="normal">The mean quality rating in the training data is as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> mean<span class="hljs-punctuation">(</span>wine_train<span class="hljs-operator">$</span>quality<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 5.870933
</code></pre>
    <p class="normal">If we predicted the value <code class="inlineCode">5.87</code> for every wine sample, we would have a MAE of only about <code class="inlineCode">0.67</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> MAE<span class="hljs-punctuation">(</span><span class="hljs-number">5.87</span><span class="hljs-punctuation">,</span> wine_test<span class="hljs-operator">$</span>quality<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.6722474
</code></pre>
    <p class="normal">Our regression tree (<em class="italic">MAE</em> = <em class="italic">0.59</em>) comes closer on average to the true quality score than the imputed mean (<em class="italic">MAE</em> = <em class="italic">0.67</em>), but not by much. In comparison, Cortez reported an MAE of 0.58 for the neural network model and an MAE of <code class="inlineCode">0.45</code> for the support vector machine. This suggests that there is room for improvement.</p>
    <h2 class="heading-2" id="_idParaDest-163">Step 5 – improving model performance</h2>
    <p class="normal">To improve <a id="_idIndexMarker784"/>the performance of our learner, let’s apply a model tree algorithm, which is a more complex application of trees to numeric prediction. Recall that a model tree extends regression trees by replacing the leaf nodes with regression models. This often results in more accurate results than regression trees, which use only a single numeric value for the prediction at the leaf nodes.</p>
    <p class="normal">The <a id="_idIndexMarker785"/>current state-of-the-art in model trees is <a id="_idIndexMarker786"/>the <strong class="keyWord">Cubist</strong> algorithm, which itself is an enhancement of the M5 model tree algorithm—both of which were published by J. R. Quinlan in the early 1990s. Though the implementation details are beyond the scope of this book, the Cubist algorithm involves building a decision tree, creating decision rules based on the branches of the tree, and building a regression model at each of the leaf nodes. Additional heuristics, such as pruning and boosting, are used to improve the quality of the predictions and smoothness across the range of predicted values.</p>
    <div class="note">
      <p class="normal">For more background on the Cubist and M5 algorithms, see <em class="italic">Learning With Continuous Classes, Quinlan, J. R., Proceedings of the 5th Australian Joint Conference on Artificial Intelligence, 1992, pp. 343-348. </em>Additionally, see<em class="italic"> Combining Instance-Based and Model-Based Learning, Quinlan, J. R., Proceedings of the Tenth International Conference on Machine Learning, 1993, pp. 236-243</em>.</p>
    </div>
    <p class="normal">The Cubist algorithm is available in R via the <code class="inlineCode">Cubist</code> package and the associated <code class="inlineCode">cubist()</code> function. The syntax of this function is shown in the following table:</p>
    <figure class="mediaobject"><img alt="Graphical user interface, text, application, email  Description automatically generated" src="../Images/B17290_06_19.png"/></figure>
    <p class="packt_figref">Figure 6.19: Model tree syntax</p>
    <p class="normal">We’ll fit<a id="_idIndexMarker787"/> the Cubist model tree using a slightly different syntax from what was used for the regression tree, as the <code class="inlineCode">cubist()</code> function does not accept the R formula syntax. Instead, we must specify the data frame columns used for the <code class="inlineCode">x</code> independent variables and the <code class="inlineCode">y</code> dependent variable. With the wine quality to be predicted residing in column 12, and using all other columns as predictors, the full command is as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>Cubist<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> m.cubist <span class="hljs-operator">&lt;-</span> cubist<span class="hljs-punctuation">(</span>x <span class="hljs-operator">=</span> wine_train<span class="hljs-punctuation">[</span><span class="hljs-operator">-</span><span class="hljs-number">12</span><span class="hljs-punctuation">],</span> y <span class="hljs-operator">=</span> wine_train<span class="hljs-operator">$</span>quality<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">Basic information about the model tree can be examined by typing its name:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> m.cubist
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Call:
cubist.default(x = wine_train[-12], y = wine_train$quality)
Number of samples: 3750
Number of predictors: 11
Number of committees: 1
Number of rules: 25
</code></pre>
    <p class="normal">In this <a id="_idIndexMarker788"/>output, we see that the algorithm generated 25 rules to model the wine quality. To examine some of these rules, we can apply the <code class="inlineCode">summary()</code> function to the model object. Since the complete tree is very large, only the first few lines of output depicting the first decision rule are included here:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> summary<span class="hljs-punctuation">(</span>m.cubist<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">  Rule 1: [21 cases, mean 5.0, range 4 to 6, est err 0.5]
   if
        free.sulfur.dioxide &gt; 30
        total.sulfur.dioxide &gt; 195
        total.sulfur.dioxide &lt;= 235
        sulphates &gt; 0.64
        alcohol &gt; 9.1
   then
        outcome = 573.6 + 0.0478 total.sulfur.dioxide
                  - 573 density - 0.788 alcohol
                  + 0.186 residual.sugar - 4.73 volatile.acidity
</code></pre>
    <p class="normal">You will note that the <code class="inlineCode">if</code> portion of the output is somewhat like the regression tree we built earlier. A series of decisions based on the wine properties of sulfur dioxide, sulphates, and alcohol creates a rule culminating in the final prediction. A key difference between this model tree output and the earlier regression tree output, however, is that the nodes here terminate not in a numeric prediction, but rather in a linear model.</p>
    <p class="normal">The linear model for this rule is shown in the <code class="inlineCode">then</code> output following the <code class="inlineCode">outcome =</code> statement. The numbers can be interpreted exactly the same as the multiple regression models we built earlier in this chapter. Each value is the estimated impact of the associated feature, that is, the net effect of one unit increase of that feature on the predicted wine quality. For example, the coefficient of 0.186 for residual sugar implies that for an increase of one unit of residual sugar, the wine quality rating is expected to increase by 0.186.</p>
    <p class="normal">It is important to note that the regression effects estimated by this model apply only to wine samples reaching this node; an examination of the entirety of the Cubist output reveals that a total of 25 linear models were built in this model tree, one for each decision rule, and each with different parameter estimates of the impact of residual sugar and the 10 other features.</p>
    <p class="normal">To <a id="_idIndexMarker789"/>examine the performance of this model, we’ll look at how well it performs on the unseen test data. The <code class="inlineCode">predict()</code> function gets us a vector of predicted values:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> p.cubist <span class="hljs-operator">&lt;-</span> predict<span class="hljs-punctuation">(</span>m.cubist<span class="hljs-punctuation">,</span> wine_test<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">The model tree appears to be predicting a wider range of values than the regression tree:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> summary<span class="hljs-punctuation">(</span>p.cubist<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  3.677   5.416   5.906   5.848   6.238   7.393
</code></pre>
    <p class="normal">The correlation also seems to be substantially higher:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> cor<span class="hljs-punctuation">(</span>p.cubist<span class="hljs-punctuation">,</span> wine_test<span class="hljs-operator">$</span>quality<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.6201015
</code></pre>
    <p class="normal">Furthermore, the model slightly reduced the MAE:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> MAE<span class="hljs-punctuation">(</span>wine_test<span class="hljs-operator">$</span>quality<span class="hljs-punctuation">,</span> p.cubist<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.5339725
</code></pre>
    <p class="normal">Although we did not improve a great deal beyond the regression tree, we surpassed the performance of the neural network model published by Cortez, and we are getting closer to the published MAE value of 0.45 for the support vector machine model, all while using a much simpler learning method.</p>
    <div class="packt_tip">
      <p class="normal">Not surprisingly, we have confirmed that predicting the quality of wines is a difficult problem; wine tasting, after all, is inherently subjective. If you would like additional practice, you may try revisiting this problem after reading <em class="chapterRef">Chapter 14</em>, <em class="italic">Building Better Learners</em>, which covers additional techniques that may lead to better results.</p>
    </div>
    <h1 class="heading-1" id="_idParaDest-164">Summary</h1>
    <p class="normal">In this chapter, we studied two methods for modeling numeric data. The first method, linear regression, involves fitting straight lines to data, but a technique called generalized linear modeling can adapt regression to other contexts as well. The second method uses decision trees for numeric prediction. The latter comes in two forms: regression trees, which use the average value of examples at leaf nodes to make numeric predictions, and model trees, which build a regression model at each leaf node in a hybrid approach that is, in some ways, the best of both worlds.</p>
    <p class="normal">We began to understand the utility of regression modeling by using it to investigate the causes of the Challenger space shuttle disaster. We then used linear regression modeling to calculate the expected insurance claims costs for various segments of automobile drivers. </p>
    <p class="normal">Because the relationship between the features and the target variable is well documented by the estimated regression model, we were able to identify certain demographics, such as high-mileage and late-night drivers, who may need to be charged higher insurance rates to cover their higher-than-average claims costs. We then applied logistic regression, a variant of regression used for binary classification, to the task of modeling insurance customer retention. These examples demonstrated the ability of regression to adapt flexibly to many types of real-world problems.</p>
    <p class="normal">In a somewhat less businesslike application of machine learning, regression trees and model trees were used to model the subjective quality of wines from measurable characteristics. In doing so, we learned how regression trees offer a simple way to explain the relationship between features and a numeric outcome, but the more complex model trees may be more accurate. Along the way, we learned new methods for evaluating the performance of numeric models.</p>
    <p class="normal">In stark contrast to this chapter, which covered machine learning methods that result in a clear understanding of the relationships between the input and the output, the next chapter covers methods that result in nearly incomprehensible models. The upside is that they are extremely powerful techniques—among the most powerful stock classifiers—which can be applied to both classification and numeric prediction problems.</p>
    <h1 class="heading-1" id="_idParaDest-165">Join our book’s Discord space</h1>
    <p class="normal">Join our Discord community to meet like-minded people and learn alongside more than 4000 people at:</p>
    <p class="normal"><a href="https://packt.link/r"><span class="url">https://packt.link/r</span></a></p>
    <p class="normal"><img alt="" src="../Images/r.jpg"/></p>
  </div>
</body></html>