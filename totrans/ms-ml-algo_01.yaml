- en: Machine Learning Model Fundamentals
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习模型基础
- en: Machine learning models are mathematical systems that share many common features.
    Even if, sometimes, they have been defined only from a theoretical viewpoint,
    research advancement allows us to apply several concepts to better understand
    the behavior of complex systems such as deep neural networks. In this chapter,
    we're going to introduce and discuss some fundamental elements that some skilled
    readers may already know, but that, at the same time, offer several possible interpretations
    and applications.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型是具有许多共同特征的数学系统。即使有时它们仅从理论观点定义，研究进展也使我们能够将几个概念应用于更好地理解复杂系统（如深度神经网络）的行为。在本章中，我们将介绍并讨论一些一些有经验的读者可能已经知道的基本元素，同时它们也提供了几种可能的解释和应用。
- en: 'In particular, in this chapter we''re discussing the main elements of:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是在本章中，我们讨论以下主要内容：
- en: Data-generating processes
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据生成过程
- en: Finite datasets
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有限数据集
- en: Training and test split strategies
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和测试集分割策略
- en: Cross-validation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交叉验证
- en: Capacity, bias, and variance of a model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的容量、偏差和方差
- en: Vapnik-Chervonenkis theory
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vapnik-Chervonenkis理论
- en: Cramér-Rao bound
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cramér-Rao界限
- en: Underfitting and overfitting
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欠拟合和过拟合
- en: Loss and cost functions
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失和成本函数
- en: Regularization
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化
- en: Models and data
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型和数据
- en: 'Machine learning algorithms work with data. They create associations, find
    out relationships, discover patterns, generate new samples, and more, working
    with well-defined datasets. Unfortunately, sometimes the assumptions or the conditions
    imposed on them are not clear, and a lengthy training process can result in a
    complete validation failure. Even if this condition is stronger in deep learning
    contexts, we can think of a model as a gray box (some transparency is guaranteed
    by the simplicity of many common algorithms), where a vectorial input ![](img/87da3ad2-7943-4a36-a94e-8af0883a59d2.png) is
    transformed into a vectorial output ![](img/a806507f-a7c6-41d8-95cd-609553341d11.png):'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法与数据一起工作。它们创建关联，找出关系，发现模式，生成新的样本，等等，它们与定义良好的数据集一起工作。不幸的是，有时对它们的假设或施加的条件并不明确，漫长的训练过程可能导致完全的验证失败。即使这种条件在深度学习环境中更为强烈，我们也可以将模型视为一个灰盒（许多常见算法的简单性保证了某些透明度），其中向量输入![图片](img/87da3ad2-7943-4a36-a94e-8af0883a59d2.png)被转换为向量输出![图片](img/a806507f-a7c6-41d8-95cd-609553341d11.png)：
- en: '![](img/d25b3444-8930-4e86-ae2e-46e3a2da5c0a.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d25b3444-8930-4e86-ae2e-46e3a2da5c0a.png)'
- en: Schema of a generic model parameterized with the vector θ
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 使用向量θ参数化的通用模型架构
- en: In the previous diagram, the model has been represented by a pseudo-function
    that depends on a set of parameters defined by the vector *θ*. In this section,
    we are only considering **parametric** models, although there's a family of algorithms
    that are called **non-parametric**, because they are based only on the structure
    of the data. We're going to discuss some of them in upcoming chapters.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，模型已被表示为一个依赖于由向量*θ*定义的一组参数的伪函数。在本节中，我们只考虑**参数化**模型，尽管有一系列算法被称为**非参数化**，因为它们仅基于数据的结构。我们将在接下来的章节中讨论其中的一些。
- en: The task of a parametric learning process is therefore to find the best parameter
    set that maximizes a target function whose value is proportional to the accuracy
    (or the error, if we are trying to minimize them) of the model given a specific
    input *X* and output *Y*. This definition is not very rigorous, and it will be
    improved in the following sections; however, it's useful as a way to understand
    the context we're working in.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，参数化学习过程的任务是要找到最佳参数集，以最大化目标函数的值，该值与给定特定输入*X*和输出*Y*的模型的准确性（或如果我们试图最小化它们，则为误差）成正比。这个定义并不非常严谨，将在以下章节中得到改进；然而，它作为理解我们工作环境的一种方式是有用的。
- en: 'Then, the first question to ask is: What is the nature of *X*? A machine learning
    problem is focused on learning abstract relationships that allow a consistent
    generalization when new samples are provided. More specifically, we can define
    a stochastic **data generating process** with an associated joint probability
    distribution:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，首先要问的问题是：*X*的本质是什么？机器学习问题专注于学习抽象关系，这些关系允许在提供新样本时进行一致的一般化。更具体地说，我们可以定义一个与联合概率分布相关的随机**数据生成过程**：
- en: '![](img/59525165-8c76-456b-af1a-2125fbd9e19d.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/59525165-8c76-456b-af1a-2125fbd9e19d.png)'
- en: Sometimes, it's useful to express the joint probability *p(x, y)* as a product
    of the conditional *p(y|x)*, which expresses the probability of a label given
    a sample, and the marginal probability of the samples *p(x)*. This expression
    is particularly useful when the prior probability *p(x)* is known in semi-supervised
    contexts, or when we are interested in solving problems using the **Expectation
    Maximization** (**EM**) algorithm. We're going to discuss this approach in upcoming
    chapters.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，将联合概率 *p(x, y)* 表示为条件概率 *p(y|x)* 的乘积是有用的，其中 *p(y|x)* 表示给定样本的标签概率，以及样本的边缘概率
    *p(x)*。这种表达式在半监督环境中已知先验概率 *p(x)* 时特别有用，或者当我们对使用 **期望最大化** (**EM**) 算法解决问题感兴趣时。我们将在接下来的章节中讨论这种方法。
- en: In many cases, we are not able to derive a precise distribution; however, when
    considering a dataset, we always assume that it's drawn from the original data-generating
    distribution. This condition isn't a purely theoretical assumption, because, as
    we're going to see, whenever our data points are drawn from different distributions,
    the accuracy of the model can dramatically decrease.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，我们无法推导出精确的分布；然而，当我们考虑数据集时，我们总是假设它来自原始数据生成分布。这个条件不是一个纯粹的理论假设，因为我们将会看到，当我们的数据点来自不同的分布时，模型的准确性可以显著降低。
- en: 'If we sample N **independent and identically distributed** (**i.i.d.**) values
    from *p[data]*, we can create a finite dataset *X* made up of *k*-dimensional
    real vectors:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从 *p[data]* 中采样 N 个 **独立同分布** (**i.i.d**) 的值，我们可以创建一个由 *k*-维实向量组成的有限数据集
    *X*：
- en: '![](img/c8318f7c-62e5-4071-982f-71a9ae0d688d.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c8318f7c-62e5-4071-982f-71a9ae0d688d.png)'
- en: 'In a supervised scenario, we also need the corresponding labels (with *t* output
    values):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督场景中，我们还需要相应的标签（具有 *t* 个输出值）：
- en: '![](img/e8002640-408e-4bc8-9833-24c97d03f68f.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e8002640-408e-4bc8-9833-24c97d03f68f.png)'
- en: 'When the output has more than two classes, there are different possible strategies
    to manage the problem. In classical machine learning, one of the most common approaches
    is **One-vs-All**, which is based on training *N* different binary classifiers
    where each label is evaluated against all the remaining ones. In this way, *N-1* is
    performed to determine the right class. With shallow and deep neural networks,
    instead, it''s preferable to use a **softmax** function to represent the output
    probability distribution for all classes:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当输出有超过两个类别时，有不同可能的策略来管理这个问题。在经典机器学习中，最常见的方法之一是 **One-vs-All**，它基于训练 *N* 个不同的二元分类器，其中每个标签都与所有剩余的标签进行比较。这样，*N-1*
    个分类器被用于确定正确的类别。相反，在浅层和深层神经网络中，更倾向于使用 **softmax** 函数来表示所有类别的输出概率分布：
- en: '![](img/261fd2dc-9e54-428c-8ce5-67fe2215b76e.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/261fd2dc-9e54-428c-8ce5-67fe2215b76e.png)'
- en: This kind of output (*z[i]* represents the intermediate values, and the sum
    of the terms is normalized to *1*) can be easily managed using the cross-entropy
    cost function (see the corresponding paragraph in the *Loss and cost functions*
    section).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这种输出（*z[i]* 表示中间值，项的总和归一化到 *1*）可以很容易地使用交叉熵损失函数来管理（参见 *损失和成本函数* 部分的相应段落）。
- en: Zero-centering and whitening
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 零中心化和白化
- en: 'Many algorithms show better performances (above all, in terms of training speed)
    when the dataset is symmetric (with a zero-mean). Therefore, one of the most important
    preprocessing steps is so-called **zero-centering**, which consists in subtracting
    the feature-wise mean *E[x][X]* from all samples:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 许多算法在数据集对称（具有零均值）时表现出更好的性能（特别是在训练速度方面）。因此，最重要的预处理步骤之一是所谓的 **零中心化**，它包括从所有样本中减去特征均值
    *E[x][X]*：
- en: '![](img/f19a895a-0cad-4bbc-b15f-7c52a2638ce3.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f19a895a-0cad-4bbc-b15f-7c52a2638ce3.png)'
- en: This operation, if necessary, is normally reversible, and doesn't alter relationships
    both among samples and among components of the same sample. In deep learning scenarios,
    a zero-centered dataset allows exploiting the symmetry of some activation function,
    driving to a faster convergence (we're going to discuss these details in the next
    chapters).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，这种操作通常是可逆的，并且不会改变样本之间以及同一样本成分之间的关系。在深度学习场景中，一个零中心的数据集允许利用某些激活函数的对称性，从而加速收敛（我们将在下一章中讨论这些细节）。
- en: 'Another very important preprocessing step is called **whitening**, which is
    the operation of imposing an identity covariance matrix to a zero-centered dataset:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个非常重要的预处理步骤被称为 **白化**，它是对零中心数据集施加一个单位协方差矩阵的操作：
- en: '![](img/991b3e6d-7b6a-4361-92bb-8c2c3109369c.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/991b3e6d-7b6a-4361-92bb-8c2c3109369c.png)'
- en: 'As the covariance matrix *E[x][X^TX]* is real and symmetric, it''s possible
    to eigendecompose it without the need to invert the eigenvector matrix:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 由于协方差矩阵 *E[x][X^TX]* 是实对称的，因此可以对其进行特征分解，而无需求逆特征向量矩阵：
- en: '![](img/e8ed1207-a036-4a11-a809-c009cfe887b9.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e8ed1207-a036-4a11-a809-c009cfe887b9.png)'
- en: 'The matrix *V* contains the eigenvectors (as columns), and the diagonal matrix *Ω*
    contains the eigenvalues. To solve the problem, we need to find a matrix *A,*
    such that:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 *V* 包含特征向量（作为列），对角矩阵 *Ω* 包含特征值。为了解决这个问题，我们需要找到一个矩阵 *A*，使得：
- en: '![](img/dccb1256-8529-4988-a046-e19a5f415b1b.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/dccb1256-8529-4988-a046-e19a5f415b1b.png)'
- en: 'Using the eigendecomposition previously computed, we get:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 使用之前计算的特征分解，我们得到：
- en: '![](img/cff39904-0475-4866-a9a2-2239b7914843.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cff39904-0475-4866-a9a2-2239b7914843.png)'
- en: 'Hence, the matrix *A* is:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，矩阵 *A* 是：
- en: '![](img/0ee7bf5c-1e51-4b78-ab6b-1c745f68d0b9.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0ee7bf5c-1e51-4b78-ab6b-1c745f68d0b9.png)'
- en: 'One of the main advantages of whitening is the decorrelation of the dataset,
    which allows an easier separation of the components. Furthermore, if *X* is whitened,
    any orthogonal transformation induced by the matrix *P* is also whitened:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 白化的一大优点是数据集的去相关性，这允许更容易地分离成分。此外，如果 *X* 被白化，由矩阵 *P* 诱导的任何正交变换也会被白化：
- en: '![](img/622c081f-b106-4d0c-bbef-7ce9240d4da7.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/622c081f-b106-4d0c-bbef-7ce9240d4da7.png)'
- en: Moreover, many algorithms that need to estimate parameters that are strictly
    related to the input covariance matrix can benefit from this condition, because
    it reduces the actual number of independent variables (in general, these algorithms
    work with matrices that become symmetric after applying the whitening). Another
    important advantage in the field of deep learning is that the gradients are often
    higher around the origin, and decrease in those areas where the activation functions
    (for example, the hyperbolic tangent or the sigmoid) saturate *(|x| → ∞)*. That's
    why the convergence is generally faster for whitened (and zero-centered) datasets.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，许多需要估计与输入协方差矩阵严格相关的参数的算法可以从这种条件中受益，因为它减少了实际独立变量的数量（通常，这些算法使用白化后变得对称的矩阵）。在深度学习领域的一个重要优势是梯度通常在原点附近更高，而在激活函数（例如，双曲正切或Sigmoid）饱和的区域（|x|
    → ∞）减小。这就是为什么白化（和零均值化）数据集的收敛通常更快。
- en: 'In the following graph, it''s possible to compare an **original dataset**,
    **zero-centering**, and **whitening**:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图表中，可以比较**原始数据集**、**零均值化**和**白化**：
- en: '![](img/8e2be8a1-6ba2-4eab-aa40-ec1a0b1960d7.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8e2be8a1-6ba2-4eab-aa40-ec1a0b1960d7.png)'
- en: Original dataset (left), centered version (center), whitened version (right)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据集（左），中心化版本（中心），白化版本（右）
- en: When a whitening is needed, it's important to consider some important details.
    The first one is that there's a scale difference between the real sample covariance
    and the estimation *X^TX,* often adopted with the **singular value decomposition**
    (**SVD**). The second one concerns some common classes implemented by many frameworks,
    like Scikit-Learn's `StandardScaler`. In fact, while zero-centering is a feature-wise
    operation, a whitening filter needs to be computed considering the whole covariance
    matrix (`StandardScaler` implements only unit variance, feature-wise scaling).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要白化时，考虑一些重要细节是很重要的。首先，真实样本协方差和估计 *X^TX* 之间存在尺度差异，通常采用**奇异值分解**（**SVD**）。第二个方面涉及许多框架实现的一些常见类别，如Scikit-Learn的`StandardScaler`。实际上，虽然零均值化是特征级别的操作，但白化滤波器需要考虑整个协方差矩阵（`StandardScaler`仅实现单位方差，特征级别的缩放）。
- en: 'Luckily, all Scikit-Learn algorithms that benefit from or need a whitening
    preprocessing step provide a built-in feature, so no further actions are normally
    required; however, for all readers who want to implement some algorithms directly,
    I''ve written two Python functions that can be used both for zero-centering and
    whitening. They assume a matrix *X* with a shape (*N[Samples] × n*). Moreover,
    the `whiten()` function accepts the parameter `correct`, which allows us to apply
    the scaling correction (the default value is `True`):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，所有从Scikit-Learn算法中受益或需要白化预处理步骤的算法都提供内置功能，因此通常不需要进一步操作；然而，对于所有希望直接实现一些算法的读者，我已经编写了两个Python函数，可以用于零均值化和白化。它们假设一个形状为
    (*N[Samples] × n*) 的矩阵 *X*。此外，`whiten()` 函数接受参数 `correct`，允许我们应用缩放校正（默认值为 `True`）：
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Training and validation sets
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练集和验证集
- en: 'In real problems, the number of samples is limited, and it''s usually necessary
    to split the initial set *X* (together with *Y*) into two subsets as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际问题中，样本数量是有限的，通常有必要将初始集*X*（连同*Y*）分成两个子集，如下所示：
- en: '**Training set** used to train the model'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练集**用于训练模型'
- en: '**Validation set** used to assess the score of the model without any bias,
    with samples never seen before'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证集**用于评估模型分数，而不带任何偏差，使用从未见过的样本'
- en: 'According to the nature of the problem, it''s possible to choose a split percentage
    ratio of 70% – 30% (a good practice in machine learning, where the datasets are
    relatively small), or a higher training percentage (80%, 90%, up to 99%) for deep
    learning tasks where the number of samples is very high. In both cases, we are
    assuming that the training set contains all the information required for a consistent
    generalization. In many simple cases, this is true and can be easily verified;
    but with more complex datasets, the problem becomes harder. Even if we think to
    draw all the samples from the same distribution, it can happen that a randomly
    selected test set contains features that are not present in other training samples.
    Such a condition can have a very negative impact on global accuracy and, without
    other methods, it can also be very difficult to identify. This is one of the reasons
    why, in deep learning, training sets are huge: considering the complexity of the
    features and structure of the data generating distributions, choosing large test
    sets can limit the possibility of learning particular associations.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 根据问题的性质，可以选择70% - 30%的分割百分比（在机器学习中，这是一个好的实践，因为数据集相对较小），或者对于样本数量非常高的深度学习任务，可以选择更高的训练百分比（80%，90%，甚至高达99%）。在两种情况下，我们假设训练集包含进行一致泛化所需的所有信息。在许多简单的情况下，这是真的，并且可以很容易地验证；但面对更复杂的数据集，问题就变得更加困难。即使我们认为从相同的分布中抽取所有样本，也可能发生随机选择的测试集包含其他训练样本中不存在的特征。这种条件可能会对全局准确度产生非常负面的影响，而且如果没有其他方法，也可能非常难以识别。这就是为什么在深度学习中，训练集通常很大：考虑到特征和生成数据分布的复杂性，选择大的测试集可以限制学习特定关联的可能性。
- en: 'In Scikit-Learn, it''s possible to split the original dataset using the **`train_test_split()`**
    function, which allows specifying the train/test size, and if we expect to have
    randomly shuffled sets (default). For example, if we want to split `X` and `Y`,
    with 70% training and 30% test, we can use:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scikit-Learn中，可以使用**`train_test_split()`**函数来分割原始数据集，该函数允许指定训练/测试大小，并且如果我们期望有随机打乱的数据集（默认）。例如，如果我们想将`X`和`Y`分割为70%的训练和30%的测试，我们可以使用：
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Shuffling the sets is always a good practice, in order to reduce the correlation
    between samples. In fact, we have assumed that `X` is made up of i.i.d samples,
    but several times two subsequent samples have a strong correlation, reducing the
    training performance. In some cases, it''s also useful to re-shuffle the training
    set after each training epoch; however, in the majority of our examples, we are
    going to work with the same shuffled dataset throughout the whole process. Shuffling
    has to be avoided when working with sequences and models with memory: in all those
    cases, we need to exploit the existing correlation to determine how the future
    samples are distributed.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 打乱数据集始终是一个好的实践，目的是减少样本之间的相关性。实际上，我们假设`X`由独立同分布（i.i.d）的样本组成，但有时连续两个样本之间会有很强的相关性，这会降低训练性能。在某些情况下，在每次训练周期后重新打乱训练集也是有用的；然而，在大多数我们的例子中，我们将在整个过程中使用相同打乱的数据集。当处理序列和具有记忆的模型时，必须避免打乱：在这些所有情况下，我们需要利用现有的相关性来确定未来样本的分布。
- en: When working with NumPy and Scikit-Learn, it's always a good practice to set
    the random seed to a constant value, so as to allow other people to reproduce
    the experiment with the same initial conditions. This can be achieved by calling `np.random.seed`(...)
    and using the `random-state` parameter present in many Scikit-Learn methods.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用NumPy和Scikit-Learn时，始终将随机种子设置为常数是一个好的实践，这样其他人就可以使用相同的初始条件重现实验。这可以通过调用`np.random.seed`(...)并使用许多Scikit-Learn方法中存在的`random-state`参数来实现。
- en: Cross-validation
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉验证
- en: A valid method to detect the problem of wrongly selected test sets is provided
    by the **cross-validation** technique. In particular, we're going to use the **K-Fold**
    cross-validation approach. The idea is to split the whole dataset *X* into a moving
    test set and a training set (the remaining part). The size of the test set is
    determined by the number of folds so that, during *k* iterations, the test set
    covers the whole original dataset.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一种有效的方法来检测错误选择的测试集问题是由**交叉验证**技术提供的。特别是，我们将使用**K折**交叉验证方法。想法是将整个数据集 *X* 分割为一个移动的测试集和一个训练集（剩余的部分）。测试集的大小由折数决定，以便在
    *k* 次迭代中，测试集覆盖整个原始数据集。
- en: 'In the following diagram, we see a schematic representation of the process:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图中，我们可以看到该过程的示意图：
- en: '![](img/612f3964-c562-4515-8f33-0cd32d825f0e.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/612f3964-c562-4515-8f33-0cd32d825f0e.png)'
- en: K-Fold cross-validation schema
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: K折交叉验证方案
- en: 'In this way, it''s possible to assess the accuracy of the model using different
    sampling splits, and the training process can be performed on larger datasets;
    in particular, on *(k-1)*N* samples. In an ideal scenario, the accuracy should
    be very similar in all iterations; but in most real cases, the accuracy is quite
    below average. This means that the training set has been built excluding samples
    that contain features necessary to let the model fit the separating hypersurface
    considering the real *p[data]*. We''re going to discuss these problems later in
    this chapter; however, if the standard deviation of the accuracies is too high
    (a threshold must be set according to the nature of the problem/model), that probably
    means that *X* hasn''t been drawn uniformly from *p[data],* and it''s useful to
    evaluate the impact of the outliers in a preprocessing stage. In the following
    graph, we see the plot of a 15-fold cross-validation performed on a logistic regression:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，就可以使用不同的采样分割来评估模型的准确率，并且可以在更大的数据集上进行训练过程；特别是，在 *(k-1)*N* 样本上。在理想情况下，准确率应该在所有迭代中非常相似；但在大多数实际情况下，准确率相当低于平均水平。这意味着训练集在构建时排除了包含必要特征以使模型适应考虑实际
    *p[data]* 的分离超平面的样本。我们将在本章后面讨论这些问题；然而，如果准确率的标准差过高（必须根据问题的性质/模型设置一个阈值），这可能意味着 *X*
    没有从 *p[data]* 中均匀抽取，并且在预处理阶段评估异常值的影响是有用的。在下面的图中，我们可以看到在逻辑回归上进行的15折交叉验证的图表：
- en: '![](img/27cd7180-d499-4e47-a364-33f20542a1b8.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/27cd7180-d499-4e47-a364-33f20542a1b8.png)'
- en: Cross-validation accuracies
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证准确率
- en: The values oscillate from 0.84 to 0.95, with an average (solid horizontal line)
    of 0.91\. In this particular case, considering the initial purpose was to use
    a linear classifier, we can say that all folds yield high accuracies, confirming
    that the dataset is linearly separable; however, there are some samples (excluded
    in the ninth fold) that are necessary to achieve a minimum accuracy of about 0.88.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 值在0.84到0.95之间波动，平均值为0.91（实线水平线）。在这种情况下，考虑到最初的目的本是使用线性分类器，我们可以说所有折都产生了高准确率，证实了数据集是线性可分的；然而，有一些样本（在第九折中被排除）对于达到大约0.88的最小准确率是必要的。
- en: '**K-Fold** cross-validation has different variants that can be employed to
    solve specific problems:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**K折**交叉验证有不同的变体，可以用来解决特定问题：'
- en: '**Stratified K-Fold**: A **Standard K-Fold** approach splits the dataset without
    considering the probability distribution *p(y|x)*, therefore some folds may theoretically
    contain only a limited number of labels. Stratified K-Fold, instead, tries to
    split *X* so that all the labels are equally represented.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分层K折**：标准的K折方法在分割数据集时不考虑概率分布 *p(y|x)*，因此某些折可能理论上只包含有限数量的标签。分层K折则试图分割 *X*，使得所有标签都得到均匀的表示。'
- en: '**Leave-one-out** (**LOO**): This approach is the most drastic because it creates
    *N* folds, each of them containing *N-1* training samples and only 1 test sample.
    In this way, the maximum possible number of samples is used for training, and
    it''s quite easy to detect whether the algorithm is able to learn with sufficient
    accuracy, or if it''s better to adopt another strategy. The main drawback of this
    method is that *N* models must be trained, and when *N* is very large this can
    cause a performance issue. Moreover, with a large number of samples, the probability
    that two random values are similar increases, therefore many folds will yield
    almost identical results. At the same time, LOO limits the possibilities for assessing
    the generalization ability, because a single test sample is not enough for a reasonable
    estimation.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Leave-one-out**（**LOO**）：这种方法是最激进的，因为它创建了*N*个折，每个折包含*N-1*个训练样本和1个测试样本。这样，就使用了最大可能数量的样本进行训练，并且很容易检测算法是否能够以足够的准确率学习，或者是否应该采用另一种策略。这种方法的主要缺点是必须训练*N*个模型，当*N*非常大时，这可能会引起性能问题。此外，在大量样本的情况下，两个随机值相似的概率增加，因此许多折会产生几乎相同的结果。同时，LOO限制了评估泛化能力的可能性，因为单个测试样本不足以进行合理的估计。'
- en: '**Leave-P-out** (**LPO**): In this case, the number of test samples is set
    to *p* (non-disjoint sets), so the number of folds is equal to the binomial coefficient
    of *n* over *p*. This approach mitigates LOO''s drawbacks, and it''s a trade-off
    between K-Fold and LOO. The number of folds can be very high, but it''s possible
    to control it by adjusting the number *p* of test samples; however, if *p* isn''t
    small or big enough, the binomial coefficient can *explode*. In fact, when *p*
    has about *n/2* samples, the number of folds is maximal:'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Leave-P-out**（**LPO**）：在这种情况下，测试样本的数量被设置为*p*（非不相交集合），因此折数等于*n*除以*p*的二项式系数。这种方法减轻了LOO的缺点，并且是K-Fold和LOO之间的权衡。折数可能非常高，但可以通过调整测试样本数量*p*来控制；然而，如果*p*不是足够小或足够大，二项式系数可能会**爆炸**。实际上，当*p*有大约*n/2*个样本时，折数达到最大值：'
- en: '![](img/429214fd-f1af-4070-8f50-4d4ff051c3a8.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/429214fd-f1af-4070-8f50-4d4ff051c3a8.png)'
- en: 'Scikit-Learn implements all those methods (with some other variations), but
    I suggest always using the `cross_val_score()` function, which is a helper that
    allows applying the different methods to a specific problem. In the following
    snippet based on a polynomial **Support Vector Machine** (**SVM**) and the MNIST
    digits dataset, the function is applied specifying the number of folds (parameter
    `cv`). In this way, Scikit-Learn will automatically use Stratified K-Fold for
    categorical classifications, and **Standard K-Fold** for all other cases:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn实现了所有这些方法（以及一些其他变化），但我建议始终使用`cross_val_score()`函数，这是一个辅助函数，允许将不同的方法应用于特定问题。在以下基于多项式**支持向量机**（**SVM**）和MNIST数字数据集的代码片段中，指定了折数（参数`cv`）。这样，Scikit-Learn将自动使用分层K-Fold进行分类，而对于所有其他情况则使用**标准K-Fold**：
- en: '[PRE2]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The accuracy is very high (> 0.9) in every fold, therefore we expect to have
    even higher accuracy using the LOO method. As we have 1,797 samples, we expect
    the same number of accuracies:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个折中，准确率都非常高（> 0.9），因此我们预计使用LOO方法会有更高的准确率。由于我们有1,797个样本，我们预计会有相同数量的准确率：
- en: '[PRE3]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As expected, the average score is very high, but there are still samples that
    are misclassified. As we're going to discuss, this situation could be a potential
    candidate for overfitting, meaning that the model is learning perfectly how to
    map the training set, but it's losing its ability to generalize; however, LOO
    is not a good method to measure this model ability, due to the size of the validation
    set.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，平均分数非常高，但仍有一些样本被错误分类。正如我们将要讨论的，这种情况可能是过度拟合的潜在候选者，这意味着模型完美地学习了如何映射训练集，但它失去了泛化的能力；然而，由于验证集的大小，LOO并不是衡量这种模型能力的好方法。
- en: 'We can now evaluate our algorithm with the LPO technique. Considering what was
    explained before, we have selected the smaller Iris dataset and a classification
    based on a logistic regression. As there are *N=150* samples, choosing `p = 3`,
    we get 551,300 folds:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用LPO技术评估我们的算法。考虑到之前所解释的内容，我们选择了较小的Iris数据集和基于逻辑回归的分类。由于有*N=150*个样本，选择`p
    = 3`，我们得到551,300个折：
- en: '[PRE4]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As in the previous example, we have printed only the first 100 accuracies; however,
    the global trend can be immediately understood with only a few values.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如前例所示，我们只打印了前100个准确率；然而，只需几个值就可以立即了解全局趋势。
- en: The cross-validation technique is a powerful tool that is particularly useful
    when the performance cost is not too high. Unfortunately, it's not the best choice
    for deep learning models, where the datasets are very large and the training processes
    can take even days to complete. However, as we're going to discuss, in those cases
    the right choice (the split percentage), together with an accurate analysis of
    the datasets and the employment of techniques such as normalization and regularization,
    allows fitting models that show an excellent generalization ability.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证技术是一种强大的工具，当性能成本不是太高时尤其有用。不幸的是，它并不是深度学习模型的最佳选择，因为数据集非常大，训练过程可能需要甚至几天才能完成。然而，正如我们将要讨论的，在这些情况下，正确的选择（分割百分比），结合对数据集的准确分析以及采用标准化和正则化等技术，可以使模型展现出卓越的泛化能力。
- en: Features of a machine learning model
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习模型的特点
- en: In this section, we're going to consider supervised models, and try to determine
    how it's possible to measure their theoretical potential accuracy and their ability
    to generalize correctly over all possible samples drawn from *p[data]*. The majority
    of these concepts were developed before the *deep learning age*, but continue
    to have an enormous influence on research projects. The idea of *capacity*, for
    example, is an open-ended question that neuroscientists keep on asking themselves
    about the human brain. Modern deep learning models with dozens of layers and millions
    of parameters reopened the theoretical question from a mathematical viewpoint.
    Together with this, other elements, like the limits for the variance of an estimator,
    again attracted the limelight because the algorithms are becoming more and more
    powerful, and performances that once were considered far from any feasible solution
    are now a reality. Being able to train a model, so as to exploit its full capacity,
    maximize its generalization ability, and increase the accuracy, overcoming even
    human performances, is what a deep learning engineer nowadays has to expect from
    his work.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将考虑监督模型，并试图确定如何测量它们的理论潜在准确性和它们在从 *p[data]* 中抽取的所有可能样本上正确泛化的能力。这些概念中的大多数都是在深度学习时代之前开发的，但继续对研究项目产生巨大影响。例如，“容量”的想法是神经科学家不断问自己的关于人脑的开放性问题。具有数十层和数百万参数的现代深度学习模型从数学角度重新开启了理论问题。与此相关，其他元素，如估计量方差的限制，再次成为焦点，因为算法变得越来越强大，曾经被认为远非可行解决方案的性能现在已成为现实。能够训练一个模型，以便充分利用其容量，最大化其泛化能力，并提高准确性，甚至超越人类表现，是深度学习工程师现在必须从他的工作中期待的东西。
- en: Capacity of a model
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型的容量
- en: 'If we consider a supervised model as a set of parameterized functions, we can
    define **representational capacity** as the intrinsic ability of a certain generic
    function to map a relatively large number of data distributions. To understand
    this concept, let''s consider a function *f(x)* that admits infinite derivatives,
    and rewrite it as a Taylor expansion:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将监督模型视为一组参数化函数，我们可以将**表示能力**定义为某种通用函数映射相对大量数据分布的内在能力。为了理解这个概念，让我们考虑一个允许无限导数的函数
    *f(x)*，并将其重写为泰勒展开式：
- en: '![](img/c398523d-8a07-40b9-808e-75bcf7d8ce2a.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c398523d-8a07-40b9-808e-75bcf7d8ce2a.png)'
- en: 'We can decide to take only the first *n* terms, so to have an *n*-degree polynomial
    function. Consider a simple bi-dimensional scenario with six functions (starting
    from a linear one); we can observe the different behavior with a small set of
    data points:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择只取前 *n* 项，以便得到一个 *n* 次多项式函数。考虑一个简单的二维场景，有六个函数（从线性函数开始）；我们可以通过一组小数据点观察它们的不同行为：
- en: '![](img/35abc6b6-9950-4d97-928d-4e5c0e9188a0.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/35abc6b6-9950-4d97-928d-4e5c0e9188a0.png)'
- en: Different behavior produced by six polynomial separating curves
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 六条多项式分离曲线产生的不同行为
- en: The ability to rapidly change the curvature is proportional to the degree. If
    we choose a linear classifier, we can only modify its slope (the example is always
    in a bi-dimensional space) and the intercept. Instead, if we pick a higher-degree
    function, we have more possibilities to *bend* the curvature when it's necessary.
    If we consider **n=1** and **n=2** in the plot (on the top-right, they are the
    first and the second functions), with **n=1**, we can include the dot corresponding
    to *x=11*, but this choice has a negative impact on the dot at *x=5*.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 快速改变曲率的能力与度数成正比。如果我们选择一个线性分类器，我们只能修改其斜率（例子总是在二维空间中）和截距。相反，如果我们选择一个更高阶的函数，当需要时，我们有更多可能性来*弯曲*曲率。如果我们考虑图中的**n=1**和**n=2**（在右上角，它们是第一个和第二个函数），对于**n=1**，我们可以包括对应*x=11*的点，但这种选择对*x=5*处的点有负面影响。
- en: Only a parameterized non-linear function can solve this problem efficiently,
    because this simple problem requires a representational capacity higher than the
    one provided by linear classifiers. Another classical example is the XOR function.
    For a long time, several researchers opposed perceptrons (linear neural networks),
    because they weren't able to classify a dataset generated by the XOR function.
    Fortunately, the introduction of multilayer perceptrons, with non-linear functions,
    allowed us to overcome this problem, and many whose complexity is beyond the possibilities
    of any classic machine learning model.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 只有一个参数化的非线性函数才能有效地解决这个问题，因为这个问题需要比线性分类器提供的表示能力更高的表示能力。另一个经典的例子是XOR函数。长期以来，许多研究人员反对感知器（线性神经网络），因为它们无法对由XOR函数生成的数据集进行分类。幸运的是，多层感知器的引入，以及非线性函数的使用，使我们能够克服这个问题，并且许多复杂度超出了任何经典机器学习模型的可能性。
- en: Vapnik-Chervonenkis capacity
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Vapnik-Chervonenkis容量
- en: 'A mathematical formalization of the capacity of a classifier is provided by
    the **Vapnik-Chervonenkis theory**. To introduce the definition, it''s first necessary
    to define the concept of **shattering**. If we have a class of sets *C* and a
    set *M*, we say that *C* shatters *M* if:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**Vapnik-Chervonenkis理论**为分类器的容量提供了一个数学形式化。为了引入定义，首先需要定义**分割**的概念。如果我们有一个集合类*C*和一个集合*M*，我们说*C*分割*M*，如果：'
- en: '![](img/80c863ff-90ee-47a2-b994-702c455c4256.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图表](img/80c863ff-90ee-47a2-b994-702c455c4256.png)'
- en: 'In other words, given any subset of *M*, it can be obtained as the intersection
    of a particular instance of *C (c[j])* and *M* itself. If we now consider a model
    as a parameterized function:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，对于任何*M*的子集，它都可以作为*C (c[j])*的一个特定实例和*M*本身的交集来获得。如果我们现在将一个模型视为一个参数化函数：
- en: '![](img/cca58a91-5886-45c6-8e00-c6851c414995.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图表](img/cca58a91-5886-45c6-8e00-c6851c414995.png)'
- en: 'We want to determine its capacity in relation to a finite dataset *X*:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想确定它与有限数据集*X*的容量关系：
- en: '![](img/043d5bff-1238-4688-bfe2-af3f015df786.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图表](img/043d5bff-1238-4688-bfe2-af3f015df786.png)'
- en: According to the Vapnik-Chervonenkis theory, we can say that the model *f* shatters
    *X* if there are no classification errors for every possible label assignment.
    Therefore, we can define the **Vapnik-Chervonenkis-capacity** or **VC-capacity**
    (sometimes called **VC-dimension**) as the maximum cardinality of a subset of
    *X* so that *f* can shatter it.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Vapnik-Chervonenkis理论，我们可以说模型*f*分割*X*，如果对于每个可能的标签分配都没有分类错误。因此，我们可以定义**Vapnik-Chervonenkis容量**或**VC容量**（有时称为**VC维数**）为能够分割*X*的*X*的子集的最大基数。
- en: 'For example, if we consider a linear classifier in a bi-dimensional space,
    the VC-capacity is equal to 3, because it''s always possible to label three samples
    so that *f* shatters them; however, it''s impossible to do it in all situations
    where *N > 3*. The XOR problem is an example that needs a VC-capacity higher than
    *3*. Let''s explore the following plot:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们考虑一个在二维空间中的线性分类器，其VC容量等于3，因为总是有可能标记三个样本，使得*f*将它们分割；然而，在*N > 3*的所有情况下，这是不可能的。XOR问题是一个需要VC容量高于*3*的例子。让我们探索以下图表：
- en: '![](img/7bae2238-7baf-40be-ba8d-bdd5bf6e5c78.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图表](img/7bae2238-7baf-40be-ba8d-bdd5bf6e5c78.png)'
- en: XOR problem with different separating curves
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 不同分割曲线的XOR问题
- en: This particular label choice makes the set non-linearly separable. The only
    way to overcome this problem is to use higher-order functions (or non-linear ones).
    The curve lines (belonging to a classifier whose VC-capacity is greater than *3*)
    can separate both the upper-left and the lower-right regions from the remaining
    space, but no straight line can do the same (while it can always separate one
    point from the other three).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这种特定的标签选择使得集合不可线性分离。克服这个问题的唯一方法就是使用高阶函数（或非线性函数）。曲线线（属于一个 VC 容量大于 *3* 的分类器）可以分离上左和下右区域与剩余空间，但没有直线可以做到这一点（尽管它总是可以分离一个点与其他三个点）。
- en: Bias of an estimator
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 估计器的偏差
- en: 'Let''s now consider a parameterized model with a single vectorial parameter
    (this isn''t a limitation, but only a didactic choice):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑一个具有单个向量参数的参数化模型（这并不是一个限制，而只是一个教学选择）：
- en: '![](img/265f15b6-0886-46fa-a7a7-52bce47a74bf.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图表](img/265f15b6-0886-46fa-a7a7-52bce47a74bf.png)'
- en: 'The goal of a learning process is to estimate the parameter *θ* so as, for
    example, to maximize the accuracy of a classification. We define the **bias of
    an estimator** (in relation to a parameter *θ*):'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 学习过程的目标是估计参数 *θ*，以便例如最大化分类的准确度。我们定义估计器的 **偏差**（与参数 *θ* 相关）：
- en: '![](img/401cedb3-a8cc-4622-beaa-8cc42a42960c.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![高阶函数图](img/401cedb3-a8cc-4622-beaa-8cc42a42960c.png)'
- en: In other words, the bias is the difference between the expected value of the
    estimation and the real parameter value. Remember that the estimation is a function
    of *X*, and cannot be considered a constant in the sum.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，偏差是估计的期望值与真实参数值之间的差异。记住，估计是 *X* 的函数，不能在求和中被视为常数。
- en: 'An estimator is said to be **unbiased **if:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个估计器被称为 **无偏**，那么：
- en: '![](img/cd879fa7-6777-4f37-b6f8-8848fec1b9ea.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![欠拟合图](img/cd879fa7-6777-4f37-b6f8-8848fec1b9ea.png)'
- en: 'Moreover, the estimator is defined as **consistent** if the sequence of estimations
    converges (at least with probability 1) to the real value when *k → ∞*:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果估计序列收敛（至少以概率 1）到真实值当 *k → ∞* 时，则估计器被定义为 **一致**的：
- en: '![](img/6882f82d-093d-4424-9e3e-7f64a94caf69.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![偏差图](img/6882f82d-093d-4424-9e3e-7f64a94caf69.png)'
- en: Given a dataset *X* whose samples are drawn from *p[data]*, the accuracy of
    an estimator is inversely proportional to its bias. Low-bias (or unbiased) estimators
    are able to map the dataset *X* with high-precision levels, while high-bias estimators
    are very likely to have a capacity that isn't high enough for the problem to solve,
    and therefore their ability to detect the whole dynamic is poor.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个数据集 *X*，其样本是从 *p[data]* 中抽取的，估计器的准确性与它的偏差成反比。低偏差（或无偏差）的估计器能够以高精度水平映射数据集
    *X*，而高偏差估计器很可能没有足够的能力来解决该问题，因此它们检测整个动态的能力较差。
- en: 'Let''s now compute the derivative of the bias with respect to the vector *θ*
    (it will be useful later):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算偏差相对于向量 *θ* 的导数（它将在以后很有用）：
- en: '![](img/73459af7-d774-4d7b-a8d3-0890d6b020b6.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![欠拟合图](img/73459af7-d774-4d7b-a8d3-0890d6b020b6.png)'
- en: 'Consider that the last equation, thanks to the linearity of *E[•]*, holds also
    if we add a term that doesn''t depend on *x* to the estimation of *θ*. In fact,
    in line with the laws of probability, it''s easy to verify that:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到最后一个方程，由于 *E[•]* 的线性，即使我们在 *θ* 的估计中添加一个不依赖于 *x* 的项，该方程也成立。实际上，根据概率定律，很容易验证：
- en: '![](img/463b0abc-e0c6-4f5f-9427-394e2ec5033a.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![欠拟合图](img/463b0abc-e0c6-4f5f-9427-394e2ec5033a.png)'
- en: Underfitting
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 欠拟合
- en: 'A model with a high bias is likely to underfit the training set. Let''s consider
    the scenario shown in the following graph:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差大的模型很可能会欠拟合训练集。让我们考虑以下图表所示的情景：
- en: '![](img/08db0bc9-de16-4408-85f6-50eb3777211b.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![高阶函数图](img/08db0bc9-de16-4408-85f6-50eb3777211b.png)'
- en: 'Underfitted classifier: The curve cannot separate correctly the two classes'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 欠拟合的分类器：曲线无法正确区分两个类别
- en: Even if the problem is very hard, we could try to adopt a linear model and,
    at the end of the training process, the slope and the intercept of the separating
    line are about -1 and 0 (as shown in the plot); however, if we measure the accuracy,
    we discover that it's close to 0! Independently from the number of iterations,
    this model will never be able to learn the association between *X* and *Y*. This
    condition is called **underfitting**, and its major indicator is a very low training
    accuracy. Even if some data preprocessing steps can improve the accuracy, when
    a model is underfitted, the only valid solution is to adopt a higher-capacity
    model.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 即使问题非常困难，我们也可以尝试采用线性模型，在训练过程的最后，分离线的斜率和截距大约是-1和0（如图所示）；然而，如果我们测量准确率，我们会发现它接近0！无论迭代次数多少，这个模型永远无法学习到*X*和*Y*之间的关联。这种条件被称为**欠拟合**，其主要指标是训练准确率非常低。即使某些数据预处理步骤可以提高准确率，当模型欠拟合时，唯一有效的解决方案是采用更高容量的模型。
- en: 'In a machine learning task, our goal is to achieve the maximum accuracy, starting
    from the training set and then moving on to the validation set. More formally,
    we can say that we want to improve our models so to get as close as possible to **Bayes
    accuracy**. This is not a well-defined value, but a theoretical upper limit that
    is possible to achieve using an estimator. In the following diagram, we see a
    representation of this process:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习任务中，我们的目标是实现最大的准确率，从训练集开始，然后过渡到验证集。更正式地说，我们希望改进我们的模型，以便尽可能接近**贝叶斯准确率**。这不是一个明确定义的值，而是一个理论上可能通过估计量实现的极限。在下面的图中，我们可以看到这个过程的表现：
- en: '![](img/4b30abbf-a779-4669-8059-b85bd2c8cc22.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4b30abbf-a779-4669-8059-b85bd2c8cc22.png)'
- en: Accuracy level diagram
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率水平图
- en: Bayes accuracy is often a purely theoretical limit and, for many tasks, it's
    almost impossible to achieve using even biological systems; however, advancements
    in the field of deep learning allow to create models that have a target accuracy
    slightly below the Bayes one. In general, there's no closed form for determining
    the Bayes accuracy, therefore human abilities are considered as a benchmark. In
    the previous classification example, a human being is immediately able to distinguish
    among different dot classes, but the problem can be very hard for a limited-capacity
    classifier. Some of the models we're going to discuss can solve this problem with
    a very high target accuracy, but at this point, we run another risk that can be
    understood after defining the concept of variance of an estimator.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯准确率通常是一个纯粹的理论极限，对于许多任务来说，即使使用生物系统也几乎不可能实现；然而，深度学习领域的进步允许创建目标准确率略低于贝叶斯准确率的模型。一般来说，贝叶斯准确率没有封闭形式，因此人类能力被视为基准。在先前的分类示例中，人类能够立即区分不同的点类，但对于一个容量有限的分类器来说，这个问题可能非常困难。我们将讨论的一些模型可以通过非常高的目标准确率解决这个问题，但在这个阶段，我们面临另一个风险，这可以在定义估计量方差的概念后理解。
- en: Variance of an estimator
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 估计量方差
- en: 'At the beginning of this chapter, we have defined the data generating process
    *p[data]*, and we have assumed that our dataset *X* has been drawn from this distribution;
    however, we don''t want to learn existing relationships limited to *X*, but we
    expect our model to be able to generalize correctly to any other subset drawn
    from *p[data]*. A good measure of this ability is provided by the **variance of
    the estimator**:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的开头，我们定义了数据生成过程*p[data]*，并假设我们的数据集*X*是从这个分布中抽取的；然而，我们不想学习仅限于*X*的现有关系，而是期望我们的模型能够正确泛化到从*p[data]*中抽取的任何其他子集。衡量这种能力的好方法是**估计量的方差**：
- en: '![](img/94edc141-6802-4678-85fc-f90776cce7e7.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/94edc141-6802-4678-85fc-f90776cce7e7.png)'
- en: The variance can be also defined as the square of the standard error (analogously
    to the standard deviation). A high variance implies dramatic changes in the accuracy
    when new subsets are selected, because the model has probably reached a very high
    training accuracy through an over-learning of a limited set of relationships,
    and it has almost completely lost its ability to generalize.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 方差也可以定义为标准误差的平方（类似于标准差）。高方差意味着当选择新的子集时，准确率会有很大的变化，因为模型可能通过过度学习一组有限的关系而达到了非常高的训练准确率，并且几乎完全失去了泛化的能力。
- en: Overfitting
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过拟合
- en: 'If underfitting was the consequence of a low capacity and a high bias, **overfitting**
    is a phenomenon that a high variance can detect. In general, we can observe a
    very high training accuracy (even close to the Bayes level), but not a poor validation
    accuracy. This means that the capacity of the model is high enough or even excessive
    for the task (the higher the capacity, the higher the probability of large variances),
    and that the training set isn''t a good representation of *p[data]*. To understand
    the problem, consider the following classification scenarios:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果欠拟合是低能力和高偏差的结果，**过拟合**是一种高方差可以检测的现象。一般来说，我们可以观察到非常高的训练准确率（甚至接近贝叶斯水平），但验证准确率并不差。这意味着模型的能力对于任务来说足够高，甚至过高（能力越高，大方差的可能性越高），并且训练集并不是*p[data]*的良好表示。为了理解这个问题，考虑以下分类场景：
- en: '![](img/5fb37d6e-4e42-4234-9d3a-252c435db94b.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5fb37d6e-4e42-4234-9d3a-252c435db94b.png)'
- en: Acceptable fitting (left), overfitted classifier (right)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 可接受的拟合（左侧），过拟合的分类器（右侧）
- en: The left plot has been obtained using logistic regression, while, for the right
    one, the algorithm is SVM with a sixth-degree polynomial kernel. If we consider
    the second model, the decision boundaries seem much more precise, with some samples
    just over them. Considering the shapes of the two subsets, it would be possible
    to say that a non-linear SVM can better capture the dynamics; however, if we sample
    another dataset from *p[data]* and the diagonal *tail* becomes wider, logistic
    regression continues to classify the points correctly, while the SVM accuracy
    decreases dramatically. The second model is very likely to be overfitted, and
    some corrections are necessary. When the validation accuracy is much lower than
    the training one, a good strategy is to increase the number of training samples
    to consider the real *p[data]*. In fact, it can happen that a training set is
    built starting from a hypothetical distribution that doesn't reflect the real
    one; or the number of samples used for the validation is too high, reducing the
    amount of information carried by the remaining samples. Cross-validation is a
    good way to assess the quality of datasets, but it can always happen that we find
    completely new subsets (for example, generated when the application is deployed
    in a production environment) that are misclassified, even if they were supposed
    to belong to *p[data]*. If it's not possible to enlarge the training set, data
    augmentation could be a valid solution, because it allows creating artificial
    samples (for images, it's possible to mirror, rotate, or blur them) starting from
    the information stored in the known ones. Other strategies to prevent overfitting
    are based on a technique called **regularization**, which we're going to discuss
    in the last part of this chapter. For now, we can say that the effect of regularization
    is similar to a partial linearization, which implies a capacity reduction with
    a consequent variance decrease.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的图是通过逻辑回归得到的，而右侧的图使用的是具有六次多项式核的SVM算法。如果我们考虑第二个模型，决策边界看起来要精确得多，有些样本刚好在边界之上。考虑到两个子集的形状，可以说非线性SVM能更好地捕捉动态变化；然而，如果我们从*p[data]*中采样另一个数据集，并且对角线*尾部*变宽，逻辑回归仍然能够正确分类点，而SVM的准确度会大幅下降。第二个模型很可能过拟合，需要进行一些修正。当验证准确率远低于训练准确率时，一个好的策略是增加训练样本的数量，以考虑真实的*p[data]*。实际上，可能会发生这样的情况：训练集是从一个假设的分布中构建的，这个分布并不反映真实情况；或者用于验证的样本数量过高，减少了剩余样本所携带的信息量。交叉验证是评估数据集质量的好方法，但总是有可能发现完全新的子集（例如，当应用程序在生产环境中部署时生成的），即使它们本应属于*p[data]*。如果无法扩大训练集，数据增强可能是一个有效的解决方案，因为它允许从已知信息中创建人工样本（对于图像，可以镜像、旋转或模糊它们）。防止过拟合的其他策略基于一种称为**正则化**的技术，我们将在本章的最后部分讨论。现在，我们可以这样说，正则化的效果类似于部分线性化，这意味着能力降低，随之而来的是方差减少。
- en: The Cramér-Rao bound
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 克拉美罗界
- en: 'If it''s theoretically possible to create an unbiased model (even asymptotically),
    this is not true for variance. To understand this concept, it''s necessary to
    introduce an important definition: the **Fisher information**. If we have a parameterized
    model and a data-generating process *p[data]*, we can define the likelihood function by
    considering the following parameters:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在理论上可以创建一个无偏模型（即使是渐近的），那么对于方差来说这并不成立。为了理解这个概念，有必要引入一个重要的定义：**费舍尔信息**。如果我们有一个参数化的模型和一个数据生成过程
    *p[data]*，我们可以通过考虑以下参数来定义似然函数：
- en: '![](img/b00ac51d-ebc5-471b-a1b4-5fd56ccc391d.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b00ac51d-ebc5-471b-a1b4-5fd56ccc391d.png)'
- en: 'This function allows measuring how well the model describes the original data
    generating process. The shape of the likelihood can vary substantially, from well-defined,
    peaked curves, to almost flat surfaces. Let''s consider the following graph, showing
    two examples based on a single parameter:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数可以用来衡量模型描述原始数据生成过程的好坏。似然的形状可能会有很大的变化，从定义良好、尖锐的曲线到几乎平坦的表面。让我们考虑以下图表，展示了基于单个参数的两个例子：
- en: '![](img/df9b41c2-7a3d-4f4a-812a-7caf3f276f38.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/df9b41c2-7a3d-4f4a-812a-7caf3f276f38.png)'
- en: Very peaked likelihood (left), flatter likelihood (right)
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 非常尖锐的似然（左），较平的似然（右）
- en: 'We can immediately understand that, in the first case, the maximum likelihood
    can be easily reached by gradient ascent, because the surface is very peaked.
    In the second case, instead, the gradient magnitude is smaller, and it''s rather
    easy to stop before reaching the actual maximum because of numerical imprecisions
    or tolerances. In worst cases, the surface can be almost flat in very large regions,
    with a corresponding gradient close to zero. Of course, we''d like to always work
    with very sharp and peaked likelihood functions, because they carry more information
    about their maximum. More formally, the Fisher information quantifies this value.
    For a single parameter, it is defined as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以立即理解，在第一种情况下，通过梯度上升可以很容易地达到最大似然，因为表面非常尖锐。而在第二种情况下，梯度的大小较小，由于数值不精确或容差，很容易在达到实际最大值之前停止。在最坏的情况下，表面可以在非常大的区域内几乎平坦，相应的梯度接近零。当然，我们希望始终使用非常尖锐和尖锐的似然函数，因为它们携带更多关于其最大值的信息。更正式地说，费舍尔信息量化了这个值。对于单个参数，它被定义为以下：
- en: '![](img/6d13c559-8c9f-4b50-922f-8c930c542e4e.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6d13c559-8c9f-4b50-922f-8c930c542e4e.png)'
- en: The Fisher information is an unbounded non-negative number that is proportional
    to the amount of information carried by the log-likelihood; the use of logarithm
    has no impact on the gradient ascent, but it simplifies complex expressions by
    turning products into sums. This value can be interpreted as the *speed* of the
    gradient when the function is reaching the maximum; therefore, higher values imply
    better approximations, while a hypothetical value of zero means that the probability
    to determine the right parameter estimation is also null.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 费舍尔信息是一个无界的非负数，它与对数似然所携带的信息量成正比；使用对数对梯度上升没有影响，但它通过将乘积转换为和来简化复杂表达式。这个值可以解释为当函数达到最大值时梯度的*速度*；因此，更高的值意味着更好的近似，而一个假设的零值意味着确定正确参数估计的概率也是零。
- en: 'When working with a set of *K* parameters, the Fisher information becomes a
    positive semidefinite matrix:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 当与一组 *K* 个参数一起工作时，费舍尔信息成为一个正半定矩阵：
- en: '![](img/031c41a3-23a9-443d-92c3-00cf3942a56b.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/031c41a3-23a9-443d-92c3-00cf3942a56b.png)'
- en: 'This matrix is symmetric, and also has another important property: when a value
    is zero, it means that the corresponding couple of parameters are orthogonal for
    the purpose of the maximum likelihood estimation, and they can be considered separately.
    In many real cases, if a value is close to zero, it determines a very low correlation
    between parameters and, even if it''s not mathematically rigorous, it''s possible
    to decouple them anyway.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这个矩阵是对称的，并且还有一个重要的特性：当一个值为零时，这意味着对应的参数对在最大似然估计中是正交的，并且可以单独考虑。在许多实际情况下，如果一个值接近零，它决定了参数之间非常低的关联性，即使它不是数学上严格的，仍然可以解耦它们。
- en: 'At this point, it''s possible to introduce the **Cramér-Rao bound**, which
    states that for every unbiased estimator that adopts *x* (with probability distribution
    *p(x; θ)*) as a measure set, the variance of any estimator of *θ* is always lower-bounded
    according to the following inequality:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，可以引入**克拉美罗界**，它表明对于采用*x*（具有概率分布*p(x; θ)*）作为测量集的任何无偏估计量，任何*θ*估计量的方差总是根据以下不等式有下界：
- en: '![](img/f4feb22e-34c0-4ef6-8387-0ce820e04e61.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f4feb22e-34c0-4ef6-8387-0ce820e04e61.png)'
- en: 'In fact, considering initially a generic estimator and exploiting Cauchy-Schwarz
    inequality with the variance and the Fisher information (which are both expressed
    as expected values), we obtain:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，考虑最初一个通用的估计量，并利用柯西-施瓦茨不等式与方差和费舍尔信息（两者都表示为期望值），我们得到：
- en: '![](img/d41065f1-a330-4809-9a87-42a4f6dc3140.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d41065f1-a330-4809-9a87-42a4f6dc3140.png)'
- en: 'Now, if we use the expression for derivatives of the bias with respect to *θ*,
    considering that the expected value of the estimation of *θ* doesn''t depend on
    *x*, we can rewrite the right side of the inequality as:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们使用关于*θ*的偏导数的表达式，考虑到*θ*的估计值的期望值不依赖于*x*，我们可以将不等式的右侧重写为：
- en: '![](img/32bc0088-4bba-47f1-8330-2ba98612280d.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/32bc0088-4bba-47f1-8330-2ba98612280d.png)'
- en: 'If the estimator is unbiased, the derivative on the right side is equal to
    zero, therefore, we get:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果估计量是无偏的，则右侧的导数等于零，因此，我们得到：
- en: '![](img/6c3948c3-6c9e-4c21-87c3-ab9ed67f26bc.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6c3948c3-6c9e-4c21-87c3-ab9ed67f26bc.png)'
- en: 'In other words, we can try to reduce the variance, but it will be always lower-bounded
    by the inverse Fisher information. Therefore, given a dataset and a model, there''s
    always a limit to the ability to generalize. In some cases, this measure is easy
    to determine; however, its real value is theoretical, because it provides the
    likelihood function with another fundamental property: it carries all the information
    needed to estimate the worst case for variance. This is not surprising: when we
    discussed the capacity of a model, we saw how different functions could drive
    to higher or lower accuracies. If the training accuracy is high enough, this means
    that the capacity is appropriate or even excessive for the problem; however, we
    haven''t considered the role of the likelihood *p(X| θ)*.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们可以尝试减少方差，但它总是由费舍尔信息的倒数有下界。因此，给定一个数据集和一个模型，总是存在泛化能力的极限。在某些情况下，这个度量很容易确定；然而，它的真实值是理论上的，因为它为似然函数提供了另一个基本属性：它携带了估计方差最坏情况所需的所有信息。这并不令人惊讶：当我们讨论模型的容量时，我们看到了不同的函数如何导致更高的或更低的精度。如果训练精度足够高，这意味着容量对于问题来说是适当的，甚至过多；然而，我们还没有考虑似然*p(X|
    θ)*的作用。
- en: 'High-capacity models, in particular, with small or low-informative datasets,
    can drive to flat likelihood surfaces with a higher probability than lower-capacity
    models. Therefore, the Fisher information tends to become smaller, because there
    are more and more parameter sets that yield similar probabilities, and this, at
    the end of the day, drives to higher variances and an increased risk of overfitting.
    To conclude this section, it''s useful to consider a general empirical rule derived
    from the **Occam''s razor** principle: whenever a simpler model can explain a
    phenomenon with enough accuracy, it doesn''t make sense to increase its capacity.
    A simpler model is always preferable (when the performance is good and it represents
    accurately the specific problem), because it''s normally faster both in the training
    and in the inference phases, and more efficient. When talking about deep neural
    networks, this principle can be applied in a more precise way, because it''s easier
    to increase or decrease the number of layers and neurons until the desired accuracy
    has been achieved.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 高容量模型，特别是当数据集小或信息量低时，比低容量模型更有可能驱动到平坦似然表面。因此，费舍尔信息往往会变得较小，因为越来越多的参数集会产生相似的似然值，而这最终会导致更高的方差和过拟合的风险增加。为了总结本节，考虑从**奥卡姆剃刀**原则推导出的一个一般经验法则是有用的：当更简单的模型能够足够准确地解释一个现象时，增加其容量是没有意义的。一个更简单的模型总是更可取（当性能良好且能准确代表特定问题时），因为它在训练和推理阶段通常都更快，也更高效。当谈到深度神经网络时，这个原则可以以更精确的方式应用，因为更容易增加或减少层数和神经元数量，直到达到所需的精度。
- en: Loss and cost functions
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失和成本函数
- en: 'At the beginning of this chapter, we discussed the concept of generic target
    function so as to optimize in order to solve a machine learning problem. More
    formally, in a supervised scenario, where we have finite datasets *X* and *Y*:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的开头，我们讨论了通用目标函数的概念，以便优化以解决机器学习问题。更正式地说，在一个监督场景中，我们拥有有限的数据集 *X* 和 *Y*：
- en: '![](img/288f4481-7e25-4442-b623-076e744dd043.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/288f4481-7e25-4442-b623-076e744dd043.png)'
- en: '![](img/51650bd3-6e1d-4955-b417-a9474554c3df.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/51650bd3-6e1d-4955-b417-a9474554c3df.png)'
- en: 'We can define the generic **loss function** for a single sample as:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以为单个样本定义一个通用的**损失函数**：
- en: '![](img/c6ed2c5c-3bba-46b1-959a-de58d115ce0f.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c6ed2c5c-3bba-46b1-959a-de58d115ce0f.png)'
- en: '*J* is a function of the whole parameter set, and must be proportional to the
    error between the true label and the predicted. Another important property is
    convexity. In many real cases, this is an almost impossible condition; however,
    it''s always useful to look for convex loss functions, because they can be easily
    optimized through the gradient descent method. We''re going to discuss this topic
    in [Chapter 9](cb8a2a42-13fc-40ed-b6f0-56e4843284d7.xhtml), *Neural Networks for
    Machine Learning*. However, for now, it''s useful to consider a loss function
    as an intermediate between our training process and a pure mathematical optimization.
    The missing link is the complete data. As already discussed, *X* is drawn from
    *p[data]*, so it should represent the true distribution. Therefore, when minimizing
    the loss function, we''re considering a potential subset of points, and never
    the whole real dataset. In many cases, this isn''t a limitation, because, if the
    bias is null and the variance is small enough, the resulting model will show a
    good generalization ability (high training and validation accuracy); however,
    considering the data generating process, it''s useful to introduce another measure
    called **expected risk**:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '*J* 是整个参数集的函数，必须与真实标签和预测之间的误差成正比。另一个重要属性是凸性。在许多实际情况下，这是一个几乎不可能的条件；然而，寻找凸损失函数总是有用的，因为它们可以通过梯度下降法轻松优化。我们将在[第9章](cb8a2a42-13fc-40ed-b6f0-56e4843284d7.xhtml)，“用于机器学习的神经网络”中讨论这个话题。然而，现在，将损失函数视为训练过程和纯数学优化之间的中介是有用的。缺失的环节是完整的数据。正如已经讨论过的，*X*
    是从 *p[data]* 中抽取的，因此它应该代表真实分布。因此，当最小化损失函数时，我们正在考虑一个潜在的点子集，而不是整个真实数据集。在许多情况下，这并不是一个限制，因为，如果偏差为零且方差足够小，所得到的模型将显示出良好的泛化能力（高训练和验证准确率）；然而，考虑到数据生成过程，引入另一个称为**预期风险**的度量是有用的：'
- en: '![](img/c98b2f7f-894d-4bfe-9245-71acd5e0351f.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c98b2f7f-894d-4bfe-9245-71acd5e0351f.png)'
- en: 'This value can be interpreted as an average of the loss function over all possible
    samples drawn from *p*[*data*.] Minimizing the expected risk implies the maximization
    of the global accuracy. When working with a finite number of training samples,
    instead, it''s common to define a **cost function **(often called a loss function
    as well, and not to be confused with the log-likelihood):'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这个值可以解释为从 *p*[*data*] 中抽取的所有可能样本的损失函数的平均值。最小化预期风险意味着全局准确性的最大化。相反，当使用有限数量的训练样本时，通常定义一个**成本函数**（通常也称为损失函数，不要与对数似然混淆）：
- en: '![](img/f105b89b-e2d5-41c6-9f18-029d2ce04ab4.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f105b89b-e2d5-41c6-9f18-029d2ce04ab4.png)'
- en: 'This is the actual function that we''re going to minimize and, divided by the
    number of samples (a factor that doesn''t have any impact), it''s also called
    **empirical risk**, because it''s an approximation (based on real data) of the
    expected risk. In other words, we want to find a set of parameters so that:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们要最小化的实际函数，除以样本数（一个没有影响的因素），它也称为**经验风险**，因为它是对预期风险的一个近似（基于真实数据）。换句话说，我们想要找到一组参数，使得：
- en: '![](img/66c92f36-b45f-4a28-a69c-f969de7bc3d5.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/66c92f36-b45f-4a28-a69c-f969de7bc3d5.png)'
- en: 'When the cost function has more than two parameters, it''s very difficult and
    perhaps even impossible to understand its internal structure; however, we can
    analyze some potential conditions using a bidimensional diagram:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 当成本函数有超过两个参数时，理解其内部结构非常困难，甚至可能不可能；然而，我们可以使用二维图分析一些潜在条件：
- en: '![](img/88baf2eb-e5aa-4562-b4c1-786facba4717.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/88baf2eb-e5aa-4562-b4c1-786facba4717.png)'
- en: Different kinds of points in a bidimensional scenario
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在二维场景中不同类型的点
- en: 'The different situations we can observe are:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到的不同情况是：
- en: The **starting point**, where the cost function is usually very high due to
    the error.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**起点**，由于误差，通常成本函数在这里非常高。'
- en: '**Local minima**, where the gradient is null (and the second derivative is
    positive). They are candidates for the optimal parameter set, but unfortunately,
    if the concavity isn''t too deep, an inertial movement or some noise can easily
    move the point away.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**局部最小值**，梯度为零（且二阶导数为正）。它们是最佳参数集的候选者，但不幸的是，如果凹度不够深，惯性运动或一些噪声可以轻易地将点移开。'
- en: '**Ridges** (or **local maxima**), where the gradient is null, and the second
    derivative is negative. They are unstable points, because a minimum perturbation
    allows escaping, reaching lower-cost areas.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**脊**（或**局部最大值**），梯度为零，二阶导数为负。它们是不稳定点，因为最小的扰动允许逃离，达到更低成本的区域。'
- en: '**Plateaus**, or the region where the surface is almost flat and the gradient
    is close to zero. The only way to escape a plateau is to keep a residual kinetic
    energy—we''re going to discuss this concept when talking about neural optimization
    algorithms ([Chapter 9](cb8a2a42-13fc-40ed-b6f0-56e4843284d7.xhtml), *Neural Networks
    for Machine Learning*).'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平台**，或表面几乎平坦且梯度接近零的区域。逃离平台唯一的方法是保持残余动能——我们将在讨论神经优化算法时介绍这个概念（[第9章](cb8a2a42-13fc-40ed-b6f0-56e4843284d7.xhtml)，*Neural
    Networks for Machine Learning*）。'
- en: '**Global minimum**, the point we want to reach to optimize the cost function.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全局最小值**，我们想要达到以优化成本函数的点。'
- en: 'Even if local minima are likely when the number of parameters is small, they
    become very unlikely when the model has a large number of parameters. In fact,
    an *n*-dimensional point *θ^** is a local minimum for a convex function (and here,
    we''re assuming *L* to be convex) only if:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 即使当参数数量较小时，局部最小值很可能是存在的，但当模型具有大量参数时，它们变得非常不可能。事实上，一个 *n*-维点 *θ^** 对于一个凸函数（这里我们假设
    *L* 是凸的）来说是一个局部最小值，仅当：
- en: '![](img/db82f2e0-e5fb-403d-9566-4e5a594a518e.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/db82f2e0-e5fb-403d-9566-4e5a594a518e.png)'
- en: 'The second condition imposes a positive semi-definite Hessian matrix (equivalently,
    all principal minors *H[n]* made with the first *n* rows and *n* columns must
    be non-negative), therefore all itseigenvalues *λ[0], λ[1], ..., λ[N]* must be
    non-negative. This probability decreases with the number of parameters (*H* is
    a *n×n* square matrix and has *n* eigenvalues), and becomes close to zero in deep
    learning models where the number of weights can be in the order of 10,000,000
    (or even more). The reader interested in a complete mathematical proof can read
    *High Dimensional Spaces*, *Deep Learning and Adversarial Examples*,*Dube S.,
    arXiv:1801.00634 [cs.CV]*. As a consequence, a more common condition to consider
    is instead the presence of **saddle points**, where the eigenvalues have different
    signs and the orthogonal directional derivatives are null, even if the points
    are neither local maxima nor minima. Consider, for example, the following plot:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个条件要求海森矩阵是正半定的（等价地，由前n行和n列构成的所有主子矩阵 *H[n]* 必须是非负的），因此所有特征值 *λ[0]，λ[1]，...，λ[N]*
    必须是非负的。这个概率随着参数数量的增加而降低（*H* 是一个 *n×n* 的方阵，并且有 *n* 个特征值），在深度学习模型中，权重的数量可以达到1000万（甚至更多）时，这个概率接近于零。对完整数学证明感兴趣的读者可以阅读
    *High Dimensional Spaces*，*Deep Learning and Adversarial Examples*，*Dube S., arXiv:1801.00634
    [cs.CV]*。因此，一个更常见的条件是考虑**鞍点**的存在，其中特征值具有不同的符号，正交方向导数为零，即使这些点既不是局部最大值也不是局部最小值。例如，考虑以下图形：
- en: '![](img/7fa93bda-0cb6-45ee-af55-2f70f817120b.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7fa93bda-0cb6-45ee-af55-2f70f817120b.png)'
- en: Saddle point in a bidimensional scenario
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 二维场景中的鞍点
- en: 'The function is *y=x3* whose first and second derivatives are *y''=3x2* and
    *y''''=6x*. Therefore, *y''(0)=y''''(0)=0*. In this case (single-valued function),
    this point is also called a **point of inflection**, because at *x=0,* the function
    shows a change in the concavity. In three dimensions, it''s easier to understand
    why a saddle point has been called in this way. Consider, for example, the following
    plot:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 函数是 *y=x3*，其第一和二阶导数分别是 *y'=3x2* 和 *y''=6x*。因此，*y'(0)=y''(0)=0*。在这种情况下（单值函数），这个点也被称为**拐点**，因为在
    *x=0* 时，函数显示出凹性的变化。在三维空间中，更容易理解为什么鞍点被这样称呼。例如，考虑以下图形：
- en: '![](img/6c4185f6-5242-468e-901b-d2fde6dd3193.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6c4185f6-5242-468e-901b-d2fde6dd3193.png)'
- en: Saddle point in a three-dimensional scenario
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 三维场景中的鞍点
- en: The surface is very similar to a horse saddle, and if we project the point on
    an orthogonal plane, *XZ* is a minimum, while on another plane (*YZ*) it is a
    maximum. Saddle points are quite dangerous, because many simpler optimization
    algorithms can slow down and even stop, losing the ability to find the right direction.
    In [Chapter 9](cb8a2a42-13fc-40ed-b6f0-56e4843284d7.xhtml), *Neural Networks for
    Machine Learning*, we're going to discuss some methods that are able to mitigate
    this kind of problem, allowing deep models to converge.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 表面非常类似于马鞍，如果我们把点投影到一个正交平面上，*XZ*是一个最小值，而在另一个平面上（*YZ*）它是一个最大值。鞍点相当危险，因为许多更简单的优化算法可能会减慢甚至停止，失去找到正确方向的能力。在[第9章](cb8a2a42-13fc-40ed-b6f0-56e4843284d7.xhtml)，《机器学习的神经网络》中，我们将讨论一些能够减轻这种问题的方法，允许深度模型收敛。
- en: Examples of cost functions
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 成本函数的例子
- en: In this section, we expose some common **cost functions** that are employed
    in both classification and regression tasks. Some of them will be extensively
    adopted in our examples in the next chapters, particularly when discussing training
    processes in shallow and deep neural networks.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了在分类和回归任务中常用的某些**成本函数**。其中一些将在下一章的示例中广泛采用，尤其是在讨论浅层和深层神经网络中的训练过程时。
- en: Mean squared error
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 均方误差
- en: '**Mean squared error** is one of the most common regression cost functions.
    Its generic expression is:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '**均方误差**是最常见的回归成本函数之一。其通用表达式是：'
- en: '![](img/a427b719-280a-41a6-94b7-b83646e36208.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a427b719-280a-41a6-94b7-b83646e36208.png)'
- en: This function is differentiable at every point of its domain and it's convex,
    so it can be optimized using the **stochastic gradient descent** (**SGD**) algorithm;
    however, there's a drawback when employed in regressions where there are outliers.
    As its value is always quadratic when the distance between the prediction and
    the actual value (corresponding to an outlier) is large, the relative error is
    high, and this can lead to an unacceptable correction.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数在其定义域的每个点上都是可微的，并且是凸函数，因此可以使用**随机梯度下降（SGD**）算法进行优化；然而，当在存在异常值的情况下用于回归时，存在一个缺点。因为当预测值与实际值（对应于异常值）之间的距离很大时，其值总是二次的，相对误差很高，这可能导致无法接受的校正。
- en: Huber cost function
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Huber成本函数
- en: As explained, mean squared error isn't robust to outliers, because it's always
    quadratic independently of the distance between actual value and prediction. To
    overcome this problem, it's possible to employ the **H****uber** **cost function**,
    which is based on threshold *t[H]*, so that for distances less than *t[H]*, its
    behavior is quadratic, while for a distance greater than *t*[*H*,] it becomes
    linear, reducing the entity of the error and, therefore, the relative importance
    of the outliers.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如所述，均方误差对异常值不稳健，因为它总是二次的，而不管实际值与预测值之间的距离如何。为了克服这个问题，可以采用基于阈值 *t[H]* 的**Huber**成本函数，这样对于小于
    *t[H]* 的距离，其行为是二次的，而对于大于 *t*[*H*,] 的距离，它变为线性，减少了误差的大小，因此，减少了异常值的相对重要性。
- en: 'The analytical expression is:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 分析表达式是：
- en: '![](img/e437919e-1f6f-4eb2-9110-e896fe57eede.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e437919e-1f6f-4eb2-9110-e896fe57eede.png)'
- en: Hinge cost function
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hinge成本函数
- en: 'This cost function is adopted by SVM, where the goal is to maximize the distance
    between the separation boundaries (where the support vector lies). It''s analytic
    expression is:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这个成本函数被SVM采用，其目标是最大化分离边界（支持向量所在的位置）之间的距离。其分析表达式是：
- en: '![](img/68006894-0de8-4ef9-8ba5-ae93b4bc5123.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/68006894-0de8-4ef9-8ba5-ae93b4bc5123.png)'
- en: 'Contrary to the other examples, this cost function is not optimized using classic
    stochastic gradient descent methods, because it''s not differentiable at all points
    where:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他示例不同，这个成本函数不是使用经典随机梯度下降方法进行优化的，因为它在所有点上的不可微性：
- en: '![](img/dee44504-4715-4432-9ec6-018f66643b6e.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/dee44504-4715-4432-9ec6-018f66643b6e.png)'
- en: For this reason, SVM algorithms are optimized using quadratic programming techniques.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，SVM算法使用二次规划技术进行优化。
- en: Categorical cross-entropy
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**类别交叉熵**'
- en: '**Categorical cross-entropy** is the most diffused classification cost function,
    adopted by logistic regression and the majority of neural architectures. The generic
    analytical expression is:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**类别交叉熵**是最常见的分类成本函数，被逻辑回归和大多数神经网络架构采用。其通用分析表达式是：'
- en: '![](img/dfd97220-2510-4838-b413-094a8458eca9.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/dfd97220-2510-4838-b413-094a8458eca9.png)'
- en: 'This cost function is convex and can be easily optimized using stochastic gradient
    descent techniques; moreover, it has another important interpretation. If we are
    training a classifier, our goal is to create a model whose distribution is as
    similar as possible to *pdata*. This condition can be achieved by minimizing the
    Kullback-Leibler divergence between the two distributions:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这个损失函数是凸的，并且可以使用随机梯度下降技术轻松优化；此外，它还有一个重要的解释。如果我们正在训练一个分类器，我们的目标是创建一个分布尽可能接近 *pdata*
    的模型。这个条件可以通过最小化两个分布之间的 Kullback-Leibler 散度来实现：
- en: '![](img/90dd1a5a-55d3-4cdf-840c-1f69b68d7b12.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/90dd1a5a-55d3-4cdf-840c-1f69b68d7b12.png)'
- en: 'In the previous expression, *p[M]* is the distribution generated by the model.
    Now, if we rewrite the divergence, we get:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的表达式中，*p[M]* 是模型生成的分布。现在，如果我们重新写散度，我们得到：
- en: '![](img/ebe06332-3966-489c-a572-4974fd6c5b1b.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ebe06332-3966-489c-a572-4974fd6c5b1b.png)'
- en: The first term is the entropy of the data-generating distribution, and it doesn't
    depend on the model parameters, while the second one is the cross-entropy. Therefore,
    if we minimize the cross-entropy, we also minimize the Kullback-Leibler divergence,
    forcing the model to reproduce a distribution that is very similar to *p[data]*.
    This is a very elegant explanation as to why the cross-entropy cost function is
    an excellent choice for classification problems.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个项是数据生成分布的熵，它不依赖于模型参数，而第二个项是交叉熵。因此，如果我们最小化交叉熵，我们也会最小化 Kullback-Leibler 散度，迫使模型重现一个与
    *p[data]* 非常相似的分布。这是关于为什么交叉熵损失函数是分类问题的一个非常好的选择的一个非常优雅的解释。
- en: Regularization
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化
- en: 'When a model is ill-conditioned or prone to overfitting, **regularization**
    offers some valid tools to mitigate the problems. From a mathematical viewpoint,
    a regularizer is a penalty added to the cost function, so to impose an extra-condition
    on the evolution of the parameters:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个模型条件不良或容易过拟合时，**正则化**提供了一些有效的工具来缓解这些问题。从数学角度来看，正则化器是添加到损失函数中的惩罚，以对参数的演变施加额外条件：
- en: '![](img/8e782293-ca76-4e45-881d-59c4358382fe.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8e782293-ca76-4e45-881d-59c4358382fe.png)'
- en: The parameter *λ* controls the strength of the regularization, which is expressed
    through the function *g(θ)*. A fundamental condition on *g(θ)* is that it must
    be differentiable so that the new composite cost function can still be optimized
    using SGD algorithms. In general, any regular function can be employed; however,
    we normally need a function that can contrast the indefinite growth of the parameters.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 参数 *λ* 控制正则化的强度，这通过函数 *g(θ)* 来表达。对 *g(θ)* 的一个基本条件是它必须是可微的，这样新的复合损失函数仍然可以使用 SGD
    算法进行优化。一般来说，任何正则函数都可以使用；然而，我们通常需要一个可以对比参数不定增长的函数。
- en: 'To understand the principle, let''s consider the following diagram:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解原理，让我们考虑以下图表：
- en: '![](img/9bd85fcd-10ce-48c9-a538-70f3791a3f9c.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9bd85fcd-10ce-48c9-a538-70f3791a3f9c.png)'
- en: Interpolation with a linear curve (left) and a parabolic one (right)
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 使用线性曲线（左侧）和抛物线（右侧）进行插值
- en: In the first diagram, the model is linear and has two parameters, while in the
    second one, it is quadratic and has three parameters. We already know that the
    second option is more prone to overfitting, but if we apply a regularization term,
    it's possible to avoid the growth of a (first quadratic parameter), transforming
    the model into a linearized version. Of course, there's a difference between choosing
    a lower-capacity model and applying a regularization constraint. In fact, in the
    first case, we are renouncing the possibility offered by the extra capacity, running
    the risk of increasing the bias, while with regularization we keep the same model
    but optimize it so to reduce the variance. Let's now explore the most common regularization
    techniques.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个图表中，模型是线性的并且有两个参数，而在第二个图表中，它是二次的并且有三个参数。我们已经知道第二个选项更容易过拟合，但是如果我们应用正则化项，就有可能避免（第一个二次参数）的增长，将模型转换为线性化版本。当然，选择低容量模型和应用正则化约束之间有一个区别。事实上，在前一种情况下，我们放弃了额外容量提供的可能性，冒着增加偏差的风险，而通过正则化，我们保持相同的模型但优化它以减少方差。现在让我们探索最常见的正则化技术。
- en: Ridge
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ridge
- en: '**Ridge** regularization (also known as **Tikhonov regularization**) is based
    on the squared L2-norm of the parameter vector:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '**Ridge** 正则化（也称为 **Tikhonov 正则化**）基于参数向量的平方 L2 范数：'
- en: '![](img/521e6e3e-6215-4bd0-9bc8-f0f4fdf34fa6.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/521e6e3e-6215-4bd0-9bc8-f0f4fdf34fa6.png)'
- en: This penalty avoids an infinite growth of the parameters (for this reason, it's
    also known as **weight shrinkage**), and it's particularly useful when the model
    is ill-conditioned, or there is multicollinearity, due to the fact that the samples
    are completely independent (a relatively common condition).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这种惩罚避免了参数的无穷增长（因此，它也被称为**权重收缩**），当模型条件不良或存在多重共线性（由于样本完全独立，这是一个相对常见的情况）时，特别有用。
- en: 'In the following diagram, we see a schematic representation of the Ridge regularization
    in a bidimensional scenario:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图表中，我们看到在二维场景中岭回归正则化的示意图：
- en: '![](img/75db56e9-e05e-4678-8e7c-5cceee9d68c3.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/75db56e9-e05e-4678-8e7c-5cceee9d68c3.png)'
- en: Ridge (L2) regularization
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归（L2）正则化
- en: The zero-centered circle represents the Ridge boundary, while the shaded surface
    is the original cost function. Without regularization, the minimum (**w[1]**,
    **w[2]**) has a magnitude (for example, the distance from the origin) which is about
    double the one obtained by applying a Ridge constraint, confirming the expected
    shrinkage. When applied to regressions solved with the **Ordinary Least Squares**
    (**OLS**) algorithm, it's possible to prove that there always exists a Ridge coefficient,
    so that the weights are shrunk with respect the OLS ones. The same result, with
    some restrictions, can be extended to other cost functions.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 以零为中心的圆表示岭回归边界，而阴影表面是原始成本函数。没有正则化时，最小值（**w[1]**，**w[2]**）的幅度（例如，从原点到距离）大约是应用岭约束后获得的两倍，这证实了预期的收缩。当应用于使用**普通最小二乘法**（**OLS**）算法解决的回归时，可以证明总存在一个岭系数，使得权重相对于OLS权重收缩。在有些限制下，同样的结果可以扩展到其他成本函数。
- en: Lasso
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Lasso
- en: '**Lasso** regularization is based on the *L1*-norm of the parameter vector:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '**Lasso**正则化基于参数向量的*L1*范数：'
- en: '![](img/6ad5e737-1b12-46f0-acd6-a5ac455964ba.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6ad5e737-1b12-46f0-acd6-a5ac455964ba.png)'
- en: 'Contrary to Ridge, which shrinks all the weights, Lasso can shift the smallest
    one to zero, creating a sparse parameter vector. The mathematical proof is beyond
    the scope of this book; however, it''s possible to understand it intuitively by
    considering the following diagram (bidimensional):'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 与岭回归不同，岭回归会缩小所有权重，而Lasso可以将最小的权重移至零，从而创建一个稀疏的参数向量。数学证明超出了本书的范围；然而，通过考虑以下图表（二维）可以直观地理解它：
- en: '![](img/e8e2449c-e33a-4aae-b483-117c651dd76c.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e8e2449c-e33a-4aae-b483-117c651dd76c.png)'
- en: Lasso (L1) regularization
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: Lasso（L1）正则化
- en: 'The zero-centered square represents the Lasso boundaries. If we consider a
    generic line, the probability of being tangential to the square is higher at the
    corners, where at least one (exactly one in a bidimensional scenario) parameter
    is null. In general, if we have a vectorial convex function *f(x)* (we provide
    a definition of convexity in [Chapter 5](8d541a43-8790-4a91-a79b-e48496f75d90.xhtml),
    *EM Algorithm and Applications*), we can define:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 以零为中心的正方形表示Lasso边界。如果我们考虑一条通用线，那么在角落处与正方形相切的概率更高，在这些角落处至少有一个（在二维场景中恰好一个）参数为零。一般来说，如果我们有一个向量凸函数
    *f(x)*（我们在[第5章](8d541a43-8790-4a91-a79b-e48496f75d90.xhtml)，*EM算法与应用*中提供了凸性的定义），我们可以定义：
- en: '![](img/ba397d1e-6086-45de-9651-4358e965dec3.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ba397d1e-6086-45de-9651-4358e965dec3.png)'
- en: As any *L[p]*-norm is convex, as well as the sum of convex functions, *g(x)*
    is also convex. The regularization term is always non-negative, therefore the
    minimum corresponds to the norm of the null vector. When minimizing *g(x)*, we
    need to also consider the contribution of the gradient of the norm in the ball
    centered in the origin where, however, the partial derivatives don't exist. Increasing
    the value of *p*, the norm becomes smoothed around the origin, and the partial
    derivatives approach zero for *|x[i]| → 0*.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 由于任何*L[p]*范数都是凸的，以及凸函数之和也是凸的，因此*g(x)*也是凸的。正则化项始终是非负的，因此最小值对应于零向量的范数。当最小化*g(x)*时，我们还需要考虑以原点为中心的球体中范数梯度的贡献，然而，在该球体中，偏导数不存在。增加*p*的值，范数在原点周围变得平滑，并且当*|x[i]| →
    0*时，偏导数接近零。
- en: On the other side, with *p=1* (excluding the *L[0]*-norm and all the norms with
    *p ∈ ]0, 1[* that allow an even stronger sparsity, but are non-convex), the partial
    derivatives are always +1 or -1, according to the sign of *x[i] (x[i] ≠ 0)*. Therefore,
    it's *easier* for the *L[1]*-norm to push the smallest components to zero, because
    the contribution to the minimization (for example, with a gradient descent) is
    independent of *x*[*i*, ]while an *L[2]*-normdecreases its *speed* when approaching
    the origin. This is a non-rigorous explanation of the sparsity achieved using
    the *L[1]*-norm. In fact, we also need to consider the term *f(x)*, which bounds
    the value of the global minimum; however, it may help the reader to develop an
    intuitive understanding of the concept. It's possible to find further and mathematically
    rigorous details in *Optimization for Machine Learning, (e*dited by) *Sra S.*,
    *Nowozin S.*, *Wright S. J.*, *The MIT Press*.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Lasso regularization is particularly useful whenever a sparse representation
    of a dataset is needed. For example, we could be interested in finding the feature
    vectors corresponding to a group of images. As we expect to have many features
    but only a subset present in each image, applying the Lasso regularization allows
    forcing all the smallest coefficients to become null, suppressing the presence
    of the secondary features. Another potential application is latent semantic analysis,
    where our goal is to describe the documents belonging to a corpus in terms of
    a limited number of topics. All these methods can be summarized in a technique
    called **sparse coding**, where the objective is to reduce the dimensionality
    of a dataset (also in non-linear scenarios) by extracting the most representative
    atoms, using different approaches to achieve sparsity.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: ElasticNet
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In many real cases, it''s useful to apply both Ridge and Lasso regularization
    in order to force weight shrinkage and a global sparsity. It is possible by employing
    the **ElasticNet** regularization, defined as:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2812dfb4-78aa-48ab-a9a5-684d1c674d40.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
- en: The strength of each regularization is controlled by the parameters *λ[1]* and *λ[2]*.
    ElasticNet can yield excellent results whenever it's necessary to mitigate overfitting
    effects while encouraging sparsity. We are going to apply all the regularization
    techniques when discussing some deep learning architectures.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Early stopping
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Even though it''s a pure regularization technique, **early stopping** is often
    considered as a *last resort* when all other approaches to prevent overfitting
    and maximize validation accuracy fail. In many cases (above all, in deep learning
    scenarios), it''s possible to observe a typical behavior of the training process
    considering both training and the validation cost functions:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2df7b673-ba20-47dd-a49b-9c9ce0d9fc0d.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
- en: Example of early stopping before the beginning of ascending phase of U-curve
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: During the first epochs, both costs decrease, but it can happen that after a
    *threshold* epoch *e[s]*, the validation cost starts increasing. If we continue
    with the training process, this results in overfitting the training set and increasing
    the variance. For this reason, when there are no other options, it's possible
    to prematurely stop the training process. In order to do so, it's necessary to
    store the last parameter vector before the beginning of a new iteration and, in
    the case of no improvements or the accuracy worsening, to stop the process and
    recover the last parameters. As explained, this procedure must never be considered
    as the best choice, because a better model or an improved dataset could yield
    higher performances. With early stopping, there's no way to verify alternatives,
    therefore it must be adopted only at the last stage of the process and never at
    the beginning. Many deep learning frameworks such as Keras include helpers to
    implement an early stopping callback; however, it's important to check whether
    the last parameter vector is the one stored before the last epoch or the one corresponding
    to *e[s]*. In this case, it could be useful to repeat the training process, stopping
    it at the epoch previous to *e[s]* (where the minimum validation cost has been
    achieved).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在最初的几个时期，两个代价都会下降，但可能会发生这样的情况：在某个*阈值时期*e[s]之后，验证代价开始上升。如果我们继续训练过程，这会导致训练集过拟合并增加方差。因此，当没有其他选择时，可能需要提前停止训练过程。为了做到这一点，有必要在新的迭代开始之前存储最后一个参数向量，在没有改进或准确度下降的情况下，停止过程并恢复最后一个参数。正如所解释的，这个程序永远不应该被视为最佳选择，因为更好的模型或改进的数据集可能会产生更高的性能。使用提前停止，无法验证替代方案，因此它只能在过程的最后阶段采用，绝对不能在开始时采用。许多深度学习框架，如Keras，都包括实现提前停止回调的帮助器；然而，重要的是要检查最后一个参数向量是在最后一个时期之前存储的，还是对应于*e[s]*的。在这种情况下，重复训练过程，在e[s]*之前的时期停止（在那里达到了最小的验证代价）可能是有用的。
- en: Summary
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed fundamental concepts shared by almost any machine
    learning model. In the first part, we have introduced the data generating process,
    as a generalization of a finite dataset. We explained which are the most common
    strategies to split a finite dataset into a training block and a validation set,
    and we introduced cross-validation, with some of the most important variants,
    as one of the best approaches to avoid the limitations of a static split.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了几乎所有机器学习模型共有的基本概念。在第一部分，我们介绍了数据生成过程，作为有限数据集的泛化。我们解释了将有限数据集划分为训练块和验证集的最常见策略，并介绍了交叉验证，这是避免静态划分局限性的最佳方法之一。
- en: 'In the second part, we discussed the main properties of an estimator: capacity,
    bias, and variance. We also introduced the Vapnik-Chervonenkis theory, which is
    a mathematical formalization of the concept of representational capacity, and
    we analyzed the effects of high biases and high variances. In particular, we discussed
    effects called underfitting and overfitting, defining the relationship with high
    bias and high variance.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二部分，我们讨论了估计量的主要特性：容量、偏差和方差。我们还介绍了Vapnik-Chervonenkis理论，这是表示能力概念的数学形式化，并分析了高偏差和高方差的影响。特别是，我们讨论了欠拟合和过拟合等效应，并定义了它们与高偏差和高方差之间的关系。
- en: In the third part, we introduced the loss and cost functions, first as proxies
    of the expected risk, and then we detailed some common situations that can be
    experienced during an optimization problem. We also exposed some common cost functions,
    together with their main features. In the last part, we discussed regularization,
    explaining how it can mitigate the effects of overfitting.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三部分，我们介绍了损失函数和代价函数，首先作为预期风险的代理，然后详细介绍了优化问题中可能遇到的一些常见情况。我们还介绍了常见的代价函数及其主要特征。在最后一部分，我们讨论了正则化，解释了它如何减轻过拟合的影响。
- en: In the next chapter, [Chapter 2](f6860c85-a228-4e63-96ac-22a0caf1a767.xhtml), *Introduction
    to Semi-Supervised Learning*, we're going to introduce semi-supervised learning,
    focusing our attention on the concepts of transductive and inductive learning.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，[第二章](f6860c85-a228-4e63-96ac-22a0caf1a767.xhtml)，*半监督学习简介*中，我们将介绍半监督学习，重点关注归纳学习和演绎学习等概念。
