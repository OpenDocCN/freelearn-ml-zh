- en: Machine Learning Model Fundamentals
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning models are mathematical systems that share many common features.
    Even if, sometimes, they have been defined only from a theoretical viewpoint,
    research advancement allows us to apply several concepts to better understand
    the behavior of complex systems such as deep neural networks. In this chapter,
    we're going to introduce and discuss some fundamental elements that some skilled
    readers may already know, but that, at the same time, offer several possible interpretations
    and applications.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, in this chapter we''re discussing the main elements of:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Data-generating processes
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finite datasets
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and test split strategies
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-validation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Capacity, bias, and variance of a model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vapnik-Chervonenkis theory
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cramér-Rao bound
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Underfitting and overfitting
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loss and cost functions
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Models and data
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Machine learning algorithms work with data. They create associations, find
    out relationships, discover patterns, generate new samples, and more, working
    with well-defined datasets. Unfortunately, sometimes the assumptions or the conditions
    imposed on them are not clear, and a lengthy training process can result in a
    complete validation failure. Even if this condition is stronger in deep learning
    contexts, we can think of a model as a gray box (some transparency is guaranteed
    by the simplicity of many common algorithms), where a vectorial input ![](img/87da3ad2-7943-4a36-a94e-8af0883a59d2.png) is
    transformed into a vectorial output ![](img/a806507f-a7c6-41d8-95cd-609553341d11.png):'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d25b3444-8930-4e86-ae2e-46e3a2da5c0a.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
- en: Schema of a generic model parameterized with the vector θ
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: In the previous diagram, the model has been represented by a pseudo-function
    that depends on a set of parameters defined by the vector *θ*. In this section,
    we are only considering **parametric** models, although there's a family of algorithms
    that are called **non-parametric**, because they are based only on the structure
    of the data. We're going to discuss some of them in upcoming chapters.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: The task of a parametric learning process is therefore to find the best parameter
    set that maximizes a target function whose value is proportional to the accuracy
    (or the error, if we are trying to minimize them) of the model given a specific
    input *X* and output *Y*. This definition is not very rigorous, and it will be
    improved in the following sections; however, it's useful as a way to understand
    the context we're working in.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, the first question to ask is: What is the nature of *X*? A machine learning
    problem is focused on learning abstract relationships that allow a consistent
    generalization when new samples are provided. More specifically, we can define
    a stochastic **data generating process** with an associated joint probability
    distribution:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/59525165-8c76-456b-af1a-2125fbd9e19d.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: Sometimes, it's useful to express the joint probability *p(x, y)* as a product
    of the conditional *p(y|x)*, which expresses the probability of a label given
    a sample, and the marginal probability of the samples *p(x)*. This expression
    is particularly useful when the prior probability *p(x)* is known in semi-supervised
    contexts, or when we are interested in solving problems using the **Expectation
    Maximization** (**EM**) algorithm. We're going to discuss this approach in upcoming
    chapters.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, we are not able to derive a precise distribution; however, when
    considering a dataset, we always assume that it's drawn from the original data-generating
    distribution. This condition isn't a purely theoretical assumption, because, as
    we're going to see, whenever our data points are drawn from different distributions,
    the accuracy of the model can dramatically decrease.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'If we sample N **independent and identically distributed** (**i.i.d.**) values
    from *p[data]*, we can create a finite dataset *X* made up of *k*-dimensional
    real vectors:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c8318f7c-62e5-4071-982f-71a9ae0d688d.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: 'In a supervised scenario, we also need the corresponding labels (with *t* output
    values):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e8002640-408e-4bc8-9833-24c97d03f68f.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
- en: 'When the output has more than two classes, there are different possible strategies
    to manage the problem. In classical machine learning, one of the most common approaches
    is **One-vs-All**, which is based on training *N* different binary classifiers
    where each label is evaluated against all the remaining ones. In this way, *N-1* is
    performed to determine the right class. With shallow and deep neural networks,
    instead, it''s preferable to use a **softmax** function to represent the output
    probability distribution for all classes:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/261fd2dc-9e54-428c-8ce5-67fe2215b76e.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: This kind of output (*z[i]* represents the intermediate values, and the sum
    of the terms is normalized to *1*) can be easily managed using the cross-entropy
    cost function (see the corresponding paragraph in the *Loss and cost functions*
    section).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Zero-centering and whitening
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Many algorithms show better performances (above all, in terms of training speed)
    when the dataset is symmetric (with a zero-mean). Therefore, one of the most important
    preprocessing steps is so-called **zero-centering**, which consists in subtracting
    the feature-wise mean *E[x][X]* from all samples:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f19a895a-0cad-4bbc-b15f-7c52a2638ce3.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: This operation, if necessary, is normally reversible, and doesn't alter relationships
    both among samples and among components of the same sample. In deep learning scenarios,
    a zero-centered dataset allows exploiting the symmetry of some activation function,
    driving to a faster convergence (we're going to discuss these details in the next
    chapters).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'Another very important preprocessing step is called **whitening**, which is
    the operation of imposing an identity covariance matrix to a zero-centered dataset:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/991b3e6d-7b6a-4361-92bb-8c2c3109369c.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: 'As the covariance matrix *E[x][X^TX]* is real and symmetric, it''s possible
    to eigendecompose it without the need to invert the eigenvector matrix:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e8ed1207-a036-4a11-a809-c009cfe887b9.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: 'The matrix *V* contains the eigenvectors (as columns), and the diagonal matrix *Ω*
    contains the eigenvalues. To solve the problem, we need to find a matrix *A,*
    such that:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dccb1256-8529-4988-a046-e19a5f415b1b.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: 'Using the eigendecomposition previously computed, we get:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cff39904-0475-4866-a9a2-2239b7914843.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: 'Hence, the matrix *A* is:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ee7bf5c-1e51-4b78-ab6b-1c745f68d0b9.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: 'One of the main advantages of whitening is the decorrelation of the dataset,
    which allows an easier separation of the components. Furthermore, if *X* is whitened,
    any orthogonal transformation induced by the matrix *P* is also whitened:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/622c081f-b106-4d0c-bbef-7ce9240d4da7.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: Moreover, many algorithms that need to estimate parameters that are strictly
    related to the input covariance matrix can benefit from this condition, because
    it reduces the actual number of independent variables (in general, these algorithms
    work with matrices that become symmetric after applying the whitening). Another
    important advantage in the field of deep learning is that the gradients are often
    higher around the origin, and decrease in those areas where the activation functions
    (for example, the hyperbolic tangent or the sigmoid) saturate *(|x| → ∞)*. That's
    why the convergence is generally faster for whitened (and zero-centered) datasets.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following graph, it''s possible to compare an **original dataset**,
    **zero-centering**, and **whitening**:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8e2be8a1-6ba2-4eab-aa40-ec1a0b1960d7.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: Original dataset (left), centered version (center), whitened version (right)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: When a whitening is needed, it's important to consider some important details.
    The first one is that there's a scale difference between the real sample covariance
    and the estimation *X^TX,* often adopted with the **singular value decomposition**
    (**SVD**). The second one concerns some common classes implemented by many frameworks,
    like Scikit-Learn's `StandardScaler`. In fact, while zero-centering is a feature-wise
    operation, a whitening filter needs to be computed considering the whole covariance
    matrix (`StandardScaler` implements only unit variance, feature-wise scaling).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 'Luckily, all Scikit-Learn algorithms that benefit from or need a whitening
    preprocessing step provide a built-in feature, so no further actions are normally
    required; however, for all readers who want to implement some algorithms directly,
    I''ve written two Python functions that can be used both for zero-centering and
    whitening. They assume a matrix *X* with a shape (*N[Samples] × n*). Moreover,
    the `whiten()` function accepts the parameter `correct`, which allows us to apply
    the scaling correction (the default value is `True`):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Training and validation sets
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In real problems, the number of samples is limited, and it''s usually necessary
    to split the initial set *X* (together with *Y*) into two subsets as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '**Training set** used to train the model'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Validation set** used to assess the score of the model without any bias,
    with samples never seen before'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'According to the nature of the problem, it''s possible to choose a split percentage
    ratio of 70% – 30% (a good practice in machine learning, where the datasets are
    relatively small), or a higher training percentage (80%, 90%, up to 99%) for deep
    learning tasks where the number of samples is very high. In both cases, we are
    assuming that the training set contains all the information required for a consistent
    generalization. In many simple cases, this is true and can be easily verified;
    but with more complex datasets, the problem becomes harder. Even if we think to
    draw all the samples from the same distribution, it can happen that a randomly
    selected test set contains features that are not present in other training samples.
    Such a condition can have a very negative impact on global accuracy and, without
    other methods, it can also be very difficult to identify. This is one of the reasons
    why, in deep learning, training sets are huge: considering the complexity of the
    features and structure of the data generating distributions, choosing large test
    sets can limit the possibility of learning particular associations.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'In Scikit-Learn, it''s possible to split the original dataset using the **`train_test_split()`**
    function, which allows specifying the train/test size, and if we expect to have
    randomly shuffled sets (default). For example, if we want to split `X` and `Y`,
    with 70% training and 30% test, we can use:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Shuffling the sets is always a good practice, in order to reduce the correlation
    between samples. In fact, we have assumed that `X` is made up of i.i.d samples,
    but several times two subsequent samples have a strong correlation, reducing the
    training performance. In some cases, it''s also useful to re-shuffle the training
    set after each training epoch; however, in the majority of our examples, we are
    going to work with the same shuffled dataset throughout the whole process. Shuffling
    has to be avoided when working with sequences and models with memory: in all those
    cases, we need to exploit the existing correlation to determine how the future
    samples are distributed.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: When working with NumPy and Scikit-Learn, it's always a good practice to set
    the random seed to a constant value, so as to allow other people to reproduce
    the experiment with the same initial conditions. This can be achieved by calling `np.random.seed`(...)
    and using the `random-state` parameter present in many Scikit-Learn methods.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A valid method to detect the problem of wrongly selected test sets is provided
    by the **cross-validation** technique. In particular, we're going to use the **K-Fold**
    cross-validation approach. The idea is to split the whole dataset *X* into a moving
    test set and a training set (the remaining part). The size of the test set is
    determined by the number of folds so that, during *k* iterations, the test set
    covers the whole original dataset.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, we see a schematic representation of the process:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/612f3964-c562-4515-8f33-0cd32d825f0e.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
- en: K-Fold cross-validation schema
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 'In this way, it''s possible to assess the accuracy of the model using different
    sampling splits, and the training process can be performed on larger datasets;
    in particular, on *(k-1)*N* samples. In an ideal scenario, the accuracy should
    be very similar in all iterations; but in most real cases, the accuracy is quite
    below average. This means that the training set has been built excluding samples
    that contain features necessary to let the model fit the separating hypersurface
    considering the real *p[data]*. We''re going to discuss these problems later in
    this chapter; however, if the standard deviation of the accuracies is too high
    (a threshold must be set according to the nature of the problem/model), that probably
    means that *X* hasn''t been drawn uniformly from *p[data],* and it''s useful to
    evaluate the impact of the outliers in a preprocessing stage. In the following
    graph, we see the plot of a 15-fold cross-validation performed on a logistic regression:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/27cd7180-d499-4e47-a364-33f20542a1b8.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: Cross-validation accuracies
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: The values oscillate from 0.84 to 0.95, with an average (solid horizontal line)
    of 0.91\. In this particular case, considering the initial purpose was to use
    a linear classifier, we can say that all folds yield high accuracies, confirming
    that the dataset is linearly separable; however, there are some samples (excluded
    in the ninth fold) that are necessary to achieve a minimum accuracy of about 0.88.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '**K-Fold** cross-validation has different variants that can be employed to
    solve specific problems:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '**Stratified K-Fold**: A **Standard K-Fold** approach splits the dataset without
    considering the probability distribution *p(y|x)*, therefore some folds may theoretically
    contain only a limited number of labels. Stratified K-Fold, instead, tries to
    split *X* so that all the labels are equally represented.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Leave-one-out** (**LOO**): This approach is the most drastic because it creates
    *N* folds, each of them containing *N-1* training samples and only 1 test sample.
    In this way, the maximum possible number of samples is used for training, and
    it''s quite easy to detect whether the algorithm is able to learn with sufficient
    accuracy, or if it''s better to adopt another strategy. The main drawback of this
    method is that *N* models must be trained, and when *N* is very large this can
    cause a performance issue. Moreover, with a large number of samples, the probability
    that two random values are similar increases, therefore many folds will yield
    almost identical results. At the same time, LOO limits the possibilities for assessing
    the generalization ability, because a single test sample is not enough for a reasonable
    estimation.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Leave-P-out** (**LPO**): In this case, the number of test samples is set
    to *p* (non-disjoint sets), so the number of folds is equal to the binomial coefficient
    of *n* over *p*. This approach mitigates LOO''s drawbacks, and it''s a trade-off
    between K-Fold and LOO. The number of folds can be very high, but it''s possible
    to control it by adjusting the number *p* of test samples; however, if *p* isn''t
    small or big enough, the binomial coefficient can *explode*. In fact, when *p*
    has about *n/2* samples, the number of folds is maximal:'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/429214fd-f1af-4070-8f50-4d4ff051c3a8.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
- en: 'Scikit-Learn implements all those methods (with some other variations), but
    I suggest always using the `cross_val_score()` function, which is a helper that
    allows applying the different methods to a specific problem. In the following
    snippet based on a polynomial **Support Vector Machine** (**SVM**) and the MNIST
    digits dataset, the function is applied specifying the number of folds (parameter
    `cv`). In this way, Scikit-Learn will automatically use Stratified K-Fold for
    categorical classifications, and **Standard K-Fold** for all other cases:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The accuracy is very high (> 0.9) in every fold, therefore we expect to have
    even higher accuracy using the LOO method. As we have 1,797 samples, we expect
    the same number of accuracies:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As expected, the average score is very high, but there are still samples that
    are misclassified. As we're going to discuss, this situation could be a potential
    candidate for overfitting, meaning that the model is learning perfectly how to
    map the training set, but it's losing its ability to generalize; however, LOO
    is not a good method to measure this model ability, due to the size of the validation
    set.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now evaluate our algorithm with the LPO technique. Considering what was
    explained before, we have selected the smaller Iris dataset and a classification
    based on a logistic regression. As there are *N=150* samples, choosing `p = 3`,
    we get 551,300 folds:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As in the previous example, we have printed only the first 100 accuracies; however,
    the global trend can be immediately understood with only a few values.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: The cross-validation technique is a powerful tool that is particularly useful
    when the performance cost is not too high. Unfortunately, it's not the best choice
    for deep learning models, where the datasets are very large and the training processes
    can take even days to complete. However, as we're going to discuss, in those cases
    the right choice (the split percentage), together with an accurate analysis of
    the datasets and the employment of techniques such as normalization and regularization,
    allows fitting models that show an excellent generalization ability.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证技术是一种强大的工具，当性能成本不是太高时尤其有用。不幸的是，它并不是深度学习模型的最佳选择，因为数据集非常大，训练过程可能需要甚至几天才能完成。然而，正如我们将要讨论的，在这些情况下，正确的选择（分割百分比），结合对数据集的准确分析以及采用标准化和正则化等技术，可以使模型展现出卓越的泛化能力。
- en: Features of a machine learning model
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习模型的特点
- en: In this section, we're going to consider supervised models, and try to determine
    how it's possible to measure their theoretical potential accuracy and their ability
    to generalize correctly over all possible samples drawn from *p[data]*. The majority
    of these concepts were developed before the *deep learning age*, but continue
    to have an enormous influence on research projects. The idea of *capacity*, for
    example, is an open-ended question that neuroscientists keep on asking themselves
    about the human brain. Modern deep learning models with dozens of layers and millions
    of parameters reopened the theoretical question from a mathematical viewpoint.
    Together with this, other elements, like the limits for the variance of an estimator,
    again attracted the limelight because the algorithms are becoming more and more
    powerful, and performances that once were considered far from any feasible solution
    are now a reality. Being able to train a model, so as to exploit its full capacity,
    maximize its generalization ability, and increase the accuracy, overcoming even
    human performances, is what a deep learning engineer nowadays has to expect from
    his work.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将考虑监督模型，并试图确定如何测量它们的理论潜在准确性和它们在从 *p[data]* 中抽取的所有可能样本上正确泛化的能力。这些概念中的大多数都是在深度学习时代之前开发的，但继续对研究项目产生巨大影响。例如，“容量”的想法是神经科学家不断问自己的关于人脑的开放性问题。具有数十层和数百万参数的现代深度学习模型从数学角度重新开启了理论问题。与此相关，其他元素，如估计量方差的限制，再次成为焦点，因为算法变得越来越强大，曾经被认为远非可行解决方案的性能现在已成为现实。能够训练一个模型，以便充分利用其容量，最大化其泛化能力，并提高准确性，甚至超越人类表现，是深度学习工程师现在必须从他的工作中期待的东西。
- en: Capacity of a model
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型的容量
- en: 'If we consider a supervised model as a set of parameterized functions, we can
    define **representational capacity** as the intrinsic ability of a certain generic
    function to map a relatively large number of data distributions. To understand
    this concept, let''s consider a function *f(x)* that admits infinite derivatives,
    and rewrite it as a Taylor expansion:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将监督模型视为一组参数化函数，我们可以将**表示能力**定义为某种通用函数映射相对大量数据分布的内在能力。为了理解这个概念，让我们考虑一个允许无限导数的函数
    *f(x)*，并将其重写为泰勒展开式：
- en: '![](img/c398523d-8a07-40b9-808e-75bcf7d8ce2a.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c398523d-8a07-40b9-808e-75bcf7d8ce2a.png)'
- en: 'We can decide to take only the first *n* terms, so to have an *n*-degree polynomial
    function. Consider a simple bi-dimensional scenario with six functions (starting
    from a linear one); we can observe the different behavior with a small set of
    data points:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择只取前 *n* 项，以便得到一个 *n* 次多项式函数。考虑一个简单的二维场景，有六个函数（从线性函数开始）；我们可以通过一组小数据点观察它们的不同行为：
- en: '![](img/35abc6b6-9950-4d97-928d-4e5c0e9188a0.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/35abc6b6-9950-4d97-928d-4e5c0e9188a0.png)'
- en: Different behavior produced by six polynomial separating curves
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 六条多项式分离曲线产生的不同行为
- en: The ability to rapidly change the curvature is proportional to the degree. If
    we choose a linear classifier, we can only modify its slope (the example is always
    in a bi-dimensional space) and the intercept. Instead, if we pick a higher-degree
    function, we have more possibilities to *bend* the curvature when it's necessary.
    If we consider **n=1** and **n=2** in the plot (on the top-right, they are the
    first and the second functions), with **n=1**, we can include the dot corresponding
    to *x=11*, but this choice has a negative impact on the dot at *x=5*.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Only a parameterized non-linear function can solve this problem efficiently,
    because this simple problem requires a representational capacity higher than the
    one provided by linear classifiers. Another classical example is the XOR function.
    For a long time, several researchers opposed perceptrons (linear neural networks),
    because they weren't able to classify a dataset generated by the XOR function.
    Fortunately, the introduction of multilayer perceptrons, with non-linear functions,
    allowed us to overcome this problem, and many whose complexity is beyond the possibilities
    of any classic machine learning model.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Vapnik-Chervonenkis capacity
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A mathematical formalization of the capacity of a classifier is provided by
    the **Vapnik-Chervonenkis theory**. To introduce the definition, it''s first necessary
    to define the concept of **shattering**. If we have a class of sets *C* and a
    set *M*, we say that *C* shatters *M* if:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/80c863ff-90ee-47a2-b994-702c455c4256.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
- en: 'In other words, given any subset of *M*, it can be obtained as the intersection
    of a particular instance of *C (c[j])* and *M* itself. If we now consider a model
    as a parameterized function:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cca58a91-5886-45c6-8e00-c6851c414995.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
- en: 'We want to determine its capacity in relation to a finite dataset *X*:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/043d5bff-1238-4688-bfe2-af3f015df786.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
- en: According to the Vapnik-Chervonenkis theory, we can say that the model *f* shatters
    *X* if there are no classification errors for every possible label assignment.
    Therefore, we can define the **Vapnik-Chervonenkis-capacity** or **VC-capacity**
    (sometimes called **VC-dimension**) as the maximum cardinality of a subset of
    *X* so that *f* can shatter it.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if we consider a linear classifier in a bi-dimensional space,
    the VC-capacity is equal to 3, because it''s always possible to label three samples
    so that *f* shatters them; however, it''s impossible to do it in all situations
    where *N > 3*. The XOR problem is an example that needs a VC-capacity higher than
    *3*. Let''s explore the following plot:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7bae2238-7baf-40be-ba8d-bdd5bf6e5c78.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
- en: XOR problem with different separating curves
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: This particular label choice makes the set non-linearly separable. The only
    way to overcome this problem is to use higher-order functions (or non-linear ones).
    The curve lines (belonging to a classifier whose VC-capacity is greater than *3*)
    can separate both the upper-left and the lower-right regions from the remaining
    space, but no straight line can do the same (while it can always separate one
    point from the other three).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Bias of an estimator
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now consider a parameterized model with a single vectorial parameter
    (this isn''t a limitation, but only a didactic choice):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/265f15b6-0886-46fa-a7a7-52bce47a74bf.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: 'The goal of a learning process is to estimate the parameter *θ* so as, for
    example, to maximize the accuracy of a classification. We define the **bias of
    an estimator** (in relation to a parameter *θ*):'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/401cedb3-a8cc-4622-beaa-8cc42a42960c.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
- en: In other words, the bias is the difference between the expected value of the
    estimation and the real parameter value. Remember that the estimation is a function
    of *X*, and cannot be considered a constant in the sum.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'An estimator is said to be **unbiased **if:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd879fa7-6777-4f37-b6f8-8848fec1b9ea.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: 'Moreover, the estimator is defined as **consistent** if the sequence of estimations
    converges (at least with probability 1) to the real value when *k → ∞*:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6882f82d-093d-4424-9e3e-7f64a94caf69.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: Given a dataset *X* whose samples are drawn from *p[data]*, the accuracy of
    an estimator is inversely proportional to its bias. Low-bias (or unbiased) estimators
    are able to map the dataset *X* with high-precision levels, while high-bias estimators
    are very likely to have a capacity that isn't high enough for the problem to solve,
    and therefore their ability to detect the whole dynamic is poor.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now compute the derivative of the bias with respect to the vector *θ*
    (it will be useful later):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73459af7-d774-4d7b-a8d3-0890d6b020b6.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
- en: 'Consider that the last equation, thanks to the linearity of *E[•]*, holds also
    if we add a term that doesn''t depend on *x* to the estimation of *θ*. In fact,
    in line with the laws of probability, it''s easy to verify that:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/463b0abc-e0c6-4f5f-9427-394e2ec5033a.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
- en: Underfitting
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A model with a high bias is likely to underfit the training set. Let''s consider
    the scenario shown in the following graph:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08db0bc9-de16-4408-85f6-50eb3777211b.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
- en: 'Underfitted classifier: The curve cannot separate correctly the two classes'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Even if the problem is very hard, we could try to adopt a linear model and,
    at the end of the training process, the slope and the intercept of the separating
    line are about -1 and 0 (as shown in the plot); however, if we measure the accuracy,
    we discover that it's close to 0! Independently from the number of iterations,
    this model will never be able to learn the association between *X* and *Y*. This
    condition is called **underfitting**, and its major indicator is a very low training
    accuracy. Even if some data preprocessing steps can improve the accuracy, when
    a model is underfitted, the only valid solution is to adopt a higher-capacity
    model.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'In a machine learning task, our goal is to achieve the maximum accuracy, starting
    from the training set and then moving on to the validation set. More formally,
    we can say that we want to improve our models so to get as close as possible to **Bayes
    accuracy**. This is not a well-defined value, but a theoretical upper limit that
    is possible to achieve using an estimator. In the following diagram, we see a
    representation of this process:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b30abbf-a779-4669-8059-b85bd2c8cc22.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
- en: Accuracy level diagram
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Bayes accuracy is often a purely theoretical limit and, for many tasks, it's
    almost impossible to achieve using even biological systems; however, advancements
    in the field of deep learning allow to create models that have a target accuracy
    slightly below the Bayes one. In general, there's no closed form for determining
    the Bayes accuracy, therefore human abilities are considered as a benchmark. In
    the previous classification example, a human being is immediately able to distinguish
    among different dot classes, but the problem can be very hard for a limited-capacity
    classifier. Some of the models we're going to discuss can solve this problem with
    a very high target accuracy, but at this point, we run another risk that can be
    understood after defining the concept of variance of an estimator.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Variance of an estimator
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the beginning of this chapter, we have defined the data generating process
    *p[data]*, and we have assumed that our dataset *X* has been drawn from this distribution;
    however, we don''t want to learn existing relationships limited to *X*, but we
    expect our model to be able to generalize correctly to any other subset drawn
    from *p[data]*. A good measure of this ability is provided by the **variance of
    the estimator**:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/94edc141-6802-4678-85fc-f90776cce7e7.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
- en: The variance can be also defined as the square of the standard error (analogously
    to the standard deviation). A high variance implies dramatic changes in the accuracy
    when new subsets are selected, because the model has probably reached a very high
    training accuracy through an over-learning of a limited set of relationships,
    and it has almost completely lost its ability to generalize.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If underfitting was the consequence of a low capacity and a high bias, **overfitting**
    is a phenomenon that a high variance can detect. In general, we can observe a
    very high training accuracy (even close to the Bayes level), but not a poor validation
    accuracy. This means that the capacity of the model is high enough or even excessive
    for the task (the higher the capacity, the higher the probability of large variances),
    and that the training set isn''t a good representation of *p[data]*. To understand
    the problem, consider the following classification scenarios:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5fb37d6e-4e42-4234-9d3a-252c435db94b.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
- en: Acceptable fitting (left), overfitted classifier (right)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: The left plot has been obtained using logistic regression, while, for the right
    one, the algorithm is SVM with a sixth-degree polynomial kernel. If we consider
    the second model, the decision boundaries seem much more precise, with some samples
    just over them. Considering the shapes of the two subsets, it would be possible
    to say that a non-linear SVM can better capture the dynamics; however, if we sample
    another dataset from *p[data]* and the diagonal *tail* becomes wider, logistic
    regression continues to classify the points correctly, while the SVM accuracy
    decreases dramatically. The second model is very likely to be overfitted, and
    some corrections are necessary. When the validation accuracy is much lower than
    the training one, a good strategy is to increase the number of training samples
    to consider the real *p[data]*. In fact, it can happen that a training set is
    built starting from a hypothetical distribution that doesn't reflect the real
    one; or the number of samples used for the validation is too high, reducing the
    amount of information carried by the remaining samples. Cross-validation is a
    good way to assess the quality of datasets, but it can always happen that we find
    completely new subsets (for example, generated when the application is deployed
    in a production environment) that are misclassified, even if they were supposed
    to belong to *p[data]*. If it's not possible to enlarge the training set, data
    augmentation could be a valid solution, because it allows creating artificial
    samples (for images, it's possible to mirror, rotate, or blur them) starting from
    the information stored in the known ones. Other strategies to prevent overfitting
    are based on a technique called **regularization**, which we're going to discuss
    in the last part of this chapter. For now, we can say that the effect of regularization
    is similar to a partial linearization, which implies a capacity reduction with
    a consequent variance decrease.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: The Cramér-Rao bound
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If it''s theoretically possible to create an unbiased model (even asymptotically),
    this is not true for variance. To understand this concept, it''s necessary to
    introduce an important definition: the **Fisher information**. If we have a parameterized
    model and a data-generating process *p[data]*, we can define the likelihood function by
    considering the following parameters:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b00ac51d-ebc5-471b-a1b4-5fd56ccc391d.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
- en: 'This function allows measuring how well the model describes the original data
    generating process. The shape of the likelihood can vary substantially, from well-defined,
    peaked curves, to almost flat surfaces. Let''s consider the following graph, showing
    two examples based on a single parameter:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df9b41c2-7a3d-4f4a-812a-7caf3f276f38.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
- en: Very peaked likelihood (left), flatter likelihood (right)
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'We can immediately understand that, in the first case, the maximum likelihood
    can be easily reached by gradient ascent, because the surface is very peaked.
    In the second case, instead, the gradient magnitude is smaller, and it''s rather
    easy to stop before reaching the actual maximum because of numerical imprecisions
    or tolerances. In worst cases, the surface can be almost flat in very large regions,
    with a corresponding gradient close to zero. Of course, we''d like to always work
    with very sharp and peaked likelihood functions, because they carry more information
    about their maximum. More formally, the Fisher information quantifies this value.
    For a single parameter, it is defined as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d13c559-8c9f-4b50-922f-8c930c542e4e.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
- en: The Fisher information is an unbounded non-negative number that is proportional
    to the amount of information carried by the log-likelihood; the use of logarithm
    has no impact on the gradient ascent, but it simplifies complex expressions by
    turning products into sums. This value can be interpreted as the *speed* of the
    gradient when the function is reaching the maximum; therefore, higher values imply
    better approximations, while a hypothetical value of zero means that the probability
    to determine the right parameter estimation is also null.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: 'When working with a set of *K* parameters, the Fisher information becomes a
    positive semidefinite matrix:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/031c41a3-23a9-443d-92c3-00cf3942a56b.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
- en: 'This matrix is symmetric, and also has another important property: when a value
    is zero, it means that the corresponding couple of parameters are orthogonal for
    the purpose of the maximum likelihood estimation, and they can be considered separately.
    In many real cases, if a value is close to zero, it determines a very low correlation
    between parameters and, even if it''s not mathematically rigorous, it''s possible
    to decouple them anyway.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, it''s possible to introduce the **Cramér-Rao bound**, which
    states that for every unbiased estimator that adopts *x* (with probability distribution
    *p(x; θ)*) as a measure set, the variance of any estimator of *θ* is always lower-bounded
    according to the following inequality:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f4feb22e-34c0-4ef6-8387-0ce820e04e61.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
- en: 'In fact, considering initially a generic estimator and exploiting Cauchy-Schwarz
    inequality with the variance and the Fisher information (which are both expressed
    as expected values), we obtain:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d41065f1-a330-4809-9a87-42a4f6dc3140.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
- en: 'Now, if we use the expression for derivatives of the bias with respect to *θ*,
    considering that the expected value of the estimation of *θ* doesn''t depend on
    *x*, we can rewrite the right side of the inequality as:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/32bc0088-4bba-47f1-8330-2ba98612280d.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
- en: 'If the estimator is unbiased, the derivative on the right side is equal to
    zero, therefore, we get:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c3948c3-6c9e-4c21-87c3-ab9ed67f26bc.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
- en: 'In other words, we can try to reduce the variance, but it will be always lower-bounded
    by the inverse Fisher information. Therefore, given a dataset and a model, there''s
    always a limit to the ability to generalize. In some cases, this measure is easy
    to determine; however, its real value is theoretical, because it provides the
    likelihood function with another fundamental property: it carries all the information
    needed to estimate the worst case for variance. This is not surprising: when we
    discussed the capacity of a model, we saw how different functions could drive
    to higher or lower accuracies. If the training accuracy is high enough, this means
    that the capacity is appropriate or even excessive for the problem; however, we
    haven''t considered the role of the likelihood *p(X| θ)*.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'High-capacity models, in particular, with small or low-informative datasets,
    can drive to flat likelihood surfaces with a higher probability than lower-capacity
    models. Therefore, the Fisher information tends to become smaller, because there
    are more and more parameter sets that yield similar probabilities, and this, at
    the end of the day, drives to higher variances and an increased risk of overfitting.
    To conclude this section, it''s useful to consider a general empirical rule derived
    from the **Occam''s razor** principle: whenever a simpler model can explain a
    phenomenon with enough accuracy, it doesn''t make sense to increase its capacity.
    A simpler model is always preferable (when the performance is good and it represents
    accurately the specific problem), because it''s normally faster both in the training
    and in the inference phases, and more efficient. When talking about deep neural
    networks, this principle can be applied in a more precise way, because it''s easier
    to increase or decrease the number of layers and neurons until the desired accuracy
    has been achieved.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Loss and cost functions
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the beginning of this chapter, we discussed the concept of generic target
    function so as to optimize in order to solve a machine learning problem. More
    formally, in a supervised scenario, where we have finite datasets *X* and *Y*:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/288f4481-7e25-4442-b623-076e744dd043.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
- en: '![](img/51650bd3-6e1d-4955-b417-a9474554c3df.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
- en: 'We can define the generic **loss function** for a single sample as:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c6ed2c5c-3bba-46b1-959a-de58d115ce0f.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: '*J* is a function of the whole parameter set, and must be proportional to the
    error between the true label and the predicted. Another important property is
    convexity. In many real cases, this is an almost impossible condition; however,
    it''s always useful to look for convex loss functions, because they can be easily
    optimized through the gradient descent method. We''re going to discuss this topic
    in [Chapter 9](cb8a2a42-13fc-40ed-b6f0-56e4843284d7.xhtml), *Neural Networks for
    Machine Learning*. However, for now, it''s useful to consider a loss function
    as an intermediate between our training process and a pure mathematical optimization.
    The missing link is the complete data. As already discussed, *X* is drawn from
    *p[data]*, so it should represent the true distribution. Therefore, when minimizing
    the loss function, we''re considering a potential subset of points, and never
    the whole real dataset. In many cases, this isn''t a limitation, because, if the
    bias is null and the variance is small enough, the resulting model will show a
    good generalization ability (high training and validation accuracy); however,
    considering the data generating process, it''s useful to introduce another measure
    called **expected risk**:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c98b2f7f-894d-4bfe-9245-71acd5e0351f.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
- en: 'This value can be interpreted as an average of the loss function over all possible
    samples drawn from *p*[*data*.] Minimizing the expected risk implies the maximization
    of the global accuracy. When working with a finite number of training samples,
    instead, it''s common to define a **cost function **(often called a loss function
    as well, and not to be confused with the log-likelihood):'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f105b89b-e2d5-41c6-9f18-029d2ce04ab4.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: 'This is the actual function that we''re going to minimize and, divided by the
    number of samples (a factor that doesn''t have any impact), it''s also called
    **empirical risk**, because it''s an approximation (based on real data) of the
    expected risk. In other words, we want to find a set of parameters so that:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/66c92f36-b45f-4a28-a69c-f969de7bc3d5.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
- en: 'When the cost function has more than two parameters, it''s very difficult and
    perhaps even impossible to understand its internal structure; however, we can
    analyze some potential conditions using a bidimensional diagram:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/88baf2eb-e5aa-4562-b4c1-786facba4717.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
- en: Different kinds of points in a bidimensional scenario
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'The different situations we can observe are:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: The **starting point**, where the cost function is usually very high due to
    the error.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Local minima**, where the gradient is null (and the second derivative is
    positive). They are candidates for the optimal parameter set, but unfortunately,
    if the concavity isn''t too deep, an inertial movement or some noise can easily
    move the point away.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ridges** (or **local maxima**), where the gradient is null, and the second
    derivative is negative. They are unstable points, because a minimum perturbation
    allows escaping, reaching lower-cost areas.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Plateaus**, or the region where the surface is almost flat and the gradient
    is close to zero. The only way to escape a plateau is to keep a residual kinetic
    energy—we''re going to discuss this concept when talking about neural optimization
    algorithms ([Chapter 9](cb8a2a42-13fc-40ed-b6f0-56e4843284d7.xhtml), *Neural Networks
    for Machine Learning*).'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Global minimum**, the point we want to reach to optimize the cost function.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Even if local minima are likely when the number of parameters is small, they
    become very unlikely when the model has a large number of parameters. In fact,
    an *n*-dimensional point *θ^** is a local minimum for a convex function (and here,
    we''re assuming *L* to be convex) only if:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db82f2e0-e5fb-403d-9566-4e5a594a518e.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: 'The second condition imposes a positive semi-definite Hessian matrix (equivalently,
    all principal minors *H[n]* made with the first *n* rows and *n* columns must
    be non-negative), therefore all itseigenvalues *λ[0], λ[1], ..., λ[N]* must be
    non-negative. This probability decreases with the number of parameters (*H* is
    a *n×n* square matrix and has *n* eigenvalues), and becomes close to zero in deep
    learning models where the number of weights can be in the order of 10,000,000
    (or even more). The reader interested in a complete mathematical proof can read
    *High Dimensional Spaces*, *Deep Learning and Adversarial Examples*,*Dube S.,
    arXiv:1801.00634 [cs.CV]*. As a consequence, a more common condition to consider
    is instead the presence of **saddle points**, where the eigenvalues have different
    signs and the orthogonal directional derivatives are null, even if the points
    are neither local maxima nor minima. Consider, for example, the following plot:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7fa93bda-0cb6-45ee-af55-2f70f817120b.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
- en: Saddle point in a bidimensional scenario
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'The function is *y=x3* whose first and second derivatives are *y''=3x2* and
    *y''''=6x*. Therefore, *y''(0)=y''''(0)=0*. In this case (single-valued function),
    this point is also called a **point of inflection**, because at *x=0,* the function
    shows a change in the concavity. In three dimensions, it''s easier to understand
    why a saddle point has been called in this way. Consider, for example, the following
    plot:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c4185f6-5242-468e-901b-d2fde6dd3193.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
- en: Saddle point in a three-dimensional scenario
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: The surface is very similar to a horse saddle, and if we project the point on
    an orthogonal plane, *XZ* is a minimum, while on another plane (*YZ*) it is a
    maximum. Saddle points are quite dangerous, because many simpler optimization
    algorithms can slow down and even stop, losing the ability to find the right direction.
    In [Chapter 9](cb8a2a42-13fc-40ed-b6f0-56e4843284d7.xhtml), *Neural Networks for
    Machine Learning*, we're going to discuss some methods that are able to mitigate
    this kind of problem, allowing deep models to converge.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Examples of cost functions
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we expose some common **cost functions** that are employed
    in both classification and regression tasks. Some of them will be extensively
    adopted in our examples in the next chapters, particularly when discussing training
    processes in shallow and deep neural networks.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Mean squared error
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Mean squared error** is one of the most common regression cost functions.
    Its generic expression is:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a427b719-280a-41a6-94b7-b83646e36208.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
- en: This function is differentiable at every point of its domain and it's convex,
    so it can be optimized using the **stochastic gradient descent** (**SGD**) algorithm;
    however, there's a drawback when employed in regressions where there are outliers.
    As its value is always quadratic when the distance between the prediction and
    the actual value (corresponding to an outlier) is large, the relative error is
    high, and this can lead to an unacceptable correction.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Huber cost function
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As explained, mean squared error isn't robust to outliers, because it's always
    quadratic independently of the distance between actual value and prediction. To
    overcome this problem, it's possible to employ the **H****uber** **cost function**,
    which is based on threshold *t[H]*, so that for distances less than *t[H]*, its
    behavior is quadratic, while for a distance greater than *t*[*H*,] it becomes
    linear, reducing the entity of the error and, therefore, the relative importance
    of the outliers.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'The analytical expression is:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e437919e-1f6f-4eb2-9110-e896fe57eede.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
- en: Hinge cost function
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This cost function is adopted by SVM, where the goal is to maximize the distance
    between the separation boundaries (where the support vector lies). It''s analytic
    expression is:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/68006894-0de8-4ef9-8ba5-ae93b4bc5123.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: 'Contrary to the other examples, this cost function is not optimized using classic
    stochastic gradient descent methods, because it''s not differentiable at all points
    where:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dee44504-4715-4432-9ec6-018f66643b6e.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
- en: For this reason, SVM algorithms are optimized using quadratic programming techniques.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Categorical cross-entropy
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Categorical cross-entropy** is the most diffused classification cost function,
    adopted by logistic regression and the majority of neural architectures. The generic
    analytical expression is:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dfd97220-2510-4838-b413-094a8458eca9.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: 'This cost function is convex and can be easily optimized using stochastic gradient
    descent techniques; moreover, it has another important interpretation. If we are
    training a classifier, our goal is to create a model whose distribution is as
    similar as possible to *pdata*. This condition can be achieved by minimizing the
    Kullback-Leibler divergence between the two distributions:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/90dd1a5a-55d3-4cdf-840c-1f69b68d7b12.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
- en: 'In the previous expression, *p[M]* is the distribution generated by the model.
    Now, if we rewrite the divergence, we get:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ebe06332-3966-489c-a572-4974fd6c5b1b.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
- en: The first term is the entropy of the data-generating distribution, and it doesn't
    depend on the model parameters, while the second one is the cross-entropy. Therefore,
    if we minimize the cross-entropy, we also minimize the Kullback-Leibler divergence,
    forcing the model to reproduce a distribution that is very similar to *p[data]*.
    This is a very elegant explanation as to why the cross-entropy cost function is
    an excellent choice for classification problems.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When a model is ill-conditioned or prone to overfitting, **regularization**
    offers some valid tools to mitigate the problems. From a mathematical viewpoint,
    a regularizer is a penalty added to the cost function, so to impose an extra-condition
    on the evolution of the parameters:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8e782293-ca76-4e45-881d-59c4358382fe.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
- en: The parameter *λ* controls the strength of the regularization, which is expressed
    through the function *g(θ)*. A fundamental condition on *g(θ)* is that it must
    be differentiable so that the new composite cost function can still be optimized
    using SGD algorithms. In general, any regular function can be employed; however,
    we normally need a function that can contrast the indefinite growth of the parameters.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the principle, let''s consider the following diagram:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9bd85fcd-10ce-48c9-a538-70f3791a3f9c.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
- en: Interpolation with a linear curve (left) and a parabolic one (right)
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: In the first diagram, the model is linear and has two parameters, while in the
    second one, it is quadratic and has three parameters. We already know that the
    second option is more prone to overfitting, but if we apply a regularization term,
    it's possible to avoid the growth of a (first quadratic parameter), transforming
    the model into a linearized version. Of course, there's a difference between choosing
    a lower-capacity model and applying a regularization constraint. In fact, in the
    first case, we are renouncing the possibility offered by the extra capacity, running
    the risk of increasing the bias, while with regularization we keep the same model
    but optimize it so to reduce the variance. Let's now explore the most common regularization
    techniques.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Ridge
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Ridge** regularization (also known as **Tikhonov regularization**) is based
    on the squared L2-norm of the parameter vector:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/521e6e3e-6215-4bd0-9bc8-f0f4fdf34fa6.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
- en: This penalty avoids an infinite growth of the parameters (for this reason, it's
    also known as **weight shrinkage**), and it's particularly useful when the model
    is ill-conditioned, or there is multicollinearity, due to the fact that the samples
    are completely independent (a relatively common condition).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, we see a schematic representation of the Ridge regularization
    in a bidimensional scenario:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/75db56e9-e05e-4678-8e7c-5cceee9d68c3.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
- en: Ridge (L2) regularization
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: The zero-centered circle represents the Ridge boundary, while the shaded surface
    is the original cost function. Without regularization, the minimum (**w[1]**,
    **w[2]**) has a magnitude (for example, the distance from the origin) which is about
    double the one obtained by applying a Ridge constraint, confirming the expected
    shrinkage. When applied to regressions solved with the **Ordinary Least Squares**
    (**OLS**) algorithm, it's possible to prove that there always exists a Ridge coefficient,
    so that the weights are shrunk with respect the OLS ones. The same result, with
    some restrictions, can be extended to other cost functions.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Lasso
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Lasso** regularization is based on the *L1*-norm of the parameter vector:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ad5e737-1b12-46f0-acd6-a5ac455964ba.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
- en: 'Contrary to Ridge, which shrinks all the weights, Lasso can shift the smallest
    one to zero, creating a sparse parameter vector. The mathematical proof is beyond
    the scope of this book; however, it''s possible to understand it intuitively by
    considering the following diagram (bidimensional):'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e8e2449c-e33a-4aae-b483-117c651dd76c.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
- en: Lasso (L1) regularization
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'The zero-centered square represents the Lasso boundaries. If we consider a
    generic line, the probability of being tangential to the square is higher at the
    corners, where at least one (exactly one in a bidimensional scenario) parameter
    is null. In general, if we have a vectorial convex function *f(x)* (we provide
    a definition of convexity in [Chapter 5](8d541a43-8790-4a91-a79b-e48496f75d90.xhtml),
    *EM Algorithm and Applications*), we can define:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba397d1e-6086-45de-9651-4358e965dec3.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
- en: As any *L[p]*-norm is convex, as well as the sum of convex functions, *g(x)*
    is also convex. The regularization term is always non-negative, therefore the
    minimum corresponds to the norm of the null vector. When minimizing *g(x)*, we
    need to also consider the contribution of the gradient of the norm in the ball
    centered in the origin where, however, the partial derivatives don't exist. Increasing
    the value of *p*, the norm becomes smoothed around the origin, and the partial
    derivatives approach zero for *|x[i]| → 0*.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: On the other side, with *p=1* (excluding the *L[0]*-norm and all the norms with
    *p ∈ ]0, 1[* that allow an even stronger sparsity, but are non-convex), the partial
    derivatives are always +1 or -1, according to the sign of *x[i] (x[i] ≠ 0)*. Therefore,
    it's *easier* for the *L[1]*-norm to push the smallest components to zero, because
    the contribution to the minimization (for example, with a gradient descent) is
    independent of *x*[*i*, ]while an *L[2]*-normdecreases its *speed* when approaching
    the origin. This is a non-rigorous explanation of the sparsity achieved using
    the *L[1]*-norm. In fact, we also need to consider the term *f(x)*, which bounds
    the value of the global minimum; however, it may help the reader to develop an
    intuitive understanding of the concept. It's possible to find further and mathematically
    rigorous details in *Optimization for Machine Learning, (e*dited by) *Sra S.*,
    *Nowozin S.*, *Wright S. J.*, *The MIT Press*.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Lasso regularization is particularly useful whenever a sparse representation
    of a dataset is needed. For example, we could be interested in finding the feature
    vectors corresponding to a group of images. As we expect to have many features
    but only a subset present in each image, applying the Lasso regularization allows
    forcing all the smallest coefficients to become null, suppressing the presence
    of the secondary features. Another potential application is latent semantic analysis,
    where our goal is to describe the documents belonging to a corpus in terms of
    a limited number of topics. All these methods can be summarized in a technique
    called **sparse coding**, where the objective is to reduce the dimensionality
    of a dataset (also in non-linear scenarios) by extracting the most representative
    atoms, using different approaches to achieve sparsity.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: ElasticNet
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In many real cases, it''s useful to apply both Ridge and Lasso regularization
    in order to force weight shrinkage and a global sparsity. It is possible by employing
    the **ElasticNet** regularization, defined as:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2812dfb4-78aa-48ab-a9a5-684d1c674d40.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
- en: The strength of each regularization is controlled by the parameters *λ[1]* and *λ[2]*.
    ElasticNet can yield excellent results whenever it's necessary to mitigate overfitting
    effects while encouraging sparsity. We are going to apply all the regularization
    techniques when discussing some deep learning architectures.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Early stopping
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Even though it''s a pure regularization technique, **early stopping** is often
    considered as a *last resort* when all other approaches to prevent overfitting
    and maximize validation accuracy fail. In many cases (above all, in deep learning
    scenarios), it''s possible to observe a typical behavior of the training process
    considering both training and the validation cost functions:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2df7b673-ba20-47dd-a49b-9c9ce0d9fc0d.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
- en: Example of early stopping before the beginning of ascending phase of U-curve
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: During the first epochs, both costs decrease, but it can happen that after a
    *threshold* epoch *e[s]*, the validation cost starts increasing. If we continue
    with the training process, this results in overfitting the training set and increasing
    the variance. For this reason, when there are no other options, it's possible
    to prematurely stop the training process. In order to do so, it's necessary to
    store the last parameter vector before the beginning of a new iteration and, in
    the case of no improvements or the accuracy worsening, to stop the process and
    recover the last parameters. As explained, this procedure must never be considered
    as the best choice, because a better model or an improved dataset could yield
    higher performances. With early stopping, there's no way to verify alternatives,
    therefore it must be adopted only at the last stage of the process and never at
    the beginning. Many deep learning frameworks such as Keras include helpers to
    implement an early stopping callback; however, it's important to check whether
    the last parameter vector is the one stored before the last epoch or the one corresponding
    to *e[s]*. In this case, it could be useful to repeat the training process, stopping
    it at the epoch previous to *e[s]* (where the minimum validation cost has been
    achieved).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed fundamental concepts shared by almost any machine
    learning model. In the first part, we have introduced the data generating process,
    as a generalization of a finite dataset. We explained which are the most common
    strategies to split a finite dataset into a training block and a validation set,
    and we introduced cross-validation, with some of the most important variants,
    as one of the best approaches to avoid the limitations of a static split.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 'In the second part, we discussed the main properties of an estimator: capacity,
    bias, and variance. We also introduced the Vapnik-Chervonenkis theory, which is
    a mathematical formalization of the concept of representational capacity, and
    we analyzed the effects of high biases and high variances. In particular, we discussed
    effects called underfitting and overfitting, defining the relationship with high
    bias and high variance.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: In the third part, we introduced the loss and cost functions, first as proxies
    of the expected risk, and then we detailed some common situations that can be
    experienced during an optimization problem. We also exposed some common cost functions,
    together with their main features. In the last part, we discussed regularization,
    explaining how it can mitigate the effects of overfitting.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, [Chapter 2](f6860c85-a228-4e63-96ac-22a0caf1a767.xhtml), *Introduction
    to Semi-Supervised Learning*, we're going to introduce semi-supervised learning,
    focusing our attention on the concepts of transductive and inductive learning.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
