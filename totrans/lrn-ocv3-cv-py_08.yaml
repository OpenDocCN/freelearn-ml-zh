- en: Chapter 8. Tracking Objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will explore the vast topic of object tracking, which is
    the process of locating a moving object in a movie or video feed from a camera.
    Real-time object tracking is a critical task in many computer vision applications
    such as surveillance, perceptual user interfaces, augmented reality, object-based
    video compression, and driver assistance.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking objects can be accomplished in several ways, with the optimal technique
    being largely dependent on the task at hand. We will learn how to identify moving
    objects and track them across frames.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting moving objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first task that needs to be accomplished for us to be able to track anything
    in a video is to identify those regions of a video frame that correspond to moving
    objects.
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to track objects in a video, all of them fulfilling a slightly
    different purpose. For example, you may want to track anything that moves, in
    which case differences between frames are going to be of help; you may want to
    track a hand moving in a video, in which case Meanshift based on the color of
    the skin is the most appropriate solution; you may want to track a particular
    object of which you know the aspect, in which case techniques such as template
    matching will be of help.
  prefs: []
  type: TYPE_NORMAL
- en: Object tracking techniques can get quite complex, let's explore them in the
    ascending order of difficulty, starting from the simplest technique.
  prefs: []
  type: TYPE_NORMAL
- en: Basic motion detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first and most intuitive solution is to calculate the differences between
    frames, or between a frame considered "background" and all the other frames.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at an example of this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'After the necessary imports, we open the video feed obtained from the default
    system camera, and we set the first frame as the background of the entire feed.
    Each frame read from that point onward is processed to calculate the difference
    between the background and the frame itself. This is a trivial operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we get to do that, though, we need to prepare our frame for processing.
    The first thing we do is convert the frame to grayscale and blur it a bit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You may wonder about the blurring: the reason why we blur the image is that,
    in each video feed, there''s a natural noise coming from natural vibrations, changes
    in lighting, and the noise generated by the camera itself. We want to smooth this
    noise out so that it doesn''t get detected as motion and consequently get tracked.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that our frame is grayscaled and smoothed, we can calculate the difference
    compared to the background (which has also been grayscaled and smoothed), and
    obtain a map of differences. This is not the only processing step, though. We''re
    also going to apply a threshold, so as to obtain a black and white image, and
    dilate the image so holes and imperfections get normalized, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that eroding and dilating can also act as a noise filter, much like the
    blurring we applied, and that it can also be obtained in one function call using
    `cv2.morphologyEx`, we show both steps explicitly for transparency purposes. All
    that is left to do at this point is to find the contours of all the white blobs
    in the calculated difference map, and display them. Optionally, we only display
    contours for rectangles greater than an arbitrary threshold, so tiny movements
    are not displayed. Naturally, this is up to you and your application needs. With
    a constant lighting and a very noiseless camera, you may wish to have no threshold
    on the minimum size of the contours. This is how we display the rectangles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'OpenCV offers two very handy functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cv2.findContours`: This function computes the contours of subjects in an image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cv2.boundinRect`: This function calculates their bounding box'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So there you have it, a basic motion detector with rectangles around subjects.
    The final result is something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Basic motion detection](img/image00242.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: For such a simple technique, this is quite accurate. However, there are a few
    drawbacks that make this approach unsuitable for all business needs, most notably
    the fact that you need a first "default" frame to set as a background. In situations
    such as—for example—outdoor cameras, with lights changing quite constantly, this
    process results in a quite inflexible approach, so we need a bit more intelligence
    into our system. That's where background subtractors come into play.
  prefs: []
  type: TYPE_NORMAL
- en: Background subtractors – KNN, MOG2, and GMG
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenCV provides a class called `BackgroundSubtractor`, which is a handy way
    to operate foreground and background segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: This works similarly to the GrabCut algorithm we analyzed in [Chapter 3](part0023.xhtml#aid-LTSU1
    "Chapter 3. Processing Images with OpenCV 3"), *Processing Images with OpenCV
    3*, however, `BackgroundSubtractor` is a fully fledged class with a plethora of
    methods that not only perform background subtraction, but also improve background
    detection in time through machine learning and lets you save the classifier to
    a file.
  prefs: []
  type: TYPE_NORMAL
- en: 'To familiarize ourselves with `BackgroundSubtractor`, let''s look at a basic
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s go through this in order. First of all, let''s talk about the background
    subtractor object. There are three background subtractors available in OpenCV
    3: **K-Nearest Neighbors** (**KNN**), **Mixture of Gaussians** (**MOG2**), and
    **Geometric Multigrid** (**GMG**), corresponding to the algorithm used to compute
    the background subtraction.'
  prefs: []
  type: TYPE_NORMAL
- en: You may remember that we already elaborated on the topic of foreground and background
    detection in [Chapter 5](part0043.xhtml#aid-190861 "Chapter 5. Detecting and Recognizing
    Faces"), *Depth Estimation and Segmentation*, in particular when we talked about
    GrabCut and Watershed.
  prefs: []
  type: TYPE_NORMAL
- en: So why do we need the `BackgroundSubtractor` classes? The main reason behind
    this is that `BackgroundSubtractor` classes are specifically built with video
    analysis in mind, which means that the OpenCV `BackgroundSubtractor` classes "learn"
    something about the environment with every frame. For example, with GMG, you can
    specify the number of frames used to initialize the video analysis, with the default
    being 120 (roughly 5 seconds with average cameras). The constant aspect about
    the `BackgroundSubtractor` classes is that they operate a comparison between frames
    and they store a history, which allows them to improve motion analysis results
    as time passes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another fundamental (and frankly, quite amazing) feature of the `BackgroundSubtractor`
    classes is the ability to compute shadows. This is absolutely vital for an accurate
    reading of video frames; by detecting shadows, you can exclude shadow areas (by
    thresholding them) from the objects you detected, and concentrate on the real
    features. It also greatly reduces the unwanted "merging" of objects. An image
    comparison will give you a good idea of the concept I''m trying to illustrate.
    Here''s a sample of background subtraction without shadow detection:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Background subtractors – KNN, MOG2, and GMG](img/image00243.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here''s an example of shadow detection (with shadows thresholded):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Background subtractors – KNN, MOG2, and GMG](img/image00244.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that shadow detection isn''t absolutely perfect, but it helps bring the
    object contours back to the object''s original shape. Let''s take a look at a
    reimplemented example of motion detection utilizing `BackgroundSubtractorKNN`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result of the accuracy of the subtractor, and its ability to detect shadows,
    we obtain a really precise motion detection, in which even objects that are next
    to each other don''t get merged into one detection, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Background subtractors – KNN, MOG2, and GMG](img/image00245.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: That's a remarkable result for fewer than 30 lines of code!
  prefs: []
  type: TYPE_NORMAL
- en: 'The core of the entire program is the `apply()` method of the background subtractor;
    it computes a foreground mask, which can be used as a basis for the rest of the
    processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Once a foreground mask is obtained, we can apply a threshold: the foreground
    mask has white values for the foreground and gray for shadows; thus, in the thresholded
    image, all pixels that are not almost pure white (244-255) are binarized to 0
    instead of 1.'
  prefs: []
  type: TYPE_NORMAL
- en: 'From there, we proceed with the same approach we adopted for the basic motion
    detection example: identifying objects, detecting contours, and drawing them on
    the original frame.'
  prefs: []
  type: TYPE_NORMAL
- en: Meanshift and CAMShift
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Background subtraction is a really effective technique, but not the only one
    available to track objects in a video. Meanshift is an algorithm that tracks objects
    by finding the maximum density of a discrete sample of a probability function
    (in our case, a region of interest in an image) and recalculating it at the next
    frame, which gives the algorithm an indication of the direction in which the object
    has moved.
  prefs: []
  type: TYPE_NORMAL
- en: This calculation gets repeated until the centroid matches the original one,
    or remains unaltered even after consecutive iterations of the calculation. This
    final matching is called **convergence**. For reference, the algorithm was first
    described in the paper, *The estimation of the gradient of a density function,
    with applications in pattern recognition*, *Fukunaga K. and Hoestetler L.*, *IEEE*,
    *1975*, which is available at [http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=1055330&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D1055330](http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=1055330&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D1055330)
    (note that this paper is not free for download).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a visual representation of this process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Meanshift and CAMShift](img/image00246.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Aside from the theory, Meanshift is very useful when tracking a particular region
    of interest in a video, and this has a series of implications; for example, if
    you don't know a priori what the region you want to track is, you're going to
    have to manage this cleverly and develop programs that dynamically start tracking
    (and cease tracking) certain areas of the video, depending on arbitrary criteria.
    One example could be that you operate object detection with a trained SVM, and
    then start using Meanshift to track a detected object.
  prefs: []
  type: TYPE_NORMAL
- en: Let's not make our life complicated from the very beginning, though; let's first
    get familiar with Meanshift, and then use it in more complex scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by simply marking a region of interest and keeping track of it,
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, I supplied the HSV values for tracking some shades of
    lilac, and here''s the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Meanshift and CAMShift](img/image00247.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: If you ran the code on your machine, you'd notice how the Meanshift window actually
    looks for the specified color range; if it doesn't find it, you'll just see the
    window wobbling (it actually looks a bit impatient). If an object with the specified
    color range enters the window, the window will then start tracking it.
  prefs: []
  type: TYPE_NORMAL
- en: Let's examine the code so that we can fully understand how Meanshift performs
    this tracking operation.
  prefs: []
  type: TYPE_NORMAL
- en: Color histograms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before showing the code for the preceding example, though, here is a not-so-brief
    digression on color histograms and the two very important built-in functions of
    OpenCV: `calcHist` and `calcBackProject`.'
  prefs: []
  type: TYPE_NORMAL
- en: The function, `calcHist`, calculates color histograms of an image, so the next
    logical step is to explain the concept of color histograms. A color histogram
    is a representation of the color distribution of an image. On the *x* axis of
    the representation, we have color values, and on the *y* axis, we have the number
    of pixels corresponding to the color values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at a visual representation of this concept, hoping the adage, "a
    picture speaks a thousand words", will apply in this instance too:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Color histograms](img/image00248.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The picture shows a representation of a color histogram with one column per
    value from 0 to 180 (note that OpenCV uses H values 0-180\. Other systems may
    use 0-360 or 0-255).
  prefs: []
  type: TYPE_NORMAL
- en: Aside from Meanshift, color histograms are used for a number of different and
    useful image and video processing operations.
  prefs: []
  type: TYPE_NORMAL
- en: The calcHist function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `calcHist()` function in OpenCV has the following Python signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The description of the parameters (as taken from the official OpenCV documentation)
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Parameter | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `images` | This parameter is the source arrays. They all should have the
    same depth, `CV_8U` or `CV_32F` , and the same size. Each of them can have an
    arbitrary number of channels. |'
  prefs: []
  type: TYPE_TB
- en: '| `channels` | This parameter is the list of the `dims` channels used to compute
    the histogram. |'
  prefs: []
  type: TYPE_TB
- en: '| `mask` | This parameter is the optional mask. If the matrix is not empty,
    it must be an 8-bit array of the same size as `images[i]`. The nonzero mask elements
    mark the array elements counted in the histogram. |'
  prefs: []
  type: TYPE_TB
- en: '| `histSize` | This parameter is the array of histogram sizes in each dimension.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `ranges` | This parameter is the array of the `dims` arrays of the histogram
    `bin` boundaries in each dimension. |'
  prefs: []
  type: TYPE_TB
- en: '| `hist` | This parameter is the output histogram, which is a dense or sparse
    `dims` (dimensional) array. |'
  prefs: []
  type: TYPE_TB
- en: '| `accumulate` | This parameter is the accumulation flag. If it is set, the
    histogram is not cleared in the beginning when it is allocated. This feature enables
    you to compute a single histogram from several sets of arrays, or to update the
    histogram in time. |'
  prefs: []
  type: TYPE_TB
- en: 'In our example, we calculate the histograms of the region of interest like
    so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This can be interpreted as the calculation of color histograms for an array
    of images containing only the region of interest in the HSV space. In this region,
    we compute only the image values corresponding to the mask values not equal to
    0, with `18` histogram columns, and with each histogram having `0` as the lower
    boundary and `180` as the upper boundary.
  prefs: []
  type: TYPE_NORMAL
- en: This is rather convoluted to describe but, once you have familiarized yourself
    with the concept of a histogram, the pieces of the puzzle should click into place.
  prefs: []
  type: TYPE_NORMAL
- en: The calcBackProject function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The other function that covers a vital role in the Meanshift algorithm (but
    not only this) is `calcBackProject`, which is short for **histogram back** **projection**
    (calculation). A histogram back projection is so called because it takes a histogram
    and projects it back onto an image, with the result being the probability that
    each pixel will belong to the image that generated the histogram in the first
    place. Therefore, `calcBackProject` gives a probability estimation that a certain
    image is equal or similar to a model image (from which the original histogram
    was generated).
  prefs: []
  type: TYPE_NORMAL
- en: Again, if you thought `calcHist` was a bit convoluted, `calcBackProject` is
    probably even more complex!
  prefs: []
  type: TYPE_NORMAL
- en: In summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `calcHist` function extracts a color histogram from an image, giving a statistical
    representation of the colors in an image, and `calcBackProject` helps in calculating
    the probability of each pixel of an image belonging to the original image.
  prefs: []
  type: TYPE_NORMAL
- en: Back to the code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s get back to our example. First our usual imports, and then we mark the
    initial region of interest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we extract and convert the ROI to HSV color space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we create a mask to include all pixels of the ROI with HSV values between
    the lower and upper bounds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we calculate the histograms of the ROI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: After the histograms are calculated, the values are normalized to be included
    within the range 0-255.
  prefs: []
  type: TYPE_NORMAL
- en: 'Meanshift performs a number of iterations before reaching convergence; however,
    this convergence is not assured. So, OpenCV allows us to pass so-called termination
    criteria, which is a way to specify the behavior of Meanshift with regard to terminating
    the series of calculations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In this particular case, we're specifying a behavior that instructs Meanshift
    to stop calculating the centroid shift after ten iterations or if the centroid
    has moved at least 1 pixel. That first flag (`EPS` or `CRITERIA_COUNT`) indicates
    we're going to use either of the two criteria (count or "epsilon", meaning the
    minimum movement).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a histogram calculated, and termination criteria for Meanshift,
    we can start our usual infinite loop, grab the current frame from the camera,
    and start processing it. The first thing we do is switch to HSV color space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have an HSV array, we can operate the long awaited histogram back
    projection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of `calcBackProject` is a matrix. If you printed it to console,
    it looks more or less like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Each pixel is represented with its probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'This matrix can the finally be passed into Meanshift, together with the track
    window and the termination criteria as outlined by the Python signature of `cv2.meanShift`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'So here it is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we calculate the new coordinates of the window, draw a rectangle to
    display it in the frame, and then show it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s it. You should by now have a good idea of color histograms, back projections,
    and Meanshift. However, there remains one issue to be resolved with the preceding
    program: the size of the window does not change with the size of the object in
    the frames being tracked.'
  prefs: []
  type: TYPE_NORMAL
- en: One of the authorities in computer vision and author of the seminal book, *Learning
    OpenCV*, *Gary Bradski*, *O'Reilly*, published a paper in 1988 to improve the
    accuracy of Meanshift, and described a new algorithm called **Continuously Adaptive
    Meanshift** (**CAMShift**), which is very similar to Meanshift but also adapts
    the size of the track window when Meanshift reaches convergence.
  prefs: []
  type: TYPE_NORMAL
- en: CAMShift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While CAMShift adds complexity to Meanshift, the implementation of the the preceding
    program using CAMShift is surprisingly (or not?) similar to the Meanshift example,
    with the main difference being that, after the call to `CamShift`, the rectangle
    is drawn with a particular rotation that follows the rotation of the object being
    tracked.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the code reimplemented with CAMShift:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The difference between the CAMShift code and the Meanshift one lies in these
    four lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The method signature of `CamShift` is identical to Meanshift.
  prefs: []
  type: TYPE_NORMAL
- en: The `boxPoints` function finds the vertices of a rotated rectangle, while the
    polylines function draws the lines of the rectangle on the frame.
  prefs: []
  type: TYPE_NORMAL
- en: 'By now, you should be familiar with the three approaches we adopted for tracking
    objects: basic motion detection, Meanshift, and CAMShift.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now explore another technique: the Kalman filter.'
  prefs: []
  type: TYPE_NORMAL
- en: The Kalman filter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Kalman filter is an algorithm mainly (but not only) developed by Rudolf
    Kalman in the late 1950s, and has found practical application in many fields,
    particularly navigation systems for all sorts of vehicles from nuclear submarines
    to aircrafts.
  prefs: []
  type: TYPE_NORMAL
- en: The Kalman filter operates recursively on streams of noisy input data (which
    in computer vision is normally a video feed) to produce a statistically optimal
    estimate of the underlying system state (the position inside the video).
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a quick example to conceptualize the Kalman filter and translate
    the preceding (purposely broad and generic) definition into plainer English. Think
    of a small red ball on a table, and imagine you have a camera pointing at the
    scene. You identify the ball as the subject to be tracked, and flick it with your
    fingers. The ball will start rolling on the table, following the laws of motion
    we're familiar with.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the ball is rolling at a speed of 1 meter per second (1 m/s) in a particular
    direction, you don''t need the Kalman filter to estimate where the ball will be
    in 1 second''s time: it will be 1 meter away. The Kalman filter applies these
    laws to predict an object''s position in the current video frame based on observations
    gathered in the previous frames. Naturally, the Kalman filter cannot know about
    a pencil on the table deflecting the course of the ball, but it can adjust for
    this kind of unforeseeable event.'
  prefs: []
  type: TYPE_NORMAL
- en: Predict and update
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'From the preceding description, we gather that the Kalman filter algorithm
    is divided into two phases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Predict**: In the first phase, the Kalman filter uses the covariance calculated
    up to the current point in time to estimate the object''s new position'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Update**: In the second phase, it records the object''s position and adjusts
    the covariance for the next cycle of calculations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This adjustment is—in OpenCV terms—a correction, hence the API of the `KalmanFilter`
    class in the Python bindings of OpenCV is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We can deduce that, in our programs, we will call `predict()` to estimate the
    position of an object, and `correct()` to instruct the Kalman filter to adjust
    its calculations.
  prefs: []
  type: TYPE_NORMAL
- en: An example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ultimately, we will aim to use the Kalman filter in combination with CAMShift
    to obtain the highest degree of accuracy and performance. However, before we go
    into such levels of complexity, let''s analyze a simple example, specifically
    one that seems to be very common on the Web when it comes to the Kalman filter
    and OpenCV: mouse tracking.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we will draw an empty frame and two lines: one corresponding
    to the actual movement of the mouse, and the other corresponding to the Kalman
    filter prediction. Here''s the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'As usual, let''s analyze it step by step. After the packages import, we create
    an empty frame, of size 800 x 800, and then initialize the arrays that will take
    the coordinates of the measurements and predictions of the mouse movements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we declare the mouse move `Callback` function, which is going to handle
    the drawing of the tracking. The mechanism is quite simple; we store the last
    measurements and last prediction, correct the Kalman with the current measurement,
    calculate the Kalman prediction, and finally draw two lines, from the last measurement
    to the current and from the last prediction to the current:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to initialize the window and set the `Callback` function.
    OpenCV handles mouse events with the `setMouseCallback` function; specific events
    must be handled using the first parameter of the `Callback` (event) function that
    determines what kind of event has been triggered (click, move, and so on):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we''re ready to create the Kalman filter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The Kalman filter class takes optional parameters in its constructor (from
    the OpenCV documentation):'
  prefs: []
  type: TYPE_NORMAL
- en: '`dynamParams`: This parameter states the dimensionality of the state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MeasureParams`: This parameter states the dimensionality of the measurement'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ControlParams`: This parameter states the dimensionality of the control'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vector.type`: This parameter states the type of the created matrices that
    should be `CV_32F` or `CV_64F`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I found the preceding parameters (both for the constructor and the Kalman properties)
    to work very well.
  prefs: []
  type: TYPE_NORMAL
- en: 'From this point on, the program is straightforward; every mouse movement triggers
    a Kalman prediction, both the actual position of the mouse and the Kalman prediction
    are drawn in the frame, which is continuously displayed. If you move your mouse
    around, you''ll notice that, if you make a sudden turn at high speed, the prediction
    line will have a wider trajectory, which is consistent with the momentum of the
    mouse movement at the time. Here''s a sample result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![An example](img/image00249.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A real-life example – tracking pedestrians
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Up to this point, we have familiarized ourselves with the concepts of motion
    detection, object detection, and object tracking, so I imagine you are anxious
    to put this newfound knowledge to good use in a real-life scenario. Let's do just
    that by examining the video feed of a surveillance camera and tracking pedestrians
    in it.
  prefs: []
  type: TYPE_NORMAL
- en: First of all, we need a sample video; if you download the OpenCV source, you
    will find the perfect video file for this purpose in `<opencv_dir>/samples/data/768x576.avi`.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the perfect asset to analyze, let's start building the application.
  prefs: []
  type: TYPE_NORMAL
- en: The application workflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The application will adhere to the following logic:'
  prefs: []
  type: TYPE_NORMAL
- en: Examine the first frame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Examine the following frames and perform background subtraction to identify
    pedestrians in the scene at the start of the scene.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Establish an ROI per pedestrian, and use Kalman/CAMShift to track giving an
    ID to each pedestrian.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Examine the next frames for new pedestrians entering the scene.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If this were a real-world application, you would probably store pedestrian information
    to obtain information such as the average permanence of a pedestrian in the scene
    and most likely routes. However, this is all beyond the remit of this example
    application.
  prefs: []
  type: TYPE_NORMAL
- en: In a real-world application, you would make sure to identify new pedestrians
    entering the scene, but for now, we'll focus on tracking those objects that are
    in the scene at the start of the video, utilizing the CAMShift and Kalman filter
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: You will find the code for this application in `chapter8/surveillance_demo/`
    of the code repository.
  prefs: []
  type: TYPE_NORMAL
- en: A brief digression – functional versus object-oriented programming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although most programmers are either familiar (or work on a constant basis)
    with **Object-oriented Programming** (**OOP**), I have found that, the more the
    years pass, the more I prefer **Functional Programming** (**FP**) solutions.
  prefs: []
  type: TYPE_NORMAL
- en: For those not familiar with the terminology, FP is a programming paradigm adopted
    by many languages that treats programs as the evaluation of mathematical functions,
    allows functions to return functions, and permits functions as arguments in a
    function. The strength of FP does not only reside in what it can do, but also
    in what it can avoid, or aims at avoiding side-effects and changing states. If
    the topic of functional programming has sparked an interest, make sure to check
    out languages such as Haskell, Clojure, or ML.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: What is a side-effect in programming terms? You can define a side effect as
    any function that changes any value that does not depend on the function's input.
    Python, along with many other languages, is susceptible to causing side-effects
    because—much like, for example, JavaScript—it allows access to global variables
    (and sometimes this access to global variables can be accidental!).
  prefs: []
  type: TYPE_NORMAL
- en: Another major issue encountered with languages that are not purely functional
    is the fact that a function's result will change over time, depending on the state
    of the variables involved. If a function takes an object as an argument—for example—and
    the computation relies on the internal state of that object, the function will
    return different results according to the changes in the object's state. This
    is something that very typically happens in languages, such as C and C++, in functions
    where one or more of the arguments are references to objects.
  prefs: []
  type: TYPE_NORMAL
- en: Why this digression? Because so far I have illustrated concepts using mostly
    functions; I did not shy away from accessing global variables where this was the
    simplest and most robust approach. However, the next program we will examine will
    contain OOP. So why do I choose to adopt OOP while advocating FP? Because OpenCV
    has quite an opinionated approach, which makes it hard to implement a program
    with a purely functional or object-oriented approach.
  prefs: []
  type: TYPE_NORMAL
- en: For example, any drawing function, such as `cv2.rectangle` and `cv2.circle`,
    modifies the argument passed into it. This approach contravenes one of the cardinal
    rules of functional programming, which is to avoid side-effects and changing states.
  prefs: []
  type: TYPE_NORMAL
- en: 'Out of curiosity, you could—in Python—redeclare the API of these drawing functions
    in a way that is more FP-friendly. For example, you could rewrite `cv2.rectangle`
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This approach—while computationally more expensive due to the `copy()` operation—allows
    the explicit reassignment of a frame, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'To conclude this digression, I will reiterate a belief very often mentioned
    in all programming forums and resources: there is no such thing as the best language
    or paradigm, only the best tool for the job in hand.'
  prefs: []
  type: TYPE_NORMAL
- en: So let's get back to our program and explore the implementation of a surveillance
    application, tracking moving objects in a video.
  prefs: []
  type: TYPE_NORMAL
- en: The Pedestrian class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main rationale behind the creation of a `Pedestrian` class is the nature
    of the Kalman filter. The Kalman filter can predict the position of an object
    based on historical observations and correct the prediction based on the actual
    data, but it can only do that for one object.
  prefs: []
  type: TYPE_NORMAL
- en: As a consequence, we need one Kalman filter per object tracked.
  prefs: []
  type: TYPE_NORMAL
- en: So the `Pedestrian` class will act as a holder for a Kalman filter, a color
    histogram (calculated on the first detection of the object and used as a reference
    for the subsequent frames), and information about the region of interest, which
    will be used by the CAMShift algorithm (the `track_window` parameter).
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we store the ID of each pedestrian for some fancy real-time info.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the `Pedestrian` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: At the core of the program lies the background subtractor object, which lets
    us identify regions of interest corresponding to moving objects.
  prefs: []
  type: TYPE_NORMAL
- en: When the program starts, we take each of these regions and instantiate a `Pedestrian`
    class, passing the ID (a simple counter), and the frame and track window coordinates
    (so we can extract the **Region of Interest** (**ROI**), and, from this, the HSV
    histogram of the ROI).
  prefs: []
  type: TYPE_NORMAL
- en: 'The constructor function (`__init__` in Python) is more or less an aggregation
    of all the previous concepts: given an ROI, we calculate its histogram, set up
    a Kalman filter, and associate it to a property (`self.kalman`) of the object.'
  prefs: []
  type: TYPE_NORMAL
- en: In the `update` method, we pass the current frame and convert it to HSV so that
    we can calculate the back projection of the pedestrian's HSV histogram.
  prefs: []
  type: TYPE_NORMAL
- en: We then use either CAMShift or Meanshift (depending on the argument passed;
    Meanshift is the default if no arguments are passed) to track the movement of
    the pedestrian, and correct the Kalman filter for that pedestrian with the actual
    position.
  prefs: []
  type: TYPE_NORMAL
- en: We also draw both CAMShift/Meanshift (with a surrounding rectangle) and Kalman
    (with a dot), so you can observe Kalman and CAMShift/Meanshift go nearly hand
    in hand, except for sudden movements that cause Kalman to have to readjust.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we print some pedestrian information on the top-left corner of the image.
  prefs: []
  type: TYPE_NORMAL
- en: The main program
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have a `Pedestrian` class holding all specific information for each
    object, let's take a look at the main function in the program.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we load a video (it could be a webcam), and then we initialize a background
    subtractor, setting 20 frames as the frames affecting the background model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We also create the main display window, and then set up a pedestrians dictionary
    and a `firstFrame` flag, which we''re going to use to allow a few frames for the
    background subtractor to build history, so it can better identify moving objects.
    To help with this, we also set up a frame counter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we start the loop. We read camera frames (or video frames) one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We let `BackgroundSubtractorKNN` build the history for the background model,
    so we don''t actually process the first 20 frames; we only pass them into the
    subtractor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we process the frame with the approach explained earlier in the chapter,
    by applying a process of dilation and erosion on the foreground mask so as to
    obtain easily identifiable blobs and their bounding boxes. These are obviously
    moving objects in the frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the contours are identified, we instantiate one pedestrian per contour
    for the first frame only (note that I set a minimum area for the contour to further
    denoise our detection):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, for each pedestrian detected, we perform an `update` method passing the
    current frame, which is needed in its original color space, because the pedestrian
    objects are responsible for drawing their own information (text and Meanshift/CAMShift
    rectangles, and Kalman filter tracking):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We set the `firstFrame` flag to `False`, so we don''t instantiate any more
    pedestrians; we just keep track of the ones we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we show the result in the display window. The program can be exited
    by pressing the *Esc* key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'There you have it: CAMShift/Meanshift working in tandem with the Kalman filter
    to track moving objects. All being well, you should obtain a result similar to
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The main program](img/image00250.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In this screenshot, the blue rectangle is the CAMShift detection and the green
    rectangle is the Kalman filter prediction with its center at the blue circle.
  prefs: []
  type: TYPE_NORMAL
- en: Where do we go from here?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This program constitutes a basis for your application domain''s needs. There
    are many improvements that can be made building on the program above to suit an
    application''s additional requirements. Consider the following examples:'
  prefs: []
  type: TYPE_NORMAL
- en: You could destroy a pedestrian object if Kalman predicts its position to be
    outside the frame
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You could check whether each detected moving object is corresponding to existing
    pedestrian instances, and if not, create an instance for it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You could train an SVM and operate classification on each moving object to establish
    whether or not the moving object is of the nature you intend to track (for instance,
    a dog might enter the scene but your application requires to only track humans)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whatever your needs, hopefully this chapter will have provided you with the
    necessary knowledge to build applications that satisfy your requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter explored the vast and complex topic of video analysis and tracking
    objects.
  prefs: []
  type: TYPE_NORMAL
- en: We learned about video background subtraction with a basic motion detection
    technique that calculates frame differences, and then moved to more complex and
    efficient tools such as `BackgroundSubtractor`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then explored two very important video analysis algorithms: Meanshift and
    CAMShift. In the course of this, we talked in detail about color histograms and
    back projections. We also familiarized ourselves with the Kalman filter, and its
    usefulness in a computer vision context. Finally, we put all our knowledge together
    in a sample surveillance application, which tracks moving objects in a video.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that our foundation in OpenCV and machine learning is solidifying, we are
    ready to tackle artificial neural networks and dive deeper into artificial intelligence
    with OpenCV and Python in the next chapter.
  prefs: []
  type: TYPE_NORMAL
