<html><head></head><body>
<div id="_idContainer065" class="Content">
<h1 id="_idParaDest-93"><a id="_idTextAnchor097"></a>
 Classification</h1>
</div>
<div id="_idContainer066" class="Content">
<h2>Learning Objectives</h2>
<p>By the end of this chapter, you will be able to:</p>
<ul>
<li class="bullets">Describe the fundamental concepts of classification</li>
<li class="bullets">Load and preprocess data for classification</li>
<li class="bullets">Implement k-nearest neighbor and support vector machine classifiers</li>
</ul>
<p>In chapter will focus on the goals of classification, and learn about k-nearest neighbors and support vector machines.</p>
</div>
<div id="_idContainer067" class="Content">
<p class="hidden" data-amznremoved-m8="true" data-amznremoved="mobi7">4</p>
</div>
<div id="_idContainer075" class="Content">
<h2 id="_idParaDest-94"><a id="_idTextAnchor098"></a>
 Introduction</h2>
<p>In this chapter, we will learn about classifiers, especially the k-nearest neighbor classifier and support vector machines. We will use this classification to categorize data. Just as we did for regression, we will build a classifier based on training data, and test the performance of our classifier using testing data.</p>
<h2 id="_idParaDest-95"><a id="_idTextAnchor099"></a>
 The Fundamentals of Classification</h2>
<p>While regression focuses on creating a model that best fits our data to predict the future, classification is all about creating a model that separates our data into separate classes.</p>
<p>Assuming that you have some data belonging to separate classes, classification helps you predict the class a new data point belongs to. A classifier is a model that determines the label value belonging to any data point in the domain. Suppose you have a set of points, <strong class="inline _idGenCharOverride-1">P = {p1, p2, p3, ..., pm}</strong>
 , and another set of points, <strong class="inline _idGenCharOverride-1">Q = {q1, q2, q3, ..., qn}</strong>
 . You treat these points as members of different classes. For simplicity, we could imagine that <strong class="inline _idGenCharOverride-1">P</strong>
 contains credit-worthy individuals, and <strong class="inline _idGenCharOverride-1">Q</strong>
 contains individuals that are risky in terms of their credit repayment tendencies.</p>
<p>You can divide the state space so that all points in <strong class="inline _idGenCharOverride-1">P</strong>
 are on one cluster of the state space, and then disjoint from the state space cluster containing all points in <strong class="inline _idGenCharOverride-1">Q</strong>
 . Once you find these bounded spaces, called <strong class="keyword _idGenCharOverride-2">clusters</strong>
 , inside the state space, you have successfully performed <strong class="bold _idGenCharOverride-2">clustering</strong>
 .</p>
<p>Suppose we have a point, <strong class="inline _idGenCharOverride-1">x,</strong>
 that's not equal to any of the previous points. Does point <strong class="inline _idGenCharOverride-1">x</strong>
 belong to cluster <strong class="inline _idGenCharOverride-1">P</strong>
 or cluster <strong class="inline _idGenCharOverride-1">Q</strong>
 ? The answer to this question is a <strong class="bold _idGenCharOverride-2">classification exercise</strong>
 , because we classify point <strong class="inline _idGenCharOverride-1">x</strong>
 .</p>
<p class="_idGenParaOverride-1">Classification is usually determined by proximity. The closer point <strong class="inline _idGenCharOverride-1">x</strong>
 is to points in cluster <strong class="inline _idGenCharOverride-1">P</strong>
 , the more likely it is that it belongs to cluster <strong class="inline _idGenCharOverride-1">P</strong>
 . This is the idea behind nearest neighbor classification. In the case of k-nearest neighbor classification, we find the k-nearest neighbor of point <strong class="inline _idGenCharOverride-1">x</strong>
 and classify it according to the maximum number of the nearest neighbors from the same class. Besides k-nearest neighbor, we will also use support vector machines for classification. In this chapter, we will be covering this credit scoring method in detail.</p>
<div></div>
<p>We could either assemble random dummy data ourselves, or we could choose to use an online dataset with hundreds of data points. To make this learning experience as realistic as possible, we will choose the latter. Let's continue with an exercise that lets us download some data that we can use for classification. A popular place for downloading machine learning datasets is <a href="https://archive.ics.uci.edu/ml/datasets.html">https://archive.ics.uci.edu/ml/datasets.html</a>
 . You can find five different datasets on credit approval. We will now load the dataset on German credit approvals, because the size of 1,000 data points is perfect for an example, and its documentation is available.</p>
<p>
<strong class="bold _idGenCharOverride-2">The</strong>
 <strong class="inline _idGenCharOverride-1">german</strong>
 <strong class="bold _idGenCharOverride-2">dataset is available in the CSV format</strong>
</p>
<p>CSV stands for comma-separated values. A CSV file is a simple text file, where each line of the file contains a data point in your dataset. The attributes of the data point are given in a fixed order, separated by a separator character such as a comma. This character may not occur in the data, otherwise we would not know if the separator character is part of the data or serves as a separator. Although the name comma-separated values suggests that the separator character is a comma, it is not always the case. For instance, in our example, the separator character is a space. CSV files are used in many applications, including Excel. CSV is a great, lean interface between different applications.</p>
<h3 id="_idParaDest-96"><a id="_idTextAnchor100"></a>
 Exercise 10: Loading Datasets</h3>
<ol>
<li value="1">Visit <a href="https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29">https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29</a>
 . The data files are located at <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/">https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/</a>
 .<p class="_idGenParaOverride-2">Load the data from the space-separated <strong class="inline _idGenCharOverride-1">german.data</strong>
 file. Make sure that you add headers to your DataFrame so that you can reference your features and labels by name instead of column number.</p>
<p class="_idGenParaOverride-2">Save the <strong class="inline _idGenCharOverride-1">german.data</strong>
 file locally. Insert header data into your CSV file.</p>
<p class="_idGenParaOverride-2">The first few lines of the dataset are as follows:</p>
<p class="snippet _idGenParaOverride-3">A11 6 A34 A43 1169 A65 A75 4 A93 A101 4 A121 67 A143 A152 2 A173 1 A192 A201 1</p>
<p class="snippet _idGenParaOverride-3">A12 48 A32 A43 5951 A61 A73 2 A92 A101 2 A121 22 A143 A152 1 A173 1 A191 A201 2</p>
<p class="snippet _idGenParaOverride-4">A14 12 A34 A46 2096 A61 A74 2 A93 A101 3 A121 49 A143 A152 1 A172 2 A191 A201 1</p>
<div></div>
<p class="_idGenParaOverride-2">The explanation to interpret this data is in the <strong class="inline _idGenCharOverride-1">german.doc</strong>
 file, where you can see the list of attributes. These attributes are: Status of existing checking account (A11 – A14), Duration (numeric, number of months), Credit history (A30 - A34), Purpose of credit (A40 – A410), Credit amount (numeric), Savings account/bonds (A61 – A65), Present employment since (A71 – A75), Disposable income percent rate (numeric), Personal status and sex (A91 – A95), Other debtors and guarantors (A101 – A103), Present residence since (numeric), Property (A121 – A124), Age (numeric, years), Other installment plans (A141 – A143), Housing (A151 – A153), Number of existing credits at this bank, Job (A171 – A174), Number of people being liable to provide maintenance for (numeric) Telephone (A191 – A192) and Foreign worker (A201 – A202).</p>
<p class="_idGenParaOverride-2">The result of classification would be as follows: <strong class="inline _idGenCharOverride-1">1 means good debtor, while 2 means bad debtor</strong>
 .</p>
<p class="_idGenParaOverride-2">Our task is to determine how to separate the state space of twenty input variables into two clusters: good debtors and bad debtors.</p>
</li>
<li value="2">We will use the pandas library to load the data. Before loading the data, though, I suggest adding a header to the <strong class="inline _idGenCharOverride-1">german.data</strong>
 file. Insert the following header line before the first line:<p class="snippet _idGenParaOverride-3">CheckingAccountStatus DurationMonths CreditHistory CreditPurpose CreditAmount SavingsAccount EmploymentSince DisposableIncomePercent PersonalStatusSex OtherDebtors PresentResidenceMonths Property Age OtherInstallmentPlans Housing NumberOfExistingCreditsInBank Job LiabilityNumberOfPeople Phone ForeignWorker CreditScore</p>
<p class="_idGenParaOverride-2">Notice that the preceding header is just one line, which means that there is no newline character until the end of the 21st label, <strong class="inline _idGenCharOverride-1">CreditScore</strong>
 .</p>
<h4 class="_idGenParaOverride-5">Note</h4>
<p class="callout _idGenParaOverride-5">The header is of help because pandas can interpret the first line as the column name. In fact, this is the default behavior of the <strong class="inline _idGenCharOverride-3">read_csv</strong>
 method of pandas. The first line of the <strong class="inline _idGenCharOverride-3">.csv</strong>
 file is going to be the header, and the rest of the lines are the actual data.</p>
<p class="_idGenParaOverride-2">Let's import the CSV data using the <strong class="inline _idGenCharOverride-1">pandas.read_csv</strong>
 method:</p>
<p class="snippet _idGenParaOverride-3">import pandas</p>
<p class="snippet _idGenParaOverride-4">data_frame = pandas.read_csv('german.data', sep=' ')</p>
<div></div>
</li>
<li value="3">The first argument of <strong class="inline _idGenCharOverride-1">read_csv</strong>
 is the file path. If you saved it to the E drive of your Windows PC, for instance, then you can also write an absolute path there: <em class="italics _idGenCharOverride-4">e:\german.data</em>
 .</li>
</ol>
<div class="_idGenObjectLayout-1">
<div id="_idContainer068" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00042.jpg" alt="" />
</div>
</div>
<h6>Figure 4.1: Table displaying list of attributes in respective cells</h6>
<p>Let's see the format of the data. The <strong class="inline _idGenCharOverride-1">data_frame.head()</strong>
 call prints the first five rows of the CSV file, structured by the pandas DataFrame:</p>
<p class="snippet">data_frame.head()</p>
<p>The output will be as follows:</p>
<p class="snippet">  CheckingAccountStatus DurationMonths CreditHistory CreditPurpose \</p>
<p class="snippet">0                 A11             6         A34         A43</p>
<p class="snippet">..</p>
<p class="snippet">4                 A11             24         A33         A40</p>
<p class="snippet">   CreditAmount SavingsAccount EmploymentSince DisposableIncomePercent \</p>
<p class="snippet">0         1169            A65             A75                        4</p>
<p class="snippet">..</p>
<p class="snippet">4         4870            A61             A73                        3</p>
<p class="snippet">  PersonalStatusSex OtherDebtors     ...     Property Age \</p>
<p class="snippet">0             A93         A101     ...         A121 67</p>
<p class="snippet">..</p>
<p class="snippet">4             A93         A101     ...         A124 53</p>
<p class="snippet">   OtherInstallmentPlans Housing NumberOfExistingCreditsInBank Job \</p>
<p class="snippet">0                 A143    A152                             2 A173</p>
<p class="snippet">..</p>
<p class="snippet">4                 A143    A153                             2 A173</p>
<p class="snippet">  LiabilityNumberOfPeople Phone ForeignWorker CreditScore</p>
<p class="snippet">0                     1 A192         A201         1</p>
<p class="snippet">..</p>
<p class="snippet">4                     2 A191         A201         2</p>
<p class="snippet">[5 rows x 21 columns]</p>
<p>We have successfully loaded the data into the DataFrame.</p>
<h3 id="_idParaDest-97"><a id="_idTextAnchor101"></a>
 Data Preprocessing</h3>
<p>Before building a classifier, we are better off formatting our data so that we can keep relevant data in the most suitable format for classification, and removing all data that we are not interested in.</p>
<p>1. <strong class="bold _idGenCharOverride-2">Replacing or dropping values</strong>
</p>
<p>For instance, if there are <strong class="inline _idGenCharOverride-1">N/A</strong>
 (or <strong class="inline _idGenCharOverride-1">NA</strong>
 ) values in the dataset, we may be better off substituting these values with a numeric value we can handle. NA stands for Not Available. We may choose to ignore rows with NA values or replace them with an outlier value. An outlier value is a value such as -1,000,000 that clearly stands out from regular values in the dataset. The replace method of a DataFrame does this type of replacement. The replacement of NA values with an outlier looks as follows:</p>
<p class="snippet">data_frame.replace('NA', -1000000, inplace=True)</p>
<p>The replace method changes all NA values to numeric values.</p>
<p>This numeric value should be far from any reasonable values in the DataFrame. Minus one million is recognized by the classifier as an exception, assuming that only positive values are there.</p>
<p>The alternative to replacing unavailable data with extreme values is dropping the rows that have unavailable data:</p>
<p class="snippet _idGenParaOverride-1">data_frame.dropna(0, inplace=True)</p>
<div></div>
<p>The first argument specifies that we drop rows, not columns. The second argument specifies that we perform the drop operation, without cloning the DataFrame. Dropping the NA values is less desirable, as you often lose a reasonable chunk of your dataset.</p>
<p>2. <strong class="bold _idGenCharOverride-2">Dropping columns</strong>
</p>
<p>If there is a column we do not want to include in the classification, we are better off dropping it. Otherwise, the classifier may detect false patterns in places where there is absolutely no correlation. For instance, your phone number itself is very unlikely to correlate with your credit score. It is a 9 to 12-digit number that may very easily feed the classifier with a lot of noise. So we drop the phone column.</p>
<p class="snippet">data_frame.drop(['Phone'], 1, inplace=True)</p>
<p>The second argument indicates that we drop columns, not rows. The first argument is an enumeration of the columns we would like to drop. The <strong class="inline _idGenCharOverride-1">inplace</strong>
 argument is so that the call modifies the original DataFrame.</p>
<p>3. <strong class="bold _idGenCharOverride-2">Transforming data</strong>
</p>
<p>Oftentimes, the data format we are working with is not always optimal for the classification process. We may want to transform our data into a different format for multiple reasons, such as the following:</p>
<ul>
<li>To highlight aspects of data we are interested in (for example, Minmax scaling or normalization)</li>
<li>To drop aspects of data we are not interested in (for example, Binarization)</li>
<li>Label encoding</li>
</ul>
<p>
<strong class="keyword _idGenCharOverride-2">Minmax scaling</strong>
 can be performed by the <strong class="inline _idGenCharOverride-1">MinMaxScaler</strong>
 method of the scikit preprocessing utility:</p>
<p class="snippet">from sklearn import preprocessing</p>
<p class="snippet">data = np.array([</p>
<p class="snippet">    [19, 65],</p>
<p class="snippet">    [4, 52],</p>
<p class="snippet">    [2, 33]</p>
<p class="snippet">])</p>
<p class="snippet _idGenParaOverride-1">preprocessing.MinMaxScaler(feature_range=(0,1)).fit_transform(data)</p>
<div></div>
<p>The output is as follows:</p>
<p class="snippet"> array([[1.        , 1.        ],</p>
<p class="snippet">     [0.11764706, 0.59375 ],</p>
<p class="snippet">     [0.        , 0.        ]])</p>
<p>
<strong class="inline _idGenCharOverride-1">MinMaxScaler</strong>
 scales each column in the data so that the lowest number in the column becomes 0, the highest number becomes 1, and all of the values in between are proportionally scaled between zero and one.</p>
<p>
<strong class="keyword _idGenCharOverride-2">Binarization</strong>
 transforms data into ones and zeros based on a condition:</p>
<p class="snippet">preprocessing.Binarizer(threshold=10).transform(data)</p>
<p class="snippet">array([[1, 1],</p>
<p class="snippet">     [0, 1],</p>
<p class="snippet">     [0, 1]])</p>
<p>
<strong class="keyword _idGenCharOverride-2">Label encoding</strong>
 is important for preparing your features for scikit-learn to process. While some of your features are string labels, scikit-learn expects this data to be numbers.</p>
<p>This is where the preprocessing library of scikit-learn comes into play.</p>
<h4>Note</h4>
<p class="callout">You might have noticed that in the credit scoring example, there were two data files. One contained labels in string form, and the other in integer form. I asked you to load the data with string labels on purpose so that you got some experience of how to preprocess data properly with the label encoder.</p>
<p>Label encoding is not rocket science. It creates a mapping between string labels and numeric values so that we can supply numbers to scikit-learn:</p>
<p class="snippet">from sklearn import preprocessing</p>
<p class="snippet">labels = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']</p>
<p class="snippet">label_encoder = preprocessing.LabelEncoder()</p>
<p class="snippet _idGenParaOverride-1">label_encoder.fit(labels)</p>
<div></div>
<p>Let's enumerate the encoding:</p>
<p class="snippet">[x for x in enumerate(label_encoder.classes_)]</p>
<p>The output will be as follows:</p>
<p class="snippet">[(0, 'Friday'),</p>
<p class="snippet"> (1, 'Monday'),</p>
<p class="snippet"> (2, 'Thursday'),</p>
<p class="snippet"> (3, 'Tuesday'),</p>
<p class="snippet"> (4, 'Wednesday')]</p>
<p>We can use the encoder to transform values:</p>
<p class="snippet">encoded_values = label_encoder.transform(['Wednesday', 'Friday'])</p>
<p>The output will be as follows:</p>
<p class="snippet"> array([4, 0], dtype=int64)</p>
<p>The inverse transformation that transforms encoded values back to labels is performed by the <strong class="inline _idGenCharOverride-1">inverse_transform</strong>
 function:</p>
<p class="snippet">label_encoder.inverse_transform([0, 4])</p>
<p>The output will be as follows:</p>
<p class="snippet"> array(['Wednesday', 'Friday'], dtype='&lt;U9')</p>
<h3 id="_idParaDest-98">Exercise 11: Pre-Processing Data<a id="_idTextAnchor102"></a>
</h3>
<p>In this exercise, we will use a dataset with pandas.</p>
<ol>
<li class="ParaOverride-1" value="1">Load the CSV data of the 2017-2018 January kickstarter projects from <a href="https://github.com/TrainingByPackt/Artificial-Intelligence-and-Machine-Learning-Fundamentals/blob/master/Lesson04/Exercise%2011%20Pre-processing%20Data/ks-projects-201801.csv">https://github.com/TrainingByPackt/Artificial-Intelligence-and-Machine-Learning-Fundamentals/blob/master/Lesson04/Exercise%2011%20Pre-processing%20Data/ks-projects-201801.csv</a>
 and apply the preprocessing steps on the loaded data.<h4 class="_idGenParaOverride-5">Note</h4>
<p class="callout _idGenParaOverride-6">Note that you need a working internet connection to complete this exercise.</p>
<div></div>
</li>
<li value="2">If you open the file, you will see that you don't have to bother adding a header, because it is included in the CSV file:<p class="snippet _idGenParaOverride-3">ID,name,category,main_category,currency,deadline,goal,launched,pledged,state,backers,country,usd pledged,usd_pledged_real,usd_goal_real</p>
</li>
<li value="3">Import the data and create a DataFrame using pandas:<p class="snippet _idGenParaOverride-3">import pandas</p>
<p class="snippet _idGenParaOverride-3">data_frame = pandas.read_csv('ks-projects-201801.csv', sep=',')</p>
<p class="snippet _idGenParaOverride-3">data_frame.head()</p>
</li>
<li value="4">The previous command prints the first five entries belonging to the dataset. We can see the name and format of each column. Now that we have the data, it's time to perform some preprocessing steps.</li>
<li value="5">Suppose you have some NA or N/A values in the dataset. You can replace them with the following <strong class="inline _idGenCharOverride-1">replace</strong>
 operations:<p class="snippet _idGenParaOverride-3">   data_frame.replace('NA', -1000000, inplace=True)</p>
<p class="snippet _idGenParaOverride-3">   data_frame.replace('N/A', -1000000, inplace=True)</p>
</li>
<li value="6">When performing classification or regression, keeping the ID column is just asking for trouble. In most cases, the ID does not correlate with the end result. Therefore, it makes sense to drop the ID column:<p class="snippet _idGenParaOverride-3">data_frame.drop(['ID'], 1, inplace=True)</p>
</li>
<li value="7">Suppose we are only interested in whether the projects had backers or not. This is a perfect case for binarization:<p class="snippet _idGenParaOverride-3">from sklearn import preprocessing</p>
<p class="snippet _idGenParaOverride-3">preprocessing.Binarizer(threshold=1).transform([data_frame['backers']])</p>
</li>
<li value="8">The output will be as follows:<p class="snippet _idGenParaOverride-3"> array([[0, 1, 1, ..., 0, 1, 1]], dtype=int64)</p>
<h4 class="_idGenParaOverride-5">Note</h4>
<p class="callout _idGenParaOverride-6">We are discarding the resulting binary array. To make use of the binary data, we would have to replace the backers column with it. We will omit this step for simplicity.</p>
<div></div>
</li>
<li value="9">Let's encode the labels so that they become numeric values that can be interpreted by the classifier:<p class="snippet _idGenParaOverride-3">labels = ['AUD', 'CAD', 'CHF', 'DKK', 'EUR', 'GBP', 'HKD', 'JPY', 'MXN', 'NOK', 'NZD', 'SEK', 'SGD', 'USD']</p>
<p class="snippet _idGenParaOverride-3">label_encoder = preprocessing.LabelEncoder()</p>
<p class="snippet _idGenParaOverride-3">label_encoder.fit(labels)</p>
<p class="snippet _idGenParaOverride-3">label_encoder.transform(data_frame['currency'])</p>
</li>
<li value="10">The output will be as follows:<p class="snippet _idGenParaOverride-3"> array([ 5, 13, 13, ..., 13, 13, 13], dtype=int64)</p>
</li>
</ol>
<p>You have to know about all possible labels that can occur in your file. The documentation is responsible for providing you with the available options. In the unlikely case that the documentation is not available for you, you have to reverse engineer the possible values from the file.</p>
<p>Once the encoded array is returned, the same problem holds as in the previous point: we have to make use of these values by replacing the <strong class="inline _idGenCharOverride-1">currency</strong>
 column of the DataFrame with these new values.</p>
<h3 id="_idParaDest-99"><a id="_idTextAnchor103"></a>
 Minmax Scaling of the Goal Column</h3>
<p>When Minmax scaling was introduced, you saw that instead of scaling the values of each vector in a matrix, the values of each coordinate in each vector were scaled together. This is how the matrix structure describes a dataset. One vector contains all attributes of a data point. When scaling just one attribute, we have to transpose the column we wish to scale.</p>
<p>You learned about the transpose operation of NumPy in <em class="italics _idGenCharOverride-4">Chapter 1</em>
 , <em class="italics _idGenCharOverride-4">Principles of Artificial Intelligence</em>
 :</p>
<p class="snippet">import numpy as np</p>
<p class="snippet">values_to_scale = np.mat([data_frame['goal']]).transpose()</p>
<p>Then, we have to apply the <strong class="inline _idGenCharOverride-1">MinMaxScaler</strong>
 to scale the transposed values. To get the results in one array, we can transpose the results back to their original form:</p>
<p class="snippet">preprocessing</p>
<p class="snippet">    .MinMaxScaler(feature_range=(0,1))</p>
<p class="snippet">    .fit_transform(values_to_scale)</p>
<p class="snippet _idGenParaOverride-1">    .transpose()</p>
<div></div>
<p>The output is as follows:</p>
<p class="snippet">array([[9.999900e-06, 2.999999e-04, 4.499999e-04, ..., 1.499999e-04, 1.499999e-04, 1.999990e-05]])</p>
<p>The values look weird because there were some high goals on Kickstarter, possibly using seven figure values. Instead of linear Minmax scaling, it is also possible to use the magnitude and scale logarithmically, counting how many digits the goal price has. This is another transformation that could make sense for reducing the complexity of the classification exercise.</p>
<p>As always, you have to place the results in the corresponding column of the DataFrame to make use of the transformed values.</p>
<p>We will stop preprocessing here. Hopefully, the usage of these different methods is now clear, and you will have a strong command of using these preprocessing methods in the future.</p>
<h3 id="_idParaDest-100"><a id="_idTextAnchor104"></a>
 Identifying Features and Labels</h3>
<p>Similar to regression, in classification, we must also separate our features and labels. Continuing from the original example, our features are all columns, except the last one, which contains the result of the credit scoring. Our only label is the credit scoring column.</p>
<p>We will use NumPy arrays to store our features and labels:</p>
<p class="snippet">import numpy as np</p>
<p class="snippet">features = np.array(data_frame.drop(['CreditScore'], 1))</p>
<p class="snippet">label = np.array(data_frame['CreditScore'])</p>
<p>Now that we are ready with our features and labels, we can use this data for cross-validation.</p>
<h3 id="_idParaDest-101"><a id="_idTextAnchor105"></a>
 Cross-Validation with scikit-learn</h3>
<p>Another pertinent point about regression is that we can use cross-validation to train and test our model. This process is exactly the same as in the case of regression problems:</p>
<p class="snippet">from sklearn import model_selection</p>
<p class="snippet">features_train, features_test, label_train, label_test =</p>
<p class="snippet">    model_selection.train_test_split(</p>
<p class="snippet">        features,</p>
<p class="snippet">        label,</p>
<p class="snippet">        test_size=0.1</p>
<p class="snippet">    )</p>
<p>The <strong class="inline _idGenCharOverride-1">train_test_split</strong>
 method shuffles and then splits our features and labels into a training dataset and a testing dataset. We can specify the size of the testing dataset as a number between <strong class="inline _idGenCharOverride-1">0</strong>
 and <strong class="inline _idGenCharOverride-1">1</strong>
 . A <strong class="inline _idGenCharOverride-1">test_size</strong>
 of <strong class="inline _idGenCharOverride-1">0.1</strong>
 means that <strong class="inline _idGenCharOverride-1">10%</strong>
 of the data will go into the testing dataset.</p>
<h3 id="_idParaDest-102"><a id="_idTextAnchor106"></a>
 Activity 7: Preparing Credit Data for Classification</h3>
<p><a id="_idTextAnchor107"></a>
 In this section, we will discuss how to prepare data for a classifier. We will be using <strong class="inline _idGenCharOverride-1">german.data</strong>
 from <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/">https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/</a>
 as an example and will prepare the data for training and testing a classifier. Make sure that all of your labels are numeric, and that the values are prepared for classification. Use 80% of the data points as training data:</p>
<ol>
<li class="ParaOverride-1" value="1">Save <strong class="inline _idGenCharOverride-1">german.data</strong>
 from <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/">https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/</a>
 and open it in a text editor such as Sublime Text or Atom. Add the header row to it.</li>
<li value="2">Import the data file using pandas and replace the NA values with an outlier value.</li>
<li value="3">Perform label encoding. Transform all of the labels in the data frame into integers.</li>
<li value="4">Separate features from labels. We can apply the same method as the one we saw in the theory section.</li>
<li value="5">Perform scaling of the training and testing data together. Use <strong class="inline _idGenCharOverride-1">MinMaxScaler</strong>
 from Scikit's Preprocessing library.</li>
<li value="6">The final step is cross-validation. Shuffle our data and use 80% of all data for training and 20% for testing.<h4 class="_idGenParaOverride-5">Note</h4>
<p class="callout _idGenParaOverride-5">The solution to this activity is available at page 276.</p>
</li>
</ol>
<h3 id="_idParaDest-103"><a id="_idTextAnchor108"></a>
 The k-nearest neighbor Classifier</h3>
<p class="_idGenParaOverride-1">We will continue from where we left off in the first topic. We have training and testing data, and it is now time to prepare our classifier to perform k-nearest neighbor classification. After introducing the K-Nearest Neighbor algorithm, we will use scikit-learn to perform classification.</p>
<div></div>
<h3 id="_idParaDest-104"><a id="_idTextAnchor109"></a>
 Introducing the K-Nearest Neighbor Algorithm</h3>
<p>The goal of classification algorithms is to divide data so that we can determine which data points belong to which region. Suppose that a set of classified points is given. Our task is to determine which class a new data point belongs to.</p>
<p>The k-nearest neighbor classifier receives classes of data points with given feature and label values. The goal of the algorithm is to classify data points. These data points contain feature coordinates, and the objective of the classification is to determine the label values. Classification is based on proximity. Proximity is defined as a Euclidean distance. Point <strong class="inline _idGenCharOverride-1">A</strong>
 is closer to point <strong class="inline _idGenCharOverride-1">B</strong>
 than to point <strong class="inline _idGenCharOverride-1">C</strong>
 if the Euclidean distance between <strong class="inline _idGenCharOverride-1">A</strong>
 and <strong class="inline _idGenCharOverride-1">B</strong>
 is shorter than the Euclidean distance between <strong class="inline _idGenCharOverride-1">A</strong>
 and <strong class="inline _idGenCharOverride-1">C</strong>
 .</p>
<p>The k-nearest neighbor classifier gets the k-nearest neighbors of a data point. The label belonging to point A is the most frequently occurring label value among the k-nearest neighbors of point A. Determining the value of <strong class="inline _idGenCharOverride-1">K</strong>
 is a non-obvious task. Obviously, if there are two groups, such as credit-worthy and not credit-worthy, we need K to be 3 or greater, because otherwise, with <strong class="inline _idGenCharOverride-1">K=2</strong>
 , we could easily have a tie between the number of neighbors. In general, though, the value of K does not depend on the number of groups or the number of features.</p>
<p>A special case of k-nearest neighbors is when <strong class="inline _idGenCharOverride-1">K=1</strong>
 . In this case, the classification boils down to finding the nearest neighbor of a point. <strong class="inline _idGenCharOverride-1">K=1</strong>
 most often gives us significantly worse results than <strong class="inline _idGenCharOverride-1">K=3</strong>
 or greater.</p>
<h3 id="_idParaDest-105"><a id="_idTextAnchor110"></a>
 Distance Functions</h3>
<p>Many distance metrics could work with the k-nearest neighbor algorithm. We will now calculate the Euclidean and the Manhattan distance of two data points. The Euclidean distance is a generalization of the way we calculate the distance of two points in the plane or in a three-dimensional space.</p>
<p>The distance between points <strong class="inline _idGenCharOverride-1">A = (a1, a2, …, an)</strong>
 and <strong class="inline _idGenCharOverride-1">B=(b1, b2, …, bn)</strong>
 is the length of the line segment connecting these two points:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer069" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00043.jpg" alt="Figure 4.1 Distance between points A and B" />
</div>
</div>
<h6 class="_idGenParaOverride-1">Figure 4.2: Distance between points A and B</h6>
<div></div>
<p>Technically, we don't need to calculate the square root when we are just looking for the nearest neighbors, because the square root is a monotone function.</p>
<p>As we will use the Euclidean distance in this book, let's see how to calculate the distance of multiple points using one scikit-learn function call. We have to import <strong class="inline _idGenCharOverride-1">euclidean_distances</strong>
 from <strong class="inline _idGenCharOverride-1">sklearn.metrics.pairwise</strong>
 . This function accepts two sets of points and returns a matrix that contains the pairwise distance of each point from the first and the second sets of points:</p>
<p class="snippet">from sklearn.metrics.pairwise import euclidean_distances</p>
<p class="snippet">points = [[2,3], [3,7], [1,6]]</p>
<p class="snippet">euclidean_distances([[4,4]], points)</p>
<p>The output is as follows:</p>
<p class="snippet">array([[2.23606798, 3.16227766, 3.60555128]])</p>
<p>For instance, the distance of (4,4) and (3,7) is approximately 3.162.</p>
<p>We can also calculate the Euclidean distances between points in the same set:</p>
<p class="snippet">euclidean_distances(points)</p>
<p class="snippet">array([[0.        , 4.12310563, 3.16227766],</p>
<p class="snippet">     [4.12310563, 0.        , 2.23606798],</p>
<p class="snippet">     [3.16227766, 2.23606798, 0.        ]])</p>
<p>
<strong class="bold _idGenCharOverride-2">The Manhattan/Hamming Distance</strong>
</p>
<p>The Hamming and Manhattan distances represent the same formula.</p>
<p>The Manhattan distance relies on calculating the absolute value of the difference of the coordinates of the data points:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer070" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00044.jpg" alt="Figure 4.2 The Manhattan and Hamming Distance" />
</div>
</div>
<h6>Figure 4.3: The Manhattan and Hamming Distance</h6>
<p class="_idGenParaOverride-1">The Euclidean distance is a more accurate generalization of distance, while the Manhattan distance is slightly easier to calculate.</p>
<div></div>
<h3 id="_idParaDest-106"><a id="_idTextAnchor111"></a>
 Exercise 12: Illustrating the K-nearest Neighbor Classifier Algorithm</h3>
<p>Suppose we have a list of employee data. Our features are the numbers of hours worked per week and yearly salary. Our label indicates whether an employee has stayed with our company for more than two years. The length of stay is represented by a zero if it is less than two years, and a one in case it is greater than or equal to two years.</p>
<p>We would like to create a 3-nearest neighbor classifier that determines whether an employee stays with our company for at least two years.</p>
<p>Then, we would like to use this classifier to predict whether an employee with a request to work 32 hours a week and earning 52,000 dollars per year is going to stay with the company for two years or not.</p>
<p>The dataset is as follows:</p>
<p class="snippet">employees = [</p>
<p class="snippet">    [20, 50000, 0],</p>
<p class="snippet">    [24, 45000, 0],</p>
<p class="snippet">    [32, 48000, 0],</p>
<p class="snippet">    [24, 55000, 0],</p>
<p class="snippet">    [40, 50000, 0],</p>
<p class="snippet">    [40, 62000, 1],</p>
<p class="snippet">    [40, 48000, 1],</p>
<p class="snippet">    [32, 55000, 1],</p>
<p class="snippet">    [40, 72000, 1],</p>
<p class="snippet">    [32, 60000, 1]</p>
<p class="snippet">]</p>
<ol>
<li class="ParaOverride-1" value="1">Scale the features:<p class="snippet _idGenParaOverride-3">import matplotlib.pyplot as plot</p>
<p class="snippet _idGenParaOverride-3">from sklearn import preprocessing</p>
<p class="snippet _idGenParaOverride-3">import numpy as np</p>
<p class="snippet _idGenParaOverride-3">from sklearn.preprocessing import MinMaxScaler</p>
<p class="snippet _idGenParaOverride-3">scaled_employees = preprocessing.MinMaxScaler(feature_range=(0,1))</p>
<p class="snippet _idGenParaOverride-4">    .fit_transform(employees)</p>
<div></div>
<p class="_idGenParaOverride-2">The scaled result is as follows:</p>
<p class="snippet _idGenParaOverride-3">array([[0.        , 0.18518519, 0.        ],</p>
<p class="snippet _idGenParaOverride-3">     [0.2     , 0.        , 0.        ],</p>
<p class="snippet _idGenParaOverride-3">     [0.6     , 0.11111111, 0.        ],</p>
<p class="snippet _idGenParaOverride-3">     [0.2     , 0.37037037, 0.        ],</p>
<p class="snippet _idGenParaOverride-3">     [1.        , 0.18518519, 0.        ],</p>
<p class="snippet _idGenParaOverride-3">     [1.        , 0.62962963, 1.        ],</p>
<p class="snippet _idGenParaOverride-3">     [1.        , 0.11111111, 1.        ],</p>
<p class="snippet _idGenParaOverride-3">     [0.6     , 0.37037037, 1.        ],</p>
<p class="snippet _idGenParaOverride-3">     [1.        , 1.        , 1.        ],</p>
<p class="snippet _idGenParaOverride-3">     [0.6     , 0.55555556, 1.        ]])</p>
<p class="_idGenParaOverride-2">It makes sense to scale our requested employee as well at this point: <em class="italics _idGenCharOverride-4">[32, 52000]</em>
 becomes <em class="italics _idGenCharOverride-4">[ (32-24)/(40 - 24), (52000-45000)/(72000 - 45000)] = [0.5, 0.25925925925925924]</em>
 .</p>
</li>
<li value="2">Plot these points on a two-dimensional plane such that the first two coordinates represent a point on the plane, and the third coordinate determines the color of the point:<p class="snippet _idGenParaOverride-3">import matplotlib.pyplot as plot</p>
<p class="snippet _idGenParaOverride-3">[&amp;#9;</p>
<p class="snippet _idGenParaOverride-3">    plot.scatter(x[0], x[1], color = 'g' if x[2] &gt; 0.5 else 'r')</p>
<p class="snippet _idGenParaOverride-3">    for x in scaled_employees</p>
<p class="snippet _idGenParaOverride-3">] + [plot.scatter(0.5, 0.25925925925925924, color='b')]</p>
<p class="_idGenParaOverride-2">The output is as follows:</p>
<div id="_idContainer071" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00045.jpg" alt="Figure 4.3 Points plotted on a two-dimensional plane" />
</div>
<h6 class="_idGenParaOverride-8">Figure 4.4: Points plotted on a two-dimensional plane</h6>
<div></div>
</li>
<li value="3">To calculate the distance of the blue point and all the other points, we will apply the transpose function from <em class="italics _idGenCharOverride-4">Chapter 1, Principles of AI</em>
 . If we transpose the <strong class="inline _idGenCharOverride-1">scaledEmployee</strong>
 matrix, we get three arrays of ten. The feature values are in the first two arrays. We can simply use the <strong class="inline _idGenCharOverride-1">[:2]</strong>
 index to keep them. Then, transposing this matrix back to its original form gives us the array of feature data points:<p class="snippet _idGenParaOverride-3">scaled_employee_features = scaled_employees.transpose()[:2].transpose()</p>
<p class="snippet _idGenParaOverride-3">scaled_employee_features</p>
<p class="_idGenParaOverride-2">The output is as follows:</p>
<p class="snippet _idGenParaOverride-3"> array([[0.        , 0.18518519],</p>
<p class="snippet _idGenParaOverride-3">    [0.2     , 0.        ],</p>
<p class="snippet _idGenParaOverride-3">    [0.6     , 0.11111111],</p>
<p class="snippet _idGenParaOverride-3">    [0.2     , 0.37037037],</p>
<p class="snippet _idGenParaOverride-3">    [1.        , 0.18518519],</p>
<p class="snippet _idGenParaOverride-3">    [1.        , 0.62962963],</p>
<p class="snippet _idGenParaOverride-3">    [1.        , 0.11111111],</p>
<p class="snippet _idGenParaOverride-3">    [0.6     , 0.37037037],</p>
<p class="snippet _idGenParaOverride-3">    [1.        , 1.        ],</p>
<p class="snippet _idGenParaOverride-3">     [0.6     , 0.55555556]])</p>
</li>
<li value="4">Calculate the Euclidean distance using:<p class="snippet _idGenParaOverride-3">from sklearn.metrics.pairwise import euclidean_distances</p>
<p class="snippet _idGenParaOverride-3">euclidean_distances(</p>
<p class="snippet _idGenParaOverride-3">    [[0.5, 0.25925925925925924]],</p>
<p class="snippet _idGenParaOverride-3">    scaled_employee_features</p>
<p class="snippet _idGenParaOverride-3">)</p>
<p class="_idGenParaOverride-2">The output is as follows:</p>
<p class="snippet _idGenParaOverride-3"> array([[0.50545719, 0.39650393, 0.17873968, 0.31991511, 0.50545719,</p>
<p class="snippet _idGenParaOverride-3">        0.62223325, 0.52148622, 0.14948471, 0.89369841, 0.31271632]])</p>
</li>
</ol>
<p>The shortest distances are as follows:</p>
<ul>
<li>
<strong class="inline _idGenCharOverride-1">0.14948471</strong>
 for the point <strong class="inline _idGenCharOverride-1">[0.6, 0.37037037, 1.]</strong>
</li>
<li>
<strong class="inline _idGenCharOverride-1">0.17873968</strong>
 for the point <strong class="inline _idGenCharOverride-1">[0.6, 0.11111111, 0.]</strong>
</li>
<li class="_idGenParaOverride-1">
<strong class="inline _idGenCharOverride-1">0.31271632</strong>
 for the point [<strong class="inline _idGenCharOverride-1">0.6, 0.55555556, 1.]</strong>
</li>
<li style="list-style: none; display: inline"><div></div>
</li>
</ul>
<p>As two out of the three points have a label of 1, we found two green points and one red point. This means that our 3-nearest neighbor classifier classified the new employee as being more likely to stay for at least two years than not at all.</p>
<h4>Note</h4>
<p class="callout">Although, the fourth point just missed the top three by a very small margin. In fact, our algorithm would have found a tie if there were two points of a different color that had the third smallest distance from the target. In case of a race condition in distances, there could be a tie. This is an edge case, though, which should almost never occur in real-life problems.</p>
<h3 id="_idParaDest-107"><a id="_idTextAnchor112"></a>
 Exercise 13: k-nearest Neighbor Classification in scikit-learn</h3>
<ol>
<li class="ParaOverride-1" value="1">Split our data into four categories: <strong class="inline _idGenCharOverride-1">training</strong>
 and <strong class="inline _idGenCharOverride-1">testing</strong>
 , <strong class="inline _idGenCharOverride-1">features</strong>
 , and <strong class="inline _idGenCharOverride-1">labels</strong>
 :<p class="snippet _idGenParaOverride-3">from sklearn import model_selection</p>
<p class="snippet _idGenParaOverride-3">import pandas</p>
<p class="snippet _idGenParaOverride-3">import numpy as np</p>
<p class="snippet _idGenParaOverride-3">from sklearn import preprocessing</p>
<p class="snippet _idGenParaOverride-3">features_train, features_test, label_train, label_test =</p>
<p class="snippet _idGenParaOverride-3">model_selection.train_test_split(</p>
<p class="snippet _idGenParaOverride-3">    scaled_features,</p>
<p class="snippet _idGenParaOverride-3">    label,</p>
<p class="snippet _idGenParaOverride-3">    test_size=0.2</p>
<p class="snippet _idGenParaOverride-3">)</p>
</li>
<li value="2">Create a K-Nearest Neighbor classifier to perform this classification:<p class="snippet _idGenParaOverride-3">from sklearn import neighbors</p>
<p class="snippet _idGenParaOverride-3">classifier = neighbors.KNeighborsClassifier()</p>
<p class="snippet _idGenParaOverride-3">classifier.fit(features_train, label_train)</p>
<p class="_idGenParaOverride-2">Since we have not mentioned the value of K, the default is <em class="italics _idGenCharOverride-4">5</em>
 .</p>
</li>
<li value="3">Check how well our classifier performs on the test data:<p class="snippet _idGenParaOverride-3">classifier.score(features_test, label_test)</p>
<p class="_idGenParaOverride-2">The output is <strong class="inline _idGenCharOverride-1">0.665</strong>
 .</p>
</li>
</ol>
<p class="_idGenParaOverride-1">You might find higher ratings with other datasets, but it is understandable that more than 20 features may easily contain some random noise that makes it difficult to classify data.</p>
<div></div>
<h3 id="_idParaDest-108"><a id="_idTextAnchor113"></a>
 Exercise 14: Prediction with the k-nearest neighbors classifier</h3>
<p>This code is built on the code of previous exercise.</p>
<ol>
<li class="ParaOverride-1" value="1">We'll create a data point that we will classify by taking the i<em class="italics CharOverride-1">th</em>
 element of the i<em class="italics CharOverride-1">th</em>
 test data point:<p class="snippet _idGenParaOverride-3">data_point = [None] * 20</p>
<p class="snippet _idGenParaOverride-3">for i in range(20):</p>
<p class="snippet _idGenParaOverride-3">    data_point[i] = features_test[i][i]</p>
<p class="snippet _idGenParaOverride-3">data_point = np.array(data_point)</p>
</li>
<li value="2">We have a one-dimensional array. The classifier expects an array containing data point arrays. Therefore, we must reshape our data point into an array of data points:<p class="snippet _idGenParaOverride-3">data_point = data_point.reshape(1, -1)</p>
</li>
<li value="3">With this, we have created a completely random persona, and we are interested in whether they are classified as credit-worthy or not:<p class="snippet _idGenParaOverride-3">credit_rating = classifier.predict(data_point)</p>
<p class="_idGenParaOverride-2">Now, we can safely use prediction to determine the credit rating of the data point:</p>
<p class="snippet _idGenParaOverride-3">classifier.predict(data_point)</p>
<p class="_idGenParaOverride-2">The output is as follows:</p>
<p class="snippet _idGenParaOverride-3">array([1], dtype=int64)</p>
</li>
</ol>
<p>We have successfully rated a new user based on input data.</p>
<h3 id="_idParaDest-109"><a id="_idTextAnchor114"></a>
 Parameterization of the k-nearest neighbor Classifier in scikit-learn</h3>
<p>You can access the documentation of the k-nearest neighbor classifier here: <a href="http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html">http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html</a>
 .</p>
<p>The parameterization of the classifier may fine-tune the accuracy of your classifier. Since we haven't learned all of the possible variations of k-nearest neighbor, we will concentrate on the parameters that you already understand based on this topic.</p>
<p class="_idGenParaOverride-1">
<strong class="bold _idGenCharOverride-2">n_neighbors</strong>
 : This is the k value of the k-nearest neighbor algorithm. The default value is 5.</p>
<div></div>
<p>
<strong class="bold _idGenCharOverride-2">metric</strong>
 : When creating the classifier, you will see a weird name – "Minkowski". Don't worry about this name – you have learned about the first and second order Minkowski metric already. This metric has a power parameter. For <em class="italics _idGenCharOverride-4">p=1</em>
 , the Minkowski metric is the same as the Manhattan metric. For <em class="italics _idGenCharOverride-4">p=2</em>
 , the Minkowski metric is the same as the Euclidean metric.</p>
<p>
<em class="italics _idGenCharOverride-4">p</em>
 : This is the power of the Minkowski metric. The default value is 2.</p>
<p>You have to specify these parameters once you create the classifier:</p>
<p class="snippet">classifier = neighbors.KNeighborsClassifier(n_neighbors=50)</p>
<h3 id="_idParaDest-110"><a id="_idTextAnchor115"></a>
 Activity 8: Increasing the Accuracy of Credit Scoring</h3>
<p>In this section, we will learn how the parameterization of the k-nearest neighbor classifier affects the end result. The accuracy of credit scoring is currently quite low: 66.5%. Find a way to increase it by a few percentage points. To ensure that this happens correctly, you will need to have done the previous exercises.</p>
<p>There are many ways to complete this exercise. In this solution, I will show you one way to increase the credit score, which will be done by changing the parameterization:</p>
<ol>
<li class="ParaOverride-1" value="1">Increase the k-value of the k-nearest neighbor classifier from the default 5 to 10, 15, 25, and 50.</li>
<li value="2">Run this classifier for all four <strong class="inline _idGenCharOverride-1">n_neighbors</strong>
 values and observe the results.</li>
<li value="3">Higher K values do not necessarily mean a better score. In this example, though, <strong class="inline _idGenCharOverride-1">K=50</strong>
 yielded a better result than <strong class="inline _idGenCharOverride-1">K=5</strong>
 .<h4 class="_idGenParaOverride-5">Note</h4>
<p class="callout _idGenParaOverride-5">The solution to this activity is available at page 280.</p>
</li>
</ol>
<h2 id="_idParaDest-111"><a id="_idTextAnchor116"></a>
 Classification with Support Vector Machines</h2>
<p class="_idGenParaOverride-1">We first used support vector machines for regression in Chapter 3, <em class="italics _idGenCharOverride-4">Regression</em>
 . In this topic, you will find out how to use support vector machines for classification. As always, we will use scikit-learn to run our examples in practice.</p>
<div></div>
<h3 id="_idParaDest-112"><a id="_idTextAnchor117"></a>
 What are Support Vector Machine Classifiers?</h3>
<p>The goal of a support vector machines defined on an n-dimensional vector space is to find a surface in that n-dimensional space that separates the data points in that space into multiple classes.</p>
<p>In two dimensions, this surface is often a straight line. In three dimensions, the support vector machines often finds a plane. In general, the support vector machines finds a hyperplane. These surfaces are optimal in the sense that, based on the information available to the machine, it optimizes the separation of the n-dimensional spaces.</p>
<p>The optimal separator found by the support vector machines is called the <strong class="bold _idGenCharOverride-2">best separating hyperplane</strong>
 .</p>
<p>A support vector machines is used to find one surface that separates two sets of data points. In other words, support vector machines are <strong class="bold _idGenCharOverride-2">binary classifiers</strong>
 . This does not mean that support vector machines can only be used for binary classification. Although we were only talking about one plane, support vector machines can be used to partition a space into any number of classes by generalizing the task itself.</p>
<p>The separator surface is optimal in the sense that it maximizes the distance of each data point from the separator surface.</p>
<p>A vector is a mathematical structure defined on an n-dimensional space having a magnitude (length) and a direction. In two dimensions, you draw the vector (x, y) from the origin to the point (x, y). Based on geometry, you can calculate the length of the vector using the Pythagorean theorem, and the direction of the vector by calculating the angle between the horizontal axis and the vector.</p>
<p>For instance, in two dimensions, the vector (3, -4) has the following magnitude:</p>
<p>
<strong class="inline _idGenCharOverride-1">sqrt( 3 * 3 + 4 * 4 ) = sqrt( 25 ) = 5</strong>
</p>
<p>And it has the following direction:</p>
<p class="_idGenParaOverride-1">
<strong class="inline _idGenCharOverride-1">np.arctan(-4/3) / 2 / np.pi * 360 = -53.13010235415597 degrees</strong>
</p>
<div></div>
<h3 id="_idParaDest-113"><a id="_idTextAnchor118"></a>
 Understanding Support Vector Machines</h3>
<p>Suppose that two sets of points, <strong class="bold _idGenCharOverride-2">Red</strong>
 and <strong class="bold _idGenCharOverride-2">Blue</strong>
 , are given. For simplicity, we can imagine a two-dimensional plane with two features: one mapped on the horizontal axis, and one on the vertical axis.</p>
<p>The objective of the support vector machine is to find the best separating line that separates points <strong class="bold _idGenCharOverride-2">A</strong>
 , <strong class="bold _idGenCharOverride-2">D</strong>
 , <strong class="bold _idGenCharOverride-2">C</strong>
 , <strong class="bold _idGenCharOverride-2">B</strong>
 , and <strong class="bold _idGenCharOverride-2">H</strong>
 from points <strong class="bold _idGenCharOverride-2">E</strong>
 , <strong class="bold _idGenCharOverride-2">F</strong>
 , and <strong class="bold _idGenCharOverride-2">G</strong>
 :</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer072" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00046.jpg" alt="Figure 4.4 Line separating red and blue members" />
</div>
</div>
<h6 class="_idGenParaOverride-1">Figure 4.5: Line separating red and blue members</h6>
<div></div>
<p>Separation is not always that obvious. For instance, if there is a blue point in between E, F, and G, there is no line that could separate all points without errors. If points in the blue class form a full circle around the points in the red class, there is no straight line that could separate the two sets:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer073" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00047.jpg" alt="Figure 4.5 Graph with two outlier points" />
</div>
</div>
<h6>Figure 4.6: Graph with two outlier points</h6>
<p class="_idGenParaOverride-1">For instance, in the preceding graph, we tolerate two outlier points, O and P.</p>
<div></div>
<p>In the following solution, we do not tolerate outliers, and instead of a line, we create a best separating path consisting of two half lines:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer074" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00048.jpg" alt="Figure 4.6 Graph removing the separation of the two outliers" />
</div>
</div>
<h6>Figure 4.7: Graph removing the separation of the two outliers</h6>
<p>A perfect separation of all data points is rarely worth the resources. Therefore, the support vector machine can be regularized to simplify and restrict the definition of the best separating shape and to allow outliers.</p>
<p>The <strong class="keyword _idGenCharOverride-2">regularization parameter</strong>
 of a support vector machine determines the rate of error to allow or forbid misclassifications.</p>
<p class="_idGenParaOverride-1">A support vector machine has a kernel parameter. A linear kernel strictly uses a linear equation for describing the best separating hyperplane. A polynomial kernel uses a polynomial, while an exponential kernel uses an exponential expression to describe the hyperplane.</p>
<div></div>
<p>A margin is an area centered around the separator and is bounded by the points closest to the separator. A balanced margin has points from each class that are equidistant from the line.</p>
<p>When it comes to defining the allowed error rate of the best separating hyperplane, a gamma parameter decides whether only the points near the separator count in determining the position of the separator, or whether the points farthest from the line count, too. The higher the gamma, the lower the amount of points that influence the location of the separator.</p>
<h3 id="_idParaDest-114"><a id="_idTextAnchor119"></a>
 Support Vector Machines in scikit-learn</h3>
<p>Our entry point is the end result of previous activity. Once we have split the training and test data, we are ready to set up the classifier:</p>
<p class="snippet">features_train, features_test, label_train, label_test = model_selection</p>
<p class="snippet">    .train_test_split(</p>
<p class="snippet">        scaled_features,</p>
<p class="snippet">        label,</p>
<p class="snippet">        test_size=0.2</p>
<p class="snippet">    )</p>
<p>Instead of using the K-Nearest Neighbor classifier, we will use the <strong class="inline _idGenCharOverride-1">svm.SVC()</strong>
 classifier:</p>
<p class="snippet">from sklearn import svm</p>
<p class="snippet">classifier = svm.SVC()</p>
<p class="snippet">classifier.fit(features_train, label_train)</p>
<p class="snippet"># Let's can check how well our classifier performs on the</p>
<p class="snippet"># test data:</p>
<p class="snippet">classifier.score(features_test, label_test)</p>
<p>The output is <strong class="inline _idGenCharOverride-1">0.745</strong>
 .</p>
<p class="_idGenParaOverride-1">It seems that the default support vector machine classifier of scikit-learn does a slightly better job than the k-nearest neighbor classifier.</p>
<div></div>
<h3 id="_idParaDest-115"><a id="_idTextAnchor120"></a>
 Parameters of the scikit-learn SVM</h3>
<p>The following are the parameters of the scikit-learn SVM:</p>
<p>
<strong class="keyword _idGenCharOverride-2">Kernel</strong>
 : This is a string or callable parameter specifying the kernel used in the algorithm. The predefined kernels are linear, poly, rbf, sigmoid, and precomputed. The default value is rbf.</p>
<p>
<strong class="keyword _idGenCharOverride-2">Degree</strong>
 : When using a polynomial, you can specify the degree of the polynomial. The default value is 3.</p>
<p>
<strong class="keyword _idGenCharOverride-2">Gamma</strong>
 : This is the kernel coefficient for rbf, poly, and sigmoid. The default value is auto, computed as 1/<strong class="inline _idGenCharOverride-1">number_of_features</strong>
 .</p>
<p>
<strong class="keyword _idGenCharOverride-2">C</strong>
 : This is a floating-point number with a default of 1.0 describing the penalty parameter of the error term.</p>
<p>You can read about rest parameters in the reference documentation at <a href="http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html">http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html</a>
 .</p>
<p>Here is an example of SVM:</p>
<p class="snippet">classifier = svm.SVC(kernel="poly", C=2, degree=4, gamma=0.05)</p>
<h3 id="_idParaDest-116"><a id="_idTextAnchor121"></a>
 Activity 9: Support Vector Machine Optimization in scikit-learn</h3>
<p>In this section, we will discuss how to use the different parameters of a support vector machine classifier. We will be using, comparing, and contrasting the different support vector regression classifier parameters you have learned about and will find a set of parameters resulting in the highest classification data on the training and testing data that we loaded and prepared in the previous activity. To ensure that you can complete this activity, you will need to have completed the first activity of this chapter.</p>
<p>We will try out a few combinations. You may have to choose different parameters and check the results:</p>
<ol>
<li class="ParaOverride-1" value="1">Let's first choose the linear kernel and check the classifier's fit and score.</li>
<li value="2">Once you are done with that, choose the polynomial kernel of degree 4, C=2, and gamma=0.05 and check the classifier's fit and score.</li>
<li class="_idGenParaOverride-1" value="3">Then, choose the polynomial kernel of degree 4, C=2, and gamma=0.25 and check the classifier's fit and score.<div></div>
</li>
<li value="4">After that, select the polynomial kernel of degree 4, C=2, and gamma=0.5 and check the classifier's fit and score.</li>
<li value="5">Choose the next classifier as sigmoid kernel.</li>
<li value="6">Lastly, choose the default kernel with a gamma of 0.15 and check the classifier's fit and score.<h4 class="_idGenParaOverride-5">Note</h4>
<p class="callout _idGenParaOverride-5">The solution for this activity can be found on page 280.</p>
</li>
</ol>
<h2 id="_idParaDest-117"><a id="_idTextAnchor122"></a>
 Summary</h2>
<p>In this chapter, we learned the basics of classification. After discovering the goals of classification, and loading and formatting data, we discovered two classification algorithms: K-Nearest Neighbors and support vector machines. We used custom classifiers based on these two methods to predict values. In the next chapter, we will use trees for predictive analysis.</p>
</div>
</body></html>