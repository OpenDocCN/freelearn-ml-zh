- en: '*Chapter 11*: Hyperparameter Tuning and Automated Machine Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第11章*：超参数调整和自动机器学习'
- en: In the previous chapter, we learned how to train **convolutional neural networks**
    and complex **deep neural networks**. When training these models, we are often
    confronted with difficult choices in terms of the various parameters we should
    use, such as the number of layers, filter dimensions, the type and order of layers,
    regularization, batch size, learning rate, the number of epochs, and many more.
    And this is not only the case for DNNs – the same challenges arise when we need
    to select the correct preprocessing steps, features, models, and model parameters
    in statistical ML approaches.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们学习了如何训练**卷积神经网络**和复杂的**深度神经网络**。在训练这些模型时，我们经常面临各种参数的艰难选择，例如层数、滤波器维度、层的类型和顺序、正则化、批量大小、学习率、训练轮数等。这不仅适用于DNN，当我们需要选择正确的预处理步骤、特征、模型和模型参数时，统计机器学习方法也会出现同样的挑战。
- en: In this chapter, we will look at optimizing the training process to remove some
    of the non-optimal human choices in ML. This will help you train better models
    faster and more efficiently without manual intervention. First, we will explore
    **hyperparameter optimization** (also called **HyperDrive** in Azure Machine Learning),
    a standard technique for optimizing parameters in an ML process. By evaluating
    different sampling techniques for hyperparameter sampling, such as random sampling,
    grid sampling, and Bayesian optimization, you will learn how to efficiently trade
    model runtime for model performance.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨优化训练过程以移除机器学习（ML）中的一些非最佳人类选择。这将帮助您更快、更高效地训练更好的模型，而无需人工干预。首先，我们将探讨**超参数优化**（在Azure机器学习中也称为**HyperDrive**），这是一种优化ML过程中参数的标准技术。通过评估不同的超参数采样技术，例如随机采样、网格采样和贝叶斯优化，您将学习如何有效地在模型运行时间和模型性能之间进行权衡。
- en: In the second half of this chapter, we will look at model optimization by automating
    the complete end-to-end ML training process using **Automated Machine Learning**.
    This process is also often referred to as **AutoML**. Using Automated Machine
    Learning, we can optimize preprocessing, feature engineering, model selection,
    hyperparameter tuning, and model stacking all in one abstract optimization pipeline.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后半部分，我们将探讨通过使用**自动机器学习**自动化完整的端到端ML训练过程来进行模型优化。这个过程也常被称为**AutoML**。使用自动机器学习，我们可以在一个抽象的优化管道中优化预处理、特征工程、模型选择、超参数调整和模型堆叠。
- en: One benefit of Azure Machine Learning is that both parameter optimization (HyperDrive)
    and model optimization (Automated Machine Learning) are supported in the same
    generalized way. This means we can deploy both to an auto-scaling training cluster,
    store the best model or parameter combination on disk, and then deploy the best
    model to production without ever leaving our notebook environment.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Azure机器学习的一个好处是，参数优化（HyperDrive）和模型优化（自动机器学习）都支持相同的通用方式。这意味着我们可以将它们部署到自动扩展的训练集群中，将最佳模型或参数组合存储在磁盘上，然后在不离开笔记本环境的情况下将最佳模型部署到生产环境中。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Finding the optimal model parameters with HyperDrive
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用HyperDrive找到最佳模型参数
- en: Finding the optimal model with Automated Machine Learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自动机器学习找到最佳模型
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will use the following Python libraries and versions to
    create decision-tree based ensemble classifiers:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用以下Python库和版本来创建基于决策树的集成分类器：
- en: '`azureml-core 1.34.0`'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`azureml-core 1.34.0`'
- en: '`azureml-sdk 1.34.0`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`azureml-sdk 1.34.0`'
- en: Similar to the previous chapters, you can run this code using either a local
    Python interpreter or a notebook environment hosted in Azure Machine Learning.
    However, all the scripts need to be scheduled in Azure Machine Learning training
    clusters.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章类似，您可以使用本地Python解释器或Azure机器学习中的笔记本环境运行此代码。然而，所有脚本都需要在Azure机器学习训练集群中安排。
- en: 'All the code examples in this chapter can be found in this book''s GitHub repository:
    [https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter11](https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter11).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有代码示例都可以在这个书的GitHub仓库中找到：[https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter11](https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter11)。
- en: Finding the optimal model parameters with HyperDrive
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用HyperDrive寻找最佳模型参数
- en: In ML, we typically deal with either parametric or non-parametric models. Models
    represent the distribution of the training data to make predictions for unseen
    data from the same distribution. While parametric models (such as linear regression,
    logistic regression, and neural networks) represent the training data distribution
    by using a learned set of parameters, non-parametric models describe the training
    data distribution through other traits, such as decision trees (all tree-based
    classifiers), training samples (k-nearest neighbors), or weighted training samples
    (support vector machine).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们通常处理参数模型或非参数模型。模型代表训练数据的分布，以对同一分布中的未见数据进行预测。虽然参数模型（如线性回归、逻辑回归和神经网络）通过使用学习到的参数集来表示训练数据分布，但非参数模型通过其他特征来描述训练数据分布，如决策树（所有基于树的分类器）、训练样本（k-最近邻）或加权的训练样本（支持向量机）。
- en: '**Parametric models** such as linear or logistic regression are typically defined
    by a constant number of parameters that are independent of the training data.
    These models make strong assumptions about the training data, so they often require
    fewer training samples. As a result, both training and inferencing are usually
    very fast.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**参数模型**，如线性或逻辑回归，通常由一个与训练数据无关的常数参数数量定义。这些模型对训练数据做出了强烈的假设，因此通常需要较少的训练样本。因此，训练和推理通常都非常快。'
- en: In comparison, for **non-parametric models** such as decision trees or k-nearest
    neighbors, the number of traits usually increases with the number of training
    samples. While these models don't assume anything about the training data distribution,
    many training samples are required. This often leads to slow training and slow
    interference performance.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，对于**非参数模型**，如决策树或k-最近邻，特征的数量通常随着训练样本数量的增加而增加。虽然这些模型不对训练数据的分布做任何假设，但通常需要许多训练样本。这往往会导致训练速度慢和干扰性能慢。
- en: 'The term **hyperparameter** refers to all the parameters that are used to configure
    and tune the training process of parametric or non-parametric models. The following
    is a list of some typical hyperparameters in a neural network:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 术语**超参数**指的是用于配置和调整参数模型或非参数模型训练过程的全部参数。以下是一个神经网络中一些典型超参数的列表：
- en: The number of hidden layers
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层的数量
- en: The number of units per layer
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每层的单元数量
- en: Batch size
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批处理大小
- en: Filter dimensions
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤维度
- en: Learning rate
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率
- en: Regularization terms
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化项
- en: Dropout
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout
- en: Loss metric
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失度量
- en: The number of hyperparameters and parameter values for training a simple ML
    model is astonishing. Have you ever caught yourself manually tweaking a parameter
    in your training processes, such as the number of splits in a decision-based classifier
    or the number of units in a neural network classifier? If so, you are not alone!
    However, it's very important to accept that manually tweaking parameters requires
    deep expertise in the specific model or model configuration. However, we can't
    possibly be an expert in every type of statistical modeling, ML, and optimization
    to tune all the possible parameters manually. Given that the number of parameter
    choices is enormous, it is not feasible to try all possible combinations, so we
    need to find a better way to optimize them.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个简单的机器学习模型所需的超参数数量和参数值是惊人的。你是否曾经发现自己手动调整训练过程中的参数，比如决策分类器中的分割数量或神经网络分类器中的单元数量？如果是这样，你并不孤单！然而，接受手动调整参数需要对该特定模型或模型配置有深厚的专业知识是非常重要的。然而，我们不可能成为每种统计建模、机器学习和优化的专家，以便手动调整所有可能的参数。鉴于参数选择数量巨大，尝试所有可能的组合是不切实际的，因此我们需要找到一种更好的方法来优化它们。
- en: Not only can we not possibly try all the distinct combinations of parameters
    manually, but in many cases, we also can't possibly predict the outcome of a tweak
    in a hyperparameter, even with expert knowledge. In such scenarios, we can start
    looking at finding the optimal set of parameters automatically. This process is
    called **hyperparameter tuning** or **hyperparameter search**.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅不可能手动尝试所有不同的参数组合，而且在许多情况下，即使有专家知识，我们也无法预测超参数调整的结果。在这种情况下，我们可以开始寻找自动找到最佳参数集的方法。这个过程被称为**超参数调整**或**超参数搜索**。
- en: Hyperparameter tuning entails automatically testing a model's performance against
    different sets of hyperparameter combinations and ultimately choosing the best
    combination of hyperparameters. The definition of the *best performance* depends
    on the chosen metric and validation method. For example, stratified-fold cross-validation
    with the f1-score metric will yield a different set of optimal parameters than
    the accuracy metric with k-fold cross-validation.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调整涉及自动测试模型针对不同超参数组合集的性能，并最终选择最佳的超参数组合。最佳性能的定义取决于选择的指标和验证方法。例如，使用f1分数指标的分层交叉验证将产生与k折交叉验证的准确度指标不同的最佳参数集。
- en: One reason why we are discussing hyperparameter tuning (and Automated Machine
    Learning later) in this book is that we have a competitive advantage from using
    elastic cloud computing infrastructure. While it is difficult to train hundreds
    of models sequentially on your laptop, it is easy to train thousands of models
    in parallel in the cloud using cheap auto-scaling compute. Using cheap cloud storage,
    we can also persist all potentially good models for later analysis. Many of the
    recent ML papers have shown that we can often achieve better results by using
    more compute power and/or better parameter choices.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这本书中讨论超参数调整（以及稍后讨论的自动机器学习）的一个原因是，我们使用弹性云计算基础设施具有竞争优势。虽然在你笔记本电脑上按顺序训练数百个模型是困难的，但在云中使用廉价的自动扩展计算并行训练数千个模型则很容易。使用廉价的云存储，我们还可以持久化所有潜在的好模型以供后续分析。许多最近的机器学习论文表明，通过使用更多的计算能力或更好的参数选择，我们通常可以实现更好的结果。
- en: Before we begin tuning hyperparameters, we want to remind you of the importance
    of a baseline model. For many practical ML tasks, you should be able to achieve
    good performance using a single tree-based ensemble classifier or a pre-trained
    neural network with default parameters. If this is not the case, hyperparameter
    tuning won't magically output parameters for a top-performing best-in-class model.
    In this case, it would be better to go back to data preprocessing and feature
    engineering to build a better baseline model first, before tuning the batch sizes,
    the number of hidden units, or the number of trees.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始调整超参数之前，我们想提醒您基线模型的重要性。对于许多实际的机器学习任务，您应该能够使用单个基于树的集成分类器或具有默认参数的预训练神经网络实现良好的性能。如果情况不是这样，超参数调整不会神奇地输出顶级最佳模型的参数。在这种情况下，最好是先回到数据预处理和特征工程，构建一个更好的基线模型，然后再调整批量大小、隐藏单元数量或树的数量。
- en: Another issue to avoid with hyperparameter tuning is overfitting and focusing
    on the wrong performance metric or validation method. As with any other optimization
    technique, hyperparameter tuning will yield the best parameter combination for
    a given loss function or metric. Therefore, it is essential to validate your loss
    function before starting hyperparameter tuning.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在超参数调整中需要避免的另一个问题是过拟合以及关注错误的表现指标或验证方法。与任何其他优化技术一样，超参数调整将产生给定损失函数或指标的最佳参数组合。因此，在开始超参数调整之前验证你的损失函数是至关重要的。
- en: As with most other techniques in ML, there are multiple ways to find the best
    hyperparameters for a model. The most popular techniques are *grid search*, *random
    search*, and *Bayesian optimization*. In this chapter, we will investigate all
    three of them, discuss their strengths and weaknesses, and experiment with practical
    examples.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 与机器学习中的大多数其他技术一样，有多种方法可以找到模型的最佳超参数。最流行的技术是网格搜索、随机搜索和贝叶斯优化。在本章中，我们将研究这三种方法，讨论它们的优缺点，并通过实际示例进行实验。
- en: Sampling all possible parameter combinations using grid search
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用网格搜索对所有可能的参数组合进行采样
- en: '**Grid search** (or **grid sampling**) is a popular technique for finding the
    optimal hyperparameters from a parameter grid by testing every possible parameter
    combination of the multi-dimensional parameter grid. For every parameter (continuous
    or categorical), we need to define all the values or value ranges that should
    be tested. Popular ML libraries provide tools to create these parameter grids
    efficiently.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**网格搜索**（或**网格采样**）是一种通过测试多维参数网格的每个可能的参数组合来从参数网格中找到最佳超参数的流行技术。对于每个参数（连续或分类），我们需要定义所有应该测试的值或值范围。流行的机器学习库提供工具来有效地创建这些参数网格。'
- en: 'Two properties differentiate grid search from other hyperparameter sampling
    methods:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 两个特性将网格搜索与其他超参数采样方法区分开来：
- en: All parameter combinations are assumed to be independent of each other, which
    means they can be tested in parallel. Therefore, given a set of 100 possible parameter
    combinations, we can start 100 models to test all the combinations in parallel.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设所有参数组合都是相互独立的，这意味着它们可以并行测试。因此，给定100种可能的参数组合，我们可以启动100个模型来并行测试所有组合。
- en: By testing all possible parameter combinations, we can ensure that we search
    for a global optimum rather than a local optimum.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过测试所有可能的参数组合，我们可以确保我们寻找的是全局最优解，而不是局部最优解。
- en: Grid search works perfectly for smaller ML models with only a few hyperparameters
    but grows exponentially with every additional parameter because it adds a new
    dimension to the parameter grid.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索对于只有少数超参数的小型机器学习模型工作得很好，但随着每个额外参数的增加，它会呈指数增长，因为它为参数网格添加了一个新的维度。
- en: 'Let''s look at how grid search can be implemented using Azure Machine Learning.
    In Azure Machine Learning, the hyperparameter tuning functionality lives in the
    `hyperdrive` package. Here is what we are going to do:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用Azure机器学习实现网格搜索。在Azure机器学习中，超参数调整功能位于`hyperdrive`包中。以下是我们要做的事情：
- en: Create a grid sampling configuration
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个网格采样配置
- en: Define a primary metric to define the tuning goal
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个主要指标来定义调整目标
- en: Create a `hyperdrive` configuration
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`hyperdrive`配置
- en: Submit the `hyperdrive` configuration as an experiment to Azure Machine Learning
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`hyperdrive`配置作为实验提交到Azure机器学习
- en: 'Let''s look at these steps in more detail:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看这些步骤：
- en: 'First, we must create the grid sampling configuration by defining the parameter
    choices and ranges for grid sampling, as shown in the following code block:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们必须通过定义网格采样的参数选择和范围来创建网格采样配置，如下面的代码块所示：
- en: '[PRE0]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the preceding code, we defined a parameter grid using discrete parameter
    choices along three parameter dimensions – the number of neurons in the first
    layer, the number of neurons in the second layer, and the training batch size.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用离散的参数选择定义了一个参数网格，沿着三个参数维度——第一层的神经元数量、第二层的神经元数量和训练批次大小。
- en: 'The parameter names are formatted as command-line arguments because they will
    be passed as arguments to the training script. So, we need to make sure that the
    training script can configure parameters via command-line arguments. The following
    code shows what this could look like in your training example:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 参数名称格式化为命令行参数，因为它们将被作为参数传递给训练脚本。因此，我们需要确保训练脚本可以通过命令行参数配置参数。以下代码显示了在您的训练示例中这可能是什么样子：
- en: '[PRE1]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: With grid sampling, we can test all the possible combinations of these parameters.
    This will result in a total of 32 runs (*4 x 4 x 2*) that we could theoretically
    run in parallel, as the training runs, and the parameter configurations are independent
    of each other. In this case, the total number of required training runs is obvious
    as we are only using discrete parameter ranges. Later, we will see that this is
    not the case for random sampling and Bayesian optimization. For these other methods,
    we sample from a continuous distribution, so the number of training runs won't
    be bounded. We will also see that the number of parallel runs can affect the optimization
    process when parameter choices are not independent. So, let's appreciate the simplicity
    of the grid sampling solution for a small number of discrete parameters.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 使用网格采样，我们可以测试这些参数的所有可能组合。这将导致总共32次运行（*4 x 4 x 2*），理论上我们可以并行运行，因为训练运行和参数配置是相互独立的。在这种情况下，所需的总训练运行次数很明显，因为我们只使用离散的参数范围。稍后，我们将看到这并不适用于随机采样和贝叶斯优化。对于这些其他方法，我们从连续分布中进行采样，因此训练运行的次数不会有限制。我们还将看到，当参数选择不是独立时，并行运行的次数会影响优化过程。因此，让我们欣赏网格采样解决方案在少量离散参数上的简单性。
- en: 'Next, we need to define a metric that measures the performance of each parameter
    combination. This metric can be any numeric value that is logged by the training
    script. Please note that this metric does not need to be the same as the loss
    function – it can be any measurement that you would like to use to compare different
    parameter pairs. Have a look at the following example. Here, we have decided to
    maximize the `accuracy` metric and defined the following parameters:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要定义一个度量标准，用于衡量每个参数组合的性能。这个度量标准可以是训练脚本记录的任何数值。请注意，这个度量标准不需要与损失函数相同——它可以是你想要用来比较不同参数对度的任何测量。看看下面的例子。在这里，我们决定最大化`accuracy`度量标准，并定义了以下参数：
- en: '[PRE2]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the preceding code, we chose the `accuracy` metric, which is what we want
    to maximize. Here, you can see that we simply specified any metric name as a string.
    To use this metric to evaluate hyperparameter optimization runs, the training
    script needs to log a metric with this exact name. We saw this in the previous
    chapters, where we emitted metrics for an Azure Machine Learning run.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们选择了`accuracy`度量标准，这是我们想要最大化的。在这里，你可以看到我们只是简单地指定了任何度量名称作为字符串。为了使用此度量标准来评估超参数优化运行，训练脚本需要记录具有此确切名称的度量。我们已经在之前的章节中看到了这一点，其中我们为Azure
    Machine Learning运行发出了度量。
- en: 'We must use the same metric name of `primary_metric_name` to define and log
    a metric that can be picked up by `hyperdrive` to evaluate the run in the training
    script:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们必须使用相同的`primary_metric_name`度量名称来定义和记录一个可以被`hyperdrive`在训练脚本中评估的度量：
- en: '[PRE3]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Before we continue, recall the script run configuration from the previous chapters.
    Similar to the previous chapters, we must configure a CPU-based Azure Machine
    Learning training cluster defined as `aml_cluster` and an environment called `tf_env`
    containing all the relevant packages for running TensorFlow:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们继续之前，回想一下之前章节中的脚本运行配置。类似于之前的章节，我们必须配置一个基于CPU的Azure Machine Learning训练集群，定义为`aml_cluster`，以及一个包含运行TensorFlow所需的所有相关包的环境`tf_env`：
- en: '[PRE4]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, we can initialize the `hyperdrive` configuration, which consists of the
    estimator, the sampling grid, the optimization metric, and the number of runs
    and concurrent runs:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以初始化`hyperdrive`配置，它由估计器、采样网格、优化度量标准以及运行数和并发运行数组成：
- en: '[PRE5]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In grid sampling, the number of runs should correspond with the number of possible
    parameter combinations. As it is a required attribute, we need to compute this
    value and pass it here. The maximum number of concurrent runs in grid sampling
    is limited only by the number of nodes in your Azure Machine Learning cluster.
    We are using a four-node cluster, so we have set the number to `4` to maximize
    concurrency.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在网格采样中，运行的次数应该与可能的参数组合数相对应。由于这是一个必需的属性，我们需要计算这个值并将其传递到这里。网格采样的最大并发运行数仅受你的Azure
    Machine Learning集群中节点数的限制。我们使用了一个四节点集群，因此我们将数字设置为`4`以最大化并发性。
- en: 'Finally, we can submit the `hyperdrive` configuration to an experiment, which
    will execute all the concurrent child runs on the specified compute target:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以将`hyperdrive`配置提交给一个实验，该实验将在指定的计算目标上执行所有并发子运行：
- en: '[PRE6]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The preceding snippet will kick off the training process, build and register
    new Docker images if needed, initialize and scale up the nodes in the cluster,
    and finally run the training scripts on the cluster. Each script will be parameterized
    using a unique parameter combination from the sampling grid. The following screenshot
    shows the resulting experiment run. We can go to this page by clicking on the
    link that is returned from the preceding code snippet:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码片段将启动训练过程，如果需要，将构建和注册新的Docker镜像，初始化并扩展集群中的节点，并最终在集群上运行训练脚本。每个脚本将使用采样网格中唯一的参数组合进行参数化。下面的截图显示了生成的实验运行。我们可以通过点击前面代码片段返回的链接来访问这个页面：
- en: '![Figure 11.1 – Grid sampling overview ](img/B17928_11_01.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图11.1 – 网格采样概述](img/B17928_11_01.jpg)'
- en: Figure 11.1 – Grid sampling overview
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1 – 网格采样概述
- en: Here, we can see the sampling policy's name, which is **GRID**, and the configured
    parameter space. These parameters will be applied as command-line arguments to
    the training script.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到采样策略的名称，它是**GRID**，以及配置的参数空间。这些参数将被作为命令行参数应用到训练脚本中。
- en: As you may have guessed already, not everything is great when you must sample
    all the possible parameter combinations from a multi-dimensional grid. As the
    number of hyperparameters grows, so do the dimensions of the grid. And each dimension
    of parameters adds a magnitude of new parameter configurations that need to be
    tested. And don't forget that testing a parameter's configuration usually means
    performing training, cross-validation, and test set predictions on your model,
    which can take a significant number of resources.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如您可能已经猜到的，当你必须从一个多维网格中采样所有可能的参数组合时，并非一切都很完美。随着超参数数量的增加，网格的维度也会增加。并且每个参数维度都会增加一个数量级的参数配置，需要对其进行测试。别忘了，测试参数配置通常意味着在你的模型上执行训练、交叉验证和测试集预测，这可能需要大量的资源。
- en: 'Imagine that you want to search for the best parameter combination for five
    parameters with 10 different values for each parameter. Let''s assume the following:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想为五个参数寻找最佳参数组合，每个参数有 10 个不同的值。让我们假设以下情况：
- en: We are testing 105 (*10 x 10 x 10 x 10 x 10*) parameter combinations.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们正在测试 10^5 (*10 x 10 x 10 x 10 x 10*) 个参数组合。
- en: One training run takes only 2 minutes.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一次训练运行只需 2 分钟。
- en: We are performing four-fold cross-validation.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们正在进行四折交叉验证。
- en: Here, we would end up with 555 days (*2min x 4 x 10^5 = 800,000min*) of combined
    training time. While you could decrease the total runtime by running parameter
    combinations in parallel, other methods exist that are better suited for large
    numbers of parameters, such as random sampling. Let's see how we can limit the
    required runtime of the parameter optimization search by sampling parameter configurations
    at random.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们最终会有 555 天（*2min x 4 x 10^5 = 800,000min*）的累计训练时间。虽然你可以通过并行运行参数组合来减少总运行时间，但还有其他更适合大量参数的方法，例如随机抽样。让我们看看如何通过随机抽样参数配置来限制参数优化搜索所需的运行时间。
- en: Testing random combinations using random search
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用随机搜索测试随机组合
- en: '**Random search** is another popular hyperparameter sampling method that''s
    similar to grid search. The main difference is that instead of testing all the
    possible parameter combinations, only a few combinations are randomly selected
    and tested. The main idea is that grid search often samples nearby parameter configurations
    that have little effect on model performance. Therefore, we waste a lot of time
    chasing similarly bad solutions where we could use our time to try diverse and
    hopefully more successful parameter configurations.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机搜索**是另一种流行的超参数抽样方法，类似于网格搜索。主要区别在于，它不是测试所有可能的参数组合，而是随机选择并测试几个组合。主要思想是，网格搜索通常采样对模型性能影响不大的参数配置。因此，我们浪费了大量时间追逐类似的不良解决方案，而我们本可以用这些时间尝试更多样化和可能更成功的参数配置。'
- en: When you're dealing with large amounts of hyperparameters (for example, more
    than 5), random search will find a good set of hyperparameters much faster than
    grid search – however, it might not be the optimal result. Even so, in many cases,
    it will be a reasonable trade-off to use random search over grid search to improve
    prediction performance with hyperparameter tuning.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当你处理大量的超参数（例如，超过 5 个）时，随机搜索会比网格搜索更快地找到一组好的超参数 – 然而，它可能不是最佳结果。即便如此，在许多情况下，使用随机搜索而不是网格搜索来提高预测性能，通过超参数调整进行权衡将是合理的。
- en: In random search, parameters are usually sampled from a continuous distribution
    instead of discrete parameter choices being used. This leads to a slightly different
    way of defining the parameter grid. Instead of providing choices for distinct
    values, we can define a distribution function for each parameter to draw random
    values from a continuous range.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机搜索中，参数通常是从连续分布中抽取的，而不是使用离散的参数选择。这导致定义参数网格的方式略有不同。我们不是为不同的值提供选择，而是可以为每个参数定义一个分布函数，以从连续范围内抽取随机值。
- en: Like grid search, all parameter combinations are independent if they're drawn
    with replacement, which means they can be fully parallelized. If a parameter grid
    with 10,000 distinct parameter configurations is provided, we can run and test
    all the models in parallel.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 与网格搜索类似，如果参数组合是带替换抽取的，则所有参数组合都是独立的，这意味着它们可以被完全并行化。如果提供了一个包含 10,000 个不同参数配置的参数网格，我们就可以并行运行并测试所有模型。
- en: 'Let''s look at random search in Azure Machine Learning:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看 Azure Machine Learning 中的随机搜索：
- en: 'As with all other hyperparameter optimization methods, we find the random sampling
    method in the `hyperdrive` package. As we discussed previously, we can now define
    probability distribution functions such as `normal` and `uniform` for each parameter
    instead of choosing only discrete parameters:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与所有其他超参数优化方法一样，我们在`hyperdrive`包中找到了随机采样方法。正如我们之前讨论的，我们现在可以为每个参数定义概率分布函数，例如`normal`和`uniform`，而不是只选择离散参数：
- en: '[PRE7]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Using continuous parameter ranges is not the only difference in random sampling.
    Due to the possibility of sampling an infinite amount of parameter configurations
    from a continuous range, we need a way to specify the duration of the search.
    We can use the `max_total_runs` and `max_duration_minutes` parameters to define
    the expected runtime in minutes or to limit the amount of sampled parameter configurations.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用连续参数范围是随机采样中的唯一不同之处。由于可以从连续范围中采样无限数量的参数配置，我们需要一种方法来指定搜索的持续时间。我们可以使用`max_total_runs`和`max_duration_minutes`参数来定义预期的运行时间（以分钟为单位）或限制采样参数配置的数量。
- en: 'Let''s test 25 different configurations and run the hyperparameter tuning process
    for a maximum of 60 minutes. We must set the following parameters:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们测试25种不同的配置，并将超参数调整过程运行最长60分钟。我们必须设置以下参数：
- en: '[PRE8]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We will reuse the same metric that we defined in the previous section, namely
    *accuracy*. The `hyperdrive` configuration looks as follows:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将重用之前定义的相同指标，即*准确率*。`hyperdrive`配置如下所示：
- en: '[PRE9]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Similar to the previous example, we must submit the `hyperdrive` configuration
    to Azure Machine Learning from the authoring runtime, which will schedule all
    the optimization runs on the compute target:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与之前的示例类似，我们必须从作者运行时提交`hyperdrive`配置到Azure Machine Learning，这将安排所有优化运行在计算目标上：
- en: '[PRE10]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Random sampling is an excellent choice for testing large numbers of tunable
    hyperparameters or sampling values from a continuous range. However, instead of
    optimizing the parameter configurations step by step, we simply try all those
    configurations at random and compare how they perform.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 随机采样是测试大量可调超参数或从连续范围中采样值的绝佳选择。然而，我们不是逐步优化参数配置，而是简单地随机尝试所有这些配置，并比较它们的性能。
- en: In the next section, we will learn how to find a good parameter combination
    faster by stopping training runs early. In the subsequent section, *Optimizing
    parameter choices using Bayesian optimization*, we will look at a more elegant
    way of navigating through the parameter space in hyperparameter tuning by using
    optimization.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何通过提前停止训练运行来更快地找到好的参数组合。在随后的章节*使用贝叶斯优化优化参数选择*中，我们将探讨在超参数调整中通过优化导航参数空间的一种更优雅的方法。
- en: Converging faster using early termination
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提前终止更快地收敛
- en: Both the grid and random sampling techniques test models for poor parameter
    choices and hence spend precious compute resources on fitting poorly parameterized
    models to your training data. **Early termination** is a technique that stops
    a training run early if the intermediate results look worse than other runs. It
    is a great solution for speeding up expensive hyperparameter optimization techniques.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 网格和随机采样技术都会测试模型对参数选择的不足，因此会浪费宝贵的计算资源来拟合参数较差的模型到您的训练数据中。**提前终止**是一种在中间结果看起来比其他运行更差时提前停止训练的技术。这是加快昂贵超参数优化技术的绝佳解决方案。
- en: In general, you should always try to use early termination when using either
    grid or random sampling. You get no benefit from training all the parameter combinations
    if the results are a lot worse than for some of the existing runs.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，您应该始终尝试在使用网格或随机采样时使用提前终止。如果结果比一些现有运行差得多，那么训练所有参数组合就没有任何好处。
- en: Once we understand the idea of canceling poor-performing runs, we need to find
    a way to specify a threshold of when a run should be canceled – we refer to this
    threshold as the **termination policy**. Azure Machine Learning provides the most
    popular termination policies, namely **bandit**, **median stopping**, and **truncation
    selection**. Let's take a look at them and see what their differences are.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们理解了取消表现不佳的运行的想法，我们需要找到一种方法来指定何时应该取消运行的阈值——我们将这个阈值称为**终止策略**。Azure Machine
    Learning提供了最流行的终止策略，即**探索者**、**中位数停止**和**截断选择**。让我们来看看它们，并了解它们之间的区别。
- en: 'Before we get into the details, though, let''s learn how to configure early
    termination. In Azure Machine Learning, we can parameterize the different early
    termination policies with two global properties, namely `evaluation_interval`
    and `delay_evaluation`. These parameters control how often the early termination
    policy is tested. An example of using these parameters are as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入了解细节之前，让我们学习如何配置早期终止。在Azure Machine Learning中，我们可以使用两个全局属性对不同的早期终止策略进行参数化，即`evaluation_interval`和`delay_evaluation`。这些参数控制早期终止策略被测试的频率。以下是如何使用这些参数的示例：
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The units of both parameters are in intervals. An `run.log()`. For example,
    when you're training a neural network, an interval will equal one training epoch.
    The `delay_evaluation` parameter controls how many intervals we want to wait after
    the start to test the early termination policy for the first time. In the preceding
    example, we configured it as `10`, so we wait for 10 epochs before testing the
    early termination policy.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个参数的单位都是间隔。例如，当你训练一个神经网络时，一个间隔等于一个训练epoch。`delay_evaluation`参数控制我们在第一次测试早期终止策略之前需要等待多少个间隔。在先前的例子中，我们将其配置为`10`，这意味着我们等待10个epoch之后才测试早期终止策略。
- en: Then, every other policy evaluation is configured using the `evaluation_interval`
    parameter. It describes how many iterations need to pass until the next test.
    In the preceding example, we set `evaluation_interval` to `1`, which is also the
    default value. This means that we test the early termination policy every interval
    after the `delay_evaluation` interval – here, every 1 iteration. Let's look into
    the three termination policies in more detail.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，每个其他策略评估都使用`evaluation_interval`参数进行配置。它描述了需要经过多少次迭代才能进行下一次测试。在先前的例子中，我们将`evaluation_interval`设置为`1`，这也是默认值。这意味着我们在`delay_evaluation`间隔之后每隔一个间隔测试早期终止策略——在这里，每1次迭代。让我们更详细地看看三种终止策略。
- en: The median stopping policy
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 中值终止策略
- en: 'Let''s start with the easiest termination policy – the **median stopping policy**.
    It takes no other arguments than the two default arguments, which control when
    and how often the policy should be tested. The median stopping policy keeps track
    of the running average of the primary metric across all experiment runs. Whenever
    the median policy is evaluated, it will test whether the current metric is above
    the median of all running experiments and stop those runs that are below. The
    following code shows how to create a median stopping early termination policy
    for any hyperparameter tuning script:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从最简单的终止策略开始——**中值终止策略**。它不需要除了两个默认参数之外的其他参数，这两个参数控制策略何时以及多久应该被测试。中值终止策略会跟踪所有实验运行中主要指标的平均值。每当评估中值策略时，它都会测试当前指标是否高于所有运行实验的中位数，并停止那些低于中位数的运行。以下代码展示了如何为任何超参数调整脚本创建一个中值终止的早期终止策略：
- en: '[PRE12]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As we can see, it's quite simple to construct a median stopping policy as it
    is only configured by the two default parameters. Due to its simplicity, it is
    a very effective method for reducing the runtime of your hyperparameter optimization
    script. The early termination policy is then applied to the `hyperdrive` configuration
    file using the `policy` parameter. Now, let's look at the truncation selection
    policy.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，构建中值终止策略非常简单，因为它只由两个默认参数配置。由于其简单性，它是一种非常有效的方法，可以减少你的超参数优化脚本的运行时间。然后，使用`policy`参数将早期终止策略应用于`hyperdrive`配置文件。现在，让我们看看截断选择策略。
- en: The truncation selection policy
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 截断选择策略
- en: 'Unlike the median stopping policy, the `truncation_percentage` parameter:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 与中值终止策略不同，`truncation_percentage`参数：
- en: '[PRE13]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In the preceding example, we set the `truncation_percentage` value to `10`.
    This means that whenever the early termination policy is executed, it will kill
    the lowest-performing 10% of runs. We must also increase the `evaluation_interval`
    value to `5` as we don''t want to kill runs every epoch, as shown in the following
    example:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在先前的例子中，我们将`truncation_percentage`值设置为`10`。这意味着每当早期终止策略执行时，它将终止表现最差的10%的运行。我们还必须将`evaluation_interval`值增加到`5`，因为我们不想像以下示例中那样在每个epoch结束时终止运行：
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This early termination policy makes sense when only very few training resources
    are available, and we want to aggressively prune the number of runs each time
    the early termination policy is evaluated. Let's look at the final policy – the
    bandit policy.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这种早期终止策略在只有很少的训练资源可用，并且我们希望在每次评估早期终止策略时积极修剪运行数量时是有意义的。让我们看看最终的策略——bandit策略。
- en: The bandit policy
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Bandit策略
- en: The `slack_factor` or `slack_amount` parameter. The `slack_factor` parameter
    describes the relative deviation from the best metric, whereas the `slack_amount`
    parameter describes the absolute deviation from the best primary metric.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`slack_factor`或`slack_amount`参数。`slack_factor`参数描述了相对于最佳指标的相对偏差，而`slack_amount`参数描述了相对于最佳主要指标的绝对偏差。'
- en: 'Let''s look at an example. Here, we will configure `hyperdrive` by configuring
    a `slack_factor` parameter of `0.2` and testing an accuracy value (*bigger is
    better*). As we did previously, we will set the `evaluation_interval` value to
    `5` and the `evaluation_delay` value to `10` intervals:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子。在这里，我们将通过配置`slack_factor`参数为`0.2`并测试一个准确度值（*越大越好*）来配置`hyperdrive`。像之前一样，我们将`evaluation_interval`值设置为`5`，将`evaluation_delay`值设置为`10`个间隔：
- en: '[PRE15]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let''s say that the best-performing run yields an accuracy of 0.8 after epoch
    10, which is when the early termination policy gets applied for the first time.
    Now, all the runs that are performing up to 20% worse than the best metric are
    killed. We can compute the relative deviation from an accuracy of 0.8 by using
    the following function:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 假设性能最佳的运行在10个epoch后产生了0.8的准确度，这是早期终止策略第一次被应用的时候。现在，所有性能比最佳指标差20%以上的运行都将被终止。我们可以通过以下函数计算从0.8准确度出发的相对偏差：
- en: '*0.8/(1 + 0.2) = 0.67*'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*0.8/(1 + 0.2) = 0.67*'
- en: Hence, all the runs that yield a performance that's lower than 0.67 will get
    canceled by the early termination policy.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，所有性能低于0.67的运行都将被早期终止策略取消。
- en: A HyperDrive configuration with the termination policy
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 带有终止策略的HyperDrive配置
- en: 'To create a `hyperdrive` configuration, we need to pass the early termination
    policy using the `policy` parameter. Here is an example of using grid search sampling
    and the previously defined bandit policy:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个`hyperdrive`配置，我们需要使用`policy`参数传递早期终止策略。以下是一个使用网格搜索采样和之前定义的bandit策略的示例：
- en: '[PRE16]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The bandit policy is a good trade-off between the median stopping and the truncation
    selection policy that works well in many cases. You can rest assured that only
    a well-performing subset of all the hyperparameter configurations will be run
    and evaluated for multiple intervals.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Bandit策略是中值停止和截断选择策略之间的一种良好权衡，后者在许多情况下都表现良好。你可以放心，只有所有超参数配置中表现良好的子集将在多个间隔内运行和评估。
- en: 'Let''s submit this HyperDrive configuration as an experiment to Azure Machine
    Learning. We can use the `RunDetails` method that we saw in the previous chapters
    to output additional information about the hyperparameter tuning experiment, such
    as scheduling and parameter information, a visualization of the training performance,
    and a parallel coordinate chart showing the parameter dimensions:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个HyperDrive配置作为一个实验提交到Azure机器学习。我们可以使用之前章节中看到的`RunDetails`方法来输出关于超参数调整实验的附加信息，例如调度和参数信息、训练性能的可视化，以及显示参数维度的并行坐标图：
- en: '[PRE17]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'If you run the preceding code, it will run the hyperparameter search for the
    configured policies. Once the experiment is running, you will see the specified
    metric for the individual parameter combinations and iterations as a chart in
    a widget:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行前面的代码，它将运行配置策略的超参数搜索。一旦实验开始运行，你将看到作为小部件中图表的指定指标，对于单个参数组合和迭代：
- en: '![Figure 11.2 – HyperDrive – the performance of runs ](img/B17928_11_02.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图11.2 – HyperDrive – 运行的性能](img/B17928_11_02.jpg)'
- en: Figure 11.2 – HyperDrive – the performance of runs
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2 – HyperDrive – 运行的性能
- en: 'Besides looking at the defined metric, you can select other visualizations
    that show the sampled parameters, such as on a parallel coordinates plot, or as
    two- and three-dimensional scatter plots. Here, you can see which parameter combinations
    yield high model accuracy:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 除了查看定义的指标外，你还可以选择其他可视化，显示采样参数，例如在并行坐标图上，或作为二维和三维散点图。在这里，你可以看到哪些参数组合产生了高模型准确度：
- en: '![Figure 11.3 – HyperDrive – visualization of the results ](img/B17928_11_03.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图11.3 – HyperDrive – 结果的可视化](img/B17928_11_03.jpg)'
- en: Figure 11.3 – HyperDrive – visualization of the results
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3 – HyperDrive – 结果的可视化
- en: In this section, you learned that applying an early termination policy to your
    hyperparameter optimization script is a simple but extremely effective way to
    reduce the number of poorly performing training runs. With just a few lines of
    code, we can reduce the number of training runs to a minimum and only finish those
    that are yielding promising results.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你了解到将早期终止策略应用于你的超参数优化脚本是一种简单但极其有效的方法，可以减少表现不佳的训练运行次数。只需几行代码，我们就可以将训练运行的次数减少到最小，并且只完成那些产生有希望结果的任务。
- en: Important Note
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: When you're using hyperparameter optimization with random or grid sampling,
    *always* use an early termination policy.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用随机或网格采样进行超参数优化时，*始终*使用早期终止策略。
- en: Optimizing parameter choices using Bayesian optimization
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用贝叶斯优化优化参数选择
- en: In the previous examples, we evaluated different parameter configurations sampled
    from a grid or at random without any optimization or strategic parameter choice.
    This had the benefit that all the configurations were independent and could be
    evaluated in parallel. However, imagine using an ML model to help us find the
    best parameter combination for a large multi-dimensional parameter space. That's
    exactly what **Bayesian optimization** does in the domain of hyperparameter tuning.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们评估了从网格或随机采样的不同参数配置，而没有进行任何优化或战略性的参数选择。这有一个好处，即所有配置都是独立的，并且可以并行评估。然而，想象一下使用ML模型帮助我们找到大型多维参数空间中最佳参数组合的情况。这正是**贝叶斯优化**在超参数调整领域所做的事情。
- en: 'The job of an optimization method is to find the optimal value (that is, a
    minimum or maximum) of a predefined objective function. In hyperparameter tuning,
    we are faced with a very similar problem: we want to find the parameter configuration
    that yields the best-predefined evaluation metric for an ML model.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 优化方法的工作是找到预定义目标函数的最优值（即最小值或最大值）。在超参数调整中，我们面临一个非常类似的问题：我们想要找到产生最佳预定义评估指标的参数配置。
- en: 'So, how does optimization work for hyperparameter search? First, we must define
    a hyperplane – a multi-dimensional grid where we can sample our parameter configurations.
    In the following diagram, we can see such a plane for two parameters along the
    *x* and *y* axes. The *z*-axis represents the performance of the model that is
    being tested using the parameters at this specific location:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，超参数搜索的优化是如何工作的呢？首先，我们必须定义一个超平面——一个多维网格，我们可以从中采样参数配置。在下面的图中，我们可以看到沿着*x*和*y*轴的两个参数的这样一个平面。*z*轴表示使用该特定位置的参数测试的模型的性能：
- en: '![Figure 11.4 – The Rastrigin function ](img/B17928_11_04.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图11.4 – Rastrigin函数](img/B17928_11_04.jpg)'
- en: Figure 11.4 – The Rastrigin function
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4 – Rastrigin函数
- en: The preceding diagram shows the multi-dimensional Rastrigin function, as an
    example of something extremely hard to optimize. In hyperparameter tuning, we
    often face a similar problem in that finding the optimal solution is difficult
    – just like finding the global minimum in the Rastrigin function.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图显示了多维Rastrigin函数，作为一个极其难以优化的例子。在超参数调整中，我们经常面临类似的问题，即找到最优解是困难的——就像在Rastrigin函数中找到全局最小值一样。
- en: Then, we must sample points from this plane and test the first (few) parameter
    configurations. We assume that the parameters are not independent and that the
    model will have similar performance when using similar nearby parameters. However,
    each evaluation only yields a noisy value of the true model performance. Using
    these assumptions, we can use **Gaussian processes** to combine the model evaluations
    into a multi-variate continuous Gaussian. Next, we can compute the points for
    the highest expected improvements on this Gaussian. These points will yield new
    samples to test with our model.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们必须从这个平面上采样点并测试第一个（几个）参数配置。我们假设参数不是独立的，并且当使用相似的邻近参数时，模型将具有相似的性能。然而，每次评估只能得到真实模型性能的噪声值。利用这些假设，我们可以使用**高斯过程**将模型评估组合成一个多元连续高斯分布。接下来，我们可以计算在这个高斯上预期改进最高的点。这些点将产生新的样本，以便用我们的模型进行测试。
- en: Luckily, we don't have to implement the algorithm ourselves, but many ML libraries
    provide a hyperparameter optimization algorithm out of the box. In Azure Machine
    Learning, we can use the **Bayesian sampling method**, which helps us pick good
    parameter configurations to optimize the predefined metric.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们不必自己实现算法，许多机器学习库都提供了开箱即用的超参数优化算法。在Azure机器学习中，我们可以使用**贝叶斯采样方法**，这有助于我们选择好的参数配置来优化预定义的指标。
- en: 'The parameter grid is defined similarly to the random sampling technique –
    that is, by using a continuous or discrete parameter space for all the parameter
    values, as shown in the following code block:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 参数网格的定义与随机采样技术类似——即通过使用连续或离散的参数空间来定义所有参数值，如下面的代码块所示：
- en: '[PRE18]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Before we continue, we need to keep one thing in mind. The Bayesian sampling
    technique tries to predict well-performing parameter configurations based on the
    results of the previously tested parameters. This means that the parameter choices
    and runs are not independent anymore. We can't run all the experiments in parallel
    at the same time as we need the results of some experiments to sample new parameters.
    Therefore, we need to set an additional parameter to control how many training
    runs should run concurrently.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，我们需要记住一件事。贝叶斯采样技术试图根据先前测试的参数结果来预测性能良好的参数配置。这意味着参数选择和运行不再独立。我们不能同时并行运行所有实验，因为我们需要某些实验的结果来采样新的参数。因此，我们需要设置一个额外的参数来控制应该同时运行多少个训练运行。
- en: 'We can do this using the `max_concurrent_runs` parameter. To let the Bayesian
    optimization technique converge, it is recommended to set this value to a small
    value, for example, in the range of 2-10\. Let''s set the value to 4 for this
    experiment and the number of total runs to 100\. This means that we are using
    25 iterations for the Bayesian optimization method, where we explore four parameter
    configurations concurrently at a time:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`max_concurrent_runs`参数来实现这一点。为了使贝叶斯优化技术收敛，建议将此值设置为较小的值，例如，在2-10的范围内。让我们将此实验的值设置为4，并将总运行次数设置为100。这意味着我们正在使用25次迭代来应用贝叶斯优化方法，其中我们一次探索四个参数配置：
- en: '[PRE19]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let''s kick off the experiment with Bayesian sampling:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用贝叶斯采样来启动实验：
- en: '[PRE20]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Unfortunately, this technique can't be parallelized further to finish faster
    as all the parameter choices are dependent on the results of the previous iteration.
    However, due to the optimization step, it generally yields good results in a relatively
    short amount of time.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，由于所有参数选择都依赖于前一次迭代的输出，这种技术无法进一步并行化以更快地完成。然而，由于优化步骤，它通常在相对较短的时间内产生良好的结果。
- en: Another downside of Bayesian optimization or optimization for hyperparameter
    tuning is that the optimization requires each result of each run with the defined
    parameter configuration to compute the new parameter choices. Therefore, we can't
    use early termination together with Bayesian sampling as the training would be
    stopped earlier, which means no accurate metric can be computed.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯优化或超参数调优的另一个缺点是，优化需要计算每个运行中定义的参数配置的结果，以确定新的参数选择。因此，我们不能与贝叶斯采样一起使用早期终止，因为训练将提前停止，这意味着无法计算准确的指标。
- en: Important Note
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Early termination doesn't work for optimization techniques such as Bayesian
    optimization because it requires the final testing score to compute the parameter
    gradient.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 对于贝叶斯优化等优化技术，早期终止不起作用，因为它需要计算最终测试分数来计算参数梯度。
- en: 'Once you''ve played around with using ML to optimize an ML model, you may already
    think about taking it one step further: why should we stop at optimizing hyperparameters,
    and why shouldn''t we optimize model choices, network structures, or model stacking
    altogether?'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你尝试使用机器学习来优化机器学习模型，你可能已经考虑将这一步更进一步：为什么我们应该止步于优化超参数，为什么我们不应该优化模型选择、网络结构或模型堆叠？
- en: And this is a perfectly valid thought. No human can test all the variations
    of different ML models, different parameter configurations, and different nested
    models together. In the next section, we will do exactly this and optimize not
    just parameters but also model architecture and preprocessing steps using Automated
    Machine Learning
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个完全合理的想法。没有人能够测试所有不同机器学习模型、不同参数配置和不同嵌套模型的变体。在下一节中，我们将做这件事，不仅优化参数，还将使用自动机器学习优化模型架构和预处理步骤。
- en: Finding the optimal model with Automated Machine Learning
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自动化机器学习寻找最佳模型
- en: '**Automated Machine Learning** is an exciting new trend that many (if not all)
    cloud providers follow. The aim is to provide a service to users that automatically
    preprocesses your data, selects an ML model, and trains and optimizes the model
    to fit your training data to optimize a specified error metric. This will create
    and train a fully automated end-to-end ML pipeline that only needs your labeled
    training data and target metric as input. Here is a list of steps that Automated
    Machine Learning optimizes for you:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**自动化机器学习**是一个令人兴奋的新趋势，许多（如果不是所有）云服务提供商都在追随。目标是向用户提供一种服务，该服务可以自动预处理您的数据，选择机器学习模型，并训练和优化模型以适应您的训练数据，从而优化指定的误差指标。这将创建和训练一个完全自动化的端到端机器学习管道，只需您的标记训练数据和目标指标作为输入。以下是自动化机器学习为您优化的步骤列表：'
- en: Data preprocessing
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据预处理
- en: Feature engineering
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程
- en: Model selection
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型选择
- en: Hyperparameter tuning
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数调整
- en: Model ensembling
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型集成
- en: While most experienced ML engineers or data scientists would be very cautious
    about the effectiveness of such an automated approach, it still has a ton of benefits,
    which will be explained in this section. If you like the idea of hyperparameter
    tuning, then you will find value in Automated Machine Learning.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大多数经验丰富的机器学习工程师或数据科学家可能会对这种自动化方法的有效性非常谨慎，但它仍然有很多好处，将在本节中解释。如果您喜欢超参数调整的想法，那么您会发现自动化机器学习很有价值。
- en: A good way to think about Automated Machine Learning is that it performs a hyperparameter
    search over the complete end-to-end ML pipeline, similar to Bayesian optimization,
    but over a much larger parameter space. The parameters are now individual steps
    in the end-to-end ML pipeline, which should be automated. The great thing about
    Automated Machine Learning is that instead of going through the dumb sampling
    of all possible parameter choices, it will predict how well certain preprocessing
    steps and models will perform on a dataset before actually training a model. This
    process is called **meta-learning** and will help the optimization process yield
    great candidate solutions for the pipeline without spending time being evaluated.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑自动化机器学习的一个好方法是，它在完整的端到端机器学习管道上执行超参数搜索，类似于贝叶斯优化，但参数空间要大得多。现在，这些参数是端到端机器学习管道中的单独步骤，应该实现自动化。自动化机器学习的优点在于，它不会像愚蠢地采样所有可能的参数选择那样，而是在实际训练模型之前，预测某些预处理步骤和模型在数据集上的表现。这个过程被称为**元学习**，并将帮助优化过程产生对管道的候选解决方案，而无需花费时间进行评估。
- en: The unfair advantage of Automated Machine Learning
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动化机器学习的优势
- en: Let's evaluate the advantages of Automated Machine Learning If we look at the
    list of automated steps we mentioned earlier, each one requires days for an experienced
    data scientist to explore, evaluate, and fine-tune. Even steps such as selecting
    the correct model, such as either LightGBM or XGBoost for gradient-based tree
    ensemble classification, are non-trivial as they require experience and knowledge
    of both tools. Moreover, we all know that those two are only a tiny subset of
    all the possible options for a classification model. If we look at hyperparameter
    tuning and model stacking, we can immediately tell that the amount of work that's
    required to build a great ensemble model is non-trivial.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们评估自动化机器学习的优势。如果我们看看我们之前提到的自动化步骤列表，每个步骤都需要经验丰富的数据科学家花费数天时间来探索、评估和微调。即使是选择正确的模型，例如用于基于梯度的树集成分类的LightGBM或XGBoost，也是非平凡的，因为它们需要对这些工具的经验和知识。此外，我们都知道这两个只是所有可能的分类模型选项的一个非常小的子集。如果我们看看超参数调整和模型堆叠，我们可以立即看出构建一个优秀的集成模型所需的工作量是非平凡的。
- en: This is not only a problem of knowledge or expertise. It's also very time-consuming.
    Automated Machine Learning aims to replace manual steps with automated best practices,
    applying continuously improving rules, and heavily optimizing every possible human
    choice. It's very similar to hyperparameter tuning but for the complete end-to-end
    process. A machine will find the best parameters much faster and much more accurately
    than a human by using optimization instead of manual selection.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅是一个知识或专业知识的问题，而且也非常耗时。自动化机器学习的目标是用自动化的最佳实践来替代手动步骤，应用持续改进的规则，并对每个可能的人类选择进行大量优化。它与超参数调整非常相似，但针对的是完整的端到端过程。通过使用优化而不是手动选择，机器将比人类更快、更准确地找到最佳参数。
- en: 'We can also look at Automated Machine Learning from a different perspective,
    namely as a **machine learning as a service** (**MLaaS**) product: data in, model
    (or prediction endpoint) out. By now, you should be aware that each step of building
    an end-to-end ML pipeline is a thorough, complicated, and time-consuming task.
    Even when you can choose the correct model and tuning parameters using Bayesian
    optimization, the cost of building this infrastructure and operating it is significant.
    In this case, choosing MLaaS would provide you with an ML infrastructure for a
    fraction of the usual cost.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以从另一个角度看待自动化机器学习，即作为 **机器学习即服务（MLaaS**） 产品：输入数据，输出模型（或预测端点）。到目前为止，你应该已经意识到，构建端到端机器学习管道的每一步都是一个彻底、复杂且耗时的任务。即使你可以使用贝叶斯优化选择正确的模型和调整参数，构建和运营此基础设施的成本也是显著的。在这种情况下，选择
    MLaaS 将为你提供通常成本的一小部分机器学习基础设施。
- en: There is another reason why the idea of Automated Machine Learning is very interesting.
    It separates the ML part from your data-fitting problem and leaves you with what
    you should know best – the data. Similar to using a managed service in the cloud
    (for example, a managed database), which lets you focus on implementing business
    logic rather than operating infrastructure, Automated Machine Learning will allow
    you to use a managed ML pipeline built on best practices and optimization by using
    data instead of specific ML algorithms.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化机器学习的想法之所以非常有趣，还有另一个原因。它将机器学习部分与你的数据拟合问题分开，让你专注于你最擅长的数据。类似于在云中使用托管服务（例如，托管数据库），这让你可以专注于实现业务逻辑而不是操作基础设施，自动化机器学习将允许你使用基于最佳实践和数据优化而不是特定机器学习算法的托管机器学习管道。
- en: 'This also leads to the reason why Automated Machine Learning is still a great
    fit for many (mature) companies – it reduces a prediction problem to the most
    important tasks:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是为什么自动化机器学习仍然非常适合许多（成熟）公司的原因——它将预测问题简化为最重要的任务：
- en: Data acquisition
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据获取
- en: Data cleansing
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据清洗
- en: Data labeling
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据标注
- en: Selecting an error metric
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择错误度量标准
- en: We don't want to judge anyone, but ML practitioners often like to skip these
    topics and dive right into the fun parts, namely feature engineering, model selection,
    parameterization, stacking, and tuning. Therefore, a good start for every ML project
    is to start with an Automated Machine Learning baseline model, because it will
    force you to focus only on the data side. After achieving a good initial score,
    you can always go ahead and start further feature engineering and build a model
    if needed.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不想评判任何人，但机器学习从业者往往喜欢跳过这些话题，直接进入有趣的环节，即特征工程、模型选择、参数化、堆叠和调整。因此，每个机器学习项目的良好开端是从自动化机器学习基线模型开始，因为它将迫使你只关注数据方面。在取得良好的初始分数后，你总是可以继续进行进一步的特征工程，并在需要时构建模型。
- en: Now that we've talked about the Automated Machine Learning trend being reasonable
    and that you could benefit from it in one way or another, let's dive deep into
    some examples and code. We will look at the different capabilities of Azure Automated
    Machine Learning, a product of Azure Machine Learning, as applied in a standard
    end-to-end ML pipeline.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了自动化机器学习趋势的合理性以及你可以在某种程度上从中受益，让我们深入探讨一些示例和代码。我们将查看 Azure 自动化机器学习（Azure
    机器学习的一个产品）的不同功能，它应用于标准的端到端机器学习管道。
- en: Before we jump into the code, let's take a look at what problem Azure Automated
    Machine Learning can tackle. In general, we can decide between *classification*,
    *regression*, and *time series forecasting* in Automated Machine Learning As we
    know from the previous chapters, time series forecasting is simply a variant of
    regression, where all the predicted values are in the future.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入代码之前，让我们先看看 Azure 自动化机器学习可以解决哪些问题。一般来说，在自动化机器学习中，我们可以选择 *分类*、*回归* 和 *时间序列预测*。正如我们从前几章所知，时间序列预测只是回归的一种变体，其中所有预测值都在未来。
- en: 'Hence, the most important task after choosing the correct ML task is choosing
    the proper error metric that should be optimized. The following list shows all
    the error metrics that are supported:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，选择正确的机器学习任务之后的最重要的任务是选择应该优化的适当错误度量标准。以下列表显示了所有受支持的错误度量标准：
- en: '`accuracy`, `AUC_weighted`, `average_precision_score_weighted`, `norm_macro_recall`,
    and `precision_score_weighted`'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`accuracy`、`AUC_weighted`、`average_precision_score_weighted`、`norm_macro_recall`
    和 `precision_score_weighted`'
- en: '`spearman_correlation`, `normalized_root_mean_squared_error`, `r2_score`, and
    `normalized_mean_absolute_error`'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spearman_correlation`, `normalized_root_mean_squared_error`, `r2_score`, 和
    `normalized_mean_absolute_error`'
- en: You should be familiar with most of these metrics as they are variants of the
    most popular error metrics for classification and regression.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该熟悉这些大多数指标，因为它们是最受欢迎的错误度量分类和回归的变体。
- en: Among the supported models, there's LogisticRegression, SGD, MultinomialNaiveBayes,
    SVM, KNN, Random Forest, ExtremeRandomTrees, LigthtGBM, GradientBoosting, DNN,
    Lasso, Arima, Prophet, and more. The great thing about a managed service in the
    cloud is that this list will most likely grow in the future and add the most recent
    state-of-the-art models. However, this list should be thought of just as additional
    information for you, since the idea of Automated Machine Learning is that the
    models are automatically chosen for you. However, according to the user's preference,
    individual models can be allow- or deny-listed for Automated Machine Learning.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在支持的模型中，包括LogisticRegression, SGD, MultinomialNaiveBayes, SVM, KNN, Random Forest,
    ExtremeRandomTrees, LigthtGBM, GradientBoosting, DNN, Lasso, Arima, Prophet等。在云中托管服务的优点在于，这个列表很可能会在未来增长，并添加最新的最先进模型。然而，这个列表应该被视为仅为您提供的附加信息，因为自动机器学习的理念是模型会自动为您选择。然而，根据用户的偏好，可以为自动机器学习允许或拒绝列表中的单个模型。
- en: With all this in mind, let's look at a classification example that uses Automated
    Machine Learning
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到所有这些，让我们看看一个使用自动机器学习的分类示例。
- en: A classification example with Automated Machine Learning
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动机器学习的一个分类示例
- en: When you're using new technology, it's always good to take a step back and think
    about what the technology could be capable of. Let's use the same approach to
    figure out how automated preprocessing could help us in a typical ML project and
    where its limitations will be.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 当您使用新技术时，总是好的退一步思考这项技术可能具备的能力。让我们使用相同的方法来了解自动预处理如何帮助我们在一个典型的机器学习项目中，以及其局限性在哪里。
- en: 'Automated Machine Learning is great for applying best-practice transformations
    to your dataset: applying date/time transformations, as well as the normalization
    and standardization of your data when using linear regression, handling missing
    data or dropping low-variance features, and so on. A long list of features is
    provided by Microsoft that is expected to grow in the future.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 自动机器学习非常适合将最佳实践转换应用于您的数据集：应用日期/时间转换，以及在应用线性回归时对数据进行归一化和标准化，处理缺失数据或删除低方差特征等。微软提供了一系列功能，预计未来会增长。
- en: Let's recall what we learned in [*Chapter 7*](B17928_07_ePub.xhtml#_idTextAnchor112),
    *Advanced Feature Extraction with NLP*. While Automated Machine Learning can detect
    free text and convert it into a numeric feature vector, it won't be able to understand
    the semantic meaning of the data in your business domain. Therefore, it will be
    able to transform your textual data, but if you need to semantically encode your
    text or categorical data, you have to implement that yourself.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下在[*第7章*](B17928_07_ePub.xhtml#_idTextAnchor112)中学习的内容，*使用NLP的高级特征提取*。虽然自动机器学习可以检测自由文本并将其转换为数值特征向量，但它无法理解您业务领域中的数据语义。因此，它能够转换您的文本数据，但如果您需要语义编码文本或分类数据，您必须自行实现。
- en: Another thing to remember is that Automated Machine Learning will not try to
    infer any correlations between different feature dimensions in your training data.
    Hence, if you want to combine two categorical columns into a combined feature
    column (for example, using one-hot-encoding, mean embedding, and so on), then
    you will have to implement this on your own.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 另一点需要记住的是，自动机器学习不会尝试推断训练数据中不同特征维度之间的任何相关性。因此，如果您想将两个分类列合并为一个组合特征列（例如，使用one-hot-encoding，mean
    embedding等），那么您将必须自行实现这一点。
- en: 'In Automated Machine Learning there are two different sets of preprocessors
    – the `preprocess` argument is specified. If you have worked with scikit-learn
    before, then most of the following preprocessing techniques should be fairly familiar
    to you:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动机器学习中，有两个不同的预处理集 – 指定了`preprocess`参数。如果您之前使用过scikit-learn，那么以下大多数预处理技术应该相当熟悉：
- en: '`StandardScaler`: Normalization – mean subtraction and scaling a feature to
    unit variance.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StandardScaler`: 归一化 – 减去平均值并将特征缩放到单位方差。'
- en: '`MinMaxScaler`: Normalization – scaling a feature by the minimum and maximum.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MinMaxScaler`: 归一化 – 通过最小值和最大值缩放特征。'
- en: '`MaxAbsScaler`: Normalization – scaling a feature by the maximum absolute value.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RobustScaler`: Normalization – scaling a feature to the quantile range.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PCA`: Linear dimensionality reduction based on PCA.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TruncatedSVD`: Linear dimensionality reduction-based truncated **singular
    value decomposition** (**SVD**). Contrary to PCA, this estimator does not center
    the data beforehand.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SparseNormalizer`: Normalization – each sample is normalized independently.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Complex preprocessing is referred to as **featurization**. These preprocessing
    steps are more complicated and apply various tasks during Automated Machine Learning
    optimization. As a user of Azure Automated Machine Learning, you can expect this
    list to grow and include new state-of-the-art transformations as they become available.
    The following list shows the various featurization steps:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '**Drop high cardinality or no variance features**: Drops high cardinality (for
    example, hashes, IDs, or GUIDs) or no variance (for example, all values missing
    or the same value across all rows) features.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Impute missing values**: Imputes missing values for numerical features (mean
    imputation) and categorical features (mode imputation).'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generate additional features**: Generates additional features derived from
    date/time (for example, year, month, day, day of the week, day of the year, quarter,
    week of the year, hour, minute, and second) and text features (term frequency
    based on n-grams).'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transform and encode**: Encodes categorical features using one-hot encoding
    (low cardinality) and one-hot-hash encoding (high cardinality). Transforms numeric
    features with few unique values into categorical features.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Word embeddings**: Uses a pre-trained embedding model to convert text into
    aggregated feature vectors using mean embeddings.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Target encodings**: Performs target encoding on categorical features.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text target encoding**: Performs target encoding on text features using a
    bag-of-words model.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weight of evidence**: Calculates the correlation of categorical columns to
    the target column through the weight of evidence and outputs a new feature per
    column per class.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cluster distance**: Trains a k-means clustering model on all the numerical
    columns and computes the distance of each feature to its centroid before outputting
    a new feature per column per cluster.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start with a simple Automated Machine Learning classification task that
    also uses preprocessing.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by defining a dictionary containing the Automated Machine Learning
    configuration. To enable standard preprocessing such as scaling, normalization,
    and PCA/SVD, we need to set the `preprocess` property to `true`. For advanced
    preprocessing and feature engineering, we need to set the `featurization` property
    to `auto`. The following code block shows all these settings:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Using this configuration, we can now load a dataset using `pandas`. As shown
    in the following snippet, we are loading the `titanic` dataset and specifying
    the target column as a string. This column is required later for configuring Automated
    Machine Learning:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此配置，我们现在可以使用`pandas`加载数据集。如下面的代码片段所示，我们正在加载`titanic`数据集，并将目标列指定为字符串。这个列在稍后配置自动机器学习时是必需的：
- en: '[PRE22]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Important Note
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: When you're using Automated Machine Learning and the local execution context,
    you can use a pandas DataFrame as the input source. However, when you execute
    the training process on a remote cluster, you need to wrap the data in an Azure
    Machine Learning dataset.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用自动机器学习并且处于本地执行上下文时，你可以使用pandas DataFrame作为输入源。然而，当你在一个远程集群上执行训练过程时，你需要将数据包装在Azure机器学习数据集中。
- en: 'Whenever we use a black-box classifier, we should also hold out a test set
    to verify the test performance of the model to validate generalization. Therefore,
    we must split the data into training and test sets:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们使用黑盒分类器时，我们也应该保留一个测试集来验证模型的测试性能，以验证泛化能力。因此，我们必须将数据分成训练集和测试集：
- en: '[PRE23]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, we can supply all the required parameters to the Automated Machine
    Learning configuration constructor. In this example, we are using a local execution
    target to train the Automated Machine Learning experiment. However, we can also
    provide an Azure Machine Learning dataset and submit the experiment to our training
    cluster:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将所有必需的参数提供给自动机器学习配置构造函数。在这个例子中，我们使用本地执行目标来训练自动机器学习实验。然而，我们也可以提供一个Azure机器学习数据集并将实验提交到我们的训练集群：
- en: '[PRE24]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Let''s submit the Automated Machine Learning configuration as an experiment
    to the defined compute target and wait for completion. We can output the run details:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将自动机器学习配置作为实验提交到定义的计算目标，并等待完成。我们可以输出运行详情：
- en: '[PRE25]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Similar to `HyperDriveConfig`, we can see that `RunDetails` for Automated Machine
    Learning shows a lot of useful information about your current experiment. Not
    only can you see all of your scheduled and running models, but you also get a
    nice visualization of the trained models and their training performance. The following
    screenshot shows the accuracy of the first 14 runs of the Automated Machine Learning
    experiment:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 与`HyperDriveConfig`类似，我们可以看到自动机器学习的`RunDetails`显示了关于你当前实验的大量有用信息。你不仅可以查看所有已安排和正在运行的模式，还可以获得训练模型及其训练性能的精美可视化。以下截图显示了自动机器学习实验的前14次运行的准确度：
- en: '![Figure 11.5 – Automated Machine Learning – visualization of the results ](img/B17928_11_05.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图11.5 – 自动机器学习 – 结果的可视化](img/B17928_11_05.jpg)'
- en: Figure 11.5 – Automated Machine Learning – visualization of the results
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5 – 自动机器学习 – 结果的可视化
- en: 'Finally, after 15 minutes, we can retrieve the best ML pipeline from the Automated
    Machine Learning run. From now on, we will refer to this pipeline simply as the
    **model**, as all the preprocessing steps are packed into the model, which itself
    is a pipeline of operations. We can use the following code to retrieve the pipeline:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，经过15分钟后，我们可以从自动机器学习运行中检索最佳ML管道。从现在起，我们将简单地称这个管道为**模型**，因为所有预处理步骤都打包在这个模型中，它本身就是一个操作管道。我们可以使用以下代码来检索管道：
- en: '[PRE26]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The resulting fitted pipeline (called `best_model`) can now be used exactly
    like a scikit-learn estimator. We can store it on disk, register it to the model
    store, deploy it to a *container instance*, or simply evaluate it on the test
    set. We will see this in more detail in [*Chapter 14*](B17928_14_ePub.xhtml#_idTextAnchor217),
    *Model Deployment, Endpoints, and Operations*. Finally, we want to evaluate the
    best model. To do so, we will take the testing set that we separated from the
    dataset beforehand and predict the output on the fitted model:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 得到的拟合管道（称为`best_model`）现在可以像scikit-learn估计器一样使用。我们可以将其存储在磁盘上，将其注册到模型存储中，部署到*容器实例*，或者简单地评估测试集上的结果。我们将在[*第14章*](B17928_14_ePub.xhtml#_idTextAnchor217)中更详细地了解，*模型部署、端点和操作*。最后，我们想要评估最佳模型。为此，我们将使用之前从数据集中分离出的测试集，并在拟合模型上预测输出：
- en: '[PRE27]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In the preceding code, we used the `accuracy_score` function from scikit-learn
    to compute the accuracy of the final model. These steps are all you need to perform
    classification on a dataset using automatically preprocessed data and fitted models.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用了scikit-learn中的`accuracy_score`函数来计算最终模型的准确度。这些步骤就是你在使用自动预处理数据和拟合模型对数据集进行分类时需要执行的所有步骤。
- en: Summary
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced hyperparameter optimization through **HyperDrive**
    and model optimization through **Automated Machine Learning** Both techniques
    can help you efficiently retrieve the best model for your ML task.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了通过**HyperDrive**进行超参数优化和通过**自动机器学习**进行模型优化。这两种技术都可以帮助你高效地检索到适合你的机器学习任务的最佳模型。
- en: '**Grid sampling** works great with classical ML models, and also when the number
    of tunable parameters is fixed. All the values on a discrete parameter grid are
    evaluated. In **random sampling**, we can apply a continuous distribution for
    the parameter space and select as many parameter choices as we can fit into the
    configured training duration. Random sampling performs better on a large number
    of parameters. Both sampling techniques can/should be tuned using an **early stopping
    criterion**.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '**网格采样**与经典机器学习模型配合得很好，当可调整参数的数量固定时也是如此。离散参数网格上的所有值都会被评估。在**随机采样**中，我们可以为参数空间应用连续分布，并选择尽可能多的参数选择，以适应配置的训练时长。随机采样在大量参数上表现更好。这两种采样技术都可以/应该使用**早期停止标准**进行调整。'
- en: Unlike random and grid sampling, **Bayesian optimization** probes the model
    performance to optimize the following parameter choices. This means that each
    set of parameter choices and the resulting model performance are used to compute
    the next best parameter choices. Therefore, Bayesian optimization uses ML to optimize
    parameter choices for your ML model. Since the underlying Gaussian process requires
    the resulting model performance, early stopping does not work with Bayesian optimization.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 与随机和网格采样不同，**贝叶斯优化**通过探测模型性能来优化以下参数选择。这意味着每一组参数选择和由此产生的模型性能都会用来计算下一组最佳参数选择。因此，贝叶斯优化使用机器学习来优化你的机器学习模型的参数选择。由于底层高斯过程需要模型性能的结果，早期停止在贝叶斯优化中不起作用。
- en: We also learned that Automated Machine Learning is a generalization of Bayesian
    optimization on the complete end-to-end ML pipeline. Instead of choosing only
    hyperparameters, we also choose pre-processing, feature engineering, model selection,
    and model stacking methods and optimize those together. Automated Machine Learning
    speeds up this process by predicting which models will perform well on your data
    instead of blindly trying all possible combinations. Both techniques are essential
    for a great ML project; Automated Machine Learning lets you focus on the data
    and labeling first, while hyperparameter tuning lets you optimize a specific model.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还了解到，自动机器学习是对完整端到端机器学习管道中贝叶斯优化的泛化。我们不仅选择超参数，还选择预处理、特征工程、模型选择和模型堆叠方法，并将它们一起优化。自动机器学习通过预测哪些模型会在你的数据上表现良好，而不是盲目地尝试所有可能的组合，从而加快了这个过程。这两种技术对于优秀的机器学习项目都是必不可少的；自动机器学习让你首先关注数据和标注，而超参数调整则让你优化特定模型。
- en: In the next chapter, we will look at training DNNs where the data or the model
    parameters don't fit into the memory of a single machine anymore, and therefore
    distributed learning is required.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨训练深度神经网络（DNNs）的情况，其中数据或模型参数不再适合单个机器的内存，因此需要分布式学习。
