<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer317">
<h1 class="chapter-number" id="_idParaDest-182"><a id="_idTextAnchor239"/>13</h1>
<h1 id="_idParaDest-183"><a id="_idTextAnchor240"/>Using H2O AutoML with Other Technologies</h1>
<p>In the last few chapters, we have been exploring how we can use H2O AutoML in production. We saw how we can use H2O models as POJOs and MOJOs as portable objects that can make predictions. However, in actual production environments, you will often be using multiple technologies to meet various technical requirements. The collaboration of such technologies plays a big role in the seamless functionality of your system.</p>
<p>Thus, it is important to know how we can use H2O models in collaboration with other commonly used technologies in the ML domain. In this chapter, we shall explore and implement H2O with some of these technologies and see how we can build systems that can work together to provide a collaborative benefit.</p>
<p>First, we will investigate how we can host an H2O prediction service as a web service using the <strong class="bold">Spring Boot</strong> application. Then, we will explore how we can perform real-time prediction using H2O with <strong class="bold">Apache Storm</strong>.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Using H2O AutoML and Spring Boot </li>
<li>Using H2O AutoML and Apache Storm</li>
</ul>
<p>By the end of this chapter, you should have a better understanding of how you can use models trained using H2O AutoML with different technologies to make predictions in different scenarios.</p>
<h1 id="_idParaDest-184"><a id="_idTextAnchor241"/>Technical requirements</h1>
<p>For this chapter, you will require the following:</p>
<ul>
<li>The latest version of your preferred web browser.</li>
<li>An <strong class="bold">Integrated Development Environment</strong> (<strong class="bold">IDE</strong>) of your choice.</li>
<li>All the experiments conducted in this chapter have been performed using IntelliJ IDE on an Ubuntu Linux system. You are free to follow along using the same setup or perform the same experiments using IDEs and operating systems that you are comfortable with.</li>
</ul>
<p>All code examples for this chapter can be found on GitHub at <a href="https://github.com/PacktPublishing/Practical-Automated-Machine-Learning-on-H2O/tree/main/Chapter%2013">https://github.com/PacktPublishing/Practical-Automated-Machine-Learning-on-H2O/tree/main/Chapter%2013</a>.</p>
<p>Let’s jump right into the first section, where we’ll learn how to host models trained using H2O AutoML on a web application created using Spring Boot.</p>
<h1 id="_idParaDest-185"><a id="_idTextAnchor242"/>Using H2O AutoML and Spring Boot</h1>
<p>In today’s times, most software services that are created are hosted on the internet, where they <a id="_idIndexMarker1245"/>can be made accessible to all internet users. All of this is done using web applications hosted on web servers. Even prediction services that use ML can be made available to the public by hosting them on web applications. </p>
<p>The <strong class="bold">Spring Framework</strong> is one <a id="_idIndexMarker1246"/>of the most commonly used open source web application frameworks to create websites and web applications. It is based on the Java platform and, as such, can be <a id="_idIndexMarker1247"/>run on any system with a JVM. <strong class="bold">Spring Boot</strong> is an extension <a id="_idIndexMarker1248"/>of the Spring Framework that provides a preconfigured setup for your web application out of the box. This helps you quickly set up your web application without the need to implement the underlying pipelining needed to configure and host your web service.</p>
<p>So, let’s dive into the implementation by understanding the problem statement.</p>
<h2 id="_idParaDest-186"><a id="_idTextAnchor243"/>Understanding the problem statement</h2>
<p>Let’s assume <a id="_idIndexMarker1249"/>you are working for a wine manufacturing company. The officials have a requirement where they want to automate the process of calculating the quality of wine and its color. The service should be available as a web service where the quality assurance executive can provide some information about the wine’s attributes, and the service uses these details and an underlying ML model to predict the quality of the wine as well as its color.</p>
<p>So, technically, we will need two models to make the full prediction. One will be a regression model that predicts the quality of the wine, while the other will be a classification model that predicts the color of the wine.</p>
<p>We can use <a id="_idIndexMarker1250"/>a combination of the Red Wine Quality and <a id="_idIndexMarker1251"/>White Wine Quality datasets and run H2O AutoML on it to train the models. You can find the datasets at https://archive.ics.uci.edu/ml/datasets/Wine+Quality. The combined dataset is already present at <a href="https://github.com/PacktPublishing/Practical-Automated-Machine-Learning-on-H2O/tree/main/Chapter%2013/h2o_spring_boot/h2o_spring_boot">https://github.com/PacktPublishing/Practical-Automated-Machine-Learning-on-H2O/tree/main/Chapter%2013/h2o_spring_boot/h2o_spring_boot</a>.</p>
<p>The following screenshot shows a sample of the dataset:</p>
<div>
<div class="IMG---Figure" id="_idContainer304">
<img alt="Figure 13.1 – Wine quality and color dataset " height="288" src="image/B17298_13_001.jpg" width="685"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.1 – Wine quality and color dataset</p>
<p>This dataset consists of the following features:</p>
<ul>
<li><strong class="bold">fixed acidity</strong>: This <a id="_idIndexMarker1252"/>feature explains the amount of acidity that is non-volatile, meaning it does not evaporate over a certain period.</li>
<li><strong class="bold">volatile acidity</strong>: This <a id="_idIndexMarker1253"/>feature explains the amount of acidity that is volatile, meaning it will evaporate over a certain period.</li>
<li><strong class="bold">citric acid</strong>: This <a id="_idIndexMarker1254"/>feature explains the amount of citric acid present in the wine.</li>
<li><strong class="bold">residual sugar</strong>: This <a id="_idIndexMarker1255"/>feature explains the amount of residual sugar present in the wine.</li>
<li><strong class="bold">chlorides</strong>: This <a id="_idIndexMarker1256"/>feature explains the number of chlorides present in the wine.</li>
<li><strong class="bold">free sulfur dioxide</strong>: This <a id="_idIndexMarker1257"/>feature explains the amount of free sulfur dioxide present in the wine.</li>
<li><strong class="bold">total sulfur dioxide</strong>: This <a id="_idIndexMarker1258"/>feature explains the amount of total sulfur dioxide present in the wine.</li>
<li><strong class="bold">density</strong>: This <a id="_idIndexMarker1259"/>feature explains the density of the wine.</li>
<li><strong class="bold">pH</strong>: This feature <a id="_idIndexMarker1260"/>explains the pH value of the wine, with 0 being the most acidic and 14 being the most basic. </li>
<li><strong class="bold">sulphates</strong>: This <a id="_idIndexMarker1261"/>feature explains the number of sulfates present in the wine.</li>
<li><strong class="bold">alcohol</strong>: This <a id="_idIndexMarker1262"/>feature explains the amount of alcohol present in the wine.</li>
<li><strong class="bold">quality</strong>: This is <a id="_idIndexMarker1263"/>the response column, which notes the quality of the wine. 0 indicates that the wine is very bad, while 10 indicates that the wine is excellent.</li>
<li><strong class="bold">color</strong>: This <a id="_idIndexMarker1264"/>feature represents the color of the wine.</li>
</ul>
<p>Now that we understand the problem statement and the dataset that we will be working with, let’s design the architecture to show how this web service will work.</p>
<h2 id="_idParaDest-187"><a id="_idTextAnchor244"/>Designing the architecture </h2>
<p>Before we dive deep into the implementation of the service, let’s look at the overall architecture <a id="_idIndexMarker1265"/>of how all of the technologies should work together. The following is the architecture diagram of the wine quality and color prediction web service:</p>
<div>
<div class="IMG---Figure" id="_idContainer305">
<img alt="Figure 13.2 – Architecture of the wine quality and color prediction web service " height="290" src="image/B17298_13_002.jpg" width="752"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.2 – Architecture of the wine quality and color prediction web service</p>
<p>Let’s <a id="_idIndexMarker1266"/>understand the various components of this architecture:</p>
<ul>
<li><strong class="bold">Client</strong>: This is the person – or in this case, the wine quality assurance executive – who will be using the application. The client communicates with the web application by making a POST request to it, passing the attributes of the wine, and getting the quality and color of the wine as a prediction response.</li>
<li><strong class="bold">Spring Boot Application</strong>: This is the web application that runs on a web server and is responsible for performing the computation processes. In our scenario, this is the application that will be accepting the POST request from the client, feeding the data to the model, getting the prediction results, and sending the results back to the client as a response.</li>
<li><strong class="bold">Tomcat Web server</strong>: The web server is nothing but the software and hardware that handles the HTTP communication over the internet. For our scenario, we shall be using the Apache Tomcat web server. Apache Tomcat is a free and open source HTTP web server written in Java. The web server is responsible for forwarding client requests to the web application.</li>
<li><strong class="bold">Models</strong>: These are the trained models in the form of POJOs that will be loaded onto the Spring Boot application. The application will use these POJOs using the <strong class="source-inline">h2o-genmodel</strong> library to make predictions.</li>
<li><strong class="bold">H2O server</strong>: Mode<a id="_idTextAnchor245"/>ls will be trained using the H2O server. As we saw in <a href="B17298_01.xhtml#_idTextAnchor017"><em class="italic">Chapter 1</em></a>, <em class="italic">Understanding H2O AutoML Basics</em>, we can run H2O AutoML on an H2O server. We shall do the same for our scenario by starting an H2O server, training <a id="_idIndexMarker1267"/>the models using H2O AutoML, and then downloading the trained models as POJOs so that we can load them into the Spring Boot application.</li>
<li><strong class="bold">Dataset</strong>: This is the wine quality dataset that we are using to train our models. As stated in the previous section, this dataset is a combination of the Red Wine Quality and White Wine Quality datasets.</li>
</ul>
<p>Now that we have a good understanding of how we are going to create our wine quality and color prediction web service, let’s move on to its implementation.</p>
<h2 id="_idParaDest-188"><a id="_idTextAnchor246"/>Working on the implementation</h2>
<p>This service <a id="_idIndexMarker1268"/>has already been built and is available on GitHub. The code base can be found at <a href="https://github.com/PacktPublishing/Practical-Automated-Machine-Learning-on-H2O/tree/main/Chapter%2013/h2o_spring_boot">https://github.com/PacktPublishing/Practical-Automated-Machine-Learning-on-H2O/tree/main/Chapter%2013/h2o_spring_boot</a>.</p>
<p>Before we dive into the code, make sure your system meets the following minimum requirements:</p>
<ul>
<li>Java version 8 and above</li>
<li>The latest version of Maven, preferably version 3.8.6</li>
<li>Python version 3.7 and above</li>
<li>H2O Python library installed using pip3</li>
<li>Git installed on your system</li>
</ul>
<p>First, we will clone the GitHub repository, open it in our preferred IDE, and go through the files to understand the whole process. The following steps have been performed on <em class="italic">Ubuntu 22.04 LTS</em> and we <a id="_idIndexMarker1269"/>are using <strong class="bold">IntelliJ IDEA</strong> <em class="italic">version 2022.1.4</em> as the IDE. Feel free to use any IDE of your choice that supports Maven and the Spring Framework for better support.</p>
<p>So, clone <a id="_idIndexMarker1270"/>the GitHub repository and navigate to <strong class="source-inline">Chapter 13/h2o_spring_boot/</strong>. Then, you start your IDE and open the project. Once you have opened the project, you should get a directory structure similar to the following:</p>
<div>
<div class="IMG---Figure" id="_idContainer306">
<img alt="Figure 13.3 – Directory structure of h2o_wine_predictor " height="511" src="image/B17298_13_003.jpg" width="568"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.3 – Directory structure of h2o_wine_predictor</p>
<p>The directory <a id="_idIndexMarker1271"/>structure consists of the following important files:</p>
<ul>
<li><strong class="source-inline">pom.xml</strong>: A <strong class="bold">Project Object Model</strong> (<strong class="bold">POM</strong>) is the fundamental unit of the Maven <a id="_idIndexMarker1272"/>build automation tool. It is an XML file that contains all the information about all the dependencies needed, as well as the configurations needed to correctly build the application.</li>
<li><strong class="source-inline">script.py</strong>: This is the Python script that we will use to train our models on the wine quality dataset. The script starts an H2O server instance, imports the dataset, and then runs AutoML to train the models. We shall look at it in more detail later.</li>
<li><strong class="source-inline">src/main/java/com.h2o_wine_predictor.demo/api/PredictionController.java</strong>: This is the controller file that has the request mapping to direct the POST request to execute the mapped function. The function eventually calls the actual business logic where predictions are made using the ML models and the response is sent back.</li>
<li><strong class="source-inline">src/main/java/com.h2o_wine_predictor.demo/service/PredictionService.java</strong>: This is the actual file where the business logic of making predictions resides. This function imports the POJO models and the h2o-genmodel library and uses them to predict the data received from the controller.</li>
<li><strong class="source-inline">src/main/java/com.h2o_wine_predictor.demo/Demo</strong>: This is the main function of the Spring Boot application. If you want to start the Spring Boot application, you must execute this main function, which starts the Apache Tomcat server that hosts the web application. </li>
<li><strong class="source-inline">src/main/resources/winequality-combined.csv</strong>: This is where the actual CSV dataset is stored. The Python script that trains the H2O models picks the dataset from this path and starts training the models.</li>
</ul>
<p>You may have noticed that we don’t have the model POJO files anywhere in the directory. So, let’s build those. Refer to the <strong class="source-inline">script.py</strong> Python file and let’s understand what is being done line by line.</p>
<p>The <a id="_idIndexMarker1273"/>code for <strong class="source-inline">script.py</strong> is as follows:</p>
<ol>
<li>The script starts by importing the dependencies:<p class="source-code">import h2o</p><p class="source-code">import shutil</p><p class="source-code">from h2o.automl import H2OautoML</p></li>
<li>Once importing is done, the script initializes the H2O server:<p class="source-code">h2o.init()</p></li>
<li>Once the H2O server is up and running, the script imports the dataset from the <strong class="source-inline">src/main/resources</strong> directory:<p class="source-code">wine_quality_dataframe = h2o.import_file(path = "sec/main/resources/winequality_combined.csv")</p></li>
<li>Since the column color is categorical, the script sets it to <strong class="source-inline">factor</strong>:<p class="source-code">wine_quality_dataframe["color"] = wine_quality_dataframe["color"].asfactor()</p></li>
<li>Finally, you will need a training and validation DataFrame to train and validate your model during training. Therefore, the script also splits the DataFrame into a 70/30 ratio:<p class="source-code">train, valid = wine_quality_dataframe.split_frame(ratios=[.7])</p></li>
<li>Now that the DataFrames are ready, we can begin the training process for training the first model, which is the classification model to classify the color of the wine. So, the script sets the label and features, as follows:<p class="source-code">label = "color"</p><p class="source-code">features = ["fixed acidity", "volatile acidity", "citric acid", "residual sugar", "chlorides", "free sulfur dioxide", "total sulfur dioxide", "density", "pH", "sulphates", "alcohol"]</p></li>
<li>Now that the training data is ready, we can create the H2O AutoML object and begin the model training. The following script does this:<p class="source-code">aml_for_color_predictor = H2OAutoML(max_models=10, seed=123, exclude_algos=["StackedEnsemble"], max_runtime_secs=300)</p><p class="source-code">aml_for_color_predictor.train(x = features, y = label, training_frame=train, validation_frame = valid)</p></li>
</ol>
<p>When <a id="_idIndexMarker1274"/>initializing the <strong class="source-inline">H2OautoML</strong> object, we set the <strong class="source-inline">exclude_algos</strong> parameter with the <strong class="source-inline">StackedEnsemble</strong> value. This is done as stacked ensemble models are not supported by POJOs, as we learned in <a href="B17298_10.xhtml#_idTextAnchor196"><em class="italic">Chapter 10</em></a>, <em class="italic">Working with Plain Old Java Objects (POJOs)</em>.</p>
<p>This starts the AutoML model training process. Some <strong class="source-inline">print</strong> statements will help you observe the progress and results of the model training process.</p>
<ol>
<li value="8">Once the model training process is done, the script will retrieve the leader model and download it as a POJO with the correct name – that is, <strong class="source-inline">WineColorPredictor</strong> – and place it in the <strong class="source-inline">tmp</strong> directory:<p class="source-code">model = aml_for_color_predictor.leader</p><p class="source-code">model.model_id = "WineColorPredictor"</p><p class="source-code">print(model)</p><p class="source-code">model.download_pojo(path="tmp")</p></li>
<li>Next, the script will do the same for the next model – that is, the regression model – to predict the quality of the wine. It slightly tweaks the label and sets it to <strong class="source-inline">quality</strong>. The rest of the steps are the same:<p class="source-code">label="quality"</p><p class="source-code">aml_for_quality_predictor = H2OAutoML(max_models=10, seed=123, exclude_algos=["StackedEnsemble"], max_runtime_secs=300)</p><p class="source-code">aml_for_quality_predictor.train(x = features, y = label, training_frame=train, validation_frame = valid)</p></li>
<li>Once the training is finished, the script will extract the leader model, name it <strong class="source-inline">WineQualityPredictor</strong>, and download it as a POJO in the <strong class="source-inline">tmp</strong> directory:<p class="source-code">model = aml_for_color_predictor.leader</p><p class="source-code">model.model_id = "WineQualityPredictor"</p><p class="source-code">print(model)</p><p class="source-code">model.download_pojo(path="tmp")</p></li>
<li>Now that <a id="_idIndexMarker1275"/>we have both model POJOs downloaded, we need to move them to the <strong class="source-inline">src/main/java/com.h2o_wine_predictor.demo/model/</strong> directory. But before we do that, we will also need to add the POJOs to the <strong class="source-inline">com.h2o.wine_predictor.demo</strong> package so that the <strong class="source-inline">PredictionService.java</strong> file can import the models. So, the script does this by creating a new file, adding the package inclusion instruction line to the file, appending the rest of the original POJO file, and saving the file in the <strong class="source-inline">src/main/java/com.h2o_wine_predictor.demo/model/</strong> directory:<p class="source-code">with open("tmp/WineColorPredictor.java", "r") as raw_model_POJO:</p><p class="source-code">     with open("src/main/java/com.h2o_wine_predictor.demo/model/ WineColorPredictor.java", "w") as model_POJO:</p><p class="source-code">           model_POJO.write(f'package com.h2o_wine_predictor.demo;\n' + raw_model_POJO.read())</p></li>
<li>It does the same for the <strong class="source-inline">WineQualityPredictor</strong> model:<p class="source-code">with open("tmp/WineQualityPredictor.java", "r") as raw_model_POJO:</p><p class="source-code">     with open("src/main/java/com.h2o_wine_predictor.demo/model/ WineQualityPredictor.java", "w") as model_POJO:</p><p class="source-code">           model_POJO.write(f'package com.h2o_wine_predictor.demo;\n' + raw_model_POJO.read())</p></li>
<li>Finally, it deletes the <strong class="source-inline">tmp</strong> directory to clean everything up:<p class="source-code">shutil.rmtree("tmp")</p></li>
</ol>
<p>So, let’s run <a id="_idIndexMarker1276"/>this script and generate our models. You can do so by executing the following command in your Terminal:</p>
<p class="source-code">python3 script.py</p>
<p>This should generate the respective model POJO files in the <strong class="source-inline">src/main/java/com.h2o_wine_predictor.demo/model/</strong> directory.</p>
<p>Now, let’s observe the <strong class="source-inline">PredictionService</strong> file in the <strong class="source-inline">src/main/java/com.h2o_wine_predictor.demo/service</strong> directory.</p>
<p>The <strong class="source-inline">PredictionService</strong> class inside the <strong class="source-inline">PredictionService</strong> file has the following <a id="_idIndexMarker1277"/>attributes:</p>
<ul>
<li><strong class="source-inline">wineColorPredictorModel</strong>: This is an attribute of the <strong class="source-inline">EasyPredictModelWrapper</strong> type. It is a class from the h2o-genmodel library that is imported by the <strong class="source-inline">PredictionService</strong> file. We use this attribute to load the <strong class="source-inline">WineColorPredictor</strong> model that we just generated using <strong class="source-inline">script.py</strong>. We shall use this attribute to make predictions on the incoming request later.</li>
<li><strong class="source-inline">wineQualityPredictorModel</strong>: Similar to <strong class="source-inline">wineColorPredictorModel</strong>, this is the wine quality equivalent attribute that uses the same <strong class="source-inline">EasyPredictModelWrapper</strong>. This attribute will be used to load the <strong class="source-inline">WineQualityPredictor</strong> model and use it to make predictions on the quality of the wine.</li>
</ul>
<p>Now that we <a id="_idIndexMarker1278"/>understand the attributes of this file, let’s check out the methods, which are as follows:</p>
<ul>
<li><strong class="source-inline">createJsonResponse()</strong>: This function is pretty straightforward in the sense that it takes the binomial classification prediction result from the <strong class="source-inline">WineColorPredictor</strong> model and the regression prediction result from the <strong class="source-inline">WineQualityPredictor</strong> model and combines them into a JSON response that the web application sends back to the client.</li>
<li><strong class="source-inline">predictColor()</strong>: This function uses the <strong class="source-inline">wineColorPredictorModel</strong> attribute of the <strong class="source-inline">PredictionService</strong> class to make predictions on the data. It outputs the prediction result of the color of the wine as a <strong class="source-inline">BinomialModelPrediction</strong> object, which is a part of the h2o-genmodel library.</li>
<li><strong class="source-inline">predictQuality()</strong>: This function uses the <strong class="source-inline">wineQualityPredictorModel</strong> attribute of the <strong class="source-inline">PredictionService</strong> class to make predictions on the data. It outputs the prediction result of the quality of the wine as a <strong class="source-inline">RegressionModelPrediction</strong> object, which is part of the h2o-genmodel library.</li>
<li><strong class="source-inline">fillRowDataFromHttpRequest()</strong>: This function is responsible for converting the feature values received from the POST request into a <strong class="source-inline">RowData</strong> object that will be passed to <strong class="source-inline">wineQualityPredictorModel</strong> and <strong class="source-inline">wineColorPredictorModel</strong> to make predictions. <strong class="source-inline">RowData</strong> is an object from the h2o-genmodel library.</li>
<li><strong class="source-inline">getPrediction()</strong>: This is called by <strong class="source-inline">PredictionController</strong>, which passes the feature values as a map to make predictions on. This function internally calls all the previously mentioned functions and orchestrates the entire prediction process: </li>
</ul>
<ol>
<li value="1">It gets the feature values from the POST request as input. It passes these values, which are in the form of <strong class="source-inline">Map</strong> objects, to <strong class="source-inline">fillRowDataFromHttpRequest()</strong>, which converts them into the <strong class="source-inline">RowData</strong> type. </li>
<li>Then, it passes this <strong class="source-inline">RowData</strong> to the <strong class="source-inline">predictColor()</strong> and <strong class="source-inline">predictQuality()</strong> functions to get the prediction values. </li>
<li>Afterward, it passes these results to the <strong class="source-inline">createJsonResponse()</strong> function to create <a id="_idIndexMarker1279"/>an appropriate JSON response with the prediction values and returns the JSON to <strong class="source-inline">PredictionController</strong>, where the controller returns it to the client.</li>
</ol>
<p>Now that we have had a chance to go through the important parts of the whole project, let’s go ahead and run the application so that we can have the web service running locally on our machines. Then, we will run a simple <strong class="source-inline">cURL</strong> command with the wine quality feature values and see if we get the predictions as a response.</p>
<p>To start the application, you can do the following:</p>
<ul>
<li>If you are using IntelliJ IDE, then you can directly click on the green play button in the top-right corner of the IDE.</li>
<li>Alternatively, you can directly run it from your command line by executing the following command inside the project directory where the <strong class="source-inline">pom.xml</strong> file is:<p class="source-code">mvn spring-boot:run -e</p></li>
</ul>
<p>If everything is working fine, then you should get an output similar to the following:</p>
<div>
<div class="IMG---Figure" id="_idContainer307">
<img alt="Figure 13.4 – Successful Spring Boot application run output " height="861" src="image/B17298_13_004.jpg" width="1622"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.4 – Successful Spring Boot application run output</p>
<p>Now that <a id="_idIndexMarker1280"/>the Spring Boot application is running, the only thing remaining is to test this out by making a POST request call to the web service running on <strong class="source-inline">localhost:8082</strong>.</p>
<p>Open another Terminal and execute the following <strong class="source-inline">curl</strong> command to make a prediction request:</p>
<p class="source-code">curl -X POST localhost:8082/api/v1/predict -H "Content-Type: application/json" -d '{"fixed acidity":6.8,"volatile acidity":0.18,"citric acid":0.37,"residual sugar":1.6,"chlorides":0.055,"free sulfur dioxide":47,"total sulfur dioxide":154,"density":0.9934,"pH":3.08," ,"sulphates":0.45,"alcohol":9.1}'</p>
<p>The request should go to the web application, where the application will extract the feature values, convert them into the <strong class="source-inline">RowData</strong> object type, pass <strong class="source-inline">RowData</strong> to the prediction function, get the prediction results, convert the prediction results into an appropriate <strong class="source-inline">JSON</strong>, and get the <strong class="source-inline">JSON</strong> back as a response. This should look as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer308">
<img alt="Figure 13.5 – Prediction result from the Spring Boot web application " height="266" src="image/B17298_13_005.jpg" width="965"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.5 – Prediction result from the Spring Boot web application</p>
<p>From the JSON response, you can see that the predicted color of the wine is <strong class="source-inline">white</strong> and that its quality is <strong class="source-inline">5.32</strong>.</p>
<p>Congratulations! You have just implemented an ML prediction service on a Spring Boot web application. You can further expand this service by adding a frontend that takes the feature values <a id="_idIndexMarker1281"/>as input and a button that, upon being clicked, creates a POST body of all those values and sends the API request to the backend. Feel free to experiment with this project as there is plenty of scope for how you can use H2O model POJOs on a web service.</p>
<p>In the next section, we’ll learn how to make real-time predictions using H2O AutoML, along with another interesting technology called Apache Storm.</p>
<h1 id="_idParaDest-189"><a id="_idTextAnchor247"/>Using H2O AutoML and Apache Storm</h1>
<p><strong class="bold">Apache Storm</strong> is an open <a id="_idIndexMarker1282"/>source data analysis and computation <a id="_idIndexMarker1283"/>tool for processing large amounts of stream data in real <a id="_idIndexMarker1284"/>time. In the real world, you will often have plenty of systems that continuously generate large amounts of data. You may need to make some computations or run some processes on this data to extract useful information as it is generated in real time.</p>
<h2 id="_idParaDest-190"><a id="_idTextAnchor248"/>What is Apache Storm?</h2>
<p>Let’s take the <a id="_idIndexMarker1285"/>example of a <strong class="bold">log system</strong> in a very heavily used web service. Assuming that this web <a id="_idIndexMarker1286"/>service receives millions of requests per second, it is going to generate tons of logs. And you already have a system in place that stores these logs in your database. Now, this log data will eventually pile up and you will have petabytes of log data stored in your database. Querying all this historical data to process it in one go is going to be very slow and time-consuming.</p>
<p>What you can do is process the data as it is generated. This is where Apache Storm comes into play. You can configure your Apache Storm application to perform the needed processing and direct your log data to flow through it and then store it in your database. This will streamline the processing, making it real-time.</p>
<p>Apache Storm can <a id="_idIndexMarker1287"/>be used for multiple use cases, such as real-time analytics, <strong class="bold">Extract-Transform-Load</strong> (<strong class="bold">ETL</strong>) data in data pipelines, and even ML. What makes Apache Storm the go-to solution for real-time processing is because of how fast it is. A benchmarking test performed by the Apache Foundation found Apache Storm to process around a million tuples per second per node. Apache Storm is also very scalable and fault-tolerant, which guarantees that it will process all the incoming real-time data.</p>
<p>So, let’s dive deep into the architecture of Apache Storm to understand how it works.</p>
<h3>Understanding the architecture of Apache Storm</h3>
<p>Apache Storm uses <a id="_idIndexMarker1288"/>cluster <a id="_idIndexMarker1289"/>computing, similar to how <strong class="bold">Hadoop</strong> and even H2O work. Consider the following architectural diagram of Apache Storm:</p>
<div>
<div class="IMG---Figure" id="_idContainer309">
<img alt="Figure 13.6 – Architecture of Apache Storm " height="391" src="image/B17298_13_006.jpg" width="798"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.6 – Architecture of Apache Storm</p>
<p>Apache Storm <a id="_idIndexMarker1290"/>distinguishes the nodes in its cluster into two categories – a master node and a worker node. The features of these nodes are as follows:</p>
<ul>
<li><strong class="bold">Master Node</strong>: The <a id="_idIndexMarker1291"/>master node runs a special daemon <a id="_idIndexMarker1292"/>called <strong class="bold">Nimbus</strong>. The Nimbus daemon is responsible for distributing the data among all the worker nodes in the cluster. It also monitors failures and will resend the data to other nodes once a failure is detected, ensuring that no data is left out of processing.</li>
<li><strong class="bold">Worker Node</strong>: The <a id="_idIndexMarker1293"/>worker nodes run a daemon called the <strong class="bold">Supervisor</strong>. The <a id="_idIndexMarker1294"/>Supervisor daemon is the service that is constantly listening for work and starts or stops the underlying processes as necessary for the computation.</li>
</ul>
<p>The communication between the master node and the worker nodes using their respective daemons <a id="_idIndexMarker1295"/>is done using the <strong class="bold">Zookeeper cluster</strong>. In short, the Zookeeper cluster is a centralized service that maintains configuration and synchronization services for stateless groups. In this scenario, the master node and the worker nodes are stateless and fast-failing services. All the state details are stored in the Zookeeper cluster. This is beneficial as keeping the nodes stateless helps with fault tolerance as the nodes can be brought back to life and they will start working as if nothing had happened.</p>
<p class="callout-heading">Tip</p>
<p class="callout">If you are interested in understanding the various concepts and technicalities of Zookeeper, then feel <a id="_idIndexMarker1296"/>free to explore it in detail at <a href="https://zookeeper.apache.org/">https://zookeeper.apache.org/</a>.</p>
<p>Before we move <a id="_idIndexMarker1297"/>on to the implementation part of Apache Storm, we need to be aware of certain concepts that are important to understand how Apache Storm works. The different concepts are as follows:</p>
<ul>
<li><strong class="bold">Tuples</strong>: Apache Storm uses <a id="_idIndexMarker1298"/>a data model called Tuple as its primary unit of data that is to be processed. It is a named list of values and can be an object of any type. Apache Storm supports all primitive data types out of the box. But it can also support custom objects, which can be deserialized into primitive types.</li>
<li><strong class="bold">Streams</strong>: Streams are <a id="_idIndexMarker1299"/>unbounded sequences of tuples. A stream represents the path from where your data flows from one transformation to the next. The basic primitives that Apache Storm provides for doing these transformations are spouts and bolts:<ul><li><strong class="bold">Spouts</strong>: A spout is <a id="_idIndexMarker1300"/>a source for a stream. It is at the start of the stream from where it reads the data from the outside world. It takes this data from the outside world and sends it to a bolt.</li><li><strong class="bold">Bolt</strong>: A bolt is a <a id="_idIndexMarker1301"/>process that consumes data from single or multiple streams, transforms or processes it, and then outputs the result. You can link multiple bolts one after the other while feeding the output of one bolt as input to the next to perform complex processing. Bolts can run functions, filter data, perform aggregation, and even store data in databases. You can perform any kind of functionality you want on a bolt.</li></ul></li>
<li><strong class="bold">Topologies</strong>: The <a id="_idIndexMarker1302"/>entire orchestration of how data will be processed in real time using streams, spouts, and bolts <a id="_idIndexMarker1303"/>in the form of a <strong class="bold">Directed Acyclic Graph</strong> (<strong class="bold">DAG</strong>) is called a <strong class="bold">topology</strong>. You need to submit this topology to the Nimbus <a id="_idIndexMarker1304"/>daemon using the main function of Apache Storm. The topology graph contains nodes and edges, just like a regular graph structure. Each node contains processing logic and each edge shows how data is to be transferred <a id="_idIndexMarker1305"/>between two nodes. Both the Nimbus and the topology are <strong class="bold">Apache Thrift</strong> structures, which are special type systems that allow programmers to use native types in any programming language.</li>
</ul>
<p class="callout-heading">Tip</p>
<p class="callout">You can <a id="_idIndexMarker1306"/>learn more about Apache Thrift by going to <a href="https://thrift.apache.org/docs/types">https://thrift.apache.org/docs/types</a>.</p>
<p>Now that you have a better understanding of what Apache Storm is and the various concepts involved in its implementation, we can move on to the implementation part of this section, starting with installing Apache Storm.</p>
<p class="callout-heading">Tip</p>
<p class="callout">Apache Storm is a very powerful and sophisticated system. It has plenty of applicability outside of just machine learning and also has plenty of features and support. If you want to learn <a id="_idIndexMarker1307"/>more about Apache Storm, go to <a href="https://storm.apache.org/">https://storm.apache.org/</a>.</p>
<h3>Installing Apache Storm</h3>
<p>Let’s start <a id="_idIndexMarker1308"/>by noting down the basic requirements for installing Apache Storm. They are as follows:</p>
<ul>
<li>Java version greater than Java 8</li>
<li>The latest version of Maven, preferably version 3.8.6</li>
</ul>
<p>So, make sure these basic requirements are already installed on your system. Now, let’s start by <a id="_idIndexMarker1309"/>downloading the Apache Storm repo. You can find the repo at <a href="https://github.com/apache/storm">https://github.com/apache/storm</a>.</p>
<p>So, execute the following command to clone the repository to your system:</p>
<p class="source-code">git clone https://github.com/apache/storm.git</p>
<p>Once the download is finished, you can open the <strong class="source-inline">storm</strong> folder to get a glimpse of its contents. You will notice that there are tons of files, so it can be overwhelming when you’re trying to figure out where to start. Don’t worry – we’ll work on very simple examples that <a id="_idIndexMarker1310"/>should be enough to give you a basic idea of how Apache Storm works. Then, you can branch out from there to get a better understanding of what Apache Storm has to offer.</p>
<p>Now, open your Terminal and navigate to the cloned repo. You will need to locally build Apache Storm itself before you can go about implementing any of the Apache Storm features. You need to do this as locally building Apache Storm generates important JAR files that get installed in your <strong class="source-inline">$HOME/.m2/repository</strong> folder. This is the folder where Maven will pick up the JAR dependencies when you build your Apache Storm application.</p>
<p>So, locally build Apache Storm by executing the following command at the root of the repository:</p>
<p class="source-code">mvn clean install -DskipTests=true</p>
<p>The build might take some time, considering that Maven will be building several JAR files that are important dependencies to your application. So, while that is happening, let’s understand the problem statement that we will be working on.</p>
<h2 id="_idParaDest-191"><a id="_idTextAnchor249"/>Understanding the problem statement</h2>
<p>Let’s assume you are working for a medical company. The medical officials have a requirement, where <a id="_idIndexMarker1311"/>they want to create a system that predicts whether the person is likely to suffer from any complications after surviving a heart failure or whether they are safe to be discharged. The catch is that this prediction service will be used by all the hospitals in the country, and they need immediate prediction results so that the doctors can decide whether to keep the patient admitted for a few days to monitor their health or decide to discharge them.</p>
<p>So, the machine learning problem is that there will be streams of data that our system will need to make immediate predictions. We can set up a Apache Storm application that streams all the data into the prediction service and deploys model POJOs trained using H2O AutoML to make the predictions.</p>
<p>We can <a id="_idIndexMarker1312"/>train the models on the Heart Failure Clinical dataset, which can be found at <a href="https://archive.ics.uci.edu/ml/datasets/Heart+failure+clinical+records">https://archive.ics.uci.edu/ml/datasets/Heart+failure+clinical+records</a>.</p>
<p>The following <a id="_idIndexMarker1313"/>screenshot shows some sample content from the dataset:</p>
<div>
<div class="IMG---Figure" id="_idContainer310">
<img alt="Figure 13.7 – Heart Failure Clinical dataset " height="230" src="image/B17298_13_007.jpg" width="925"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.7 – Heart Failure Clinical dataset</p>
<p>This dataset <a id="_idIndexMarker1314"/>consists of the following features:</p>
<ul>
<li><strong class="bold">age</strong>: This feature indicates the age of the patient in years</li>
<li><strong class="bold">anemia</strong>: This feature indicates the decrease of red blood cells or hemoglobin, where <strong class="source-inline">1</strong> indicates yes and <strong class="source-inline">0</strong> indicates no</li>
<li><strong class="bold">high blood pressure</strong>: This feature indicates if the patient has hypertension, where <strong class="source-inline">1</strong> indicates yes and <strong class="source-inline">0</strong> indicates no</li>
<li><strong class="bold">creatinine phosphokinase</strong>: This feature indicates the level of the CPK enzyme <a id="_idIndexMarker1315"/>in the blood in <strong class="bold">micrograms per liter</strong> (<strong class="bold">mcg/L</strong>)</li>
<li><strong class="bold">diabetes</strong>: This feature indicates if the patient has diabetes, where <strong class="source-inline">1</strong> indicates yes and <strong class="source-inline">0</strong> indicates no</li>
<li><strong class="bold">ejection fraction</strong>: This feature indicates the percentage of blood leaving the heart at each contraction</li>
<li><strong class="bold">platelets</strong>: This feature <a id="_idIndexMarker1316"/>indicates the platelets in the blood in kilo platelets per <strong class="bold">milliliter</strong> (<strong class="bold">ml</strong>)</li>
<li><strong class="bold">sex</strong>: This feature indicates if the patient is a man or woman, where <strong class="source-inline">1</strong> indicates the patient is a woman and <strong class="source-inline">0</strong> indicates the patient is a man</li>
<li><strong class="bold">serum creatinine</strong>: This <a id="_idIndexMarker1317"/>feature indicates the level of serum creatinine in the blood in <strong class="bold">milligrams per deciliter</strong> (<strong class="bold">mg/dL</strong>)</li>
<li><strong class="bold">serum sodium</strong>: This <a id="_idIndexMarker1318"/>feature indicates the level of serum sodium in the blood <strong class="bold">milliequivalent per liter</strong> (<strong class="bold">mEq/L</strong>)</li>
<li><strong class="bold">smoking</strong>: This feature indicates if the patient smokes or not, where <strong class="source-inline">1</strong> indicates yes and <strong class="source-inline">0</strong> indicates no</li>
<li><strong class="bold">time</strong>: This feature indicates the number of follow-ups in days</li>
<li><strong class="bold">complications</strong>: This feature indicates if the patient faced any complications during the follow-up period, where <strong class="source-inline">1</strong> indicates yes and <strong class="source-inline">0</strong> indicates no</li>
</ul>
<p>Now that we understand the problem statement and the dataset that we will be working with, let’s design the architecture of how we can use Apache Storm and H2O AutoML to solve this problem.</p>
<h2 id="_idParaDest-192"><a id="_idTextAnchor250"/>Designing the architecture </h2>
<p>Let’s look <a id="_idIndexMarker1319"/>at the overall architecture of how all the technologies should work together. Refer to the following architecture diagram of the heart failure complication prediction service:</p>
<div>
<div class="IMG---Figure" id="_idContainer311">
<img alt="Figure 13.8 – Architecture diagram of using H2O AutoML with Apache Storm " height="1016" src="image/B17298_13_008.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.8 – Architecture diagram of using H2O AutoML with Apache Storm</p>
<p>Let’s <a id="_idIndexMarker1320"/>understand the various components of the architecture:</p>
<ul>
<li><strong class="source-inline">script.py</strong>: From an architectural point of view, the solution is pretty simple. First, we train the models using H2O AutoML, which can be easily done by using this script, which imports the dataset, sets the label and features, and runs AutoML. The leader model can then be extracted as a POJO, which we can later use in Apache Storm to make predictions.</li>
<li><strong class="bold">Data Spout</strong>: We will have a spout in Apache Storm that constantly reads data and passes it to the <strong class="bold">Prediction Bolt</strong> in real time. </li>
<li><strong class="bold">Prediction Bolt</strong>: This bolt contains the prediction service that imports the trained model POJO and uses it to make predictions. </li>
<li><strong class="bold">Classification Bolt</strong>: The results from the Prediction Bolt are passed to this bolt. This bolt <a id="_idIndexMarker1321"/>classifies the results as potential complications and no complications based on the binary classification result from the Prediction Bolt.</li>
</ul>
<p>Now that we have designed a simple and good solution, let’s move on to its implementation.</p>
<h2 id="_idParaDest-193"><a id="_idTextAnchor251"/>Working on the implementation</h2>
<p>This service <a id="_idIndexMarker1322"/>is already available on GitHub. The code base can be found at <a href="https://github.com/PacktPublishing/Practical-Automated-Machine-Learning-on-H2O/tree/main/Chapter%2013/h2o_apache_storm/h2o_storm">https://github.com/PacktPublishing/Practical-Automated-Machine-Learning-on-H2O/tree/main/Chapter%2013/h2o_apache_storm/h2o_storm</a>.</p>
<p>So, download the repo and navigate to <strong class="source-inline">/Chapter 13/h2o_apache_storm/h2o_storm/</strong>.</p>
<p>You will see that we have two folders. One is the <strong class="source-inline">storm-starter</strong> directory, while the other is the <strong class="source-inline">storm-streaming</strong> directory. Let’s focus on the <strong class="source-inline">storm-streaming</strong> directory first. Start your IDE and open the <strong class="source-inline">storm-streaming</strong> project. Once you open the project, you should see a directory structure similar to the following:</p>
<div>
<div class="IMG---Figure" id="_idContainer312">
<img alt="Figure 13.9 – storm_streaming directory structure " height="341" src="image/B17298_13_009.jpg" width="899"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.9 – storm_streaming directory structure</p>
<p>This directory <a id="_idIndexMarker1323"/>structure consists of the following important files:</p>
<ul>
<li><strong class="source-inline">scripty.py</strong>: This is the Python script that we will use to train our models on the heart failure complication dataset. The script starts an H2O server instance, imports the dataset, and then runs AutoML to train the models. We shall look at this in more detail later.</li>
<li><strong class="source-inline">H2ODataSpout.java</strong>: This is the Java file that contains the Apache Storm spout and its functionality. It reads the data from the <strong class="source-inline">live_data.csv</strong> file and forwards individual observations one at a time to the bolts, simulating the real-time flow of data.</li>
<li><strong class="source-inline">H2OStormStarter.java</strong>: This is a Java file that contains the Apache Storm topology with the two bolts – the Prediction Bolt and Classification Bolt classes. We shall start our Apache Storm service using this file.</li>
<li><strong class="source-inline">training_data.csv</strong>: This is the dataset that contains a part of the heart failure complication data that we will be using to train our models.</li>
<li><strong class="source-inline">live_data.csv</strong>: This is the dataset that contains the heart failure complication data that we will be using to simulate the real-time inflow of data into our Apache Storm application.</li>
</ul>
<p>Unlike the previous experiments, where we made changes in a separate application repository, for this experiment, we shall make changes in Apache Storm’s repository.</p>
<p>The following <a id="_idIndexMarker1324"/>steps have been performed on <em class="italic">Ubuntu 22.04 LTS</em>; <em class="italic">IntelliJ IDEA version 2022.1.4</em> has been used as the IDE. Feel free to use any IDE of your choice that supports the Maven framework for better support.</p>
<p>Let’s start by understanding the model training script, <strong class="source-inline">script.py</strong>. The code for <strong class="source-inline">script.py</strong> is as follows:</p>
<ol>
<li value="1">First, the script imports the dependencies:<p class="source-code">import h2o</p><p class="source-code">from h2o.automl import H2OautoML</p></li>
<li>Once importing is done, the H2O server is initialized:<p class="source-code">h2o.init()</p></li>
<li>Once the H2O server is up and running, the script imports the <strong class="source-inline">training_data.csv</strong> file:<p class="source-code">wine_quality_dataframe = h2o.import_file(path = "training_data.csv")</p></li>
<li>Now that the DataFrame has been imported, we can begin the training process for training <a id="_idIndexMarker1325"/>the models using AutoML. So, the script sets the label and features, as follows:<p class="source-code">label = "complications"</p><p class="source-code">features = ["age", "anemia", "creatinine_phosphokinase", "diabetes", "ejection_fraction", "high_blood_pressure", "platelets", "serum_creatinine ", "serum_sodium", "sex", "smoking", "time"]</p></li>
<li>Now, we can create the H2O AutoML object and begin the model training:<p class="source-code">aml_for_complications = H2OAutoML(max_models=10, seed=123, exclude_algos=["StackedEnsemble"], max_runtime_secs=300)</p><p class="source-code">aml_for_complications.train(x = features, y = label, training_frame = wine_quality_dataframe )</p></li>
</ol>
<p>Since POJOs are not supported for stacked ensemble models, we set the <strong class="source-inline">exclude_algos</strong> parameter with the <strong class="source-inline">StackedEnsemble</strong> value. </p>
<p>This starts the AutoML model training process. Some <strong class="source-inline">print</strong> statements are in here that will help you observe the progress and results of the model training process.</p>
<ol>
<li value="6">Once the model training process is done, the script retrieves the leader model and downloads it as a POJO with the correct name – that is, <strong class="source-inline">HeartFailureComplications</strong> – and places it in the <strong class="source-inline">tmp</strong> directory:<p class="source-code">model = aml_for_color_predictor.leader</p><p class="source-code">model.model_id = "HeartFailureComplications"</p><p class="source-code">print(model)</p><p class="source-code">model.download_pojo(path="tmp")</p></li>
</ol>
<p>So, let’s run this script and generate our model. Executing the following command in your Terminal:</p>
<p class="source-code"><strong class="bold">python3 script.py</strong></p>
<p>This should generate the respective model POJO files in the <strong class="source-inline">tmp</strong> directory.</p>
<p>Now, let’s investigate the next file in the repository: <strong class="source-inline">H2ODataSpout.java</strong>. The <strong class="source-inline">H2ODataSpout</strong> class in the Java file has a few attributes and functions that are important for <a id="_idIndexMarker1326"/>building the Apache Storm applications. We won’t focus on them much, but let’s have a look at the functions that do play a bigger <a id="_idIndexMarker1327"/>role in the business logic of the applications. They are as follows:</p>
<ul>
<li><strong class="source-inline">nextTuple()</strong>: This function contains the logic of reading the data from the <strong class="source-inline">live_data.csv</strong> file and emits the data row by row to the Prediction Bolt. Let’s have a quick look at the code:<ol><li>First, you have the sleep timer. Apache Storm, as we know, is a super-fast real-time data processing system. Observing our live data flowing through the system will be difficult for us, so the <strong class="source-inline">sleep</strong> function ensures that there is a delay of 1,000 milliseconds so that we can easily observe the flow of data and see the results:</li></ol><p class="source-code">Util.sleep(1000)</p></li>
</ul>
<ol>
<li value="2">The function then instantiates the <strong class="source-inline">live_data.csv</strong> file into the program:</li>
</ol>
<p class="source-code">File file = new File("live_data.csv")</p>
<ol>
<li value="3">The code then declares the <strong class="source-inline">observation</strong> variable. This is nothing but the individual row data that will be read and stored in this variable by the spout:</li>
</ol>
<p class="source-code">String[] observation = null;</p>
<ol>
<li value="4">Then, we have the logic where the spout program reads the row in the data. Which row to read is decided by the <strong class="source-inline">_cnt</strong> atomic integer, which gets incremented <a id="_idIndexMarker1328"/>as the spout reads and emits the row to the Prediction Bolt in an infinite loop. This infinite loop simulates the continuous flow of data, despite <strong class="source-inline">live_data.csv</strong> containing only limited data:</li>
</ol>
<p class="source-code">try {</p>
<p class="source-code">      String line="";</p>
<p class="source-code">      BufferedReader br = new BufferedReader(new FileReader(file));</p>
<p class="source-code">      while (i++&lt;=_cnt.get()) line = br.readLine(); // stream thru to next line</p>
<p class="source-code">      observation = line.split(",");</p>
<p class="source-code">    } catch (Exception e) {</p>
<p class="source-code">      e.printStackTrace();</p>
<p class="source-code">      _cnt.set(0);</p>
<p class="source-code">    }</p>
<ol>
<li value="5">Then, we have the atomic number increment so that the next iteration picks up the next row in the data:</li>
</ol>
<p class="source-code">_cnt.getAndIncrement();</p>
<p class="source-code">if (_cnt.get() == 1000) _cnt.set(0);</p>
<ol>
<li value="6">Finally, we have the <strong class="source-inline">_collector.emit()</strong> function, which emits the row data so that it’s stored in <strong class="source-inline">_collector</strong>, which, in turn, is consumed by the Prediction Bolt:</li>
</ol>
<p class="source-code">_collector.emit(new Values(observation));</p>
<ul>
<li><strong class="source-inline">declareOutputFields()</strong>: In this method, we declare the headers of our data. We can extract and use the headers from our trained AutoML model POJO using its <strong class="source-inline">NAMES</strong> attribute:<p class="source-code">LinkedList&lt;String&gt; fields_list = new LinkedList&lt;String&gt;(Arrays.asList(ComplicationPredictorModel.NAMES));</p><p class="source-code">fields_list.add(0,"complication");</p><p class="source-code">String[] fields = fields_list.toArray(new String[fields_list.size()])<a id="_idTextAnchor252"/>;</p><p class="source-code">declarer.declare(new Fields(fields));</p></li>
<li><em class="italic">Other miscellaneous functions</em>: The remaining <strong class="source-inline">open()</strong>, <strong class="source-inline">close()</strong>, <strong class="source-inline">ack()</strong>, <strong class="source-inline">fail()</strong>, and <strong class="source-inline">getComponentConfiguration()</strong> functions are supportive functions for <a id="_idIndexMarker1329"/>error handling and preprocessing or postprocessing activities that you might want to do in the spout. To keep this experiment simple, we won’t dwell on them too much.</li>
</ul>
<p>Moving on, let’s investigate the <strong class="source-inline">H2OStormStarter.java</strong> file. This file contains both bolts that are needed for performing the predictions and classification, as well as the <strong class="source-inline">h2o_storm()</strong> function, which builds the Apache Storm topology and passes it onto the Apache Storm cluster. Let’s dive <a id="_idIndexMarker1330"/>deep into the individual attributes:</p>
<ul>
<li><strong class="source-inline">class PredictionBolt</strong>: This is the <strong class="source-inline">Bolt</strong> class and is responsible for obtaining the class probabilities of the heart failure complication dataset. It imports the H2O model POJO and uses it to calculate the class probabilities of the incoming row data. It has three functions – <strong class="source-inline">prepare()</strong>, <strong class="source-inline">execute()</strong> and <strong class="source-inline">declareOutputFields()</strong>. We shall only focus on the <strong class="source-inline">execute</strong> function since it contains the execution logic of the bolt; the rest are supportive functions. The <strong class="source-inline">execute</strong> function contains the following code:<ol><li value="1">The very first thing this function does is import the H2O model POJO:</li></ol><p class="source-code">HeartFailureComplications h2oModel = new HeartFailureComplications();</p></li>
</ul>
<ol>
<li value="2">Then, it extracts the input tuple values from its parameter variables and stores them in the <strong class="source-inline">raw_data</strong> variable:</li>
</ol>
<p class="source-code">ArrayList&lt;String&gt; stringData = new ArrayList&lt;String&gt;();</p>
<p class="source-code">for (Object tuple_value : tuple.getValues()) stringData.add((String) tuple_value);</p>
<p class="source-code">String[] rawData = stringData.toArray(new String[stringData.size()]);</p>
<ol>
<li value="3">Next, the code <a id="_idIndexMarker1331"/>categorically maps all the categorical data in the row: </li>
</ol>
<p class="source-code">double data[] = new double[rawData.length-1]; </p>
<p class="source-code">String[] columnName = tuple.getFields().toList().toArray(new String[tuple.size()]);</p>
<p class="source-code">for (int I = 1; i &lt; rawData.length; ++i) {</p>
<p class="source-code">   data[i-1] = h2oModel.getDomainValues(columnName[i]) == null</p>
<p class="source-code">             ? Double.valueOf(rawData[i])</p>
<p class="source-code">            : h2oModel.mapEnum(h2oModel.getColIdx(columnName[i]), rawData[i]);</p>
<p class="source-code">}</p>
<ol>
<li value="4">Then, the code gets the prediction and emits the results:</li>
</ol>
<p class="source-code">double[] predictions = new double [h2oModel.nclasses()+1];</p>
<p class="source-code">h2oModel.score0(data, predictions);</p>
<p class="source-code">_collector.emit(tuple, new Values(rawData[0], predictions[1]));</p>
<ol>
<li value="5">Finally, the code acknowledges the tuple so that the spout is informed about its consumption and won’t resend the tuple for retry:</li>
</ol>
<p class="source-code">_collector.ack(tuple);</p>
<ul>
<li><strong class="bold">Classifier Bolt</strong>: This bolt receives the input from the Prediction Bolt. The functionality of this bolt is very simple: it takes the class probabilities from the input and compares them against the threshold value to determine what the predicted <a id="_idIndexMarker1332"/>outcome will be. Similar to the Prediction Bolt, this <strong class="source-inline">Bolt</strong> class also has some supportive functions, along with the main <strong class="source-inline">execute()</strong> function. Let’s dive deep into this to understand what is going on in the function:<ul><li>The function simply computes if there is a possibility of <em class="italic">Possible Complication</em> or <em class="italic">No Complications</em> based on the <strong class="source-inline">_threshold</strong> value and emits the result back:<p class="source-code">_collector.emit(tuple, new Values(expected, complicationProb &lt;= _threshold ? "No Complication" : "Possible Complication"));</p><p class="source-code">_collector.ack(tuple);</p></li></ul></li>
<li><strong class="source-inline">h2o_storm()</strong>: This is the main function of the application and builds the topology using <strong class="source-inline">H2ODataSpout</strong> and the two bolts – Prediction Bolt and Classifier Bolt. Let’s have a deeper look into its functionality.<ol><li value="1">First, the function instantiates <strong class="source-inline">TopologyBuilder()</strong>:</li></ol><p class="source-code">TopologyBuilder builder = new TopologyBuilder();</p></li>
</ul>
<ol>
<li value="2">Using this object, it builds the topology by setting the spout and the bolts, as follows:</li>
</ol>
<p class="source-code">builder.setSpout("inputDataRow", new H2ODataSpout(), 10);</p>
<p class="source-code">builder.setBolt("scoreProbabilities", new PredictionBolt(), 3).shuffleGrouping("inputDataRow");</p>
<p class="source-code">builder.setBolt("classifyResults", new ClassifierBolt(), 3).shuffleGrouping("scoreProbabilities");</p>
<ol>
<li value="3">Apache Storm also needs some configuration data to set up its cluster. Since we are creating a simple example, we can just use the default configurations, as follows:</li>
</ol>
<p class="source-code">Config conf = new Config();</p>
<ol>
<li value="4">Finally, it creates a cluster and submits the topology it created, along with the configuration:</li>
</ol>
<p class="source-code">LocalCluster cluster = new LocalCluster();</p>
<p class="source-code">cluster.submitTopology("HeartComplicationPredictor", conf, builder.createTopology());</p>
<ol>
<li value="5">After that, there are some functions to wrap the whole experiment together. The <strong class="source-inline">Util.sleep()</strong> function is used to pause for an hour so that Apache Storm <a id="_idIndexMarker1333"/>can loop over the functionality indefinitely while simulating a continuous flow of real-time data. The <strong class="source-inline">cluster.killTopology()</strong> function kills the <strong class="source-inline">HeartComplicationPredictor</strong> topology, which stops the simulation in the cluster. Finally, the <strong class="source-inline">cluster.shutdown()</strong> function brings down the Apache Storm cluster, freeing up the resources:</li>
</ol>
<p class="source-code">Utils.sleep(1000 * 60 * 60);</p>
<p class="source-code">cluster.killTopology("HeartComplicationPredictor");</p>
<p class="source-code">cluster.shutdown();</p>
<p>Now that we have a better understanding of the contents of the files and how we are going to be running our service, let’s proceed and look at the contents of the <strong class="source-inline">storm-starter</strong> project. The directory structure will be as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer313">
<img alt="Figure 13.10 – storm-starter directory structure " height="412" src="image/B17298_13_010.jpg" width="729"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.10 – storm-starter directory structure</p>
<p>The <strong class="source-inline">src</strong> directory contains several different types of Apache Storm topology samples that you can choose <a id="_idIndexMarker1334"/>to experiment with. I highly recommend that you do so as that will help you get a better understanding of how versatile Apache Storm is when it comes to configuring your streaming service for different needs.</p>
<p>However, we shall perform this experiment in the <strong class="source-inline">test</strong> directory to keep our files isolated from the ones in the <strong class="source-inline">src</strong> directory. So, let’s see how we can run this experiment.</p>
<p>Follow these steps to build and run the experiment:</p>
<ol>
<li value="1">In the <strong class="source-inline">storm-streaming</strong> directory, run the <strong class="source-inline">script.py</strong> file to generate the H2O model POJO. The script should run H2O AutoML and generate a leaderboard. The leader model will be extracted, renamed <strong class="source-inline">HeartFailureComplications</strong>, and downloaded as a POJO. Run the following command in your Terminal:<p class="source-code"><strong class="bold">python3 script.py</strong></p></li>
<li>The <strong class="source-inline">HeartFailureComplications</strong> POJO will be imported by the other files in the <strong class="source-inline">storm-starter</strong> project, so to ensure that it can be correctly imported by files in the same package, we need to add this POJO to that same package. So, modify the POJO file to add the <strong class="source-inline">storm.starter</strong> package as the first line.</li>
<li>Now, move the <strong class="source-inline">HeartFailureComplications</strong> POJO file, the <strong class="source-inline">H2ODataSpout.java</strong> file, and the <strong class="source-inline">H2OStormStarted.java</strong> file inside the <strong class="source-inline">storm-starter</strong> repository inside its <strong class="source-inline">storm-starter/test/jvm/org.apache.storm.starter</strong> directory.</li>
<li>Next, we need <a id="_idIndexMarker1335"/>to import the <strong class="source-inline">h2o-model.jar</strong> file into the <strong class="source-inline">storm-starter</strong> project. We can do so by adding the following dependency to the <strong class="source-inline">pom.xml</strong> file of the experiment, as follows:<p class="source-code">&lt;dependency&gt;</p><p class="source-code">    &lt;groupId&gt;ai.h2o&lt;/groupId&gt;</p><p class="source-code">    &lt;artifactId&gt;h2o-genmodel&lt;/artifactId&gt;</p><p class="source-code">    &lt;version&gt;3.36.1.3&lt;/version&gt;</p><p class="source-code">&lt;/dependency&gt;</p></li>
</ol>
<p>Your directory should now look as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer314">
<img alt="Figure 13.11 – storm-starter directory structure after file transfers " height="477" src="image/B17298_13_011.jpg" width="999"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.11 – storm-starter directory structure after file transfers</p>
<ol>
<li value="5">Finally, we will <a id="_idIndexMarker1336"/>run this project by right-clicking on the <strong class="source-inline">H2OStormStarter.java</strong> file and running it. You should get a stream of constant output that demonstrates your spout and bolt in action. This can be seen in the following s<a id="_idTextAnchor253"/>creenshot:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer315">
<img alt="Figure 13.12 – Heart complication prediction output in Apache Storm " height="511" src="image/B17298_13_012.jpg" width="1563"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.12 – Heart complication prediction output in Apache Storm</p>
<p>If you observe the results closely, you should see that there are executors in the logs; all the Apache Storm spouts and bolts are internal executor processes that run on the cluster. You will also see <a id="_idIndexMarker1337"/>the prediction probabilities besides each tuple. This should look as follows:</p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<div>
<div class="IMG---Figure" id="_idContainer316">
<img alt="Figure 13.13 – Heart complication prediction result " height="384" src="image/B17298_13_013.jpg" width="991"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.13 – Heart complication prediction result</p>
<p>Congratulations – we have just covered another design pattern that shows us how we can use models trained using H2O AutoML to make real-time predictions on streaming data using Apache Storm. This concludes the last experiment of this chapter.</p>
<h1 id="_idParaDest-194"><a id="_idTextAnchor254"/>Summary</h1>
<p>In this chapter, we focused on how we can implement models that have been trained using H2O AutoML in different scenarios using different technologies to make predictions on different kinds of data.</p>
<p>We started by implementing an AutoML leader model in a scenario where we tried to make predictions on data over a web service. We created a simple web service that was hosted on localhost using Spring Boot and the Apache Tomcat web server. We trained the model on data using AutoML, extracted the leader model as a POJO, and loaded that POJO as a class in the web application. By doing this, the application was able to use the model to make predictions on the data that it received as a POST request, responding with the prediction results.</p>
<p>Then, we looked into another design pattern where we aimed to make predictions on real-time data. We had to implement a system that can simulate the real-time flow of data. We did this with Apache Storm. First, we dived deep into understanding what Apache Storm is, its architecture, and how it works by using spouts and bolts. Using this knowledge, we built a real-time data streaming application. We deployed our AutoML trained model in a Prediction Bolt where the Apache Storm application was able to use the model to make predictions on the real-time streaming data.</p>
<p>This concludes the final chapter of this book. There are still innumerable features, concepts, and design patterns that we can work with while using H2O AutoML. The more you experiment with this technology, the better you will get at implementing it. Thus, it is highly recommended that you keep experimenting with this technology and discover new ways of solving ML problems while automating your ML workflows.</p>
</div>
</div></body></html>