- en: Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations! You have finished this book's introductory section, in which
    you have explored a great number of topics, and if you were able to follow it,
    you are prepared to start the journey of understanding the inner workings of many
    machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore some effective and simple approaches for automatically
    finding interesting data conglomerates, and so begin to research the reasons for
    natural groupings in data.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: A line-by-line implementation of an example of the K-means algorithm, with explanations
    of the data structures and routines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A thorough explanation of the **k-nearest neighbors (K-NN)** algorithm, using
    a code example to explain the whole process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional methods of determining the optimal number of groups representing
    a set of samples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grouping as a human activity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Humans typically tend to agglomerate everyday elements into groups of similar
    features. This feature of the human mind can also be replicated by an algorithm.
    Conversely, one of the simplest operations that can be initially applied to any
    unlabeled dataset is to group elements around common features.
  prefs: []
  type: TYPE_NORMAL
- en: As we have described, in this stage of the development of the discipline, clustering
    is taught as an introductory theme that's applied to the simplest categories of
    element sets.
  prefs: []
  type: TYPE_NORMAL
- en: But as an author, I recommend researching this domain, because the community
    is hinting that the current model's performance will all reach a plateau, before
    aiming for the full generalization of tasks in AI. And what kinds of method are
    the main candidates for the next stages of crossing the frontier towards AI? Unsupervised
    methods, in the form of very sophisticated variations of the methods explained
    here.
  prefs: []
  type: TYPE_NORMAL
- en: But let's not digress for now, and let's begin with the simplest of grouping
    criteria, the distance to a common center, which is called **K-means**.
  prefs: []
  type: TYPE_NORMAL
- en: Automating the clustering process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The grouping of information for clustering follows a common pattern for all
    techniques. Basically, we have an initialization stage, followed by the iterative
    insertion of new elements, after which the new group relationships are updated.
    This process continues until the stop criteria is met, where the group characterization
    is finished. The following flow diagram illustrates this process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e8b3b8ac-4c99-4c46-85f3-2cc3a19196df.png)'
  prefs: []
  type: TYPE_IMG
- en: General scheme for a clustering algorithm
  prefs: []
  type: TYPE_NORMAL
- en: After we get a clear sense of the overall process, let's start working with
    several cases where this scheme is applied, starting with **K-means**.
  prefs: []
  type: TYPE_NORMAL
- en: Finding a common center - K-means
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here we go! After some necessary preparation review, we will finally start to
    learn from data; in this case, we are looking to label data we observe in real
    life.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we have the following elements:'
  prefs: []
  type: TYPE_NORMAL
- en: A set of N-dimensional elements of numeric type
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A predetermined number of groups (this is tricky because we have to make an
    educated guess)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A set of common representative points for each group (called **centroids**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main objective of this method is to split the dataset into an arbitrary
    number of clusters, each of which can be represented by the mentioned centroids.
  prefs: []
  type: TYPE_NORMAL
- en: 'The word centroid comes from the mathematics world, and has been translated
    to calculus and physics. Here we find a classical representation of the analytical
    calculation of a triangle''s centroid:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c4e6e22-e42e-40f1-ac57-e3e33a1d3fb3.png)'
  prefs: []
  type: TYPE_IMG
- en: Graphical depiction of the centroid finding scheme for a triangle
  prefs: []
  type: TYPE_NORMAL
- en: 'The centroid of a finite set of *k* points, *x[1 ,]x[2], ..., x[k]* in *R^n*,
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e3fa38b1-903b-45f5-8b83-9110e30a4696.png)'
  prefs: []
  type: TYPE_IMG
- en: Analytic definition of centroid
  prefs: []
  type: TYPE_NORMAL
- en: So, now that we have defined this central metric, let's ask the question, "*What
    does it have to do with the grouping of data elements?*"
  prefs: []
  type: TYPE_NORMAL
- en: To answer this, we must first understand the concept of **distance to a centroid.**
    Distance has many definitions, which could be linear, quadratic, and other forms.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s review some of the main distance types, and then we will mention
    which one is normally used:'
  prefs: []
  type: TYPE_NORMAL
- en: In this review, we will work with 2D variables when defining the measure types,
    as a mean of simplification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the following distance types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Euclidean distance:** This distance metric calculates the distance in the
    form of a straight line between two points, or has the following formula:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/1878fb81-cfab-4c9a-8d97-a795402cd224.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Chebyshev distance**: This distance is equivalent to the maximum distance,
    along any of the axes. It''s also called the **chess** distance, because it gives
    the minimum quantity of moves a king needs to get from the initial point to the
    final point. Its defined by the following formula:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/370b76dd-41f1-4534-b64b-02e57756ac1d.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Manhattan distance**: This distance is the equivalent to going from one point
    to another in a city, with unit squares. This L1-type distance sums the number
    of horizontal units advanced, and the number of vertical ones. Its formula is
    as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/1f39a02e-79b1-4e3d-b620-4899a5a8e9ae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following diagram further explains the formulas for the different types
    of distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/864586ba-845d-4477-b165-21f2c4472c38.png)'
  prefs: []
  type: TYPE_IMG
- en: Graphical representation of some of the most well known distance types
  prefs: []
  type: TYPE_NORMAL
- en: The distance metric chosen for K-means is the Euclidean distance, which is easy
    to compute and scales well to many dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have all the elements, it''s time to define the criteria we will
    use to define which label we will assign to any given sample. Let''s summarize
    the learning rule with the following statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '"A sample will be assigned to the group represented by the closest centroid."'
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this method is to minimize the sum of squared distances from the
    cluster’s members to the actual centroid of all clusters that contain samples.
    This is also known as **minimization of inertia.**
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, we can see the results of a typical K-means algorithm
    applied to a sample population of blob-like groups, with a preset number of clusters
    of 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0bedcf60-5ed3-453b-91d1-aa195356380f.png)'
  prefs: []
  type: TYPE_IMG
- en: Typical result of a clustering process using K-means, with a seed of 3 centroids
  prefs: []
  type: TYPE_NORMAL
- en: K-means is a simple and effective algorithm that can be used to obtain a quick
    idea of how a dataset is organized. Its main differentiation is that objects belonging
    to the same class will share a common distance center, which will be incrementally
    upgraded with the addition of each new sample.
  prefs: []
  type: TYPE_NORMAL
- en: Pros and cons of K-means
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The advantages of this method are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It scales very well (most of the calculations can be run in parallel)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has been used in a very large range of applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'But simplicity also has some costs (the no silver bullet rule applies):'
  prefs: []
  type: TYPE_NORMAL
- en: It requires a priori knowledge (the number of possible clusters should be known
    beforehand)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The outlier values can skew the values of the centroids, as they have the same
    weight as any other sample
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we assume that the figure is convex and isotropic, it doesn’t work very well
    with non blob alike clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-means algorithm breakdown
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The mechanism of the K-means algorithm can be summarized with the following
    flowchart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f85cd455-dc0a-4cff-8f3b-db941c744c8b.png)'
  prefs: []
  type: TYPE_IMG
- en: Flowchart of the K-means process
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s describe the process in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: We start with the unclassified samples and take K elements as the starting centroids.
    There are also possible simplifications of this algorithm that take the first
    element in the element list for the sake of brevity.
  prefs: []
  type: TYPE_NORMAL
- en: We then calculate the distances between the samples and the first chosen samples,
    and so we get the first calculated centroids (or other representative values).
    You can see in the moving centroids in the illustration a shift toward a more
    intuitive (and mathematically correct) center location.
  prefs: []
  type: TYPE_NORMAL
- en: After the centroids change, their displacement will cause the individual distances
    to change, and so the cluster membership may change.
  prefs: []
  type: TYPE_NORMAL
- en: This is the time when we recalculate the centroids and repeat the first steps,
    in case the stop condition isn’t met.
  prefs: []
  type: TYPE_NORMAL
- en: 'The stopping conditions can be of various types:'
  prefs: []
  type: TYPE_NORMAL
- en: After n-iterations. It could be that either we chose a very large number, and
    we’ll have unnecessary rounds of computing, or it could converge slowly, and we
    will have very unconvincing results if the centroid doesn't have a very stable
    mean.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A possible better criterion for the convergence of the iterations is to take
    a look at the changes of the centroids, whether in total displacement or total
    cluster element switches. The last one is employed normally, so we will stop the
    process once there are no more elements changing from their current cluster to
    another one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The N iterations condition could also be used as a last resort, because it could
    lead to very long processes where no observable change is observed on a large
    number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try to summarize the process of K-NN clustering visually, going through
    a few steps and looking at how the clusters evolve through time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce222ab9-8ca8-44e8-a91f-45ef5c908a75.png)'
  prefs: []
  type: TYPE_IMG
- en: Graphical example of the cluster reconfiguration loop
  prefs: []
  type: TYPE_NORMAL
- en: In subfigure 1, we start seeding the clusters with possible centroids at random
    places, to which we assign the closest data elements; and then in subfigure 2,
    we reconfigure the centroids to the center of the new clusters, which in turn
    reconfigures the clusters again (subfigure 3), until we reach a stationary status.
    The element aggregation could also be made element by element, which will trigger
    softer reconfigurations. This will be the implementation strategy for the practical
    part of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: K-means implementations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will review the concept of K-means with a practical sample,
    from the very basic concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will import the libraries we need. In order to improve the understanding
    of the algorithms, we will use the `numpy` library. Then we will use the well-known
    `matplotlib` library for the graphical representation of the algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: These will be a number of 2D elements, and will then generate the candidate
    centers, which will be of four 2D elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to generate a dataset, normally a random number generator is used,
    but in this case we want to set the samples to predetermined numbers, for convenience,
    and also to allow you to repeat these procedures manually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Let's represent the sample's center. First, we will initialize a new `matplotlib`
    figure, with the corresponding axes. The `fig` object will allow us to change
    the parameters of all the figures.
  prefs: []
  type: TYPE_NORMAL
- en: The `plt` and `ax` variable names are a standarized way to refer to the plot
    in general and one of the plots' axes.
  prefs: []
  type: TYPE_NORMAL
- en: So let’s try to have an idea of what the samples look like. This will be done
    through the `scatter` drawing type of the `matplotlib` library. It takes as parameters
    the *x* coordinates, the *y* coordinates, size (in points squared), the marker
    type, and color.
  prefs: []
  type: TYPE_NORMAL
- en: There are a variety of markers to choose from, such as point (`.`), circle (`o`),
    square (`s`). To see the full list, visit [https://matplotlib.org/api/markers_api.html.](https://matplotlib.org/api/markers_api.html)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now take a look at the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c394d54d-8441-4e7d-a7d7-df3090a154c0.png)'
  prefs: []
  type: TYPE_IMG
- en: Initial clusters status, Centers as black squares
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s define a function that, given a new sample, will return a list with the
    distances to all the current centroids in order to assign this new sample to one
    of them, and afterward, recalculate the centroids:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Then we need a function that will build, one by one, the step-by-step graphic
    of our application.
  prefs: []
  type: TYPE_NORMAL
- en: It expects a maximum of 12 subplots, and the `plotnumber` parameter will determine
    the position on the 6 x 2 matrix (`620` will be the upper-left subplot, 621 the
    following to the right, and so on writing order).
  prefs: []
  type: TYPE_NORMAL
- en: 'After that, for each picture we will do a scatterplot of the clustered samples,
    and then of the current centroid position:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The following function, called `kmeans`, will use the previous distance function
    to store the centroid that the samples are assigned to (it will be a number from
    `1` to `K`).
  prefs: []
  type: TYPE_NORMAL
- en: The main loop will go from sample `0` to `N`, and for each one, it will look
    for the closest centroid, assign the centroid number to index `n` of the clusters
    array, and sum the samples' coordinates to its currently assigned centroid.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, to get the sample, we use the `bincount` method to count the number of
    samples for each centroid, and by building a `divisor` array, we divide the sum
    of a class elements by the previous `divisor` array, and there we have the new
    centroids:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now it’s time to kickstart the K-means process, using the initial samples and
    centers we set up at first. The current algorithm will show how the clusters are
    evolving, starting from a few elements, into their final state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bccc3f75-1b2f-4dbd-891f-8879acc742d9.png)'
  prefs: []
  type: TYPE_IMG
- en: Depiction of the clustering process, with the centroids represented as black
    squares
  prefs: []
  type: TYPE_NORMAL
- en: Nearest neighbors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: K-NN is another classical method of clustering. It builds groups of samples,
    supposing that each new sample will have the same class as its neighbors, without
    looking for a global representative central sample. Instead, it looks at the environment,
    looking for the most frequent class on each new sample's environment.
  prefs: []
  type: TYPE_NORMAL
- en: Mechanics of K-NN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: K-NN can be implemented in many configurations, but in this chapter we will
    use the semi-supervised approach, starting from a certain number of already assigned
    samples, and later guessing the cluster membership using the main criteria.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, we have a breakdown of the algorithm. It can be summarized
    with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d9e62a74-25fe-4835-af90-154861815e43.png)'
  prefs: []
  type: TYPE_IMG
- en: Flowchart for the K-NN clustering process
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go over all the following involved steps, in a simplified form:'
  prefs: []
  type: TYPE_NORMAL
- en: We place the previously known samples on the data structures.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then read the next sample to be classified, and calculate the Euclidean distance
    from the new sample to every sample in the training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We decide the class of the new element by selecting the class of the nearest
    sample, by Euclidean distance. The K-NN method requires the vote of the K closest
    samples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We repeat the procedure until there are no more remaining samples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This picture will give us a idea of how the new samples are being added. In
    this case, we use a `K` of `1`, for simplicity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a5f405c-465a-4427-9cda-db942851fb95.png)'
  prefs: []
  type: TYPE_IMG
- en: Sample application of a K-NN loop
  prefs: []
  type: TYPE_NORMAL
- en: K-NN can be implemented in more than one of the configurations that we have
    learned, but in this chapter, we will use the semi-supervised approach; we will
    start from a certain number of already assigned samples, and we will later guess
    the cluster membership based on the characteristics of the training set.
  prefs: []
  type: TYPE_NORMAL
- en: Pros and cons of K-NN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The advantages of this method are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Simplicity**: There''s no need to tune parameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No formal training needed**: We just need more training examples to improve
    the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The disadvantage is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It is computationally expensive - in a naive approach, all distances between
    points and every new sample have to be calculated, except when caching is implemented.
  prefs: []
  type: TYPE_NORMAL
- en: K-NN sample implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this simple implementation of the K-NN method, we will use the NumPy and
    Matplotlib libraries. Also, as we will be generating a synthetic dataset for better
    comprehension, we will use the `make_blobs` method from scikit-learn, which will
    generate well-defined and separated groups of information so we have a sure reference
    for our implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Importing the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'So, it''s time to generate the data samples for this example. The parameters
    of `make_blobs` are the number of samples, the number of features or dimensions,
    the quantity of centers or groups, whether the samples have to be shuffled, and
    the standard deviation of the cluster, to control how dispersed the group samples
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a representation of the generated sample blobs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/994e5376-ce65-4ba5-a01c-5c77a6b94ac0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Firstly, let''s define our `distance` function, which will be necessary to
    find the neighbors of all the new elements. We basically provide one sample, and
    return the distance between the provided new element and all their counterparts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `add_sample` function will receive a new 2D sample, the current dataset,
    and an array marking the group of the corresponding sample (from `0` to `3` in
    this case). In this case, we use `argpartition` to get the indices of the three
    nearest neighbors of the new sample, and then we use them to extract a subset
    of the `features` array. Then, `bincount` will return the count of any of the
    different classes on that three-element subset, and then with `argmax`, we will
    choose the index (in this case, the class number) of the group with the most elements
    in that two-element set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we define our main `knn` function, which takes the new data to be added
    and uses the original classified data, represented by the `data` and `features`
    parameters, to decide the classes of the new elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, it''s time to kickstart the process. For this reason, we define a
    set of new samples in the range of -10, 10 on both the `x` and `y` dimensions,
    and we will call our `knn` routine with it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now it''s time to represent the final results. First, we will represent the
    initial samples, which are much more well-formed than our random values, and then
    our final values, represented by an empty square (`c=''none''`), so that they
    will serve as a marker of those samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2fadc21f-1aac-4680-b539-0c6efa4933d6.png)'
  prefs: []
  type: TYPE_IMG
- en: Final clustering status (new classified items are marked with a square)
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding graph, we can see how our simple model of three neighbors works
    well to qualify and reform the grouping as the process advances. As the graph
    shows, the new groupings aren't necessarily of a circular form; they change according
    to the way the incoming data progresses.
  prefs: []
  type: TYPE_NORMAL
- en: Going beyond the basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we are done reviewing illustrative cases of the two main clustering
    techniques, let's explore some more advanced metrics and techniques so we can
    have them in our toolbox.
  prefs: []
  type: TYPE_NORMAL
- en: The Elbow method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the questions that may have arisen when implementing K-means could have
    been "how do I know that the target number of clusters is the best or most representative
    for the dataset?"
  prefs: []
  type: TYPE_NORMAL
- en: For this task, we have the **Elbow** method. It consists of a unique statistical
    measure of the total group dispersion in a grouping. It works by repeating the
    K-means procedure, using an increasing number of initial clusters, and calculating
    the total intra-cluster internal distance for all the groups.
  prefs: []
  type: TYPE_NORMAL
- en: 'Normally, the method will start with a very high value (except if we start
    with the right number of centroids), and then we will observe that the total intra-cluster
    distance drops quite quickly, until we reach a point where it doesn''t change
    significantly. Congratulations, we have found the Elbow point, so-called for being
    an inflection on the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/16f79328-bc5a-400c-86cb-a4a631832f49.png)'
  prefs: []
  type: TYPE_IMG
- en: Graphical depiction of the error evolution, as the number of clusters increases,
    and the inflection point.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding the accuracy of this indicator, the Elbow method is, as you can see,
    a heuristic and not mathematically determined, but can be of use if you want to
    make a quick estimate of the right number of clusters, especially when the curve
    changes very abruptly at some point.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have covered the simplest but still very practical machine
    learning models in an eminently practical way to get us started on the complexity
    scale.
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, where we will cover several regression techniques,
    it will be time to go and solve a new type of problem that we have not worked
    on, even if it's possible to solve the problem with clustering methods (regression),
    using new mathematical tools for approximating unknown values. In it, we will
    model past data using mathematical functions, and try to model new output based
    on those modeling functions.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Thorndike, Robert L, *Who belongs in the family?,* Psychometrika18.4 (1953):
    267-276.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Steinhaus, H, *Sur la division des corp materiels en parties.* Bull. Acad.
    Polon. Sci 1 (1956): 801–804.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MacQueen, James, *Some methods for classification and analysis of multivariate
    observations.* Proceedings of the fifth Berkeley symposium on mathematical statistics
    and probability. Vol. 1\. No. 14\. 1967.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cover, Thomas, and Peter Hart*, *Nearest neighbor pattern classification.*
    IEEE transactions on information theory 13.1 (1967): 21-27.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
