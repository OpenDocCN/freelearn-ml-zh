- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tuning Your Models with AMLS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Tuning your models** is an important step in your data science journey. The
    objective of a data science workload is to provide the best model on unseen data
    in the shortest duration of time. In order to provide a reliable model, not only
    are you required to tune the features that are the inputs to your model but you
    also need to tune the parameters of your model itself. Model parameters, also
    known as **hyperparameters**, can have a significant impact on the performance
    of your trained model. Tuning a model can take a lot of effort and involves trial
    and error. Several frameworks can be leveraged to automate this task. AMLS provides
    this functionality, which we will explore in this chapter. AMLS allows you to
    define model parameters that should be tuned to find the best model through the
    use of a special type of job referred to as a **sweep job**. These hyperparameters
    will be defined for a given AMLS job, and AMLS will run many trials and determine
    the best model for the hyperparameters within the range of possible defined values.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore how AMLS enables hyperparameter tuning through
    a sweep job.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding model parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sampling hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding sweep jobs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a sweep job with grid sampling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a sweep job with random sampling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a sweep job with Bayesian sampling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reviewing the results of a sweep job
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to access your workspace, recall the steps from the previous chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [https://ml.azure.com](https://ml.azure.com).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select your workspace name.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the workspace **user interface** (**UI**), on the left-hand side, click on
    **Compute**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the **Compute** screen, select your compute instance and select **Start**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Start compute](img/B18003_04_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – Start compute
  prefs: []
  type: TYPE_NORMAL
- en: Your compute instance will change from a **Stopped** status to a **Starting**
    status.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the previous chapter, we cloned the Git repository – if you have not already
    done so, continue to follow the steps provided here. If you have already cloned
    the repository, skip to *step 7*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open the terminal on your compute instance. Note that the path will include
    your user in the directory. Type the following into the terminal to clone the
    sample notebooks into your working directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Clicking on the refresh icon shown in *Figure 4**.2* will update and refresh
    the notebooks displayed on your screen:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.2 – The refresh icon](img/B18003_04_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – The refresh icon
  prefs: []
  type: TYPE_NORMAL
- en: 'Review the notebooks in your `Azure-Machine-Learning-Engineering` directory.
    This will display the files cloned into your working directory as shown in *Figure
    4**.3*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Azure-Machine-Learning-Engineering](img/B18003_04_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Azure-Machine-Learning-Engineering
  prefs: []
  type: TYPE_NORMAL
- en: Understanding model parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In your data science workload, as you define your features, you determine which
    parameters should be leveraged by your model. However, depending on the algorithm
    selected, you can control the training behavior by altering the parameters of
    the model itself – this is known as **hyperparameter tuning**. Using hyperparameter
    tuning, we can explore a variety of model parameters to identify the best model
    parameters to establish the best model result. To evaluate the model results,
    a **primary metric** is selected. A primary metric is defined as the key metric
    for evaluating the model. Every time a hyperparameter is changed, the primary
    metric will either go up or down in value and based on the primary metric, that
    will yield a better or worse model.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will create a logistic regression model by leveraging `sklearn`’s
    implementation of logistic regression with an `sklearn` pipeline. For a logistic
    regression model, there are several model parameters that we can tune to improve
    the performance of our model. For a logistic regression model, one parameter is
    the penalty term. Defining the penalty term as `l2`, referred to as ridge regression,
    is used by the logistic regression estimator to apply a penalty to a model when
    the model is overly complex. This uses the `l2` normalization on the coefficients
    of your model as the penalty. Typically, the `l2` normalization prevents the model
    from being overfitted by penalizing complex models. The `l2` norm squares the
    model coefficients, sums them, and takes the square root of the value. When creating
    the model, we can select the `l1` norm, referred to as lasso regression, which
    would be the sum of the absolute value of the coefficients as penalty terms. Changing
    the code from leveraging a `l2` normalization penalty to a `l1` normalization
    penalty would leverage the technique of hyperparameter tuning or tuning our model
    parameters. If we were not going to rely on hyperparameter tuning to select the
    penalty term, we would say that lasso regression generally performs better than
    ridge regression when few predictor variables are significant, and ridge regression
    generally performs better when there are many significant predictor variables.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the penalty term, we can also specify the value of `C`, which
    is the inverse of the regularization strength, as a hyperparameter to tune for
    our model.
  prefs: []
  type: TYPE_NORMAL
- en: The last hyperparameter we will tune is `max_iter`, which is the maximum iterations
    that the solver can take before converging. As you consider the model parameters
    you would like to explore when building your model, you are defining the search
    space. The **search space** is a concept that defines the parameters and the range
    of values possible during hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have an understanding of hyperparameter tuning, and defining a search
    space, we will explore the method for selecting the combinations of model hyperparameters
    to leverage within an AMLS job designed to tune these parameters, known as a **sweep
    job**.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Inside the search space, hyperparameters are either continuous or discrete
    values. Continuous hyperparameters can be in a continuous range of values, while
    discrete hyperparameters are only able to use certain values. For logistic regression,
    the penalty term can have one of two discrete values: `l1` or `l2`. AMLS can use
    either a list or a range for setting hyperparameters, as we will see when we dig
    into the code.'
  prefs: []
  type: TYPE_NORMAL
- en: For the hyperparameter of `C`, we could define it as a discrete value, or we
    could define C to be a value in a continuous range with a specified distribution.
  prefs: []
  type: TYPE_NORMAL
- en: For the `max_iter` hyperparameter, the default value for the `sklearn` logistic
    regression model is `100`. We could set this to a discrete value such as `penality_term`,
    or a uniform value such as `C`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shown in *Figure 4**.4* defines the search space for the
    penalty term, the inverse regularization strength of the model, and the maximum
    iterations as choices, which are discrete values for a defined job command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Defining the search space](img/B18003_04_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Defining the search space
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the search space has been defined, we can select what type of sampling
    we would like our sweep job to run. For each parameter defined in the search space,
    trials are created based on the sampled model hyperparameters. There are three
    types of sampling available with sweep jobs in AMLS: **random**, **grid**, and
    **Bayesian**.'
  prefs: []
  type: TYPE_NORMAL
- en: Grid sampling will create a trial per hyperparameter combination. As an example,
    if we had specified our search space to check for `l1` and `l2`, and we selected
    `C` to be discrete values of `0.01`, `.1`, `1`, and `10` and `max_iter` to be
    `10`, `100`, `150`, and `200`, then there would be `2x4x4=32` trials created.
    Given grid sampling will create a trial per hyperparameter combination, grid sampling
    only supports discrete hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: When leveraging random sampling, the hyperparameter values are randomly selected
    during the trials.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4**.5* shows the hyperparameters defined in the search space here for
    both discrete and continuous values. In this code, the value for `C` follows a
    uniform distribution between values of `0.01` and `10` for a defined job command,
    which shows how we can define `C` as a continuous value, as opposed to with the
    grid job, when `C` was defined as a value from a list as shown in *Figure 4**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Discrete and continuous hyperparameters](img/B18003_04_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – Discrete and continuous hyperparameters
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, since `C` is a continuous hyperparameter, we are not
    able to leverage grid sampling, but we can leverage either random or Bayesian
    sampling.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian sampling leverages the output on the primary metric of the previous
    trial to determine the next set of hyperparameters to run.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Grid space sampling does not support continuous hyperparameters – it will only
    support choice hyperparameters containing discrete values.
  prefs: []
  type: TYPE_NORMAL
- en: Given that hyperparameters can be discrete or continuous, we will define how
    exhaustive the search on the grid space should be and, importantly, how to end
    a job that is searching across a search space to provide the best primary metric
    for a given model. In the next section, we will explore how to set up our job
    to effectively and efficiently search for the best hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding sweep jobs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sweep jobs in AMLS enable a data scientist to define the hyperparameters to
    explore in a single job. During the job, this will automate the task of searching
    for the hyperparameters that will provide a model with the best results for the
    primary metric-creating trials. In a run of a job, multiple trials are created
    and evaluated for the hyperparameters that are defined within the search space
    based on the sampling method selected. By defining the search space, we can create
    a single run of a job for testing multiple hypotheses at a single time rather
    than re-writing code and re-running jobs, reducing the time spent exploring the
    search space.
  prefs: []
  type: TYPE_NORMAL
- en: 'To leverage the hyperparameters in your job, your code needs to be updated
    to leverage these new parameters by passing them into your code through the Python
    `ArgumentParser` shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Passing a parameter list into the job](img/B18003_04_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – Passing a parameter list into the job
  prefs: []
  type: TYPE_NORMAL
- en: Now that the arguments have been passed into the main function, they can be
    leveraged in the model training script by passing them into the `model_train`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the code for passing parameters into the `model_train` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – Passing parameters for model training](img/B18003_04_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – Passing parameters for model training
  prefs: []
  type: TYPE_NORMAL
- en: In the `model_train` function, you can leverage the hyperparameters as you build
    the logistic regression model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the code for passing parameters into the logistic regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – Leveraging hyperparameters](img/B18003_04_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – Leveraging hyperparameters
  prefs: []
  type: TYPE_NORMAL
- en: As with all AML SDK v2 jobs, a sweep job is defined initially as a `job` command.
    In the job command, you specify the code, the location of the file, the command
    with its parameters, inputs, the environment, the compute, and a display name,
    but for the sweep job, we also specify the hyperparameters as we saw in *Figures
    4.4* and *4.5*.
  prefs: []
  type: TYPE_NORMAL
- en: Once the job command is updated to include the hyperparameters, you can specify
    the sweep parameters for the command. In the sweep method, you specify the compute
    that you are going to leverage, the sampling algorithm, the primary metric, and
    the goal. The sampling algorithm can be set to `random`, `bayesian`, or `grid`
    as discussed earlier. The primary metric is the metric that is required to be
    logged in the job that you would like to evaluate the trials against. The goal
    specifies how you would like the primary metric to be evaluated. The goal can
    be to minimize or maximize the primary metric, which is used to judge your model’s
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for the sweep of the job command is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Setting the sweep parameters](img/B18003_04_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – Setting the sweep parameters
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 4**.9* here, you can see the values for sweep parameters being set.
    In this case, we will evaluate the `test_AUC` primary metric to be maximized by
    leveraging a grid sampling algorithm on the compute cluster named `cpu-cluster`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To set the limits for a sweep job, we specify the maximum total trials – `max_total_trials`
    – which defaults to `1000`. The maximum concurrent trials – `max_concurrent_trials`
    – which defaults to the number specified by `max_total_trials` if not set, will
    specify how many concurrent trials should be run at any given time. An additional
    parameter to set is the timeout: `timeout`. The timeout is in seconds. The timeout
    is for the entire sweep job, which defaults to a value of `100800`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Setting job limits for grid sampling](img/B18003_04_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – Setting job limits for grid sampling
  prefs: []
  type: TYPE_NORMAL
- en: Note that in the grid sampling experiment explored in this chapter, the maximum
    number of total runs is actually 32, but if we added additional choices as hyperparameters,
    setting the trial limits will ensure the job does not exceed a total number of
    60 trials in *Figure* *4**.10* here.
  prefs: []
  type: TYPE_NORMAL
- en: With random sampling, hyperparameters are selected at random, so the job limit
    here will be very important to set. As stated, `max_total_trials` defaults to
    `1000`, so we can see that running a job with `max_total_trials` equal to `120`
    will result in only `120` trials being created for a given job run.
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting the maximum limits of the sweep job is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Setting job limits for random sampling](img/B18003_04_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – Setting job limits for random sampling
  prefs: []
  type: TYPE_NORMAL
- en: When leveraging Bayesian sampling, previous trial information is leveraged to
    determine the next parameters for which to search in the search space. With grid
    and random sampling, each trial is independent of other trials. Given each trial
    for grid sampling and random sampling is independent, once the primary metric
    has been determined in a given trial, if it will not yield the best model, there
    is no need for the code to continue running. These trials can be terminated early,
    so compute resources can be used for the next trial primary metric evaluation
    instead.
  prefs: []
  type: TYPE_NORMAL
- en: To enable early termination, AMLS sweep jobs incorporate the concept of an **early
    termination policy**. An early termination policy will prematurely end a given
    trial after the primary metric is logged if the defined criteria are not met.
    For early termination, there are several policies that AMLS supports, including
    **none**, a **truncation** selection policy, a **median** stopping policy, and
    a **bandit** policy.
  prefs: []
  type: TYPE_NORMAL
- en: If no early termination policy is selected, the trial run will execute until
    completion – as explained earlier, this is required for Bayesian sampling. However,
    when grid or random sampling is leveraged, if a truncation policy is selected,
    AMLS will terminate trials based on the selected early termination policy.
  prefs: []
  type: TYPE_NORMAL
- en: Truncation policies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If a truncation policy is selected, AMLS will cancel a percentage of the lowest-performing
    runs based on the value in `truncation_percentage`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the sample code for an early termination policy that starts
    evaluations at an interval of `1`, and terminates the lowest `75`% of all trials
    using a truncation policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – Truncation early termination policy](img/B18003_04_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – Truncation early termination policy
  prefs: []
  type: TYPE_NORMAL
- en: When executing a sweep job with a truncation early termination policy, in the
    **Overview** tab in AMLS, you can review the policy on the **Job Overview** screen.
    Regardless of the early termination policy selected, it can be viewed on the **Job**
    **Overview** screen.
  prefs: []
  type: TYPE_NORMAL
- en: 'An early termination policy is shown as follows in the workspace:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13 – Early termination policy for a sweep job](img/B18003_04_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.13 – Early termination policy for a sweep job
  prefs: []
  type: TYPE_NORMAL
- en: In addition to a truncation policy, we can also set a median policy, which we
    will look at next.
  prefs: []
  type: TYPE_NORMAL
- en: Median policies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another type of early termination policy is the median stopping policy. This
    policy will prematurely end trials based on the median value across all the training
    trials. If a given trial is worse than the median average, it will be terminated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the sample code for an early termination policy that starts evaluation
    at an interval of 2, and terminates any trial that is below the median of all
    trials:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.14 – Median early termination policy](img/B18003_04_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.14 – Median early termination policy
  prefs: []
  type: TYPE_NORMAL
- en: Bandit policies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An early termination policy known as the bandit policy will end runs prematurely
    when the primary metric is not within a certain range of the current most successful
    trial. This range is defined by `slack_factor`. At the time of evaluation, the
    best primary metric value is divided by (1+ `slack_factor`), and if a trial does
    not have a primary metric that is better than that value, it will be terminated
    early. If a data scientist would prefer to set a value rather than setting a ratio
    with `slack_factor`, `slack_amount` can be used instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.15 – Bandit early termination policy](img/B18003_04_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.15 – Bandit early termination policy
  prefs: []
  type: TYPE_NORMAL
- en: To provide some context to the bandit policy, let us assume that the primary
    metric at the interval of `2` so far has the best metric of `.85` for our `test_AUC`.
    In the example here, if the metric is below `.85`/`1.1` or `.773`, then it will
    be terminated.
  prefs: []
  type: TYPE_NORMAL
- en: As the training becomes increasingly complex, establishing the ability to terminate
    trials prematurely ensures that compute resources are not consumed when the desired
    result will ultimately not be achieved.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore setting up a sweep job that leverages grid
    sampling to determine the best model hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a sweep job with grid sampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier in the chapter, we cloned our sample notebook to leverage this material.
    The notebook for this chapter, `'``Chapter 4` `- Hyperparameter Tuning'`, provides
    a review on creating a job command to create a logistic regression model by leveraging
    an `sklearn` pipeline and `mlflow` capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code is then updated and placed into a new directory – the `hyperparametertune`
    folder. The code leverages python’s `argparse` module, which enables you to pass
    parameters into scripts. To run the script that has been generated by this notebook,
    we will create a job command and update the job command to include the hyperparameters
    as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.16 – Job sweep command with hyperparameters for grid sampling](img/B18003_04_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.16 – Job sweep command with hyperparameters for grid sampling
  prefs: []
  type: TYPE_NORMAL
- en: Note that the hyperparameters have been included as inputs to the command, but
    their values are added to the command in line 22 in the preceding figure. This
    could have been done as a single command, but for illustrative purposes, it is
    provided separately as an update to `grid_sampling_job_command`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the command is prepared, we call the sweep method providing the compute,
    the sampling algorithm, the primary metric, and the goal. Recall that here we
    will expect to see 32 runs, so setting `max_total_trials` to `60` will not be
    a hit, but if we were to update the hyperparameters to include more choices, we
    could hit `max_total_trials`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.17 – Calling the sweep method during grid sampling](img/B18003_04_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.17 – Calling the sweep method during grid sampling
  prefs: []
  type: TYPE_NORMAL
- en: Later in the notebook when we look at random sampling and Bayesian sampling,
    we will define the value of `C` as a continuous value – however, to leverage grid
    sampling, we have defined it to be a choice, which is a list of discrete values.
  prefs: []
  type: TYPE_NORMAL
- en: 'To submit `grid_sweep_job`, we pass the command to `ml_client`, which was created
    to manage the connection to AMLS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.18 – Running the sweep job](img/B18003_04_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.18 – Running the sweep job
  prefs: []
  type: TYPE_NORMAL
- en: As the sweep job is executing, it can be helpful to get the status of the running
    job. Given we are leveraging `mlflow` to log the metrics of the `sklearn` model,
    we can request all trials for a given parent `run_id` value, which is the run
    for `grid_sweep_job`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code snippet, we pass in our `experiment_id` and `run_id`
    values and get back the job status:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.19 – Getting the status of the sweep job](img/B18003_04_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.19 – Getting the status of the sweep job
  prefs: []
  type: TYPE_NORMAL
- en: We search across the run for a given `experiment_id` value. Our `experiment_id`
    value will be `'chapter4'`, so we request all runs that have been made for `'chapter4'`
    and further refine that list based on `tags.mlflow.parentRunId`, which is the
    `run_id` value associated with the grid sampling run. In line 3 of *Figure 4**.19*,
    we request runs that have a `run_id` value associated with the run of the grid
    sweep job. If we get back a value of `0`, then we know that we have to wait for
    the run to be established in AMLS. Once there are runs provided, we move on to
    the next `while` loop, where we are checking whether all the runs have been completed.
    Once all the runs are completed for the grid sweep job, the message on line 24
    will be printed.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations on leveraging a sweep job with grid sampling! Grid sampling
    works well when you have a search space that contains discrete values for your
    hyperparameters. Next, we will explore leveraging a sweep job with random sampling
    to enable use to explore continuous values for our hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a sweep job for random sampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw with setting up a command for grid sampling, a command for random
    sampling is simply a job command with the hyperparameters included in the command.
    One difference between the `grid` command and the `random` command is that the
    hyperparameters can be continuous in a random sampling sweep job.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the code for the job command for random sampling:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.20 – Sweep job command with hyperparameters for random sampling](img/B18003_04_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.20 – Sweep job command with hyperparameters for random sampling
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 4**.20* in line 25, the value of `C` is defined to follow
    a uniform distribution from `0.01` to `10.0`, making this a continuous hyperparameter
    across the search space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just as with the grid sampling sweep job, we set parameters for our sweep but
    specify them using a random sampling algorithm as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.21 – Calling the sweep method during random sampling](img/B18003_04_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.21 – Calling the sweep method during random sampling
  prefs: []
  type: TYPE_NORMAL
- en: Given the parameters for the sweep job and the limits have been set, we are
    ready to execute the sweep job. In order to execute the code, again we leverage
    an `ml_client` method to create or update a job as shown in *Figure 4**.18*.
  prefs: []
  type: TYPE_NORMAL
- en: Now you have gone through grid and random sampling, there is an additional type
    of hyperparameter sampling that can be applied, which we will look at next.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a sweep job for Bayesian sampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier in the chapter, we cloned our sample notebook to leverage this material.
    The notebook for this chapter, `'``Chapter 4` `- Hyperparameter Tuning'`, provides
    a review on creating a job command to create a `sklearn` pipeline and `mlflow`
    capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: The code is then updated and placed into a new directory – the `hyperparametertune`
    folder, which leverages Python’s `argparse` module and enables you to pass parameters
    into scripts. To run the script that has been generated by this notebook, we will
    create a job command and update the job command to include the hyperparameters
    as shown in the code snippet here. Conveniently, the job command is the same as
    was found for the random sampling displayed in *Figure 4**.16*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The only difference here is that the sampling algorithm is defined as `bayesian`,
    as shown in the figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.22 – Calling a sweep method using Bayesian sampling](img/B18003_04_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.22 – Calling a sweep method using Bayesian sampling
  prefs: []
  type: TYPE_NORMAL
- en: In order to execute the code, again, we leverage the `ml_client`’s method to
    create or update a job and continue by leveraging the `get_job_status` method
    as shown in *Figure 4**.19*.
  prefs: []
  type: TYPE_NORMAL
- en: You have now made it through grid, random, and Bayesian hyperparameter tuning
    job commands in AMLS. We will continue reviewing the results of the job within
    AMLS.
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing results of a sweep job
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recall that for a single trial of a sweep job, several trials will be created
    to determine the model that will provide the best primary metric. Clicking on
    **jobs** in the left-hand menu pane of your AMLS workspace will display a list
    of your jobs for review. By now, you have run the sample notebook provided for
    this chapter – let’s review your results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clicking on the **chapter4** job brings you to the jobs that you have performed
    as part of this chapter. Selecting the display name will drill into a given job’s
    details. Let us start by reviewing the results of the jobs that we have run as
    shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.23 – Job results](img/B18003_04_023.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.23 – Job results
  prefs: []
  type: TYPE_NORMAL
- en: 'To review the results of the job, follow the next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Clicking on the **Experiment** name, **Chapter04**, will bring us to the different
    job sweeps that have been performed as part of this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Clicking on a given run of your experiment will provide the details for a given
    job run as shown in *Figure 4**.24*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.24 – Job sweep details](img/B18003_04_024.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.24 – Job sweep details
  prefs: []
  type: TYPE_NORMAL
- en: 'Key information was captured as metadata, providing traceability for a given
    job run. This information includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The sampling policy
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The parameter space
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The primary metric
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The best trial
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The number of runs completed
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Clicking on the trial hyperlink from the run details brings up the trial with
    the best results for the selected primary metric:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.25 – Best trial run](img/B18003_04_025.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.25 – Best trial run
  prefs: []
  type: TYPE_NORMAL
- en: 'Clicking on the **Metrics** tab for the best trial run provides the metrics
    for the evaluation of a given trial:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.26 – Metrics for the best trial run of the sweep job](img/B18003_04_026.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.26 – Metrics for the best trial run of the sweep job
  prefs: []
  type: TYPE_NORMAL
- en: 'To review all the trials created for a job run holistically, you can click
    on the job run and head over to the **Trials** tab as shown in *Figure 4**.27*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.27 – Trial runs for a grid sampling sweep job](img/B18003_04_027.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.27 – Trial runs for a grid sampling sweep job
  prefs: []
  type: TYPE_NORMAL
- en: Not only are you able to review each metric individually through the AMLS workspace
    UI for a given run of a job but you can also view a parallel coordinates chart,
    displaying how the different hyperparameters selected impacted the primary metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'To review the search space and its impact on the primary metric, review the
    parallel coordinates chart in the **Trials** tab:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.28 – Grid search space parallel coordinates chart](img/B18003_04_028.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.28 – Grid search space parallel coordinates chart
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the parallel coordinates chart, AMLS provides a 2D scatter chart
    and a 3D scatter chart for evaluating hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 'The 3D scatter chart for hyperparameter tuning is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.29 – 3D scatter chart](img/B18003_04_029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.29 – 3D scatter chart
  prefs: []
  type: TYPE_NORMAL
- en: We have seen how to graphically view and compare models through charts in the
    AML Studio, but we are not limited to accessing this valuable information through
    the UI.
  prefs: []
  type: TYPE_NORMAL
- en: Not only are we able to retrieve the best trial of the sweep job through the
    AMLS Studio by reviewing results in the experiment tab but we are also able to
    access this information by leveraging the AMLS Python SDK v2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Putting the results from the job run into a pandas DataFrame is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.30 – Getting the job results into a pandas DataFrame](img/B18003_04_030.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.30 – Getting the job results into a pandas DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end of the notebook, we have included the code for returning a pandas
    DataFrame for the completed job run:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.31 – Sorting the job trials to get the highest test_AUC for trials](img/B18003_04_031.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.31 – Sorting the job trials to get the highest test_AUC for trials
  prefs: []
  type: TYPE_NORMAL
- en: 'Given we have the pandas DataFrame properly sorted by the primary metric, we
    can easily pull out the `run_id` value for the best trial run as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.32 – Getting the best run_id value from the sorted pandas DataFrame](img/B18003_04_032.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.32 – Getting the best run_id value from the sorted pandas DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the best run retrieved into the `best_run_id` variable as shown
    here, we can leverage the best model from the sweep job.
  prefs: []
  type: TYPE_NORMAL
- en: 'Loading the model from the best sweep job trial is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.33 – Loading the best model](img/B18003_04_033.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.33 – Loading the best model
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model has been loaded and given we are using the virtual environment
    that was used to create the model, we can carry out inference using the loaded
    model as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.34 – Inference using the best model from the sweep job](img/B18003_04_034.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.34 – Inference using the best model from the sweep job
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations – you have retrieved the best trial run for a given sweep job
    from the SDK v2! This run information can be viewed in the UI as well as programmatically.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have explored what model parameters are and how a sweep
    job can be leveraged to tune hyperparameters that are defined for a given model.
    We have also explored options for setting up sweep jobs based on the search space
    and sampling methodology selected. AMLS provides the ability to sweep across the
    search space to tune a model, automating the process of hyperparameter tuning
    on a compute cluster, which will shut itself down in the idle period after the
    trials are completed, consuming compute resources wisely.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to setting up a sweep job, you have been able to review your results
    in the Studio as well as in the code – providing intuitive insight into the best-performing
    model for your use case. Now that you have completed the chapter, be sure to turn
    off your compute resources to save cost.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will show you how to leverage AMLS to take over the
    time-consuming task of model development. This functionality is not only available
    through the SDK and the CLI, but in the AMLS Studio itself as well.
  prefs: []
  type: TYPE_NORMAL
