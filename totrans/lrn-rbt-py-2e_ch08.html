<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis">
<head>
  <meta charset="UTF-8"/>
  <title>Building ChefBot Hardware and the Integration of Software</title>
  <link type="text/css" rel="stylesheet" media="all" href="style.css"/>
  <link type="text/css" rel="stylesheet" media="all" href="core.css"/>
</head>
<body>
  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building ChefBot Hardware and the Integration of Software</h1>
                </header>
            
            <article>
                
<p>In <a href="lrn-rbt-py-2e_ch03.html"><span class="ChapterrefPACKT">Chapter 3</span></a>, <em>Modeling a Differential Robot Using ROS and URDF</em>, we looked at the ChefBot chassis design. In this chapter, we will learn how to assemble this robot using those parts. We will also look at the final interfacing of the sensors and other electronic components of this robot with Tiva-C LaunchPad. After the interfacing, we will learn how to interface the robot with the PC and implement autonomous navigation using SLAM and AMCL in the real robot.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Building ChefBot hardware</li>
<li>Configuring the ChefBot PC and packages</li>
<li>Interfacing the ChefBot sensors with Tiva-C Launchpad</li>
<li>Embedded code for ChefBot</li>
<li>Understanding ChefBot ROS packages</li>
<li>Implementing SLAM on ChefBot</li>
<li>Autonomous navigation in ChefBot</li>
</ul>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>To test the application and codes in this chapter, you will need an Ubuntu 16.04 LTS PC/laptop with ROS Kinetic installed.</p>
<p>You will also need fabricated robot chassis parts for assembling the robot.</p>
<p>You should have all the sensors and other hardware components that can be integrated in the robot.</p>
<p>We have already discussed interfacing individual robot components and sensors with Launchpad. In this chapter, we will try to interface the necessary robotic components and sensors of ChefBot and program it in such a way that it will receive the values from all sensors and control the information from the PC. Launchpad will send all sensor values to the PC via a serial port and also receive control information (such as reset commands, speed data, and so on) from the PC.</p>
<p>After receiving Serial port data&#160;from the Tiva C Launchpad, a ROS Python node will receive the serial values and convert them to ROS topics. There are other ROS&#160; nodes present in the PC that subscribe to these sensor topics and compute robot odometry. The data from the wheel encoders and IMU values combine to calculate the odometry of the robot. The robot detects obstacles by subscribing to the ultrasonic sensor topic and laser scan and controls the speed of the wheel motors using the PID node. This node converts the linear velocity command to a differential wheel velocity command. After running these nodes, we can run SLAM to map the area, and after running SLAM, we can run the AMCL nodes for localization and autonomous navigation.</p>
<p>In the first section of this chapter, <em>Building ChefBot hardware</em>, we will learn how to assemble the ChefBot hardware using the body parts and electronic components of the robot.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building ChefBot hardware</h1>
                </header>
            
            <article>
                
<p>The first section of the robot that needs to be configured is the base plate. The base plate consists of two motors and their attached wheels, the caster wheels, and the base plate supports. The following image shows the top and bottom view of the base plate:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="images/7c2f37b0-96b3-4745-806a-0f0ff4d719f2.png" style="width:49.42em;height:38.42em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Base plate with motors, wheels, and caster wheels</div>
<p>The base plate has a radius of 15 cm, and the motors and their attached wheels are mounted on the opposite sides of the plate by cutting two sections from the base plate. Two rubber caster wheels are mounted on opposite sides of the base plate to achieve a good balance and support for the robot. We can either choose ball caster wheels or rubber caster wheels for this robot. The wires of the two motors are taken to the top of the base plate through a hole in the center of the base plate. To extend the layers of the robot, we will put base plate supports to connect the following layers. Now, let's look at the next layer with the middle plate and connecting tubes. There are hollow tubes to connect the base plate and the middle plate. The hollow tubes can be connected to the base plate support.</p>
<p>The following image shows the middle plate and connecting tubes:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="images/6a94335c-c911-4d6a-a941-7afb5e878bf3.png" style="width:26.67em;height:20.50em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Middle plate with connecting tubes</div>
<p>The connecting tube will connect the base plate and the middle plate. There are four hollow tubes to connect the base plate to the middle plate. One end of these tubes is hollow, which can fit the base plate support, and the other end has a hard plastic fitting with a hole for a screw. The middle plate has no support, except for four holes for the connecting tubes:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="images/7d21f5c1-0a85-4bc9-94e5-76991cc24ab2.png" style="width:26.58em;height:19.17em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Fully assembled robot body</div>
<p>The middle plate male connector helps to connect the middle plate and the top of the base plate tubes. We can fit the top plate at the top of the middle plate tubes using the four supports on the back of the top plate. We can insert the top plate's female connector into the top plate support. Now we have the fully assembled body of the robot.</p>
<p>The bottom layer of the robot can be used to put the <strong>printed circuit board</strong> (<strong>PCB</strong>) and battery. In the middle layer, we can put the Kinect/Orbecc and Intel NUC. We can put a speaker and mic if needed. We can use the top plate to carry food. The following image shows the PCB prototype of the robot; it consists of Tiva-C LaunchPad, a motor driver, level shifters, and provisions to connect two motors, ultrasonic sensors, and IMU:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="images/8b46a5d7-63c6-4f6d-a747-f5838d0d76dd.png" style="width:48.42em;height:36.33em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">ChefBot PCB prototype</div>
<p>The board is powered by a 12 V battery placed on the base plate. The two motors can be directly connected to the M1 and M2 male connectors. The NUC PC and Kinect are placed on the middle plate. The LaunchPad board and Kinect should be connected to the NUC PC via USB. The PC and Kinect are powered using the same 12 V battery itself. We can use a lead-acid or lithium-polymer battery. Here, we are using a lead-acid cell for testing purposes. Later, we will migrate to a lithium-polymer battery for better performance and better backup. The following image shows a diagram of the complete, assembled ChefBot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="images/6c4a26a5-22c1-422a-85f4-e2887838113b.png" style="width:31.08em;height:27.25em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Fully assembled robot body</div>
<p>After assembling all the parts of the robot, we will start working with the robot software. ChefBot's embedded code and ROS packages are available in the codes under <kbd>chapter_8</kbd>. Let's get that code and start working with the software.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Configuring ChefBot PC and setting ChefBot ROS packages</h1>
                </header>
            
            <article>
                
<p>In ChefBot, we are using Intel's NUC PC to handle the robot sensor data and the processing of the data. After procuring the NUC PC, we have to install Ubuntu 16.04 LTS. After the installation of Ubuntu, install the complete ROS and its packages that we mentioned in the previous chapters. We can configure this PC separately, and after the configuration of all the settings, we can put this into the robot. The following are the procedures to install the ChefBot packages on the NUC PC.</p>
<p>Clone ChefBot's software packages from GitHub using the following command:</p>
<pre>    <strong>$ git clone https://github.com/qboticslabs/learning_robotics_2nd_ed</strong>  </pre>
<p>We can clone this code in our laptop and copy the <kbd>ChefBot</kbd> folder to Intel's NUC PC. The <kbd>ChefBot</kbd> folder consists of the ROS packages of the ChefBot hardware. In the NUC PC, create a ROS catkin workspace, copy the <kbd>ChefBot</kbd> folder, and move it inside the <kbd>src</kbd> directory of the catkin workspace.</p>
<p>Build and install the source code of ChefBot by simply using the following command. This should be executed inside the <kbd>catkin</kbd> workspace we created:</p>
<pre>    <strong>$ catkin_make</strong>  </pre>
<p>If all dependencies are properly installed in the NUC, then the ChefBot packages will build and install in this system. After setting the ChefBot packages on the NUC PC, we can switch to the embedded code for ChefBot. Now, we can connect all the sensors in LaunchPad. After uploading the code in LaunchPad, we can again look at the ROS packages and how to run them. The cloned code from GitHub contains the Tiva-C LaunchPad code, which is going to be explained in the next section.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Interfacing ChefBot sensors to the Tiva-C LaunchPad</h1>
                </header>
            
            <article>
                
<p>We have looked at the interfacing of the individual sensors that we are going to use in ChefBot. In this section, we will learn how to integrate sensors into the LaunchPad board. The Energia code to program Tiva-C LaunchPad is available in the cloned files at GitHub. The connection diagram showing the connection of the Tiva-C LaunchPad with the sensors is as follows. From this diagram, we learn how the sensors are interconnected with LaunchPad:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="images/7f7029c2-a70a-4eee-bc8d-7562c0e74bc9.png" style="width:54.25em;height:36.67em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Sensor-interfacing diagram of ChefBot</div>
<p>M1 and M2 are two differential-drive motors that we are using in this robot. The kind of motor we are going to use here is a DC geared motor with an encoder from Pololu. The motor terminals are connected to the dual <strong>VNH2SP30</strong> motor driver from Pololu. One of the motors is connected with reverse polarity because in differential steering, one motor rotates in the opposite direction to the other. If we send the same control signal to both the motors, each motor will rotate in the opposite direction. To avoid this condition, we will swap the cables of one motor. The motor driver is connected to Tiva-C LaunchPad through a 3.3 V-5 V bidirectional level shifter. One of the level shifters we will use here is available at <a href="https://www.sparkfun.com/products/12009"><span class="URLPACKT">https://www.sparkfun.com/products/12009</span></a>.</p>
<p>The two channels of each encoder are connected to LaunchPad using a level shifter. At the moment, we are using one ultrasonic distance sensor for obstacle detection. In future, we could increase the number of sensors if required. To get a good odometry estimate, we will put the IMU sensor MPU 6050 through an I2C interface. The pins are directly connected to LaunchPad because MPU6050 is 3.3 V compatible. To reset LaunchPad from the ROS nodes, we are allocating one pin as the output and connecting it to the reset pin of LaunchPad. When a specific character is sent to LaunchPad, it will set the output pin to high and reset the device. In some situations, the error from the calculation may accumulate and affect the navigation of the robot. We are resetting LaunchPad to clear this error. To monitor the battery level, we are allocating another pin to read the battery value. This feature is not currently implemented in the Energia code.</p>
<p>The code you downloaded from GitHub consists of the embedded code and the dependent libraries needed to compile this code. We can see the main section of the code here, and there is no need to explain all of the sections because we have already looked at them.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Embedded code for ChefBot</h1>
                </header>
            
            <article>
                
<p>The main sections of the LaunchPad code are discussed in this section. The following are the header files used in the code:</p>
<pre>//Library to communicate with I2C devices 
#include "Wire.h" 
//I2C communication library for MPU6050 
#include "I2Cdev.h" 
//MPU6050 interfacing library 
#include "MPU6050_6Axis_MotionApps20.h" 
//Processing incoming serial data 
#include &lt;Messenger.h&gt; 
//Contain definition of maximum limits of various data type 
#include &lt;limits.h&gt;</pre>
<p>The main libraries used in this code are for the purposes of communicating with MPU 6050 and processing the incoming serial data to LaunchPad. MPU 6050 can provide the orientation in quaternion or Euler values using the inbuilt <strong>digital motion processor</strong> (<strong>DMP</strong>). The functions to access DMP are written in <kbd>MPU6050_6Axis_MotionApps20.h</kbd>. This library has dependencies such as <kbd>I2Cdev.h</kbd> and <kbd>Wire.h</kbd>; that's why we are including this header as well. These two libraries are used for I2C communication. The <kbd>Messenger.h</kbd> library allows you to handle a stream of text data from any source and will help you to extract the data from it. The <kbd>limits.h</kbd> header contains the definitions of the maximum limits of various data types.</p>
<p>After we include the header files, we need to create an object to handle MPU6050 and process the incoming serial data using the <kbd>Messenger</kbd> class:</p>
<pre>//Creating MPU6050 Object 
MPU6050 accelgyro(0x68); 
//Messenger object 
Messenger Messenger_Handler = Messenger(); </pre>
<p>After declaring the messenger object, the main section deals with assigning pins for the motor driver, encoder, ultrasonic sensor, MPU 6050, reset, and battery pins. Once we have assigned the pins, we can look at the <kbd>setup()</kbd> function of the code. The definition of the <kbd>setup()</kbd> function is given in the following code:</p>
<pre>//Setup serial, encoders, ultrasonic, MPU6050 and Reset functions 
void setup() 
{ 
  //Init Serial port with 115200 baud rate 
  Serial.begin(115200); 
  //Setup Encoders 
  SetupEncoders(); 
  //Setup Motors 
  SetupMotors(); 
  //Setup Ultrasonic 
  SetupUltrasonic(); 
  //Setup MPU 6050 
  Setup_MPU6050(); 
  //Setup Reset pins 
  SetupReset(); 
  //Set up Messenger object handler 
  Messenger_Handler.attach(OnMssageCompleted); 
}</pre>
<p>The preceding function contains a custom routine to configure and allocate pins for all of the sensors. This function will initialize serial communication with a 115,200 baud rate and set pins for the encoder, motor driver, ultrasonic sensors, and MPU6050. The <kbd>SetupReset()</kbd> function will assign a pin to reset the device, as shown in the preceding connection diagram. We have already seen the setup routines of each of the sensors in the previous chapters, so there is no need to explain the definition of each of these functions. The <kbd>Messenger</kbd> class handler is attached to a function called <kbd>OnMssageCompleted()</kbd>, which will be called when data is input to the <kbd>Messenger_Handler</kbd>.</p>
<p>The following is the main <kbd>loop()</kbd> function of the code. The main purpose of this function is to read and process serial data, as well as send available sensor values:</p>
<pre>void loop() 
{ 
    //Read from Serial port 
    Read_From_Serial(); 
    //Send time information through serial port 
    Update_Time(); 
    //Send encoders values through serial port 
    Update_Encoders(); 
    //Send ultrasonic values through serial port 
    Update_Ultra_Sonic(); 
    //Update motor speed values with corresponding speed received from PC and send speed values through serial port 
    Update_Motors(); 
    //Send MPU 6050 values through serial port 
    Update_MPU6050(); 
    //Send battery values through serial port 
    Update_Battery(); 
} </pre>
<p>The <kbd>Read_From_Serial()</kbd> function will read serial data from the PC and feed data to the <kbd>Messenger_Handler</kbd> handler for processing purposes. The <kbd>Update_Time()</kbd> function will update the time after each operation in the embedded board. We can take this time value to be processed in the PC or use the PC's time instead.</p>
<p>We can compile the code in Energia's IDE and burn the code in LaunchPad. After uploading the code, we can look at the ROS nodes for handling the LaunchPad sensor values.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Writing a ROS Python driver for ChefBot</h1>
                </header>
            
            <article>
                
<p>After uploading the embedded code to LaunchPad, the next step is to handle the serial data from LaunchPad and convert it to ROS topics for further processing. The <kbd>launchpad_node.py</kbd> ROS Python driver node interfaces Tiva-C LaunchPad with ROS. The <kbd>launchpad_node.py</kbd> file is in the <kbd>script</kbd> folder, which is inside the <kbd>ChefBot_bringup</kbd> package. The following is an explanation of the important code sections of <kbd>launchpad_node.py</kbd>:</p>
<pre>#ROS Python client 
import rospy 
import sys 
import time 
import math 
 
#This python module helps to receive values from serial port which execute in a thread 
from SerialDataGateway import SerialDataGateway 
#Importing required ROS data types for the code 
from std_msgs.msg import Int16,Int32, Int64, Float32, <br/> String, Header, UInt64 
#Importing ROS data type for IMU 
from sensor_msgs.msg import Imu </pre>
<p>The <kbd>launchpad_node.py</kbd> file imports the preceding modules. The main module we can see is <kbd>SerialDataGateway</kbd>. This is a custom module written to receive serial data from the LaunchPad board in a thread. We also need some data types of ROS to handle the sensor data. The main function of the node is given in the following code snippet:</p>
<pre>if __name__ =='__main__': 
  rospy.init_node('launchpad_ros',anonymous=True) 
  launchpad = Launchpad_Class() 
  try: 
 
    launchpad.Start() 
    rospy.spin() 
  except rospy.ROSInterruptException: 
    rospy.logwarn("Error in main function") 
 
  launchpad.Reset_Launchpad() 
  launchpad.Stop()</pre>
<p>The main class of this node is called <kbd>Launchpad_Class()</kbd>. This class contains all the methods to start, stop, and convert serial data to ROS topics. In the main function, we will create an object of the <kbd>Launchpad_Class()</kbd>. After creating the object, we will call the <kbd>Start()</kbd> method, which will start the serial communication between Tiva-C LaunchPad and the PC. If we interrupt the driver node by typing <em><span class="KeyPACKT">Ctrl</span></em> + <em><span class="KeyPACKT">C</span></em>, it will reset LaunchPad and stop the serial communication between the PC and LaunchPad.</p>
<p>The following code snippet is from the constructor function of <kbd>Launchpad_Class()</kbd>. In the following snippet, we will retrieve the port and baud rate of the LaunchPad board from the ROS parameters and initialize the <kbd>SerialDateGateway</kbd> object using these parameters. The <kbd>SerialDataGateway</kbd> object calls the <kbd>_HandleReceivedLine()</kbd> function inside this class when any incoming serial data arrives at the serial port. This function will process each line of serial data and extract, convert, and insert it in the appropriate headers of each ROS topic data type:</p>
<pre>#Get serial port and baud rate of Tiva C Launchpad 
port = rospy.get_param("~port", "/dev/ttyACM0") 
baudRate = int(rospy.get_param("~baudRate", 115200)) 
 
################################################################# 
rospy.loginfo("Starting with serial port: <br/> " + port + ", baud rate: " + str(baudRate))#Initializing SerialDataGateway object with serial port, baud<br/>  rate and callback function to handle incoming serial dataself._SerialDataGateway = SerialDataGateway(port, <br/> baudRate, self._HandleReceivedLine) 
rospy.loginfo("Started serial communication") 
 
 
###################################################################Subscribers and Publishers 
 
#Publisher for left and right wheel encoder values 
self._Left_Encoder = rospy.Publisher('lwheel',Int64,queue_size <br/> = 10)self._Right_Encoder = rospy.Publisher('rwheel',Int64,queue_size <br/> = 10)</pre>
<pre>#Publisher for Battery level(for upgrade purpose) 
self._Battery_Level = <br/> rospy.Publisher('battery_level',Float32,queue_size = 10) 
#Publisher for Ultrasonic distance sensor 
self._Ultrasonic_Value = <br/> rospy.Publisher('ultrasonic_distance',Float32,queue_size = 10) 
 
#Publisher for IMU rotation quaternion values 
self._qx_ = rospy.Publisher('qx',Float32,queue_size = 10) 
self._qy_ = rospy.Publisher('qy',Float32,queue_size = 10) 
self._qz_ = rospy.Publisher('qz',Float32,queue_size = 10) 
self._qw_ = rospy.Publisher('qw',Float32,queue_size = 10) 
 
#Publisher for entire serial data 
self._SerialPublisher = rospy.Publisher('serial', <br/> String,queue_size=10)</pre>
<p>We will create the ROS publisher object for sensors such as the encoder, IMU, and ultrasonic sensor, as well as for the entirety of the serial data for debugging purposes. We will also subscribe the speed commands to the left-hand side and right-hand side wheel of the robot. When a speed command arrives on the topic, it calls the respective callbacks to send speed commands to the robot's LaunchPad:</p>
<pre>self._left_motor_speed = rospy.Subscriber('left_wheel_speed',Float32,self._Update_Left_Speed) 
self._right_motor_speed = rospy.Subscriber('right_wheel_speed',Float32,self._Update_Right_Speed) </pre>
<p>After setting the ChefBot driver node, we need to interface the robot with a ROS navigation stack in order to perform autonomous navigation. The basic requirement for doing autonomous navigation is that the robot driver nodes receive velocity commands from the ROS navigational stack. The robot can be controlled using teleoperation. In addition to these features, the robot must be able to compute its positional or odometry data and generate the tf data to be sent into the navigational stack. There must be a PID controller to control the robot's motor velocity. The following ROS package helps us to perform these functions. The <kbd>differential_drive</kbd> package contains nodes to perform the preceding operation. We are reusing these nodes in our package to implement these functionalities. You can find the <kbd>differential_drive</kbd> package in ROS at <a href="http://wiki.ros.org/differential_drive"><span class="URLPACKT">http://wiki.ros.org/differential_drive</span></a>.</p>
<p>The following diagram shows how these nodes communicate with each other:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="images/95cb50af-83f4-41cc-9742-0a43880b8852.png" style="width:55.42em;height:27.50em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Block diagram of the robot showing the ROS nodes</div>
<p>The purpose of each node in the <kbd>ChefBot_bringup</kbd> package is as follows:</p>
<p><kbd>twist_to_motors.py</kbd>: This node will convert a ROS <kbd>Twist</kbd> command or linear and angular velocity to an individual motor velocity target. The target velocities are published at a rate of the <kbd>~rate</kbd> (measured in Hertz) and the publish <kbd>timeout_ticks</kbd> time's velocity after the <kbd>Twist</kbd> message stops. The following are the topics and parameters that will be published and subscribed to by this node:</p>
<p><strong>Publishing topics:</strong></p>
<p><kbd>lwheel_vtarget(std_msgs/Float32)</kbd>: This is the target velocity of the left wheel (measured in m/s).</p>
<p><kbd>rwheel_vtarget</kbd> (<kbd>std_msgs</kbd>/<kbd>Float32</kbd>): This is the target velocity of the right wheel (measured in m/s).</p>
<p><strong>Subscribing topics:</strong></p>
<p><kbd>Twist</kbd> (<kbd>geometry_msgs</kbd>/<kbd>Twist</kbd>): This is the target <kbd>Twist</kbd> command for the robot. The linear velocity in the x-direction and the angular velocity theta of the Twist messages are used in this robot.</p>
<p><strong>Important ROS parameters:</strong></p>
<p><kbd>~base_width</kbd> (<kbd>float, default: 0.1</kbd>): This is the distance between the robot's two wheels in meters.</p>
<p><kbd>~rate</kbd> (<kbd>int, default: 50</kbd>): This is the rate at which the velocity target is published (Hertz).</p>
<p><kbd>~timeout_ticks</kbd> (<kbd>int, default:2</kbd>): This is the number of the velocity target message published after stopping the Twist messages.</p>
<p><kbd>pid_velocity.py</kbd>: This is a simple PID controller to control the speed of each motor by taking feedback from the wheel encoders. In a differential drive system, we need one PID controller for each wheel. It will read the encoder data from each wheel and control the speed of each wheel.</p>
<p><strong>Publishing topics:</strong></p>
<p><kbd>motor_cmd</kbd> (<kbd>Float32</kbd>): This is the final output of the PID controller that goes to the motor. We can change the range of the PID output using the <kbd>out_min</kbd> and <kbd>out_max</kbd> ROS parameter.</p>
<p><kbd>wheel_vel</kbd> (<kbd>Float32</kbd>): This is the current velocity of the robot wheel in m/s.</p>
<p><strong>Subscribing topics:</strong></p>
<p><kbd>wheel</kbd> (<kbd>Int16</kbd>): This topic is the output of a rotary encoder. There are individual topics for each encoder of the robot.</p>
<p><kbd>wheel_vtarget</kbd> (<kbd>Float32</kbd>): This is the target velocity in m/s.</p>
<p><strong>Important parameters:</strong></p>
<p><kbd>~Kp</kbd> (<kbd>float</kbd><kbd>,default: 10</kbd>): This parameter is the proportional gain of the PID controller.</p>
<p><kbd>~Ki</kbd> (<kbd>float, default: 10</kbd>): This parameter is the integral gain of the PID controller.</p>
<p><kbd>~Kd</kbd> (<kbd>float, default: 0.001</kbd>): This parameter is the derivative gain of the PID controller.</p>
<p><kbd>~out_min</kbd> (<kbd>float, default: 255</kbd>): This is the minimum limit of the velocity value to the motor. This parameter limits the velocity's value to the motor called the <kbd>wheel_vel</kbd> topic.</p>
<p><kbd>~out_max</kbd> (<kbd>float, default: 255</kbd>): This is the maximum limit of the <kbd>wheel_vel</kbd> topic (measured in Hertz).</p>
<p><kbd>~rate</kbd> (<kbd>float, default: 20</kbd>): This is the rate of publishing the <kbd>wheel_vel</kbd> topic.</p>
<p><kbd>ticks_meter</kbd> (<kbd>float, default: 20</kbd>): This is the number of wheel encoder ticks per meter. This is a global parameter because it's used in other nodes too.</p>
<p><kbd>vel_threshold</kbd> (<kbd>float, default: 0.001</kbd>): If the robot velocity drops below this parameter, we consider the wheel as stationary. If the velocity of the wheel is less than <kbd>vel_threshold</kbd>, we consider it as zero.</p>
<p><kbd>encoder_min</kbd> (<kbd>int, default: 32768</kbd>): This is the minimum value of encoder reading.</p>
<p><kbd>encoder_max</kbd> (<kbd>int, default: 32768</kbd>): This is the maximum value of encoder reading.</p>
<p><kbd>wheel_low_wrap</kbd> (<kbd>int, default: 0.3 * (encoder_max - encoder_min) + encoder_min</kbd>): These values decide whether the odometry is in a negative or positive direction.</p>
<p><kbd>wheel_high_wrap</kbd> (<kbd>int, default: 0.7 * (encoder_max - encoder_min) + encoder_min</kbd>): These values decide whether the odometry is in a negative or positive direction.</p>
<p><kbd>diff_tf.py</kbd>: This node computes the transformation of odometry and broadcasts between the odometry frame and the robot's base frame.</p>
<p><strong>Publishing topics:</strong></p>
<p><kbd>odom</kbd> (<kbd>nav_msgs</kbd>/<kbd>odometry</kbd>): This publishes the odometry (the current pose and twist of the robot).</p>
<p><kbd>tf</kbd>: This provides the transformation between the odometry frame and the robot base link.</p>
<p><strong>Subscribing topics:</strong></p>
<p><kbd>lwheel</kbd> (<kbd>std_msgs</kbd>/<kbd>Int16</kbd>), <kbd>rwheel</kbd> (<kbd>std_msgs</kbd>/<kbd>Int16</kbd>): These are the output values from the left and right encoders of the robot.</p>
<ul>
<li><kbd>ChefBot_keyboard_teleop.py</kbd>: This node sends the <kbd>Twist</kbd> command using controls from the keyboard.</li>
</ul>
<p><strong>Publishing topics:</strong></p>
<p><kbd>cmd_vel_mux</kbd>/<kbd>input</kbd>/<kbd>teleop</kbd> (<kbd>geometry_msgs</kbd>/<kbd>Twist</kbd>): This publishes the Twist messages using keyboard commands.</p>
<p>Now that we have looked at the nodes in the <kbd>ChefBot_bringup</kbd> <span>package, we will look at the functions of the launch files.</span></p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Understanding ChefBot ROS launch files</h1>
                </header>
            
            <article>
                
<p style="color: black">We will now look at the functions of each of the launch files of the <kbd>ChefBot_bringup</kbd> package:</p>
<ul>
<li><kbd>robot_standalone.launch</kbd>: The main function of this launch file is to start nodes such as <kbd>launchpad_node</kbd>, <kbd>pid_velocity</kbd>, <kbd>diff_tf,</kbd> and <kbd>twist_to_motor</kbd> to get sensor values from the robot and to send the command velocity to the robot.</li>
<li><kbd>keyboard_teleop.launch</kbd>: This launch file will start teleoperation using the keyboard. It starts the <kbd>ChefBot_keyboard_teleop.py</kbd> node to perform the keyboard teleoperation.</li>
<li><kbd>3dsensor.launch</kbd> : This file will launch the Kinect OpenNI drivers and start publishing the RGB and depth stream. It will also start the depth-to-laser scanner node, which will convert point cloud data to laser scan data.</li>
<li><kbd>gmapping_demo.launch</kbd>: This launch file will start the SLAM gmapping nodes to map the area surrounding the robot.</li>
<li><kbd>amcl_demo.launch</kbd>: Using AMCL, the robot can localize and predict where it stands on the map. After localizing the robot on the map, we can command the robot to move to a position on the map. Then the robot can move autonomously from its current position to the goal position.</li>
<li><kbd>view_robot.launch</kbd>: This launch file displays the robot URDF model in RViz.</li>
<li><kbd>view_navigation.launch</kbd>: This launch file displays all the sensors necessary for the navigation of the robot.</li>
</ul>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Working with ChefBot Python nodes and launch files</h1>
                </header>
            
            <article>
                
<p>We have already set ChefBot ROS packages in Intel's NUC PC and uploaded the embedded code to the LaunchPad board. The next step is to put the NUC PC on the robot, configure the remote connection from the laptop to the robot, test each node, and work with ChefBot's launch files to perform autonomous navigation.</p>
<p>The main device we should have before working with ChefBot is a good wireless router. The robot and the remote laptop have to connect across the same network. If the robot PC and remote laptop are on the same network, the user can connect from the remote laptop to the robot PC through SSH using its IP. Before putting the robot PC in the robot, we should connect the robot PC to the wireless network so that once it's connected to the wireless network, it will remember the connection details. When the robot powers up, the PC should automatically connect to the wireless network. Once the robot PC is connected to a wireless network, we can put it in the actual robot. The following diagram shows the connection diagram of the robot and remote PC:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="images/a41b1ab5-d147-4580-bd52-200c97833f09.png" style="width:35.08em;height:10.75em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Wireless connection diagram of the robot and remote PC</div>
<p>The preceding diagram assumes that the ChefBot's IP is <kbd>192.168.1.106</kbd> and the remote PC's IP is <kbd>192.168.1.101</kbd>.</p>
<p>We can remotely access the ChefBot terminal using SSH. We can use the following command to log in to ChefBot, where <kbd>robot</kbd> is the username of the ChefBot PC:</p>
<pre>    <strong>$ ssh robot@192.168.1.106</strong>  </pre>
<p>When you log in to the ChefBot PC, it will ask for the robot PC password. After entering the password of the robot PC, we can access the robot PC terminal. After logging in to the robot PC, we can start testing ChefBot's ROS nodes and test whether we receive the serial values from the LaunchPad board inside ChefBot. Note that you should log in to the ChefBot PC again through SSH if you are using a new terminal.</p>
<p>If the <kbd>ChefBot_bringup</kbd> package is properly installed on the PC, and if the LaunchPad board is connected, then before running the ROS driver node, we can run the <kbd>miniterm.py</kbd> tool to check whether the serial values come to the PC properly via USB. We can find the serial device name using the <kbd>dmesg</kbd> command. We can run <kbd>miniterm.py</kbd> using the following command:</p>
<pre>    <strong>$ miniterm.py /dev/ttyACM0 115200</strong>  </pre>
<p>If it shows the permission denied message, set the permission of the USB device by writing rules on the <kbd>udev</kbd> folder, which we did in <a href="lrn-rbt-py-2e_ch06.html"><span class="ChapterrefPACKT">Chapter 6</span></a>, <em>Interfacing Actuators and Sensors to the Robot Controller</em>, or we can temporarily change the permission using the following command. Here, we are assuming that <kbd>ttyACM0</kbd> is the device name of LaunchPad. If the device name is different in your PC, then you have to use that name instead of <kbd>ttyACM0</kbd>:</p>
<pre>    <strong>$ sudo chmod 777 /dev/ttyACM0</strong>  </pre>
<p>If everything works fine, we will get values such as those shown in the following screenshot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="images/d3892aa3-1b0d-434f-b562-a548b050cc15.png" style="width:22.75em;height:23.92em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Output of miniterm.py</div>
<p>The letter <kbd>b</kbd> is used to indicate the battery reading of the robot; currently, it's not implemented. The value is set to zero now. These values are coming from the Tiva C Launchpad. There are different approaches to sense the voltage using a microcontroller board. One of the approaches is given below&#160;<span>(<a href="http://www.instructables.com/id/Arduino-Battery-Voltage-Indicator/">http://www.instructables.com/id/Arduino-Battery-Voltage-Indicator/</a>).&#160;</span>The letter <kbd>t</kbd> indicates the total time elapsed (in microseconds) after the robot starts running the embedded code. The second value is the time taken to complete one entire operation in LaunchPad (measured in seconds). We can use this value if we are performing real-time calculations of the parameters of the robot. At the moment, we are not using this value, but we may use it in the future. The letter <kbd>e</kbd> indicates the values of the left and right encoder respectively. Both the values are zero here because the robot is not moving. The letter <kbd>u</kbd> indicates the values from the ultrasonic distance sensor. The distance value we get is in centimeters. The letter <kbd>s</kbd> indicates the current wheel speed of the robot. This value is used for inspection purposes. Actually, speed is a control output from the PC itself.</p>
<p>To convert this serial data to ROS topics, we have to run the drive node called <kbd>launchpad_node.py</kbd>. The following code shows how to execute this node.</p>
<p>First, we have to run <kbd>roscore</kbd> before starting any nodes:</p>
<pre>    <strong>$ roscore</strong>  </pre>
<p>Run <kbd>launchpad_node.py</kbd> using the following command:</p>
<pre>    <strong>$ rosrun ChefBot_bringup launchpad_node.py</strong>  </pre>
<p>If everything works fine, we will get the following output in node in the running terminal:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="images/33b4d922-bdcb-40e3-825b-47f0a5ac855b.png" style="width:38.42em;height:4.75em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Output of launchpad_node.py</div>
<p>After running <kbd>launchpad_node.py</kbd>, we will see the following topics generated, as shown in the following screenshot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="images/c6bca7d0-4626-4670-8832-22137656699d.png" style="width:20.58em;height:14.33em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Topics generated by launchpad_node.py</div>
<p>We can view the serial data received by the driver node by subscribing to the <kbd>/serial</kbd> topic. We can use it for debugging purposes. If the serial topic shows the same data that we saw in <kbd>miniterm.py</kbd>, then we can confirm that the nodes are working fine. The following screenshot is the output of the <kbd>/serial</kbd> topic:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="images/75a326ff-9de1-4dc0-9de6-91e89366dd20.png" style="width:30.08em;height:8.00em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Output of the /serial topic published by the LaunchPad node</div>
<p>After setting the <kbd>ChefBot_bringup</kbd> package, we can start working with the autonomous navigation of ChefBot. Currently, we are accessing only the ChefBot PC's terminal. To visualize the robot's model, sensor data, maps, and so on, we have to use RViz in the user's PC. We have to do some configuration in the robot and user PC to perform this operation. It should be noted that the user's PC should have the same software setup as the ChefBot PC.</p>
<p>The first thing we have to do is to set the ChefBot PC as a ROS master. We can set the ChefBot PC as the ROS master by setting the <kbd>ROS_MASTER_URI</kbd> value. The <kbd>ROS_MASTER_URI</kbd> setting is a required setting; it informs the nodes about the <strong>uniform resource identifier</strong> (<strong>URI</strong>) of the ROS master. When you set the same <kbd>ROS_MASTER_URI</kbd> for the ChefBot PC and the remote PC, we can access the topics of the ChefBot PC in the remote PC. So, if we run RViz locally, then it will visualize the topics generated in the ChefBot PC.</p>
<p>Assume that the ChefBot PC IP is <kbd>192.168.1.106</kbd> and the remote PC IP is <kbd>192.168.1.10</kbd>. You can set a static IP for Chefbot PC and remote PC so that the IP will always be the same all test otherwise if it is automatic, you may get different IP in each test. To set <kbd>ROS_MASTER_URI</kbd> in each system, the following command should be included in the <kbd>.bashrc</kbd> file in the <kbd>home</kbd> folder. The following diagram shows the setup needed to include the <kbd>.bashrc</kbd> file in each system:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="images/e82976a6-208d-4f60-9012-869eb6ee0428.png" style="width:46.50em;height:22.17em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Network configuration for ChefBot</div>
<p>Add these lines at the bottom of <kbd>.bashrc</kbd> on each PC and change the IP address according to your network.</p>
<p>After we establish these settings, we can just start <kbd>roscore</kbd> on the ChefBot PC terminal and execute the <kbd>rostopic list</kbd> command on the remote PC.</p>
<p>If you see any topics, you are done with the settings. We can first run the robot using the keyboard teleoperation to check the robot's functioning and confirm whether we get the sensor values.</p>
<p>We can start the robot driver and other nodes using the following command. Note that this should execute in the ChefBot terminal after logging in using SSH:</p>
<pre>    <strong>$ roslaunch ChefBot_bringup robot_standalone.launch</strong>  </pre>
<p>After launching the robot driver and nodes, start the keyboard teleoperation using the following command. This also has to be done on the new terminal of the ChefBot PC:</p>
<pre>    <strong>$ roslaunch ChefBot_bringup keyboard_teleop.launch</strong>  </pre>
<p>To activate Kinect, execute the following command. This command is also executed on the ChefBot terminal:</p>
<pre>    <strong>$roslaunch ChefBot_bringup 3dsensor_kinect.launch</strong>  </pre>
<p>If you are using Orbecc Astra, use the following launch file to start the sensor:</p>
<pre>    <strong>$ roslaunch ChefBot_bringup 3d_sensor_astra.launch</strong>  </pre>
<p>To view the sensor data, we can execute the following command. This will view the robot model in RViz and should be executed in the remote PC. If we set up the <kbd>ChefBot_bringup</kbd> package in the remote PC, we can access the following command and visualize the robot model and sensor data from the ChefBot PC:</p>
<pre>    <strong>$ roslaunch ChefBot_bringup view_robot.launch</strong></pre>
<p>The following screenshot is the output of RViz. We can see the <span class="packt_screen">LaserScan</span> and <span class="packt_screen">PointCloud</span> mapped data in the screenshots:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="images/4f16c9b4-6fc2-488d-bcc2-247eb87466cc.png"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">ChefBot LaserScan data in RViz</div>
<p>The preceding screenshot shows <span class="packt_screen">LaserScan</span> in RViz. We need to tick the <span class="packt_screen">LaserScan</span> topic from the left-hand side section of RViz to show the laser scan data. The laser scan data is marked on the viewport. If you want to watch the point cloud data from Kinect/Astra, click on the <span class="packt_screen">Add</span> button on the left-hand side of RViz and select <span class="packt_screen">PointCloud2</span> from the pop-up window. Select <span class="packt_screen">Topic |</span><kbd>/camera/depth_registered</kbd> from the list and you will see an image similar to the one shown in the following screenshot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="images/1e4d4e54-42a6-420c-ab92-84d50d2a5030.png"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">ChefBot with PointCloud data</div>
<p>After working with sensors, we can perform SLAM to map the room. The following procedure helps us to start SLAM on this robot.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Working with SLAM on ROS to build a map of the room</h1>
                </header>
            
            <article>
                
<p>To perform gmapping, we have to execute the following commands.</p>
<p>The following command starts the robot driver in the ChefBot terminal:</p>
<pre>    <strong>$ roslaunch ChefBot_bringup robot_standalone.launch</strong>  </pre>
<p>The following command starts the gmapping process. Note that it should be executed on the ChefBot terminal:</p>
<pre>    <strong>$ roslaunch ChefBot_bringup gmapping_demo.launch</strong>  </pre>
<p>Gmapping will only work if the odometry value that is received is proper. If the odometry value is received from the robot, we will receive the following message for the preceding command. If we get this message, we can confirm that gmapping will work fine:</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/8d3735e1-9c32-4413-925d-3030d845ab55.png" style="width:45.42em;height:4.25em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">ChefBot with PointCloud data</div>
<p>To start the keyboard teleoperation, use the following command:</p>
<pre>    <strong>$ roslaunch ChefBot_bringup keyboard_teleop.launch</strong>  </pre>
<p>To view the map that is being created, we need to start RViz on the remote system using the following command:</p>
<pre>    <strong>$ roslaunch ChefBot_bringup view_navigation.launch</strong>  </pre>
<p>After viewing the robot in RViz, you can move the robot using the keyboard and see the map being created. When it has mapped the entire area, we can save the map using the following command on the ChefBot PC terminal:</p>
<pre>    <strong>$rosrun map_server map_saver -f ~/test_map</strong></pre>
<p>In the preceding code, <kbd>test_map</kbd> is the name of the map being stored in the <kbd>home</kbd> folder. The following screenshot shows the map of a room created by the robot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="images/07e93661-11a3-45c2-a87a-c03139613dfe.png" style="width:25.75em;height:28.92em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Mapping a room</div>
<p>After the map is stored, we can work with the localization and autonomous navigation functionalities using ROS.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Working with ROS localization and navigation</h1>
                </header>
            
            <article>
                
<p>After building the map, close all the applications and rerun the robot driver using the following command:</p>
<pre>    <strong>$ roslaunch ChefBot_bringup robot_standalone.launch</strong></pre>
<p>Start the localization and navigation on the stored map using the following command:</p>
<pre>    <strong>$ roslaunch ChefBot_bringup amcl_demo.launch map_file:=~/test_map.yaml</strong>  </pre>
<p>Start viewing the robot using the following command in the remote PC:</p>
<pre>    <strong>$ roslaunch ChefBot_bringup view_navigation.launch</strong>  </pre>
<p>In RViz, we may need to specify the initial pose of the robot using the <span class="packt_screen">2D Pose Estimate</span> button. We can change the robot pose on the map using this button. If the robot is able to access the map, then we can use the <span class="packt_screen">2D Nav Goal</span> button to command the robot to move to the desired position. When we start the localization, we can see the particle cloud around the robot by using the AMCL algorithm:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="images/728e0ecb-ea30-44f4-80f6-ccec521b372a.png" style="width:24.33em;height:23.58em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Localizing the robot using AMCL</div>
<p>The following is a screenshot of the robot as it navigates autonomously from its current position to the goal position. The goal position is marked as a black dot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="images/cdec1d3e-d4c5-419c-bde4-81730c3156dc.png" style="width:21.00em;height:19.17em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Autonomous navigation using a map</div>
<p>The black line from the robot to the black dot is the robot's planned path to reach the goal position. If the robot is not able to locate the map, we might need to fine-tune the parameter files in the <kbd>ChefBot_bringup</kbd><kbd>param</kbd> folder. For more fine-tuning details, you can go through the AMCL package on ROS at <a href="http://wiki.ros.org/amcl"><span class="URLPACKT">http://wiki.ros.org/amcl</span></a>.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>This chapter was about assembling the hardware of ChefBot and integrating the embedded and ROS code into the robot to perform autonomous navigation. We saw the robot hardware parts that were manufactured using the design from <a href="lrn-rbt-py-2e_ch06.html"><span class="ChapterrefPACKT">Chapter 6</span></a>, <em>Interfacing Actuators and Sensors to the Robot Controller</em>. We assembled the individual sections of the robot and connected the prototype PCB we designed for the robot. This consisted of the LaunchPad board, motor driver, left shifter, ultrasonic sensor, and IMU. The LaunchPad board was flashed with the new embedded code, which can interface with all sensors in the robot and can send or receive data from the PC.</p>
<p>After looking at the embedded code, we configured the ROS Python driver node to interface with the serial data from the LaunchPad board. After interfacing with the LaunchPad board, we computed the odometry data and differential drive control using nodes from the <kbd>differential_drive</kbd> package that was in the ROS repository. We interfaced the robot with the ROS navigation stack. This enabled us to use SLAM and AMCL for autonomous navigation. We also looked at SLAM and AMCL, created a map, and commanded the robot to navigate autonomously.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What is the use of the robot ROS driver node?</li>
<li>What is the role of the PID controller in navigation?</li>
<li>How do you convert encoder data to odometry data?</li>
<li>What is the role of SLAM in robot navigation?</li>
<li>What is the role of AMCL in robot navigation?</li>
</ol>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p>You can read more about the robotic vision package in ROS from the following links:</p>
<ul>
<li><a href="http://wiki.ros.org/gmapping">http://wiki.ros.org/gmapping</a></li>
<li><a href="http://wiki.ros.org/amcl">http://wiki.ros.org/amcl</a></li>
</ul>


            </article>

            
        </section>
    </div>
</body>
</html>