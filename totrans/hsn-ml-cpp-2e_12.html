<html><head></head><body>
		<div id="_idContainer942">
			<h1 class="chapter-number" id="_idParaDest-248"><a id="_idTextAnchor660"/>12</h1>
			<h1 id="_idParaDest-249"><a id="_idTextAnchor661"/>Exporting and Importing Models</h1>
			<p>In this chapter, we’ll discuss how to save and load model parameters during and after training. This is important because model training can take days or even weeks. Saving intermediate results allows us to load them later for evaluation or <span class="No-Break">production use.</span></p>
			<p>Such regular save operations can be beneficial in the case of a random application crash. Another substantial <a id="_idIndexMarker1415"/>feature of any <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) framework is its ability to export the model architecture, which allows us to share models between frameworks and makes model deployment easier. The main topic of this chapter is to show how to export and import model parameters such as weights and bias values <a id="_idIndexMarker1416"/>with different C++ libraries. The second part of this chapter is all about the <strong class="bold">Open Neural Network Exchange</strong> (<strong class="bold">ONNX</strong>) format, which is currently gaining popularity among different ML frameworks and can be used to share trained models. This format is suitable for sharing model architectures as well as <span class="No-Break">model parameters.</span></p>
			<p>The following topics will be covered in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>ML model serialization APIs in <span class="No-Break">C++ libraries</span></li>
				<li>Delving into the <span class="No-Break">ONNX form<a id="_idTextAnchor662"/>a<a id="_idTextAnchor663"/>t</span></li>
			</ul>
			<h1 id="_idParaDest-250"><a id="_idTextAnchor664"/>Technical requirements</h1>
			<p>The following are the technical requirements for <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>The <span class="No-Break"><strong class="source-inline">Dlib</strong></span><span class="No-Break"> library</span></li>
				<li>The <span class="No-Break"><strong class="source-inline">mlpack</strong></span><span class="No-Break"> library</span></li>
				<li>The <span class="No-Break"><strong class="source-inline">F</strong></span><span class="No-Break"><strong class="source-inline">lashlight</strong></span><span class="No-Break"> library</span></li>
				<li>The <span class="No-Break"><strong class="source-inline">pytorch</strong></span><span class="No-Break"> library</span></li>
				<li>The <span class="No-Break"><strong class="source-inline">onnxruntime</strong></span><span class="No-Break"> framework</span></li>
				<li>A modern C++ compiler with <span class="No-Break">C++20 support</span></li>
				<li>CMake build system version &gt;= <span class="No-Break">3.8</span></li>
			</ul>
			<p>The code files for this chapter can be found in this book’s GitHub <span class="No-Break">repository: </span><a href="https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter12"><span class="No-Break">https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter12</span></a><span class="No-Break"><span class="P---URL"><a id="_idTextAnchor665"/></span></span><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-251"><a id="_idTextAnchor666"/>ML model serialization APIs in C++ libraries</h1>
			<p>In this <a id="_idIndexMarker1417"/>section, we’ll discuss the ML model sharing <a id="_idIndexMarker1418"/>APIs that are available in the <strong class="source-inline">Dlib</strong>, <strong class="source-inline">F</strong><strong class="source-inline">lashlight</strong>, <strong class="source-inline">mlpack</strong>, and <strong class="source-inline">pytorch</strong> libraries. There are three main types of sharing <a id="_idIndexMarker1419"/>ML models among the different <span class="No-Break">C++ libraries:</span></p>
			<ul>
				<li>Share model <span class="No-Break">parameters (weights)</span></li>
				<li>Share the entire <span class="No-Break">model’s architecture</span></li>
				<li>Share both the model architecture and its <span class="No-Break">trained parameters</span></li>
			</ul>
			<p>In the following sections, we’ll look at what API is available in each library and emphasize what type of sharing <span class="No-Break">it suppo<a id="_idTextAnchor667"/>rts.</span></p>
			<h2 id="_idParaDest-252"><a id="_idTextAnchor668"/>Model serialization with Dlib</h2>
			<p>The <strong class="source-inline">Dlib</strong> library uses <a id="_idIndexMarker1420"/>the serialization API for <strong class="source-inline">decision_function</strong> and neural network objects. Let’s learn how to use it by implementing a <span class="No-Break">real example.</span></p>
			<p>First, we’ll <a id="_idIndexMarker1421"/>define the types for the neural network, regression kernel, and <span class="No-Break">training sample:</span></p>
			<pre class="source-code">
using namespace Dlib;
using NetworkType = loss_mean_squared&lt;fc&lt;1, input&lt;matrix&lt;double&gt;&gt;&gt;&gt;;
using SampleType = matrix&lt;double, 1, 1&gt;;
using KernelType = linear_kernel&lt;SampleType&gt;;</pre>			<p>Then, we’ll generate the training data with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
size_t n = 1000;
std::vector&lt;matrix&lt;double&gt;&gt; x(n);
std::vector&lt;float&gt; y(n);
std::random_device rd;
std::mt19937 re(rd());
std::uniform_real_distribution&lt;float&gt; dist(-1.5, 1.5);
// generate data
for (size_t i = 0; i &lt; n; ++i) {
  x[i](0, 0) = i;
  y[i] = func(i) + dist(re);
}</pre>			<p>Here, <strong class="source-inline">x</strong> represents the predictor variable, while <strong class="source-inline">y</strong> represents the target variable. The target variable, <strong class="source-inline">y</strong>, is salted with uniform random noise to simulate real data. These variables <a id="_idIndexMarker1422"/>have a linear dependency, which is defined with the <span class="No-Break">following function:</span></p>
			<pre class="source-code">
double func(double x) {
  return 4. + 0.3 * x;
}</pre>			<p>Once we’ve generated the data, we normalize it using the <strong class="source-inline">vector_normalizer</strong> type object. Objects of this type can be reused after training to normalize data with the learned mean and standard deviation. The following snippet shows how <span class="No-Break">it’s implemented:</span></p>
			<pre class="source-code">
vector_normalizer&lt;matrix&lt;double&gt;&gt; normalizer_x;
normalizer_x.train(x);
for (size_t i = 0; i &lt; x.size(); ++i) {
  x[i] = normalizer_x(x[i]);
}</pre>			<p>Finally, we train the <strong class="source-inline">decision_function</strong> object for kernel ridge regression with the <strong class="source-inline">krr_trainer</strong> <span class="No-Break">type object:</span></p>
			<pre class="source-code">
void TrainAndSaveKRR(const std::vector&lt;matrix&lt;double&gt;&gt;&amp; x,
                     const std::vector&lt;float&gt;&amp; y) {
  krr_trainer&lt;KernelType&gt; trainer;
  trainer.set_kernel(KernelType());
  decision_function&lt;KernelType&gt; df = trainer.train(x, y);
  serialize("Dlib-krr.dat") &lt;&lt; df;
}</pre>			<p>Note that <a id="_idIndexMarker1423"/>we initialized the trainer object with the instance of the <span class="No-Break"><strong class="source-inline">KernelType</strong></span><span class="No-Break"> object.</span></p>
			<p>Now that we have the trained <strong class="source-inline">decision_function</strong> object, we can serialize it into a file with a stream object that’s returned by the <span class="No-Break"><strong class="source-inline">serialize</strong></span><span class="No-Break"> function:</span></p>
			<pre class="source-code">
serialize("Dlib-krr.dat") &lt;&lt; df;</pre>			<p>This function takes the name of the file for storage as an input argument and returns an output stream object. We used the <strong class="source-inline">&lt;&lt;</strong> operator to put the learned weights of the regression model into the file. The serialization approach we used in the preceding code example only saves <span class="No-Break">model parameters.</span></p>
			<p>The same approach can be used to serialize almost all ML models in the <strong class="source-inline">D</strong><strong class="source-inline">lib</strong> library. The following code shows how to use it to serialize the parameters of a <span class="No-Break">neural network:</span></p>
			<pre class="source-code">
void TrainAndSaveNetwork(
    const std::vector&lt;matrix&lt;double&gt;&gt;&amp; x,
    const std::vector&lt;float&gt;&amp; y) {
  NetworkType network;
  sgd solver;
  dnn_trainer&lt;NetworkType&gt; trainer(network, solver);
  trainer.set_learning_rate(0.0001);
  trainer.set_mini_batch_size(50);
  trainer.set_max_num_epochs(300);
  trainer.be_verbose();
  trainer.train(x, y);
  network.clean();
  serialize("Dlib-net.dat") &lt;&lt; network;
  net_to_xml(network, "net.xml");
}</pre>			<p>For neural networks, there’s also the <strong class="source-inline">net_to_xml</strong> function, which saves the model structure. However, there’s no function to load this saved structure into our program in the library API. It’s the user’s responsibility to implement a <span class="No-Break">loading function.</span></p>
			<p>The <strong class="source-inline">net_to_xml</strong> function <a id="_idIndexMarker1424"/>exists if we wish to share the model between frameworks, as depicted in the <span class="No-Break"><strong class="source-inline">Dlib</strong></span><span class="No-Break"> documentation.</span></p>
			<p>To check <a id="_idIndexMarker1425"/>that parameter serialization works as expected, we can generate new test data to evaluate a loaded model <span class="No-Break">on it:</span></p>
			<pre class="source-code">
std::cout &lt;&lt; "Target values \n";
std::vector&lt;matrix&lt;double&gt;&gt; new_x(5);
for (size_t i = 0; i &lt; 5; ++i) {
  new_x[i].set_size(1, 1);
  new_x[i](0, 0) = i;
  new_x[i] = normalizer_x(new_x[i]);
  std::cout &lt;&lt; func(i) &lt;&lt; std::endl;
}</pre>			<p>Note that we’ve reused the <strong class="source-inline">normalizer</strong> object. In general, the parameters of the <strong class="source-inline">normalizer</strong> object should also be serialized and loaded because, during evaluation, we need to transform new data into the same statistical characteristics that we used for the <span class="No-Break">training data.</span></p>
			<p>To load <a id="_idIndexMarker1426"/>a serialized object in the <strong class="source-inline">Dlib</strong> library, we can use the <strong class="source-inline">deserialize</strong> function. This function takes the filename and returns the input <span class="No-Break">stream object:</span></p>
			<pre class="source-code">
void LoadAndPredictKRR(
    const std::vector&lt;matrix&lt;double&gt;&gt;&amp; x) {
  decision_function&lt;KernelType&gt; df;
  deserialize("Dlib-krr.dat") &gt;&gt; df;
  // Predict
  std::cout &lt;&lt; "KRR predictions \n";
  for (auto&amp; v : x) {
    auto p = df(v);
    std::cout &lt;&lt; static_cast&lt;double&gt;(p) &lt;&lt; std::endl;
  }
}</pre>			<p>As we discussed previously, in the <strong class="source-inline">Dlib</strong> library, serialization only stores model parameters. So, to load them, we need to use the model object with the same properties that it had before serialization <span class="No-Break">was performed.</span></p>
			<p>For a regression model, this means that we should instantiate a decision function object with the same <span class="No-Break">kernel type.</span></p>
			<p>For a neural network model, this means that we should instantiate a network object of the same type that we used for serialization, as can be seen in the following <span class="No-Break">code block:</span></p>
			<pre class="source-code">
void LoadAndPredictNetwork(
    const std::vector&lt;matrix&lt;double&gt;&gt;&amp; x) {
  NetworkType network;
  deserialize("Dlib-net.dat") &gt;&gt; network;
  // Predict
  auto predictions = network(x);
  std::cout &lt;&lt; "Net predictions \n";
  for (auto p : predictions) {
    std::cout &lt;&lt; static_cast&lt;double&gt;(p) &lt;&lt; std::endl;
  }
}</pre>			<p>In this <a id="_idIndexMarker1427"/>section, we saw that the <strong class="source-inline">Dlib</strong> serialization API allows us to save and load ML model parameters but has limited options to serialize and load model architectures. In the next section, we’ll look at the <strong class="source-inline">Shogun</strong> library model’s <span class="No-Break">serial<a id="_idTextAnchor669"/>ization API.</span></p>
			<h2 id="_idParaDest-253"><a id="_idTextAnchor670"/>Model serialization with Flashlight</h2>
			<p>The <strong class="source-inline">Flashlight</strong> library can <a id="_idIndexMarker1428"/>save and load models and parameters into a binary format. It uses the <strong class="source-inline">Cereal</strong> C++ library internally for serialization. An <a id="_idIndexMarker1429"/>example of this functionality is shown in the <span class="No-Break">following example.</span></p>
			<p>As in the previous example, we’ll start by creating some sample <span class="No-Break">training data:</span></p>
			<pre class="source-code">
int64_t n = 10000;
auto x = fl::randn({n});
auto y = x * 0.3f + 0.4f;
// Define dataset
std::vector&lt;fl::Tensor&gt; fields{x, y};
auto dataset = std::make_shared&lt;fl::TensorDataset&gt;(fields);
fl::BatchDataset batch_dataset(dataset, /*batch_size=*/64);</pre>			<p>Here, we created a vector, <strong class="source-inline">x</strong>, with random data and used it to create our target variable, <strong class="source-inline">y</strong>, by applying <a id="_idIndexMarker1430"/>a linear dependency formula. We wrapped our independent and target vectors into a <strong class="source-inline">BatchDataset</strong> object called <strong class="source-inline">batch_dataset</strong>, which we’ll use to train a sample <span class="No-Break">neural network.</span></p>
			<p>The following code shows our neural <span class="No-Break">network definition:</span></p>
			<pre class="source-code">
fl::Sequential model;
model.add(fl::View({1, 1, 1, -1}));
model.add(fl::Linear(1, 8));
model.add(fl::ReLU());
model.add(fl::Linear(8, 16));
model.add(fl::ReLU());
model.add(fl::Linear(16, 32));
model.add(fl::ReLU());
model.add(fl::Linear(32, 1));</pre>			<p>As you can see, it’s the same feedforward network that we used in the previous example, but this time <span class="No-Break">for Flashlight.</span></p>
			<p>The following code sample shows how to train <span class="No-Break">the model:</span></p>
			<pre class="source-code">
auto loss = fl::MeanSquaredError();
float learning_rate = 0.01;
float momentum = 0.5;
auto sgd = fl::SGDOptimizer(model.params(), 
                            learning_rate,
                            momentum);
const int epochs = 5;
for (int epoch_i = 0; epoch_i &lt; epochs; ++epoch_i) {
  for (auto&amp; batch : batch_dataset) {
    sgd.zeroGrad();
    auto predicted = model(fl::input(batch[0]));
    auto local_batch_size = batch[0].shape().dim(0);
    auto target =
        fl::reshape(batch[1], {1, 1, 1, local_batch_size});
    auto loss_value = loss(predicted, fl::noGrad(target));
    loss_value.backward();
    sgd.step();
  }
}</pre>			<p>Here, we used <a id="_idIndexMarker1431"/>the same training approach that we used previously. First, we defined the <strong class="source-inline">loss</strong> object and the <strong class="source-inline">sgd</strong> optimizer object. Then, we used the two loops over epochs and over batches to train the model. In the internal loop, we applied the model to get new predicted values from training batch data. Then, we used the <strong class="source-inline">loss</strong> object to calculate the MSE value with the batch target values. We also used the <strong class="source-inline">backward</strong> method of the loss value variable to calculate the gradients. Finally, we used the <strong class="source-inline">sgd</strong> optimizer object to update the model parameters with the <span class="No-Break"><strong class="source-inline">step</strong></span><span class="No-Break"> method.</span></p>
			<p>Now that we have the trained model, we have two ways to save it in the <span class="No-Break"><strong class="source-inline">F</strong></span><span class="No-Break"><strong class="source-inline">lashlight</strong></span><span class="No-Break"> library:</span></p>
			<ol>
				<li>Serialize the whole model with the architecture <span class="No-Break">and weights.</span></li>
				<li>Serialize only the <span class="No-Break">model weights.</span></li>
			</ol>
			<p>For the first option—that is, serialize the whole model with the architecture—we can do <span class="No-Break">the following:</span></p>
			<pre class="source-code">
fl::save("model.dat", model);</pre>			<p>Here, <strong class="source-inline">model.dat</strong> is the name of the file where we’ll save the model. To load such a file, we can use the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
fl::Sequential model_loaded;
fl::load("model.dat", model_loaded);</pre>			<p>In this case, we created a new empty <a id="_idIndexMarker1432"/>object called <strong class="source-inline">model_loaded</strong> . This new object is just the <strong class="source-inline">fl::Sequential</strong> container object without particular layers. All the layers and parameter values were loaded with the <strong class="source-inline">fl::load</strong> function. Once we’ve loaded the model, we can use it <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
auto predicted = model_loaded(fl::noGrad(new_x));</pre>			<p>Here, <strong class="source-inline">new_x</strong> is some new data that we’re using for <span class="No-Break">evaluation purposes.</span></p>
			<p>Such an <a id="_idIndexMarker1433"/>approach when you store the whole model can be useful for applications that contain different models but have the same input and output interfaces as it can help you easily change or upgrade a model in production, <span class="No-Break">for example.</span></p>
			<p>The second option, which involves only saving the parameter (weight) values of a network, can be useful if we need to retrain a model regularly or if we share or reuse only some part of the model or its parameters. To do this, we can use the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
fl::save("model_params.dat", model.params());</pre>			<p>Here, we used the <strong class="source-inline">params</strong> method of the <strong class="source-inline">model</strong> object to get all the model’s parameters. This method returns the <strong class="source-inline">std::vector</strong> sequence of parameters for all model sub-modules. So, you can only manage some of them. To load saved parameters, we can use the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
std::vector&lt;fl::Variable&gt; params;
fl::load("model_params.dat", params);
for (int i = 0; i &lt; static_cast&lt;int&gt;(params.size()); ++i) {
  model.setParams(params[i], i);
}</pre>			<p>First, we created the empty <strong class="source-inline">params</strong> container. Then, with the <strong class="source-inline">fl::load</strong> function, we loaded parameter values into it. To be able to update particular sub-module parameter values, we used the <strong class="source-inline">setParams</strong> method. The <strong class="source-inline">'setParams'</strong> method takes a value and an integer position where we want to set this value. We saved all the model parameters <a id="_idIndexMarker1434"/>so that we could take them back into the <span class="No-Break">model sequentially.</span></p>
			<p>Unfortunately, there’s no way to load models and weights from other formats into the <strong class="source-inline">Flashlight</strong> library. So, if you need to load from another format, you have to write a converter and use the <strong class="source-inline">setParams</strong> method to set particular values. In the next section, we’ll delve into the <strong class="source-inline">mlpack</strong> library’s<a id="_idTextAnchor671"/> <span class="No-Break">serialization API.</span></p>
			<h2 id="_idParaDest-254"><a id="_idTextAnchor672"/>Model serialization with mlpack</h2>
			<p>The <strong class="source-inline">mlpack</strong> library only <a id="_idIndexMarker1435"/>implements model parameter serialization. This serialization is based on functionality that exists in the Armadillo math library, which <a id="_idIndexMarker1436"/>is used as the backend for mlpack. This means we can save parameter values in different file formats using the mlpack API. They are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">CSV</strong>: Denoted by <strong class="source-inline">.csv</strong>, or <span class="No-Break">optionally </span><span class="No-Break"><strong class="source-inline">.txt</strong></span></li>
				<li><strong class="bold">ASCII</strong>: Denoted <span class="No-Break">by </span><span class="No-Break"><strong class="source-inline">.txt</strong></span></li>
				<li><strong class="bold">Armadillo ASCII</strong>: Also denoted <span class="No-Break">by </span><span class="No-Break"><strong class="source-inline">.txt</strong></span></li>
				<li><strong class="bold">PGM</strong>: Denoted <span class="No-Break">by </span><span class="No-Break"><strong class="source-inline">.pgm</strong></span></li>
				<li><strong class="bold">PPM</strong>: Denoted <span class="No-Break">by </span><span class="No-Break"><strong class="source-inline">.ppm</strong></span></li>
				<li><strong class="bold">Raw binary</strong>: Denoted <span class="No-Break">by </span><span class="No-Break"><strong class="source-inline">.bin</strong></span></li>
				<li><strong class="bold">Armadillo binary</strong>: Also denoted <span class="No-Break">by </span><span class="No-Break"><strong class="source-inline">.bin</strong></span></li>
				<li><strong class="bold">HDF5</strong>: Denoted by <strong class="source-inline">.hdf5</strong>, <strong class="source-inline">.hdf</strong>, <strong class="source-inline">.h5</strong>, <span class="No-Break">or </span><span class="No-Break"><strong class="source-inline">.he5</strong></span></li>
			</ul>
			<p>Let’s look at a minimal example of model creation and parameter management with mlpack. First, we need a model. The following code shows the function we can use to <span class="No-Break">create one:</span></p>
			<pre class="source-code">
using ModelType = FFN&lt;MeanSquaredError, ConstInitialization&gt;;
ModelType make_model() {
  MeanSquaredError loss;
  ConstInitialization init(0.);
  ModelType model(loss, init);
  model.Add&lt;Linear&gt;(8);
  model.Add&lt;ReLU&gt;();
  model.Add&lt;Linear&gt;(16);
  model.Add&lt;ReLU&gt;();
  model.Add&lt;Linear&gt;(32);
  model.Add&lt;ReLU&gt;();
  model.Add&lt;Linear&gt;(1);
  return model;
}</pre>			<p>The <strong class="source-inline">create_model</strong> function <a id="_idIndexMarker1437"/>creates the feedforward network <a id="_idIndexMarker1438"/>with several linear layers. Note that we made this model use <strong class="source-inline">MSE</strong> as the loss function and added the zero parameter initializer. Now that we have a model, we need some data to train it. The following code shows how to create linear <span class="No-Break">dependent data:</span></p>
			<pre class="source-code">
size_t n = 10000;
arma::mat x = arma::randn(n).t();
arma::mat y = x * 0.3f + 0.4f;</pre>			<p>Here, we created two single-dimensional vectors, similar to what we did for the <strong class="source-inline">Flashlight</strong> sample but using the Armadillo matrix API. Notice that we used the <strong class="source-inline">t()</strong> transpose method for the <strong class="source-inline">x</strong> vector since mlpack uses the column dimension for its <span class="No-Break">training features.</span></p>
			<p>Now, we can connect all the components and perform <span class="No-Break">model training:</span></p>
			<pre class="source-code">
ens::Adam optimizer;
auto model = make_model();
model.Train(x, y, optimizer);</pre>			<p>Here, we created the <strong class="source-inline">Adam</strong> algorithm optimizer object and used it in the model’s <strong class="source-inline">Train</strong> method with the two data vectors we created previously. Now, we have the trained model and are ready to save its parameters. This can be done <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
data::Save("model.bin", model.Parameters(), true);</pre>			<p>By default, the <strong class="source-inline">data::Save</strong> function automatically determines the file format to save with based on the provided filename extension. Here, we used the <strong class="source-inline">Parameters</strong> method <a id="_idIndexMarker1439"/>of the model object to get the parameter values. This method returns a big matrix with all values. We also passed <strong class="source-inline">true</strong> as the third parameter to make the <strong class="source-inline">save</strong> function throw an exception in case of failure. By default, it will just return <strong class="source-inline">false</strong>; this is something you have to <span class="No-Break">check manually.</span></p>
			<p>We can use the <strong class="source-inline">mlpack::data::Load</strong> function to load parameter values, as <span class="No-Break">shown here:</span></p>
			<pre class="source-code">
auto new_model = make_model();
data::Load("model.bin", new_model.Parameters());</pre>			<p>Here, we created the <strong class="source-inline">new_model</strong> object; this is the same model but with parameters initialized as zero. Then, we used the <strong class="source-inline">mlpack::data::Load</strong> function to load parameter values from the file. Once again, we used the <strong class="source-inline">Parameters</strong> method to get the reference to the internal parameter values matrix and passed it to the <strong class="source-inline">load</strong> function. The third argument of the <strong class="source-inline">load</strong> function we set to <strong class="source-inline">true</strong> so that we can get an exception in case <span class="No-Break">of errors.</span></p>
			<p>Now that we’ve initialized the model, we can use it to <span class="No-Break">make predictions:</span></p>
			<pre class="source-code">
arma::mat predictions;
new_model.Predict(new_x, predictions);</pre>			<p>Here, we created an output matrix, <strong class="source-inline">prediction</strong>, and used the <strong class="source-inline">Predict</strong> method of the <strong class="source-inline">new_model</strong> object for model evaluation. Note that <strong class="source-inline">new_x</strong> is some new data that we wish to get <span class="No-Break">predictions for.</span></p>
			<p>Note that <a id="_idIndexMarker1440"/>you can’t load other frameworks’ file formats into mlpack, so you have to create converters if you need them. In the next section, we’ll look at the <strong class="source-inline">pytorch</strong> lib<a id="_idTextAnchor673"/>rary’s <span class="No-Break">serialization API.</span></p>
			<h2 id="_idParaDest-255"><a id="_idTextAnchor674"/>Model serialization with PyTorch</h2>
			<p>In this <a id="_idIndexMarker1441"/>section, we’ll discuss two approaches to network parameter serialization that are available in the <strong class="source-inline">pytorch</strong> <span class="No-Break">C++ library:</span></p>
			<ul>
				<li>The <span class="No-Break"><strong class="source-inline">torch::save</strong></span><span class="No-Break"> function</span></li>
				<li>An object of the <strong class="source-inline">torch::serialize::OutputArchive</strong> type for writing parameters into the <span class="No-Break"><strong class="source-inline">OutputArchive</strong></span><span class="No-Break"> object</span></li>
			</ul>
			<p>Let’s start by pre<a id="_idTextAnchor675"/>paring the <span class="No-Break">neural network.</span></p>
			<h3>Initializing the neural network</h3>
			<p>Let’s start <a id="_idIndexMarker1442"/>by generating the training data. The following code snippet shows how we can <span class="No-Break">do this:</span></p>
			<pre class="source-code">
torch::DeviceType device = torch::cuda::is_available()
? torch::DeviceType::CUDA
: torch::DeviceType::CPU;</pre>			<p>Usually, we want to utilize as many hardware resources as possible. So, first, we checked whether a GPU with CUDA technology was available in the system by using the <span class="No-Break"><strong class="source-inline">torch::cuda::is_available()</strong></span><span class="No-Break"> call:</span></p>
			<pre class="source-code">
std::random_device rd;
std::mt19937 re(rd());
std::uniform_real_distribution&lt;float&gt; dist(-0.1f, 0.1f);</pre>			<p>We defined <a id="_idIndexMarker1443"/>the <strong class="source-inline">dist</strong> object so that we could generate the uniformly distributed real values in the <strong class="source-inline">–1</strong> to <span class="No-Break"><strong class="source-inline">1</strong></span><span class="No-Break"> range:</span></p>
			<pre class="source-code">
size_t n = 1000;
torch::Tensor x;
torch::Tensor y;
{
  std::vector&lt;float&gt; values(n);
  std::iota(values.begin(), values.end(), 0);
  std::shuffle(values.begin(), values.end(), re);
  std::vector&lt;torch::Tensor&gt; x_vec(n);
  std::vector&lt;torch::Tensor&gt; y_vec(n);
  for (size_t i = 0; i &lt; n; ++i) {
    x_vec[i] = torch::tensor(
        values[i],
        torch::dtype(torch::kFloat).device(
                                    device).requires_grad(false));
    y_vec[i] = torch::tensor(
        (func(values[i]) + dist(re)),
        torch::dtype(torch::kFloat).device(
                                    device).requires_grad(false));
  }
  x = torch::stack(x_vec);
  y = torch::stack(y_vec);
}</pre>			<p>Then, we generated 1,000 predictor variable values and shuffled them. For each value, we calculated the target value with the linear function that we used in the previous examples—that is, <strong class="source-inline">func</strong>. Here’s what this <span class="No-Break">looks like:</span></p>
			<pre class="source-code">
float func(float x) {
  return 4.f + 0.3f * x;
}</pre>			<p>Then, all the values were moved into the <strong class="source-inline">torch::Tensor</strong> objects with <strong class="source-inline">torch::tensor</strong> function calls. Notice that we used a previously detected device for tensor creation. Once we moved all the values to tensors, we used the <strong class="source-inline">torch::stack</strong> function to concatenate the predictor and target values in two distinct <a id="_idIndexMarker1444"/>single tensors. This was required so that we could perform data normalization with the <strong class="source-inline">pytorch</strong> library’s linear <span class="No-Break">algebra routines:</span></p>
			<pre class="source-code">
auto x_mean = torch::mean(x, /*dim*/ 0);
auto x_std = torch::std(x, /*dim*/ 0);
x = (x - x_mean) / x_std;</pre>			<p>Finally, we used the <strong class="source-inline">torch::mean</strong> and <strong class="source-inline">torch::std</strong> functions to calculate the mean and standard deviation of the predictor values and <span class="No-Break">normalized them.</span></p>
			<p>In the following code, we’re defining the <strong class="source-inline">NetImpl</strong> class, which implements our <span class="No-Break">neural network:</span></p>
			<pre class="source-code">
class NetImpl : public torch::nn::Module {
 public:
  NetImpl() {
    l1_ = torch::nn::Linear(torch::nn::LinearOptions(
                                1, 8).with_bias(true));
    register_module("l1", l1_);
    l2_ = torch::nn::Linear(torch::nn::LinearOptions(
                                8, 4).with_bias(true));
    register_module("l2", l2_);
    l3_ = torch::nn::Linear(torch::nn::LinearOptions(
                                4, 1).with_bias(true));
    register_module("l3", l3_);
    // initialize weights
    for (auto m : modules(false)) {
      if (m-&gt;name().find("Linear") != std::string::npos) {
        for (auto&amp; p : m-&gt;named_parameters()) {
          if (p.key().find("weight") != std::string::npos) {
            torch::nn::init::normal_(p.value(), 0, 0.01);
                    }
          if (p.key().find("bias") != std::string::npos) {
            torch::nn::init::zeros_(p.value());
          }
        }
      }
    }
  }
torch::Tensor forward(torch::Tensor x) {
  auto y = l1_(x);
  y = l2_(y);
  y = l3_(y);
  return y;
}
private:
  torch::nn::Linear l1_{nullptr};
  torch::nn::Linear l2_{nullptr};
  torch::nn::Linear l3_{nullptr};
}
TORCH_MODULE(Net);</pre>			<p>Here, we defined our neural network model as a network with three fully connected neuron layers with a linear activation function. Each layer is of the <span class="No-Break"><strong class="source-inline">torch::nn::Linear</strong></span><span class="No-Break"> type.</span></p>
			<p>In the <a id="_idIndexMarker1445"/>constructor of our model, we initialized all the network parameters with small random values. We did this by iterating over all the network modules (see the <strong class="source-inline">modules</strong> method call) and applying the <strong class="source-inline">torch::nn::init::normal_</strong> function to the parameters that were returned by the <strong class="source-inline">named_parameters()</strong> module’s method. Biases were initialized to zeros with the <strong class="source-inline">torch::nn::init::zeros_</strong> function. The <strong class="source-inline">named_parameters()</strong> method returned <a id="_idIndexMarker1446"/>objects consisting of a string name and a tensor value, so for initialization, we used its <span class="No-Break"><strong class="source-inline">value</strong></span><span class="No-Break"> method.</span></p>
			<p>Now, we can train the model with our generated training data. The following code shows how we can train <span class="No-Break">our model:</span></p>
			<pre class="source-code">
Net model;
model-&gt;to(device);
// initialize optimizer -----------------------------------
double learning_rate = 0.01;
torch::optim::Adam optimizer(model-&gt;parameters(),
torch::optim::AdamOptions(learning_rate).weight_decay(0.00001));
// training
int64_t batch_size = 10;
int64_t batches_num = static_cast&lt;int64_t&gt;(n) / batch_size;
int epochs = 10;
for (int epoch = 0; epoch &lt; epochs; ++epoch) {
  // train the model
  // -----------------------------------------------
  model-&gt;train();  // switch to the training mode
  // Iterate the data
  double epoch_loss = 0;
  for (int64_t batch_index = 0; batch_index &lt; batches_num;
       ++batch_index) {
    auto batch_x =
        x.narrow(0, batch_index * batch_size, batch_size)
            .unsqueeze(1);
    auto batch_y =
        y.narrow(0, batch_index * batch_size, batch_size)
            .unsqueeze(1);
    // Clear gradients
    optimizer.zero_grad();
    // Execute the model on the input data
    torch::Tensor prediction = model-&gt;forward(batch_x);
    torch::Tensor loss =
        torch::mse_loss(prediction, batch_y);
    // Compute gradients of the loss and parameters of
    // our model
    loss.backward();
    // Update the parameters based on the calculated
    // gradients.
    optimizer.step();
  }
}</pre>			<p>To utilize all <a id="_idIndexMarker1447"/>our hardware resources, we moved the model to the selected computational device. Then, we initialized an optimizer. In our case, the optimizer used the <strong class="source-inline">Adam</strong> algorithm. After, we ran a standard training loop over the epochs where, for each epoch, we took the training batch, cleared the optimizer’s gradients, performed a forward pass, computed the loss, performed a backward pass, and updated the model weights with the <span class="No-Break">optimizer step.</span></p>
			<p>To select a batch of training data from the dataset, we used the tensor’s <strong class="source-inline">narrow</strong> method, which returned a new tensor with a reduced dimension. This function takes a new number of dimensions as the first parameter, the start position as the second parameter, and the number of elements to remain as the third parameter. We also used the <strong class="source-inline">unsqueeze</strong> method to add a batch dimension; this is required by the PyTorch API for the <span class="No-Break">forward pass.</span></p>
			<p>As we <a id="_idIndexMarker1448"/>mentioned previously, there are two approaches we can use to serialize model parameters in <strong class="source-inline">pytorch</strong> in the C++ API (the Python API provides even<a id="_idTextAnchor676"/> more reach). Let’s look <span class="No-Break">at them.</span></p>
			<h3>Using the torch::save and torch::load functions</h3>
			<p>The first <a id="_idIndexMarker1449"/>approach we can take to save model <a id="_idIndexMarker1450"/>parameters is using the <strong class="source-inline">torch::save</strong> function, which recursively saves parameters from the <span class="No-Break">passed module:</span></p>
			<pre class="source-code">
torch::save(model, "pytorch_net.pt");</pre>			<p>To use it <a id="_idIndexMarker1451"/>correctly with our custom modules, we need to register all <a id="_idIndexMarker1452"/>the sub-modules in the parent one with the <strong class="source-inline">register_module</strong> <span class="No-Break">module’s method.</span></p>
			<p>To load the saved parameters, we can use the <span class="No-Break"><strong class="source-inline">torch::load</strong></span><span class="No-Break"> function:</span></p>
			<pre class="source-code">
Net model_loaded;
torch::load(model_loaded, "pytorch_net.pt");</pre>			<p>The function fills the passed module parameters with <a id="_idTextAnchor677"/>the values that are read from <span class="No-Break">a file.</span></p>
			<h3>Using PyTorch archive objects</h3>
			<p>The second <a id="_idIndexMarker1453"/>approach is to use an object of the <strong class="source-inline">torch::serialize::OutputArchive</strong> type and write the parameters we want to save into it. The following code shows how to implement the <strong class="source-inline">SaveWeights</strong> method for our model. This method writes all the parameters and buffers that exist in our module to the <strong class="source-inline">archive</strong> object, and then it uses the <strong class="source-inline">save_to</strong> method to write them in <span class="No-Break">a file:</span></p>
			<pre class="source-code">
void NetImpl::SaveWeights(const std::string&amp; file_name) {
  torch::serialize::OutputArchive archive;
  auto parameters = named_parameters(true /*recurse*/);
  auto buffers = named_buffers(true /*recurse*/);
  for (const auto&amp; param : parameters) {
    if (param.value().defined()) {
      archive.write(param.key(), param.value());
    }
  }
  for (const auto&amp; buffer : buffers) {
    if (buffer.value().defined()) {
      archive.write(buffer.key(), buffer.value(),
                    /*is_buffer*/ true);
    }
  }
  archive.save_to(file_name);
}</pre>			<p>It’s also important to save buffer tensors. Buffers can be retrieved from a module with the <strong class="source-inline">named_buffers</strong> module’s method. These objects represent the intermediate values that are used to evaluate different modules. For example, we can be running mean and standard deviation values for the batch normalization module. In this case, we need them <a id="_idIndexMarker1454"/>to continue being trained if we used serialization to save the intermediate steps and if our training process was stopped for <span class="No-Break">some reason.</span></p>
			<p>To load parameters that have been saved this way, we can use the <strong class="source-inline">torch::serialize::InputArchive</strong> object. The following code shows how to implement the <strong class="source-inline">LoadWeights</strong> method for <span class="No-Break">our model:</span></p>
			<pre class="source-code">
void NetImpl::LoadWeights(const std::string&amp; file_name) {
  torch::serialize::InputArchive archive;
  archive.load_from(file_name);
  torch::NoGradGuard no_grad;
  auto parameters = named_parameters(true /*recurse*/);
  auto buffers = named_buffers(true /*recurse*/);
  for (auto&amp; param : parameters) {
      archive.read(param.key(), param.value());
  }
  for (auto&amp; buffer : buffers) {
      archive.read(buffer.key(), buffer.value(),
          /*is_buffer*/ true);
  }
}</pre>			<p>Here, the <strong class="source-inline">LoadWeights</strong> method uses the <strong class="source-inline">load_from</strong> method of the <strong class="source-inline">archive</strong> object to load parameters from the file. First, we took the parameters and buffers from our module with the <strong class="source-inline">named_parameters</strong> and <strong class="source-inline">named_buffers</strong> methods and filled in their <a id="_idIndexMarker1455"/>values incrementally with the <strong class="source-inline">read</strong> method of the <span class="No-Break"><strong class="source-inline">archive</strong></span><span class="No-Break"> object.</span></p>
			<p>Notice that we used an instance of the <strong class="source-inline">torch::NoGradGuard</strong> class to tell the <strong class="source-inline">pytorch</strong> library that we won’t be performing any model calculation or graph-related operations. It’s essential to do this because the <strong class="source-inline">pytorch</strong> library’s construct calculation graph and any unrelated operations can lead <span class="No-Break">to errors.</span></p>
			<p>Now, we can use the new instance of our <strong class="source-inline">model_loaded</strong> model with <strong class="source-inline">load</strong> parameters to evaluate the model on some test data. Note that we need to switch the model to the evaluation model with the <strong class="source-inline">eval</strong> method. Generated test data values should also be converted into tensor objects with the <strong class="source-inline">torch::tensor</strong> function and moved to the same computational device that our model uses. The following code shows how we can <span class="No-Break">implement this:</span></p>
			<pre class="source-code">
model_loaded-&gt;to(device);
model_loaded-&gt;eval();
std::cout &lt;&lt; "Test:\n";
for (int i = 0; i &lt; 5; ++i) {
  auto x_val = static_cast&lt;float&gt;(i) + 0.1f;
  auto tx = torch::tensor(
      x_val, torch::dtype(torch::kFloat).device(device));
  tx = (tx - x_mean) / x_std;
  auto ty = torch::tensor(
      func(x_val),
      torch::dtype(torch::kFloat).device(device));
  torch::Tensor prediction = model_loaded-&gt;forward(tx);
  std::cout &lt;&lt; "Target:" &lt;&lt; ty &lt;&lt; std::endl;
  std::cout &lt;&lt; "Prediction:" &lt;&lt; prediction &lt;&lt; std::endl;
}</pre>			<p>In this section, we looked at two types of serialization in the <strong class="source-inline">pytorch</strong> library. The first approach <a id="_idIndexMarker1456"/>involved using the <strong class="source-inline">torch::save</strong> and <strong class="source-inline">torch::load</strong> functions, which easily save and load all the model parameters, respectively. The second approach involved using objects of the <strong class="source-inline">torch::serialize::InputArchive</strong> and <strong class="source-inline">torch::serialize::OutputArchive</strong> types so that we can select what parameters we want to save <span class="No-Break">and load.</span></p>
			<p>In the next section, we’ll discuss the ONNX file format, which allows us to share our ML model architecture and mod<a id="_idTextAnchor678"/>el parameters among <span class="No-Break">different frameworks.</span></p>
			<h1 id="_idParaDest-256"><a id="_idTextAnchor679"/>Delving into the ONNX format</h1>
			<p>The ONNX format is <a id="_idIndexMarker1457"/>a special file format that’s used to share neural network architectures and parameters between different frameworks. It’s based on Google’s Protobuf format and library. The reason why this format exists is to test and run the same neural network model in different environments and on <span class="No-Break">different devices.</span></p>
			<p>Usually, researchers use a programming framework that they know how to use to develop a model, and then run this model in a different environment for production purposes or if they want to share their model with other researchers or developers. This format is supported by all leading frameworks, including PyTorch, TensorFlow, MXNet, and others. However, there’s a lack of support for this format from the C++ API of these frameworks and at the time of writing, they only have a Python interface for dealing with the ONNX format. Despite this, Microsoft <a id="_idIndexMarker1458"/>provides the <strong class="source-inline">onnxruntime</strong> framework to run inference with this format directly with different backbends, such as CUDA, CPUs, or even <span class="No-Break">NVIDIA TensorRT.</span></p>
			<p>Before we dive <a id="_idIndexMarker1459"/>into the specifics of using the framework for our use case, it’s important to consider certain limitations so that we can approach the problem statement in a well-rounded way. Sometimes, exporting to the ONNX format can be problematic due to the lack of certain operators or functions, which can limit the types of models that can be exported. Also, there can be limited support for dynamic dimensions for tensors and limited support for conditional operators, which limits our ability to use models with dynamic computational graphs and implement complex algorithms. These limitations depend on the target hardware. You’ll find that embedded devices have the most restrictions and that some of these problems can only be found in the inference runtime. However, there is one big advantage of using ONNX—usually, it’s possible to run such a model on a variety of different tensor math <span class="No-Break">acceleration hardware.</span></p>
			<p>TorchScript has fewer limitations for model operators and structure than ONNX. It’s usually possible to export models with dynamic computational graphs that have been traced with all the required branches. However, there can be restrictions on hardware where you’ll have to infer your model. For example, usually, it’s not possible to use mobile GPUs or NPUs for inference with TorchScript. ExecuTorch should solve this problem in <span class="No-Break">the future.</span></p>
			<p>To utilize the available hardware as much as possible, we can use different inference engines from particular vendors. Usually, it’s possible to convert a model in the ONNX format or that’s using another method into its internal format to perform inference on a specific GPU or NPU. Examples of such engines include OpenVINO for Intel hardware, TensorRT from NVIDIA, ArmNN for ARM-based processors, and QNN for <span class="No-Break">Qualcomm NPUs.</span></p>
			<p>Now that we’ve understood the best way in which we can utilize the framework, let’s understand how to use the ResNet neural network architecture for <span class="No-Break">image classification.</span></p>
			<h2 id="_idParaDest-257"><a id="_idTextAnchor680"/>Using the ResNet architecture for image classification</h2>
			<p>Generally, we, as developers, don’t need to know how the ONNX format works internally <a id="_idIndexMarker1460"/>because we’re only interested in the files where the model has been saved. As mentioned previously, internally, the ONNX format is a Protobuf-formatted file. The following <a id="_idIndexMarker1461"/>code shows the first part of the ONNX file, which describes how to use the ResNet neural network architecture for <span class="No-Break">image classification:</span></p>
			<pre class="source-code">
ir_version: 3
graph {
  node {
  input: "data"
  input: "resnetv24_batchnorm0_gamma"
  input: "resnetv24_batchnorm0_beta"
  input: "resnetv24_batchnorm0_running_mean"
  input: "resnetv24_batchnorm0_running_var"
  output: "resnetv24_batchnorm0_fwd"
  name: "resnetv24_batchnorm0_fwd"
  op_type: "BatchNormalization"
  attribute {
      name: "epsilon"
      f: 1e-05
      type: FLOAT
  }
  attribute {
      name: "momentum"
      f: 0.9
      type: FLOAT
  }
  attribute {
      name: "spatial"
      i: 1
      type: INT
  }
}
node {
  input: "resnetv24_batchnorm0_fwd"
  input: "resnetv24_conv0_weight"
  output: "resnetv24_conv0_fwd"
  name: "resnetv24_conv0_fwd"
  op_type: "Conv"
  attribute {
      name: "dilations"
      ints: 1
      ints: 1
      type: INTS
  }
  attribute {
      name: "group"
      i: 1
      type: INT
  }
  attribute {
      name: "kernel_shape"
      ints: 7
      ints: 7
      type: INTS
  }
  attribute {
      name: "pads"
      ints: 3
      ints: 3
      ints: 3
      ints: 3
      type: INTS
  }
  attribute {
      name: "strides"
      ints: 2
      ints: 2
      type: INTS
  }
}
...
}</pre>			<p>Usually, ONNX files come in binary format to reduce file size and increase <span class="No-Break">loading speed.</span></p>
			<p>Now, let’s learn how to use the <strong class="source-inline">onnxruntime</strong> API to load and run ONNX models. The ONNX community provides pre-trained models for the most popular neural network architectures in the publicly available Model <span class="No-Break">Zoo (</span><a href="https://github.com/onnx/models"><span class="No-Break">https://github.com/onnx/models</span></a><span class="No-Break">).</span></p>
			<p>There are a lot of ready-to-use models that can be used to solve different ML tasks. For example, we can use the <strong class="source-inline">ResNet-50</strong> model for image classification <span class="No-Break">tasks (</span><a href="https://github.com/onnx/models/tree/main/validated/vision/classification/resnet/model/resnet50-v1-7.onnx"><span class="No-Break">https://github.com/onnx/models/tree/main/validated/vision/classification/resnet/model/resnet50-v1-7.onnx</span></a><span class="No-Break">).</span></p>
			<p>For <a id="_idIndexMarker1462"/>this model, we have to download the corresponding <strong class="source-inline">synset</strong> file with image class descriptions to be able to return classification results in a human-readable manner. You can find the file <span class="No-Break">at </span><a href="https://github.com/onnx/models/blob/main/validated/vision/classification/synset.txt"><span class="No-Break">https://github.com/onnx/models/blob/main/validated/vision/classification/synset.txt</span></a><span class="No-Break">.</span></p>
			<p>To be <a id="_idIndexMarker1463"/>able to use the <strong class="source-inline">onnxruntime</strong> C++ API, we have to use the <span class="No-Break">following header:</span></p>
			<pre class="source-code">
#include &lt;onnxruntime_cxx_api.h&gt;</pre>			<p>Then, we have to create the global shared <strong class="source-inline">onnxruntime</strong> environment and a model evaluation session, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
Ort::Env env;
Ort::Session session(env,
                     "resnet50-v1-7.onnx",
                     Ort::SessionOptions{nullptr});</pre>			<p>The <strong class="source-inline">session</strong> object takes a model’s filename as its input argument and automatically loads it. Here, we passed the name of the downloaded model. The last parameter is the <strong class="source-inline">SessionOptions</strong> type object, which can be used to specify a particular device executor, such as CUDA. The <strong class="source-inline">env</strong> object holds some shared runtime state. The most valuable state is the logging data and the logging level, which can configured with a <span class="No-Break">constructor argument.</span></p>
			<p>Once we’ve loaded a model, we can access its parameters, such as the number of model inputs, the number of model outputs, and parameter names. Such information will be very useful if you didn’t know it beforehand because you need input parameter names to run inference. We can discover such model information <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
void show_model_info(const Ort::Session&amp; session) {
  Ort::AllocatorWithDefaultOptions allocator;</pre>			<p>Here, we <a id="_idIndexMarker1464"/>created a function header and initialized the memory allocator for strings. Now, we can print the <a id="_idIndexMarker1465"/>input <span class="No-Break">parameter information:</span></p>
			<pre class="source-code">
  auto num_inputs = session.GetInputCount();
  for (size_t i = 0; i &lt; num_inputs; ++i) {
    auto input_name = session.GetInputNameAllocated(i,
                                           allocator);
    std::cout &lt;&lt; "Input name " &lt;&lt; i &lt;&lt; " : " &lt;&lt; input_name
              &lt;&lt; std::endl;
    Ort::TypeInfo type_info = session.GetInputTypeInfo(i);
    auto tensor_info =
        type_info.GetTensorTypeAndShapeInfo();
    auto tensor_shape = tensor_info.GetShape();
    std::cout &lt;&lt; "Input shape " &lt;&lt; i &lt;&lt; " : ";
    for (size_t j = 0; j &lt; tensor_shape.size(); ++j)
      std::cout &lt;&lt; tensor_shape[j] &lt;&lt; " ";
    std::cout &lt;&lt; std::endl;
  }</pre>			<p>Once we’ve <a id="_idIndexMarker1466"/>discovered the input parameters, we can print the output parameter information, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
 auto num_outputs = session.GetOutputCount();
  for (size_t i = 0; i &lt; num_outputs; ++i) {
    auto output_name = session.GetOutputNameAllocated(i,
                                             allocator);
  std::cout &lt;&lt; "Output name " &lt;&lt; i &lt;&lt; " : " &lt;&lt;
                       output_name &lt;&lt; std::endl;
  Ort::TypeInfo type_info = session.GetOutputTypeInfo(i);
  auto tensor_info = type_info.GetTensorTypeAndShapeInfo();
  auto tensor_shape = tensor_info.GetShape();
  std::cout &lt;&lt; "Output shape " &lt;&lt; i &lt;&lt; " : ";
  for (size_t j = 0; j &lt; tensor_shape.size(); ++j)
    std::cout &lt;&lt; tensor_shape[j] &lt;&lt; " ";
  std::cout &lt;&lt; std::endl;
  }
}</pre>			<p>Here, we <a id="_idIndexMarker1467"/>used the <strong class="source-inline">session</strong> object to <a id="_idIndexMarker1468"/>discover model properties. Using the <strong class="source-inline">GetInputCount</strong> and <strong class="source-inline">GetOutputCount</strong> methods, we got the number of corresponding <a id="_idIndexMarker1469"/>input and <a id="_idIndexMarker1470"/>output parameters. Then, we used the <strong class="source-inline">GetInputNameAllocated</strong> and <strong class="source-inline">GetOutputNameAllocated</strong> methods to get the parameter <a id="_idIndexMarker1471"/>names by their indices. Notice that these <a id="_idIndexMarker1472"/>methods require the <strong class="source-inline">allocator</strong> object. Here, we used the default one that was initialized at the top of the <span class="No-Break"><strong class="source-inline">show_model_info</strong></span><span class="No-Break"> function.</span></p>
			<p>We can get <a id="_idIndexMarker1473"/>the additional parameter type information with <a id="_idIndexMarker1474"/>the <strong class="source-inline">GetInputTypeInfo</strong> and <strong class="source-inline">GetOutputTypeInfo</strong> methods by using their corresponding parameter indices. Then, by using these parameter type information objects, we can get the tensor information <a id="_idIndexMarker1475"/>with the <strong class="source-inline">GetTensorTypeAndShapeInfo</strong> method. The most important piece of information here is the tensor shape that we got with the <strong class="source-inline">GetShape</strong> method of the <strong class="source-inline">tensor_onfo</strong> object. It’s important because we need to use particular shapes for the model input and output tensors. The shape is represented as a vector of integers. Now, using the <strong class="source-inline">show_model_info</strong> function, we can get model input and output parameter information, create the corresponding tensors, and fill them <span class="No-Break">with data.</span></p>
			<p>In our case, the input is a tensor of size <strong class="source-inline">1 x 3 x 224 x 224</strong>, which represents the RGB image for classification. The <strong class="source-inline">onnxruntime</strong> session object takes <strong class="source-inline">Ort::Value</strong> type <a id="_idIndexMarker1476"/>objects as input and fills them <span class="No-Break">as outputs.</span></p>
			<p>The following snippet shows how to prepare the input tensor for <span class="No-Break">the model:</span></p>
			<pre class="source-code">
constexpr const int width = 224;
constexpr const int height = 224;
std::array&lt;int64_t, 4&gt; input_shape{1, 3, width, height};
std::vector&lt;float&gt; input_image(3 * width * height);
read_image(argv[3], width, height, input_image);
auto memory_info = Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator,
                                              OrtMemTypeCPU);
Ort::Value input_tensor =
    Ort::Value::CreateTensor&lt;float&gt;(memory_ info,
                                    input_image.data(),
                                    input_image.size(),
                                    input_shape.data(),
                                    input_shape.size());</pre>			<p>First, we defined constants that represent an input image’s width and height. Then, we created the <strong class="source-inline">input_shape</strong> object, which defines the full shape of the tensor, including its batch dimension. With the shape, we created the <strong class="source-inline">input_image</strong> vector to hold the exact image data. This data container was filled with the <strong class="source-inline">read_image</strong> function, something we’ll take a closer look at shortly. Finally, we created the <strong class="source-inline">input_tensor</strong> object <a id="_idIndexMarker1477"/>with the <strong class="source-inline">Ort::Value::CreateTensor</strong> function, which takes the <strong class="source-inline">memory_info</strong> object and the <a id="_idIndexMarker1478"/>references to the data and shape containers. The <strong class="source-inline">memory_info</strong> object was created with parameters to allocate the input tensor on the host CPU device. The output tensor can be created in the <span class="No-Break">same way:</span></p>
			<pre class="source-code">
std::array&lt;int64_t, 2&gt; output_shape{1, 1000};
std::vector&lt;float&gt; result(1000);
Ort::Value output_tensor =
    Ort::Value::CreateTensor&lt;float&gt;(memory_ info,
                                    result.data(),
                                    result.size(),
                                    output_shape.data(),
                                    output_shape.size());</pre>			<p>Note that the <strong class="source-inline">onnxruntime</strong> API allows us to create an empty output tensor that will be initialized automatically. We can do this <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
Ort::Value output_tensor{nullptr};</pre>			<p>Now, we can use the <strong class="source-inline">Run</strong> method for <span class="No-Break">evaluation purposes:</span></p>
			<pre class="source-code">
const char* input_names[] = {"data"};
const char* output_names[] = {"resnetv17_dense0_fwd"};
Ort::RunOptions run_options;
session.Run(run_options,
            input_names,
            &amp;input_tensor,
            1,
            output_names,
            &amp;output_tensor,
            1);
                 </pre>			<p>Here, we defined the input and output parameters’ names and constants and created the <strong class="source-inline">run_options</strong> object with default initialization. The <strong class="source-inline">run_options</strong> object can be used to log verbosity configuration, while the <strong class="source-inline">Run</strong> method can be used to evaluate the model. Notice that the input and output tensors were passed as pointers to arrays with corresponding element numbers. In our case, we specified single input and <span class="No-Break">output elements.</span></p>
			<p>The <a id="_idIndexMarker1479"/>output of this model <a id="_idIndexMarker1480"/>is image scores (probabilities) for each of the 1,000 classes of the <strong class="source-inline">ImageNet</strong> dataset, which was used to train the model. The following code shows how to decode the <span class="No-Break">model’s output:</span></p>
			<pre class="source-code">
std::map&lt;size_t, std::string&gt; classes = read_classes("synset.txt");
std::vector&lt;std::pair&lt;float, size_t&gt;&gt; pairs;
for (size_t i = 0; i &lt; result.size(); i++) {
  if (result[i] &gt; 0.01f) {  // threshold check
    pairs.push_back(std::make_pair(
        output[i], i + 1));  // 0 –//background
  }
}
std::sort(pairs.begin(), pairs.end());
std::reverse(pairs.begin(), pairs.end());
pairs.resize(std::min(5UL, pairs.size()));
for (auto&amp; p : pairs) {
  std::cout &lt;&lt; "Class " &lt;&lt; p.second &lt;&lt; " Label "
            &lt;&lt; classes.at(p.second) &lt;&lt; « Prob « &lt;&lt; p.first
            &lt;&lt; std::endl;
}</pre>			<p>Here, we iterated over each element of the result tensor data—that is, the <strong class="source-inline">result</strong> vector object we initialized earlier. This <strong class="source-inline">result</strong> object was filled with actual data values during model evaluation. Then, we placed the score values and class indices in the <a id="_idIndexMarker1481"/>vector of corresponding pairs. This vector was sorted by score, in descending order. Then, we printed <a id="_idIndexMarker1482"/>five classes with the <span class="No-Break">maximum score.</span></p>
			<p>In this section, we looked at an example of how to deal with the ONNX format with the <strong class="source-inline">onnxruntime</strong> framework. However, we still need to learn how to load input <a id="_idTextAnchor681"/>images into tensor objects, something we use for the <span class="No-Break">model’s input.</span></p>
			<h2 id="_idParaDest-258"><a id="_idTextAnchor682"/>Loading images into onnxruntime tensors</h2>
			<p>Let’s learn how to load image data according to the model’s input requirements and memory <a id="_idIndexMarker1483"/>layout. Previously, we initialized an <strong class="source-inline">input_image</strong> vector of the corresponding size. The model <a id="_idIndexMarker1484"/>expects the input images to be normalized and <a id="_idIndexMarker1485"/>three-channel RGB images whose shapes are <strong class="source-inline">N x 3 x H x W</strong>, where <em class="italic">N</em> is the batch size and <em class="italic">H</em> and <em class="italic">W</em> are expected to be at least 224 pixels wide. Normalization assumes that the images are loaded into the<strong class="source-inline">[0, 1]</strong> range and then normalized using means equal to <strong class="source-inline">[0.485, 0.456, 0.406]</strong> and standard deviations equal to <strong class="source-inline">[0.229, </strong><span class="No-Break"><strong class="source-inline">0.224, 0.225]</strong></span><span class="No-Break">.</span></p>
			<p>Let’s assume that we have the following function definition to <span class="No-Break">load images:</span></p>
			<pre class="source-code">
void read_image(const std::string&amp; file_name,
                       int width,
                       int height,
                       std::vector&lt;float&gt;&amp; image_data)
...
}</pre>			<p>Let’s <a id="_idIndexMarker1486"/>write its <a id="_idIndexMarker1487"/>implementation. To load images, we’ll <a id="_idIndexMarker1488"/>use the <span class="No-Break"><strong class="source-inline">OpenCV</strong></span><span class="No-Break"> library:</span></p>
			<pre class="source-code">
// load image
auto image = cv::imread(file_name, cv::IMREAD_COLOR);
if (!image.cols || !image.rows) {
  return {};
}
if (image.cols != width || image.rows != height) {
  // scale image to fit
  cv::Size scaled(
      std::max(height * image.cols / image.rows, width),
      std::max(height, width * image.rows / image.cols));
  cv::resize(image, image, scaled);
  // crop image to fit
  cv::Rect crop((image.cols - width) / 2,
                (image.rows - height) / 2, width, height);
  image = image(crop);
}</pre>			<p>Here, we read the image from a file with the <strong class="source-inline">cv::imread</strong> function. If the image dimensions aren’t equal to the ones that have been specified, we need to resize the image with the <strong class="source-inline">cv::resize</strong> function and then crop the image if the image’s dimensions exceed the ones that have <span class="No-Break">been specified.</span></p>
			<p>Then, we must convert the image into the floating-point type and <span class="No-Break">RGB format:</span></p>
			<pre class="source-code">
image.convertTo(image, CV_32FC3);
cv::cvtColor(image, image, cv::COLOR_BGR2RGB);</pre>			<p>Once <a id="_idIndexMarker1489"/>formatting is complete, we can split the <a id="_idIndexMarker1490"/>image into three separate channels <a id="_idIndexMarker1491"/>with red, green, and blue colors. We should also normalize the color values. The following code shows how to <span class="No-Break">do this:</span></p>
			<pre class="source-code">
std::vector&lt;cv::Mat&gt; channels(3);
cv::split(image, channels);
std::vector&lt;double&gt; mean = {0.485, 0.456, 0.406};
std::vector&lt;double&gt; stddev = {0.229, 0.224, 0.225};
size_t i = 0;
for (auto&amp; c : channels) {
  c = ((c / 255) - mean[i]) / stddev[i];
  ++i;
}</pre>			<p>Here, each channel was subtracted by the corresponding mean and divided by the corresponding standard deviation for the <span class="No-Break">normalization process.</span></p>
			<p>Then, we should concatenate <span class="No-Break">the channels:</span></p>
			<pre class="source-code">
cv::vconcat(channels[0], channels[1], image);
cv::vconcat(image, channels[2], image);
assert(image.isContinuous());</pre>			<p>In this case, the normalized channels were concatenated into one contiguous image with the <span class="No-Break"><strong class="source-inline">cv::vconcat</strong></span><span class="No-Break"> function.</span></p>
			<p>The following code shows how to copy an OpenCV image into the <span class="No-Break"><strong class="source-inline">image_data</strong></span><span class="No-Break"> vector:</span></p>
			<pre class="source-code">
std::vector&lt;int64_t&gt; dims = {1, 3, height, width};
std::copy_n(reinterpret_cast&lt;float*&gt;(image.data),
image.size().area(),
image_data.begin());</pre>			<p>Here, the image data was copied into a vector of floats, which was initialized with the specified <a id="_idIndexMarker1492"/>dimensions. The OpenCV image data was accessed with the <strong class="source-inline">cv::Mat::data</strong> type member. We cast the <a id="_idIndexMarker1493"/>image data into the floating-point <a id="_idIndexMarker1494"/>type because this member variable is of the <strong class="source-inline">unsigned char *</strong> type. The pixel’s data was copied with the standard <strong class="source-inline">std::copy_n</strong> function. This function was used to fill the <strong class="source-inline">input_image</strong> vector with actual image data. Then, the reference to the <strong class="source-inline">input_image</strong> vector data was used in the <strong class="source-inline">CreateTensor</strong> function to initialize the <span class="No-Break"><strong class="source-inline">Ort::Value</strong></span><span class="No-Break"> object.</span></p>
			<p>Another important function that was used in the ONNX format example was a function that can read class<a id="_idTextAnchor683"/> definitions from a <strong class="source-inline">synset</strong> file. We’ll take a look at this in the <span class="No-Break">next section.</span></p>
			<h2 id="_idParaDest-259"><a id="_idTextAnchor684"/>Reading the class definition file</h2>
			<p>In this example, we <a id="_idIndexMarker1495"/>used the <strong class="source-inline">read_classes</strong> function to load the <a id="_idIndexMarker1496"/>map of objects. Here, the key was an image <a id="_idIndexMarker1497"/>class index and the value was a textual class description. This function is trivial and reads the <strong class="source-inline">synset</strong> file line by line. In such a file, each line contains a number and a class description string, separated by a space. The following code shows <span class="No-Break">its definition:</span></p>
			<pre class="source-code">
using Classes = std::map&lt;size_t, std::string&gt;;
Classes read_classes(const std::string&amp; file_name) {
  Classes classes;
  std::ifstream file(file_name);
  if (file) {
    std::string line;
    std::string id;
    std::string label;
    std::string token;
    size_t idx = 1;
   while (std::getline(file, line)) {
      std::stringstream line_stream(line);
      size_t i = 0;
      while (std::getline(line_stream, token, ' ')) {
        switch (i) {
          case 0:
            id = token;
            break;
          case 1:
            label = token;
            break;
        }
        token.clear();
        ++i;
      }
      classes.insert({idx, label});
      ++idx;
    }
  }
  return classes;</pre>			<p>Notice that <a id="_idIndexMarker1498"/>we used the <strong class="source-inline">std::getline</strong> function in <a id="_idIndexMarker1499"/>the internal <strong class="source-inline">while</strong> loop to tokenize a <a id="_idIndexMarker1500"/>single line string. We did this by specifying the third parameter that defines the delimiter <span class="No-Break">character value.</span></p>
			<p>In this section, we learned how to load the <strong class="source-inline">synset</strong> file, which represents the correspondence between class names and their IDs. We used this information to map a class ID that we go<a id="_idTextAnchor685"/>t as a classification result to its string representation, which we showed to <span class="No-Break">a user.</span></p>
			<h1 id="_idParaDest-260"><a id="_idTextAnchor686"/>Summary</h1>
			<p>In this chapter, we learned how to save and load model parameters in different ML frameworks. We saw that all the frameworks we used in the <strong class="source-inline">Flashlight</strong>, <strong class="source-inline">mlpack</strong>, <strong class="source-inline">Dlib</strong>, and <strong class="source-inline">pytorch</strong> libraries have an API for model parameter serialization. Usually, these are quite simple functions that work with model objects and some input and output streams. We also discussed the serialization API, which can be used to save and load the overall model architecture. At the time of writing, some of the frameworks we used don’t fully support such functionality. For example, the <strong class="source-inline">Dlib</strong> library can export neural networks in XML format but can’t load them. The PyTorch C++ API lacks exporting functionality, but it can load and evaluate model architectures that have been exported from the Python API with its TorchScript functionality. However, the <strong class="source-inline">pytorch</strong> library does provide access to the library API, which allows us to load and evaluate models saved in the ONNX format from C++. However, note that you can export a model into the ONNX format from the PyTorch Python API that was previously exported into TorchScript and loaded, <span class="No-Break">for example.</span></p>
			<p>We also briefly looked at the ONNX format and realized that it’s quite a popular format for sharing models among different ML frameworks. It supports almost all operations and objects that are used to serialize complex neural network models effectively. At the time of writing, it’s supported by all popular ML frameworks, including TensorFlow, PyTorch, MXNet, and others. Also, Microsoft provides the ONNX runtime implementation, which allows us to run the ONNX model’s inference without having to depend on any <span class="No-Break">other frameworks.</span></p>
			<p>At the end of this chapter, we developed a C++ application that can be used to run inference on the ResNet-50 model, which was trained and exported in ONNX format. This application was made with the onnxruntime C++ API so that we could load the model and evaluate it on the loaded image <span class="No-Break">for classification.</span></p>
			<p>In the next chapter, we’ll discuss h<a id="_idTextAnchor687"/>ow to deploy ML models that have been developed with C++ libraries to <span class="No-Break">mobile devices.</span></p>
			<h1 id="_idParaDest-261"><a id="_idTextAnchor688"/>Further reading</h1>
			<ul>
				<li>Dlib <span class="No-Break">documentation: </span><a href="http://dlib.net/"><span class="No-Break">http://Dlib.net/</span></a></li>
				<li>PyTorch C++ <span class="No-Break">API: </span><a href="https://pytorch.org/cppdocs/"><span class="No-Break">https://pytorch.org/cppdocs/</span></a></li>
				<li>ONNX official <span class="No-Break">page: </span><a href="https://onnx.ai/"><span class="No-Break">https://onnx.ai/</span></a></li>
				<li>ONNX Model <span class="No-Break">Zoo: </span><a href="https://github.com/onnx/models"><span class="No-Break">https://github.com/onnx/models</span></a></li>
				<li>ONNX ResNet models for image <span class="No-Break">classification: </span><a href="https://github.com/onnx/models/blob/main/validated/vision/classification/resnet"><span class="No-Break">https://github.com/onnx/models/blob/main/validated/vision/classification/resnet</span></a></li>
				<li><strong class="source-inline">onnxruntime</strong> C++ <span class="No-Break">examples: </span><a href="https://github.com/microsoft/onnxruntime-inference-examples/tree/main/c_cxx"><span class="No-Break">https://github.com/microsoft/onnxruntime-inference-examples/tree/main/c_cxx</span></a></li>
				<li>Flashlight <span class="No-Break">documentation: </span><a href="https://fl.readthedocs.io/en/stable/index.html"><span class="No-Break">https://fl.readthedocs.io/en/stable/index.html</span></a></li>
				<li>mlpack <span class="No-Break">documentation: </span><a href="https://rcppmlpack.github.io/mlpack-doxygen/"><span class="No-Break">https://rcppmlpack.github.io/mlpack-doxygen/</span></a></li>
			</ul>
		</div>
	</body></html>