- en: '*Chapter 7*: Detecting Pedestrians and Traffic Lights'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations on covering deep learning and progressing to this new section!
    Now that you know the basics of how to build and tune neural networks, it is time
    to move toward more advanced topics.
  prefs: []
  type: TYPE_NORMAL
- en: If you remember, in [*Chapter 1*](B16322_01_Final_NM_ePUB.xhtml#_idTextAnchor017),
    *OpenCV Basics and Camera Calibration*, we already detected pedestrians using
    OpenCV. In this chapter, we will learn how to detect objects using a very powerful
    neural network called **Single Shot MultiBox Detector** (**SSD**), and we will
    use it to detect not only pedestrians but also vehicles and traffic lights. In
    addition, we will train a neural network to detect the color of the traffic lights
    using transfer learning, a powerful technique that can help you achieve good results
    using a relatively small dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Detecting pedestrians, vehicles, and traffic lights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collecting images with CARLA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object detection with **Single Shot MultiBox Detector** (**SSD**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting the color of a traffic lights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding transfer learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ideas behind Inception
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recognizing traffic lights and their colors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To be able to use the code explained in this chapter, you need to have installed
    the following tools and modules:'
  prefs: []
  type: TYPE_NORMAL
- en: The Carla simulator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3.7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The NumPy module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The TensorFlow module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Keras module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The OpenCV-Python module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A GPU (recommended)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code for this chapter can be found at [https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter7](https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter7).
  prefs: []
  type: TYPE_NORMAL
- en: 'The Code in Action videos for this chapter can be found here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://bit.ly/3o8C79Q](https://bit.ly/3o8C79Q)'
  prefs: []
  type: TYPE_NORMAL
- en: Detecting pedestrians, vehicles, and traffic lights with SSD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When a self-driving car is on a road, it surely needs to know where the lanes
    are and detect obstacles (including people!) that can be present on the road,
    and it also needs to detect traffic signs and traffic lights.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will take a big step forward, as we will learn how to detect
    pedestrians, vehicles, and traffic lights, including the traffic light colors.
    We will use Carla to generate the images that we need.
  prefs: []
  type: TYPE_NORMAL
- en: 'Solving our task is a two-step process:'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, we will detect vehicles, pedestrians, and traffic lights (no color
    information), where we will use a pre-trained neural network called SSD.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we will detect the color of the traffic lights, where we will need to
    train a neural network starting from a pre-trained neural network called **Inception
    v3**, using a technique called transfer learning, and we will also need to collect
    a small dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So, let's begin by using Carla to collect the images.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting some images with Carla
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need some pictures of a street, with pedestrians, vehicles, and traffic lights.
    We can use Carla for that, but this time, we will discuss in further detail how
    to collect the dataset with Carla. You can find Carla at [https://carla.org/](https://carla.org/).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the binaries for Linux and Windows on the Carla GitHub page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/carla-simulator/carla/releases](https://github.com/carla-simulator/carla/releases)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The installation instruction can be found on the Carla website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://carla.readthedocs.io/en/latest/start_quickstart/](https://carla.readthedocs.io/en/latest/start_quickstart/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are using Linux, Carla starts with the `CarlaUE4.sh` command, while
    on Windows, it is called `CarlaUE4.exe`. We will just call it `CarlaUE4`. You
    can run it without arguments, or you could manually set the resolution, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In Carla, you can move around the track using some keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '*W*: Forward'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*S*: Backward'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A*: Left, sideways'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*D*: Right, sideways'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, in Carla, you can use the mouse, pressing the left mouse button
    and moving the cursor to change the angle of the view and to move along other
    angles.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Carla – default track](img/Figure_7.1_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – Carla – default track
  prefs: []
  type: TYPE_NORMAL
- en: While the server is sometimes useful, you probably want to run some of the files
    present in `PythonAPI\util` and `PythonAPI\examples`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this task, we are going to change track, using `Town01`. You can do this
    using the `PythonAPI\util\config.py` file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'You should now see a different track:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – The Town01 track](img/Figure_7.2_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – The Town01 track
  prefs: []
  type: TYPE_NORMAL
- en: 'Your city is empty, so we need to add some vehicles and some pedestrians. We
    can do this using `PythonAPI\examples\spawn_npc.py`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The `-w` parameter specifies the number of walkers, and `–n` the number of
    vehicles, that you want to create. Now, you should see some action:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – The Town01 track with vehicles and pedestrians](img/Figure_7.3_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – The Town01 track with vehicles and pedestrians
  prefs: []
  type: TYPE_NORMAL
- en: Much better.
  prefs: []
  type: TYPE_NORMAL
- en: Carla is intended to run as a server to which you can connect multiple clients,
    which should allow more interesting simulations.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you run Carla, it starts a server. You can go around a bit using the server,
    but most likely, you will want to run a client as it can provide much more functionality.
    If you run a client, you will have two windows with Carla, which are expected:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run a client using `PythonAPI\examples\manual_control.py`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You might see something like the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.4 – The Town01 track using manual_control.py](img/Figure_7.4_B16322.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 7.4 – The Town01 track using manual_control.py
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can see a lot of statistics on the left, and you can toggle them using the
    *F1* key. You will notice that now you have a vehicle, and you can change it with
    the Backspace key.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can move with the same keys as before, but this time, the behavior is more
    useful and realistic, as there is some physical simulation. You can also use the
    arrow keys to move.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can use the *Tab* key to change camera, and the *C* key changes the weather,
    as we can see in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.5 – The Town01 track; hard rain at noon and clear sky at sunset](img/Figure_7.5_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – The Town01 track; hard rain at noon and clear sky at sunset
  prefs: []
  type: TYPE_NORMAL
- en: 'Carla has many sensors, one of which is the RGB camera, and you can switch
    between them using `` ` ``, the backtick key. Now, refer to the following screenshots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – The Town01 track – left: depth (raw), right: semantic segmentation](img/Figure_7.6_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.6 – The Town01 track – left: depth (raw), right: semantic segmentation'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding screenshots show a couple of very interesting sensors:'
  prefs: []
  type: TYPE_NORMAL
- en: The depth sensor, which provides the distance from the camera for each pixel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The semantic segmentation sensor, which classifies every object using a different
    color
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At the time of writing, the full list of camera sensors is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Camera RGB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Camera depth (raw)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Camera depth (grayscale)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Camera depth (logarithmic grayscale)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Camera semantic segmentation (CityScapes Palette)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lidar (raycast)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic Vision Sensor** (**DVS**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Camera RGB distorted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lidar is a sensor that detects the distance of an object using a laser; the
    DVS, also called the neuromorphic camera, is a camera that records local changes
    of brightness, overcoming some limitations of RGB cameras. Camera RGB distorted
    is just an RGB camera simulating the effects of a lens, and of course, you can
    customize the distortion as needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the Lidar camera view:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – The Lidar camera view](img/Figure_7.7_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – The Lidar camera view
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the output of DVS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – DVS](img/Figure_7.8_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – DVS
  prefs: []
  type: TYPE_NORMAL
- en: You can now just go around and collect some images from the RGB camera, or you
    can use the ones in the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have some images, it is time to detect pedestrians, vehicles, and
    traffic lights, using a pre-trained network called SSD.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding SSD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In previous chapters, we created a classifier, a neural network able to tell
    what is present in a picture from a predefined set of options. Later in this chapter,
    we will see a pre-trained neural network that can classify images in a very precise
    way.
  prefs: []
  type: TYPE_NORMAL
- en: SSD stands out compared to many neural networks, as it is able to detect multiple
    objects in the same picture. The details of SSD are a bit complicated, and if
    you are interested, you can check the *Further reading* section for some inspiration.
  prefs: []
  type: TYPE_NORMAL
- en: Not only can SSD detect multiple objects, but it can also output the area where
    the object is present! Internally, this is done by checking 8,732 positions at
    different aspect ratios. SSD is also fast enough that with a good GPU, it can
    be used to analyze videos in real time.
  prefs: []
  type: TYPE_NORMAL
- en: But where can we find SSD? The answer is the TensorFlow detection model zoo.
    Let's see what this is.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering the TensorFlow detection model zoo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The TensorFlow detection model zoo is a useful collection of pre-trained neural
    networks, which supports several architectures trained on several datasets. We
    are interested in SSD, so we will focus on that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of the datasets supported by the model zoo, we are interested in COCO. **COCO**
    is the Microsoft **Common Objects in Context** dataset, a collection of 2,500,000
    (2.5 million) images, classified by type. You can find a link with the 90 labels
    of COCO in the *Further reading* section, but we are interested in the following
    ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '`1`: `person`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`3`: `car`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`6`: `bus`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`8`: `truck`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`10`: `traffic light`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You might also be interested in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`2`: `bicycle`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`4`: `motorcycle`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`13`: `stop sign`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Notably, SSD trained on COCO is available on several versions, using different
    neural networks as the backend to reach the desired speed/precision ratio. Refer
    to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – The TensorFlow detection model zoo of SSDs trained on COCO](img/Figure_7.9_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – The TensorFlow detection model zoo of SSDs trained on COCO
  prefs: []
  type: TYPE_NORMAL
- en: Here, the `mAP` column is the mean average precision, so the higher the better.
    MobileNet is a neural network developed to perform particularly well on mobiles
    and embedded devices, and thanks to its performance, it is a classical choice
    for SSD when you need to perform inference in real time.
  prefs: []
  type: TYPE_NORMAL
- en: To detect the objects on the road, we will use an SSD built using **ResNet50**
    as a backbone, a neural network with 50 layers developed by Microsoft Research
    Asia. A characteristic of ResNet is the presence of **skip connections**, shortcuts
    that can connect a layer to another one, skipping some layers in the middle. This
    helps in solving the **vanishing gradient problem**. With deep neural networks,
    the gradient during training can become so small that the network can basically
    stop learning.
  prefs: []
  type: TYPE_NORMAL
- en: But how do we use `ssd_resnet_50_fpn_coco`, our selected model? Let's check
    it out!
  prefs: []
  type: TYPE_NORMAL
- en: Downloading and loading SSD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'On the model zoo page, if you click on **ssd_resnet_50_fpn_coco**, you get
    a URL that Keras needs to download the model from; at the time of writing, the
    URL is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The full name of the model is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To load the model, you can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If this is the first time that you have run this code, it will take more time
    because Keras will download the model and save it on your hard drive.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have loaded the model, is time to use it to detect some objects.
  prefs: []
  type: TYPE_NORMAL
- en: Running SSD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Running SSD requires just a few lines of code. You can load the image (with
    a resolution of 299x299) with OpenCV, then you need to convert the image into
    a tensor, a type of multi-dimensional array used by TensorFlow that is similar
    to NumPy arrays. Refer to the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Please note that we feed the network with an `RGB` image, not `BGR`. You might
    remember from the previous chapters that OpenCV uses pictures in `BGR` format,
    so we need to pay attention to the order of the channels.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, running SSD is quite easy, but the output is relatively complex,
    and it needs some code to be converted into a useful and more compact form. The
    `output` variable is a Python dictionary, but the values that it contains are
    tensors, so you need to convert them.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, printing `output[''num_detections'']`, which contains the number
    of predictions (for example, objects found in the image), would give the following
    as a result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: For the conversion, we can use `int()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'All the other tensors are arrays, and they can be converted using their `numpy()`
    function. So then, your code might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'There are still the following two things to fix:'
  prefs: []
  type: TYPE_NORMAL
- en: The detection classes are floating point, while, as they are our labels, they
    should be integers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The coordinates of the boxes are in percentage form.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can fix these problems with just a few lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s apply SSD to this image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10 – Image from Town01](img/Figure_7.10_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – Image from Town01
  prefs: []
  type: TYPE_NORMAL
- en: 'We get this output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what the code means:'
  prefs: []
  type: TYPE_NORMAL
- en: '`detection_scores`: A higher score means higher confidence in the prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`detection_classes`: The predicted labels – in this case, truck (`8`), traffic
    light (`10`), bus (`6`), and car (`3`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`detection_boxes`: The original boxes, with coordinates in percentage form.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_detections`: The number of predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`boxes`: The boxes with coordinates converted to the resolution of the original
    image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please notice that three predictions are basically in the same area, and they
    are ordered by score. We will need to fix this overlapping.
  prefs: []
  type: TYPE_NORMAL
- en: To be able to better see what has been detected, we will now annotate the image.
  prefs: []
  type: TYPE_NORMAL
- en: Annotating the image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To properly annotate the image, we need to perform the following operations:'
  prefs: []
  type: TYPE_NORMAL
- en: Consider only the labels that are interesting to us.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove the overlapping of labels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Draw a rectangle on each prediction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write the label and its score.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To remove the overlapping labels, it's enough to compare them, and if the center
    of the boxes is similar, we will keep only the label with a higher score.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.11 – Image from Town01, annotated with SSD only](img/Figure_7.11_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 – Image from Town01, annotated with SSD only
  prefs: []
  type: TYPE_NORMAL
- en: It's a good starting point, even if other images are not recognized so well.
    The vehicle has been recognized as a truck, which is not completely accurate,
    but we don't really care about that.
  prefs: []
  type: TYPE_NORMAL
- en: The main problem is that we know that there is a traffic light, but we don't
    know the color.Unfortunately, SSD cannot help us; we need to do it by ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will develop a neural network that is able to detect
    the color of the traffic light, using a technique called transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting the color of a traffic light
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In principle, we could try to detect the color of a traffic light using some
    computer vision technique – for example, checking the red and the green channel
    could be a starting point. In addition, verifying the luminosity of the bottom
    and upper part of the crossing light should help. This could work, even if some
    crossing lights can be problematic.
  prefs: []
  type: TYPE_NORMAL
- en: However, we will use deep learning, as this task is well-suited to exploring
    more advanced techniques. We will also go the extra mile to use a small dataset,
    even though it would be easy for us to create a big dataset; the reason being
    that we don't always have the luxury of easily increasing the size of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be able to detect the color of a traffic light, we need to complete three
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Collect a dataset of crossing lights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a neural network to recognize the color.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the network with SSD to get the final result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is a traffic light dataset that you could use, the *Bosch Small Traffic
    Lights* dataset; however, we will generate our own dataset using Carla. Let's
    see how.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a traffic light dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are going to create a dataset using Carla. In principle, it could be as big
    as we want. The bigger the better, but a big dataset would make training slower
    and, of course, it will require some more time to be created. In our case, as
    the task is simple, we will create a relatively small dataset of a few hundreds
    of images. We will explore **transfer learning** later, a technique that can be
    used when the dataset is not particularly big.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: On GitHub, you can find the dataset that I created for this task, but if you
    have some time, collecting the dataset by yourself could be a nice exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating this dataset is a three-step task:'
  prefs: []
  type: TYPE_NORMAL
- en: Collect images of streets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find and crop all the traffic lights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Classify the traffic lights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first task of collecting images is very simple. Just start Carla using `manual_control.py`
    and press the *R* key. Carla will start recording and it will stop after you press
    *R* again.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider that we want to record four types of images:'
  prefs: []
  type: TYPE_NORMAL
- en: Red lights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yellow lights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Green lights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The back of traffic lights (negative samples)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reason why we want to collect the back of the traffic light is that SSD
    is recognizing it as a traffic light, but we have no use for it, so we don't want
    to use it. These are **negative samples**, and they could also include pieces
    of road or buildings or anything that SSD wrongly classifies as a traffic light.
  prefs: []
  type: TYPE_NORMAL
- en: So, while you record your images, please try to get enough samples for each
    category.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are some example of images that you might want to collect:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.12 – Town01 – left: red light, right: green light](img/Figure_7.12_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.12 – Town01 – left: red light, right: green light'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second step is applying SSD and extracting the image of the crossing light.
    It''s quite simple; refer to the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the previous code, assuming that the `out` variable contains the result of
    running the SSD, calling `model(input_tensor)`, and `idx` contains the current
    detection among the predictions, you just need to select the detections containing
    traffic lights and crop them using the coordinates that we computed earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'I ended up with 291 detections, with images like these:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.13 – Town01, from the left: small red light, small green light,
    green light, yellow light, back of a traffic light, piece of a building wrongly
    classified as a traffic light](img/Figure_7.13_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.13 – Town01, from the left: small red light, small green light, green
    light, yellow light, back of a traffic light, piece of a building wrongly classified
    as a traffic light'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the images have different resolutions and ratios, and that's
    perfectly fine.
  prefs: []
  type: TYPE_NORMAL
- en: There are also some images that are completely unrelated, such as a piece of
    a building, and these are excellent negative samples for things that are not a
    traffic light because SSD misclassified them, so it is also a way to improve the
    output of SSD.
  prefs: []
  type: TYPE_NORMAL
- en: The last step is just classifying the images. With a few hundred pictures of
    this type, it takes only a few minutes. You can create a directory for each label
    and move the appropriate images there.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations, now you have a custom dataset to detect the color of traffic
    lights.
  prefs: []
  type: TYPE_NORMAL
- en: As you know, the dataset is small, so, as we already said, we are going to use
    transfer learning. The next section will explain what it is.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding transfer learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transfer learning is a very appropriate name. From a conceptual point of view,
    indeed, it is about taking what a neural network learned on one task and transferring
    this knowledge to a different but related task.
  prefs: []
  type: TYPE_NORMAL
- en: There are several approaches to transfer learning; we will discuss two, and
    we will choose one of them and use it to detect the colors of the traffic lights.
    In both cases, the starting point is a neural network that has been pre-trained
    on a similar task – for example, a classification of images. We will talk more
    about this in the next section, *Getting to know ImageNet*. We are focusing on
    a **Convolutional Neural Network** (**CNN**) used as a classifier, as this is
    what we need to recognize the color of the traffic lights.
  prefs: []
  type: TYPE_NORMAL
- en: The first approach is to load the pre-trained neural network, adapting the number
    of outputs to the new problem (either replacing some or all the dense layers or
    sometimes just adding an additional dense layer), and basically keep training
    it on the new dataset. You might need to use a small learning rate. This approach
    might work if the number of samples in the new dataset is smaller than the dataset
    used for the original training, but still significantly big. For example, the
    size of our custom dataset could be 10% of the site of the original dataset. One
    drawback is that training might take a long time, as typically you are training
    a relatively big network.
  prefs: []
  type: TYPE_NORMAL
- en: A second approach, the one that we are going to follow, is similar to the first
    one, but you are going to freeze all the convolutional layers, meaning that their
    parameters are fixed and will not change during training. This has the advantage
    that training is much faster as you don't need to train the convolutional layers.
    The idea here is that the convolutional layers have been trained on a huge dataset,
    and they are able to detect so many features that it is going to be fine for the
    new task also, while the real classifier, composed of the dense layers, can be
    replaced and trained from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Intermediate approaches are also possible, where you train some of the convolutional
    layers, but typically keep at least the first layers frozen.
  prefs: []
  type: TYPE_NORMAL
- en: Before seeing how to do transfer learning with Keras, let's think a bit more
    about what we just discussed. A key assumption is that this hypothetical network
    that we want to learn from has been trained on a huge dataset, where the network
    can learn to recognize many features and patterns. Turns out that there is a very
    big dataset that fits the bill—ImageNet. Let's talk a bit more about it.
  prefs: []
  type: TYPE_NORMAL
- en: Getting to know ImageNet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ImageNet is a huge dataset, and at the time of writing, it is composed of 14,197,122
    (over 14 million) images! In reality, it does not provide the images, but just
    the URLs to download the images. Those images are classified on 27 categories
    and a total of 21,841 subcategories! These subcategories, called synsets, are
    based on a classification hierarchy called **WordNet**.
  prefs: []
  type: TYPE_NORMAL
- en: 'ImageNet has been a very influential dataset, thanks also to a competition
    used to measure the advancements in computer vision: the **ImageNet Large-Scale
    Visual Recognition Challenge** (**ILSVRC**).'
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the main categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '`amphibian`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`animal`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`appliance`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bird`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`covering`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fabric`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fish`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`flower`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`food`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fruit`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fungus`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`furniture`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`geological formation`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`invertebrate`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mammal`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`musical instrument`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`plant`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reptile`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sport`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`structure`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tool`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tree`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`utensil`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vegetable`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vehicle`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`person`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of subcategories is impressively high; as an example, the `tree`
    category has 993 subcategories, covered by more than half a million of pictures!
  prefs: []
  type: TYPE_NORMAL
- en: Surely, a neural network performing well on this dataset will be very good at
    recognizing patterns on many types of images, and it might also have a quite big
    capacity. So, yes, it will overfit your dataset, but as we know how to deal with
    overfitting, we will keep an eye on this problem, but not get too worried about
    it.
  prefs: []
  type: TYPE_NORMAL
- en: As so much research has been devoted to performing well on ImageNet, it's not
    surprising that many of the most influential neural networks have been trained
    on it.
  prefs: []
  type: TYPE_NORMAL
- en: One, in particular, stood out when it first appeared in 2012—AlexNet. Let's
    see why.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering AlexNet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When AlexNet was released in 2012, it was over 10% more precise than the best
    neural network of the time! Clearly, these solutions have been studied extensively,
    and some are now very common.
  prefs: []
  type: TYPE_NORMAL
- en: 'AlexNet introduced several ground-breaking innovations:'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-GPU training where AlexNet was trained half on one GPU and half on another
    one, allowing the model to be twice the size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReLU activation instead of Tanh, which apparently allowed the training to be
    six times faster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overlapping pooling added, where AlexNet used max-pooling of 3x3, but the pooling
    area is moved only by 2x2, meaning that there is an overlap between pools. According
    to the original paper, this gave a 0.3–0.4% improvement in accuracy. In Keras,
    you can achieve similar overlapping pooling using `MaxPooling2D(pool_size=(3,3),
    strides=(2,2))`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With over 60 million parameters, AlexNet was pretty big, so to reduce overfitting,
    it made extensive use of data augmentation and dropout.
  prefs: []
  type: TYPE_NORMAL
- en: 'While AlexNet was state-of-the-art and ground-breaking in 2012, by today''s
    standards, it is quite inefficient. In the next section, we will discuss a neural
    network that can achieve substantially better accuracy than AlexNet using only
    one-tenth of the parameters: **Inception**.'
  prefs: []
  type: TYPE_NORMAL
- en: The ideas behind Inception
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Having a huge dataset like ImageNet at our disposal is great, but it would be
    much easier to have a neural network already trained with it. It turns out that
    Keras provides several of them. One is ResNet, which we have already encountered.
    Another one, which is very influential and with great innovations, is Inception.
    Let's talk a bit about it.
  prefs: []
  type: TYPE_NORMAL
- en: Inception is a family of neural networks, meaning that there are several of
    them, refining the initial concept. Inception was designed by Google, and the
    version that participated in the ILSVRC 2014 (ImageNet) competition and won is
    called **GoogLeNet**, in honor of the LeNet architecture.
  prefs: []
  type: TYPE_NORMAL
- en: If you are wondering whether Inception took its name from the famous movie *Inception*,
    yes it did, because they wanted to go deeper! And Inception is a deep network,
    with a version called `InceptionResNetV2` arriving at a staggering 572 layers!
    This, of course, is if we count every layer, including the activations. We are
    going to use Inception v3, which has *only* 159 layers.
  prefs: []
  type: TYPE_NORMAL
- en: We will focus on Inception v1, because it is easier, and we will briefly discuss
    some improvements added later, as they can be a source of inspiration for you.
  prefs: []
  type: TYPE_NORMAL
- en: A key observation made by Google is that given the variety of positions where
    a subject can be on a picture, it is difficult to know in advance which kernel
    size of a convolutional layer would be the best, so they added 1x1, 3x3, and 5x5
    convolutions in parallel to cover the main cases, plus max pooling, as it is often
    useful, and concatenated the results. One advantage of doing it in parallel is
    that the network does not get too deep, which keeps the training easier.
  prefs: []
  type: TYPE_NORMAL
- en: 'What we just described is the **naïve** Inception block:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.14 – Naïve Inception block](img/Figure_7.14_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.14 – Naïve Inception block
  prefs: []
  type: TYPE_NORMAL
- en: You might have noticed a 1x1 convolution. What's that? Just multiplying a channel
    by a number? Not exactly. The 1x1 convolution is very cheap to perform, as there
    is only 1 multiplication instead of 9 (as in 3x3 convolutions) or 25 (as in 5x5
    convolutions), and it can be used to change the number of filters. In addition,
    you can add a ReLU, introducing a non-linear operation that increases the complexity
    of the functions that the network can learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'This module is called naïve because it is too computationally expensive. As
    the number of channels grows, the 3x3 and 5x5 convolutions become slow. The solution
    is to put 1x1 convolutions in front of them, to reduce the number of channels
    where the more expensive convolutions need to operate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.15 – Inception block including dimension reductions](img/Figure_7.15_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.15 – Inception block including dimension reductions
  prefs: []
  type: TYPE_NORMAL
- en: The key to understanding this block is to remember that the 1x1 convolutions
    are used to reduce the channels and improve performance significantly.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the first Inception block in GoogLeNet has an input of 192 channels,
    and the 5x5 convolution would create 32 channels, so the number of multiplications
    would be proportional to 25 x 32 x 192 = 153,600.
  prefs: []
  type: TYPE_NORMAL
- en: They added a 1x1 convolution with an output of 16 filters, so the number of
    multiplications would be proportional to 16 x 192 + 25 x 32 x 16 = 3,072 + 12,800
    = 15,872\. Almost a 10x reduction. Not bad!
  prefs: []
  type: TYPE_NORMAL
- en: One more thing. For the concatenation to work, all the convolutions need to
    have an output of the same size, which means that they need the padding, which
    keeps the same resolution of the input image. And what about max pooling? It also
    needs to have an output of the same size as the convolutions, so even if it finds
    the maximum in a 3x3 grid, it cannot reduce the size.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Keras, this means it would be something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The `strides` parameter indicates how many pixels to move after calculating
    the maximum. By default, it's set at the same value of `pool_size`, which in our
    example would reduce the size 3 times. Setting it to `(1, 1)` with the same padding
    has the effect of not changing the size. The Conv2D layers also have a `strides`
    parameter, which can be used to reduce the size of the output; however, usually
    it is more effective to do so using a max pooling layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inception v2 introduced some optimizations, among them the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A 5x5 convolution is similar to two stacked 3x3 convolutions, but slower, so
    they refactored it with 3x3 convolutions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A 3x3 convolution is equivalent to a 1x3 convolution followed by a 3x1 convolution,
    but using two convolutions is 33% faster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inception v3 introduced the following optimizations:'
  prefs: []
  type: TYPE_NORMAL
- en: Factorized 7x7 convolutions, created using several smaller and faster convolutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some batch normalization layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inception-ResNet introduced the residual connections typical of ResNet, to skip
    some layers.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have a better understanding of the concepts behind Inception, let's
    see how to use it in Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Using Inception for image classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Loading Inception in Keras could not be simpler, as we can see here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'As Inception can tell us the content of an image, let''s try it with the test
    image that we used at the beginning of the book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'It is indeed right: our image depicts a sea lion from the Galapagos Islands:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.16 – Recognized by Inception as a sea lion with 0.99184495 confidence](img/Figure_7.16_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.16 – Recognized by Inception as a sea lion with 0.99184495 confidence
  prefs: []
  type: TYPE_NORMAL
- en: But we want to use Inception for transfer learning, not for image classification,
    so we need to use it in a different way. Let's see how.
  prefs: []
  type: TYPE_NORMAL
- en: Using Inception for transfer learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The loading for transfer learning is a bit different, because we need to remove
    the classifier on top of Inception, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: With `input_shape`, we use the original size of Inception, but you could use
    a different shape, as long as it has 3 channels and the resolution is at least
    75x75.
  prefs: []
  type: TYPE_NORMAL
- en: The important parameter is `include_top`, as setting it to `False` would remove
    the top part of Inception—the classifier with the dense filters—which means that
    the network will be ready for transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now create a neural network that is based on Inception but can be modified
    by us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can add a classifier on top of it, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We added some dropout, as we expect Inception to overfit quite a lot on our
    dataset. But please pay attention to the `GlobalAveragePooling2D`. What it does
    is compute the average of the channels.
  prefs: []
  type: TYPE_NORMAL
- en: We could use `Flatten`, but as Inception outputs 2,048 convolutional channels
    of 8x8, and we are using a dense layer with 1,024 neurons, the number of parameters
    would be huge—134,217,728! Using `GlobalAveragePooling2D`, we need only 2,097,152
    parameters. Even counting the parameters of Inception, the saving is quite significant—24,427,812
    parameters instead of 156,548,388.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is one more thing that we need to do: freeze the layers of Inception
    that we don''t want to train. In this case, we want to freeze all of them, but
    this might not always be the case. This is how you can freeze them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s check how our network looks. Inception is too big, so I will only show
    the data of the parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Please note that `summary()` will actually print two summaries: one for Inception
    and one for our network; this is the output of the first summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Model: "sequential_1"`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the first layer is Inception. In the second summary, you also
    have a confirmation that Inception has the layers frozen, because we have more
    than 21 million non-trainable parameters, matching exactly the total number of
    parameters of Inception.
  prefs: []
  type: TYPE_NORMAL
- en: 'To reduce the overfitting and to compensate for the small dataset, we will
    use data augmentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: I only applied a small rotation because traffic lights are usually pretty straight,
    and I also added only a small width shift because the traffic lights are detected
    by a neural network (SSD), so the cut tends to be very consistent. I also added
    a higher height shift because I saw that SSD sometimes wrongly cut the traffic
    lights, removing one-third of it.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the network is ready, we just need to feed it our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Feeding our dataset to Inception
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s assume that you loaded your dataset in two variables: `images` and `labels`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inception needs some preprocessing, to map the values of the images to the
    `[-1, +1]` range. Keras has a function that takes care of this, `preprocess_input()`.
    Please take care to import it from the `keras.applications.inception_v3` module,
    because there are other functions with the same name and different behavior in
    other modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We need to divide the dataset into training and validation, which is easy, but
    we also need to randomize the order, to be sure that the split is meaningful;
    for example, my code loads all the images with the same label, so a split without
    randomization would put only one or two labels in validation, and one of them
    might even not be present in training.
  prefs: []
  type: TYPE_NORMAL
- en: 'NumPy has a very handy function to generate new index positions, `permutation()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you can use **for comprehension**, a feature of Python, to change the
    order in your lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: If your labels are numeric, you can use `to_categorical()` to convert them into
    one-hot encoding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, it''s just a matter of slicing. We will use 20% of the samples for validation,
    so the code can be like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Now, you can train the network as usual. Let's see how it performs!
  prefs: []
  type: TYPE_NORMAL
- en: Performance with transfer learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The performance of the model is very good:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Yes, 100% accuracy and validation accuracy! Not bad, not bad. Actually, it's
    very rewarding. However, the dataset was very simple, so it was fair to expect
    a very good result.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the graph of the losses:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.17 – Losses with transfer learning from Inception](img/Figure_7.17_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.17 – Losses with transfer learning from Inception
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that despite the great results, the network does not work too
    well on my test images. Probably, it overfits, and I suspect that as the images
    are normally smaller than the native resolution of Inception, the network might
    experience patterns that are a result of interpolation instead of true patterns
    in the images, and maybe get confused by them, but that's just my theory.
  prefs: []
  type: TYPE_NORMAL
- en: To get good results, we need to try harder.
  prefs: []
  type: TYPE_NORMAL
- en: Improving transfer learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can assume that the network is overfitting, and the standard response would
    be to increase the dataset. In this case, it would be easy to do that, but let's
    pretend that we cannot do it, so we can explore other options that can be useful
    to you in similar cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some very easy things that we can do to reduce overfitting:'
  prefs: []
  type: TYPE_NORMAL
- en: Increase the variety of the data augmentation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increase dropout.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Despite the fact that Inception is clearly able to process tasks much more
    complex than this, it is not optimized for this specific task, and it is also
    possible that it could benefit from a bigger classifier, so I will add a layer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the new data augmentation, after a few tests:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is the new model, with more dropout and an additional layer:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: I added a dropout after the global average pooling, to reduce overfitting, and
    I also added a batch normalization layer, which can also help to reduce overfitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, I added a dense layer, but I did not put a dropout on it, because I noticed
    the network had problems training with so much dropout.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Even if we don''t want to increase the dataset, we can still do something about
    it. Let''s look at the distribution of the classes:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is the result:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you see, the dataset has much more green than yellow or red and not many
    negative samples.
  prefs: []
  type: TYPE_NORMAL
- en: In general, it is not good to have imbalanced labels, and the network was indeed
    predicting more green lights than there were, because statistically, it is more
    rewarding to predict a green than any other label. To improve this situation,
    we can instruct Keras to customize the loss in a way that predicting a wrong red
    would be worse than predicting a wrong green, as this would have an effect similar
    to making the dataset balanced.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can do this with these two lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the loss penalty would be less for green (label 0) than for
    the other ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is how the network performs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Not much different than before, but this time, the network works better and
    nails all the traffic lights in my test images. This should be a reminder to not
    trust the validation accuracy completely unless you are sure that your validation
    dataset is excellent.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the graph of the losses:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.18 – Losses with transfer learning from Inception, improved](img/Figure_7.18_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.18 – Losses with transfer learning from Inception, improved
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a good network, it's time to complete our task, using the new
    network in combination with SSD, as detailed in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Recognizing traffic lights and their colors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are almost done. From the code using SSD, we just need to manage the traffic
    light in a different way. So, when the label is `10` (traffic light), we need
    to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Crop the area with the traffic light.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resize it to 299x299.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocess it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run it through our network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, we will get the prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: If you run the code of this chapter that is in GitHub, the label `0` is the
    green light, `1` is yellow, `2` is red, and `3` means that it is not a traffic
    light.
  prefs: []
  type: TYPE_NORMAL
- en: 'The whole process involves first detecting objects with SSD, and then using
    our network to detect the color of traffic lights, if any are present in the image,
    as explained in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.19 – A diagram showing how to use SSD and our network together](img/Figure_7.19_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.19 – A diagram showing how to use SSD and our network together
  prefs: []
  type: TYPE_NORMAL
- en: 'These are examples obtained running SSD followed by our network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.20 – Some detections with traffic lights](img/Figure_7.20_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.20 – Some detections with traffic lights
  prefs: []
  type: TYPE_NORMAL
- en: 'The colors of the traffic lights are now detected properly. There are some
    false detections: for example, in the preceding figure, the image on the right
    marks a person where there is a tree. Unfortunately, this can happen. In a video,
    we could require detection for a few frames before accepting it, always considering
    that in a real self-driving car, you cannot introduce high latency, because the
    car needs to react quickly to what''s happening on the street.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we focused on pre-trained neural networks, and how we can leverage
    them for our purposes. We combined two neural networks to detect pedestrians,
    vehicles, and traffic lights, including their color. We first discussed how to
    use Carla to collect images, and then we discovered SSD, a powerful neural network
    that stands out for its capacity to detect not only objects, but also their position
    in an image. We also saw the TensorFlow detection model zoo and how to use Keras
    to download the desired version of SSD, trained on a dataset called COCO.
  prefs: []
  type: TYPE_NORMAL
- en: In the second part of the chapter, we discussed a powerful technique called
    transfer learning, and we studied some of the solutions of a neural network called
    Inception, which we trained on our dataset using transfer learning, to be able
    to detect the colors of traffic lights. In the process, we also talked about ImageNet,
    and we saw how achieving 100% validation accuracy was misleading, and as a result,
    we had to reduce the overfitting to improve the real precision of the network.
    In the end, we succeeded in using the two networks together—one to detect pedestrians,
    vehicles, and traffic lights, and one to detect the color of the traffic lights.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to build knowledge about the road, it's time to move forward
    to the next task—driving! In the next chapter, we will literally sit in the driving
    seat (of Carla), and teach our neural network how to drive, using a technique
    called behavioral cloning, where our neural network will try to mimic our behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You should now be able to answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is SSD?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is Inception?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does it mean to freeze a layer?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can SSD detect the color of a traffic light?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is transfer learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you name some techniques to reduce overfitting?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you describe the idea behind the Inception block?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'SSD: [https://arxiv.org/abs/1512.02325](https://arxiv.org/abs/1512.02325)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TensorFlow model zoo: [https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'COCO labels: [https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_label_map.pbtxt](https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_label_map.pbtxt)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vanishing gradient problem: [https://en.wikipedia.org/wiki/Vanishing_gradient_problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Bosch Small Traffic Lights dataset: [https://hci.iwr.uni-heidelberg.de/content/bosch-small-traffic-lights-dataset](https://hci.iwr.uni-heidelberg.de/content/bosch-small-traffic-lights-dataset)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ImageNet: [http://www.image-net.org/](http://www.image-net.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inception paper: [https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43022.pdf](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43022.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AlexNet paper: [https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
