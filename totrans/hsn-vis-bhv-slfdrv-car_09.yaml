- en: '*Chapter 7*: Detecting Pedestrians and Traffic Lights'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 7 章*：检测行人和交通灯'
- en: Congratulations on covering deep learning and progressing to this new section!
    Now that you know the basics of how to build and tune neural networks, it is time
    to move toward more advanced topics.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜您完成了深度学习的学习，并进入了这个新的章节！现在您已经了解了如何构建和调整神经网络的基础知识，是时候转向更高级的主题了。
- en: If you remember, in [*Chapter 1*](B16322_01_Final_NM_ePUB.xhtml#_idTextAnchor017),
    *OpenCV Basics and Camera Calibration*, we already detected pedestrians using
    OpenCV. In this chapter, we will learn how to detect objects using a very powerful
    neural network called **Single Shot MultiBox Detector** (**SSD**), and we will
    use it to detect not only pedestrians but also vehicles and traffic lights. In
    addition, we will train a neural network to detect the color of the traffic lights
    using transfer learning, a powerful technique that can help you achieve good results
    using a relatively small dataset.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还记得，在 [*第 1 章*](B16322_01_Final_NM_ePUB.xhtml#_idTextAnchor017)，*OpenCV 基础和相机标定*，我们已经使用
    OpenCV 检测了行人。在本章中，我们将学习如何使用一个非常强大的神经网络——**单次多框检测器**（**SSD**）来检测对象，我们将使用它来检测行人、车辆和交通灯。此外，我们将通过迁移学习训练一个神经网络来检测交通灯的颜色，迁移学习是一种强大的技术，可以帮助您使用相对较小的数据集获得良好的结果。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Detecting pedestrians, vehicles, and traffic lights
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测行人和交通灯
- en: Collecting images with CARLA
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 CARLA 收集图像
- en: Object detection with **Single Shot MultiBox Detector** (**SSD**)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 **单次多框检测器**（**SSD**）进行目标检测
- en: Detecting the color of a traffic lights
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测交通灯的颜色
- en: Understanding transfer learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解迁移学习
- en: The ideas behind Inception
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Inception 的理念
- en: Recognizing traffic lights and their colors
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别交通灯及其颜色
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To be able to use the code explained in this chapter, you need to have installed
    the following tools and modules:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要能够使用本章中解释的代码，您需要安装以下工具和模块：
- en: The Carla simulator
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carla 模拟器
- en: Python 3.7
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 3.7
- en: The NumPy module
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy 模块
- en: The TensorFlow module
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 模块
- en: The Keras module
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras 模块
- en: The OpenCV-Python module
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenCV-Python 模块
- en: A GPU (recommended)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 GPU（推荐）
- en: The code for this chapter can be found at [https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter7](https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter7).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在 [https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter7](https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter7)
    找到。
- en: 'The Code in Action videos for this chapter can be found here:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的“代码实战”视频可以在以下位置找到：
- en: '[https://bit.ly/3o8C79Q](https://bit.ly/3o8C79Q)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://bit.ly/3o8C79Q](https://bit.ly/3o8C79Q)'
- en: Detecting pedestrians, vehicles, and traffic lights with SSD
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 SSD 检测行人、车辆和交通灯
- en: When a self-driving car is on a road, it surely needs to know where the lanes
    are and detect obstacles (including people!) that can be present on the road,
    and it also needs to detect traffic signs and traffic lights.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当自动驾驶汽车在道路上行驶时，它肯定需要知道车道在哪里，并检测可能存在于道路上的障碍物（包括人！），它还需要检测交通标志和交通灯。
- en: In this chapter, we will take a big step forward, as we will learn how to detect
    pedestrians, vehicles, and traffic lights, including the traffic light colors.
    We will use Carla to generate the images that we need.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将迈出重要的一步，因为我们将学习如何检测行人、车辆和交通灯，包括交通灯的颜色。我们将使用 Carla 生成所需的图像。
- en: 'Solving our task is a two-step process:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 解决我们的任务是两步过程：
- en: Firstly, we will detect vehicles, pedestrians, and traffic lights (no color
    information), where we will use a pre-trained neural network called SSD.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将检测车辆、行人和交通灯（无颜色信息），我们将使用一个名为 SSD 的预训练神经网络。
- en: Then, we will detect the color of the traffic lights, where we will need to
    train a neural network starting from a pre-trained neural network called **Inception
    v3**, using a technique called transfer learning, and we will also need to collect
    a small dataset.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将检测交通灯的颜色，这需要我们从名为 **Inception v3** 的预训练神经网络开始训练一个神经网络，使用迁移学习技术，并且我们还需要收集一个小数据集。
- en: So, let's begin by using Carla to collect the images.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们首先使用 Carla 来收集图像。
- en: Collecting some images with Carla
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Carla 收集一些图像
- en: We need some pictures of a street, with pedestrians, vehicles, and traffic lights.
    We can use Carla for that, but this time, we will discuss in further detail how
    to collect the dataset with Carla. You can find Carla at [https://carla.org/](https://carla.org/).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一些带有行人、车辆和交通灯的街道照片。我们可以使用 Carla 来做这件事，但这次，我们将更详细地讨论如何使用 Carla 收集数据集。您可以在
    [https://carla.org/](https://carla.org/) 找到 Carla。
- en: 'You can find the binaries for Linux and Windows on the Carla GitHub page:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 Carla GitHub 页面上找到 Linux 和 Windows 的二进制文件：
- en: '[https://github.com/carla-simulator/carla/releases](https://github.com/carla-simulator/carla/releases)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/carla-simulator/carla/releases](https://github.com/carla-simulator/carla/releases)'
- en: 'The installation instruction can be found on the Carla website:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 安装说明可以在 Carla 网站上找到：
- en: '[https://carla.readthedocs.io/en/latest/start_quickstart/](https://carla.readthedocs.io/en/latest/start_quickstart/)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://carla.readthedocs.io/en/latest/start_quickstart/](https://carla.readthedocs.io/en/latest/start_quickstart/)'
- en: 'If you are using Linux, Carla starts with the `CarlaUE4.sh` command, while
    on Windows, it is called `CarlaUE4.exe`. We will just call it `CarlaUE4`. You
    can run it without arguments, or you could manually set the resolution, as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用 Linux，则使用 `CarlaUE4.sh` 命令启动 Carla，而在 Windows 上，它被称为 `CarlaUE4.exe`。我们将其称为
    `CarlaUE4`。您可以在没有参数的情况下运行它，或者您可以手动设置分辨率，如下所示：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In Carla, you can move around the track using some keys:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Carla 中，您可以使用一些键在轨道周围移动：
- en: '*W*: Forward'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*W*：向前'
- en: '*S*: Backward'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*S*：向后'
- en: '*A*: Left, sideways'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*A*：向左，侧向'
- en: '*D*: Right, sideways'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*D*：向右，侧向'
- en: In addition, in Carla, you can use the mouse, pressing the left mouse button
    and moving the cursor to change the angle of the view and to move along other
    angles.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在 Carla 中，您可以使用鼠标，按住左键并移动光标来改变视角的角度并沿其他角度移动。
- en: 'You should see something like this:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下类似的内容：
- en: '![Figure 7.1 – Carla – default track](img/Figure_7.1_B16322.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.1 – Carla – 默认轨道](img/Figure_7.1_B16322.jpg)'
- en: Figure 7.1 – Carla – default track
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 – Carla – 默认轨道
- en: While the server is sometimes useful, you probably want to run some of the files
    present in `PythonAPI\util` and `PythonAPI\examples`.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然服务器有时很有用，但您可能想运行一些位于 `PythonAPI\util` 和 `PythonAPI\examples` 中的文件。
- en: 'For this task, we are going to change track, using `Town01`. You can do this
    using the `PythonAPI\util\config.py` file, as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个任务，我们将使用 `Town01` 改变轨道。您可以使用 `PythonAPI\util\config.py` 文件这样做，如下所示：
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You should now see a different track:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在应该看到一个不同的轨道：
- en: '![Figure 7.2 – The Town01 track](img/Figure_7.2_B16322.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.2 – Town01 轨道](img/Figure_7.2_B16322.jpg)'
- en: Figure 7.2 – The Town01 track
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – Town01 轨道
- en: 'Your city is empty, so we need to add some vehicles and some pedestrians. We
    can do this using `PythonAPI\examples\spawn_npc.py`, as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 您的城市是空的，因此我们需要添加一些车辆和一些行人。我们可以使用 `PythonAPI\examples\spawn_npc.py` 来完成，如下所示：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `-w` parameter specifies the number of walkers, and `–n` the number of
    vehicles, that you want to create. Now, you should see some action:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`-w` 参数指定要创建的行人数量，而 `–n` 指定要创建的车辆数量。现在，您应该看到一些动作：'
- en: '![Figure 7.3 – The Town01 track with vehicles and pedestrians](img/Figure_7.3_B16322.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3 – 带有车辆和行人的 Town01 轨道](img/Figure_7.3_B16322.jpg)'
- en: Figure 7.3 – The Town01 track with vehicles and pedestrians
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – 带有车辆和行人的 Town01 轨道
- en: Much better.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 好多了。
- en: Carla is intended to run as a server to which you can connect multiple clients,
    which should allow more interesting simulations.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Carla 设计为作为服务器运行，您可以连接多个客户端，这应该允许进行更有趣的模拟。
- en: 'When you run Carla, it starts a server. You can go around a bit using the server,
    but most likely, you will want to run a client as it can provide much more functionality.
    If you run a client, you will have two windows with Carla, which are expected:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当您运行 Carla 时，它启动一个服务器。您可以使用服务器四处走动，但您可能更希望运行一个客户端，因为它可以提供更多功能。如果您运行一个客户端，您将有两个带有
    Carla 的窗口，这是预期的：
- en: 'Let''s run a client using `PythonAPI\examples\manual_control.py`, as follows:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用 `PythonAPI\examples\manual_control.py` 运行一个客户端，如下所示：
- en: '[PRE3]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You might see something like the following:'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可能会看到以下类似的内容：
- en: '![Figure 7.4 – The Town01 track using manual_control.py](img/Figure_7.4_B16322.jpg)'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 7.4 – 使用 manual_control.py 的 Town01 轨道](img/Figure_7.4_B16322.jpg)'
- en: Figure 7.4 – The Town01 track using manual_control.py
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.4 – 使用 manual_control.py 的 Town01 轨道
- en: You can see a lot of statistics on the left, and you can toggle them using the
    *F1* key. You will notice that now you have a vehicle, and you can change it with
    the Backspace key.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可以在左侧看到很多统计数据，并且可以使用 *F1* 键切换它们。您会注意到现在您有一辆车，并且可以使用退格键更改它。
- en: You can move with the same keys as before, but this time, the behavior is more
    useful and realistic, as there is some physical simulation. You can also use the
    arrow keys to move.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以使用与之前相同的按键移动，但这次，行为更加有用和逼真，因为有一些物理模拟。你还可以使用箭头键进行移动。
- en: 'You can use the *Tab* key to change camera, and the *C* key changes the weather,
    as we can see in the following screenshot:'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以使用*Tab*键切换相机，*C*键更改天气，正如我们可以在以下截图中所见：
- en: '![Figure 7.5 – The Town01 track; hard rain at noon and clear sky at sunset](img/Figure_7.5_B16322.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图7.5 – Town01赛道；中午大雨和日落时晴朗的天空](img/Figure_7.5_B16322.jpg)'
- en: Figure 7.5 – The Town01 track; hard rain at noon and clear sky at sunset
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 – Town01赛道；中午大雨和日落时晴朗的天空
- en: 'Carla has many sensors, one of which is the RGB camera, and you can switch
    between them using `` ` ``, the backtick key. Now, refer to the following screenshots:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Carla有许多传感器，其中之一是RGB相机，你可以使用`` ` ``（反引号键）在它们之间切换。现在，请参考以下截图：
- en: '![Figure 7.6 – The Town01 track – left: depth (raw), right: semantic segmentation](img/Figure_7.6_B16322.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图7.6 – Town01赛道 – 左：深度（原始），右：语义分割](img/Figure_7.6_B16322.jpg)'
- en: 'Figure 7.6 – The Town01 track – left: depth (raw), right: semantic segmentation'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 – Town01赛道 – 左：深度（原始），右：语义分割
- en: 'The preceding screenshots show a couple of very interesting sensors:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 上述截图显示了几款非常有趣的传感器：
- en: The depth sensor, which provides the distance from the camera for each pixel
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度传感器，为每个像素提供从摄像机到距离
- en: The semantic segmentation sensor, which classifies every object using a different
    color
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义分割传感器，使用不同的颜色对每个对象进行分类
- en: 'At the time of writing, the full list of camera sensors is as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，完整的摄像机传感器列表如下：
- en: Camera RGB
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摄像机RGB
- en: Camera depth (raw)
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摄像机深度（原始）
- en: Camera depth (grayscale)
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摄像机深度（灰度）
- en: Camera depth (logarithmic grayscale)
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摄像机深度（对数灰度）
- en: Camera semantic segmentation (CityScapes Palette)
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摄像机语义分割（CityScapes调色板）
- en: Lidar (raycast)
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激光雷达（射线投射）
- en: '**Dynamic Vision Sensor** (**DVS**)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动态视觉传感器**（**DVS**）'
- en: Camera RGB distorted
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摄像机RGB畸变
- en: Lidar is a sensor that detects the distance of an object using a laser; the
    DVS, also called the neuromorphic camera, is a camera that records local changes
    of brightness, overcoming some limitations of RGB cameras. Camera RGB distorted
    is just an RGB camera simulating the effects of a lens, and of course, you can
    customize the distortion as needed.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 激光雷达是一种使用激光检测物体距离的传感器；DVS，也称为神经形态相机，是一种记录亮度局部变化的相机，克服了RGB相机的一些局限性。摄像机RGB畸变只是一个模拟镜头效果的RGB相机，当然，你可以根据需要自定义畸变。
- en: 'The following screenshot shows the Lidar camera view:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了激光雷达摄像机的视图：
- en: '![Figure 7.7 – The Lidar camera view](img/Figure_7.7_B16322.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图7.7 – 激光雷达摄像机视图](img/Figure_7.7_B16322.jpg)'
- en: Figure 7.7 – The Lidar camera view
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 – 激光雷达摄像机视图
- en: 'The following screenshot shows the output of DVS:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了DVS的输出：
- en: '![Figure 7.8 – DVS](img/Figure_7.8_B16322.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图7.8 – DVS](img/Figure_7.8_B16322.jpg)'
- en: Figure 7.8 – DVS
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 – DVS
- en: You can now just go around and collect some images from the RGB camera, or you
    can use the ones in the GitHub repository.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以四处走动，从RGB相机收集一些图像，或者你可以使用GitHub仓库中的那些。
- en: Now that we have some images, it is time to detect pedestrians, vehicles, and
    traffic lights, using a pre-trained network called SSD.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一些图像了，是时候使用名为SSD的预训练网络来检测行人、车辆和交通灯了。
- en: Understanding SSD
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解SSD
- en: In previous chapters, we created a classifier, a neural network able to tell
    what is present in a picture from a predefined set of options. Later in this chapter,
    we will see a pre-trained neural network that can classify images in a very precise
    way.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们创建了一个分类器，一个能够从预定义的选项集中识别图片中内容的神经网络。在本章的后面部分，我们将看到一个预训练的神经网络，它能够非常精确地对图像进行分类。
- en: SSD stands out compared to many neural networks, as it is able to detect multiple
    objects in the same picture. The details of SSD are a bit complicated, and if
    you are interested, you can check the *Further reading* section for some inspiration.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 与许多神经网络相比，SSD脱颖而出，因为它能够在同一张图片中检测多个对象。SSD的细节有些复杂，如果你感兴趣，可以查看“进一步阅读”部分以获取一些灵感。
- en: Not only can SSD detect multiple objects, but it can also output the area where
    the object is present! Internally, this is done by checking 8,732 positions at
    different aspect ratios. SSD is also fast enough that with a good GPU, it can
    be used to analyze videos in real time.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅SSD可以检测多个物体，它还可以输出物体存在的区域！在内部，这是通过检查不同宽高比下的8,732个位置来实现的。SSD也足够快，在有良好GPU的情况下，它可以用来实时分析视频。
- en: But where can we find SSD? The answer is the TensorFlow detection model zoo.
    Let's see what this is.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们可以在哪里找到SSD？答案是TensorFlow检测模型动物园。让我们看看这是什么。
- en: Discovering the TensorFlow detection model zoo
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发现TensorFlow检测模型动物园
- en: The TensorFlow detection model zoo is a useful collection of pre-trained neural
    networks, which supports several architectures trained on several datasets. We
    are interested in SSD, so we will focus on that.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow检测模型动物园是一个有用的预训练神经网络集合，它支持在多个数据集上训练的多个架构。我们感兴趣的是SSD，因此我们将专注于这一点。
- en: 'Of the datasets supported by the model zoo, we are interested in COCO. **COCO**
    is the Microsoft **Common Objects in Context** dataset, a collection of 2,500,000
    (2.5 million) images, classified by type. You can find a link with the 90 labels
    of COCO in the *Further reading* section, but we are interested in the following
    ones:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型动物园支持的各个数据集中，我们感兴趣的是COCO。**COCO**是微软的**Common Objects in Context**数据集，一个包含2,500,000（250万）张图片的集合，按类型分类。你可以在*进一步阅读*部分找到COCO的90个标签的链接，但我们感兴趣的是以下这些：
- en: '`1`: `person`'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1`: `person`'
- en: '`3`: `car`'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`3`: `car`'
- en: '`6`: `bus`'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`6`: `bus`'
- en: '`8`: `truck`'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`8`: `truck`'
- en: '`10`: `traffic light`'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`10`: `traffic light`'
- en: 'You might also be interested in the following:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还对以下内容感兴趣：
- en: '`2`: `bicycle`'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`2`: `bicycle`'
- en: '`4`: `motorcycle`'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`4`: `motorcycle`'
- en: '`13`: `stop sign`'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`13`: `stop sign`'
- en: 'Notably, SSD trained on COCO is available on several versions, using different
    neural networks as the backend to reach the desired speed/precision ratio. Refer
    to the following screenshot:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，在COCO上训练的SSD有多个版本，使用不同的神经网络作为后端以达到所需的速度/精度比。请参考以下截图：
- en: '![Figure 7.9 – The TensorFlow detection model zoo of SSDs trained on COCO](img/Figure_7.9_B16322.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图7.9 – 在COCO上训练的SSD的TensorFlow检测模型动物园](img/Figure_7.9_B16322.jpg)'
- en: Figure 7.9 – The TensorFlow detection model zoo of SSDs trained on COCO
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 – 在COCO上训练的SSD的TensorFlow检测模型动物园
- en: Here, the `mAP` column is the mean average precision, so the higher the better.
    MobileNet is a neural network developed to perform particularly well on mobiles
    and embedded devices, and thanks to its performance, it is a classical choice
    for SSD when you need to perform inference in real time.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`mAP`列是平均精度均值，所以越高越好。MobileNet是一个专为在移动设备和嵌入式设备上表现良好而开发的神经网络，由于其性能，它是在需要实时进行推理时SSD的经典选择。
- en: To detect the objects on the road, we will use an SSD built using **ResNet50**
    as a backbone, a neural network with 50 layers developed by Microsoft Research
    Asia. A characteristic of ResNet is the presence of **skip connections**, shortcuts
    that can connect a layer to another one, skipping some layers in the middle. This
    helps in solving the **vanishing gradient problem**. With deep neural networks,
    the gradient during training can become so small that the network can basically
    stop learning.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检测道路上的物体，我们将使用以**ResNet50**作为骨干网络的SSD，这是一个由微软亚洲研究院开发的具有50层的神经网络。ResNet的一个特点是存在**跳跃连接**，这些捷径可以将一层连接到另一层，跳过中间的一些层。这有助于解决**梯度消失问题**。在深度神经网络中，训练过程中的梯度可能会变得非常小，以至于网络基本上停止学习。
- en: But how do we use `ssd_resnet_50_fpn_coco`, our selected model? Let's check
    it out!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们如何使用我们选定的模型`ssd_resnet_50_fpn_coco`？让我们来看看！
- en: Downloading and loading SSD
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载和加载SSD
- en: 'On the model zoo page, if you click on **ssd_resnet_50_fpn_coco**, you get
    a URL that Keras needs to download the model from; at the time of writing, the
    URL is as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型动物园页面，如果你点击**ssd_resnet_50_fpn_coco**，你会得到一个Keras需要从中下载模型的URL；在撰写本文时，URL如下：
- en: '[PRE4]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The full name of the model is the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的全名如下：
- en: '[PRE5]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To load the model, you can use the following code:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 要加载模型，你可以使用以下代码：
- en: '[PRE6]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If this is the first time that you have run this code, it will take more time
    because Keras will download the model and save it on your hard drive.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你第一次运行这段代码，它将花费更多时间，因为Keras将下载模型并将其保存在你的硬盘上。
- en: Now that we have loaded the model, is time to use it to detect some objects.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经加载了模型，是时候用它来检测一些物体了。
- en: Running SSD
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行SSD
- en: 'Running SSD requires just a few lines of code. You can load the image (with
    a resolution of 299x299) with OpenCV, then you need to convert the image into
    a tensor, a type of multi-dimensional array used by TensorFlow that is similar
    to NumPy arrays. Refer to the following code:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 运行SSD只需要几行代码。你可以使用OpenCV加载图像（分辨率为299x299），然后你需要将图像转换为张量，这是一种由TensorFlow使用的多维数组类型，类似于NumPy数组。参考以下代码：
- en: '[PRE7]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Please note that we feed the network with an `RGB` image, not `BGR`. You might
    remember from the previous chapters that OpenCV uses pictures in `BGR` format,
    so we need to pay attention to the order of the channels.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们向网络输入的是`RGB`图像，而不是`BGR`。你可能还记得前几章中提到的OpenCV使用的是`BGR`格式的图片，因此我们需要注意通道的顺序。
- en: As you can see, running SSD is quite easy, but the output is relatively complex,
    and it needs some code to be converted into a useful and more compact form. The
    `output` variable is a Python dictionary, but the values that it contains are
    tensors, so you need to convert them.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，运行SSD相当简单，但输出相对复杂，需要一些代码将其转换为有用且更紧凑的形式。`output`变量是一个Python字典，但它包含的值是张量，因此你需要将它们转换。
- en: 'For example, printing `output[''num_detections'']`, which contains the number
    of predictions (for example, objects found in the image), would give the following
    as a result:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，打印`output['num_detections']`，它包含预测的数量（例如，图像中找到的对象），将得到以下结果：
- en: '[PRE8]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: For the conversion, we can use `int()`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 对于转换，我们可以使用`int()`。
- en: 'All the other tensors are arrays, and they can be converted using their `numpy()`
    function. So then, your code might look like this:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 所有其他张量都是数组，并且可以使用它们的`numpy()`函数进行转换。因此，你的代码可能看起来像这样：
- en: '[PRE9]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'There are still the following two things to fix:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然有以下两个问题需要修复：
- en: The detection classes are floating point, while, as they are our labels, they
    should be integers.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测类别是浮点数，而作为我们的标签，它们应该是整数。
- en: The coordinates of the boxes are in percentage form.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 框的坐标以百分比形式表示。
- en: 'We can fix these problems with just a few lines of code:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用几行代码修复这些问题：
- en: '[PRE10]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let''s apply SSD to this image:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将SSD应用于这张图像：
- en: '![Figure 7.10 – Image from Town01](img/Figure_7.10_B16322.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图7.10 – 来自Town01的图像](img/Figure_7.10_B16322.jpg)'
- en: Figure 7.10 – Image from Town01
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 – 来自Town01的图像
- en: 'We get this output:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '[PRE11]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This is what the code means:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是代码的含义：
- en: '`detection_scores`: A higher score means higher confidence in the prediction.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`detection_scores`：分数越高，预测的置信度越高。'
- en: '`detection_classes`: The predicted labels – in this case, truck (`8`), traffic
    light (`10`), bus (`6`), and car (`3`).'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`detection_classes`：预测的标签 – 在这种情况下，卡车（`8`）、交通灯（`10`）、公交车（`6`）和汽车（`3`）。'
- en: '`detection_boxes`: The original boxes, with coordinates in percentage form.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`detection_boxes`：原始框，坐标以百分比形式表示。'
- en: '`num_detections`: The number of predictions.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_detections`：预测的数量。'
- en: '`boxes`: The boxes with coordinates converted to the resolution of the original
    image.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`boxes`：将坐标转换为原始图像分辨率的框。'
- en: Please notice that three predictions are basically in the same area, and they
    are ordered by score. We will need to fix this overlapping.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，三个预测基本上在同一区域，并且它们按分数排序。我们需要修复这种重叠。
- en: To be able to better see what has been detected, we will now annotate the image.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地看到检测到的内容，我们现在将标注图像。
- en: Annotating the image
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标注图像
- en: 'To properly annotate the image, we need to perform the following operations:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确标注图像，我们需要执行以下操作：
- en: Consider only the labels that are interesting to us.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只考虑对我们有意义的标签。
- en: Remove the overlapping of labels.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除标签的重叠。
- en: Draw a rectangle on each prediction.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个预测上画一个矩形。
- en: Write the label and its score.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 写上标签及其分数。
- en: To remove the overlapping labels, it's enough to compare them, and if the center
    of the boxes is similar, we will keep only the label with a higher score.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 要移除重叠的标签，只需比较它们，如果框的中心相似，我们只保留分数更高的标签。
- en: 'This is the result:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这是结果：
- en: '![Figure 7.11 – Image from Town01, annotated with SSD only](img/Figure_7.11_B16322.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图7.11 – 来自Town01的图像，仅使用SSD进行标注](img/Figure_7.11_B16322.jpg)'
- en: Figure 7.11 – Image from Town01, annotated with SSD only
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11 – 来自Town01的图像，仅使用SSD进行标注
- en: It's a good starting point, even if other images are not recognized so well.
    The vehicle has been recognized as a truck, which is not completely accurate,
    but we don't really care about that.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很好的起点，即使其他图像的识别效果不是很好。车辆被识别为卡车，这并不完全准确，但我们并不真的关心这一点。
- en: The main problem is that we know that there is a traffic light, but we don't
    know the color.Unfortunately, SSD cannot help us; we need to do it by ourselves.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 主要问题是我们知道有交通灯，但我们不知道它的颜色。不幸的是，SSD无法帮助我们；我们需要自己来做。
- en: In the next section, we will develop a neural network that is able to detect
    the color of the traffic light, using a technique called transfer learning.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将开发一个能够通过称为迁移学习的技术检测交通灯颜色的神经网络。
- en: Detecting the color of a traffic light
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测交通灯的颜色
- en: In principle, we could try to detect the color of a traffic light using some
    computer vision technique – for example, checking the red and the green channel
    could be a starting point. In addition, verifying the luminosity of the bottom
    and upper part of the crossing light should help. This could work, even if some
    crossing lights can be problematic.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 原则上，我们可以尝试使用一些计算机视觉技术来检测交通灯的颜色——例如，检查红色和绿色通道可能是一个起点。此外，验证交叉灯底部和上部的亮度也有助于。这可能会有效，即使一些交叉灯可能会有问题。
- en: However, we will use deep learning, as this task is well-suited to exploring
    more advanced techniques. We will also go the extra mile to use a small dataset,
    even though it would be easy for us to create a big dataset; the reason being
    that we don't always have the luxury of easily increasing the size of the dataset.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们将使用深度学习，因为这项任务非常适合探索更高级的技术。我们还将不遗余力地使用一个小数据集，尽管我们很容易创建一个大数据集；原因在于我们并不总是有轻松增加数据集大小的奢侈。
- en: 'To be able to detect the color of a traffic light, we need to complete three
    steps:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 要能够检测交通灯的颜色，我们需要完成三个步骤：
- en: Collect a dataset of crossing lights.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集交叉灯的数据集。
- en: Train a neural network to recognize the color.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个神经网络来识别颜色。
- en: Use the network with SSD to get the final result.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用带有SSD的网络来获取最终结果。
- en: There is a traffic light dataset that you could use, the *Bosch Small Traffic
    Lights* dataset; however, we will generate our own dataset using Carla. Let's
    see how.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个你可以使用的交通灯数据集，即*Bosch Small Traffic Lights*数据集；然而，我们将使用Carla生成我们自己的数据集。让我们看看怎么做。
- en: Creating a traffic light dataset
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建交通灯数据集
- en: We are going to create a dataset using Carla. In principle, it could be as big
    as we want. The bigger the better, but a big dataset would make training slower
    and, of course, it will require some more time to be created. In our case, as
    the task is simple, we will create a relatively small dataset of a few hundreds
    of images. We will explore **transfer learning** later, a technique that can be
    used when the dataset is not particularly big.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Carla创建一个数据集。原则上，它可以大如我们所愿。越大越好，但大数据集会使训练变慢，当然，创建它也需要更多的时间。在我们的案例中，由于任务简单，我们将创建一个相对较小的、包含数百张图片的数据集。我们将在稍后探索**迁移学习**，这是一种在数据集不是特别大时可以使用的技巧。
- en: Tip
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: On GitHub, you can find the dataset that I created for this task, but if you
    have some time, collecting the dataset by yourself could be a nice exercise.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在GitHub上，你可以找到我为这个任务创建的数据集，但如果你有时间，自己收集数据集可以是一项很好的练习。
- en: 'Creating this dataset is a three-step task:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 创建这个数据集是一个三步任务：
- en: Collect images of streets.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集街道图片。
- en: Find and crop all the traffic lights.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到并裁剪所有的交通灯。
- en: Classify the traffic lights.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对交通灯进行分类。
- en: The first task of collecting images is very simple. Just start Carla using `manual_control.py`
    and press the *R* key. Carla will start recording and it will stop after you press
    *R* again.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 收集图片的第一项任务非常简单。只需使用`manual_control.py`启动Carla并按*R*键。Carla将开始录制，并在你再次按*R*键后停止。
- en: 'Consider that we want to record four types of images:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要记录四种类型的图片：
- en: Red lights
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 红灯
- en: Yellow lights
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黄灯
- en: Green lights
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绿灯
- en: The back of traffic lights (negative samples)
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交通灯背面（负样本）
- en: The reason why we want to collect the back of the traffic light is that SSD
    is recognizing it as a traffic light, but we have no use for it, so we don't want
    to use it. These are **negative samples**, and they could also include pieces
    of road or buildings or anything that SSD wrongly classifies as a traffic light.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要收集交通灯背面的原因是因为SSD将其识别为交通灯，但我们没有用，所以我们不希望使用它。这些都是**负样本**，它们也可能包括道路或建筑或任何SSD错误分类为交通灯的东西。
- en: So, while you record your images, please try to get enough samples for each
    category.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在你记录图片时，请尽量为每个类别收集足够的样本。
- en: 'These are some example of images that you might want to collect:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是一些你可能想要收集的图片示例：
- en: '![Figure 7.12 – Town01 – left: red light, right: green light](img/Figure_7.12_B16322.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.12 – Town01 – 左：红灯，右：绿灯](img/Figure_7.12_B16322.jpg)'
- en: 'Figure 7.12 – Town01 – left: red light, right: green light'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.12 – Town01 – 左：红灯，右：绿灯
- en: 'The second step is applying SSD and extracting the image of the crossing light.
    It''s quite simple; refer to the following code:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是应用 SSD 并提取交叉灯的图像。这很简单；参考以下代码：
- en: '[PRE12]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the previous code, assuming that the `out` variable contains the result of
    running the SSD, calling `model(input_tensor)`, and `idx` contains the current
    detection among the predictions, you just need to select the detections containing
    traffic lights and crop them using the coordinates that we computed earlier.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，假设 `out` 变量包含运行 SSD 的结果，调用 `model(input_tensor)`，并且 `idx` 包含当前预测中的当前检测，你只需要选择包含交通灯的检测，并使用我们之前计算的坐标进行裁剪。
- en: 'I ended up with 291 detections, with images like these:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我最终得到了 291 个检测，图像如下：
- en: '![Figure 7.13 – Town01, from the left: small red light, small green light,
    green light, yellow light, back of a traffic light, piece of a building wrongly
    classified as a traffic light](img/Figure_7.13_B16322.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.13 – Town01，从左至右：小红灯，小绿灯，绿灯，黄灯，交通灯背面，被错误分类为交通灯的建筑的一部分](img/Figure_7.13_B16322.jpg)'
- en: 'Figure 7.13 – Town01, from the left: small red light, small green light, green
    light, yellow light, back of a traffic light, piece of a building wrongly classified
    as a traffic light'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13 – Town01，从左至右：小红灯，小绿灯，绿灯，黄灯，交通灯背面，被错误分类为交通灯的建筑的一部分
- en: As you can see, the images have different resolutions and ratios, and that's
    perfectly fine.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，图像具有不同的分辨率和比例，这是完全可以接受的。
- en: There are also some images that are completely unrelated, such as a piece of
    a building, and these are excellent negative samples for things that are not a
    traffic light because SSD misclassified them, so it is also a way to improve the
    output of SSD.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 也有一些图像与交通灯完全无关，例如一块建筑的一部分，这些是很好的负样本，因为 SSD 将它们错误分类，因此这也是提高 SSD 输出的方法之一。
- en: The last step is just classifying the images. With a few hundred pictures of
    this type, it takes only a few minutes. You can create a directory for each label
    and move the appropriate images there.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步只是对图像进行分类。有了几百张这种类型的图片，只需要几分钟。你可以为每个标签创建一个目录，并将适当的图片移动到那里。
- en: Congratulations, now you have a custom dataset to detect the color of traffic
    lights.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你，现在你有一个用于检测交通灯颜色的自定义数据集了。
- en: As you know, the dataset is small, so, as we already said, we are going to use
    transfer learning. The next section will explain what it is.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所知，数据集很小，因此，正如我们之前所说的，我们将使用转移学习。下一节将解释它是什么。
- en: Understanding transfer learning
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解转移学习
- en: Transfer learning is a very appropriate name. From a conceptual point of view,
    indeed, it is about taking what a neural network learned on one task and transferring
    this knowledge to a different but related task.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 转移学习是一个非常恰当的名字。从概念上看，的确，它涉及到将神经网络在一个任务上学习到的知识转移到另一个不同但相关的任务上。
- en: There are several approaches to transfer learning; we will discuss two, and
    we will choose one of them and use it to detect the colors of the traffic lights.
    In both cases, the starting point is a neural network that has been pre-trained
    on a similar task – for example, a classification of images. We will talk more
    about this in the next section, *Getting to know ImageNet*. We are focusing on
    a **Convolutional Neural Network** (**CNN**) used as a classifier, as this is
    what we need to recognize the color of the traffic lights.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 转移学习有几种方法；我们将讨论两种，并选择其中一种来检测交通灯的颜色。在两种情况下，起点都是一个在类似任务上预训练过的神经网络 – 例如，图像分类。我们将在下一节“了解
    ImageNet”中更多地讨论这一点。我们专注于用作分类器的 **卷积神经网络** (**CNN**)，因为这是我们识别交通灯颜色的需要。
- en: The first approach is to load the pre-trained neural network, adapting the number
    of outputs to the new problem (either replacing some or all the dense layers or
    sometimes just adding an additional dense layer), and basically keep training
    it on the new dataset. You might need to use a small learning rate. This approach
    might work if the number of samples in the new dataset is smaller than the dataset
    used for the original training, but still significantly big. For example, the
    size of our custom dataset could be 10% of the site of the original dataset. One
    drawback is that training might take a long time, as typically you are training
    a relatively big network.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法是加载预训练的神经网络，将输出的数量调整到新问题（替换一些或所有密集层，有时只是添加一个额外的密集层），并在新的数据集上基本保持训练。你可能需要使用较小的学习率。如果新数据集中的样本数量小于用于原始训练的数据集，但仍然相当大，这种方法可能有效。例如，我们自定义数据集的大小可能是原始数据集位置的10%。一个缺点是训练可能需要很长时间，因为你通常在训练一个相对较大的网络。
- en: A second approach, the one that we are going to follow, is similar to the first
    one, but you are going to freeze all the convolutional layers, meaning that their
    parameters are fixed and will not change during training. This has the advantage
    that training is much faster as you don't need to train the convolutional layers.
    The idea here is that the convolutional layers have been trained on a huge dataset,
    and they are able to detect so many features that it is going to be fine for the
    new task also, while the real classifier, composed of the dense layers, can be
    replaced and trained from scratch.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法，我们将要采用的方法，与第一种方法类似，但你将冻结所有卷积层，这意味着它们的参数是固定的，在训练过程中不会改变。这有一个优点，即训练速度更快，因为你不需要训练卷积层。这里的想法是，卷积层已经在大型数据集上进行了训练，并且能够检测到许多特征，对于新任务来说也将是可行的，而真正的分类器，由密集层组成，可以被替换并从头开始训练。
- en: Intermediate approaches are also possible, where you train some of the convolutional
    layers, but typically keep at least the first layers frozen.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 中间方法也是可能的，其中你训练一些卷积层，但通常至少保持第一层冻结。
- en: Before seeing how to do transfer learning with Keras, let's think a bit more
    about what we just discussed. A key assumption is that this hypothetical network
    that we want to learn from has been trained on a huge dataset, where the network
    can learn to recognize many features and patterns. Turns out that there is a very
    big dataset that fits the bill—ImageNet. Let's talk a bit more about it.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解如何使用Keras进行迁移学习之前，让我们再思考一下我们刚才讨论的内容。一个关键假设是我们想要从中学习的这个假设网络已经在大型数据集上进行了训练，其中网络可以学习识别许多特征和模式。结果发现，有一个非常大的数据集符合要求——ImageNet。让我们再谈谈它。
- en: Getting to know ImageNet
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解ImageNet
- en: ImageNet is a huge dataset, and at the time of writing, it is composed of 14,197,122
    (over 14 million) images! In reality, it does not provide the images, but just
    the URLs to download the images. Those images are classified on 27 categories
    and a total of 21,841 subcategories! These subcategories, called synsets, are
    based on a classification hierarchy called **WordNet**.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ImageNet是一个巨大的数据集，在撰写本文时，它由14,197,122（超过1400万）张图片组成！实际上，它并不提供图片，只是提供下载图片的URL。这些图片被分类在27个类别和总共21,841个子类别中！这些子类别，称为synsets，基于称为**WordNet**的分类层次结构。
- en: 'ImageNet has been a very influential dataset, thanks also to a competition
    used to measure the advancements in computer vision: the **ImageNet Large-Scale
    Visual Recognition Challenge** (**ILSVRC**).'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ImageNet已经是一个非常具有影响力的数据集，这也要归功于用于衡量计算机视觉进步的竞赛：**ImageNet大规模视觉识别挑战**（**ILSVRC**）。
- en: 'These are the main categories:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是主要类别：
- en: '`amphibian`'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`两栖动物`'
- en: '`animal`'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`动物`'
- en: '`appliance`'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`家用电器`'
- en: '`bird`'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`鸟类`'
- en: '`covering`'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`覆盖物`'
- en: '`device`'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`设备`'
- en: '`fabric`'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`织物`'
- en: '`fish`'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`鱼类`'
- en: '`flower`'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`花卉`'
- en: '`food`'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`食物`'
- en: '`fruit`'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`水果`'
- en: '`fungus`'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`真菌`'
- en: '`furniture`'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`家具`'
- en: '`geological formation`'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`地质构造`'
- en: '`invertebrate`'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`无脊椎动物`'
- en: '`mammal`'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`哺乳动物`'
- en: '`musical instrument`'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`乐器`'
- en: '`plant`'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`植物`'
- en: '`reptile`'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`爬行动物`'
- en: '`sport`'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`运动`'
- en: '`structure`'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`结构`'
- en: '`tool`'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`工具`'
- en: '`tree`'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`树木`'
- en: '`utensil`'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`器具`'
- en: '`vegetable`'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`蔬菜`'
- en: '`vehicle`'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`车辆`'
- en: '`person`'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`人物`'
- en: The number of subcategories is impressively high; as an example, the `tree`
    category has 993 subcategories, covered by more than half a million of pictures!
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 子类别的数量非常高；例如，`树木`类别有993个子类别，覆盖了超过五十万张图片！
- en: Surely, a neural network performing well on this dataset will be very good at
    recognizing patterns on many types of images, and it might also have a quite big
    capacity. So, yes, it will overfit your dataset, but as we know how to deal with
    overfitting, we will keep an eye on this problem, but not get too worried about
    it.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在这个数据集上表现良好的神经网络将非常擅长识别多种类型图像上的模式，并且它可能也有相当大的容量。所以，是的，它将过度拟合你的数据集，但正如我们所知如何处理过拟合，我们将密切关注这个问题，但不会过于担心。
- en: As so much research has been devoted to performing well on ImageNet, it's not
    surprising that many of the most influential neural networks have been trained
    on it.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大量研究致力于在ImageNet上表现良好，因此许多最有影响力的神经网络都在其上进行了训练，这并不令人惊讶。
- en: One, in particular, stood out when it first appeared in 2012—AlexNet. Let's
    see why.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在2012年首次出现时，其中一个特别引人注目的是AlexNet。让我们看看原因。
- en: Discovering AlexNet
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发现AlexNet
- en: When AlexNet was released in 2012, it was over 10% more precise than the best
    neural network of the time! Clearly, these solutions have been studied extensively,
    and some are now very common.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 当AlexNet在2012年发布时，它的准确率比当时最好的神经网络高出10%以上！显然，这些解决方案已经得到了广泛的研究，其中一些现在非常常见。
- en: 'AlexNet introduced several ground-breaking innovations:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet引入了几个开创性的创新：
- en: Multi-GPU training where AlexNet was trained half on one GPU and half on another
    one, allowing the model to be twice the size.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多GPU训练，其中AlexNet在一半的GPU上训练，另一半在另一个GPU上，使得模型的大小翻倍。
- en: ReLU activation instead of Tanh, which apparently allowed the training to be
    six times faster.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用ReLU激活而不是Tanh，这显然使得训练速度提高了六倍。
- en: Overlapping pooling added, where AlexNet used max-pooling of 3x3, but the pooling
    area is moved only by 2x2, meaning that there is an overlap between pools. According
    to the original paper, this gave a 0.3–0.4% improvement in accuracy. In Keras,
    you can achieve similar overlapping pooling using `MaxPooling2D(pool_size=(3,3),
    strides=(2,2))`.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加了重叠池化，其中AlexNet使用了3x3的最大池化，但池化区域仅移动2x2，这意味着池化区域之间存在重叠。根据原始论文，这提高了0.3–0.4%的准确率。在Keras中，你可以使用`MaxPooling2D(pool_size=(3,3),
    strides=(2,2))`实现类似的重叠池化。
- en: With over 60 million parameters, AlexNet was pretty big, so to reduce overfitting,
    it made extensive use of data augmentation and dropout.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet拥有超过6000万个参数，相当大，因此为了减少过拟合，它广泛使用了数据增强和dropout。
- en: 'While AlexNet was state-of-the-art and ground-breaking in 2012, by today''s
    standards, it is quite inefficient. In the next section, we will discuss a neural
    network that can achieve substantially better accuracy than AlexNet using only
    one-tenth of the parameters: **Inception**.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然2012年的AlexNet在当时的标准下是当时最先进和开创性的，但按照今天的标准，它相当低效。在下一节中，我们将讨论一个神经网络，它只需AlexNet十分之一的参数就能实现显著更高的准确率：**Inception**。
- en: The ideas behind Inception
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Inception背后的理念
- en: Having a huge dataset like ImageNet at our disposal is great, but it would be
    much easier to have a neural network already trained with it. It turns out that
    Keras provides several of them. One is ResNet, which we have already encountered.
    Another one, which is very influential and with great innovations, is Inception.
    Let's talk a bit about it.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个像ImageNet这样的大型数据集是非常好的，但有一个已经用这个数据集训练好的神经网络会更容易。结果发现，Keras提供了几个这样的神经网络。一个是ResNet，我们已经遇到了。另一个非常有影响力且具有重大创新的是Inception。让我们简单谈谈它。
- en: Inception is a family of neural networks, meaning that there are several of
    them, refining the initial concept. Inception was designed by Google, and the
    version that participated in the ILSVRC 2014 (ImageNet) competition and won is
    called **GoogLeNet**, in honor of the LeNet architecture.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: Inception是一系列神经网络，意味着有多个，它们对初始概念进行了细化。Inception是由谷歌设计的，在2014年ILSVRC（ImageNet）竞赛中参赛并获胜的版本被称为**GoogLeNet**，以纪念LeNet架构。
- en: If you are wondering whether Inception took its name from the famous movie *Inception*,
    yes it did, because they wanted to go deeper! And Inception is a deep network,
    with a version called `InceptionResNetV2` arriving at a staggering 572 layers!
    This, of course, is if we count every layer, including the activations. We are
    going to use Inception v3, which has *only* 159 layers.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想知道Inception的名字是否来自著名的电影*Inception*，是的，它确实如此，因为他们想要更深入！Inception是一个深度网络，有一个名为`InceptionResNetV2`的版本达到了惊人的572层！当然，这是如果我们计算每个层，包括激活层的话。我们将使用Inception
    v3，它只有*159层*。
- en: We will focus on Inception v1, because it is easier, and we will briefly discuss
    some improvements added later, as they can be a source of inspiration for you.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重点关注Inception v1，因为它更容易，我们还将简要讨论后来添加的一些改进，因为它们可以成为你的灵感来源。
- en: A key observation made by Google is that given the variety of positions where
    a subject can be on a picture, it is difficult to know in advance which kernel
    size of a convolutional layer would be the best, so they added 1x1, 3x3, and 5x5
    convolutions in parallel to cover the main cases, plus max pooling, as it is often
    useful, and concatenated the results. One advantage of doing it in parallel is
    that the network does not get too deep, which keeps the training easier.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌的一个关键观察是，由于一个主题在图片中可能出现的各种位置，很难提前知道卷积层的最佳核大小，所以他们并行添加了1x1、3x3和5x5卷积，以覆盖主要情况，加上最大池化，因为它通常很有用，并将结果连接起来。这样做的一个优点是网络不会太深，这使得训练更容易。
- en: 'What we just described is the **naïve** Inception block:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才描述的是**天真**的Inception块：
- en: '![Figure 7.14 – Naïve Inception block](img/Figure_7.14_B16322.jpg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![图7.14 – 天真的Inception块](img/Figure_7.14_B16322.jpg)'
- en: Figure 7.14 – Naïve Inception block
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14 – 天真的Inception块
- en: You might have noticed a 1x1 convolution. What's that? Just multiplying a channel
    by a number? Not exactly. The 1x1 convolution is very cheap to perform, as there
    is only 1 multiplication instead of 9 (as in 3x3 convolutions) or 25 (as in 5x5
    convolutions), and it can be used to change the number of filters. In addition,
    you can add a ReLU, introducing a non-linear operation that increases the complexity
    of the functions that the network can learn.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到了一个1x1的卷积。那是什么？只是将一个通道乘以一个数字？并不完全是这样。1x1卷积执行起来非常便宜，因为它只有1次乘法，而不是3x3卷积中的9次或5x5卷积中的25次，并且它可以用来改变滤波器的数量。此外，你可以添加一个ReLU，引入一个非线性操作，这会增加网络可以学习的函数的复杂性。
- en: 'This module is called naïve because it is too computationally expensive. As
    the number of channels grows, the 3x3 and 5x5 convolutions become slow. The solution
    is to put 1x1 convolutions in front of them, to reduce the number of channels
    where the more expensive convolutions need to operate:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模块被称为天真，因为它计算成本太高。随着通道数的增加，3x3和5x5卷积变得缓慢。解决方案是在它们前面放置1x1卷积，以减少更昂贵的卷积需要操作的通道数：
- en: '![Figure 7.15 – Inception block including dimension reductions](img/Figure_7.15_B16322.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![图7.15 – 包含维度减少的Inception块](img/Figure_7.15_B16322.jpg)'
- en: Figure 7.15 – Inception block including dimension reductions
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15 – 包含维度减少的Inception块
- en: The key to understanding this block is to remember that the 1x1 convolutions
    are used to reduce the channels and improve performance significantly.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这个块的关键是要记住，1x1卷积用于减少通道数并显著提高性能。
- en: For example, the first Inception block in GoogLeNet has an input of 192 channels,
    and the 5x5 convolution would create 32 channels, so the number of multiplications
    would be proportional to 25 x 32 x 192 = 153,600.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，GoogLeNet中的第一个Inception块有192个通道，5x5的卷积会创建32个通道，所以乘法次数将与25 x 32 x 192 = 153,600成比例。
- en: They added a 1x1 convolution with an output of 16 filters, so the number of
    multiplications would be proportional to 16 x 192 + 25 x 32 x 16 = 3,072 + 12,800
    = 15,872\. Almost a 10x reduction. Not bad!
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 他们添加了一个输出为16个滤波器的1x1卷积，所以乘法次数将与16 x 192 + 25 x 32 x 16 = 3,072 + 12,800 = 15,872成比例。几乎减少了10倍。不错！
- en: One more thing. For the concatenation to work, all the convolutions need to
    have an output of the same size, which means that they need the padding, which
    keeps the same resolution of the input image. And what about max pooling? It also
    needs to have an output of the same size as the convolutions, so even if it finds
    the maximum in a 3x3 grid, it cannot reduce the size.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一件事。为了使连接操作生效，所有的卷积都需要有相同大小的输出，这意味着它们需要填充，以保持输入图像相同的分辨率。那么最大池化呢？它也需要有与卷积相同大小的输出，所以即使它在3x3的网格中找到最大值，也无法减小尺寸。
- en: 'In Keras, this means it would be something like this:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中，这可能是这样的：
- en: '[PRE13]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The `strides` parameter indicates how many pixels to move after calculating
    the maximum. By default, it's set at the same value of `pool_size`, which in our
    example would reduce the size 3 times. Setting it to `(1, 1)` with the same padding
    has the effect of not changing the size. The Conv2D layers also have a `strides`
    parameter, which can be used to reduce the size of the output; however, usually
    it is more effective to do so using a max pooling layer.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '`strides` 参数表示在计算最大值后移动多少像素。默认情况下，它设置为与 `pool_size` 相同的值，在我们的例子中这将使大小减少 3 倍。将其设置为
    `(1, 1)` 并使用相同的填充将不会改变大小。Conv2D 层也有一个 `strides` 参数，可以用来减小输出的大小；然而，通常使用最大池化层这样做更有效。'
- en: 'Inception v2 introduced some optimizations, among them the following:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: Inception v2 引入了一些优化，其中以下是一些：
- en: A 5x5 convolution is similar to two stacked 3x3 convolutions, but slower, so
    they refactored it with 3x3 convolutions.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 5x5 卷积类似于两个堆叠的 3x3 卷积，但速度较慢，因此他们用 3x3 卷积重构了它。
- en: A 3x3 convolution is equivalent to a 1x3 convolution followed by a 3x1 convolution,
    but using two convolutions is 33% faster.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3x3 卷积相当于一个 1x3 卷积后跟一个 3x1 卷积，但使用两个卷积要快 33%。
- en: 'Inception v3 introduced the following optimizations:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: Inception v3 引入了以下优化：
- en: Factorized 7x7 convolutions, created using several smaller and faster convolutions
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用几个较小的、较快的卷积创建的分解 7x7 卷积
- en: Some batch normalization layers
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些批量归一化层
- en: Inception-ResNet introduced the residual connections typical of ResNet, to skip
    some layers.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: Inception-ResNet 引入了 ResNet 典型的残差连接，以跳过一些层。
- en: Now that you have a better understanding of the concepts behind Inception, let's
    see how to use it in Keras.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你对 Inception 背后的概念有了更好的理解，让我们看看如何在 Keras 中使用它。
- en: Using Inception for image classification
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Inception 进行图像分类
- en: 'Loading Inception in Keras could not be simpler, as we can see here:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中加载 Inception 简单得不能再简单了，正如我们在这里可以看到的：
- en: '[PRE14]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As Inception can tell us the content of an image, let''s try it with the test
    image that we used at the beginning of the book:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Inception 可以告诉我们图像的内容，让我们尝试使用我们在本书开头使用的测试图像：
- en: '[PRE15]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This is the result:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 这是结果：
- en: '[PRE16]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'It is indeed right: our image depicts a sea lion from the Galapagos Islands:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 的确是正确的：我们的图像描绘了来自加拉帕戈斯群岛的海狮：
- en: '![Figure 7.16 – Recognized by Inception as a sea lion with 0.99184495 confidence](img/Figure_7.16_B16322.jpg)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.16 – Inception 以 0.99184495 的置信度识别为海狮](img/Figure_7.16_B16322.jpg)'
- en: Figure 7.16 – Recognized by Inception as a sea lion with 0.99184495 confidence
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.16 – Inception 以 0.99184495 的置信度识别为海狮
- en: But we want to use Inception for transfer learning, not for image classification,
    so we need to use it in a different way. Let's see how.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们想使用 Inception 进行迁移学习，而不是图像分类，因此我们需要以不同的方式使用它。让我们看看如何。
- en: Using Inception for transfer learning
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Inception 进行迁移学习
- en: 'The loading for transfer learning is a bit different, because we need to remove
    the classifier on top of Inception, as follows:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习的加载略有不同，因为我们需要移除 Inception 上方的分类器，如下所示：
- en: '[PRE17]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: With `input_shape`, we use the original size of Inception, but you could use
    a different shape, as long as it has 3 channels and the resolution is at least
    75x75.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `input_shape`，我们使用 Inception 的原始大小，但你可以使用不同的形状，只要它有 3 个通道，并且分辨率至少为 75x75。
- en: The important parameter is `include_top`, as setting it to `False` would remove
    the top part of Inception—the classifier with the dense filters—which means that
    the network will be ready for transfer learning.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的参数是 `include_top`，将其设置为 `False` 将会移除 Inception 的顶部部分——具有密集滤波器的分类器——这意味着网络将准备好进行迁移学习。
- en: 'We will now create a neural network that is based on Inception but can be modified
    by us:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将创建一个基于 Inception 但可以由我们修改的神经网络：
- en: '[PRE18]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, we can add a classifier on top of it, as follows:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以在其上方添加一个分类器，如下所示：
- en: '[PRE19]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We added some dropout, as we expect Inception to overfit quite a lot on our
    dataset. But please pay attention to the `GlobalAveragePooling2D`. What it does
    is compute the average of the channels.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 我们添加了一些 dropout，因为我们预计 Inception 在我们的数据集上会过度拟合很多。但请注意 `GlobalAveragePooling2D`。它所做的就是计算通道的平均值。
- en: We could use `Flatten`, but as Inception outputs 2,048 convolutional channels
    of 8x8, and we are using a dense layer with 1,024 neurons, the number of parameters
    would be huge—134,217,728! Using `GlobalAveragePooling2D`, we need only 2,097,152
    parameters. Even counting the parameters of Inception, the saving is quite significant—24,427,812
    parameters instead of 156,548,388.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`Flatten`，但由于Inception输出2,048个8x8的卷积通道，而我们使用了一个有1,024个神经元的密集层，参数数量会非常大——134,217,728！使用`GlobalAveragePooling2D`，我们只需要2,097,152个参数。即使计算Inception的参数，节省也是相当显著的——从156,548,388个参数减少到24,427,812个参数。
- en: 'There is one more thing that we need to do: freeze the layers of Inception
    that we don''t want to train. In this case, we want to freeze all of them, but
    this might not always be the case. This is how you can freeze them:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要做一件事：冻结我们不想训练的Inception层。在这种情况下，我们想冻结所有层，但这可能并不总是这样。这是冻结它们的方法：
- en: '[PRE20]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let''s check how our network looks. Inception is too big, so I will only show
    the data of the parameters:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下我们的网络看起来如何。Inception太大，所以我只会显示参数数据：
- en: '[PRE21]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Please note that `summary()` will actually print two summaries: one for Inception
    and one for our network; this is the output of the first summary:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`summary()`实际上会打印出两个摘要：一个用于Inception，一个用于我们的网络；这是第一个摘要的输出：
- en: '`Model: "sequential_1"`'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '`Model: "sequential_1"`'
- en: '[PRE22]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As you can see, the first layer is Inception. In the second summary, you also
    have a confirmation that Inception has the layers frozen, because we have more
    than 21 million non-trainable parameters, matching exactly the total number of
    parameters of Inception.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，第一层是Inception。在第二个摘要中，您也确认了Inception的层被冻结了，因为我们有超过2100万个不可训练的参数，正好与Inception的总参数数相匹配。
- en: 'To reduce the overfitting and to compensate for the small dataset, we will
    use data augmentation:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少过拟合并补偿小数据集，我们将使用数据增强：
- en: '[PRE23]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: I only applied a small rotation because traffic lights are usually pretty straight,
    and I also added only a small width shift because the traffic lights are detected
    by a neural network (SSD), so the cut tends to be very consistent. I also added
    a higher height shift because I saw that SSD sometimes wrongly cut the traffic
    lights, removing one-third of it.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我只进行了一小点旋转，因为交通灯通常很直，我也只添加了很小的宽度偏移，因为交通灯是由一个神经网络（SSD）检测的，所以切割往往非常一致。我还添加了更高的高度偏移，因为我看到SSD有时会错误地切割交通灯，移除其三分之一。
- en: Now that the network is ready, we just need to feed it our dataset.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，网络已经准备好了，我们只需要给它喂入我们的数据集。
- en: Feeding our dataset to Inception
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将我们的数据集输入到Inception中
- en: 'Let''s assume that you loaded your dataset in two variables: `images` and `labels`.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您已经将数据集加载到两个变量中：`images`和`labels`。
- en: 'Inception needs some preprocessing, to map the values of the images to the
    `[-1, +1]` range. Keras has a function that takes care of this, `preprocess_input()`.
    Please take care to import it from the `keras.applications.inception_v3` module,
    because there are other functions with the same name and different behavior in
    other modules:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: Inception需要一些预处理，将图像的值映射到`[-1, +1]`范围。Keras有一个函数可以处理这个问题，`preprocess_input()`。请注意，要从`keras.applications.inception_v3`模块导入它，因为其他模块中也有相同名称但不同行为的函数：
- en: '[PRE24]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We need to divide the dataset into training and validation, which is easy, but
    we also need to randomize the order, to be sure that the split is meaningful;
    for example, my code loads all the images with the same label, so a split without
    randomization would put only one or two labels in validation, and one of them
    might even not be present in training.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将数据集分为训练集和验证集，这很简单，但我们还需要随机化顺序，以确保分割有意义；例如，我的代码加载了所有具有相同标签的图像，所以如果没有随机化，验证集中只会有一两个标签，其中一个甚至可能不在训练集中。
- en: 'NumPy has a very handy function to generate new index positions, `permutation()`:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy有一个非常方便的函数来生成新的索引位置，`permutation()`：
- en: '[PRE25]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then, you can use **for comprehension**, a feature of Python, to change the
    order in your lists:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用Python的一个特性——**for comprehension**，来改变列表中的顺序：
- en: '[PRE26]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: If your labels are numeric, you can use `to_categorical()` to convert them into
    one-hot encoding.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的标签是数字，您可以使用`to_categorical()`将它们转换为one-hot编码。
- en: 'Now, it''s just a matter of slicing. We will use 20% of the samples for validation,
    so the code can be like this:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，只是切片的问题。我们将使用20%的样本进行验证，所以代码可以像这样：
- en: '[PRE27]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Now, you can train the network as usual. Let's see how it performs!
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以像往常一样训练网络。让我们看看它的表现如何！
- en: Performance with transfer learning
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迁移学习的性能
- en: 'The performance of the model is very good:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的性能非常好：
- en: '[PRE28]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Yes, 100% accuracy and validation accuracy! Not bad, not bad. Actually, it's
    very rewarding. However, the dataset was very simple, so it was fair to expect
    a very good result.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，100%的准确率和验证准确率！不错，不错。实际上，这非常令人满意。然而，数据集非常简单，所以预期一个非常好的结果是公平的。
- en: 'This is the graph of the losses:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 这是损失图的图表：
- en: '![Figure 7.17 – Losses with transfer learning from Inception](img/Figure_7.17_B16322.jpg)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![图7.17 – 使用Inception迁移学习的损失](img/Figure_7.17_B16322.jpg)'
- en: Figure 7.17 – Losses with transfer learning from Inception
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.17 – 使用Inception迁移学习的损失
- en: The problem is that despite the great results, the network does not work too
    well on my test images. Probably, it overfits, and I suspect that as the images
    are normally smaller than the native resolution of Inception, the network might
    experience patterns that are a result of interpolation instead of true patterns
    in the images, and maybe get confused by them, but that's just my theory.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，尽管结果很好，但网络在我的测试图像上表现并不太好。可能是因为图像通常比Inception的原生分辨率小，网络可能会遇到由插值产生的模式，而不是图像中的真实模式，并且可能被它们所困惑，但这只是我的理论。
- en: To get good results, we need to try harder.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得好的结果，我们需要更加努力。
- en: Improving transfer learning
  id: totrans-340
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 改进迁移学习
- en: We can assume that the network is overfitting, and the standard response would
    be to increase the dataset. In this case, it would be easy to do that, but let's
    pretend that we cannot do it, so we can explore other options that can be useful
    to you in similar cases.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以假设网络正在过拟合，标准的应对措施是增加数据集。在这种情况下，这样做很容易，但让我们假设我们无法这样做，因此我们可以探索其他在这种情况下可能对你有用的选项。
- en: 'There are some very easy things that we can do to reduce overfitting:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做一些非常简单的事情来减少过拟合：
- en: Increase the variety of the data augmentation.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加数据增强的多样性。
- en: Increase dropout.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加dropout。
- en: 'Despite the fact that Inception is clearly able to process tasks much more
    complex than this, it is not optimized for this specific task, and it is also
    possible that it could benefit from a bigger classifier, so I will add a layer:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Inception显然能够处理比这更复杂的任务，但它并不是为此特定任务优化的，而且它也可能从更大的分类器中受益，所以我将添加一个层：
- en: 'This is the new data augmentation, after a few tests:'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是经过几次测试后的新数据增强：
- en: '[PRE29]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This is the new model, with more dropout and an additional layer:'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是新模型，具有更多的dropout和额外的层：
- en: '[PRE30]'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: I added a dropout after the global average pooling, to reduce overfitting, and
    I also added a batch normalization layer, which can also help to reduce overfitting.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我在全局平均池化之后添加了一个dropout，以减少过拟合，我还添加了一个批量归一化层，这也有助于减少过拟合。
- en: Then, I added a dense layer, but I did not put a dropout on it, because I noticed
    the network had problems training with so much dropout.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我添加了一个密集层，但我没有在上面添加dropout，因为我注意到网络在大量dropout的情况下训练存在问题。
- en: 'Even if we don''t want to increase the dataset, we can still do something about
    it. Let''s look at the distribution of the classes:'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 即使我们不希望增加数据集，我们仍然可以对此采取一些措施。让我们看看类别的分布：
- en: '[PRE31]'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This is the result:'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是结果：
- en: '[PRE32]'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: As you see, the dataset has much more green than yellow or red and not many
    negative samples.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，数据集中绿色比黄色或红色多得多，而且负面样本不多。
- en: In general, it is not good to have imbalanced labels, and the network was indeed
    predicting more green lights than there were, because statistically, it is more
    rewarding to predict a green than any other label. To improve this situation,
    we can instruct Keras to customize the loss in a way that predicting a wrong red
    would be worse than predicting a wrong green, as this would have an effect similar
    to making the dataset balanced.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 通常来说，标签不平衡并不是一个好的现象，因为实际上网络预测的绿灯比实际存在的绿灯要多，这在统计上意味着预测绿灯比预测其他标签更有利可图。为了改善这种情况，我们可以指导Keras以这种方式自定义损失函数，即预测错误的红灯比预测错误绿灯更糟糕，这样会产生类似于使数据集平衡的效果。
- en: 'You can do this with these two lines of code:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以用这两行代码做到这一点：
- en: '[PRE33]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The following is the result:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 以下结果是：
- en: '[PRE34]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: As you can see, the loss penalty would be less for green (label 0) than for
    the other ones.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，对于绿色（标签0）的损失惩罚比其他标签要小。
- en: 'This is how the network performs:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 这是网络的表现方式：
- en: '[PRE35]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Not much different than before, but this time, the network works better and
    nails all the traffic lights in my test images. This should be a reminder to not
    trust the validation accuracy completely unless you are sure that your validation
    dataset is excellent.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前没有太大不同，但这次，网络表现更好，并准确识别了我测试图像中的所有交通灯。这应该是一个提醒，不要完全相信验证准确率，除非你确信你的验证数据集非常优秀。
- en: 'This is the graph of the losses:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 这是损失图的图表：
- en: '![Figure 7.18 – Losses with transfer learning from Inception, improved](img/Figure_7.18_B16322.jpg)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
  zh: '![图7.18 – 使用Inception进行迁移学习后的损失，改进](img/Figure_7.18_B16322.jpg)'
- en: Figure 7.18 – Losses with transfer learning from Inception, improved
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.18 – 使用Inception进行迁移学习后的损失，改进
- en: Now that we have a good network, it's time to complete our task, using the new
    network in combination with SSD, as detailed in the next section.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个好的网络，是时候完成我们的任务了，使用新的网络结合SSD，如下一节所述。
- en: Recognizing traffic lights and their colors
  id: totrans-370
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别交通灯及其颜色
- en: 'We are almost done. From the code using SSD, we just need to manage the traffic
    light in a different way. So, when the label is `10` (traffic light), we need
    to do the following:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎完成了。从使用SSD的代码中，我们只需要以不同的方式管理交通灯。因此，当标签是`10`（交通灯）时，我们需要做以下操作：
- en: Crop the area with the traffic light.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 裁剪带有交通灯的区域。
- en: Resize it to 299x299.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整大小为299x299。
- en: Preprocess it.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理它。
- en: Run it through our network.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过我们的网络运行它。
- en: 'Then, we will get the prediction:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将得到预测结果：
- en: '[PRE36]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: If you run the code of this chapter that is in GitHub, the label `0` is the
    green light, `1` is yellow, `2` is red, and `3` means that it is not a traffic
    light.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行GitHub上本章的代码，标签`0`代表绿灯，`1`代表黄灯，`2`代表红灯，而`3`表示不是交通灯。
- en: 'The whole process involves first detecting objects with SSD, and then using
    our network to detect the color of traffic lights, if any are present in the image,
    as explained in the following diagram:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 整个过程首先涉及使用SSD检测物体，然后使用我们的网络检测图像中是否存在交通灯的颜色，如以下图解所示：
- en: '![Figure 7.19 – A diagram showing how to use SSD and our network together](img/Figure_7.19_B16322.jpg)'
  id: totrans-380
  prefs: []
  type: TYPE_IMG
  zh: '![图7.19 – 展示如何结合使用SSD和我们的网络的图解](img/Figure_7.19_B16322.jpg)'
- en: Figure 7.19 – A diagram showing how to use SSD and our network together
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.19 – 展示如何结合使用SSD和我们的网络的图解
- en: 'These are examples obtained running SSD followed by our network:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是在运行SSD后运行我们的网络获得的示例：
- en: '![Figure 7.20 – Some detections with traffic lights](img/Figure_7.20_B16322.jpg)'
  id: totrans-383
  prefs: []
  type: TYPE_IMG
  zh: '![图7.20 – 带有交通灯的一些检测](img/Figure_7.20_B16322.jpg)'
- en: Figure 7.20 – Some detections with traffic lights
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.20 – 带有交通灯的一些检测
- en: 'The colors of the traffic lights are now detected properly. There are some
    false detections: for example, in the preceding figure, the image on the right
    marks a person where there is a tree. Unfortunately, this can happen. In a video,
    we could require detection for a few frames before accepting it, always considering
    that in a real self-driving car, you cannot introduce high latency, because the
    car needs to react quickly to what''s happening on the street.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 现在交通灯的颜色已经正确检测。有一些误检：例如，在前面的图中，右边的图像标记了一个有树的地方。不幸的是，这种情况可能发生。在视频中，我们可以在接受之前要求检测几帧，始终考虑到在真正的自动驾驶汽车中，你不能引入高延迟，因为汽车需要快速对街道上发生的事情做出反应。
- en: Summary
  id: totrans-386
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we focused on pre-trained neural networks, and how we can leverage
    them for our purposes. We combined two neural networks to detect pedestrians,
    vehicles, and traffic lights, including their color. We first discussed how to
    use Carla to collect images, and then we discovered SSD, a powerful neural network
    that stands out for its capacity to detect not only objects, but also their position
    in an image. We also saw the TensorFlow detection model zoo and how to use Keras
    to download the desired version of SSD, trained on a dataset called COCO.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们专注于预训练的神经网络，以及我们如何利用它们来实现我们的目的。我们将两个神经网络结合起来检测行人、车辆和交通灯，包括它们的颜色。我们首先讨论了如何使用Carla收集图像，然后发现了SSD，这是一个强大的神经网络，因其能够检测物体及其在图像中的位置而突出。我们还看到了TensorFlow检测模型库以及如何使用Keras下载在名为COCO的数据集上训练的SSD的所需版本。
- en: In the second part of the chapter, we discussed a powerful technique called
    transfer learning, and we studied some of the solutions of a neural network called
    Inception, which we trained on our dataset using transfer learning, to be able
    to detect the colors of traffic lights. In the process, we also talked about ImageNet,
    and we saw how achieving 100% validation accuracy was misleading, and as a result,
    we had to reduce the overfitting to improve the real precision of the network.
    In the end, we succeeded in using the two networks together—one to detect pedestrians,
    vehicles, and traffic lights, and one to detect the color of the traffic lights.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第二部分，我们讨论了一种称为迁移学习的强大技术，并研究了Inception神经网络的一些解决方案，我们使用迁移学习在我们的数据集上训练它，以便能够检测交通灯的颜色。在这个过程中，我们还讨论了ImageNet，并看到了达到100%验证准确率是如何具有误导性的，因此，我们必须减少过拟合以提高网络的真正精度。最终，我们成功地使用两个网络一起工作——一个用于检测行人、车辆和交通灯，另一个用于检测交通灯的颜色。
- en: Now that we know how to build knowledge about the road, it's time to move forward
    to the next task—driving! In the next chapter, we will literally sit in the driving
    seat (of Carla), and teach our neural network how to drive, using a technique
    called behavioral cloning, where our neural network will try to mimic our behavior.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经知道了如何构建关于道路的知识，那么现在是时候前进到下一个任务——驾驶！在下一章中，我们将真正地坐在驾驶座上（Carla的驾驶座），并使用一种称为行为克隆的技术来教我们的神经网络如何驾驶，我们的神经网络将尝试模仿我们的行为。
- en: Questions
  id: totrans-390
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'You should now be able to answer the following questions:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在应该能够回答以下问题：
- en: What is SSD?
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是SSD？
- en: What is Inception?
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是Inception？
- en: What does it mean to freeze a layer?
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 冻结一层意味着什么？
- en: Can SSD detect the color of a traffic light?
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SSD能否检测交通灯的颜色？
- en: What is transfer learning?
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是迁移学习？
- en: Can you name some techniques to reduce overfitting?
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能列举一些减少过拟合的技术吗？
- en: Can you describe the idea behind the Inception block?
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能描述Inception块背后的想法吗？
- en: Further reading
  id: totrans-399
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'SSD: [https://arxiv.org/abs/1512.02325](https://arxiv.org/abs/1512.02325)'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SSD：[https://arxiv.org/abs/1512.02325](https://arxiv.org/abs/1512.02325)
- en: 'TensorFlow model zoo: [https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md)'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow模型库：[https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md)
- en: 'COCO labels: [https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_label_map.pbtxt](https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_label_map.pbtxt)'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: COCO标签：[https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_label_map.pbtxt](https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_label_map.pbtxt)
- en: 'Vanishing gradient problem: [https://en.wikipedia.org/wiki/Vanishing_gradient_problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度消失问题：[https://en.wikipedia.org/wiki/Vanishing_gradient_problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)
- en: 'The Bosch Small Traffic Lights dataset: [https://hci.iwr.uni-heidelberg.de/content/bosch-small-traffic-lights-dataset](https://hci.iwr.uni-heidelberg.de/content/bosch-small-traffic-lights-dataset)'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 博世小型交通灯数据集：[https://hci.iwr.uni-heidelberg.de/content/bosch-small-traffic-lights-dataset](https://hci.iwr.uni-heidelberg.de/content/bosch-small-traffic-lights-dataset)
- en: 'ImageNet: [http://www.image-net.org/](http://www.image-net.org/)'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ImageNet：[http://www.image-net.org/](http://www.image-net.org/)
- en: 'Inception paper: [https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43022.pdf](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43022.pdf)'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Inception论文：[https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43022.pdf](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43022.pdf)
- en: 'AlexNet paper: [https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AlexNet论文：[https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)
