<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Predictive Modeling</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>Building a linear classifier using <strong>support vector machines</strong><span> (</span><strong>SVMs</strong><span>)</span></li>
<li>Building a nonlinear classifier using <span>SVMs</span></li>
<li>Tackling class imbalance</li>
<li>Extracting confidence measurements</li>
<li>Finding optimal hyperparameters</li>
<li>Building an event predictor</li>
<li>Estimating traffic</li>
<li>Simplifying a machine learning workflow using TensorFlow</li>
<li>Implementing the stacking method</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>To address the recipes in this chapter, you need the following files (available on GitHub):</p>
<ul>
<li><kbd>svm.py</kbd></li>
<li><kbd>data_multivar.txt</kbd></li>
<li><kbd>svm_imbalance.py</kbd></li>
<li><kbd>data_multivar_imbalance.txt</kbd></li>
<li><kbd>svm_confidence.py</kbd></li>
<li><kbd>perform_grid_search.py</kbd></li>
<li><kbd>building_event_binary.txt</kbd></li>
<li><kbd>building_event_multiclass.txt</kbd></li>
<li><kbd><kbd>event.py</kbd></kbd></li>
<li><kbd>traffic_data.txt</kbd></li>
<li><kbd>traffic.py</kbd></li>
<li><kbd>IrisTensorflow.py</kbd></li>
<li><kbd>stacking.py</kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p><strong>Predictive modeling</strong> is probably one of the most exciting fields in data analytics. It has gained a lot of attention in recent years due to massive amounts of data being available in many different verticals. It is very commonly used in areas concerning data mining to forecast future trends.</p>
<p>Predictive modeling is an analysis technique that is used to predict the future behavior of a system. It is a collection of algorithms that can identify the relationship between independent input variables and the target responses. We create a mathematical model, based on observations, and then use this model to estimate what's going to happen in the future.</p>
<p>In predictive modeling, we need to collect data with known responses to train our model. Once we create this model, we validate it using some metrics, and then use it to predict future values. We can use many different types of algorithms to create a predictive model. In this chapter, we will use SVMs to build linear and nonlinear models.</p>
<p>A predictive model is built using a number of features that are likely to influence the behavior of the system. For example, to estimate weather conditions, we may use various types of data, such as temperature, barometric pressure, precipitation, and other atmospheric processes. Similarly, when we deal with other types of systems, we need to decide what factors are likely to influence its behavior and include them as part of the feature vector before training our model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a linear classifier using SVMs</h1>
                </header>
            
            <article>
                
<p>SVMs are supervised learning models that we can use to create classifiers and regressors. An SVM solves a system of mathematical equations and finds the best separating boundary between two sets of points. Let's see how to build a linear classifier using an SVM.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Let's visualize our data to understand the problem at hand. We will use the <kbd>svm.py</kbd> file for this. Before we build the SVM, let's understand our data. We will use the <kbd>data_multivar.txt</kbd> file that's already provided to you. Let's see how to to visualize the data:</p>
<ol>
<li>Create a new Python file and add the following lines to it <span>(t</span><span>he full code is in the <kbd>svm.py</kbd></span><span> </span><span>file which has already been provided to you</span><span>)</span>:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np 
import matplotlib.pyplot as plt 
 
import utilities  
 
# Load input data 
input_file = 'data_multivar.txt' 
X, y = utilities.load_data(input_file) </pre>
<ol start="2">
<li>We just imported a couple of packages and named the input file. Let's look at the <kbd>load_data()</kbd> method:</li>
</ol>
<pre style="padding-left: 60px"># Load multivar data in the input file 
def load_data(input_file): 
    X = [] 
    y = [] 
    with open(input_file, 'r') as f: 
        for line in f.readlines(): 
            data = [float(x) for x in line.split(',')] 
            X.append(data[:-1]) 
            y.append(data[-1])  
 
    X = np.array(X) 
    y = np.array(y) 
 
    return X, y </pre>
<ol start="3">
<li>We need to separate the data into classes, as follows:</li>
</ol>
<pre style="padding-left: 60px">class_0 = np.array([X[i] for i in range(len(X)) if y[i]==0]) 
class_1 = np.array([X[i] for i in range(len(X)) if y[i]==1]) </pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="4">
<li>Now that we have separated the data, let's plot it:</li>
</ol>
<pre style="padding-left: 60px">plt.figure() 
plt.scatter(class_0[:,0], class_0[:,1], facecolors='black', edgecolors='black', marker='s') 
plt.scatter(class_1[:,0], class_1[:,1], facecolors='None', edgecolors='black', marker='s') 
plt.title('Input data') 
plt.show() </pre>
<p style="padding-left: 60px">If you run this code, you will see the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-951 image-border" src="assets/2eaae6f7-a135-4d73-95d5-8f2391f2b33d.png" style="width:128.75em;height:66.33em;"/></p>
<p>The preceding consists of two types of points—<strong>solid squares</strong> and <strong>empty squares</strong>. In machine learning lingo, we say that our data consists of two classes. Our goal is to build a model that can separate the solid squares from the empty squares.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will learn how to build a linear classifier using SVMs:</p>
<ol>
<li>We need to split our dataset into training and testing datasets. Add the following lines to the same Python file:</li>
</ol>
<pre style="padding-left: 60px"># Train test split and SVM training 
from sklearn import cross_validation 
from sklearn.svm import SVC 
 
X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.25, random_state=5) </pre>
<ol start="2">
<li>Let's initialize the SVM object using a <kbd>linear</kbd> kernel. Add the following lines to the file:</li>
</ol>
<pre style="padding-left: 60px">params = {'kernel': 'linear'} 
classifier = SVC(**params, gamma='auto') </pre>
<ol start="3">
<li>We are now ready to train the linear SVM classifier:</li>
</ol>
<pre style="padding-left: 60px">classifier.fit(X_train, y_train) </pre>
<ol start="4">
<li>We can now see how the classifier performs:</li>
</ol>
<pre style="padding-left: 60px">utilities.plot_classifier(classifier, X_train, y_train, 'Training dataset') 
plt.show() </pre>
<p style="padding-left: 60px">If you run this code, you will get the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-952 image-border" src="assets/f84ac53f-cb82-4d3a-8a16-0cf166e316fa.png" style="width:126.67em;height:65.00em;"/></p>
<p class="mce-root"/>
<p style="padding-left: 60px">The <kbd>plot_classifier</kbd> function is the same as we discussed in <a href="f552bbc7-5e56-41b8-8e8d-915cc1bd53ab.xhtml">Chapter 1</a>, <em>The Realm of Supervised Learning</em>. It has a couple of minor additions.</p>
<div style="padding-left: 60px" class="packt_tip">You can check out the <kbd>utilities.py</kbd> file already provided to you for more details.</div>
<ol start="5">
<li>Let's see how this performs on the test dataset. Add the following lines to the <kbd>svm.py</kbd> file:</li>
</ol>
<pre style="padding-left: 60px">y_test_pred = classifier.predict(X_test) 
utilities.plot_classifier(classifier, X_test, y_test, 'Test dataset') 
plt.show()</pre>
<p style="padding-left: 60px">If you run this code, you will see the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-953 image-border" src="assets/eddb9d17-57c0-4936-8873-7a962c1a6bfa.png" style="width:126.92em;height:64.58em;"/></p>
<p style="padding-left: 60px">As you can see, the classifier boundaries on the input data are clearly identified.</p>
<ol start="6">
<li>Let's compute the accuracy for the training set. Add the following lines to the same file:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.metrics import classification_report 
 
target_names = ['Class-' + str(int(i)) for i in set(y)]<br/>print("\n" + "#"*30)<br/>print("\nClassifier performance on training dataset\n")<br/>print(classification_report(y_train, classifier.predict(X_train), target_names=target_names))<br/>print("#"*30 + "\n")</pre>
<p style="padding-left: 60px">If you run this code, you will see the following on your Terminal:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-954 image-border" src="assets/2acae4ae-b68e-4230-bee1-b7a3ed3c7da8.png" style="width:30.42em;height:10.08em;"/></p>
<ol start="7">
<li>Finally, let's see the classification report for the testing dataset:</li>
</ol>
<pre style="padding-left: 60px">print("#"*30)<br/>print("\nClassification report on test dataset\n")<br/>print(classification_report(y_test, y_test_pred, target_names=target_names))<br/>print("#"*30 + "\n")</pre>
<ol start="8">
<li>If you run this code, you will see the following on the Terminal:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-955 image-border" src="assets/cfadb22c-c1e3-4aa0-a941-5f64a3c06a70.png" style="width:30.33em;height:10.33em;"/></p>
<p class="mce-root"/>
<p>From the output screenshot where we visualized the data, we can see that the solid squares are completely surrounded by empty squares. This means that the data is not linearly separable. We cannot draw a nice straight line to separate the two sets of points! Hence, we need a nonlinear classifier to separate these datapoints.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>SVMs are a set of supervised learning methods that can be used for both classification and regression. Given two classes of linearly separable multidimensional patterns, among all the possible separating hyperplanes, the SVM algorithm determines the one able to separate the classes with the greatest possible margin. The margin is the minimum distance of the points in the two classes in the training set from the hyperplane identified. </p>
<p>Maximization of the margin is linked to generalization. If the training set patterns are classified with a large margin, you can hope that even test-set patterns close to the boundary between the classes are managed correctly. In the following, you can see three lines (<strong>l1</strong>, <strong>l2</strong>, and <strong>l3</strong>). Line <strong>l1</strong> does not separate the two classes, line <strong>l2</strong> separates them, but with a small margin, while line <strong>l3</strong> maximizes the distance between the two classes:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-956 image-border" src="assets/204252af-0a4c-4aa1-ac10-b13f0d018eb1.png" style="width:19.50em;height:17.33em;"/></p>
<p>SVMs can be used to separate classes that cannot be separated with a linear classifier. Object coordinates are mapped into a space called a <strong>feature space</strong> using non-linear functions, called <strong>characteristic functions</strong>. This space is highly multidimensional, in which the two classes can be separated with a linear classifier. So, the initial space is remapped in the new space, at which point the classifier is identified and then returned to the initial space.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p><strong>SVMs</strong> constitute a class of learning machines recently introduced in the literature. SVMs derive from concepts concerning the statistical theory of learning and present theoretical generalization properties. The theory that governs the functioning mechanisms of SVMs was introduced by Vapnik in 1965 (statistical learning theory), and was more recently perfected, in 1995, by Vapnik himself, and others. SVMs are one of the most widely used tools for pattern classification. Instead of estimating the probability densities of classes, Vapnik suggests directly solving the problem of interest, that is, to determine the decisional surfaces between the classes (classification boundaries).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>Refer to the official documentation of the <kbd>sklearn.svm.SVC()</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html">https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html</a></li>
<li>Refer to <em>Support Vector Machine Tutorial</em> (from Columbia University): <a href="http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf">http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf</a></li>
<li>Refer to <em>Support Vector Machines</em> - Lecture notes (by Andrew Ng from Stanford <span>University</span>): <a href="http://cs229.stanford.edu/notes/cs229-notes3.pdf">http://cs229.stanford.edu/notes/cs229-notes3.pdf</a></li>
<li><em>Tutorial on Support Vector Machine</em> <span>(from Washington State University): <a href="https://course.ccs.neu.edu/cs5100f11/resources/jakkula.pdf">https://course.ccs.neu.edu/cs5100f11/resources/jakkula.pdf</a></span></li>
<li><em>SVM Tutorial</em>: <a href="http://web.mit.edu/zoya/www/SVM.pdf">http://web.mit.edu/zoya/www/SVM.pdf</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a nonlinear classifier using SVMs</h1>
                </header>
            
            <article>
                
<p>An SVM provides a variety of options to build a nonlinear classifier. We need to build a nonlinear classifier using various kernels. In this recipe, let's consider two cases here. When we want to represent a curvy boundary between two sets of points, we can either do this using a polynomial function or a radial basis function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will use the same file used in the previous recipe, <em>Building a linear classifier using SVMs</em>, but in this case, we will use a different kernel to deal with a markedly nonlinear problem.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how to build a nonlinear classifier using SVMs:</p>
<ol>
<li>For the first case, let's use a polynomial kernel to build a nonlinear classifier. In the same Python file (<kbd>svm.py</kbd>), search for the following line:</li>
</ol>
<pre style="padding-left: 60px">params = {'kernel': 'linear'} </pre>
<p style="padding-left: 60px">Replace this line with the following:</p>
<pre style="padding-left: 60px">params = {'kernel': 'poly', 'degree': 3} </pre>
<p style="padding-left: 60px">This means that we use a polynomial function with <kbd>degree</kbd> as <kbd>3</kbd>. If we increase the degree, this means we allow the polynomial to be curvier. However, curviness comes at a cost, in the sense that it will take more time to train because it's more computationally expensive.</p>
<ol start="2">
<li>If you run this code now, you will get the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-957 image-border" src="assets/95a043a1-9e6c-4225-81bb-f7ae523c53bf.png" style="width:126.75em;height:65.25em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="3">
<li>You will also see the following classification report printed on your Terminal:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-958 image-border" src="assets/753776b5-ad66-441d-9c1f-e34492074839.png" style="width:30.75em;height:10.50em;"/></p>
<ol start="4">
<li>We can also use a radial basis function kernel to build a nonlinear classifier. In the same Python file, search for the following line:</li>
</ol>
<pre style="padding-left: 60px">params = {'kernel': 'poly', 'degree': 3} </pre>
<ol start="5">
<li class="packt_nosymbol">Replace this line with the following one:</li>
</ol>
<pre style="padding-left: 60px">params = {'kernel': 'rbf'} </pre>
<ol start="6">
<li>If you run this code now, you will get the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-959 image-border" src="assets/75773f4a-5e75-462d-b1b6-c41949ef9137.png" style="width:126.75em;height:65.58em;"/></p>
<p class="mce-root"/>
<ol start="7">
<li>You will also see the following classification report printed on your Terminal:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-960 image-border" src="assets/fded2682-2b0a-4fff-86dd-6a1691409256.png" style="width:31.08em;height:10.50em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In this recipe, we have used an SVM classifier to <span>find the best separating boundary between a dataset of points by solving a system of mathematical equations.</span> To address a nonlinear problem, we used Kernel methods. Kernel methods are thus named for Kernel functions, which are used to operate in the feature space without calculating data coordinates in space, but rather by calculating the internal product between images of all copies of data in the function space. The calculation of the internal product is often computationally cheaper than the explicit calculation of the coordinates. This method is called the <strong>Kernel stratagem</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>The main point of the SVM is that a generic problem can always be solved as long as you <span>carefully </span>choose the kernel and all its parameters—for example, going to make a total overfitting of the input dataset. The problem with this method is that it scales quite badly with the size of the dataset, as it is classically attributed to a D2 factor, even if, in this sense, faster implementations can be obtained by optimizing this aspect. The problem is identifying the best kernel and providing it with the best parameters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><em>Support Vector Machines and Kernel Methods</em> (from Carnegie Mellon's School of Computer Science): <a href="https://www.cs.cmu.edu/~ggordon/SVMs/new-svms-and-kernels.pdf">https://www.cs.cmu.edu/~ggordon/SVMs/new-svms-and-kernels.pdf</a></li>
<li><em>Support Vector Machines and Kernel Methods</em> (from the Department of Computer Science, National Taiwan University): <a href="https://www.csie.ntu.edu.tw/~cjlin/talks/postech.pdf">https://www.csie.ntu.edu.tw/~cjlin/talks/postech.pdf</a></li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tackling class imbalance</h1>
                </header>
            
            <article>
                
<p>Until now, we dealt with problems where we had a similar number of datapoints in all our classes. In the real world, we might not be able to get data in such an orderly fashion. Sometimes, the number of datapoints in one class is a lot more than the number of datapoints in other classes. If this happens, then the classifier tends to get biased. The boundary won't reflect the true nature of your data, just because there is a big difference in the number of datapoints between the two classes. Therefore, it is important to account for this discrepancy and neutralize it so that our classifier remains impartial.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will use a new dataset, named <kbd>data_multivar_imbalance.txt</kbd>, in which there are three values for each line; the first two represent the coordinates of the point, the third, the class to which the point belongs. Our aim is, once again, to build a classifier, but this time, we will have to face a data-balancing problem.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how to tackle class imbalance:</p>
<ol>
<li>Let's import the libraries:</li>
</ol>
<pre>import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn.svm import SVC<br/>import utilities</pre>
<ol start="2">
<li>Let's load the data (<kbd><span>data_multivar_imbalance.txt</span></kbd>):</li>
</ol>
<pre style="padding-left: 60px">input_file = 'data_multivar_imbalance.txt' 
X, y = utilities.load_data(input_file) <br/><br/></pre>
<ol start="3">
<li>Let's visualize the data. The code for visualization is exactly the same as it was in the previous recipe. You can also find it in the file named <kbd>svm_imbalance.py</kbd>, already provided to you:</li>
</ol>
<pre style="padding-left: 60px"># Separate the data into classes based on 'y'<br/>class_0 = np.array([X[i] for i in range(len(X)) if y[i]==0])<br/>class_1 = np.array([X[i] for i in range(len(X)) if y[i]==1])<br/># Plot the input data<br/>plt.figure()<br/>plt.scatter(class_0[:,0], class_0[:,1], facecolors='black', edgecolors='black', marker='s')<br/>plt.scatter(class_1[:,0], class_1[:,1], facecolors='None', edgecolors='black', marker='s')<br/>plt.title('Input data')<br/>plt.show()</pre>
<ol start="4">
<li>If you run it, you will see the following:</li>
</ol>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-961 image-border" src="assets/d0f0f3ab-87e7-4ebc-9320-b88eb858b978.png" style="width:86.08em;height:45.92em;"/></p>
<ol start="5">
<li><span>Let's build an SVM with a linear kernel. The code is the same as it was in the previous recipe,</span> <em>Building a nonlinear classifier using SVM</em><em>s</em><span>:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">from sklearn import model_selection<br/>X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=5)<br/>params = {'kernel': 'linear'}<br/>classifier = SVC(**params, gamma='auto')<br/>classifier.fit(X_train, y_train)<br/>utilities.plot_classifier(classifier, X_train, y_train, 'Training dataset')<br/><span>plt.show()</span></pre>
<ol start="6">
<li>Let's print a <span>classification report:</span></li>
</ol>
<pre style="padding-left: 60px">from sklearn.metrics import classification_report<br/>target_names = ['Class-' + str(int(i)) for i in set(y)]<br/>print("\n" + "#"*30)<br/>print("\nClassifier performance on training dataset\n")<br/>print(classification_report(y_train, classifier.predict(X_train), target_names=target_names))<br/>print("#"*30 + "\n")<br/>print("#"*30)<br/>print("\nClassification report on test dataset\n")<br/>print(classification_report(y_test, y_test_pred, target_names=target_names))<br/>print("#"*30 + "\n")</pre>
<ol start="7">
<li>If you run it<span>, you will see the following:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-962 image-border" src="assets/a24acb1e-3b1a-4706-8c62-81a0e68375eb.png" style="width:83.00em;height:42.50em;"/></p>
<ol start="8">
<li>You might wonder why there's no boundary here! Well, this is because the classifier is unable to separate the two classes at all, resulting in 0% accuracy for <kbd>Class-0</kbd>. You will also see a classification report printed on your Terminal, as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-963 image-border" src="assets/35bb2949-b30d-4e69-86b4-b332bcfc08d7.png" style="width:31.25em;height:8.17em;"/></p>
<ol start="9">
<li class="packt_nosymbol">As we expected, <kbd>Class-0</kbd> has 0% precision, so let's go ahead and fix this! In the Python file, search for the following line:</li>
</ol>
<pre style="padding-left: 60px">params = {'kernel': 'linear'}</pre>
<ol start="10">
<li class="packt_nosymbol">Replace the preceding line with the following:</li>
</ol>
<pre style="padding-left: 60px">params = {'kernel': 'linear', 'class_weight': 'balanced'}  </pre>
<ol start="11">
<li class="packt_nosymbol">The <kbd>class_weight</kbd> parameter will count the number of datapoints in each class to adjust the weights so that the imbalance doesn't adversely affect the performance.</li>
<li>You will get the following output once you run this code:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-964 image-border" src="assets/24b86379-bcaf-4034-8595-1f224f6505a2.png" style="width:43.42em;height:21.92em;"/></p>
<ol start="13">
<li>Let's look at the classification report:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-965 image-border" src="assets/2ef0355b-2065-4e8d-a8dd-c1445d6395ae.png" style="width:31.75em;height:8.33em;"/></p>
<ol start="14">
<li>As we can see, <kbd>Class-0</kbd> is now detected with nonzero percentage accuracy.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In this recipe, we have used a SVM classifier to find the best separating boundary between a dataset of points. To address a data-balancing problem, we<span> once again</span> used the linear Kernel method, but we implemented a <kbd>class_weight</kbd><span> keyword</span> in the <kbd>fit</kbd> method. The <kbd>class_weight</kbd> variable is a dictionary in the form <kbd>{class_label: value}</kbd>, where <kbd>value</kbd> is a floating-point number greater than 0 that modifies the <em>C</em> parameter of the class <kbd>(class_label)</kbd>, setting it with a new value, obtained by multiplying the old <em>C</em> value with that specified in the value attribute (<em>C * value</em>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p><em>C</em> is a hyperparameter that determines the penalty for the incorrect classification of an observation. So, we used a weight for the classes to manage unbalanced classes. In this way, we will assign a new value of <em>C</em> to the classes, defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0290b38f-9856-45fd-bee8-b57fb1163007.png" style="width:6.67em;height:1.25em;"/></p>
<p>Where <em><span class="underline">C</span></em> is the penalty, <em>w<sub>i</sub></em> is a weight inversely proportional to class i's frequency, and <em>C<sub>i</sub></em> is the <em>C</em> value for class <em>i</em>. This method suggests increasing the penalty to classify the less represented classes so as to prevent them from being outclassed by the most represented class. </p>
<p>In the <kbd>scikit-learn</kbd> library, when using SVC, we can set the values for <em>C<sub>i</sub></em> automatically by setting <kbd>class_weight='balanced'</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><em>Support Vector Machines</em>—official documentation of the <kbd>scikit-learn</kbd> library: <a href="https://scikit-learn.org/stable/modules/svm.html">https://scikit-learn.org/stable/modules/svm.html</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extracting confidence measurements</h1>
                </header>
            
            <article>
                
<p>It would be nice to know the confidence with which we classify unknown data. When a new datapoint is classified into a known category, we can train the SVM to compute the confidence level of that output as well. A <em>confidence level</em> refers to the probability that the value of a parameter falls within a specified range of values.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p><span>In this recipe, we will use an SVM classifier to </span><span>find the best separating boundary between a dataset of points. In addition, we will also perform a measure of the confidence level of the results obtained.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how to extract confidence measurements:</p>
<ol>
<li>The full code is given in the <kbd>svm_confidence.py</kbd> file, already provided to you. We will discuss the code of the recipe here. Let's define some input data:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn.svm import SVC<br/>import utilities<br/><br/># Load input data<br/>input_file = 'data_multivar.txt'<br/>X, y = utilities.load_data(input_file)</pre>
<ol start="2">
<li class="mce-root">At this point, we split the data for training and testing, and then we will build the classifier:</li>
</ol>
<pre style="color: black;padding-left: 60px">from sklearn import model_selection<br/>X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=5)<br/>params = {'kernel': 'rbf'}<br/>classifier = SVC(**params, gamma='auto')<br/>classifier.fit(X_train, y_train)</pre>
<ol start="3">
<li class="mce-root">Define the input datapoint<span>:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">input_datapoints = np.array([[2, 1.5], [8, 9], [4.8, 5.2], [4, 4], [2.5, 7], [7.6, 2], [5.4, 5.9]])</pre>
<ol start="4">
<li>Let's measure the distance from the boundary:</li>
</ol>
<pre style="padding-left: 60px">print("Distance from the boundary:")<br/>for i in input_datapoints:<br/>    print(i, '--&gt;', classifier.decision_function([i])[0])</pre>
<ol start="5">
<li>You will see the following printed on your Terminal:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-966 image-border" src="assets/d4f5f260-388d-4a8a-bd54-361368f0bb5d.png" style="width:23.33em;height:9.50em;"/></p>
<ol start="6">
<li>The distance from the boundary gives us some information about the datapoint, but it doesn't exactly tell us how confident the classifier is about the output tag. To do this, we need <strong>Platt scaling</strong>. This is a method that converts the distance measure into a probability measure between classes. Let's go ahead and train an SVM using Platt scaling:</li>
</ol>
<pre style="padding-left: 60px"># Confidence measure 
params = {'kernel': 'rbf', 'probability': True} 
classifier = SVC(**params, gamma='auto') </pre>
<p style="padding-left: 60px">The <kbd>probability</kbd> parameter tells the SVM that it should train to compute the probabilities as well.</p>
<ol start="7">
<li>Let's train the classifier:</li>
</ol>
<pre style="padding-left: 60px">classifier.fit(X_train, y_train) </pre>
<p class="mce-root"/>
<ol start="8">
<li>Let's compute the confidence measurements for these input datapoints:</li>
</ol>
<pre style="padding-left: 60px">print("Confidence measure:")<br/>for i in input_datapoints:<br/>    print(i, '--&gt;', classifier.predict_proba([i])[0])</pre>
<p style="padding-left: 60px">The <kbd>predict_proba</kbd> function measures the confidence value.</p>
<ol start="9">
<li>You will see the following on your Terminal:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-967 image-border" src="assets/6568daab-8c90-42f1-9377-1ee2b73da4e1.png" style="width:24.33em;height:8.92em;"/></p>
<ol start="10">
<li>Let's see where the points are with respect to the boundary:</li>
</ol>
<pre style="padding-left: 60px">utilities.plot_classifier(classifier, input_datapoints, [0]*len(input_datapoints), 'Input datapoints', 'True') </pre>
<ol start="11">
<li>If you run this, you will get the following:</li>
</ol>
<p class="packt_figure CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-968 image-border" src="assets/9a4adf88-34c4-431f-ae20-d0e2f4b79423.png" style="width:83.92em;height:42.58em;"/></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In this recipe, we built a classifier based on SVM. Once the classifier was obtained, we used a set of points to measure the distance of those points from the boundary and then measured the confidence levels for each of those points. When estimating a parameter, the simple identification of a single value is often not sufficient. It is therefore advisable to accompany the estimate of a parameter with a plausible range of values ​​for that parameter, which is defined as the confidence interval. It is therefore associated with a cumulative probability value that indirectly, in terms of probability, characterizes its amplitude with respect to the maximum values ​​assumed by the random variable that measures the probability that the random event described by that variable in question falls into this interval and is equal to this area <span>graphically,</span> subtended by the probability distribution curve of the random variable in that specific interval.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>The confidence interval measures the reliability of a statistic, such as an opinion poll. For example, if 40% of the sample interviewed declare to choose a certain product, it can be inferred with a level of confidence of 99% that a percentage between 30 and 50 of the total consumer population will be expressed in favor of that product. From the same sample interviewed, with a 90% confidence interval, it can be assumed that the percentage of opinions favorable to that product is now between 37% and 43%.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>Refer to the official documentation of the <kbd>sklearn.svm.SVC.decision_</kbd> function<span>: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.decision_function">https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.decision_function</a></span></li>
<li>Refer to <em>Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods</em>: <a href="https://www.researchgate.net/publication/2594015_Probabilistic_Outputs_for_Support_Vector_Machines_and_Comparisons_to_Regularized_Likelihood_Methods">https://www.researchgate.net/publication/2594015_Probabilistic_Outputs_for_Support_Vector_Machines_and_Comparisons_to_Regularized_Likelihood_Methods</a></li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Finding optimal hyperparameters</h1>
                </header>
            
            <article>
                
<p>As discussed in the previous chapter, hyperparameters are important for determining the performance of a classifier. Let's see how to extract optimal hyperparameters for SVMs. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p><span>In machine learning algorithms, various parameters are obtained during the learning process. In contrast, hyperparameters are set before the learning process begins. Given these hyperparameters, the training algorithm learns the parameters from the data. In this recipe, we will extract hyperparameters for a model based on an SVM algorithm using the grid search method. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how to find optimal hyperparameters:</p>
<ol>
<li>The full code is given in the <kbd>perform_grid_search.py</kbd> file that's already provided to you. We start importing the libraries:</li>
</ol>
<pre style="padding-left: 60px">from sklearn import svm<br/>from sklearn import model_selection<br/>from sklearn.model_selection import GridSearchCV<br/>from sklearn.metrics import classification_report<br/>import pandas as pd<br/>import utilities </pre>
<ol start="2">
<li>Then, we load the data:</li>
</ol>
<pre style="padding-left: 60px">input_file = 'data_multivar.txt'<br/>X, y = utilities.load_data(input_file)</pre>
<ol start="3">
<li>We split the data into a train and test dataset:</li>
</ol>
<pre style="padding-left: 60px">X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=5)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="4">
<li>Now, we will use cross-validation here, which we covered in the previous recipes. Once you load the data and split it into training and testing datasets, add the following to the file:</li>
</ol>
<pre style="padding-left: 60px"># Set the parameters by cross-validation<br/>parameter_grid = {"C": [1, 10, 50, 600],<br/>                  'kernel':['linear','poly','rbf'],<br/>                  "gamma": [0.01, 0.001],<br/>                  'degree': [2, 3]}</pre>
<ol start="5">
<li>Let's define the metrics that we want to use:</li>
</ol>
<pre style="padding-left: 60px">metrics = ['precision'] </pre>
<ol start="6">
<li>Let's start the search for optimal hyperparameters for each of the metrics:</li>
</ol>
<pre style="padding-left: 60px">for metric in metrics:<br/><br/>    print("#### Grid Searching optimal hyperparameters for", metric)<br/>          <br/>    classifier = GridSearchCV(svm.SVC(C=1), <br/>            parameter_grid, cv=5,scoring=metric,return_train_score=True) <br/><br/>    classifier.fit(X_train, y_train)</pre>
<ol start="7">
<li>Let's look at the scores:</li>
</ol>
<pre style="padding-left: 60px">    print("Scores across the parameter grid:")<br/>    GridSCVResults = pd.DataFrame(classifier.cv_results_)<br/>    for i in range(0,len(GridSCVResults)):<br/>        print(GridSCVResults.params[i], '--&gt;', round(GridSCVResults.mean_test_score[i],3))    </pre>
<ol start="8">
<li>Let's print the best parameter set:</li>
</ol>
<pre style="padding-left: 60px">    print("Highest scoring parameter set:", classifier.best_params_)</pre>
<ol start="9">
<li>If you run this code, you will see the following on your Terminal:</li>
</ol>
<pre style="padding-left: 60px" class="CDPAlignLeft CDPAlign"><strong>#### Grid Searching optimal hyperparameters for precision</strong><br/><strong>Scores across the parameter grid:</strong><br/><strong>{'C': 1, 'degree': 2, 'gamma': 0.01, 'kernel': 'linear'} --&gt; 0.676</strong><br/><strong>{'C': 1, 'degree': 2, 'gamma': 0.01, 'kernel': 'poly'} --&gt; 0.527</strong><br/><strong>{'C': 1, 'degree': 2, 'gamma': 0.01, 'kernel': 'rbf'} --&gt; 0.98</strong><br/><strong>{'C': 1, 'degree': 2, 'gamma': 0.001, 'kernel': 'linear'} --&gt; 0.676</strong><br/><strong>{'C': 1, 'degree': 2, 'gamma': 0.001, 'kernel': 'poly'} --&gt; 0.533</strong><br/>...<br/><strong>...</strong><br/><strong>{'C': 600, 'degree': 2, 'gamma': 0.001, 'kernel': 'linear'} --&gt; 0.676</strong><br/><strong>{'C': 600, 'degree': 2, 'gamma': 0.001, 'kernel': 'poly'} --&gt; 0.9</strong><br/><strong>{'C': 600, 'degree': 2, 'gamma': 0.001, 'kernel': 'rbf'} --&gt; 0.983</strong><br/><strong>{'C': 600, 'degree': 3, 'gamma': 0.01, 'kernel': 'linear'} --&gt; 0.676</strong><br/><strong>{'C': 600, 'degree': 3, 'gamma': 0.01, 'kernel': 'poly'} --&gt; 0.884</strong><br/><strong>{'C': 600, 'degree': 3, 'gamma': 0.01, 'kernel': 'rbf'} --&gt; 0.967</strong><br/><strong>{'C': 600, 'degree': 3, 'gamma': 0.001, 'kernel': 'linear'} --&gt; 0.676</strong><br/><strong>{'C': 600, 'degree': 3, 'gamma': 0.001, 'kernel': 'poly'} --&gt; 0.533</strong><br/><strong>{'C': 600, 'degree': 3, 'gamma': 0.001, 'kernel': 'rbf'} --&gt; 0.983</strong><br/><strong>Highest scoring parameter set: {'C': 10, 'degree': 2, 'gamma': 0.01, 'kernel': 'rbf'}</strong></pre>
<ol start="10">
<li>As we can see in the preceding output, it searches for all the optimal hyperparameters. In this case, the hyperparameters are the type of <kbd>kernel,</kbd> the <kbd>C</kbd> value, and <kbd>gamma</kbd>. It will try out various combinations of these parameters to find the best parameters. Let's test it out on the testing dataset:</li>
</ol>
<pre style="padding-left: 60px">    y_true, y_pred = y_test, classifier.predict(X_test)<br/>    print("Full performance report:\n")<br/>    print(classification_report(y_true, y_pred))</pre>
<ol start="11">
<li>If you run this code, you will see the following on your Terminal:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-969 image-border" src="assets/b4a7a2e2-2389-453e-8e2f-402da58c1b62.png" style="width:32.83em;height:8.83em;"/></p>
<ol start="12">
<li>We have previously said that there are different techniques for optimizing hyperparameters. We'll apply the <kbd>RandomizedSearchCV</kbd> method. To do this, just use the same data and change the classifier. To the code just seen, we add a further section:</li>
</ol>
<pre style="padding-left: 60px"># Perform a randomized search on hyper parameters<br/>from sklearn.model_selection import RandomizedSearchCV<br/>parameter_rand = {'C': [1, 10, 50, 600],<br/>                  'kernel':['linear','poly','rbf'],<br/>                  'gamma': [0.01, 0.001],<br/>                  'degree': [2, 3]}<br/>metrics = ['precision']<br/>for metric in metrics:<br/>    print("#### Randomized Searching optimal hyperparameters for", metric)<br/>    classifier = RandomizedSearchCV(svm.SVC(C=1), <br/>             param_distributions=parameter_rand,n_iter=30,           <br/>             cv=5,return_train_score=True)<br/>    classifier.fit(X_train, y_train)<br/>    print("Scores across the parameter grid:")<br/>    RandSCVResults = pd.DataFrame(classifier.cv_results_)<br/>    for i in range(0,len(RandSCVResults)):<br/>         print(RandSCVResults.params[i], '--&gt;', <br/>                 round(RandSCVResults.mean_test_score[i]</pre>
<ol start="13">
<li>If you run this code, you will see the following on your Terminal:</li>
</ol>
<pre style="padding-left: 60px"><strong>#### Randomized Searching optimal hyperparameters for precision</strong><br/><strong>Scores across the parameter grid:</strong><br/><strong>{'kernel': 'rbf', 'gamma': 0.001, 'degree': 2, 'C': 50} --&gt; 0.671</strong><br/><strong>{'kernel': 'rbf', 'gamma': 0.01, 'degree': 3, 'C': 600} --&gt; 0.951</strong><br/><strong>{'kernel': 'linear', 'gamma': 0.01, 'degree': 3, 'C': 50} --&gt; 0.591</strong><br/><strong>{'kernel': 'poly', 'gamma': 0.01, 'degree': 2, 'C': 10} --&gt; 0.804</strong><br/>...<br/><strong>...</strong><br/><strong>{'kernel': 'rbf', 'gamma': 0.01, 'degree': 3, 'C': 10} --&gt; 0.92</strong><br/><strong>{'kernel': 'poly', 'gamma': 0.001, 'degree': 3, 'C': 600} --&gt; 0.533</strong><br/><strong>{'kernel': 'linear', 'gamma': 0.001, 'degree': 2, 'C': 10} --&gt; 0.591</strong><br/><strong>{'kernel': 'poly', 'gamma': 0.01, 'degree': 3, 'C': 50} --&gt; 0.853</strong><br/><strong>{'kernel': 'linear', 'gamma': 0.001, 'degree': 2, 'C': 600} --&gt; 0.591</strong><br/><strong>{'kernel': 'poly', 'gamma': 0.01, 'degree': 3, 'C': 10} --&gt; 0.844</strong><br/><strong>Highest scoring parameter set: {'kernel': 'rbf', 'gamma': 0.01, 'degree': 3, 'C': 600}</strong></pre>
<ol start="14">
<li>Let's test it out on the testing dataset<span>:</span></li>
</ol>
<pre style="padding-left: 60px"><span> print("Highest scoring parameter set:", classifier.best_params_)<br/> y_true, y_pred = y_test, classifier.predict(X_test)<br/> print("Full performance report:\n")<br/> print(classification_report(y_true, y_pred))<br/> <br/></span></pre>
<p class="mce-root"/>
<ol start="15">
<li>The following results are returned:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-970 image-border" src="assets/2f2ff1b0-4f29-447e-9c83-094d99c10f74.png" style="width:32.83em;height:8.50em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In the previous recipe, <em>Building a nonlinear classifier using SVMs</em>, we repeatedly modified the kernel of the SVM algorithm to obtain an improvement in the classification of data. On the basis of the hyperparameter definition given at the beginning of the recipe, it is clear that the kernel represents a hyperparameter. In this recipe, we randomly set the value for this hyperparameter and checked the results to find out which value determines the best performance. However, a random selection of algorithm parameters may be inadequate.</p>
<p>Furthermore, it is difficult to compare the performance of different algorithms by setting the parameters randomly, because an algorithm can perform better than another with a different set of parameters. And if the parameters are changed, the algorithm may have worse results than the other algorithms.</p>
<p>As a result, the random selection of parameter values ​​is not the best approach we can take to find the best performance for our model. On the contrary, it would be advisable to develop an algorithm that automatically finds the best parameters for a particular model. <span>There are several methods for searching for hyperparameters, such as the following: g</span>rid search, randomized search, and Bayesian optimization. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The grid search algorithm</h1>
                </header>
            
            <article>
                
<p>The <strong>grid search</strong> algorithm does this by automatically looking for the set of hyperparameters that detracts from the best performance of the model.</p>
<p>The <kbd>sklearn.model_selection.GridSearchCV()</kbd><span> function performs an exhaustive search over specified parameter values for an estimator. <strong>Exhaustive search</strong> (also named direct search, or brute force) is a comprehensive examination of all possibilities, and therefore represents an efficient solution method in which every possibility is tested to determine whether it is the solution.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The randomized search algorithm</h1>
                </header>
            
            <article>
                
<p>Unlike the <kbd>GridSearchCV</kbd> method, not all parameter values are tested in this method, but the parameter settings are sampled in a fixed number. The parameter settings that are tested are set through the <kbd>n_iter</kbd> attribute. S<span>ampling without replacement is performed if </span>the parameters are presented as a list. If at least one parameter is supplied as a distribution, substitution sampling is used.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Bayesian optimization algorithm</h1>
                </header>
            
            <article>
                
<p>The aim of a Bayesian hyperparameter optimizer is to construct a probability model of the objective function and use it to select the hyperparameters that work best for use in the real objective function. Bayesian statistics allow us to foresee not only a value, but a distribution, and this is the success of this methodology.</p>
<p>The Bayesian method, when compared with the two methods already dealt with (grid search and random search), stores the results of the past evaluation, which it uses to form a probabilistic model that associates the hyperparameters with a probability of a score on the objective function.</p>
<p>This model is called a <strong>surrogate</strong> of the objective function and is much easier to optimize than the objective function itself. This result is obtained by following this procedure:</p>
<ol>
<li>A surrogate probability model of the objective function is constructed.</li>
<li>The hyperparameters that give the best results on the surrogate are searched.</li>
<li>These hyperparameters are applied to the real objective function.</li>
<li>The surrogate model is updated by incorporating the new results.</li>
<li>Repeat steps 2–4 until you reach the pre-established iterations or the maximum time.</li>
</ol>
<p>In this way, the surrogate probability model is updated after each evaluation of the objective function. To use a Bayesian hyperparameter optimizer, several libraries are available: <kbd>scikit-optimize</kbd>, <kbd>spearmint</kbd>, and <kbd>SMAC3</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>Commonly, hyperparameters are all those values that can be freely set by the user, and that are generally optimized, maximizing the accuracy on the validation data with appropriate research. Even the choice of a technique rather than another can be seen as a categorical hyperparameter, which has as many values as the methods we can choose from.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The official documentation of the <span><kbd>sklearn.model_selection.GridSearchCV()</kbd> function:<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV"> https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV</a></span></li>
<li><em>Hyperparameter optimization</em> (from Wikipedia): <a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization">https://en.wikipedia.org/wiki/Hyperparameter_optimization </a></li>
<li><em>Spearmint Bayesian optimization</em> (from GitHub): <a href="https://github.com/HIPS/Spearmint">https://github.com/HIPS/Spearmint</a></li>
<li>The SMAC3 official documentation: <a href="https://automl.github.io/SMAC3/stable/">https://automl.github.io/SMAC3/stable/</a></li>
<li><em>A Tutorial on Bayesian Optimization for Machine Learning</em> (from the School of Engineering and Applied Sciences, Harvard University): <a href="https://www.iro.umontreal.ca/~bengioy/cifar/NCAP2014-summerschool/slides/Ryan_adams_140814_bayesopt_ncap.pdf">https://www.iro.umontreal.ca/~bengioy/cifar/NCAP2014-summerschool/slides/Ryan_adams_140814_bayesopt_ncap.pdf</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building an event predictor</h1>
                </header>
            
            <article>
                
<p>Let's apply all of this knowledge <span>from this chapter</span> to a real-world problem. We will build an SVM to predict the number of people going in and out of a building. The dataset is available at <a href="https://archive.ics.uci.edu/ml/datasets/CalIt2+Building+People+Counts"><span class="URLPACKT">https://archive.ics.uci.edu/ml/datasets/CalIt2+Building+People+Counts</span></a>. We will use a slightly modified version of this dataset so that it's easier to analyze. The modified data is available in the <kbd>building_event_binary.txt</kbd> and the <kbd>building_event_multiclass.txt</kbd> files that are already provided to you. In this recipe, we will learn how to build an event predictor.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Let's understand the data format before we start building the model. Each line in <kbd>building_event_binary.txt</kbd> consists of six comma-separated strings. The ordering of these six strings is as follows:</p>
<ul>
<li>Day</li>
<li>Date</li>
<li>Time</li>
<li>The number of people going out of the building</li>
<li>The number of people coming into the building</li>
<li><strong>The output indicating whether or not it's an event</strong></li>
</ul>
<p>The first five strings form the input data, and our task is to predict whether or not an event is going on in the building.</p>
<p>Each line in <kbd>building_event_multiclass.txt</kbd> consists of six comma-separated strings. This is more granular than the previous file, in the sense that the output is the exact type of event going on in the building. The ordering of these six strings is as follows:</p>
<ul>
<li>Day</li>
<li>Date</li>
<li>Time</li>
<li>The number of people going out of the building</li>
<li>The number of people coming into the building</li>
<li><strong>The output indicating the type of event</strong></li>
</ul>
<p>The first five strings form the input data, and our task is to predict what type of event is going on in the building.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how to build an event predictor:</p>
<ol>
<li>We will use <kbd>event.py</kbd> that's already provided to you for reference. Create a new Python file, and add the following lines:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np 
from sklearn import preprocessing 
from sklearn.svm import SVC 
 
input_file = 'building_event_binary.txt' 
 
# Reading the data 
X = [] 
count = 0 
with open(input_file, 'r') as f: 
    for line in f.readlines(): 
        data = line[:-1].split(',') 
        X.append([data[0]] + data[2:]) 
 
X = np.array(X) </pre>
<p style="padding-left: 60px">We just loaded all the data into <kbd>X</kbd>.</p>
<ol start="2">
<li>Let's convert the data into numerical form:</li>
</ol>
<pre style="padding-left: 60px"># Convert string data to numerical data 
label_encoder = []  
X_encoded = np.empty(X.shape) 
for i,item in enumerate(X[0]): 
    if item.isdigit(): 
        X_encoded[:, i] = X[:, i] 
    else: 
        label_encoder.append(preprocessing.LabelEncoder()) 
        X_encoded[:, i] = label_encoder[-1].fit_transform(X[:, i]) 
 
X = X_encoded[:, :-1].astype(int) 
y = X_encoded[:, -1].astype(int) </pre>
<ol start="3">
<li>Let's train the SVM using the radial basis function, Platt scaling, and class balancing:</li>
</ol>
<pre style="padding-left: 60px"># Build SVM 
params = {'kernel': 'rbf', 'probability': True, 'class_weight': 'balanced'}  
classifier = SVC(**params, gamma='auto') 
classifier.fit(X, y) </pre>
<ol start="4">
<li>We are now ready to perform cross-validation:</li>
</ol>
<pre style="padding-left: 60px">from sklearn import model_selection<br/><br/>accuracy = model_selection.cross_val_score(classifier, <br/>        X, y, scoring='accuracy', cv=3)<br/>print("Accuracy of the classifier: " + str(round(100*accuracy.mean(), 2)) + "%")</pre>
<p class="mce-root"/>
<ol start="5">
<li>Let's test our SVM on a new datapoint:</li>
</ol>
<pre style="padding-left: 60px"># Testing encoding on single data instance<br/>input_data = ['Tuesday', '12:30:00','21','23']<br/>input_data_encoded = [-1] * len(input_data)<br/>count = 0<br/><br/>for i,item in enumerate(input_data):<br/>    if item.isdigit():<br/>        input_data_encoded[i] = int(input_data[i])<br/>    else:<br/>        input_data_encoded[i] = int(label_encoder[count].transform([input_data[i]]))<br/>        count = count + 1 <br/><br/>input_data_encoded = np.array(input_data_encoded)<br/><br/># Predict and print(output for a particular datapoint<br/>output_class = classifier.predict([input_data_encoded])<br/>print("Output class:", label_encoder[-1].inverse_transform(output_class)[0])</pre>
<ol start="6">
<li>If you run this code, you will see the following output on your Terminal:</li>
</ol>
<pre style="padding-left: 60px"><strong>Accuracy of the classifier: 93.95%</strong><br/><strong>Output class: noevent </strong> </pre>
<ol start="7">
<li>If you use the <kbd>building_event_multiclass.txt</kbd> file as the input data file instead of <kbd>building_event_binary.txt</kbd>, you will see the following output on your Terminal:</li>
</ol>
<pre style="padding-left: 60px"><strong>Accuracy of the classifier: 65.33%</strong><br/><strong>Output class: eventA</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In this recipe, we used data obtained from observations of people who flowed in and out of a building during 15 weeks, and at 48 time intervals per day. We therefore built a classifier able to predict the presence of an event such as a conference in the building, which determines an increase in the number of people present in the building for that period of time.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>Later in the recipe, we used the same classifier on a different database to also predict the type of event that is held within the building.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The official documentation of the <kbd>sklearn.svm.SVC()</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html">https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html</a></li>
<li><span>The o</span>fficial documentation of the <kbd>sklearn.model_selection.cross_validate()</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate">https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Estimating traffic</h1>
                </header>
            
            <article>
                
<p>An interesting application of SVMs is to predict traffic, based on related data. In the previous recipe, we used an SVM as a classifier. In this recipe, we will use an SVM as a regressor to estimate the traffic.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>We will use the dataset available at <a href="https://archive.ics.uci.edu/ml/datasets/Dodgers+Loop+Sensor"><span class="URLPACKT">https://archive.ics.uci.edu/ml/datasets/Dodgers+Loop+Sensor</span></a>. This is a dataset that counts the number of cars passing by during baseball games at the Los Angeles Dodgers home stadium. We will use a slightly modified form of that dataset so that it's easier to analyze. You can use the <kbd>traffic_data.txt</kbd> file, already provided to you. Each line in this file contains comma-separated strings formatted in the following manner:</p>
<ul>
<li>Day</li>
<li>Time</li>
<li>The opponent team</li>
<li>Whether or not a baseball game is going on</li>
<li>The number of cars passing by</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how to estimate traffic:</p>
<ol>
<li>Let's see how to build an SVM regressor. We will use <kbd>traffic.py</kbd> that's already provided to you as a reference. Create a new Python file, and add the following lines:</li>
</ol>
<pre style="padding-left: 60px"># SVM regressor to estimate traffic 
 
import numpy as np 
from sklearn import preprocessing 
from sklearn.svm import SVR 
 
input_file = 'traffic_data.txt' 
 
# Reading the data 
X = [] 
count = 0 
with open(input_file, 'r') as f: 
    for line in f.readlines(): 
        data = line[:-1].split(',') 
        X.append(data) 
 
X = np.array(X) </pre>
<p style="padding-left: 60px">We loaded all the input data into <kbd>X</kbd>.</p>
<ol start="2">
<li>Let's encode this data:</li>
</ol>
<pre style="padding-left: 60px"># Convert string data to numerical data 
label_encoder = []  
X_encoded = np.empty(X.shape) 
for i,item in enumerate(X[0]): 
    if item.isdigit(): 
        X_encoded[:, i] = X[:, i] 
    else: 
        label_encoder.append(preprocessing.LabelEncoder()) 
        X_encoded[:, i] = label_encoder[-1].fit_transform(X[:, i]) 
 
X = X_encoded[:, :-1].astype(int) 
y = X_encoded[:, -1].astype(int) </pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="3">
<li>Let's build and train the SVM regressor using the radial basis function:</li>
</ol>
<pre style="padding-left: 60px"># Build SVR 
params = {'kernel': 'rbf', 'C': 10.0, 'epsilon': 0.2}  
regressor = SVR(**params) 
regressor.fit(X, y) </pre>
<p style="padding-left: 60px">In the preceding lines, the <kbd>C</kbd> parameter specifies the penalty for misclassification and <kbd>epsilon</kbd> specifies the limit within which no penalty is applied.</p>
<ol start="4">
<li>Let's perform cross-validation to check the performance of the regressor:</li>
</ol>
<pre style="padding-left: 60px"># Cross validation<br/>import sklearn.metrics as sm<br/><br/>y_pred = regressor.predict(X)<br/>print("Mean absolute error =", round(sm.mean_absolute_error(y, y_pred), 2))</pre>
<ol start="5">
<li>Let's test it on a datapoint:</li>
</ol>
<pre style="padding-left: 60px"># Testing encoding on single data instance<br/>input_data = ['Tuesday', '13:35', 'San Francisco', 'yes']<br/>input_data_encoded = [-1] * len(input_data)<br/>count = 0<br/>for i,item in enumerate(input_data):<br/>    if item.isdigit():<br/>        input_data_encoded[i] = int(input_data[i])<br/>    else:<br/>        input_data_encoded[i] = int(label_encoder[count].transform([input_data[i]]))<br/>        count = count + 1 <br/><br/>input_data_encoded = np.array(input_data_encoded)<br/><br/># Predict and print output for a particular datapoint<br/>print("Predicted traffic:", int(regressor.predict([input_data_encoded])[0]))</pre>
<ol start="6">
<li>If you run this code, you will see the following printed on your Terminal:</li>
</ol>
<pre style="padding-left: 60px"><strong>    Mean absolute error = 4.08
    Predicted traffic: 29</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In this recipe, we used data collected by a sensor on the 101 North Highway in Los Angeles, near the stadium where the Dodgers play. This position is sufficiently close to the stadium to detect the increase in traffic that occurs during a match.</p>
<p>The observations were made over 25 weeks, over 288 time intervals per day (every 5 minutes). We built a regressor based on the SVM algorithm to predict the presence of a baseball game at the Dodgers stadium. In particular, we can estimate the number of cars that pass that position on the basis of the value assumed by the following predictors: day, time, the opponent team, and whether or not a baseball game is going on.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p><strong>Support vector regression</strong> (<strong>SVR</strong>) is based on the same principles as SVMs. In fact, SVR is adapted from SVMs, where the dependent variable is numeric rather than categorical. One of the main advantages of using SVR is that it is a nonparametric technique.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The official documentation of the <kbd>sklearn.metrics.mean_absolute_error()</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html</a></li>
<li><em>Linear Regression and Support Vector Regression</em> (from the University of Adelaide): <a href="https://cs.adelaide.edu.au/~chhshen/teaching/ML_SVR.pdf">https://cs.adelaide.edu.au/~chhshen/teaching/ML_SVR.pdf</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Simplifying machine learning workflow using TensorFlow</h1>
                </header>
            
            <article>
                
<p><strong>TensorFlow</strong> is an open source numerical calculation library. The library was created by Google programmers. It provides all the tools necessary to build deep learning models and offers developers a black-box interface to program.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will introduce the <span>TensorFlow framework, using a simple neural network to classify the <kbd>iris</kbd> species. We will use the <kbd>iris</kbd> dataset, which has 50 samples from the following species:</span></p>
<ul>
<li><span>Iris setosa</span></li>
<li><span>Iris virginica</span></li>
<li><span>Iris versicolor</span></li>
</ul>
<p><span>Four features are measured from each sample, namely the l</span><span>ength and the width of the sepals and petals, in centimeters.<br/></span></p>
<p><span>The following variables are contained:</span></p>
<ul>
<li><span>Sepal length in cm<br/></span></li>
<li><span>Sepal width in cm<br/></span></li>
<li><span>Petal length in cm<br/></span></li>
<li><span>Petal width in cm<br/></span></li>
<li><span>Class: <kbd>setosa</kbd>, <kbd>versicolor</kbd>, or <kbd>virginica</kbd></span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how to simplify machine learning workflow using TensorFlow:</p>
<ol>
<li><span><span>We start, as always, by importing the libraries:</span></span></li>
</ol>
<pre style="padding-left: 60px">from sklearn import datasets<br/><span>from sklearn import model_selection<br/></span>import tensorflow as tf</pre>
<p style="padding-left: 60px"><span>The first two libraries are imported only to load and split the data. The third library loads the <kbd>tensorflow</kbd> library.<br/></span></p>
<ol start="2">
<li>Load the <kbd>iris</kbd> dataset:</li>
</ol>
<pre style="padding-left: 60px">iris = datasets.load_iris()</pre>
<p class="mce-root"/>
<ol start="3">
<li>Load and split the features and classes:</li>
</ol>
<pre style="padding-left: 60px">x_train, x_test, y_train, y_test = model_selection.train_test_split(iris.data, <br/><span>                                                                    iris.target, <br/></span><span>                                                                    test_size=0.7, <br/></span><span>                                                                    random_state=1)</span></pre>
<p style="padding-left: 60px">The data is split into 70% for training and 30% for testing. The <span><kbd>random_state=1</kbd> parameter is the seed used by the random number generator.</span></p>
<ol start="4">
<li>Now we will build a simple neural network with one hidden layer and 10 nodes:</li>
</ol>
<pre style="padding-left: 60px">feature_columns = tf.contrib.learn.infer_real_valued_columns_from_input(x_train)<br/>classifier_tf = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns, <br/>                                                 hidden_units=[10], <br/>                                                 n_classes=3)</pre>
<ol start="5">
<li>Then we fit the network:</li>
</ol>
<pre style="padding-left: 60px">classifier_tf.fit(x_train, y_train, steps=5000)</pre>
<ol start="6">
<li>We will then make the predictions:</li>
</ol>
<pre style="padding-left: 60px">predictions = list(classifier_tf.predict(x_test, as_iterable=True))</pre>
<ol start="7">
<li>Finally, we will calculate the <kbd>accuracy</kbd> metric of the model:</li>
</ol>
<pre style="padding-left: 60px">n_items = y_test.size<br/>accuracy = (y_test == predictions).sum() / n_items<br/>print("Accuracy :", accuracy)</pre>
<p style="padding-left: 60px">The following result is returned:</p>
<pre style="padding-left: 60px"><strong>Accuracy : 0.9333333333333333</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In this recipe, we used the <kbd>tensorflow</kbd> library to build a simple neural network to classify <span>iris species from four features measured. In this way, we saw how simple it is to implement a model based on a machine learning algorithm using the <kbd>tensorflow</kbd> library. This topic, and on deep neural networks in general, will be analyzed in detail in</span> <a href="01c4a476-990c-40bf-8720-b8f71b2953d4.xhtml">Chapter 13</a>, <em>Deep Neural Networks</em><span>.</span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>TensorFlow provides native APIs in Python, C, C++, Java, Go, and Rust. The third-party APIs available are in C#, R, and Scala. Since October 2017, it has integrated eager execution functionality which allows the immediate execution of the operations referred to by Python.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The official documentation of the <span><kbd>tensorflow</kbd> library: <a href="https://www.tensorflow.org/tutorials">https://www.tensorflow.org/tutorials</a></span></li>
<li><em>Tensorflow for Deep Learning Research</em> (from Stanford University): <a href="http://web.stanford.edu/class/cs20si/">http://web.stanford.edu/class/cs20si/</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing a stacking method</h1>
                </header>
            
            <article>
                
<p>A combination of different approaches leads to better results: this statement works in different aspects of our life and also adapts to algorithms based on machine learning. Stacking is the process of combining various machine learning algorithms. This technique is due to David H. Wolpert, an American mathematician, physicist, and computer scientist. </p>
<p>In this recipe, we will learn how to implement a stacking method.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>We will use the <kbd>heamy</kbd> library to stack the two models that we just used in the previous recipes. The <kbd>heamy</kbd> library <span>is a set of useful tools for competitive data science.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how to implement a stacking method:</p>
<ol>
<li><span><span>We start by importing the libraries:</span></span></li>
</ol>
<pre style="padding-left: 60px">from heamy.dataset import Dataset<br/><span>from heamy.estimator import Regressor<br/></span><span>from heamy.pipeline import ModelsPipeline<br/></span><br/>from sklearn.datasets import load_boston<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.ensemble import RandomForestRegressor<br/>from sklearn.linear_model import LinearRegression<br/>from sklearn.metrics import mean_absolute_error</pre>
<ol>
<li><span>Load the <kbd>boston</kbd> dataset, already used in <a href="f552bbc7-5e56-41b8-8e8d-915cc1bd53ab.xhtml">Chapter 1</a>,</span> <span><em>The Realm of Supervised Learning</em></span>, <span>for the <em>Estimating housing prices</em> recipe:</span></li>
</ol>
<pre style="padding-left: 60px">data = load_boston()</pre>
<ol start="3">
<li>Split the data:</li>
</ol>
<pre style="padding-left: 60px">X, y = data['data'], data['target']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=2)</pre>
<ol start="4">
<li>Let's create the dataset:</li>
</ol>
<pre style="padding-left: 60px">Data = Dataset(X_train,y_train,X_test)</pre>
<ol start="5">
<li>Now we can build the two models that we will use in the stacking procedure:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">RfModel = Regressor(dataset=Data, estimator=RandomForestRegressor, parameters={'n_estimators': 50},name='rf')<br/>LRModel = Regressor(dataset=Data, estimator=LinearRegression, parameters={'normalize': True},name='lr')</pre>
<ol start="6">
<li>It's time to stack these models:</li>
</ol>
<pre style="padding-left: 60px">Pipeline = ModelsPipeline(RfModel,LRModel)<br/>StackModel = Pipeline.stack(k=10,seed=2)</pre>
<ol start="7">
<li>Now we will train a <kbd>LinearRegression</kbd> model on stacked data:</li>
</ol>
<pre style="padding-left: 60px">Stacker = Regressor(dataset=StackModel, estimator=LinearRegression)</pre>
<ol start="8">
<li>Finally, we will calculate the results to validate the model:</li>
</ol>
<pre style="padding-left: 60px">Results = Stacker.predict()<br/>Results = Stacker.validate(k=10,scorer=mean_absolute_error)</pre>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Stacked generalization works by deducing the biases of the classifier/regressor relative to a supplied learning dataset. This deduction works by generalizing into a second space whose inputs are the hypotheses of the original generalizers and whose output is the correct hypothesis. When used with multiple generators, stacked generalization is an alternative to cross-validation. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>Stacking tries to exploit the advantages of each algorithm by ignoring or correcting their disadvantages. It can be seen as a mechanism that corrects errors in your algorithms. Another library to perform a stacking procedure is <span>StackNet. </span></p>
<p><strong>StackNet</strong> is a fra<span>mework implemented in Java based on Wolpert's stacked generalization on multiple levels to improve accuracy in machine learning predictive problems. The StackNet model functions as a neural network in which the transfer function takes the form of any supervised machine learning algorithm.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><span>The official documentation of the <kbd>heamy</kbd> library:<a href="https://heamy.readthedocs.io/en/latest/index.html"> https://heamy.readthedocs.io/en/latest/index.html</a></span></li>
<li><span>The o</span>fficial documentation of the <kbd>StackNet</kbd> framework: <a href="https://github.com/kaz-Anova/StackNet">https://github.com/kaz-Anova/StackNet</a></li>
<li><em>Stacked Generalization</em> by David H. Wolpert: <a href="http://www.machine-learning.martinsewell.com/ensembles/stacking/Wolpert1992.pdf">http://www.machine-learning.martinsewell.com/ensembles/stacking/Wolpert1992.pdf</a></li>
</ul>


            </article>

            
        </section>
    </body></html>