- en: Chapter 2. Approaching Simple Linear Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having set up all your working tools (directly installing Python and IPython
    or using a scientific distribution), you are now ready to start using linear models
    to incorporate new abilities into the software you plan to build, especially predictive
    capabilities. Up to now, you have developed software solutions based on certain
    specifications you defined (or specifications that others have handed to you).
    Your approach has always been to tailor the response of the program to particular
    inputs, by writing code carefully mapping every single situation to a specific,
    predetermined response. Reflecting on it, by doing so you were just incorporating
    practices that you (or others) have learned from experience.
  prefs: []
  type: TYPE_NORMAL
- en: However, the world is complex, and sometimes your experience is not enough to
    make your software smart enough to make a difference in a fairly competitive business
    or in challenging problems with many different and mutable facets.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will start exploring an approach that is different from
    manual programming. We are going to present an approach that enables the software
    to self-learn the correct answers to particular inputs, provided you can define
    the problem in terms of data and target response and that you can incorporate
    in the processes some of your domain expertise—for instance, choosing the right
    features for prediction. Therefore, your experience will go on being critical
    when it comes to creating your software, though in the form of learning from data.
    In fact, your software will be learning from data accordingly to your specifications.
    We are also going to illustrate how it is possible to achieve this by resorting
    to one of the simplest methods for deriving knowledge from data: linear models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, in this chapter, we are going to discuss the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding what problems machine learning can solve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What problems a regression model can solve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The strengths and weaknesses of correlation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How correlations extends to a simple regression model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The when, what, and why of a regression model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The essential mathematics behind gradient descent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the process, we will be using some statistical terminology and concepts in
    order to provide you with the prospect of linear regression in the larger frame
    of statistics, though our approach will remain practical, offering you the tools
    and hints to start building linear models using Python and thus enrich your software
    development.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a regression problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Thanks to machine learning algorithms, deriving knowledge from data is possible.
    Machine learning has solid roots in years of research: it has really been a long
    journey since the end of the fifties, when Arthur Samuel clarified machine learning
    as being a "field of study that gives computers the ability to learn without being
    explicitly programmed."'
  prefs: []
  type: TYPE_NORMAL
- en: The data explosion (the availability of previously unrecorded amounts of data)
    has enabled the widespread usage of both recent and classic machine learning techniques
    and made them high-performance techniques. If nowadays you can talk by voice to
    your mobile phone and expect it to answer properly to you, acting as your secretary
    (such as Siri or Google Now), it is uniquely because of machine learning. The
    same holds true for every application based on machine learning such as face recognition,
    search engines, spam filters, recommender systems for books/music/movies, handwriting
    recognition, and automatic language translation.
  prefs: []
  type: TYPE_NORMAL
- en: Some other actual usages of machine learning algorithms are somewhat less obvious,
    but nevertheless important and profitable, such as credit rating and fraud detection,
    algorithmic trading, advertising profiling on the Web, and health diagnostics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally speaking, machine learning algorithms can learn in three ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised learning**: This is when we present labeled examples to learn
    from. For instance, when we want to be able to predict the selling price of a
    house in advance in a real estate market, we can get the historical prices of
    houses and have a supervised learning algorithm successfully figure out how to
    associate the prices to the house characteristics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised learning**: This is when we present examples without any hint,
    leaving it to the algorithm to create a label. For instance, when we need to figure
    out how the groups inside a customer database can be partitioned into similar
    segments based on their characteristics and behaviors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reinforcement learning**: This is when we present examples without labels,
    as in unsupervised learning, but get feedback from the environment as to whether
    label guessing is correct or not. For instance, when we need software to act successfully
    in a competitive setting, such as a videogame or the stock market, we can use
    reinforcement learning. In this case, the software will then start acting in the
    setting and it will learn directly from its errors until it finds a set of rules
    that ensure its success.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear models and supervised learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unsupervised learning has important applications in robotic vision and automatic
    feature creation, and reinforcement learning is critical for developing autonomous
    AI (for instance, in robotics, but also in creating intelligent software agents);
    however, supervised learning is most important in data science because it allows
    us to accomplish something the human race has aspired to for ages: prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prediction** has applications in business and for general usefulness, enabling
    us to take the best course of action since we know from predictions the likely
    outcome of a situation. Prediction can make us successful in our decisions and
    actions, and since ancient times has been associated with magic or great wisdom.'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning is no magic at all, though it may look like sorcery to some
    people, as Sir Arthur Charles Clarke stated, "any sufficiently advanced technology
    is indistinguishable from magic." Supervised learning, based on human achievements
    in mathematics and statistics, helps to leverage human experience and observations
    and turn them into precise predictions in a way that no human mind could. However,
    supervised learning can predict only in certain favorable conditions. It is paramount
    to have examples from the past at hand from which we can extract rules and hints
    that can support wrapping up a highly likely prediction given certain premises.
  prefs: []
  type: TYPE_NORMAL
- en: In one way or another, no matter the exact formulation of the machine learning
    algorithm, the idea is that you can tell the outcome because there have been certain
    premises in the observed past that led to particular conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: In mathematical formalism, we call the outcome we want to predict the response
    or target variable and we usually label it using the lower case letter *y*.
  prefs: []
  type: TYPE_NORMAL
- en: The premises are instead called the **predictive** **variables**, or simply
    attributes or features, and they are labeled as a lowercase *x* if there is a
    single one and by an uppercase *X* if there are many. Using the uppercase letter
    *X* we intend to use matrix notation, since we can also treat the *y* as a response
    vector (technically a column vector) and the *X* as a matrix containing all values
    of the feature vectors, each arranged into a separate column of the matrix.
  prefs: []
  type: TYPE_NORMAL
- en: It is also important to always keep a note of the dimensions of *X* and *y*;
    thus, by convention, we can call *n* the number of observations and *p* the number
    of variables. Consequently our *X* will be a matrix of size (*n*, *p*), and our
    *y* will always be a vector of size *n*.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Throughout the book, we will also have recourse to statistical notation, which
    is actually a bit more explicit and verbose. A statistical formula tries to give
    an idea of all the predictors involved in the formula (we will show an example
    of this later) whereas matrix notation is more implicit.
  prefs: []
  type: TYPE_NORMAL
- en: We can affirm that, when we are learning to predict from data in a supervised
    way, we are actually building a function that can answer the question about how
    *X* can imply *y*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using these new matrix symbolic notations, we can define a function, a functional
    mapping that can translate *X* values into *y* without error or with an acceptable
    margin of error. We can affirm that all our work will be to determinate a function
    of the following kind:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear models and supervised learning](img/00005.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: When the function is specified, and we have in mind a certain algorithm with
    certain parameters and an *X* matrix made up of certain data, conventionally we
    can refer to it as a hypothesis. The term is suitable because we can intend our
    function as a ready hypothesis, set with all its parameters, to be tested if working
    more or less well in predicting our target *y*.
  prefs: []
  type: TYPE_NORMAL
- en: Before talking about the function (the supervised algorithm that does all the
    magic), we should first spend some time reasoning about what feeds the algorithm
    itself. We have already introduced the matrix *X*, the predictive variables, and
    the vector *y*, the target answer variable; now it is time to explain how we can
    extract them from our data and what exactly their role is in a learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Reflecting on predictive variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Reflecting on the role of your predictive variable in a supervised algorithm,
    there are a few caveats that you have to keep in mind throughout our illustrations
    in the book, and yes, they are very important and decisive.
  prefs: []
  type: TYPE_NORMAL
- en: 'To store the predictive variables, we use a matrix, usually called the *X*
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reflecting on predictive variables](img/00006.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In this example, our *X* is made up of only one variable and it contains *n*
    cases (or observations).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you would like to know when to use a variable or feature, just consider that
    in machine learning *feature* and *attribute* are terms that are favored over
    *variable*, which has a definitively statistical flavor hinting at something that
    varies. Depending on the context and audience, you can effectively use one or
    the other.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Python code, you can build a one-column matrix structure by typing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Using the NumPy `array` we can quickly derive a vector and a matrix. If you
    start from a Python list, you will get a vector (which is neither a row nor a
    column vector, actually). By using the `reshape` method, you can transform it
    into a row or column vector, based on your specifications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Real-world data usually need matrices that are more complex, and real-world
    matrices comprise uncountable different data columns (the variety element of big
    data). Most likely, a standard *X* matrix will have more columns, so the notation
    we will be referring to is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reflecting on predictive variables](img/00007.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, our matrix has more variables, all *p* variables, so its size is *n* x
    *p*. In Python, there are two methods to make up such a data matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'You just have to transform with the `array` function a list of lists, where
    each internal list is a row matrix; or you create a vector with your data and
    then reshape it in the shape of your desired matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In NumPy there are also special functions for rapidly creating matrices of
    ones and zeros. As an argument, just specify the intended (`x`, `y`) shape in
    a tuple:'
  prefs: []
  type: TYPE_NORMAL
- en: '`all_zeros = np.zeros((5,3))`'
  prefs: []
  type: TYPE_NORMAL
- en: '`all_ones = np.ones((5,3))`'
  prefs: []
  type: TYPE_NORMAL
- en: The information present in the set of observations from the past, that we are
    using as *X*, can deeply affect how we are going to build the link between our
    *X* and the *y*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, usually it is the case that we do not know the full range of possible
    associations between *X* and *y* because:'
  prefs: []
  type: TYPE_NORMAL
- en: We have just observed a certain *X*, so our experience of *y* for a given *X*
    is biased, and this is a sampling bias because, as in a lottery, we have drawn
    only certain numbers in a game and not all the available ones
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We never observed certain *(X, y)* associations (please note the formulation
    in a tuple, indicating the interconnection between *X* and *y*), because they
    never happened before, but that does not exclude them from happening in the future
    (and incidentally we are striving to forecast the future)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is little to do with the second problem, (we can extrapolate the future
    only through the directions pointed out by the past), but you can actually check
    how recent the data you are using is. If you are trying to forecast in a context
    that is very susceptible to changes and mutable from day to day, you have to keep
    in mind that your data could quickly become outdated and you may be unable to
    guess new trends. An example of a mutable context where we constantly need to
    update models is the advertising sector (where the competitive scenery is frail
    and continually changing). Consequently, you continually need to gather fresher
    data that could allow you to build a much more effective supervised algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'As for the first problem, you can solve it using more and more cases from different
    sources. The more you sample, the more likely your drawn set of *X* will resemble
    a complete set of possible and true associations of *X* with *y*. This is understandable
    via an important idea in probability and statistics: the law of large numbers.'
  prefs: []
  type: TYPE_NORMAL
- en: The law of large numbers suggests that, as the number of your experiments grows,
    so the likelihood that the average of their results will represent the true value
    (that the experiments themselves are trying to figure out) will increase.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised algorithms learn from large samples of historical data, called **batches**,
    fetched all at once from large data repositories, such as databases or data lakes.
    Alternatively, they also could pick the examples that are most useful for their
    learning by themselves and ignore the bulk of the data (this is called **active
    learning** and it is a kind of semi-supervised learning that we won't discuss
    here).
  prefs: []
  type: TYPE_NORMAL
- en: If our environment is fast-paced, they also could just stream data as it is
    available, continuously adapting to any new association between the predictive
    variables and the response (this is called online learning and we will discuss
    it in [Chapter 7](part0046_split_000.html#1BRPS2-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 7. Online and Batch Learning"), *Online and Batch Learning*).
  prefs: []
  type: TYPE_NORMAL
- en: Another important aspect of the *X* matrix of predictors to be considered is
    that up to now we assumed that we could deterministically derive the response
    *y* using the information in the matrix *X*. Unfortunately this is not always
    so in the real world and it is not rare that you actually try to figure out your
    response *y* using a completely wrong set of predictive *X*. In such cases, you
    have to figure out that you are actually wasting your time in trying to fit something
    working between your *X* and *y* and that you should look for some different *X*
    (again more data in the sense of more variables).
  prefs: []
  type: TYPE_NORMAL
- en: 'According to the model used, having more variables and cases is usually beneficial
    under different points of view. More cases reduces the possibility of learning
    from a biased and limited set of observations. Many algorithms can better estimate
    their internal parameters (and produce more accurate predictions) if trained using
    large sets of observations. Also, having more variables at hand can be beneficial,
    but in the sense that it increases the chance of having explicative features to
    be used for machine learning. Many algorithms are in fact sensitive to redundant
    information and noise present in features, consequently requiring some feature
    selection to reduce the predictors involved in the model. This is quite the case
    with linear regression, which can surely take advantage of more cases for its
    training, but it should also receive a parsimonious and efficient set of features
    to perform at its best. Another important aspect to know about the *X* matrix
    is that it should be made up solely of numbers. Therefore, it really matters what
    you are working with. You can work with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Physical measurements, which are always OK because they are naturally numbers
    (for example, height)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Human measurements, which are a bit less OK, but are still fine when they have
    a certain order (that is, all numbers that we give as scores based on our judgment)
    and so they can be converted into rank numbers (such as 1, 2, and 3 for the first,
    second, and third values, and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We call such values quantitative measurements. We expect quantitative measurement
    to be continuous and that means a quantitative variable can take any real positive
    or negative number as a valid value. Human measurements are usually only positive,
    starting from zero or one, so it is just a fair approximation to consider them
    quantitative.
  prefs: []
  type: TYPE_NORMAL
- en: For physical measurements, in statistics, we distinguish between interval and
    ratio variables. The difference is that ratio variables have a natural zero whereas
    in interval data the zero is an arbitrary one. A good example is temperature;
    in fact, unless you use the Kelvin scale, whose zero is an absolute one, both
    Fahrenheit and Celsius have arbitrary scales. The main implication is about the
    ratios (if the zero is arbitrary, the ratio is also arbitrary).
  prefs: []
  type: TYPE_NORMAL
- en: 'Human measurements that are numerical are called **ordinal variables**. Unlike
    interval data, ordinal data does not have a natural zero. Moreover, the interval
    between each value on an interval scale is equal and regular; however, in an ordinal
    scale, though the distance between the values is the same, their real distance
    could be very different. Let''s think of a scale made of three textual values:
    good, average, and bad. Next, let''s say that we arbitrarily decide that good
    is 3, average is 2, and bad is 1\. We call this arbitrary assignment of values
    ordinal encoding. Now, from a mathematical point of view, though the interval
    between 3 and 2 in respect of the interval from 2 to 1 is the same (that is, one
    point), are we really sure that the real distance between good and average is
    the same as that from average and bad? For instance, in terms of customer satisfaction,
    does it costs the same effort going from an evaluation of bad to one of average
    and from one of average to one of excellent?'
  prefs: []
  type: TYPE_NORMAL
- en: Qualitative measurements (for example, a value judgment such as good, average,
    or bad, or an attribute such as being colored red, green, or blue) need some work
    to be done, some clever data manipulation, but they can still be part of our *X*
    matrix using the right transformation. Even more unstructured qualitative information
    (such as text, sound, or a drawing) can be transformed and reduced to a pool of
    numbers and can be ingested into an *X* matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Qualitative variables can be stored as numbers into single-value vectors or
    they can have a vector for each class. In such a case, we are talking about binary
    variables (also called dummy variables in statistical language).
  prefs: []
  type: TYPE_NORMAL
- en: We are going to discuss in greater detail how to transform the data at hand,
    especially if its type is qualitative, into an input matrix suitable for supervised
    learning in [Chapter 5](part0035_split_000.html#11C3M2-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 5. Data Preparation"), *Data Preparation*.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a starting point before working on the data itself, it is necessary to question
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The quality of the data—that is, whether the data available can really represent
    the right information pool for extracting *X*-*y* rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The quantity of data—that is, checking how much data is available, keeping in
    mind that, for building robust machine learning solutions, it is safer to have
    a large variety of variables and cases (at least when you're dealing with thousands
    of examples)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The extension of data in time—that is, checking how much time the data spans
    in the past (since we are learning from the past)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reflecting on response variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Reflecting on the role of the response variable, our attention should be first
    drawn to what type of variable we are going to predict, because that will distinguish
    the type of supervised problem to be solved.
  prefs: []
  type: TYPE_NORMAL
- en: If our response variable is a quantitative one, a numeric value, our problem
    will be a regression one. Ordinal variables can be solved as a regression problem,
    especially if they take many different distinct values. The output of a regression
    supervised algorithm is a value that can be directly used and compared with other
    predicted values and with the real response values used for learning.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, as an example of a regression problem, in the real estate business
    a regression model could predict the value of a house just from some information
    about its location and its characteristics, allowing an immediate discovery of
    market prices that are too cheap or too expensive by using the model's predictions
    as an indicator of a fair fact-based estimation (if we can reconstruct the price
    by a model, it is surely well justified by the value of the measurable characteristics
    we used as our predictors).
  prefs: []
  type: TYPE_NORMAL
- en: If our response variable is a qualitative one, our problem is one of classification.
    If we have to guess between just two classes, our problem is called **a binary
    classification**; otherwise, if more classes are involved, it is a called a multi-label
    classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if we want to guess the winner in a game between two football
    teams, we have a binary classification problem because we just need to know if
    the first team will win or not (the two classes are *team wins*, *team loses*).
    A multi-label classification could instead be used to predict which football team
    among a certain number will win (so in our prediction, the classes to be guessed
    are the teams).
  prefs: []
  type: TYPE_NORMAL
- en: 'Ordinal variables, if they do not take many distinct values, can be solved
    as a multi-label classification problem. For instance, if you have to guess the
    final ranking of a team in a football championship, you could try to predict its
    final position in the leader board as a class. Consequently, in this ordinal problem
    you have to guess many classes corresponding to different positions in the championship:
    class 1 could represent the first position, class 2 the second position, and so
    on. In conclusion, you could figure the final ranking of a team as the positional
    class whose likelihood of winning is the greatest.'
  prefs: []
  type: TYPE_NORMAL
- en: As for the output, classification algorithms can provide both classification
    into a precise class and an estimate of the probability of being part of any of
    the classes at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Continuing with examples from the real estate business, a classification model
    could predict if a house could be a bargain or if it could increase its value
    given its location and its characteristics, thus allowing a careful investment
    selection.
  prefs: []
  type: TYPE_NORMAL
- en: The most noticeable problem with response variables is their exactness. Measurement
    errors in regression problems and misclassification in classification ones can
    damage the ability of your model to perform well on real data by providing inaccurate
    information to be learned. In addition, biased information (such as when you provide
    cases of a certain class and not from all those available) can hurt the capacity
    of your model to predict in real-life situations because it will lead the model
    to look at data from a non-realistic point of view. Inaccuracies in the response
    variable are more difficult and more dangerous for your model than problems with
    your features.
  prefs: []
  type: TYPE_NORMAL
- en: 'For single predictors, the outcome variable *y* is also a vector. In NumPy,
    you just set it up as a generic vector or as a column vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The family of linear models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The family of linear models is so named because the function that specifies
    the relationship between the *X*, the predictors, and the *y*, the target, is
    a linear combination of the *X* values. A linear combination is just a sum where
    each addendum value is modified by a weight. Therefore, a linear model is simply
    a smarter form of a summation.
  prefs: []
  type: TYPE_NORMAL
- en: Of course there is a trick in this summation that makes the predictors perform
    like they do while predicting the answer value. As we mentioned before, the predictors
    should tell us something, they should give us some hint about the answer variable;
    otherwise any machine learning algorithm won't work properly. We can predict our
    response because the information about the answer is already somewhere inside
    the features, maybe scattered, twisted, or transformed, but it is just there.
    Machine learning just gathers and reconstructs such information.
  prefs: []
  type: TYPE_NORMAL
- en: In linear models, such inner information is rendered obvious and extracted by
    the weights used for the summation. If you actually manage to have some meaningful
    predictors, the weights will just do all the heavy work to extract it and transform
    it into a proper and exact answer.
  prefs: []
  type: TYPE_NORMAL
- en: Since the *X* matrix is a numeric one, the sum of its elements will result in
    a number itself. Linear models are consequently the right tool for solving any
    regression problem, but they are not limited to just guessing real numbers. By
    a transformation of the response variable, they can be enabled to predict counts
    (positive integer numbers) and probabilities relative to being part of a certain
    group or class (or not).
  prefs: []
  type: TYPE_NORMAL
- en: 'In statistics, the linear model family is called the **generalized linear model**
    (**GLM**). By means of special link functions, proper transformation of the answer
    variable, proper constraints on the weights and different optimization procedures
    (the learning procedures), GLM can solve a very wide range of different problems.
    In this book, our treatise won''t extend beyond what is necessary to the statistical
    field. However, we will propose a couple of models of the larger family of the
    GLM, namely linear regression and logistic regression; both methods are appropriate
    to solve the two most basic problems in data science: regression and classification.'
  prefs: []
  type: TYPE_NORMAL
- en: Because linear regression does not require any particular transformation of
    the answer variable and because it is conceptually the real foundation of linear
    models, we will start by understanding how it works. To make things easier, we
    will start from the case of a linear model using just a single predictor variable,
    a so-called **simple linear regression**. The predictive power of a simple linear
    regression is very limited in comparison with its multiple form, where many predictors
    at once contribute to the model. However, it is much easier to understand and
    figure out its functioning.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing to discover simple linear regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We provide some practical examples in Python throughout the book and do not
    leave explanations about the various regression models at a purely theoretical
    level. Instead, we will explore together some example datasets, and systematically
    illustrate to you the commands necessary to achieve a working regression model,
    interpret its structure, and deploy a predicting application.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A dataset is a data structure containing predictive variables and sometimes
    response ones. For machine learning purposes, it can be structured or semi-structured
    into a matrix form, in the shape of a table with rows and columns.
  prefs: []
  type: TYPE_NORMAL
- en: For the initial presentation of the linear regression in its simple version
    (using only one predictive variable to forecast the response variable), we have
    chosen a couple of datasets relative to real estate evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Real estate is quite an interesting topic for an automatic predictive model
    since there is quite a lot of freely available data from censuses and, being an
    open market, even more data can be scraped from websites monitoring the market
    and its offers. Moreover, because the renting or buying of a house is quite an
    important economic decision for many individuals, online services that help to
    gather and digest the large amounts of available information are indeed a good
    business model idea.
  prefs: []
  type: TYPE_NORMAL
- en: The first dataset is quite a historical one. Taken from the paper by Harrison,
    D. and Rubinfeld, D.L. *Hedonic Housing Prices and the Demand for Clean Air* (J.
    Environ. Economics & Management, vol.5, 81-102, 1978), the dataset can be found
    in many analysis packages and is present at the UCI Machine Learning Repository
    ([https://archive.ics.uci.edu/ml/datasets/Housing](https://archive.ics.uci.edu/ml/datasets/Housing)).
  prefs: []
  type: TYPE_NORMAL
- en: The dataset is made up of 506 census tracts of Boston from the 1970 census and
    it features 21 variables regarding various aspects that could impact real estate
    value. The target variable is the median monetary value of the houses, expressed
    in thousands of USD. Among the available features, there are some fairly obvious
    ones such as the number of rooms, the age of the buildings, and the crime levels
    in the neighborhood, and some others that are a bit less obvious, such as the
    pollution concentration, the availability of nearby schools, the access to highways,
    and the distance from employment centers.
  prefs: []
  type: TYPE_NORMAL
- en: The second dataset from the Carnegie Mellon University Statlib repository ([https://archive.ics.uci.edu/ml/datasets/Housing](https://archive.ics.uci.edu/ml/datasets/Housing))
    contains 20,640 observations derived from the 1990 US Census. Each observation
    is a series of statistics (9 predictive variables) regarding a block group—that
    is, approximately 1,425 individuals living in a geographically compact area. The
    target variable is an indicator of the house value of that block (technically
    it is the natural logarithm of the median house value at the time of the census).
    The predictor variables are basically median income.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset has been used in Pace and Barry (1997), *Sparse Spatial Autoregressions*,
    *Statistics and Probability Letters*, ([http://www.spatial-statistics.com/pace_manuscripts/spletters_ms_dir/statistics_prob_lets/pdf/fin_stat_letters.pdf](http://www.spatial-statistics.com/pace_manuscripts/spletters_ms_dir/statistics_prob_lets/pdf/fin_stat_letters.pdf)),
    a paper on regression analysis including spatial variables (information about
    places including their position or their nearness to other places in the analysis).
    The idea behind the dataset is that variations of house values can be explained
    by exogenous variables (that is, external to the house itself) representing population,
    the density of buildings, and the population's affluence aggregated by area.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for downloading the data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Starting from the basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will start exploring the first dataset, the Boston dataset, but before delving
    into numbers, we will upload a series of helpful packages that will be used during
    the rest of the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are working from an IPython Notebook, running the following command
    in a cell will instruct the Notebook to represent any graphic output in the Notebook
    itself (otherwise, if you are not working on IPython, just ignore the command
    because it won''t work in IDEs such as Python''s IDLE or Spyder):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: To immediately select the variables that we need, we just frame all the data
    available into a Pandas data structure, `DataFrame`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inspired by a similar data structure present in the R statistical language,
    a `DataFrame` renders data vectors of different types easy to handle under the
    same dataset variable, offering at the same time much convenient functionality
    for handling missing values and manipulating data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: At this point, we are ready to build our first regression model, learning directly
    from the data present in our Pandas DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned, linear regression is just a simple summation, but it is indeed
    not the simplest model possible. The simplest is the statistical mean. In fact,
    you can simply guess by always using the same constant number, and the mean very
    well absolves such a role because it is a powerful descriptive number for data
    summary.
  prefs: []
  type: TYPE_NORMAL
- en: The mean works very well with normally distributed data but often it is quite
    suitable even for different distributions. A normally distributed curve is a distribution
    of data that is symmetric and has certain characteristics regarding its shape
    (a certain height and spread).
  prefs: []
  type: TYPE_NORMAL
- en: The characteristics of a normal distribution are defined by formulas and there
    are appropriate statistical tests to find out if your variable is normal or not,
    since many other distributions resemble the bell shape of the normal one and many
    different normal distributions are generated by different mean and variance parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The key to understanding if a distribution is normal is the **probability density
    function** (**PDF**), a function describing the probability of values in the distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of a normal distribution, the PDF is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Starting from the basics](img/00008.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In such a formulation, the symbol *µ* represents the mean (which coincides
    with the median and the mode) and the symbol *σ* is the variance. Based on different
    means and variances, we can calculate different value distributions, as the following
    code demonstrates and visualizes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![Starting from the basics](img/00009.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Because of its properties, the normal distribution is a fundamental distribution
    in statistics since all statistical models involve working on normal variables.
    In particular, when the mean is zero and the variance is one (unit variance),
    the normal distribution, called a standard normal distribution under such conditions,
    has even more favorable characteristics for statistical models.
  prefs: []
  type: TYPE_NORMAL
- en: Anyway, in the real world, normally distributed variables are instead rare.
    Consequently, it is important to verify that the actual distribution we are working
    on is not so far from an ideal normal one or it will pose problems in your expected
    results. Normally distributed variables are an important requirement for statistical
    models (such as mean and, in certain aspects, linear regression). On the contrary,
    machine learning models do not depend on any previous assumption about how your
    data should be distributed. But, as a matter of fact, even machine learning models
    work well if data has certain characteristics, so working with a normally distributed
    variable is preferable to other distributions. Throughout the book, we will provide
    warnings about what to look for and check when building and applying machine learning
    solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the calculation of a mean, relevant problems can arise if the distribution
    is not symmetric and there are extreme cases. In such an occurrence, the extreme
    cases will tend to draw the mean estimate towards them, which consequently won''t
    match with the bulk of the data. Let''s then calculate the mean of the value of
    the 506 tracts in Boston:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we calculated the mean using a method available in the Pandas
    DataFrame; however, the NumPy function `mean` can be also called to calculate
    a mean from an array of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In terms of a mathematical formulation, we can express this simple solution
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Starting from the basics](img/00010.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can now evaluate the results by measuring the error produced in predicting
    the real *y* values by this rule. Statistics suggest that, to measure the difference
    between the prediction and the real value, we should square the differences and
    then sum them all. This is called **the squared sum of errors**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have calculated it, we can visualize it as a distribution of errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![Starting from the basics](img/00011.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The plot shows how frequent certain errors are in respect of their values. Therefore,
    you will immediately notice that most errors are around zero (there is a high
    density around that value). Such a situation can be considered a good one, since
    in most cases the mean is a good approximation, but some errors are really very
    far from the zero and they can attain considerable values (don't forget that the
    errors are squared, anyway, so the effect is emphasized). When trying to figure
    out such values, your approach will surely lead to a relevant error and we should
    find a way to minimize it using a more sophisticated approach.
  prefs: []
  type: TYPE_NORMAL
- en: A measure of linear relationship
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Evidently, the mean is not a good representative of certain values, but it is
    certainly a good baseline to start from. Certainly, an important problem with
    the mean is its being fixed, whereas the target variable is changeable. However,
    if we assume that the target variable changes because of the effect of some other
    variable we are measuring, then we can adjust the mean with respect to the variations
    in cause.
  prefs: []
  type: TYPE_NORMAL
- en: One improvement on our previous approach could be to build a mean conditional
    on certain values of another variable (or even more than one) actually related
    to our target, whose variation is somehow similar to the variation of the target
    one.
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, if we know the dynamics we want to predict with our model, we can
    try to look for variables that we know can impact the answer values.
  prefs: []
  type: TYPE_NORMAL
- en: In the real estate business, we actually know that usually the larger a house
    is, the more expensive it is; however, this rule is just part of the story and
    the price is affected by many other considerations. For the moment, we will keep
    it simple and just assume that an extension to a house is a factor that positively
    affects the price, and consequently, more space equals more costs when building
    the house (more land, more construction materials, more work, and consequently
    a higher price).
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have a variable that we know should change with our target and we just
    need to measure it and extend our initial formula based on constant values with
    something else.
  prefs: []
  type: TYPE_NORMAL
- en: 'In statistics, there is a measure that helps to measure how (in the sense of
    how much and in what direction) two variables relate to each other: **correlation**.'
  prefs: []
  type: TYPE_NORMAL
- en: In correlation, a few steps are to be considered. First, your variables have
    to be standardized (or your result won't be a correlation but a covariation, a
    measure of association that is affected by the scale of the variables you are
    working with).
  prefs: []
  type: TYPE_NORMAL
- en: In statistical *Z* score standardization, you subtract from each variable its
    mean and then you divide the result by the standard deviation. The resulting transformed
    variable will have a mean of 0 and a standard deviation of 1 (or unit variance,
    since variance is the squared standard deviation).
  prefs: []
  type: TYPE_NORMAL
- en: 'The formula for standardizing a variable is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A measure of linear relationship](img/00012.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This can be achieved in Python using a simple function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: After standardizing, you compare the squared difference of each variable with
    its own mean. If the two differences agree in sign, their multiplication will
    become positive (evidence that they have the same directionality); however, if
    they differ, the multiplication will turn negative. By summing all the multiplications
    between the squared differences, and dividing them by the number of observations,
    you will finally get the correlation which will be a number ranging from -1 to
    1.
  prefs: []
  type: TYPE_NORMAL
- en: The absolute value of the correlation will provide you with the intensity of
    the relation between the two variables compared, 1 being a sign of a perfect match
    and zero a sign of complete independence between them (they have no relation between
    them). The sign instead will hint at the proportionality; positive is direct (when
    one grows the other does the same), negative is indirect (when one grows, the
    other shrinks).
  prefs: []
  type: TYPE_NORMAL
- en: 'Covariance can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A measure of linear relationship](img/00013.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Whereas, Pearson''s correlation can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A measure of linear relationship](img/00014.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s check these two formulations directly on Python. As you may have noticed,
    Pearson''s correlation is really covariance calculated on standardized variables,
    so we define the `correlation` function as a wrapper of both the `covariance`
    and `standardize` ones (you can find all these functions ready to be imported
    from `Scipy`; we are actually recreating them here just to help you understand
    how they work):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Our correlation estimation for the relation between the value of the target
    variable and the average number of rooms in houses in the area is 0.695, which
    is positive and remarkably strong, since the maximum positive score of a correlation
    is 1.0.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As a way to estimate if a correlation is relevant or not, just square it; the
    result will represent the percentage of the variance shared by the two variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s graph what happens when we correlate two variables. Using a **scatterplot**,
    we can easily visualize the two involved variables. A scatterplot is a graph where
    the values of two variables are treated as Cartesian coordinates; thus, for every
    (*x*, *y*) value a point is represented in the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![A measure of linear relationship](img/00015.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The scatterplot also plots the average value for both the target and the predictor
    variables as dashed lines. This divides the plot into four quadrants. If we compare
    it with the previous covariance and correlation formulas, we can understand why
    the correlation value was close to 1: in the bottom-right and in top-left quadrants,
    there are just a few mismatching points where one of variables is above its average
    and the other is below its own.'
  prefs: []
  type: TYPE_NORMAL
- en: A perfect match (correlation values of 1 or -1) is possible only when the points
    are in a straight line (and all points are therefore concentrated in the right-uppermost
    and left-lowermost quadrants). Thus, correlation is a measure of linear association,
    of how close to a straight line your points are. Ideally, having all your points
    on a single line favors a perfect mapping of your predictor variable to your target.
  prefs: []
  type: TYPE_NORMAL
- en: Extending to linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear regression tries to fit a line through a given set of points, choosing
    the best fit. The best fit is the line that minimizes the summed squared difference
    between the value dictated by the line for a certain value of *x* and its corresponding
    *y* values. (It is optimizing the same squared error that we met before when checking
    how good a mean was as a predictor.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Since linear regression is a line; in bi-dimensional space (*x*, *y*), it takes
    the form of the classical formula of a line in a Cartesian plane: *y = mx + q*,
    where *m* is the angular coefficient (expressing the angle between the line and
    the *x* axis) and *q* is the intercept between the line and the *x* axis.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, machine learning indicates the correct expression for a linear regression
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Extending to linear regression](img/00016.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, again, *X* is a matrix of the predictors, *β* is a matrix of coefficients,
    and *β[0]* is a constant value called the **bias** (it is the same as the Cartesian
    formulation, only the notation is different).
  prefs: []
  type: TYPE_NORMAL
- en: We can better understand its functioning mechanism by seeing it in action with
    Python, first using the `StatsModels` package, then using the Scikit-learn one.
  prefs: []
  type: TYPE_NORMAL
- en: Regressing with Statsmodels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Statsmodels is a package designed with statistical analysis in mind; therefore,
    its function offers quite a rich output of statistical checks and information.
    Scalability is not an issue for the package; therefore, it is really a good starting
    point for learning, but is certainly not the optimal solution if you have to crunch
    quite large datasets (or even big data) because of its optimization algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two different methods (two modules) to work out a linear regression
    with Statsmodels:'
  prefs: []
  type: TYPE_NORMAL
- en: '`statsmodels.api`: This works with distinct predictor and answer variables
    and requires you to define any transformation of the variables on the predictor
    variable, including adding the intercept'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`statsmodels.formula.api`: This works in a similar way to R, allowing you to
    specify a functional form (the formula of the summation of the predictors)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will illustrate our example using the `statsModels.api`; however, we will
    also show you an alternative method with `statsmodels.formula.api`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first step, let''s upload both the modules of Statsmodels, naming them
    as conventionally indicated in the package documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'As a second step, it is necessary to define the *y* and *X* variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The *X* variable needs to be extended by a constant value `()`; the bias will
    be calculated accordingly. In fact, as you remember, the formula of a linear regression
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Regressing with Statsmodels](img/00017.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'However, using `StatsModels.api`, the formula actually becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Regressing with Statsmodels](img/00018.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This can be interpreted as a combination of the variables in *X*, multiplied
    by its corresponding *β* value.
  prefs: []
  type: TYPE_NORMAL
- en: Consequently, the predictor *X* now contains both the predictive variable and
    a unit constant. Also, *β* is no longer a single coefficient, but a vector of
    coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a visual confirmation of this by requiring the first values of
    the Pandas DataFrame using the `head` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![Regressing with Statsmodels](img/00019.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'At this point, we just need to set the initialization of the linear regression
    calculation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, we need to ask for the estimation of the regression coefficients, the
    *β* vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'If we had wanted to manage the same result using the `StatsModels.formula.api`,
    we should have typed the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The previous two code lines simultaneously comprise both steps seen together,
    without requiring any particular variable preparation since the bias is automatically
    incorporated. In fact, the specification about how the linear regression should
    work is incorporated into the string `target ~ RM`, where the variable name left
    of the tilde (`~`) indicates the answer variable, the variable name (or names,
    in the case of a multiple regression analysis) on the right being for the predictor.
  prefs: []
  type: TYPE_NORMAL
- en: Actually, `smf.ols` expects quite a different input compared to `sm.OLS`, because
    it can accept our entire original dataset (it selects what variables are to be
    used by using the provided formula), whereas `sm.OLS` expects a matrix containing
    just the features to be used for prediction. Consequently, some caution has to
    be exercised when using two such different approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'A summary (a method of the fitted model) can quickly tell you everything that
    you need to know about regression analysis. In case you have tried `statsmodesl.formula.api`,
    we also re-initialize the linear regression using the `StatsModels.api` since
    they are not working on the same *X* and our following code relies on `sm.OLS`
    specifications:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![Regressing with Statsmodels](img/00020.jpeg)![Regressing with Statsmodels](img/00021.jpeg)![Regressing
    with Statsmodels](img/00022.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: You will receive quite a long series of tables containing many statistical tests
    and information. Though quite daunting at the beginning, you actually do not need
    all these outputs, unless the purpose of your analysis is a statistical one. Data
    science is mainly concerned with real models working on predicting real data,
    not on formally correct specifications of statistical problems. Nevertheless,
    some of these outputs are still useful for successful model building and we are
    going to provide you with an insight into the main figures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before explaining the outputs, we first need to extract two elements from the
    fitted model: the coefficients and the predictions calculated on the data on which
    we built the model. They both are going to come in very handy during the following
    explanations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The coefficient of determination
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s start from the first table of results. The first table is divided into
    two columns. The first one contains a description of the fitted model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dep. Variable**: It just reminds you what the target variable was'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model**: Another reminder of the model that you have fitted, the OLS is ordinary
    least squares, another way to refer to linear regression'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Method**: The parameters fitting method (in this case least squares, the
    classical computation method)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No. Observations**: The number of observations that have been used'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DF Residuals**: The degrees of freedom of the residuals, which is the number
    of observations minus the number of parameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DF Model**: The number of estimated parameters in the model (excluding the
    constant term from the count)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second table gives a more interesting picture, focusing how good the fit
    of the linear regression model is and pointing out any possible problems with
    the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**R-squared**: This is the coefficient of determination, a measure of how well
    the regression does with respect to a simple mean.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adj. R-squared**: This is the coefficient of determination adjusted based
    on the number of parameters in a model and the number of observations that helped
    build it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F-statistic**: This is a measure telling you if, from a statistical point
    of view, all your coefficients, apart from the bias and taken together, are different
    from zero. In simple words, it tells you if your regression is really better than
    a simple average.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prob (F-statistic)**: This is the probability that you got that F-statistic
    just by lucky chance due to the observations that you have used (such a probability
    is actually called the **p-value** of F-statistic). If it is low enough you can
    be confident that your regression is really better than a simple mean. Usually
    in statistics and science a test probability has to be equal or lower than 0.05
    (a conventional criterion of statistical significance) for having such a confidence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AIC**: This is the **Akaike Information Criterion**. AIC is a score that
    evaluates the model based on the number of observations and the complexity of
    the model itself. The lesser the AIC score, the better. It is very useful for
    comparing different models and for statistical variable selection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BIC**: This is the **Bayesian Information Criterion**. It works as AIC, but
    it presents a higher penalty for models with more parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of these statistics make sense when we are dealing with more than one predictor
    variable, so they will be discussed in the next chapter. Thus, for the moment,
    as we are working with a simple linear regression, the two measures that are worth
    examining closely are F-statistic and R-squared. F-statistic is actually a test
    that doesn't tell you too much if you have enough observations and you can count
    on a minimally correlated predictor variable. Usually it shouldn't be much of
    a concern in a data science project.
  prefs: []
  type: TYPE_NORMAL
- en: R-squared is instead much more interesting because it tells you how much better
    your regression model is in comparison to a single mean. It does so by providing
    you with a percentage of the unexplained variance of a mean as a predictor that
    actually your model was able to explain.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to compute the measure yourself, you just have to calculate the
    sum of squared errors of the mean of the target variable. That''s your baseline
    of unexplained variance (the variability in house prices that in our example we
    want to explain by a model). If from that baseline you subtract the sum of squared
    errors of your regression model, you will get the residual sum of squared errors,
    which can be compared using a division with your baseline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When working with floats, rounding errors are possible, so don't be afraid if
    some of the lesser decimals don't match in your calculations; if they match the
    8th decimal, you can be quite confident that the result is the same.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, if you can reduce your sum of squared errors of the regression to zero,
    you will get the maximum percentage of explained variance—that is, a score of
    1.
  prefs: []
  type: TYPE_NORMAL
- en: The R-squared measure is also comparable with the percentage that you obtain
    squaring the correlation between your predictor and the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, it is 0.484, which actually is exactly our R-squared correlation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: As we have seen, R-squared is perfectly aligned with the squared errors that
    the linear regression is trying to minimize; thus, a better R-squared means a
    better model. However, there are some problems with the measure (and with linear
    regression itself, actually) when working with more predictors at once. Again,
    we have to wait until we model more predictors at once; therefore, just for a
    simple linear regression, a better R-squared should hint at a better model.
  prefs: []
  type: TYPE_NORMAL
- en: Meaning and significance of coefficients
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The second output table informs us about the coefficients and provides us with
    a series of tests. These tests can make us confident that we have not been fooled
    by a few extreme observations in the foundations of our analysis or by some other
    problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '**coef**: The estimated coefficient'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**std err**: The standard error of the estimate of the coefficient; the larger
    it is, the more uncertain the estimation of the coefficient'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**t**: The t-statistic value, a measure indicating whether the coefficient
    true value is different from zero'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**P > |t|**: The p-value indicating the probability that the coefficient is
    different from zero just by chance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**[95.0% Conf. Interval]**: The lower and upper values of the coefficient,
    considering 95% of all the chances of having different observations and so different
    estimated coefficients'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From a data science viewpoint, t-tests and confidence bounds are not very useful
    because we are mostly interested in verifying whether our regression is working
    while predicting answer variables. Consequently, we will focus just on the `coef`
    value (the estimated coefficients) and on their standard error.
  prefs: []
  type: TYPE_NORMAL
- en: The coefficients are the most important output that we can obtain from our regression
    model because they allow us to re-create the weighted summation that can predict
    our outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, our coefficients are *−34.6706* for the bias (also called the
    **intercept**, recalling the formula for a line in a Cartesian space) and *9.1021*
    for the `RM` variable. Recalling our formula, we can plug in the numbers we obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Meaning and significance of coefficients](img/00023.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, if you replace the betas and *X* with the estimated coefficients, and
    the variables'' names with *−34.6706* and *9.1021*, everything becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Meaning and significance of coefficients](img/00024.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, if you know the average number of rooms in an area of Boston, you can
    make a quick estimate of the expected value. For instance, *x[RM]* is `4.55`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We have to notice two points here. First, in such a formulation, the beta of
    each variable becomes its *unit change* measure, which corresponds to the change
    the outcome will undergo if the variable increases by one unit. In our case, our
    average room space becomes `5.55`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The increase for a unit change in *x[RM]* corresponds to a change in the outcome
    equivalent to *β[RM]*. The other point to be noticed is that, if our average room
    space becomes 1 or 2, our estimated value will turn negative, which is completely
    unrealistic. This is because the mapping between predictor and the target variable
    happened in a delimited bound of values of the predictor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Whenever we try to estimate our answer values using an *x* (or a set of *X*)
    that is outside the boundaries we used for fitting the model, we risk a response
    that has not been optimized at all by the linear regression calculations. Expressed
    in another way, linear regression can learn what it sees, and, unless there is
    a clear linear functional form between the predictor and the target (they can
    be truly expressed as a line), you risk weird estimations when your predictors
    have an unusual value. In other words, a linear regression can always work within
    the range of values it learned from (this is called **interpolation**) but can
    provide correct values for its learning boundaries (a different predictive activity
    called **extrapolation**) only in certain conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we previously mentioned, the number of observations used for fitting the
    model is of paramount importance to obtain a robust and reliable linear regression
    model. The more observations, the less likely the model is to be surprised by
    unusual values when running in production.
  prefs: []
  type: TYPE_NORMAL
- en: Standard errors instead are very important because they signal a weak or unclear
    relationship between the predictor and the answer. You can notice this by dividing
    the standard error by its beta. If the ratio is 0.5 or even larger, then it's
    a clear sign that the model has little confidence that it provided you with the
    right coefficient estimates. Having more cases is always the solution because
    it can reduce the standard errors of the coefficients and improve our estimates;
    however, there are also other methods to reduce errors, such as removing the redundant
    variance present among the features by a principal component analysis or selecting
    a parsimonious set of predictors by greedy selections. All these topics will be
    discussed when we work with multiple predictors; at this point in the book, we
    will illustrate the remedies to such a problem.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the fitted values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The last table deals with an analysis of the residuals of the regression. The
    residuals are the difference between the target values and the predicted fitted
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Skewness**: This is a measure of the symmetry of the residuals around the
    mean. For symmetric distributed residuals, the value should be around zero. A
    positive value indicates a long tail to the right; a negative value a long tail
    to the left.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kurtosis**: This is a measure of the shape of the distribution of the residuals.
    A bell-shaped distribution has a zero measure. A negative value points to a too
    flat distribution; a positive one has too great a peak.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Omnibus D''Angostino''s test**: This is a combined statistical test for skewness
    and kurtosis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prob(Omnibus)**: This is the Omnibus statistic turned into a probability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Jarque-Bera**: This is another test of skewness and kurtosis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prob (JB)**: This is the JB statistic turned into a probability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Durbin-Watson**: This is a test for the presence of correlation among the
    residuals (relevant during analysis of time-based data).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cond. No**: This is a test for multicollinearity (we will deal with the concept
    of multicollinearity when working with many predictors).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A close analysis of residuals is quite relevant in statistical practice since
    it can highlight the presence of serious problems with regression analysis. When
    working with a single variable it is interesting to visually check its residuals
    to figure out if there are strange cases or if the residuals don''t distribute
    randomly. In particular, it is important to keep an eye out for any of these three
    problems showing up:'
  prefs: []
  type: TYPE_NORMAL
- en: Values too far from the average. Large standardized residuals hint at a serious
    difficulty when modeling such observations. Also, in the process of learning these
    values, the regression coefficients may have been distorted.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Different variance in respect of the value of the predictor. If the linear regression
    is an average conditioned on the predictor, dishomogeneous variance points out
    that the regression is not working properly when the predictor has certain values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Strange shapes in the cloud of residual points may indicate that you need a
    more complex model for the data you are analyzing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In our case, we can easily compute the residuals by subtracting the fitted
    values from the answer variable and then plotting the resulting standardized residuals
    in a graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '![Evaluating the fitted values](img/00025.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The resulting scatterplot indicates that the residuals show some of the problems
    we previously indicated as a warning that something is not going well with your
    regression analysis. First, there are a few points lying outside the band delimited
    by the two dotted lines at normalized residual values −3 and +3 (a range that
    should hypothetically cover 99.7% of values if the residuals have a normal distribution).
    These are surely influential points with large errors and they can actually make
    the entire linear regression under-perform. We will talk about possible solutions
    to this problem when we discuss outliers in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Then, the cloud of points is not at all randomly scattered, showing different
    variances at different values of the predictor variable (the **abscissa axis**)
    and you can spot unexpected patterns (points in a straight line, or the core points
    placed in a kind of U shape).
  prefs: []
  type: TYPE_NORMAL
- en: We are not at all surprised; the average number of rooms is likely a good predictor
    but it is not the only cause, or it has to be rethought as a direct cause (the
    number of rooms indicates a larger house, but what if the rooms are smaller than
    average?). This leads us to discuss whether a strong correlation really makes
    a variable a good working candidate for a linear relationship.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation is not causation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Actually, seeing a correlation between your predictor and your target variable,
    and managing to model it successfully using a linear regression, doesn't really
    mean that there is a causal relation between the two (though your regression may
    work very well, and even optimally).
  prefs: []
  type: TYPE_NORMAL
- en: Though using a data science approach, instead of a statistical one, will guarantee
    a certain efficacy in your model, it is easy to fall into some mistakes when having
    no clue why your target variable is correlated with a predictor.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will tell you about six different reasons, and offer a cautionary word to
    help you handle such predictors without difficulty:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Direct causation**: *x* causes *y*; for instance, in the real estate business
    the value is directly proportional to the size of the house in square meters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reciprocal effects**: *x* causes *y* but it is also influenced by *y*. This
    is quite typical of many macro-economic dynamics where the effect of a policy
    augments or diminishes its effects. As an example in real estate, high crime rates
    in an area can lower its prices but lower prices mean that the area could quickly
    become even more degraded and dangerous.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spurious causation**: This happens when the real cause is actually *z*, which
    causes both *x* and *y*; consequently it is just a fallacious illusion that *x*
    implies *y* because it is *z* behind the scenes. For instance, the presence of
    expensive art shops and galleries may seem to correlate with house prices; in
    reality, both are determined by the presence of affluent residents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Indirect causation**: *x* in reality is not causing *y* but it is causing
    something else, which then causes *y*. A good municipality investing in infrastructures
    after higher taxes can indirectly affect house prices because the area becomes
    more comfortable to live in, thus attracting more demand. Higher taxes, and thus
    more investments, indirectly affect house prices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conditional effect**: *x* causes *y* in respect of the values of another
    variable *z*; for instance, when *z* has certain values *x* is not influencing
    *y* but, when *z* takes particular values, the *x* starts impacting *y*. We also
    call this situation interaction. For instance the presence of schools in an area
    can become an attractor when the crime rate is low, so it affects house prices
    only when there is little criminality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random effect**: Any recorded correlation between *x* and *y* has been due
    to a lucky sampling selection; in reality there is no relationship with *y* at
    all.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ideal case is when you have a direct causation; then, you will have a predictor
    in your model that will always provide you with the best values to derive your
    responses.
  prefs: []
  type: TYPE_NORMAL
- en: In the other cases, it is likely that the imperfect cause-effect relationship
    with the target variable will lead to more noisy estimates, especially in production
    when you will have to work with data not seen before by the model.
  prefs: []
  type: TYPE_NORMAL
- en: Reciprocal effects are more typical of econometric models. They require special
    types of regression analysis. Including them in your regression analysis may improve
    your model; however, their role may be underestimated.
  prefs: []
  type: TYPE_NORMAL
- en: Spurious and indirect causes will add some noise to your *x* and *y* relationship;
    this could bring noisier estimates (larger standard errors). Often, the solution
    is to get more observations for your analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Conditional effects, if not caught, can limit your model's ability to produce
    accurate estimates. If you are not aware of any of them, given your domain knowledge
    of the problem, it is a good step to check for any of them using some automatic
    procedure to test possible interactions between the variables.
  prefs: []
  type: TYPE_NORMAL
- en: Random effects are the worst possible thing that could happen to your model,
    but they are easily avoided if you follow the data science procedure that we will
    be describing in [Chapter 6](part0041_split_000.html#173722-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 6. Achieving Generalization"), *Achieving Generalization*, when we deal
    with all the actions necessary to validate your model's results.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting with a regression model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we plug the coefficients into the regression formula, predicting is just
    a matter of applying new data to the vector of coefficients by a matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you can rely on the fitted model by providing it with an array containing
    new cases. In the following example, you can see how, given the `Xp` variable
    with a single new case, this is easily predicted using the `predict` method on
    the fitted model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'A nice usage of the `predict` method is to project the fitted predictions on
    our previous scatterplot to allow us to visualize the price dynamics in respect
    of our predictor, the average number of rooms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '![Predicting with a regression model](img/00026.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Besides the `predict` method, generating the predictions is quite easy by just
    using the `dot` function in `NumPy`. After preparing an *X* matrix containing
    both the variable data and the bias (a column of ones) and the coefficient vectors,
    all you have to do is to multiply the matrix by the vector. The result will itself
    be a vector of length equal to the number of observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: A comparison of the results obtained by the `predict` method and this simple
    multiplication will reveal a perfect match. Because predicting from a linear regression
    is simple, if necessary you could even implement this multiplication on your application
    in a language different from Python. In such a case, you will just need to find
    a matrix calculation library or program a function by yourself. To our knowledge,
    you can easily write such a function even in the SQL script language.
  prefs: []
  type: TYPE_NORMAL
- en: Regressing with Scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we have seen while working with the `StatsModels` package, a linear model
    can be built using a more oriented machine learning package such as Scikit-learn.
    Using the `linear_model` module, we can set a linear regression model specifying
    that the predictors shouldn''t be normalized and that our model should have a
    bias:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Data preparation, instead, requires counting the observations and carefully
    preparing the predictor array to specify its two dimensions (if left as a vector,
    the fitting procedure will raise an error):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'After completing all the previous steps, we can fit the model using the `fit`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: A very convenient feature of the Scikit-learn package is that all the models,
    no matter their type of complexity, share the same methods. The `fit` method is
    always used for fitting and it expects an *X* and a *y* (when the model is a supervised
    one). Instead, the two common methods for making an exact prediction (always for
    regression) and its probability (when the model is probabilistic) are `predict`
    and `predict_proba`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'After fitting the model, we can inspect the vector of the coefficients and
    the bias constant:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `predict` method and slicing the first 10 elements of the resulting
    list, we output the first 10 predicted values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'As previously seen, if we prepare a new matrix and we add a constant, we can
    calculate the results by ourselves using a simple matrix–vector multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, the result of the product provides us with the same estimates
    as the `predict` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: At this point, it would be natural to question the usage of such a `linear_model`
    module. Compared with the previous functions offered by Statsmodels, Scikit-learn
    seems to offer little statistical output, and one seemingly with many linear regression
    features stripped out. In reality, it offers exactly what is needed in data science
    and it is perfectly fast-performing when dealing with large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are working on IPython, just try the following simple test to generate
    a large dataset and check the performance of the two versions of linear regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'After generating ten million observations of a single variable, start by measuring
    using the `%%time` magic function for IPython. This magic function automatically
    computes how long it takes to complete the calculations in the IPython cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, it is the turn of the Statsmodels package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Though a single variable is involved in the model, Statsmodels's default algorithms
    prove to be three times slower than Scikit-learn. We will repeat this test in
    the next chapter, too, when using more predictive variables in one go and other
    different `fit` methods.
  prefs: []
  type: TYPE_NORMAL
- en: Minimizing the cost function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the core of linear regression, there is the search for a line''s equation
    that it is able to minimize the sum of the squared errors of the difference between
    the line''s *y* values and the original ones. As a reminder, let''s say our regression
    function is called `h`, and its predictions `h(X)`, as in this formulation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Minimizing the cost function](img/00027.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Consequently, our cost function to be minimized is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Minimizing the cost function](img/00028.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There are quite a few methods to minimize it, some performing better than others
    in the presence of large quantities of data. Among the better performers, the
    most important ones are **Pseudoinverse** (you can find this in books on statistics),
    **QR factorization**, and **gradient descent**.
  prefs: []
  type: TYPE_NORMAL
- en: Explaining the reason for using squared errors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Looking under the hood of a linear regression analysis, at first it could be
    puzzling to realize that we are striving to minimize the squared differences between
    our estimates and the data from which we are building the model. Squared differences
    are not as intuitively explainable as absolute differences (the difference without
    a sign).
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if you have to predict a monetary value, such as the price of
    a stock or the return from an advertising activity, you are more interested in
    knowing your absolute error, not your R-squared one, which could be perceived
    as misleading (since with squares larger losses are emphasized).
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned before, linear regression takes its steps from the statistical
    knowledge domain, and there are actually quite a few reasons in statistics that
    make minimizing a squared error preferable to minimizing an absolute one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, such reasons are quite complex and too technical and consequently
    beyond the real scope of this book; however, from a high-level point of view,
    a good and reasonable explanation is that squaring nicely achieves two very important
    objectives:'
  prefs: []
  type: TYPE_NORMAL
- en: It removes negative values; therefore opposite errors won't reciprocally cancel
    each other when summed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It emphasizes larger differences, because as they are squared they will proportionally
    increase the sum of the errors compared to a simple sum of absolute values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minimizing the squared differences with an estimator leads us to use the mean
    (as we suggested before as a basic model, without providing any justification
    for it).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s just check together using Python, without developing all the formulations.
    Let''s define an `x` vector of values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s also define a function returning the cost function as squared differences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `fmin` minimization procedure offered by the `scipy` package, we
    try to figure out, for a vector (which will be our x vector of values), the value
    that makes the least squared summation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We just output our best `e` value and verify if it actually is the mean of
    the `x` vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'If instead we try to figure out what minimizes the sum of absolute errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: We will find out that it is the median, not the mean. Unfortunately, the median
    does not have the same statistical properties as the mean.
  prefs: []
  type: TYPE_NORMAL
- en: Pseudoinverse and other optimization methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There is an analytical formula for solving a regression analysis and getting
    a vector of coefficients out of data, minimizing the cost function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Pseudoinverse and other optimization methods](img/00029.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Demonstrating this equation goes beyond the practical scope of this book, but
    we can test it using the power of Python coding.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can therefore directly solve for this by using `np.linalg.inv` from `NumPy`
    to obtain the inverse of a matrix, or alternative methods such as solving for
    *w* in linear equations that are called normal equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Pseudoinverse and other optimization methods](img/00030.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here the function `np.linalg.solve` can do all the calculations for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The only problem in solving a linear regression using these approaches is complexity,
    possibly some loss in accuracy of the computation when directly calculating the
    inverse using `np.linalg.inv`, and, naturally, the fact that the *X^TX* multiplication
    has to be invertible (sometimes it isn't when using multiple variables that are
    strongly related to each other).
  prefs: []
  type: TYPE_NORMAL
- en: Even using another algorithm (QR factorization, a core algorithm in Statsmodels
    that can overcome some previously quoted numeric misbehaviors), the worst performance
    can be estimated to be *O(n³)*; that is, cubic complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Using Pseudoinverse (in NumPy, `np.linalg.pinv`) can help achieve a *O(n^m)*
    complexity where *m* is estimated to be <2.37 (approximately quadratic, then).
  prefs: []
  type: TYPE_NORMAL
- en: This can really be a great limitation in being able to quickly estimate linear
    regression analysis. In fact, if you are working with *10³* observations, a feasible
    number of observations in statistical analysis, it will take at worst *10⁹* computations;
    however, when working with data science projects, which easily reach *10⁶* observations,
    the number of computations required to find the solution to a regression problem
    may rocket to *10^(18)*.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent at work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As an alternative to the usual classical optimization algorithms, the gradient
    descent technique is able to minimize the cost function of a linear regression
    analysis using far fewer computations. Gradient descent complexity ranks in the
    order *O(n*p)*, thus making learning regression coefficients feasible even in
    the occurrence of a large *n* (which stands for the number of observations) and
    a large *p* (number of variables).
  prefs: []
  type: TYPE_NORMAL
- en: The method works by leveraging a simple heuristic that gradually converges on
    the optimal solution starting from a random one. Explaining it simply, it resembles
    walking blind in the mountains. If you want to descend to the lowest valley, even
    if you don't know and can't see the path, you can proceed approximately by going
    downhill for a while, then stopping, then going downhill again and so on, always
    aiming at each stage for where the surface descends until you arrive at a point
    when you cannot descend anymore. Hopefully, at that point you will have reached
    your destination.
  prefs: []
  type: TYPE_NORMAL
- en: In such a situation, your only risk is happening on an intermediate valley (where
    there is a wood or a lake, for instance) and mistaking it for your desired arrival
    because the land stops descending there.
  prefs: []
  type: TYPE_NORMAL
- en: In an optimization process, such a situation is defined as a local minimum (whereas
    your target is a global minimum instead, the best minimum possible) and it is
    a possible outcome of your journey downhill depending on the function you are
    working on minimizing. The good news is, in any case, that the error function
    of the linear model family is a bowl-shaped one (technically our cost function
    is a concave one) and it is unlikely that you can get caught anywhere if you descend
    properly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The necessary steps to work out a gradient-descent-based solution are easily
    described, given our cost function for a set of coefficients (the vector *w*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gradient descent at work](img/00031.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We first start by choosing a random initialization for *w*, by choosing some
    random numbers (taken from a standardized normal curve, for instance, having a
    zero mean and unit variance).
  prefs: []
  type: TYPE_NORMAL
- en: Then we start reiterating an update of the values of *w* (opportunely using
    the gradient descent computations) until the marginal improvement from the previous
    *J(w)* is small enough to let us figure out that we have finally reached an optimum
    minimum.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can opportunely update our coefficients, one by one, by subtracting from
    each of them a portion alpha (*α*, the learning rate) of the partial derivative
    of the cost function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gradient descent at work](img/00032.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, in our formula, *w[j]* is to be intended as a single coefficient (we
    are iterating over them). After resolving the partial derivative, the final resolution
    form is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gradient descent at work](img/00033.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Simplifying everything, our gradient for the coefficient of *x* is just the
    average of our predicted values multiplied by their respective *x* value.
  prefs: []
  type: TYPE_NORMAL
- en: Alpha, called the **learning rate**, is very important in the process, because,
    if it is too large, it may cause the optimization to detour and fail. You have
    to think of each gradient as a jump or as a run in a direction. If you fully take
    it, you may happen to pass over the optimum minimum and end up in another rising
    slope. Too many consecutive long steps may even force you to climb up the cost
    slope, worsening your initial position (given by a cost function that is its summed
    square, the loss of an overall score of fitness).
  prefs: []
  type: TYPE_NORMAL
- en: Using a small alpha, gradient descent won't jump beyond the solution but it
    may take a much longer time to reach the desired minimum. How to choose the right
    alpha is a matter of trial and error; anyway, starting from an alpha such as 0.01
    is never a bad choice, based on our experience in many optimization problems.
  prefs: []
  type: TYPE_NORMAL
- en: Naturally, the gradient, given the same alpha, will in any case produce shorter
    steps as you approach the solution. Visualizing the steps in a graph can really
    give you a hint about whether gradient descent is working out a solution or not.
  prefs: []
  type: TYPE_NORMAL
- en: Though quite conceptually simple (it is based on an intuition that we have surely
    applied ourselves to move step-by-step, directing where we can optimize our result),
    gradient descent is very effective and indeed scalable when working with real
    data. Such interesting characteristics have elevated it to the core optimization
    algorithm in machine learning; it is not limited to just the linear model family,
    but it can also be extended, for instance, to neural networks for the process
    of back propagation, which updates all the weights of the neural net in order
    to minimize training errors. Surprisingly, gradient descent is also at the core
    of another complex machine learning algorithm, gradient boosting tree ensembles,
    where we have an iterative process minimizing the errors using a simpler learning
    algorithm (a so-called **weak learner** because it is limited by a high bias)
    to progress towards optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a first implementation in Python. We will slightly modify it in the
    next chapter to make it work efficiently with more predictors than one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, after defining the response variable, selecting our predictor (the `RM`
    feature, the average number of rooms per dwelling), and adding a bias (the constant
    number `1`), we are ready in the following code to define all the functions in
    our optimization process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'After finally defining all the functions necessary for gradient descent to
    work, we can start optimizing it for a solution to our single regression problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Scikit-learn `linear_regression` (and other linear models present in the linear
    methods module) are actually powered by gradient descent, making Scikit-learn
    our favorite choice when working in data science projects with large and big data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced linear regression as a supervised machine learning
    algorithm. We explained its functional form, its relationship with the statistical
    measures of mean and correlation, and we tried to build a simple linear regression
    model on the Boston house prices data. After doing that we finally glanced at
    how regression works under the hood by proposing its key mathematical formulations
    and their translation into Python code.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will continue our discourse about linear regression,
    extending our predictors to multiple variables and carrying on our explanation
    where we left it suspended during our initial illustration with a single variable.
    We will also point out the most useful transformations you can apply to data to
    make it suitable for processing by a linear regression algorithm.
  prefs: []
  type: TYPE_NORMAL
