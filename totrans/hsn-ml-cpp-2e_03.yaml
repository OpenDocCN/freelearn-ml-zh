- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Measuring Performance and Selecting Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter describes bias and variance effects and their pathological cases,
    which usually appear when training **machine learning** (**ML**) models.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to deal with overfitting by using regularization
    and discuss different techniques we can use. We will also consider different model
    performance estimation metrics and how they can be used to detect training problems.
    Toward the end of this chapter, we will look at how to find the best hyperparameters
    for a model by introducing the grid search technique and its implementation in
    C++.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Performance metrics for ML models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding bias and variance characteristics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model selection with the grid search technique
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, you will need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A modern C++ compiler with C++20 support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CMake build system version >= 3.10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Dlib` library'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mlpack` library'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Flashlight` library'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Plotcpp` library'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code files for this chapter can be found in the following GitHub repo:
    [https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter03](https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter03)'
  prefs: []
  type: TYPE_NORMAL
- en: Performance metrics for ML models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we develop or implement a particular ML algorithm, we need to estimate
    how well it works. In other words, we need to estimate how well it solves our
    task. Usually, we use some numeric metrics for algorithm performance estimation.
    An example of such a metric could be a value of **mean squared error** (**MSE**)
    that’s been calculated for target and predicted values. We can use this value
    to estimate how distant our predictions are from the target values we used for
    training. Another use case for performance metrics is their use as objective functions
    in optimization processes. Some performance metrics are used for manual observations,
    though others can be used for optimization purposes too.
  prefs: []
  type: TYPE_NORMAL
- en: 'Performance metrics are different for each of the ML algorithm types. In [*Chapter
    1*](B19849_01.xhtml#_idTextAnchor015), *Introduction to Machine Learning with
    C++*, we discussed that two main categories of ML algorithms exist: **regression
    algorithms** and **classification algorithms**. There are other types of algorithms
    in the ML discipline, but these two are the most common ones. This section will
    go over the most popular performance metrics for regression and classification
    algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: Regression metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Regression task metrics are used to measure how close predicted values are to
    ground truth ones. Such measurements can help us estimate the prediction quality
    of the algorithm. Under regression metrics, there are four main metrics, which
    we will dive into in the following subsections.
  prefs: []
  type: TYPE_NORMAL
- en: MSE and RMSE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'MSE is a widely used metric for regression algorithms to estimate their quality.
    It is an average squared difference between the predictions and ground truth values.
    This is given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B19849_Formula_021.png) is the number of predictions and ground
    truth items, ![](img/B19849_Formula_031.png) is the ground truth value for the
    *i*th item, and ![](img/B19849_Formula_041.png) is the prediction value for the
    *i*th item.
  prefs: []
  type: TYPE_NORMAL
- en: MSE is often used as a target loss function for optimization algorithms because
    it is smoothly differentiable and is a convex function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **root mean squared error** (**RMSE**) metric is usually used to estimate
    performance, such as when we need to give bigger weights to higher errors (to
    penalize them). We can interpret this as the standard deviation of the differences
    between predictions and ground truth values. This is given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_051.jpg)'
  prefs: []
  type: TYPE_IMG
- en: To calculate MSE with the `Dlib` library, there exists the `mean_squared_error`
    function, which takes two floating-point vectors and returns the MSE.
  prefs: []
  type: TYPE_NORMAL
- en: The `mlpack` library provides the `MeanSquaredError` class with the static `Evaluate`
    function that runs an algorithm prediction and calculates the MSE.
  prefs: []
  type: TYPE_NORMAL
- en: The `Flashlight` library also has the `MeanSquaredError` class; objects to this
    class can be used as a loss function so that it has forward and backward functions.
    Also, this library has the `MSEMeter` class that measures the MSE between targets
    and predictions and can be used for performance tracking.
  prefs: []
  type: TYPE_NORMAL
- en: Mean absolute error
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Mean absolute error** (**MAE**) is another popular metric that’s used for
    quality estimation for regression algorithms. The MAE metric is a linear function
    with equally weighted prediction errors. It means that it does not take into account
    the direction of errors, which can be problematic in some cases. For example,
    if a model consistently underestimates or overestimates the true value, the MAE
    will still give a low score, even though the model may not be performing well.
    But this metric is more robust for outliers than RMSE. It is given by the following
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_061.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can use the `MeanSquaredError` class in the `Flashlight` library to calculate
    this type of error. The `MeanSquaredError` class implements the loss functionality
    so that it has the forward/backward functions. Unfortunately, there is no specific
    functionality for the MAE calculation in the `Dlib` and `mlpack` libraries, but
    it can be easily implemented with their linear algebra backends.
  prefs: []
  type: TYPE_NORMAL
- en: R-squared
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The R-squared metric is also known as a **coefficient of determination**. It
    is used to measure how well our independent variables (features from the training
    set) describe the problem and explain the variability of dependent variables (prediction
    values). Higher values tell us that the model explains our data well enough, while
    lower values tell us that the model makes many errors. This is given by the following
    equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_071.jpg)![](img/B19849_Formula_081.jpg)![](img/B19849_Formula_092.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B19849_Formula_10.png) is the number of predictions and ground
    truth items, ![](img/B19849_Formula_111.png) is the ground truth value for the
    *i*th item, and ![](img/B19849_Formula_122.png) is the prediction value for the
    *i*th item.
  prefs: []
  type: TYPE_NORMAL
- en: The only problem with this metric is that adding new independent variables may
    increase R-squared in some cases, so it’s crucial to consider the quality and
    relevance of these variables and to avoid overfitting the model. It may seem that
    the model begins to explain data better, but this isn’t true— this value only
    increases if there are more training items.
  prefs: []
  type: TYPE_NORMAL
- en: There are no out-of-the-box functions for calculating this metric in the `Flashlight`
    library; however, it is simple to implement it with linear algebra functions.
  prefs: []
  type: TYPE_NORMAL
- en: There is the `r_squared` function in the `Dlib` library for computing the R-squared
    coefficient between matching elements of two `std::vector` instances.
  prefs: []
  type: TYPE_NORMAL
- en: The `m``lpack` library has the `R2Score` class, which has the static `Evaluate`
    function that runs prediction for the specified algorithm and calculates the R-squared
    error.
  prefs: []
  type: TYPE_NORMAL
- en: Adjusted R-squared
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The adjusted R-squared metric was designed to solve the previously described
    problem of the R-squared metric. It is the same as the R-squared metric but with
    a penalty for a large number of independent variables. The main idea is that if
    new independent variables improve the model’s quality, the values of this metric
    increase; otherwise, they decrease. This can be given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *k* is the number of parameters and *n* is the number of samples.
  prefs: []
  type: TYPE_NORMAL
- en: Classification metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we start discussing classification metrics, we have to introduce an
    important concept called the **confusion matrix**. Let’s assume that we have two
    classes and an algorithm that assigns them to an object. Here, the confusion matrix
    will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ![](img/B19849_Formula_141.png) | ![](img/B19849_Formula_15.png) |'
  prefs: []
  type: TYPE_TB
- en: '| ![](img/B19849_Formula_161.png) | **True** **positive** (**TP**) | **False**
    **positive** (**FP**) |'
  prefs: []
  type: TYPE_TB
- en: '| ![](img/B19849_Formula_17.png) | **False** **negative** (**FN**) | **True**
    **negative** (**TN**) |'
  prefs: []
  type: TYPE_TB
- en: Here, ![](img/B19849_Formula_18.png) is the predicted class of the object and
    ![](img/B19849_Formula_19.png) is the ground truth label. The confusion matrix
    is an abstraction that we use to calculate different classification metrics. It
    gives us the number of items that were classified correctly and misclassified.
    It also provides us with information about the misclassification type. The false
    negatives are items that our algorithm incorrectly classified as negative ones,
    while the false positives are items that our algorithm incorrectly classified
    as positive ones. In this section, we’ll learn how to use this matrix and calculate
    different classification performance metrics.
  prefs: []
  type: TYPE_NORMAL
- en: A confusion matrix can be created in the `mlpack` library with the `ConfusionMatrix`
    function; this function works only for discrete data/categorical data.
  prefs: []
  type: TYPE_NORMAL
- en: The `Dlib` library also has instruments to get a confusion matrix. There is
    the `test_multiclass_decision` function that tests a multi-class decision function
    and returns a confusion matrix describing the results. It also has the `test_sequence_labeler`
    class that tests the `labeler` object against the given samples and labels and
    returns a confusion matrix summarizing the results.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One of the most obvious classification metrics is accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This provides us with a ratio of all positive predictions to all others. In
    general, this metric is not very useful because it doesn’t show us the real picture
    in terms of cases with an odd number of classes. Let’s consider a spam classification
    task and assume we have 10 spam letters and 100 non-spam letters. Our algorithm
    predicted 90 of them correctly as non-spam and classified only 5 spam letters
    correctly. In this case, accuracy will have the following value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'However, if the algorithm predicts all letters as non-spam, then its accuracy
    should be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This example shows that our model still doesn’t work because it is unable to
    predict all spam letters, but the accuracy value is good enough.
  prefs: []
  type: TYPE_NORMAL
- en: To calculate accuracy in the `Flashlight` library, we use the `FrameErrorMeter`
    class that can estimate the accuracy or the error rate depending on user settings.
  prefs: []
  type: TYPE_NORMAL
- en: The `mlpack` library has the `Accuracy` class with the static `Evaluate` function
    that runs classification for the specified algorithm and calculates the accuracy
    value.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the `Dlib` library doesn’t have functions to calculate accuracy
    values, so if needed, the function should be implemented with the linear algebra
    backend.
  prefs: []
  type: TYPE_NORMAL
- en: Precision and recall
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To estimate algorithm quality for each classification class, we will introduce
    two metrics: **precision** and **recall**. The following diagram shows all objects
    that are used in classification and how they have been marked according to the
    algorithm’s results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Precision and recall](img/B19849_03_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – Precision and recall
  prefs: []
  type: TYPE_NORMAL
- en: The circle in the center contains *selected elements*—the elements our algorithm
    predicted as positive ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'Precision is proportional to the number of correctly classified items within
    selected ones that are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_231.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Recall is proportional to the number of correctly classified items within all
    ground truth positive items that are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_241.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Another name for recall is **sensitivity**. Let’s assume that we are interested
    in the detection of positive items—let’s call them relevant ones. So, we use the
    recall value as a measure of an algorithm’s ability to detect relevant items and
    the precision value as a measure of an algorithm’s ability to see the differences
    between classes. These measures do not depend on the number of objects in each
    of the classes, and we can use them for imbalanced dataset classification.
  prefs: []
  type: TYPE_NORMAL
- en: There are several approaches to calculate these metrics in the `Dlib` library.
    There is the `average_precision` function that can be used to calculate the precision
    value directly. Also, there is the `test_ranking_function` function that tests
    the given ranking function on the provided data and can return the mean average
    precision. Another way is to use the `test_sequence_segmenter` class that tests
    a `segmenter` object against the given samples and returns the precision, recall,
    and F1-score, where `sequence_segmenter` is an object for segmenting a sequence
    of objects into a set of non-overlapping chunks.
  prefs: []
  type: TYPE_NORMAL
- en: The `mlpack` library has two classes—`Precision` and `Recall`—with static `Evaluate`
    functions that run classification for the specified algorithm and calculate precision
    and recall correspondingly.
  prefs: []
  type: TYPE_NORMAL
- en: The `Flashlight` library doesn’t have the functionality to calculate these values.
  prefs: []
  type: TYPE_NORMAL
- en: F-score
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In many cases, it is useful to have only one metric that shows the classification’s
    quality. For example, it makes sense to use some algorithms to search for the
    best hyperparameters, such as the grid search algorithm, which will be discussed
    later in this chapter. Such algorithms usually use one metric to compare different
    classification results after applying various parameter values during the search
    process. One of the most popular metrics for this case is the F-measure (or the
    F-score), which can be given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_251.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B19849_Formula_26.png) is the precision metric weight. Usually,
    the ![](img/B19849_Formula_27.png) value is equal to *1*. In such a case, we have
    the multiplier value equal to *2*, which gives us ![](img/B19849_Formula_281.png)
    if the *precision* = *1* and the *recall* = *1*. In other cases, when the precision
    value or the recall value tends to be zero, the F-measure value will also decrease.
  prefs: []
  type: TYPE_NORMAL
- en: The `Dlib` library provides the F1-score calculation only in the scope of the
    `test_sequence_segmenter` class functionality, which tests a `segmenter` object
    against the given samples and returns the precision, recall, and F1-score.
  prefs: []
  type: TYPE_NORMAL
- en: There is the `F1` class in the `mlpack` library that has the `Evaluate` function,
    which can be used to calculate the F1-score value by running a classification
    with the specified algorithm and data.
  prefs: []
  type: TYPE_NORMAL
- en: The `Flashlight` library doesn’t have the functionality to calculate F-score
    values.
  prefs: []
  type: TYPE_NORMAL
- en: AUC-ROC
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Usually, a classification algorithm will not return a concrete class identifier
    but a probability of an object belonging to some class. So, we usually use a threshold
    to decide whether an object belongs to a class or not. The most apparent threshold
    is 0.5, but it can work incorrectly in the case of imbalanced data (when we have
    a lot of values for one class and significantly fewer for another class).
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the methods we can use to estimate a model without the actual threshold
    is the value of the **Area Under the Receiver Operating Characteristic Curve**
    (**AUC-ROC)**. This curve is a line from (0,0) to (1,1) in coordinates of the
    **True Positive Rate** (**TPR**) and the **False Positive** **Rate** (**FPR**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_291.jpg)![](img/B19849_Formula_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The TPR value is equal to the recall, while the FPR value is proportional to
    the number of objects of the negative class that were classified incorrectly (they
    should be positive). In an ideal case, when there are no classification errors,
    we have `FPR = 0`, `TPR = 1`, and the area under the `1`. In the case of random
    predictions, the area under the ROC curve will be equal to `0.5` because we will
    have an equal number of TP and FP classifications:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – ROC](img/B19849_03_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – ROC
  prefs: []
  type: TYPE_NORMAL
- en: Each point on the curve corresponds to some threshold value. Notice that the
    curve’s steepness is an essential characteristic because we want to minimize the
    FPR, so we usually want this curve to tend to point (0,1). We can also successfully
    use the AUC-ROC metric with imbalanced datasets.
  prefs: []
  type: TYPE_NORMAL
- en: There is the `compute_roc_curve` function in the `Dlib` library that computes
    the ROC curve of the given data.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the `Flashlight` and `mlpack` libraries don’t have the functionality
    to compute the AUC-ROC metric.
  prefs: []
  type: TYPE_NORMAL
- en: Log-Loss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The logistic loss function value (the Log-Loss) is used as a target loss function
    for optimization purposes. It is given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can understand the Log-Loss value as the accuracy being corrected but with
    penalties for incorrect predictions. This function gives significant penalties,
    even for single misclassified objects, so all outlier objects in the data should
    be processed separately or removed from the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: There is the `loss_binary_log` class in the `Dlib` library that implements the
    Log-Loss, which is appropriate for binary classification problems. This class
    is designed to be used as a **neural network** (**NN**) module.
  prefs: []
  type: TYPE_NORMAL
- en: The `Flashlight` library has the `BinaryCrossEntropy` class that computes the
    binary cross-entropy loss between an input tensor `x` and a target tensor `y`.
    Also, the main aim of this class is the loss function implementation for NN training.
  prefs: []
  type: TYPE_NORMAL
- en: The `CrossEntropyError` class in the `mlpack` library also represents a loss
    function for NN construction; it has forward and backward functions. So, it is
    used to measure the network’s performance according to the cross-entropy between
    the input and target distributions.
  prefs: []
  type: TYPE_NORMAL
- en: In the current section, we learned about performance estimation metrics that
    can give you a clearer picture of your model accuracy, precision, and other performance
    characteristics. In the following section, we will learn about bias and variance
    and how to estimate and fix model prediction characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding bias and variance characteristics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bias and variance characteristics are used to predict model behavior. For example,
    the high variance effect, also known as **overfitting**, is a phenomenon in ML
    where the constructed model explains the examples from the training set but works
    relatively poorly on the examples that did not participate in the training process.
    This occurs because while training a model, random patterns will start appearing
    that are normally absent from the general population. The opposite of overfitting
    is known as **underfitting**, which corresponds to the high bias effect. This
    happens when the trained model becomes unable to predict patterns in new data
    or even in the training data. Such an effect can be the result of a limited training
    dataset or weak model design.
  prefs: []
  type: TYPE_NORMAL
- en: Before we go any further and describe what they mean, we should consider **validation**.
    Validation is a technique that’s used to test model performance. It estimates
    how well the model makes predictions on new data. New data is data that we did
    not use for the training process. To perform validation, we usually divide our
    initial dataset into two or three parts. One part should contain most of the data
    and will be used for training, while the other ones will be used to validate and
    test the model. Usually, validation is performed for iterative algorithms after
    one training cycle (often called an **epoch**). Alternatively, we perform testing
    after the overall training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Validation and testing operations evaluate the model on the data we have excluded
    from the training process, which results in the values of the performance metrics
    that we chose for this particular model. For example, the original dataset can
    be divided into the following parts: 80% for training, 10% for validation, and
    10% for testing. The values of these validation metrics can be used to estimate
    model and prediction error trends. The most crucial issue for validation and testing
    is that the data for them should always be from the same distribution as the training
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the rest of this chapter, we will use the polynomial regression model
    to show different prediction behaviors. The polynomial degree will be used as
    a hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: Bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Bias is a prediction characteristic that tells us about the distance between
    model predictions and ground truth values. Usually, we use the term *high bias*
    or *underfitting* to say that model prediction is too far from the ground truth
    values, which means that the model generalization ability is weak. Consider the
    following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Regression model predictions with the polynomial degree equal
    to 1](img/B19849_03_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – Regression model predictions with the polynomial degree equal to
    1
  prefs: []
  type: TYPE_NORMAL
- en: This graph shows the original values, the values used for validation, and a
    line that represents the polynomial regression model output. In this case, the
    polynomial degree is equal to *1*. We can see that the predicted values do not
    describe the original data at all, so we can say that this model has a high bias.
    Also, we can plot validation metrics for each training cycle to get more information
    about the training process and the model’s behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following graph shows the MAE metric values for the training process of
    the polynomial regression model, where the polynomial degree is equal to *1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Train and validation loss values](img/B19849_03_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – Train and validation loss values
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the lines for the metric values for the train and validation
    data are parallel and distant enough. Moreover, these lines do not change their
    direction after numerous training iterations. These facts also tell us that the
    model has a high bias because, for a regular training process, validation metric
    values should be close to the training values.
  prefs: []
  type: TYPE_NORMAL
- en: To deal with high bias, we can add more features to the training samples. For
    example, increasing the polynomial degree for the polynomial regression model
    adds more features; these all-new features describe the original training sample
    because each additional polynomial term is based on the original sample value.
  prefs: []
  type: TYPE_NORMAL
- en: Variance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Variance is a prediction characteristic that tells us about the variability
    of model predictions; in other words, how big the range of output values can be.
    Usually, we use the term *high variance* or *overfitting* in the case when a model
    tries to incorporate many training samples very precisely. In such a case, the
    model cannot provide a good approximation for new data but has excellent performance
    on the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following graph shows the behavior of the polynomial regression model,
    with the polynomial degree equal to `15`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Regression model predictions with the polynomial degree equal
    to 15](img/B19849_03_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – Regression model predictions with the polynomial degree equal to
    15
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that the model incorporates almost all the training data. Notice
    that the training data is indicated as `orig` in the plot’s legend, while the
    data used for validation is indicated as `val` in the plot’s legend. We can see
    that these two sets of data—training data and validation data—are somehow distant
    from each other and that our model misses the validation data because of a lack
    of approximation. The following graph shows the MAE values for the learning process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – Validation error](img/B19849_03_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – Validation error
  prefs: []
  type: TYPE_NORMAL
- en: We can see that after approximately 75 learning iterations, the model began
    to predict training data much better, and the error value became lower. However,
    for the validation data, the MAE values began to increase. To deal with high variance,
    we can use special regularization techniques, which we will discuss in the following
    sections. We can also increase the number of training samples and decrease the
    number of features in one sample to reduce high variance.
  prefs: []
  type: TYPE_NORMAL
- en: The performance metrics plots we discussed in the preceding paragraphs can be
    drawn at the runtime of the training process. We can use them to monitor the training
    process to see high bias or high variance problems. Notice that for the polynomial
    regression model, MAE is a better performance characteristic than MSE or RMSE
    because squared functions average errors too much. Moreover, even a straight-line
    model can have low MSE values for such data because errors from both sides of
    the line compensate for each other. The choice between MAE and MSE depends on
    the specific task and goals of the project. If the overall accuracy of predictions
    is important, then MAE may be preferable. But MAE gives equal weight to all errors,
    regardless of their magnitude. This means that it is not sensitive to outliers,
    which can significantly affect the overall error. So, if it is necessary to minimize
    large errors, then MSE can give more accurate results. In some cases, you can
    use both metrics to analyze the performance of the model in more depth.
  prefs: []
  type: TYPE_NORMAL
- en: Normal training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Consider the case of a training process where the model has balanced bias and
    variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 – Predictions when a model was trained ideally](img/B19849_03_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – Predictions when a model was trained ideally
  prefs: []
  type: TYPE_NORMAL
- en: 'In this graph, we can see that the polynomial regression model’s output for
    the polynomial degree is equal to eight. The output values are close to both the
    training data and validation data. The following graph shows the MAE values during
    the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8 – Loss values when a model was trained ideally](img/B19849_03_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 – Loss values when a model was trained ideally
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the MAE value decreases consistently and that the predicted
    values for the training and validation data become close to the ground truth values.
    This means that the model’s hyperparameters were good enough to balance bias and
    variance.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Regularization is a technique that’s used to reduce model overfitting. There
    are two main approaches to regularization. The first one is known as training
    data preprocessing. The second one is loss function modification. The main idea
    of the loss function modification technique is to add terms to the loss function
    that penalize algorithm results, thereby leading to significant variance. The
    idea of training data preprocessing techniques is to add more distinct training
    samples. Usually, in such an approach, new training samples are generated by augmenting
    existing ones. In general, both approaches add some prior knowledge about the
    task domain to the model. This additional information helps us with variance regularization.
    Therefore, we can conclude that regularization is any technique that leads to
    minimizing generalization errors.
  prefs: []
  type: TYPE_NORMAL
- en: L1 regularization – Lasso
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'L1 regularization is an additional term to the loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This additional term adds the absolute value of the magnitude of parameters
    as a penalty. Here, *λ* is a regularization coefficient. Higher values of this
    coefficient lead to stronger regularization and can lead to underfitting. Sometimes,
    this type of regularization is called **Least Absolute Shrinkage and Selection
    Operator** (**Lasso**) regularization. The general idea behind L1 regularization
    is to penalize less important features. We can think of it as a feature selection
    process because as the optimization proceeds, some of the coefficients (for example,
    in linear regression) become zero, indicating that those features are not contributing
    to the model’s performance. We end up with a sparse model with fewer features.
  prefs: []
  type: TYPE_NORMAL
- en: L2 regularization – Ridge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'L2 regularization is also an additional term to the loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_331.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This additional term adds a squared value of the magnitude of parameters as
    a penalty. This penalty shrinks the magnitude of the parameters toward zero. *λ*
    is also a coefficient of regularization. Its higher values lead to stronger regularization
    and can lead to underfitting because the model becomes too constrained and unable
    to learn complex relationships in the data. Another name for this regularization
    type is **ridge regularization**. Unlike L1 regularization, this type does not
    have a feature selection characteristic. Instead, we can interpret it as a model
    smoothness configurator. In addition, L2 regularization is computationally more
    efficient for gradient descent-based optimizers because its differentiation has
    an analytical solution.
  prefs: []
  type: TYPE_NORMAL
- en: In the `Dlib` library, regularization mechanisms are usually integrated into
    algorithm implementations —for example, the `rr_trainer` class that represents
    a tool for performing linear ridge regression, which is the regularized **least
    squares support vector** **machine** (**LSSVM**).
  prefs: []
  type: TYPE_NORMAL
- en: There is the `LRegularizer` class in the `mlpack` library that implements a
    generalized L-regularizer, allowing both L1 and L2 regularization methods for
    NNs. Some algorithm implementations, such as the `LARS` class can train a LARS/LASSO/Elastic
    Net model and has L1 and L2 regularization parameters. The `LinearRegression`
    class has a regularization parameter for ridge regression.
  prefs: []
  type: TYPE_NORMAL
- en: There is no standalone functionality in the `Flashlight` library for regularization.
    All regularizations are integrated into the NN optimization algorithm implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The data augmentation process can be treated as regularization because it adds
    some prior knowledge about the problem to the model. This approach is common in
    **computer vision** (**CV**) tasks such as image classification or object detection.
    In such cases, when we can see that the model begins to overfit and does not have
    enough training data, we can augment the images we already have to increase the
    size of our dataset and provide more distinct training samples. Image augmentations
    are random image rotations, cropping and translations, mirroring flips, scaling,
    and proportion changes. But data augmentation should be carefully designed for
    the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: If the generated data is too similar to the original data, it can lead to overfitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can introduce noise or artifacts into the dataset, which can degrade the
    quality of the resulting models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The augmented data may not accurately reflect the real-world distribution of
    data, leading to a domain shift between the training and test sets. This can result
    in poor generalization performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Early stopping
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Stopping the training process early can also be interpreted as a form of regularization.
    This means that if we detected that the model started to overfit, we can stop
    the training process. In this case, the model will have parameters once the training
    has stopped.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization for NNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: L1 and L2 regularizations are widely used to train NNs and are usually called
    **weight decay**. Data augmentation also plays an essential role in the training
    processes for NNs. Other regularization methods can be used in NNs. For example,
    Dropout is a particular type of regularization that was developed especially for
    NNs. This algorithm randomly drops some NN nodes; it makes other nodes more insensitive
    to the weights of other nodes, which means the model becomes more robust and stops
    overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will see how to select model hyperparameters with
    automated algorithms versus manual tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Model selection with the grid search technique
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is necessary to have a set of proper hyperparameter values to create a good
    ML model. The reason for this is that having random values leads to controversial
    results and behaviors that are not expected by the practitioner. There are several
    approaches we can follow to choose the best set of hyperparameter values. We can
    try to use hyperparameters from algorithms we have already trained that are similar
    to our task. We can also try to find some heuristics and tune them manually. However,
    this task can be automated. The grid search technique is an automated approach
    for searching for the best hyperparameter values. It uses the cross-validation
    technique for model performance estimation.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have already discussed what the validation process is. It is used to estimate
    the model’s performance data that we haven’t used for training. If we have a limited
    or small training dataset, randomly sampling the validation data from the original
    dataset leads to the following problems:'
  prefs: []
  type: TYPE_NORMAL
- en: The size of the original dataset is reduced
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is the probability of leaving data that’s important for validation in
    the training part
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To solve these problems, we can use the cross-validation approach. The main
    idea behind it is to split the original dataset in such a way that all the data
    will be used for training and validation. Then, the training and validation processes
    are performed for all partitions, and the resulting values are averaged.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most well-known method of cross-validation is *K*-fold cross-validation,
    where K refers to the number of folds or partitions used to split the dataset
    The idea is to divide the dataset into *K* blocks of the same size. Then, we use
    one of the blocks for validation and the others for training. We repeat this process
    *K* times, each time choosing a different block for validation, and in the end,
    we average all the results. The data splitting scheme during the whole cross-validation
    cycle looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Divide the dataset into *K* blocks of the same size.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select one of the blocks for validation and the remaining *K*-1 blocks for training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat this process, making sure that each block is used for validation and
    the rest are used for training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Average the results of the performance metrics that were calculated for the
    validation sets on each iteration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following diagram shows the cross-validation cycle:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – K-fold validation scheme](img/B19849_03_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 – K-fold validation scheme
  prefs: []
  type: TYPE_NORMAL
- en: Grid search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main idea behind the grid search approach is to create a grid of the most
    reasonable hyperparameter values. The grid is used to generate a reasonable number
    of distinct parameter sets quickly. We should have some prior knowledge about
    the task domain to initialize the minimum and maximum values for grid generation,
    or we can initialize the grid with some reasonable broad ranges. However, if the
    chosen ranges are too broad, the process of searching for parameters can take
    a long time and will require a significant amount of computational resources.
  prefs: []
  type: TYPE_NORMAL
- en: At each step, the grid search algorithm chooses a set of hyperparameter values
    and trains a model. After that, the training step algorithm uses the K-fold cross-validation
    technique to estimate model performance. We should also define a single model
    performance estimation metric for model comparison that the algorithm will calculate
    at each training step for every model. After completing the model training process
    with each set of parameters from every grid cell, the algorithm chooses the best
    set of hyperparameter values by comparing the metric’s values and selecting the
    best one. Usually, the set with the smallest value is the best one.
  prefs: []
  type: TYPE_NORMAL
- en: Consider an implementation of this algorithm in different libraries. Our task
    is to select the best set of hyperparameters for the polynomial regression model,
    which gives us the best curve that fits the given data. The data in this example
    is some cosine function values with some random noise.
  prefs: []
  type: TYPE_NORMAL
- en: mlpack example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `mlpack` library contains a special `HyperParameterTuner` class to do hyperparameter
    searches with different algorithms in both discrete and continuous spaces. The
    default search algorithm is grid search. This class is a template and should be
    specialized for a concrete task. The general definition is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the main template parameters are the algorithm that we want
    to find, hyperparameters for the performance metric, and the cross-validation
    algorithm. Let’s define a `HyperParameterTuner` object to search for the best
    regularization value for the linear ridge regression algorithm. The definition
    will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here, `LinearRegression` is the target algorithm class, `MSE` is the performance
    metric class that calculates the MSE, and `SimpleCV` is the class that implements
    cross-validation. It splits data into two sets, training and validation, and then
    runs training on the training set and evaluates performance on the validation
    set. Also, we see that we pass the `validation_size` parameter into the constructor.
    It has a value of 0.2, which means usage of 80% of data for training and the remaining
    20% for evaluation with MSE. The two following constructor parameters are our
    training dataset; `samples` are just samples, and `labels` are corresponding labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how we can generate a training dataset for these examples. It will
    take the two following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Generating data that follows some predefined pattern—for example, 2D normally
    distributed points, plus some noise
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data normalization
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following sample shows how to generate data using the Armadillo library,
    which is the `mlpack` mathematical backend:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Notice that for samples, we used the `arma::mat` type, and for labels, the `arma::rowvec`
    type. So, samples are placed into the matrix entity and labels into a one-dimensional
    vector correspondingly. Also, we used the `arma::randn` function to generate normally
    distributed data and noise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can normalize data in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We used the object of the `StandardScaler` class from the `mlpack` library to
    perform normalization. This object should be first trained on some data with the
    `Fit` method to learn mean and variance, and then it can be applied to other data
    with the `Transform` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s discuss how to prepare the data and how to define the hyperparameter
    tuner object. So, we are ready to launch the grid search for the best regularization
    values; the following sample shows how to do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We defined a `lambdas` vector with our search space and then called the `Optimize`
    method of the hyperparameter tuner object. You can see that the return value is
    a tuple, and we use the `std::tie` function to extract the specific value. The
    `Optimize` method takes a variable number of arguments depending on the ML algorithm
    we use in the search, and each argument will define a search space for each hyperparameter
    used in the algorithm. The constructor of the `LinearRegression` class has only
    one `lambda` parameter. After the search is finished, we use the best-searched
    parameter directly, or we can get the best-optimized model object, as shown in
    the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now try the new model on new data to see how it works. At first, we
    will generate data and normalize it with a pre-trained scaler object, as shown
    in the following sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Here, we used the `arma::linspace` function to get a linearly distributed range
    of data. This function produces a vector; we wrote additional code that transforms
    this vector into a matrix object. Then, we used the already trained `sample_scaler`
    object to normalize the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following sample shows how to use the model with the best parameter we
    found with the grid search:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The one important thing you have to notice is that the ML algorithm used for
    a grid search should be supported by `SimpleCV` or other validation classes. If
    it doesn’t have a default implementation, you will need to provide it yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Optuna with Flashlight example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is no support for any hyperparameter tuning algorithms in the Flashlight
    library, but we can use an external tool named Optuna to deal with ML programs
    that we want to search the best hyperparameters for. The main idea is to use some
    **inter-process communication** (**IPC**) approach to run training with different
    parameters and get some performance metric values after training.
  prefs: []
  type: TYPE_NORMAL
- en: Optuna is a hyperparameter tuning framework designed to be used with different
    ML libraries and programs. It is implemented with the Python programming language,
    so the main area of its application is tools that have some Python APIs. But Optuna
    also has a **command-line interface** (**CLI**) that can be used with tools that
    don’t support Python. Another way to use such tools is to call them from Python,
    passing their command-line parameters and reading their standard output. This
    book will show an example of such type of Optuna usage, because writing a Python
    program is more useful than creating Bash scripts for the same automatization
    tasks, from the author’s point of view.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use Optuna for hyperparameter tuning, we need to complete the three following
    stages:'
  prefs: []
  type: TYPE_NORMAL
- en: Define an objective function for optimization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a study object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run optimization process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s see how we can write a simple Optuna program in Python to search for the
    best parameter for a polynomial regression algorithm written in C++ with the Flashlight
    library.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to import the required Python libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to define an objective function; it’s a function that is called
    by the Optuna tuning algorithm. It takes the `Trial` class object that contains
    a set of hyperparameter distributions and should return a performance metric value.
    The exact hyperparameter values should be sampled from the passed distributions.
    The following sample shows how we implement such a function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In this code, we used a family of functions in the `trial` object to sample
    concrete hyperparameter values. We sampled the `lr` learning rate with the call
    of the `suggest_float` method, and the `d` polynomial degree and the `bs` batch
    size with the `suggest_int` method. You can see that the signatures of these methods
    are pretty much the same. They take the name of hyperparameter, the low and the
    high bounds of a value range, and they can take a step value, which we didn’t
    use. These methods can sample values from discrete space and from continuous space
    too. The `suggest_float` method samples from a continuous space, and the `suggest_int`
    method samples from a discrete space.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we called the `run` method from the `subprocess` module; it launches another
    process in the system. This method takes the array of strings of command-line
    parameters and some other parameters —in our case, the `stdout` redirection. This
    redirection is needed because we want to get the process’s output in a return
    value of the `run` method call; you can see this in the last lines as `result.stdout`,
    which is converted from string to floating-point values and is interpreted as
    the MSE.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having the objective function, we can define a `study` object. This object
    tells Optuna how to tune hyperparameters. The two main characteristics of this
    object are the optimization direction and a search space for the hyperparameter
    sampler algorithm. The following sample shows how to define a discrete search
    space for our task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In this Python code, we defined a `search_space` dictionary with three items.
    Each item has the key string and the array value. The keys are `learning_rate`,
    `polynomial_degree`, and `batch_size`. After we define the search space, we can
    create a `study` object; the following sample shows this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We used the `create_study` function from the `optuna` module and passed three
    parameters: `study_name`, `direction`, and `sampler`. The optimization direction
    we specified will be minimization, as we want to minimize MSE. For the `sampler`
    object, we used `GridSampler`, because we want to implement the grid search approach,
    and we initialized it with our search space.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step is to apply an optimization process and get the best hyperparameters.
    We can do this in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We used the `optimize` method of our `study` object. You can see that it took
    a single parameter —our `objective` function that calls the external process to
    try sampled hyperparameters. The result of optimization was stored in the `study`
    object in the `best_value` and the `best_params` fields. The `best_value` field
    contains the best MSE value, and the `best_params` field contains the dictionary
    with the best hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: This is the minimal sample of how to use Optuna. This framework has a wide variety
    of tuning and sampling algorithms, and a real application can be much more complicated.
    Also, the use of Python saves us from writing a lot of boilerplate code for the
    CLI approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a short look at a polynomial regression implementation with the
    Flashlight library. I will show only the most important parts; a full example
    can be found here: [https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/blob/main/Chapter03/flashlight/grid_fl.cc](https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/blob/main/Chapter03/flashlight/grid_fl.cc).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first important part is that our program should take all hyperparameters
    from the command-line argument. It can be implemented in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: First, we checked that we had enough command-line arguments by comparing `argc`
    parameter with the required number. In the fail case, we print a help message.
    But in the successful case, we read all hyperparameters from the `argv` parameter
    and convert them from strings to the appropriate types.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, our program generates 2D cosine function points and mixes them with noise.
    To approximate a nonlinear function with linear regression, we can convert a simple
    Ax+b approach to a more complex polynomial like the following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '`a1*x+a2*x^2+a3*x^3+...+an*x^n + b`'
  prefs: []
  type: TYPE_NORMAL
- en: It means that we have to choose some polynomial degree and convert our single-dimensional
    `x` value to a multidimensional one by raising `x` elements to the corresponding
    powers. So, the polynomial degree is the most important hyperparameter in this
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Flashlight library doesn’t have any special implementation for regression
    algorithms because this library is oriented toward NN algorithms. But regression
    can be easily implemented with the gradient descent approach; the following code
    sample shows how it can be done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: First, we defined learnable variables for the Flashlight `autograd` system;
    they are weights for each power of `X` and bias term. Then, we ran a loop for
    a specified number of epochs and the second loop over data batches to make the
    calculation vectorized; it makes computations more effective and makes the learning
    process less noise-dependent. For each batch of training data, we calculated predictions
    by getting a polynomial value; see the line with a call of the `matmul` function.
    The objective of the `MeanSquareError` class was used to get the loss function
    value. To calculate the corresponding gradients, see the `mse_func.forward` and
    `mse_func.backward` calls. Then, we updated our polynomial weights and biases
    with the learning rate and corresponding gradients.
  prefs: []
  type: TYPE_NORMAL
- en: 'All these concepts will be described in detail in the following chapters. The
    next important part is the `error` and `mse` value calculations. The `error` value
    is the Flashlight tensor object that contains the average MSE for the whole epoch,
    and the `mse` value is the floating-point value of this single-value tensor. This
    `mse` variable is printed to the standard output stream of the program at the
    end of the training, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We read this value in our Python program and return the result of our Optuna
    objective function for the given set of hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Dlib example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `Dlib` library also contains all the necessary functionality for the grid
    search algorithm. However, we should use functions instead of classes. The following
    code snippet shows the `CrossValidationScore` function’s definition. This function
    performs cross-validation and returns the value of the performance metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The `CrossValidationScore` function takes the hyperparameters that were set
    as arguments. Inside this function, we defined a trainer for a model with the
    `svr_trainer` class, which implements kernel ridge regression based on the `Shogun`
    library example.
  prefs: []
  type: TYPE_NORMAL
- en: After we defined the model, we used the `cross_validate_regression_trainer()`
    function to train the model with the cross-validation approach. This function
    automatically splits our data into folds, with its last argument being the number
    of folds. The `cross_validate_regression_trainer()` function returns the matrix,
    along with the values of different performance metrics. Notice that we do not
    need to define them because they are predefined in the library’s implementation.
  prefs: []
  type: TYPE_NORMAL
- en: The first value in this matrix is the average MSE value. We used this value
    as a function result. However, there is no strong requirement for what value this
    function should return; the requirement is that the return value should be numeric
    and comparable. Also, notice that we defined the `CrossValidationScore` function
    as a lambda to simplify access to the training data container defined in the outer
    scope.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can search for the best parameters that were set with the `find_min_global`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This function takes the cross-validation function, the container with minimum
    values for parameter ranges, the container with maximum values for parameter ranges,
    and the number of cross-validation repeats. Notice that the initialization values
    for parameter ranges should go in the same order as the arguments that were defined
    in the `CrossValidationScore` function. Then, we can extract the best hyperparameters
    and train our model with them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We used the same model definition as in the `CrossValidationScore` function.
    For the training process, we used all of our training data. The `train` method
    of the `trainer` object was used to complete the training process. The training
    result is a function that takes a single sample as an argument and returns a prediction
    value.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed how to estimate an ML model’s performance and
    what metrics can be used for such estimation. We considered different metrics
    for regression and classification tasks and what characteristics they have. We
    also saw how performance metrics can be used to determine the model’s behavior
    and looked at bias and variance characteristics. We looked at some high bias (underfitting)
    and high variance (overfitting) problems and considered how to solve them. We
    also learned about regularization approaches, which are often used to deal with
    overfitting. We then studied what validation is and how it is used in the cross-validation
    technique. We saw that the cross-validation technique allows us to estimate model
    performance while training limited data. In the last section, we combined an evaluation
    metric and cross-validation in the grid search algorithm, which we can use to
    select the best set of hyperparameters for our model.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll learn about ML algorithms we can use to solve concrete
    problems. The next topic we will discuss in depth is clustering—the procedure
    of splitting the original set of objects into groups classified by properties.
    We will look at different clustering approaches and their characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Choosing the Right Metric for Evaluating Machine Learning Models—Part* *1*:
    [https://medium.com/usf-msds/choosing-the-right-metric-for-machine-learning-models-part-1-a99d7d7414e4](https://medium.com/usf-msds/choosing-the-right-metric-for-machine-learning-models-part-1-a99d7d7414e4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Understand Regression Performance* *Metrics*: [https://becominghuman.ai/understand-regression-performance-metrics-bdb0e7fcc1b3](https://becominghuman.ai/understand-regression-performance-metrics-bdb0e7fcc1b3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Classification Performance* *Metrics*: [https://nlpforhackers.io/classification-performance-metrics/](https://nlpforhackers.io/classification-performance-metrics/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*REGULARIZATION: An important concept in Machine* *Learning*: [https://towardsdatascience.com/regularization-for-machine-learning-67c37b132d61](https://towardsdatascience.com/regularization-for-machine-learning-67c37b132d61)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An overview of regularization techniques in **deep learning** (**DL**) (with
    Python code): [https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques](https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Understanding the Bias-Variance* *Tradeoff*: [https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DL – Overfitting: [https://towardsdatascience.com/combating-overfitting-in-deep-learning-efb0fdabfccc](https://towardsdatascience.com/combating-overfitting-in-deep-learning-efb0fdabfccc)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A Gentle Introduction to k-fold* *Cross-Validation*: [https://machinelearningmastery.com/k-fold-cross-validation/](https://machinelearningmastery.com/k-fold-cross-validation/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 2: Machine Learning Algorithms'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, we’ll show you how to implement different well-known machine learning
    models (algorithms) using a variety of C++ frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part comprises the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B19849_04.xhtml#_idTextAnchor228), *Clustering*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B19849_05.xhtml#_idTextAnchor258), *Anomaly Detection*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B19849_06.xhtml#_idTextAnchor301), *Dimensionality Reduction*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B19849_07.xhtml#_idTextAnchor383), *Classification*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B19849_08.xhtml#_idTextAnchor438), *Recommender Systems*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B19849_09.xhtml#_idTextAnchor496), *Ensemble Learning*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
