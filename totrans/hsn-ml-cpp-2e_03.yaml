- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Measuring Performance and Selecting Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量性能和选择模型
- en: This chapter describes bias and variance effects and their pathological cases,
    which usually appear when training **machine learning** (**ML**) models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章描述了偏差和方差效应及其病理情况，这些情况通常出现在训练**机器学习**（ML）模型时。
- en: In this chapter, we will learn how to deal with overfitting by using regularization
    and discuss different techniques we can use. We will also consider different model
    performance estimation metrics and how they can be used to detect training problems.
    Toward the end of this chapter, we will look at how to find the best hyperparameters
    for a model by introducing the grid search technique and its implementation in
    C++.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何通过使用正则化来处理过拟合，并讨论我们可以使用的不同技术。我们还将考虑不同的模型性能估计指标以及它们如何被用来检测训练问题。在本章的末尾，我们将探讨如何通过介绍网格搜索技术和其在
    C++ 中的实现来找到模型的最佳超参数。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Performance metrics for ML models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习模型的性能指标
- en: Understanding bias and variance characteristics
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解偏差和方差特性
- en: Model selection with the grid search technique
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用网格搜索技术进行模型选择
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'For this chapter, you will need the following:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，你需要以下内容：
- en: A modern C++ compiler with C++20 support
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持 C++20 的现代 C++ 编译器
- en: CMake build system version >= 3.10
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CMake 构建系统版本 >= 3.10
- en: '`Dlib` library'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Dlib` 库'
- en: '`mlpack` library'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlpack` 库'
- en: '`Flashlight` library'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Flashlight` 库'
- en: '`Plotcpp` library'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Plotcpp` 库'
- en: 'The code files for this chapter can be found in the following GitHub repo:
    [https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter03](https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter03)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在以下 GitHub 仓库中找到：[https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter03](https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter03)
- en: Performance metrics for ML models
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习模型的性能指标
- en: When we develop or implement a particular ML algorithm, we need to estimate
    how well it works. In other words, we need to estimate how well it solves our
    task. Usually, we use some numeric metrics for algorithm performance estimation.
    An example of such a metric could be a value of **mean squared error** (**MSE**)
    that’s been calculated for target and predicted values. We can use this value
    to estimate how distant our predictions are from the target values we used for
    training. Another use case for performance metrics is their use as objective functions
    in optimization processes. Some performance metrics are used for manual observations,
    though others can be used for optimization purposes too.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开发或实施特定的机器学习算法时，我们需要评估其工作效果。换句话说，我们需要评估其解决我们任务的能力。通常，我们使用一些数值指标来估计算法性能。这样的指标可以是针对目标和预测值计算出的**均方误差**（MSE）的值。我们可以使用这个值来估计我们的预测值与用于训练的目标值之间的距离。性能指标的另一个用途是在优化过程中作为目标函数。一些性能指标用于手动观察，而其他指标也可以用于优化目的。
- en: 'Performance metrics are different for each of the ML algorithm types. In [*Chapter
    1*](B19849_01.xhtml#_idTextAnchor015), *Introduction to Machine Learning with
    C++*, we discussed that two main categories of ML algorithms exist: **regression
    algorithms** and **classification algorithms**. There are other types of algorithms
    in the ML discipline, but these two are the most common ones. This section will
    go over the most popular performance metrics for regression and classification
    algorithms.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法类型的不同性能指标。在[*第1章*](B19849_01.xhtml#_idTextAnchor015)《使用 C++ 的机器学习入门》中，我们讨论了存在两种主要的机器学习算法类别：**回归算法**和**分类算法**。机器学习学科中还有其他类型的算法，但这两类是最常见的。本节将介绍回归和分类算法最流行的性能指标。
- en: Regression metrics
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归指标
- en: Regression task metrics are used to measure how close predicted values are to
    ground truth ones. Such measurements can help us estimate the prediction quality
    of the algorithm. Under regression metrics, there are four main metrics, which
    we will dive into in the following subsections.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 回归任务指标用于衡量预测值与真实值之间的接近程度。这类测量可以帮助我们评估算法的预测质量。在回归指标下，有四个主要指标，我们将在以下小节中深入探讨。
- en: MSE and RMSE
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MSE 和 RMSE
- en: 'MSE is a widely used metric for regression algorithms to estimate their quality.
    It is an average squared difference between the predictions and ground truth values.
    This is given by the following equation:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: MSE是回归算法广泛使用的指标，用于估计其质量。它是预测值与真实值之间平均平方差的度量。这由以下公式给出：
- en: '![](img/B19849_Formula_011.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_011.jpg)'
- en: Here, ![](img/B19849_Formula_021.png) is the number of predictions and ground
    truth items, ![](img/B19849_Formula_031.png) is the ground truth value for the
    *i*th item, and ![](img/B19849_Formula_041.png) is the prediction value for the
    *i*th item.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B19849_Formula_021.png)是预测和真实项的数量，![](img/B19849_Formula_031.png)是第*i*项的真实值，![](img/B19849_Formula_041.png)是第*i*项的预测值。
- en: MSE is often used as a target loss function for optimization algorithms because
    it is smoothly differentiable and is a convex function.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: MSE经常用作优化算法的目标损失函数，因为它平滑可微且是凸函数。
- en: 'The **root mean squared error** (**RMSE**) metric is usually used to estimate
    performance, such as when we need to give bigger weights to higher errors (to
    penalize them). We can interpret this as the standard deviation of the differences
    between predictions and ground truth values. This is given by the following equation:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**均方根误差**（**RMSE**）指标通常用于估计性能，例如当我们需要给更高的误差更大的权重（以惩罚它们）时。我们可以将其解释为预测值与真实值之间差异的标准差。这由以下公式给出：'
- en: '![](img/B19849_Formula_051.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_051.jpg)'
- en: To calculate MSE with the `Dlib` library, there exists the `mean_squared_error`
    function, which takes two floating-point vectors and returns the MSE.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`Dlib`库计算MSE，存在一个`mean_squared_error`函数，它接受两个浮点向量并返回MSE。
- en: The `mlpack` library provides the `MeanSquaredError` class with the static `Evaluate`
    function that runs an algorithm prediction and calculates the MSE.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '`mlpack`库提供了具有静态`Evaluate`函数的`MeanSquaredError`类，该函数运行算法预测并计算MSE。'
- en: The `Flashlight` library also has the `MeanSquaredError` class; objects to this
    class can be used as a loss function so that it has forward and backward functions.
    Also, this library has the `MSEMeter` class that measures the MSE between targets
    and predictions and can be used for performance tracking.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`Flashlight`库也有`MeanSquaredError`类；该类的对象可以用作损失函数，因此具有前向和后向函数。此外，这个库还有一个`MSEMeter`类，用于测量目标和预测之间的MSE，可用于性能跟踪。'
- en: Mean absolute error
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 平均绝对误差
- en: '**Mean absolute error** (**MAE**) is another popular metric that’s used for
    quality estimation for regression algorithms. The MAE metric is a linear function
    with equally weighted prediction errors. It means that it does not take into account
    the direction of errors, which can be problematic in some cases. For example,
    if a model consistently underestimates or overestimates the true value, the MAE
    will still give a low score, even though the model may not be performing well.
    But this metric is more robust for outliers than RMSE. It is given by the following
    equation:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**平均绝对误差**（**MAE**）是另一个用于回归算法质量估计的流行指标。MAE指标是一个具有等权预测误差的线性函数。这意味着它不考虑误差的方向，这在某些情况下可能会出现问题。例如，如果一个模型持续低估或高估真实值，即使模型可能表现不佳，MAE仍然会给出一个低分。但这个指标比RMSE对异常值更稳健。它由以下公式给出：'
- en: '![](img/B19849_Formula_061.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_061.jpg)'
- en: We can use the `MeanSquaredError` class in the `Flashlight` library to calculate
    this type of error. The `MeanSquaredError` class implements the loss functionality
    so that it has the forward/backward functions. Unfortunately, there is no specific
    functionality for the MAE calculation in the `Dlib` and `mlpack` libraries, but
    it can be easily implemented with their linear algebra backends.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`Flashlight`库中的`MeanSquaredError`类来计算这种类型的误差。`MeanSquaredError`类实现了损失功能，因此具有前向/后向函数。不幸的是，`Dlib`和`mlpack`库中没有专门用于MAE计算的函数，但可以使用它们的线性代数后端轻松实现。
- en: R-squared
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: R-squared
- en: 'The R-squared metric is also known as a **coefficient of determination**. It
    is used to measure how well our independent variables (features from the training
    set) describe the problem and explain the variability of dependent variables (prediction
    values). Higher values tell us that the model explains our data well enough, while
    lower values tell us that the model makes many errors. This is given by the following
    equations:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: R-squared指标也被称为**确定系数**。它用于衡量我们的独立变量（训练集的特征）如何描述问题并解释因变量的可变性（预测值）。较高的值告诉我们模型足够好地解释了我们的数据，而较低的值告诉我们模型犯了错误。这由以下方程给出：
- en: '![](img/B19849_Formula_071.jpg)![](img/B19849_Formula_081.jpg)![](img/B19849_Formula_092.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_071.jpg)![](img/B19849_Formula_081.jpg)![](img/B19849_Formula_092.jpg)'
- en: Here, ![](img/B19849_Formula_10.png) is the number of predictions and ground
    truth items, ![](img/B19849_Formula_111.png) is the ground truth value for the
    *i*th item, and ![](img/B19849_Formula_122.png) is the prediction value for the
    *i*th item.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B19849_Formula_10.png)是预测和真实值项的数量，![](img/B19849_Formula_111.png)是第*i*个项的真实值，而![](img/B19849_Formula_122.png)是第*i*个项的预测值。
- en: The only problem with this metric is that adding new independent variables may
    increase R-squared in some cases, so it’s crucial to consider the quality and
    relevance of these variables and to avoid overfitting the model. It may seem that
    the model begins to explain data better, but this isn’t true— this value only
    increases if there are more training items.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个指标的唯一问题是添加新的独立变量可能会在某些情况下增加R-squared，因此考虑这些变量的质量和相关性以及避免过度拟合模型至关重要。这似乎表明模型开始更好地解释数据，但这并不正确——这个值只有在有更多训练项时才会增加。
- en: There are no out-of-the-box functions for calculating this metric in the `Flashlight`
    library; however, it is simple to implement it with linear algebra functions.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Flashlight`库中没有现成的函数来计算这个指标；然而，使用线性代数函数实现它很简单。
- en: There is the `r_squared` function in the `Dlib` library for computing the R-squared
    coefficient between matching elements of two `std::vector` instances.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Dlib`库中有一个用于计算两个`std::vector`实例匹配元素之间的R-squared系数的`r_squared`函数。
- en: The `m``lpack` library has the `R2Score` class, which has the static `Evaluate`
    function that runs prediction for the specified algorithm and calculates the R-squared
    error.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`m``lpack`库中的`R2Score`类有一个静态的`Evaluate`函数，该函数运行指定算法的预测并计算R-squared误差。'
- en: Adjusted R-squared
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调整后的R-squared
- en: 'The adjusted R-squared metric was designed to solve the previously described
    problem of the R-squared metric. It is the same as the R-squared metric but with
    a penalty for a large number of independent variables. The main idea is that if
    new independent variables improve the model’s quality, the values of this metric
    increase; otherwise, they decrease. This can be given by the following equation:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 调整后的R-squared指标旨在解决之前描述的R-squared指标的问题。它与R-squared指标相同，但为大量独立变量添加了惩罚。主要思想是，如果新的独立变量提高了模型的质量，这个指标的值会增加；否则，它们会减少。这可以由以下方程给出：
- en: '![](img/B19849_Formula_13.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_13.jpg)'
- en: Here, *k* is the number of parameters and *n* is the number of samples.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*k*是参数的数量，*n*是样本的数量。
- en: Classification metrics
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类指标
- en: 'Before we start discussing classification metrics, we have to introduce an
    important concept called the **confusion matrix**. Let’s assume that we have two
    classes and an algorithm that assigns them to an object. Here, the confusion matrix
    will look like this:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始讨论分类指标之前，我们必须介绍一个重要的概念，称为**混淆矩阵**。假设我们有两个类别和一个将这些类别分配给对象的算法。在这里，混淆矩阵将看起来像这样：
- en: '|  | ![](img/B19849_Formula_141.png) | ![](img/B19849_Formula_15.png) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](img/B19849_Formula_141.png) | ![](img/B19849_Formula_15.png) |'
- en: '| ![](img/B19849_Formula_161.png) | **True** **positive** (**TP**) | **False**
    **positive** (**FP**) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| ![](img/B19849_Formula_161.png) | **True** **positive** (**TP**) | **False**
    **positive** (**FP**) |'
- en: '| ![](img/B19849_Formula_17.png) | **False** **negative** (**FN**) | **True**
    **negative** (**TN**) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| ![](img/B19849_Formula_17.png) | **False** **negative** (**FN**) | **True**
    **negative** (**TN**) |'
- en: Here, ![](img/B19849_Formula_18.png) is the predicted class of the object and
    ![](img/B19849_Formula_19.png) is the ground truth label. The confusion matrix
    is an abstraction that we use to calculate different classification metrics. It
    gives us the number of items that were classified correctly and misclassified.
    It also provides us with information about the misclassification type. The false
    negatives are items that our algorithm incorrectly classified as negative ones,
    while the false positives are items that our algorithm incorrectly classified
    as positive ones. In this section, we’ll learn how to use this matrix and calculate
    different classification performance metrics.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B19849_Formula_18.png)是对象的预测类别，![](img/B19849_Formula_19.png)是真实标签。混淆矩阵是我们用来计算不同分类指标的抽象。它给出了正确分类和错误分类的项目数量。它还提供了关于错误分类类型的信息。假阴性是我们算法错误地将项目分类为负的项目，而假阳性是我们算法错误地将项目分类为正的项目。在本节中，我们将学习如何使用这个矩阵并计算不同的分类性能指标。
- en: A confusion matrix can be created in the `mlpack` library with the `ConfusionMatrix`
    function; this function works only for discrete data/categorical data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在`mlpack`库中可以使用`ConfusionMatrix`函数创建混淆矩阵；此函数仅适用于离散数据/分类数据。
- en: The `Dlib` library also has instruments to get a confusion matrix. There is
    the `test_multiclass_decision` function that tests a multi-class decision function
    and returns a confusion matrix describing the results. It also has the `test_sequence_labeler`
    class that tests the `labeler` object against the given samples and labels and
    returns a confusion matrix summarizing the results.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dlib`库也有工具来获取混淆矩阵。有`test_multiclass_decision`函数，它测试多类决策函数并返回一个描述结果的混淆矩阵。它还有一个`test_sequence_labeler`类，该类测试`labeler`对象与给定的样本和标签，并返回一个总结结果的混淆矩阵。'
- en: Accuracy
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准确率
- en: 'One of the most obvious classification metrics is accuracy:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最明显的分类指标之一是准确率：
- en: '![](img/B19849_Formula_20.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_20.jpg)'
- en: 'This provides us with a ratio of all positive predictions to all others. In
    general, this metric is not very useful because it doesn’t show us the real picture
    in terms of cases with an odd number of classes. Let’s consider a spam classification
    task and assume we have 10 spam letters and 100 non-spam letters. Our algorithm
    predicted 90 of them correctly as non-spam and classified only 5 spam letters
    correctly. In this case, accuracy will have the following value:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了所有正面预测与所有其他预测的比率。通常，这个指标并不很有用，因为它没有显示在类别数量为奇数的情况下的真实情况。让我们考虑一个垃圾邮件分类任务，并假设我们有10封垃圾邮件和100封非垃圾邮件。我们的算法正确地将90封预测为非垃圾邮件，并且只正确分类了5封垃圾邮件。在这种情况下，准确率将具有以下值：
- en: '![](img/B19849_Formula_21.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_21.jpg)'
- en: 'However, if the algorithm predicts all letters as non-spam, then its accuracy
    should be as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果算法预测所有字母都不是垃圾邮件，那么其准确率应该如下：
- en: '![](img/B19849_Formula_22.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_22.jpg)'
- en: This example shows that our model still doesn’t work because it is unable to
    predict all spam letters, but the accuracy value is good enough.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子表明，我们的模型仍然不起作用，因为它无法预测所有垃圾邮件，但准确率值已经足够好。
- en: To calculate accuracy in the `Flashlight` library, we use the `FrameErrorMeter`
    class that can estimate the accuracy or the error rate depending on user settings.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Flashlight`库中计算准确率时，我们使用`FrameErrorMeter`类，该类可以根据用户设置估计准确率或错误率。
- en: The `mlpack` library has the `Accuracy` class with the static `Evaluate` function
    that runs classification for the specified algorithm and calculates the accuracy
    value.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`mlpack`库有`Accuracy`类，其中包含静态的`Evaluate`函数，该函数运行指定算法的分类并计算准确率值。'
- en: Unfortunately, the `Dlib` library doesn’t have functions to calculate accuracy
    values, so if needed, the function should be implemented with the linear algebra
    backend.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，`Dlib`库没有计算准确率值的函数，因此如果需要，该函数应该使用线性代数后端实现。
- en: Precision and recall
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 精确度和召回率
- en: 'To estimate algorithm quality for each classification class, we will introduce
    two metrics: **precision** and **recall**. The following diagram shows all objects
    that are used in classification and how they have been marked according to the
    algorithm’s results:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了估计每个分类类的算法质量，我们将介绍两个指标：**精确度**和**召回率**。以下图表显示了在分类中使用的所有对象以及它们根据算法结果是如何被标记的：
- en: '![Figure 3.1 – Precision and recall](img/B19849_03_1.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图3.1 – 精确度和召回率](img/B19849_03_1.jpg)'
- en: Figure 3.1 – Precision and recall
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 – 精确度和召回率
- en: The circle in the center contains *selected elements*—the elements our algorithm
    predicted as positive ones.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 中心圆圈包含 *选定元素*——我们的算法预测为正的元素。
- en: 'Precision is proportional to the number of correctly classified items within
    selected ones that are defined as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 精度与所选项目中正确分类的项目数量成正比，所选项目如下定义：
- en: '![](img/B19849_Formula_231.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_231.jpg)'
- en: 'Recall is proportional to the number of correctly classified items within all
    ground truth positive items that are defined as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率与所有真实正样本中正确分类的项目数量成正比，定义如下：
- en: '![](img/B19849_Formula_241.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_241.jpg)'
- en: Another name for recall is **sensitivity**. Let’s assume that we are interested
    in the detection of positive items—let’s call them relevant ones. So, we use the
    recall value as a measure of an algorithm’s ability to detect relevant items and
    the precision value as a measure of an algorithm’s ability to see the differences
    between classes. These measures do not depend on the number of objects in each
    of the classes, and we can use them for imbalanced dataset classification.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率的另一个名称是 **灵敏度**。假设我们感兴趣的是检测正项目——让我们称它们为相关项目。因此，我们使用召回率作为衡量算法检测相关项目能力的指标，使用精度值作为衡量算法区分类别之间差异能力的指标。这些指标不依赖于每个类别中的对象数量，我们可以使用它们进行不平衡数据集分类。
- en: There are several approaches to calculate these metrics in the `Dlib` library.
    There is the `average_precision` function that can be used to calculate the precision
    value directly. Also, there is the `test_ranking_function` function that tests
    the given ranking function on the provided data and can return the mean average
    precision. Another way is to use the `test_sequence_segmenter` class that tests
    a `segmenter` object against the given samples and returns the precision, recall,
    and F1-score, where `sequence_segmenter` is an object for segmenting a sequence
    of objects into a set of non-overlapping chunks.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `Dlib` 库中，有几种计算这些指标的方法。有 `average_precision` 函数，可以直接用来计算精度值。还有 `test_ranking_function`
    函数，它测试给定的排名函数在提供的数据上的表现，并可以返回平均平均精度。另一种方法是使用 `test_sequence_segmenter` 类，它测试一个
    `segmenter` 对象与给定样本的匹配，并返回精度、召回率和 F1 分数，其中 `sequence_segmenter` 是一个将对象序列分割成一系列非重叠块的对象。
- en: The `mlpack` library has two classes—`Precision` and `Recall`—with static `Evaluate`
    functions that run classification for the specified algorithm and calculate precision
    and recall correspondingly.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`mlpack` 库有两个类——`Precision` 和 `Recall`——它们具有静态的 `Evaluate` 函数，这些函数运行指定算法的分类并相应地计算精度和召回率。'
- en: The `Flashlight` library doesn’t have the functionality to calculate these values.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`Flashlight` 库没有计算这些值的函数。'
- en: F-score
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F 分数
- en: 'In many cases, it is useful to have only one metric that shows the classification’s
    quality. For example, it makes sense to use some algorithms to search for the
    best hyperparameters, such as the grid search algorithm, which will be discussed
    later in this chapter. Such algorithms usually use one metric to compare different
    classification results after applying various parameter values during the search
    process. One of the most popular metrics for this case is the F-measure (or the
    F-score), which can be given as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，只使用一个指标来展示分类的质量是有用的。例如，使用一些算法来搜索最佳超参数是有意义的，比如后面章节中将要讨论的网格搜索算法。这类算法通常在搜索过程中应用各种参数值后，使用一个指标来比较不同的分类结果。在这种情况下，最流行的指标之一是
    F-measure（或 F 分数），它可以表示如下：
- en: '![](img/B19849_Formula_251.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_251.jpg)'
- en: Here, ![](img/B19849_Formula_26.png) is the precision metric weight. Usually,
    the ![](img/B19849_Formula_27.png) value is equal to *1*. In such a case, we have
    the multiplier value equal to *2*, which gives us ![](img/B19849_Formula_281.png)
    if the *precision* = *1* and the *recall* = *1*. In other cases, when the precision
    value or the recall value tends to be zero, the F-measure value will also decrease.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B19849_Formula_26.png) 是精度指标权重。通常，![](img/B19849_Formula_27.png)
    的值等于 *1*。在这种情况下，我们有一个乘数值等于 *2*，如果 *精度* = *1* 且 *召回率* = *1*，则给出 ![](img/B19849_Formula_281.png)。在其他情况下，当精度值或召回率值趋向于零时，F-measure
    值也会降低。
- en: The `Dlib` library provides the F1-score calculation only in the scope of the
    `test_sequence_segmenter` class functionality, which tests a `segmenter` object
    against the given samples and returns the precision, recall, and F1-score.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dlib` 库仅在 `test_sequence_segmenter` 类功能范围内提供 F1 分数的计算，该功能测试 `segmenter` 对象与给定样本，并返回精确度、召回率和
    F1 分数。'
- en: There is the `F1` class in the `mlpack` library that has the `Evaluate` function,
    which can be used to calculate the F1-score value by running a classification
    with the specified algorithm and data.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`mlpack` 库中有一个名为 `F1` 的类，它具有 `Evaluate` 函数，可以通过运行指定算法和数据进行的分类来计算 F1 分数值。'
- en: The `Flashlight` library doesn’t have the functionality to calculate F-score
    values.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`Flashlight` 库没有计算 F 分数值的功能。'
- en: AUC-ROC
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AUC-ROC
- en: Usually, a classification algorithm will not return a concrete class identifier
    but a probability of an object belonging to some class. So, we usually use a threshold
    to decide whether an object belongs to a class or not. The most apparent threshold
    is 0.5, but it can work incorrectly in the case of imbalanced data (when we have
    a lot of values for one class and significantly fewer for another class).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，分类算法不会返回具体的类标识符，而是返回一个对象属于某个类的概率。因此，我们通常使用一个阈值来决定一个对象是否属于某个类。最明显的阈值是 0.5，但在数据不平衡的情况下（当我们有一个类有很多值，而另一个类显著较少时），它可能工作不正确。
- en: 'One of the methods we can use to estimate a model without the actual threshold
    is the value of the **Area Under the Receiver Operating Characteristic Curve**
    (**AUC-ROC)**. This curve is a line from (0,0) to (1,1) in coordinates of the
    **True Positive Rate** (**TPR**) and the **False Positive** **Rate** (**FPR**):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用的一种方法来估计没有实际阈值的模型是 **接收者操作特征曲线下面积**（**AUC-ROC**）的值。这条曲线是 **真正例率**（**TPR**）和
    **假正例率**（**FPR**）坐标中的从 (0,0) 到 (1,1) 的线：
- en: '![](img/B19849_Formula_291.jpg)![](img/B19849_Formula_30.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_291.jpg)![](img/B19849_Formula_30.jpg)'
- en: 'The TPR value is equal to the recall, while the FPR value is proportional to
    the number of objects of the negative class that were classified incorrectly (they
    should be positive). In an ideal case, when there are no classification errors,
    we have `FPR = 0`, `TPR = 1`, and the area under the `1`. In the case of random
    predictions, the area under the ROC curve will be equal to `0.5` because we will
    have an equal number of TP and FP classifications:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: TPR 值等于召回率，而 FPR 值与被错误分类的负类对象的数量成比例（它们应该是正的）。在理想情况下，当没有分类错误时，我们有 `FPR = 0`，`TPR
    = 1`，并且 `1` 下的面积。在随机预测的情况下，ROC 曲线下方的面积将等于 `0.5`，因为我们会有相等数量的 TP 和 FP 分类：
- en: '![Figure 3.2 – ROC](img/B19849_03_2.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.2 – ROC](img/B19849_03_2.jpg)'
- en: Figure 3.2 – ROC
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 – ROC
- en: Each point on the curve corresponds to some threshold value. Notice that the
    curve’s steepness is an essential characteristic because we want to minimize the
    FPR, so we usually want this curve to tend to point (0,1). We can also successfully
    use the AUC-ROC metric with imbalanced datasets.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 曲线上的每个点都对应某个阈值值。请注意，曲线的陡峭程度是一个重要特征，因为我们希望最小化 FPR，所以我们通常希望这条曲线趋向于点 (0,1)。我们也可以在数据不平衡的数据集中成功使用
    AUC-ROC 指标。
- en: There is the `compute_roc_curve` function in the `Dlib` library that computes
    the ROC curve of the given data.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dlib` 库中有一个名为 `compute_roc_curve` 的函数，用于计算给定数据的 ROC 曲线。'
- en: Unfortunately, the `Flashlight` and `mlpack` libraries don’t have the functionality
    to compute the AUC-ROC metric.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，`Flashlight` 和 `mlpack` 库没有计算 AUC-ROC 指标的函数。
- en: Log-Loss
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Log-Loss
- en: 'The logistic loss function value (the Log-Loss) is used as a target loss function
    for optimization purposes. It is given by the following equation:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑损失函数值（Log-Loss）用作优化目标的目标损失函数。它由以下方程给出：
- en: '![](img/B19849_Formula_31.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_31.jpg)'
- en: We can understand the Log-Loss value as the accuracy being corrected but with
    penalties for incorrect predictions. This function gives significant penalties,
    even for single misclassified objects, so all outlier objects in the data should
    be processed separately or removed from the dataset.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将 Log-Loss 值理解为经过校正的准确度，但会对错误预测进行惩罚。这个函数对单个错误分类的对象也给予显著的惩罚，因此数据中的所有异常对象都应该单独处理或从数据集中删除。
- en: There is the `loss_binary_log` class in the `Dlib` library that implements the
    Log-Loss, which is appropriate for binary classification problems. This class
    is designed to be used as a **neural network** (**NN**) module.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dlib` 库中有一个 `loss_binary_log` 类实现了 Log-Loss，这对于二元分类问题来说是合适的。这个类被设计成用作**神经网络**（**NN**）模块。'
- en: The `Flashlight` library has the `BinaryCrossEntropy` class that computes the
    binary cross-entropy loss between an input tensor `x` and a target tensor `y`.
    Also, the main aim of this class is the loss function implementation for NN training.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`Flashlight` 库中的 `BinaryCrossEntropy` 类可以计算输入张量 `x` 和目标张量 `y` 之间的二进制交叉熵损失。此外，这个类的主要目的是实现神经网络训练中的损失函数。'
- en: The `CrossEntropyError` class in the `mlpack` library also represents a loss
    function for NN construction; it has forward and backward functions. So, it is
    used to measure the network’s performance according to the cross-entropy between
    the input and target distributions.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`mlpack` 库中的 `CrossEntropyError` 类也代表了神经网络构建中的损失函数；它具有前向和后向函数。因此，它被用来根据输入和目标分布之间的交叉熵来衡量网络的表现。'
- en: In the current section, we learned about performance estimation metrics that
    can give you a clearer picture of your model accuracy, precision, and other performance
    characteristics. In the following section, we will learn about bias and variance
    and how to estimate and fix model prediction characteristics.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了性能估计指标，这些指标可以为你提供关于模型准确度、精确度以及其他性能特征的更清晰的认识。在下一节中，我们将学习偏差和方差以及如何估计和修复模型预测特征。
- en: Understanding bias and variance characteristics
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解偏差和方差特征
- en: Bias and variance characteristics are used to predict model behavior. For example,
    the high variance effect, also known as **overfitting**, is a phenomenon in ML
    where the constructed model explains the examples from the training set but works
    relatively poorly on the examples that did not participate in the training process.
    This occurs because while training a model, random patterns will start appearing
    that are normally absent from the general population. The opposite of overfitting
    is known as **underfitting**, which corresponds to the high bias effect. This
    happens when the trained model becomes unable to predict patterns in new data
    or even in the training data. Such an effect can be the result of a limited training
    dataset or weak model design.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差和方差特征用于预测模型行为。例如，高方差效应，也称为**过拟合**，是机器学习中的一种现象，即构建的模型解释了训练集中的示例，但在未参与训练过程的示例上表现相对较差。这是因为当训练模型时，随机模式开始出现，而这些模式通常在一般人群中是缺失的。与过拟合相反的是**欠拟合**，它对应于高偏差效应。这发生在训练模型变得无法预测新数据或训练数据中的模式时。这种效应可能是有限的训练数据集或弱模型设计的后果。
- en: Before we go any further and describe what they mean, we should consider **validation**.
    Validation is a technique that’s used to test model performance. It estimates
    how well the model makes predictions on new data. New data is data that we did
    not use for the training process. To perform validation, we usually divide our
    initial dataset into two or three parts. One part should contain most of the data
    and will be used for training, while the other ones will be used to validate and
    test the model. Usually, validation is performed for iterative algorithms after
    one training cycle (often called an **epoch**). Alternatively, we perform testing
    after the overall training process.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进一步描述它们的意义之前，我们应该考虑**验证**。验证是一种用于测试模型性能的技术。它估计模型在新数据上做出预测的好坏。新数据是我们没有用于训练过程的数据。为了进行验证，我们通常将我们的初始数据集分成两到三个部分。其中一部分应该包含大部分数据，并将用于训练，而其他部分将用于验证和测试模型。通常，验证是在一个训练周期（通常称为**epoch**）之后对迭代算法进行的。或者，我们在整体训练过程之后进行测试。
- en: 'Validation and testing operations evaluate the model on the data we have excluded
    from the training process, which results in the values of the performance metrics
    that we chose for this particular model. For example, the original dataset can
    be divided into the following parts: 80% for training, 10% for validation, and
    10% for testing. The values of these validation metrics can be used to estimate
    model and prediction error trends. The most crucial issue for validation and testing
    is that the data for them should always be from the same distribution as the training
    data.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 验证和测试操作评估的是我们从训练过程中排除的数据上的模型，这导致了我们为这个特定模型选择的性能指标值。例如，原始数据集可以分成以下几部分：80%用于训练，10%用于验证，10%用于测试。这些验证指标值可以用来估计模型和预测误差趋势。验证和测试最关键的问题是，它们的数据应该始终来自与训练数据相同的分布。
- en: Throughout the rest of this chapter, we will use the polynomial regression model
    to show different prediction behaviors. The polynomial degree will be used as
    a hyperparameter.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的剩余部分，我们将使用多项式回归模型来展示不同的预测行为。多项式度数将用作超参数。
- en: Bias
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 偏差
- en: 'Bias is a prediction characteristic that tells us about the distance between
    model predictions and ground truth values. Usually, we use the term *high bias*
    or *underfitting* to say that model prediction is too far from the ground truth
    values, which means that the model generalization ability is weak. Consider the
    following graph:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差是预测特征，它告诉我们模型预测值与真实值之间的距离。通常，我们使用术语*高偏差*或*欠拟合*来说明模型预测值与真实值相差太远，这意味着模型的泛化能力较弱。考虑以下图表：
- en: '![Figure 3.3 – Regression model predictions with the polynomial degree equal
    to 1](img/B19849_03_3.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.3 – 多项式度数为 1 的回归模型预测](img/B19849_03_3.jpg)'
- en: Figure 3.3 – Regression model predictions with the polynomial degree equal to
    1
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3 – 多项式度数为 1 的回归模型预测
- en: This graph shows the original values, the values used for validation, and a
    line that represents the polynomial regression model output. In this case, the
    polynomial degree is equal to *1*. We can see that the predicted values do not
    describe the original data at all, so we can say that this model has a high bias.
    Also, we can plot validation metrics for each training cycle to get more information
    about the training process and the model’s behavior.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 此图表显示了原始值、用于验证的值以及代表多项式回归模型输出的线条。在这种情况下，多项式度数等于 *1*。我们可以看到，预测值根本无法描述原始数据，因此我们可以说这个模型具有高偏差。此外，我们可以绘制每个训练周期的验证指标，以获取更多关于训练过程和模型行为的详细信息。
- en: 'The following graph shows the MAE metric values for the training process of
    the polynomial regression model, where the polynomial degree is equal to *1*:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了多项式回归模型训练过程的 MAE 指标值，其中多项式度数等于 *1*：
- en: '![Figure 3.4 – Train and validation loss values](img/B19849_03_4.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.4 – 训练和验证损失值](img/B19849_03_4.jpg)'
- en: Figure 3.4 – Train and validation loss values
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 – 训练和验证损失值
- en: We can see that the lines for the metric values for the train and validation
    data are parallel and distant enough. Moreover, these lines do not change their
    direction after numerous training iterations. These facts also tell us that the
    model has a high bias because, for a regular training process, validation metric
    values should be close to the training values.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，训练数据和验证数据的指标值线条是平行且足够远的。此外，这些线条在多次训练迭代后不会改变方向。这些事实也告诉我们，模型具有高偏差，因为在常规训练过程中，验证指标值应该接近训练值。
- en: To deal with high bias, we can add more features to the training samples. For
    example, increasing the polynomial degree for the polynomial regression model
    adds more features; these all-new features describe the original training sample
    because each additional polynomial term is based on the original sample value.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理高偏差，我们可以在训练样本中添加更多特征。例如，对于多项式回归模型，增加多项式度数会添加更多特征；这些全新的特征描述了原始训练样本，因为每个额外的多项式项都是基于原始样本值。
- en: Variance
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方差
- en: Variance is a prediction characteristic that tells us about the variability
    of model predictions; in other words, how big the range of output values can be.
    Usually, we use the term *high variance* or *overfitting* in the case when a model
    tries to incorporate many training samples very precisely. In such a case, the
    model cannot provide a good approximation for new data but has excellent performance
    on the training data.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 方差是预测特征，它告诉我们模型预测的变异性；换句话说，输出值范围可以有多大。通常，当模型试图非常精确地包含许多训练样本时，我们使用术语*高方差*或*过拟合*。在这种情况下，模型不能为新数据提供良好的近似，但在训练数据上表现优异。
- en: 'The following graph shows the behavior of the polynomial regression model,
    with the polynomial degree equal to `15`:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了多项式回归模型的行为，多项式次数等于`15`：
- en: '![Figure 3.5 – Regression model predictions with the polynomial degree equal
    to 15](img/B19849_03_5.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图3.5 – 多项式次数等于15的回归模型预测](img/B19849_03_5.jpg)'
- en: Figure 3.5 – Regression model predictions with the polynomial degree equal to
    15
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5 – 多项式次数等于15的回归模型预测
- en: 'We can see that the model incorporates almost all the training data. Notice
    that the training data is indicated as `orig` in the plot’s legend, while the
    data used for validation is indicated as `val` in the plot’s legend. We can see
    that these two sets of data—training data and validation data—are somehow distant
    from each other and that our model misses the validation data because of a lack
    of approximation. The following graph shows the MAE values for the learning process:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，模型几乎包含了所有训练数据。注意，在图表的图例中，训练数据被标记为`orig`，而用于验证的数据被标记为`val`。我们可以看到，这两组数据——训练数据和验证数据——在某种程度上是分离的，并且我们的模型由于缺乏近似而错过了验证数据。以下图表显示了学习过程的MAE值：
- en: '![Figure 3.6 – Validation error](img/B19849_03_6.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图3.6 – 验证误差](img/B19849_03_6.jpg)'
- en: Figure 3.6 – Validation error
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6 – 验证误差
- en: We can see that after approximately 75 learning iterations, the model began
    to predict training data much better, and the error value became lower. However,
    for the validation data, the MAE values began to increase. To deal with high variance,
    we can use special regularization techniques, which we will discuss in the following
    sections. We can also increase the number of training samples and decrease the
    number of features in one sample to reduce high variance.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在大约75次学习迭代后，模型开始更好地预测训练数据，错误值降低。然而，对于验证数据，MAE值开始上升。为了处理高方差，我们可以使用特殊的正则化技术，我们将在以下章节中讨论。我们还可以增加训练样本的数量并减少一个样本中的特征数量，以降低高方差。
- en: The performance metrics plots we discussed in the preceding paragraphs can be
    drawn at the runtime of the training process. We can use them to monitor the training
    process to see high bias or high variance problems. Notice that for the polynomial
    regression model, MAE is a better performance characteristic than MSE or RMSE
    because squared functions average errors too much. Moreover, even a straight-line
    model can have low MSE values for such data because errors from both sides of
    the line compensate for each other. The choice between MAE and MSE depends on
    the specific task and goals of the project. If the overall accuracy of predictions
    is important, then MAE may be preferable. But MAE gives equal weight to all errors,
    regardless of their magnitude. This means that it is not sensitive to outliers,
    which can significantly affect the overall error. So, if it is necessary to minimize
    large errors, then MSE can give more accurate results. In some cases, you can
    use both metrics to analyze the performance of the model in more depth.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前几段中讨论的性能指标图可以在训练过程的运行时绘制。我们可以使用它们来监控训练过程，以查看高偏差或高方差问题。请注意，对于多项式回归模型，MAE比MSE或RMSE是更好的性能特征，因为平方函数过度平均了误差。此外，即使是直线模型，对于此类数据也可以有低MSE值，因为线两边的误差相互补偿。MAE和MSE之间的选择取决于特定任务和项目的目标。如果预测的整体准确性很重要，那么MAE可能更可取。但MAE对所有误差给予相同的权重，无论其大小。这意味着它对异常值不敏感，异常值可能会显著影响整体误差。因此，如果需要最小化大误差，那么MSE可以给出更准确的结果。在某些情况下，你可以使用这两个指标来更深入地分析模型的性能。
- en: Normal training
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正常训练
- en: 'Consider the case of a training process where the model has balanced bias and
    variance:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这样一个训练过程，其中模型具有平衡的偏差和方差：
- en: '![Figure 3.7 – Predictions when a model was trained ideally](img/B19849_03_7.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图3.7 – 模型理想训练时的预测](img/B19849_03_7.jpg)'
- en: Figure 3.7 – Predictions when a model was trained ideally
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7 – 模型理想训练时的预测值
- en: 'In this graph, we can see that the polynomial regression model’s output for
    the polynomial degree is equal to eight. The output values are close to both the
    training data and validation data. The following graph shows the MAE values during
    the training process:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图中，我们可以看到多项式回归模型的多项式次数为八。输出值接近训练数据和验证数据。以下图表显示了训练过程中的MAE值：
- en: '![Figure 3.8 – Loss values when a model was trained ideally](img/B19849_03_8.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图3.8 – 模型理想训练时的损失值](img/B19849_03_8.jpg)'
- en: Figure 3.8 – Loss values when a model was trained ideally
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8 – 模型理想训练时的损失值
- en: We can see that the MAE value decreases consistently and that the predicted
    values for the training and validation data become close to the ground truth values.
    This means that the model’s hyperparameters were good enough to balance bias and
    variance.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到MAE值持续下降，并且训练数据和验证数据的预测值接近真实值。这意味着模型的超参数足够好，可以平衡偏差和方差。
- en: Regularization
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化
- en: Regularization is a technique that’s used to reduce model overfitting. There
    are two main approaches to regularization. The first one is known as training
    data preprocessing. The second one is loss function modification. The main idea
    of the loss function modification technique is to add terms to the loss function
    that penalize algorithm results, thereby leading to significant variance. The
    idea of training data preprocessing techniques is to add more distinct training
    samples. Usually, in such an approach, new training samples are generated by augmenting
    existing ones. In general, both approaches add some prior knowledge about the
    task domain to the model. This additional information helps us with variance regularization.
    Therefore, we can conclude that regularization is any technique that leads to
    minimizing generalization errors.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化是一种用于减少模型过拟合的技术。正则化主要有两种方法。第一种被称为训练数据预处理。第二种是损失函数修改。损失函数修改技术的主要思想是在损失函数中添加惩罚算法结果的项，从而引起显著的方差。训练数据预处理技术的思想是添加更多独特的训练样本。通常，在这种方法中，通过增强现有样本来生成新的训练样本。总的来说，这两种方法都将关于任务域的一些先验知识添加到模型中。这些附加信息有助于我们进行方差正则化。因此，我们可以得出结论，正则化是任何导致最小化泛化误差的技术。
- en: L1 regularization – Lasso
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: L1正则化 – Lasso
- en: 'L1 regularization is an additional term to the loss function:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: L1正则化是损失函数的一个附加项：
- en: '![](img/B19849_Formula_32.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_32.jpg)'
- en: This additional term adds the absolute value of the magnitude of parameters
    as a penalty. Here, *λ* is a regularization coefficient. Higher values of this
    coefficient lead to stronger regularization and can lead to underfitting. Sometimes,
    this type of regularization is called **Least Absolute Shrinkage and Selection
    Operator** (**Lasso**) regularization. The general idea behind L1 regularization
    is to penalize less important features. We can think of it as a feature selection
    process because as the optimization proceeds, some of the coefficients (for example,
    in linear regression) become zero, indicating that those features are not contributing
    to the model’s performance. We end up with a sparse model with fewer features.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这个附加项将参数大小的绝对值作为惩罚。在这里，*λ* 是正则化系数。这个系数的值越高，正则化越强，可能会导致欠拟合。有时，这种正则化被称为**最小绝对收缩和选择算子**（**Lasso**）正则化。L1正则化的基本思想是惩罚不那么重要的特征。我们可以将其视为一个特征选择过程，因为在优化过程中，一些系数（例如，在线性回归中）变为零，这表明这些特征没有对模型的性能做出贡献。我们最终得到一个具有较少特征的稀疏模型。
- en: L2 regularization – Ridge
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: L2正则化 – Ridge
- en: 'L2 regularization is also an additional term to the loss function:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: L2正则化是损失函数的一个附加项：
- en: '![](img/B19849_Formula_331.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_331.jpg)'
- en: This additional term adds a squared value of the magnitude of parameters as
    a penalty. This penalty shrinks the magnitude of the parameters toward zero. *λ*
    is also a coefficient of regularization. Its higher values lead to stronger regularization
    and can lead to underfitting because the model becomes too constrained and unable
    to learn complex relationships in the data. Another name for this regularization
    type is **ridge regularization**. Unlike L1 regularization, this type does not
    have a feature selection characteristic. Instead, we can interpret it as a model
    smoothness configurator. In addition, L2 regularization is computationally more
    efficient for gradient descent-based optimizers because its differentiation has
    an analytical solution.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这个附加项将参数幅度的平方值作为惩罚。这种惩罚将参数幅度缩小到零。*λ*也是正则化的系数。其较高值会导致更强的正则化，并可能导致欠拟合，因为模型变得过于约束，无法学习数据中的复杂关系。这种正则化类型的另一个名称是**岭回归**。与L1正则化不同，这种类型没有特征选择特性。相反，我们可以将其解释为模型平滑度配置器。此外，对于基于梯度的优化器，L2正则化在计算上更有效，因为其微分有解析解。
- en: In the `Dlib` library, regularization mechanisms are usually integrated into
    algorithm implementations —for example, the `rr_trainer` class that represents
    a tool for performing linear ridge regression, which is the regularized **least
    squares support vector** **machine** (**LSSVM**).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Dlib`库中，正则化机制通常集成到算法实现中——例如，`rr_trainer`类代表执行线性岭回归的工具，这是一种正则化的**最小二乘支持向量机**（**LSSVM**）。
- en: There is the `LRegularizer` class in the `mlpack` library that implements a
    generalized L-regularizer, allowing both L1 and L2 regularization methods for
    NNs. Some algorithm implementations, such as the `LARS` class can train a LARS/LASSO/Elastic
    Net model and has L1 and L2 regularization parameters. The `LinearRegression`
    class has a regularization parameter for ridge regression.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在`mlpack`库中有一个`LRegularizer`类，它实现了一种广义L-正则化器，允许NN使用L1和L2正则化方法。一些算法实现，如`LARS`类，可以训练LARS/LASSO/Elastic
    Net模型，并具有L1和L2正则化参数。`LinearRegression`类有一个岭回归的正则化参数。
- en: There is no standalone functionality in the `Flashlight` library for regularization.
    All regularizations are integrated into the NN optimization algorithm implementations.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Flashlight`库中没有独立的正则化功能。所有正则化都集成到NN优化算法实现中。
- en: Data augmentation
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据增强
- en: 'The data augmentation process can be treated as regularization because it adds
    some prior knowledge about the problem to the model. This approach is common in
    **computer vision** (**CV**) tasks such as image classification or object detection.
    In such cases, when we can see that the model begins to overfit and does not have
    enough training data, we can augment the images we already have to increase the
    size of our dataset and provide more distinct training samples. Image augmentations
    are random image rotations, cropping and translations, mirroring flips, scaling,
    and proportion changes. But data augmentation should be carefully designed for
    the following reasons:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强过程可以被视为正则化，因为它向模型添加了一些关于问题的先验知识。这种方法在**计算机视觉**（**CV**）任务中很常见，例如图像分类或目标检测。在这种情况下，当我们看到模型开始过拟合且没有足够的训练数据时，我们可以增强我们已有的图像，以增加数据集的大小并提供更多独特的训练样本。图像增强包括随机图像旋转、裁剪和转换、镜像翻转、缩放和比例变化。但是，数据增强应该谨慎设计，以下是一些原因：
- en: If the generated data is too similar to the original data, it can lead to overfitting.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果生成数据与原始数据过于相似，可能会导致过拟合。
- en: It can introduce noise or artifacts into the dataset, which can degrade the
    quality of the resulting models.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可能会向数据集引入噪声或伪影，这可能会降低结果的模型质量。
- en: The augmented data may not accurately reflect the real-world distribution of
    data, leading to a domain shift between the training and test sets. This can result
    in poor generalization performance.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增强的数据可能无法准确反映真实世界数据的分布，导致训练集和测试集之间的领域偏移。这可能导致泛化性能不佳。
- en: Early stopping
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提前停止
- en: Stopping the training process early can also be interpreted as a form of regularization.
    This means that if we detected that the model started to overfit, we can stop
    the training process. In this case, the model will have parameters once the training
    has stopped.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 提前停止训练过程也可以被解释为一种正则化形式。这意味着如果我们检测到模型开始过拟合，我们可以停止训练过程。在这种情况下，一旦训练停止，模型将具有参数。
- en: Regularization for NNs
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NN的正则化
- en: L1 and L2 regularizations are widely used to train NNs and are usually called
    **weight decay**. Data augmentation also plays an essential role in the training
    processes for NNs. Other regularization methods can be used in NNs. For example,
    Dropout is a particular type of regularization that was developed especially for
    NNs. This algorithm randomly drops some NN nodes; it makes other nodes more insensitive
    to the weights of other nodes, which means the model becomes more robust and stops
    overfitting.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: L1 和 L2 正则化在训练神经网络（NNs）时被广泛使用，通常被称为**权重衰减**。数据增强也在神经网络训练过程中发挥着至关重要的作用。在神经网络中还可以使用其他正则化方法。例如，Dropout
    是一种特别为神经网络开发的正则化方法。此算法随机丢弃一些神经网络节点；它使得其他节点对其他节点的权重更加不敏感，这意味着模型变得更加鲁棒，并停止过拟合。
- en: In the following sections, we will see how to select model hyperparameters with
    automated algorithms versus manual tuning.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将看到如何使用自动化算法与手动调整来选择模型超参数。
- en: Model selection with the grid search technique
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用网格搜索技术进行模型选择
- en: It is necessary to have a set of proper hyperparameter values to create a good
    ML model. The reason for this is that having random values leads to controversial
    results and behaviors that are not expected by the practitioner. There are several
    approaches we can follow to choose the best set of hyperparameter values. We can
    try to use hyperparameters from algorithms we have already trained that are similar
    to our task. We can also try to find some heuristics and tune them manually. However,
    this task can be automated. The grid search technique is an automated approach
    for searching for the best hyperparameter values. It uses the cross-validation
    technique for model performance estimation.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 需要有一组合适的超参数值来创建一个好的机器学习（ML）模型。这是因为随机值会导致有争议的结果和从业者未预期的行为。我们可以遵循几种方法来选择最佳的超参数值集。我们可以尝试使用与我们任务相似的已训练算法的超参数。我们还可以尝试找到一些启发式方法并手动调整它们。然而，这项任务可以自动化。网格搜索技术是一种自动化的方法，用于搜索最佳超参数值。它使用交叉验证技术来估计模型性能。
- en: Cross-validation
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉验证
- en: 'We have already discussed what the validation process is. It is used to estimate
    the model’s performance data that we haven’t used for training. If we have a limited
    or small training dataset, randomly sampling the validation data from the original
    dataset leads to the following problems:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了验证过程是什么。它用于估计我们尚未用于训练的模型性能数据。如果我们有一个有限的或小的训练数据集，从原始数据集中随机采样验证数据会导致以下问题：
- en: The size of the original dataset is reduced
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始数据集的大小减小
- en: There is the probability of leaving data that’s important for validation in
    the training part
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有可能将重要的验证数据留在训练部分
- en: To solve these problems, we can use the cross-validation approach. The main
    idea behind it is to split the original dataset in such a way that all the data
    will be used for training and validation. Then, the training and validation processes
    are performed for all partitions, and the resulting values are averaged.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，我们可以使用交叉验证方法。其背后的主要思想是以一种方式将原始数据集分割，使得所有数据都将用于训练和验证。然后，对所有分区执行训练和验证过程，并对结果进行平均。
- en: 'The most well-known method of cross-validation is *K*-fold cross-validation,
    where K refers to the number of folds or partitions used to split the dataset
    The idea is to divide the dataset into *K* blocks of the same size. Then, we use
    one of the blocks for validation and the others for training. We repeat this process
    *K* times, each time choosing a different block for validation, and in the end,
    we average all the results. The data splitting scheme during the whole cross-validation
    cycle looks like this:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 最著名的交叉验证方法是 *K*-折交叉验证，其中 K 指的是用于分割数据集的折数或分区数。其思想是将数据集划分为相同大小的 *K* 个块。然后，我们使用其中一个块进行验证，其余的用于训练。我们重复这个过程
    *K* 次，每次选择不同的块进行验证，最后平均所有结果。在整个交叉验证周期中的数据分割方案如下：
- en: Divide the dataset into *K* blocks of the same size.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集划分为相同大小的 *K* 个块。
- en: Select one of the blocks for validation and the remaining *K*-1 blocks for training.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个块进行验证，其余 *K*-1 个块用于训练。
- en: Repeat this process, making sure that each block is used for validation and
    the rest are used for training.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复此过程，确保每个块都用于验证，其余的用于训练。
- en: Average the results of the performance metrics that were calculated for the
    validation sets on each iteration.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个迭代中计算的验证集性能指标的结果进行平均。
- en: 'The following diagram shows the cross-validation cycle:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了交叉验证周期：
- en: '![Figure 3.9 – K-fold validation scheme](img/B19849_03_9.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.9 – K 折验证方案](img/B19849_03_9.jpg)'
- en: Figure 3.9 – K-fold validation scheme
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9 – K 折验证方案
- en: Grid search
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网格搜索
- en: The main idea behind the grid search approach is to create a grid of the most
    reasonable hyperparameter values. The grid is used to generate a reasonable number
    of distinct parameter sets quickly. We should have some prior knowledge about
    the task domain to initialize the minimum and maximum values for grid generation,
    or we can initialize the grid with some reasonable broad ranges. However, if the
    chosen ranges are too broad, the process of searching for parameters can take
    a long time and will require a significant amount of computational resources.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索方法背后的主要思想是创建一个包含最合理超参数值的网格。该网格用于快速生成一定数量的不同参数集。我们应该对任务领域有一些先验知识，以初始化网格生成的最小和最大值，或者我们可以使用一些合理的广泛范围初始化网格。然而，如果选择的范围太广，搜索参数的过程可能需要很长时间，并且需要大量的计算资源。
- en: At each step, the grid search algorithm chooses a set of hyperparameter values
    and trains a model. After that, the training step algorithm uses the K-fold cross-validation
    technique to estimate model performance. We should also define a single model
    performance estimation metric for model comparison that the algorithm will calculate
    at each training step for every model. After completing the model training process
    with each set of parameters from every grid cell, the algorithm chooses the best
    set of hyperparameter values by comparing the metric’s values and selecting the
    best one. Usually, the set with the smallest value is the best one.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个步骤中，网格搜索算法选择一组超参数值并训练一个模型。之后，训练步骤算法使用 K 折交叉验证技术来估计模型性能。我们还应该定义一个用于模型比较的单个模型性能估计指标，算法将在每个训练步骤为每个模型计算该指标。在完成每个网格单元中每一组参数的模型训练过程后，算法通过比较指标值并选择最佳值来选择最佳的超参数值集。通常，具有最小值的集合是最好的。
- en: Consider an implementation of this algorithm in different libraries. Our task
    is to select the best set of hyperparameters for the polynomial regression model,
    which gives us the best curve that fits the given data. The data in this example
    is some cosine function values with some random noise.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑在不同库中实现此算法。我们的任务是选择多项式回归模型的最佳超参数集，以获得最佳曲线，该曲线适合给定的数据。本例中的数据是一些带有随机噪声的余弦函数值。
- en: mlpack example
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: mlpack 示例
- en: 'The `mlpack` library contains a special `HyperParameterTuner` class to do hyperparameter
    searches with different algorithms in both discrete and continuous spaces. The
    default search algorithm is grid search. This class is a template and should be
    specialized for a concrete task. The general definition is the following:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '`mlpack` 库包含一个特殊的 `HyperParameterTuner` 类，用于在离散和连续空间中使用不同算法进行超参数搜索。默认搜索算法是网格搜索。此类是一个模板，应该针对具体任务进行特殊化。一般定义如下：'
- en: '[PRE0]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can see that the main template parameters are the algorithm that we want
    to find, hyperparameters for the performance metric, and the cross-validation
    algorithm. Let’s define a `HyperParameterTuner` object to search for the best
    regularization value for the linear ridge regression algorithm. The definition
    will be the following:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，主要模板参数是我们想要找到的算法、性能度量的超参数以及交叉验证算法。让我们定义一个 `HyperParameterTuner` 对象来搜索线性岭回归算法的最佳正则化值。定义如下：
- en: '[PRE1]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, `LinearRegression` is the target algorithm class, `MSE` is the performance
    metric class that calculates the MSE, and `SimpleCV` is the class that implements
    cross-validation. It splits data into two sets, training and validation, and then
    runs training on the training set and evaluates performance on the validation
    set. Also, we see that we pass the `validation_size` parameter into the constructor.
    It has a value of 0.2, which means usage of 80% of data for training and the remaining
    20% for evaluation with MSE. The two following constructor parameters are our
    training dataset; `samples` are just samples, and `labels` are corresponding labels.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`LinearRegression` 是目标算法类，`MSE` 是计算 MSE 的性能度量类，而 `SimpleCV` 是实现交叉验证的类。它将数据分为两个集合，训练集和验证集，然后在训练集上运行训练，并在验证集上评估性能。此外，我们注意到我们将
    `validation_size` 参数传递给构造函数。它的值为 0.2，这意味着使用 80% 的数据进行训练，剩余的 20% 用于使用 MSE 进行评估。接下来的两个构造函数参数是我们的训练数据集；`samples`
    只是样本，而 `labels` 是相应的标签。
- en: 'Let’s see how we can generate a training dataset for these examples. It will
    take the two following steps:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何为这些示例生成训练数据集。它将包括以下两个步骤：
- en: Generating data that follows some predefined pattern—for example, 2D normally
    distributed points, plus some noise
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成遵循某些预定义模式的数据——例如，二维正态分布的点，加上一些噪声
- en: Data normalization
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据归一化
- en: 'The following sample shows how to generate data using the Armadillo library,
    which is the `mlpack` mathematical backend:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了如何使用 Armadillo 库生成数据，它是 `mlpack` 数学后端：
- en: '[PRE2]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Notice that for samples, we used the `arma::mat` type, and for labels, the `arma::rowvec`
    type. So, samples are placed into the matrix entity and labels into a one-dimensional
    vector correspondingly. Also, we used the `arma::randn` function to generate normally
    distributed data and noise.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于样本，我们使用了 `arma::mat` 类型，对于标签，我们使用了 `arma::rowvec` 类型。因此，样本被放置在矩阵实体中，标签则相应地放入一维向量中。此外，我们还使用了
    `arma::randn` 函数来生成正态分布的数据和噪声。
- en: 'Now, we can normalize data in the following way:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以按以下方式对数据进行归一化：
- en: '[PRE3]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We used the object of the `StandardScaler` class from the `mlpack` library to
    perform normalization. This object should be first trained on some data with the
    `Fit` method to learn mean and variance, and then it can be applied to other data
    with the `Transform` method.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了来自 `mlpack` 库的 `StandardScaler` 类的对象来进行归一化。这个对象应该首先使用 `Fit` 方法在数据上训练以学习均值和方差，然后它可以使用
    `Transform` 方法应用于其他数据。
- en: 'Now, let’s discuss how to prepare the data and how to define the hyperparameter
    tuner object. So, we are ready to launch the grid search for the best regularization
    values; the following sample shows how to do it:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论如何准备数据和如何定义超参数调优对象。这样，我们就准备好启动网格搜索以找到最佳的规范化值；以下示例展示了如何进行：
- en: '[PRE4]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We defined a `lambdas` vector with our search space and then called the `Optimize`
    method of the hyperparameter tuner object. You can see that the return value is
    a tuple, and we use the `std::tie` function to extract the specific value. The
    `Optimize` method takes a variable number of arguments depending on the ML algorithm
    we use in the search, and each argument will define a search space for each hyperparameter
    used in the algorithm. The constructor of the `LinearRegression` class has only
    one `lambda` parameter. After the search is finished, we use the best-searched
    parameter directly, or we can get the best-optimized model object, as shown in
    the following code snippet:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个包含搜索空间的 `lambdas` 向量，然后调用了超参数调优对象的 `Optimize` 方法。你可以看到返回值是一个元组，我们使用 `std::tie`
    函数来提取特定值。`Optimize` 方法根据我们在搜索中使用的机器学习算法的变量数量接受参数，每个参数将为算法中使用的每个超参数定义一个搜索空间。`LinearRegression`
    类的构造函数只有一个 `lambda` 参数。搜索完成后，我们可以直接使用搜索到的最佳参数，或者我们可以获取最佳优化的模型对象，如下面的代码片段所示：
- en: '[PRE5]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can now try the new model on new data to see how it works. At first, we
    will generate data and normalize it with a pre-trained scaler object, as shown
    in the following sample:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以尝试在新数据上使用新的模型来查看其效果。首先，我们将生成数据并使用预训练的缩放器对象对其进行归一化，如下面的示例所示：
- en: '[PRE6]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here, we used the `arma::linspace` function to get a linearly distributed range
    of data. This function produces a vector; we wrote additional code that transforms
    this vector into a matrix object. Then, we used the already trained `sample_scaler`
    object to normalize the data.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了 `arma::linspace` 函数来获取线性分布的数据范围。这个函数产生一个向量；我们编写了额外的代码将这个向量转换成矩阵对象。然后，我们使用了已经训练好的
    `sample_scaler` 对象来归一化数据。
- en: 'The following sample shows how to use the model with the best parameter we
    found with the grid search:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了如何使用网格搜索找到的最佳参数来使用模型：
- en: '[PRE7]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The one important thing you have to notice is that the ML algorithm used for
    a grid search should be supported by `SimpleCV` or other validation classes. If
    it doesn’t have a default implementation, you will need to provide it yourself.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须注意的一个重要事项是，用于网格搜索的机器学习算法应该由 `SimpleCV` 或其他验证类支持。如果没有默认实现，你需要自己提供。
- en: Optuna with Flashlight example
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Optuna 与 Flashlight 示例
- en: There is no support for any hyperparameter tuning algorithms in the Flashlight
    library, but we can use an external tool named Optuna to deal with ML programs
    that we want to search the best hyperparameters for. The main idea is to use some
    **inter-process communication** (**IPC**) approach to run training with different
    parameters and get some performance metric values after training.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: Flashlight 库中不支持任何超参数调整算法，但我们可以使用一个名为 Optuna 的外部工具来处理我们想要搜索最佳超参数的机器学习程序。主要思想是使用某种
    **进程间通信**（**IPC**）方法以不同的参数运行训练，并在训练后获取一些性能指标值。
- en: Optuna is a hyperparameter tuning framework designed to be used with different
    ML libraries and programs. It is implemented with the Python programming language,
    so the main area of its application is tools that have some Python APIs. But Optuna
    also has a **command-line interface** (**CLI**) that can be used with tools that
    don’t support Python. Another way to use such tools is to call them from Python,
    passing their command-line parameters and reading their standard output. This
    book will show an example of such type of Optuna usage, because writing a Python
    program is more useful than creating Bash scripts for the same automatization
    tasks, from the author’s point of view.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Optuna 是一个超参数调整框架，旨在与不同的机器学习库和程序一起使用。它使用 Python 编程语言实现，因此其主要应用领域是具有一些 Python
    API 的工具。但 Optuna 还有一个 **命令行界面**（**CLI**），可以与不支持 Python 的工具一起使用。另一种使用此类工具的方法是从
    Python 中调用它们，传递它们的命令行参数并读取它们的标准输出。本书将展示此类 Optuna 使用示例，因为从作者的角度来看，编写 Python 程序比为相同的自动化任务创建
    Bash 脚本更有用。
- en: 'To use Optuna for hyperparameter tuning, we need to complete the three following
    stages:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Optuna 进行超参数调整，我们需要完成以下三个阶段：
- en: Define an objective function for optimization.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个用于优化的目标函数。
- en: Create a study object.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个研究对象。
- en: Run optimization process.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行优化过程。
- en: Let’s see how we can write a simple Optuna program in Python to search for the
    best parameter for a polynomial regression algorithm written in C++ with the Flashlight
    library.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何用 Python 编写一个简单的 Optuna 程序来搜索用 Flashlight 库编写的 C++ 多项式回归算法的最佳参数。
- en: 'First, we need to import the required Python libraries:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要导入所需的 Python 库：
- en: '[PRE8]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, we need to define an objective function; it’s a function that is called
    by the Optuna tuning algorithm. It takes the `Trial` class object that contains
    a set of hyperparameter distributions and should return a performance metric value.
    The exact hyperparameter values should be sampled from the passed distributions.
    The following sample shows how we implement such a function:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要定义一个目标函数；这是一个由 Optuna 调优算法调用的函数。它接受一个包含一组超参数分布的 `Trial` 类对象，并应返回一个性能指标值。确切的超参数值应从传递的分布中进行采样。以下示例展示了我们如何实现此类函数：
- en: '[PRE9]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In this code, we used a family of functions in the `trial` object to sample
    concrete hyperparameter values. We sampled the `lr` learning rate with the call
    of the `suggest_float` method, and the `d` polynomial degree and the `bs` batch
    size with the `suggest_int` method. You can see that the signatures of these methods
    are pretty much the same. They take the name of hyperparameter, the low and the
    high bounds of a value range, and they can take a step value, which we didn’t
    use. These methods can sample values from discrete space and from continuous space
    too. The `suggest_float` method samples from a continuous space, and the `suggest_int`
    method samples from a discrete space.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，我们使用 `trial` 对象中的一系列函数来采样具体的超参数值。我们通过调用 `suggest_float` 方法采样 `lr` 学习率，并通过
    `suggest_int` 方法采样 `d` 多项式度和 `bs` 批处理大小。您可以看到这些方法的签名几乎相同。它们接受超参数的名称、值范围的低和高界限，并且可以接受一个步长值，我们没有使用。这些方法可以从离散空间和连续空间中采样值。`suggest_float`
    方法从连续空间中采样，而 `suggest_int` 方法从离散空间中采样。
- en: Then, we called the `run` method from the `subprocess` module; it launches another
    process in the system. This method takes the array of strings of command-line
    parameters and some other parameters —in our case, the `stdout` redirection. This
    redirection is needed because we want to get the process’s output in a return
    value of the `run` method call; you can see this in the last lines as `result.stdout`,
    which is converted from string to floating-point values and is interpreted as
    the MSE.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们调用了`subprocess`模块的`run`方法；它在系统中启动另一个进程。此方法接受命令行参数的字符串数组和一些其他参数——在我们的案例中，是`stdout`重定向。这种重定向是必要的，因为我们希望将进程的输出作为`run`方法调用返回值的返回值；你可以在最后几行看到`result.stdout`，它被转换为浮点值，并被解释为MSE。
- en: 'Having the objective function, we can define a `study` object. This object
    tells Optuna how to tune hyperparameters. The two main characteristics of this
    object are the optimization direction and a search space for the hyperparameter
    sampler algorithm. The following sample shows how to define a discrete search
    space for our task:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在拥有目标函数的情况下，我们可以定义一个`study`对象。这个对象告诉Optuna如何调整超参数。这个对象的两个主要特性是优化方向和超参数采样算法的搜索空间。以下示例展示了如何为我们的任务定义一个离散的搜索空间：
- en: '[PRE10]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In this Python code, we defined a `search_space` dictionary with three items.
    Each item has the key string and the array value. The keys are `learning_rate`,
    `polynomial_degree`, and `batch_size`. After we define the search space, we can
    create a `study` object; the following sample shows this:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段Python代码中，我们定义了一个包含三个条目的`search_space`字典。每个条目都有一个键字符串和数组值。键是`learning_rate`、`polynomial_degree`和`batch_size`。在定义搜索空间之后，我们可以创建一个`study`对象；以下示例展示了这一点：
- en: '[PRE11]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We used the `create_study` function from the `optuna` module and passed three
    parameters: `study_name`, `direction`, and `sampler`. The optimization direction
    we specified will be minimization, as we want to minimize MSE. For the `sampler`
    object, we used `GridSampler`, because we want to implement the grid search approach,
    and we initialized it with our search space.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了`optuna`模块中的`create_study`函数，并传递了三个参数：`study_name`、`direction`和`sampler`。我们指定的优化方向将是最小化，因为我们想最小化MSE。对于`sampler`对象，我们使用了`GridSampler`，因为我们想实现网格搜索方法，并且我们用我们的搜索空间初始化了它。
- en: 'The last step is to apply an optimization process and get the best hyperparameters.
    We can do this in the following way:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是应用优化过程以获取最佳超参数。我们可以用以下方式完成：
- en: '[PRE12]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We used the `optimize` method of our `study` object. You can see that it took
    a single parameter —our `objective` function that calls the external process to
    try sampled hyperparameters. The result of optimization was stored in the `study`
    object in the `best_value` and the `best_params` fields. The `best_value` field
    contains the best MSE value, and the `best_params` field contains the dictionary
    with the best hyperparameters.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了`study`对象的`optimize`方法。你可以看到它只接受一个参数——我们的`objective`函数，它调用外部进程尝试采样超参数。优化的结果存储在`study`对象的`best_value`和`best_params`字段中。`best_value`字段包含最佳MSE值，而`best_params`字段包含包含最佳超参数的字典。
- en: This is the minimal sample of how to use Optuna. This framework has a wide variety
    of tuning and sampling algorithms, and a real application can be much more complicated.
    Also, the use of Python saves us from writing a lot of boilerplate code for the
    CLI approach.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最小化的Optuna使用示例。这个框架有各种各样的调整和采样算法，实际应用可能要复杂得多。此外，使用Python可以让我们避免为CLI方法编写大量的模板代码。
- en: 'Let’s take a short look at a polynomial regression implementation with the
    Flashlight library. I will show only the most important parts; a full example
    can be found here: [https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/blob/main/Chapter03/flashlight/grid_fl.cc](https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/blob/main/Chapter03/flashlight/grid_fl.cc).'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要地看看使用Flashlight库的多项式回归实现。我将只展示最重要的部分；完整的示例可以在以下链接找到：[https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/blob/main/Chapter03/flashlight/grid_fl.cc](https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/blob/main/Chapter03/flashlight/grid_fl.cc)。
- en: 'The first important part is that our program should take all hyperparameters
    from the command-line argument. It can be implemented in the following way:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个重要部分是，我们的程序应该从命令行参数中获取所有超参数。它可以按以下方式实现：
- en: '[PRE13]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: First, we checked that we had enough command-line arguments by comparing `argc`
    parameter with the required number. In the fail case, we print a help message.
    But in the successful case, we read all hyperparameters from the `argv` parameter
    and convert them from strings to the appropriate types.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们通过比较`argc`参数与所需数量来检查我们是否有足够的命令行参数。在失败的情况下，我们打印一条帮助信息。但在成功的情况下，我们从`argv`参数中读取所有超参数，并将它们从字符串转换为适当的类型。
- en: 'Then, our program generates 2D cosine function points and mixes them with noise.
    To approximate a nonlinear function with linear regression, we can convert a simple
    Ax+b approach to a more complex polynomial like the following one:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们的程序生成2D余弦函数点并混合噪声。为了用线性回归近似非线性函数，我们可以将简单的Ax+b方法转换为更复杂的多项式，如下所示：
- en: '`a1*x+a2*x^2+a3*x^3+...+an*x^n + b`'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '`a1*x+a2*x^2+a3*x^3+...+an*x^n + b`'
- en: It means that we have to choose some polynomial degree and convert our single-dimensional
    `x` value to a multidimensional one by raising `x` elements to the corresponding
    powers. So, the polynomial degree is the most important hyperparameter in this
    algorithm.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们必须选择某个多项式次数，并通过将`x`元素提升到相应的幂来将单维`x`值转换为多维值。因此，多项式次数是这个算法中最重要的超参数。
- en: 'The Flashlight library doesn’t have any special implementation for regression
    algorithms because this library is oriented toward NN algorithms. But regression
    can be easily implemented with the gradient descent approach; the following code
    sample shows how it can be done:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Flashlight库没有为回归算法提供任何特殊的实现，因为这个库是针对NN算法的。但是，可以使用梯度下降方法轻松实现回归；以下代码示例显示了如何实现：
- en: '[PRE14]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: First, we defined learnable variables for the Flashlight `autograd` system;
    they are weights for each power of `X` and bias term. Then, we ran a loop for
    a specified number of epochs and the second loop over data batches to make the
    calculation vectorized; it makes computations more effective and makes the learning
    process less noise-dependent. For each batch of training data, we calculated predictions
    by getting a polynomial value; see the line with a call of the `matmul` function.
    The objective of the `MeanSquareError` class was used to get the loss function
    value. To calculate the corresponding gradients, see the `mse_func.forward` and
    `mse_func.backward` calls. Then, we updated our polynomial weights and biases
    with the learning rate and corresponding gradients.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们为Flashlight的`autograd`系统定义了可学习的变量；它们是`X`的每个幂的权重和偏置项。然后，我们运行指定数量的epoch的循环，以及数据批次的第二个循环，以使计算向量化；这使得计算更有效，并使学习过程对噪声的依赖性降低。对于每个训练数据批次，我们通过获取多项式值来计算预测；请参阅调用`matmul`函数的行。`MeanSquareError`类的目的是获取损失函数值。为了计算相应的梯度，请参阅`mse_func.forward`和`mse_func.backward`调用。然后，我们使用学习率和相应的梯度更新我们的多项式权重和偏置。
- en: 'All these concepts will be described in detail in the following chapters. The
    next important part is the `error` and `mse` value calculations. The `error` value
    is the Flashlight tensor object that contains the average MSE for the whole epoch,
    and the `mse` value is the floating-point value of this single-value tensor. This
    `mse` variable is printed to the standard output stream of the program at the
    end of the training, as follows:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些概念将在以下章节中详细描述。下一个重要部分是`error`和`mse`值的计算。`error`值是包含整个epoch平均MSE的Flashlight张量对象，而`mse`值是这个单值张量的浮点值。这个`mse`变量在训练结束时打印到程序的标准化输出流中，如下所示：
- en: '[PRE15]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We read this value in our Python program and return the result of our Optuna
    objective function for the given set of hyperparameters.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在我们的Python程序中读取这个值，并返回给定超参数集的Optuna目标函数的结果。
- en: Dlib example
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dlib示例
- en: 'The `Dlib` library also contains all the necessary functionality for the grid
    search algorithm. However, we should use functions instead of classes. The following
    code snippet shows the `CrossValidationScore` function’s definition. This function
    performs cross-validation and returns the value of the performance metric:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dlib`库也包含网格搜索算法的所有必要功能。然而，我们应该使用函数而不是类。以下代码片段显示了`CrossValidationScore`函数的定义。这个函数执行交叉验证并返回性能指标值：'
- en: '[PRE16]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `CrossValidationScore` function takes the hyperparameters that were set
    as arguments. Inside this function, we defined a trainer for a model with the
    `svr_trainer` class, which implements kernel ridge regression based on the `Shogun`
    library example.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '`CrossValidationScore`函数接收作为参数设置的超参数。在这个函数内部，我们使用`svr_trainer`类定义了一个模型的训练器，该类基于`Shogun`库的示例实现了基于核的岭回归。'
- en: After we defined the model, we used the `cross_validate_regression_trainer()`
    function to train the model with the cross-validation approach. This function
    automatically splits our data into folds, with its last argument being the number
    of folds. The `cross_validate_regression_trainer()` function returns the matrix,
    along with the values of different performance metrics. Notice that we do not
    need to define them because they are predefined in the library’s implementation.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们定义了模型之后，我们使用`cross_validate_regression_trainer()`函数通过交叉验证方法来训练模型。这个函数自动将我们的数据分割成多个折，其最后一个参数是折数。`cross_validate_regression_trainer()`函数返回一个矩阵，以及不同性能指标的价值。请注意，我们不需要定义它们，因为它们在库的实现中是预定义的。
- en: The first value in this matrix is the average MSE value. We used this value
    as a function result. However, there is no strong requirement for what value this
    function should return; the requirement is that the return value should be numeric
    and comparable. Also, notice that we defined the `CrossValidationScore` function
    as a lambda to simplify access to the training data container defined in the outer
    scope.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 这个矩阵的第一个值是平均均方误差（MSE）值。我们使用这个值作为函数的结果。然而，对于这个函数应该返回什么值没有强烈的要求；要求是返回值应该是数值型的并且可以比较。另外，请注意，我们将`CrossValidationScore`函数定义为lambda表达式，以简化对在外部作用域中定义的训练数据容器的访问。
- en: 'Next, we can search for the best parameters that were set with the `find_min_global`
    function:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用`find_min_global`函数搜索最佳参数：
- en: '[PRE17]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This function takes the cross-validation function, the container with minimum
    values for parameter ranges, the container with maximum values for parameter ranges,
    and the number of cross-validation repeats. Notice that the initialization values
    for parameter ranges should go in the same order as the arguments that were defined
    in the `CrossValidationScore` function. Then, we can extract the best hyperparameters
    and train our model with them:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数接收交叉验证函数、参数范围的最小值容器、参数范围的最大值容器以及交叉验证重复次数。请注意，参数范围的初始化值应该按照与在`CrossValidationScore`函数中定义的参数相同的顺序排列。然后，我们可以提取最佳超参数并使用它们来训练我们的模型：
- en: '[PRE18]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We used the same model definition as in the `CrossValidationScore` function.
    For the training process, we used all of our training data. The `train` method
    of the `trainer` object was used to complete the training process. The training
    result is a function that takes a single sample as an argument and returns a prediction
    value.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了与`CrossValidationScore`函数中相同的模型定义。对于训练过程，我们使用了所有的训练数据。使用`trainer`对象的`train`方法来完成训练过程。训练结果是一个函数，它接受一个单独的样本作为参数并返回一个预测值。
- en: Summary
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed how to estimate an ML model’s performance and
    what metrics can be used for such estimation. We considered different metrics
    for regression and classification tasks and what characteristics they have. We
    also saw how performance metrics can be used to determine the model’s behavior
    and looked at bias and variance characteristics. We looked at some high bias (underfitting)
    and high variance (overfitting) problems and considered how to solve them. We
    also learned about regularization approaches, which are often used to deal with
    overfitting. We then studied what validation is and how it is used in the cross-validation
    technique. We saw that the cross-validation technique allows us to estimate model
    performance while training limited data. In the last section, we combined an evaluation
    metric and cross-validation in the grid search algorithm, which we can use to
    select the best set of hyperparameters for our model.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了如何估计机器学习模型的性能以及可以使用哪些指标进行此类估计。我们考虑了回归和分类任务的不同指标以及它们的特征。我们还看到了如何使用性能指标来确定模型的行为，并探讨了偏差和方差特征。我们研究了某些高偏差（欠拟合）和高方差（过拟合）问题，并考虑了如何解决这些问题。我们还了解了正则化方法，这些方法通常用于处理过拟合。然后，我们研究了验证是什么以及它在交叉验证技术中的应用。我们看到交叉验证技术允许我们在训练有限数据的同时估计模型性能。在最后一节中，我们将评估指标和交叉验证结合到网格搜索算法中，我们可以使用它来选择模型的最佳超参数集。
- en: In the next chapter, we’ll learn about ML algorithms we can use to solve concrete
    problems. The next topic we will discuss in depth is clustering—the procedure
    of splitting the original set of objects into groups classified by properties.
    We will look at different clustering approaches and their characteristics.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习可以使用来解决具体问题的机器学习算法。我们将深入讨论的下一个主题是聚类——将原始对象集按属性分成组的程序。我们将探讨不同的聚类方法和它们的特性。
- en: Further reading
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*Choosing the Right Metric for Evaluating Machine Learning Models—Part* *1*:
    [https://medium.com/usf-msds/choosing-the-right-metric-for-machine-learning-models-part-1-a99d7d7414e4](https://medium.com/usf-msds/choosing-the-right-metric-for-machine-learning-models-part-1-a99d7d7414e4)'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*选择评估机器学习模型的正确指标——第1部分*: [https://medium.com/usf-msds/choosing-the-right-metric-for-machine-learning-models-part-1-a99d7d7414e4](https://medium.com/usf-msds/choosing-the-right-metric-for-machine-learning-models-part-1-a99d7d7414e4)'
- en: '*Understand Regression Performance* *Metrics*: [https://becominghuman.ai/understand-regression-performance-metrics-bdb0e7fcc1b3](https://becominghuman.ai/understand-regression-performance-metrics-bdb0e7fcc1b3)'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*理解回归性能指标*: [https://becominghuman.ai/understand-regression-performance-metrics-bdb0e7fcc1b3](https://becominghuman.ai/understand-regression-performance-metrics-bdb0e7fcc1b3)'
- en: '*Classification Performance* *Metrics*: [https://nlpforhackers.io/classification-performance-metrics/](https://nlpforhackers.io/classification-performance-metrics/)'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分类性能指标*: [https://nlpforhackers.io/classification-performance-metrics/](https://nlpforhackers.io/classification-performance-metrics/)'
- en: '*REGULARIZATION: An important concept in Machine* *Learning*: [https://towardsdatascience.com/regularization-for-machine-learning-67c37b132d61](https://towardsdatascience.com/regularization-for-machine-learning-67c37b132d61)'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*正则化：机器学习中的一个重要概念*: [https://towardsdatascience.com/regularization-for-machine-learning-67c37b132d61](https://towardsdatascience.com/regularization-for-machine-learning-67c37b132d61)'
- en: 'An overview of regularization techniques in **deep learning** (**DL**) (with
    Python code): [https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques](https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques)'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习（DL）中正则化技术的概述（带Python代码）：[https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques](https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques)
- en: '*Understanding the Bias-Variance* *Tradeoff*: [https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229)'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*理解偏差-方差权衡*: [https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229)'
- en: 'DL – Overfitting: [https://towardsdatascience.com/combating-overfitting-in-deep-learning-efb0fdabfccc](https://towardsdatascience.com/combating-overfitting-in-deep-learning-efb0fdabfccc)'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DL – 过拟合：[https://towardsdatascience.com/combating-overfitting-in-deep-learning-efb0fdabfccc](https://towardsdatascience.com/combating-overfitting-in-deep-learning-efb0fdabfccc)
- en: '*A Gentle Introduction to k-fold* *Cross-Validation*: [https://machinelearningmastery.com/k-fold-cross-validation/](https://machinelearningmastery.com/k-fold-cross-validation/)'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*k折交叉验证的温和介绍*: [https://machinelearningmastery.com/k-fold-cross-validation/](https://machinelearningmastery.com/k-fold-cross-validation/)'
- en: 'Part 2: Machine Learning Algorithms'
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分：机器学习算法
- en: In this part, we’ll show you how to implement different well-known machine learning
    models (algorithms) using a variety of C++ frameworks.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在这部分，我们将向您展示如何使用各种C++框架实现不同的知名机器学习模型（算法）。
- en: 'This part comprises the following chapters:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包括以下章节：
- en: '[*Chapter 4*](B19849_04.xhtml#_idTextAnchor228), *Clustering*'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第4章*](B19849_04.xhtml#_idTextAnchor228), *聚类*'
- en: '[*Chapter 5*](B19849_05.xhtml#_idTextAnchor258), *Anomaly Detection*'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第5章*](B19849_05.xhtml#_idTextAnchor258), *异常检测*'
- en: '[*Chapter 6*](B19849_06.xhtml#_idTextAnchor301), *Dimensionality Reduction*'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第6章*](B19849_06.xhtml#_idTextAnchor301), *降维*'
- en: '[*Chapter 7*](B19849_07.xhtml#_idTextAnchor383), *Classification*'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第7章*](B19849_07.xhtml#_idTextAnchor383), *分类*'
- en: '[*Chapter 8*](B19849_08.xhtml#_idTextAnchor438), *Recommender Systems*'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第8章*](B19849_08.xhtml#_idTextAnchor438), *推荐系统*'
- en: '[*Chapter 9*](B19849_09.xhtml#_idTextAnchor496), *Ensemble Learning*'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第9章*](B19849_09.xhtml#_idTextAnchor496), *集成学习*'
