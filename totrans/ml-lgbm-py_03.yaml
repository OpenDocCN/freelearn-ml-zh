- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An Overview of LightGBM in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we looked at ensemble learning methods for decision
    trees. Both **bootstrap aggregation** (**bagging**) and gradient boosting were
    discussed in detail, with practical examples of how to apply the techniques in
    scikit-learn. We also showed how **gradient-boosted decision trees** (**GBDTs**)
    are slow to train and may underperform on some problems.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter introduces LightGBM, a gradient-boosting framework that uses tree-based
    learners. We look at the innovations and optimizations LightGBM makes to the ensemble
    learning methods. Further details and examples are given for using LightGBM practically
    via Python. Finally, the chapter includes a modeling example using LightGBM, incorporating
    more advanced techniques for model validation and parameter optimization.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of the chapter, you will have a thorough understanding of the theoretical
    and practical properties of LightGBM, allowing us to dive deeper into using LightGBM
    for data science and production systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main topics of this chapter are set out here:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing LightGBM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with LightGBM in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building LightGBM models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The chapter includes examples and code excerpts illustrating how to use LightGBM
    in Python. Complete examples and instructions for setting up a suitable environment
    for this chapter are available a[t https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3)-3.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing LightGBM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LightGBM is an open source, gradient-boosting framework for tree-based ensembles
    ([https://github.com/microsoft/LightGBM](https://github.com/microsoft/LightGBM)).
    LightGBM focuses on efficiency in speed, memory usage, and improved accuracy,
    especially for problems with high dimensionality and large data sizes.
  prefs: []
  type: TYPE_NORMAL
- en: 'LightGBM was first introduced in the paper *LightGBM: A Highly Efficient Gradient
    Boosting Decision* *Tree* [1].'
  prefs: []
  type: TYPE_NORMAL
- en: The efficiency and accuracy of LightGBM are achieved via several technical and
    theoretical optimizations to the standard ensemble learning methods, particularly
    GBDTs. Additionally, LightGBM supports distributed training of ensembles with
    optimizations in network communication and support for GPU-based training of tree
    ensembles.
  prefs: []
  type: TYPE_NORMAL
- en: 'LightGBM supports many **machine learning** (**ML**) applications: regression,
    binary and multiclass classification, cross-entropy loss functions, and ranking
    via LambdaRank.'
  prefs: []
  type: TYPE_NORMAL
- en: The LightGBM algorithm is also very customizable via its hyperparameters. It
    supports many metrics and features, including **Dropouts meet Multiple Additive
    Regression Trees** (**DART**), bagging (random forests), continuous training,
    multiple metrics, and early stopping.
  prefs: []
  type: TYPE_NORMAL
- en: This section reviews the theoretical and practical optimizations LightGBM utilizes,
    including a detailed overview of the hyperparameters to control LightGBM features.
  prefs: []
  type: TYPE_NORMAL
- en: LightGBM optimizations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At its core, LightGBM implements the same ensemble algorithms we discussed in
    the previous chapter. However, LightGBM applies theoretical and technical optimizations
    to improve performance and accuracy while significantly reducing memory usage.
    Next, we discuss the most significant optimizations implemented in LightGBM.
  prefs: []
  type: TYPE_NORMAL
- en: Computational complexity in GBDTs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we must understand where the inefficiency in building GBDTs stems from
    to understand how LightGBM improves the efficiency of GBDTs. The most computationally
    complex part of the GBDT algorithm is training the regression tree for each iteration.
    More specifically, finding the optimal split is very expensive. Pre-sort-based
    algorithms are among the most popular methods for finding the best splits [2],
    [3]. A naïve approach requires the data to be sorted by feature for every decision
    node with algorithmic complexity O(#data × #feature). Pre-sort-based algorithms
    sort the data once before training, which reduces the complexity when building
    a decision node to O(#data) [2]. Even with pre-sorting, the complexity is too
    high for large datasets when finding splits for decision nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: Histogram-based sampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An alternative approach to pre-sorting involves building histograms for continuous
    features [4]. The continuous values are added into discrete bins when building
    these **feature histograms**. Instead of using the data directly when calculating
    the splits for decision nodes, we can now use the histogram bins. Constructing
    the histograms has a complexity of O(#data). However, the complexity for building
    a decision node now reduces to O(#bins), and since the number of bins is much
    smaller than the amount of data, this significantly speeds up the process of building
    regression trees, as illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Creating feature histograms from continuous features allows
    calculating splits for decision nodes using bin boundary values instead of having
    to sample each data point, significantly reducing the algorithm’s complexity since
    #bins << #data](img/B16690_03_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1 – Creating feature histograms from continuous features allows calculating
    splits for decision nodes using bin boundary values instead of having to sample
    each data point, significantly reducing the algorithm’s complexity since #bins
    << #data'
  prefs: []
  type: TYPE_NORMAL
- en: A secondary optimization that stems from using histograms is “histogram subtraction”
    for building the histograms for the leaves. Instead of calculating the histogram
    for each leaf, we can subtract the leaf’s neighbor’s histogram from the parent’s
    histogram. Choosing the leaf with the smaller amount of data leads to a smaller
    O(#data) complexity for the first leaf and O(#bin) complexity for the second leaf
    due to histogram subtraction.
  prefs: []
  type: TYPE_NORMAL
- en: 'A third optimization that LightGBM applies using histograms is to reduce the
    memory cost. Feature pre-sorting requires a supporting data structure (a dictionary)
    for each feature. No such data structures are required when building histograms,
    reducing memory costs. Further, since #bins is small, a smaller data type, such
    as `uint8_t`, can store the training data, reducing memory usage.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Detailed information regarding the algorithms for building feature histograms
    is available in the paper *CLOUDS: A decision tree classifier for large* *datasets*
    [4].'
  prefs: []
  type: TYPE_NORMAL
- en: Exclusive Feature Bundling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Exclusive Feature Bundling** (**EFB**) is another data-based optimization
    that LightGBM applies when working with sparse data (sparse data is pervasive
    in high-dimensional datasets). When the feature data is sparse, it’s common to
    find that many features are *mutually exclusive*, signifying they never present
    non-zero values simultaneously. Combining these features into a single one is
    generally safe, given this exclusivity. EFB is illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Building a feature bundle from two mutually exclusive features](img/B16690_03_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – Building a feature bundle from two mutually exclusive features
  prefs: []
  type: TYPE_NORMAL
- en: 'Bundling mutually exclusive features allows building the same feature histograms
    as from the individual features [1]. The optimization reduces the complexity of
    building feature histograms from O(#data × #feature) to O(#data × #bundle). For
    datasets where there are many mutually exclusive features, this dramatically improves
    performance since # bundle ≪ #feature. Detailed algorithms for, and proof of the
    correctness of, EFB are available in [1].'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient-based One-Side Sampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A final data-based optimization available in the LightGBM framework is **Gradient-based
    One-Side Sampling** (**GOSS**) [1]. GOSS is a method of discarding training data
    samples that no longer contribute significantly to the training process, effectively
    reducing the training data size and speeding up the process.
  prefs: []
  type: TYPE_NORMAL
- en: We can use the gradient calculation of each sample to determine its importance.
    If the gradient change is small, it implies that the training error was also small,
    and we can infer that the tree is well fitted to the specific data instance [1].
    One option would be to discard all instances with small gradients. However, this
    changes the distribution of the training data, reducing the tree’s ability to
    generalize. GOSS is a method for choosing which instances to keep in the training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To maintain the data distribution, GOSS is applied as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The data samples are sorted by the absolute value of their gradients.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The top a × 100% instances are then selected (instances with large gradients).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A random sample of b × 100% instances is then taken from the rest of the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A factor is added to the loss function (for these instances) to amplify their
    influence: 1 − a _ b , thereby compensating for the underrepresentation of data
    with small gradients.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Therefore, GOSS samples a large portion of instances with large gradients and
    a random portion of instances with small gradients and amplifies the influence
    of the small gradients when calculating information gain.
  prefs: []
  type: TYPE_NORMAL
- en: The downsampling enabled by GOSS can significantly reduce the amount of data
    processed during training (and the training time for the GBDTs), especially in
    the case of large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Best-first tree growth
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most common method for building decision trees is to grow the tree by level
    (that is, one level at a time). LightGBM uses an alternative approach and grows
    the tree leaf-wise or best-first. The leaf-wise approach selects an existing leaf
    with the most significant change in the loss of the tree and builds the tree from
    there. The downside of this approach is that if the dataset is small, the tree
    is likely to overfit the data. A maximum depth has to be set to counteract this.
    However, if the number of leaves to construct is fixed, leaf-wise tree building
    is shown to outperform level-wise algorithms [5].
  prefs: []
  type: TYPE_NORMAL
- en: L1 and L2 regularization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LightGBM supports both L1 and L2 regularization of the objective function when
    training the regression trees in the ensemble. From [*Chapter 1*](B16690_01.xhtml#_idTextAnchor014)*,
    Introducing Machine Learning*, we recall that regularization is a way to control
    overfitting. In the case of decision trees, simpler, shallow trees overfit less.
  prefs: []
  type: TYPE_NORMAL
- en: 'To support L1 and L2 regularization, we extend the objective function with
    a regularization term, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: obj = L(y, F(x)) + Ω(w)
  prefs: []
  type: TYPE_NORMAL
- en: Here, L(y, F(x)) is the loss function discussed in [*Chapter 2*](B16690_02.xhtml#_idTextAnchor036)*,
    Ensemble Learning – Bagging and Boosting*, and Ω(w) is the regularization function
    defined over w, the leaf scores (the leaf score is the output calculated from
    the leaf as per *step 2.3* in the GBDT algorithm defined in [*Chapter 2*](B16690_02.xhtml#_idTextAnchor036)*,
    Ensemble Learning – Bagging* *and Boosting*).
  prefs: []
  type: TYPE_NORMAL
- en: The regularization term effectively adds a penalty to the objective function,
    where we aim to penalize more complex trees prone to overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are multiple definitions for Ω. A typical implementation for the terms
    in decision trees is this:'
  prefs: []
  type: TYPE_NORMAL
- en: Ω(w) = α∑ i n |w i| + λ∑ i n w i 2
  prefs: []
  type: TYPE_NORMAL
- en: Here, α∑ i n |w i| is the L1 regularization term, controlled by the parameter
    α, 0 ≤ α ≤ 1, and λ∑ i n w i 2 is the L2 regularization term, controlled by the
    parameter λ.
  prefs: []
  type: TYPE_NORMAL
- en: L1 regularization has the effect of driving leaf scores to zero by penalizing
    leaves with large absolute outputs. *Smaller leaf outputs have a smaller effect
    on the tree’s prediction, effectively simplifying* *the tree*.
  prefs: []
  type: TYPE_NORMAL
- en: L2 regularization is similar but has an outsized effect on outliers’ leaves
    due to taking the square of the output.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, when larger trees are built (trees with more leaves, and therefore
    a large w vector), both sum terms for Ω(w) increase, increasing the objective
    function output. Therefore, *larger trees are penalized*, and overfitting is reduced.
  prefs: []
  type: TYPE_NORMAL
- en: Summary of LightGBM optimizations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In summary, LightGBM improves upon the standard ensemble algorithms by doing
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing histogram-based sampling of features to reduce the computational
    cost of finding optimal splits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating exclusive feature bundles to reduce the number of features in sparse
    datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying GOSS to downsample the training data without losing accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building trees leaf-wise to improve accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overfitting can be controlled through L1 and L2 regularization and other control
    parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In conjunction, the optimizations improve the computational performance of LightGBM
    by **orders of magnitude** (**OOM**) over the standard GBDT algorithm. Additionally,
    LightGBM is implemented in C++ with a Python interface, which results in much
    faster code than Python-based GBDTs, such as in scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, LightGBM also has support for improved data-parallel and feature-parallel
    distributed training. Distributed training and GPU support are discussed in a
    later [*Chapter 11*](B16690_11.xhtml#_idTextAnchor177)*, Distributed and GPU-Based
    Learning* *with LightGBM*.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LightGBM exposes many parameters that can be used to customize the training
    process, goals, and performance. Next, we discuss the most notable parameters
    and how they may be used to control specific phenomena.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The core LightGBM framework is developed in C++ but includes APIs to work with
    LightGBM in C, Python, and R. The parameters discussed in this section are the
    framework parameters and are exposed differently by each API. The following section
    discusses the parameters available when using Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are **core framework parameters** used to control the optimization
    process and goal:'
  prefs: []
  type: TYPE_NORMAL
- en: '`objective`: LightGBM supports the following optimization objectives, among
    others—`regression` (including regression applications with other loss functions
    such as Huber and Fair), `binary` (classification), `multiclass` (classification),
    `cross-entropy`, and `lambdarank` for ranking problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`boosting`: The boosting parameter controls the boosting type. By default,
    this is set to `gbdt`, the standard GBDT algorithm. The other options are `dart`
    and `rf` for random forests. The random forest mode does not perform boosting
    but instead builds a random forest.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_iterations` (or `n_estimators`): Controls the number of boosting iterations
    and, therefore, the number of trees built.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_leaves`: Controls the maximum number of leaves in a single tree.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learning_rate`: Controls the learning, or shrinkage rate, which is the contribution
    of each tree to the overall prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LightGBM also provides many parameters to control the learning process. We’ll
    discuss these parameters relative to how they may be used to tune specific aspects
    of training.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following control parameters can be used to improve **accuracy**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`boosting`: Use `dart`, which has been shown to outperform standard GBDTs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learning_rate`: The learning rate must be tuned alongside `num_iterations`
    for better accuracy. A small learning rate with a large value for `num_iterations`
    leads to better accuracy at the expense of optimization speed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_leaves`: A larger number of leaves improves accuracy but may lead to overfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_bin`: The maximum number of bins in which features are bucketed when constructing
    histograms. A larger `max_bin` size slows the training and uses more memory but
    may improve accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following **learning control parameters** can be used to deal with **overfitting**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`bagging_fraction` and `bagging_freq`: Setting both parameters enables feature
    bagging. Bagging may be used in addition to boosting and doesn’t force the use
    of a random forest. Enabling bagging reduces overfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`early_stopping_round`: Enables early stopping and controls the number of iterations
    used to determine whether training should be stopped. Training is stopped if no
    improvement is made to any metric in the iterations set by `early_stopping_round`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_data_in_leaf`: The minimum samples allowed in a leaf. Larger values reduce
    overfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_gain_to_split`: The minimum amount of information gain required to perform
    a split. Higher values reduce overfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reg_alpha`: Controls L1 regularization. Higher values reduce overfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reg_lambda`: Controls L2 regularization. Higher values reduce overfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_depth`: Controls the maximum depth of individual trees. Shallower trees
    reduce overfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_drop`: Controls the maximum number of dropped trees when using the DART
    algorithm (is only used when `boosting` is set to `dart`). A larger value reduces
    overfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`extra_trees`: Enables the **Extremely Randomized Trees** (**ExtraTrees**)
    algorithm. LightGBM then chooses a split threshold at random for each feature.
    Enabling Extra-Trees can reduce overfitting. The parameter can be used in conjunction
    with any boosting mode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The parameters discussed here include only some of the parameters available
    in LightGBM and focus on improving accuracy and overfitting. A complete list of
    parameters [is available at the following link: https://lightgbm.rea](https://lightgbm.readthedocs.io/en/latest/Parameters.xhtml)dthedocs.io/en/latest/Parameters.xhtml.'
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of LightGBM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LightGBM is designed to be more efficient and effective than traditional methods.
    It is particularly well known for its ability to handle large datasets. However,
    as with any algorithm or framework, it also has its limitations and potential
    disadvantages, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sensitive to overfitting**: LightGBM can be sensitive to overfitting, especially
    with small or noisy datasets. Care should be taken to monitor and control for
    overfitting when using LightGBM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimal performance requires tuning**: As discussed previously, LightGBM
    has many hyperparameters that need to be properly tuned to get the best performance
    from the algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lack of representation learning**: Unlike **deep learning** (**DL**) approaches,
    which excel at learning from raw data, LightGBM requires feature engineering to
    be applied to the data before learning. Feature engineering is a time-consuming
    process that requires domain knowledge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Handling sequential data**: LightGBM is not inherently designed for working
    with sequential data such as time series. For LightGBM to be used with time-series
    data, feature engineering needs to be applied to create lagged features and capture
    temporal dependencies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complex interactions and non-linearities**: LightGBM is a decision-tree-driven
    approach that might be incapable of capturing complex feature interactions and
    non-linearities. Proper feature engineering needs to be applied to ensure the
    algorithm models these.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although these are potential limitations of using the algorithm, they may not
    apply to all use cases. LightGBM is often a very effective tool in the right circumstances.
    As with any model, understanding the trade-offs is vital to making the right choice
    for your application.
  prefs: []
  type: TYPE_NORMAL
- en: In the next session, we look at getting started using the various LightGBM APIs
    with Python.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with LightGBM in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LightGBM is implemented in C++ but has official C, R, and Python APIs. This
    section discusses the Python APIs that are available for working with LightGBM.
    LightGBM provides three Python APIs: the standard **LightGBM** API, the **scikit-learn**
    API (which is fully compatible with other scikit-learn functionality), and a **Dask**
    API for working with Dask. Dask is a parallel computing library discussed in [*Chapter
    11*](B16690_11.xhtml#_idTextAnchor177)*, Distribu*[*ted and GPU-Based Lea*](https://www.dask.org/)*rning
    with* *LightGBM* ([https://www.dask.org/](https://www.dask.org/)).'
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the rest of the book, we mainly use the scikit-learn API for LightGBM,
    but let’s first look at the standard Python API.
  prefs: []
  type: TYPE_NORMAL
- en: LightGBM Python API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The best way to dive into the Python API is with a hands-on example. The following
    are excerpts from a code listing that illustrates the use of the LightGBM Python
    API. The complete code example is available at [https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3).
  prefs: []
  type: TYPE_NORMAL
- en: 'LightGBM needs to be imported. The import is often abbreviated as `lgb`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: LightGBM provides a `Dataset` wrapper class to work with data. `Dataset` supports
    a variety of formats. Commonly, it is used to wrap a `numpy` array or a `pandas`
    DataFrame. `Dataset` also accepts a `Path` to a CSV, TSV, LIBSVM text file, or
    LightGBM `Dataset` binary file. When a path is supplied, LightGBM loads the data
    from the disk.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we load our Forest Cover dataset from `sklearn` and wrap the `numpy`
    arrays in a LightGBM `Dataset`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We subtract 1 from the `y_train` and `y_test` arrays because the classes supplied
    by `sklearn` are labeled in the range [1, 7], whereas LightGBM expects zero-indexed
    class labels in the range [0, 7].
  prefs: []
  type: TYPE_NORMAL
- en: 'We cannot set up the parameters for training. We’ll be using the following
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We are using the standard GBDT as a boosting type and setting the objective
    to multiclass classification for seven classes. During training, we are going
    to capture the `auc_mu` metric. AU C μ is a multiclass adaptation of the **area
    under the receiver operating characteristic curve** (**AUC**), as defined by Kleiman
    and Page [6].
  prefs: []
  type: TYPE_NORMAL
- en: We set `num_leaves` and `learning_rate` to reasonable values for the problem.
    Finally, we specify `force_row_wise` as `True`, a recommended setting for large
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'LightGBM’s training function also supports **callbacks**. A callback is a hook
    into the training process that is executed each boosting iteration. To illustrate
    their purpose, we’ll be using the following callbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We use the `log_evaluation` callback with a period of 15, which logs (prints)
    our metrics to standard output every 15 boosting iterations. We also set a `record_evaluation`
    callback that captures our evaluation metrics in the `metrics` dictionary. We
    also specify an `early_stopping` callback, with stopping rounds set to 15\. The
    `early_stopping` callback stops training if no validation metrics improve after
    the specified number of stopping rounds.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we also use the `reset_parameter` callback to implement **learning
    rate decay**. The decay function is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `reset_parameter` callback takes a function as input. The function receives
    the current iteration and returns the parameter value. Learning rate decay is
    a technique where we decrease the value of the learning rate over time. Learning
    rate decay improved the overall accuracy achieved. Ideally, we want the initial
    trees to have a more significant impact on correcting the prediction errors. In
    contrast, later on, we want to reduce the impact of additional trees and have
    them make minor adjustments to the errors. We implement a slight exponential decay
    that reduces the learning rate from 0.09 to 0.078 throughout training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we are ready for training. We use `lgb.train` to train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We use 150 boosting rounds (or boosted trees). In conjunction with a lower learning
    rate, having many boosting rounds should improve accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'After training, we can use `lgb.predict` to get predictions for our test set
    and calculate the F1 score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The LightGBM predict function outputs an array of activations, one for each
    class. Therefore, we use `np.argmax` to choose the class with the highest activation
    as the predicted class. LightGBM also has support for some plotting functions.
    For instance, we can use `plot_metric` to plot our AU C μ results as captured
    in the `metrics`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The results of this are shown in *Figure 3**.3*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – A plot of the ​AU ​C​ <?AID d835?><?AID df41?>​​​ metric per
    training iteration created using lgb.plot_metric](img/B16690_03_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – A plot of the AU C 𝝁 metric per training iteration created using
    lgb.plot_metric
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the preceding code should produce a LightGBM GBDT tree with an F1 score
    of around 0.917, in line with the score the Random Forest and Extra-Trees algorithms
    achieved in [*Chapter 2*](B16690_02.xhtml#_idTextAnchor036)*, Ensemble Learning
    – Bagging and Boosting*. However, LightGBM is significantly faster in reaching
    these accuracies. LightGBM completed the training in just 37 seconds on our hardware:
    this is 4.5 times faster than running Extra-Trees on the same problem and hardware
    and 60-70 times faster than scikit-learn’s `GradientBoostingClassifier` in our
    testing.'
  prefs: []
  type: TYPE_NORMAL
- en: LightGBM scikit-learn API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We now take a look at the scikit-learn Python API for LightGBM. The scikit-learn
    API provides four classes: `LGBMModel`, `LGBMClassifier`, `LGBMRegressor`, and
    `LGBMRanker`. Each of these provides the same functionality as the LightGBM Python
    API, but with the same convenient scikit-learn interfaces we have worked with
    before. Additionally, the scikit-learn classes are compatible and interoperable
    with the rest of the scikit-learn ecosystem.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s replicate the previous example using the scikit-learn API.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset is loaded precisely as before. The scikit-learn API doesn’t require
    wrapping the data in a `Dataset` object. We also don’t have to zero-index our
    target classes, as scikit-learn supports any label for the classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The scikit-learn API also supports LightGBM callbacks; as such, we use the
    same callbacks as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We then create the `LGBMClassifier` exactly as we would any other scikit-learn
    model. When creating the classifier, we also set the parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we do not have to specify the number of classes; scikit-learn infers
    this automatically. We then call `fit` on the model, passing the training and
    test data along with our callbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we evaluate our model with the F1 score. We don’t have to use `np.argmax`
    on the predictions as this is done automatically with the scikit-learn API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Overall, we can see that using LightGBM via the scikit-learn API is more straightforward
    than the standard Python API. The scikit-learn API was also approximately 40%
    faster than the LightGBM API on our hardware. This section examined the ins and
    outs of using the various Python APIs available for LightGBM. The following section
    looks at training LightGBM models using the scikit-learn API.
  prefs: []
  type: TYPE_NORMAL
- en: Building LightGBM models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section provides an end-to-end example of solving a real-world problem
    using LightGBM. We provide a more detailed look at data preparation for a problem
    and explain how to find suitable parameters for our algorithms. We use multiple
    variants of LightGBM to explore relative performance and compare them against
    random forests.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we delve into solving a problem, we need to discuss a better way of validating
    algorithm performance. Splitting the data into two or three subsets is standard
    practice when training a model. The training data is used to train the model,
    the validation data is a hold-out set used to validate the data during training,
    and the test data is used to validate the performance after training.
  prefs: []
  type: TYPE_NORMAL
- en: In previous examples, we have done this split only once, building a single training
    and test to train and validate the model. The issue with this approach is that
    our model could get “lucky.” If, by chance, our test set closely matches the training
    data but is not representative of real-world data, we would report a good test
    error, even though we can’t be confident of our model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative is to do the dataset splitting multiple times and train the model
    multiple times, once for each split. This approach is called **cross-validation**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common application of cross-validation is *k-fold cross-validation*.
    With k-fold cross-validation, we choose a value, *k*, and partition the (shuffled)
    dataset into *k* subsamples (or folds). We then repeat the training process *k*
    times, using a different subset as the validation data and all other subsets as
    training data. The model’s performance is calculated as the mean (or median) score
    across all folds. The following diagram illustrates this process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – k-fold cross-validation with k = 3; the original dataset is
    shuffled and split into 3 equal parts (or folds); training and validation are
    repeated for each combination of subsampled data, and the average performance
    is reported](img/B16690_03_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – k-fold cross-validation with k = 3; the original dataset is shuffled
    and split into 3 equal parts (or folds); training and validation are repeated
    for each combination of subsampled data, and the average performance is reported
  prefs: []
  type: TYPE_NORMAL
- en: Using a high value for *k* reduces the chance that the model coincidentally
    shows good performance and indicates how the model might perform in the real world.
    However, the entire training process is repeated for each fold, which could be
    computationally expensive and time-consuming. Therefore, we need to balance the
    resources available with the need to validate the model. A typical value for *k*
    is 5 (the default for scikit-learn), also called 5-fold cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: Stratified k-fold validation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A problem that might arise with k-fold cross-validation is that, due to chance,
    a fold may contain samples from only a single class. **Stratified sampling** solves
    this issue by preserving the percentage of samples for each class when creating
    folds. In this way, each fold has the same distribution of classes as the original
    dataset. When applied to cross-validation, this technique is called stratified
    k-fold cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: Parameter optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Parameter optimization**, also called parameter tuning, is the process of
    finding good hyperparameters for the model and training process specific to the
    problem being solved. In the previous examples of training models, we have been
    setting the model and training algorithm’s parameters based on intuition and minimal
    experimentation. There is no guarantee that the parameter choices were optimal
    for the optimization problem.'
  prefs: []
  type: TYPE_NORMAL
- en: But how might we go about finding the best parameter choices? A naïve strategy
    is to try an extensive range of values for a parameter, find the best value, and
    then repeat the process for the following parameter. However, it is frequently
    the case that parameters are **co-dependent**. When we change one parameter, the
    optimal value for another might differ. An excellent example of co-dependence
    in GBDTs is the number of boosting rounds and the learning rate. Having a small
    learning rate necessitates more boosting rounds. Therefore, optimizing the learning
    rate and then, independently, the number of boosting rounds is unlikely to produce
    optimal results. *Both parameters must be optimized* *in unison*.
  prefs: []
  type: TYPE_NORMAL
- en: Grid search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An approach that accounts for parameter co-dependence is grid search. With grid
    search, a parameter grid is set up. The grid consists of a range of values to
    try for each parameter we are optimizing. An exhaustive search is then performed,
    training and validating the model on each possible combination of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of a parameter grid for three parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Each parameter is specified with a range of possible values. The previous grid
    would require 150 trails to search.
  prefs: []
  type: TYPE_NORMAL
- en: Since grid search is exhaustive, it has the advantage that it is guaranteed
    to find the best combination of parameters within the ranges specified. However,
    the downside to grid search is the cost. Trying each possible combination of parameters
    is very expensive and quickly becomes intractable for many parameters and large
    parameter ranges.
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn provides a utility class to implement grid search and perform cross-validation
    at the same time. `GridSearchCV` takes a model, a parameter grid, and the number
    of cross-validation folds as parameters. `GridSearchCV` then proceeds to search
    the grid for the best parameters, using cross-validation to validate the performance
    for each combination of parameters. We’ll illustrate the use of `GridSearchCV`
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Parameter optimization is a crucial part of the modeling process. Finding suitable
    parameters for a model could be the difference between a successful or failed
    process. However, as discussed previously, parameter optimization is also often
    enormously expensive regarding time and computational complexity, necessitating
    a trade-off between cost and performance.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting student academic success
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now move on to our example. We build a model to predict students’ dropout
    rate based on a range of social and economic factors using LightGBM [7] ([https://archive-beta.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success](https://archive-beta.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success)).
    The data is available in CSV format. We start by exploring the data.
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory data analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One of the most fundamental properties of any dataset is the shape: the rows
    and columns our data consists of. It’s also an excellent way to validate that
    the data read succeeded. Here, our data consists of 4,424 rows and 35 columns.
    Taking a random sample of the data gives us a sense of the columns and their values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can run `df.info()` to see all the columns, their non-null counts,
    and their data types:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Running the preceding code shows us that most columns are integer types, except
    for the `Target` column, with a few floats in between. The `Target` column is
    listed as type `object`; if we look at the values in the sample, we can see the
    `Target` column consists of `Graduate`, `Dropout`, and `Enrolled` strings. LightGBM
    can’t work with strings as targets, so we’ll map these to integer values before
    training our models.
  prefs: []
  type: TYPE_NORMAL
- en: We can also run `df.describe()` to get a statistical description (mean, standard
    deviation, min, max, and percentiles) of the values in each column. Calculating
    descriptive statistics helps check the bounds of the data (not a big problem with
    working with decision tree models) and check for outliers. For this dataset, there
    aren’t any data bounds or outlier concerns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to check for duplicated and missing values. We need to drop the
    rows containing missing values or impute appropriate substitutes if there are
    any missing values. We can check for missing values using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Running the preceding code shows us there are no missing values for this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'To locate duplicates, we can run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: There are also no duplicates in the dataset. If there were any duplicated data,
    we would drop the extra rows.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to check the distribution of the target class to ensure it is
    balanced. Here, we show a histogram that indicates the target class distribution.
    We create the histogram using Seaborn’s `countplot()` method, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 3.5 – Distribution of target class in the academic success dataset](img/B16690_03_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – Distribution of target class in the academic success dataset
  prefs: []
  type: TYPE_NORMAL
- en: Although not perfectly balanced, the target distribution is not overly skewed
    to any one class, and we don’t have to perform any compensating action.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have found that our dataset is suitable for modeling (we still need
    to remap `Target`) and clean (it does not contain missing or duplicated values
    and is well balanced). We can now take a deeper look at some features, starting
    with feature correlation. The following code plots a correlation heatmap. Pairwise
    Pearson correlations are calculated using `df.corr()`. The screenshot that follows
    the snippet shows a correlation heatmap built using pairwise Pearson correlations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 3.6 – Pairwise Pearson feature correlation of the academic success
    dataset](img/B16690_03_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – Pairwise Pearson feature correlation of the academic success dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see three patterns of correlations: first-semester credits, enrollments,
    evaluations, and approvals are all correlated. First-semester and second-semester
    values for these are also correlated. These correlations imply that students tend
    to see through the year once enrolled instead of dropping out mid-semester. Although
    correlated, the correlations aren’t strong enough to consider dropping any features.'
  prefs: []
  type: TYPE_NORMAL
- en: The third correlation pattern is between `Nacionality` and `International`,
    which are strongly correlated.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The word *Nacionality* refers to *nationality*. We have retained the spelling
    from the original dataset here too for the purpose of consistency.
  prefs: []
  type: TYPE_NORMAL
- en: 'A closer look at `Nacionality` shows that almost all rows have a single value:
    the country where the dataset was collected. The strong correlation implies the
    same for `International`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows a stacked bar plot of the nationalities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 – Distribution of the ‘Nacionality’ feature, showing almost all
    rows have a single value in the academic success dataset](img/B16690_03_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – Distribution of the ‘Nacionality’ feature, showing almost all rows
    have a single value in the academic success dataset
  prefs: []
  type: TYPE_NORMAL
- en: The distribution of `'Nacionality'` and `'International'` means that they are
    not very informative (nearly all rows have the same value), so we can drop them
    from the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we notice the `''Gender''` feature. When working with gender, it’s
    always good to check for bias. We can visualize the distribution of the `''Gender''`
    feature relative to the target classes using a histogram. The results are shown
    in the screenshot that follows this code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 3.8 – Distribution of the ‘Gender’ feature in the academic success
    dataset](img/B16690_03_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 – Distribution of the ‘Gender’ feature in the academic success dataset
  prefs: []
  type: TYPE_NORMAL
- en: There is a slight bias toward female students, but not enough to warrant concern.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can now prepare our dataset for modeling. We must map our `Target` values
    to integers and drop the `Nacionality` and `International` features. We also need
    to remove the spaces in the feature names. LightGBM cannot work with spaces in
    the names; we can replace them with underscores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We train and compare four models: a LightGBM GBDT, a LightGBM DART tree, a
    LightGBM DART tree with GOSS, and a scikit-learn random forest.'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll perform parameter optimization with 5-fold cross-validation using `GridSearchCV`
    to ensure good performance for the models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code sets up the parameter optimization for the GBDT. A similar
    pattern is followed for the other models, which can be seen in the source code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Running the preceding code takes some time, but once completed, it prints the
    best parameters found along with the score of the best model.
  prefs: []
  type: TYPE_NORMAL
- en: 'After all the models are trained, we can evaluate each using F1-scoring, taking
    the mean of 5-fold cross-validation, using the best parameters found. The following
    code illustrates how to do this for the GBDT model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Jupyter notebooks for [the parameter optimization for each model are available
    in the GitHub repository: https://github.com/Pack](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3)tPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table summarizes the best parameter values found and the cross-validated
    F1 scores for each model:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **Learning** **Rate** | **Max** **Depth** | **Min Child** **Samples**
    | **N** **Estimators** | **Num** **Leaves** | **Min** **Samples Leaf** | **Min**
    **Samples Split** | **F1 score** |'
  prefs: []
  type: TYPE_TB
- en: '| GBDT | 0.1 | - | 10 | 100 | 32 | N/A | N/A | 0.716 |'
  prefs: []
  type: TYPE_TB
- en: '| DART | 0.1 | 128 | 30 | 150 | 128 | N/A | N/A | 0.703 |'
  prefs: []
  type: TYPE_TB
- en: '| DART (GOSS) | 0.1 | 128 | 30 | 150 | 128 | N/A | N/A | 0.703 |'
  prefs: []
  type: TYPE_TB
- en: '| Random Forest | N/A | N/A | N/A | 150 | N/A | 10 | 20 | 0.665 |'
  prefs: []
  type: TYPE_TB
- en: Table 3.1 – Summary of best parameters found for each model with the corresponding
    F1 scores
  prefs: []
  type: TYPE_NORMAL
- en: As we can see from the table, the LightGBM models performed much better than
    the scikit-learn random forest. Both DART models achieved nearly the same F1 score,
    with GOSS having a slightly lower F1 score (the table values are rounded to 3
    digits).
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our end-to-end example of exploring a dataset and building an
    optimized model for the dataset (using parameter grid search). We look at more
    complicated datasets in the coming chapters and delve deeper into analyzing model
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced LightGBM as a library to train boosted machines efficiently.
    We looked at where the complexity of building GBDTs comes from and the features
    in LightGBM that address them, such as histogram-based sampling, feature bundling,
    and GOSS. We also reviewed LightGBM’s most important hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: We also gave a detailed overview of using LightGBM in Python, covering both
    the LightGBM Python API and the scikit-learn API. We then built our first tuned
    models using LightGBM to predict student academic performance, utilizing cross-validation
    and grid-search-based parameter optimization.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we compare LightGBM against another popular gradient-boosting
    library, XGBoost, and DL techniques for tabular data.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '| *[**1]* | *G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye and
    T.-Y. Liu, “LightGBM: A Highly Efficient Gradient Boosting Decision Tree,” in
    Advances in Neural Information Processing* *Systems, 2017.* |'
  prefs: []
  type: TYPE_TB
- en: '| *[**2]* | *M. Mehta, R. Agrawal and J. Rissanen, “SLIQ: A fast scalable classifier
    for data mining,” in Advances in Database Technology—EDBT’96: 5th International
    Conference on Extending Database Technology Avignon, France, March 25-29, 1996
    Proceedings* *5, 1996.* |'
  prefs: []
  type: TYPE_TB
- en: '| *[**3]* | *J. Shafer, R. Agrawal, M. Mehta and others, “SPRINT: A scalable
    parallel classifier for data mining,” in* *Vldb, 1996.* |'
  prefs: []
  type: TYPE_TB
- en: '| *[**4]* | *S. Ranka and V. Singh, “CLOUDS: A decision tree classifier for
    large datasets,” in Proceedings of the 4th Knowledge Discovery and Data Mining*
    *Conference, 1998.* |'
  prefs: []
  type: TYPE_TB
- en: '| *[**5]* | *H. Shi, “Best-first decision tree* *learning,” 2007.* |'
  prefs: []
  type: TYPE_TB
- en: '| *[**6]* | *R. Kleiman and D. Page, “Aucμ: A performance metric for multi-class
    machine learning models,” in International Conference on Machine* *Learning, 2019.*
    |'
  prefs: []
  type: TYPE_TB
- en: '| *[**7]* | *V. Realinho, J. Machado, L. Baptista and M. V. Martins, Predicting
    student dropout and academic success,* *Zenodo, 2021.* |'
  prefs: []
  type: TYPE_TB
