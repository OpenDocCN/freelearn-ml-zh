- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An Overview of LightGBM in Python
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we looked at ensemble learning methods for decision
    trees. Both **bootstrap aggregation** (**bagging**) and gradient boosting were
    discussed in detail, with practical examples of how to apply the techniques in
    scikit-learn. We also showed how **gradient-boosted decision trees** (**GBDTs**)
    are slow to train and may underperform on some problems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This chapter introduces LightGBM, a gradient-boosting framework that uses tree-based
    learners. We look at the innovations and optimizations LightGBM makes to the ensemble
    learning methods. Further details and examples are given for using LightGBM practically
    via Python. Finally, the chapter includes a modeling example using LightGBM, incorporating
    more advanced techniques for model validation and parameter optimization.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: By the end of the chapter, you will have a thorough understanding of the theoretical
    and practical properties of LightGBM, allowing us to dive deeper into using LightGBM
    for data science and production systems.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'The main topics of this chapter are set out here:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Introducing LightGBM
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with LightGBM in Python
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building LightGBM models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The chapter includes examples and code excerpts illustrating how to use LightGBM
    in Python. Complete examples and instructions for setting up a suitable environment
    for this chapter are available a[t https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3)-3.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Introducing LightGBM
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LightGBM is an open source, gradient-boosting framework for tree-based ensembles
    ([https://github.com/microsoft/LightGBM](https://github.com/microsoft/LightGBM)).
    LightGBM focuses on efficiency in speed, memory usage, and improved accuracy,
    especially for problems with high dimensionality and large data sizes.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'LightGBM was first introduced in the paper *LightGBM: A Highly Efficient Gradient
    Boosting Decision* *Tree* [1].'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: The efficiency and accuracy of LightGBM are achieved via several technical and
    theoretical optimizations to the standard ensemble learning methods, particularly
    GBDTs. Additionally, LightGBM supports distributed training of ensembles with
    optimizations in network communication and support for GPU-based training of tree
    ensembles.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'LightGBM supports many **machine learning** (**ML**) applications: regression,
    binary and multiclass classification, cross-entropy loss functions, and ranking
    via LambdaRank.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: The LightGBM algorithm is also very customizable via its hyperparameters. It
    supports many metrics and features, including **Dropouts meet Multiple Additive
    Regression Trees** (**DART**), bagging (random forests), continuous training,
    multiple metrics, and early stopping.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: This section reviews the theoretical and practical optimizations LightGBM utilizes,
    including a detailed overview of the hyperparameters to control LightGBM features.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚å›é¡¾äº†LightGBMä½¿ç”¨çš„ç†è®ºå’Œå®è·µä¼˜åŒ–ï¼ŒåŒ…æ‹¬æ§åˆ¶LightGBMç‰¹å¾çš„è¶…å‚æ•°çš„è¯¦ç»†æ¦‚è¿°ã€‚
- en: LightGBM optimizations
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LightGBMä¼˜åŒ–
- en: At its core, LightGBM implements the same ensemble algorithms we discussed in
    the previous chapter. However, LightGBM applies theoretical and technical optimizations
    to improve performance and accuracy while significantly reducing memory usage.
    Next, we discuss the most significant optimizations implemented in LightGBM.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å…¶æ ¸å¿ƒï¼ŒLightGBMå®ç°äº†æˆ‘ä»¬åœ¨ä¸Šä¸€ç« ä¸­è®¨è®ºçš„ç›¸åŒé›†æˆç®—æ³•ã€‚ç„¶è€Œï¼ŒLightGBMé€šè¿‡ç†è®ºå’ŒæŠ€æœ¯çš„ä¼˜åŒ–æ¥æé«˜æ€§èƒ½å’Œå‡†ç¡®æ€§ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘å†…å­˜ä½¿ç”¨ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è®¨è®ºLightGBMä¸­å®æ–½çš„æœ€æ˜¾è‘—çš„ä¼˜åŒ–ã€‚
- en: Computational complexity in GBDTs
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GBDTä¸­çš„è®¡ç®—å¤æ‚åº¦
- en: 'First, we must understand where the inefficiency in building GBDTs stems from
    to understand how LightGBM improves the efficiency of GBDTs. The most computationally
    complex part of the GBDT algorithm is training the regression tree for each iteration.
    More specifically, finding the optimal split is very expensive. Pre-sort-based
    algorithms are among the most popular methods for finding the best splits [2],
    [3]. A naÃ¯ve approach requires the data to be sorted by feature for every decision
    node with algorithmic complexity O(#data Ã— #feature). Pre-sort-based algorithms
    sort the data once before training, which reduces the complexity when building
    a decision node to O(#data) [2]. Even with pre-sorting, the complexity is too
    high for large datasets when finding splits for decision nodes.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 'é¦–å…ˆï¼Œæˆ‘ä»¬å¿…é¡»ç†è§£æ„å»ºGBDTä¸­çš„ä½æ•ˆæ€§æ¥æºï¼Œæ‰èƒ½ç†è§£LightGBMå¦‚ä½•æé«˜GBDTçš„æ•ˆç‡ã€‚GBDTç®—æ³•ä¸­æœ€è®¡ç®—å¤æ‚çš„éƒ¨åˆ†æ˜¯æ¯æ¬¡è¿­ä»£çš„å›å½’æ ‘è®­ç»ƒã€‚æ›´å…·ä½“åœ°è¯´ï¼Œæ‰¾åˆ°æœ€ä¼˜åˆ†å‰²æ˜¯éå¸¸æ˜‚è´µçš„ã€‚åŸºäºé¢„æ’åºçš„ç®—æ³•æ˜¯å¯»æ‰¾æœ€ä½³åˆ†å‰²çš„æœ€æµè¡Œæ–¹æ³•ä¹‹ä¸€[2]ï¼Œ[3]ã€‚ä¸€ç§ç®€å•çš„æ–¹æ³•è¦æ±‚å¯¹æ¯ä¸ªå†³ç­–èŠ‚ç‚¹æŒ‰ç‰¹å¾å¯¹æ•°æ®è¿›è¡Œæ’åºï¼Œç®—æ³•å¤æ‚åº¦ä¸ºO(#data
    Ã— #feature)ã€‚åŸºäºé¢„æ’åºçš„ç®—æ³•åœ¨è®­ç»ƒå‰å¯¹æ•°æ®è¿›è¡Œä¸€æ¬¡æ’åºï¼Œè¿™é™ä½äº†æ„å»ºå†³ç­–èŠ‚ç‚¹çš„å¤æ‚åº¦åˆ°O(#data) [2]ã€‚å³ä½¿æœ‰é¢„æ’åºï¼Œå½“å¯»æ‰¾å†³ç­–èŠ‚ç‚¹çš„åˆ†å‰²æ—¶ï¼Œå¤æ‚åº¦å¯¹äºå¤§å‹æ•°æ®é›†æ¥è¯´ä»ç„¶å¤ªé«˜ã€‚'
- en: Histogram-based sampling
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åŸºäºç›´æ–¹å›¾çš„é‡‡æ ·
- en: 'An alternative approach to pre-sorting involves building histograms for continuous
    features [4]. The continuous values are added into discrete bins when building
    these **feature histograms**. Instead of using the data directly when calculating
    the splits for decision nodes, we can now use the histogram bins. Constructing
    the histograms has a complexity of O(#data). However, the complexity for building
    a decision node now reduces to O(#bins), and since the number of bins is much
    smaller than the amount of data, this significantly speeds up the process of building
    regression trees, as illustrated in the following diagram:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„æ’åºçš„å¦ä¸€ç§æ–¹æ³•æ¶‰åŠä¸ºè¿ç»­ç‰¹å¾æ„å»ºç›´æ–¹å›¾[4]ã€‚åœ¨æ„å»ºè¿™äº›**ç‰¹å¾ç›´æ–¹å›¾**æ—¶ï¼Œè¿ç»­å€¼è¢«æ·»åŠ åˆ°ç¦»æ•£çš„ç®±ä¸­ã€‚åœ¨è®¡ç®—å†³ç­–èŠ‚ç‚¹çš„åˆ†å‰²æ—¶ï¼Œæˆ‘ä»¬ä¸å†ç›´æ¥ä½¿ç”¨æ•°æ®ï¼Œè€Œæ˜¯ç°åœ¨å¯ä»¥ä½¿ç”¨ç›´æ–¹å›¾ç®±ã€‚æ„å»ºç›´æ–¹å›¾çš„å¤æ‚åº¦ä¸ºO(#data)ã€‚ç„¶è€Œï¼Œæ„å»ºå†³ç­–èŠ‚ç‚¹çš„å¤æ‚åº¦ç°åœ¨é™ä½åˆ°O(#bins)ï¼Œç”±äºç®±çš„æ•°é‡è¿œå°äºæ•°æ®é‡ï¼Œè¿™æ˜¾è‘—åŠ å¿«äº†æ„å»ºå›å½’æ ‘çš„è¿‡ç¨‹ï¼Œå¦‚ä¸‹é¢çš„å›¾æ‰€ç¤ºï¼š
- en: '![Figure 3.1 â€“ Creating feature histograms from continuous features allows
    calculating splits for decision nodes using bin boundary values instead of having
    to sample each data point, significantly reducing the algorithmâ€™s complexity since
    #bins << #data](img/B16690_03_1.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾3.1 â€“ ä»è¿ç»­ç‰¹å¾åˆ›å»ºç‰¹å¾ç›´æ–¹å›¾å…è®¸ä½¿ç”¨ç®±è¾¹ç•Œå€¼æ¥è®¡ç®—å†³ç­–èŠ‚ç‚¹çš„åˆ†å‰²ï¼Œè€Œä¸æ˜¯å¿…é¡»å¯¹æ¯ä¸ªæ•°æ®ç‚¹è¿›è¡Œé‡‡æ ·ï¼Œè¿™æ˜¾è‘—é™ä½äº†ç®—æ³•çš„å¤æ‚æ€§ï¼Œå› ä¸º#bins
    << #data](img/B16690_03_1.jpg)'
- en: 'Figure 3.1 â€“ Creating feature histograms from continuous features allows calculating
    splits for decision nodes using bin boundary values instead of having to sample
    each data point, significantly reducing the algorithmâ€™s complexity since #bins
    << #data'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾3.1 â€“ ä»è¿ç»­ç‰¹å¾åˆ›å»ºç‰¹å¾ç›´æ–¹å›¾å…è®¸ä½¿ç”¨ç®±è¾¹ç•Œå€¼æ¥è®¡ç®—å†³ç­–èŠ‚ç‚¹çš„åˆ†å‰²ï¼Œè€Œä¸æ˜¯å¿…é¡»å¯¹æ¯ä¸ªæ•°æ®ç‚¹è¿›è¡Œé‡‡æ ·ï¼Œè¿™æ˜¾è‘—é™ä½äº†ç®—æ³•çš„å¤æ‚æ€§ï¼Œå› ä¸º#bins
    << #data'
- en: A secondary optimization that stems from using histograms is â€œhistogram subtractionâ€
    for building the histograms for the leaves. Instead of calculating the histogram
    for each leaf, we can subtract the leafâ€™s neighborâ€™s histogram from the parentâ€™s
    histogram. Choosing the leaf with the smaller amount of data leads to a smaller
    O(#data) complexity for the first leaf and O(#bin) complexity for the second leaf
    due to histogram subtraction.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±ä½¿ç”¨ç›´æ–¹å›¾äº§ç”Ÿçš„äºŒçº§ä¼˜åŒ–æ˜¯â€œç›´æ–¹å›¾å‡æ³•â€ï¼Œç”¨äºæ„å»ºå¶å­çš„ç›´æ–¹å›¾ã€‚æˆ‘ä»¬ä¸éœ€è¦ä¸ºæ¯ä¸ªå¶å­è®¡ç®—ç›´æ–¹å›¾ï¼Œè€Œæ˜¯å¯ä»¥ä»çˆ¶ç›´æ–¹å›¾ä¸­å‡å»å¶å­çš„é‚»å±…ç›´æ–¹å›¾ã€‚é€‰æ‹©æ•°æ®é‡è¾ƒå°çš„å¶å­ä¼šå¯¼è‡´ç¬¬ä¸€ä¸ªå¶å­çš„O(#data)å¤æ‚åº¦è¾ƒå°ï¼Œç”±äºç›´æ–¹å›¾å‡æ³•ï¼Œç¬¬äºŒä¸ªå¶å­çš„O(#bin)å¤æ‚åº¦è¾ƒå°ã€‚
- en: 'A third optimization that LightGBM applies using histograms is to reduce the
    memory cost. Feature pre-sorting requires a supporting data structure (a dictionary)
    for each feature. No such data structures are required when building histograms,
    reducing memory costs. Further, since #bins is small, a smaller data type, such
    as `uint8_t`, can store the training data, reducing memory usage.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBMä½¿ç”¨ç›´æ–¹å›¾åº”ç”¨çš„ä¸€ç§ç¬¬ä¸‰ç§ä¼˜åŒ–æ˜¯å‡å°‘å†…å­˜æˆæœ¬ã€‚ç‰¹å¾é¢„æ’åºéœ€è¦ä¸ºæ¯ä¸ªç‰¹å¾æä¾›ä¸€ä¸ªæ”¯æŒæ•°æ®ç»“æ„ï¼ˆä¸€ä¸ªå­—å…¸ï¼‰ã€‚åœ¨æ„å»ºç›´æ–¹å›¾æ—¶ä¸éœ€è¦è¿™æ ·çš„æ•°æ®ç»“æ„ï¼Œä»è€Œé™ä½äº†å†…å­˜æˆæœ¬ã€‚æ­¤å¤–ï¼Œç”±äº#binså¾ˆå°ï¼Œå¯ä»¥ä½¿ç”¨è¾ƒå°çš„æ•°æ®ç±»å‹ï¼Œå¦‚`uint8_t`æ¥å­˜å‚¨è®­ç»ƒæ•°æ®ï¼Œä»è€Œå‡å°‘å†…å­˜ä½¿ç”¨ã€‚
- en: 'Detailed information regarding the algorithms for building feature histograms
    is available in the paper *CLOUDS: A decision tree classifier for large* *datasets*
    [4].'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºæ„å»ºç‰¹å¾ç›´æ–¹å›¾ç®—æ³•çš„è¯¦ç»†ä¿¡æ¯å¯åœ¨è®ºæ–‡ã€ŠCLOUDSï¼šç”¨äºå¤§å‹*æ•°æ®é›†*çš„å†³ç­–æ ‘åˆ†ç±»å™¨ã€‹[4]ä¸­æ‰¾åˆ°ã€‚
- en: Exclusive Feature Bundling
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç‹¬å®¶åŠŸèƒ½æ†ç»‘
- en: '**Exclusive Feature Bundling** (**EFB**) is another data-based optimization
    that LightGBM applies when working with sparse data (sparse data is pervasive
    in high-dimensional datasets). When the feature data is sparse, itâ€™s common to
    find that many features are *mutually exclusive*, signifying they never present
    non-zero values simultaneously. Combining these features into a single one is
    generally safe, given this exclusivity. EFB is illustrated in the following diagram:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç‹¬å®¶åŠŸèƒ½æ†ç»‘**ï¼ˆ**EFB**ï¼‰æ˜¯LightGBMåœ¨å¤„ç†ç¨€ç–æ•°æ®ï¼ˆç¨€ç–æ•°æ®åœ¨é«˜ç»´æ•°æ®é›†ä¸­æ™®éå­˜åœ¨ï¼‰æ—¶åº”ç”¨çš„ä¸€ç§åŸºäºæ•°æ®çš„ä¼˜åŒ–ã€‚å½“ç‰¹å¾æ•°æ®ç¨€ç–æ—¶ï¼Œé€šå¸¸ä¼šå‘ç°è®¸å¤šç‰¹å¾æ˜¯**ç›¸äº’æ’æ–¥**çš„ï¼Œè¿™æ„å‘³ç€å®ƒä»¬æ°¸è¿œä¸ä¼šåŒæ—¶å‘ˆç°éé›¶å€¼ã€‚è€ƒè™‘åˆ°è¿™ç§æ’ä»–æ€§ï¼Œå°†è¿™äº›ç‰¹å¾ç»„åˆæˆä¸€ä¸ªå•ä¸€çš„ç‰¹å¾é€šå¸¸æ˜¯å®‰å…¨çš„ã€‚EFBåœ¨ä»¥ä¸‹å›¾ä¸­å±•ç¤ºï¼š'
- en: '![Figure 3.2 â€“ Building a feature bundle from two mutually exclusive features](img/B16690_03_2.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾3.2 â€“ ä»ä¸¤ä¸ªç›¸äº’æ’æ–¥çš„ç‰¹å¾æ„å»ºç‰¹å¾æ†ç»‘](img/B16690_03_2.jpg)'
- en: Figure 3.2 â€“ Building a feature bundle from two mutually exclusive features
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾3.2 â€“ ä»ä¸¤ä¸ªç›¸äº’æ’æ–¥çš„ç‰¹å¾æ„å»ºç‰¹å¾æ†ç»‘
- en: 'Bundling mutually exclusive features allows building the same feature histograms
    as from the individual features [1]. The optimization reduces the complexity of
    building feature histograms from O(#data Ã— #feature) to O(#data Ã— #bundle). For
    datasets where there are many mutually exclusive features, this dramatically improves
    performance since # bundle â‰ª #feature. Detailed algorithms for, and proof of the
    correctness of, EFB are available in [1].'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 'å°†ç›¸äº’æ’æ–¥çš„ç‰¹å¾æ†ç»‘åœ¨ä¸€èµ·ï¼Œå¯ä»¥æ„å»ºä¸å•ä¸ªç‰¹å¾ç›¸åŒçš„ç‰¹å¾ç›´æ–¹å›¾[1]ã€‚è¿™ç§ä¼˜åŒ–å°†æ„å»ºç‰¹å¾ç›´æ–¹å›¾çš„å¤æ‚åº¦ä»O(#æ•°æ® Ã— #ç‰¹å¾)é™ä½åˆ°O(#æ•°æ® Ã—
    #æ†ç»‘)ã€‚å¯¹äºå­˜åœ¨è®¸å¤šç›¸äº’æ’æ–¥ç‰¹å¾çš„æ•°æ®åº“ï¼Œè¿™æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œå› ä¸º#æ†ç»‘è¿œå°äº#ç‰¹å¾ã€‚EFBçš„è¯¦ç»†ç®—æ³•åŠå…¶æ­£ç¡®æ€§çš„è¯æ˜å¯åœ¨[1]ä¸­æ‰¾åˆ°ã€‚'
- en: Gradient-based One-Side Sampling
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åŸºäºæ¢¯åº¦çš„å•ä¾§é‡‡æ ·
- en: A final data-based optimization available in the LightGBM framework is **Gradient-based
    One-Side Sampling** (**GOSS**) [1]. GOSS is a method of discarding training data
    samples that no longer contribute significantly to the training process, effectively
    reducing the training data size and speeding up the process.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBMæ¡†æ¶ä¸­å¯ç”¨çš„æœ€åä¸€ç§åŸºäºæ•°æ®çš„ä¼˜åŒ–æ˜¯**åŸºäºæ¢¯åº¦çš„å•ä¾§é‡‡æ ·**ï¼ˆ**GOSS**ï¼‰[1]ã€‚GOSSæ˜¯ä¸€ç§ä¸¢å¼ƒä¸å†å¯¹è®­ç»ƒè¿‡ç¨‹æœ‰æ˜¾è‘—è´¡çŒ®çš„è®­ç»ƒæ•°æ®æ ·æœ¬çš„æ–¹æ³•ï¼Œä»è€Œæœ‰æ•ˆåœ°å‡å°‘äº†è®­ç»ƒæ•°æ®çš„å¤§å°å¹¶åŠ å¿«äº†è¿‡ç¨‹ã€‚
- en: We can use the gradient calculation of each sample to determine its importance.
    If the gradient change is small, it implies that the training error was also small,
    and we can infer that the tree is well fitted to the specific data instance [1].
    One option would be to discard all instances with small gradients. However, this
    changes the distribution of the training data, reducing the treeâ€™s ability to
    generalize. GOSS is a method for choosing which instances to keep in the training
    data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ¯ä¸ªæ ·æœ¬çš„æ¢¯åº¦è®¡ç®—æ¥ç¡®å®šå…¶é‡è¦æ€§ã€‚å¦‚æœæ¢¯åº¦å˜åŒ–å¾ˆå°ï¼Œè¿™è¡¨æ˜è®­ç»ƒè¯¯å·®ä¹Ÿå¾ˆå°ï¼Œæˆ‘ä»¬å¯ä»¥æ¨æ–­å‡ºæ ‘å¯¹ç‰¹å®šæ•°æ®å®ä¾‹æ‹Ÿåˆå¾—å¾ˆå¥½[1]ã€‚ä¸€ä¸ªé€‰æ‹©æ˜¯ä¸¢å¼ƒæ‰€æœ‰æ¢¯åº¦å°çš„å®ä¾‹ã€‚ç„¶è€Œï¼Œè¿™æ”¹å˜äº†è®­ç»ƒæ•°æ®çš„åˆ†å¸ƒï¼Œå‡å°‘äº†æ ‘æ³›åŒ–çš„èƒ½åŠ›ã€‚GOSSæ˜¯ä¸€ç§é€‰æ‹©ä¿ç•™åœ¨è®­ç»ƒæ•°æ®ä¸­çš„å®ä¾‹çš„æ–¹æ³•ã€‚
- en: 'To maintain the data distribution, GOSS is applied as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä¿æŒæ•°æ®åˆ†å¸ƒï¼ŒGOSSæŒ‰ç…§ä»¥ä¸‹æ–¹å¼åº”ç”¨ï¼š
- en: The data samples are sorted by the absolute value of their gradients.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ•°æ®æ ·æœ¬æŒ‰å…¶æ¢¯åº¦çš„ç»å¯¹å€¼æ’åºã€‚
- en: The top a Ã— 100% instances are then selected (instances with large gradients).
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç„¶åé€‰æ‹©å‰a Ã— 100%çš„å®ä¾‹ï¼ˆæ¢¯åº¦å¤§çš„å®ä¾‹ï¼‰ã€‚
- en: A random sample of b Ã— 100% instances is then taken from the rest of the data.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç„¶åä»å‰©ä½™çš„æ•°æ®ä¸­éšæœºæŠ½å–b Ã— 100%çš„å®ä¾‹æ ·æœ¬ã€‚
- en: 'A factor is added to the loss function (for these instances) to amplify their
    influence: 1 âˆ’ aÂ _Â bÂ , thereby compensating for the underrepresentation of data
    with small gradients.'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨æŸå¤±å‡½æ•°ï¼ˆå¯¹äºè¿™äº›å®ä¾‹ï¼‰ä¸­æ·»åŠ ä¸€ä¸ªå› å­ä»¥æ”¾å¤§å…¶å½±å“ï¼š1 âˆ’ aÂ _Â bï¼Œä»è€Œè¡¥å¿å°æ¢¯åº¦æ•°æ®çš„ä»£è¡¨æ€§ä¸è¶³ã€‚
- en: Therefore, GOSS samples a large portion of instances with large gradients and
    a random portion of instances with small gradients and amplifies the influence
    of the small gradients when calculating information gain.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼ŒGOSSä»å…·æœ‰å¤§æ¢¯åº¦çš„å®ä¾‹ä¸­é‡‡æ ·å¤§é‡å®ä¾‹ï¼Œå¹¶ä»å…·æœ‰å°æ¢¯åº¦çš„å®ä¾‹ä¸­éšæœºé‡‡æ ·ä¸€éƒ¨åˆ†å®ä¾‹ï¼Œåœ¨è®¡ç®—ä¿¡æ¯å¢ç›Šæ—¶æ”¾å¤§å°æ¢¯åº¦çš„å½±å“ã€‚
- en: The downsampling enabled by GOSS can significantly reduce the amount of data
    processed during training (and the training time for the GBDTs), especially in
    the case of large datasets.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: GOSSå¯ç”¨çš„ä¸‹é‡‡æ ·å¯ä»¥æ˜¾è‘—å‡å°‘è®­ç»ƒè¿‡ç¨‹ä¸­å¤„ç†çš„æ•°æ®é‡ï¼ˆä»¥åŠGBDTçš„è®­ç»ƒæ—¶é—´ï¼‰ï¼Œå°¤å…¶æ˜¯åœ¨å¤§å‹æ•°æ®é›†çš„æƒ…å†µä¸‹ã€‚
- en: Best-first tree growth
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æœ€ä½³ä¼˜å…ˆæ ‘å¢é•¿
- en: The most common method for building decision trees is to grow the tree by level
    (that is, one level at a time). LightGBM uses an alternative approach and grows
    the tree leaf-wise or best-first. The leaf-wise approach selects an existing leaf
    with the most significant change in the loss of the tree and builds the tree from
    there. The downside of this approach is that if the dataset is small, the tree
    is likely to overfit the data. A maximum depth has to be set to counteract this.
    However, if the number of leaves to construct is fixed, leaf-wise tree building
    is shown to outperform level-wise algorithms [5].
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºå†³ç­–æ ‘æœ€å¸¸è§çš„æ–¹æ³•æ˜¯æŒ‰å±‚æ¬¡å¢é•¿ï¼ˆå³ï¼Œä¸€æ¬¡å¢é•¿ä¸€ä¸ªå±‚æ¬¡ï¼‰ã€‚LightGBMé‡‡ç”¨äº†ä¸€ç§æ›¿ä»£æ–¹æ³•ï¼Œé€šè¿‡å¶èŠ‚ç‚¹æˆ–æœ€ä½³ä¼˜å…ˆçš„æ–¹å¼å¢é•¿æ ‘ã€‚å¶èŠ‚ç‚¹æ–¹æ³•é€‰æ‹©å…·æœ‰æœ€å¤§æŸå¤±å˜åŒ–çš„ç°æœ‰å¶èŠ‚ç‚¹ï¼Œå¹¶ä»é‚£é‡Œæ„å»ºæ ‘ã€‚è¿™ç§æ–¹æ³•çš„ä¸€ä¸ªç¼ºç‚¹æ˜¯ï¼Œå¦‚æœæ•°æ®é›†å¾ˆå°ï¼Œæ ‘å¾ˆå¯èƒ½ä¼šè¿‡æ‹Ÿåˆæ•°æ®ã€‚å¿…é¡»è®¾ç½®æœ€å¤§æ·±åº¦æ¥æŠµæ¶ˆè¿™ä¸€ç‚¹ã€‚ç„¶è€Œï¼Œå¦‚æœæ„å»ºçš„å¶èŠ‚ç‚¹æ•°é‡æ˜¯å›ºå®šçš„ï¼Œå¶èŠ‚ç‚¹æ ‘æ„å»ºå·²è¢«è¯æ˜ä¼˜äºå±‚æ¬¡ç®—æ³•[5]ã€‚
- en: L1 and L2 regularization
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: L1 å’Œ L2 æ­£åˆ™åŒ–
- en: LightGBM supports both L1 and L2 regularization of the objective function when
    training the regression trees in the ensemble. From [*Chapter 1*](B16690_01.xhtml#_idTextAnchor014)*,
    Introducing Machine Learning*, we recall that regularization is a way to control
    overfitting. In the case of decision trees, simpler, shallow trees overfit less.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM åœ¨é›†æˆä¸­è®­ç»ƒå›å½’æ ‘æ—¶æ”¯æŒç›®æ ‡å‡½æ•°çš„ L1 å’Œ L2 æ­£åˆ™åŒ–ã€‚ä» [*ç¬¬1ç« *](B16690_01.xhtml#_idTextAnchor014)
    *ä»‹ç»æœºå™¨å­¦ä¹ * ä¸­ï¼Œæˆ‘ä»¬å›å¿†èµ·æ­£åˆ™åŒ–æ˜¯æ§åˆ¶è¿‡æ‹Ÿåˆçš„ä¸€ç§æ–¹æ³•ã€‚åœ¨å†³ç­–æ ‘çš„æƒ…å†µä¸‹ï¼Œæ›´ç®€å•ã€æ›´æµ…çš„æ ‘è¿‡æ‹Ÿåˆè¾ƒå°‘ã€‚
- en: 'To support L1 and L2 regularization, we extend the objective function with
    a regularization term, as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ”¯æŒL1å’ŒL2æ­£åˆ™åŒ–ï¼Œæˆ‘ä»¬é€šè¿‡æ·»åŠ æ­£åˆ™åŒ–é¡¹æ‰©å±•äº†ç›®æ ‡å‡½æ•°ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: obj = L(y, F(x)) + Î©(w)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: obj = L(y, F(x)) + Î©(w)
- en: Here, L(y, F(x)) is the loss function discussed in [*Chapter 2*](B16690_02.xhtml#_idTextAnchor036)*,
    Ensemble Learning â€“ Bagging and Boosting*, and Î©(w) is the regularization function
    defined over w, the leaf scores (the leaf score is the output calculated from
    the leaf as per *step 2.3* in the GBDT algorithm defined in [*Chapter 2*](B16690_02.xhtml#_idTextAnchor036)*,
    Ensemble Learning â€“ Bagging* *and Boosting*).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼ŒL(y, F(x)) æ˜¯åœ¨[*ç¬¬2ç« *](B16690_02.xhtml#_idTextAnchor036)ä¸­è®¨è®ºçš„æŸå¤±å‡½æ•°ï¼Œ*é›†æˆå­¦ä¹  â€“ Bagging
    å’Œ Boosting*ï¼Œè€Œ Î©(w) æ˜¯å®šä¹‰åœ¨ w ä¸Šçš„æ­£åˆ™åŒ–å‡½æ•°ï¼Œå³å¶å¾—åˆ†ï¼ˆå¶å¾—åˆ†æ˜¯æ ¹æ® GBDT ç®—æ³•ä¸­å®šä¹‰çš„ *æ­¥éª¤ 2.3* è®¡ç®—çš„å¶è¾“å‡ºï¼Œè¯¥ç®—æ³•åœ¨[*ç¬¬2ç« *](B16690_02.xhtml#_idTextAnchor036)ä¸­è®¨è®ºï¼Œ*é›†æˆå­¦ä¹ 
    â€“ Bagging å’Œ Boosting*ï¼‰ã€‚
- en: The regularization term effectively adds a penalty to the objective function,
    where we aim to penalize more complex trees prone to overfitting.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£åˆ™åŒ–é¡¹æœ‰æ•ˆåœ°å‘ç›®æ ‡å‡½æ•°æ·»åŠ äº†æƒ©ç½šï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æƒ©ç½šæ›´å¤æ‚çš„æ ‘ï¼Œè¿™äº›æ ‘å®¹æ˜“è¿‡æ‹Ÿåˆã€‚
- en: 'There are multiple definitions for Î©. A typical implementation for the terms
    in decision trees is this:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Î© æœ‰å¤šä¸ªå®šä¹‰ã€‚å†³ç­–æ ‘ä¸­è¿™äº›é¡¹çš„å…¸å‹å®ç°å¦‚ä¸‹ï¼š
- en: Î©(w) = Î±âˆ‘Â iÂ nÂ |wÂ i| + Î»âˆ‘Â iÂ nÂ wÂ iÂ 2
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Î©(w) = Î±âˆ‘Â iÂ nÂ |wÂ i| + Î»âˆ‘Â iÂ nÂ wÂ iÂ 2
- en: Here, Î±âˆ‘Â iÂ nÂ |wÂ i| is the L1 regularization term, controlled by the parameter
    Î±, 0 â‰¤ Î± â‰¤ 1, and Î»âˆ‘Â iÂ nÂ wÂ iÂ 2 is the L2 regularization term, controlled by the
    parameter Î».
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼ŒÎ±âˆ‘Â iÂ nÂ |wÂ i| æ˜¯ç”±å‚æ•° Î± æ§åˆ¶çš„ L1 æ­£åˆ™åŒ–é¡¹ï¼Œ0 â‰¤ Î± â‰¤ 1ï¼Œè€Œ Î»âˆ‘Â iÂ nÂ wÂ iÂ 2 æ˜¯ç”±å‚æ•° Î» æ§åˆ¶çš„ L2 æ­£åˆ™åŒ–é¡¹ã€‚
- en: L1 regularization has the effect of driving leaf scores to zero by penalizing
    leaves with large absolute outputs. *Smaller leaf outputs have a smaller effect
    on the treeâ€™s prediction, effectively simplifying* *the tree*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: L1 æ­£åˆ™åŒ–é€šè¿‡æƒ©ç½šå…·æœ‰å¤§ç»å¯¹è¾“å‡ºçš„å¶èŠ‚ç‚¹ï¼Œå°†å¶å¾—åˆ†é©±åŠ¨åˆ°é›¶ã€‚*è¾ƒå°çš„å¶è¾“å‡ºå¯¹æ ‘çš„é¢„æµ‹å½±å“è¾ƒå°ï¼Œä»è€Œæœ‰æ•ˆåœ°ç®€åŒ–äº†* *æ ‘*ã€‚
- en: L2 regularization is similar but has an outsized effect on outliersâ€™ leaves
    due to taking the square of the output.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: L2 æ­£åˆ™åŒ–ç±»ä¼¼ï¼Œä½†ç”±äºè¾“å‡ºå–å¹³æ–¹ï¼Œå¯¹å¼‚å¸¸å€¼å¶èŠ‚ç‚¹æœ‰æ›´å¤§çš„å½±å“ã€‚
- en: Finally, when larger trees are built (trees with more leaves, and therefore
    a large w vector), both sum terms for Î©(w) increase, increasing the objective
    function output. Therefore, *larger trees are penalized*, and overfitting is reduced.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œå½“æ„å»ºè¾ƒå¤§çš„æ ‘ï¼ˆå…·æœ‰æ›´å¤šå¶èŠ‚ç‚¹ï¼Œå› æ­¤å…·æœ‰è¾ƒå¤§çš„ w å‘é‡ï¼‰æ—¶ï¼ŒÎ©(w) çš„ä¸¤ä¸ªæ±‚å’Œé¡¹éƒ½ä¼šå¢åŠ ï¼Œä»è€Œå¢åŠ ç›®æ ‡å‡½æ•°çš„è¾“å‡ºã€‚å› æ­¤ï¼Œ*è¾ƒå¤§çš„æ ‘ä¼šå—åˆ°æƒ©ç½š*ï¼Œä»è€Œå‡å°‘è¿‡æ‹Ÿåˆã€‚
- en: Summary of LightGBM optimizations
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LightGBMä¼˜åŒ–æ€»ç»“
- en: 'In summary, LightGBM improves upon the standard ensemble algorithms by doing
    the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“æ¥è¯´ï¼ŒLightGBMé€šè¿‡ä»¥ä¸‹æ–¹å¼æ”¹è¿›äº†æ ‡å‡†é›†æˆç®—æ³•ï¼š
- en: Implementing histogram-based sampling of features to reduce the computational
    cost of finding optimal splits
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ç°åŸºäºç›´æ–¹å›¾çš„é‡‡æ ·ç‰¹å¾ä»¥å‡å°‘å¯»æ‰¾æœ€ä¼˜åˆ†å‰²çš„è®¡ç®—æˆæœ¬
- en: Calculating exclusive feature bundles to reduce the number of features in sparse
    datasets
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡è®¡ç®—ç‹¬å®¶ç‰¹å¾åŒ…æ¥å‡å°‘ç¨€ç–æ•°æ®é›†ä¸­çš„ç‰¹å¾æ•°é‡
- en: Applying GOSS to downsample the training data without losing accuracy
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åº”ç”¨GOSSä»¥åœ¨ä¸æŸå¤±å‡†ç¡®æ€§çš„æƒ…å†µä¸‹å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œä¸‹é‡‡æ ·
- en: Building trees leaf-wise to improve accuracy
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»¥å¶èŠ‚ç‚¹çš„æ–¹å¼æ„å»ºæ ‘ä»¥æé«˜å‡†ç¡®æ€§
- en: Overfitting can be controlled through L1 and L2 regularization and other control
    parameters
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡L1å’ŒL2æ­£åˆ™åŒ–ä»¥åŠå…¶ä»–æ§åˆ¶å‚æ•°å¯ä»¥æ§åˆ¶è¿‡æ‹Ÿåˆ
- en: In conjunction, the optimizations improve the computational performance of LightGBM
    by **orders of magnitude** (**OOM**) over the standard GBDT algorithm. Additionally,
    LightGBM is implemented in C++ with a Python interface, which results in much
    faster code than Python-based GBDTs, such as in scikit-learn.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“åˆä¼˜åŒ–ï¼Œè¿™äº›ä¼˜åŒ–å°†LightGBMçš„è®¡ç®—æ€§èƒ½æé«˜äº†ä¸æ ‡å‡†GBDTç®—æ³•ç›¸æ¯”çš„**æ•°é‡çº§**ï¼ˆ**OOM**ï¼‰ã€‚æ­¤å¤–ï¼ŒLightGBMæ˜¯ç”¨C++å®ç°çš„ï¼Œå…·æœ‰Pythonæ¥å£ï¼Œè¿™ä½¿å¾—ä»£ç æ¯”åŸºäºPythonçš„GBDTï¼ˆå¦‚scikit-learnï¼‰å¿«å¾—å¤šã€‚
- en: Finally, LightGBM also has support for improved data-parallel and feature-parallel
    distributed training. Distributed training and GPU support are discussed in a
    later [*Chapter 11*](B16690_11.xhtml#_idTextAnchor177)*, Distributed and GPU-Based
    Learning* *with LightGBM*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼ŒLightGBMè¿˜æ”¯æŒæ”¹è¿›çš„æ•°æ®å¹¶è¡Œå’Œç‰¹å¾å¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒã€‚åˆ†å¸ƒå¼è®­ç»ƒå’ŒGPUæ”¯æŒå°†åœ¨åé¢çš„[*ç¬¬11ç« *](B16690_11.xhtml#_idTextAnchor177)*ï¼Œä½¿ç”¨LightGBMçš„åˆ†å¸ƒå¼å’ŒåŸºäºGPUçš„å­¦ä¹ *ä¸­è®¨è®ºã€‚
- en: Hyperparameters
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¶…å‚æ•°
- en: LightGBM exposes many parameters that can be used to customize the training
    process, goals, and performance. Next, we discuss the most notable parameters
    and how they may be used to control specific phenomena.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBMå…¬å¼€äº†è®¸å¤šå‚æ•°ï¼Œå¯ç”¨äºè‡ªå®šä¹‰è®­ç»ƒè¿‡ç¨‹ã€ç›®æ ‡å’Œæ€§èƒ½ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è®¨è®ºæœ€æ˜¾è‘—çš„å‚æ•°ä»¥åŠå®ƒä»¬å¦‚ä½•ç”¨äºæ§åˆ¶ç‰¹å®šç°è±¡ã€‚
- en: Note
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: The core LightGBM framework is developed in C++ but includes APIs to work with
    LightGBM in C, Python, and R. The parameters discussed in this section are the
    framework parameters and are exposed differently by each API. The following section
    discusses the parameters available when using Python.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¸å¿ƒLightGBMæ¡†æ¶æ˜¯ç”¨C++å¼€å‘çš„ï¼Œä½†åŒ…æ‹¬ç”¨äºåœ¨Cã€Pythonå’ŒRä¸­ä¸LightGBMä¸€èµ·å·¥ä½œçš„APIã€‚æœ¬èŠ‚è®¨è®ºçš„å‚æ•°æ˜¯æ¡†æ¶å‚æ•°ï¼Œå¹¶ä¸”æ¯ä¸ªAPIä»¥ä¸åŒçš„æ–¹å¼æš´éœ²ã€‚ä»¥ä¸‹ç« èŠ‚å°†è®¨è®ºä½¿ç”¨Pythonæ—¶å¯ç”¨å‚æ•°ã€‚
- en: 'The following are **core framework parameters** used to control the optimization
    process and goal:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ç”¨ä»¥æ§åˆ¶ä¼˜åŒ–è¿‡ç¨‹å’Œç›®æ ‡çš„**æ ¸å¿ƒæ¡†æ¶å‚æ•°**ï¼š
- en: '`objective`: LightGBM supports the following optimization objectives, among
    othersâ€”`regression` (including regression applications with other loss functions
    such as Huber and Fair), `binary` (classification), `multiclass` (classification),
    `cross-entropy`, and `lambdarank` for ranking problems.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ç›®æ ‡`ï¼šLightGBMæ”¯æŒä»¥ä¸‹ä¼˜åŒ–ç›®æ ‡ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºâ€”â€”`å›å½’`ï¼ˆåŒ…æ‹¬å…·æœ‰Huberå’ŒFairç­‰æŸå¤±å‡½æ•°çš„å›å½’åº”ç”¨ï¼‰ï¼Œ`äºŒå…ƒ`ï¼ˆåˆ†ç±»ï¼‰ï¼Œ`å¤šç±»`ï¼ˆåˆ†ç±»ï¼‰ï¼Œ`äº¤å‰ç†µ`ï¼Œä»¥åŠç”¨äºæ’åºé—®é¢˜çš„`lambdarank`ã€‚'
- en: '`boosting`: The boosting parameter controls the boosting type. By default,
    this is set to `gbdt`, the standard GBDT algorithm. The other options are `dart`
    and `rf` for random forests. The random forest mode does not perform boosting
    but instead builds a random forest.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`boosting`ï¼šæå‡å‚æ•°æ§åˆ¶æå‡ç±»å‹ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæ­¤å‚æ•°è®¾ç½®ä¸º`gbdt`ï¼Œå³æ ‡å‡†GBDTç®—æ³•ã€‚å…¶ä»–é€‰é¡¹æ˜¯`dart`å’Œ`rf`ï¼Œç”¨äºéšæœºæ£®æ—ã€‚éšæœºæ£®æ—æ¨¡å¼ä¸æ‰§è¡Œæå‡ï¼Œè€Œæ˜¯æ„å»ºéšæœºæ£®æ—ã€‚'
- en: '`num_iterations` (or `n_estimators`): Controls the number of boosting iterations
    and, therefore, the number of trees built.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_iterations`ï¼ˆæˆ–`n_estimators`ï¼‰ï¼šæ§åˆ¶æå‡è¿­ä»£æ¬¡æ•°ï¼Œå› æ­¤ä¹Ÿæ§åˆ¶æ„å»ºçš„æ ‘çš„æ•°é‡ã€‚'
- en: '`num_leaves`: Controls the maximum number of leaves in a single tree.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_leaves`ï¼šæ§åˆ¶å•ä¸ªæ ‘ä¸­çš„æœ€å¤§å¶èŠ‚ç‚¹æ•°ã€‚'
- en: '`learning_rate`: Controls the learning, or shrinkage rate, which is the contribution
    of each tree to the overall prediction.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate`ï¼šæ§åˆ¶å­¦ä¹ æˆ–æ”¶ç¼©ç‡ï¼Œå³æ¯ä¸ªæ ‘å¯¹æ•´ä½“é¢„æµ‹çš„è´¡çŒ®ã€‚'
- en: LightGBM also provides many parameters to control the learning process. Weâ€™ll
    discuss these parameters relative to how they may be used to tune specific aspects
    of training.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBMè¿˜æä¾›äº†è®¸å¤šå‚æ•°æ¥æ§åˆ¶å­¦ä¹ è¿‡ç¨‹ã€‚æˆ‘ä»¬å°†è®¨è®ºè¿™äº›å‚æ•°ç›¸å¯¹äºå®ƒä»¬å¦‚ä½•ç”¨äºè°ƒæ•´è®­ç»ƒçš„ç‰¹å®šæ–¹é¢ã€‚
- en: 'The following control parameters can be used to improve **accuracy**:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ§åˆ¶å‚æ•°å¯ç”¨äºæé«˜**å‡†ç¡®æ€§**ï¼š
- en: '`boosting`: Use `dart`, which has been shown to outperform standard GBDTs.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`boosting`ï¼šä½¿ç”¨`dart`ï¼Œè¿™å·²è¢«è¯æ˜ä¼˜äºæ ‡å‡†GBDTã€‚'
- en: '`learning_rate`: The learning rate must be tuned alongside `num_iterations`
    for better accuracy. A small learning rate with a large value for `num_iterations`
    leads to better accuracy at the expense of optimization speed.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_leaves`: A larger number of leaves improves accuracy but may lead to overfitting.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_bin`: The maximum number of bins in which features are bucketed when constructing
    histograms. A larger `max_bin` size slows the training and uses more memory but
    may improve accuracy.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following **learning control parameters** can be used to deal with **overfitting**:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '`bagging_fraction` and `bagging_freq`: Setting both parameters enables feature
    bagging. Bagging may be used in addition to boosting and doesnâ€™t force the use
    of a random forest. Enabling bagging reduces overfitting.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`early_stopping_round`: Enables early stopping and controls the number of iterations
    used to determine whether training should be stopped. Training is stopped if no
    improvement is made to any metric in the iterations set by `early_stopping_round`.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_data_in_leaf`: The minimum samples allowed in a leaf. Larger values reduce
    overfitting.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_gain_to_split`: The minimum amount of information gain required to perform
    a split. Higher values reduce overfitting.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reg_alpha`: Controls L1 regularization. Higher values reduce overfitting.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reg_lambda`: Controls L2 regularization. Higher values reduce overfitting.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_depth`: Controls the maximum depth of individual trees. Shallower trees
    reduce overfitting.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_drop`: Controls the maximum number of dropped trees when using the DART
    algorithm (is only used when `boosting` is set to `dart`). A larger value reduces
    overfitting.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`extra_trees`: Enables the **Extremely Randomized Trees** (**ExtraTrees**)
    algorithm. LightGBM then chooses a split threshold at random for each feature.
    Enabling Extra-Trees can reduce overfitting. The parameter can be used in conjunction
    with any boosting mode.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The parameters discussed here include only some of the parameters available
    in LightGBM and focus on improving accuracy and overfitting. A complete list of
    parameters [is available at the following link: https://lightgbm.rea](https://lightgbm.readthedocs.io/en/latest/Parameters.xhtml)dthedocs.io/en/latest/Parameters.xhtml.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of LightGBM
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LightGBM is designed to be more efficient and effective than traditional methods.
    It is particularly well known for its ability to handle large datasets. However,
    as with any algorithm or framework, it also has its limitations and potential
    disadvantages, including the following:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '**Sensitive to overfitting**: LightGBM can be sensitive to overfitting, especially
    with small or noisy datasets. Care should be taken to monitor and control for
    overfitting when using LightGBM.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimal performance requires tuning**: As discussed previously, LightGBM
    has many hyperparameters that need to be properly tuned to get the best performance
    from the algorithm.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lack of representation learning**: Unlike **deep learning** (**DL**) approaches,
    which excel at learning from raw data, LightGBM requires feature engineering to
    be applied to the data before learning. Feature engineering is a time-consuming
    process that requires domain knowledge.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç¼ºä¹è¡¨ç¤ºå­¦ä¹ **ï¼šä¸æ“…é•¿ä»åŸå§‹æ•°æ®ä¸­å­¦ä¹ çš„ **æ·±åº¦å­¦ä¹ **ï¼ˆ**DL**ï¼‰æ–¹æ³•ä¸åŒï¼ŒLightGBM åœ¨å­¦ä¹ ä¹‹å‰éœ€è¦åº”ç”¨ç‰¹å¾å·¥ç¨‹åˆ°æ•°æ®ä¸Šã€‚ç‰¹å¾å·¥ç¨‹æ˜¯ä¸€ä¸ªè€—æ—¶ä¸”éœ€è¦é¢†åŸŸçŸ¥è¯†çš„è¿‡ç¨‹ã€‚'
- en: '**Handling sequential data**: LightGBM is not inherently designed for working
    with sequential data such as time series. For LightGBM to be used with time-series
    data, feature engineering needs to be applied to create lagged features and capture
    temporal dependencies.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¤„ç†åºåˆ—æ•°æ®**ï¼šLightGBM æœ¬èº«å¹¶ä¸æ˜¯ä¸ºå¤„ç†åºåˆ—æ•°æ®ï¼ˆå¦‚æ—¶é—´åºåˆ—ï¼‰è€Œè®¾è®¡çš„ã€‚ä¸ºäº†ä½¿ç”¨ LightGBM å¤„ç†æ—¶é—´åºåˆ—æ•°æ®ï¼Œéœ€è¦åº”ç”¨ç‰¹å¾å·¥ç¨‹æ¥åˆ›å»ºæ»åç‰¹å¾å¹¶æ•æ‰æ—¶é—´ä¾èµ–æ€§ã€‚'
- en: '**Complex interactions and non-linearities**: LightGBM is a decision-tree-driven
    approach that might be incapable of capturing complex feature interactions and
    non-linearities. Proper feature engineering needs to be applied to ensure the
    algorithm models these.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¤æ‚äº¤äº’å’Œéçº¿æ€§**ï¼šLightGBM æ˜¯ä¸€ç§ä»¥å†³ç­–æ ‘ä¸ºé©±åŠ¨çš„æ–¹æ³•ï¼Œå¯èƒ½æ— æ³•æ•æ‰å¤æ‚çš„ç‰¹å¾äº¤äº’å’Œéçº¿æ€§ã€‚éœ€è¦åº”ç”¨é€‚å½“çš„ç‰¹å¾å·¥ç¨‹æ¥ç¡®ä¿ç®—æ³•èƒ½å¤Ÿå»ºæ¨¡è¿™äº›ã€‚'
- en: Although these are potential limitations of using the algorithm, they may not
    apply to all use cases. LightGBM is often a very effective tool in the right circumstances.
    As with any model, understanding the trade-offs is vital to making the right choice
    for your application.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶è¿™äº›æ˜¯ä½¿ç”¨è¯¥ç®—æ³•çš„æ½œåœ¨å±€é™æ€§ï¼Œä½†å®ƒä»¬å¯èƒ½å¹¶ä¸é€‚ç”¨äºæ‰€æœ‰ç”¨ä¾‹ã€‚åœ¨é€‚å½“çš„æƒ…å¢ƒä¸‹ï¼ŒLightGBM ç»å¸¸æ˜¯ä¸€ä¸ªéå¸¸æœ‰æ•ˆçš„å·¥å…·ã€‚ä¸ä»»ä½•æ¨¡å‹ä¸€æ ·ï¼Œç†è§£æƒè¡¡å¯¹äºä¸ºæ‚¨çš„åº”ç”¨ç¨‹åºåšå‡ºæ­£ç¡®çš„é€‰æ‹©è‡³å…³é‡è¦ã€‚
- en: In the next session, we look at getting started using the various LightGBM APIs
    with Python.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨å¦‚ä½•ä½¿ç”¨ Python çš„å„ç§ LightGBM API å¼€å§‹ä½¿ç”¨ã€‚
- en: Getting started with LightGBM in Python
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åœ¨ Python ä¸­å¼€å§‹ä½¿ç”¨ LightGBM
- en: 'LightGBM is implemented in C++ but has official C, R, and Python APIs. This
    section discusses the Python APIs that are available for working with LightGBM.
    LightGBM provides three Python APIs: the standard **LightGBM** API, the **scikit-learn**
    API (which is fully compatible with other scikit-learn functionality), and a **Dask**
    API for working with Dask. Dask is a parallel computing library discussed in [*Chapter
    11*](B16690_11.xhtml#_idTextAnchor177)*, Distribu*[*ted and GPU-Based Lea*](https://www.dask.org/)*rning
    with* *LightGBM* ([https://www.dask.org/](https://www.dask.org/)).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM ä½¿ç”¨ C++ å®ç°ï¼Œä½†æä¾›äº†å®˜æ–¹çš„ Cã€R å’Œ Python APIã€‚æœ¬èŠ‚è®¨è®ºå¯ç”¨äºä¸ LightGBM ä¸€èµ·å·¥ä½œçš„ Python APIã€‚LightGBM
    æä¾›äº†ä¸‰ä¸ª Python APIï¼šæ ‡å‡†çš„ **LightGBM** APIã€ä¸å…¶å®ƒ scikit-learn åŠŸèƒ½å®Œå…¨å…¼å®¹çš„ **scikit-learn**
    APIï¼Œä»¥åŠç”¨äºä¸ Dask ä¸€èµ·å·¥ä½œçš„ **Dask** APIã€‚Dask æ˜¯åœ¨ç¬¬ [*ç¬¬ 11 ç« *](B16690_11.xhtml#_idTextAnchor177)*
    ä¸­è®¨è®ºçš„å¹¶è¡Œè®¡ç®—åº“ï¼Œ*åˆ†å¸ƒå¼å’ŒåŸºäº GPU çš„å­¦ä¹ * ([https://www.dask.org/](https://www.dask.org/))ã€‚
- en: Throughout the rest of the book, we mainly use the scikit-learn API for LightGBM,
    but letâ€™s first look at the standard Python API.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ä¹¦çš„å…¶ä½™éƒ¨åˆ†ï¼Œæˆ‘ä»¬ä¸»è¦ä½¿ç”¨ LightGBM çš„ scikit-learn APIï¼Œä½†è®©æˆ‘ä»¬é¦–å…ˆçœ‹çœ‹æ ‡å‡†çš„ Python APIã€‚
- en: LightGBM Python API
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LightGBM Python API
- en: The best way to dive into the Python API is with a hands-on example. The following
    are excerpts from a code listing that illustrates the use of the LightGBM Python
    API. The complete code example is available at [https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±å…¥äº†è§£ Python API çš„æœ€ä½³æ–¹å¼æ˜¯é€šè¿‡åŠ¨æ‰‹ç¤ºä¾‹ã€‚ä»¥ä¸‹æ˜¯ä»ä»£ç åˆ—è¡¨ä¸­æ‘˜å½•çš„ç‰‡æ®µï¼Œè¯´æ˜äº† LightGBM Python API çš„ä½¿ç”¨ã€‚å®Œæ•´çš„ä»£ç ç¤ºä¾‹å¯åœ¨
    [https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3)
    æ‰¾åˆ°ã€‚
- en: 'LightGBM needs to be imported. The import is often abbreviated as `lgb`:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: éœ€è¦å¯¼å…¥ LightGBMã€‚å¯¼å…¥é€šå¸¸ç®€å†™ä¸º `lgb`ï¼š
- en: '[PRE0]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: LightGBM provides a `Dataset` wrapper class to work with data. `Dataset` supports
    a variety of formats. Commonly, it is used to wrap a `numpy` array or a `pandas`
    DataFrame. `Dataset` also accepts a `Path` to a CSV, TSV, LIBSVM text file, or
    LightGBM `Dataset` binary file. When a path is supplied, LightGBM loads the data
    from the disk.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM æä¾›äº†ä¸€ä¸ª `Dataset` åŒ…è£…ç±»æ¥å¤„ç†æ•°æ®ã€‚`Dataset` æ”¯æŒå¤šç§æ ¼å¼ã€‚é€šå¸¸ï¼Œå®ƒç”¨äºåŒ…è£… `numpy` æ•°ç»„æˆ– `pandas`
    DataFrameã€‚`Dataset` è¿˜æ¥å— CSVã€TSVã€LIBSVM æ–‡æœ¬æ–‡ä»¶æˆ– LightGBM `Dataset` äºŒè¿›åˆ¶æ–‡ä»¶çš„ `Path`ã€‚å½“æä¾›è·¯å¾„æ—¶ï¼ŒLightGBM
    ä¼šä»ç£ç›˜åŠ è½½æ•°æ®ã€‚
- en: 'Here, we load our Forest Cover dataset from `sklearn` and wrap the `numpy`
    arrays in a LightGBM `Dataset`:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä» `sklearn` åŠ è½½æˆ‘ä»¬çš„ Forest Cover æ•°æ®é›†ï¼Œå¹¶å°† `numpy` æ•°ç»„åŒ…è£…åœ¨ LightGBM çš„ `Dataset`
    ä¸­ï¼š
- en: '[PRE1]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We subtract 1 from the `y_train` and `y_test` arrays because the classes supplied
    by `sklearn` are labeled in the range [1, 7], whereas LightGBM expects zero-indexed
    class labels in the range [0, 7].
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»`y_train`å’Œ`y_test`æ•°ç»„ä¸­å‡å»1ï¼Œå› ä¸º`sklearn`æä¾›çš„ç±»åˆ«æ ‡ç­¾åœ¨èŒƒå›´[1, 7]å†…ï¼Œè€ŒLightGBMæœŸæœ›é›¶ç´¢å¼•çš„ç±»åˆ«æ ‡ç­¾åœ¨èŒƒå›´[0,
    7]å†…ã€‚
- en: 'We cannot set up the parameters for training. Weâ€™ll be using the following
    parameters:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ— æ³•è®¾ç½®è®­ç»ƒçš„å‚æ•°ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹å‚æ•°ï¼š
- en: '[PRE2]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We are using the standard GBDT as a boosting type and setting the objective
    to multiclass classification for seven classes. During training, we are going
    to capture the `auc_mu` metric. AU CÂ Î¼ is a multiclass adaptation of the **area
    under the receiver operating characteristic curve** (**AUC**), as defined by Kleiman
    and Page [6].
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨æ ‡å‡†çš„GBDTä½œä¸ºæå‡ç±»å‹ï¼Œå¹¶å°†ç›®æ ‡è®¾ç½®ä¸ºä¸ƒç±»çš„å¤šåˆ†ç±»ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†æ•è·`auc_mu`æŒ‡æ ‡ã€‚AU CÂ Î¼æ˜¯å¤šç±»ç‰ˆæœ¬çš„**å—è¯•è€…å·¥ä½œç‰¹å¾æ›²çº¿ä¸‹é¢ç§¯**ï¼ˆ**AUC**ï¼‰ï¼Œå¦‚Kleimanå’ŒPage
    [6]æ‰€å®šä¹‰ã€‚
- en: We set `num_leaves` and `learning_rate` to reasonable values for the problem.
    Finally, we specify `force_row_wise` as `True`, a recommended setting for large
    datasets.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†`num_leaves`å’Œ`learning_rate`è®¾ç½®ä¸ºé€‚åˆè¯¥é—®é¢˜çš„åˆç†å€¼ã€‚æœ€åï¼Œæˆ‘ä»¬æŒ‡å®š`force_row_wise`ä¸º`True`ï¼Œè¿™æ˜¯å¤§å‹æ•°æ®é›†çš„ä¸€ä¸ªæ¨èè®¾ç½®ã€‚
- en: 'LightGBMâ€™s training function also supports **callbacks**. A callback is a hook
    into the training process that is executed each boosting iteration. To illustrate
    their purpose, weâ€™ll be using the following callbacks:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBMçš„è®­ç»ƒå‡½æ•°ä¹Ÿæ”¯æŒ**å›è°ƒ**ã€‚å›è°ƒæ˜¯è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸€ä¸ªé’©å­ï¼Œåœ¨æ¯ä¸ªæå‡è¿­ä»£ä¸­æ‰§è¡Œã€‚ä¸ºäº†è¯´æ˜å®ƒä»¬çš„ç›®çš„ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹å›è°ƒï¼š
- en: '[PRE3]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We use the `log_evaluation` callback with a period of 15, which logs (prints)
    our metrics to standard output every 15 boosting iterations. We also set a `record_evaluation`
    callback that captures our evaluation metrics in the `metrics` dictionary. We
    also specify an `early_stopping` callback, with stopping rounds set to 15\. The
    `early_stopping` callback stops training if no validation metrics improve after
    the specified number of stopping rounds.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨`log_evaluation`å›è°ƒï¼Œå‘¨æœŸä¸º15ï¼Œå®ƒæ¯15æ¬¡æå‡è¿­ä»£å°†æˆ‘ä»¬çš„æŒ‡æ ‡è®°å½•ï¼ˆæ‰“å°ï¼‰åˆ°æ ‡å‡†è¾“å‡ºã€‚æˆ‘ä»¬è¿˜è®¾ç½®äº†ä¸€ä¸ª`record_evaluation`å›è°ƒï¼Œå®ƒå°†æˆ‘ä»¬çš„è¯„ä¼°æŒ‡æ ‡æ•è·åœ¨`metrics`å­—å…¸ä¸­ã€‚æˆ‘ä»¬è¿˜æŒ‡å®šäº†ä¸€ä¸ª`early_stopping`å›è°ƒï¼Œåœæ­¢è½®æ¬¡è®¾ç½®ä¸º15ã€‚å¦‚æœç»è¿‡æŒ‡å®šçš„åœæ­¢è½®æ¬¡åæ²¡æœ‰éªŒè¯æŒ‡æ ‡æ”¹è¿›ï¼Œ`early_stopping`å›è°ƒå°†åœæ­¢è®­ç»ƒã€‚
- en: 'Finally, we also use the `reset_parameter` callback to implement **learning
    rate decay**. The decay function is defined as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨`reset_parameter`å›è°ƒæ¥å®ç°**å­¦ä¹ ç‡è¡°å‡**ã€‚è¡°å‡å‡½æ•°å®šä¹‰å¦‚ä¸‹ï¼š
- en: '[PRE4]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `reset_parameter` callback takes a function as input. The function receives
    the current iteration and returns the parameter value. Learning rate decay is
    a technique where we decrease the value of the learning rate over time. Learning
    rate decay improved the overall accuracy achieved. Ideally, we want the initial
    trees to have a more significant impact on correcting the prediction errors. In
    contrast, later on, we want to reduce the impact of additional trees and have
    them make minor adjustments to the errors. We implement a slight exponential decay
    that reduces the learning rate from 0.09 to 0.078 throughout training.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`reset_parameter`å›è°ƒæ¥å—ä¸€ä¸ªå‡½æ•°ä½œä¸ºè¾“å…¥ã€‚è¯¥å‡½æ•°æ¥æ”¶å½“å‰è¿­ä»£æ¬¡æ•°å¹¶è¿”å›å‚æ•°å€¼ã€‚å­¦ä¹ ç‡è¡°å‡æ˜¯ä¸€ç§æŠ€æœ¯ï¼Œéšç€æ—¶é—´çš„æ¨ç§»é™ä½å­¦ä¹ ç‡çš„å€¼ã€‚å­¦ä¹ ç‡è¡°å‡æé«˜äº†æ•´ä½“å‡†ç¡®åº¦ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›åˆå§‹æ ‘å¯¹çº æ­£é¢„æµ‹é”™è¯¯æœ‰æ›´å¤§çš„å½±å“ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåæœŸæˆ‘ä»¬å¸Œæœ›å‡å°‘é¢å¤–æ ‘çš„å½±å“ï¼Œå¹¶è®©å®ƒä»¬å¯¹é”™è¯¯è¿›è¡Œå¾®å°è°ƒæ•´ã€‚æˆ‘ä»¬åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­å®æ–½äº†ä¸€ç§è½»å¾®çš„æŒ‡æ•°è¡°å‡ï¼Œå°†å­¦ä¹ ç‡ä»0.09é™ä½åˆ°0.078ã€‚'
- en: 'Now, we are ready for training. We use `lgb.train` to train the model:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å·²ç»å‡†å¤‡å¥½è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬ä½¿ç”¨`lgb.train`æ¥è®­ç»ƒæ¨¡å‹ï¼š
- en: '[PRE5]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We use 150 boosting rounds (or boosted trees). In conjunction with a lower learning
    rate, having many boosting rounds should improve accuracy.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨150æ¬¡æå‡è½®æ¬¡ï¼ˆæˆ–æå‡æ ‘ï¼‰ã€‚ç»“åˆè¾ƒä½çš„å­¦ä¹ ç‡ï¼Œæ‹¥æœ‰è®¸å¤šæå‡è½®æ¬¡åº”è¯¥å¯ä»¥æé«˜å‡†ç¡®åº¦ã€‚
- en: 'After training, we can use `lgb.predict` to get predictions for our test set
    and calculate the F1 score:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`lgb.predict`æ¥è·å–æµ‹è¯•é›†çš„é¢„æµ‹å¹¶è®¡ç®—F1åˆ†æ•°ï¼š
- en: '[PRE6]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The LightGBM predict function outputs an array of activations, one for each
    class. Therefore, we use `np.argmax` to choose the class with the highest activation
    as the predicted class. LightGBM also has support for some plotting functions.
    For instance, we can use `plot_metric` to plot our AU CÂ Î¼ results as captured
    in the `metrics`:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBMçš„é¢„æµ‹å‡½æ•°è¾“å‡ºä¸€ä¸ªæ¿€æ´»æ•°ç»„ï¼Œæ¯ä¸ªç±»åˆ«ä¸€ä¸ªã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨`np.argmax`æ¥é€‰æ‹©å…·æœ‰æœ€é«˜æ¿€æ´»çš„ç±»åˆ«ä½œä¸ºé¢„æµ‹ç±»åˆ«ã€‚LightGBMä¹Ÿæ”¯æŒä¸€äº›ç»˜å›¾å‡½æ•°ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`plot_metric`æ¥ç»˜åˆ¶æˆ‘ä»¬åœ¨`metrics`ä¸­æ•è·çš„AU
    CÂ Î¼ç»“æœï¼š
- en: '[PRE7]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The results of this are shown in *Figure 3**.3*.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ç»“æœåœ¨*å›¾3**.3*ä¸­æ˜¾ç¤ºã€‚
- en: '![Figure 3.3 â€“ A plot of the â€‹AUâ€‰â€‹Câ€‹â€¯<?AID d835?><?AID df41?>â€‹â€‹â€‹ metric per
    training iteration created using lgb.plot_metric](img/B16690_03_3.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 â€“ A plot of the AU CÂ ğ metric per training iteration created using
    lgb.plot_metric
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the preceding code should produce a LightGBM GBDT tree with an F1 score
    of around 0.917, in line with the score the Random Forest and Extra-Trees algorithms
    achieved in [*Chapter 2*](B16690_02.xhtml#_idTextAnchor036)*, Ensemble Learning
    â€“ Bagging and Boosting*. However, LightGBM is significantly faster in reaching
    these accuracies. LightGBM completed the training in just 37 seconds on our hardware:
    this is 4.5 times faster than running Extra-Trees on the same problem and hardware
    and 60-70 times faster than scikit-learnâ€™s `GradientBoostingClassifier` in our
    testing.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: LightGBM scikit-learn API
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We now take a look at the scikit-learn Python API for LightGBM. The scikit-learn
    API provides four classes: `LGBMModel`, `LGBMClassifier`, `LGBMRegressor`, and
    `LGBMRanker`. Each of these provides the same functionality as the LightGBM Python
    API, but with the same convenient scikit-learn interfaces we have worked with
    before. Additionally, the scikit-learn classes are compatible and interoperable
    with the rest of the scikit-learn ecosystem.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s replicate the previous example using the scikit-learn API.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset is loaded precisely as before. The scikit-learn API doesnâ€™t require
    wrapping the data in a `Dataset` object. We also donâ€™t have to zero-index our
    target classes, as scikit-learn supports any label for the classes:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The scikit-learn API also supports LightGBM callbacks; as such, we use the
    same callbacks as before:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We then create the `LGBMClassifier` exactly as we would any other scikit-learn
    model. When creating the classifier, we also set the parameters:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Note that we do not have to specify the number of classes; scikit-learn infers
    this automatically. We then call `fit` on the model, passing the training and
    test data along with our callbacks:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, we evaluate our model with the F1 score. We donâ€™t have to use `np.argmax`
    on the predictions as this is done automatically with the scikit-learn API:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Overall, we can see that using LightGBM via the scikit-learn API is more straightforward
    than the standard Python API. The scikit-learn API was also approximately 40%
    faster than the LightGBM API on our hardware. This section examined the ins and
    outs of using the various Python APIs available for LightGBM. The following section
    looks at training LightGBM models using the scikit-learn API.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Building LightGBM models
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section provides an end-to-end example of solving a real-world problem
    using LightGBM. We provide a more detailed look at data preparation for a problem
    and explain how to find suitable parameters for our algorithms. We use multiple
    variants of LightGBM to explore relative performance and compare them against
    random forests.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we delve into solving a problem, we need to discuss a better way of validating
    algorithm performance. Splitting the data into two or three subsets is standard
    practice when training a model. The training data is used to train the model,
    the validation data is a hold-out set used to validate the data during training,
    and the test data is used to validate the performance after training.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: In previous examples, we have done this split only once, building a single training
    and test to train and validate the model. The issue with this approach is that
    our model could get â€œlucky.â€ If, by chance, our test set closely matches the training
    data but is not representative of real-world data, we would report a good test
    error, even though we canâ€™t be confident of our modelâ€™s performance.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: An alternative is to do the dataset splitting multiple times and train the model
    multiple times, once for each split. This approach is called **cross-validation**.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common application of cross-validation is *k-fold cross-validation*.
    With k-fold cross-validation, we choose a value, *k*, and partition the (shuffled)
    dataset into *k* subsamples (or folds). We then repeat the training process *k*
    times, using a different subset as the validation data and all other subsets as
    training data. The modelâ€™s performance is calculated as the mean (or median) score
    across all folds. The following diagram illustrates this process:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 â€“ k-fold cross-validation with k = 3; the original dataset is
    shuffled and split into 3 equal parts (or folds); training and validation are
    repeated for each combination of subsampled data, and the average performance
    is reported](img/B16690_03_4.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 â€“ k-fold cross-validation with k = 3; the original dataset is shuffled
    and split into 3 equal parts (or folds); training and validation are repeated
    for each combination of subsampled data, and the average performance is reported
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Using a high value for *k* reduces the chance that the model coincidentally
    shows good performance and indicates how the model might perform in the real world.
    However, the entire training process is repeated for each fold, which could be
    computationally expensive and time-consuming. Therefore, we need to balance the
    resources available with the need to validate the model. A typical value for *k*
    is 5 (the default for scikit-learn), also called 5-fold cross-validation.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Stratified k-fold validation
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A problem that might arise with k-fold cross-validation is that, due to chance,
    a fold may contain samples from only a single class. **Stratified sampling** solves
    this issue by preserving the percentage of samples for each class when creating
    folds. In this way, each fold has the same distribution of classes as the original
    dataset. When applied to cross-validation, this technique is called stratified
    k-fold cross-validation.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Parameter optimization
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Parameter optimization**, also called parameter tuning, is the process of
    finding good hyperparameters for the model and training process specific to the
    problem being solved. In the previous examples of training models, we have been
    setting the model and training algorithmâ€™s parameters based on intuition and minimal
    experimentation. There is no guarantee that the parameter choices were optimal
    for the optimization problem.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: But how might we go about finding the best parameter choices? A naÃ¯ve strategy
    is to try an extensive range of values for a parameter, find the best value, and
    then repeat the process for the following parameter. However, it is frequently
    the case that parameters are **co-dependent**. When we change one parameter, the
    optimal value for another might differ. An excellent example of co-dependence
    in GBDTs is the number of boosting rounds and the learning rate. Having a small
    learning rate necessitates more boosting rounds. Therefore, optimizing the learning
    rate and then, independently, the number of boosting rounds is unlikely to produce
    optimal results. *Both parameters must be optimized* *in unison*.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Grid search
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An approach that accounts for parameter co-dependence is grid search. With grid
    search, a parameter grid is set up. The grid consists of a range of values to
    try for each parameter we are optimizing. An exhaustive search is then performed,
    training and validating the model on each possible combination of parameters.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of a parameter grid for three parameters:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Each parameter is specified with a range of possible values. The previous grid
    would require 150 trails to search.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Since grid search is exhaustive, it has the advantage that it is guaranteed
    to find the best combination of parameters within the ranges specified. However,
    the downside to grid search is the cost. Trying each possible combination of parameters
    is very expensive and quickly becomes intractable for many parameters and large
    parameter ranges.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn provides a utility class to implement grid search and perform cross-validation
    at the same time. `GridSearchCV` takes a model, a parameter grid, and the number
    of cross-validation folds as parameters. `GridSearchCV` then proceeds to search
    the grid for the best parameters, using cross-validation to validate the performance
    for each combination of parameters. Weâ€™ll illustrate the use of `GridSearchCV`
    in the next section.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Parameter optimization is a crucial part of the modeling process. Finding suitable
    parameters for a model could be the difference between a successful or failed
    process. However, as discussed previously, parameter optimization is also often
    enormously expensive regarding time and computational complexity, necessitating
    a trade-off between cost and performance.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Predicting student academic success
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now move on to our example. We build a model to predict studentsâ€™ dropout
    rate based on a range of social and economic factors using LightGBM [7] ([https://archive-beta.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success](https://archive-beta.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success)).
    The data is available in CSV format. We start by exploring the data.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory data analysis
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One of the most fundamental properties of any dataset is the shape: the rows
    and columns our data consists of. Itâ€™s also an excellent way to validate that
    the data read succeeded. Here, our data consists of 4,424 rows and 35 columns.
    Taking a random sample of the data gives us a sense of the columns and their values:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we can run `df.info()` to see all the columns, their non-null counts,
    and their data types:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Running the preceding code shows us that most columns are integer types, except
    for the `Target` column, with a few floats in between. The `Target` column is
    listed as type `object`; if we look at the values in the sample, we can see the
    `Target` column consists of `Graduate`, `Dropout`, and `Enrolled` strings. LightGBM
    canâ€™t work with strings as targets, so weâ€™ll map these to integer values before
    training our models.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: We can also run `df.describe()` to get a statistical description (mean, standard
    deviation, min, max, and percentiles) of the values in each column. Calculating
    descriptive statistics helps check the bounds of the data (not a big problem with
    working with decision tree models) and check for outliers. For this dataset, there
    arenâ€™t any data bounds or outlier concerns.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to check for duplicated and missing values. We need to drop the
    rows containing missing values or impute appropriate substitutes if there are
    any missing values. We can check for missing values using the following code:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Running the preceding code shows us there are no missing values for this dataset.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'To locate duplicates, we can run the following code:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: There are also no duplicates in the dataset. If there were any duplicated data,
    we would drop the extra rows.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to check the distribution of the target class to ensure it is
    balanced. Here, we show a histogram that indicates the target class distribution.
    We create the histogram using Seabornâ€™s `countplot()` method, like so:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![Figure 3.5 â€“ Distribution of target class in the academic success dataset](img/B16690_03_5.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 â€“ Distribution of target class in the academic success dataset
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Although not perfectly balanced, the target distribution is not overly skewed
    to any one class, and we donâ€™t have to perform any compensating action.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have found that our dataset is suitable for modeling (we still need
    to remap `Target`) and clean (it does not contain missing or duplicated values
    and is well balanced). We can now take a deeper look at some features, starting
    with feature correlation. The following code plots a correlation heatmap. Pairwise
    Pearson correlations are calculated using `df.corr()`. The screenshot that follows
    the snippet shows a correlation heatmap built using pairwise Pearson correlations:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![Figure 3.6 â€“ Pairwise Pearson feature correlation of the academic success
    dataset](img/B16690_03_6.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 â€“ Pairwise Pearson feature correlation of the academic success dataset
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see three patterns of correlations: first-semester credits, enrollments,
    evaluations, and approvals are all correlated. First-semester and second-semester
    values for these are also correlated. These correlations imply that students tend
    to see through the year once enrolled instead of dropping out mid-semester. Although
    correlated, the correlations arenâ€™t strong enough to consider dropping any features.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: The third correlation pattern is between `Nacionality` and `International`,
    which are strongly correlated.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: The word *Nacionality* refers to *nationality*. We have retained the spelling
    from the original dataset here too for the purpose of consistency.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'A closer look at `Nacionality` shows that almost all rows have a single value:
    the country where the dataset was collected. The strong correlation implies the
    same for `International`:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The following screenshot shows a stacked bar plot of the nationalities:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 â€“ Distribution of the â€˜Nacionalityâ€™ feature, showing almost all
    rows have a single value in the academic success dataset](img/B16690_03_7.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 â€“ Distribution of the â€˜Nacionalityâ€™ feature, showing almost all rows
    have a single value in the academic success dataset
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: The distribution of `'Nacionality'` and `'International'` means that they are
    not very informative (nearly all rows have the same value), so we can drop them
    from the dataset.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we notice the `''Gender''` feature. When working with gender, itâ€™s
    always good to check for bias. We can visualize the distribution of the `''Gender''`
    feature relative to the target classes using a histogram. The results are shown
    in the screenshot that follows this code snippet:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![Figure 3.8 â€“ Distribution of the â€˜Genderâ€™ feature in the academic success
    dataset](img/B16690_03_8.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 â€“ Distribution of the â€˜Genderâ€™ feature in the academic success dataset
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: There is a slight bias toward female students, but not enough to warrant concern.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Modeling
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can now prepare our dataset for modeling. We must map our `Target` values
    to integers and drop the `Nacionality` and `International` features. We also need
    to remove the spaces in the feature names. LightGBM cannot work with spaces in
    the names; we can replace them with underscores:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We train and compare four models: a LightGBM GBDT, a LightGBM DART tree, a
    LightGBM DART tree with GOSS, and a scikit-learn random forest.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Weâ€™ll perform parameter optimization with 5-fold cross-validation using `GridSearchCV`
    to ensure good performance for the models.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code sets up the parameter optimization for the GBDT. A similar
    pattern is followed for the other models, which can be seen in the source code:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Running the preceding code takes some time, but once completed, it prints the
    best parameters found along with the score of the best model.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'After all the models are trained, we can evaluate each using F1-scoring, taking
    the mean of 5-fold cross-validation, using the best parameters found. The following
    code illustrates how to do this for the GBDT model:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Jupyter notebooks for [the parameter optimization for each model are available
    in the GitHub repository: https://github.com/Pack](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3)tPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table summarizes the best parameter values found and the cross-validated
    F1 scores for each model:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **Learning** **Rate** | **Max** **Depth** | **Min Child** **Samples**
    | **N** **Estimators** | **Num** **Leaves** | **Min** **Samples Leaf** | **Min**
    **Samples Split** | **F1 score** |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
- en: '| GBDT | 0.1 | - | 10 | 100 | 32 | N/A | N/A | 0.716 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
- en: '| DART | 0.1 | 128 | 30 | 150 | 128 | N/A | N/A | 0.703 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
- en: '| DART (GOSS) | 0.1 | 128 | 30 | 150 | 128 | N/A | N/A | 0.703 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
- en: '| Random Forest | N/A | N/A | N/A | 150 | N/A | 10 | 20 | 0.665 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
- en: Table 3.1 â€“ Summary of best parameters found for each model with the corresponding
    F1 scores
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: As we can see from the table, the LightGBM models performed much better than
    the scikit-learn random forest. Both DART models achieved nearly the same F1 score,
    with GOSS having a slightly lower F1 score (the table values are rounded to 3
    digits).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our end-to-end example of exploring a dataset and building an
    optimized model for the dataset (using parameter grid search). We look at more
    complicated datasets in the coming chapters and delve deeper into analyzing model
    performance.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced LightGBM as a library to train boosted machines efficiently.
    We looked at where the complexity of building GBDTs comes from and the features
    in LightGBM that address them, such as histogram-based sampling, feature bundling,
    and GOSS. We also reviewed LightGBMâ€™s most important hyperparameters.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: We also gave a detailed overview of using LightGBM in Python, covering both
    the LightGBM Python API and the scikit-learn API. We then built our first tuned
    models using LightGBM to predict student academic performance, utilizing cross-validation
    and grid-search-based parameter optimization.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we compare LightGBM against another popular gradient-boosting
    library, XGBoost, and DL techniques for tabular data.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '| *[**1]* | *G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye and
    T.-Y. Liu, â€œLightGBM: A Highly Efficient Gradient Boosting Decision Tree,â€ in
    Advances in Neural Information Processing* *Systems, 2017.* |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
- en: '| *[**2]* | *M. Mehta, R. Agrawal and J. Rissanen, â€œSLIQ: A fast scalable classifier
    for data mining,â€ in Advances in Database Technologyâ€”EDBTâ€™96: 5th International
    Conference on Extending Database Technology Avignon, France, March 25-29, 1996
    Proceedings* *5, 1996.* |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
- en: '| *[**3]* | *J. Shafer, R. Agrawal, M. Mehta and others, â€œSPRINT: A scalable
    parallel classifier for data mining,â€ in* *Vldb, 1996.* |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
- en: '| *[**4]* | *S. Ranka and V. Singh, â€œCLOUDS: A decision tree classifier for
    large datasets,â€ in Proceedings of the 4th Knowledge Discovery and Data Mining*
    *Conference, 1998.* |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
- en: '| *[**5]* | *H. Shi, â€œBest-first decision tree* *learning,â€ 2007.* |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
- en: '| *[**6]* | *R. Kleiman and D. Page, â€œAucÎ¼: A performance metric for multi-class
    machine learning models,â€ in International Conference on Machine* *Learning, 2019.*
    |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
- en: '| *[**7]* | *V. Realinho, J. Machado, L. Baptista and M. V. Martins, Predicting
    student dropout and academic success,* *Zenodo, 2021.* |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
