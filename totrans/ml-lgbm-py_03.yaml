- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An Overview of LightGBM in Python
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we looked at ensemble learning methods for decision
    trees. Both **bootstrap aggregation** (**bagging**) and gradient boosting were
    discussed in detail, with practical examples of how to apply the techniques in
    scikit-learn. We also showed how **gradient-boosted decision trees** (**GBDTs**)
    are slow to train and may underperform on some problems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This chapter introduces LightGBM, a gradient-boosting framework that uses tree-based
    learners. We look at the innovations and optimizations LightGBM makes to the ensemble
    learning methods. Further details and examples are given for using LightGBM practically
    via Python. Finally, the chapter includes a modeling example using LightGBM, incorporating
    more advanced techniques for model validation and parameter optimization.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: By the end of the chapter, you will have a thorough understanding of the theoretical
    and practical properties of LightGBM, allowing us to dive deeper into using LightGBM
    for data science and production systems.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'The main topics of this chapter are set out here:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Introducing LightGBM
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with LightGBM in Python
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building LightGBM models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The chapter includes examples and code excerpts illustrating how to use LightGBM
    in Python. Complete examples and instructions for setting up a suitable environment
    for this chapter are available a[t https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3)-3.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Introducing LightGBM
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LightGBM is an open source, gradient-boosting framework for tree-based ensembles
    ([https://github.com/microsoft/LightGBM](https://github.com/microsoft/LightGBM)).
    LightGBM focuses on efficiency in speed, memory usage, and improved accuracy,
    especially for problems with high dimensionality and large data sizes.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'LightGBM was first introduced in the paper *LightGBM: A Highly Efficient Gradient
    Boosting Decision* *Tree* [1].'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: The efficiency and accuracy of LightGBM are achieved via several technical and
    theoretical optimizations to the standard ensemble learning methods, particularly
    GBDTs. Additionally, LightGBM supports distributed training of ensembles with
    optimizations in network communication and support for GPU-based training of tree
    ensembles.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'LightGBM supports many **machine learning** (**ML**) applications: regression,
    binary and multiclass classification, cross-entropy loss functions, and ranking
    via LambdaRank.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: The LightGBM algorithm is also very customizable via its hyperparameters. It
    supports many metrics and features, including **Dropouts meet Multiple Additive
    Regression Trees** (**DART**), bagging (random forests), continuous training,
    multiple metrics, and early stopping.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: This section reviews the theoretical and practical optimizations LightGBM utilizes,
    including a detailed overview of the hyperparameters to control LightGBM features.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本节回顾了LightGBM使用的理论和实践优化，包括控制LightGBM特征的超参数的详细概述。
- en: LightGBM optimizations
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LightGBM优化
- en: At its core, LightGBM implements the same ensemble algorithms we discussed in
    the previous chapter. However, LightGBM applies theoretical and technical optimizations
    to improve performance and accuracy while significantly reducing memory usage.
    Next, we discuss the most significant optimizations implemented in LightGBM.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，LightGBM实现了我们在上一章中讨论的相同集成算法。然而，LightGBM通过理论和技术的优化来提高性能和准确性，同时显著减少内存使用。接下来，我们将讨论LightGBM中实施的最显著的优化。
- en: Computational complexity in GBDTs
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GBDT中的计算复杂度
- en: 'First, we must understand where the inefficiency in building GBDTs stems from
    to understand how LightGBM improves the efficiency of GBDTs. The most computationally
    complex part of the GBDT algorithm is training the regression tree for each iteration.
    More specifically, finding the optimal split is very expensive. Pre-sort-based
    algorithms are among the most popular methods for finding the best splits [2],
    [3]. A naïve approach requires the data to be sorted by feature for every decision
    node with algorithmic complexity O(#data × #feature). Pre-sort-based algorithms
    sort the data once before training, which reduces the complexity when building
    a decision node to O(#data) [2]. Even with pre-sorting, the complexity is too
    high for large datasets when finding splits for decision nodes.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '首先，我们必须理解构建GBDT中的低效性来源，才能理解LightGBM如何提高GBDT的效率。GBDT算法中最计算复杂的部分是每次迭代的回归树训练。更具体地说，找到最优分割是非常昂贵的。基于预排序的算法是寻找最佳分割的最流行方法之一[2]，[3]。一种简单的方法要求对每个决策节点按特征对数据进行排序，算法复杂度为O(#data
    × #feature)。基于预排序的算法在训练前对数据进行一次排序，这降低了构建决策节点的复杂度到O(#data) [2]。即使有预排序，当寻找决策节点的分割时，复杂度对于大型数据集来说仍然太高。'
- en: Histogram-based sampling
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于直方图的采样
- en: 'An alternative approach to pre-sorting involves building histograms for continuous
    features [4]. The continuous values are added into discrete bins when building
    these **feature histograms**. Instead of using the data directly when calculating
    the splits for decision nodes, we can now use the histogram bins. Constructing
    the histograms has a complexity of O(#data). However, the complexity for building
    a decision node now reduces to O(#bins), and since the number of bins is much
    smaller than the amount of data, this significantly speeds up the process of building
    regression trees, as illustrated in the following diagram:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 预排序的另一种方法涉及为连续特征构建直方图[4]。在构建这些**特征直方图**时，连续值被添加到离散的箱中。在计算决策节点的分割时，我们不再直接使用数据，而是现在可以使用直方图箱。构建直方图的复杂度为O(#data)。然而，构建决策节点的复杂度现在降低到O(#bins)，由于箱的数量远小于数据量，这显著加快了构建回归树的过程，如下面的图所示：
- en: '![Figure 3.1 – Creating feature histograms from continuous features allows
    calculating splits for decision nodes using bin boundary values instead of having
    to sample each data point, significantly reducing the algorithm’s complexity since
    #bins << #data](img/B16690_03_1.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图3.1 – 从连续特征创建特征直方图允许使用箱边界值来计算决策节点的分割，而不是必须对每个数据点进行采样，这显著降低了算法的复杂性，因为#bins
    << #data](img/B16690_03_1.jpg)'
- en: 'Figure 3.1 – Creating feature histograms from continuous features allows calculating
    splits for decision nodes using bin boundary values instead of having to sample
    each data point, significantly reducing the algorithm’s complexity since #bins
    << #data'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '图3.1 – 从连续特征创建特征直方图允许使用箱边界值来计算决策节点的分割，而不是必须对每个数据点进行采样，这显著降低了算法的复杂性，因为#bins
    << #data'
- en: A secondary optimization that stems from using histograms is “histogram subtraction”
    for building the histograms for the leaves. Instead of calculating the histogram
    for each leaf, we can subtract the leaf’s neighbor’s histogram from the parent’s
    histogram. Choosing the leaf with the smaller amount of data leads to a smaller
    O(#data) complexity for the first leaf and O(#bin) complexity for the second leaf
    due to histogram subtraction.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由使用直方图产生的二级优化是“直方图减法”，用于构建叶子的直方图。我们不需要为每个叶子计算直方图，而是可以从父直方图中减去叶子的邻居直方图。选择数据量较小的叶子会导致第一个叶子的O(#data)复杂度较小，由于直方图减法，第二个叶子的O(#bin)复杂度较小。
- en: 'A third optimization that LightGBM applies using histograms is to reduce the
    memory cost. Feature pre-sorting requires a supporting data structure (a dictionary)
    for each feature. No such data structures are required when building histograms,
    reducing memory costs. Further, since #bins is small, a smaller data type, such
    as `uint8_t`, can store the training data, reducing memory usage.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM使用直方图应用的一种第三种优化是减少内存成本。特征预排序需要为每个特征提供一个支持数据结构（一个字典）。在构建直方图时不需要这样的数据结构，从而降低了内存成本。此外，由于#bins很小，可以使用较小的数据类型，如`uint8_t`来存储训练数据，从而减少内存使用。
- en: 'Detailed information regarding the algorithms for building feature histograms
    is available in the paper *CLOUDS: A decision tree classifier for large* *datasets*
    [4].'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 关于构建特征直方图算法的详细信息可在论文《CLOUDS：用于大型*数据集*的决策树分类器》[4]中找到。
- en: Exclusive Feature Bundling
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 独家功能捆绑
- en: '**Exclusive Feature Bundling** (**EFB**) is another data-based optimization
    that LightGBM applies when working with sparse data (sparse data is pervasive
    in high-dimensional datasets). When the feature data is sparse, it’s common to
    find that many features are *mutually exclusive*, signifying they never present
    non-zero values simultaneously. Combining these features into a single one is
    generally safe, given this exclusivity. EFB is illustrated in the following diagram:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**独家功能捆绑**（**EFB**）是LightGBM在处理稀疏数据（稀疏数据在高维数据集中普遍存在）时应用的一种基于数据的优化。当特征数据稀疏时，通常会发现许多特征是**相互排斥**的，这意味着它们永远不会同时呈现非零值。考虑到这种排他性，将这些特征组合成一个单一的特征通常是安全的。EFB在以下图中展示：'
- en: '![Figure 3.2 – Building a feature bundle from two mutually exclusive features](img/B16690_03_2.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图3.2 – 从两个相互排斥的特征构建特征捆绑](img/B16690_03_2.jpg)'
- en: Figure 3.2 – Building a feature bundle from two mutually exclusive features
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 – 从两个相互排斥的特征构建特征捆绑
- en: 'Bundling mutually exclusive features allows building the same feature histograms
    as from the individual features [1]. The optimization reduces the complexity of
    building feature histograms from O(#data × #feature) to O(#data × #bundle). For
    datasets where there are many mutually exclusive features, this dramatically improves
    performance since # bundle ≪ #feature. Detailed algorithms for, and proof of the
    correctness of, EFB are available in [1].'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '将相互排斥的特征捆绑在一起，可以构建与单个特征相同的特征直方图[1]。这种优化将构建特征直方图的复杂度从O(#数据 × #特征)降低到O(#数据 ×
    #捆绑)。对于存在许多相互排斥特征的数据库，这显著提高了性能，因为#捆绑远小于#特征。EFB的详细算法及其正确性的证明可在[1]中找到。'
- en: Gradient-based One-Side Sampling
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于梯度的单侧采样
- en: A final data-based optimization available in the LightGBM framework is **Gradient-based
    One-Side Sampling** (**GOSS**) [1]. GOSS is a method of discarding training data
    samples that no longer contribute significantly to the training process, effectively
    reducing the training data size and speeding up the process.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM框架中可用的最后一种基于数据的优化是**基于梯度的单侧采样**（**GOSS**）[1]。GOSS是一种丢弃不再对训练过程有显著贡献的训练数据样本的方法，从而有效地减少了训练数据的大小并加快了过程。
- en: We can use the gradient calculation of each sample to determine its importance.
    If the gradient change is small, it implies that the training error was also small,
    and we can infer that the tree is well fitted to the specific data instance [1].
    One option would be to discard all instances with small gradients. However, this
    changes the distribution of the training data, reducing the tree’s ability to
    generalize. GOSS is a method for choosing which instances to keep in the training
    data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用每个样本的梯度计算来确定其重要性。如果梯度变化很小，这表明训练误差也很小，我们可以推断出树对特定数据实例拟合得很好[1]。一个选择是丢弃所有梯度小的实例。然而，这改变了训练数据的分布，减少了树泛化的能力。GOSS是一种选择保留在训练数据中的实例的方法。
- en: 'To maintain the data distribution, GOSS is applied as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持数据分布，GOSS按照以下方式应用：
- en: The data samples are sorted by the absolute value of their gradients.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据样本按其梯度的绝对值排序。
- en: The top a × 100% instances are then selected (instances with large gradients).
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后选择前a × 100%的实例（梯度大的实例）。
- en: A random sample of b × 100% instances is then taken from the rest of the data.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后从剩余的数据中随机抽取b × 100%的实例样本。
- en: 'A factor is added to the loss function (for these instances) to amplify their
    influence: 1 − a _ b , thereby compensating for the underrepresentation of data
    with small gradients.'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在损失函数（对于这些实例）中添加一个因子以放大其影响：1 − a _ b，从而补偿小梯度数据的代表性不足。
- en: Therefore, GOSS samples a large portion of instances with large gradients and
    a random portion of instances with small gradients and amplifies the influence
    of the small gradients when calculating information gain.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，GOSS从具有大梯度的实例中采样大量实例，并从具有小梯度的实例中随机采样一部分实例，在计算信息增益时放大小梯度的影响。
- en: The downsampling enabled by GOSS can significantly reduce the amount of data
    processed during training (and the training time for the GBDTs), especially in
    the case of large datasets.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: GOSS启用的下采样可以显著减少训练过程中处理的数据量（以及GBDT的训练时间），尤其是在大型数据集的情况下。
- en: Best-first tree growth
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最佳优先树增长
- en: The most common method for building decision trees is to grow the tree by level
    (that is, one level at a time). LightGBM uses an alternative approach and grows
    the tree leaf-wise or best-first. The leaf-wise approach selects an existing leaf
    with the most significant change in the loss of the tree and builds the tree from
    there. The downside of this approach is that if the dataset is small, the tree
    is likely to overfit the data. A maximum depth has to be set to counteract this.
    However, if the number of leaves to construct is fixed, leaf-wise tree building
    is shown to outperform level-wise algorithms [5].
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 构建决策树最常见的方法是按层次增长（即，一次增长一个层次）。LightGBM采用了一种替代方法，通过叶节点或最佳优先的方式增长树。叶节点方法选择具有最大损失变化的现有叶节点，并从那里构建树。这种方法的一个缺点是，如果数据集很小，树很可能会过拟合数据。必须设置最大深度来抵消这一点。然而，如果构建的叶节点数量是固定的，叶节点树构建已被证明优于层次算法[5]。
- en: L1 and L2 regularization
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: L1 和 L2 正则化
- en: LightGBM supports both L1 and L2 regularization of the objective function when
    training the regression trees in the ensemble. From [*Chapter 1*](B16690_01.xhtml#_idTextAnchor014)*,
    Introducing Machine Learning*, we recall that regularization is a way to control
    overfitting. In the case of decision trees, simpler, shallow trees overfit less.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM 在集成中训练回归树时支持目标函数的 L1 和 L2 正则化。从 [*第1章*](B16690_01.xhtml#_idTextAnchor014)
    *介绍机器学习* 中，我们回忆起正则化是控制过拟合的一种方法。在决策树的情况下，更简单、更浅的树过拟合较少。
- en: 'To support L1 and L2 regularization, we extend the objective function with
    a regularization term, as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持L1和L2正则化，我们通过添加正则化项扩展了目标函数，如下所示：
- en: obj = L(y, F(x)) + Ω(w)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: obj = L(y, F(x)) + Ω(w)
- en: Here, L(y, F(x)) is the loss function discussed in [*Chapter 2*](B16690_02.xhtml#_idTextAnchor036)*,
    Ensemble Learning – Bagging and Boosting*, and Ω(w) is the regularization function
    defined over w, the leaf scores (the leaf score is the output calculated from
    the leaf as per *step 2.3* in the GBDT algorithm defined in [*Chapter 2*](B16690_02.xhtml#_idTextAnchor036)*,
    Ensemble Learning – Bagging* *and Boosting*).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，L(y, F(x)) 是在[*第2章*](B16690_02.xhtml#_idTextAnchor036)中讨论的损失函数，*集成学习 – Bagging
    和 Boosting*，而 Ω(w) 是定义在 w 上的正则化函数，即叶得分（叶得分是根据 GBDT 算法中定义的 *步骤 2.3* 计算的叶输出，该算法在[*第2章*](B16690_02.xhtml#_idTextAnchor036)中讨论，*集成学习
    – Bagging 和 Boosting*）。
- en: The regularization term effectively adds a penalty to the objective function,
    where we aim to penalize more complex trees prone to overfitting.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化项有效地向目标函数添加了惩罚，我们的目标是惩罚更复杂的树，这些树容易过拟合。
- en: 'There are multiple definitions for Ω. A typical implementation for the terms
    in decision trees is this:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Ω 有多个定义。决策树中这些项的典型实现如下：
- en: Ω(w) = α∑ i n |w i| + λ∑ i n w i 2
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Ω(w) = α∑ i n |w i| + λ∑ i n w i 2
- en: Here, α∑ i n |w i| is the L1 regularization term, controlled by the parameter
    α, 0 ≤ α ≤ 1, and λ∑ i n w i 2 is the L2 regularization term, controlled by the
    parameter λ.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，α∑ i n |w i| 是由参数 α 控制的 L1 正则化项，0 ≤ α ≤ 1，而 λ∑ i n w i 2 是由参数 λ 控制的 L2 正则化项。
- en: L1 regularization has the effect of driving leaf scores to zero by penalizing
    leaves with large absolute outputs. *Smaller leaf outputs have a smaller effect
    on the tree’s prediction, effectively simplifying* *the tree*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: L1 正则化通过惩罚具有大绝对输出的叶节点，将叶得分驱动到零。*较小的叶输出对树的预测影响较小，从而有效地简化了* *树*。
- en: L2 regularization is similar but has an outsized effect on outliers’ leaves
    due to taking the square of the output.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: L2 正则化类似，但由于输出取平方，对异常值叶节点有更大的影响。
- en: Finally, when larger trees are built (trees with more leaves, and therefore
    a large w vector), both sum terms for Ω(w) increase, increasing the objective
    function output. Therefore, *larger trees are penalized*, and overfitting is reduced.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当构建较大的树（具有更多叶节点，因此具有较大的 w 向量）时，Ω(w) 的两个求和项都会增加，从而增加目标函数的输出。因此，*较大的树会受到惩罚*，从而减少过拟合。
- en: Summary of LightGBM optimizations
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LightGBM优化总结
- en: 'In summary, LightGBM improves upon the standard ensemble algorithms by doing
    the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，LightGBM通过以下方式改进了标准集成算法：
- en: Implementing histogram-based sampling of features to reduce the computational
    cost of finding optimal splits
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现基于直方图的采样特征以减少寻找最优分割的计算成本
- en: Calculating exclusive feature bundles to reduce the number of features in sparse
    datasets
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过计算独家特征包来减少稀疏数据集中的特征数量
- en: Applying GOSS to downsample the training data without losing accuracy
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用GOSS以在不损失准确性的情况下对训练数据进行下采样
- en: Building trees leaf-wise to improve accuracy
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以叶节点的方式构建树以提高准确性
- en: Overfitting can be controlled through L1 and L2 regularization and other control
    parameters
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过L1和L2正则化以及其他控制参数可以控制过拟合
- en: In conjunction, the optimizations improve the computational performance of LightGBM
    by **orders of magnitude** (**OOM**) over the standard GBDT algorithm. Additionally,
    LightGBM is implemented in C++ with a Python interface, which results in much
    faster code than Python-based GBDTs, such as in scikit-learn.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 结合优化，这些优化将LightGBM的计算性能提高了与标准GBDT算法相比的**数量级**（**OOM**）。此外，LightGBM是用C++实现的，具有Python接口，这使得代码比基于Python的GBDT（如scikit-learn）快得多。
- en: Finally, LightGBM also has support for improved data-parallel and feature-parallel
    distributed training. Distributed training and GPU support are discussed in a
    later [*Chapter 11*](B16690_11.xhtml#_idTextAnchor177)*, Distributed and GPU-Based
    Learning* *with LightGBM*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，LightGBM还支持改进的数据并行和特征并行分布式训练。分布式训练和GPU支持将在后面的[*第11章*](B16690_11.xhtml#_idTextAnchor177)*，使用LightGBM的分布式和基于GPU的学习*中讨论。
- en: Hyperparameters
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数
- en: LightGBM exposes many parameters that can be used to customize the training
    process, goals, and performance. Next, we discuss the most notable parameters
    and how they may be used to control specific phenomena.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM公开了许多参数，可用于自定义训练过程、目标和性能。接下来，我们将讨论最显著的参数以及它们如何用于控制特定现象。
- en: Note
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The core LightGBM framework is developed in C++ but includes APIs to work with
    LightGBM in C, Python, and R. The parameters discussed in this section are the
    framework parameters and are exposed differently by each API. The following section
    discusses the parameters available when using Python.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 核心LightGBM框架是用C++开发的，但包括用于在C、Python和R中与LightGBM一起工作的API。本节讨论的参数是框架参数，并且每个API以不同的方式暴露。以下章节将讨论使用Python时可用参数。
- en: 'The following are **core framework parameters** used to control the optimization
    process and goal:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是用以控制优化过程和目标的**核心框架参数**：
- en: '`objective`: LightGBM supports the following optimization objectives, among
    others—`regression` (including regression applications with other loss functions
    such as Huber and Fair), `binary` (classification), `multiclass` (classification),
    `cross-entropy`, and `lambdarank` for ranking problems.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`目标`：LightGBM支持以下优化目标，包括但不限于——`回归`（包括具有Huber和Fair等损失函数的回归应用），`二元`（分类），`多类`（分类），`交叉熵`，以及用于排序问题的`lambdarank`。'
- en: '`boosting`: The boosting parameter controls the boosting type. By default,
    this is set to `gbdt`, the standard GBDT algorithm. The other options are `dart`
    and `rf` for random forests. The random forest mode does not perform boosting
    but instead builds a random forest.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`boosting`：提升参数控制提升类型。默认情况下，此参数设置为`gbdt`，即标准GBDT算法。其他选项是`dart`和`rf`，用于随机森林。随机森林模式不执行提升，而是构建随机森林。'
- en: '`num_iterations` (or `n_estimators`): Controls the number of boosting iterations
    and, therefore, the number of trees built.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_iterations`（或`n_estimators`）：控制提升迭代次数，因此也控制构建的树的数量。'
- en: '`num_leaves`: Controls the maximum number of leaves in a single tree.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_leaves`：控制单个树中的最大叶节点数。'
- en: '`learning_rate`: Controls the learning, or shrinkage rate, which is the contribution
    of each tree to the overall prediction.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate`：控制学习或收缩率，即每个树对整体预测的贡献。'
- en: LightGBM also provides many parameters to control the learning process. We’ll
    discuss these parameters relative to how they may be used to tune specific aspects
    of training.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM还提供了许多参数来控制学习过程。我们将讨论这些参数相对于它们如何用于调整训练的特定方面。
- en: 'The following control parameters can be used to improve **accuracy**:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下控制参数可用于提高**准确性**：
- en: '`boosting`: Use `dart`, which has been shown to outperform standard GBDTs.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`boosting`：使用`dart`，这已被证明优于标准GBDT。'
- en: '`learning_rate`: The learning rate must be tuned alongside `num_iterations`
    for better accuracy. A small learning rate with a large value for `num_iterations`
    leads to better accuracy at the expense of optimization speed.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_leaves`: A larger number of leaves improves accuracy but may lead to overfitting.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_bin`: The maximum number of bins in which features are bucketed when constructing
    histograms. A larger `max_bin` size slows the training and uses more memory but
    may improve accuracy.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following **learning control parameters** can be used to deal with **overfitting**:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '`bagging_fraction` and `bagging_freq`: Setting both parameters enables feature
    bagging. Bagging may be used in addition to boosting and doesn’t force the use
    of a random forest. Enabling bagging reduces overfitting.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`early_stopping_round`: Enables early stopping and controls the number of iterations
    used to determine whether training should be stopped. Training is stopped if no
    improvement is made to any metric in the iterations set by `early_stopping_round`.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_data_in_leaf`: The minimum samples allowed in a leaf. Larger values reduce
    overfitting.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_gain_to_split`: The minimum amount of information gain required to perform
    a split. Higher values reduce overfitting.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reg_alpha`: Controls L1 regularization. Higher values reduce overfitting.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reg_lambda`: Controls L2 regularization. Higher values reduce overfitting.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_depth`: Controls the maximum depth of individual trees. Shallower trees
    reduce overfitting.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_drop`: Controls the maximum number of dropped trees when using the DART
    algorithm (is only used when `boosting` is set to `dart`). A larger value reduces
    overfitting.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`extra_trees`: Enables the **Extremely Randomized Trees** (**ExtraTrees**)
    algorithm. LightGBM then chooses a split threshold at random for each feature.
    Enabling Extra-Trees can reduce overfitting. The parameter can be used in conjunction
    with any boosting mode.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The parameters discussed here include only some of the parameters available
    in LightGBM and focus on improving accuracy and overfitting. A complete list of
    parameters [is available at the following link: https://lightgbm.rea](https://lightgbm.readthedocs.io/en/latest/Parameters.xhtml)dthedocs.io/en/latest/Parameters.xhtml.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of LightGBM
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LightGBM is designed to be more efficient and effective than traditional methods.
    It is particularly well known for its ability to handle large datasets. However,
    as with any algorithm or framework, it also has its limitations and potential
    disadvantages, including the following:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '**Sensitive to overfitting**: LightGBM can be sensitive to overfitting, especially
    with small or noisy datasets. Care should be taken to monitor and control for
    overfitting when using LightGBM.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimal performance requires tuning**: As discussed previously, LightGBM
    has many hyperparameters that need to be properly tuned to get the best performance
    from the algorithm.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lack of representation learning**: Unlike **deep learning** (**DL**) approaches,
    which excel at learning from raw data, LightGBM requires feature engineering to
    be applied to the data before learning. Feature engineering is a time-consuming
    process that requires domain knowledge.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏表示学习**：与擅长从原始数据中学习的 **深度学习**（**DL**）方法不同，LightGBM 在学习之前需要应用特征工程到数据上。特征工程是一个耗时且需要领域知识的过程。'
- en: '**Handling sequential data**: LightGBM is not inherently designed for working
    with sequential data such as time series. For LightGBM to be used with time-series
    data, feature engineering needs to be applied to create lagged features and capture
    temporal dependencies.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理序列数据**：LightGBM 本身并不是为处理序列数据（如时间序列）而设计的。为了使用 LightGBM 处理时间序列数据，需要应用特征工程来创建滞后特征并捕捉时间依赖性。'
- en: '**Complex interactions and non-linearities**: LightGBM is a decision-tree-driven
    approach that might be incapable of capturing complex feature interactions and
    non-linearities. Proper feature engineering needs to be applied to ensure the
    algorithm models these.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂交互和非线性**：LightGBM 是一种以决策树为驱动的方法，可能无法捕捉复杂的特征交互和非线性。需要应用适当的特征工程来确保算法能够建模这些。'
- en: Although these are potential limitations of using the algorithm, they may not
    apply to all use cases. LightGBM is often a very effective tool in the right circumstances.
    As with any model, understanding the trade-offs is vital to making the right choice
    for your application.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些是使用该算法的潜在局限性，但它们可能并不适用于所有用例。在适当的情境下，LightGBM 经常是一个非常有效的工具。与任何模型一样，理解权衡对于为您的应用程序做出正确的选择至关重要。
- en: In the next session, we look at getting started using the various LightGBM APIs
    with Python.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨如何使用 Python 的各种 LightGBM API 开始使用。
- en: Getting started with LightGBM in Python
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Python 中开始使用 LightGBM
- en: 'LightGBM is implemented in C++ but has official C, R, and Python APIs. This
    section discusses the Python APIs that are available for working with LightGBM.
    LightGBM provides three Python APIs: the standard **LightGBM** API, the **scikit-learn**
    API (which is fully compatible with other scikit-learn functionality), and a **Dask**
    API for working with Dask. Dask is a parallel computing library discussed in [*Chapter
    11*](B16690_11.xhtml#_idTextAnchor177)*, Distribu*[*ted and GPU-Based Lea*](https://www.dask.org/)*rning
    with* *LightGBM* ([https://www.dask.org/](https://www.dask.org/)).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM 使用 C++ 实现，但提供了官方的 C、R 和 Python API。本节讨论可用于与 LightGBM 一起工作的 Python API。LightGBM
    提供了三个 Python API：标准的 **LightGBM** API、与其它 scikit-learn 功能完全兼容的 **scikit-learn**
    API，以及用于与 Dask 一起工作的 **Dask** API。Dask 是在第 [*第 11 章*](B16690_11.xhtml#_idTextAnchor177)*
    中讨论的并行计算库，*分布式和基于 GPU 的学习* ([https://www.dask.org/](https://www.dask.org/))。
- en: Throughout the rest of the book, we mainly use the scikit-learn API for LightGBM,
    but let’s first look at the standard Python API.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的其余部分，我们主要使用 LightGBM 的 scikit-learn API，但让我们首先看看标准的 Python API。
- en: LightGBM Python API
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LightGBM Python API
- en: The best way to dive into the Python API is with a hands-on example. The following
    are excerpts from a code listing that illustrates the use of the LightGBM Python
    API. The complete code example is available at [https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 深入了解 Python API 的最佳方式是通过动手示例。以下是从代码列表中摘录的片段，说明了 LightGBM Python API 的使用。完整的代码示例可在
    [https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3)
    找到。
- en: 'LightGBM needs to be imported. The import is often abbreviated as `lgb`:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 需要导入 LightGBM。导入通常简写为 `lgb`：
- en: '[PRE0]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: LightGBM provides a `Dataset` wrapper class to work with data. `Dataset` supports
    a variety of formats. Commonly, it is used to wrap a `numpy` array or a `pandas`
    DataFrame. `Dataset` also accepts a `Path` to a CSV, TSV, LIBSVM text file, or
    LightGBM `Dataset` binary file. When a path is supplied, LightGBM loads the data
    from the disk.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM 提供了一个 `Dataset` 包装类来处理数据。`Dataset` 支持多种格式。通常，它用于包装 `numpy` 数组或 `pandas`
    DataFrame。`Dataset` 还接受 CSV、TSV、LIBSVM 文本文件或 LightGBM `Dataset` 二进制文件的 `Path`。当提供路径时，LightGBM
    会从磁盘加载数据。
- en: 'Here, we load our Forest Cover dataset from `sklearn` and wrap the `numpy`
    arrays in a LightGBM `Dataset`:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们从 `sklearn` 加载我们的 Forest Cover 数据集，并将 `numpy` 数组包装在 LightGBM 的 `Dataset`
    中：
- en: '[PRE1]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We subtract 1 from the `y_train` and `y_test` arrays because the classes supplied
    by `sklearn` are labeled in the range [1, 7], whereas LightGBM expects zero-indexed
    class labels in the range [0, 7].
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从`y_train`和`y_test`数组中减去1，因为`sklearn`提供的类别标签在范围[1, 7]内，而LightGBM期望零索引的类别标签在范围[0,
    7]内。
- en: 'We cannot set up the parameters for training. We’ll be using the following
    parameters:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法设置训练的参数。我们将使用以下参数：
- en: '[PRE2]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We are using the standard GBDT as a boosting type and setting the objective
    to multiclass classification for seven classes. During training, we are going
    to capture the `auc_mu` metric. AU C μ is a multiclass adaptation of the **area
    under the receiver operating characteristic curve** (**AUC**), as defined by Kleiman
    and Page [6].
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用标准的GBDT作为提升类型，并将目标设置为七类的多分类。在训练过程中，我们将捕获`auc_mu`指标。AU C μ是多类版本的**受试者工作特征曲线下面积**（**AUC**），如Kleiman和Page
    [6]所定义。
- en: We set `num_leaves` and `learning_rate` to reasonable values for the problem.
    Finally, we specify `force_row_wise` as `True`, a recommended setting for large
    datasets.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`num_leaves`和`learning_rate`设置为适合该问题的合理值。最后，我们指定`force_row_wise`为`True`，这是大型数据集的一个推荐设置。
- en: 'LightGBM’s training function also supports **callbacks**. A callback is a hook
    into the training process that is executed each boosting iteration. To illustrate
    their purpose, we’ll be using the following callbacks:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM的训练函数也支持**回调**。回调是训练过程中的一个钩子，在每个提升迭代中执行。为了说明它们的目的，我们将使用以下回调：
- en: '[PRE3]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We use the `log_evaluation` callback with a period of 15, which logs (prints)
    our metrics to standard output every 15 boosting iterations. We also set a `record_evaluation`
    callback that captures our evaluation metrics in the `metrics` dictionary. We
    also specify an `early_stopping` callback, with stopping rounds set to 15\. The
    `early_stopping` callback stops training if no validation metrics improve after
    the specified number of stopping rounds.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`log_evaluation`回调，周期为15，它每15次提升迭代将我们的指标记录（打印）到标准输出。我们还设置了一个`record_evaluation`回调，它将我们的评估指标捕获在`metrics`字典中。我们还指定了一个`early_stopping`回调，停止轮次设置为15。如果经过指定的停止轮次后没有验证指标改进，`early_stopping`回调将停止训练。
- en: 'Finally, we also use the `reset_parameter` callback to implement **learning
    rate decay**. The decay function is defined as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还使用`reset_parameter`回调来实现**学习率衰减**。衰减函数定义如下：
- en: '[PRE4]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `reset_parameter` callback takes a function as input. The function receives
    the current iteration and returns the parameter value. Learning rate decay is
    a technique where we decrease the value of the learning rate over time. Learning
    rate decay improved the overall accuracy achieved. Ideally, we want the initial
    trees to have a more significant impact on correcting the prediction errors. In
    contrast, later on, we want to reduce the impact of additional trees and have
    them make minor adjustments to the errors. We implement a slight exponential decay
    that reduces the learning rate from 0.09 to 0.078 throughout training.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`reset_parameter`回调接受一个函数作为输入。该函数接收当前迭代次数并返回参数值。学习率衰减是一种技术，随着时间的推移降低学习率的值。学习率衰减提高了整体准确度。理想情况下，我们希望初始树对纠正预测错误有更大的影响。相比之下，后期我们希望减少额外树的影响，并让它们对错误进行微小调整。我们在整个训练过程中实施了一种轻微的指数衰减，将学习率从0.09降低到0.078。'
- en: 'Now, we are ready for training. We use `lgb.train` to train the model:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好进行训练。我们使用`lgb.train`来训练模型：
- en: '[PRE5]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We use 150 boosting rounds (or boosted trees). In conjunction with a lower learning
    rate, having many boosting rounds should improve accuracy.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用150次提升轮次（或提升树）。结合较低的学习率，拥有许多提升轮次应该可以提高准确度。
- en: 'After training, we can use `lgb.predict` to get predictions for our test set
    and calculate the F1 score:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们可以使用`lgb.predict`来获取测试集的预测并计算F1分数：
- en: '[PRE6]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The LightGBM predict function outputs an array of activations, one for each
    class. Therefore, we use `np.argmax` to choose the class with the highest activation
    as the predicted class. LightGBM also has support for some plotting functions.
    For instance, we can use `plot_metric` to plot our AU C μ results as captured
    in the `metrics`:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM的预测函数输出一个激活数组，每个类别一个。因此，我们使用`np.argmax`来选择具有最高激活的类别作为预测类别。LightGBM也支持一些绘图函数。例如，我们可以使用`plot_metric`来绘制我们在`metrics`中捕获的AU
    C μ结果：
- en: '[PRE7]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The results of this are shown in *Figure 3**.3*.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果在*图3**.3*中显示。
- en: '![Figure 3.3 – A plot of the ​AU ​C​ <?AID d835?><?AID df41?>​​​ metric per
    training iteration created using lgb.plot_metric](img/B16690_03_3.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – A plot of the AU C 𝝁 metric per training iteration created using
    lgb.plot_metric
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the preceding code should produce a LightGBM GBDT tree with an F1 score
    of around 0.917, in line with the score the Random Forest and Extra-Trees algorithms
    achieved in [*Chapter 2*](B16690_02.xhtml#_idTextAnchor036)*, Ensemble Learning
    – Bagging and Boosting*. However, LightGBM is significantly faster in reaching
    these accuracies. LightGBM completed the training in just 37 seconds on our hardware:
    this is 4.5 times faster than running Extra-Trees on the same problem and hardware
    and 60-70 times faster than scikit-learn’s `GradientBoostingClassifier` in our
    testing.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: LightGBM scikit-learn API
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We now take a look at the scikit-learn Python API for LightGBM. The scikit-learn
    API provides four classes: `LGBMModel`, `LGBMClassifier`, `LGBMRegressor`, and
    `LGBMRanker`. Each of these provides the same functionality as the LightGBM Python
    API, but with the same convenient scikit-learn interfaces we have worked with
    before. Additionally, the scikit-learn classes are compatible and interoperable
    with the rest of the scikit-learn ecosystem.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Let’s replicate the previous example using the scikit-learn API.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset is loaded precisely as before. The scikit-learn API doesn’t require
    wrapping the data in a `Dataset` object. We also don’t have to zero-index our
    target classes, as scikit-learn supports any label for the classes:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The scikit-learn API also supports LightGBM callbacks; as such, we use the
    same callbacks as before:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We then create the `LGBMClassifier` exactly as we would any other scikit-learn
    model. When creating the classifier, we also set the parameters:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Note that we do not have to specify the number of classes; scikit-learn infers
    this automatically. We then call `fit` on the model, passing the training and
    test data along with our callbacks:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, we evaluate our model with the F1 score. We don’t have to use `np.argmax`
    on the predictions as this is done automatically with the scikit-learn API:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Overall, we can see that using LightGBM via the scikit-learn API is more straightforward
    than the standard Python API. The scikit-learn API was also approximately 40%
    faster than the LightGBM API on our hardware. This section examined the ins and
    outs of using the various Python APIs available for LightGBM. The following section
    looks at training LightGBM models using the scikit-learn API.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Building LightGBM models
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section provides an end-to-end example of solving a real-world problem
    using LightGBM. We provide a more detailed look at data preparation for a problem
    and explain how to find suitable parameters for our algorithms. We use multiple
    variants of LightGBM to explore relative performance and compare them against
    random forests.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we delve into solving a problem, we need to discuss a better way of validating
    algorithm performance. Splitting the data into two or three subsets is standard
    practice when training a model. The training data is used to train the model,
    the validation data is a hold-out set used to validate the data during training,
    and the test data is used to validate the performance after training.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: In previous examples, we have done this split only once, building a single training
    and test to train and validate the model. The issue with this approach is that
    our model could get “lucky.” If, by chance, our test set closely matches the training
    data but is not representative of real-world data, we would report a good test
    error, even though we can’t be confident of our model’s performance.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: An alternative is to do the dataset splitting multiple times and train the model
    multiple times, once for each split. This approach is called **cross-validation**.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common application of cross-validation is *k-fold cross-validation*.
    With k-fold cross-validation, we choose a value, *k*, and partition the (shuffled)
    dataset into *k* subsamples (or folds). We then repeat the training process *k*
    times, using a different subset as the validation data and all other subsets as
    training data. The model’s performance is calculated as the mean (or median) score
    across all folds. The following diagram illustrates this process:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – k-fold cross-validation with k = 3; the original dataset is
    shuffled and split into 3 equal parts (or folds); training and validation are
    repeated for each combination of subsampled data, and the average performance
    is reported](img/B16690_03_4.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – k-fold cross-validation with k = 3; the original dataset is shuffled
    and split into 3 equal parts (or folds); training and validation are repeated
    for each combination of subsampled data, and the average performance is reported
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Using a high value for *k* reduces the chance that the model coincidentally
    shows good performance and indicates how the model might perform in the real world.
    However, the entire training process is repeated for each fold, which could be
    computationally expensive and time-consuming. Therefore, we need to balance the
    resources available with the need to validate the model. A typical value for *k*
    is 5 (the default for scikit-learn), also called 5-fold cross-validation.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Stratified k-fold validation
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A problem that might arise with k-fold cross-validation is that, due to chance,
    a fold may contain samples from only a single class. **Stratified sampling** solves
    this issue by preserving the percentage of samples for each class when creating
    folds. In this way, each fold has the same distribution of classes as the original
    dataset. When applied to cross-validation, this technique is called stratified
    k-fold cross-validation.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Parameter optimization
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Parameter optimization**, also called parameter tuning, is the process of
    finding good hyperparameters for the model and training process specific to the
    problem being solved. In the previous examples of training models, we have been
    setting the model and training algorithm’s parameters based on intuition and minimal
    experimentation. There is no guarantee that the parameter choices were optimal
    for the optimization problem.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: But how might we go about finding the best parameter choices? A naïve strategy
    is to try an extensive range of values for a parameter, find the best value, and
    then repeat the process for the following parameter. However, it is frequently
    the case that parameters are **co-dependent**. When we change one parameter, the
    optimal value for another might differ. An excellent example of co-dependence
    in GBDTs is the number of boosting rounds and the learning rate. Having a small
    learning rate necessitates more boosting rounds. Therefore, optimizing the learning
    rate and then, independently, the number of boosting rounds is unlikely to produce
    optimal results. *Both parameters must be optimized* *in unison*.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Grid search
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An approach that accounts for parameter co-dependence is grid search. With grid
    search, a parameter grid is set up. The grid consists of a range of values to
    try for each parameter we are optimizing. An exhaustive search is then performed,
    training and validating the model on each possible combination of parameters.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of a parameter grid for three parameters:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Each parameter is specified with a range of possible values. The previous grid
    would require 150 trails to search.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Since grid search is exhaustive, it has the advantage that it is guaranteed
    to find the best combination of parameters within the ranges specified. However,
    the downside to grid search is the cost. Trying each possible combination of parameters
    is very expensive and quickly becomes intractable for many parameters and large
    parameter ranges.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn provides a utility class to implement grid search and perform cross-validation
    at the same time. `GridSearchCV` takes a model, a parameter grid, and the number
    of cross-validation folds as parameters. `GridSearchCV` then proceeds to search
    the grid for the best parameters, using cross-validation to validate the performance
    for each combination of parameters. We’ll illustrate the use of `GridSearchCV`
    in the next section.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Parameter optimization is a crucial part of the modeling process. Finding suitable
    parameters for a model could be the difference between a successful or failed
    process. However, as discussed previously, parameter optimization is also often
    enormously expensive regarding time and computational complexity, necessitating
    a trade-off between cost and performance.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Predicting student academic success
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now move on to our example. We build a model to predict students’ dropout
    rate based on a range of social and economic factors using LightGBM [7] ([https://archive-beta.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success](https://archive-beta.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success)).
    The data is available in CSV format. We start by exploring the data.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory data analysis
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One of the most fundamental properties of any dataset is the shape: the rows
    and columns our data consists of. It’s also an excellent way to validate that
    the data read succeeded. Here, our data consists of 4,424 rows and 35 columns.
    Taking a random sample of the data gives us a sense of the columns and their values:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we can run `df.info()` to see all the columns, their non-null counts,
    and their data types:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Running the preceding code shows us that most columns are integer types, except
    for the `Target` column, with a few floats in between. The `Target` column is
    listed as type `object`; if we look at the values in the sample, we can see the
    `Target` column consists of `Graduate`, `Dropout`, and `Enrolled` strings. LightGBM
    can’t work with strings as targets, so we’ll map these to integer values before
    training our models.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: We can also run `df.describe()` to get a statistical description (mean, standard
    deviation, min, max, and percentiles) of the values in each column. Calculating
    descriptive statistics helps check the bounds of the data (not a big problem with
    working with decision tree models) and check for outliers. For this dataset, there
    aren’t any data bounds or outlier concerns.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to check for duplicated and missing values. We need to drop the
    rows containing missing values or impute appropriate substitutes if there are
    any missing values. We can check for missing values using the following code:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Running the preceding code shows us there are no missing values for this dataset.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'To locate duplicates, we can run the following code:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: There are also no duplicates in the dataset. If there were any duplicated data,
    we would drop the extra rows.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to check the distribution of the target class to ensure it is
    balanced. Here, we show a histogram that indicates the target class distribution.
    We create the histogram using Seaborn’s `countplot()` method, like so:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![Figure 3.5 – Distribution of target class in the academic success dataset](img/B16690_03_5.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – Distribution of target class in the academic success dataset
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Although not perfectly balanced, the target distribution is not overly skewed
    to any one class, and we don’t have to perform any compensating action.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have found that our dataset is suitable for modeling (we still need
    to remap `Target`) and clean (it does not contain missing or duplicated values
    and is well balanced). We can now take a deeper look at some features, starting
    with feature correlation. The following code plots a correlation heatmap. Pairwise
    Pearson correlations are calculated using `df.corr()`. The screenshot that follows
    the snippet shows a correlation heatmap built using pairwise Pearson correlations:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![Figure 3.6 – Pairwise Pearson feature correlation of the academic success
    dataset](img/B16690_03_6.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – Pairwise Pearson feature correlation of the academic success dataset
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see three patterns of correlations: first-semester credits, enrollments,
    evaluations, and approvals are all correlated. First-semester and second-semester
    values for these are also correlated. These correlations imply that students tend
    to see through the year once enrolled instead of dropping out mid-semester. Although
    correlated, the correlations aren’t strong enough to consider dropping any features.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: The third correlation pattern is between `Nacionality` and `International`,
    which are strongly correlated.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: The word *Nacionality* refers to *nationality*. We have retained the spelling
    from the original dataset here too for the purpose of consistency.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'A closer look at `Nacionality` shows that almost all rows have a single value:
    the country where the dataset was collected. The strong correlation implies the
    same for `International`:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The following screenshot shows a stacked bar plot of the nationalities:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 – Distribution of the ‘Nacionality’ feature, showing almost all
    rows have a single value in the academic success dataset](img/B16690_03_7.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – Distribution of the ‘Nacionality’ feature, showing almost all rows
    have a single value in the academic success dataset
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: The distribution of `'Nacionality'` and `'International'` means that they are
    not very informative (nearly all rows have the same value), so we can drop them
    from the dataset.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we notice the `''Gender''` feature. When working with gender, it’s
    always good to check for bias. We can visualize the distribution of the `''Gender''`
    feature relative to the target classes using a histogram. The results are shown
    in the screenshot that follows this code snippet:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![Figure 3.8 – Distribution of the ‘Gender’ feature in the academic success
    dataset](img/B16690_03_8.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 – Distribution of the ‘Gender’ feature in the academic success dataset
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: There is a slight bias toward female students, but not enough to warrant concern.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Modeling
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can now prepare our dataset for modeling. We must map our `Target` values
    to integers and drop the `Nacionality` and `International` features. We also need
    to remove the spaces in the feature names. LightGBM cannot work with spaces in
    the names; we can replace them with underscores:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We train and compare four models: a LightGBM GBDT, a LightGBM DART tree, a
    LightGBM DART tree with GOSS, and a scikit-learn random forest.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: We’ll perform parameter optimization with 5-fold cross-validation using `GridSearchCV`
    to ensure good performance for the models.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code sets up the parameter optimization for the GBDT. A similar
    pattern is followed for the other models, which can be seen in the source code:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Running the preceding code takes some time, but once completed, it prints the
    best parameters found along with the score of the best model.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'After all the models are trained, we can evaluate each using F1-scoring, taking
    the mean of 5-fold cross-validation, using the best parameters found. The following
    code illustrates how to do this for the GBDT model:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Jupyter notebooks for [the parameter optimization for each model are available
    in the GitHub repository: https://github.com/Pack](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3)tPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table summarizes the best parameter values found and the cross-validated
    F1 scores for each model:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **Learning** **Rate** | **Max** **Depth** | **Min Child** **Samples**
    | **N** **Estimators** | **Num** **Leaves** | **Min** **Samples Leaf** | **Min**
    **Samples Split** | **F1 score** |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
- en: '| GBDT | 0.1 | - | 10 | 100 | 32 | N/A | N/A | 0.716 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
- en: '| DART | 0.1 | 128 | 30 | 150 | 128 | N/A | N/A | 0.703 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
- en: '| DART (GOSS) | 0.1 | 128 | 30 | 150 | 128 | N/A | N/A | 0.703 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
- en: '| Random Forest | N/A | N/A | N/A | 150 | N/A | 10 | 20 | 0.665 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
- en: Table 3.1 – Summary of best parameters found for each model with the corresponding
    F1 scores
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: As we can see from the table, the LightGBM models performed much better than
    the scikit-learn random forest. Both DART models achieved nearly the same F1 score,
    with GOSS having a slightly lower F1 score (the table values are rounded to 3
    digits).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our end-to-end example of exploring a dataset and building an
    optimized model for the dataset (using parameter grid search). We look at more
    complicated datasets in the coming chapters and delve deeper into analyzing model
    performance.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced LightGBM as a library to train boosted machines efficiently.
    We looked at where the complexity of building GBDTs comes from and the features
    in LightGBM that address them, such as histogram-based sampling, feature bundling,
    and GOSS. We also reviewed LightGBM’s most important hyperparameters.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: We also gave a detailed overview of using LightGBM in Python, covering both
    the LightGBM Python API and the scikit-learn API. We then built our first tuned
    models using LightGBM to predict student academic performance, utilizing cross-validation
    and grid-search-based parameter optimization.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we compare LightGBM against another popular gradient-boosting
    library, XGBoost, and DL techniques for tabular data.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '| *[**1]* | *G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye and
    T.-Y. Liu, “LightGBM: A Highly Efficient Gradient Boosting Decision Tree,” in
    Advances in Neural Information Processing* *Systems, 2017.* |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
- en: '| *[**2]* | *M. Mehta, R. Agrawal and J. Rissanen, “SLIQ: A fast scalable classifier
    for data mining,” in Advances in Database Technology—EDBT’96: 5th International
    Conference on Extending Database Technology Avignon, France, March 25-29, 1996
    Proceedings* *5, 1996.* |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
- en: '| *[**3]* | *J. Shafer, R. Agrawal, M. Mehta and others, “SPRINT: A scalable
    parallel classifier for data mining,” in* *Vldb, 1996.* |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
- en: '| *[**4]* | *S. Ranka and V. Singh, “CLOUDS: A decision tree classifier for
    large datasets,” in Proceedings of the 4th Knowledge Discovery and Data Mining*
    *Conference, 1998.* |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
- en: '| *[**5]* | *H. Shi, “Best-first decision tree* *learning,” 2007.* |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
- en: '| *[**6]* | *R. Kleiman and D. Page, “Aucμ: A performance metric for multi-class
    machine learning models,” in International Conference on Machine* *Learning, 2019.*
    |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
- en: '| *[**7]* | *V. Realinho, J. Machado, L. Baptista and M. V. Martins, Predicting
    student dropout and academic success,* *Zenodo, 2021.* |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
