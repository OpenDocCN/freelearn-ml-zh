<html><head></head><body>
		<div id="_idContainer342">
			<h1 id="_idParaDest-113"><em class="italic"><a id="_idTextAnchor118"/>Chapter 12</em><span class="superscript">: Introducing Hyperparameter Tuning Decision Map</span></h1>
			<p><span class="superscript">Getting too much information can sometimes lead to confusion, which can, in turn, lead back to adopting the simplest option. We learned about numerous hyperparameter tuning methods in the previous chapters. Although we have discussed the ins and outs of each method, it will be very useful for us to have a single source of truth that can be used to help us decide which method to use in which situation.</span></p>
			<p><span class="superscript">In this chapter, you’ll be introduced to the </span><strong class="bold">Hyperparameter Tuning Decision Map</strong> (<strong class="bold">HTDM</strong>), which summarizes all of the discussed hyperparameter tuning methods into a simple decision map based on many aspects, including the properties of the hyperparameter space, the complexity of the objective function, training data size, computational resources availability, prior knowledge availability, and the types of ML algorithms we are working with. There will be also three study cases that show how to utilize HTDM in practice.</p>
			<p>By the end of this chapter, you’ll be able to utilize HTDM in practice to help you decide which hyperparameter tuning method to be adopted in your specific situation. </p>
			<p>In this chapter, we will cover the following main topics:</p>
			<ul>
				<li>Getting familiar with HTDM</li>
				<li>Case study 1 – using HTDM with a CatBoost classifier</li>
				<li>Case study 2 – using HTDM with a conditional hyperparameter space</li>
				<li>Case study 3 – using HTDM with prior knowledge of the hyperparameter values</li>
			</ul>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor119"/>Getting familiar with HTDM</h1>
			<p>HTDM is designed to help you decide which hyperparameter tuning method should be adopted <a id="_idIndexMarker605"/>in a particular situation (see <em class="italic">Figure 12.1</em>). Here, the situation is defined based on six aspects:</p>
			<ul>
				<li>Hyperparameter space properties, including the size of the space, types of hyperparameter values (numerical only or mixed), and whether it contains conditional hyperparameters or not</li>
				<li>Objective function complexity: whether it is a cheap or expensive objective function</li>
				<li>Computational resource availability: whether or not you have enough parallel computational resources</li>
				<li>Training data size: whether you have a few, moderate, or a large number of training samples</li>
				<li>Prior knowledge availability: whether you have prior knowledge of the good range of hyperparameter values</li>
				<li>Types of ML algorithms: whether you are working with a small, medium, or large-sized model, and whether you are working with a traditional ML or deep learning type of algorithm</li>
			</ul>
			<p>This can be seen in the following diagram:</p>
			<div>
				<div id="_idContainer329" class="IMG---Figure">
					<img src="image/B18753_12_001.jpg" alt="Figure 12.1 – Hyperparameter Tuning Decision Map&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.1 – HTDM</p>
			<p>The definition of <em class="italic">Small</em>, <em class="italic">Medium</em>, and <em class="italic">Large</em> in HTDM is very subjective. However, you can refer <a id="_idIndexMarker606"/>to the following table as a rule of thumb:</p>
			<div>
				<div id="_idContainer330" class="IMG---Figure">
					<img src="image/B18753_12_002.jpg" alt="Figure 12.2 – Rule of thumb of size definition&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.2 – Rule of thumb of size definition</p>
			<p>The following <a id="_idIndexMarker607"/>important notes may also help us decide which hyperparameter tuning method we should adopt in a particular situation:</p>
			<div>
				<div id="_idContainer331" class="IMG---Figure">
					<img src="image/B18753_12_003_(a).jpg" alt="Figure 12.3 – Important notes for each hyperparameter tuning method&#13;&#10;"/>
				</div>
			</div>
			<div>
				<div id="_idContainer332" class="IMG---Figure">
					<img src="image/B18753_12_003_(b).jpg" alt="Figure 12.3 – Important notes for each hyperparameter tuning method&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.3 – Important notes for each hyperparameter tuning method</p>
			<p>In this section, we discussed HTDM, along with several additional important notes to help you <a id="_idIndexMarker608"/>decide which hyperparameter tuning method you should adopt in a particular situation. In the next few sections, we will learn how to utilize HTDM in practice through several interesting study cases.</p>
			<h1 id="_idParaDest-115"><a id="_idTextAnchor120"/>Case study 1 – using HTDM with a CatBoost classifier </h1>
			<p>Let’s say <a id="_idIndexMarker609"/>we are training a classifier based on the marketing campaign data that was introduced in <a href="B18753_07_ePub.xhtml#_idTextAnchor062"><em class="italic">Chapter 7</em></a><em class="italic">, Hyperparameter Tuning via scikit</em>. Here, we are utilizing CatBoost (see <a href="B18753_11_ePub.xhtml#_idTextAnchor110"><em class="italic">Chapter 11</em></a><em class="italic">, Understanding Hyperparameters of Popular Algorithms</em>) as <a id="_idIndexMarker610"/>the classifier. This is our first time working with the given data. The laptop we are using only has a single-core CPU and the hyperparameter space is defined as follows. Note that we are not working with a conditional hyperparameter space:</p>
			<ul>
				<li><strong class="source-inline">iterations</strong>: <strong class="source-inline">randint(5,200)</strong></li>
				<li><strong class="source-inline">depth</strong>: <strong class="source-inline">randint(3,10)</strong></li>
				<li><strong class="source-inline">learning_rate</strong>: <strong class="source-inline">np.linspace(1e-5,1e-3,20)</strong></li>
				<li><strong class="source-inline">l2_leaf_reg</strong>: <strong class="source-inline">np.linspace(1,30,30)</strong></li>
				<li><strong class="source-inline">one_hot_max_size</strong>: <strong class="source-inline">randint(2,15)</strong></li>
			</ul>
			<p>Based on the given case description, we can try to utilize HTDM to help us choose which hyperparameter tuning suits the condition the best. First of all, we know that we do not have any prior knowledge or meta-learning results of the good hyperparameter values on the given data. This means we will only focus on the right branch of the first node in HTDM, as shown here:</p>
			<div>
				<div id="_idContainer333" class="IMG---Figure">
					<img src="image/B18753_12_004.jpg" alt="Figure 12.4 – Case study 1, no prior knowledge&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.4 – Case study 1, no prior knowledge</p>
			<p>We <a id="_idIndexMarker611"/>know that we are not working with a conditional hyperparameter space. This means we will <a id="_idIndexMarker612"/>only focus on the right branch of the second node, as shown here:</p>
			<div>
				<div id="_idContainer334" class="IMG---Figure">
					<img src="image/B18753_12_005.jpg" alt="Figure 12.5 – Case study 1, not a conditional hyperparameter space&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.5 – Case study 1, not a conditional hyperparameter space</p>
			<p>Based on a rough estimation, our CatBoost model’s size should be in the range of small to medium-sized. This means we will only focus on the left and bottom branches of the third node, as shown here:</p>
			<div>
				<div id="_idContainer335" class="IMG---Figure">
					<img src="image/B18753_12_006.jpg" alt="Figure 12.6 – Case study 1, small to medium model size&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.6 – Case study 1, small to medium model size</p>
			<p>We <a id="_idIndexMarker613"/>also have a medium-sized hyperparameter space that consists of only numerical values. This <a id="_idIndexMarker614"/>means our options are Coarse-to-Fine, Random Search, PSO, Simulated Annealing, and Genetic Algorithm. It is worth noting that even though our hyperparameter space consists of only numerical values, we can still utilize hyperparameter tuning methods that work with mixed types of values:</p>
			<div>
				<div id="_idContainer336" class="IMG---Figure">
					<img src="image/B18753_12_007.jpg" alt="Figure 12.7 – Case study 1, medium-sized hyperparameter space with only numerical values&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.7 – Case study 1, medium-sized hyperparameter space with only numerical values</p>
			<p>So, how <a id="_idIndexMarker615"/>do we choose a hyperparameter tuning method from the selected options? First, we know that PSO only works very well on the continuous type of hyperparameter values <a id="_idIndexMarker616"/>while we also have integers in the hyperparameter space. Thus, we can remove PSO from our opti<a id="_idTextAnchor121"/>ons. This leaves us with the remaining four options. One easy and effective way to choose the best hyperparameter tuning method is by choosing the simplest method, which is the Random Search method.</p>
			<p>In this section, we discussed the first case study on how to utilize HTDM in practice. In the next section, we will do the same using another interesting case study.</p>
			<h1 id="_idParaDest-116"><a id="_idTextAnchor122"/>Case study 2 – using HTDM with a conditional hyperparameter space</h1>
			<p>Let’s <a id="_idIndexMarker617"/>say we are faced with a similar condition as in the previous section but now, we <a id="_idIndexMarker618"/>are working with a conditional hyperparameter space, as defined here:</p>
			<pre class="source-code">one_hot_max_size = randint(2,15)</pre>
			<pre class="source-code">iterations = randint(5,200)</pre>
			<pre class="source-code">If iterations &lt; 50:</pre>
			<pre class="source-code">   depth = randint(3,10)</pre>
			<pre class="source-code">   learning_rate = np.linspace(5e-4,1e-3,10)</pre>
			<pre class="source-code">   l2_leaf_reg = np.linspace(1,15,20)</pre>
			<pre class="source-code">elif iterations &lt; 100:</pre>
			<pre class="source-code">   depth = randint(3,7)</pre>
			<pre class="source-code">   learning_rate = np.linspace(1e-5,5e-4,10)</pre>
			<pre class="source-code">l2_leaf_reg = np.linspace(5,20,20)</pre>
			<pre class="source-code">else:</pre>
			<pre class="source-code">     depth = randint(3,5)</pre>
			<pre class="source-code">     learning_rate = np.linspace(1e-6,5e-5,10)</pre>
			<pre class="source-code">l2_leaf_reg = np.linspace(5,30,20)</pre>
			<p>Based <a id="_idIndexMarker619"/>on the given case description, we can try to utilize HTDM again to help us choose which hyperparameter tuning method suits the condition the best. Here, similar <a id="_idIndexMarker620"/>to the previous study case, we know that we do not have any prior knowledge or meta-learning results of the good hyperparameter values on the given data. This means we will only focus on the right branch of the first node in HTDM (see <em class="italic">Figure 12.4</em>). However, in this case, we are now working with a conditional hyperparameter space. This means we will only focus on the left branch of the second node, as shown here:</p>
			<div>
				<div id="_idContainer337" class="IMG---Figure">
					<img src="image/B18753_12_008.jpg" alt="Figure 12.8 – Case study 2, a conditional hyperparameter space&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.8 – Case study 2, a conditional hyperparameter space</p>
			<p>Since <a id="_idIndexMarker621"/>we have more than 10,000 samples of training data (see <a href="B18753_07_ePub.xhtml#_idTextAnchor062"><em class="italic">Chapter 7</em></a><em class="italic">, Hyperparameter Tuning via scikit</em>), we only have two hyperparameter tuning methods to choose from based on HTDM, namely the BOHB or Random Search method (see <em class="italic">Figure 12.9</em>). Choosing Random Search over BOHB surely is a wise choice if we only <a id="_idIndexMarker622"/>compare them based on the simplicity of the implementation since we need to install the Microsoft NNI package just to adopt the BOHB method (see <em class="italic">Figure 12.3</em>). </p>
			<p>However, we know that we are working with a model that is not very small, and BOHB can decide which subspace needs to be searched based on previous experiences, not based on luck. Thus, in theory, BOHB will be a better choice to save us time searching for the best set of hyperparameters. So, which method should we pick? It’s up to your discretion:</p>
			<div>
				<div id="_idContainer338" class="IMG---Figure">
					<img src="image/B18753_12_009.jpg" alt="Figure 12.9 – Case study 2, large training data&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.9 – Case study 2, large training data</p>
			<p>In this <a id="_idIndexMarker623"/>section, we discussed the second case study on how to utilize HTDM in practice. In <a id="_idIndexMarker624"/>the next section, we will do the same using another interesting case study.</p>
			<h1 id="_idParaDest-117"><a id="_idTextAnchor123"/>Case study 3 – using HTDM with prior knowledge of the hyperparameter values</h1>
			<p>Let’s <a id="_idIndexMarker625"/>say, in this case, we are also faced with a similar condition as in the previous case <a id="_idIndexMarker626"/>study, but this time, we have prior knowledge of the good hyperparameter values for the given data since one of the data scientists in our team has worked with the same data previously. This means we will only focus on the left branch of the first node in HTDM, as shown here:</p>
			<div>
				<div id="_idContainer339" class="IMG---Figure">
					<img src="image/B18753_12_010.jpg" alt="Figure 12.10 – Case study 3, have prior knowledge&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.10 – Case study 3, have prior knowledge</p>
			<p>Based <a id="_idIndexMarker627"/>on the given case description, we know that we do not have enough parallel computational resources since we only have a single-core CPU. This means we will only focus <a id="_idIndexMarker628"/>on the right branch of the second node, as shown here:</p>
			<div>
				<div id="_idContainer340" class="IMG---Figure">
					<img src="image/B18753_12_011.jpg" alt="Figure 12.11 – Case study 3, not enough parallel computational resources&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.11 – Case study 3, not enough parallel computational resources</p>
			<p>We also know that we have a medium-sized hyperparameter space that only consists of numerical types of values. This means our options are SMAC, TPE, and Metis:</p>
			<div>
				<div id="_idContainer341" class="IMG---Figure">
					<img src="image/B18753_12_012.jpg" alt="Figure 12.12 – Case study 3, medium-sized hyperparameter space with only numerical values&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.12 – Case study 3, medium-sized hyperparameter space with only numerical values</p>
			<p>Based <a id="_idIndexMarker629"/>on the preceding diagram, we know that SMAC works best when the hyperparameter space is dominated by categorical hyperparameters, which is not the case here. Thus, we can remove SMAC from our options. If we try to decide based on the <a id="_idIndexMarker630"/>implementation popularity, then TPE is the one we should choose since it’s implemented in Hyperopt, Optuna, and NNI, while Metis is only implemented in NNI. However, one of the main selling points of Metis is its ability to suggest the set of hyperparameters we should test in our next trial. So, which method should we pick? It’s up to you.</p>
			<p>In this section, we discussed the third case study on how to utilize HTDM in practice. Now, let’s summarize this chapter.</p>
			<h1 id="_idParaDest-118"><a id="_idTextAnchor124"/>Summary</h1>
			<p>In this chapter, we summarized all of the hyperparameter tuning methods we’ve discussed so far in a simple decision map called HTDM. This can help you to choose which method is the most suitable for your specific problem. We also discussed several important notes for each of the hyperparameter tuning methods and saw how to utilize the HTDM in practice. From now on, you’ll be able to utilize HTDM in practice to help you decide which hyperparameter tuning method to adopt in your specific situation.</p>
			<p>In the next chapter, we’ll discuss the need to track hyperparameter tuning experiments and learn how to do so using several open source packages. </p>
		</div>
		<div>
			<div id="_idContainer343">
			</div>
		</div>
	</body></html>