- en: Classification - Spam Email Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What makes you you? I have dark hair, pale skin, and Asiatic features. I wear
    glasses. My facial structure is vaguely round, with extra subcutaneous fat in
    my cheeks compared to my peers. What I have done is describe the features of my
    face. Each of these features described can be thought of as a point within a probability
    continuum. What is the probability of having dark hair? Among my friends, dark
    hair is a very common feature, and so are glasses (a remarkable statistic is out
    of the 300 people or so I polled on my Facebook page, 281 of them require prescription
    glasses). The epicanthic folds of my eyes are probably less common, as is the
    extra subcutaneous fat in my cheeks.
  prefs: []
  type: TYPE_NORMAL
- en: Why am I bringing up my facial features in a chapter about spam classification?
    It's because the principles are the same. If I show you a photo of a human face,
    what is the probability that the photo is of me? We can say that the probability
    that the photo is a photo of my face is a combination of the probability of having
    dark hair, the probability of having pale skin, the probability of having an epicanthic
    fold, and so on, and so forth. From a Naive point of view, we can think of each
    of the features independently contributing to the probability that the photo is
    me—the fact that I have an epicanthic fold in my eyes is independent from the
    fact that my skin is of a yellow pallor. But, of course, with recent advancements
    in genetics, this has been shown to be patently untrue. These features are, in
    real life, correlated with one another. We will explore this in a future chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Despite a real-life dependence of probability, we can still assume the Naive
    position and think of these probabilities as independent contributions to the
    probability that the photo is one of my face.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will build a email spam classification system using a Naive
    Bayes algorithm, which can be used beyond email spam classification. Along the
    way, we will explore the very basics of natural language processing, and how probability
    is inherently tied to the very language we use. A probabilistic understanding
    of language will be built up from the ground with the introduction of the **term
    frequency-inverse document frequency** (**TF-IDF**), which will then be translated
    into Bayesian probabilities, which is used to classify the emails.
  prefs: []
  type: TYPE_NORMAL
- en: The project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'What we want to do is simple: given an email, is it kosher (which we call ham),
    or is it a spam email? We will be using the `LingSpam` database. The emails from
    that database are a little dated—spammers update their techniques and words all
    the time. However, I chose the `LingSpam` corpus for a good reason: it is already
    nicely preprocessed. The original scope of this chapter was to introduce the preprocessing
    of emails; however, the topic of preprocessing options for natural language is
    itself a topic for an entire book, so we will use a dataset that has already been
    preprocessed. This allows us to focus more on the mechanics of a very elegant
    algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: Fear not, though, as I will actually walk through the brief basics of preprocessing.
    Be warned, however, that the level of complexity jumps up in a very steep curve,
    so be prepared to be sucked into a black hole of many hours on preprocessing natural
    language. At the end of this chapter, I will also recommend some libraries that
    will be useful for preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory data analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s jump into the data. The `LingSpam` corpus comes with four variants of
    the same corpus: `bare`, `lemm`, `lemm_stop`, and `stop`. In each variant, there
    are ten parts and each part contains multiple files. Each file represents an email.
    Files with a `spmsg` prefix in its name are spam, while the rest are ham. An example
    email looks as follows (from the `bare` variant):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are some things to note about this particular email:'
  prefs: []
  type: TYPE_NORMAL
- en: This is an email about linguistics—specifically, about the parsing of a natural
    sentence into multiple **noun phrases** (**np**). This is a largely irrelevant
    fact to the project at hand. I do, however, think it's a good idea to go through
    the topics, if only to provide a sanity check on manual occasions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is an email and a person attached to this email—the dataset is not particularly
    anonymized. This has some implications in the future of machine learning, which
    I will explore in the final chapter of this book.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The email is very nicely split into fields (that is, space separated for each
    word).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The email has a `Subject` line.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first two points are particularly noteworthy. Sometimes, the subject matter
    actually matters in machine learning. In our case, we can build our algorithms
    to be blind—they can be used generically across all emails. But there are times
    where being context-sensitive will bring new heights to your machine-learning
    algorithms. The second thing to note is anonymity. We live in an age where software
    flaws are often the downfall of companies. Doing machine learning on non-anonymous
    datasets are often fraught with biases. We should try to anonymize data as much
    as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When dealing with natural language sentences, the first activity is typically
    to tokenize the sentence. Given a sentence that reads such as `The child was learning
    a new word and was using it excessively. "Shan't!", she cried`. We need to split
    the sentence into the components that make up the sentence. We call each component
    a token, hence the name of the process is **tokenization**. Here's one possible
    tokenization method, in which we do a simple `strings.Split(a, " ")`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a simple program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the output we will get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now think about this in the context of adding words to a dictionary to learn.
    Let''s say we want to use the same set of English words to form a new sentence:
    `she shan''t be learning excessively.` (Forgive the poor implications in the sentence).
    We add it to our program, and see if it shows up in the dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This leads to the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'A superior tokenization algorithm would yield a result as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'A particular thing to note is that the symbols and punctuation are now tokens.
    Another particular thing to note is `shan''t` is now split into two tokens: `sha` and
    `n''t`. The word `shan''t` is a contraction of *shall* and *not*; therefore, it
    is tokenized into two words. This is a tokenization strategy that is unique to
    English. Another unique point of English is that words are separated by a boundary
    marker—the humble space. In languages where there are no word boundary markers,
    such as Chinese or Japanese, the process of tokenization becomes significantly
    more complicated. Add to that languages such as Vietnamese, where there are markers
    for boundaries of syllables, but not words, and you have a very complicated tokenizer
    at hand.'
  prefs: []
  type: TYPE_NORMAL
- en: The details of a good tokenization algorithm are fairly complicated, and tokenization
    is worthy of a book to itself, so we `shan't` cover it here.
  prefs: []
  type: TYPE_NORMAL
- en: The best part about the `LingSpam` corpus is that the tokenization has already
    been done. Some notes such as compound words and contractions are not tokenized
    into different tokens such as the example of `shan't`. They are treated as a single
    word. For the purposes of a spam classifier, this is fine. However, when working
    with different types of NLP projects, the reader might want to consider better
    tokenization strategies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a final note about tokenization strategies: English is not a particularly
    regular language. Despite this, regular expressions are useful for small datasets.
    For this project, you may get away with the following regular expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '``const re = `([A-Z])(\.[A-Z])+\.?|\w+(-\w+)*|\$?\d+(\.\d+)?%?|\.\.\.|[][.,;"''?():-_`
    + "`]"``'
  prefs: []
  type: TYPE_NORMAL
- en: Normalizing and lemmatizing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, I wrote that all the words in the second example,
    `she shan''t be excessively learned`, are already in the dictionary from the first
    sentence. The observant reader might note the word `be` isn''t actually in the
    dictionary. From a linguistics point of view, that isn''t necessarily false. The
    word `be` is the root word of `is`, of which `was` is the past tense. Here, there
    is a notion that instead of just adding the words directly, we should add the
    root word. This is called **lemmatization**. Continuing from the previous example,
    the following are the lemmatized words from the first sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Again, here I would like to point out some inconsistencies that will be immediately
    obvious to the observant reader. Specifically, the word `excessively` has the
    root word of `excess`. So why was `excessively` listed? Again, the task of lemmatization
    isn't exactly a straightforward lookup of the root word in a dictionary. Often,
    in complex NLP related tasks, the words have to be lemmatized according to the
    context they are in. That's beyond the scope of this chapter because, as before,
    it's a fairly involved topic that could span an entire chapter of a book on NLP
    preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s go back to the topic of adding a word to a dictionary. Another useful
    thing to do is to normalize the words. In English, this typically means lowercasing
    the text, replacing unicode combination characters and the like. In the Go ecosystem,
    there is an extended standard library package that does just this: `golang.org/x/text/unicode/norm`.
    In particular, if we are going to work on real datasets, I personally prefer a
    NFC normalization schema. A good resource on string normalization is on the Go
    blog post as well: [https://blog.golang.org/normalization](https://blog.golang.org/normalization).
    The content is not specific to Go, and is a good guide to string normalization
    in general.'
  prefs: []
  type: TYPE_NORMAL
- en: The `LingSpam` corpus comes with variants that are normalized (by lowercasing
    and NFC) and lemmatized. They can be found in the `lemm` and `lemm_stop` variants
    of the corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Stopwords
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By reading this, I would assume the reader is familiar with English. And you
    may have noticed that some words are used more often than others. Words such as `the`,
    `there`, `from`, and so on. The task of classifying whether an email is spam or
    ham is inherently statistical in nature. When certain words are used often in
    a document (such as an email), it conveys more weight about what that document
    is about. For example, I received an email today about cats (I am a patron of
    the Cat Protection Society). The word `cat` or `cats` occurred eleven times out
    of the 120 or so words. It would not be difficult to assume that the email is
    about cats.
  prefs: []
  type: TYPE_NORMAL
- en: However, the word `the` showed up 19 times. If we were to classify the topic
    of the email by a count of words, the email would be classified under the topic
    `the`. Connective words such as these are useful in understanding the specific
    context of the sentences, but for a Naïve statistical analysis, they often add
    nothing more than noise. So, we have to remove them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Stopwords are often specific to projects, and I''m not a particular fan of
    removing them outright. However, the `LingSpam` corpus has two variants: `stop` and
    `lemm_stop`, which has the stopwords list applied, and the stopwords removed.'
  prefs: []
  type: TYPE_NORMAL
- en: Ingesting the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, without much further ado, let''s write some code to ingest the data. First,
    we need a data structure of a training example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The reason for this is so that we can parse our files into a list of `Example`.
    The function is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, I used `filepath.Glob` to find a list of files that matches the pattern
    within the specific directory, which is hardcoded. It doesn''t have to be hardcoded
    in your actual code, but hardcoding the path makes for simpler demo programs.
    For each of the matching filenames, we parse the file using the `ingestOneFile` function.
    Then we check whether the filename contains `spmsg` as a prefix. If it does, we
    create an `Example` that has `Spam` as its class. Otherwise, it will be marked
    as `Ham`. In the later sections of this chapter, I will walk through the `Class` type
    and the rationale for choosing it. For now, here''s the `ingestOneFile` function.
    Take note of its simplicity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Handling errors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a central thesis in some programming language theories that errors
    in most programs happen at the boundary. While there are many interpretations
    of this thesis (boundaries of what? Some scholars think it's at the boundaries
    of functions; some think it's at the boundaries of computation), what is certainly
    true from experience is that boundaries of I/O are where the most errors happen.
    Hence, we have to be extra careful when dealing with input and output.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the purposes of ingesting the files, we define an `errList` type as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: That way we can continue, even if an error happens while reading a file. The
    error will be bubbled back all the way to the top without causing any panic.
  prefs: []
  type: TYPE_NORMAL
- en: The classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we continue to build our classifier, let''s imagine what the main function
    will look as follows. It will look something similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The use of `Train` and `Predict` as exported methods are useful in guiding
    us on what to build next. From the sketch in the preceding code block, we need
    a `Classifier` type, that has `Train` and `Predict` at the very least. So we''ll
    start by doing that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: So, now, it becomes a question of how the classifier works.
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The classifier is a Naive Bayes classifier. To break it down, Naive in the
    phrase Naive Bayes means that we are assuming that all the input features are
    independent. To understand how the classifier works, an additional component needs
    to be introduced first: the **term frequency**-**inverse frequency** (**TF**-**IF**)
    pair of statistics.'
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'TF-IDF, per its namesake, is comprised of two statistics: **term frequency**
    (**TF**) and **inverse document frequency** (**IDF**).'
  prefs: []
  type: TYPE_NORMAL
- en: The central thesis to TF is that if a word (called a **term**) occurs many times
    in a document, it means that the document revolves more around that word. It makes
    sense; look at your emails. The keywords typically revolve around a central topic.
    But TF is a lot more simplistic than that. There is no notion of topics. It's
    just a count of how many times a word happens in a document.
  prefs: []
  type: TYPE_NORMAL
- en: 'IDF, on the other hand, is a statistic that determines how important a term
    is to a document. In the examples we''ve seen, do note that the word `Subject`,
    with a capital `S` occurs once in both types of documents: spam and ham. In broad
    strokes, IDF is calculated by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ef3186a5-8b35-4ae4-9d6c-169b821834dc.png).'
  prefs: []
  type: TYPE_NORMAL
- en: The exact formula varies and there are subtleties to each variation, but all
    adhere to the notion of dividing the total number of documents over the frequency
    of the term.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the purposes of our project, we will be using the `tf-idf` library from
    `go-nlp`, which is a repository of NLP-related libraries for Go. To install it,
    simply run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: It is an extremely well, tested library, with 100% test coverage.
  prefs: []
  type: TYPE_NORMAL
- en: When used together, ![](img/45393d95-43fe-4cf0-874e-c4c23c09f1ea.png) represents
    a useful weighting scheme for calculating the importance of a word in a document.
    It may seem simple, but it is very powerful, especially when used in the context
    of probability.
  prefs: []
  type: TYPE_NORMAL
- en: Do note that TF-IDF cannot strictly be interpreted as a probability. There are
    some theoretical nastiness that presents itself when strictly interpreting IDF
    as a probability. Hence, in the context of this project, we will be treating TF-IDF
    as a sort of weighting scheme to a probability.
  prefs: []
  type: TYPE_NORMAL
- en: Now we are ready to talk about the basics of the Naive Bayes algorithm. But
    first I'd like to further emphasize certain intuitions of Bayes' theorem.
  prefs: []
  type: TYPE_NORMAL
- en: Conditional probability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll start with the notion of conditional probability. To set a scene, we''ll
    consider several fruit types:'
  prefs: []
  type: TYPE_NORMAL
- en: Apple
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avocado
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Banana
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pineapple
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nectarine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mango
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strawberry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For each fruit type, we will have several instances of those fruits—so we could
    have a green Granny Smith and a red Red Delicious in the class of apples. Likewise,
    we could have ripe and unripe fruits—mangoes and bananas could be yellow (ripe)
    or green (unripe), for example. Lastly, we can also classify these fruits by what
    kind of fruit it is—tropical (avocado, banana, pineapple, and mango) versus non-tropical
    fruits:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Fruit** | **Can be green** | **Can be yellow** | **Can be red** | **Is
    tropical** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Apple** | yes | no | yes | no |'
  prefs: []
  type: TYPE_TB
- en: '| **Avocado** | yes | no | no | yes |'
  prefs: []
  type: TYPE_TB
- en: '| **Banana** | yes | yes | no | yes |'
  prefs: []
  type: TYPE_TB
- en: '| **Lychee** | yes | no | yes | yes |'
  prefs: []
  type: TYPE_TB
- en: '| **Mango** | yes | yes | no | yes |'
  prefs: []
  type: TYPE_TB
- en: '| **Nectarine** | no | yes | yes | no |'
  prefs: []
  type: TYPE_TB
- en: '| **Pineapple** | yes | yes | no | yes |'
  prefs: []
  type: TYPE_TB
- en: '| **Strawberry** | yes | no | yes | no |'
  prefs: []
  type: TYPE_TB
- en: I would like you to now imagine you're blindfolded and you pick a fruit. I will
    then describe a feature of the fruit, and you would guess the fruit.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say the fruit you picked has a yellow outside. What are the possible fruits?
    Nectarines, bananas, pineapples, and mangoes come to mind. If you pick one of
    the options you would have a one in four chance of being correct. We call this
    the probability of yellow ![](img/dce6e753-bfdf-40ed-9684-1a0af16d0bad.png). The
    numerator is the number of yeses along the `Can be yellow` column, and the denominator
    is the total number of rows.
  prefs: []
  type: TYPE_NORMAL
- en: If I give you another feature about the fruit, you can improve your odds. Let's
    say I tell you that the fruit is tropical. Now you have a one in three chance of
    being right—nectarines has been eliminated from the possible choices.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can ask this question: If we know a fruit is tropical, what is the probability
    that the fruit is yellow? The answer is 3/5\. From the preceding table, we can
    see that there are five tropical fruits and three of them are yellow. This is
    called a **conditional probability**. We write it in a formula such as this (for
    the more mathematically inclined, this is the Kolmogorov definition of conditional
    probability):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e921c7f2-f074-4779-b639-d00531b48477.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is how you read the formula: the probability of *A* given *B* is known,
    and we will need to get the probability of *A AND B* happening at the same time
    and the probability of *B* itself.'
  prefs: []
  type: TYPE_NORMAL
- en: The conditional probability of a fruit being yellow, given that it's tropical
    is three in five; there are actually a lot of tropical fruits that are yellow—tropical
    conditions allow for greater depositions of carotinoids and vitamin C during the
    growth of the fruit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at a tabulated result can yield an easier understanding of conditional
    probability. However, it must be noted that the conditional probability *can*
    be calculated. Specifically, to calculate the conditional probability, this is
    the formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5d0ea89d-9e2a-49d1-a27c-95425ba0b4b5.png)'
  prefs: []
  type: TYPE_IMG
- en: The probability of a fruit being yellow *and* tropical (![](img/2ca0214e-2b30-4f5f-890d-f3d377d1c099.png) )
    is three in eight; there are three such fruits, out of a total of eight. The probability
    of a fruit being tropical (![](img/d379de41-c16a-4b6b-97de-6fa1633949e1.png))
    is five in eight; there are five topical fruits out of the eight listed.
  prefs: []
  type: TYPE_NORMAL
- en: 'And now, we are finally ready to figure out how we got to that one in three number.
    The probability of each class of fruits is uniform. If you had to choose randomly,
    you would get it right one in eight of the time. We can rephrase the question
    to this: What is the probability of a fruit being a banana given that it''s yellow
    and tropical?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s rewrite this as a formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d03e4a8c-f03d-464a-971a-5b17c15c517d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It is important that we relied on a special trick to perform the analysis of
    the preceding probabilities. Specifically, we acted as though each *yes* represents
    a singular example existing, while a *no* indicates that there are no examples,
    or, in short, this table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Fruit** | **Is Green** | **Is Yellow** | **Is Red** | **Is Tropical** |'
  prefs: []
  type: TYPE_TB
- en: '| **Apple** | 1 | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **Avocado** | 1 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **Banana** | 1 | 1 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **Lychee ** | 1 | 0 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **Mango** | 1 | 1 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **Nectarine** | 0 | 1 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **Pineapple** | 1 | 1 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **Strawberry** | 1 | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: This will be important for analysis for the spam detection project. The numbers
    in each would be the number of occurrences within the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ve seen from the previous examples, that we need features, such as whether
    a fruit can be green, yellow, or red, or whether it''s tropical. We''re now focused
    on the project at hand. What should the features be?:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Class** | **???** | **???** | **???** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Spam** |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| **Ham** |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: What makes up an email? Words make an email. So, it would be appropriate to
    consider the appearance of each word feature. We can take it further, and take
    the intuition that we have developed previously with TF-IDF and instead use the
    frequency of the words among the document types. Instead of counting 1 for the
    existence, we count the total number of times a word exists in the document types.
  prefs: []
  type: TYPE_NORMAL
- en: 'The table would look something as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Class** | **Has XXX** | **Has Site** | **Has Free** | **Has Linguistics**
    | **...** |'
  prefs: []
  type: TYPE_TB
- en: '| **Spam** | 200 | 189 | 70 | 2 | ... |'
  prefs: []
  type: TYPE_TB
- en: '| **Ham** | 1 | 2 | 55 | 120 | ... |'
  prefs: []
  type: TYPE_TB
- en: That also means that there are many features. We can certainly try to enumerate
    all possible calculations. But doing so would be tedious and quite computationally
    intensive. Instead, we can try to be clever about it. Specifically, we will use
    another definition of conditional probability to do the trick to reduce the amount
    of computations that needs to be done.
  prefs: []
  type: TYPE_NORMAL
- en: Bayes' theorem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A conditional probability formula can also be written as Bayes'' theorem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0a5ddfe-b14b-4efa-8c8b-e41b9e0c7dc6.png)'
  prefs: []
  type: TYPE_IMG
- en: We call ![](img/163c55c0-c926-4f98-ac7b-3d89861ee3d7.png) the prior probability. ![](img/b453362e-b08e-403e-a7d9-d12feb0a7520.png) is
    called the **likelihood**. These are the things we're interested in, as ![](img/6a6f5b33-fc93-4124-887a-b1284152abf6.png) is
    essentially a constant anyway.
  prefs: []
  type: TYPE_NORMAL
- en: The theory at this point is a little dry. How does this relate to our project?
  prefs: []
  type: TYPE_NORMAL
- en: 'For one, we can rewrite the generic Bayes'' theorem to one that fits our project:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3841b295-8be7-4370-9e26-37628bba8f67.png)'
  prefs: []
  type: TYPE_IMG
- en: This formula perfectly encapsulates our project; given a document made up of
    words, what is the probability that it's `Ham` or `Spam`? In the next section,
    I will show you how to translate this formula into a very powerful classifier,
    in fewer than 100 lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: Implementating the classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the earlier parts of the chapter, we sketched out a dummy `Classifier` type
    that does nothing. Let''s make it do something now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, there are introductions to a few things. Let''s walk them through one
    by one:'
  prefs: []
  type: TYPE_NORMAL
- en: We'll start with the `corpus.Corpus` type.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is a type imported from the `corpus package`, which is a subpackage of
    the NLP library for Go, `lingo`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To install `lingo`, simply run `go get -u github.com/chewxy/lingo/...`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To use the `corpus `package, simply import it like so: `import "github.com/chewxy/lingo/corpus"`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bear in mind that in the near future, the package will change to `github.com/go-nlp/lingo`.
    If you are reading this after January 2019, use the new address.
  prefs: []
  type: TYPE_NORMAL
- en: 'A `corpus.Corpus` object simply maps from a word to an integer. The reason
    for doing this is twofold:'
  prefs: []
  type: TYPE_NORMAL
- en: '**It saves on memory**: A `[]int` uses considerably less memory than `[]string`.
    Once a corpus has been converted to be IDs, the memory for the strings can be
    freed. The purpose of this is to provide an alternative to string interning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**String interning is fickle**: String interning is a procedure where for the
    entire program''s memory, only exactly one copy of the string exists. This turns
    out to be harder than expected for most tasks. Integers provide a more stable
    interning procedure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we are faced with two fields which are arrays. Specifically, `tfidfs [MAXCLASS]*tfidf.TFIDF` and
    `totals [MAXCLASS]float64`. At this point, it might be a good idea to talk about
    the `Class` type.
  prefs: []
  type: TYPE_NORMAL
- en: Class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We were introduced to the `Class` type when we were writing the ingestion code.
    This is the definition of `Class`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In other words, `Ham` is `0`, `Spam` is `1`, and `MAXCLASS` is `2`. They're
    all constant values and can't be changed at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'It would be prudent to note upfront, that there are limitations to this approach.
    In particular, it means that you have to know before running the program how many
    classes there will be. In our case, we know that there will be at most two classes:
    `Spam` or `Ham`. If we know there is a third class, say `Prosciutto`, for example,
    then we can code it as a value before `MAXCLASS`. There are many reasons for using
    a constant numerical value typed as a `Class`. Two of the primary reasons would
    be correctness and performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine we have a function that takes `Class` as an input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Someone who uses this function outside this library may pass in `3` as the
    class: `ExportedFn(Class(3))`. We can instantly tell if the value is valid if
    we have a validation function that looks something as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Granted, this is not as nice as other languages, such as Haskell, where you
    could just do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'And let the compiler check for you if that is at the call site, whether the
    value passed in was valid or not. We still want the correctness, so we defer the
    checks to the runtime. `ExportedFn` now reads as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The notion of data types with ranges of valid value is not a revolutionary notion.
    Ada for example, has bounded ranges since the 1990s. And the best part about using
    a constant value as a range with `MAXCLASS` is that we can fake the range checks
    and do them at runtime. In this respect, Go is more or less the same as Python,
    Java, or other unsafe languages. Where this truly shines however, is in performance.
  prefs: []
  type: TYPE_NORMAL
- en: A tip for good software engineering practice is to make your program as knowable
    by the human as possible without sacrificing understanding or neatness. Using
    constant numerical values (or enums) generally allows the human programmer to
    understand the constrains that the value is allowed to have. Having constant string
    values, as we will see in the next section, exposes the programmer to unconstrained
    values. This is where bugs usually happen.
  prefs: []
  type: TYPE_NORMAL
- en: Note that in the `Classifier` struct, both `tfidfs` and `totals` are arrays.
    Unlike slices, arrays in Go do not require an extra layer of indirection when
    accessing values. This makes things a tiny bit faster. But in order to truly understand
    the tradeoffs of this design, we need to look at alternative designs for `Class` and
    with them the alternative designs of the fields, `tfidfs` and `totals`.
  prefs: []
  type: TYPE_NORMAL
- en: Alternative class design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we imagine an alternative design of `Class`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'With this change, we will have to update the definition of `Classifier`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Consider now the steps required to get the totals of class `Ham`:'
  prefs: []
  type: TYPE_NORMAL
- en: The string has to be hashed
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The hash will be used to look up the bucket where the data for `totals` is stored
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An indirection is made to the bucket and the data is retrieved and returned
    to the user
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Consider now the steps required to get the totals of class `Ham` if the class
    design was the original:'
  prefs: []
  type: TYPE_NORMAL
- en: Since `Ham` is a number, we can directly compute the location of the data for
    retrieval and return to the user.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By using a constant value and a numeric definition of the type `Class`, and
    an array type for `totals`, we are able to skip two steps. This yields very slight
    performance improvements. In this project, they're mostly negligible, until your
    data gets to a certain size.
  prefs: []
  type: TYPE_NORMAL
- en: The aim of this section on the `Class` design is to instill a sense of mechanical
    sympathy. If you understand how the machine works, you can design very fast machine
    learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: All this said and done, there is one assumption that underpins this entire exercise.
    This is a `main` package. If you're designing a package that will be reused on
    different datasets, the tradeoff considerations are significantly different. In
    the context of software engineering, overgeneralizing your package often leads
    to leaky abstractions that are hard to debug. Better to write slightly more concrete
    and specific data structures that are purpose built.
  prefs: []
  type: TYPE_NORMAL
- en: Classifier part II
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the main considerations is that a Naive Bayes classifier is a very simple
    program, and very difficult to get wrong. The entire program is in fact fewer
    than 100 lines. Let's look at it further.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have sketched out so far the method `Train`, which will train the classifier
    on a given set of inputs. Here''s how it looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: So here it's very clear that `Train` is an ![](img/8becaf0d-f9e5-4b14-a6a7-dba5530d07d9.png) operation.
    But the function is structured in such a way that it would be trivial to parallelize
    the calls to `c.trainOne`. Within the context of this project, this wasn't necessary
    because the program was able to complete in under a second. However, if you are
    adapting this program for larger and more varied datasets, it may be instructive
    to parallelize the calls. The `Classifier` and `tfidf.TFIDF` structs have mutexes
    in them to allow for these sorts of extensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'But what''s more interesting is the `trainOne` example. Looking at it, all
    it seems to do is to add each word to the corpus, get its ID, and then add the
    ID to the `doc` type. `doc`, incidentally, is defined as such:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This definition is done to fit into the interface that `tfidf.TFIDF.Add` accepts.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look closer at the `trainOne` method. After making the `doc`, the words
    from the example are added to the corpus, while the IDs are then put into the
    `doc`. The `doc` is then added to the `tfidf.TFIDF` of the relevant class.
  prefs: []
  type: TYPE_NORMAL
- en: At first glance, there isn't much training here; we're just adding to the TF
    statistic.
  prefs: []
  type: TYPE_NORMAL
- en: The real magic happens in the `Predict` and `Score `methods.
  prefs: []
  type: TYPE_NORMAL
- en: '`Score` is defined as such:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Given a tokenized sentence, we want to return the `scores` of each class. The
    idea is so that we can then look through the `scores` and find the class with
    the highest score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The `Score` function is worth a deeper look because that's where all the magic
    happens. First, we check the classifier is ready to score. An online machine learning
    system learns as new data comes in. This design means that the classifier cannot
    be used in an online fashion. All the training needs to be done up front. Once
    that training is done, the classifier will be locked, and won't train any further.
    Any new data will have to be part of a different run.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Postprocess` method is quite simple. Having recorded all the TF statistics,
    we now want to calculate the relative importance of each term to the documents.
    The `tfidf` package comes with a simple `Log`-based calculation of the IDF, but
    you can use any other IDF calculating function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'It is important to note that there is an update to the document count of each
    class: `t.Docs = docs` to the sum of all the documents seen. This was because
    as we were adding to the term frequency of each class, the `tfidf.TFIDF` struct
    wouldn''t be aware of documents in other classes.'
  prefs: []
  type: TYPE_NORMAL
- en: The reason we would want to calculate the IDF is to control the values a bit
    more.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that the conditional probability can be written in the Bayes'' theorem
    form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/badb5fed-45e2-4754-9e26-b828ab836786.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s familiarize ourselves with the formula, once again by restating it in
    English, first by familiarizing ourselves with the terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd6757b2-66c0-4bb3-aef7-7d932918498b.png): This is the **prior probability** of
    a class. If we have a pool of email messages and we randomly pick one out, what
    is the probability that the email is `Ham` or `Spam`? This largely corresponds
    to the dataset that we have. From the exploratory analysis, we know that the ratio
    between `Ham` and `Spam` is around 80:20.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/75331005-fca9-4cb6-aa00-a1f76789cbb0.png): This is the **likelihood** of
    any random document belongs to a class. Because a document is comprised of individual
    words, we simply make a Naïve assumption that these words are independent of one
    another. So we want the probability of ![](img/14e84a1b-e473-4149-bcfa-66b3ec599b4c.png).
    Assuming the words are independent gives us the ability to simply multiply the
    probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, to put it in English:'
  prefs: []
  type: TYPE_NORMAL
- en: The conditional probability of a class being Ham given a document is the result
    of multiplying the prior probability of a document being ham and the likelihood
    that the document is Ham.
  prefs: []
  type: TYPE_NORMAL
- en: The observant reader may note that I have elided explanation of ![](img/9001d080-13de-44e8-87e2-29688dc8aca6.png).
    The reason is simple. Consider what the probability of the document is. It's simply
    the multiplication of all the probabilities of a word in the corpus. It doesn't
    in anyway interact with the `Class`. It could well be a constant.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we run into another problem if we do use probabilities multiplied.
    Multiplying probabilities tend to yield smaller and smaller numbers. Computers
    do not have true rational numbers. `float64` is a neat trick to mask the fundamental
    limitations that a computer has. You will frequently run into edge cases where
    the numbers become too small or too big when working on machine learning problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, for this case, we have an elegant solution: We can elect to work
    in the log domain. Instead of considering the likelihood, we would consider the
    log likelihood. Upon taking logs, multiplication becomes addition. This allows
    us to keep it out of sight, and out of mind. For most cases, this project included,
    this is a fine choice. There may be cases where you wish to normalize the probabilities.
    Then, ignoring the denominator wouldn''t work well.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at some code on how to write `priors`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The priors are essentially the proportion of `Ham` or `Spam` to the sum of
    all documents. This is fairly simple. To compute the likelihood, let''s look at
    the loop in `Score`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We incorporate the likelihood function into the scoring function simply for
    ease of understanding. But the important takeaway of the likelihood function is
    that we''re summing the probabilities of the word given the class. How do you
    calculate ![](img/6a3e0ad0-7fa3-4a97-8b09-49bea7234a2b.png) ? such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: First, we check whether the word has been seen. If the word hasn't been seen
    before, then we return a default value `tiny`—a small non-zero value that won't
    cause a division-by-zero error.
  prefs: []
  type: TYPE_NORMAL
- en: The probability of a word occurring in a class is simply its frequency divided
    by the number of words seen by the class. But we want to go a bit further; we
    want to control for frequent words being too important a factor in deciding the
    probability of the class, so we multiply it by the IDF that we had calculated
    earlier. And that's how you'd get the probabilities of the word given a class.
  prefs: []
  type: TYPE_NORMAL
- en: After we have the probability, we take the log of it, and then add it to the
    score.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we have all the pieces. Let''s look at how to put it all together:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first `ingest` the dataset and then split the data out into training and
    cross validation sets. The dataset is split into ten parts for a k-fold cross-validation.
    We won''t do that. Instead, we''ll do a single fold cross-validation by holding
    out 30% of the data for cross-validation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We then train the classifier and then check to see whether the classifier can
    predict its own dataset well:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'After training the classifier, we perform a cross-validation on the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Here, I also added an `unseen` and `totalWords` count, as a simple statistic
    to see how well the classifier can generalize when encountering previously unseen
    words.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Additionally, because we know ahead of time that the dataset comprises roughly
    80% `Ham` and 20% `Spam`, we have a baseline to beat. Simply put, we could write
    a classifier that does this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Imagine we have such a classifier. Then it would be right 80% of the time!
    For us to know that our classifier is good, it would have to beat a baseline.
    For the purposes of this chapter, we simply print out the statistics and tweak
    accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'So, this is what the final `main` function looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Running it on `bare`, this is the result I get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'To see the effects of removing stopwords and lemmatization, we simply switch
    to using the `lemm_stop` dataset, and this is the result I get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Either way, the classifier is brutally effective.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, I have shown the basics of what a Naive Bayes classifier looks
    like—a classifier written with the fundamental understanding of statistics will
    trump any publicly available library any day.
  prefs: []
  type: TYPE_NORMAL
- en: The classifier itself is fewer than 100 lines of code, but with it comes a great
    deal of power. Being able to perform classification with 98% or greater accuracy
    is no mean feat.
  prefs: []
  type: TYPE_NORMAL
- en: 'A note on the 98% figure: This is not state of the art. State of the art is
    in the high 99.xx%. The main reason why there is a race for that final percent
    is because of scale. Imagine you''re Google and you''re running Gmail. A 0.01%
    error means millions of emails being misclassified. That means many unhappy customers.'
  prefs: []
  type: TYPE_NORMAL
- en: For the most part, in machine learning, the case of whether to go for newer
    untested methods really depends on the scale of your problems. In my experience
    from the past 10 years doing machine learning, most companies do not reach that
    scale of data. As such, the humble Naive Bayes classifier would serve very well.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we shall look at one of the most vexing issues that humans
    face: time.'
  prefs: []
  type: TYPE_NORMAL
