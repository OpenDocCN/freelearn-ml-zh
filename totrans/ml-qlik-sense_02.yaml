- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine Learning Algorithms and Models with Qlik
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning algorithms have become an integral part of our lives, from
    the personalization of online ads to the recommendation systems on streaming platforms.
    These algorithms are responsible for making intelligent decisions based on data,
    without being explicitly programmed.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning algorithms refer to a set of mathematical models and techniques
    that enable software to learn patterns and relationships from data, allowing them
    to make predictions and decisions. These algorithms can be broadly categorized
    into supervised, unsupervised, semi-supervised, and reinforcement learning algorithms.
    Each type of algorithm has its own unique characteristics and applications, suiting
    them to a wide range of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will provide an overview of machine learning algorithms
    and their applications, focusing on algorithms used in Qlik tools. Here is what
    you will learn as a part of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understand regression models and how to use these
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand different clustering algorithms and decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand the basics of boosting algorithms, especially the one used in Qlik
    AutoML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand the basics of neural networks and other advanced machine learning
    models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Qlik AutoML was using the following algorithms at the time of writing this
    book:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Binary and multiclass** **classification problems:**'
  prefs: []
  type: TYPE_NORMAL
- en: CatBoost Classification, Elastic Net Regression, Gaussian Naive Bayes, Lasso
    Regression, LightGBM Classification, Logistic Regression, Random Forest Classification,
    XGBoost Classification
  prefs: []
  type: TYPE_NORMAL
- en: '**Regression problems:**'
  prefs: []
  type: TYPE_NORMAL
- en: CatBoost Regression, LightGBM Regression, Linear Regression, Random Forest Regression,
    SGD Regression, XGBoost Regression
  prefs: []
  type: TYPE_NORMAL
- en: Some of these algorithms will be covered in more detail in the coming sections.
  prefs: []
  type: TYPE_NORMAL
- en: Regression models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Regression models** are a type of supervised machine-learning model used
    to predict continuous numerical values for a target variable based on one or more
    input variables. In other words, regression models are used to estimate the relationships
    between the input variables and the output variable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are various types of regression models used in machine learning, some
    of which include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear Regression**: This is a type of regression model that assumes a linear
    relationship between the input variables and the output variable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Polynomial Regression**: This is a type of regression model that assumes
    a polynomial relationship between the input variables and the output variable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logistic Regression**: This is a type of regression model used to predict
    binary or categorical outcomes. It estimates the probability of an event occurring
    based on the input variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ridge Regression**: This is a type of linear regression model that uses regularization
    to prevent overfitting of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lasso Regression**: This is another type of linear regression model that
    uses regularization to prevent overfitting of the model. It is particularly useful
    when dealing with datasets that have a large number of features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we are going to take a closer look at some of the regression
    models in the preceding list.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Linear regression is used for modeling the relationship between a dependent
    variable (also known as the target or response variable) and one or more independent
    variables (also known as the explanatory or predictor variables). The goal of
    linear regression is to find the best-fit line (or hyperplane) that can predict
    the dependent variable based on the values of the independent variables. In other
    words, linear regression tries to find a linear equation that relates the input
    variables to the output variable. The equation takes the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: Y = mX + b + e
  prefs: []
  type: TYPE_NORMAL
- en: where *Y* is the dependent variable, *X* is the independent variable, *m* is
    the slope of the line, *b* is the intercept, and *e* represents the error of the
    model. The goal of linear regression is to find the best values of *m* and *b*
    that minimize the difference between the predicted values and the actual values
    of the dependent variable. In simple linear regression, we have one independent
    variable and in multiple linear regression, we will have multiple independent
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We want to investigate the relationship between the number of hours a student
    studies and their exam score. We collect data from 10 students, recording the
    number of hours they studied and their corresponding exam scores. The data is
    shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Hours** **studied (X)** | **Exam** **score (Y)** |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 60 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 70 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 80 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 85 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 90 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 95 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 100 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 105 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 110 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 115 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2.1: Exam score data'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use a simple linear regression model to model the relationship between
    the two variables, with the number of hours studied as the independent variable
    (*X*) and the exam score as the dependent variable (*Y*). The model takes the
    following form:'
  prefs: []
  type: TYPE_NORMAL
- en: Y = mX + b + e
  prefs: []
  type: TYPE_NORMAL
- en: 'where *b* is the intercept (the value of *Y* when *X=0*) and *m* is the slope
    (the rate at which *Y* changes with respect to *X*). To estimate the values of
    *b* and *m*, we can use the least squares method. Solving for the regression equation,
    we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Y = 5 * X + 55 + e
  prefs: []
  type: TYPE_NORMAL
- en: This equation tells us that, on average, for every additional hour studied,
    a student can expect to score 5 points higher on the exam. The intercept of 55
    tells us that a student who studies 0 hours can expect to score 55 points (which
    may not be realistic, but it’s just the mathematical extrapolation from the model).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use this model to make predictions about exam scores based on the number
    of hours studied. For example, if a student studies for 7 hours, we can estimate
    their exam score to be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Y = 5 * 7+ 55 + e = 90 + e
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Logistic regression is a statistical method used for binary classification problems
    where the outcome variable is categorical and has only two possible values, such
    as “yes” or “no,” “pass” or “fail,” or “spam” or “not spam.” It is a type of regression
    analysis that models the relationship between the independent variables and the
    dependent variable by estimating the probability of the binary outcome.
  prefs: []
  type: TYPE_NORMAL
- en: The logistic regression model uses a logistic function, also known as the sigmoid
    function, to model the relationship between the input variables and the binary
    outcome. The sigmoid function transforms the input values into a range between
    0 and 1, which represents the probability of the binary outcome.
  prefs: []
  type: TYPE_NORMAL
- en: The logistic regression model can be trained using a maximum likelihood estimation
    method to find the parameters that maximize the likelihood of the observed data
    given the model. These parameters can then be used to predict the probability
    of the binary outcome for new input data.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression is widely used in medical diagnosis, credit scoring, and
    marketing analysis. It is a popular algorithm due to its simplicity and interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s say we want to predict whether or not a customer will purchase a product
    based on their age and income. We have a dataset of 90 customers, where each row
    represents a customer and the columns represent their age, income, and whether
    or not they purchased the product (0 for not purchased, 1 for purchased).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use logistic regression to model the probability of a customer purchasing
    the product based on their age and income. The logistic regression model can be
    expressed as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: p(purchase) =  1  ______________________________   (1 + exp(− (β0 + β1 * age
    + β2 * income)))
  prefs: []
  type: TYPE_NORMAL
- en: where β0, β1, and β2 are the parameters of the model that we need to estimate
    from the data.
  prefs: []
  type: TYPE_NORMAL
- en: We can estimate these parameters using maximum likelihood estimation. Once we
    have estimated the parameters, we can use the model to predict the probability
    of a customer purchasing the product for new customers based on their age and
    income.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if a new customer is 35 years old and has an income of $50,000,
    we can use the model to predict their probability of purchasing the product as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: p(purchase) =  1  ____________________________   (1 + exp(− (β0 + β1 * 35 +
    β2 * 50000)))
  prefs: []
  type: TYPE_NORMAL
- en: We can then use a decision threshold, such as 0.5, to determine whether to classify
    the customer as a purchaser or non-purchaser. Note that a choice of threshold
    can affect the trade-off between precision and recall.
  prefs: []
  type: TYPE_NORMAL
- en: We can solve the above problem with R and Python using the corresponding libraries.
    Let’s take a look at how to do that. In the following examples, we are going to
    use a sample dataset called `customer_data.csv`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an overview of the datafile:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Age** | **Income** | **purchased** |'
  prefs: []
  type: TYPE_TB
- en: '| 22 | 20000 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 35 | 80000 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 42 | 50000 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 27 | 30000 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 48 | 70000 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 38 | 60000 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 41 | 45000 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 29 | 35000 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 33 | 40000 | 1 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2.2: Customer data'
  prefs: []
  type: TYPE_NORMAL
- en: Example solution with R
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following code reads customer data from a CSV file, builds a logistic regression
    model to predict the probability of a customer making a purchase based on their
    age and income, and then predicts the probability for a new customer and provides
    a corresponding prediction message based on the probability value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Example solution with Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This code reads customer data from a CSV file into a pandas DataFrame, uses
    scikit-learn’s `LogisticRegression` class to build a logistic regression model
    to predict purchase probabilities based on age and income, and then predicts the
    probability for a new customer and provides a corresponding prediction message
    based on the probability value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The example will print “**The** **customer is predicted to purchase the product**”
    with a probability of 0.81.
  prefs: []
  type: TYPE_NORMAL
- en: Lasso regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lasso regression or least absolute shrinkage and selection operator (also known
    as L1 regularization) is a type of linear regression method used for variable
    selection and regularization. It is a regression technique that adds a penalty
    term to the sum of squared errors, which includes the absolute values of the regression
    coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: The lasso regression algorithm aims to minimize the residual sum of squares
    subject to the constraint that the sum of absolute values of the coefficients
    is less than or equal to a constant value. This constraint causes some coefficients
    to be shrunk toward zero, resulting in sparse models, whereas some features have
    zero coefficients, effectively excluding them from the model.
  prefs: []
  type: TYPE_NORMAL
- en: Lasso regression is particularly useful when dealing with high-dimensional datasets,
    where the number of features (or predictors) is much larger than the number of
    observations. It can also help to overcome problems such as overfitting in linear
    regression models, where the model becomes too complex and fits the training data
    too well but fails to generalize well to new data.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have a dataset that contains information about houses and their sale prices,
    including features such as the number of bedrooms, the size of the lot, the age
    of the house, and the location. We want to build a model that can predict the
    sale price of a house based on these features.
  prefs: []
  type: TYPE_NORMAL
- en: To build this model, we can use Lasso regression. We start by splitting our
    dataset into a training set and a test set. We then use the training set to fit
    a Lasso regression model with a specific value of the regularization parameter
    (alpha). We can tune this parameter using cross-validation to find the best value
    that results in the lowest error on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have trained our model, we can use it to make predictions on new data
    by inputting the values of the features and computing the corresponding sale price.
    The Lasso regression model will automatically select the most important features
    for prediction by shrinking the coefficients of less important features toward
    zero.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s say our Lasso regression model selected the number of bedrooms
    and the location as the most important features for predicting the sale price
    and shrunk the coefficients of the other features to zero. We can use this information
    to inform our decision-making when buying or selling houses.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at how a sample solution would work in both R and Python.
    Both examples use the California Housing dataset, split the data into training
    and testing sets, fit a Lasso regression model, predict on the test set, and evaluate
    the model’s performance on the testing set using the RMSE metric.
  prefs: []
  type: TYPE_NORMAL
- en: Example solution with R
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The code performs a linear regression using the Lasso regularization (L1 penalty)
    to predict the median house values based on a housing dataset. The dataset is
    loaded from a specific URL, and after preprocessing, it is split into training
    and testing sets. The `glmnet`library is used to build the model, and the **Root
    Mean Squared Error** (**RMSE**) is calculated to evaluate the model’s performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Example solution with Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following code begins by importing pandas and assigning it the alias `pd`
    for convenience. Next, it reads data from a CSV file hosted at a specific URL
    and creates a DataFrame named “housing” to hold the dataset. To handle categorical
    data effectively, the code performs one-hot encoding on the `ocean_proximity`
    column, converting it into multiple binary columns.
  prefs: []
  type: TYPE_NORMAL
- en: Data cleanliness is vital for reliable models, so the script takes care of missing
    values by removing any rows containing NaN entries from the DataFrame. The dataset
    is then split into training and testing sets using the `train_test_split` function
    from scikit-learn, where 80% of the data is used for training and the remaining
    20% for testing.
  prefs: []
  type: TYPE_NORMAL
- en: Now comes the machine-learning part. The script imports the `LassoCV` class
    from scikit-learn, which is a linear regression model with L1 regularization (Lasso).
    `LassoCV` performs cross-validation to find the optimal regularization strength
    (alpha) from a predefined set of values. The model is then trained on the training
    data using the “`fit`” method.
  prefs: []
  type: TYPE_NORMAL
- en: After training, the model is put to the test. Predictions are made on the testing
    data using the trained `LassoCV` model, and the performance of the model is assessed
    using the RMSE metric. The RMSE represents the deviation between the predicted
    `median_house_value` and the actual target values in the testing data. A lower
    RMSE indicates better predictive accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the script concludes by displaying the calculated RMSE value, providing
    insight into how well the `LassoCV` model performs in predicting `median_house_value`
    on unseen data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In the Python example, we have to one-hot encode the `ocean_proximity` feature
    before splitting the data into training and testing sets, which will allow the
    Lasso regression model to use the feature in the model. Both models will give
    us predictions and print RMSE of around 67,000 to 68,000 depending on the version
    of the libraries used.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we took a closer look at Lasso regression (L1 regularization).
    L2 regularization is used with Ridge regression. Lasso and Ridge regression differ
    mainly in the type of regularization they apply, their impact on feature selection,
    and their handling of multicollinearity. Lasso tends to produce sparse models
    with feature selection, while Ridge maintains all features but with smaller coefficients,
    making it more suitable when multicollinearity is a concern or if you want to
    control the magnitude of coefficients. The choice between them depends on your
    specific modeling goals and the nature of your data. We are not going to dive
    into the details of Ridge regression in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we have investigated various linear regression models and how
    to implement these using R and Python. Linear regression models are an essential
    part of machine learning and understanding the principles is an important skill.
    In the next section, we will take a closer look into clustering algorithms, decision
    trees, and random forests.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering algorithms, decision trees, and random forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Clustering algorithms** are used for unsupervised learning tasks, which means
    they are used to find patterns in data without any predefined labels or categories.
    The goal of clustering algorithms is to group similar data points together in
    clusters, while keeping dissimilar data points separate.'
  prefs: []
  type: TYPE_NORMAL
- en: There are several types of clustering algorithms, including K-means, hierarchical
    clustering, and density-based clustering. K-means is a popular clustering algorithm
    that works by dividing a dataset into K clusters, where K is a predefined number
    of clusters. Hierarchical clustering is another clustering algorithm that creates
    a hierarchy of clusters based on the similarity between data points. Density-based
    clustering algorithms, such as DBSCAN, group together data points that are closely
    packed together in high-density regions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Decision trees**, on the other hand, are used for supervised learning tasks,
    which means they are used to make predictions or decisions based on input data
    with predefined labels or categories. A decision tree is a tree-like structure
    that consists of nodes and branches, where each node represents a feature or attribute,
    and each branch represents a decision based on that feature. The goal of a decision
    tree is to create a model that can accurately predict the label or category of
    a new input based on its features.'
  prefs: []
  type: TYPE_NORMAL
- en: There are several types of decision trees, including ID3, C4.5, and CART. The
    ID3 algorithm is a popular decision tree algorithm that works by choosing the
    attribute with the highest information gain as the root node, and recursively
    building the tree by selecting attributes that maximize information gain at each
    level. The C4.5 algorithm is an improved version of ID3 that can handle continuous
    and discrete data, and CART is another decision tree algorithm that can handle
    both classification and regression tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '**Random forests** combine multiple decision trees to create a more accurate
    and robust model. A random forest consists of a large number of decision trees,
    each trained on a different subset of the data and using a random subset of the
    available features. This helps to reduce overfitting and increase the generalization
    ability of the model.'
  prefs: []
  type: TYPE_NORMAL
- en: The random subset of features used for each tree is selected randomly from the
    available features, with a new subset selected for each tree. This ensures that
    each tree in the forest is different and provides a diverse set of predictions.
    During training, each tree in the forest is grown to its full depth, and predictions
    are made by aggregating the predictions of all the trees in the forest.
  prefs: []
  type: TYPE_NORMAL
- en: The aggregation process can vary depending on the task at hand. For classification
    tasks, the most common method is to use a majority vote, where the final prediction
    is the class that is predicted by the most trees in the forest. For regression
    tasks, the most common method is to use the average prediction of all the trees
    in the forest.
  prefs: []
  type: TYPE_NORMAL
- en: Random forests have several advantages over a single decision tree, including
    improved accuracy, reduced overfitting, and robustness to noise and outliers in
    the data. They are also relatively easy to use and can handle a wide range of
    input features and data types. However, they can be computationally expensive
    to train and can be difficult to interpret and visualize, especially when dealing
    with a large number of trees.
  prefs: []
  type: TYPE_NORMAL
- en: In the next sections, we will take a closer look at some of the clustering and
    decision tree algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: K-means clustering is a popular algorithm used to partition a set of data points
    into K clusters, where K is a predefined number. The algorithm works by iteratively
    assigning data points to the nearest cluster centroid and updating the cluster
    centroids based on the new assignments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a simple step-by-step overview of how the K-means algorithm works:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize K centroids randomly from the data points.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign each data point to the nearest centroid based on the Euclidean distance
    between the data point and the centroid.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the centroids of each cluster by computing the mean of all the data points
    assigned to that cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 2 and 3 until the centroids no longer move significantly or a maximum
    number of iterations is reached.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The goal of the K-means algorithm is to minimize the sum of squared distances
    between each data point and its assigned cluster centroid, also known as the “inertia.”
    The algorithm can be sensitive to the initial random selection of centroids, so
    it’s often a good idea to run the algorithm multiple times with different initializations
    and select the solution with the lowest inertia.
  prefs: []
  type: TYPE_NORMAL
- en: K-means is a fast and effective algorithm for clustering data, but it does have
    some limitations. It assumes that the clusters are spherical and of equal size,
    and it can be sensitive to outliers and noise in the data. Additionally, determining
    the optimal number of clusters, K, can be challenging and may require some domain
    knowledge or trial and error.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The iris dataset contains measurements of four features (sepal length, sepal
    width, petal length, and petal width) for 150 iris flowers, with 50 flowers from
    each of three species (setosa, versicolor, and virginica). We can use K-means
    clustering to group these flowers into distinct clusters based on their feature
    values.
  prefs: []
  type: TYPE_NORMAL
- en: To do this, we first select the features we want to cluster on and preprocess
    the data by scaling the features to have zero mean and unit variance. Scaling
    is done to ensure that we have equal influence of features. We then apply the
    K-means algorithm to the preprocessed data, specifying the number of clusters
    K we want to create. In this case, we might choose K=3 to correspond to the three
    iris species.
  prefs: []
  type: TYPE_NORMAL
- en: The K-means algorithm then partitions the flowers into K clusters based on their
    feature values, with each cluster represented by its centroid (the mean feature
    values of the flowers assigned to the cluster). We can examine the resulting clusters
    and their centroids to gain insights into the structure of the iris dataset.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we might find that one cluster contains flowers with smaller sepal
    and petal dimensions, which could correspond to the setosa species. Another cluster
    might contain flowers with larger petal dimensions and intermediate sepal dimensions,
    which could correspond to the versicolor species. The third cluster might contain
    flowers with larger sepal dimensions and larger petal dimensions, which could
    correspond to the virginica species.
  prefs: []
  type: TYPE_NORMAL
- en: By using K-means clustering to group the iris flowers based on their feature
    values, we can gain a deeper understanding of the structure of the dataset and
    potentially identify patterns and relationships in data. Let’s see how the above
    example would look in R and Python.
  prefs: []
  type: TYPE_NORMAL
- en: Example with R
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following R code uses the “iris” dataset, a popular dataset in the machine-learning
    community. It performs K-means clustering on the dataset’s four numeric attributes:
    `Sepal.Length`, `Sepal.Width`, `Petal.Length`, and `Petal.Width`. The code sets
    a random seed for reproducibility and applies the K-means algorithm with three
    cluster centers. After the clustering is performed, the code displays the cluster
    assignments for each data point, indicating which cluster each observation belongs
    to (represented by values 1, 2, or 3). K-means clustering aims to group similar
    data points into clusters and is a common technique for unsupervised machine learning
    tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Example with Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following Python code uses the scikit-learn library to perform K-means clustering
    on the Iris dataset. The Iris dataset is loaded using `load_iris()` from scikit-learn,
    containing measurements of iris flowers’ sepal length, sepal width, petal length,
    and petal width, along with their corresponding species labels.
  prefs: []
  type: TYPE_NORMAL
- en: The script extracts the four feature columns for clustering and stores them
    in the `iris_cluster` variable. Then, it imports the `KMeans` class from scikit-learn’s
    `sklearn.cluster` module.
  prefs: []
  type: TYPE_NORMAL
- en: The K-means algorithm is applied to the feature data (`iris_cluster`) with the
    number of clusters (`n_clusters`) set to `3`. The `random_state` parameter is
    set to `123` to ensure the reproducibility of the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'After clustering, the code prints the cluster assignments for each data point
    in the Iris dataset. Each data point is assigned a cluster label (0, 1, or 2),
    indicating the group it belongs to according to the K-means clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Both of the code examples will print the cluster assignments for the iris dataset.
    The result looks similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This is a simple example. In practice, you would likely spend more time exploring
    and visualizing the data, tuning the number of clusters, and interpreting the
    results of the clustering algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: ID3 decision tree
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**ID3** (**Iterative Dichotomiser 3**) is a popular algorithm for building
    decision trees. The ID3 algorithm was developed by Ross Quinlan in 1986 and is
    based on the concept of information entropy.'
  prefs: []
  type: TYPE_NORMAL
- en: The goal of a decision tree is to create a model that can be used to make predictions
    or classify new instances based on their characteristics. A decision tree consists
    of a set of nodes and edges, where each node represents a decision or a test on
    one or more input variables, and each edge represents the possible outcome of
    that decision.
  prefs: []
  type: TYPE_NORMAL
- en: The ID3 algorithm works by recursively partitioning the data based on the input
    variables with the highest information gain. Information gain is a measure of
    the reduction in entropy or impurity that results from splitting the data on a
    particular input variable. The algorithm selects the input variable that maximizes
    information gain at each step, until all instances in a given partition belong
    to the same class or a stopping criterion is met.
  prefs: []
  type: TYPE_NORMAL
- en: The ID3 algorithm has several advantages, including its simplicity and efficiency
    in handling large datasets with categorical variables. However, it has limitations
    in handling continuous variables and overfitting, which can be addressed by using
    modified algorithms such as C4.5 and CART.
  prefs: []
  type: TYPE_NORMAL
- en: In the next example, we can see how ID3 works in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this example, we are using the following animal-related dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Animal** | **Has fur?** | **Has feathers?** | **Eats meat?** | **Classification**
    |'
  prefs: []
  type: TYPE_TB
- en: '| Dog | Yes | No | Yes | Mammal |'
  prefs: []
  type: TYPE_TB
- en: '| Cat | Yes | No | Yes | Mammal |'
  prefs: []
  type: TYPE_TB
- en: '| Parrot | No | Yes | No | Bird |'
  prefs: []
  type: TYPE_TB
- en: '| Eagle | No | Yes | Yes | Bird |'
  prefs: []
  type: TYPE_TB
- en: '| Snake | No | No | Yes | Reptile |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2.3: Animal characteristics data'
  prefs: []
  type: TYPE_NORMAL
- en: Our goal is to build a decision tree that can classify animals based on their
    features.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s calculate the entropy of the entire dataset. Entropy measures the
    impurity of a dataset. A dataset with all the same class labels has an entropy
    of 0, while a dataset with an equal number of examples from each class has an
    entropy of 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'A general equation to calculate entropy can be represented in the the following
    way:'
  prefs: []
  type: TYPE_NORMAL
- en: E(S) = − ∑ i=1 n p i log 2p i
  prefs: []
  type: TYPE_NORMAL
- en: In this equation, *E(S)* is the entropy of a set *S*, *n* is the number of classes
    in *S*, and *pi* is the proportion of the number of elements in *S* that belong
    to class *i*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we have *n=3* classes: Mammal, Bird, and Reptile. The number
    of animals in each class is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mammal: 2 (Dog, Cat)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bird: 2 (Parrot, Eagle)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reptile: 1 (Snake)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Therefore, the probabilities are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: p Mammal = 2 / 5 = 0.4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: p Bird = 2 / 5 = 0.4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: p Reptile = 1 / 5 = 0.2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Substituting these values into the entropy formula, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: E(S) = − (0.4log 20.4 + 0.4log 20.4 + 0.2log 20.2) ≈ 1.52193
  prefs: []
  type: TYPE_NORMAL
- en: So the entropy of the “Classification” attribute is approximately 1.52.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s calculate the information gain of the “Has fur?” attribute. Information
    gain is a measure of how much a given attribute or feature in a dataset contributes
    to reducing the uncertainty in the classification of the data. In decision trees,
    information gain is used to select the best attribute to use for splitting the
    data at each node of the tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'The information gain formula is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: IG(S, A) = E(S) −  ∑ v∈Values(A) |S v| _ |S| E(S v)
  prefs: []
  type: TYPE_NORMAL
- en: where *A* is the attribute (in this case, “Has fur?”), *v* is a possible value
    of the attribute, *Values(A)* is the set of possible values, *|Sv|* is the number
    of animals in the dataset that have attribute *A=v*, and *E(Sv)* is the entropy
    of the subset of animals that have attribute *A=v*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can split the data based on whether the animals have fur or not. The subsets
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Has fur: Dog, Cat'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'No fur: Parrot, Eagle, Snake'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The proportion of animals in each subset is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|S Has fur| = 2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|S No fur| = 3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To calculate E(S Has fur), we need to count the number of animals in each class
    that have fur:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mammal: 2 (Dog, Cat)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bird: 0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reptile: 0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Therefore, the probabilities are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: p Mammal = 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: p Bird = 0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: p Reptile = 0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Substituting these values into the entropy formula, we get the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: E(S Has fur) = − 1log 21 = 0
  prefs: []
  type: TYPE_NORMAL
- en: 'Entropy of 0 means that the set is perfectly classified: in this case, all
    animals with fur are mammals.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate E(S No fur), we need to count the number of animals in each class
    that don’t have fur:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mammal: 0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bird: 2 (Parrot, Eagle)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reptile: 1 (Snake)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Therefore, the probabilities are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: p Mammal = 0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: p Bird = 2 / 3 ≈ 0.67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: p Reptile = 1 / 3 ≈ 0.33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Substituting these values into the entropy formula, we get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: E(S No fur) = − (0 + 0.67log 20.67 + 0.33log 20.33) ≈ 0.9183
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can substitute these values into the information gain formula:'
  prefs: []
  type: TYPE_NORMAL
- en: IG(S, Has fur?) = E(S) −  ∑ v∈Values(Has fur?) |S v| _ |S| E(S v)
  prefs: []
  type: TYPE_NORMAL
- en: = 1.52193 − ( 2 _ 5  × 0 +  3 _ 5  × 0.9183) ≈ 0.971
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the information gained for the “`Has` `fur`?” attribute is approximately
    0.971.
  prefs: []
  type: TYPE_NORMAL
- en: We could also calculate the information gained for the “`Has` `feathers`?” and
    “`Eats` `meat`?” attributes by using the same formula and splitting the data based
    on whether the animals have feathers or not or whether they eat meat or not. The
    attribute with the highest information gain would be selected to split the data
    at the root of the decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: Information gain for “Has feathers?” is also 0.971, and for “Eats meat?” it
    is 0.322\. In this case, we will select “Has fur?” for our root node.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at the same example with R and Python. Both code snippets
    will load the animal dataset, create the decision tree model, visualize it, and
    test the tree with new data. At the end, we can see that the new animal is classified
    as `Mammal`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final decision tree looks like the one in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1: Final decision tree (printed using the following R code)](img/B19863_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1: Final decision tree (printed using the following R code)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the example with R:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The Python example is similar to the preceding R example, but in Python, we
    also need to convert categorical features to numerical using one-hot encoding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This is a really simple example of a decision tree using the ID3 algorithm.
    In real-world examples, we would most likely have a lot of data and therefore
    multiple nodes and branches in our final tree.
  prefs: []
  type: TYPE_NORMAL
- en: We have now learned how clustering algorithms, decision trees, and random forests
    work. These algorithms are an important part of machine learning and are commonly
    used for classification. In the next section, we will take a closer look into
    boosting algorithms and Naive Bayes.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting algorithms and Naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Boosting** is a machine learning technique that involves creating an ensemble
    of weak learners to form a strong learner. The idea behind boosting algorithms
    is to iteratively train models on the data, where each new model attempts to correct
    the errors of the previous model. Boosting algorithms are widely used in supervised
    learning tasks, such as classification and regression.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several key types of boosting algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**AdaBoost (Adaptive Boosting):** AdaBoost is one of the earliest and most
    popular boosting algorithms. It starts by training a base classifier on the entire
    dataset and then sequentially trains additional classifiers on the samples that
    the previous classifiers got wrong. The final prediction is made by taking a weighted
    sum of the predictions of all the classifiers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gradient Boosting:** Gradient Boosting is another popular boosting algorithm
    that works by iteratively adding new models to the ensemble, each trained to minimize
    the error of the previous models. Gradient Boosting is used in regression and
    classification problems and has been shown to achieve state-of-the-art results
    in many applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**XGBoost:** XGBoost (Extreme Gradient Boosting) is a popular and highly optimized
    implementation of the Gradient Boosting algorithm. XGBoost uses a regularized
    objective function and a variety of techniques to reduce overfitting and improve
    accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting algorithms are known for their ability to improve the accuracy of machine-learning
    models by reducing bias and variance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Naive Bayes** is a simple but effective algorithm used for classification
    tasks. It is based on Bayes’ theorem, which states that the probability of a hypothesis
    (in this case, a class label) is updated in the light of new evidence (in this
    case, the feature values of a new data point). The algorithm assumes that the
    features are independent of each other, which is why it is called “naïve.”'
  prefs: []
  type: TYPE_NORMAL
- en: In Naive Bayes, the probability of a data point belonging to a particular class
    is calculated by multiplying the prior probability of that class by the likelihood
    of the data point given that class. The algorithm then selects the class with
    the highest probability as the predicted class for that data point.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several variants of the Naive Bayes algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gaussian Naive Bayes**: Used when the features are continuous and assumed
    to be normally distributed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multinomial Naive Bayes:** Used when the features are discrete and represent
    counts or frequencies (such as in text classification)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bernoulli Naive Bayes:** A variant of Multinomial Naive Bayes, used when
    the features are binary (such as in spam filtering)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naive Bayes is a simple and efficient algorithm that works well on high-dimensional
    datasets with sparse features. It is widely used in natural-language processing,
    spam filtering, sentiment analysis, and other classification tasks. However, the
    assumption of feature independence may not hold true in all cases, and the algorithm
    may not perform well if the data violates this assumption.
  prefs: []
  type: TYPE_NORMAL
- en: In the next sections, we will take a closer look into some of the boosting and
    Naive Bayes algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**XGBoost** (**eXtreme Gradient Boosting**) is an open source machine learning
    library that is designed to be highly efficient, flexible, and scalable. It is
    an implementation of gradient-boosting algorithms that can be used for both classification
    and regression problems.'
  prefs: []
  type: TYPE_NORMAL
- en: It is based on the gradient-boosting framework and uses an ensemble of decision
    trees to make predictions. XGBoost is designed to handle large-scale and high-dimensional
    data and provides various techniques to prevent overfitting, such as regularization
    and early stopping.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look into simple examples of XGBoost with R and Python. In these
    examples, we will use the iris dataset that we have already used in earlier chapters
    of this book. We will split the data into train and test datasets and then train
    our XGBoost model to predict the species. At the end, we will test the model with
    our test data and evaluate the model performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the example with R:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the example with Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Gaussian Naive Bayes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Gaussian Naive Bayes (GNB) is a variant of the Naive Bayes algorithm that assumes
    a Gaussian (normal) distribution for the input variables. In GNB, the probability
    distribution of each input variable is estimated separately for each class using
    the training data.
  prefs: []
  type: TYPE_NORMAL
- en: During the testing phase, the model calculates the likelihood of the input features
    belonging to each class based on the Gaussian distribution parameters estimated
    during training. Then, the model applies Bayes’ theorem to calculate the posterior
    probability of each class, given the input features. The class with the highest
    posterior probability is then assigned to the input.
  prefs: []
  type: TYPE_NORMAL
- en: GNB is called “naïve” because it makes a strong assumption that the input features
    are conditionally independent, given the class label. This assumption simplifies
    the model and makes the computation of the posterior probabilities tractable.
    However, this assumption may not hold in practice for some datasets, and thus,
    the model may not perform well. Nonetheless, GNB can be a fast and accurate classifier
    for datasets where the independence assumption holds.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look into a similar classification problem, as we did with XGBoost,
    using Gaussian Naive Bayes. Here is the example code with both R and Python. Once
    again, we are using the iris dataset to classify the different species.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the example with R:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the example with Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now we have familiarized ourselves with the concepts of boosting algorithms
    and Naive Bayes. These methods are used widely in Qlik AutoML, and understanding
    the concepts of these algorithms is an essential skill to work with machine-learning
    problems. In our next section, we will take a closer look at some of the advanced
    machine-learning algorithms, including neural networks, deep learning, and natural-language
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks, deep learning, and natural-language models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Neural networks** are a type of machine-learning algorithm that is inspired
    by the structure and function of the human brain. They are composed of layers
    of interconnected nodes or artificial neurons that process and transmit information.'
  prefs: []
  type: TYPE_NORMAL
- en: In a neural network, the input data is fed into the first layer of nodes, which
    applies a set of mathematical transformations to the data and produces an output.
    The output of the first layer is then fed into the second layer, which applies
    another set of transformations to produce another output, and so on until the
    final output is produced.
  prefs: []
  type: TYPE_NORMAL
- en: 'The connections between the nodes in the neural network have weights that are
    adjusted during the learning process to optimize the network’s ability to make
    accurate predictions or classifications. This is typically achieved using an optimization
    algorithm such as stochastic gradient descent. An example of the structure of
    a neural network is visualized in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2: High-level architecture of a neural network](img/B19863_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.2: High-level architecture of a neural network'
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks have been used to solve a wide range of machine-learning problems,
    including image and speech recognition, natural language processing, and predictive
    modeling. They have been shown to be effective in many applications due to their
    ability to learn complex, non-linear relationships between input and output data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Deep learning** involves the use of neural networks with multiple layers.
    It has achieved significant success in a wide range of applications, including
    computer vision, speech recognition, natural-language processing, and game playing.
    Some notable examples include the use of deep learning in image recognition tasks,
    such as identifying objects in photos, and in natural language processing tasks,
    such as language translation and sentiment analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: One of the key advantages of deep learning is its ability to automatically learn
    features from data without the need for manual feature engineering. This makes
    it possible to train models on large datasets with many features, which can be
    computationally challenging or even impossible with traditional machine learning
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: However, training deep neural networks can also be challenging due to the large
    number of parameters and the risk of overfitting. To address these challenges,
    researchers have developed a variety of techniques, including regularization methods,
    dropout, and batch normalization, that can improve the performance and stability
    of deep neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: '**Natural-language models** are a type of machine-learning model that can process
    and understand human language. These models are trained on large amounts of text
    data, such as books, articles, and social media posts, and learn to generate coherent
    and semantically meaningful responses to natural language input.'
  prefs: []
  type: TYPE_NORMAL
- en: One common type of natural-language model is the language model, which is trained
    to predict the probability of a sequence of words given a context. For example,
    a language model might be trained to predict the probability of the word “pizza,”
    given the context “I am in the mood for something to eat that is typically round
    and covered in toppings.”
  prefs: []
  type: TYPE_NORMAL
- en: Another type of natural-language model is the text-generation model, which can
    be used to generate natural-language text, such as news articles, stories, and
    chatbot responses. These models can be trained to generate text that is similar
    to a particular style or genre, or even to imitate the writing style of a particular
    author.
  prefs: []
  type: TYPE_NORMAL
- en: Natural-language models have a wide range of applications, including language
    translation, sentiment analysis, chatbots and virtual assistants, and text summarization.
    Recent advances in deep learning and natural language processing have led to the
    development of powerful language models such as GPT-3, which can generate coherent
    and human-like text on a wide range of topics. Qlik Insight Advisor is one example
    of a product that has integrated the natural-language model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have gained an overview of different machine-learning algorithms.
    We have discovered how different algorithms can be used to solve problems and
    how they function. We started this chapter by getting familiar with some of the
    most common regression algorithms and gained knowledge on how to use these in
    R and Python. We discovered how to utilize clustering, decision trees, and random
    forests with practical examples.
  prefs: []
  type: TYPE_NORMAL
- en: In the later part of this chapter, we moved on to more complex algorithms and
    learned how different boosting algorithms, neural networks, and other advanced
    models function. These models are utilized in Qlik AutoML, and it’s important
    to know how each model is structured. After reading this chapter, you now have
    a basic understanding of the models and are prepared to utilize these with Qlik
    tools. We will use most of these algorithms in the later parts of this book.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will focus on data literacy in a machine-learning context.
    Data literacy is a hot topic, and it is also an important concept in the world
    of machine learning. To be able to create a well-functioning model and interpret
    the results from it, we must be able to understand the data. This is where data
    literacy comes into play.
  prefs: []
  type: TYPE_NORMAL
