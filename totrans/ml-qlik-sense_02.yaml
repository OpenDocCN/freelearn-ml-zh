- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Machine Learning Algorithms and Models with Qlik
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Qlik 的机器学习算法和模型
- en: Machine learning algorithms have become an integral part of our lives, from
    the personalization of online ads to the recommendation systems on streaming platforms.
    These algorithms are responsible for making intelligent decisions based on data,
    without being explicitly programmed.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法已成为我们生活的重要组成部分，从在线广告的个性化到流媒体平台的推荐系统。这些算法负责根据数据做出智能决策，而不需要明确编程。
- en: Machine learning algorithms refer to a set of mathematical models and techniques
    that enable software to learn patterns and relationships from data, allowing them
    to make predictions and decisions. These algorithms can be broadly categorized
    into supervised, unsupervised, semi-supervised, and reinforcement learning algorithms.
    Each type of algorithm has its own unique characteristics and applications, suiting
    them to a wide range of tasks.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法是指一组数学模型和技术，使软件能够从数据中学习模式和关系，从而使其能够做出预测和决策。这些算法可以广泛分为监督学习、无监督学习、半监督学习和强化学习算法。每种类型的算法都有其独特的特征和应用，适用于广泛的任务。
- en: 'In this chapter, we will provide an overview of machine learning algorithms
    and their applications, focusing on algorithms used in Qlik tools. Here is what
    you will learn as a part of this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将概述机器学习算法及其应用，重点关注 Qlik 工具中使用的算法。作为本章的一部分，你将学习以下内容：
- en: Understand regression models and how to use these
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解回归模型及其使用方法
- en: Understand different clustering algorithms and decision trees
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解不同的聚类算法和决策树
- en: Understand the basics of boosting algorithms, especially the one used in Qlik
    AutoML
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解提升算法的基础，特别是 Qlik AutoML 中使用的提升算法
- en: Understand the basics of neural networks and other advanced machine learning
    models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解神经网络和其他高级机器学习模型的基础
- en: Note
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Qlik AutoML was using the following algorithms at the time of writing this
    book:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本书时，Qlik AutoML 正在使用以下算法：
- en: '**Binary and multiclass** **classification problems:**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**二元和多类** **分类问题**：'
- en: CatBoost Classification, Elastic Net Regression, Gaussian Naive Bayes, Lasso
    Regression, LightGBM Classification, Logistic Regression, Random Forest Classification,
    XGBoost Classification
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: CatBoost 分类，Elastic Net 回归，高斯朴素贝叶斯，Lasso 回归，LightGBM 分类，逻辑回归，随机森林分类，XGBoost
    分类
- en: '**Regression problems:**'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**回归问题**：'
- en: CatBoost Regression, LightGBM Regression, Linear Regression, Random Forest Regression,
    SGD Regression, XGBoost Regression
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: CatBoost 回归，LightGBM 回归，线性回归，随机森林回归，SGD 回归，XGBoost 回归
- en: Some of these algorithms will be covered in more detail in the coming sections.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些算法将在接下来的章节中更详细地介绍。
- en: Regression models
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归模型
- en: '**Regression models** are a type of supervised machine-learning model used
    to predict continuous numerical values for a target variable based on one or more
    input variables. In other words, regression models are used to estimate the relationships
    between the input variables and the output variable.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**回归模型**是一种监督式机器学习模型，用于根据一个或多个输入变量预测目标变量的连续数值。换句话说，回归模型用于估计输入变量和输出变量之间的关系。'
- en: 'There are various types of regression models used in machine learning, some
    of which include the following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中使用了各种类型的回归模型，其中一些包括以下内容：
- en: '**Linear Regression**: This is a type of regression model that assumes a linear
    relationship between the input variables and the output variable.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性回归**：这是一种假设输入变量和输出变量之间存在线性关系的回归模型。'
- en: '**Polynomial Regression**: This is a type of regression model that assumes
    a polynomial relationship between the input variables and the output variable.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多项式回归**：这是一种假设输入变量和输出变量之间存在多项式关系的回归模型。'
- en: '**Logistic Regression**: This is a type of regression model used to predict
    binary or categorical outcomes. It estimates the probability of an event occurring
    based on the input variables.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逻辑回归**：这是一种用于预测二元或分类结果的回归模型。它根据输入变量估计事件发生的概率。'
- en: '**Ridge Regression**: This is a type of linear regression model that uses regularization
    to prevent overfitting of the model.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**岭回归**：这是一种使用正则化来防止模型过拟合的线性回归模型。'
- en: '**Lasso Regression**: This is another type of linear regression model that
    uses regularization to prevent overfitting of the model. It is particularly useful
    when dealing with datasets that have a large number of features.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Lasso回归**：这是另一种线性回归模型，它使用正则化来防止模型过拟合。当处理具有大量特征的数据库时，它特别有用。'
- en: In the next section, we are going to take a closer look at some of the regression
    models in the preceding list.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将更详细地研究前面列表中的一些回归模型。
- en: Linear regression
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归
- en: 'Linear regression is used for modeling the relationship between a dependent
    variable (also known as the target or response variable) and one or more independent
    variables (also known as the explanatory or predictor variables). The goal of
    linear regression is to find the best-fit line (or hyperplane) that can predict
    the dependent variable based on the values of the independent variables. In other
    words, linear regression tries to find a linear equation that relates the input
    variables to the output variable. The equation takes the following form:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归用于建模因变量（也称为目标或响应变量）与一个或多个自变量（也称为解释或预测变量）之间的关系。线性回归的目标是找到最佳拟合线（或超平面），可以根据自变量的值预测因变量。换句话说，线性回归试图找到一个线性方程，将输入变量与输出变量联系起来。该方程采用以下形式：
- en: Y = mX + b + e
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Y = mX + b + e
- en: where *Y* is the dependent variable, *X* is the independent variable, *m* is
    the slope of the line, *b* is the intercept, and *e* represents the error of the
    model. The goal of linear regression is to find the best values of *m* and *b*
    that minimize the difference between the predicted values and the actual values
    of the dependent variable. In simple linear regression, we have one independent
    variable and in multiple linear regression, we will have multiple independent
    variables.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *Y* 是因变量，*X* 是自变量，*m* 是直线的斜率，*b* 是截距，*e* 代表模型的误差。线性回归的目标是找到最佳值 *m* 和 *b*，以最小化预测值与因变量实际值之间的差异。在简单线性回归中，我们有一个自变量，而在多元线性回归中，我们将有多个自变量。
- en: Example
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Example
- en: 'We want to investigate the relationship between the number of hours a student
    studies and their exam score. We collect data from 10 students, recording the
    number of hours they studied and their corresponding exam scores. The data is
    shown here:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要研究学生学习小时数与他们的考试成绩之间的关系。我们从10名学生那里收集数据，记录他们学习的小时数和相应的考试成绩。数据如下所示：
- en: '| **Hours** **studied (X)** | **Exam** **score (Y)** |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| **学习小时数 (X)** | **考试成绩 (Y)** |'
- en: '| 2 | 60 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 60 |'
- en: '| 3 | 70 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 70 |'
- en: '| 4 | 80 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 80 |'
- en: '| 5 | 85 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 85 |'
- en: '| 6 | 90 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 90 |'
- en: '| 7 | 95 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 95 |'
- en: '| 8 | 100 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 100 |'
- en: '| 9 | 105 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 105 |'
- en: '| 10 | 110 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 110 |'
- en: '| 11 | 115 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 115 |'
- en: 'Table 2.1: Exam score data'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.1：考试成绩数据
- en: 'We can use a simple linear regression model to model the relationship between
    the two variables, with the number of hours studied as the independent variable
    (*X*) and the exam score as the dependent variable (*Y*). The model takes the
    following form:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用简单的线性回归模型来建模两个变量之间的关系，其中学习的小时数作为自变量（*X*）和考试成绩作为因变量（*Y*）。该模型采用以下形式：
- en: Y = mX + b + e
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Y = mX + b + e
- en: 'where *b* is the intercept (the value of *Y* when *X=0*) and *m* is the slope
    (the rate at which *Y* changes with respect to *X*). To estimate the values of
    *b* and *m*, we can use the least squares method. Solving for the regression equation,
    we get the following:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *b* 是截距（当 *X=0* 时 *Y* 的值）和 *m* 是斜率（*Y* 相对于 *X* 的变化率）。为了估计 *b* 和 *m* 的值，我们可以使用最小二乘法。求解回归方程，我们得到以下结果：
- en: Y = 5 * X + 55 + e
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Y = 5 * X + 55 + e
- en: This equation tells us that, on average, for every additional hour studied,
    a student can expect to score 5 points higher on the exam. The intercept of 55
    tells us that a student who studies 0 hours can expect to score 55 points (which
    may not be realistic, but it’s just the mathematical extrapolation from the model).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程告诉我们，平均而言，每多学习一个小时，学生可以在考试中期望多得5分。截距55告诉我们，如果一个学生没有学习任何小时数，他可以期望得到55分（这可能不太现实，但这只是从模型中进行的数学外推）。
- en: 'We can use this model to make predictions about exam scores based on the number
    of hours studied. For example, if a student studies for 7 hours, we can estimate
    their exam score to be as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个模型根据学习的小时数来预测考试成绩。例如，如果一个学生学习了7个小时，我们可以估计他们的考试成绩如下：
- en: Y = 5 * 7+ 55 + e = 90 + e
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Y = 5 * 7 + 55 + e = 90 + e
- en: Logistic regression
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Logistic regression is a statistical method used for binary classification problems
    where the outcome variable is categorical and has only two possible values, such
    as “yes” or “no,” “pass” or “fail,” or “spam” or “not spam.” It is a type of regression
    analysis that models the relationship between the independent variables and the
    dependent variable by estimating the probability of the binary outcome.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是一种用于二元分类问题的统计方法，其中结果变量是分类的，并且只有两个可能的值，例如“是”或“否”，“通过”或“失败”，或“垃圾邮件”或“非垃圾邮件”。它是一种回归分析方法，通过估计二元结果的概率来模拟自变量和因变量之间的关系。
- en: The logistic regression model uses a logistic function, also known as the sigmoid
    function, to model the relationship between the input variables and the binary
    outcome. The sigmoid function transforms the input values into a range between
    0 and 1, which represents the probability of the binary outcome.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归模型使用逻辑函数，也称为Sigmoid函数，来模拟输入变量和二元结果之间的关系。Sigmoid函数将输入值转换为介于0和1之间的范围，这代表二元结果的概率。
- en: The logistic regression model can be trained using a maximum likelihood estimation
    method to find the parameters that maximize the likelihood of the observed data
    given the model. These parameters can then be used to predict the probability
    of the binary outcome for new input data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归模型可以使用最大似然估计方法来训练，以找到最大化给定模型观察数据的似然性的参数。然后，可以使用这些参数来预测新输入数据的二元结果的概率。
- en: Logistic regression is widely used in medical diagnosis, credit scoring, and
    marketing analysis. It is a popular algorithm due to its simplicity and interpretability.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归在医学诊断、信用评分和营销分析中得到广泛应用。它是一种流行的算法，因为它简单且可解释。
- en: Example
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例
- en: Let’s say we want to predict whether or not a customer will purchase a product
    based on their age and income. We have a dataset of 90 customers, where each row
    represents a customer and the columns represent their age, income, and whether
    or not they purchased the product (0 for not purchased, 1 for purchased).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要根据客户的年龄和收入预测他们是否会购买产品。我们有一个包含90个客户的数据库，其中每一行代表一个客户，列代表他们的年龄、收入以及他们是否购买了产品（未购买为0，购买为1）。
- en: 'We can use logistic regression to model the probability of a customer purchasing
    the product based on their age and income. The logistic regression model can be
    expressed as the following:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用逻辑回归来根据客户的年龄和收入建模他们购买产品的概率。逻辑回归模型可以表示为以下形式：
- en: p(purchase) =  1  ______________________________   (1 + exp(− (β0 + β1 * age
    + β2 * income)))
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: p(购买) =  1  ______________________________   (1 + exp(− (β0 + β1 * 年龄 + β2 *
    收入)))
- en: where β0, β1, and β2 are the parameters of the model that we need to estimate
    from the data.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 β0、β1和β2是我们需要从数据中估计的模型参数。
- en: We can estimate these parameters using maximum likelihood estimation. Once we
    have estimated the parameters, we can use the model to predict the probability
    of a customer purchasing the product for new customers based on their age and
    income.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用最大似然估计来估计这些参数。一旦我们估计了参数，我们就可以使用该模型根据新客户的年龄和收入预测他们购买产品的概率。
- en: 'For example, if a new customer is 35 years old and has an income of $50,000,
    we can use the model to predict their probability of purchasing the product as
    follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果一个新客户年龄为35岁，收入为$50,000，我们可以使用该模型预测他们购买产品的概率如下：
- en: p(purchase) =  1  ____________________________   (1 + exp(− (β0 + β1 * 35 +
    β2 * 50000)))
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: p(购买) =  1  ____________________________   (1 + exp(− (β0 + β1 * 35 + β2 * 50000)))
- en: We can then use a decision threshold, such as 0.5, to determine whether to classify
    the customer as a purchaser or non-purchaser. Note that a choice of threshold
    can affect the trade-off between precision and recall.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用决策阈值，例如0.5，来确定是否将客户分类为购买者或非购买者。请注意，阈值的选择可能会影响精确度和召回率之间的权衡。
- en: We can solve the above problem with R and Python using the corresponding libraries.
    Let’s take a look at how to do that. In the following examples, we are going to
    use a sample dataset called `customer_data.csv`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用R和Python以及相应的库来解决上述问题。让我们看看如何做到这一点。在以下示例中，我们将使用一个名为`customer_data.csv`的样本数据集。
- en: 'Here is an overview of the datafile:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是数据文件的概述：
- en: '| **Age** | **Income** | **purchased** |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| **年龄** | **收入** | **购买** |'
- en: '| 22 | 20000 | 0 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 22 | 20000 | 0 |'
- en: '| 35 | 80000 | 1 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 35 | 80000 | 1 |'
- en: '| 42 | 50000 | 0 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 42 | 50000 | 0 |'
- en: '| 27 | 30000 | 0 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 27 | 30000 | 0 |'
- en: '| 48 | 70000 | 1 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 48 | 70000 | 1 |'
- en: '| 38 | 60000 | 1 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 38 | 60000 | 1 |'
- en: '| 41 | 45000 | 0 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 41 | 45000 | 0 |'
- en: '| 29 | 35000 | 0 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 29 | 35000 | 0 |'
- en: '| 33 | 40000 | 1 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 33 | 40000 | 1 |'
- en: 'Table 2.2: Customer data'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.2：客户数据
- en: Example solution with R
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用R的示例解决方案
- en: 'The following code reads customer data from a CSV file, builds a logistic regression
    model to predict the probability of a customer making a purchase based on their
    age and income, and then predicts the probability for a new customer and provides
    a corresponding prediction message based on the probability value:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码从CSV文件中读取客户数据，构建一个逻辑回归模型来根据客户的年龄和收入预测其购买的概率，然后根据概率值预测新客户的概率并提供相应的预测信息：
- en: '[PRE0]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Example solution with Python
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Python的示例解决方案
- en: 'This code reads customer data from a CSV file into a pandas DataFrame, uses
    scikit-learn’s `LogisticRegression` class to build a logistic regression model
    to predict purchase probabilities based on age and income, and then predicts the
    probability for a new customer and provides a corresponding prediction message
    based on the probability value:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码将客户数据从CSV文件读取到pandas DataFrame中，使用scikit-learn的`LogisticRegression`类构建逻辑回归模型来根据年龄和收入预测购买概率，然后根据概率值预测新客户的概率并提供相应的预测信息：
- en: '[PRE1]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The example will print “**The** **customer is predicted to purchase the product**”
    with a probability of 0.81.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 示例将打印“**该客户被预测为购买产品**”，概率为0.81。
- en: Lasso regression
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Lasso回归
- en: Lasso regression or least absolute shrinkage and selection operator (also known
    as L1 regularization) is a type of linear regression method used for variable
    selection and regularization. It is a regression technique that adds a penalty
    term to the sum of squared errors, which includes the absolute values of the regression
    coefficients.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Lasso回归或最小绝对收缩和选择算子（也称为L1正则化）是一种用于变量选择和正则化的线性回归方法。它是一种回归技术，在平方误差和中加入惩罚项，包括回归系数的绝对值。
- en: The lasso regression algorithm aims to minimize the residual sum of squares
    subject to the constraint that the sum of absolute values of the coefficients
    is less than or equal to a constant value. This constraint causes some coefficients
    to be shrunk toward zero, resulting in sparse models, whereas some features have
    zero coefficients, effectively excluding them from the model.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Lasso回归算法旨在最小化残差平方和，同时满足系数绝对值之和小于或等于一个常数的约束。这个约束导致一些系数被缩小到零，从而产生稀疏模型，而一些特征具有零系数，实际上将它们从模型中排除。
- en: Lasso regression is particularly useful when dealing with high-dimensional datasets,
    where the number of features (or predictors) is much larger than the number of
    observations. It can also help to overcome problems such as overfitting in linear
    regression models, where the model becomes too complex and fits the training data
    too well but fails to generalize well to new data.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Lasso回归在处理高维数据集时特别有用，其中特征（或预测因子）的数量远大于观测数。它还可以帮助克服线性回归模型中的过拟合问题，其中模型变得过于复杂，并且对训练数据拟合得很好，但无法很好地推广到新数据。
- en: Example
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例
- en: We have a dataset that contains information about houses and their sale prices,
    including features such as the number of bedrooms, the size of the lot, the age
    of the house, and the location. We want to build a model that can predict the
    sale price of a house based on these features.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个包含关于房屋及其销售价格的信息的数据集，包括诸如卧室数量、地块大小、房屋年龄和位置等特征。我们希望构建一个模型，可以根据这些特征预测房屋的销售价格。
- en: To build this model, we can use Lasso regression. We start by splitting our
    dataset into a training set and a test set. We then use the training set to fit
    a Lasso regression model with a specific value of the regularization parameter
    (alpha). We can tune this parameter using cross-validation to find the best value
    that results in the lowest error on the test set.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 构建此模型，我们可以使用Lasso回归。我们首先将数据集分为训练集和测试集。然后，我们使用训练集来拟合一个具有特定正则化参数（alpha）值的Lasso回归模型。我们可以通过交叉验证调整此参数，以找到在测试集上产生最低误差的最佳值。
- en: Once we have trained our model, we can use it to make predictions on new data
    by inputting the values of the features and computing the corresponding sale price.
    The Lasso regression model will automatically select the most important features
    for prediction by shrinking the coefficients of less important features toward
    zero.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们训练了我们的模型，我们就可以通过输入特征的值并计算相应的销售价格来使用它对新数据进行预测。Lasso回归模型将自动选择最重要的特征进行预测，通过将不太重要的特征的系数缩小到零。
- en: For example, let’s say our Lasso regression model selected the number of bedrooms
    and the location as the most important features for predicting the sale price
    and shrunk the coefficients of the other features to zero. We can use this information
    to inform our decision-making when buying or selling houses.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们的Lasso回归模型选择了卧室数量和位置作为预测销售价格的最重要特征，并将其他特征的系数缩小到零。我们可以利用这些信息来指导我们在购房或售房时的决策。
- en: Let’s take a look at how a sample solution would work in both R and Python.
    Both examples use the California Housing dataset, split the data into training
    and testing sets, fit a Lasso regression model, predict on the test set, and evaluate
    the model’s performance on the testing set using the RMSE metric.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个示例解决方案如何在R和Python中工作。这两个示例都使用了加利福尼亚住房数据集，将数据分为训练集和测试集，拟合Lasso回归模型，在测试集上进行预测，并使用RMSE指标评估模型在测试集上的性能。
- en: Example solution with R
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: R示例解决方案
- en: 'The code performs a linear regression using the Lasso regularization (L1 penalty)
    to predict the median house values based on a housing dataset. The dataset is
    loaded from a specific URL, and after preprocessing, it is split into training
    and testing sets. The `glmnet`library is used to build the model, and the **Root
    Mean Squared Error** (**RMSE**) is calculated to evaluate the model’s performance:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 代码使用Lasso正则化（L1惩罚）进行线性回归，根据住房数据集预测中值房价。数据集从特定URL加载，预处理后分为训练集和测试集。使用`glmnet`库构建模型，并计算**均方根误差**（**RMSE**）以评估模型性能：
- en: '[PRE2]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Example solution with Python
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python示例解决方案
- en: The following code begins by importing pandas and assigning it the alias `pd`
    for convenience. Next, it reads data from a CSV file hosted at a specific URL
    and creates a DataFrame named “housing” to hold the dataset. To handle categorical
    data effectively, the code performs one-hot encoding on the `ocean_proximity`
    column, converting it into multiple binary columns.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码首先导入pandas库，并将其别名设置为`pd`以方便使用。接下来，它从特定URL托管的一个CSV文件中读取数据，并创建一个名为“housing”的DataFrame来存储数据集。为了有效地处理分类数据，代码对`ocean_proximity`列执行了一元编码，将其转换为多个二进制列。
- en: Data cleanliness is vital for reliable models, so the script takes care of missing
    values by removing any rows containing NaN entries from the DataFrame. The dataset
    is then split into training and testing sets using the `train_test_split` function
    from scikit-learn, where 80% of the data is used for training and the remaining
    20% for testing.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的清洁性对于可靠的模型至关重要，因此脚本通过从DataFrame中删除包含NaN条目的任何行来处理缺失值。然后，使用scikit-learn的`train_test_split`函数将数据集分为训练集和测试集，其中80%的数据用于训练，剩余的20%用于测试。
- en: Now comes the machine-learning part. The script imports the `LassoCV` class
    from scikit-learn, which is a linear regression model with L1 regularization (Lasso).
    `LassoCV` performs cross-validation to find the optimal regularization strength
    (alpha) from a predefined set of values. The model is then trained on the training
    data using the “`fit`” method.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是机器学习部分。脚本从scikit-learn中导入`LassoCV`类，这是一个具有L1正则化（Lasso）的线性回归模型。`LassoCV`通过交叉验证从预定义的值集中找到最优的正则化强度（alpha）。然后，使用“`fit`”方法在训练数据上训练模型。
- en: After training, the model is put to the test. Predictions are made on the testing
    data using the trained `LassoCV` model, and the performance of the model is assessed
    using the RMSE metric. The RMSE represents the deviation between the predicted
    `median_house_value` and the actual target values in the testing data. A lower
    RMSE indicates better predictive accuracy.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，模型接受测试。使用训练好的`LassoCV`模型在测试数据上做出预测，并使用RMSE指标评估模型的性能。RMSE表示预测的`median_house_value`与测试数据中的实际目标值之间的偏差。较低的RMSE表示更好的预测准确性。
- en: 'Finally, the script concludes by displaying the calculated RMSE value, providing
    insight into how well the `LassoCV` model performs in predicting `median_house_value`
    on unseen data:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，脚本通过显示计算出的RMSE值来结束，这为`LassoCV`模型在预测未见数据中的`median_house_value`时的表现提供了洞察：
- en: '[PRE3]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the Python example, we have to one-hot encode the `ocean_proximity` feature
    before splitting the data into training and testing sets, which will allow the
    Lasso regression model to use the feature in the model. Both models will give
    us predictions and print RMSE of around 67,000 to 68,000 depending on the version
    of the libraries used.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python示例中，我们必须在将数据分割成训练集和测试集之前对`ocean_proximity`特征进行one-hot编码，这将允许Lasso回归模型在模型中使用该特征。这两个模型都会给出预测并打印出大约67,000到68,000的RMSE，具体取决于使用的库版本。
- en: Note
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In this section, we took a closer look at Lasso regression (L1 regularization).
    L2 regularization is used with Ridge regression. Lasso and Ridge regression differ
    mainly in the type of regularization they apply, their impact on feature selection,
    and their handling of multicollinearity. Lasso tends to produce sparse models
    with feature selection, while Ridge maintains all features but with smaller coefficients,
    making it more suitable when multicollinearity is a concern or if you want to
    control the magnitude of coefficients. The choice between them depends on your
    specific modeling goals and the nature of your data. We are not going to dive
    into the details of Ridge regression in this chapter.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们更详细地研究了Lasso回归（L1正则化）。L2正则化与Ridge回归一起使用。Lasso和Ridge回归的主要区别在于它们应用的正则化类型、对特征选择的影响以及处理多重共线性。Lasso倾向于产生具有特征选择的稀疏模型，而Ridge则保持所有特征，但系数较小，这使得它在存在多重共线性问题时或当你想控制系数幅度时更为合适。它们之间的选择取决于你的具体建模目标和数据性质。我们不会在本章中深入探讨Ridge回归的细节。
- en: In this chapter, we have investigated various linear regression models and how
    to implement these using R and Python. Linear regression models are an essential
    part of machine learning and understanding the principles is an important skill.
    In the next section, we will take a closer look into clustering algorithms, decision
    trees, and random forests.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们研究了各种线性回归模型以及如何使用R和Python实现这些模型。线性回归模型是机器学习的重要组成部分，理解其原理是一项重要的技能。在下一节中，我们将更详细地研究聚类算法、决策树和随机森林。
- en: Clustering algorithms, decision trees, and random forests
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类算法、决策树和随机森林
- en: '**Clustering algorithms** are used for unsupervised learning tasks, which means
    they are used to find patterns in data without any predefined labels or categories.
    The goal of clustering algorithms is to group similar data points together in
    clusters, while keeping dissimilar data points separate.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**聚类算法**用于无监督学习任务，这意味着它们用于在数据中寻找没有预定义标签或类别的模式。聚类算法的目标是将相似的数据点分组到簇中，同时将不相似的数据点分开。'
- en: There are several types of clustering algorithms, including K-means, hierarchical
    clustering, and density-based clustering. K-means is a popular clustering algorithm
    that works by dividing a dataset into K clusters, where K is a predefined number
    of clusters. Hierarchical clustering is another clustering algorithm that creates
    a hierarchy of clusters based on the similarity between data points. Density-based
    clustering algorithms, such as DBSCAN, group together data points that are closely
    packed together in high-density regions.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法有几种类型，包括K-means、层次聚类和基于密度的聚类。K-means是一种流行的聚类算法，它通过将数据集划分为K个簇来工作，其中K是预定义的簇数。层次聚类是另一种聚类算法，它根据数据点之间的相似性创建簇的层次结构。基于密度的聚类算法，如DBSCAN，将紧密聚集在高度密集区域的数据点分组在一起。
- en: '**Decision trees**, on the other hand, are used for supervised learning tasks,
    which means they are used to make predictions or decisions based on input data
    with predefined labels or categories. A decision tree is a tree-like structure
    that consists of nodes and branches, where each node represents a feature or attribute,
    and each branch represents a decision based on that feature. The goal of a decision
    tree is to create a model that can accurately predict the label or category of
    a new input based on its features.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**决策树**，另一方面，用于监督学习任务，这意味着它们用于根据具有预定义标签或类别的输入数据进行预测或决策。决策树是一种树状结构，由节点和分支组成，其中每个节点代表一个特征或属性，每个分支代表基于该特征的决策。决策树的目标是创建一个模型，可以根据其特征准确预测新输入的标签或类别。'
- en: There are several types of decision trees, including ID3, C4.5, and CART. The
    ID3 algorithm is a popular decision tree algorithm that works by choosing the
    attribute with the highest information gain as the root node, and recursively
    building the tree by selecting attributes that maximize information gain at each
    level. The C4.5 algorithm is an improved version of ID3 that can handle continuous
    and discrete data, and CART is another decision tree algorithm that can handle
    both classification and regression tasks.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树有多种类型，包括ID3、C4.5和CART。ID3算法是一种流行的决策树算法，它通过选择具有最高信息增益的属性作为根节点，并通过在每个级别上选择最大化信息增益的属性递归地构建树。C4.5算法是ID3的改进版本，可以处理连续和离散数据，而CART是另一种可以处理分类和回归任务的决策树算法。
- en: '**Random forests** combine multiple decision trees to create a more accurate
    and robust model. A random forest consists of a large number of decision trees,
    each trained on a different subset of the data and using a random subset of the
    available features. This helps to reduce overfitting and increase the generalization
    ability of the model.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机森林**通过结合多个决策树来创建一个更准确和鲁棒的模型。随机森林由大量决策树组成，每棵树都在数据的不同子集上训练，并使用可用的随机特征子集。这有助于减少过拟合并提高模型的泛化能力。'
- en: The random subset of features used for each tree is selected randomly from the
    available features, with a new subset selected for each tree. This ensures that
    each tree in the forest is different and provides a diverse set of predictions.
    During training, each tree in the forest is grown to its full depth, and predictions
    are made by aggregating the predictions of all the trees in the forest.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为每棵树使用的随机特征子集是从可用特征中随机选择的，每棵树选择一个新的子集。这确保了森林中的每棵树都是不同的，并提供了多样化的预测集。在训练过程中，森林中的每棵树都会生长到其最大深度，并通过聚合森林中所有树的预测来进行预测。
- en: The aggregation process can vary depending on the task at hand. For classification
    tasks, the most common method is to use a majority vote, where the final prediction
    is the class that is predicted by the most trees in the forest. For regression
    tasks, the most common method is to use the average prediction of all the trees
    in the forest.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合过程可能因任务的不同而有所差异。对于分类任务，最常见的方法是使用多数投票法，其中最终预测是森林中预测最多的树的类别。对于回归任务，最常见的方法是使用森林中所有树的平均预测。
- en: Random forests have several advantages over a single decision tree, including
    improved accuracy, reduced overfitting, and robustness to noise and outliers in
    the data. They are also relatively easy to use and can handle a wide range of
    input features and data types. However, they can be computationally expensive
    to train and can be difficult to interpret and visualize, especially when dealing
    with a large number of trees.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林相对于单一决策树具有多个优势，包括提高准确性、减少过拟合以及对于数据中的噪声和异常值的鲁棒性。它们也相对容易使用，可以处理广泛的输入特征和数据类型。然而，它们的训练可能计算成本高昂，并且难以解释和可视化，尤其是在处理大量树的情况下。
- en: In the next sections, we will take a closer look at some of the clustering and
    decision tree algorithms.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将更详细地探讨一些聚类和决策树算法。
- en: K-means clustering
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K-means聚类
- en: K-means clustering is a popular algorithm used to partition a set of data points
    into K clusters, where K is a predefined number. The algorithm works by iteratively
    assigning data points to the nearest cluster centroid and updating the cluster
    centroids based on the new assignments.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: K-means聚类是一种流行的算法，用于将一组数据点划分为K个聚类，其中K是一个预定义的数字。该算法通过迭代地将数据点分配到最近的聚类质心，并根据新的分配更新聚类质心来工作。
- en: 'Here’s a simple step-by-step overview of how the K-means algorithm works:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是K-means算法的简单步骤概述：
- en: Initialize K centroids randomly from the data points.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从数据点中随机初始化K个质心。
- en: Assign each data point to the nearest centroid based on the Euclidean distance
    between the data point and the centroid.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据数据点和质心之间的欧几里得距离，将每个数据点分配到最近的质心。
- en: Update the centroids of each cluster by computing the mean of all the data points
    assigned to that cluster.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过计算分配给每个聚类的所有数据点的平均值来更新每个聚类的质心。
- en: Repeat steps 2 and 3 until the centroids no longer move significantly or a maximum
    number of iterations is reached.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤2和3，直到质心不再显著移动或达到最大迭代次数。
- en: The goal of the K-means algorithm is to minimize the sum of squared distances
    between each data point and its assigned cluster centroid, also known as the “inertia.”
    The algorithm can be sensitive to the initial random selection of centroids, so
    it’s often a good idea to run the algorithm multiple times with different initializations
    and select the solution with the lowest inertia.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: K-means算法的目标是最小化每个数据点到其分配簇质心的平方距离之和，也称为“惯性”。该算法对初始随机选择质心可能很敏感，因此多次运行算法并选择具有最低惯性的解决方案通常是一个好主意。
- en: K-means is a fast and effective algorithm for clustering data, but it does have
    some limitations. It assumes that the clusters are spherical and of equal size,
    and it can be sensitive to outliers and noise in the data. Additionally, determining
    the optimal number of clusters, K, can be challenging and may require some domain
    knowledge or trial and error.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: K-means是一种快速有效的聚类数据算法，但它也有一些局限性。它假设簇是球形且大小相等，并且对数据中的异常值和噪声很敏感。此外，确定最佳簇数K可能具有挑战性，可能需要一些领域知识或试错。
- en: Example
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例
- en: The iris dataset contains measurements of four features (sepal length, sepal
    width, petal length, and petal width) for 150 iris flowers, with 50 flowers from
    each of three species (setosa, versicolor, and virginica). We can use K-means
    clustering to group these flowers into distinct clusters based on their feature
    values.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 鸢尾花数据集包含了150朵鸢尾花的四个特征（花瓣长度、花瓣宽度、萼片长度和萼片宽度）的测量值，其中每种鸢尾花（setosa、versicolor和virginica）各有50朵。我们可以使用K-means聚类方法根据它们的特征值将这些花朵分组到不同的簇中。
- en: To do this, we first select the features we want to cluster on and preprocess
    the data by scaling the features to have zero mean and unit variance. Scaling
    is done to ensure that we have equal influence of features. We then apply the
    K-means algorithm to the preprocessed data, specifying the number of clusters
    K we want to create. In this case, we might choose K=3 to correspond to the three
    iris species.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，我们首先选择我们想要聚类的特征，并通过缩放特征使其具有零均值和单位方差来预处理数据。缩放是为了确保特征具有相等的影响力。然后，我们将K-means算法应用于预处理后的数据，指定我们想要创建的簇数K。在这种情况下，我们可能会选择K=3，以对应三种鸢尾花物种。
- en: The K-means algorithm then partitions the flowers into K clusters based on their
    feature values, with each cluster represented by its centroid (the mean feature
    values of the flowers assigned to the cluster). We can examine the resulting clusters
    and their centroids to gain insights into the structure of the iris dataset.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，K-means算法根据特征值将花朵划分为K个簇，每个簇由其质心（分配给簇的花朵的平均特征值）表示。我们可以检查这些簇及其质心，以深入了解鸢尾花数据集的结构。
- en: For example, we might find that one cluster contains flowers with smaller sepal
    and petal dimensions, which could correspond to the setosa species. Another cluster
    might contain flowers with larger petal dimensions and intermediate sepal dimensions,
    which could correspond to the versicolor species. The third cluster might contain
    flowers with larger sepal dimensions and larger petal dimensions, which could
    correspond to the virginica species.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可能会发现一个簇包含花瓣和萼片尺寸较小的花朵，这可能对应于setosa物种。另一个簇可能包含花瓣尺寸较大而萼片尺寸中等的花朵，这可能对应于versicolor物种。第三个簇可能包含萼片和花瓣尺寸都较大的花朵，这可能对应于virginica物种。
- en: By using K-means clustering to group the iris flowers based on their feature
    values, we can gain a deeper understanding of the structure of the dataset and
    potentially identify patterns and relationships in data. Let’s see how the above
    example would look in R and Python.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用K-means聚类方法根据特征值对鸢尾花进行分组，我们可以更深入地了解数据集的结构，并可能识别出数据中的模式和关系。让我们看看上述示例在R和Python中的样子。
- en: Example with R
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用R的示例
- en: 'The following R code uses the “iris” dataset, a popular dataset in the machine-learning
    community. It performs K-means clustering on the dataset’s four numeric attributes:
    `Sepal.Length`, `Sepal.Width`, `Petal.Length`, and `Petal.Width`. The code sets
    a random seed for reproducibility and applies the K-means algorithm with three
    cluster centers. After the clustering is performed, the code displays the cluster
    assignments for each data point, indicating which cluster each observation belongs
    to (represented by values 1, 2, or 3). K-means clustering aims to group similar
    data points into clusters and is a common technique for unsupervised machine learning
    tasks:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的R代码使用“iris”数据集，这是机器学习社区中一个流行的数据集。它对数据集的四个数值属性执行K-means聚类：`Sepal.Length`（花萼长度）、`Sepal.Width`（花萼宽度）、`Petal.Length`（花瓣长度）和`Petal.Width`（花瓣宽度）。代码设置了一个随机种子以确保可重复性，并使用三个聚类中心应用K-means算法。聚类完成后，代码显示每个数据点的聚类分配，指示每个观测值属于哪个聚类（用值1、2或3表示）。K-means聚类旨在将相似数据点分组到聚类中，是无监督机器学习任务中的一种常见技术：
- en: '[PRE4]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Example with Python
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python示例
- en: The following Python code uses the scikit-learn library to perform K-means clustering
    on the Iris dataset. The Iris dataset is loaded using `load_iris()` from scikit-learn,
    containing measurements of iris flowers’ sepal length, sepal width, petal length,
    and petal width, along with their corresponding species labels.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的Python代码使用scikit-learn库在Iris数据集上执行K-means聚类。使用scikit-learn的`load_iris()`函数加载数据集，包含鸢尾花花瓣长度、花瓣宽度、花萼长度和花萼宽度的测量值，以及它们对应的物种标签。
- en: The script extracts the four feature columns for clustering and stores them
    in the `iris_cluster` variable. Then, it imports the `KMeans` class from scikit-learn’s
    `sklearn.cluster` module.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本提取了四个特征列用于聚类，并将它们存储在`iris_cluster`变量中。然后，它从scikit-learn的`sklearn.cluster`模块导入`KMeans`类。
- en: The K-means algorithm is applied to the feature data (`iris_cluster`) with the
    number of clusters (`n_clusters`) set to `3`. The `random_state` parameter is
    set to `123` to ensure the reproducibility of the results.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: K-means算法应用于特征数据(`iris_cluster`)，将聚类数量(`n_clusters`)设置为`3`。将`random_state`参数设置为`123`以确保结果的再现性。
- en: 'After clustering, the code prints the cluster assignments for each data point
    in the Iris dataset. Each data point is assigned a cluster label (0, 1, or 2),
    indicating the group it belongs to according to the K-means clustering:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类后，代码打印出Iris数据集中每个数据点的聚类分配。每个数据点被分配一个聚类标签（0、1或2），指示它根据K-means聚类属于哪个组：
- en: '[PRE5]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Both of the code examples will print the cluster assignments for the iris dataset.
    The result looks similar to the following:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个代码示例都将打印出Iris数据集的聚类分配。结果看起来类似于以下：
- en: '[PRE6]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This is a simple example. In practice, you would likely spend more time exploring
    and visualizing the data, tuning the number of clusters, and interpreting the
    results of the clustering algorithm.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的例子。在实践中，你可能会花更多的时间探索和可视化数据，调整聚类数量，并解释聚类算法的结果。
- en: ID3 decision tree
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ID3决策树
- en: '**ID3** (**Iterative Dichotomiser 3**) is a popular algorithm for building
    decision trees. The ID3 algorithm was developed by Ross Quinlan in 1986 and is
    based on the concept of information entropy.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '**ID3**（**迭代二分器3**）是构建决策树的一种流行算法。ID3算法由Ross Quinlan于1986年开发，基于信息熵的概念。'
- en: The goal of a decision tree is to create a model that can be used to make predictions
    or classify new instances based on their characteristics. A decision tree consists
    of a set of nodes and edges, where each node represents a decision or a test on
    one or more input variables, and each edge represents the possible outcome of
    that decision.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的目标是创建一个模型，可以用来根据其特征对新的实例进行预测或分类。决策树由一组节点和边组成，其中每个节点代表一个或多个输入变量的决策或测试，每条边代表该决策的可能结果。
- en: The ID3 algorithm works by recursively partitioning the data based on the input
    variables with the highest information gain. Information gain is a measure of
    the reduction in entropy or impurity that results from splitting the data on a
    particular input variable. The algorithm selects the input variable that maximizes
    information gain at each step, until all instances in a given partition belong
    to the same class or a stopping criterion is met.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ID3算法通过递归地根据具有最高信息增益的输入变量对数据进行分区来工作。信息增益是数据在特定输入变量上分割后熵或纯度减少的度量。算法在每个步骤中选择最大化信息增益的输入变量，直到给定分区中的所有实例都属于同一类或满足停止条件。
- en: The ID3 algorithm has several advantages, including its simplicity and efficiency
    in handling large datasets with categorical variables. However, it has limitations
    in handling continuous variables and overfitting, which can be addressed by using
    modified algorithms such as C4.5 and CART.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ID3算法具有几个优点，包括其处理具有分类变量的大数据集的简单性和效率。然而，它在处理连续变量和过拟合方面存在局限性，这些问题可以通过使用修改后的算法（如C4.5和CART）来解决。
- en: In the next example, we can see how ID3 works in practice.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个例子中，我们可以看到ID3在实际中的应用。
- en: Example
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例
- en: 'In this example, we are using the following animal-related dataset:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用以下与动物相关的数据集：
- en: '| **Animal** | **Has fur?** | **Has feathers?** | **Eats meat?** | **Classification**
    |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| **动物** | **有毛？** | **有羽毛？** | **吃肉？** | **分类** |'
- en: '| Dog | Yes | No | Yes | Mammal |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 狗 | 是 | 否 | 是 | 哺乳动物 |'
- en: '| Cat | Yes | No | Yes | Mammal |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 猫 | 是 | 否 | 是 | 哺乳动物 |'
- en: '| Parrot | No | Yes | No | Bird |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 鹦鹉 | 否 | 是 | 否 | 鸟 |'
- en: '| Eagle | No | Yes | Yes | Bird |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 鹰 | 否 | 是 | 是 | 鸟 |'
- en: '| Snake | No | No | Yes | Reptile |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 蛇 | 否 | 否 | 是 | 爬行动物 |'
- en: 'Table 2.3: Animal characteristics data'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.3：动物特征数据
- en: Our goal is to build a decision tree that can classify animals based on their
    features.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目的是构建一个决策树，可以根据动物的特征对动物进行分类。
- en: First, let’s calculate the entropy of the entire dataset. Entropy measures the
    impurity of a dataset. A dataset with all the same class labels has an entropy
    of 0, while a dataset with an equal number of examples from each class has an
    entropy of 1.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们计算整个数据集的熵。熵衡量数据集的杂乱程度。所有类标签都相同的数据集熵为0，而每个类中示例数量相等的数据集熵为1。
- en: 'A general equation to calculate entropy can be represented in the the following
    way:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 计算熵的一般方程可以表示为以下方式：
- en: E(S) = − ∑ i=1 n p i log 2p i
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: E(S) = − ∑ i=1 n p i log 2p i
- en: In this equation, *E(S)* is the entropy of a set *S*, *n* is the number of classes
    in *S*, and *pi* is the proportion of the number of elements in *S* that belong
    to class *i*.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，*E(S)* 是集合 *S* 的熵，*n* 是 *S* 中的类别数，*pi* 是 *S* 中属于类别 *i* 的元素数量的比例。
- en: 'In this example, we have *n=3* classes: Mammal, Bird, and Reptile. The number
    of animals in each class is as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们有 *n=3* 个类别：哺乳动物、鸟和爬行动物。每个类别中的动物数量如下：
- en: 'Mammal: 2 (Dog, Cat)'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哺乳动物：2（狗，猫）
- en: 'Bird: 2 (Parrot, Eagle)'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鸟：2（鹦鹉，鹰）
- en: 'Reptile: 1 (Snake)'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 爬行动物：1（蛇）
- en: 'Therefore, the probabilities are as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，概率如下：
- en: p Mammal = 2 / 5 = 0.4
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: p 哺乳动物 = 2 / 5 = 0.4
- en: p Bird = 2 / 5 = 0.4
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: p 鸟 = 2 / 5 = 0.4
- en: p Reptile = 1 / 5 = 0.2
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: p 爬行动物 = 1 / 5 = 0.2
- en: 'Substituting these values into the entropy formula, we get the following:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些值代入熵公式，我们得到以下结果：
- en: E(S) = − (0.4log 20.4 + 0.4log 20.4 + 0.2log 20.2) ≈ 1.52193
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: E(S) = − (0.4log 20.4 + 0.4log 20.4 + 0.2log 20.2) ≈ 1.52193
- en: So the entropy of the “Classification” attribute is approximately 1.52.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，“分类”属性的熵大约为1.52。
- en: Next, let’s calculate the information gain of the “Has fur?” attribute. Information
    gain is a measure of how much a given attribute or feature in a dataset contributes
    to reducing the uncertainty in the classification of the data. In decision trees,
    information gain is used to select the best attribute to use for splitting the
    data at each node of the tree.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们计算“有毛？”属性的增益信息。信息增益是衡量数据集中给定属性或特征对数据分类不确定性减少的贡献程度的度量。在决策树中，信息增益用于在每个树的节点处选择用于分割数据的最佳属性。
- en: 'The information gain formula is as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 信息增益公式如下：
- en: IG(S, A) = E(S) −  ∑ v∈Values(A) |S v| _ |S| E(S v)
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: IG(S, A) = E(S) −  ∑ v∈Values(A) |S v| _ |S| E(S v)
- en: where *A* is the attribute (in this case, “Has fur?”), *v* is a possible value
    of the attribute, *Values(A)* is the set of possible values, *|Sv|* is the number
    of animals in the dataset that have attribute *A=v*, and *E(Sv)* is the entropy
    of the subset of animals that have attribute *A=v*.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *A* 是属性（在这种情况下，“有毛吗？”），*v* 是属性的可能的值，*Values(A)* 是属性的可能的值的集合，*|Sv|* 是数据集中具有属性
    *A=v* 的动物的数量，*E(Sv)* 是具有属性 *A=v* 的动物的子集的熵。
- en: 'We can split the data based on whether the animals have fur or not. The subsets
    are as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据动物是否有毛来分割数据。子集如下：
- en: 'Has fur: Dog, Cat'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有毛：狗，猫
- en: 'No fur: Parrot, Eagle, Snake'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无毛：鹦鹉，鹰，蛇
- en: 'The proportion of animals in each subset is as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 每个子集中动物的比例如下：
- en: '|S Has fur| = 2'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '|S Has fur| = 2'
- en: '|S No fur| = 3'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '|S No fur| = 3'
- en: 'To calculate E(S Has fur), we need to count the number of animals in each class
    that have fur:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算 E(S Has fur)，我们需要计算每个类别中有毛的动物的数量：
- en: 'Mammal: 2 (Dog, Cat)'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哺乳动物：2（狗，猫）
- en: 'Bird: 0'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鸟：0
- en: 'Reptile: 0'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 爬行动物：0
- en: 'Therefore, the probabilities are as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，概率如下：
- en: p Mammal = 1
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: p 哺乳动物 = 1
- en: p Bird = 0
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: p 鸟 = 0
- en: p Reptile = 0
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: p 爬行动物 = 0
- en: 'Substituting these values into the entropy formula, we get the following results:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些值代入熵公式，我们得到以下结果：
- en: E(S Has fur) = − 1log 21 = 0
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: E(S 有毛) = − 1log2(1) = 0
- en: 'Entropy of 0 means that the set is perfectly classified: in this case, all
    animals with fur are mammals.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 0的熵意味着该集合被完美分类：在这种情况下，所有有毛发的动物都是哺乳动物。
- en: 'To calculate E(S No fur), we need to count the number of animals in each class
    that don’t have fur:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算 E(S 无毛)，我们需要计算每个类别中没有毛发的动物数量：
- en: 'Mammal: 0'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哺乳动物：0
- en: 'Bird: 2 (Parrot, Eagle)'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鸟：2（鹦鹉，鹰）
- en: 'Reptile: 1 (Snake)'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 爬行动物：1（蛇）
- en: 'Therefore, the probabilities are as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，概率如下：
- en: p Mammal = 0
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: p 哺乳动物 = 0
- en: p Bird = 2 / 3 ≈ 0.67
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: p 鸟 = 2 / 3 ≈ 0.67
- en: p Reptile = 1 / 3 ≈ 0.33
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: p 爬行动物 = 1 / 3 ≈ 0.33
- en: 'Substituting these values into the entropy formula, we get the following result:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些值代入熵公式，我们得到以下结果：
- en: E(S No fur) = − (0 + 0.67log 20.67 + 0.33log 20.33) ≈ 0.9183
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: E(S 无毛) = − (0 + 0.67log2(0.67) + 0.33log2(0.33)) ≈ 0.9183
- en: 'Now we can substitute these values into the information gain formula:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将这些值代入信息增益公式：
- en: IG(S, Has fur?) = E(S) −  ∑ v∈Values(Has fur?) |S v| _ |S| E(S v)
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: IG(S, 有毛？) = E(S) − ∑v∈Values(有毛？) |Sv| / |S| E(Sv)
- en: = 1.52193 − ( 2 _ 5  × 0 +  3 _ 5  × 0.9183) ≈ 0.971
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: = 1.52193 − (2 × 5 × 0 + 3 × 5 × 0.9183) ≈ 0.971
- en: Therefore, the information gained for the “`Has` `fur`?” attribute is approximately
    0.971.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于“有毛？”属性获得的信息量大约为 0.971。
- en: We could also calculate the information gained for the “`Has` `feathers`?” and
    “`Eats` `meat`?” attributes by using the same formula and splitting the data based
    on whether the animals have feathers or not or whether they eat meat or not. The
    attribute with the highest information gain would be selected to split the data
    at the root of the decision tree.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以通过使用相同的公式并根据动物是否有羽毛或是否吃肉来分割数据来计算“有羽毛？”和“吃肉？”属性获得的信息量。信息增益最高的属性将被选为在决策树根部分割数据。
- en: Information gain for “Has feathers?” is also 0.971, and for “Eats meat?” it
    is 0.322\. In this case, we will select “Has fur?” for our root node.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: “有羽毛？”的信息增益也是 0.971，而“吃肉？”是 0.322。在这种情况下，我们将选择“有毛？”作为我们的根节点。
- en: Let’s take a look at the same example with R and Python. Both code snippets
    will load the animal dataset, create the decision tree model, visualize it, and
    test the tree with new data. At the end, we can see that the new animal is classified
    as `Mammal`.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看使用 R 和 Python 的相同示例。这两个代码片段都将加载动物数据集，创建决策树模型，可视化它，并用新数据测试树。最后，我们可以看到新的动物被分类为“哺乳动物”。
- en: 'The final decision tree looks like the one in the following figure:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的决策树看起来像下面这个图：
- en: '![Figure 2.1: Final decision tree (printed using the following R code)](img/B19863_02_01.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.1：最终决策树（使用以下 R 代码打印）](img/B19863_02_01.jpg)'
- en: 'Figure 2.1: Final decision tree (printed using the following R code)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1：最终决策树（使用以下 R 代码打印）
- en: 'Here is the example with R:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是使用 R 的示例：
- en: '[PRE7]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The Python example is similar to the preceding R example, but in Python, we
    also need to convert categorical features to numerical using one-hot encoding:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: Python 示例与前面的 R 示例类似，但在 Python 中，我们还需要使用 one-hot 编码将分类特征转换为数值：
- en: '[PRE8]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This is a really simple example of a decision tree using the ID3 algorithm.
    In real-world examples, we would most likely have a lot of data and therefore
    multiple nodes and branches in our final tree.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个使用 ID3 算法的决策树的简单示例。在现实世界的例子中，我们很可能有大量数据，因此最终树中会有很多节点和分支。
- en: We have now learned how clustering algorithms, decision trees, and random forests
    work. These algorithms are an important part of machine learning and are commonly
    used for classification. In the next section, we will take a closer look into
    boosting algorithms and Naive Bayes.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经学习了聚类算法、决策树和随机森林是如何工作的。这些算法是机器学习的重要组成部分，常用于分类。在下一节中，我们将更深入地探讨提升算法和朴素贝叶斯。
- en: Boosting algorithms and Naive Bayes
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升算法和朴素贝叶斯
- en: '**Boosting** is a machine learning technique that involves creating an ensemble
    of weak learners to form a strong learner. The idea behind boosting algorithms
    is to iteratively train models on the data, where each new model attempts to correct
    the errors of the previous model. Boosting algorithms are widely used in supervised
    learning tasks, such as classification and regression.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '**提升**是一种机器学习技术，涉及创建一个弱学习者的集成以形成一个强学习者。提升算法背后的思想是迭代地在数据上训练模型，其中每个新的模型都试图纠正先前模型的错误。提升算法在监督学习任务中广泛使用，如分类和回归。'
- en: 'There are several key types of boosting algorithms:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 提升算法有几种关键类型：
- en: '**AdaBoost (Adaptive Boosting):** AdaBoost is one of the earliest and most
    popular boosting algorithms. It starts by training a base classifier on the entire
    dataset and then sequentially trains additional classifiers on the samples that
    the previous classifiers got wrong. The final prediction is made by taking a weighted
    sum of the predictions of all the classifiers.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AdaBoost（自适应提升）**：AdaBoost是最早且最受欢迎的提升算法之一。它首先在全部数据集上训练一个基分类器，然后依次在先前分类器出错的数据样本上训练额外的分类器。最终的预测是通过取所有分类器预测的加权总和来完成的。'
- en: '**Gradient Boosting:** Gradient Boosting is another popular boosting algorithm
    that works by iteratively adding new models to the ensemble, each trained to minimize
    the error of the previous models. Gradient Boosting is used in regression and
    classification problems and has been shown to achieve state-of-the-art results
    in many applications.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度提升**：梯度提升是另一种流行的提升算法，它通过迭代地向集成中添加新模型，每个新模型都训练以最小化先前模型的误差。梯度提升用于回归和分类问题，并在许多应用中显示出达到最先进的结果。'
- en: '**XGBoost:** XGBoost (Extreme Gradient Boosting) is a popular and highly optimized
    implementation of the Gradient Boosting algorithm. XGBoost uses a regularized
    objective function and a variety of techniques to reduce overfitting and improve
    accuracy.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**XGBoost**：XGBoost（极端梯度提升）是梯度提升算法的一种流行且高度优化的实现。XGBoost使用正则化目标函数和多种技术来减少过拟合并提高准确性。'
- en: Boosting algorithms are known for their ability to improve the accuracy of machine-learning
    models by reducing bias and variance.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 提升算法因其能够通过减少偏差和方差来提高机器学习模型的准确性而闻名。
- en: '**Naive Bayes** is a simple but effective algorithm used for classification
    tasks. It is based on Bayes’ theorem, which states that the probability of a hypothesis
    (in this case, a class label) is updated in the light of new evidence (in this
    case, the feature values of a new data point). The algorithm assumes that the
    features are independent of each other, which is why it is called “naïve.”'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '**朴素贝叶斯**是一种简单但有效的分类算法。它基于贝叶斯定理，该定理指出，在新的证据（在这种情况下，为新数据点的特征值）的背景下，假设（在这种情况下，为类别标签）的概率被更新。该算法假设特征之间相互独立，这就是为什么它被称为“朴素”。'
- en: In Naive Bayes, the probability of a data point belonging to a particular class
    is calculated by multiplying the prior probability of that class by the likelihood
    of the data point given that class. The algorithm then selects the class with
    the highest probability as the predicted class for that data point.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在朴素贝叶斯中，计算数据点属于特定类别的概率是通过将该类别的先验概率与给定该类别的数据点的似然性相乘来完成的。然后，算法选择概率最高的类别作为该数据点的预测类别。
- en: 'There are several variants of the Naive Bayes algorithm:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯算法有几种变体：
- en: '**Gaussian Naive Bayes**: Used when the features are continuous and assumed
    to be normally distributed'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高斯朴素贝叶斯**：当特征是连续的，并且假设为正态分布时使用。'
- en: '**Multinomial Naive Bayes:** Used when the features are discrete and represent
    counts or frequencies (such as in text classification)'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多项式朴素贝叶斯**：当特征是离散的，表示计数或频率时（例如在文本分类中）使用。'
- en: '**Bernoulli Naive Bayes:** A variant of Multinomial Naive Bayes, used when
    the features are binary (such as in spam filtering)'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**伯努利朴素贝叶斯**：是多项式朴素贝叶斯的一种变体，当特征是二进制时（例如在垃圾邮件过滤中）使用。'
- en: Naive Bayes is a simple and efficient algorithm that works well on high-dimensional
    datasets with sparse features. It is widely used in natural-language processing,
    spam filtering, sentiment analysis, and other classification tasks. However, the
    assumption of feature independence may not hold true in all cases, and the algorithm
    may not perform well if the data violates this assumption.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯是一个简单且高效的算法，在具有稀疏特征的高维数据集上表现良好。它在自然语言处理、垃圾邮件过滤、情感分析和其他分类任务中得到广泛应用。然而，特征独立性假设在所有情况下可能并不成立，如果数据违反了这个假设，算法可能表现不佳。
- en: In the next sections, we will take a closer look into some of the boosting and
    Naive Bayes algorithms.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将更深入地研究一些提升和朴素贝叶斯算法。
- en: XGBoost
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XGBoost
- en: '**XGBoost** (**eXtreme Gradient Boosting**) is an open source machine learning
    library that is designed to be highly efficient, flexible, and scalable. It is
    an implementation of gradient-boosting algorithms that can be used for both classification
    and regression problems.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '**XGBoost**（**极梯度提升**）是一个开源的机器学习库，旨在高效、灵活和可扩展。它是对梯度提升算法的实现，可用于分类和回归问题。'
- en: It is based on the gradient-boosting framework and uses an ensemble of decision
    trees to make predictions. XGBoost is designed to handle large-scale and high-dimensional
    data and provides various techniques to prevent overfitting, such as regularization
    and early stopping.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 它基于梯度提升框架，并使用决策树的集成来进行预测。XGBoost 设计用于处理大规模和高维数据，并提供各种技术来防止过拟合，例如正则化和提前停止。
- en: Let’s take a look into simple examples of XGBoost with R and Python. In these
    examples, we will use the iris dataset that we have already used in earlier chapters
    of this book. We will split the data into train and test datasets and then train
    our XGBoost model to predict the species. At the end, we will test the model with
    our test data and evaluate the model performance.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看使用 R 和 Python 的 XGBoost 的简单示例。在这些示例中，我们将使用本书早期章节中已经使用过的鸢尾花数据集。我们将数据分为训练集和测试集，然后训练我们的
    XGBoost 模型来预测物种。最后，我们将使用测试数据测试模型并评估模型性能。
- en: 'Here’s the example with R:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是使用 R 的示例：
- en: '[PRE9]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here is the example with Python:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是使用 Python 的示例：
- en: '[PRE10]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Gaussian Naive Bayes
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高斯朴素贝叶斯
- en: Gaussian Naive Bayes (GNB) is a variant of the Naive Bayes algorithm that assumes
    a Gaussian (normal) distribution for the input variables. In GNB, the probability
    distribution of each input variable is estimated separately for each class using
    the training data.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯朴素贝叶斯（GNB）是朴素贝叶斯算法的一种变体，它假设输入变量服从高斯（正态）分布。在 GNB 中，每个输入变量的概率分布是使用训练数据分别对每个类别进行估计的。
- en: During the testing phase, the model calculates the likelihood of the input features
    belonging to each class based on the Gaussian distribution parameters estimated
    during training. Then, the model applies Bayes’ theorem to calculate the posterior
    probability of each class, given the input features. The class with the highest
    posterior probability is then assigned to the input.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试阶段，模型根据训练期间估计的高斯分布参数计算输入特征属于每个类的可能性。然后，模型应用贝叶斯定理来计算给定输入特征的每个类的后验概率。然后，将具有最高后验概率的类别分配给输入。
- en: GNB is called “naïve” because it makes a strong assumption that the input features
    are conditionally independent, given the class label. This assumption simplifies
    the model and makes the computation of the posterior probabilities tractable.
    However, this assumption may not hold in practice for some datasets, and thus,
    the model may not perform well. Nonetheless, GNB can be a fast and accurate classifier
    for datasets where the independence assumption holds.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: GNB 被称为“朴素”是因为它假设输入特征在给定类别标签的条件下是条件独立的。这个假设简化了模型并使得后验概率的计算变得可行。然而，这个假设在某些数据集中可能不成立，因此模型可能表现不佳。尽管如此，GNB
    对于独立性假设成立的数据集来说可以是一个快速且准确的分类器。
- en: Let’s take a look into a similar classification problem, as we did with XGBoost,
    using Gaussian Naive Bayes. Here is the example code with both R and Python. Once
    again, we are using the iris dataset to classify the different species.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看与 XGBoost 类似的分类问题，使用高斯朴素贝叶斯。以下是使用 R 和 Python 的示例代码。再次，我们使用鸢尾花数据集来分类不同的物种。
- en: 'Here is the example with R:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是使用 R 的示例：
- en: '[PRE11]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here is the example with Python:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是使用 Python 的示例：
- en: '[PRE12]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now we have familiarized ourselves with the concepts of boosting algorithms
    and Naive Bayes. These methods are used widely in Qlik AutoML, and understanding
    the concepts of these algorithms is an essential skill to work with machine-learning
    problems. In our next section, we will take a closer look at some of the advanced
    machine-learning algorithms, including neural networks, deep learning, and natural-language
    models.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了提升算法和朴素贝叶斯的概念。这些方法在Qlik AutoML中得到广泛应用，理解这些算法的概念是处理机器学习问题的一项基本技能。在下一节中，我们将更深入地探讨一些高级机器学习算法，包括神经网络、深度学习和自然语言模型。
- en: Neural networks, deep learning, and natural-language models
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络、深度学习和自然语言模型
- en: '**Neural networks** are a type of machine-learning algorithm that is inspired
    by the structure and function of the human brain. They are composed of layers
    of interconnected nodes or artificial neurons that process and transmit information.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '**神经网络**是一种受人类大脑结构和功能启发的机器学习算法。它们由多层相互连接的节点或人工神经元组成，这些节点处理和传输信息。'
- en: In a neural network, the input data is fed into the first layer of nodes, which
    applies a set of mathematical transformations to the data and produces an output.
    The output of the first layer is then fed into the second layer, which applies
    another set of transformations to produce another output, and so on until the
    final output is produced.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中，输入数据被输入到第一层节点，这些节点对数据进行一系列数学变换并产生输出。第一层的输出随后被输入到第二层，该层应用另一组变换以产生另一个输出，依此类推，直到产生最终输出。
- en: 'The connections between the nodes in the neural network have weights that are
    adjusted during the learning process to optimize the network’s ability to make
    accurate predictions or classifications. This is typically achieved using an optimization
    algorithm such as stochastic gradient descent. An example of the structure of
    a neural network is visualized in the following figure:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中节点的连接在学习过程中会调整权重，以优化网络做出准确预测或分类的能力。这通常是通过使用随机梯度下降等优化算法来实现的。以下图示展示了神经网络结构的示例：
- en: '![Figure 2.2: High-level architecture of a neural network](img/B19863_02_02.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![图2.2：神经网络的概要架构](img/B19863_02_02.jpg)'
- en: 'Figure 2.2: High-level architecture of a neural network'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2：神经网络的概要架构
- en: Neural networks have been used to solve a wide range of machine-learning problems,
    including image and speech recognition, natural language processing, and predictive
    modeling. They have been shown to be effective in many applications due to their
    ability to learn complex, non-linear relationships between input and output data.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络已被用于解决广泛的机器学习问题，包括图像和语音识别、自然语言处理和预测建模。由于它们能够学习输入和输出数据之间的复杂非线性关系，它们在许多应用中已被证明是有效的。
- en: '**Deep learning** involves the use of neural networks with multiple layers.
    It has achieved significant success in a wide range of applications, including
    computer vision, speech recognition, natural-language processing, and game playing.
    Some notable examples include the use of deep learning in image recognition tasks,
    such as identifying objects in photos, and in natural language processing tasks,
    such as language translation and sentiment analysis.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度学习**涉及使用多层神经网络。它在包括计算机视觉、语音识别、自然语言处理和游戏在内的广泛应用中取得了显著的成功。一些值得注意的例子包括在图像识别任务中使用深度学习，例如在照片中识别物体，以及在自然语言处理任务中使用深度学习，例如语言翻译和情感分析。'
- en: One of the key advantages of deep learning is its ability to automatically learn
    features from data without the need for manual feature engineering. This makes
    it possible to train models on large datasets with many features, which can be
    computationally challenging or even impossible with traditional machine learning
    methods.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的一个关键优势是它能够自动从数据中学习特征，而无需手动特征工程。这使得在具有许多特征的庞大数据集上训练模型成为可能，这在传统机器学习方法中可能是计算上具有挑战性，甚至是不可能的。
- en: However, training deep neural networks can also be challenging due to the large
    number of parameters and the risk of overfitting. To address these challenges,
    researchers have developed a variety of techniques, including regularization methods,
    dropout, and batch normalization, that can improve the performance and stability
    of deep neural networks.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于参数数量庞大和过拟合的风险，训练深度神经网络也可能具有挑战性。为了解决这些挑战，研究人员已经开发出各种技术，包括正则化方法、dropout和批量归一化，这些技术可以提高深度神经网络的表现力和稳定性。
- en: '**Natural-language models** are a type of machine-learning model that can process
    and understand human language. These models are trained on large amounts of text
    data, such as books, articles, and social media posts, and learn to generate coherent
    and semantically meaningful responses to natural language input.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言模型**是一种能够处理和理解人类语言的机器学习模型。这些模型在大量文本数据上训练，如书籍、文章和社交媒体帖子，并学会对自然语言输入生成连贯且语义上有意义的响应。'
- en: One common type of natural-language model is the language model, which is trained
    to predict the probability of a sequence of words given a context. For example,
    a language model might be trained to predict the probability of the word “pizza,”
    given the context “I am in the mood for something to eat that is typically round
    and covered in toppings.”
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的自然语言模型是语言模型，它被训练来预测给定上下文中单词序列的概率。例如，一个语言模型可能被训练来预测在“我想要吃一些通常是圆形并覆盖着配料的东西”的上下文中，“披萨”这个词的概率。
- en: Another type of natural-language model is the text-generation model, which can
    be used to generate natural-language text, such as news articles, stories, and
    chatbot responses. These models can be trained to generate text that is similar
    to a particular style or genre, or even to imitate the writing style of a particular
    author.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种自然语言模型是文本生成模型，它可以用来生成自然语言文本，如新闻文章、故事和聊天机器人响应。这些模型可以被训练生成与特定风格或类型相似或甚至模仿特定作者写作风格的文本。
- en: Natural-language models have a wide range of applications, including language
    translation, sentiment analysis, chatbots and virtual assistants, and text summarization.
    Recent advances in deep learning and natural language processing have led to the
    development of powerful language models such as GPT-3, which can generate coherent
    and human-like text on a wide range of topics. Qlik Insight Advisor is one example
    of a product that has integrated the natural-language model.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言模型有广泛的应用，包括语言翻译、情感分析、聊天机器人和虚拟助手以及文本摘要。深度学习和自然语言处理领域的最新进展导致了像GPT-3这样的强大语言模型的发展，这些模型可以在广泛的主题上生成连贯且类似人类的文本。Qlik
    Insight Advisor是集成了自然语言模型的产品之一。
- en: Summary
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have gained an overview of different machine-learning algorithms.
    We have discovered how different algorithms can be used to solve problems and
    how they function. We started this chapter by getting familiar with some of the
    most common regression algorithms and gained knowledge on how to use these in
    R and Python. We discovered how to utilize clustering, decision trees, and random
    forests with practical examples.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们概述了不同的机器学习算法。我们发现了不同的算法如何被用来解决问题以及它们是如何工作的。我们通过熟悉一些最常见的回归算法开始本章，并获得了在R和Python中使用这些算法的知识。我们通过实际例子发现了如何利用聚类、决策树和随机森林。
- en: In the later part of this chapter, we moved on to more complex algorithms and
    learned how different boosting algorithms, neural networks, and other advanced
    models function. These models are utilized in Qlik AutoML, and it’s important
    to know how each model is structured. After reading this chapter, you now have
    a basic understanding of the models and are prepared to utilize these with Qlik
    tools. We will use most of these algorithms in the later parts of this book.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后期部分，我们转向了更复杂的算法，并学习了不同的提升算法、神经网络以及其他高级模型是如何工作的。这些模型在Qlik AutoML中得到应用，了解每个模型的架构是很重要的。阅读完本章后，你现在对模型有了基本的了解，并准备好使用Qlik工具来利用这些模型。本书的后期部分我们将使用这些算法中的大部分。
- en: In the next chapter, we will focus on data literacy in a machine-learning context.
    Data literacy is a hot topic, and it is also an important concept in the world
    of machine learning. To be able to create a well-functioning model and interpret
    the results from it, we must be able to understand the data. This is where data
    literacy comes into play.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将关注机器学习环境下的数据素养。数据素养是一个热门话题，同时也是机器学习领域中的一个重要概念。为了能够创建一个运行良好的模型并解释其结果，我们必须能够理解数据。这正是数据素养发挥作用的地方。
