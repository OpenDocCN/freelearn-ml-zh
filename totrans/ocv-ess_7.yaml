- en: Chapter 7. What Is He Doing? Motion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will show you different techniques related to motion, as
    estimated from video frames. After a short introduction and definitions, we will
    show you how to read video frames captured from a camera. Then, we will tackle
    the all-important Optical Flow technique. In the third section, we will show you
    different functions that can be used for tracking. The Motion history and Background
    subtraction techniques are explained in the fourth and fifth sections, respectively.
    Finally, image alignment with the ECC method is explained. Every example has been
    developed and tested for the latest version of OpenCV in GitHub. Most of the functions
    can work in the previous versions equally, leading to some changes that will be
    discussed. Most of the functions introduced in this chapter are in the `video`
    module.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To test the latest source code available in GitHub, go to [https://github.com/itseez/opencv](https://github.com/itseez/opencv)
    and download the library code as a ZIP file. Then unzip it to a local folder and
    follow the same steps described in [Chapter 1](part0014_split_000.html#page "Chapter 1. Getting
    Started"), *Getting Started*, to compile and install the library.
  prefs: []
  type: TYPE_NORMAL
- en: Motion history
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Motion is a very important topic in Computer Vision. Once we detect and isolate
    an object or person of interest, we can extract valuable data such as positions,
    velocity, acceleration, and so on. This information can be used for action recognition,
    behavior pattern studies, video stabilization, augmented reality, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The Optical Flow technique is a pattern of an object's apparent motion. Surfaces
    and edges in a visual scene are caused by relative motion between an observer
    and scene or between the camera and the scene. The concept of the Optical Flow
    technique is central in Computer Vision and is associated with techniques/tasks
    such as motion detection, object segmentation, time-to-control information, focus
    of expansion calculations, luminance, motion compensated encoding, and stereo
    disparity measurement.
  prefs: []
  type: TYPE_NORMAL
- en: '**Video tracking** consists of locating a moving object (or multiple objects)
    over time using videos captured from a camera or file. The aim of video tracking
    is to associate target objects in consecutive video frames. It has a variety of
    uses, some of which are video editing, medical imaging, traffic control, augmented
    reality, video communication and compression, security and surveillance, and human-computer
    interaction.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Motion** templates were invented at the MIT Media Lab by Bobick and David
    in 1996\. The use of the motion templates is a simple yet robust technique that
    tracks general movement. OpenCV motion template functions only work with single
    channel images. A silhouette (or part of a silhouette) of an object is needed.
    These silhouettes can be obtained in different ways. For example, segmentation
    techniques can be used to detect the interest object and then perform tracking
    with motion templates. Another option is to use the Background subtraction technique
    to detect foreground objects and then track them. There are other techniques too,
    although, in this chapter, we will see two examples that use the Background subtraction
    technique.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Background subtraction** is a technique by which an image foreground or region
    of interest is extracted for further processing, for example, people, cars, text,
    and so on. The Background subtraction technique is a widely used approach for
    detecting moving objects in videos captured from static cameras. The essence of
    the Background subtraction technique is to detect the moving objects from differences
    between current frames and a reference image taken without target objects present,
    which is usually called a background image.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image alignment** can be seen as a mapping between the coordinate systems
    of two or more images taken from different points of view. The first step is,
    therefore, the choice of an appropriate geometric transformation that adequately
    models this mapping. This algorithm can be used in a wide range of applications,
    such as image registration, object tracking, super-resolution, and visual surveillance
    by moving cameras.'
  prefs: []
  type: TYPE_NORMAL
- en: Reading video sequences
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To process a video sequence, we should be able to read each frame. OpenCV has
    developed an easy-to-use framework that can work with video files and camera input.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code is a `videoCamera` example that works with a video captured
    from a video camera. This example is a modification of an example in [Chapter
    1](part0014_split_000.html#page "Chapter 1. Getting Started"), *Getting Started*,
    and we will use it as the basic structure for other examples in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code example creates a window that shows you the grayscale video's
    camera capture. To initiate the capture, an instance of the `VideoCapture` class
    has been created with the zero-based camera index. Then, we check whether the
    video capture can be successfully initiated. Each frame is then read from the
    video sequence using the `read` method. This video sequence is converted to grayscale
    using the `cvtColor` method with the `COLOR_BGR2GRAY` parameter and is displayed
    on the screen until the user presses the *Esc* key. Then, the video sequence is
    finally released. The previous frame is stored because it will be used for some
    examples that follow.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `COLOR_BGR2GRAY` parameter can be used in OpenCV 3.0\. In the previous versions,
    we also had `CV_BGR2GRAY`.
  prefs: []
  type: TYPE_NORMAL
- en: In the summary, we have shown you a simple method that works with video sequences
    using a video camera. Most importantly, we have learned how to access each video
    frame and can now make any type of frame processing.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With regard to video and audio formats supported by OpenCV, more information
    can be found at the [ffmpeg.org](http://ffmpeg.org) website, which presents a
    complete open source and cross-platform solution for audio and video reading,
    recording, converting, and streaming. The OpenCV classes that work with video
    files are built on top of this library. The [Xvid.org](http://Xvid.org) website
    offers you an open source video codec library based on the MPEG-4 standard for
    video compression. This codec library has a competitor called DivX, which offers
    you proprietary but free codec and software tools.
  prefs: []
  type: TYPE_NORMAL
- en: The Lucas-Kanade optical flow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **Lucas-Kanade** (**LK**) algorithm was originally proposed in 1981, and
    it has become one of the most successful methods available in Computer Vision.
    Currently, this method is typically applied to a subset of key points in the input
    image. This method assumes that optical flow is a necessary constant in a local
    neighborhood of the pixel that is under consideration and solves the basic Optical
    Flow technique equations you can see equation (1), for each pixel (x, y) on that
    neighborhood. The method also assumes that displacements between two consecutive
    frames are small and are approximately a way to get an over-constrained system
    of the considered points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now focus on the **Pyramidal Lucas-Kanade** method, which estimates
    the optical flow in a pyramid using the `calcOpticalFlowPyrLK()` function. This
    method first estimates the optical flow at the top of the pyramid, thus avoiding
    the problems caused by violations of our assumptions of small and coherent motion.
    The motion estimate from this first level is then used as the starting point to
    estimate motion at the next level, as shown in the pyramid in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Lucas-Kanade optical flow](img/00047.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Pyramidal Lucas-Kanade
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example uses the `maxMovementLK` function to implement a motion
    detector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding example shows you a window with each movement. If there is a
    large movement, a message is displayed on the screen. Firstly, we need to obtain
    a set of appropriate key points in the image on which we can estimate the optical
    flow. The `goodFeaturesToTrack()` function uses the method that was originally
    proposed by Shi and Tomasi to solve this problem in a reliable way, although you
    can also use other functions to detect important and easy-to-track features (see
    [Chapter 5](part0042_split_000.html#page "Chapter 5. Focusing on the Interesting
    2D Features"), *Focusing on the Interesting 2D Features*). `MAX_FEATURES` is set
    to `500` to limit the number of key points. The Lucas-Kanade method parameters
    are then set and `calcOpticalFlowPyrLK()` is called. When the function returns,
    the status (`status`) array is checked to see which points were successfully tracked
    and that the new set of points (`new_features`) with the estimated positions is
    used. Lines are drawn to represent the motion, and if there is a displacement
    greater than `MAX_MOVEMENT`—for example—100, a message is shown on the screen.
    We can see two screen captures, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Lucas-Kanade optical flow](img/00048.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Output of the maxMovementLK example
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the modified `videoCamera` example, we have applied the `maxMovementLK()`
    function to detect large movements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This method is computationally efficient because tracking is only performed
    on important or interesting points.
  prefs: []
  type: TYPE_NORMAL
- en: The Gunnar-Farneback optical flow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Gunnar-Farneback** algorithm was developed to produce dense Optical Flow
    technique results (that is, on a dense grid of points). The first step is to approximate
    each neighborhood of both frames by quadratic polynomials. Afterwards, considering
    these quadratic polynomials, a new signal is constructed by a global displacement.
    Finally, this global displacement is calculated by equating the coefficients in
    the quadratic polynomials' yields.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now see the implementation of this method, which uses the `calcOpticalFlowFarneback()`function.
    The following is an example (`maxMovementFarneback`) that uses this function to
    detect the maximum movement as shown in the previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This function receives two consecutive frames, estimates the optical flow with
    different parameters, and returns an array with the same size as the input frame,
    where each pixel is actually a point (`Point2f`) that represents the displacement
    for that pixel. Firstly, different parameters are set for this function. Of course,
    you can also use your own criteria to configure the performance. Then, with these
    parameters, the Optical Flow technique is performed between each two consecutive
    frames. Consequently, we obtain an array with the estimations for each pixel,
    which is `optical_flow`. Finally, the movements that are greater than `MIN_MOVEMENT`
    are displayed on the screen. If the largest movement is greater than `MAX_MOVEMENT`,
    then an `INTRUDER` message is displayed.
  prefs: []
  type: TYPE_NORMAL
- en: Understandably, this method is quite slow because the Optical Flow technique
    is computed over each pixel on the frame. The output of this algorithm is similar
    to the previous method, although it's much slower.
  prefs: []
  type: TYPE_NORMAL
- en: The Mean-Shift tracker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Mean-Shift** method allows you to locate the maximum of a density function
    given discrete data sampled from that function. It is, therefore, useful for detecting
    the modes of this density. Mean-Shift is an iterative method, and an initial estimation
    is needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm can be used for visual tracking. In this case, the color histogram
    of the tracked object is used to compute the confidence map. The simplest of such
    algorithm would create a confidence map in the new image based on the object histogram
    taken from the previous image, and Mean-Shift is used to find the peak of the
    confidence map near the object''s previous position. The confidence map is a probability
    density function on the new image, assigning each pixel of the new image a probability,
    which is the probability of the pixel color occurring in the object in the previous
    image. Next, we show you an example (`trackingMeanShift`) using this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This example shows you a window with an initial centered rectangle where the
    tracking is performed. First, the criteria parameter is set. The function that
    implements the method needs three parameters: the main image, the interest area
    that we want to search, and the term criteria for different modes of tracking.
    Finally, a rectangle is obtained from `meanShift()`, and `search_window` is drawn
    on the main image.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a modified `videoCamera` example, we apply this method for tracking.
    A static window of the screen is used to search. Of course, you can manually adjust
    another window or use other functions to detect interest objects and then perform
    the tracking on them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can see the following two screen captures:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Mean-Shift tracker](img/00049.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Output of the trackingMeanShift example
  prefs: []
  type: TYPE_NORMAL
- en: The CamShift tracker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `CamShift` (**Continuously Adaptive Mean Shift**) algorithm is an image
    segmentation method that was introduced by Gary Bradski of OpenCV fame in 1998\.
    It differs from `MeanShift` in that a search window adjusts itself in size. If
    we have a well-segmented distribution (for example, face features that stay compact),
    this method will automatically adjust itself to the face sizes as the person moves
    closer or farther from the camera.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can find a `CamShift` reference at [http://docs.opencv.org/trunk/doc/py_tutorials/py_video/py_meanshift/py_meanshift.html](http://docs.opencv.org/trunk/doc/py_tutorials/py_video/py_meanshift/py_meanshift.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now see the following example (`trackingCamShift`) using this method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This function structure is very similar to the one in the preceding section;
    the only difference is that a bounding rectangle is returned from `CamShift()`.
  prefs: []
  type: TYPE_NORMAL
- en: The Motion templates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Motion template is a technique in image processing for finding a small part
    of an image or silhouette that matches a template image. This template matcher
    is used to make comparisons with respect to similarity and to examine the likeness
    or difference. Templates might potentially require sampling of a large number
    of points. However, it is possible to reduce these numbers of points by reducing
    the resolution of the search; another technique to improve these templates is
    to use pyramid images.
  prefs: []
  type: TYPE_NORMAL
- en: In OpenCV's examples (`[opencv_source_code]/samples/c/motempl.c`), a related
    program can be found.
  prefs: []
  type: TYPE_NORMAL
- en: The Motion history template
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We now assume that we have a good silhouette or template. New silhouettes are
    then captured and overlaid using the current time stamp as the weight. These sequentially
    fading silhouettes record the history of the previous movement and are thus referred
    to as the Motion history template. Silhouettes whose time stamp is more than a
    specified `DURATION` older than the current time stamp are set to zero. We have
    created a simple example (`motionHistory`) using the `updateMotionHistory()`OpenCV
    function on two frames as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `THRESH_BINARY` parameter can be used on OpenCV 3.0\. In the previous versions,
    we also had `CV_THRESH_BINARY`.
  prefs: []
  type: TYPE_NORMAL
- en: This example shows you a window where the motion history is drawn. The first
    step is to obtain a silhouette; the Background subtraction technique is used for
    this. The difference in the absolute value is obtained from the two input frames.
    In the second step, a binary thresholding is applied to remove noise from the
    silhouette. Then, the current time is obtained. The final step is to perform the
    updating of the Motion history template using OpenCV's function.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have also set `DURATION` to `5`. Note that it is necessary to initialize
    `INITIAL_TIME` and `history`. Besides, we can use this function call from the
    modified `videoCamera` example as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: To use the `clock()` function, which gets the current time, we need to include
    `<ctime>`. Some screen captures will be shown in which a person is walking in
    front of the camera.
  prefs: []
  type: TYPE_NORMAL
- en: '![The Motion history template](img/00050.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Output of the motionHistory example
  prefs: []
  type: TYPE_NORMAL
- en: The Motion gradient
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once the Motion templates have a collection of object silhouettes overlaid
    in time, we can obtain the directions of movement by computing the gradients of
    the `history` image. The following example (`motionGradient`) computes the gradients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'A screen capture is shown with a person moving his head in front of the camera
    (see the following screenshot). Each line represents the gradient for each pixel.
    Different frames also overlap at a `t` time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Motion gradient](img/00051.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Output of the motionGradient example (a person is moving his head in front of
    the camera).
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding example shows you a window that displays the directions of movement.
    As the first step, the parameters are set (the maximum and minimum gradient value
    to be detected). The second step uses the `calcMotionGradient()` function to obtain
    a matrix of the gradient direction angles. Finally, to show the results, these
    angles are drawn on the screen using a default distance, which is `dist`. Again,
    we can use this function from the following modified `videoCamera` example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The Background subtraction technique
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Background subtraction technique consists of obtaining the important objects
    over a background.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see the methods available in OpenCV for the Background subtraction
    technique. Currently, the following four important techniques are required for
    this task:'
  prefs: []
  type: TYPE_NORMAL
- en: '**MOG** (**Mixture-of-Gaussian**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MOG2**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GMG** (**Geometric MultiGrip**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**KNN** (**K-Nearest Neighbors**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we are going to see an example (`backgroundSubKNN`) using the KNN technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `createBackgroundSubtractorKNN` method has only been included in Version
    3.0 of OpenCV.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Background subtracted frame and screen capture are shown in the following
    screenshot in which a person is walking in front of the camera:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Background subtraction technique](img/00052.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Output of the backgroundSubKNN example
  prefs: []
  type: TYPE_NORMAL
- en: The preceding example shows you two windows with the subtracted background images
    and draws contours of the person found. First, parameters are set as the distance
    threshold between background and each frame to detect objects (`dist2Threshol`)
    and the disabling of the shadow detection (`detectShadows`). In the second step,
    using the `createBackgroundSubtractorKNN()` function, a background subtractor
    is created and a smart pointer construct is used (`Ptr<>`) so that we will not
    have to release it. The third step is to read each frame, if possible. Using the
    `apply()` and `getBackgroundImage()` methods, the foreground and background images
    are obtained. The fifth step is to reduce the foreground noise by applying a morphological
    closing operation (in the erosion—`erode()`—and dilation—`dilate()`—order). Then,
    contours are detected on the foreground image and then they're drawn. Finally,
    the background and current frame image are shown.
  prefs: []
  type: TYPE_NORMAL
- en: Image alignment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenCV now implements the ECC algorithm, which is only available as of Version
    3.0\. This method estimates the geometric transformation (warp) between the input
    and template frames and returns the warped input frame, which must be close to
    the first template. The estimated transformation is the one that maximizes the
    correlation coefficient between the template and the warped input frame. In the
    OpenCV examples (`[opencv_source_code]/samples/cpp/image_alignment.cpp`), a related
    program can be found.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The ECC algorithm is based on the ECC criterion of the paper *Parametric Image
    Alignment Using Enhanced Correlation Coefficient Maximization*. You can find this
    at [http://xanthippi.ceid.upatras.gr/people/evangelidis/george_files/PAMI_2008.pdf](http://xanthippi.ceid.upatras.gr/people/evangelidis/george_files/PAMI_2008.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now going to see an example (`findCameraMovement`) that uses this ECC
    technique using the `findTransformECC()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Some screen captures are shown in the following screenshot. The left-column
    frames represent the initial and template frames. The upper-right image is the
    current frame and the lower-right image is the warped frame.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image alignment](img/00053.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Output of the findCameraMovement example.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code example shows you four windows: the initial template, the initial
    frame, the current frame, and the warped frame. The first step is to set the initial
    parameters as `warp_mode` (`MOTION_HOMOGRAPHY`). The second step is to check whether
    the video camera is opened and to obtain a template, which will be calculated
    using a centered rectangle. When the *C* key is pressed, this area will be captured
    as the template. The third step is to read the next frame and convert it to a
    gray frame. The `findTransformECC()` function is applied to calculate `warp_matrix`
    with this matrix, and using `warpPerspective()`, the camera movement can be corrected
    using `warped_frame`.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter covered an important subject in Computer Vision. Motion detection
    is an essential task, and in this chapter, we have provided the reader with the
    insight and samples that are required for the most useful methods available in
    OpenCV: working with video sequences (see the `videoCamera` example), the Optical
    Flow technique (see the `maxMovementLK` and `maxMovementFarneback` examples),
    tracking (see the `trackingMeanShift` and `trackingCamShift` examples), the Motion
    templates (see the `motionHistory` and `motionGradient` examples), the Background
    subtraction technique (see the `backgroundSubKNN` example), and image alignment
    (see the `findCameraMovement` example).'
  prefs: []
  type: TYPE_NORMAL
- en: What else?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Within the OpenCV libraries, there are other functions that deal with motion.
    Other Optical Flow technique methods are implemented, such as the Horn and Schunk
    (`cvCalcOpticalFlowHS`), block machine (`cvCalcOpticalFlowBM`), and simple flow
    (`calcOpticalFlowSF`) methods. A method to estimate the global movement is also
    available (`calcGlobalOrientation`). Finally, there are other methods to obtain
    backgrounds such as MOG (`createBackgroundSubtractorMOG`), MOG2 (`createBackgroundSubtractorMOG2`),
    and GMG (`createBackgroundSubtractorGMG`) methods.
  prefs: []
  type: TYPE_NORMAL
