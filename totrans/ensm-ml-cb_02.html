<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Getting Started with Ensemble Machine Learning</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft3">In this<span class="calibre5"> c</span>hapter<span class="calibre5">,</span><span class="calibre5"> </span>we'll cover the following recipes:</p>
<ul class="calibre10">
<li class="calibre11">Max-voting</li>
<li class="calibre11">Averaging</li>
<li class="calibre11">Weighted averaging</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to ensemble machine learning</h1>
                </header>
            
            <article>
                
<p class="calibre2">Simply speaking, ensemble machine learning refers to a technique that integrates output from multiple learners and is applied to a dataset to make a prediction. These multiple learners are usually referred to as base learners. When multiple base models are used to extract predictions that are combined into one single prediction, that prediction is likely to provide better accuracy than individual base learners.</p>
<p class="calibre2">Ensemble models are known for providing an advantage over single models in terms of performance. They can be applied to both regression and classification problems. You can either decide to build ensemble models with algorithms from the same family or opt to pick them from different families. If multiple models are built on the same dataset using neural networks only, then that ensemble would be called a <strong class="calibre4">homogeneous ensemble model</strong>. If multiple models are built using different algorithms, such as <strong class="calibre4">support vector machines</strong> (<strong class="calibre4">SVMs</strong>), neural networks, and random forests, then the ensemble model would be called a <strong class="calibre4">heterogeneous ensemble model</strong>.</p>
<p class="calibre2">The construction of an ensemble model requires two steps:</p>
<ol class="calibre14">
<li class="calibre11">Base learners are learners that are designed and fit on training data</li>
<li class="calibre11">The base learners are combined to form a single prediction model by using specific ensembling techniques such as max-voting, averaging, and weighted averaging</li>
</ol>
<p class="CDPAlignLeft3">The following diagram shows the structure of the ensemble model:   </p>
<p class="CDPAlignCenter"><img class="aligncenter23" src="assets/f805ffc2-b5f2-44a0-a330-a1e09b822eac.png"/></p>
<p class="CDPAlignLeft3">However, to get an ensemble model that performs well, the base learners themselves should be as accurate as possible. A common way to measure the performance of a model is to evaluate its generalization error. A generalization error is a term to measure how accurately a model is able to make a prediction, based on a new dataset that the model hasn't seen.</p>
<p class="calibre2"><span class="calibre5">To perform well, the ensemble models require a sufficient amount of data. Ensemble techniques prove to be more useful when you have large and non-linear datasets.</span></p>
<div class="packtinfobox">An ensemble model may overfit if too many models are included, although this isn't very common.</div>
<p class="calibre2">Irrespective of how well you fine-tune your models, there's always the risk of high bias or high variance. Even the best model can fail if the bias and variance aren't taken into account while training the model. Both bias and variance represent a kind of error in the predictions. In fact, the total error is comprised of bias-related error, variance-related error, and unavoidable noise-related error (or irreducible error). The noise-related error is mainly due to noise in the training data and can't be removed. However, the errors due to bias and variance can be reduced. </p>
<p class="calibre2">The total error can be expressed as follows: </p>
<pre class="calibre15">Total Error = Bias ^ 2 + Variance + Irreducible Error</pre>
<p class="calibre2">A measure such as <strong class="calibre4">mean square error</strong> (<strong class="calibre4">MSE</strong>) captures all of these errors for a continuous target variable and can be represented as follows:</p>
<p class="CDPAlignCenter"><img class="fm-editor-equation" src="assets/89f95d7f-2f1e-4f9e-b2c4-3eda6906ffb2.png"/></p>
<p class="calibre2">In this formula, <em class="calibre13">E</em> stands for the expected mean, <em class="calibre13">Y</em> <span class="calibre5">represents the actual target values and </span><img class="fm-editor-equation1" src="assets/9e4b0739-22af-4cc1-816a-a36dd771abf2.png"/> is the predicted values for the target variable. It can be broken down into its components such as bias, variance and noise as shown in the following formula:</p>
<p class="CDPAlignCenter"><img class="aligncenter24" src="assets/21b41a6e-ac17-4354-ae2b-daea48ef913e.png"/></p>
<p class="calibre2"><span class="calibre5">While bias refers to how close is the ground truth to the expected value of our estimate, the variance, on the other hand, measures the deviation from the expected estimator value. Estimators with small MSE is what is desirable. In order to minimize the MSE error, we would like to be centered (0-bias) at ground truth and have a low deviation (low variance) from the ground truth (correct) value. In other words, we'd like to be confident (low variance, low uncertainty, more peaked distribution) about the value of our estimate. High bias degrades the performance of the algorithm on the training dataset and leads to underfitting. </span><span class="calibre5">High variance, on the other hand, is characterized by low training errors and high validation errors. Having high variance reduces the performance of the learners on unseen data, leading to overfitting.</span></p>
<div class="packtinfobox"><span>Ensemble models can reduce bias and/or variance in the models.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Max-voting</h1>
                </header>
            
            <article>
                
<p class="calibre2">Max-voting, which is generally used for classification problems, is one of the simplest ways of combining predictions from multiple machine learning algorithms. </p>
<p class="calibre2">In max-voting, each base model makes a prediction and votes for each sample. Only the sample class with the highest votes is included in the final predictive class.</p>
<p class="calibre2">For example, let's say we have an online survey, in which consumers answer a question in a five-level Likert scale. We can assume that a few consumers will provide a rating of five, while others will provide a rating of four, and so on. If a majority, say more than 50% of the consumers, provide a rating of four, then the final rating is taken as four. In this example, taking the final rating as four is similar to taking a mode for all of the ratings.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="calibre2">In the following steps we will download the following packages:</p>
<p class="calibre2">To start with, import the <kbd class="calibre12">os</kbd> and <kbd class="calibre12">pandas</kbd> packages and set your working directory according to your requirements:</p>
<pre class="calibre18"># import required packages<br class="title-page-name"/>import os<br class="title-page-name"/>import pandas as pd<br class="title-page-name"/><br class="title-page-name"/># Set working directory as per your need<br class="title-page-name"/>os.chdir(".../.../Chapter 2")<br class="title-page-name"/>os.getcwd()</pre>
<p class="calibre2"><span class="calibre5">Download the <kbd class="calibre12">Cryotherapy.csv</kbd> dataset from GitHub and copy it to your working directory. Read the dataset:</span></p>
<pre class="calibre18">df_cryotherapydata = pd.read_csv("Cryotherapy.csv")</pre>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">Take a look at the data with the following code:</p>
<pre class="calibre18">df_cryotherapydata.head(5)</pre>
<p class="calibre2">We can see that the data has been read properly and has the <kbd class="calibre12">Result_of_Treatment</kbd> class variable. We then move on to creating models with <kbd class="calibre12">Result_of_Treatment</kbd> as the response variable.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="calibre25">You can create a voting ensemble model for a classification problem using the <kbd class="calibre12">VotingClassifier</kbd> class from Python's <kbd class="calibre12">scikit-learn</kbd> library. The following steps showcase an example of how to combine the predictions of the decision tree, SVMs, and logistic regression models for a classification problem:</p>
<ol start="1" class="calibre14">
<li class="calibre11">Import the required libraries for building the decision tree, SVM, and logistic regression models. We also import <kbd class="calibre12">VotingClassifier</kbd> for max-voting:</li>
</ol>
<pre class="calibre26"># Import required libraries<br class="title-page-name"/>from sklearn.tree import DecisionTreeClassifier<br class="title-page-name"/>from sklearn.svm import SVC<br class="title-page-name"/>from sklearn.linear_model import LogisticRegression<br class="title-page-name"/>from sklearn.ensemble import VotingClassifier</pre>
<ol start="2" class="calibre14">
<li class="calibre11">We then move on to building our feature set and creating our train and test datasets:</li>
</ol>
<pre class="calibre26"># We create train &amp; test sample from our dataset<br class="title-page-name"/>from sklearn.cross_validation import train_test_split<br class="title-page-name"/><br class="title-page-name"/># create feature &amp; response sets<br class="title-page-name"/>feature_columns = ['sex', 'age', 'Time', 'Number_of_Warts', 'Type', 'Area']<br class="title-page-name"/>X = df_cryotherapydata[feature_columns]<br class="title-page-name"/>Y = df_cryotherapydata['Result_of_Treatment']<br class="title-page-name"/><br class="title-page-name"/># Create train &amp; test sets<br class="title-page-name"/>X_train, X_test, Y_train, Y_test = \<br class="title-page-name"/>train_test_split(X, Y, test_size=0.20, random_state=1)</pre>
<p class="calibre2"/>
<p class="calibre2"/>
<ol start="3" class="calibre14">
<li class="calibre11">We build our models with the decision tree, SVM, and logistic regression algorithms:</li>
</ol>
<pre class="calibre26"># create the sub models<br class="title-page-name"/>estimators = []<br class="title-page-name"/><br class="title-page-name"/>dt_model = DecisionTreeClassifier(random_state=1)<br class="title-page-name"/>estimators.append(('DecisionTree', dt_model))<br class="title-page-name"/><br class="title-page-name"/>svm_model = SVC(random_state=1)<br class="title-page-name"/>estimators.append(('SupportVector', svm_model))<br class="title-page-name"/><br class="title-page-name"/>logit_model = LogisticRegression(random_state=1)<br class="title-page-name"/>estimators.append(('Logistic Regression', logit_model))</pre>
<ol start="4" class="calibre14">
<li class="calibre11">We build individual models with each of the classifiers we've chosen:</li>
</ol>
<pre class="calibre26">from sklearn.metrics import accuracy_score<br class="title-page-name"/><br class="title-page-name"/>for each_estimator in (dt_model, svm_model, logit_model):<br class="title-page-name"/>    each_estimator.fit(X_train, Y_train)<br class="title-page-name"/>    Y_pred = each_estimator.predict(X_test)<br class="title-page-name"/>    print(each_estimator.__class__.__name__, accuracy_score(Y_test, Y_pred))</pre>
<p class="calibre20">We can then see the accuracy score of each of the individual base learners:</p>
<p class="CDPAlignCenter"><img class="aligncenter25" src="assets/8525dd2f-69c0-494d-b766-f40177dc16d2.png"/></p>
<ol start="5" class="calibre14">
<li class="calibre11">We proceed to ensemble our models and use <kbd class="calibre12">VotingClassifier</kbd> to score the accuracy of the ensemble model:</li>
</ol>
<pre class="calibre27">#Using VotingClassifier() to build ensemble model with Hard Voting<br class="title-page-name"/>ensemble_model = VotingClassifier(estimators=estimators, voting='hard')<br class="title-page-name"/><br class="title-page-name"/>ensemble_model.fit(X_train,Y_train)<br class="title-page-name"/>predicted_labels = ensemble_model.predict(X_test) <br class="title-page-name"/><br class="title-page-name"/>print("Classifier Accuracy using Hard Voting: ", accuracy_score(Y_test, predicted_labels))</pre>
<p class="calibre20">We can see the accuracy score of the ensemble model using <kbd class="calibre12">Hard Voting</kbd>:</p>
<p class="CDPAlignCenter"><img class="aligncenter26" src="assets/7cfc6f63-ea79-42d0-93f7-ae8601618f96.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="calibre25"><kbd class="calibre12">VotingClassifier</kbd> implements two types of voting—<strong class="calibre4">hard</strong> and <strong class="calibre4">soft</strong> voting. In hard voting, the final class label is predicted as the class label that has been predicted most frequently by the classification models. In other words, the predictions from all classifiers are aggregated to predict the class that gets the most votes. In simple terms, it takes the mode of the predicted class labels. </p>
<p class="calibre25">In hard voting for the class labels, <img class="fm-editor-equation2" src="assets/cdcf3a61-7859-4677-ae40-0ad28f93b20b.png"/> is the prediction based on the majority voting of each classifier <img class="fm-editor-equation3" src="assets/14f699f6-50a7-48d2-9954-00f6f3b93265.png"/>, where <em class="calibre13">i=1.....n</em> observations, we have the following:</p>
<p class="CDPAlignCenter"><img class="fm-editor-equation4" src="assets/06173c0a-41c4-4350-b2ec-d5bcfa423a2d.png"/></p>
<p class="calibre25">As shown in the previous section, we have three models, one from the decision tree, one from the SVMs, and one from logistic regression. Let's say that the models classify a training observation as class 1, class 0, and class 1 respectively. Then with majority voting, we have the following:</p>
<p class="CDPAlignCenter"><img class="fm-editor-equation5" src="assets/9957aac7-8217-431a-8822-5a4e87bea84c.png"/></p>
<p class="calibre25">In this case, we would classify the observation as class 1.</p>
<p class="calibre25">In the preceding section, in<em class="calibre13"> Step 1</em>, we imported the required libraries to build our models. In <em class="calibre13">Step 2</em>, we created our feature set. We also split our data to create the training and testing samples. In <em class="calibre13">Step 3</em>, we trained three models with the decision tree, SVMs, and logistic regression respectively. In <em class="calibre13">Step 4</em>, we looked at the accuracy score of each of the base learners, while in <em class="calibre13">Step 5</em>, we ensembled the models using <kbd class="calibre12">VotingClassifier()</kbd> and looked at the accuracy score of the ensemble model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="calibre25">Many classifiers can estimate class probabilities. In this case, the class labels are predicted by averaging the class probabilities. This is called <strong class="calibre4">soft voting</strong> and is recommended for an ensemble of well-tuned classifiers.</p>
<p class="calibre2"/>
<p class="calibre25">In the <kbd class="calibre12">scikit-learn</kbd> library, many classification algorithms have the <kbd class="calibre12">predict_proba()</kbd> method to predict the class probabilities. To perform the ensemble with soft voting, simply replace <kbd class="calibre12">voting='hard'</kbd> with <kbd class="calibre12">voting='soft'</kbd> in <kbd class="calibre12">VotingClassifier()</kbd>.</p>
<p class="calibre25">The following code creates an ensemble using soft voting:</p>
<pre class="calibre28"># create the sub models<br class="title-page-name"/>estimators = []<br class="title-page-name"/><br class="title-page-name"/>dt_model = DecisionTreeClassifier(random_state=1)<br class="title-page-name"/>estimators.append(('DecisionTree', dt_model))<br class="title-page-name"/><br class="title-page-name"/>svm_model = SVC(random_state=1, probability=True)<br class="title-page-name"/>estimators.append(('SupportVector', svm_model))<br class="title-page-name"/><br class="title-page-name"/>logit_model = LogisticRegression(random_state=1)<br class="title-page-name"/>estimators.append(('Logistic Regression', logit_model))<br class="title-page-name"/><br class="title-page-name"/>for each_estimator in (dt_model, svm_model, logit_model):<br class="title-page-name"/>    each_estimator.fit(X_train, Y_train)<br class="title-page-name"/>    Y_pred = each_estimator.predict(X_test)<br class="title-page-name"/>    print(each_estimator.__class__.__name__, accuracy_score(Y_test, Y_pred))<br class="title-page-name"/><br class="title-page-name"/># Using VotingClassifier() to build ensemble model with Soft Voting<br class="title-page-name"/>ensemble_model = VotingClassifier(estimators=estimators, voting='soft')<br class="title-page-name"/>ensemble_model.fit(X_train,Y_train)<br class="title-page-name"/>predicted_labels = ensemble_model.predict(X_test) <br class="title-page-name"/>print("Classifier Accuracy using Soft Voting: ", accuracy_score(Y_test, predicted_labels))</pre>
<p class="calibre2">We get to see the accuracy from individual learners and the ensemble learner using soft voting:</p>
<p class="CDPAlignCenter"><img class="aligncenter27" src="assets/e5c04d3c-702b-4e19-9e34-631718d46d9e.png"/></p>
<div class="packttip">The <kbd class="calibre19">SVC</kbd> class can't estimate class probabilities by default, so we've set its probability hyper-parameter to <kbd class="calibre19">True</kbd> in the preceding code. With <kbd class="calibre19">probability=True</kbd>, <kbd class="calibre19">SVC</kbd> will be able to estimate class probabilities.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Averaging</h1>
                </header>
            
            <article>
                
<p class="calibre25">Averaging is usually used for regression problems or can be used while estimating the probabilities in classification tasks. Predictions are extracted from multiple models and an average of the predictions are used to make the final prediction. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="calibre2">Let us get ready to build multiple learners and see how to implement averaging:</p>
<p class="calibre2">Download the <kbd class="calibre12">whitewines.csv</kbd> <span class="calibre5">dataset </span>from GitHub and copy it to your working directory, and let's read the dataset:</p>
<pre class="calibre26">df_winedata = pd.read_csv("whitewines.csv")</pre>
<p class="calibre2">Let's take a look at the data with the following code:</p>
<pre class="calibre26">df_winedata.head(5)</pre>
<p class="calibre20">In the following screenshot, we can see that the data has been read properly:</p>
<p class="CDPAlignCenter"><img class="aligncenter28" src="assets/f3e9bfa0-d5c9-4605-b4bd-133db9ca6cc4.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="calibre25">We have a dataset that is based on the properties of wines. Using this dataset, we'll build multiple regression models with the quality as our response variable. With multiple learners, we extract multiple predictions. The averaging technique would take the average of all of the predicted values for each training sample:</p>
<ol class="calibre14">
<li class="calibre11">Import the required libraries:</li>
</ol>
<pre class="calibre26"># Import required libraries<br class="title-page-name"/>from sklearn.linear_model import LinearRegression<br class="title-page-name"/>from sklearn.tree import DecisionTreeRegressor<br class="title-page-name"/>from sklearn.svm import SVR</pre>
<p class="calibre2"/>
<p class="calibre2"/>
<ol start="2" class="calibre14">
<li class="calibre11">Create the response and feature sets:</li>
</ol>
<pre class="calibre26"># Create feature and response variable set<br class="title-page-name"/>from sklearn.cross_validation import train_test_split<br class="title-page-name"/><br class="title-page-name"/># create feature &amp; response variables<br class="title-page-name"/>feature_columns = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar','chlorides', 'free sulfur dioxide', 'total sulfur dioxide','density', 'pH', 'sulphates', 'alcohol']<br class="title-page-name"/>X = df_winedata[feature_columns]<br class="title-page-name"/>Y = df_winedata['quality']</pre>
<ol start="3" class="calibre14">
<li class="calibre11">Split the data into training and testing sets:</li>
</ol>
<pre class="calibre26"># Create train &amp; test sets<br class="title-page-name"/>X_train, X_test, Y_train, Y_test = \<br class="title-page-name"/>train_test_split(X, Y, test_size=0.20, random_state=1)</pre>
<ol start="4" class="calibre14">
<li class="calibre11">Build the base regression learners using linear regression, <kbd class="calibre12">SVR</kbd>, and a decision tree:</li>
</ol>
<pre class="calibre26"># Build base learners<br class="title-page-name"/>linreg_model = LinearRegression()<br class="title-page-name"/>svr_model = SVR()<br class="title-page-name"/>regressiontree_model = DecisionTreeRegressor()<br class="title-page-name"/><br class="title-page-name"/># Fitting the model<br class="title-page-name"/>linreg_model.fit(X_train, Y_train)<br class="title-page-name"/>svr_model.fit(X_train, Y_train)<br class="title-page-name"/>regressiontree_model.fit(X_train, Y_train)</pre>
<ol start="5" class="calibre14">
<li class="calibre11">Use the base learners to make a prediction based on the test data:</li>
</ol>
<pre class="calibre26">linreg_predictions = linreg_model.predict(X_test)<br class="title-page-name"/>svr_predictions = svr_model.predict(X_test)<br class="title-page-name"/>regtree_predictions = regressiontree_model.predict(X_test)</pre>
<ol start="6" class="calibre14">
<li class="calibre11">Add the predictions and divide by the number of base learners:</li>
</ol>
<pre class="calibre26"># We divide the summation of the predictions by 3 i.e. number of base learners <br class="title-page-name"/>average_predictions=(linreg_predictions + svr_predictions + regtree_predictions)/3</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="calibre25">In <em class="calibre13">Step 1</em>, we imported the required packages. In <em class="calibre13">Step 2</em>, we separated the feature set and the response variable from our dataset. We split our dataset into training and testing samples in <em class="calibre13">Step 3</em>.</p>
<p class="calibre25">Note that our response variable is continuous in nature. For this reason, we built our regression base learners in <em class="calibre13">Step 4</em> using linear regression, <kbd class="calibre12">SVR</kbd>, and a decision tree. In <em class="calibre13">Step 5</em>, we passed our test dataset to the <kbd class="calibre12">predict()</kbd> function to predict our response variable. And finally, in <em class="calibre13">Step 6</em>, we added all of the predictions together and divided them by the number of base learners, which is three in our example.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Weighted averaging</h1>
                </header>
            
            <article>
                
<p class="calibre25">Like averaging, weighted averaging is also used for regression tasks. Alternatively, it can be used while estimating probabilities in classification problems. Base learners are assigned different weights, which represent the importance of each model in the prediction.</p>
<div class="packtinfobox">A weight-averaged model should always be at least as good as your best model.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="calibre25"><span class="calibre5">Download the <kbd class="calibre12">wisc_bc_data.csv</kbd> dataset </span><span class="calibre5">from GitHub and copy it to your working directory. Let's read the dataset:</span></p>
<pre class="calibre28">df_cancerdata = pd.read_csv("wisc_bc_data.csv")</pre>
<p class="calibre25">Take a look at the data with the following code:</p>
<pre class="calibre28">df_cancerdata.head(5)</pre>
<p class="calibre2"/>
<p class="calibre25">We can see that the data has been read properly:</p>
<p class="CDPAlignCenter"><img class="aligncenter29" src="assets/2cc3ae34-687d-4175-a392-89311304bed7.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="calibre25">Here, we have a dataset based on the properties of cancerous tumors. Using this dataset, we'll build multiple classification models with <kbd class="calibre12">diagnosis</kbd> as our response variable. The diagnosis variable has the values, <kbd class="calibre12">B</kbd> and <kbd class="calibre12">M</kbd>, which indicate whether the tumor is benign or malignant. With multiple learners, we extract multiple predictions. The weighted averaging technique takes the average of all of the predicted values for each training sample.</p>
<p class="calibre25">In this example, we consider the predicted probabilities as the output and use the <kbd class="calibre12">predict_proba()</kbd> function of the scikit-learn algorithms to predict the class probabilities:</p>
<ol class="calibre14">
<li class="calibre11">Import the required libraries:</li>
</ol>
<pre class="calibre26"># Import required libraries<br class="title-page-name"/>from sklearn.tree import DecisionTreeClassifier<br class="title-page-name"/>from sklearn.svm import SVC<br class="title-page-name"/>from sklearn.linear_model import LogisticRegression</pre>
<ol start="2" class="calibre14">
<li class="calibre11">Create the response and feature sets:</li>
</ol>
<pre class="calibre26"># Create feature and response variable set<br class="title-page-name"/># We create train &amp; test sample from our dataset<br class="title-page-name"/>from sklearn.cross_validation import train_test_split<br class="title-page-name"/><br class="title-page-name"/># create feature &amp; response variables<br class="title-page-name"/>X = df_cancerdata.iloc[:,2:32]<br class="title-page-name"/>Y = df_cancerdata['diagnosis']</pre>
<p class="calibre2"/>
<div class="packttip"><span>W</span>e retrieved the feature columns using the <kbd class="calibre19">iloc()</kbd> function of the <kbd class="calibre19">pandas</kbd> DataFrame, which is purely integer-location based indexing for selection by position. The <kbd class="calibre19">iloc()</kbd> function takes row and column selection as its parameter, in the form: <kbd class="calibre19">data.iloc(&lt;row selection&gt;, &lt;column selection&gt;)</kbd>. The row and column selection can either be an integer list or a slice of rows and columns. For example, it might look as follows: <kbd class="calibre19">df_cancerdata.iloc(2:100, 2:30)</kbd>.</div>
<ol start="3" class="calibre14">
<li class="calibre11">We'll then split our data into training and testing sets:</li>
</ol>
<pre class="calibre26"># Create train &amp; test sets<br class="title-page-name"/>X_train, X_test, Y_train, Y_test = \<br class="title-page-name"/>train_test_split(X, Y, test_size=0.20, random_state=1)</pre>
<ol start="4" class="calibre14"/>
<ol start="4" class="calibre14">
<li class="calibre11">Build the base classifier models:</li>
</ol>
<pre class="calibre26"># create the sub models<br class="title-page-name"/>estimators = []<br class="title-page-name"/><br class="title-page-name"/>dt_model = DecisionTreeClassifier()<br class="title-page-name"/>estimators.append(('DecisionTree', dt_model))<br class="title-page-name"/><br class="title-page-name"/>svm_model = SVC(probability=True)<br class="title-page-name"/>estimators.append(('SupportVector', svm_model))<br class="title-page-name"/><br class="title-page-name"/>logit_model = LogisticRegression()<br class="title-page-name"/>estimators.append(('Logistic Regression', logit_model))</pre>
<ol start="5" class="calibre14">
<li class="calibre11">Fit the models on the test data:</li>
</ol>
<pre class="calibre26">dt_model.fit(X_train, Y_train)<br class="title-page-name"/>svm_model.fit(X_train, Y_train)<br class="title-page-name"/>logit_model.fit(X_train, Y_train)</pre>
<ol start="6" class="calibre14">
<li class="calibre11">Use the <kbd class="calibre12">predict_proba()</kbd> function to predict the class probabilities:</li>
</ol>
<pre class="calibre26">dt_predictions = dt_model.predict_proba(X_test)<br class="title-page-name"/>svm_predictions = svm_model.predict_proba(X_test)<br class="title-page-name"/>logit_predictions = logit_model.predict_proba(X_test)</pre>
<ol start="7" class="calibre14">
<li class="calibre11">Assign different weights to each of the models to get our final predictions:</li>
</ol>
<pre class="calibre26">weighted_average_predictions=(dt_predictions * 0.3 + svm_predictions * 0.4 + logit_predictions * 0.3)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="calibre25">In <em class="calibre13">S<span class="calibre5">tep</span><span class="calibre5"> </span></em><em class="calibre13">1</em>, we imported the libraries that are required to build our models. In <em class="calibre13">Step 2</em>, we created the response and feature sets. We retrieved our feature set using the <kbd class="calibre12">iloc()</kbd> function of the <kbd class="calibre12">pandas</kbd> DataFrame. In <em class="calibre13">Step 3</em>, we split our dataset into training and testing sets. In <em class="calibre13">Step 4</em>, we built our base classifiers. Kindly note that we passed <kbd class="calibre12">probability=True</kbd> to our <kbd class="calibre12">SVC</kbd> function to allow <kbd class="calibre12">SVC()</kbd> to return class probabilities. In the <kbd class="calibre12">SVC</kbd> class, the default is <kbd class="calibre12">probability=False</kbd>.</p>
<p class="calibre25">In <em class="calibre13">S<span class="calibre5">tep 5</span></em><span class="calibre5">,</span> we fitted our model to the training data. We used the <kbd class="calibre12">predict_proba()</kbd> function in <em class="calibre13">S<span class="calibre5">tep 6</span></em> to predict the class probabilities for our test observations.</p>
<p class="calibre25">Finally, in <em class="calibre13">S<span class="calibre5">tep 7</span></em>, we assigned different weights to each of our models to estimate the weighted average predictions. The question that comes up is how to choose the weights. On<span class="calibre5">e way is to sample the weights uniformly and to make sure they normalize to one and validate on the test set and repeat keeping track of weights that provide the highest accuracy. This is an example of a random search. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p class="calibre2">The following are the scikit reference links:</p>
<ul class="calibre10">
<li class="calibre11">Scikit guide to ensemble methods <a href="https://bit.ly/2oVNogs" class="calibre9">(https://bit.ly/2oVNogs)</a></li>
<li class="calibre11">Scikit guide to <kbd class="calibre12">VotingClassifier</kbd> <a href="https://bit.ly/2oW0avo" class="calibre9">(https://bit.ly/2oW0avo)</a></li>
</ul>


            </article>

            
        </section>
    </body></html>