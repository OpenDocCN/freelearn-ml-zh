- en: '*Chapter 5*: Performing Data Analysis and Visualization'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第五章*：执行数据分析和可视化'
- en: In the previous chapter, we learned how to bring our datasets to the cloud,
    define data stores in the Azure Machine Learning workspace to access them, and
    register datasets in the Azure Machine Learning dataset registry to have a good
    basis to start data preprocessing from. In this chapter, we will learn how to
    explore this raw data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何将我们的数据集带到云端，在Azure机器学习工作区中定义数据存储以访问它们，并在Azure机器学习数据集注册表中注册数据集，从而为从数据预处理开始打下良好的基础。在本章中，我们将学习如何探索这些原始数据。
- en: First, you will learn about techniques that can help you explore tabular and
    file datasets. We will also talk about how to handle missing values, how to cross-correlate
    features to understand statistical connections between them, and how to bring
    domain knowledge to this process to improve our understanding of the context and
    the quality of our data cleansing. In addition, we will learn how to use ML algorithms
    not for training but for exploring our datasets.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您将了解可以帮助您探索表格和文件数据集的技术。我们还将讨论如何处理缺失值，如何交叉关联特征以了解它们之间的统计关系，以及如何将领域知识应用到这个过程中，以改善我们对上下文和数据清洗质量的了解。此外，我们将学习如何使用机器学习算法，不是为了训练，而是为了探索我们的数据集。
- en: After that, we will apply these methods to a real-life dataset while learning
    how to work with pandas DataFrames and how to visualize the properties of our
    dataset.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们将将这些方法应用于实际数据集，同时学习如何使用pandas DataFrame以及如何可视化数据集的特性。
- en: Finally, we will look at methods that can map high-dimensional data to a low-dimensional
    plane, which will help us see similarities and relationships between data points.
    Additionally, these methods can give us clear hints on how clean our data is and
    how effective the chosen ML algorithms will be on the dataset.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将探讨可以将高维数据映射到低维平面的方法，这将帮助我们看到数据点之间的相似性和关系。此外，这些方法还可以给我们提供清晰的线索，了解我们的数据有多干净，以及所选机器学习算法在数据集上的效果如何。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Understanding data exploration techniques
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解数据探索技术
- en: Performing data analysis on a tabular dataset
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在表格数据集上执行数据分析
- en: Understanding dimensional reduction techniques
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解降维技术
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will use the following Python libraries and versions to
    perform data pre-processing and high-dimensional visualizations:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用以下Python库和版本来执行数据预处理和高维可视化：
- en: '`azureml-sdk 1.34.0`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`azureml-sdk 1.34.0`'
- en: '`azureml-widgets 1.34.0`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`azureml-widgets 1.34.0`'
- en: '`azureml-dataprep 2.20.0`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`azureml-dataprep 2.20.0`'
- en: '`pandas 1.3.2`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas 1.3.2`'
- en: '`numpy 1.19.5`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy 1.19.5`'
- en: '`scikit-learn 0.24.2`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scikit-learn 0.24.2`'
- en: '`seaborn 0.11.2`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seaborn 0.11.2`'
- en: '`plotly 5.3.1`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`plotly 5.3.1`'
- en: '`umap_learn 0.5.1`'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`umap_learn 0.5.1`'
- en: '`statsmodels 0.13.0`'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`statsmodels 0.13.0`'
- en: '`missingno 0.5.0`'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`missingno 0.5.0`'
- en: Similar to previous chapters, you can execute this code using either a local
    Python interpreter or a notebook environment hosted in Azure Machine Learning.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章类似，您可以使用本地Python解释器或Azure机器学习中的笔记本环境执行此代码。
- en: 'All code examples in this chapter can be found in the GitHub repository for
    this book: [https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter05](https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter05).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有代码示例都可以在本书的GitHub仓库中找到：[https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter05](https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter05)。
- en: Understanding data exploration techniques
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解数据探索技术
- en: Descriptive data exploration is, without a doubt, one of the most important
    steps in an ML project. If you want to clean data and build derived features or
    select an ML algorithm to predict a target variable in your dataset, then you
    need to understand your data first. Your data will define many of the necessary
    cleaning and preprocessing steps; it will define which algorithms you can choose,
    and it will ultimately define the performance of your predictive model.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 描述性数据探索无疑是机器学习项目中最重要的步骤之一。如果您想清理数据、构建派生特征或选择机器学习算法来预测数据集中的目标变量，那么您首先需要了解您的数据。您的数据将定义许多必要的清理和预处理步骤；它将定义您可以选择哪些算法，并最终定义您预测模型的性能。
- en: Hence, data exploration should be considered an important analytical step to
    understanding whether your data is informative enough to build an ML model in
    the first place. By analytical step, we mean that the exploration should be done
    as a structured analytical process rather than a set of experimental tasks. Therefore,
    we will go through a checklist of data exploration tasks that you can perform
    as an initial step in every ML project – before you start any data cleaning, preprocessing,
    feature engineering, or model selection.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，数据探索应该被视为理解数据是否足够信息以首先构建机器学习模型的重要分析步骤。通过分析步骤，我们是指探索应该作为一个结构化的分析过程来完成，而不是一系列实验任务。因此，我们将通过一系列数据探索任务清单，这些任务可以作为每个机器学习项目的初始步骤来执行——在你开始任何数据清洗、预处理、特征工程或模型选择之前。
- en: The possible tasks we can perform are tied to the type of dataset we are working
    with. A lot of datasets will come in the form of tabular data, which means we
    have either continuous or categorical features defined for each instance of the
    dataset. These datasets can be visualized as a table, and we can perform basic
    and complex mathematical operations on them. The other general type of dataset
    we may encounter will come in the form of media files. This includes images, videos,
    sound files, documents, and anything else that is not made up of data points that
    you could fit into a table structure.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以执行的可能任务与我们正在处理的数据集类型相关。许多数据集将以表格数据的形式出现，这意味着我们为数据集的每个实例定义了连续或分类特征。这些数据集可以表示为表格，我们可以在它们上执行基本的和复杂的数学运算。我们可能遇到的另一种一般类型的数据集将以媒体文件的形式出现。这包括图像、视频、声音文件、文档以及任何其他不能放入表格结构中的数据点。
- en: 'To represent these different types of datasets, Azure Machine Learning gives
    us the option to save our data in one of the following objects:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了表示这些不同类型的数据集，Azure 机器学习提供了将我们的数据保存为以下对象之一的选项：
- en: '**TabularDataset:** This class offers methods for performing basic transformations
    on tabular data and converting them into known formats such as pandas ([https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.tabulardataset](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.tabulardataset)).'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TabularDataset:** 这个类提供了对表格数据进行基本转换的方法，并将它们转换为已知格式，例如 pandas ([https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.tabulardataset](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.tabulardataset))。'
- en: '**FileDataset:** This class primarily offers filtering methods on file metadata
    ([https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.filedataset](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.filedataset)).'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FileDataset:** 这个类主要提供对文件元数据的过滤方法 ([https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.filedataset](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.filedataset))。'
- en: Both types of dataset objects can be registered to the Azure Machine Learning
    Dataset Registry for further use after preprocessing.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种类型的数据集对象都可以注册到 Azure 机器学习数据集注册表中，以便在预处理后进一步使用。
- en: Judging only by the methods that are available in those two classes, it becomes
    clear that the possible tasks and operations we can perform differ greatly between
    tabular datasets and file datasets. In the next few sections, we will look at
    both types and how we can prepare them to influence the result of our ML model.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 仅从那些两个类别中可用的方法来判断，很明显，我们可以在表格数据集和文件数据集之间执行的可能任务和操作有很大差异。在接下来的几节中，我们将探讨这两种类型以及我们如何准备它们以影响我们机器学习模型的结果。
- en: Exploring and analyzing tabular datasets
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索和分析表格数据集
- en: A tabular dataset allows us to utilize the full spectrum of mathematical and
    statistical functions to analyze and transform our dataset, but in most cases,
    we do not have the time or resources to randomly run every dataset through all
    the possible techniques in our arsenal.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 表格数据集允许我们利用数学和统计函数的全谱来分析和转换我们的数据集，但在大多数情况下，我们没有时间或资源来随机将每个数据集通过我们工具箱中所有可能的技术。
- en: Choosing the right methods does not only involve having experience in analyzing
    a lot of different datasets but also subject matter expertise of the domain we
    are working in. There are areas where everyone has some general expertise (think
    the influencing factors of house prices, for example), but then there are a lot
    of areas where specialized knowledge is needed to understand the data at hand.
    Imagine that you want to increase the yield of a blast furnace creating steel.
    In such a scenario, to understand the data, you need to have intimate knowledge
    of the chemical processes in the furnace, or you need a **subject matter expert**
    to support you. In every step of exploration and analysis, we need to apply domain
    knowledge to interpret the result and relationships we see.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 选择正确的方法不仅需要分析大量不同数据集的经验，还需要我们所在领域的专业知识。有些领域每个人都有一些一般的专长（例如，房价的影响因素），但还有很多领域需要专业知识来理解手头的数据。想象一下，你想要提高炼钢高炉的产量。在这种情况下，为了理解数据，你需要对高炉中的化学过程有深入了解，或者你需要一个**领域专家**来支持你。在探索和分析的每一步中，我们需要应用领域知识来解释我们看到的结果和关系。
- en: Besides understanding the domain, we also need to understand the features in
    the datasets and their targets or labels. Imagine having a dataset made up of
    features of houses in a certain city but without their market prices. To predict
    house prices, we would need labels or target values for the price of each house.
    On the other hand, if we were to predict if an email is spam or not and we have
    a dataset that contains a bunch of emails containing a lot of metadata, this might
    be good enough to train a model through unsupervised learning.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 除了理解领域之外，我们还需要理解数据集中的特征及其目标或标签。想象一下，有一个由某个城市房屋特征组成的数据集，但没有它们的市场价格。为了预测房价，我们需要每个房屋的价格标签或目标值。另一方面，如果我们想要预测一封电子邮件是否为垃圾邮件，而我们有一个包含大量元数据的电子邮件数据集，这可能足以通过无监督学习训练模型。
- en: Therefore, to get a good understanding of the dataset, we need to thoroughly
    explore its content and get as many insights as possible on the features and the
    possible target to make good decisions.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了对数据集有一个良好的理解，我们需要彻底探索其内容，尽可能多地从特征和可能的靶标中获取洞察力，以便做出明智的决策。
- en: Important Note
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Please keep in mind that not only the feature dimensions but also the target
    variable needs to be preprocessed and analyzed thoroughly.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，不仅特征维度需要预处理和分析，目标变量也需要。
- en: 'To achieve this, we will start by looking at the following aspects of every
    feature and target vector in the dataset:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们将首先查看数据集中每个特征和目标向量的以下方面：
- en: '`datetime`, `string`, `int`, `object`)? Do we need to do a data type conversion?'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`datetime`、`string`、`int`、`object`？我们需要进行数据类型转换吗？'
- en: '**Missing data**: Are there any missing entries? How do we handle them?'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺失数据**：是否存在任何缺失条目？我们如何处理它们？'
- en: '**Inconsistent data**: Are date and time stored in different ways? Are the
    same categories written in different ways? Are there different categories with
    the same meaning in the given context?'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不一致的数据**：日期和时间是以不同的方式存储的吗？相同的类别是否以不同的方式书写？在给定上下文中，是否存在具有相同意义的不同类别？'
- en: '**Unique values**: How many unique values exist for a categorical feature?
    Are there too many? Should we create a subset of them?'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**唯一值**：对于一个分类特征，存在多少唯一值？是否太多？我们应该创建它们的子集吗？'
- en: '**Statistical properties**: What are the mean, median, and variance of a feature?
    Are there any outliers? What are the minimum and maximum values? What is the most
    common value (mode)?'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**统计特性**：一个特征的均值、中位数和方差是什么？是否存在任何异常值？最小值和最大值是什么？最常见的值（众数）是什么？'
- en: '**Statistical distribution**: How are the values distributed? Is there a data
    skew? Would normalization or scaling be useful?'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**统计分布**：值是如何分布的？是否存在数据偏斜？标准化或缩放是否有用？'
- en: '**Correlation**: How are different features correlated to each other? Are there
    features containing similar information that could be omitted? How much are my
    features correlated with the target?'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相关性**：不同的特征之间是如何相互关联的？是否存在包含相似信息可以被省略的特征？我的特征与目标的相关性有多大？'
- en: Analyzing each dimension of a dataset with more than 100 feature dimensions
    is an extremely time-consuming task. However, instead of randomly exploring feature
    dimensions, you can analyze the dimensions ordered by feature importance and significantly
    reduce your time working through the data. Like many other areas of computer science,
    it is good to use an 80/20 principle for the initial data exploration, which means
    using only 20% of the features to achieve 80% of the performance. This sets you
    up for a great start and you can always come back later to add more dimensions
    if needed.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 分析具有超过100个特征维度的数据集的每个维度是一项极其耗时的工作。然而，您可以通过按特征重要性排序的维度来分析，而不是随机探索特征维度，从而显著减少您处理数据的时间。像计算机科学的许多其他领域一样，在初始数据探索中使用80/20原则是好的，这意味着只使用20%的特征来实现80%的性能。这将为您提供一个良好的起点，您可以在需要时随时返回添加更多维度。
- en: 'Therefore, it is wise to understand the importance of the features for your
    modeling. We can do this by looking at the relationship between features and the
    target variable. There are many ways to do this, some of which are as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，了解特征对建模的重要性是明智的。我们可以通过查看特征与目标变量之间的关系来实现这一点。有许多方法可以做到这一点，以下是一些方法：
- en: '**Regression coefficient**: Used in regression'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归系数**：用于回归'
- en: '**Feature importance**: Used in classification'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征重要性**：用于分类'
- en: '**High error rates for categorical values**: Used in binary classification'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类值的错误率较高**：用于二元分类'
- en: By applying these steps, you can understand the data and gain knowledge about
    the required preprocessing tasks for your data, features, and target variables.
    Along with that, it will give you a good estimate of what difficulties you can
    expect in your prediction task, which is essential for judging the required algorithms
    and validation strategies. You will also gain insight into what possible feature
    engineering methods could be applied to your dataset and have a better understanding
    of how to select a good error metric.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用这些步骤，您可以了解数据，并获取有关数据、特征和目标变量的预处理任务的知识。此外，它将为您提供对预测任务中可能遇到的困难的良好估计，这对于判断所需的算法和验证策略至关重要。您还将深入了解可能应用于数据集的特征工程方法，并更好地理解如何选择一个好的误差度量。
- en: Important Note
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: You can use a representative subset of the data and extrapolate your hypothesis
    and insights to the whole dataset.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用数据的代表性子集，并将您的假设和见解外推到整个数据集。
- en: Once the data has been uploaded to a storage service in Azure, we can bring
    up a notebook environment and start exploring the data. The goal is to thoroughly
    explore our data in an analytical process to understand the distribution of *each*
    dimension of our data. We will perform some of these steps on a tabular dataset
    in the *Performing data analysis on a tabular dataset* section.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据已上传到Azure的存储服务中，我们就可以启动笔记本环境并开始探索数据。目标是通过对数据的深入分析过程，了解数据每个维度的分布。我们将在“在表格数据集上执行数据分析”部分执行这些步骤中的一些。
- en: But first, we will look at some of the techniques that we've discussed in more
    detail and take a quick look at file datasets.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，我们将回顾我们讨论过的某些技术，并快速查看文件数据集。
- en: Handling missing values and outliers
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理缺失值和异常值
- en: One of the first things to look for in a new dataset is **missing values** for
    each feature and target dimension. This will help you gain a deeper understanding
    of the dataset and what actions could be taken to resolve them. It is not uncommon
    to remove missing values or impute them with zeros at the beginning of a project
    – however, this approach bears the risk of not properly analyzing missing values
    in the first place and losing a lot of data points.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在新的数据集中，首先要寻找的是每个特征和目标维度的**缺失值**。这将帮助您更深入地了解数据集以及可以采取哪些措施来解决这些问题。在项目开始时删除缺失值或用零填充它们并不罕见——然而，这种方法存在风险，即最初未能正确分析缺失值，并丢失大量数据点。
- en: Important Note
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Missing values can be disguised as *valid* numeric or categorical values. Typical
    examples are minimum or maximum values, -1, 0, or NaN. Hence, if you find the
    values 32,767 (= 215-1) or 65,535 (= 216-1) appearing multiple times in an integer
    data column, they may well be missing values disguised as the maximum signed or
    unsigned 16-bit integer representation. Always assume that your data contains
    missing values and outliers in different shapes and representations. Your task
    is to uncover, find, and clean them.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失值可能被伪装成有效的数值或分类值。典型的例子是最小值或最大值，-1，0，或NaN。因此，如果你在一个整数数据列中多次发现32,767（= 215-1）或65,535（=
    216-1）这样的值，它们可能正是伪装成最大有符号或无符号16位整数表示的缺失值。始终假设你的数据包含不同形状和表示的缺失值和异常值。你的任务是揭示、找到并清理它们。
- en: 'Any prior knowledge about the data or domain will give you a competitive advantage
    when you''re working with the data. The reason for this is that you will be able
    to understand **missing values**, **outliers**, and **extremes** concerning the
    data and domain, which will help you perform better imputation, cleansing, or
    transformation. As the next step, you should look for these outliers in your data,
    specifically for the absolute number or percentages of the following:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 关于数据或领域的任何先验知识都会在你处理数据时给你带来竞争优势。这是因为你将能够理解数据和相关领域的**缺失值**、**异常值**和**极端值**，这将帮助你更好地进行插补、清理或转换。作为下一步，你应该在你的数据中寻找这些异常值，特别是以下方面的绝对数量或百分比：
- en: The null values (look for `Null`, `"Null"`, `""`, `NaN`, and so on)
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 空值（查找`Null`，`"Null"`，`""`，`NaN`等）
- en: The minimum and maximum values
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小值和最大值
- en: The most common value (`MODE`)
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最常见的值（`MODE`）
- en: The `0` value
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0`值'
- en: Any unique values
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何唯一值
- en: Once you have identified these values, you can use different preprocessing techniques
    to impute missing values and normalize or exclude dimensions.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你确定了这些值，你可以使用不同的预处理技术来填补缺失值和归一化或排除维度。
- en: 'The typical options for dealing with missing values are as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 处理缺失值的典型选项如下：
- en: '**Deletion**: Delete entire rows or columns from the dataset. This can result
    in bias or having insufficient data for training.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**删除**：从数据集中删除整个行或列。这可能导致偏差或训练数据不足。'
- en: '`Missing` for categorical features.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于分类特征，查找`Missing`。
- en: '**Column average**: Fill in the mean, median, or mode value of the entire data
    column or a subset of the column based on relationships with other features.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**列平均值**：根据与其他特征的关系，填充整个数据列或列子集的均值、中位数或众数。'
- en: '**Interpolation**: Fill in an interpolated value based on the column''s data.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**插值**：根据列的数据填充一个插值值。'
- en: '**Hot-deck imputation**: Fill in the logical previous value from the sorted
    records of the data column (useful in time series datasets).'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**热补丁插补**：从数据列的排序记录中填充逻辑前一个值（在时间序列数据集中很有用）。'
- en: 'The typical options for dealing with outliers are as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 处理异常值的典型选项如下：
- en: '**Erroneous observations**: If the value is wrong, drop either the full column
    or replace the outlier with the mean of the column.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**错误观测值**：如果值是错误的，可以删除整个列，或者用列的均值替换异常值。'
- en: '**Leave as-is**: If it contains important information and if the model does
    not get distorted by it.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**保留原样**：如果它包含重要信息，并且模型不会因此受到扭曲。'
- en: '**Cap or floor**: Cap or floor the value to a maximum deviation from the mean
    (for example, three standard deviations).'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上限或下限**：将值限制在均值最大偏差内（例如，三个标准差）。'
- en: To get more context when choosing the right way to handle missing values and
    outliers, it is useful to statistically analyze the column distribution and correlations.
    We will do this in the following sections.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在选择处理缺失值和异常值的方法时获得更多上下文，统计分析列分布和相关性是有用的。我们将在以下章节中这样做。
- en: Calculating statistical properties and visualizing data distributions
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算统计属性和可视化数据分布
- en: 'Now that you know the outliers, you can start exploring the **value distribution**
    of your dataset''s features. This will help you understand which transformation
    and normalization techniques should be applied during data preparation. Some common
    distribution statistics to look for in a continuous variable are as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道了异常值，你可以开始探索你的数据集特征的**值分布**。这将帮助你了解在数据准备过程中应该应用哪些转换和归一化技术。在连续变量中寻找的一些常见分布统计量如下：
- en: The mean or median value
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均值或中位数
- en: The minimum and maximum value
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小值和最大值
- en: The variance and standard deviation
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方差和标准差
- en: The 25th, 50th (median), and 75th percentiles
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第25、50（中位数）和第75百分位数
- en: The data skew
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据倾斜
- en: 'Common techniques for visualizing these distributions include using **boxplots**,
    **density plots**, or **histograms**. The following screenshot shows these different
    visualization techniques plotted per target class for a multi-class recognition
    dataset. Each method has advantages and disadvantages – boxplots show all the
    relevant metrics while being a bit harder to read, density plots show very smooth
    shapes while hiding some of the outliers, and histograms don''t let you spot the
    median and percentiles easily while giving you a good estimate of the data skew:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化这些分布的常用技术包括使用**箱线图**、**密度图**或**直方图**。以下截图显示了针对多类识别数据集的每个目标类别的不同可视化技术。每种方法都有其优缺点——箱线图显示了所有相关指标，但阅读起来稍微有点困难，密度图显示了非常平滑的形状，但隐藏了一些异常值，而直方图则不让你容易地找到中位数和百分位数，同时给你一个数据倾斜的良好估计：
- en: '![Figure 5.1 – A boxplot (left), a density plot (middle), and a histogram (right)
    ](img/B17928_05_01.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – 箱线图（左），密度图（中），直方图（右）](img/B17928_05_01.jpg)'
- en: Figure 5.1 – A boxplot (left), a density plot (middle), and a histogram (right)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – 箱线图（左），密度图（中），直方图（右）
- en: Here, we can see that only histograms work well for categorical data (both nominal
    and ordinal). However, you could look at the number of values per category. You
    can find the code for creating these plots in the `01_data_distribution.ipynb`
    file in this book's GitHub repository.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到，对于分类数据（无论是名义的还是序数的），只有直方图工作得很好。然而，你可以查看每个类别的值数量。你可以在本书的 GitHub 仓库中的
    `01_data_distribution.ipynb` 文件中找到创建这些图表的代码。
- en: 'Another nice way to display the value distribution versus the target rate is
    in a binary classification task. The following diagram shows the **version number**
    of Windows Defender against the malware **detection rate** (for non-touch devices)
    from the *Microsoft Malware detection dataset* ([https://www.kaggle.com/c/microsoft-malware-prediction/data](https://www.kaggle.com/c/microsoft-malware-prediction/data)):'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在二元分类任务中，另一种显示值分布与目标率关系的好方法是。以下图表显示了来自 *Microsoft 恶意软件检测数据集*（[https://www.kaggle.com/c/microsoft-malware-prediction/data](https://www.kaggle.com/c/microsoft-malware-prediction/data)）的
    Windows Defender 的 **版本号** 与恶意软件 **检测率**（针对非触摸设备）：
- en: '![Figure 5.2 – Version number versus detection rate for Windows Defender  ](img/B17928_05_02.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – Windows Defender 版本号与检测率的关系](img/B17928_05_02.jpg)'
- en: Figure 5.2 – Version number versus detection rate for Windows Defender
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – Windows Defender 版本号与检测率的关系
- en: Many statistical ML algorithms require the data to be normally distributed,
    so it needs to be normalized or standardized. Knowing the data distribution helps
    you decide which transformations need to be applied during data preparation. In
    practice, data often needs to be transformed, scaled, or normalized.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 许多统计机器学习算法需要数据呈正态分布，因此需要归一化或标准化。了解数据分布有助于你决定在数据准备期间需要应用哪些转换。在实践中，数据通常需要转换、缩放或归一化。
- en: Finding correlated dimensions
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 寻找相关维度
- en: Another common task in data exploration is looking for correlations in the dataset.
    This will help you dismiss feature dimensions that are highly correlated and thus
    may influence your ML model. In linear regression models, for example, two highly
    correlated independent variables will lead to large coefficients with opposite
    signs that ultimately cancel each other out. A much more stable regression model
    can be found by removing one of the correlated dimensions. Therefore, it is important
    not only to look at correlations between features and targets but also among features.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 数据探索中的另一个常见任务是寻找数据集中的相关性。这将帮助你排除高度相关的特征维度，从而可能影响你的机器学习模型。例如，在线性回归模型中，两个高度相关的独立变量将导致具有相反符号的大系数，最终相互抵消。通过移除一个相关维度，可以找到一个更稳定的回归模型。因此，不仅需要查看特征和目标之间的相关性，还需要查看特征之间的相关性。
- en: The `-1` (strongly negatively correlated) to `1` (strongly positively correlated).
    A `0` indicates no linear relationship between two variables.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`-1`（强负相关）到`1`（强正相关）。`0`表示两个变量之间没有线性关系。'
- en: 'The following diagram shows an example of a correlation matrix for the *California
    Housing dataset* ([https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html)),
    consisting of only continuous variables. The correlations range from `-1` to `1`
    and are colored accordingly, where red denotes a negative correlation and blue
    denotes a positive correlation. The last row shows the linear correlation between
    each feature dimension and the target variable (`MedHouseVal`). We can immediately
    tell that there is a correlation between `Longitude` and `Latitude`, between `MedHouseVal`
    and `MedInc`, and between `AveRooms` and `AveBedrms`. All of these relationships
    are relatively unsurprising:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了*加利福尼亚住房数据集*（[https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html)）的相关矩阵示例，该数据集仅包含连续变量。相关系数的范围从`-1`到`1`，并相应着色，其中红色表示负相关，蓝色表示正相关。最后一行显示了每个特征维度与目标变量（`MedHouseVal`）之间的线性相关性。我们可以立即看出，`Longitude`与`Latitude`之间存在相关性，`MedHouseVal`与`MedInc`之间存在相关性，以及`AveRooms`与`AveBedrms`之间存在相关性。所有这些关系都是相对不出意外的：
- en: '![Figure 5.3 – Correlation matrix for the California Housing dataset ](img/B17928_05_03.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图5.3 – 加利福尼亚住房数据集的相关矩阵](img/B17928_05_03.jpg)'
- en: Figure 5.3 – Correlation matrix for the California Housing dataset
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 – 加利福尼亚住房数据集的相关矩阵
- en: You can find the code for creating this correlation matrix in the `02_correlation.ipynb`
    file in this book's GitHub repository.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本书GitHub仓库中的`02_correlation.ipynb`文件中找到创建此相关矩阵的代码。
- en: It is worth mentioning that many correlation coefficients can only be between
    numerical values. Ordinal variables can be encoded, for example, using integer
    encoding and can also compute a meaningful correlation coefficient. For nominal
    data, you need to fall back on different methods, such as **Cramér's V** to compute
    the correlation. It is worth noting that the input data doesn't need to be normalized
    (linearly scaled) before you compute the correlation coefficient.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，许多相关系数只能存在于数值之间。有序变量可以通过整数编码进行编码，也可以计算出一个有意义的相关系数。对于名义数据，你需要回退到不同的方法，例如**Cramér's
    V**来计算相关性。值得注意的是，在计算相关系数之前，输入数据不需要进行归一化（线性缩放）。
- en: Measuring feature and target dependencies for regression
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测量回归中的特征和目标变量依赖性
- en: Once we have analyzed the missing values, data distribution, and correlations,
    we can start analyzing the relationship between the features and the target variable.
    This will give us a good indication of the difficulty of the prediction problem
    and, hence, the expected baseline performance, which is essential for prioritizing
    feature engineering efforts and choosing an appropriate ML model. Another great
    benefit of measuring this dependency is ranking the feature dimensions by their
    impact on the target variable, which you can use as a priority list for data exploration
    and preprocessing.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们分析了缺失值、数据分布和相关性，我们就可以开始分析特征与目标变量之间的关系。这将为我们提供预测问题难度的良好指示，从而确定预期的基线性能，这对于优先考虑特征工程努力和选择合适的机器学习模型至关重要。测量这种依赖性的另一个巨大好处是按特征维度对目标变量的影响进行排名，你可以将其用作数据探索和预处理优先级列表。
- en: In a regression task, the target variable is numerical or ordinal. Therefore,
    we can compute the correlation coefficient between the individual features and
    the target variable to compute the linear dependency between the feature and the
    target. High correlation – that is, a high absolute correlation coefficient –
    indicates that a strong linear relationship exists. This gives us a great place
    to start exploring further. However, in many practical problems, it is rare to
    see a high (linear) correlation between the feature and target variables.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归任务中，目标变量是数值或有序的。因此，我们可以计算单个特征与目标变量之间的相关系数，以计算特征与目标变量之间的线性依赖性。高相关性，即高绝对相关系数，表明存在强烈的线性关系。这为我们进一步探索提供了一个很好的起点。然而，在许多实际问题中，很少看到特征与目标变量之间存在高（线性）相关性。
- en: 'You can also visualize this dependency between the feature and the target variable
    using a **scatter plot** or **regression plot**. The following diagram shows a
    regression plot between the average number of rooms per dwelling (**RM**) and
    the median value of owner-occupied homes (**MEDV**) from the *Boston Housing dataset*.
    If the regression line is at 45 degrees, then we have a perfect linear correlation:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用**散点图**或**回归图**来可视化特征和目标变量之间的依赖关系。以下图表显示了来自*波士顿住房数据集*的平均每户住宅房间数（**RM**）与业主自住房屋的中位数价值（**MEDV**）之间的回归图。如果回归线在45度角，则表示存在完美的线性相关性：
- en: '![Figure 5.4 – Scatter plot with a regression line between the feature and
    the target  ](img/B17928_05_04.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图5.4 – 特征与目标之间的散点图，带有回归线](img/B17928_05_04.jpg)'
- en: Figure 5.4 – Scatter plot with a regression line between the feature and the
    target
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 – 特征与目标之间的散点图，带有回归线
- en: Another great approach to determining this dependency is to fit a linear or
    logistic regression model to the training data. The resulting model coefficients
    should give you a good explanation of the relationship – the higher the coefficient,
    the larger the linear (for linear regression) or marginal (for logistic regression)
    dependency on the target variable. Hence, sorting by coefficients results in a
    list of features ordered by importance. Depending on the regression type, the
    input data should be normalized or standardized.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 确定这种依赖关系的另一种有效方法是，将线性或逻辑回归模型拟合到训练数据中。得到的模型系数应该能为你提供对关系的良好解释——系数越高，对目标变量的线性（对于线性回归）或边际（对于逻辑回归）依赖性就越大。因此，按系数排序将得到一个按重要性排序的特征列表。根据回归类型，输入数据应进行归一化或标准化。
- en: 'The following screenshot shows an example of the correlation coefficients (the
    first column) of a fitted **Ordinary Least Squares** (**OLS**) regression model:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了拟合的**普通最小二乘法**（**OLS**）回归模型的关联系数（第一列）：
- en: '![Figure 5.5 – The correlation coefficients of an OLS regression model  ](img/B17928_05_05.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图5.5 – OLS回归模型的关联系数](img/B17928_05_05.jpg)'
- en: Figure 5.5 – The correlation coefficients of an OLS regression model
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 – OLS回归模型的关联系数
- en: You can find the code for creating the plot and coefficients in the `03_regression.ipynb`
    file in this book's GitHub repository.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在本书GitHub仓库中的`03_regression.ipynb`文件中找到创建图表和系数的代码。
- en: While the resulting **R-squared metric** (not shown) may not be good enough
    for a baseline model, the ordering of the coefficients can help us prioritize
    further data exploration, preprocessing, and feature engineering.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然得到的**R平方指标**（未显示）可能不足以作为基线模型，但系数的排序可以帮助我们优先考虑进一步的数据探索、预处理和特征工程。
- en: Visualizing feature and label dependency for classification
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可视化特征和标签之间的分类依赖关系
- en: In a classification task with a multi-class nominal target variable, we can't
    use the regression coefficients without preprocessing the data further. Another
    popular method that works well out of the box is fitting a simple tree-based classifier
    to the training data. Depending on the size of the training data, we could use
    a decision tree or a tree-based ensemble classifier, such as **random forest**
    or **gradient-boosted trees**. Doing so results in a feature importance ranking
    of the feature dimensions according to the chosen split criterion. In the case
    of splitting by entropy, the features would be sorted by *information gain*, which
    would indicate which variables carry the most information about the target.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有多类名义目标变量的分类任务中，我们不能在不进一步预处理数据的情况下使用回归系数。另一种流行的、开箱即用的有效方法是，将简单的基于树的分类器拟合到训练数据中。根据训练数据的大小，我们可以使用决策树或基于树的集成分类器，如**随机森林**或**梯度提升树**。这样做将根据选择的分割标准对特征维度进行特征重要性排序。在以熵分割的情况下，特征将按*信息增益*排序，这将表明哪些变量携带关于目标变量最多的信息。
- en: 'The following diagram shows the feature importance fitted by a tree-based ensemble
    classifier using the entropy criterion from the *UCI Wine Recognition dataset*([https://archive.ics.uci.edu/ml/datasets/wine](https://archive.ics.uci.edu/ml/datasets/wine)):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了使用来自*UCI葡萄酒识别数据集*（[https://archive.ics.uci.edu/ml/datasets/wine](https://archive.ics.uci.edu/ml/datasets/wine)）的熵标准由基于树的集成分类器拟合的特征重要性：
- en: '![Figure 5.6 – Feature importance of the tree-based ensemble classifier  ](img/B17928_05_06.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图5.6 – 基于树的集成分类器的特征重要性](img/B17928_05_06.jpg)'
- en: Figure 5.6 – Feature importance of the tree-based ensemble classifier
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 – 基于树的集成分类器的特征重要性
- en: The lines represent variations in the information gain of features between individual
    trees. This output is a great first step to further data analysis and exploration
    in order of feature importance. You can find the code for calculating the feature
    importance and visualizing it in the `04_feature_importance.ipynb` file in this
    book's GitHub repository.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这些线条代表单个树之间特征信息增益的变化。这个输出是进一步数据分析和按特征重要性顺序探索的绝佳第一步。您可以在本书GitHub仓库中的`04_feature_importance.ipynb`文件中找到计算特征重要性并可视化的代码。
- en: 'Here is another popular approach to discovering the separability of your dataset.
    The following screenshot shows a dataset with three classes, where one is linearly
    separable and one isn''t:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是另一种发现数据集可分性的流行方法。以下截图显示了一个包含三个类别的数据集，其中一个是线性可分的，另一个不是：
- en: '![Figure 5.7 – A linearly separable dataset (left) versus a non-linearly separable
    dataset (right) ](img/B17928_05_07.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图5.7 – 线性可分数据集（左）与非线性可分数据集（右）的比较](img/B17928_05_07.jpg)'
- en: Figure 5.7 – A linearly separable dataset (left) versus a non-linearly separable
    dataset (right)
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7 – 线性可分数据集（左）与非线性可分数据集（右）的比较
- en: You can find the code for creating these separability graphs in the `05_separability.ipynb`
    file in this book's GitHub repository.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在本书GitHub仓库中的`05_separability.ipynb`文件中找到创建这些可分性图的代码。
- en: By looking at the three clusters and the overlaps between these clusters, you
    can see that having separated clusters means that a trained classification model
    will perform very well on this dataset. On the other hand, when we know that the
    data is not linearly separable, we know that this task will require advanced feature
    engineering and modeling to produce good results.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 通过观察三个簇以及这些簇之间的重叠，您可以发现，如果簇是分离的，那么训练好的分类模型将在这个数据集上表现非常好。另一方面，当我们知道数据不是线性可分的时候，我们知道这项任务将需要高级的特征工程和建模来产生良好的结果。
- en: Exploring and analyzing file datasets
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索和分析文件数据集
- en: 'A dataset that''s made up of media files is a different beast entirely. If
    we think of images, for example, we could present every pixel as a vector of information
    and see this as one feature of the image. But what could we do in terms of exploration
    and data cleaning? Probably not much on single features. Most of the time, what
    we need to do concerns a large group of pixels or the entire image itself. Broadly
    speaking, we could think of the following aspects:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 由媒体文件组成的数据集完全是另一回事。如果我们以图像为例，我们可以将每个像素视为信息向量，并将其视为图像的一个特征。但在探索和数据处理方面我们能做什么呢？可能对单个特征来说并不多。大多数时候，我们需要关注的是大量像素或整个图像本身。从广义上讲，我们可以考虑以下方面：
- en: '**Uniformity**: All the images in the dataset should be the same size. If not,
    they need to be rescaled, which may involve centering pixel values per channel,
    possibly followed by some form of normalization.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一致性**：数据集中的所有图像都应该具有相同的大小。如果不是，它们需要被重新缩放，这可能涉及到每个通道的像素值居中，可能还伴随着某种形式的归一化。'
- en: '**Augmentation**: This involves diversifying the dataset without taking on
    new data (new images). This is useful if we have a small dataset and typically
    involves horizontal and vertical flipping, cropping, and rotating, among other
    transformations.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强**：这涉及到在不获取新数据（新图像）的情况下使数据集多样化。如果我们有一个小的数据集，这通常涉及到水平翻转、裁剪和旋转等变换。'
- en: Looking at these options, it is clear that we are trying to fix something in
    an image dataset that could have been resolved already to a great extent when
    we took the images in the first place. Therefore, the reality is that when we're
    handling most types of media files, it is paramount to bring higher concentration
    toward taking good training samples for the dataset than to desperately fix them
    in the preprocessing stage.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 观察这些选项，很明显我们正在尝试修复图像数据集中可能在我们最初拍摄图像时已经基本解决的问题。因此，现实情况是，当我们处理大多数类型的媒体文件时，将更高的注意力集中在为数据集采集好的训练样本上，比在预处理阶段拼命修复它们至关重要。
- en: 'Let''s imagine that we are a manufacturer who wants to take pictures of the
    products they produce passing on a conveyor belt to find defective products and
    discard them. Let''s say that we have production facilities around the globe.
    What would you do to make sure the pictures are taken as uniformly as possible
    while covering a lot of different scenarios? Here are some aspects to consider:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们想象一下，我们是一家制造商，想要拍摄他们生产的在传送带上通过的产品照片，以找出有缺陷的产品并丢弃它们。假设我们在全球各地都有生产基地。你将如何确保照片尽可能均匀地拍摄，同时覆盖许多不同的场景？以下是一些需要考虑的方面：
- en: '**Camera type**: We probably need the same type of camera to be taking pictures
    in the same format all around the globe.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相机类型**：我们可能需要全球各地都使用相同类型的相机以相同的格式拍照。'
- en: '**Environmental conditions**: Is the lighting similar in all places? Are the
    temperature and humidity similar in all places? This could influence the electronics
    in the camera.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**环境条件**：所有地方的光照是否相似？温度和湿度是否在所有地方都相似？这可能会影响相机的电子设备。'
- en: '**Positioning**: Is the same angle being used to take the pictures? Can we
    take pictures from vastly different angles to increase variety?'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定位**：是否使用了相同的拍摄角度？我们能否从非常不同的角度拍照以增加多样性？'
- en: These are only some points to consider when you're taking the images.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是你在拍照时需要考虑的一些点。
- en: Now, let's look at another form of file data – sound files. Let's say that we
    want to build a speech-to-text model that converts what we say into written text.
    Such models are, for example, used in voice assistants to map a request to a set
    of actions to perform.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看文件数据的一种另一种形式——声音文件。假设我们想要构建一个语音转文本模型，将我们说的话转换成书面文本。这类模型例如在语音助手中被用来将请求映射到一系列要执行的操作。
- en: 'In this context, we could use **Fourier transformations**, among other methods,
    to decompose our sound files. However, we may want to think about the samples
    or training data we want to train on and how we can increase the quality of them
    while considering the following aspects:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下，我们可以使用**傅里叶变换**等方法来分解我们的声音文件。然而，我们可能需要考虑我们想要训练的样本或训练数据，以及如何在考虑以下方面的同时提高它们的质量：
- en: '**Recording hardware**: If we have a voice assistant at home, it is probably
    the same microphone for everyone. But what if we build a voice assistant for mobile
    phones? Then, we have vastly different microphones.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**录音硬件**：如果我们家里有语音助手，可能每个人都在使用同一个麦克风。但如果我们为手机构建语音助手呢？那么，我们就有了非常不同的麦克风。'
- en: '**Environment**: We probably need recordings of voices in different environments.
    There is certainly a different sound spectrum when we are standing in a tram compared
    to when we are in a recording booth.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**环境**：我们可能需要在不同环境中录制声音。当我们站在有轨电车上时，与我们在录音棚中时，声音频谱肯定不同。'
- en: '**Pronunciation**: The *ML algorithm* in your brain may have a hard time deciphering
    different pronunciations – especially dialects. How can an actual ML model handle
    this?'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**发音**：你大脑中的*机器学习算法*可能难以解析不同的发音——尤其是方言。一个实际的机器学习模型如何处理这个问题？'
- en: These are just some points to consider when you're handling sound files. Regarding
    pronunciation, if you look at **Azure Speech Services**, you will soon realize
    that two models are running in the background – one for the acoustic and one for
    the language. Look at the requirements for samples when building a custom model
    ([https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/how-to-custom-speech-test-and-train](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/how-to-custom-speech-test-and-train))
    as this can give you a good idea of what is required when you're building such
    a model from scratch.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是处理声音文件时需要考虑的一些点。关于发音，如果你查看**Azure语音服务**，你很快就会意识到后台运行着两个模型——一个用于声学，一个用于语言。在构建自定义模型时查看样本的要求（[https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/how-to-custom-speech-test-and-train](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/how-to-custom-speech-test-and-train)），因为这可以给你一个很好的想法，当你从头开始构建这样的模型时需要什么。
- en: In summary, for file datasets, we do not have as many options to statistically
    eliminate problems, so we should concentrate on taking good and clean samples
    that simulate the kind of realistic environment we would get when the model is
    running in production.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，对于文件数据集，我们没有太多选项来从统计上消除问题，因此我们应该专注于采集好的、干净的样本，这些样本模拟了模型在生产环境中运行时可能遇到的现实环境。
- en: Now that we have familiarized ourselves with the methods to explore and analyze
    different types of datasets, let's try this out on a real tabular dataset.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了探索和分析不同类型数据集的方法，让我们尝试在一个实际的表格数据集上应用这些方法。
- en: Performing data analysis on a tabular dataset
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在表格数据集上执行数据分析
- en: If you haven't followed the steps in [*Chapter 4*](B17928_04_ePub.xhtml#_idTextAnchor071),
    *Ingesting Data and Managing Datasets*, to download the snapshot of the *Melbourne
    Housing dataset* from `melb_data.csv`, in the `mlfiles` container in your storage
    account and have this connected to a datastore called `mldemoblob` in your Azure
    Machine Learning workspace.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有遵循[*第4章*](B17928_04_ePub.xhtml#_idTextAnchor071)“数据摄取与管理数据集”中的步骤，从`melb_data.csv`下载墨尔本住房数据集的快照，在您的存储账户的`mlfiles`容器中，并将其连接到Azure
    Machine Learning工作区中的数据存储`mldemoblob`。
- en: In the following sections, we will explore the dataset, do some basic statistical
    analysis, find missing values and outliers, find correlations between features,
    and take an initial measurement of feature importance while utilizing a random
    forest model, as we saw in the *Visualizing feature and label dependency for classification*
    section of this chapter. You can either create a new Jupyter notebook and follow
    along with this book or open the `06_ dataprep_melbhousing.ipynb` file in the
    GitHub repository for this chapter.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探索数据集，进行一些基本的统计分析，查找缺失值和异常值，找出特征之间的相关性，并在使用随机森林模型的同时，对特征的重要性进行初步测量，正如我们在本章的“可视化特征和标签依赖性以进行分类”部分所看到的。您可以选择创建一个新的Jupyter笔记本并跟随本书进行，或者打开GitHub仓库中本章的`06_
    dataprep_melbhousing.ipynb`文件。
- en: Note that the steps we will perform now are not exhaustive. As shown on the
    web page for the dataset, we have 21 features to work with. So, to be thorough,
    you will have to analyze each.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们现在要执行的步骤并不全面。如数据集网页所示，我们有21个特征可以工作。因此，为了彻底分析，你必须分析每一个。
- en: This section should give you a good understanding of the types of tasks you
    can perform, but we will leave a lot of questions open for you to find answers
    for. If you need some inspiration for that, have a look at this dataset on the
    Kaggle website. You will find notebooks from a lot of users trying to analyze
    this dataset.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 本节应该能让你对可以执行的任务类型有一个很好的理解，但我们将会留下很多问题供你寻找答案。如果你需要一些灵感，可以查看Kaggle网站上的这个数据集。你将找到许多用户尝试分析这个数据集的笔记本。
- en: Finally, we will not completely transform the actual data at this point as we
    will come back to this problem in [*Chapter 6*](B17928_06_ePub.xhtml#_idTextAnchor102),
    *Feature Engineering and Labeling*, where we will learn how to select features
    and create new ones based on the statistical analysis and knowledge we will gain
    through the upcoming process.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在此处不会完全转换实际数据，因为我们将在[*第6章*](B17928_06_ePub.xhtml#_idTextAnchor102)“特征工程与标注”中回到这个问题，在那里我们将学习如何根据通过即将到来的过程获得的分析和知识来选择特征和创建新的特征。
- en: Initial exploration and cleansing of the Melbourne Housing dataset
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始探索和清洗墨尔本住房数据集
- en: 'In this section, we will load the data from a data store that is registered
    in Azure Machine Learning and look at its content. After that, we will start doing
    some basic cleaning regarding the raw data:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将从在Azure Machine Learning中注册的数据存储中加载数据，并查看其内容。之后，我们将开始进行一些关于原始数据的基本清洗：
- en: 'Download the following packages through Python PIP either separately or using
    the requirements file you can find in this book''s GitHub repository: `pandas`,
    `seaborn`, `plotly`, `scikit-learn`, `numpy`, `missingno`, `umap-learn`, and `statsmodels`.'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过Python PIP单独下载以下包或使用本书GitHub仓库中可找到的需求文件：`pandas`、`seaborn`、`plotly`、`scikit-learn`、`numpy`、`missingno`、`umap-learn`和`statsmodels`。
- en: Create a new Jupyter notebook or follow along in the one mentioned previously.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的Jupyter笔记本或跟随之前提到的笔记本进行。
- en: Connect to your ML workspace through the configuration file, as we learned previously.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过配置文件连接到您的ML工作区，正如我们之前所学的。
- en: 'Use the following code to pull the dataset to your local computer:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码将数据集拉取到您的本地计算机：
- en: '[PRE0]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, we're retrieving the data from your defined ML data store, `yourname`,
    and loading the dataset into a tabular dataset object. Adapt the path and name
    of the file in the second to last line, depending on your folder structure in
    your data store.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们正在从您定义的ML数据存储`yourname`中检索数据，并将数据集加载到一个表格数据集对象中。根据您数据存储中的文件夹结构，调整第二行最后面的文件路径和名称。
- en: 'The methods that are available on a tabular dataset object are not as abundant
    as they are for a pandas DataFrame. So, let''s transform it into a pandas DataFrame
    and have our first look at the data:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在表格数据集对象上可用的方法不如在pandas DataFrame上那么多。所以，让我们将其转换为pandas DataFrame，并首次查看数据：
- en: '[PRE1]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `pd.set_option()` method gives you access to the general settings for pandas
    operations. In this case, we want all the columns and rows to be shown and not
    truncated in the visualization. You can set this to whatever value works for you.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`pd.set_option()`方法让你可以访问pandas操作的通用设置。在这种情况下，我们希望所有列和行都在可视化中显示，而不是被截断。你可以将其设置为对你有用的任何值。'
- en: The `head()` function will give you a first look at the first five rows of the
    dataset. Have a look at them.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '`head()`函数将让你先看看数据集的前五行。看看它们。'
- en: You will see a bunch of features that make a lot of sense, such as `Suburb`,
    `Address`, and `Bathroom`. But some others might not be so clear, such as `Type`,
    `Method`, or `Distance`.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到很多有意义的特征，比如`Suburb`、`Address`和`Bathroom`。但有些其他特征可能不太清楚，比如`Type`、`Method`或`Distance`。
- en: Typically, as with any dataset, there is some form of data definition for the
    fields that are supplied with it. Have a look at the website of the datasets to
    find them.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，与任何数据集一样，对于随附的字段，都有某种形式的数据定义。查看数据集的网站以找到它们。
- en: 'Now that we''ve looked at the definition, let''s look at the so-called shape
    of the datasets, which will show us how many columns (features and labels) and
    how many rows (samples) the dataset contains:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经查看了定义，让我们看看数据集的所谓“形状”，这将显示数据集包含多少列（特征和标签）以及多少行（样本）：
- en: '[PRE2]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The preceding command shows us a dataset with 13,580 samples and 21 features/labels.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的命令显示了一个包含13,580个样本和21个特征/标签的数据集。
- en: 'Finally, run the following code so that we can look at the number of unique
    values, the number of missing values, and the data type of each feature:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，运行以下代码，以便我们可以查看每个特征的唯一值数量、缺失值数量和数据类型：
- en: '[PRE3]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'After running the preceding code, you should see something similar to the following:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码后，你应该会看到以下类似的内容：
- en: '![Figure 5.8 – Melbourne Housing dataset feature overview ](img/B17928_05_08.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图5.8 – 墨尔本住房数据集特征概述](img/B17928_05_08.jpg)'
- en: Figure 5.8 – Melbourne Housing dataset feature overview
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8 – 墨尔本住房数据集特征概述
- en: 'Looking at this table, we can make the following observations:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 看这张表格，我们可以得出以下观察结果：
- en: Four features seem to have missing values (**BuildingArea**, **YearBuilt**,
    **CouncilArea**, and **Car**).
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有四个特征似乎有缺失值（**BuildingArea**、**YearBuilt**、**CouncilArea**和**Car**）。
- en: A lot of numeric values (such as `float64` type. This is not necessarily a problem,
    but it's a waste of space since each probably fits into `int8`, `int16`, or `int32`.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多数值（如`float64`类型。这并不一定是问题，但既然每个值可能都适合`int8`、`int16`或`int32`，所以这是一种空间上的浪费。
- en: There are seven features of the `object` type, all of which are probably string
    values. We'll look at them in more detail shortly.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有七个`object`类型的特征，它们很可能都是字符串值。我们很快会详细查看它们。
- en: There is a feature called **Price**, which is probably a good label/target for
    supervised learning, such as classification.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一个名为**Price**的特征，这可能是监督学习（如分类）的一个很好的标签/目标。
- en: There is a feature named **Postcode** and a feature named **Suburb**. We may
    not need both. Judging by the unique values, **Suburb** seems to be more granular.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一个名为**Postcode**的特征和一个名为**Suburb**的特征。我们可能不需要两者都保留。从唯一值的数量来看，**Suburb**似乎更细粒度。
- en: There is a feature called **Address** and a feature called **SellerG**. Even
    though the seller of a property may have some influence on the price, we can drop
    them for now for simplicity. The same goes for addresses as they are extremely
    precise. Nearly every sample has a unique address.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一个名为**Address**的特征和一个名为**SellerG**的特征。尽管物业的卖家可能对价格有一定的影响，但我们现在可以为了简单起见先删除它们。同样，地址也是极其精确的。几乎每个样本都有一个唯一的地址。
- en: 'By looking at the seven features of the `object` type, we can see the following:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看`object`类型的七个特征，我们可以看到以下情况：
- en: '**Type**: This has **3** distinct values; our data definition shows **6**.
    We need to check this discrepancy.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类型**：这有**3**个不同的值；我们的数据定义显示**6**。我们需要检查这个差异。'
- en: '**Method**: This has **5** distinct values; our data definition shows **11**.
    We need to check this as well.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**方法**：这有**5**个不同的值；我们的数据定义显示**11**。我们也需要检查这一点。'
- en: '**SellerG**: This has **268** distinct seller names.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SellerG**：这有**268**个不同的卖家名称。'
- en: '**Address**: This has **13378** distinct values, but we have **13580** samples,
    so there seem to be multiple places with the same address. Still, we have an extreme
    amount of variety here, which makes this feature quite unimportant.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Address**：这有 **13378** 个不同的值，但我们有 **13580** 个样本，所以似乎有多个地址相同的地方。尽管如此，我们在这里有极端的多样性，这使得这个特征相当不重要。'
- en: '**Regionname**: This has **8** distinct values – that is, the regions of Melbourne.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Regionname**：这有 **8** 个不同的值——即墨尔本的区域。'
- en: '**Suburb**: This has **314** distinct values – that is, the suburbs of Melbourne.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Suburb**：这有 **314** 个不同的值——即墨尔本的郊区。'
- en: '**CouncilArea**: This has **33** distinct values and is the only categorical
    feature with missing values.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CouncilArea**：这有 **33** 个不同的值，并且是唯一具有缺失值的类别特征。'
- en: At this point, we have found some interesting information and some leads that
    show us where we need to have a look in the next phase. For now, let's drill down
    into the content of the features and do some initial dataset cleaning.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经找到了一些有趣的信息和一些线索，表明我们下一步需要查看的地方。现在，让我们深入到特征的内容并进行一些初步的数据集清理。
- en: 'Let''s start by removing some of the not so important features:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从删除一些不太重要的特征开始：
- en: '[PRE4]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As you can see, we stick with our original DataFrame, called `raw_df`, and create
    a new one called `df`. By doing this, we can add removed features at any time.
    Every row in a DataFrame has an index, so even if we filter out the rows, we can
    still match the original values.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们保留了原始的 DataFrame，称为 `raw_df`，并创建了一个新的 DataFrame，称为 `df`。通过这样做，我们可以在任何时间添加已删除的特征。DataFrame
    中的每一行都有一个索引，因此即使我们过滤掉行，我们仍然可以匹配原始值。
- en: 'Next, we will rename some columns to increase our understanding of them:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将重命名一些列以增加我们对它们的理解：
- en: '[PRE5]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'At this point, it might be a good idea to look for duplicates. Let''s run the
    following code snippet to find duplicates:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一点上，寻找重复项可能是个好主意。让我们运行以下代码片段来查找重复项：
- en: '[PRE6]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Setting `keep` to `False` will show each row that has a duplicate. Here, we
    can see that two of the rows are the same. We can look at them by using the following
    command:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `keep` 设置为 `False` 将显示每个具有重复项的行。在这里，我们可以看到有两行是相同的。我们可以通过以下命令查看它们：
- en: '[PRE7]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As you can see, these denote the same entry. So, let''s remove one of them
    using the following command:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这些表示相同的条目。所以，让我们使用以下命令删除其中一个：
- en: '[PRE8]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As this is just one sample, we can drop it by its row index. Normally, operations
    like these just return a new DataFrame, but in a lot of operations, we can use
    an attribute called `inplace` to directly overwrite the current DataFrame.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这只是一个示例，我们可以通过其行索引来删除它。通常，这类操作会返回一个新的 DataFrame，但在许多操作中，我们可以使用一个名为 `inplace`
    的属性来直接覆盖当前的 DataFrame。
- en: 'Now, let''s look at the categorical features that seem to have missing categories,
    starting with `Method`:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们看看似乎有缺失类别的分类特征，从 `Method` 开始：
- en: '[PRE9]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The categories in our datasets are `S`, `SP`, `PI`, `VB`, and `SA`. Judging
    from the list in the data definition, we can see that the only entries in the
    dataset specify where the property was sold and where we know the selling price.
    Someone has already cleaned this for us.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据集中的类别是 `S`、`SP`、`PI`、`VB` 和 `SA`。从数据定义中的列表来看，我们可以看到数据集中只指定了房产的销售地点以及我们知道的销售价格。有人已经为我们清理了这些信息。
- en: 'By looking at `Type`, we can see that single bedrooms, development sites, and
    other residential areas have been removed as well, leaving houses, units, and
    townhouses:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看 `Type`，我们可以看到单卧室、开发用地和其他住宅区域也被删除了，留下了房屋、单元和联排别墅：
- en: '[PRE10]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To make these entries a bit clearer, let''s replace the single letters with
    a full name:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这些条目更清晰，让我们将单个字母替换为全名：
- en: '[PRE11]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, let''s concentrate on the categorical features that contain a lot of entries.
    The following code shows the list of unique values in the column:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们专注于包含大量条目的分类特征。以下代码显示了该列中唯一值的列表：
- en: '[PRE12]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We will get the following result set:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到以下结果集：
- en: '[PRE13]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Here, we can see that there is a category called `None`, which contains our
    missing values, and a category called `Unavailable`. Otherwise, it seems like
    every other entry is very well defined, and there seem to be no duplicate entries
    with the same meaning; they only differ due to typing errors or spaces. Such errors
    are typically denoted as **structural errors**.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到有一个名为 `None` 的类别，其中包含我们的缺失值，还有一个名为 `Unavailable` 的类别。否则，似乎每个其他条目都定义得很好，似乎没有具有相同意义的重复条目；它们只是由于打字错误或空格而有所不同。这类错误通常被称为
    **结构错误**。
- en: By running the same command for the `Suburb` feature, we get a much larger result
    set. At this point, it gets very complicated to see structural errors, so we need
    to take a programmatic approach to check this category. Something such as pattern
    matching or fuzzy matching can be used here, but we will leave this out for now.
    Feel free to look up topics such as **fuzzy matching** and **Levenshtein distance**,
    which can be used to find groups of similar words in the result set.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对 `Suburb` 特征运行相同的命令，我们得到一个更大的结果集。在这个阶段，要看到结构错误变得非常复杂，因此我们需要采取程序化的方法来检查这个类别。这里可以使用诸如模式匹配或模糊匹配之类的技术，但我们现在先不考虑这一点。您可以自由地查找有关**模糊匹配**和**Levenshtein
    距离**的主题，这些可以在结果集中找到相似词组的组。
- en: 'Finally, we are left with one last question we had concerning the relationship
    between postcodes and suburbs and if we could get rid of one of them. So, let''s
    see how many postcodes are targeting more than one suburb:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们剩下最后一个问题，即关于邮编和郊区之间的关系，以及我们是否可以去掉其中一个。那么，让我们看看有多少邮编针对多个郊区：
- en: '[PRE14]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here, we created a new DataFrame that shows us the postcodes and the number
    of assigned suburbs. By searching for the ones that have been mapped to multiple
    suburbs, we can find the respective list. Let''s count them:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了一个新的 DataFrame，显示了邮编和分配的郊区数量。通过搜索那些被映射到多个郊区的邮编，我们可以找到相应的列表。让我们来计数：
- en: '[PRE15]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Here, we can see that 73 out of 198 postcodes refer to multiple suburbs. Nevertheless,
    every suburb has a postcode, so let''s stick with the suburbs and drop the postcodes
    from the DataFrame:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到198个邮编中有73个指向多个郊区。尽管如此，每个郊区都有一个邮编，所以让我们保留郊区，并将邮编从 DataFrame 中删除：
- en: '[PRE16]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This already looks quite nice. As a final step, we could change the data type
    from `float64` to one of the integer types (`int8`, `int16`, `int32`, or `int64`),
    but we do not know enough about the spread of the data points yet and we cannot
    do this for columns with missing values. We'll come back to this later.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这已经看起来相当好了。作为最后一步，我们可以将数据类型从 `float64` 改为整数类型之一（`int8`、`int16`、`int32` 或 `int64`），但我们还不足以了解数据点的分布情况，并且我们无法对有缺失值的列进行此操作。我们稍后再来处理这个问题。
- en: So far, we have done some basic exploration and base pruning of our dataset.
    Now, let's learn more about statistics.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经对数据集进行了一些基本的探索和基础剪枝。现在，让我们更多地了解统计特性。
- en: Running statistical analysis on the dataset
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对数据集进行统计分析
- en: 'It''s time to look at the statistical properties of our numerical features.
    To do so, run the following code snippet:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候查看我们数值特征的统计特性了。为了做到这一点，运行以下代码片段：
- en: '[PRE17]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Here, the `describe()` method will give you a table of typical statistical properties
    for the numeric features of the dataset. `T` will pivot the table, while the `apply()`
    and `lambda()` methods will help format the data points into normal numerical
    notations. Feel free to remove the `apply` methods and look at the difference.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`describe()` 方法将为您提供数据集数值特征的典型统计特性表。`T` 将表进行转置，而 `apply()` 和 `lambda()`
    方法将帮助将数据点格式化为常规数值表示。您可以自由地移除 `apply` 方法并查看差异。
- en: 'The result will show you some information, but we would like to add some more
    statistical values, including the **skew**, the **mode**, and the **number of
    values** in a feature that are equal to the mode, the maximum, and the minimum.
    With the following code, we can realize that:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将显示一些信息，但我们还想添加一些额外的统计值，包括**偏度**、**众数**以及等于众数、最大值和最小值的特征中的值的数量。通过以下代码，我们可以实现这一点：
- en: '[PRE18]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Here, we are creating a bunch of lists and appending the calculated value for
    each column in our base DataFrame to each list. We are also adding a new column
    to our distribution DataFrame, `dist_df`, for each of the property lists that
    we calculated. To ease your understanding of the code, we used Python list objects
    here. You could shorten this code by using another pandas DataFrame, which we
    leave for you as an exercise.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了一系列列表，并将我们基础 DataFrame 中每个列的计算值追加到每个列表中。我们还为每个我们计算过的属性列表添加了一个新列到我们的分布
    DataFrame，`dist_df`。为了便于您理解代码，我们在这里使用了 Python 列对象。您可以通过使用另一个 pandas DataFrame
    来缩短这段代码，这留作您的练习。
- en: 'You should see an output similar to the following after running the preceding
    code:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码后，你应该看到以下类似的输出：
- en: '![](img/B17928_05_09.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17928_05_09.jpg)'
- en: Figure 5.9 – Statistical properties of the Melbourne Housing dataset
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9 – 墨尔本住房数据集的统计特性
- en: 'Let''s see what we can deduct for each feature by looking at this table:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过查看这个表格来了解每个特征的推断：
- en: '**Price**: This is skewed to the right. Here, we will probably see a few high
    prices, which is not surprising. The highest house price is 9 million.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**价格**: 这个值向右偏斜。在这里，我们可能会看到一些高价，这并不奇怪。最高的房价是900万。'
- en: '**Distance**: This is skewed to the right, probably due to one of the samples
    being 48.1km away from the CBD in Melbourne. Interestingly enough, there are **6**
    samples with **0** distance. Sometimes, 0 is a dummy value, so we should check
    those samples. Judging by the fact that mode **11** has been set **739** times,
    the distance might not be exactly the distance from the city center, but perhaps
    the mean distance of a suburb from the city center. We should figure that out
    as well.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**距离**: 这个值向右偏斜，可能是由于一个样本距离墨尔本CBD有48.1公里。有趣的是，有**6**个样本的距离为**0**。有时，0是一个虚拟值，所以我们应该检查这些样本。根据**11**这个众数被设置了**739**次的事实，距离可能不是城市中心的精确距离，而是郊区到城市中心的平均距离。我们也应该弄清楚这一点。'
- en: '**Bedrooms**: This is skewed to the right due to lots of bedrooms in some places.
    Curiously, there are **16** samples with **0** bedrooms, which needs to be verified.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卧室**: 由于某些地方的卧室很多，这个值向右偏斜。奇怪的是，有**16**个样本的卧室数为**0**，需要核实。'
- en: '**Bathrooms**: This is similar to the distribution of the **Bedrooms** feature,
    with **34** samples having **0** bathrooms, which again is curious.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**浴室**: 这与**卧室**特征的分布相似，有**34**个样本没有浴室，这又很奇怪。'
- en: '**Parking**: This is similar to the distribution of the **Bedrooms** feature.
    There are **1026** samples with no parking spaces, which sounds reasonable.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**停车位**: 这与**卧室**特征的分布相似。有**1026**个样本没有停车位，这听起来是合理的。'
- en: '**Landsize**: This is extremely skewed (**95.24**) to the right. The maximum
    value is **433014**. If we presume we''re using square meters here, there are
    about 43 hectares of land. This isn''t impossible, but this is clearly an outlier
    and would probably distort our modeling.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**土地面积**: 这个值极度向右偏斜（**95.24**）。最大值是**433014**。如果我们假设这里使用的是平方米，那么大约有43公顷的土地。这并不是不可能的，但显然这是一个异常值，可能会扭曲我们的模型。'
- en: '**BuildingArea**: This is extremely skewed to the right due to the maximum
    value of **44515** m2\. This sounds quite improbable, so we may want to remove
    this one. Also, there are **17** samples with **0** m2, which needs to be checked.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**建筑面积**: 由于最大值为**44515**平方米，这个值极度向右偏斜。这听起来相当不可能，所以我们可能想要去掉这个值。此外，还有**17**个样本的面积为**0**平方米，需要检查。'
- en: '**YearBuilt**: This is skewed to the left due to the one building being built
    in **1196**. We may want to discard that one.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**建造年份**: 由于有一个建筑是在**1196**年建造的，这个值向左偏斜。我们可能想要丢弃这个值。'
- en: '**Longitude/Latitude**: These seem to be reasonably well distributed, but curiously
    with the **17** and **21** values being the same, respectively – specifically
    **-37** and **144**. This gives us some idea that the coordinates might not be
    as precise as we may think.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**经度/纬度**: 这些值似乎分布得相当合理，但奇怪的是，**17**和**21**的值分别相同，具体是**-37**和**144**。这让我们有了一些想法，即坐标可能没有我们想象的那么精确。'
- en: '**SuburbPropCount**: This is slightly skewed to the right. We have to analyze
    how helpful this value is.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**郊区房产数量**: 这个值略微向右偏斜。我们必须分析这个值有多有帮助。'
- en: 'Now, let''s think about what relationships we would expect and have a look
    at these between features:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑我们期望的关系，并查看这些特征之间的关系：
- en: '**Rooms with Bathrooms/Bedrooms**: If you have a look at the distribution for
    these, it becomes clear that we are not quite sure what **Rooms** means. The maximum
    for **Rooms** is **10**, while the maximum for **Bedrooms** is **20**. Looking
    at the data definition, we can see that **Bedrooms** was taken from a bunch of
    different sources, so we may have a discrepancy between those data points.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**房间与浴室/卧室**: 如果你看一下这些值的分布，就会变得清楚，我们并不完全清楚**房间**的含义。**房间**的最大值是**10**，而**卧室**的最大值是**20**。查看数据定义，我们可以看到**卧室**是从多个不同来源获取的，所以我们可能在那些数据点之间有差异。'
- en: '**BuildingArea with Rooms/Bathrooms/Bedrooms**: We would expect a positive
    correlation of some sort, but we cannot judge this from the data at hand.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**建筑面积与房间/浴室/卧室**: 我们预计会有某种正相关性，但仅凭现有数据我们无法判断。'
- en: As we can see, we can get some very good insights just from this table alone
    and have a good idea of what to look at next. We will check the **Price** and
    **BuildingArea** features for now, but in reality, we would have to follow all
    these avenues. Feel free to do this on your own and have a look at the supplied
    notebook to get some more ideas.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，仅从这张表格中我们就可以获得一些非常有价值的见解，并对下一步要查看的内容有一个很好的了解。我们现在将检查**价格**和**BuildingArea**特征，但在现实中，我们可能需要遵循所有这些途径。请随意自己尝试，并查看提供的笔记本以获取更多想法。
- en: 'First, let''s look at the `seaborn` or `plotly` library. Read up on how they
    work and differ from each other. For simplicity, we will use `plotly` for now.
    Use the following code to plot a boxplot with a data points distribution shown
    next to it:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看`seaborn`或`plotly`库。了解它们是如何工作的以及它们之间的区别。为了简单起见，我们现在将使用`plotly`。使用以下代码来绘制一个箱线图，并在旁边显示数据点的分布：
- en: '[PRE19]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You should see the following graph:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下图表：
- en: '![Figure 5.10 – Boxplot for the Price target ](img/B17928_05_10.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.10 – 价格目标的箱线图](img/B17928_05_10.jpg)'
- en: Figure 5.10 – Boxplot for the Price target
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10 – 价格目标的箱线图
- en: Hovering over the box will show you the `log` value of the **Price** vector
    and have a look again.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 悬停在箱线图上，你可以看到**价格**向量的`log`值，然后再看一眼。
- en: 'To do this, let''s add a new column to our DataFrame with the `log` value of
    **Price** and run the visualization again:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，让我们在我们的DataFrame中添加一个新的列，包含**价格**的`log`值，并再次运行可视化：
- en: '[PRE20]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This will result in the following graph:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这将得到以下图表：
- en: '![Figure 5.11 – Boxplot for the log (Price) target ](img/B17928_05_11.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.11 – log (价格) 目标的箱线图](img/B17928_05_11.jpg)'
- en: Figure 5.11 – Boxplot for the log (Price) target
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11 – log (价格) 目标的箱线图
- en: Doing this seems to be a good idea as it's distributed better. Feel free to
    check the skew of this distribution.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做似乎是个好主意，因为它的分布更好。请随意检查这个分布的偏斜。
- en: 'Now, let''s look at the **BuildingArea** feature. Once again, let''s create
    a boxplot using the following code:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看**BuildingArea**特征。再次，让我们使用以下代码创建一个箱线图：
- en: '[PRE21]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This will result in the following graph:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这将得到以下图表：
- en: '![Figure 5.12 – Boxplot of the BuildingArea feature ](img/B17928_05_12.jpg)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.12 – BuildingArea特征的箱线图](img/B17928_05_12.jpg)'
- en: Figure 5.12 – Boxplot of the BuildingArea feature
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12 – BuildingArea特征的箱线图
- en: We are greeted by a very distorted boxplot. Hovering over it, we can see **upper
    fence** at **295** m2, while **maximum** is at **44515** m2\. There is one major
    outlier and a bunch of small ones.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到一个非常扭曲的箱线图。悬停在它上面，我们可以看到**上界**在**295**平方米，而**最大值**在**44515**平方米。有一个主要异常值和一些小异常值。
- en: 'Let''s look how many samples are above **295** with the following code:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下代码查看有多少样本高于**295**：
- en: '[PRE22]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The result still shows that there are **353** samples above this threshold.
    Looking at the boxplot, this may thin out rather quickly toward 2,000 m2\. So,
    let''s check the result set for above 2,000 m2 with the following code:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 结果仍然显示有**353**个样本高于此阈值。从箱线图来看，这可能很快就会减少到2,000平方米。因此，让我们使用以下代码检查超过2,000平方米的结果集：
- en: '[PRE23]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This will give us the following output:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '![Figure 5.13 – Top four samples by BuildingArea size ](img/B17928_05_13.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.13 – 按BuildingArea大小排名前四的样本](img/B17928_05_13.jpg)'
- en: Figure 5.13 – Top four samples by BuildingArea size
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.13 – 按BuildingArea大小排名前四的样本
- en: As we can see, the largest property is 48.1 km away from the city center, so
    having a **Landsize** and **BuildingArea** in that range is feasible. However,
    if we want to understand house prices in Melbourne, this may not be that important.
    It is also in the Northern Victoria region and not in the metropolitan regions.
    We could go further here and look at the connection between these specific houses
    outside of the norm in conjunction with other features, but we will leave it at
    this for now.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，最大的房产距离市中心48.1公里，因此在这个范围内的**Landsize**和**BuildingArea**是可行的。然而，如果我们想了解墨尔本的房子价格，这可能并不那么重要。它位于北维多利亚地区，而不是大都市地区。我们可以进一步探讨这些特定房屋与正常情况外的其他特征之间的联系，但我们将就此搁置。
- en: 'Let''s drop the major outlier from our dataset using the following code:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下代码从我们的数据集中删除主要异常值：
- en: '[PRE24]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: As it just contains one sample, we can drop it by row ID.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它只包含一个样本，我们可以通过行ID将其删除。
- en: At this point, we could continue doing this kind of analysis with the rest of
    the features, but we will leave it as an exercise for you to have a deeper look
    at the rest of the features and their statistical dependencies. Now, let's continue
    by looking at what we would do after that.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以继续用其他特征进行此类分析，但我们将把它留给你作为一个练习，以便更深入地了解其他特征及其统计依赖性。现在，让我们继续看看之后我们会做什么。
- en: 'But before we continue, let''s save our dataset to Azure Machine Learning using
    the following function:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，让我们使用以下函数将我们的数据集保存到Azure机器学习：
- en: '[PRE25]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We will continue to do so during this exercise to have different version at
    our disposal later.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将继续这样做，以便以后可以拥有不同的版本。
- en: Finding and handling missing values
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 寻找和处理缺失值
- en: Our next order of business is to handle the missing values in the dataset. We
    can use a very nice extension called `missingno` to get some interesting visualizations
    of our missing values.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来的任务是处理数据集中的缺失值。我们可以使用一个非常好的扩展`missingno`来获取一些关于缺失值的有趣可视化。
- en: 'But before that, let''s run the following code to see what would happen if
    we removed all the samples with missing values:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 但在那之前，让我们运行以下代码，看看如果我们删除所有具有缺失值的样本会发生什么：
- en: '[PRE26]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As we can see, the resulting DataFrame would contain **6196** samples, which
    would be less than half of the dataset. So, it might be a good idea to handle
    missing values.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，结果DataFrame将包含**6196**个样本，这将少于数据集的一半。所以，处理缺失值可能是一个好主意。
- en: 'Now, run the following code:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，运行以下代码：
- en: '[PRE27]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This will result in the following output:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![Figure 5.14 – Structural visualization of the DataFrame and its missing values
    ](img/B17928_05_14.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![图5.14 – DataFrame及其缺失值的结构可视化](img/B17928_05_14.jpg)'
- en: Figure 5.14 – Structural visualization of the DataFrame and its missing values
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.14 – DataFrame及其缺失值的结构可视化
- en: As we can see, the **CouncilArea** feature is only missing values in the latter
    samples of the DataFrame, **Parking** is only missing in a very small part in
    the latter samples, and **BuildingArea** and **YearBuilt** are missing throughout
    the DataFrame.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，**CouncilArea**特征在DataFrame的后部样本中只有缺失值，**Parking**在后部样本中只有非常小的一部分缺失，而**BuildingArea**和**YearBuilt**在整个DataFrame中都有缺失。
- en: As we've already learned, we can perform replacement by either inventing a *new
    category* for missing categorical data or replacing them with the *mean value*
    for missing continuous data.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经学到的，我们可以通过为缺失的类别数据发明一个新的类别或用缺失的连续数据的**平均值**来替换它们来进行替换。
- en: 'Let''s start with the `Unavailable`, so let''s look at the samples with this
    category by selecting any sample with that characteristic:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从`Unavailable`开始，看看具有此类别的样本，通过选择具有该特性的任何样本：
- en: '[PRE28]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'As we can see, there is only one entry with this category. It seems to be a
    valid entry; it is just missing the name of the council area. So, let''s replace
    this entry and the missing values with a new category called `Missing` using the
    following code:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，只有一个条目属于这个类别。这似乎是一个有效的条目；它只是缺少议会区域的名字。所以，让我们使用以下代码用一个新的类别`Missing`替换这个条目和缺失值：
- en: '[PRE29]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Checking the unique values in the feature after shows us that there are no
    values in the `None` or `Unavailable` categories anymore:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征之后检查唯一值显示，`None`或`Unavailable`类别中不再有任何值：
- en: '[PRE30]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This is the simplest way to replace features. Since these are council areas
    of Melbourne and every house should be assigned to one, a better idea would be
    to find another dataset that matches suburbs or addresses to council areas and
    do a cross-reference. Feel free to search for one and do this.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 这是替换特征的最简单方法。由于这些是墨尔本的议会区域，每所房子都应该分配到一个区域，所以更好的想法是找到另一个匹配郊区或地址到议会区域的数据库，并进行交叉引用。您可以自由搜索并执行此操作。
- en: 'Continuing with the three continuous features, we can use the following code
    to replace any missing value with the mean of the column and check if there are
    still missing values left afterward:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 继续使用三个连续特征，我们可以使用以下代码将任何缺失值替换为该列的平均值，并在之后检查是否还有剩余的缺失值：
- en: '[PRE31]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The result of the final command shows the mean value we filled, **145.749**.
    Adapt this code to do the same for **YearBuilt** and **Parking**. However, you
    may want to use the *median* rather than the *mean* value for these.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 最终命令的结果显示我们填充的平均值是**145.749**。将此代码修改为对**YearBuilt**和**Parking**执行相同的操作。然而，您可能希望对这些使用**中位数**而不是**平均值**。
- en: For now, this solves the problem with missing values and is, statistically speaking,
    a reasonable approach. However, as we've discussed, this is one of the simplest
    ways to do this. A better way would be to find relationships between features
    and use them to fill in missing values. Instead of just using the mean of the
    entire dataset, we could concentrate on finding a subset of data that has similar
    characteristics as the sample with the missing value. For example, we could find
    a dependency between the number of parking spots on one side and the number of
    rooms in the house or the size of the house on the other side. Then, we could
    define a function that gives us a value for **Parking** depending on these other
    features.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，这解决了缺失值的问题，从统计学的角度来看，这是一种合理的方法。然而，正如我们讨论的那样，这是做这件事的最简单方法之一。更好的方法是在特征之间找到关系，并使用它们来填充缺失值。我们不仅可以使用整个数据集的平均值，还可以集中寻找与缺失值样本具有相似特征的数据子集。例如，我们可以找到一侧停车位数量与房屋内房间数量或房屋大小另一侧之间的依赖关系。然后，我们可以定义一个函数，根据这些其他特征给出
    **Parking** 的值。
- en: So, to handle missing values better, we need to figure out relationships, which
    we will have a look at in the next section.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了更好地处理缺失值，我们需要找出关系，我们将在下一节中查看这些关系。
- en: 'But before that, let''s register this dataset again with this description:
    `Data Cleansing 2 - replaced missing values`.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 但在那之前，让我们再次用这个描述注册这个数据集：`数据清洗 2 - 替换缺失值`。
- en: Calculating correlations and feature importance
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算相关性和特征重要性
- en: So far, we've looked at single features, their content, and their distribution.
    Now, let's look at the relationships between them.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经研究了单个特征、它们的内容以及它们的分布。现在，让我们看看它们之间的关系。
- en: 'Use the following code to produce a correlation matrix between our features
    and targets:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码生成我们特征和目标之间的相关矩阵：
- en: '[PRE32]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The resulting matrix will show you the correlation of 13 of our features, but
    not all of them. If you check the visible ones, you will see that we are missing
    everything of the `object` or `datetime` type.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的矩阵将显示我们 13 个特征的相关性，但不是全部。如果你检查可见的，你会看到我们缺少所有 `object` 或 `datetime` 类型的数据。
- en: 'So, before we analyze the matrix, let''s add the missing features by starting
    to carve out the left-over columns of the `object` type from our DataFrame:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在我们分析矩阵之前，让我们通过从我们的 DataFrame 中开始挖掘剩余的 `object` 类型列来添加缺失的特征：
- en: '[PRE33]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Here, we can see that the remaining columns are `category`, which we will now
    convert our columns into:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到剩余的列是 `category`，我们现在将我们的列转换为：
- en: '[PRE34]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'With that, we have created a DataFrame called `obj_df` with five features of
    the `category` type. Now, let''s assign each category a numeric value. For this,
    we will use the `cat.codes` method and create five new columns in our DataFrame
    with `_cat` as the name extension:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们创建了一个名为 `obj_df` 的 DataFrame，其中包含五个 `category` 类型的特征。现在，让我们为每个类别分配一个数值。为此，我们将使用
    `cat.codes` 方法，并在我们的 DataFrame 中创建五个新列，列名扩展为 `_cat`：
- en: '[PRE35]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Perfect! We have created a DataFrame with encoded categories. We will combine
    these new features with our original DataFrame, `df`, into a new DataFrame called
    `cont_df`:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 完美！我们已经创建了一个包含编码类别的 DataFrame。我们将将这些新特征与我们的原始 DataFrame，`df`，合并到一个新的 DataFrame
    中，称为 `cont_df`：
- en: '[PRE36]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The output of the preceding code shows the data types of all our columns in
    the new dataset. We can still see the `datetime` type and some original columns
    that should be of the `int` type. Let's rectify this before creating the correlation
    matrix again.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出显示了新数据集中所有列的数据类型。我们仍然可以看到 `datetime` 类型和一些应该为 `int` 类型的原始列。在再次创建相关矩阵之前，让我们纠正这个问题。
- en: 'First, let''s create a new column called `Date_Epoch` that consists of an integer
    that denotes the seconds from the epoch ([https://docs.python.org/3/library/time.html](https://docs.python.org/3/library/time.html))
    and drop the original **Date** column:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建一个名为 `Date_Epoch` 的新列，该列包含一个表示自纪元（[https://docs.python.org/3/library/time.html](https://docs.python.org/3/library/time.html)）以来的秒数的整数，并删除原始的
    **Date** 列：
- en: '[PRE37]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: We could also break **Date** apart into a **Month** column and a **Year** column,
    as they may have an impact. Feel free to add them as well.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将 **Date** 分解为 **Month** 列和 **Year** 列，因为它们可能产生影响。请随意添加它们。
- en: 'Now, let''s convert all the `float64` columns into integers, except for the
    ones where float is correct:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将所有 `float64` 列转换为整数，除了那些浮点数是正确的情况：
- en: '[PRE38]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The preceding code shows that our DataFrame is now made up of only numerical
    data types in the most optimal size and format (some features only taking up 8-bits
    of memory per value).
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码显示，我们的 DataFrame 现在由最优化大小和格式的数值数据类型组成（一些特征每个值只占用 8 位内存）。
- en: 'Now, it''s time to run the correlation matrix again. Use the same code that
    we did previously – just replace `df` with our new `cont_df`. The result should
    look as follows:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候再次运行相关性矩阵了。使用我们之前使用的相同代码 – 只需将 `df` 替换为我们的新 `cont_df`。结果应该如下所示：
- en: '![Figure 5.15 – Correlation matrix of all the features and their targets ](img/B17928_05_15.jpg)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.15 – 所有特征及其目标的相关矩阵](img/B17928_05_15.jpg)'
- en: Figure 5.15 – Correlation matrix of all the features and their targets
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.15 – 所有特征及其目标的相关矩阵
- en: 'A strong red color denotes a **positive correlation**, while a strong blue
    color denotes a **negative correlation**. Based on this, we can conclude the following:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 强烈的红色表示**正相关**，而强烈的蓝色表示**负相关**。基于此，我们可以得出以下结论：
- en: '**Rooms** is strongly correlated with **Price**, **Price_log**, **Distance**,
    **Bedrooms**, **Bathrooms**, **Parking**, and **BuildingArea**.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**房间数量** 与 **价格**、**价格对数**、**距离**、**卧室数量**、**浴室数量**、**停车位** 和 **建筑面积** 强烈相关。'
- en: '**Type** is strongly correlated with **Price**, **Price_log**, **Bedrooms**,
    **YearBuilt**, and **Rooms**.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类型** 与 **价格**、**价格对数**、**卧室数量**、**建造年份** 和 **房间数量** 强烈相关。'
- en: '**Price** is strongly correlated with **Rooms**, **Type**, **Bedrooms**, **Bathrooms**,
    **Parking**, and **BuildingArea**.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**价格** 与 **房间数量**、**类型**、**卧室数量**、**浴室数量**、**停车位** 和 **建筑面积** 强烈相关。'
- en: '**Suburb**, **Method**, **Landsize**, and **SuburbPropCount** don''t seem to
    have too much influence in their current state on other features or the target.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**郊区**、**方法**、**土地面积**和**郊区房产数量**似乎在当前状态下对其他特征或目标没有太大的影响。'
- en: Looking at these results, they are not surprising. **Suburb** has too many categories
    to be precise for anything, **Method** shouldn't have too much influence either,
    **Landsize** is probably not the biggest factor, and **SuburbPropCount** may also
    have too much variety. Possible transformations could involve either dropping
    **Suburb** and **SuburbPropCount** or mapping them to a category with much less
    variety.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 观察这些结果，它们并不令人惊讶。**郊区**有太多的类别，无法精确地用于任何事物，**方法**也不应该有太大的影响，**土地面积**可能也不是最大的因素，而**郊区房产数量**可能也有太多的变化。可能的转换包括要么删除**郊区**和**郊区房产数量**，要么将它们映射到一个变化较少的类别。
- en: 'Before we continue, let''s register `cont_df` as a version of the dataset with
    the description: `Data Cleansing 3 - all features converted to numerical values`.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，让我们将 `cont_df` 注册为具有描述：“数据清洗 3 - 所有特征转换为数值类型”的数据集版本。
- en: 'As the final task, let''s double-check what we''ve figured out so far by using
    an `06_dataprep_melbhousing.ipynb` file. There, you will see that we calculated
    the feature importance for the **Price** and **Price_log** targets. The results
    for both are shown here:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后的任务，让我们使用 `06_dataprep_melbhousing.ipynb` 文件来双重检查我们到目前为止所了解的内容。在那里，你会看到我们计算了
    **价格** 和 **价格对数** 目标特征的重要性。两者的结果都显示在这里：
- en: '![Figure 5.16 – Feature importance for Price (left) and Price_log (right) ](img/B17928_05_16.jpg)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.16 – 价格（左侧）和价格对数（右侧）的特征重要性](img/B17928_05_16.jpg)'
- en: Figure 5.16 – Feature importance for Price (left) and Price_log (right)
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.16 – 价格（左侧）和价格对数（右侧）的特征重要性
- en: As we can see, the type of the property clearly influences its price. This influence
    might not look that massive, but be aware, we are looking at logarithmical house
    prices.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，房产类型明显影响其价格。这种影响可能看起来并不那么巨大，但请注意，我们正在查看对数形式的房价。
- en: What we've learned so far matches these results. Looking at the difference between
    the graphs, we can see that adding **logarithmic scaling** to our target variable
    has strengthened the most influential feature. The **Type** feature seems to have
    a strong influence on our target.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 我们到目前为止所了解的内容与这些结果相符。通过观察图表之间的差异，我们可以看到，将**对数缩放**添加到我们的目标变量中，增强了最有影响力的特征。**类型**特征似乎对我们的目标有很强的影响。
- en: 'Let''s end this exercise by looking at this relationship using the following
    code:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下代码来结束这个练习，看看这种关系：
- en: '[PRE39]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The results of this are as follows:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果如下：
- en: '![Figure 5.17 – Correlation between Type and Price_log ](img/B17928_05_17.jpg)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.17 – 类型与价格对数之间的相关性](img/B17928_05_17.jpg)'
- en: Figure 5.17 – Correlation between Type and Price_log
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.17 – 类型与价格对数之间的相关性
- en: With that, we've completed this exercise. We were able to clean up our dataset,
    find some very good initial insights, and find a very strong correlation between
    our target variable and one of the features.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们就完成了这个练习。我们能够清理我们的数据集，发现一些非常好的初步见解，并发现我们的目标变量与一个特征之间有非常强的相关性。
- en: There are a lot of open questions left and we are still at the beginning of
    fully understanding this dataset. As an example, besides the **Price** target,
    we did not look at scaling or normalizing features, another possible requirement
    for certain algorithms.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 还有很多开放性问题，我们仍然处于完全理解这个数据集的初期。例如，除了**价格**目标之外，我们没有查看特征缩放或归一化，这是某些算法的另一个可能要求。
- en: We will continue working with this dataset in [*Chapter 6*](B17928_06_ePub.xhtml#_idTextAnchor102),
    *Feature Engineering and Labeling*. Until then, feel free to drill down into the
    secrets of this dataset or try to use your newfound knowledge on a different dataset.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[*第6章*](B17928_06_ePub.xhtml#_idTextAnchor102)，*特征工程和标记*中继续使用这个数据集。在此之前，请随意深入挖掘这个数据集的秘密，或者尝试将您新获得的知识应用于不同的数据集。
- en: Tracking figures from exploration in Azure Machine Learning
  id: totrans-347
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Azure机器学习中跟踪探索中的图表
- en: During our data exploration, we created a lot of different plots and visuals.
    Let's learn how to track them with Azure Machine Learning so that they are not
    just living in our Jupyter notebook.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据探索过程中，我们创建了大量的不同图表和可视化。让我们学习如何使用Azure机器学习跟踪它们，以便它们不仅仅存在于我们的Jupyter笔记本中。
- en: In [*Chapter 3*](B17928_03_ePub.xhtml#_idTextAnchor054), *Preparing the Azure
    Machine Learning Workspace*, we learned how to track metrics and files for ML
    experiments using Azure Machine Learning. Other important outputs of your data
    transformation and ML scripts are visualizations, figures of data distributions,
    insights about models, and the results. Therefore, Azure Machine Learning provides
    a similar way to track metrics for images, figures, and `matplotlib` references.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第3章*](B17928_03_ePub.xhtml#_idTextAnchor054)，*准备Azure机器学习工作区*中，我们学习了如何使用Azure机器学习跟踪ML实验的指标和文件。您数据转换和ML脚本的其他重要输出包括可视化、数据分布的图表、关于模型的见解以及结果。因此，Azure机器学习提供了一种类似的方式来跟踪图像、图表和`matplotlib`引用的指标。
- en: 'Let''s imagine that we created a `pairplot` of the popular *Iris Flower dataset*
    ([https://archive.ics.uci.edu/ml/datasets/iris](https://archive.ics.uci.edu/ml/datasets/iris))
    using the following code:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们想象一下，我们使用以下代码创建了一个流行的*Iris花卉数据集*（[https://archive.ics.uci.edu/ml/datasets/iris](https://archive.ics.uci.edu/ml/datasets/iris)）的`pairplot`：
- en: '[PRE40]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'With a few lines of code, we can track all the `matplotlib` figures and attach
    them to our experimentation run. To do so, we only have to pass the `matplotlib`
    reference to the `run.log_image()` method and give it an appropriate name. The
    following code shows what this would look like in an experiment:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 通过几行代码，我们可以跟踪所有的`matplotlib`图表并将它们附加到我们的实验运行中。为此，我们只需将`matplotlib`引用传递给`run.log_image()`方法，并给它一个合适的名称。以下代码显示了在实验中这会是什么样子：
- en: '[PRE41]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now, this is the amazing part. By calling the function with the `matplotlib`
    reference, Azure Machine Learning will render the figure, save it, and attach
    it to the experiment run. The following screenshot shows Azure Machine Learning
    studio with the `pairplot` image that we just created and registered attached
    to the run:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这是最令人惊奇的部分。通过调用带有`matplotlib`引用的函数，Azure机器学习将渲染图表，保存它，并将其附加到实验运行中。以下截图显示了Azure机器学习工作室，其中包含了我们刚刚创建并注册的`pairplot`图像：
- en: '![Figure 5.18 – Pairplot tracked and shown in Azure Machine Learning studio
    ](img/B17928_05_18.jpg)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![图5.18 – 在Azure机器学习工作室中跟踪并显示的Pairplot](img/B17928_05_18.jpg)'
- en: Figure 5.18 – Pairplot tracked and shown in Azure Machine Learning studio
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.18 – 在Azure机器学习工作室中跟踪并显示的Pairplot
- en: It seems like a tiny feature, but it is insanely useful in real-world experimentation.
    Get used to automatically generating plots of your data, models, and results and
    attaching them to your run. Whenever you are going through your experiments later,
    you'll have all the visualizations already attached to your run, metrics, and
    configuration.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 这似乎是一个微不足道的功能，但在现实世界的实验中非常有用。习惯于自动生成数据、模型和结果图表，并将它们附加到您的运行中。当您稍后回顾实验时，您将已经将所有可视化附加到您的运行、指标和配置中。
- en: Think about storing regression plots when you're training regression models,
    and confusion matrices and ROC curves when training classification models. Store
    your feature importance when you're training tree-based ensembles and activations
    for neural networks. You can implement this once and add a ton of useful information
    to your data and ML pipelines.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 当您在训练回归模型时，考虑存储回归图；当训练分类模型时，存储混淆矩阵和 ROC 曲线。当您在训练基于树的集成和神经网络的激活时，存储特征重要性。您可以一次性实现这一点，并将大量有用的信息添加到您的数据和机器学习管道中。
- en: Important Note
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: When you're using AutoML and HyperDrive to optimize parameters, pre-processing,
    feature engineering, and model selection, you will get a ton of generated visualizations
    out of the box to help you understand the data, model, and results.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 当您使用 AutoML 和 HyperDrive 优化参数、预处理、特征工程和模型选择时，您将获得大量生成的可视化，以帮助您理解数据、模型和结果。
- en: Now that we know how to store visualizations in the Azure Machine Learning workspace,
    let's learn how to create visuals denoting high-dimensional data.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道了如何在 Azure 机器学习工作区中存储可视化，让我们学习如何创建表示高维数据的可视化。
- en: Understanding dimensional reduction techniques
  id: totrans-362
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解降维技术
- en: 'We looked at a lot of ways to visualize data in the previous sections, but
    high-dimensional data cannot be easily and accurately visualized in two dimensions.
    To achieve this, we need a projection of some sort or an embedding technique to
    embed the feature space in two dimensions. There are many linear and non-linear
    embedding techniques that you can use to produce two-dimensional projections of
    data. The following are the most common ones:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们探讨了多种可视化数据的方法，但高维数据在二维中难以准确可视化。为了实现这一点，我们需要某种类型的投影或嵌入技术来将特征空间嵌入到二维中。您可以使用许多线性和非线性嵌入技术来生成数据的二维投影。以下是最常见的几种：
- en: '**Principal Component Analysis** (**PCA**)'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主成分分析** (**PCA**)'
- en: '**Linear Discriminant Analysis** (**LDA**)'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性判别分析** (**LDA**)'
- en: '**t-Distributed Stochastic Neighbor Embedding** (**t-SNE**)'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**t 分布随机邻域嵌入** (**t-SNE**)'
- en: '**Uniform Manifold Approximation and Projection** (**UMAP**)'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均匀流形近似和投影** (**UMAP**)'
- en: 'The following diagram shows the **LDA** and **t-SNE** embeddings for the 13-dimensional
    *UCI Wine Recognition dataset* ([https://archive.ics.uci.edu/ml/datasets/wine](https://archive.ics.uci.edu/ml/datasets/wine)).
    In the **LDA** embedding, we can see that all the classes should be linearly separable.
    That''s a lot we have learned from using two lines of code to plot the embedding
    before we have even started the model selection or training process:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了 13 维 *UCI 葡萄酒识别数据集*（[https://archive.ics.uci.edu/ml/datasets/wine](https://archive.ics.uci.edu/ml/datasets/wine)）的
    **LDA** 和 **t-SNE** 嵌入。在 **LDA** 嵌入中，我们可以看到所有类别应该是线性可分的。这就是我们在开始模型选择或训练过程之前，仅用两行代码绘制嵌入所学到的东西：
- en: '![Figure 5.19 – Supervised LDA (left) versus unsupervised t-SNE (right)  ](img/B17928_05_19.jpg)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.19 – 监督 LDA（左）与无监督 t-SNE（右）](img/B17928_05_19.jpg)'
- en: Figure 5.19 – Supervised LDA (left) versus unsupervised t-SNE (right)
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.19 – 监督 LDA（左）与无监督 t-SNE（右）
- en: Both the **LDA** and **t-SNE** embeddings are extremely helpful for judging
    the separability of the individual classes and hence the difficulty of your classification
    task. It's always good to assess how well a particular model will perform on your
    data before you start selecting and training a specific algorithm.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '**LDA** 和 **t-SNE** 嵌入对于判断单个类别的可分性和因此分类任务的难度非常有帮助。在开始选择和训练特定算法之前，始终评估特定模型在您的数据上的表现总是好的。'
- en: A great way to get quick insights and a good understanding of your data is to
    visualize it. This will also help you identify clusters in your data and irregularities
    and anomalies – all things that need to be considered in all further data processing.
    But how can you visualize a dataset with 10, 100, or 1,000 feature dimensions?
    And where should you keep the analysis?
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 通过可视化数据来快速获得洞察力和对数据的良好理解是一个很好的方法。这也有助于您识别数据中的聚类、不规则性和异常情况——所有这些都是在所有进一步的数据处理中都需要考虑的因素。但是，您如何可视化具有
    10、100 或 1,000 个特征维度的数据集？您应该在哪里保存分析？
- en: 'In this section, we will answer all these questions. First, we will look into
    the *linear* embedding techniques – **PCA**, an *unsupervised* technique, and
    **LDA**, a *supervised* technique. Then, we will compare both techniques to two
    popular *unsupervised non-linear* embedding techniques, **t-SNE** and **UMAP**,
    the latter of which is a generalized and faster version of t-SNE. Having those
    four techniques in your toolchain will help you understand datasets and create
    meaningful visualizations. We will run all these techniques against datasets of
    increasing complexity, namely the following:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '**The Iris Flower dataset**: This dataset contains three classes and four feature
    dimensions.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The UCI Wine Recognition dataset**: This dataset contains three classes and
    thirteen feature dimensions.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The MNIST Handwritten Digits dataset**: This dataset contains 10 classes
    and 784 feature dimensions (28 x 28-pixel images).'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code to generate the embeddings in this section has been omitted for brevity
    but can be found in the `07_dimensionality_reduction.ipynb` file in this book's
    GitHub repository.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised dimensional reduction using PCA
  id: totrans-378
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most popular linear dimensionality reduction technique is PCA. This is because,
    since it is an unsupervised method, it doesn't need any training labels. PCA embedding
    linearly transforms a dataset so that the resulting projection is uncorrelated.
    The axes of this project are called **principal components** and are computed
    in such a way that each has the next highest variance.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: The principal components are the directions of the highest variance in the data.
    This means that the principal components or Eigenvectors describe the strongest
    direction of the dataset, and the next dimension shows the orthogonal difference
    from the previous direction. In NLP, the main components correspond with high-level
    concepts – in recommendation engines, they correspond with user or item traits.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: PCA can be computed as the Eigenvalue decomposition of the covariance or correlation
    matrix, or on a non-square matrix, by using SVD. PCA and Eigenvalue decomposition
    are often used as data experimentation steps for visualization, whereas SVD is
    often used as dimensionality reduction for sparse datasets; for example, a Bag-of-Words
    model for NLP. We will see how SVD is used in practice in [*Chapter 7*](B17928_07_ePub.xhtml#_idTextAnchor112),
    *Advanced Feature Extraction with NLP*.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: An embedding technique can be used as a form of dimensionality reduction by
    simply removing all but the first *x* components because these first – and largest
    – components explain a certain percentage of the variance of the dataset. Hence,
    we must remove data with low variance to receive a lower-dimensional dataset.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: 'To visualize data after performing PCA in two dimensions (or after performing
    any embedding technique) is to visualize the first two components of the transformed
    dataset – the two largest principal components. The resulting data is rotated
    along the axis – the principal components – scaled, and centered at zero. The
    following diagram shows the results of PCA for the first two datasets. As you
    can see, all the visualizations have the highest variance projected across the
    *x* axis, the second-highest across the *y* axis, and so on:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.20 – PCA for the Iris Flower dataset (left) and the UCI Wine Recognition
    dataset (right) ](img/B17928_05_20.jpg)'
  id: totrans-384
  prefs: []
  type: TYPE_IMG
- en: Figure 5.20 – PCA for the Iris Flower dataset (left) and the UCI Wine Recognition
    dataset (right)
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: Here, we should acknowledge that it is a great first step that we can show all
    these three datasets in only two dimensions, and immediately recognize clusters.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: By projecting the data across the first two principal components and looking
    at the Iris Flower dataset on the left, we can see that all the clusters look
    linearly separable (in two dimensions). However, when we look at the UCI Wine
    Recognition dataset on the right, we can already tell that the clusters are not
    extremely obvious anymore. Now, 13 feature dimensions are projected along with
    the first two principal components, with the highest variance along the *x* axis
    and the second-highest variance along the *y* axis. In PCA, it's typical for the
    cluster's shape to be aligned with the *x* axis because this is how the algorithm
    works.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s run PCA on the most complex dataset – the MNIST Handwritten Digits
    dataset. The result of doing so can be seen in the following diagram:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.21 – PCA results for the MNIST Handwritten Digits dataset ](img/B17928_05_21.jpg)'
  id: totrans-389
  prefs: []
  type: TYPE_IMG
- en: Figure 5.21 – PCA results for the MNIST Handwritten Digits dataset
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: When we look at the much more complex embedding of the MNIST Handwritten Digits
    dataset, we cannot see many clusters besides maybe the cluster for **0** at the
    top. The data is centered across zero and scaled to a range between **-30** and
    **30**. Hence, we can already tell the downsides of PCA – it doesn't consider
    any target labels, which means it doesn't optimize for separable classes.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll look at a technique that takes target labels into
    account.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: Supervised dimensional reduction using LDA
  id: totrans-393
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In LDA, we linearly transform the input data – similar to PCA – and optimize
    the transformation in such a way that the resulting directions have the highest
    inter-cluster variance and the lowest intra-cluster variance. This means that
    the optimization tries to keep samples of the same cluster close to the cluster's
    mean, all while trying to keep the cluster's means as far apart as possible.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: In LDA, we also receive a linear weighted set of directions as a resulting transformation.
    The data is centered around 0 and the directions are ordered by their highest
    inter-cluster variance. Hence, in that sense, LDA is like PCA in that it takes
    target labels into account. Both LDA and PCA have no real tuning knobs, besides
    the number of components we want to keep in the projection and probably a random
    initialization seed.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the results of performing LDA on our first two
    datasets:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.22 – LDA results for the Iris Flower dataset (left) and the UCI
    Wine Recognition dataset (right) ](img/B17928_05_22.jpg)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
- en: Figure 5.22 – LDA results for the Iris Flower dataset (left) and the UCI Wine
    Recognition dataset (right)
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that the data is transformed into two dimensions in such a
    way that the cluster's means are the farthest apart from each other across the
    *x* axis. We can see the same effect for both the Iris Flower and UCI Wine Recognition
    datasets. Another interesting fact that we can observe in both embeddings is that
    the data also becomes linearly separable. We can almost put two straight lines
    in both visualizations to separate the clusters from each other.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: The LDA embedding for both datasets looks quite good in terms of how the data
    is separated by classes. From this, we can be confident that a linear classifier
    for both datasets should achieve great performance – for example, above 95% accuracy.
    While this might be just a ballpark estimate, we already know what to expect from
    a linear classifier with minimal analysis and data preprocessing.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, most real-world embeddings look a lot more like the one shown
    in the following diagram, where we used LDA on the final dataset. This is because
    most real-world datasets often have above 10 or even 100 feature dimensions:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.23 – LDA results for MNIST Handwritten Digits dataset ](img/B17928_05_23.jpg)'
  id: totrans-402
  prefs: []
  type: TYPE_IMG
- en: Figure 5.23 – LDA results for MNIST Handwritten Digits dataset
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can also see a good separation of the cluster containing the **0**
    digits at the bottom and the two clusters of fours and sixes on the left-hand
    side. All the other clusters are drawn on top of each other and don't look to
    be linearly separable.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: Hence, we can tell that a linear classifier won't perform well and will have
    maybe only around 30% accuracy – which is still a lot better than if we were to
    do this randomly. However, we can't tell what performance we would expect from
    a complex non-linear model – not even a non-parametric model such as a decision
    tree-based ensemble classifier.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, LDA performs a lot better than PCA as it takes class labels into
    account. Therefore, labeling data is something to consider when you're optimizing
    results. We will learn how to do efficient labeling in [*Chapter 6*](B17928_06_ePub.xhtml#_idTextAnchor102),
    *Feature Engineering and Labeling*.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: LDA is a great embedding technique for linearly separable datasets with less
    than 100 dimensions and categorical target variables. An extension of LDA is **Quadratic
    Discriminant Analysis** (**QDA**), which performs a non-linear projection using
    combinations of two variables. If you are dealing with continuous target variables,
    you can use a very similar technique called **analysis of variance** (**ANOVA**)
    to model the variance between clusters. The result of ANOVA transformations indicates
    whether the variance in the dataset is attributed to a combination of the variance
    of different components.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen neither PCA nor LDA performed well when separating high-dimensional
    data such as image data. In the Handwritten Digits dataset, we are dealing with
    *only* 784 feature dimensions from 28 x 28-pixel images. Imagine that your dataset
    consists of 1,024 x 1,024-pixel images – your dataset would have more than 1 million
    dimensions. Hence, we need a better embedding technique for very high-dimensional
    datasets.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: Non-linear dimensional reduction using t-SNE
  id: totrans-409
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Projecting high-dimensional datasets into two or three dimensions was extremely
    difficult and cumbersome a couple of years ago. If you wanted to visualize image
    data on a two-dimensional graph, you could use any of the previously discussed
    techniques – if they could compute a result – or try exotic embeddings such as
    self-organizing maps.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: Even though t-SNE was released in a paper in 2008 by Laurence van der Maaten
    and Geoffrey Hinton ([https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf](https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf)),
    it took until 2012 for someone to apply it to a major dataset. It was used by
    the team ranked first in the Merck Viz Kaggle competition – a rather unconventional
    way to apply a great embedding algorithm for the first time. However, since the
    end of that competition, t-SNE has been used regularly in other Kaggle competitions
    and by large companies for embedding high-dimensional datasets with great success.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: t-SNE projects high-dimensional features into a two- or three-dimensional space
    while minimizing the difference of similar points in high-and low-dimensional
    space. Hence, high-dimensional feature vectors that are close to each other are
    likely to be close to each other in the two-dimensional embedding.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows t-SNE applied to the Iris Flower and UCI Wine Recognition
    datasets. As we can see, the complex non-linear embedding doesn''t perform a lot
    better than the simple PCA or LDA techniques. However, its real power is highlighted
    in very large and high-dimensional datasets that contain up to 30 million observations
    of thousands of feature dimensions:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.24 – The t-SNE results for the Iris Flower dataset (left) and  the
    UCI Wine Recognition dataset (right) ](img/B17928_05_24.jpg)'
  id: totrans-414
  prefs: []
  type: TYPE_IMG
- en: Figure 5.24 – The t-SNE results for the Iris Flower dataset (left) and the UCI
    Wine Recognition dataset (right)
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, you can see how t-SNE performs against the MNIST
    dataset:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.25 – The t-SNE results for the MNIST Handwritten Digits dataset
    ](img/B17928_05_25.jpg)'
  id: totrans-417
  prefs: []
  type: TYPE_IMG
- en: Figure 5.25 – The t-SNE results for the MNIST Handwritten Digits dataset
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, t-SNE performs a lot better on the MNIST dataset and effortlessly
    separates the clusters of 10 handwritten digits. This suggests that 99% accuracy
    might be possible.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: 'What is beautiful with this type of visualization is not only that we can see
    that the data is separable, but we can also imagine what the confusion matrix
    will look like when a classifier gets trained on the data, simply by looking at
    the preceding visualization. Here are some observations about the data that we
    can infer from just looking at the embedding:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: 'Replace this bullet list with the following list:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: There are three clusters containing samples of digit 1, where one cluster is
    further away from the mean.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are three clusters containing samples of digit 9, where in a couple of
    cases, some of these samples are very close to the clusters for digit 1 and digit
    7 samples.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a cluster containing samples of digit 3 in the middle, that are close
    to the cluster for digit 8 samples.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a small cluster containing samples of digit 2, that are close to the
    cluster for digit 8 samples.
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The clusters containing samples for digits 3 and 9 are quite close to each other,
    so they may look similar.
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The clusters containing samples for digits 0, 4 and 6 have a very good distance
    from other clusters, suggesting that they are quite separable.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are brilliant insights since you know what to expect and what to look
    for in your data when you're manually exploring samples. It also helps you tune
    your feature engineering to, for example, try to differentiate the images for
    the **1**, **7**, and **9** digits as they will lead to the most misclassifications
    later in modeling.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: Generalizing t-SNE with UMAP
  id: totrans-429
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: UMAP for dimension reduction is an algorithm for general-purpose manifold learning
    and dimension reduction. It is a generalization of t-SNE that's based on Riemannian
    geometry and algebraic topology.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: In general, UMAP provides similar results to t-SNE with a topological approach,
    better scalability of feature dimensions, and faster computation at runtime. Since
    it is faster and performs slightly better in terms of topological structure, it
    is quickly gaining popularity.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at the embeddings for the Iris Flower and UCI Wine Recognition datasets
    again, we will see a similar effect to what we saw with t-SNE. The results are
    shown in the following diagram:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.26 – UMAP results for the Iris Flower dataset (left) and  the UCI
    Wine Recognition dataset (right) ](img/B17928_05_26.jpg)'
  id: totrans-433
  prefs: []
  type: TYPE_IMG
- en: Figure 5.26 – UMAP results for the Iris Flower dataset (left) and the UCI Wine
    Recognition dataset (right)
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: The resulting embeddings look reasonable but they aren't better than the linearly
    separable results of LDA. However, we can't measure computational performance
    by only comparing the results, and that's where UMAP shines.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: 'When it comes to higher-dimensional data, such as the MNIST Handwritten Digits
    dataset, UMAP performs exceptionally well as a two-dimensional embedding technique.
    We can see the results for UMAP on the MNIST Handwritten Digits dataset in the
    following diagram:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.26 – The UMAP results for the MNIST Handwritten Digits dataset ](img/B17928_05_27.jpg)'
  id: totrans-437
  prefs: []
  type: TYPE_IMG
- en: Figure 5.26 – The UMAP results for the MNIST Handwritten Digits dataset
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, UMAP reduces clusters to completely separable entities in the
    embedding, with minimal overlaps and a great distance between the clusters themselves.
    Making similar observations to what we made previously, for example, concerning
    the clusters of the **1** and **9** digits, are still possible, but the clusters
    look a lot more separable.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: 'From these data experimentation and visualization techniques, we would like
    you to take away the following key points:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: Perform PCA to try to analyze Eigenvectors
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform LDA or ANOVA to understand the variance of your data
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform t-SNE or UMAP embedding if you have complex high-dimensional data
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Armed with this knowledge, we can dive right into feature engineering as we
    know which data samples will be easy to handle and which samples will cause high
    misclassification rates in production.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-445
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first two parts of this chapter, you learned what techniques exist for
    you to explore and statistically analyze raw datasets and how to use them hands-on
    on a real-life dataset.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: After that, you learned about the dimensionality reduction techniques you can
    use to visualize high-dimensional datasets. There, you learned about techniques
    that are extremely useful for you to understand your data, its principal components,
    discriminant directions, and separability.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, everything you have learned in this chapter can be performed on
    a compute cluster in your Azure Machine Learning workspace, through which you
    can keep track of all the figures and outputs that are generated.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, using all the knowledge you've gained so far, you will
    dive into the topic of feature engineering, where you learn how to select and
    transform features in datasets to prepare them for ML training. In addition, you
    will have a closer look at labeling and how Azure Machine Learning can help with
    this tedious task.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
