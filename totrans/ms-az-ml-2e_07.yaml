- en: '*Chapter 5*: Performing Data Analysis and Visualization'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned how to bring our datasets to the cloud,
    define data stores in the Azure Machine Learning workspace to access them, and
    register datasets in the Azure Machine Learning dataset registry to have a good
    basis to start data preprocessing from. In this chapter, we will learn how to
    explore this raw data.
  prefs: []
  type: TYPE_NORMAL
- en: First, you will learn about techniques that can help you explore tabular and
    file datasets. We will also talk about how to handle missing values, how to cross-correlate
    features to understand statistical connections between them, and how to bring
    domain knowledge to this process to improve our understanding of the context and
    the quality of our data cleansing. In addition, we will learn how to use ML algorithms
    not for training but for exploring our datasets.
  prefs: []
  type: TYPE_NORMAL
- en: After that, we will apply these methods to a real-life dataset while learning
    how to work with pandas DataFrames and how to visualize the properties of our
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will look at methods that can map high-dimensional data to a low-dimensional
    plane, which will help us see similarities and relationships between data points.
    Additionally, these methods can give us clear hints on how clean our data is and
    how effective the chosen ML algorithms will be on the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding data exploration techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing data analysis on a tabular dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding dimensional reduction techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will use the following Python libraries and versions to
    perform data pre-processing and high-dimensional visualizations:'
  prefs: []
  type: TYPE_NORMAL
- en: '`azureml-sdk 1.34.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`azureml-widgets 1.34.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`azureml-dataprep 2.20.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pandas 1.3.2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy 1.19.5`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scikit-learn 0.24.2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seaborn 0.11.2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`plotly 5.3.1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`umap_learn 0.5.1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`statsmodels 0.13.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`missingno 0.5.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similar to previous chapters, you can execute this code using either a local
    Python interpreter or a notebook environment hosted in Azure Machine Learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'All code examples in this chapter can be found in the GitHub repository for
    this book: [https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter05](https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter05).'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding data exploration techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Descriptive data exploration is, without a doubt, one of the most important
    steps in an ML project. If you want to clean data and build derived features or
    select an ML algorithm to predict a target variable in your dataset, then you
    need to understand your data first. Your data will define many of the necessary
    cleaning and preprocessing steps; it will define which algorithms you can choose,
    and it will ultimately define the performance of your predictive model.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, data exploration should be considered an important analytical step to
    understanding whether your data is informative enough to build an ML model in
    the first place. By analytical step, we mean that the exploration should be done
    as a structured analytical process rather than a set of experimental tasks. Therefore,
    we will go through a checklist of data exploration tasks that you can perform
    as an initial step in every ML project â€“ before you start any data cleaning, preprocessing,
    feature engineering, or model selection.
  prefs: []
  type: TYPE_NORMAL
- en: The possible tasks we can perform are tied to the type of dataset we are working
    with. A lot of datasets will come in the form of tabular data, which means we
    have either continuous or categorical features defined for each instance of the
    dataset. These datasets can be visualized as a table, and we can perform basic
    and complex mathematical operations on them. The other general type of dataset
    we may encounter will come in the form of media files. This includes images, videos,
    sound files, documents, and anything else that is not made up of data points that
    you could fit into a table structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'To represent these different types of datasets, Azure Machine Learning gives
    us the option to save our data in one of the following objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**TabularDataset:** This class offers methods for performing basic transformations
    on tabular data and converting them into known formats such as pandas ([https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.tabulardataset](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.tabulardataset)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FileDataset:** This class primarily offers filtering methods on file metadata
    ([https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.filedataset](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.filedataset)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both types of dataset objects can be registered to the Azure Machine Learning
    Dataset Registry for further use after preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: Judging only by the methods that are available in those two classes, it becomes
    clear that the possible tasks and operations we can perform differ greatly between
    tabular datasets and file datasets. In the next few sections, we will look at
    both types and how we can prepare them to influence the result of our ML model.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring and analyzing tabular datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A tabular dataset allows us to utilize the full spectrum of mathematical and
    statistical functions to analyze and transform our dataset, but in most cases,
    we do not have the time or resources to randomly run every dataset through all
    the possible techniques in our arsenal.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right methods does not only involve having experience in analyzing
    a lot of different datasets but also subject matter expertise of the domain we
    are working in. There are areas where everyone has some general expertise (think
    the influencing factors of house prices, for example), but then there are a lot
    of areas where specialized knowledge is needed to understand the data at hand.
    Imagine that you want to increase the yield of a blast furnace creating steel.
    In such a scenario, to understand the data, you need to have intimate knowledge
    of the chemical processes in the furnace, or you need a **subject matter expert**
    to support you. In every step of exploration and analysis, we need to apply domain
    knowledge to interpret the result and relationships we see.
  prefs: []
  type: TYPE_NORMAL
- en: Besides understanding the domain, we also need to understand the features in
    the datasets and their targets or labels. Imagine having a dataset made up of
    features of houses in a certain city but without their market prices. To predict
    house prices, we would need labels or target values for the price of each house.
    On the other hand, if we were to predict if an email is spam or not and we have
    a dataset that contains a bunch of emails containing a lot of metadata, this might
    be good enough to train a model through unsupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, to get a good understanding of the dataset, we need to thoroughly
    explore its content and get as many insights as possible on the features and the
    possible target to make good decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Please keep in mind that not only the feature dimensions but also the target
    variable needs to be preprocessed and analyzed thoroughly.
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve this, we will start by looking at the following aspects of every
    feature and target vector in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '`datetime`, `string`, `int`, `object`)? Do we need to do a data type conversion?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missing data**: Are there any missing entries? How do we handle them?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inconsistent data**: Are date and time stored in different ways? Are the
    same categories written in different ways? Are there different categories with
    the same meaning in the given context?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unique values**: How many unique values exist for a categorical feature?
    Are there too many? Should we create a subset of them?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Statistical properties**: What are the mean, median, and variance of a feature?
    Are there any outliers? What are the minimum and maximum values? What is the most
    common value (mode)?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Statistical distribution**: How are the values distributed? Is there a data
    skew? Would normalization or scaling be useful?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Correlation**: How are different features correlated to each other? Are there
    features containing similar information that could be omitted? How much are my
    features correlated with the target?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing each dimension of a dataset with more than 100 feature dimensions
    is an extremely time-consuming task. However, instead of randomly exploring feature
    dimensions, you can analyze the dimensions ordered by feature importance and significantly
    reduce your time working through the data. Like many other areas of computer science,
    it is good to use an 80/20 principle for the initial data exploration, which means
    using only 20% of the features to achieve 80% of the performance. This sets you
    up for a great start and you can always come back later to add more dimensions
    if needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, it is wise to understand the importance of the features for your
    modeling. We can do this by looking at the relationship between features and the
    target variable. There are many ways to do this, some of which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Regression coefficient**: Used in regression'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature importance**: Used in classification'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High error rates for categorical values**: Used in binary classification'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By applying these steps, you can understand the data and gain knowledge about
    the required preprocessing tasks for your data, features, and target variables.
    Along with that, it will give you a good estimate of what difficulties you can
    expect in your prediction task, which is essential for judging the required algorithms
    and validation strategies. You will also gain insight into what possible feature
    engineering methods could be applied to your dataset and have a better understanding
    of how to select a good error metric.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: You can use a representative subset of the data and extrapolate your hypothesis
    and insights to the whole dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Once the data has been uploaded to a storage service in Azure, we can bring
    up a notebook environment and start exploring the data. The goal is to thoroughly
    explore our data in an analytical process to understand the distribution of *each*
    dimension of our data. We will perform some of these steps on a tabular dataset
    in the *Performing data analysis on a tabular dataset* section.
  prefs: []
  type: TYPE_NORMAL
- en: But first, we will look at some of the techniques that we've discussed in more
    detail and take a quick look at file datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Handling missing values and outliers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the first things to look for in a new dataset is **missing values** for
    each feature and target dimension. This will help you gain a deeper understanding
    of the dataset and what actions could be taken to resolve them. It is not uncommon
    to remove missing values or impute them with zeros at the beginning of a project
    â€“ however, this approach bears the risk of not properly analyzing missing values
    in the first place and losing a lot of data points.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Missing values can be disguised as *valid* numeric or categorical values. Typical
    examples are minimum or maximum values, -1, 0, or NaN. Hence, if you find the
    values 32,767 (= 215-1) or 65,535 (= 216-1) appearing multiple times in an integer
    data column, they may well be missing values disguised as the maximum signed or
    unsigned 16-bit integer representation. Always assume that your data contains
    missing values and outliers in different shapes and representations. Your task
    is to uncover, find, and clean them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Any prior knowledge about the data or domain will give you a competitive advantage
    when you''re working with the data. The reason for this is that you will be able
    to understand **missing values**, **outliers**, and **extremes** concerning the
    data and domain, which will help you perform better imputation, cleansing, or
    transformation. As the next step, you should look for these outliers in your data,
    specifically for the absolute number or percentages of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The null values (look for `Null`, `"Null"`, `""`, `NaN`, and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The minimum and maximum values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The most common value (`MODE`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `0` value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any unique values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you have identified these values, you can use different preprocessing techniques
    to impute missing values and normalize or exclude dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The typical options for dealing with missing values are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deletion**: Delete entire rows or columns from the dataset. This can result
    in bias or having insufficient data for training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Missing` for categorical features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Column average**: Fill in the mean, median, or mode value of the entire data
    column or a subset of the column based on relationships with other features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interpolation**: Fill in an interpolated value based on the column''s data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hot-deck imputation**: Fill in the logical previous value from the sorted
    records of the data column (useful in time series datasets).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The typical options for dealing with outliers are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Erroneous observations**: If the value is wrong, drop either the full column
    or replace the outlier with the mean of the column.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Leave as-is**: If it contains important information and if the model does
    not get distorted by it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cap or floor**: Cap or floor the value to a maximum deviation from the mean
    (for example, three standard deviations).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To get more context when choosing the right way to handle missing values and
    outliers, it is useful to statistically analyze the column distribution and correlations.
    We will do this in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating statistical properties and visualizing data distributions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that you know the outliers, you can start exploring the **value distribution**
    of your dataset''s features. This will help you understand which transformation
    and normalization techniques should be applied during data preparation. Some common
    distribution statistics to look for in a continuous variable are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The mean or median value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The minimum and maximum value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The variance and standard deviation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The 25th, 50th (median), and 75th percentiles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data skew
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Common techniques for visualizing these distributions include using **boxplots**,
    **density plots**, or **histograms**. The following screenshot shows these different
    visualization techniques plotted per target class for a multi-class recognition
    dataset. Each method has advantages and disadvantages â€“ boxplots show all the
    relevant metrics while being a bit harder to read, density plots show very smooth
    shapes while hiding some of the outliers, and histograms don''t let you spot the
    median and percentiles easily while giving you a good estimate of the data skew:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 â€“ A boxplot (left), a density plot (middle), and a histogram (right)
    ](img/B17928_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 â€“ A boxplot (left), a density plot (middle), and a histogram (right)
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that only histograms work well for categorical data (both nominal
    and ordinal). However, you could look at the number of values per category. You
    can find the code for creating these plots in the `01_data_distribution.ipynb`
    file in this book's GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another nice way to display the value distribution versus the target rate is
    in a binary classification task. The following diagram shows the **version number**
    of Windows Defender against the malware **detection rate** (for non-touch devices)
    from the *Microsoft Malware detection dataset* ([https://www.kaggle.com/c/microsoft-malware-prediction/data](https://www.kaggle.com/c/microsoft-malware-prediction/data)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 â€“ Version number versus detection rate for Windows Defender  ](img/B17928_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 â€“ Version number versus detection rate for Windows Defender
  prefs: []
  type: TYPE_NORMAL
- en: Many statistical ML algorithms require the data to be normally distributed,
    so it needs to be normalized or standardized. Knowing the data distribution helps
    you decide which transformations need to be applied during data preparation. In
    practice, data often needs to be transformed, scaled, or normalized.
  prefs: []
  type: TYPE_NORMAL
- en: Finding correlated dimensions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another common task in data exploration is looking for correlations in the dataset.
    This will help you dismiss feature dimensions that are highly correlated and thus
    may influence your ML model. In linear regression models, for example, two highly
    correlated independent variables will lead to large coefficients with opposite
    signs that ultimately cancel each other out. A much more stable regression model
    can be found by removing one of the correlated dimensions. Therefore, it is important
    not only to look at correlations between features and targets but also among features.
  prefs: []
  type: TYPE_NORMAL
- en: The `-1` (strongly negatively correlated) to `1` (strongly positively correlated).
    A `0` indicates no linear relationship between two variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows an example of a correlation matrix for the *California
    Housing dataset* ([https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html)),
    consisting of only continuous variables. The correlations range from `-1` to `1`
    and are colored accordingly, where red denotes a negative correlation and blue
    denotes a positive correlation. The last row shows the linear correlation between
    each feature dimension and the target variable (`MedHouseVal`). We can immediately
    tell that there is a correlation between `Longitude` and `Latitude`, between `MedHouseVal`
    and `MedInc`, and between `AveRooms` and `AveBedrms`. All of these relationships
    are relatively unsurprising:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 â€“ Correlation matrix for the California Housing dataset ](img/B17928_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 â€“ Correlation matrix for the California Housing dataset
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code for creating this correlation matrix in the `02_correlation.ipynb`
    file in this book's GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth mentioning that many correlation coefficients can only be between
    numerical values. Ordinal variables can be encoded, for example, using integer
    encoding and can also compute a meaningful correlation coefficient. For nominal
    data, you need to fall back on different methods, such as **CramÃ©r's V** to compute
    the correlation. It is worth noting that the input data doesn't need to be normalized
    (linearly scaled) before you compute the correlation coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring feature and target dependencies for regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once we have analyzed the missing values, data distribution, and correlations,
    we can start analyzing the relationship between the features and the target variable.
    This will give us a good indication of the difficulty of the prediction problem
    and, hence, the expected baseline performance, which is essential for prioritizing
    feature engineering efforts and choosing an appropriate ML model. Another great
    benefit of measuring this dependency is ranking the feature dimensions by their
    impact on the target variable, which you can use as a priority list for data exploration
    and preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: In a regression task, the target variable is numerical or ordinal. Therefore,
    we can compute the correlation coefficient between the individual features and
    the target variable to compute the linear dependency between the feature and the
    target. High correlation â€“ that is, a high absolute correlation coefficient â€“
    indicates that a strong linear relationship exists. This gives us a great place
    to start exploring further. However, in many practical problems, it is rare to
    see a high (linear) correlation between the feature and target variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also visualize this dependency between the feature and the target variable
    using a **scatter plot** or **regression plot**. The following diagram shows a
    regression plot between the average number of rooms per dwelling (**RM**) and
    the median value of owner-occupied homes (**MEDV**) from the *Boston Housing dataset*.
    If the regression line is at 45 degrees, then we have a perfect linear correlation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4 â€“ Scatter plot with a regression line between the feature and
    the target  ](img/B17928_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 â€“ Scatter plot with a regression line between the feature and the
    target
  prefs: []
  type: TYPE_NORMAL
- en: Another great approach to determining this dependency is to fit a linear or
    logistic regression model to the training data. The resulting model coefficients
    should give you a good explanation of the relationship â€“ the higher the coefficient,
    the larger the linear (for linear regression) or marginal (for logistic regression)
    dependency on the target variable. Hence, sorting by coefficients results in a
    list of features ordered by importance. Depending on the regression type, the
    input data should be normalized or standardized.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows an example of the correlation coefficients (the
    first column) of a fitted **Ordinary Least Squares** (**OLS**) regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5 â€“ The correlation coefficients of an OLS regression model  ](img/B17928_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 â€“ The correlation coefficients of an OLS regression model
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code for creating the plot and coefficients in the `03_regression.ipynb`
    file in this book's GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: While the resulting **R-squared metric** (not shown) may not be good enough
    for a baseline model, the ordering of the coefficients can help us prioritize
    further data exploration, preprocessing, and feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing feature and label dependency for classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a classification task with a multi-class nominal target variable, we can't
    use the regression coefficients without preprocessing the data further. Another
    popular method that works well out of the box is fitting a simple tree-based classifier
    to the training data. Depending on the size of the training data, we could use
    a decision tree or a tree-based ensemble classifier, such as **random forest**
    or **gradient-boosted trees**. Doing so results in a feature importance ranking
    of the feature dimensions according to the chosen split criterion. In the case
    of splitting by entropy, the features would be sorted by *information gain*, which
    would indicate which variables carry the most information about the target.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the feature importance fitted by a tree-based ensemble
    classifier using the entropy criterion from the *UCI Wine Recognition dataset*([https://archive.ics.uci.edu/ml/datasets/wine](https://archive.ics.uci.edu/ml/datasets/wine)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6 â€“ Feature importance of the tree-based ensemble classifier  ](img/B17928_05_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 â€“ Feature importance of the tree-based ensemble classifier
  prefs: []
  type: TYPE_NORMAL
- en: The lines represent variations in the information gain of features between individual
    trees. This output is a great first step to further data analysis and exploration
    in order of feature importance. You can find the code for calculating the feature
    importance and visualizing it in the `04_feature_importance.ipynb` file in this
    book's GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is another popular approach to discovering the separability of your dataset.
    The following screenshot shows a dataset with three classes, where one is linearly
    separable and one isn''t:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7 â€“ A linearly separable dataset (left) versus a non-linearly separable
    dataset (right) ](img/B17928_05_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 â€“ A linearly separable dataset (left) versus a non-linearly separable
    dataset (right)
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code for creating these separability graphs in the `05_separability.ipynb`
    file in this book's GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: By looking at the three clusters and the overlaps between these clusters, you
    can see that having separated clusters means that a trained classification model
    will perform very well on this dataset. On the other hand, when we know that the
    data is not linearly separable, we know that this task will require advanced feature
    engineering and modeling to produce good results.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring and analyzing file datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A dataset that''s made up of media files is a different beast entirely. If
    we think of images, for example, we could present every pixel as a vector of information
    and see this as one feature of the image. But what could we do in terms of exploration
    and data cleaning? Probably not much on single features. Most of the time, what
    we need to do concerns a large group of pixels or the entire image itself. Broadly
    speaking, we could think of the following aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Uniformity**: All the images in the dataset should be the same size. If not,
    they need to be rescaled, which may involve centering pixel values per channel,
    possibly followed by some form of normalization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Augmentation**: This involves diversifying the dataset without taking on
    new data (new images). This is useful if we have a small dataset and typically
    involves horizontal and vertical flipping, cropping, and rotating, among other
    transformations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking at these options, it is clear that we are trying to fix something in
    an image dataset that could have been resolved already to a great extent when
    we took the images in the first place. Therefore, the reality is that when we're
    handling most types of media files, it is paramount to bring higher concentration
    toward taking good training samples for the dataset than to desperately fix them
    in the preprocessing stage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s imagine that we are a manufacturer who wants to take pictures of the
    products they produce passing on a conveyor belt to find defective products and
    discard them. Let''s say that we have production facilities around the globe.
    What would you do to make sure the pictures are taken as uniformly as possible
    while covering a lot of different scenarios? Here are some aspects to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Camera type**: We probably need the same type of camera to be taking pictures
    in the same format all around the globe.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Environmental conditions**: Is the lighting similar in all places? Are the
    temperature and humidity similar in all places? This could influence the electronics
    in the camera.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Positioning**: Is the same angle being used to take the pictures? Can we
    take pictures from vastly different angles to increase variety?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are only some points to consider when you're taking the images.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at another form of file data â€“ sound files. Let's say that we
    want to build a speech-to-text model that converts what we say into written text.
    Such models are, for example, used in voice assistants to map a request to a set
    of actions to perform.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this context, we could use **Fourier transformations**, among other methods,
    to decompose our sound files. However, we may want to think about the samples
    or training data we want to train on and how we can increase the quality of them
    while considering the following aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Recording hardware**: If we have a voice assistant at home, it is probably
    the same microphone for everyone. But what if we build a voice assistant for mobile
    phones? Then, we have vastly different microphones.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Environment**: We probably need recordings of voices in different environments.
    There is certainly a different sound spectrum when we are standing in a tram compared
    to when we are in a recording booth.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pronunciation**: The *ML algorithm* in your brain may have a hard time deciphering
    different pronunciations â€“ especially dialects. How can an actual ML model handle
    this?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are just some points to consider when you're handling sound files. Regarding
    pronunciation, if you look at **Azure Speech Services**, you will soon realize
    that two models are running in the background â€“ one for the acoustic and one for
    the language. Look at the requirements for samples when building a custom model
    ([https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/how-to-custom-speech-test-and-train](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/how-to-custom-speech-test-and-train))
    as this can give you a good idea of what is required when you're building such
    a model from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, for file datasets, we do not have as many options to statistically
    eliminate problems, so we should concentrate on taking good and clean samples
    that simulate the kind of realistic environment we would get when the model is
    running in production.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have familiarized ourselves with the methods to explore and analyze
    different types of datasets, let's try this out on a real tabular dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Performing data analysis on a tabular dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you haven't followed the steps in [*Chapter 4*](B17928_04_ePub.xhtml#_idTextAnchor071),
    *Ingesting Data and Managing Datasets*, to download the snapshot of the *Melbourne
    Housing dataset* from `melb_data.csv`, in the `mlfiles` container in your storage
    account and have this connected to a datastore called `mldemoblob` in your Azure
    Machine Learning workspace.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will explore the dataset, do some basic statistical
    analysis, find missing values and outliers, find correlations between features,
    and take an initial measurement of feature importance while utilizing a random
    forest model, as we saw in the *Visualizing feature and label dependency for classification*
    section of this chapter. You can either create a new Jupyter notebook and follow
    along with this book or open the `06_ dataprep_melbhousing.ipynb` file in the
    GitHub repository for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the steps we will perform now are not exhaustive. As shown on the
    web page for the dataset, we have 21 features to work with. So, to be thorough,
    you will have to analyze each.
  prefs: []
  type: TYPE_NORMAL
- en: This section should give you a good understanding of the types of tasks you
    can perform, but we will leave a lot of questions open for you to find answers
    for. If you need some inspiration for that, have a look at this dataset on the
    Kaggle website. You will find notebooks from a lot of users trying to analyze
    this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will not completely transform the actual data at this point as we
    will come back to this problem in [*Chapter 6*](B17928_06_ePub.xhtml#_idTextAnchor102),
    *Feature Engineering and Labeling*, where we will learn how to select features
    and create new ones based on the statistical analysis and knowledge we will gain
    through the upcoming process.
  prefs: []
  type: TYPE_NORMAL
- en: Initial exploration and cleansing of the Melbourne Housing dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will load the data from a data store that is registered
    in Azure Machine Learning and look at its content. After that, we will start doing
    some basic cleaning regarding the raw data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the following packages through Python PIP either separately or using
    the requirements file you can find in this book''s GitHub repository: `pandas`,
    `seaborn`, `plotly`, `scikit-learn`, `numpy`, `missingno`, `umap-learn`, and `statsmodels`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new Jupyter notebook or follow along in the one mentioned previously.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Connect to your ML workspace through the configuration file, as we learned previously.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the following code to pull the dataset to your local computer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we're retrieving the data from your defined ML data store, `yourname`,
    and loading the dataset into a tabular dataset object. Adapt the path and name
    of the file in the second to last line, depending on your folder structure in
    your data store.
  prefs: []
  type: TYPE_NORMAL
- en: 'The methods that are available on a tabular dataset object are not as abundant
    as they are for a pandas DataFrame. So, let''s transform it into a pandas DataFrame
    and have our first look at the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `pd.set_option()` method gives you access to the general settings for pandas
    operations. In this case, we want all the columns and rows to be shown and not
    truncated in the visualization. You can set this to whatever value works for you.
  prefs: []
  type: TYPE_NORMAL
- en: The `head()` function will give you a first look at the first five rows of the
    dataset. Have a look at them.
  prefs: []
  type: TYPE_NORMAL
- en: You will see a bunch of features that make a lot of sense, such as `Suburb`,
    `Address`, and `Bathroom`. But some others might not be so clear, such as `Type`,
    `Method`, or `Distance`.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, as with any dataset, there is some form of data definition for the
    fields that are supplied with it. Have a look at the website of the datasets to
    find them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we''ve looked at the definition, let''s look at the so-called shape
    of the datasets, which will show us how many columns (features and labels) and
    how many rows (samples) the dataset contains:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding command shows us a dataset with 13,580 samples and 21 features/labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, run the following code so that we can look at the number of unique
    values, the number of missing values, and the data type of each feature:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After running the preceding code, you should see something similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8 â€“ Melbourne Housing dataset feature overview ](img/B17928_05_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 â€“ Melbourne Housing dataset feature overview
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at this table, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: Four features seem to have missing values (**BuildingArea**, **YearBuilt**,
    **CouncilArea**, and **Car**).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A lot of numeric values (such as `float64` type. This is not necessarily a problem,
    but it's a waste of space since each probably fits into `int8`, `int16`, or `int32`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are seven features of the `object` type, all of which are probably string
    values. We'll look at them in more detail shortly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a feature called **Price**, which is probably a good label/target for
    supervised learning, such as classification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a feature named **Postcode** and a feature named **Suburb**. We may
    not need both. Judging by the unique values, **Suburb** seems to be more granular.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a feature called **Address** and a feature called **SellerG**. Even
    though the seller of a property may have some influence on the price, we can drop
    them for now for simplicity. The same goes for addresses as they are extremely
    precise. Nearly every sample has a unique address.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By looking at the seven features of the `object` type, we can see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Type**: This has **3** distinct values; our data definition shows **6**.
    We need to check this discrepancy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Method**: This has **5** distinct values; our data definition shows **11**.
    We need to check this as well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SellerG**: This has **268** distinct seller names.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Address**: This has **13378** distinct values, but we have **13580** samples,
    so there seem to be multiple places with the same address. Still, we have an extreme
    amount of variety here, which makes this feature quite unimportant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regionname**: This has **8** distinct values â€“ that is, the regions of Melbourne.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Suburb**: This has **314** distinct values â€“ that is, the suburbs of Melbourne.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CouncilArea**: This has **33** distinct values and is the only categorical
    feature with missing values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At this point, we have found some interesting information and some leads that
    show us where we need to have a look in the next phase. For now, let's drill down
    into the content of the features and do some initial dataset cleaning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by removing some of the not so important features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, we stick with our original DataFrame, called `raw_df`, and create
    a new one called `df`. By doing this, we can add removed features at any time.
    Every row in a DataFrame has an index, so even if we filter out the rows, we can
    still match the original values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will rename some columns to increase our understanding of them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At this point, it might be a good idea to look for duplicates. Let''s run the
    following code snippet to find duplicates:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Setting `keep` to `False` will show each row that has a duplicate. Here, we
    can see that two of the rows are the same. We can look at them by using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, these denote the same entry. So, let''s remove one of them
    using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As this is just one sample, we can drop it by its row index. Normally, operations
    like these just return a new DataFrame, but in a lot of operations, we can use
    an attribute called `inplace` to directly overwrite the current DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at the categorical features that seem to have missing categories,
    starting with `Method`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The categories in our datasets are `S`, `SP`, `PI`, `VB`, and `SA`. Judging
    from the list in the data definition, we can see that the only entries in the
    dataset specify where the property was sold and where we know the selling price.
    Someone has already cleaned this for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'By looking at `Type`, we can see that single bedrooms, development sites, and
    other residential areas have been removed as well, leaving houses, units, and
    townhouses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'To make these entries a bit clearer, let''s replace the single letters with
    a full name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s concentrate on the categorical features that contain a lot of entries.
    The following code shows the list of unique values in the column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will get the following result set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that there is a category called `None`, which contains our
    missing values, and a category called `Unavailable`. Otherwise, it seems like
    every other entry is very well defined, and there seem to be no duplicate entries
    with the same meaning; they only differ due to typing errors or spaces. Such errors
    are typically denoted as **structural errors**.
  prefs: []
  type: TYPE_NORMAL
- en: By running the same command for the `Suburb` feature, we get a much larger result
    set. At this point, it gets very complicated to see structural errors, so we need
    to take a programmatic approach to check this category. Something such as pattern
    matching or fuzzy matching can be used here, but we will leave this out for now.
    Feel free to look up topics such as **fuzzy matching** and **Levenshtein distance**,
    which can be used to find groups of similar words in the result set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we are left with one last question we had concerning the relationship
    between postcodes and suburbs and if we could get rid of one of them. So, let''s
    see how many postcodes are targeting more than one suburb:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we created a new DataFrame that shows us the postcodes and the number
    of assigned suburbs. By searching for the ones that have been mapped to multiple
    suburbs, we can find the respective list. Let''s count them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can see that 73 out of 198 postcodes refer to multiple suburbs. Nevertheless,
    every suburb has a postcode, so let''s stick with the suburbs and drop the postcodes
    from the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This already looks quite nice. As a final step, we could change the data type
    from `float64` to one of the integer types (`int8`, `int16`, `int32`, or `int64`),
    but we do not know enough about the spread of the data points yet and we cannot
    do this for columns with missing values. We'll come back to this later.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have done some basic exploration and base pruning of our dataset.
    Now, let's learn more about statistics.
  prefs: []
  type: TYPE_NORMAL
- en: Running statistical analysis on the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It''s time to look at the statistical properties of our numerical features.
    To do so, run the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Here, the `describe()` method will give you a table of typical statistical properties
    for the numeric features of the dataset. `T` will pivot the table, while the `apply()`
    and `lambda()` methods will help format the data points into normal numerical
    notations. Feel free to remove the `apply` methods and look at the difference.
  prefs: []
  type: TYPE_NORMAL
- en: 'The result will show you some information, but we would like to add some more
    statistical values, including the **skew**, the **mode**, and the **number of
    values** in a feature that are equal to the mode, the maximum, and the minimum.
    With the following code, we can realize that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are creating a bunch of lists and appending the calculated value for
    each column in our base DataFrame to each list. We are also adding a new column
    to our distribution DataFrame, `dist_df`, for each of the property lists that
    we calculated. To ease your understanding of the code, we used Python list objects
    here. You could shorten this code by using another pandas DataFrame, which we
    leave for you as an exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should see an output similar to the following after running the preceding
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17928_05_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 â€“ Statistical properties of the Melbourne Housing dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what we can deduct for each feature by looking at this table:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Price**: This is skewed to the right. Here, we will probably see a few high
    prices, which is not surprising. The highest house price is 9 million.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distance**: This is skewed to the right, probably due to one of the samples
    being 48.1km away from the CBD in Melbourne. Interestingly enough, there are **6**
    samples with **0** distance. Sometimes, 0 is a dummy value, so we should check
    those samples. Judging by the fact that mode **11** has been set **739** times,
    the distance might not be exactly the distance from the city center, but perhaps
    the mean distance of a suburb from the city center. We should figure that out
    as well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bedrooms**: This is skewed to the right due to lots of bedrooms in some places.
    Curiously, there are **16** samples with **0** bedrooms, which needs to be verified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bathrooms**: This is similar to the distribution of the **Bedrooms** feature,
    with **34** samples having **0** bathrooms, which again is curious.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parking**: This is similar to the distribution of the **Bedrooms** feature.
    There are **1026** samples with no parking spaces, which sounds reasonable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Landsize**: This is extremely skewed (**95.24**) to the right. The maximum
    value is **433014**. If we presume we''re using square meters here, there are
    about 43 hectares of land. This isn''t impossible, but this is clearly an outlier
    and would probably distort our modeling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BuildingArea**: This is extremely skewed to the right due to the maximum
    value of **44515** m2\. This sounds quite improbable, so we may want to remove
    this one. Also, there are **17** samples with **0** m2, which needs to be checked.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**YearBuilt**: This is skewed to the left due to the one building being built
    in **1196**. We may want to discard that one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Longitude/Latitude**: These seem to be reasonably well distributed, but curiously
    with the **17** and **21** values being the same, respectively â€“ specifically
    **-37** and **144**. This gives us some idea that the coordinates might not be
    as precise as we may think.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SuburbPropCount**: This is slightly skewed to the right. We have to analyze
    how helpful this value is.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let''s think about what relationships we would expect and have a look
    at these between features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rooms with Bathrooms/Bedrooms**: If you have a look at the distribution for
    these, it becomes clear that we are not quite sure what **Rooms** means. The maximum
    for **Rooms** is **10**, while the maximum for **Bedrooms** is **20**. Looking
    at the data definition, we can see that **Bedrooms** was taken from a bunch of
    different sources, so we may have a discrepancy between those data points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BuildingArea with Rooms/Bathrooms/Bedrooms**: We would expect a positive
    correlation of some sort, but we cannot judge this from the data at hand.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we can see, we can get some very good insights just from this table alone
    and have a good idea of what to look at next. We will check the **Price** and
    **BuildingArea** features for now, but in reality, we would have to follow all
    these avenues. Feel free to do this on your own and have a look at the supplied
    notebook to get some more ideas.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s look at the `seaborn` or `plotly` library. Read up on how they
    work and differ from each other. For simplicity, we will use `plotly` for now.
    Use the following code to plot a boxplot with a data points distribution shown
    next to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.10 â€“ Boxplot for the Price target ](img/B17928_05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 â€“ Boxplot for the Price target
  prefs: []
  type: TYPE_NORMAL
- en: Hovering over the box will show you the `log` value of the **Price** vector
    and have a look again.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, let''s add a new column to our DataFrame with the `log` value of
    **Price** and run the visualization again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.11 â€“ Boxplot for the log (Price) target ](img/B17928_05_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 â€“ Boxplot for the log (Price) target
  prefs: []
  type: TYPE_NORMAL
- en: Doing this seems to be a good idea as it's distributed better. Feel free to
    check the skew of this distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at the **BuildingArea** feature. Once again, let''s create
    a boxplot using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.12 â€“ Boxplot of the BuildingArea feature ](img/B17928_05_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 â€“ Boxplot of the BuildingArea feature
  prefs: []
  type: TYPE_NORMAL
- en: We are greeted by a very distorted boxplot. Hovering over it, we can see **upper
    fence** at **295** m2, while **maximum** is at **44515** m2\. There is one major
    outlier and a bunch of small ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look how many samples are above **295** with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The result still shows that there are **353** samples above this threshold.
    Looking at the boxplot, this may thin out rather quickly toward 2,000 m2\. So,
    let''s check the result set for above 2,000 m2 with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.13 â€“ Top four samples by BuildingArea size ](img/B17928_05_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.13 â€“ Top four samples by BuildingArea size
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the largest property is 48.1 km away from the city center, so
    having a **Landsize** and **BuildingArea** in that range is feasible. However,
    if we want to understand house prices in Melbourne, this may not be that important.
    It is also in the Northern Victoria region and not in the metropolitan regions.
    We could go further here and look at the connection between these specific houses
    outside of the norm in conjunction with other features, but we will leave it at
    this for now.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s drop the major outlier from our dataset using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: As it just contains one sample, we can drop it by row ID.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we could continue doing this kind of analysis with the rest of
    the features, but we will leave it as an exercise for you to have a deeper look
    at the rest of the features and their statistical dependencies. Now, let's continue
    by looking at what we would do after that.
  prefs: []
  type: TYPE_NORMAL
- en: 'But before we continue, let''s save our dataset to Azure Machine Learning using
    the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We will continue to do so during this exercise to have different version at
    our disposal later.
  prefs: []
  type: TYPE_NORMAL
- en: Finding and handling missing values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our next order of business is to handle the missing values in the dataset. We
    can use a very nice extension called `missingno` to get some interesting visualizations
    of our missing values.
  prefs: []
  type: TYPE_NORMAL
- en: 'But before that, let''s run the following code to see what would happen if
    we removed all the samples with missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the resulting DataFrame would contain **6196** samples, which
    would be less than half of the dataset. So, it might be a good idea to handle
    missing values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.14 â€“ Structural visualization of the DataFrame and its missing values
    ](img/B17928_05_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.14 â€“ Structural visualization of the DataFrame and its missing values
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the **CouncilArea** feature is only missing values in the latter
    samples of the DataFrame, **Parking** is only missing in a very small part in
    the latter samples, and **BuildingArea** and **YearBuilt** are missing throughout
    the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: As we've already learned, we can perform replacement by either inventing a *new
    category* for missing categorical data or replacing them with the *mean value*
    for missing continuous data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the `Unavailable`, so let''s look at the samples with this
    category by selecting any sample with that characteristic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, there is only one entry with this category. It seems to be a
    valid entry; it is just missing the name of the council area. So, let''s replace
    this entry and the missing values with a new category called `Missing` using the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Checking the unique values in the feature after shows us that there are no
    values in the `None` or `Unavailable` categories anymore:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This is the simplest way to replace features. Since these are council areas
    of Melbourne and every house should be assigned to one, a better idea would be
    to find another dataset that matches suburbs or addresses to council areas and
    do a cross-reference. Feel free to search for one and do this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing with the three continuous features, we can use the following code
    to replace any missing value with the mean of the column and check if there are
    still missing values left afterward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The result of the final command shows the mean value we filled, **145.749**.
    Adapt this code to do the same for **YearBuilt** and **Parking**. However, you
    may want to use the *median* rather than the *mean* value for these.
  prefs: []
  type: TYPE_NORMAL
- en: For now, this solves the problem with missing values and is, statistically speaking,
    a reasonable approach. However, as we've discussed, this is one of the simplest
    ways to do this. A better way would be to find relationships between features
    and use them to fill in missing values. Instead of just using the mean of the
    entire dataset, we could concentrate on finding a subset of data that has similar
    characteristics as the sample with the missing value. For example, we could find
    a dependency between the number of parking spots on one side and the number of
    rooms in the house or the size of the house on the other side. Then, we could
    define a function that gives us a value for **Parking** depending on these other
    features.
  prefs: []
  type: TYPE_NORMAL
- en: So, to handle missing values better, we need to figure out relationships, which
    we will have a look at in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'But before that, let''s register this dataset again with this description:
    `Data Cleansing 2 - replaced missing values`.'
  prefs: []
  type: TYPE_NORMAL
- en: Calculating correlations and feature importance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we've looked at single features, their content, and their distribution.
    Now, let's look at the relationships between them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following code to produce a correlation matrix between our features
    and targets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The resulting matrix will show you the correlation of 13 of our features, but
    not all of them. If you check the visible ones, you will see that we are missing
    everything of the `object` or `datetime` type.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, before we analyze the matrix, let''s add the missing features by starting
    to carve out the left-over columns of the `object` type from our DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can see that the remaining columns are `category`, which we will now
    convert our columns into:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'With that, we have created a DataFrame called `obj_df` with five features of
    the `category` type. Now, let''s assign each category a numeric value. For this,
    we will use the `cat.codes` method and create five new columns in our DataFrame
    with `_cat` as the name extension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Perfect! We have created a DataFrame with encoded categories. We will combine
    these new features with our original DataFrame, `df`, into a new DataFrame called
    `cont_df`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The output of the preceding code shows the data types of all our columns in
    the new dataset. We can still see the `datetime` type and some original columns
    that should be of the `int` type. Let's rectify this before creating the correlation
    matrix again.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s create a new column called `Date_Epoch` that consists of an integer
    that denotes the seconds from the epoch ([https://docs.python.org/3/library/time.html](https://docs.python.org/3/library/time.html))
    and drop the original **Date** column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: We could also break **Date** apart into a **Month** column and a **Year** column,
    as they may have an impact. Feel free to add them as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s convert all the `float64` columns into integers, except for the
    ones where float is correct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code shows that our DataFrame is now made up of only numerical
    data types in the most optimal size and format (some features only taking up 8-bits
    of memory per value).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, it''s time to run the correlation matrix again. Use the same code that
    we did previously â€“ just replace `df` with our new `cont_df`. The result should
    look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.15 â€“ Correlation matrix of all the features and their targets ](img/B17928_05_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.15 â€“ Correlation matrix of all the features and their targets
  prefs: []
  type: TYPE_NORMAL
- en: 'A strong red color denotes a **positive correlation**, while a strong blue
    color denotes a **negative correlation**. Based on this, we can conclude the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rooms** is strongly correlated with **Price**, **Price_log**, **Distance**,
    **Bedrooms**, **Bathrooms**, **Parking**, and **BuildingArea**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Type** is strongly correlated with **Price**, **Price_log**, **Bedrooms**,
    **YearBuilt**, and **Rooms**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Price** is strongly correlated with **Rooms**, **Type**, **Bedrooms**, **Bathrooms**,
    **Parking**, and **BuildingArea**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Suburb**, **Method**, **Landsize**, and **SuburbPropCount** don''t seem to
    have too much influence in their current state on other features or the target.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking at these results, they are not surprising. **Suburb** has too many categories
    to be precise for anything, **Method** shouldn't have too much influence either,
    **Landsize** is probably not the biggest factor, and **SuburbPropCount** may also
    have too much variety. Possible transformations could involve either dropping
    **Suburb** and **SuburbPropCount** or mapping them to a category with much less
    variety.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we continue, let''s register `cont_df` as a version of the dataset with
    the description: `Data Cleansing 3 - all features converted to numerical values`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As the final task, let''s double-check what we''ve figured out so far by using
    an `06_dataprep_melbhousing.ipynb` file. There, you will see that we calculated
    the feature importance for the **Price** and **Price_log** targets. The results
    for both are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.16 â€“ Feature importance for Price (left) and Price_log (right) ](img/B17928_05_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.16 â€“ Feature importance for Price (left) and Price_log (right)
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the type of the property clearly influences its price. This influence
    might not look that massive, but be aware, we are looking at logarithmical house
    prices.
  prefs: []
  type: TYPE_NORMAL
- en: What we've learned so far matches these results. Looking at the difference between
    the graphs, we can see that adding **logarithmic scaling** to our target variable
    has strengthened the most influential feature. The **Type** feature seems to have
    a strong influence on our target.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s end this exercise by looking at this relationship using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The results of this are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.17 â€“ Correlation between Type and Price_log ](img/B17928_05_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.17 â€“ Correlation between Type and Price_log
  prefs: []
  type: TYPE_NORMAL
- en: With that, we've completed this exercise. We were able to clean up our dataset,
    find some very good initial insights, and find a very strong correlation between
    our target variable and one of the features.
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of open questions left and we are still at the beginning of
    fully understanding this dataset. As an example, besides the **Price** target,
    we did not look at scaling or normalizing features, another possible requirement
    for certain algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: We will continue working with this dataset in [*Chapter 6*](B17928_06_ePub.xhtml#_idTextAnchor102),
    *Feature Engineering and Labeling*. Until then, feel free to drill down into the
    secrets of this dataset or try to use your newfound knowledge on a different dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking figures from exploration in Azure Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During our data exploration, we created a lot of different plots and visuals.
    Let's learn how to track them with Azure Machine Learning so that they are not
    just living in our Jupyter notebook.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 3*](B17928_03_ePub.xhtml#_idTextAnchor054), *Preparing the Azure
    Machine Learning Workspace*, we learned how to track metrics and files for ML
    experiments using Azure Machine Learning. Other important outputs of your data
    transformation and ML scripts are visualizations, figures of data distributions,
    insights about models, and the results. Therefore, Azure Machine Learning provides
    a similar way to track metrics for images, figures, and `matplotlib` references.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s imagine that we created a `pairplot` of the popular *Iris Flower dataset*
    ([https://archive.ics.uci.edu/ml/datasets/iris](https://archive.ics.uci.edu/ml/datasets/iris))
    using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'With a few lines of code, we can track all the `matplotlib` figures and attach
    them to our experimentation run. To do so, we only have to pass the `matplotlib`
    reference to the `run.log_image()` method and give it an appropriate name. The
    following code shows what this would look like in an experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, this is the amazing part. By calling the function with the `matplotlib`
    reference, Azure Machine Learning will render the figure, save it, and attach
    it to the experiment run. The following screenshot shows Azure Machine Learning
    studio with the `pairplot` image that we just created and registered attached
    to the run:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.18 â€“ Pairplot tracked and shown in Azure Machine Learning studio
    ](img/B17928_05_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.18 â€“ Pairplot tracked and shown in Azure Machine Learning studio
  prefs: []
  type: TYPE_NORMAL
- en: It seems like a tiny feature, but it is insanely useful in real-world experimentation.
    Get used to automatically generating plots of your data, models, and results and
    attaching them to your run. Whenever you are going through your experiments later,
    you'll have all the visualizations already attached to your run, metrics, and
    configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Think about storing regression plots when you're training regression models,
    and confusion matrices and ROC curves when training classification models. Store
    your feature importance when you're training tree-based ensembles and activations
    for neural networks. You can implement this once and add a ton of useful information
    to your data and ML pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: When you're using AutoML and HyperDrive to optimize parameters, pre-processing,
    feature engineering, and model selection, you will get a ton of generated visualizations
    out of the box to help you understand the data, model, and results.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to store visualizations in the Azure Machine Learning workspace,
    let's learn how to create visuals denoting high-dimensional data.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding dimensional reduction techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We looked at a lot of ways to visualize data in the previous sections, but
    high-dimensional data cannot be easily and accurately visualized in two dimensions.
    To achieve this, we need a projection of some sort or an embedding technique to
    embed the feature space in two dimensions. There are many linear and non-linear
    embedding techniques that you can use to produce two-dimensional projections of
    data. The following are the most common ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Principal Component Analysis** (**PCA**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linear Discriminant Analysis** (**LDA**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**t-Distributed Stochastic Neighbor Embedding** (**t-SNE**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Uniform Manifold Approximation and Projection** (**UMAP**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows the **LDA** and **t-SNE** embeddings for the 13-dimensional
    *UCI Wine Recognition dataset* ([https://archive.ics.uci.edu/ml/datasets/wine](https://archive.ics.uci.edu/ml/datasets/wine)).
    In the **LDA** embedding, we can see that all the classes should be linearly separable.
    That''s a lot we have learned from using two lines of code to plot the embedding
    before we have even started the model selection or training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.19 â€“ Supervised LDA (left) versus unsupervised t-SNE (right)  ](img/B17928_05_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.19 â€“ Supervised LDA (left) versus unsupervised t-SNE (right)
  prefs: []
  type: TYPE_NORMAL
- en: Both the **LDA** and **t-SNE** embeddings are extremely helpful for judging
    the separability of the individual classes and hence the difficulty of your classification
    task. It's always good to assess how well a particular model will perform on your
    data before you start selecting and training a specific algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: A great way to get quick insights and a good understanding of your data is to
    visualize it. This will also help you identify clusters in your data and irregularities
    and anomalies â€“ all things that need to be considered in all further data processing.
    But how can you visualize a dataset with 10, 100, or 1,000 feature dimensions?
    And where should you keep the analysis?
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will answer all these questions. First, we will look into
    the *linear* embedding techniques â€“ **PCA**, an *unsupervised* technique, and
    **LDA**, a *supervised* technique. Then, we will compare both techniques to two
    popular *unsupervised non-linear* embedding techniques, **t-SNE** and **UMAP**,
    the latter of which is a generalized and faster version of t-SNE. Having those
    four techniques in your toolchain will help you understand datasets and create
    meaningful visualizations. We will run all these techniques against datasets of
    increasing complexity, namely the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The Iris Flower dataset**: This dataset contains three classes and four feature
    dimensions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The UCI Wine Recognition dataset**: This dataset contains three classes and
    thirteen feature dimensions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The MNIST Handwritten Digits dataset**: This dataset contains 10 classes
    and 784 feature dimensions (28 x 28-pixel images).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code to generate the embeddings in this section has been omitted for brevity
    but can be found in the `07_dimensionality_reduction.ipynb` file in this book's
    GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised dimensional reduction using PCA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most popular linear dimensionality reduction technique is PCA. This is because,
    since it is an unsupervised method, it doesn't need any training labels. PCA embedding
    linearly transforms a dataset so that the resulting projection is uncorrelated.
    The axes of this project are called **principal components** and are computed
    in such a way that each has the next highest variance.
  prefs: []
  type: TYPE_NORMAL
- en: The principal components are the directions of the highest variance in the data.
    This means that the principal components or Eigenvectors describe the strongest
    direction of the dataset, and the next dimension shows the orthogonal difference
    from the previous direction. In NLP, the main components correspond with high-level
    concepts â€“ in recommendation engines, they correspond with user or item traits.
  prefs: []
  type: TYPE_NORMAL
- en: PCA can be computed as the Eigenvalue decomposition of the covariance or correlation
    matrix, or on a non-square matrix, by using SVD. PCA and Eigenvalue decomposition
    are often used as data experimentation steps for visualization, whereas SVD is
    often used as dimensionality reduction for sparse datasets; for example, a Bag-of-Words
    model for NLP. We will see how SVD is used in practice in [*Chapter 7*](B17928_07_ePub.xhtml#_idTextAnchor112),
    *Advanced Feature Extraction with NLP*.
  prefs: []
  type: TYPE_NORMAL
- en: An embedding technique can be used as a form of dimensionality reduction by
    simply removing all but the first *x* components because these first â€“ and largest
    â€“ components explain a certain percentage of the variance of the dataset. Hence,
    we must remove data with low variance to receive a lower-dimensional dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'To visualize data after performing PCA in two dimensions (or after performing
    any embedding technique) is to visualize the first two components of the transformed
    dataset â€“ the two largest principal components. The resulting data is rotated
    along the axis â€“ the principal components â€“ scaled, and centered at zero. The
    following diagram shows the results of PCA for the first two datasets. As you
    can see, all the visualizations have the highest variance projected across the
    *x* axis, the second-highest across the *y* axis, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.20 â€“ PCA for the Iris Flower dataset (left) and the UCI Wine Recognition
    dataset (right) ](img/B17928_05_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.20 â€“ PCA for the Iris Flower dataset (left) and the UCI Wine Recognition
    dataset (right)
  prefs: []
  type: TYPE_NORMAL
- en: Here, we should acknowledge that it is a great first step that we can show all
    these three datasets in only two dimensions, and immediately recognize clusters.
  prefs: []
  type: TYPE_NORMAL
- en: By projecting the data across the first two principal components and looking
    at the Iris Flower dataset on the left, we can see that all the clusters look
    linearly separable (in two dimensions). However, when we look at the UCI Wine
    Recognition dataset on the right, we can already tell that the clusters are not
    extremely obvious anymore. Now, 13 feature dimensions are projected along with
    the first two principal components, with the highest variance along the *x* axis
    and the second-highest variance along the *y* axis. In PCA, it's typical for the
    cluster's shape to be aligned with the *x* axis because this is how the algorithm
    works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s run PCA on the most complex dataset â€“ the MNIST Handwritten Digits
    dataset. The result of doing so can be seen in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.21 â€“ PCA results for the MNIST Handwritten Digits dataset ](img/B17928_05_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.21 â€“ PCA results for the MNIST Handwritten Digits dataset
  prefs: []
  type: TYPE_NORMAL
- en: When we look at the much more complex embedding of the MNIST Handwritten Digits
    dataset, we cannot see many clusters besides maybe the cluster for **0** at the
    top. The data is centered across zero and scaled to a range between **-30** and
    **30**. Hence, we can already tell the downsides of PCA â€“ it doesn't consider
    any target labels, which means it doesn't optimize for separable classes.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll look at a technique that takes target labels into
    account.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised dimensional reduction using LDA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In LDA, we linearly transform the input data â€“ similar to PCA â€“ and optimize
    the transformation in such a way that the resulting directions have the highest
    inter-cluster variance and the lowest intra-cluster variance. This means that
    the optimization tries to keep samples of the same cluster close to the cluster's
    mean, all while trying to keep the cluster's means as far apart as possible.
  prefs: []
  type: TYPE_NORMAL
- en: In LDA, we also receive a linear weighted set of directions as a resulting transformation.
    The data is centered around 0 and the directions are ordered by their highest
    inter-cluster variance. Hence, in that sense, LDA is like PCA in that it takes
    target labels into account. Both LDA and PCA have no real tuning knobs, besides
    the number of components we want to keep in the projection and probably a random
    initialization seed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the results of performing LDA on our first two
    datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.22 â€“ LDA results for the Iris Flower dataset (left) and the UCI
    Wine Recognition dataset (right) ](img/B17928_05_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.22 â€“ LDA results for the Iris Flower dataset (left) and the UCI Wine
    Recognition dataset (right)
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that the data is transformed into two dimensions in such a
    way that the cluster's means are the farthest apart from each other across the
    *x* axis. We can see the same effect for both the Iris Flower and UCI Wine Recognition
    datasets. Another interesting fact that we can observe in both embeddings is that
    the data also becomes linearly separable. We can almost put two straight lines
    in both visualizations to separate the clusters from each other.
  prefs: []
  type: TYPE_NORMAL
- en: The LDA embedding for both datasets looks quite good in terms of how the data
    is separated by classes. From this, we can be confident that a linear classifier
    for both datasets should achieve great performance â€“ for example, above 95% accuracy.
    While this might be just a ballpark estimate, we already know what to expect from
    a linear classifier with minimal analysis and data preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, most real-world embeddings look a lot more like the one shown
    in the following diagram, where we used LDA on the final dataset. This is because
    most real-world datasets often have above 10 or even 100 feature dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.23 â€“ LDA results for MNIST Handwritten Digits dataset ](img/B17928_05_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.23 â€“ LDA results for MNIST Handwritten Digits dataset
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can also see a good separation of the cluster containing the **0**
    digits at the bottom and the two clusters of fours and sixes on the left-hand
    side. All the other clusters are drawn on top of each other and don't look to
    be linearly separable.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, we can tell that a linear classifier won't perform well and will have
    maybe only around 30% accuracy â€“ which is still a lot better than if we were to
    do this randomly. However, we can't tell what performance we would expect from
    a complex non-linear model â€“ not even a non-parametric model such as a decision
    tree-based ensemble classifier.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, LDA performs a lot better than PCA as it takes class labels into
    account. Therefore, labeling data is something to consider when you're optimizing
    results. We will learn how to do efficient labeling in [*Chapter 6*](B17928_06_ePub.xhtml#_idTextAnchor102),
    *Feature Engineering and Labeling*.
  prefs: []
  type: TYPE_NORMAL
- en: LDA is a great embedding technique for linearly separable datasets with less
    than 100 dimensions and categorical target variables. An extension of LDA is **Quadratic
    Discriminant Analysis** (**QDA**), which performs a non-linear projection using
    combinations of two variables. If you are dealing with continuous target variables,
    you can use a very similar technique called **analysis of variance** (**ANOVA**)
    to model the variance between clusters. The result of ANOVA transformations indicates
    whether the variance in the dataset is attributed to a combination of the variance
    of different components.
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen neither PCA nor LDA performed well when separating high-dimensional
    data such as image data. In the Handwritten Digits dataset, we are dealing with
    *only* 784 feature dimensions from 28 x 28-pixel images. Imagine that your dataset
    consists of 1,024 x 1,024-pixel images â€“ your dataset would have more than 1 million
    dimensions. Hence, we need a better embedding technique for very high-dimensional
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Non-linear dimensional reduction using t-SNE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Projecting high-dimensional datasets into two or three dimensions was extremely
    difficult and cumbersome a couple of years ago. If you wanted to visualize image
    data on a two-dimensional graph, you could use any of the previously discussed
    techniques â€“ if they could compute a result â€“ or try exotic embeddings such as
    self-organizing maps.
  prefs: []
  type: TYPE_NORMAL
- en: Even though t-SNE was released in a paper in 2008 by Laurence van der Maaten
    and Geoffrey Hinton ([https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf](https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf)),
    it took until 2012 for someone to apply it to a major dataset. It was used by
    the team ranked first in the Merck Viz Kaggle competition â€“ a rather unconventional
    way to apply a great embedding algorithm for the first time. However, since the
    end of that competition, t-SNE has been used regularly in other Kaggle competitions
    and by large companies for embedding high-dimensional datasets with great success.
  prefs: []
  type: TYPE_NORMAL
- en: t-SNE projects high-dimensional features into a two- or three-dimensional space
    while minimizing the difference of similar points in high-and low-dimensional
    space. Hence, high-dimensional feature vectors that are close to each other are
    likely to be close to each other in the two-dimensional embedding.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows t-SNE applied to the Iris Flower and UCI Wine Recognition
    datasets. As we can see, the complex non-linear embedding doesn''t perform a lot
    better than the simple PCA or LDA techniques. However, its real power is highlighted
    in very large and high-dimensional datasets that contain up to 30 million observations
    of thousands of feature dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.24 â€“ The t-SNE results for the Iris Flower dataset (left) and  the
    UCI Wine Recognition dataset (right) ](img/B17928_05_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.24 â€“ The t-SNE results for the Iris Flower dataset (left) and the UCI
    Wine Recognition dataset (right)
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, you can see how t-SNE performs against the MNIST
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.25 â€“ The t-SNE results for the MNIST Handwritten Digits dataset
    ](img/B17928_05_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.25 â€“ The t-SNE results for the MNIST Handwritten Digits dataset
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, t-SNE performs a lot better on the MNIST dataset and effortlessly
    separates the clusters of 10 handwritten digits. This suggests that 99% accuracy
    might be possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'What is beautiful with this type of visualization is not only that we can see
    that the data is separable, but we can also imagine what the confusion matrix
    will look like when a classifier gets trained on the data, simply by looking at
    the preceding visualization. Here are some observations about the data that we
    can infer from just looking at the embedding:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Replace this bullet list with the following list:'
  prefs: []
  type: TYPE_NORMAL
- en: There are three clusters containing samples of digit 1, where one cluster is
    further away from the mean.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are three clusters containing samples of digit 9, where in a couple of
    cases, some of these samples are very close to the clusters for digit 1 and digit
    7 samples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a cluster containing samples of digit 3 in the middle, that are close
    to the cluster for digit 8 samples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a small cluster containing samples of digit 2, that are close to the
    cluster for digit 8 samples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The clusters containing samples for digits 3 and 9 are quite close to each other,
    so they may look similar.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The clusters containing samples for digits 0, 4 and 6 have a very good distance
    from other clusters, suggesting that they are quite separable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are brilliant insights since you know what to expect and what to look
    for in your data when you're manually exploring samples. It also helps you tune
    your feature engineering to, for example, try to differentiate the images for
    the **1**, **7**, and **9** digits as they will lead to the most misclassifications
    later in modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Generalizing t-SNE with UMAP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: UMAP for dimension reduction is an algorithm for general-purpose manifold learning
    and dimension reduction. It is a generalization of t-SNE that's based on Riemannian
    geometry and algebraic topology.
  prefs: []
  type: TYPE_NORMAL
- en: In general, UMAP provides similar results to t-SNE with a topological approach,
    better scalability of feature dimensions, and faster computation at runtime. Since
    it is faster and performs slightly better in terms of topological structure, it
    is quickly gaining popularity.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at the embeddings for the Iris Flower and UCI Wine Recognition datasets
    again, we will see a similar effect to what we saw with t-SNE. The results are
    shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.26 â€“ UMAP results for the Iris Flower dataset (left) and  the UCI
    Wine Recognition dataset (right) ](img/B17928_05_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.26 â€“ UMAP results for the Iris Flower dataset (left) and the UCI Wine
    Recognition dataset (right)
  prefs: []
  type: TYPE_NORMAL
- en: The resulting embeddings look reasonable but they aren't better than the linearly
    separable results of LDA. However, we can't measure computational performance
    by only comparing the results, and that's where UMAP shines.
  prefs: []
  type: TYPE_NORMAL
- en: 'When it comes to higher-dimensional data, such as the MNIST Handwritten Digits
    dataset, UMAP performs exceptionally well as a two-dimensional embedding technique.
    We can see the results for UMAP on the MNIST Handwritten Digits dataset in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.26 â€“ The UMAP results for the MNIST Handwritten Digits dataset ](img/B17928_05_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.26 â€“ The UMAP results for the MNIST Handwritten Digits dataset
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, UMAP reduces clusters to completely separable entities in the
    embedding, with minimal overlaps and a great distance between the clusters themselves.
    Making similar observations to what we made previously, for example, concerning
    the clusters of the **1** and **9** digits, are still possible, but the clusters
    look a lot more separable.
  prefs: []
  type: TYPE_NORMAL
- en: 'From these data experimentation and visualization techniques, we would like
    you to take away the following key points:'
  prefs: []
  type: TYPE_NORMAL
- en: Perform PCA to try to analyze Eigenvectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform LDA or ANOVA to understand the variance of your data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform t-SNE or UMAP embedding if you have complex high-dimensional data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Armed with this knowledge, we can dive right into feature engineering as we
    know which data samples will be easy to handle and which samples will cause high
    misclassification rates in production.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first two parts of this chapter, you learned what techniques exist for
    you to explore and statistically analyze raw datasets and how to use them hands-on
    on a real-life dataset.
  prefs: []
  type: TYPE_NORMAL
- en: After that, you learned about the dimensionality reduction techniques you can
    use to visualize high-dimensional datasets. There, you learned about techniques
    that are extremely useful for you to understand your data, its principal components,
    discriminant directions, and separability.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, everything you have learned in this chapter can be performed on
    a compute cluster in your Azure Machine Learning workspace, through which you
    can keep track of all the figures and outputs that are generated.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, using all the knowledge you've gained so far, you will
    dive into the topic of feature engineering, where you learn how to select and
    transform features in datasets to prepare them for ML training. In addition, you
    will have a closer look at labeling and how Azure Machine Learning can help with
    this tedious task.
  prefs: []
  type: TYPE_NORMAL
