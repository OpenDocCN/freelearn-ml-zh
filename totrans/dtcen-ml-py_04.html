<html><head></head><body>
<div id="_idContainer031">
<h1 class="chapter-number" id="_idParaDest-57"><a id="_idTextAnchor056"/><span class="koboSpan" id="kobo.1.1">4</span></h1>
<h1 id="_idParaDest-58"><a id="_idTextAnchor057"/><span class="koboSpan" id="kobo.2.1">Data Labeling Is a Collaborative Process</span></h1>
<p><span class="koboSpan" id="kobo.3.1">As the field of </span><strong class="bold"><span class="koboSpan" id="kobo.4.1">artificial intelligence</span></strong><span class="koboSpan" id="kobo.5.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.6.1">AI</span></strong><span class="koboSpan" id="kobo.7.1">) continues </span><a id="_idIndexMarker104"/><span class="koboSpan" id="kobo.8.1">to evolve, publicly available tools such</span><a id="_idIndexMarker105"/><span class="koboSpan" id="kobo.9.1"> as ChatGPT, </span><strong class="bold"><span class="koboSpan" id="kobo.10.1">Large Language Model Meta AI</span></strong><span class="koboSpan" id="kobo.11.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.12.1">LLaMA</span></strong><span class="koboSpan" id="kobo.13.1">), Bard, Midjourney, and others have set a new benchmark for what's possible to achieve with structured and </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">unstructured data.</span></span></p>
<p><span class="koboSpan" id="kobo.15.1">These models obviously rely on advanced algorithms and massive amounts of data, but many people are unaware that human labeling remains a critical component in their ongoing refinement and advancement. </span><span class="koboSpan" id="kobo.15.2">As an example, ChatGPT’s model infrastructure relies on individuals reviewing and annotating data samples that are then fed back into the model to improve its understanding of natural language </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">and context.</span></span></p>
<p><span class="koboSpan" id="kobo.17.1">In this chapter, we explore how to get the most out of data collection and annotation tasks involving human labelers. </span><span class="koboSpan" id="kobo.17.2">We will cover these </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">general topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.19.1">Why we need </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">human annotators</span></span></li>
<li><span class="koboSpan" id="kobo.21.1">Understanding common challenges arising from human </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">labeling tasks</span></span></li>
<li><span class="koboSpan" id="kobo.23.1">Designing a framework for achieving </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">high-quality labels</span></span></li>
<li><span class="koboSpan" id="kobo.25.1">Best-practice approaches for motivating human annotators, avoiding bias, and dealing with ambiguity </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">in labeling</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.27.1">Firstly, let’s understand why human input is a cornerstone of data-centric </span><strong class="bold"><span class="koboSpan" id="kobo.28.1">machine </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.29.1">learning</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.30.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.31.1">ML</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.32.1">).</span></span></p>
<h1 id="_idParaDest-59"><a id="_idTextAnchor058"/><span class="koboSpan" id="kobo.33.1">Understanding the benefits of diverse human labeling</span></h1>
<p><span class="koboSpan" id="kobo.34.1">Incorporating </span><a id="_idIndexMarker106"/><span class="koboSpan" id="kobo.35.1">a diverse range of individuals and perspectives in the human labeling process offers several advantages. </span><span class="koboSpan" id="kobo.35.2">Humans bring a level of precision and accuracy to data annotation that is difficult for machines to match. </span><span class="koboSpan" id="kobo.35.3">While automated systems may struggle with ambiguity or complexity, human annotators can leverage their understanding and reasoning capabilities to make </span><span class="No-Break"><span class="koboSpan" id="kobo.36.1">informed decisions.</span></span></p>
<p><span class="koboSpan" id="kobo.37.1">Data can change over time, and new scenarios can arise that were not present in the original training data. </span><span class="koboSpan" id="kobo.37.2">Human annotators can adapt to these changes, providing updated annotations that reflect the new realities. </span><span class="koboSpan" id="kobo.37.3">This ensures that ML models remain relevant and effective as the </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">data evolves.</span></span></p>
<p><span class="koboSpan" id="kobo.39.1">Some key strengths of human labelers over programmatic labeling include </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.41.1">Domain expertise</span></strong><span class="koboSpan" id="kobo.42.1">: Labelers with subject-matter expertise can provide valuable insights and annotations that help the model better comprehend specific topics </span><span class="No-Break"><span class="koboSpan" id="kobo.43.1">and domains.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.44.1">Active learning</span></strong><span class="koboSpan" id="kobo.45.1">: This approach involves prioritizing data samples that the model finds ambiguous or challenging, enabling labelers to focus on areas where their input can have the </span><span class="No-Break"><span class="koboSpan" id="kobo.46.1">greatest impact.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.47.1">Diversity of perspectives</span></strong><span class="koboSpan" id="kobo.48.1">: A diverse group of labelers can help mitigate potential biases in the training data, leading to a fairer and more inclusive AI model. </span><span class="koboSpan" id="kobo.48.2">By involving labelers from diverse backgrounds and with varied experiences, a model can be exposed to a broader range of linguistic nuances, cultural contexts, and perspectives, improving its </span><span class="No-Break"><span class="koboSpan" id="kobo.49.1">overall performance.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.50.1">Enhanced contextual understanding</span></strong><span class="koboSpan" id="kobo.51.1">: By drawing on the experiences and knowledge of labelers from different backgrounds, the model can develop a deeper understanding of language nuances, idioms, and cultural references. </span><span class="koboSpan" id="kobo.51.2">Exposure to a wide variety of perspectives and inputs can make the model more resilient and versatile, enabling it to handle a broader range of tasks and </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">scenarios effectively.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.53.1">Quality control (QC)</span></strong><span class="koboSpan" id="kobo.54.1">: Regular audits and evaluations of labeler output can help ensure consistent annotation quality and adherence to guidelines, which is essential for effective </span><span class="No-Break"><span class="koboSpan" id="kobo.55.1">model training.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.56.1">Adherence to ethics</span></strong><span class="koboSpan" id="kobo.57.1">: There may be data or scenarios that shouldn’t end up as model input based on ethical considerations. </span><span class="koboSpan" id="kobo.57.2">In these cases, human labelers play a crucial role in helping models meet ethical standards. </span><span class="koboSpan" id="kobo.57.3">As an example, OpenAI, the company behind ChatGPT, uses human annotators to review, label, and filter out toxic “not safe for </span><span class="No-Break"><span class="koboSpan" id="kobo.58.1">work” data.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.59.1">Although human </span><a id="_idIndexMarker107"/><span class="koboSpan" id="kobo.60.1">annotation is a key part of data-centric model development, humans also add to any ML project new behaviors, biases, and risks that must be managed. </span><span class="koboSpan" id="kobo.60.2">We will now discuss these typical challenges before presenting our framework for </span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">managing them.</span></span></p>
<h1 id="_idParaDest-60"><a id="_idTextAnchor059"/><span class="koboSpan" id="kobo.62.1">Understanding common challenges arising from human labelers</span></h1>
<p><span class="koboSpan" id="kobo.63.1">Before we dive into </span><a id="_idIndexMarker108"/><span class="koboSpan" id="kobo.64.1">the best practices of labeling accuracy and consistency, we will define common challenges we must tackle through our labeling framework. </span><span class="koboSpan" id="kobo.64.2">Labeling inaccuracy and ambiguity are generally triggered by one or more of the following </span><span class="No-Break"><span class="koboSpan" id="kobo.65.1">seven causes:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.66.1">Poor instructions</span></strong><span class="koboSpan" id="kobo.67.1">: Labeling inconsistencies will arise from unclear or insufficient instructions for the data annotation task. </span><span class="koboSpan" id="kobo.67.2">If annotators are not given clear guidelines, they may make assumptions or guesses that lead to inconsistent or </span><span class="No-Break"><span class="koboSpan" id="kobo.68.1">inaccurate annotations.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.69.1">Human bias</span></strong><span class="koboSpan" id="kobo.70.1">: Bias can introduce ambiguity when the data is skewed toward a particular result or outcome, leading to inaccurate interpretations. </span><span class="koboSpan" id="kobo.70.2">A common solution is to assign multiple annotators to label the same data, choosing the most frequently occurring label as the correct one. </span><span class="koboSpan" id="kobo.70.3">However, this aggregation or voting method can sometimes exacerbate bias rather than rectify it. </span><span class="koboSpan" id="kobo.70.4">For instance, if the majority of annotators have a particular bias, their consensus may reflect this bias rather than the </span><span class="No-Break"><span class="koboSpan" id="kobo.71.1">true data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.72.1">Human error</span></strong><span class="koboSpan" id="kobo.73.1">: Annotators are humans, and humans make mistakes. </span><span class="koboSpan" id="kobo.73.2">Even the most well-trained, engaged, and focused annotators are one typo or mouse click away from applying an incorrect label leading to random noise in a dataset. </span><span class="koboSpan" id="kobo.73.3">These mistakes do occur but are unlikely to happen in a systematic way. </span><span class="koboSpan" id="kobo.73.4">Nevertheless, we need to have a way of identifying and correcting these mistakes so that they don’t introduce unnecessary </span><span class="No-Break"><span class="koboSpan" id="kobo.74.1">random noise.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.75.1">Objective versus subjective tasks</span></strong><span class="koboSpan" id="kobo.76.1">: Every task lies on a spectrum from purely </span><a id="_idIndexMarker109"/><span class="koboSpan" id="kobo.77.1">objective (with a single correct answer) to highly subjective (with many potentially correct </span><em class="italic"><span class="koboSpan" id="kobo.78.1">interpretations</span></em><span class="koboSpan" id="kobo.79.1">). </span><span class="koboSpan" id="kobo.79.2">The more subjective a task is, the more ambiguity it tends to introduce into data annotation as different annotators may have different interpretations. </span><span class="koboSpan" id="kobo.79.3">As you will learn in this chapter, even tasks that seem relatively straightforward can contain hidden layers of subjectivity at </span><span class="No-Break"><span class="koboSpan" id="kobo.80.1">the boundary.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.81.1">Difficulty</span></strong><span class="koboSpan" id="kobo.82.1">: Tasks that are inherently complex or hard to comprehend can lead to ambiguity in data annotation. </span><span class="koboSpan" id="kobo.82.2">If a task is too difficult, annotators may struggle to understand or complete it correctly, leading to inconsistent or </span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">inaccurate annotations.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.84.1">Ambiguity</span></strong><span class="koboSpan" id="kobo.85.1">: Some tasks or datasets are naturally ambiguous, meaning there’s room for multiple valid interpretations. </span><span class="koboSpan" id="kobo.85.2">This ambiguity can lead to inconsistencies in data annotation, as different annotators may interpret the same data in </span><span class="No-Break"><span class="koboSpan" id="kobo.86.1">different ways.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.87.1">Static versus variable labeling</span></strong><span class="koboSpan" id="kobo.88.1">: In static labeling, each data point is assigned a single, unchanging label. </span><span class="koboSpan" id="kobo.88.2">In contrast, variable labeling allows labels to change based on context or additional information. </span><span class="koboSpan" id="kobo.88.3">Variable labeling can introduce ambiguity as the same data point may be labeled differently in different contexts. </span><span class="koboSpan" id="kobo.88.4">This form of labeling inconsistency may also arise as annotators become more familiar with a task, which causes them to alter their perception of the definition </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">of labels.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.90.1">We will now introduce our framework for achieving accurate and consistent labels. </span><span class="koboSpan" id="kobo.90.2">It is specifically designed to identify or prevent issues stemming from these seven common </span><span class="No-Break"><span class="koboSpan" id="kobo.91.1">labeling challenges.</span></span></p>
<h1 id="_idParaDest-61"><a id="_idTextAnchor060"/><span class="koboSpan" id="kobo.92.1">Designing a framework for high-quality labels</span></h1>
<p><span class="koboSpan" id="kobo.93.1">Annotations </span><a id="_idIndexMarker110"/><span class="koboSpan" id="kobo.94.1">and reviews done by humans can be labor-intensive and susceptible to human errors and inconsistency. </span><span class="koboSpan" id="kobo.94.2">As such, the goal is to build datasets that are both accurate and consistent, requiring labels to meet accuracy standards as well as ensuring results from different annotators are within the </span><span class="No-Break"><span class="koboSpan" id="kobo.95.1">same range.</span></span></p>
<p><span class="koboSpan" id="kobo.96.1">These goals may seem obvious at first, but in reality, it can be very tricky to get human labelers to conform to the same opinion. </span><span class="koboSpan" id="kobo.96.2">On top of that, we also need to verify that a consensus opinion is not </span><span class="No-Break"><span class="koboSpan" id="kobo.97.1">biased somehow.</span></span></p>
<p><span class="koboSpan" id="kobo.98.1">Our framework for achieving high-quality human annotations consists of six dimensions. </span><span class="koboSpan" id="kobo.98.2">We will briefly summarize these dimensions before delving into a detailed explanation of how to </span><span class="No-Break"><span class="koboSpan" id="kobo.99.1">achieve them:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.100.1">Clear instructions</span></strong><span class="koboSpan" id="kobo.101.1">: To ensure high-quality labels, the instructions for the annotation task must be explicit and unambiguous. </span><span class="koboSpan" id="kobo.101.2">The annotators should have a clear understanding of what is expected of them, including details about the task, the criteria for labeling, and examples of correctly </span><span class="No-Break"><span class="koboSpan" id="kobo.102.1">labeled data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.103.1">Aligned motivations</span></strong><span class="koboSpan" id="kobo.104.1">: The annotators’ motivations should align with the goal of obtaining high-quality labels. </span><span class="koboSpan" id="kobo.104.2">This could involve rewarding accuracy, providing feedback, and creating an environment that encourages meticulous work. </span><span class="koboSpan" id="kobo.104.3">When annotators feel that their work is valuable and recognized, they are more likely to produce </span><span class="No-Break"><span class="koboSpan" id="kobo.105.1">high-quality labels.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.106.1">Subject-matter experts (SMEs)</span></strong><span class="koboSpan" id="kobo.107.1">: Utilizing annotators who are experts in the subject matter can significantly improve the quality of labels. </span><span class="koboSpan" id="kobo.107.2">These individuals possess deep knowledge and understanding of the context behind the data, enabling them to recognize subtleties and nuances that others </span><span class="No-Break"><span class="koboSpan" id="kobo.108.1">may miss.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.109.1">Iterative collaboration</span></strong><span class="koboSpan" id="kobo.110.1">: High-quality labels can be achieved through a process of iterative collaboration. </span><span class="koboSpan" id="kobo.110.2">Annotators should be encouraged to communicate and collaborate, revisiting and refining their labels based on collective feedback </span><span class="No-Break"><span class="koboSpan" id="kobo.111.1">and discussion.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.112.1">Diversity of thinking</span></strong><span class="koboSpan" id="kobo.113.1">: A diverse group of annotators brings different perspectives and interpretations to the task, which can lead to more comprehensive and robust labels. </span><span class="koboSpan" id="kobo.113.2">Diversity of thinking can help uncover blind spots and reduce bias in the </span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">labeling process.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.115.1">Dealing with ambiguity</span></strong><span class="koboSpan" id="kobo.116.1">: As ambiguity is inherent in any data annotation task, training </span><a id="_idIndexMarker111"/><span class="koboSpan" id="kobo.117.1">annotators on how to handle ambiguous cases is essential for achieving high-quality labels. </span><span class="koboSpan" id="kobo.117.2">This could include strategies such as seeking additional information, consulting with peers or supervisors, or following predefined rules for </span><span class="No-Break"><span class="koboSpan" id="kobo.118.1">ambiguous cases.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.119.1">Let’s delve into these six dimensions in detail to understand how to establish an exceptional labeling process. </span><span class="koboSpan" id="kobo.119.2">It all begins by ensuring our instructions to annotators are </span><span class="No-Break"><span class="koboSpan" id="kobo.120.1">crystal clear.</span></span></p>
<h2 id="_idParaDest-62"><a id="_idTextAnchor061"/><span class="koboSpan" id="kobo.121.1">Designing clear instructions</span></h2>
<p><span class="koboSpan" id="kobo.122.1">It may</span><a id="_idIndexMarker112"/><span class="koboSpan" id="kobo.123.1"> seem blatantly obvious that a labeling task should come with clear instructions, but as you will learn throughout this chapter, that is not necessarily an easy task. </span><span class="koboSpan" id="kobo.123.2">Assignments should be clear not just to the people creating them but also, more importantly, to the annotators who will </span><span class="No-Break"><span class="koboSpan" id="kobo.124.1">execute them.</span></span></p>
<p><span class="koboSpan" id="kobo.125.1">This challenge </span><a id="_idIndexMarker113"/><span class="koboSpan" id="kobo.126.1">has three components to it: firstly, instructions should contain specific details for tasks to be carried out reliably, regardless of who is performing them. </span><span class="koboSpan" id="kobo.126.2">Secondly, the instruction design should include ways of picking up instruction issues early and throughout the assignment. </span><span class="koboSpan" id="kobo.126.3">Thirdly, we must make sure our annotators are adequately qualified and motivated for </span><span class="No-Break"><span class="koboSpan" id="kobo.127.1">the task.</span></span></p>
<p><span class="koboSpan" id="kobo.128.1">McInnis et al. </span><span class="koboSpan" id="kobo.128.2">(2016)</span><span class="superscript"><span class="koboSpan" id="kobo.129.1">1</span></span><span class="koboSpan" id="kobo.130.1"> studied the issues that can arise if we don’t manage one or more of these components. </span><span class="koboSpan" id="kobo.130.2">The researchers looked at the impact of unclear instructions and misaligned motivations between requestors and</span><a id="_idIndexMarker114"/><span class="koboSpan" id="kobo.131.1"> annotators using </span><strong class="bold"><span class="koboSpan" id="kobo.132.1">Amazon Mechanical </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.133.1">Turk</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.134.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.135.1">AMT</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.136.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.137.1">They found that seasoned annotators use various tools and techniques to assess the quality of an assignment and the reliability of the requesters who posted it, before taking on new projects. </span><span class="koboSpan" id="kobo.137.2">They do this to pick clearly defined assignments that can be performed with</span><a id="_idIndexMarker115"/><span class="koboSpan" id="kobo.138.1"> no hiccups while avoiding tasks that will pay them little money or impact </span><span class="No-Break"><span class="koboSpan" id="kobo.139.1">their reputation.</span></span></p>
<p><span class="koboSpan" id="kobo.140.1">Basically, good </span><a id="_idIndexMarker116"/><span class="koboSpan" id="kobo.141.1">annotators will circumvent unclear assignments to avoid iterative tasks or unfair rejection of completed work (rejection of an annotator’s work results in non-payment for the work performed). </span><span class="koboSpan" id="kobo.141.2">The authors found that annotators commonly look for the following risk factors in task instructions when deciding whether to take on </span><span class="No-Break"><span class="koboSpan" id="kobo.142.1">an assignment:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.143.1">Flaws in the task or </span><span class="No-Break"><span class="koboSpan" id="kobo.144.1">interface design</span></span></li>
<li><span class="koboSpan" id="kobo.145.1">Unclear </span><span class="No-Break"><span class="koboSpan" id="kobo.146.1">evaluation criteria</span></span></li>
<li><span class="koboSpan" id="kobo.147.1">Unresponsive, arbitrary resolution </span><span class="No-Break"><span class="koboSpan" id="kobo.148.1">of rejections</span></span></li>
<li><span class="koboSpan" id="kobo.149.1">Lack of information </span><span class="No-Break"><span class="koboSpan" id="kobo.150.1">on requesters</span></span></li>
<li><span class="koboSpan" id="kobo.151.1">Inexperienced and </span><span class="No-Break"><span class="koboSpan" id="kobo.152.1">unfamiliar requesters</span></span></li>
<li><span class="koboSpan" id="kobo.153.1">Tasks with </span><span class="No-Break"><span class="koboSpan" id="kobo.154.1">poor return</span></span></li>
<li><span class="koboSpan" id="kobo.155.1">Prioritizing efficiency </span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">over quality</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.157.1">As data professionals, it is our job to provide task instructions that mitigate these seven factors, whether we are using crowdsourced annotators or in-house SMEs. </span><span class="koboSpan" id="kobo.157.2">However, sometimes you won’t know whether your instructions are clear until you use them in a </span><span class="No-Break"><span class="koboSpan" id="kobo.158.1">live setting.</span></span></p>
<p><span class="koboSpan" id="kobo.159.1">Therefore, the question is: how can we best align the understanding of a task between requestors and annotators while also ensuring that we have the right people on </span><span class="No-Break"><span class="koboSpan" id="kobo.160.1">the job?</span></span></p>
<p><span class="koboSpan" id="kobo.161.1">Liu et al. </span><span class="koboSpan" id="kobo.161.2">(2016)</span><span class="superscript"><span class="koboSpan" id="kobo.162.1">2</span></span><span class="koboSpan" id="kobo.163.1"> developed a best-practice method for this purpose called </span><strong class="bold"><span class="koboSpan" id="kobo.164.1">Gated Instruction</span></strong><span class="koboSpan" id="kobo.165.1">. </span><span class="koboSpan" id="kobo.165.2">This</span><a id="_idIndexMarker117"/><span class="koboSpan" id="kobo.166.1"> technique is used for training annotators, aligning the understanding of tasks between requestors and annotators, and identifying </span><span class="No-Break"><span class="koboSpan" id="kobo.167.1">underperforming workers.</span></span></p>
<p><span class="koboSpan" id="kobo.168.1">Gated Instruction is based on the idea that humans learn better when they are given feedback on their performance. </span><span class="koboSpan" id="kobo.168.2">This is done by providing an interactive teaching environment where users can receive feedback on their annotations and adjust their approach accordingly. </span><span class="koboSpan" id="kobo.168.3">The </span><a id="_idIndexMarker118"/><span class="koboSpan" id="kobo.169.1">goal is to create a system that can provide accurate annotations with minimal effort from </span><span class="No-Break"><span class="koboSpan" id="kobo.170.1">the user.</span></span></p>
<p><span class="koboSpan" id="kobo.171.1">The Gated Instruction</span><a id="_idIndexMarker119"/><span class="koboSpan" id="kobo.172.1"> Crowdsourcing Protocol is a simple and generalizable three-phase process designed to ensure quality data annotation. </span><span class="koboSpan" id="kobo.172.2">It includes an interactive tutorial, screening questions, and batches of questions with continued screening. </span><span class="koboSpan" id="kobo.172.3">The authors describe the protocol </span><span class="No-Break"><span class="koboSpan" id="kobo.173.1">as follows:</span></span></p>
<h3><span class="koboSpan" id="kobo.174.1">Phase I – Interactive tutorial</span></h3>
<p><span class="koboSpan" id="kobo.175.1">This phase</span><a id="_idIndexMarker120"/><span class="koboSpan" id="kobo.176.1"> involves a comprehensive tutorial that explains the task </span><span class="No-Break"><span class="koboSpan" id="kobo.177.1">at hand:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.178.1">Workers are given clear definitions of each relation and </span><span class="No-Break"><span class="koboSpan" id="kobo.179.1">tagging criteria.</span></span></li>
<li><span class="koboSpan" id="kobo.180.1">Workers practice by annotating sentences that illustrate </span><span class="No-Break"><span class="koboSpan" id="kobo.181.1">each relation.</span></span></li>
<li><span class="koboSpan" id="kobo.182.1">Immediate feedback is provided after each practice sentence to guide </span><span class="No-Break"><span class="koboSpan" id="kobo.183.1">the workers.</span></span></li>
</ol>
<h3><span class="koboSpan" id="kobo.184.1">Phase II – Screening questions</span></h3>
<p><span class="koboSpan" id="kobo.185.1">This phase is </span><a id="_idIndexMarker121"/><span class="koboSpan" id="kobo.186.1">designed to evaluate the worker’s understanding of </span><span class="No-Break"><span class="koboSpan" id="kobo.187.1">the task:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.188.1">Workers are asked to annotate a representative set of five gold </span><span class="No-Break"><span class="koboSpan" id="kobo.189.1">standard questions.</span></span></li>
<li><span class="koboSpan" id="kobo.190.1">Feedback is given on each question to help the workers understand </span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">their errors.</span></span></li>
<li><span class="koboSpan" id="kobo.192.1">Workers who fail a majority of these questions are excluded from the </span><span class="No-Break"><span class="koboSpan" id="kobo.193.1">remaining process.</span></span></li>
</ol>
<h3><span class="koboSpan" id="kobo.194.1">Phase III – Batches of questions (with continued screening)</span></h3>
<p><span class="koboSpan" id="kobo.195.1">This phase </span><a id="_idIndexMarker122"/><span class="koboSpan" id="kobo.196.1">focuses on maintaining high-quality work during the </span><span class="No-Break"><span class="koboSpan" id="kobo.197.1">task execution:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.198.1">Gold standard questions are included in the task without </span><span class="No-Break"><span class="koboSpan" id="kobo.199.1">providing feedback.</span></span></li>
<li><span class="koboSpan" id="kobo.200.1">Sets of five gold standard questions are included in each batch of 20 questions, with their frequency </span><span class="No-Break"><span class="koboSpan" id="kobo.201.1">decreasing exponentially.</span></span></li>
<li><span class="koboSpan" id="kobo.202.1">Workers who score less than 80% accuracy on the last 10 gold standard questions </span><span class="No-Break"><span class="koboSpan" id="kobo.203.1">are eliminated.</span></span></li>
</ol>
<h3><span class="koboSpan" id="kobo.204.1">General principles</span></h3>
<p><span class="koboSpan" id="kobo.205.1">These</span><a id="_idIndexMarker123"/><span class="koboSpan" id="kobo.206.1"> principles ensure the integrity and efficiency of </span><span class="No-Break"><span class="koboSpan" id="kobo.207.1">the process:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.208.1">Only workers with an AMT reputation above a certain threshold </span><span class="No-Break"><span class="koboSpan" id="kobo.209.1">are accepted.</span></span></li>
<li><span class="koboSpan" id="kobo.210.1">A link to the definitions of relations is provided throughout the task for </span><span class="No-Break"><span class="koboSpan" id="kobo.211.1">quick reference.</span></span></li>
<li><span class="koboSpan" id="kobo.212.1">Workers must correct any mistakes highlighted in the feedback </span><span class="No-Break"><span class="koboSpan" id="kobo.213.1">before proceeding.</span></span></li>
<li><span class="koboSpan" id="kobo.214.1">After each batch, feedback is provided on earnings so far and performance on gold </span><span class="No-Break"><span class="koboSpan" id="kobo.215.1">standard questions.</span></span></li>
<li><span class="koboSpan" id="kobo.216.1">Workers are reminded of a bonus upon completion of all 10 batches, encouraging consistent </span><span class="No-Break"><span class="koboSpan" id="kobo.217.1">high-quality work.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.218.1">Gated Instruction provides more accurate results than conventional instruction and screening methods since users are able to receive feedback on their annotations and adjust their approach accordingly. </span><span class="koboSpan" id="kobo.218.2">Using this approach, the authors improved precision from 0.50 to 0.77 and recall from 0.70 to 0.78 on the same dataset. </span><span class="koboSpan" id="kobo.218.3">This was in comparison to results from workers who were instructed using more traditional methods </span><span class="No-Break"><span class="koboSpan" id="kobo.219.1">of instruction.</span></span></p>
<p><span class="koboSpan" id="kobo.220.1">These two studies have used crowdsourced annotators to conduct their research, but the same frameworks are still highly relevant when your labelers are not crowdsourced workers, but SMEs with whom you have a closer </span><span class="No-Break"><span class="koboSpan" id="kobo.221.1">working relationship.</span></span></p>
<p><span class="koboSpan" id="kobo.222.1">For instance, if you’re selecting annotators from a pool of in-house colleagues, you might base your selection on someone’s particular expertise, years of experience, and interest in</span><a id="_idIndexMarker124"/><span class="koboSpan" id="kobo.223.1"> contributing to the project. </span><span class="koboSpan" id="kobo.223.2">As you will learn in the next section, annotators’ motivation to perform a task can have a big influence on how data is collected and, ultimately, how your models perform on </span><span class="No-Break"><span class="koboSpan" id="kobo.224.1">that data.</span></span></p>
<p><span class="koboSpan" id="kobo.225.1">Let’s build on these principles as we discuss how to motivate annotators and use SMEs for more complex </span><span class="No-Break"><span class="koboSpan" id="kobo.226.1">labeling assignments.</span></span></p>
<h2 id="_idParaDest-63"><a id="_idTextAnchor062"/><span class="koboSpan" id="kobo.227.1">Aligning motivations and using SMEs</span></h2>
<p><span class="koboSpan" id="kobo.228.1">While</span><a id="_idIndexMarker125"/><span class="koboSpan" id="kobo.229.1"> technology has made leaps and bounds in automating data collection, human data collectors still hold an essential place in various fields. </span><span class="koboSpan" id="kobo.229.2">They bring a level of understanding, empathy, and judgment that machines cannot replicate. </span><span class="koboSpan" id="kobo.229.3">For instance, human data collectors and annotators can interpret nuances in language, context, and emotions. </span><span class="koboSpan" id="kobo.229.4">Similarly, they can engage with respondents, build rapport, and encourage more open and </span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">honest responses.</span></span></p>
<p><span class="koboSpan" id="kobo.231.1">A significant challenge with human data collectors is maintaining their motivation and engagement in alignment with secondary uses of this data. </span><span class="koboSpan" id="kobo.231.2">Let’s discuss four factors that commonly contribute to </span><span class="No-Break"><span class="koboSpan" id="kobo.232.1">this issue.</span></span></p>
<h3><span class="koboSpan" id="kobo.233.1">#1 – Lack of purpose</span></h3>
<p><span class="koboSpan" id="kobo.234.1">If data collectors</span><a id="_idIndexMarker126"/><span class="koboSpan" id="kobo.235.1"> do not understand the significance of their work, they may feel disconnected and unmotivated. </span><span class="koboSpan" id="kobo.235.2">A data collector might ask themselves, “Why am I collecting this information if no one is going to </span><span class="No-Break"><span class="koboSpan" id="kobo.236.1">use it?”</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.237.1">Solution</span></strong><span class="koboSpan" id="kobo.238.1">: Communicate the big picture and show how every data point becomes a valuable building block of </span><span class="No-Break"><span class="koboSpan" id="kobo.239.1">ML solutions.</span></span></p>
<p><span class="koboSpan" id="kobo.240.1">We start any data collection and labeling exercise with the assumption that workers are intelligent and able to understand the importance of collecting rich, unbiased information. </span><span class="koboSpan" id="kobo.240.2">They understand that any compromise in data quality can have negative implications for its future use. </span><span class="koboSpan" id="kobo.240.3">Our role then becomes to articulate the importance of good data collection and explain its intended use. </span><span class="koboSpan" id="kobo.240.4">This articulation will significantly mitigate issues related to </span><span class="No-Break"><span class="koboSpan" id="kobo.241.1">data quality.</span></span></p>
<p><span class="koboSpan" id="kobo.242.1">A couple of years ago, Manmohan and Jonas ran a number of ML projects that required front-line staff members to collect a bunch of information through customer interviews. </span><span class="koboSpan" id="kobo.242.2">These interviews had been running for years so we already had lots of data collected, but the information was captured as text in a conversational format and therefore difficult to </span><span class="No-Break"><span class="koboSpan" id="kobo.243.1">interpret statistically.</span></span></p>
<p><span class="koboSpan" id="kobo.244.1">We wanted to </span><a id="_idIndexMarker127"/><span class="koboSpan" id="kobo.245.1">make this data easier to use for ML purposes, so we gathered all front-line workers for a presentation and workshop on the importance of data quality. </span><span class="koboSpan" id="kobo.245.2">After showing these data collectors how we used the data they collected to build specific ML solutions, they were surprised to learn how impactful their jobs </span><span class="No-Break"><span class="koboSpan" id="kobo.246.1">could be.</span></span></p>
<p><span class="koboSpan" id="kobo.247.1">As one team member said, “If only I had known that the information I collect from one person could be used to help thousands of other customers, I would have put way more effort into </span><span class="No-Break"><span class="koboSpan" id="kobo.248.1">the details.”</span></span></p>
<p><span class="koboSpan" id="kobo.249.1">In the workshop, we made data collectors take ownership of the situation by creating a series of templated questions that would improve the accuracy and signal of collected data. </span><span class="koboSpan" id="kobo.249.2">Our front-line colleagues took great pride in improving the way they could collect data for the greater good of the company and its customers. </span><span class="koboSpan" id="kobo.249.3">They had a transformed sense of ownership for data quality and the results that came from “</span><span class="No-Break"><span class="koboSpan" id="kobo.250.1">their data.”</span></span></p>
<p><span class="koboSpan" id="kobo.251.1">We were happy too as the uplift in data quality ended up boosting our model accuracy by up to 40% in </span><span class="No-Break"><span class="koboSpan" id="kobo.252.1">some instances.</span></span></p>
<h3><span class="koboSpan" id="kobo.253.1">#2 – Tedious nature of data tasks</span></h3>
<p><span class="koboSpan" id="kobo.254.1">Data collection and labeling can be repetitive and monotonous, leading to boredom and disengagement. </span><span class="koboSpan" id="kobo.254.2">This creates an underlying incentive to complete tasks with </span><span class="No-Break"><span class="koboSpan" id="kobo.255.1">minimal effort.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.256.1">Solution</span></strong><span class="koboSpan" id="kobo.257.1">: We use the following four strategies to reduce the boring parts of data collection and labeling: refining our questions, simplifying the process, eliminating unnecessary data, and automating </span><span class="No-Break"><span class="koboSpan" id="kobo.258.1">wherever possible.</span></span></p>
<p><span class="koboSpan" id="kobo.259.1">It is sometimes mindboggling how much impact you can have by teaching data collectors to ask better questions. </span><span class="koboSpan" id="kobo.259.2">More specific and targeted questions will typically yield more meaningful and engaging responses. </span><span class="koboSpan" id="kobo.259.3">This improves data quality while making the data collection process more interesting and </span><span class="No-Break"><span class="koboSpan" id="kobo.260.1">less repetitive.</span></span></p>
<p><span class="koboSpan" id="kobo.261.1">Simplifying the process could mean streamlining workflows, using more user-friendly software, or providing clear instructions and training to those involved in data collection. </span><span class="koboSpan" id="kobo.261.2">By making the process more straightforward, we can reduce the cognitive load on individuals and make the task </span><span class="No-Break"><span class="koboSpan" id="kobo.262.1">less tiresome.</span></span></p>
<p><span class="koboSpan" id="kobo.263.1">If you’re like most</span><a id="_idIndexMarker128"/><span class="koboSpan" id="kobo.264.1"> data professionals, the notion of discarding or limiting data collection may induce a degree of anxiety. </span><span class="koboSpan" id="kobo.264.2">However, it’s vital to appreciate that data collection shouldn’t mean hoarding every bit of data you come across, especially if it gets in the way of maintaining a great user experience. </span><span class="koboSpan" id="kobo.264.3">In fact, collecting unnecessary or irrelevant data can add to the monotony of the task and create clutter that hinders data analysis. </span><span class="koboSpan" id="kobo.264.4">Therefore, it’s crucial to identify and focus only on the data that’s truly relevant to your research question or </span><span class="No-Break"><span class="koboSpan" id="kobo.265.1">business objective.</span></span></p>
<p><span class="koboSpan" id="kobo.266.1">Our last simplification strategy, automation, is often a great solution to the tedium of data collection, but we prefer to only introduce automation once we have exhausted the other three strategies. </span><span class="koboSpan" id="kobo.266.2">There is no point in automating something that should be done differently or not </span><span class="No-Break"><span class="koboSpan" id="kobo.267.1">at all.</span></span></p>
<p><span class="koboSpan" id="kobo.268.1">For example, scraping tools can automatically extract large volumes of data from websites or documents, and rules-based logic or ML algorithms can label and organize data. </span><span class="koboSpan" id="kobo.268.2">You will learn some of these automation techniques in the coding chapters of </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">this book.</span></span></p>
<p><span class="koboSpan" id="kobo.270.1">A little while back, we worked with a large business to build real-time predictions of a caller’s propensity to buy investment products. </span><span class="koboSpan" id="kobo.270.2">The business wanted their call center staff to have this prediction served up to them during the call so that they could match callers with the right experts for their </span><span class="No-Break"><span class="koboSpan" id="kobo.271.1">investment needs.</span></span></p>
<p><span class="koboSpan" id="kobo.272.1">These screening calls would typically take 15 to 20 minutes to complete as call center staff painstakingly moved through pages and pages of questions relating to the caller’s personal details, demographics, investment experience, and </span><span class="No-Break"><span class="koboSpan" id="kobo.273.1">risk appetite.</span></span></p>
<p><span class="koboSpan" id="kobo.274.1">Although lots of data were being collected, it was not the right kind of information to build our prediction on. </span><span class="koboSpan" id="kobo.274.2">We identified a small handful of supplementary data points required to reliably determine someone’s product needs. </span><span class="koboSpan" id="kobo.274.3">However, adding these to the data collection process as additional questions was unfeasible because the screening calls were already long and tedious. </span><span class="koboSpan" id="kobo.274.4">To get the information we needed, we had to create a win-win scenario for callers, call center staff, and data </span><span class="No-Break"><span class="koboSpan" id="kobo.275.1">scientists alike.</span></span></p>
<p><span class="koboSpan" id="kobo.276.1">As we workshopped this challenge with the call center team, we discovered that about 20% of the existing questions could be enhanced to make the conversation more concise, while another 15% of questions could be removed entirely because they captured irrelevant or duplicate information. </span><span class="koboSpan" id="kobo.276.2">Lastly, 10% of the data collected could be prefilled or derived based on callers’ answers to </span><span class="No-Break"><span class="koboSpan" id="kobo.277.1">other questions.</span></span></p>
<p><span class="koboSpan" id="kobo.278.1">After </span><a id="_idIndexMarker129"/><span class="koboSpan" id="kobo.279.1">implementing these changes, the average call-handling time decreased by about 30% even after we added five new questions to the process. </span><span class="koboSpan" id="kobo.279.2">This reduction in call duration enabled the processing of more calls per day, leading to an increase in conversions into paying customers – a win-win for </span><span class="No-Break"><span class="koboSpan" id="kobo.280.1">everyone involved.</span></span></p>
<h3><span class="koboSpan" id="kobo.281.1">#3 – Lack of incentives</span></h3>
<p><span class="koboSpan" id="kobo.282.1">Without appropriate rewards or recognition, data collectors may lack the motivation to perform at their best. </span><span class="koboSpan" id="kobo.282.2">For instance, if a person is incentivized by the </span><em class="italic"><span class="koboSpan" id="kobo.283.1">quantity</span></em><span class="koboSpan" id="kobo.284.1"> of data points collected instead of the </span><em class="italic"><span class="koboSpan" id="kobo.285.1">quality</span></em><span class="koboSpan" id="kobo.286.1"> of individual responses, it may result in more superficial completion </span><span class="No-Break"><span class="koboSpan" id="kobo.287.1">of tasks.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.288.1">Solution</span></strong><span class="koboSpan" id="kobo.289.1">: We have seen the best results when we focus on motivating data collectors across four </span><span class="No-Break"><span class="koboSpan" id="kobo.290.1">different areas:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.291.1">Show what good looks like</span></strong><span class="koboSpan" id="kobo.292.1">: Firstly, it’s essential to set clear expectations for what constitutes high-quality work. </span><span class="koboSpan" id="kobo.292.2">Most people take pride in their work and strive to perform well when they understand the value and impact of their efforts. </span><span class="koboSpan" id="kobo.292.3">By defining the purpose and significance of the data collection or labeling task and demonstrating examples of excellent work, you provide a tangible target for your team to aim for. </span><span class="koboSpan" id="kobo.292.4">This clarity helps ensure the collected data is robust, relevant, and rich in valuable insights while </span><span class="No-Break"><span class="koboSpan" id="kobo.293.1">minimizing noise.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.294.1">Create mutual benefits</span></strong><span class="koboSpan" id="kobo.295.1">: Secondly, fostering a sense of shared success between data collectors and data users is crucial. </span><span class="koboSpan" id="kobo.295.2">When data collectors understand how their efforts contribute to the bigger picture – perhaps driving key business decisions or fueling innovative projects – they are more likely to feel invested in their work. </span><span class="koboSpan" id="kobo.295.3">This sense of ownership and contribution can significantly enhance the quality of </span><span class="No-Break"><span class="koboSpan" id="kobo.296.1">data collection.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.297.1">Provide non-monetary rewards</span></strong><span class="koboSpan" id="kobo.298.1">: The third area of focus is non-monetary rewards. </span><span class="koboSpan" id="kobo.298.2">Recognition and appreciation are powerful motivators. </span><span class="koboSpan" id="kobo.298.3">Gamification can be a highly effective tool in this regard, transforming the process of data collection into an engaging competition. </span><span class="koboSpan" id="kobo.298.4">Implementing features such as publicly displayed leaderboards, badges, or points can foster a sense of achievement and encourage healthy competition among team members. </span><span class="koboSpan" id="kobo.298.5">The longstanding concept of “Employee of the Month” not only rewards exceptional performance but also sets a standard for others to </span><span class="No-Break"><span class="koboSpan" id="kobo.299.1">aspire to.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.300.1">Provide monetary rewards</span></strong><span class="koboSpan" id="kobo.301.1">: Finally, monetary rewards can be a potent incentive. </span><span class="koboSpan" id="kobo.301.2">Tying compensation to the quality of work can drive individuals to meet or exceed set standards. </span><span class="koboSpan" id="kobo.301.3">However, this approach requires a clear framework outlining performance expectations and objective measures of performance. </span><span class="koboSpan" id="kobo.301.4">At the same time, you also need to have the ability to influence someone’s compensation, which can be difficult if your annotators and data collectors are in-house staff. </span><span class="koboSpan" id="kobo.301.5">While not always feasible, monetary incentives can be a powerful motivator when available. </span><span class="koboSpan" id="kobo.301.6">If they are not an option, doubling down on the other three areas of motivation can still yield </span><span class="No-Break"><span class="koboSpan" id="kobo.302.1">impressive results.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.303.1">In conclusion, motivating </span><a id="_idIndexMarker130"/><span class="koboSpan" id="kobo.304.1">data collectors and annotators to do a great job is multifaceted. </span><span class="koboSpan" id="kobo.304.2">It requires a mix of clear communication, mutual benefits, recognition, and </span><span class="No-Break"><span class="koboSpan" id="kobo.305.1">appropriate compensation.</span></span></p>
<h3><span class="koboSpan" id="kobo.306.1">#4 – Work environment</span></h3>
<p><span class="koboSpan" id="kobo.307.1">A stressful or unsupportive work environment can also affect motivation levels. </span><span class="koboSpan" id="kobo.307.2">If a data collector or annotator has many competing priorities, they may adopt a “close enough is good enough” attitude toward </span><span class="No-Break"><span class="koboSpan" id="kobo.308.1">data collection.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.309.1">Solution</span></strong><span class="koboSpan" id="kobo.310.1">: Creating a suitable working environment for annotators and data collectors to focus on the main task at hand basically requires you to remove any unnecessary tasks or friction getting in the way of someone performing </span><span class="No-Break"><span class="koboSpan" id="kobo.311.1">their job.</span></span></p>
<p><span class="koboSpan" id="kobo.312.1">We regularly refuse to work on data science projects unless everyone needed for the project is motivated to participate and makes themselves available for the duration of the project. </span><span class="koboSpan" id="kobo.312.2">This often requires us to negotiate for a dedicated time in the calendar with SMEs, data collectors, and </span><span class="No-Break"><span class="koboSpan" id="kobo.313.1">their managers.</span></span></p>
<p><span class="koboSpan" id="kobo.314.1">It is also important to provide a feedback mechanism for data collectors. </span><span class="koboSpan" id="kobo.314.2">That way, they can contribute to the process and call out stuff that doesn’t work or could be done better, such as questions that could be framed differently, changing the order of questions or tasks, confusing labeling requirements, and </span><span class="No-Break"><span class="koboSpan" id="kobo.315.1">so on.</span></span></p>
<p><span class="koboSpan" id="kobo.316.1">Give annotators and data collectors the necessary tools and resources to do their job swiftly and effectively. </span><span class="koboSpan" id="kobo.316.2">Examples include digitizing data collection, automating data population wherever possible, conditionally formatting drop-down options, and </span><span class="No-Break"><span class="koboSpan" id="kobo.317.1">so on.</span></span></p>
<p><span class="koboSpan" id="kobo.318.1">Continuous and timely</span><a id="_idIndexMarker131"/><span class="koboSpan" id="kobo.319.1"> feedback between project participants is also vital. </span><span class="koboSpan" id="kobo.319.2">Surprising workers with negative feedback late in the process can be demotivating and counterproductive. </span><span class="koboSpan" id="kobo.319.3">Instead, collaborate and provide feedback throughout the process to ensure alignment from the outset. </span><span class="koboSpan" id="kobo.319.4">Assume that there may be hiccups along the way, and be ready to coach your team through them proactively. </span><span class="koboSpan" id="kobo.319.5">Conversely, if you assume that everyone has understood every instruction as intended, then you’re setting yourself up </span><span class="No-Break"><span class="koboSpan" id="kobo.320.1">for failure.</span></span></p>
<p><span class="koboSpan" id="kobo.321.1">Some people don’t consider this kind of work part of the data science domain. </span><span class="koboSpan" id="kobo.321.2">In our opinion, this is where a lot of data professionals fall short. </span><span class="koboSpan" id="kobo.321.3">It’s important to recognize that human behavior and bias can be a seriously limiting factor for data science projects, and you want to reduce this impact as much as possible. </span><span class="koboSpan" id="kobo.321.4">If you don’t lean into these non-technical hurdles, you will end up with poorer data as </span><span class="No-Break"><span class="koboSpan" id="kobo.322.1">a result.</span></span></p>
<p><span class="koboSpan" id="kobo.323.1">This brings us to the next dimension in our framework: </span><span class="No-Break"><span class="koboSpan" id="kobo.324.1">iterative collaboration.</span></span></p>
<h2 id="_idParaDest-64"><a id="_idTextAnchor063"/><span class="koboSpan" id="kobo.325.1">Collaborating iteratively</span></h2>
<p><span class="koboSpan" id="kobo.326.1">Iterative collaboration </span><a id="_idIndexMarker132"/><span class="koboSpan" id="kobo.327.1">should be a central strategy for human labeling tasks. </span><span class="koboSpan" id="kobo.327.2">Basically, it entails creating an ongoing process of feedback and fine-tuning the data labeling process. </span><span class="koboSpan" id="kobo.327.3">Here are three guiding principles for implementing a collaborative approach to data labeling, validated by data labeling platform SUPA’s </span><span class="No-Break"><span class="koboSpan" id="kobo.328.1">best-practice approach</span></span><span class="No-Break"><span class="superscript"><span class="koboSpan" id="kobo.329.1">3</span></span></span><span class="No-Break"><span class="koboSpan" id="kobo.330.1">.</span></span></p>
<h3><span class="koboSpan" id="kobo.331.1">Start small and iron out any issues early</span></h3>
<p><span class="koboSpan" id="kobo.332.1">Initiating the data labeling process with smaller datasets is a practical approach. </span><span class="koboSpan" id="kobo.332.2">Instead of starting with thousands of observations, begin with a calibration batch of, say, 50 observations. </span><span class="koboSpan" id="kobo.332.3">This manageable dataset allows you to review the labels, spot potential problems, enhance instructions, and provide feedback to </span><span class="No-Break"><span class="koboSpan" id="kobo.333.1">the labelers.</span></span></p>
<p><span class="koboSpan" id="kobo.334.1">Repeat this process until you are confident that you have gone through a representative sample of the full dataset, that you have identified most edge cases, and that your labelers can perform the </span><span class="No-Break"><span class="koboSpan" id="kobo.335.1">task consistently.</span></span></p>
<p><span class="koboSpan" id="kobo.336.1">Labeling inconsistencies </span><a id="_idIndexMarker133"/><span class="koboSpan" id="kobo.337.1">across the same observations indicates a need for rule revision. </span><span class="koboSpan" id="kobo.337.2">If annotators are interpreting guidelines differently, then it’s worth understanding whether the cause is unclear labeling rules, discrepancies in the understanding and experience of individual labelers, or something entirely different. </span><span class="koboSpan" id="kobo.337.3">In other words, your labeling instructions are never set-and-forget. </span><span class="koboSpan" id="kobo.337.4">They should be under constant supervision and evolve to meet the needs of </span><span class="No-Break"><span class="koboSpan" id="kobo.338.1">the project.</span></span></p>
<h3><span class="koboSpan" id="kobo.339.1">Visualize what good looks like</span></h3>
<p><span class="koboSpan" id="kobo.340.1">Annotation rules can occasionally be vague, leading to subjective interpretations and inconsistencies. </span><span class="koboSpan" id="kobo.340.2">For instance, in an image labeling exercise, a rule such as “Only label an item when most of it is visible” might be interpreted differently by different labelers. </span><span class="koboSpan" id="kobo.340.3">Therefore, it’s important to visually show what good </span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">looks like.</span></span></p>
<p><span class="koboSpan" id="kobo.342.1">Visual illustrations are invaluable in the data labeling process, as they offer labelers explicit guidelines on how labeled objects should appear. </span><span class="koboSpan" id="kobo.342.2">We recommend showing examples of how to perform a labeling task correctly, but also examples of the opposite. </span><span class="koboSpan" id="kobo.342.3">By offering visual illustrations of good and bad labeling conventions, labelers gain a deeper understanding of the task, thereby enhancing their productivity </span><span class="No-Break"><span class="koboSpan" id="kobo.343.1">and precision.</span></span></p>
<h3><span class="koboSpan" id="kobo.344.1">Be very specific about edge cases</span></h3>
<p><span class="koboSpan" id="kobo.345.1">Edge cases are situations that deviate from the ordinary and can result in inconsistent labels due to subjective opinions. </span><span class="koboSpan" id="kobo.345.2">For instance, should a toy car be labeled as a car? </span><span class="koboSpan" id="kobo.345.3">Is a tandem bicycle one or two bicycles, or is it in its </span><span class="No-Break"><span class="koboSpan" id="kobo.346.1">own category?</span></span></p>
<p><span class="koboSpan" id="kobo.347.1">To manage edge cases effectively, you need to have a mechanism in place for annotators to flag these items for further consideration. </span><span class="koboSpan" id="kobo.347.2">If you don’t have a feedback mechanism in place, it’s likely that annotators will make up their own judgment on the spot to complete </span><span class="No-Break"><span class="koboSpan" id="kobo.348.1">the task.</span></span></p>
<p><span class="koboSpan" id="kobo.349.1">Pradhan et al. </span><span class="koboSpan" id="kobo.349.2">(2022)</span><span class="superscript"><span class="koboSpan" id="kobo.350.1">4</span></span><span class="koboSpan" id="kobo.351.1"> propose a so-called </span><em class="italic"><span class="koboSpan" id="kobo.352.1">FIND-RESOLVE-LABEL</span></em><span class="koboSpan" id="kobo.353.1"> workflow for crowdsourced annotation, aimed at addressing these three iteration steps. </span><span class="koboSpan" id="kobo.353.2">The FIND-RESOLVE-LABEL workflow is a guided labeling process designed to reveal ambiguities for a specific labeling task and </span><span class="No-Break"><span class="koboSpan" id="kobo.354.1">associated instructions.</span></span></p>
<p><span class="koboSpan" id="kobo.355.1">For instance, a labeling task might be to identify images with a woman in it. </span><span class="koboSpan" id="kobo.355.2">On the surface, this task appears rather simple, but in reality, annotators can quickly face ambiguity. </span><span class="koboSpan" id="kobo.355.3">For example, when is someone a woman, and when is someone a girl? </span><span class="koboSpan" id="kobo.355.4">Does it even matter? </span><span class="koboSpan" id="kobo.355.5">What about a statue of a woman or the </span><em class="italic"><span class="koboSpan" id="kobo.356.1">Mona Lisa</span></em><span class="koboSpan" id="kobo.357.1"> painting – does that count? </span><span class="koboSpan" id="kobo.357.2">What if the woman is only </span><span class="No-Break"><span class="koboSpan" id="kobo.358.1">partially visible?</span></span></p>
<p><span class="koboSpan" id="kobo.359.1">In reality, it can</span><a id="_idIndexMarker134"/><span class="koboSpan" id="kobo.360.1"> be very difficult to provide high-quality instructions for a given labeling task because there can be so many dimensions </span><span class="No-Break"><span class="koboSpan" id="kobo.361.1">to consider.</span></span></p>
<p><span class="koboSpan" id="kobo.362.1">The FIND-RESOLVE-LABEL workflow</span><a id="_idIndexMarker135"/><span class="koboSpan" id="kobo.363.1"> aims to discover and remove these ambiguities at the beginning of the labeling exercise. </span><span class="koboSpan" id="kobo.363.2">It consists of three key components that work together to streamline the data </span><span class="No-Break"><span class="koboSpan" id="kobo.364.1">labeling process:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.365.1">Find</span></strong><span class="koboSpan" id="kobo.366.1">: In this initial stage, annotators are provided with labeling instructions and asked to identify examples that are ambiguous based on these instructions. </span><span class="koboSpan" id="kobo.366.2">For each identified example, labelers are also asked to provide a concept tag that provides an explanation for why a certain label was chosen. </span><span class="koboSpan" id="kobo.366.3">This allows for the collection of the rationale and conceptual thinking behind labeling decisions, which can then be fed back into improved </span><span class="No-Break"><span class="koboSpan" id="kobo.367.1">labeling instructions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.368.1">Resolve</span></strong><span class="koboSpan" id="kobo.369.1">: Once the data points are identified, the next step involves resolving any ambiguities or conflicts in the data. </span><span class="koboSpan" id="kobo.369.2">This may require domain expertise to make informed decisions on how to address inconsistencies or </span><span class="No-Break"><span class="koboSpan" id="kobo.370.1">missing information.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.371.1">Label</span></strong><span class="koboSpan" id="kobo.372.1">: Finally, after resolving any issues, the data points are labeled appropriately, ensuring high-quality annotations that can be used for training </span><span class="No-Break"><span class="koboSpan" id="kobo.373.1">ML models.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.374.1">Pradhan and his team discovered that focusing on the most unclear data points and clarifying them during labeling greatly improved data quality. </span><span class="koboSpan" id="kobo.374.2">They noticed that in some ambiguous scenarios, many annotators agreed on answers that were different from what the requester regarded as correct. </span><span class="koboSpan" id="kobo.374.3">This means that even when smart answer aggregation methods are used, there’s a risk of getting incorrect labels for </span><span class="No-Break"><span class="koboSpan" id="kobo.375.1">these tasks.</span></span></p>
<p><span class="koboSpan" id="kobo.376.1">Interestingly, the study also found that workers could correctly label observations closely related to the main concept. </span><span class="koboSpan" id="kobo.376.2">This suggests we might not need to explain every possible ambiguity during the task because a well-chosen set of examples could help the team correctly label other </span><span class="No-Break"><span class="koboSpan" id="kobo.377.1">unclear examples.</span></span></p>
<p><span class="koboSpan" id="kobo.378.1">With these findings in mind, let’s explore how to deal with ambiguity among annotators in a </span><span class="No-Break"><span class="koboSpan" id="kobo.379.1">data-centric fashion.</span></span></p>
<h2 id="_idParaDest-65"><a id="_idTextAnchor064"/><span class="koboSpan" id="kobo.380.1">Dealing with ambiguity and reflecting diversity</span></h2>
<p><span class="koboSpan" id="kobo.381.1">With </span><a id="_idIndexMarker136"/><span class="koboSpan" id="kobo.382.1">the help of human annotators, we can produce datasets that are incredibly rich in information, but this sometimes requires us to tackle ambiguity in innovative ways. </span><span class="koboSpan" id="kobo.382.2">At the same time, ambiguity </span><a id="_idIndexMarker137"/><span class="koboSpan" id="kobo.383.1">can be hard to spot. </span><span class="koboSpan" id="kobo.383.2">What one person finds obvious, another person may find </span><span class="No-Break"><span class="koboSpan" id="kobo.384.1">entirely confusing.</span></span></p>
<p><span class="koboSpan" id="kobo.385.1">Companies and researchers use internal staff, volunteers, or crowdsourcing platforms such as AMT to gain access to human annotators at affordable rates. </span><span class="koboSpan" id="kobo.385.2">These labelers come from diverse backgrounds and carry different biases, all of which can impact the quality of labeling – especially when there is some element of </span><span class="No-Break"><span class="koboSpan" id="kobo.386.1">judgment involved.</span></span></p>
<p><span class="koboSpan" id="kobo.387.1">This challenge only grows as AI and ML are used to classify and generate new content from datasets that can be interpreted differently depending on context and who is doing the interpretation. </span><span class="koboSpan" id="kobo.387.2">This is demonstrated in a research paper, </span><em class="italic"><span class="koboSpan" id="kobo.388.1">The Risk of Racial Bias in Hate Speech Detection</span></em><span class="koboSpan" id="kobo.389.1">, by Sap et al. </span><span class="koboSpan" id="kobo.389.2">(2019)</span><span class="superscript"><span class="koboSpan" id="kobo.390.1">5</span></span><span class="koboSpan" id="kobo.391.1">, which investigates how annotators’ insensitivity to differences in dialect can lead to racial bias in automated hate </span><span class="No-Break"><span class="koboSpan" id="kobo.392.1">speech detection.</span></span></p>
<p><span class="koboSpan" id="kobo.393.1">The </span><a id="_idIndexMarker138"/><span class="koboSpan" id="kobo.394.1">paper states that even datasets designed specifically for detecting hate speech contain an inherent </span><a id="_idIndexMarker139"/><span class="koboSpan" id="kobo.395.1">bias toward specific groups or minority languages. </span><span class="koboSpan" id="kobo.395.2">This is because the underlying parameters are created based on the preferences of annotators who may be unaware of subtle nuances between different ethnicities </span><span class="No-Break"><span class="koboSpan" id="kobo.396.1">or languages.</span></span></p>
<p><span class="koboSpan" id="kobo.397.1">For example, some ethnicities or social groups may use colloquial language that seems rude or offensive to people from other demographics. </span><span class="koboSpan" id="kobo.397.2">For instance, the researchers discovered that labelers tended to mark phrases in African American English as being more toxic than those using General </span><span class="No-Break"><span class="koboSpan" id="kobo.398.1">American English.</span></span></p>
<p><span class="koboSpan" id="kobo.399.1">These inherent biases are difficult to avoid because annotators are rarely complete SMEs but people following general instructions. </span><span class="koboSpan" id="kobo.399.2">At the same time, labelers are unlikely to be a representative sample of the general public. </span><span class="koboSpan" id="kobo.399.3">For example, the majority of AMT participants have historically been comprised of younger individuals who are unmarried and without children. </span><span class="koboSpan" id="kobo.399.4">The vast majority of Turkers hail from only two countries, the US and India, while less than 2% come from the </span><span class="No-Break"><span class="koboSpan" id="kobo.400.1">Global South</span></span><span class="No-Break"><span class="superscript"><span class="koboSpan" id="kobo.401.1">6</span></span></span><span class="No-Break"><span class="koboSpan" id="kobo.402.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.403.1">This </span><a id="_idIndexMarker140"/><span class="koboSpan" id="kobo.404.1">problem doesn’t just pertain to the subject of hate speech. </span><span class="koboSpan" id="kobo.404.2">As such, certain dialects, lifestyles, cultural </span><a id="_idIndexMarker141"/><span class="koboSpan" id="kobo.405.1">backgrounds, and worldviews may be overrepresented, while others remain underrepresented when determining any kind of label that requires subjective interpretation. </span><span class="koboSpan" id="kobo.405.2">The resulting poor data collection can lead to an increased gap between the labels used and the real-world scenarios they are meant </span><span class="No-Break"><span class="koboSpan" id="kobo.406.1">to represent.</span></span></p>
<p><span class="koboSpan" id="kobo.407.1">The imbalance that exists in data labeling is evident in some of the world’s most widely used public training datasets. </span><span class="koboSpan" id="kobo.407.2">Research has found that two of the most common databases, </span><em class="italic"><span class="koboSpan" id="kobo.408.1">ImageNet</span></em><span class="koboSpan" id="kobo.409.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.410.1">Open Images</span></em><span class="koboSpan" id="kobo.411.1">, are biased toward the US and Europe, evident by the fact that models created from these datasets have poorer performance on images sourced from the </span><span class="No-Break"><span class="koboSpan" id="kobo.412.1">Global South</span></span><span class="No-Break"><span class="superscript"><span class="koboSpan" id="kobo.413.1">7</span></span></span><span class="No-Break"><span class="koboSpan" id="kobo.414.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.415.1">Images of grooms, for instance, receive a lower accuracy rating when they come from Ethiopia or Pakistan compared to similar images from the US. </span><span class="koboSpan" id="kobo.415.2">This particular discrepancy is due to the way objects such as “wedding” and “spices” are interpreted depending on their cultural context, with publicly available recognition systems struggling to correctly classify them when sourced from countries outside of America </span><span class="No-Break"><span class="koboSpan" id="kobo.416.1">or Europe.</span></span></p>
<h2 id="_idParaDest-66"><a id="_idTextAnchor065"/><span class="koboSpan" id="kobo.417.1">Understanding approaches for dealing with ambiguity in labeling</span></h2>
<p><span class="koboSpan" id="kobo.418.1">Labels are</span><a id="_idIndexMarker142"/><span class="koboSpan" id="kobo.419.1"> often absolute, but opinions are not. </span><span class="koboSpan" id="kobo.419.2">At the same time, humans will tend to disagree on what more abstract labels should be. </span><span class="koboSpan" id="kobo.419.3">As data-centric practitioners, we should anticipate ambiguity and disagreement among annotators and have a plan in place for </span><span class="No-Break"><span class="koboSpan" id="kobo.420.1">managing it.</span></span></p>
<p><span class="koboSpan" id="kobo.421.1">It’s worth noting that we actually </span><em class="italic"><span class="koboSpan" id="kobo.422.1">want</span></em><span class="koboSpan" id="kobo.423.1"> ambiguity to show up so that we can deal with it in the right way. </span><span class="koboSpan" id="kobo.423.2">Ambiguous scenarios can arise because labeling instructions are unclear, but they can also uncover new labels that must be included in our dataset. </span><span class="koboSpan" id="kobo.423.3">Therefore, we should try to design our labeling teams to maximize the likelihood that we will tease out disagreements if they </span><span class="No-Break"><span class="koboSpan" id="kobo.424.1">are there.</span></span></p>
<p><span class="koboSpan" id="kobo.425.1">A great way to do this is to involve a more diverse pool of annotators who are better attuned to differences in elements such as language and opinion, but in doing this, we also want to elevate the opinions of minority groups. </span><span class="koboSpan" id="kobo.425.2">Stanford University researchers Gordon et al. </span><span class="koboSpan" id="kobo.425.3">(2022)</span><span class="superscript"><span class="koboSpan" id="kobo.426.1">8</span></span><span class="koboSpan" id="kobo.427.1"> propose an approach for this purpose, called </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.428.1">Jury Learning</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.429.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.430.1">In previous research by Gordon et al. </span><span class="koboSpan" id="kobo.430.2">(2021)</span><span class="superscript"><span class="koboSpan" id="kobo.431.1">9</span></span><span class="koboSpan" id="kobo.432.1">, it was found that when accounting for labels from non-majority groups in a comment toxicity labeling task, the classifier’s performance decreased from 0.95 ROC AUC to 0.73 ROC AUC. </span><span class="koboSpan" id="kobo.432.2">This means that the classifier is not as effective when applied to comments from people who are not part of the majority group. </span><span class="koboSpan" id="kobo.432.3">In other words, it is impossible to make everyone agree, so we should consider who we listen to – not necessarily the </span><span class="No-Break"><span class="koboSpan" id="kobo.433.1">simple majority.</span></span></p>
<p><span class="koboSpan" id="kobo.434.1">Jury Learning stands in contrast to more straightforward aggregation or majority voting methods. </span><span class="koboSpan" id="kobo.434.2">Rather than basing labels on a majority rule or a probability, Jury Learning actively uses varying opinions to pick out underlying biases and suppress minority opinions. </span><span class="koboSpan" id="kobo.434.3">It is proposed as a way to integrate dissenting voices into ML systems in order to prevent them from becoming over-reliant on a single opinion or view. </span><span class="koboSpan" id="kobo.434.4">Here is how </span><span class="No-Break"><span class="koboSpan" id="kobo.435.1">it works.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.436.1">Jury Learning</span></strong><span class="koboSpan" id="kobo.437.1"> is a </span><strong class="bold"><span class="koboSpan" id="kobo.438.1">supervised ML</span></strong><span class="koboSpan" id="kobo.439.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.440.1">SML</span></strong><span class="koboSpan" id="kobo.441.1">) approach </span><a id="_idIndexMarker143"/><span class="koboSpan" id="kobo.442.1">that resolves </span><a id="_idIndexMarker144"/><span class="koboSpan" id="kobo.443.1">disagreements explicitly through the metaphor of a jury. </span><span class="koboSpan" id="kobo.443.2">This approach allows practitioners to specify whose voices their classifiers reflect, and in what proportion. </span><span class="koboSpan" id="kobo.443.3">The goal of Jury Learning is to define which people or groups determine a system’s prediction and in what proportion, allowing developers to analyze – and potentially mitigate – any potential biases that may be present in </span><span class="No-Break"><span class="koboSpan" id="kobo.444.1">the model.</span></span></p>
<p><span class="koboSpan" id="kobo.445.1">In order to use Jury Learning effectively, practitioners must first identify the jurors they wish to include in their model. </span><span class="koboSpan" id="kobo.445.2">This can be done by selecting individuals who represent different perspectives on the problem </span><span class="No-Break"><span class="koboSpan" id="kobo.446.1">at hand.</span></span></p>
<p><span class="koboSpan" id="kobo.447.1">For example, let’s</span><a id="_idIndexMarker145"/><span class="koboSpan" id="kobo.448.1"> say the task is to label whether a movie is good. </span><span class="koboSpan" id="kobo.448.2">Practitioners could select jurors (labelers) from different demographics such as age, gender, ethnicity, location, political orientation, or socio-economic status. </span><span class="koboSpan" id="kobo.448.3">Once the jurors have been selected, they must then provide input into the model’s predictions by answering questions about whether a movie is good </span><span class="No-Break"><span class="koboSpan" id="kobo.449.1">and why.</span></span></p>
<p><span class="koboSpan" id="kobo.450.1">Once all of the jurors have provided their input into the model’s predictions, practitioners can then use this data to create an aggregate prediction for each individual case. </span><span class="koboSpan" id="kobo.450.2">By considering multiple perspectives on each case, practitioners can ensure that their models are more accurate and less biased than if they had relied solely on one perspective when </span><span class="No-Break"><span class="koboSpan" id="kobo.451.1">making predictions.</span></span></p>
<p><span class="koboSpan" id="kobo.452.1">Finally, practitioners </span><a id="_idIndexMarker146"/><span class="koboSpan" id="kobo.453.1">can also use Jury Learning to analyze any potential biases present in their models by comparing the aggregate predictions made by different juries, composed of individuals from different backgrounds or perspectives. </span><span class="koboSpan" id="kobo.453.2">This comparison effectively provides us with a prediction range rather than binary labels. </span><span class="koboSpan" id="kobo.453.3">The analysis can help identify any areas where bias may be present and allow practitioners to adjust their models accordingly in order to reduce any potential bias and improve overall accuracy. </span><span class="koboSpan" id="kobo.453.4">This process is illustrated in the </span><span class="No-Break"><span class="koboSpan" id="kobo.454.1">following diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer026">
<span class="koboSpan" id="kobo.455.1"><img alt="Figure 4.1 – An overview of the Jury Learning process taken from Gordon et al. (2022)" src="image/B19297_04_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.456.1">Figure 4.1 – An overview of the Jury Learning process taken from Gordon et al. </span><span class="koboSpan" id="kobo.456.2">(2022)</span></p>
<p><span class="koboSpan" id="kobo.457.1">By now, you may have noticed that Jury Learning is not simple to set up. </span><span class="koboSpan" id="kobo.457.2">As with many data-centric approaches, Jury Learning requires extensive planning and coordination to ensure that all participants have a clear understanding of the expectations and guidelines of the process. </span><span class="koboSpan" id="kobo.457.3">This can be a time-consuming and </span><span class="No-Break"><span class="koboSpan" id="kobo.458.1">resource-intensive process.</span></span></p>
<p><span class="koboSpan" id="kobo.459.1">Jury Learning </span><a id="_idIndexMarker147"/><span class="koboSpan" id="kobo.460.1">also requires a labeler pool of a certain size to ensure the necessary diversity of opinions and views. </span><span class="koboSpan" id="kobo.460.2">This can be a challenge, particularly for projects with </span><span class="No-Break"><span class="koboSpan" id="kobo.461.1">limited resources.</span></span></p>
<p><span class="koboSpan" id="kobo.462.1">Another challenge that comes with implementing Jury Learning is the need for meta-information on labelers. </span><span class="koboSpan" id="kobo.462.2">To ensure that the results are both accurate and reliable, Jury Learning requires labelers with diverse skill sets and backgrounds. </span><span class="koboSpan" id="kobo.462.3">Gathering this information and developing a pool of labelers that meets the required standards can be a difficult task that again requires </span><span class="No-Break"><span class="koboSpan" id="kobo.463.1">upfront planning.</span></span></p>
<p><span class="koboSpan" id="kobo.464.1">Despite these challenges, Jury Learning </span><a id="_idIndexMarker148"/><span class="koboSpan" id="kobo.465.1">provides an innovative and effective way for practitioners to incorporate dissenting voices into ML models while also helping them identify and mitigate any potential biases present in their models. </span><span class="koboSpan" id="kobo.465.2">By considering multiple perspectives when making predictions and analyzing potential biases present in their models, practitioners can ensure that their ML models are both accurate and unbiased when making decisions about important tasks that are prone </span><span class="No-Break"><span class="koboSpan" id="kobo.466.1">to subjectivity.</span></span></p>
<p><span class="koboSpan" id="kobo.467.1">To round off this chapter, let’s explore how we can measure ambiguity or disagreement among </span><span class="No-Break"><span class="koboSpan" id="kobo.468.1">annotators statistically.</span></span></p>
<h1 id="_idParaDest-67"><a id="_idTextAnchor066"/><span class="koboSpan" id="kobo.469.1">Measuring labeling consistency</span></h1>
<p><span class="koboSpan" id="kobo.470.1">So far, we </span><a id="_idIndexMarker149"/><span class="koboSpan" id="kobo.471.1">have discussed a range of tools and techniques for creating consistent and high-quality annotations. </span><span class="koboSpan" id="kobo.471.2">While these elements create the foundation for good datasets, we also want to be able to measure whether our annotators are </span><span class="No-Break"><span class="koboSpan" id="kobo.472.1">performing consistently.</span></span></p>
<p><span class="koboSpan" id="kobo.473.1">To gauge annotator consistency, we recommend using two measures of labeling consistency called intra- and interobserver variability, respectively. </span><span class="koboSpan" id="kobo.473.2">These are standard terms in clinical research and refer to the degree of agreement among different measurements or evaluations made by the same observer (intra-) or by different observers (inter-). </span><span class="koboSpan" id="kobo.473.3">To simplify the explanation, consider “observer” to be interchangeable with “labeler,” “annotator, “rater,” “data collector,” and any other similar term we have used throughout </span><span class="No-Break"><span class="koboSpan" id="kobo.474.1">this chapter.</span></span></p>
<p><span class="koboSpan" id="kobo.475.1">While both intra- and interobserver variability relate to measurement consistency, they address different aspects. </span><span class="koboSpan" id="kobo.475.2">Intra-observer variability refers to the consistency of a single observer over time, while inter-observer variability refers to the consistency between different observers. </span><span class="koboSpan" id="kobo.475.3">Factors such as training, experience, and standardization of protocols can significantly </span><span class="No-Break"><span class="koboSpan" id="kobo.476.1">influence both.</span></span></p>
<p><span class="koboSpan" id="kobo.477.1">Tracking observer variability is crucial as it directly impacts the quality and reliability of your input dataset and, therefore, your model. </span><span class="koboSpan" id="kobo.477.2">If the same object is interpreted differently by various observers (inter-observer variability) or even by the same observer at different times (intra-observer variability), it could lead to inconsistencies in labeling, thereby affecting the overall quality of </span><span class="No-Break"><span class="koboSpan" id="kobo.478.1">ML outputs.</span></span></p>
<p><span class="koboSpan" id="kobo.479.1">Several factors contribute to observer variability, including lack of standardization in measurement techniques, observer fatigue, and subjective interpretations. </span><span class="koboSpan" id="kobo.479.2">As an example, someone’s judgment might be influenced by their level of experience, personal bias, or even their state of mind at the time </span><span class="No-Break"><span class="koboSpan" id="kobo.480.1">of observation.</span></span></p>
<p><span class="koboSpan" id="kobo.481.1">It’s important </span><a id="_idIndexMarker150"/><span class="koboSpan" id="kobo.482.1">to note that labeling discrepancies between two or more observers do not necessarily mean that one observer is correct and others are incorrect. </span><span class="koboSpan" id="kobo.482.2">Labeling disagreements may simply mean that the labeled object or situation is ambiguous or transitory and therefore difficult to give a </span><span class="No-Break"><span class="koboSpan" id="kobo.483.1">hard label.</span></span></p>
<p><span class="koboSpan" id="kobo.484.1">For instance, two doctors might disagree on the relative progression of an illness based on medical imaging or symptom descriptions. </span><span class="koboSpan" id="kobo.484.2">This could be because the diagnosis is uncertain rather than one doctor being more correct than </span><span class="No-Break"><span class="koboSpan" id="kobo.485.1">the other.</span></span></p>
<p><span class="koboSpan" id="kobo.486.1">A common technique for measuring variability is the </span><strong class="bold"><span class="koboSpan" id="kobo.487.1">intraclass correlation coefficient</span></strong><span class="koboSpan" id="kobo.488.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.489.1">ICC</span></strong><span class="koboSpan" id="kobo.490.1">). </span><span class="koboSpan" id="kobo.490.2">ICC</span><a id="_idIndexMarker151"/><span class="koboSpan" id="kobo.491.1"> is a statistical tool used to assess the consistency or conformity of annotations made by one or more observers measuring the same entity. </span><span class="koboSpan" id="kobo.491.2">Unlike the commonly used Pearson correlation coefficient, which measures linear relationships between variables, ICC assesses the reliability of ratings within the same group of data. </span><span class="koboSpan" id="kobo.491.3">It’s particularly useful when we want to know how strongly units in the same group resemble </span><span class="No-Break"><span class="koboSpan" id="kobo.492.1">each other.</span></span></p>
<p><span class="koboSpan" id="kobo.493.1">A high ICC value close to 1 indicates a high similarity between values from the same group. </span><span class="koboSpan" id="kobo.493.2">Conversely, a low ICC suggests less agreement among </span><span class="No-Break"><span class="koboSpan" id="kobo.494.1">the ratings.</span></span></p>
<p><span class="koboSpan" id="kobo.495.1">There are different forms of ICC, each applicable in specific circumstances. </span><span class="koboSpan" id="kobo.495.2">For instance, some forms are more suitable when we have a single measurement from each subject, while others are better suited for an average of several measurements. </span><span class="koboSpan" id="kobo.495.3">The choice of form depends on the nature of your study and the type of data </span><span class="No-Break"><span class="koboSpan" id="kobo.496.1">you have.</span></span></p>
<p><span class="koboSpan" id="kobo.497.1">The following screenshot shows six common definitions outlined by Shrout and </span><span class="No-Break"><span class="koboSpan" id="kobo.498.1">Fleiss (1979)</span></span><span class="No-Break"><span class="superscript"><span class="koboSpan" id="kobo.499.1">10</span></span></span><span class="No-Break"><span class="koboSpan" id="kobo.500.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer027">
<span class="koboSpan" id="kobo.501.1"><img alt="Figure 4.2 – Six common ICCs" src="image/B19297_04_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.502.1">Figure 4.2 – Six common ICCs</span></p>
<p><span class="koboSpan" id="kobo.503.1">To build </span><a id="_idIndexMarker152"/><span class="koboSpan" id="kobo.504.1">our intuition around the use of ICC scores, let’s work through a practical example using the </span><strong class="bold"><span class="koboSpan" id="kobo.505.1">Pingouin</span></strong><span class="koboSpan" id="kobo.506.1"> Python package. </span><span class="koboSpan" id="kobo.506.2">Pingouin is</span><a id="_idIndexMarker153"/><span class="koboSpan" id="kobo.507.1"> an open source package with a large number of useful statistical features. </span><span class="koboSpan" id="kobo.507.2">It primarily utilizes pandas and NumPy, so make sure you have these installed </span><span class="No-Break"><span class="koboSpan" id="kobo.508.1">as well.</span></span></p>
<p><span class="koboSpan" id="kobo.509.1">For our example scenario, suppose we have four wine-tasting judges rating the quality of eight different wines by giving them a score of 0 to 9. </span><span class="koboSpan" id="kobo.509.2">We would like to know whether these judges are rating the wines consistently. </span><span class="koboSpan" id="kobo.509.3">The judges’ ratings are displayed in the </span><span class="No-Break"><span class="koboSpan" id="kobo.510.1">following screenshot:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer028">
<span class="koboSpan" id="kobo.511.1"><img alt="Figure 4.3 – Wine-tasting scores from four different judges" src="image/B19297_04_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.512.1">Figure 4.3 – Wine-tasting scores from four different judges</span></p>
<ul>
<li><span class="koboSpan" id="kobo.513.1">Generally </span><a id="_idIndexMarker154"/><span class="koboSpan" id="kobo.514.1">speaking, three types of variability </span><span class="No-Break"><span class="koboSpan" id="kobo.515.1">can occur:</span></span></li>
<li><span class="koboSpan" id="kobo.516.1">Variability due to differences in the objects being assessed (suppose two different samples of the same wine had slightly </span><span class="No-Break"><span class="koboSpan" id="kobo.517.1">different tastes)</span></span></li>
<li><span class="koboSpan" id="kobo.518.1">Variability caused by the assessment of observers, for example, the difference between judges B and C’s rating of </span><span class="No-Break"><span class="koboSpan" id="kobo.519.1">wine </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.520.1">#3</span></em></span></li>
<li><span class="koboSpan" id="kobo.521.1">Variability in the use of labels; for example, everyone finds wine </span><em class="italic"><span class="koboSpan" id="kobo.522.1">#1</span></em><span class="koboSpan" id="kobo.523.1"> the worst, but three different ratings have been used to </span><span class="No-Break"><span class="koboSpan" id="kobo.524.1">rate it</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.525.1">The ICC calculation will take all </span><a id="_idIndexMarker155"/><span class="koboSpan" id="kobo.526.1">of these into account as it is based on </span><strong class="bold"><span class="koboSpan" id="kobo.527.1">analysis of variance</span></strong><span class="koboSpan" id="kobo.528.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.529.1">ANOVA</span></strong><span class="koboSpan" id="kobo.530.1">) analysis. </span><span class="koboSpan" id="kobo.530.2">Before we get to calculating ICC, we must first determine which type of measure we’re after. </span><span class="koboSpan" id="kobo.530.3">We can use the following screenshot as a guide to selecting the correct form of ICC for </span><span class="No-Break"><span class="koboSpan" id="kobo.531.1">our situation:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer029">
<span class="koboSpan" id="kobo.532.1"><img alt="Figure 4.4 – ICC model selection guide" src="image/B19297_04_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.533.1">Figure 4.4 – ICC model selection guide</span></p>
<p><span class="koboSpan" id="kobo.534.1">In our scenario, the </span><a id="_idIndexMarker156"/><span class="No-Break"><span class="koboSpan" id="kobo.535.1">following applies:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.536.1">Each judge has rated each wine once, so we can determine that all subjects have been evaluated by the same group </span><span class="No-Break"><span class="koboSpan" id="kobo.537.1">of observers</span></span></li>
<li><span class="koboSpan" id="kobo.538.1">In this case, we will assume that the four judges have been chosen randomly from a larger pool of </span><span class="No-Break"><span class="koboSpan" id="kobo.539.1">potential judges</span></span></li>
<li><span class="koboSpan" id="kobo.540.1">We are interested in the reliability of individual observers rather than the </span><span class="No-Break"><span class="koboSpan" id="kobo.541.1">average reliability</span></span></li>
</ul>
<p><em class="italic"><span class="koboSpan" id="kobo.542.1">Therefore, we’re looking to use the ICC2 calculation to determine our </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.543.1">reliability score</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.544.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.545.1">We then use the following script to run the ICC function over our wine scores. </span><span class="koboSpan" id="kobo.545.2">The Pingouin ICC operator, </span><strong class="source-inline"><span class="koboSpan" id="kobo.546.1">intraclass_corr</span></strong><span class="koboSpan" id="kobo.547.1">, will calculate and present all six common ICC measures, but we are only interested </span><span class="No-Break"><span class="koboSpan" id="kobo.548.1">in ICC2:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.549.1">
import pingouin as pg
data = pg.read_dataset('icc')
icc = pg.intraclass_corr(data=data, targets='Wine', raters='Judge',
                         ratings='Scores').round(3)
icc.set_index("Type")</span></pre> <p><span class="koboSpan" id="kobo.550.1">This produces the following output. </span><span class="koboSpan" id="kobo.550.2">Our ICC score is 0.728, which means our judges are in moderate agreement </span><span class="No-Break"><span class="koboSpan" id="kobo.551.1">on ratings:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer030">
<span class="koboSpan" id="kobo.552.1"><img alt="Figure 4.5 – Output table" src="image/B19297_04_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.553.1">Figure 4.5 – Output table</span></p>
<p><span class="koboSpan" id="kobo.554.1">It’s important </span><a id="_idIndexMarker157"/><span class="koboSpan" id="kobo.555.1">to understand that there are no strict thresholds for what constitutes an “acceptable” ICC score. </span><span class="koboSpan" id="kobo.555.2">Though there are no rigid benchmarks, some general guidelines can help interpret ICC scores. </span><span class="koboSpan" id="kobo.555.3">These ranges are not absolute and should be interpreted in the context of your specific study </span><span class="No-Break"><span class="koboSpan" id="kobo.556.1">or analysis:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.557.1">ICC less than 0.5</span></strong><span class="koboSpan" id="kobo.558.1">: This range is generally considered to indicate poor reliability. </span><span class="koboSpan" id="kobo.558.2">For instance, if you have an ICC of 0.3 for a set of ratings, it would suggest a low level of agreement </span><span class="No-Break"><span class="koboSpan" id="kobo.559.1">among raters.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.560.1">ICC between 0.5 and 0.75</span></strong><span class="koboSpan" id="kobo.561.1">: Scores in this range are typically considered to show </span><span class="No-Break"><span class="koboSpan" id="kobo.562.1">moderate reliability.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.563.1">ICC between 0.75 and 0.9</span></strong><span class="koboSpan" id="kobo.564.1">: These scores suggest good reliability. </span><span class="koboSpan" id="kobo.564.2">If you achieve an ICC of 0.8, for example, it indicates a high degree of agreement among </span><span class="No-Break"><span class="koboSpan" id="kobo.565.1">your raters.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.566.1">ICC greater than 0.90</span></strong><span class="koboSpan" id="kobo.567.1">: This range represents excellent reliability. </span><span class="koboSpan" id="kobo.567.2">An ICC of 0.95, for example, would suggest almost perfect agreement </span><span class="No-Break"><span class="koboSpan" id="kobo.568.1">among raters.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.569.1">When interpreting ICC scores, it’s also important to consider several factors that can influence their reliability. </span><span class="koboSpan" id="kobo.569.2">These include </span><span class="No-Break"><span class="koboSpan" id="kobo.570.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.571.1">Sample size</span></strong><span class="koboSpan" id="kobo.572.1">: As with many statistical measures, the ICC is sensitive to sample size. </span><span class="koboSpan" id="kobo.572.2">Larger sample sizes tend to provide more reliable estimates of </span><span class="No-Break"><span class="koboSpan" id="kobo.573.1">the ICC.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.574.1">Data outcome range</span></strong><span class="koboSpan" id="kobo.575.1">: ICC scores can also be impacted by the potential range of outcomes being assessed and the difficulty of determining annotations. </span><span class="koboSpan" id="kobo.575.2">For instance, if our wine judges could only label wines as “good” or “bad” (1, 0) rather than a range (0–9), then that would likely alter the final </span><span class="No-Break"><span class="koboSpan" id="kobo.576.1">ICC score.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.577.1">Subject variability</span></strong><span class="koboSpan" id="kobo.578.1">: The ICC is also influenced by the variability among subjects. </span><span class="koboSpan" id="kobo.578.2">High subject variability can lead to lower ICC values, even when raters are consistent in </span><span class="No-Break"><span class="koboSpan" id="kobo.579.1">their ratings.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.580.1">In practice, interpreting</span><a id="_idIndexMarker158"/><span class="koboSpan" id="kobo.581.1"> ICC scores requires understanding the context of your study, the nature of your data, and the specific form of ICC used. </span><span class="koboSpan" id="kobo.581.2">Always consider these factors when interpreting and communicating </span><span class="No-Break"><span class="koboSpan" id="kobo.582.1">your results.</span></span></p>
<p><span class="koboSpan" id="kobo.583.1">Remember – ICC scores are just one piece of the puzzle. </span><span class="koboSpan" id="kobo.583.2">They should be used in conjunction with other statistical measures and insights to provide a comprehensive understanding of your data. </span><span class="koboSpan" id="kobo.583.3">Let’s summarize what we’ve covered so far in </span><span class="No-Break"><span class="koboSpan" id="kobo.584.1">the chapter.</span></span></p>
<h1 id="_idParaDest-68"><a id="_idTextAnchor067"/><span class="koboSpan" id="kobo.585.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.586.1">Throughout this chapter, we’ve examined the critical role that humans play in ensuring data quality, particularly in the initial stages of data labeling. </span><span class="koboSpan" id="kobo.586.2">We’ve recognized that while human labelers are indispensable, they also present certain challenges, including biases </span><span class="No-Break"><span class="koboSpan" id="kobo.587.1">and inconsistencies.</span></span></p>
<p><span class="koboSpan" id="kobo.588.1">To address these issues, we’ve explored various strategies to train labelers effectively for high-quality dataset development. </span><span class="koboSpan" id="kobo.588.2">The key takeaway here is that well-trained labelers, armed with clear instructions, can significantly increase the overall quality of </span><span class="No-Break"><span class="koboSpan" id="kobo.589.1">your data.</span></span></p>
<p><span class="koboSpan" id="kobo.590.1">Improving task instructions emerged as a recurring theme, underscoring their importance in facilitating the labeling process. </span><span class="koboSpan" id="kobo.590.2">Iterative collaboration was also highlighted as an essential practice, promoting continuous improvement through feedback </span><span class="No-Break"><span class="koboSpan" id="kobo.591.1">and refinement.</span></span></p>
<p><span class="koboSpan" id="kobo.592.1">By the end of this chapter, you should have gained a comprehensive understanding of why human involvement is crucial in data-centric model building, the challenges posed by human labelers, and practical ways to overcome them. </span><span class="koboSpan" id="kobo.592.2">More importantly, you’ll have learned how to use specific frameworks to achieve quality labeling, setting a solid foundation for successful </span><span class="No-Break"><span class="koboSpan" id="kobo.593.1">ML projects.</span></span></p>
<p><span class="koboSpan" id="kobo.594.1">In the next chapter, we will build on these skills and delve deeper into the technical aspects of data cleaning and augmentation before we explore programmatic labeling techniques in </span><a href="B19297_06.xhtml#_idTextAnchor089"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.595.1">Chapter 6</span></em></span></a><em class="italic"><span class="koboSpan" id="kobo.596.1">, Techniques for Programmatic Labeling in Machine Learning</span></em><span class="koboSpan" id="kobo.597.1">. </span><span class="koboSpan" id="kobo.597.2">It’s time to get deep </span><span class="No-Break"><span class="koboSpan" id="kobo.598.1">into code!</span></span></p>
<h1 id="_idParaDest-69"><a id="_idTextAnchor068"/><span class="koboSpan" id="kobo.599.1">References</span></h1>
<ol>
<li><em class="italic"><span class="koboSpan" id="kobo.600.1">McInnis B.</span></em><span class="koboSpan" id="kobo.601.1">, </span><em class="italic"><span class="koboSpan" id="kobo.602.1">Cosley D.</span></em><span class="koboSpan" id="kobo.603.1">, </span><em class="italic"><span class="koboSpan" id="kobo.604.1">Nam C.</span></em><span class="koboSpan" id="kobo.605.1">, </span><em class="italic"><span class="koboSpan" id="kobo.606.1">Leshed G.</span></em><span class="koboSpan" id="kobo.607.1">, </span><em class="italic"><span class="koboSpan" id="kobo.608.1">Taking a HIT: Designing around Rejection, Mistrust, Risk, and Workers’ Experiences in Amazon Mechanical Turk</span></em><span class="koboSpan" id="kobo.609.1">, </span><em class="italic"><span class="koboSpan" id="kobo.610.1">Information Science &amp; Law School</span></em><span class="koboSpan" id="kobo.611.1">, </span><em class="italic"><span class="koboSpan" id="kobo.612.1">Cornell </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.613.1">University.</span></em></span><span class="No-Break"> </span><a href="https://dl.acm.org/doi/epdf/10.1145/2858036.2858539"><span class="No-Break"><span class="koboSpan" id="kobo.614.1">https://dl.acm.org/doi/epdf/10.1145/2858036.2858539</span></span></a></li>
<li><em class="italic"><span class="koboSpan" id="kobo.615.1">Liu A.</span></em><span class="koboSpan" id="kobo.616.1">, </span><em class="italic"><span class="koboSpan" id="kobo.617.1">Soderland S.</span></em><span class="koboSpan" id="kobo.618.1">, </span><em class="italic"><span class="koboSpan" id="kobo.619.1">Bragg J.</span></em><span class="koboSpan" id="kobo.620.1">, </span><em class="italic"><span class="koboSpan" id="kobo.621.1">Lin C. </span><span class="koboSpan" id="kobo.621.2">H.</span></em><span class="koboSpan" id="kobo.622.1">, </span><em class="italic"><span class="koboSpan" id="kobo.623.1">Ling X.</span></em><span class="koboSpan" id="kobo.624.1">, </span><em class="italic"><span class="koboSpan" id="kobo.625.1">Weld D. </span><span class="koboSpan" id="kobo.625.2">S.</span></em><span class="koboSpan" id="kobo.626.1">, </span><em class="italic"><span class="koboSpan" id="kobo.627.1">Effective Crowd Annotation for Relation Extraction</span></em><span class="koboSpan" id="kobo.628.1">, </span><em class="italic"><span class="koboSpan" id="kobo.629.1">Turing Center</span></em><span class="koboSpan" id="kobo.630.1">, </span><em class="italic"><span class="koboSpan" id="kobo.631.1">Department of Computer Science and Engineering</span></em><span class="koboSpan" id="kobo.632.1">, </span><em class="italic"><span class="koboSpan" id="kobo.633.1">University of </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.634.1">Washington.</span></em></span><span class="No-Break"> </span><a href="https://aclanthology.org/N16-1104.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.635.1">https://aclanthology.org/N16-1104.pdf</span></span></a></li>
<li><a href="https://www.supa.so/post/iteration-a-key-data-labeling-process-often-overlooked"><span class="koboSpan" id="kobo.636.1">https://www.supa.so/post/iteration-a-key-data-labeling-process-often-overlooked</span></a><span class="koboSpan" id="kobo.637.1">, viewed July </span><span class="No-Break"><span class="koboSpan" id="kobo.638.1">30, 2023.</span></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.639.1">Pradhan V. </span><span class="koboSpan" id="kobo.639.2">K.</span></em><span class="koboSpan" id="kobo.640.1">, </span><em class="italic"><span class="koboSpan" id="kobo.641.1">Schaekerman M.</span></em><span class="koboSpan" id="kobo.642.1">, </span><em class="italic"><span class="koboSpan" id="kobo.643.1">Lease M.</span></em><span class="koboSpan" id="kobo.644.1">, </span><em class="italic"><span class="koboSpan" id="kobo.645.1">2022</span></em><span class="koboSpan" id="kobo.646.1">, </span><em class="italic"><span class="koboSpan" id="kobo.647.1">In Search of Ambiguity: A Three-Stage Workflow Design to Clarify Annotation Guidelines for Crowd Workers</span></em><span class="koboSpan" id="kobo.648.1">, </span><em class="italic"><span class="koboSpan" id="kobo.649.1">Front. </span><span class="koboSpan" id="kobo.649.2">Artif. </span><span class="koboSpan" id="kobo.649.3">Intell.</span></em><span class="koboSpan" id="kobo.650.1">, </span><em class="italic"><span class="koboSpan" id="kobo.651.1">18 May 2022</span></em><span class="koboSpan" id="kobo.652.1">, </span><em class="italic"><span class="koboSpan" id="kobo.653.1">Sec. </span><span class="koboSpan" id="kobo.653.2">Machine Learning and Artificial Intelligence Volume 5 - </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.654.1">2022.</span></em></span><span class="No-Break"> </span><a href="https://doi.org/10.3389/frai.2022.828187"><span class="No-Break"><span class="koboSpan" id="kobo.655.1">https://doi.org/10.3389/frai.2022.828187</span></span></a></li>
<li><em class="italic"><span class="koboSpan" id="kobo.656.1">Sap, M.</span></em><span class="koboSpan" id="kobo.657.1">, </span><em class="italic"><span class="koboSpan" id="kobo.658.1">Card, D.</span></em><span class="koboSpan" id="kobo.659.1">, </span><em class="italic"><span class="koboSpan" id="kobo.660.1">Gabriel, S.</span></em><span class="koboSpan" id="kobo.661.1">, </span><em class="italic"><span class="koboSpan" id="kobo.662.1">Choi, Y.</span></em><span class="koboSpan" id="kobo.663.1">, </span><em class="italic"><span class="koboSpan" id="kobo.664.1">Smith, N. </span><span class="koboSpan" id="kobo.664.2">A.</span></em><span class="koboSpan" id="kobo.665.1">, </span><em class="italic"><span class="koboSpan" id="kobo.666.1">The Risk of Racial Bias in Hate Speech Detection</span></em><span class="koboSpan" id="kobo.667.1">, </span><em class="italic"><span class="koboSpan" id="kobo.668.1">Paul G. </span><span class="koboSpan" id="kobo.668.2">Allen School of Computer Science &amp; Engineering</span></em><span class="koboSpan" id="kobo.669.1">, </span><em class="italic"><span class="koboSpan" id="kobo.670.1">University of Washington</span></em><span class="koboSpan" id="kobo.671.1">, </span><em class="italic"><span class="koboSpan" id="kobo.672.1">Seattle</span></em><span class="koboSpan" id="kobo.673.1">, </span><em class="italic"><span class="koboSpan" id="kobo.674.1">USA</span></em><span class="koboSpan" id="kobo.675.1">, </span><em class="italic"><span class="koboSpan" id="kobo.676.1">Machine Learning Department</span></em><span class="koboSpan" id="kobo.677.1">, </span><em class="italic"><span class="koboSpan" id="kobo.678.1">Carnegie Mellon University</span></em><span class="koboSpan" id="kobo.679.1">, </span><em class="italic"><span class="koboSpan" id="kobo.680.1">Pittsburgh</span></em><span class="koboSpan" id="kobo.681.1">, </span><em class="italic"><span class="koboSpan" id="kobo.682.1">USA</span></em><span class="koboSpan" id="kobo.683.1">, </span><em class="italic"><span class="koboSpan" id="kobo.684.1">Allen Institute for Artificial Intelligence</span></em><span class="koboSpan" id="kobo.685.1">, </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.686.1">Seattle</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.687.1">, </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.688.1">USA</span></em></span></li>
<li><a href="https://venturebeat.com/business/the-ai-industry-is-built-on-geographic-and-social-inequality-research-shows/"><span class="koboSpan" id="kobo.689.1">https://venturebeat.com/business/the-ai-industry-is-built-on-geographic-and-social-inequality-research-shows/</span></a><span class="koboSpan" id="kobo.690.1">, viewed April </span><span class="No-Break"><span class="koboSpan" id="kobo.691.1">22, 2023.</span></span></li>
<li><a href="https://venturebeat.com/ai/mit-researchers-find-systematic-shortcomings-in-imagenet-data-set/"><span class="koboSpan" id="kobo.692.1">https://venturebeat.com/ai/mit-researchers-find-systematic-shortcomings-in-imagenet-data-set/</span></a><span class="koboSpan" id="kobo.693.1">, viewed April </span><span class="No-Break"><span class="koboSpan" id="kobo.694.1">22, 2023.</span></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.695.1">Gordon M. </span><span class="koboSpan" id="kobo.695.2">L.</span></em><span class="koboSpan" id="kobo.696.1">, </span><em class="italic"><span class="koboSpan" id="kobo.697.1">Lam M. </span><span class="koboSpan" id="kobo.697.2">S.</span></em><span class="koboSpan" id="kobo.698.1">, </span><em class="italic"><span class="koboSpan" id="kobo.699.1">Park J. </span><span class="koboSpan" id="kobo.699.2">S.</span></em><span class="koboSpan" id="kobo.700.1">, </span><em class="italic"><span class="koboSpan" id="kobo.701.1">Patel K.</span></em><span class="koboSpan" id="kobo.702.1">, </span><em class="italic"><span class="koboSpan" id="kobo.703.1">Hancock J.</span></em><span class="koboSpan" id="kobo.704.1">, </span><em class="italic"><span class="koboSpan" id="kobo.705.1">Hashimoto T.</span></em><span class="koboSpan" id="kobo.706.1">, </span><em class="italic"><span class="koboSpan" id="kobo.707.1">Bernstein M. </span><span class="koboSpan" id="kobo.707.2">S.</span></em><span class="koboSpan" id="kobo.708.1">, </span><em class="italic"><span class="koboSpan" id="kobo.709.1">2022</span></em><span class="koboSpan" id="kobo.710.1">. </span><em class="italic"><span class="koboSpan" id="kobo.711.1">Jury Learning: Integrating Dissenting Voices into Machine Learning Models</span></em><span class="koboSpan" id="kobo.712.1">. </span><span class="koboSpan" id="kobo.712.2">In </span><em class="italic"><span class="koboSpan" id="kobo.713.1">CHI Conference on Human Factors in Computing Systems (CHI ’22)</span></em><span class="koboSpan" id="kobo.714.1">, </span><em class="italic"><span class="koboSpan" id="kobo.715.1">April 29-May 5, 2022</span></em><span class="koboSpan" id="kobo.716.1">, </span><em class="italic"><span class="koboSpan" id="kobo.717.1">New Orleans</span></em><span class="koboSpan" id="kobo.718.1">, </span><em class="italic"><span class="koboSpan" id="kobo.719.1">LA</span></em><span class="koboSpan" id="kobo.720.1">, </span><em class="italic"><span class="koboSpan" id="kobo.721.1">USA</span></em><span class="koboSpan" id="kobo.722.1">. </span><em class="italic"><span class="koboSpan" id="kobo.723.1">ACM</span></em><span class="koboSpan" id="kobo.724.1">, </span><em class="italic"><span class="koboSpan" id="kobo.725.1">New York</span></em><span class="koboSpan" id="kobo.726.1">, </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.727.1">NY</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.728.1">, </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.729.1">USA</span></em></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.730.1">Gordon M. </span><span class="koboSpan" id="kobo.730.2">L.</span></em><span class="koboSpan" id="kobo.731.1">, </span><em class="italic"><span class="koboSpan" id="kobo.732.1">Zhou K.</span></em><span class="koboSpan" id="kobo.733.1">, </span><em class="italic"><span class="koboSpan" id="kobo.734.1">Patel K.</span></em><span class="koboSpan" id="kobo.735.1">, </span><em class="italic"><span class="koboSpan" id="kobo.736.1">Hashimoto T.</span></em><span class="koboSpan" id="kobo.737.1">, </span><em class="italic"><span class="koboSpan" id="kobo.738.1">Bernstein M.S.</span></em><span class="koboSpan" id="kobo.739.1">, </span><em class="italic"><span class="koboSpan" id="kobo.740.1">2021</span></em><span class="koboSpan" id="kobo.741.1">. </span><em class="italic"><span class="koboSpan" id="kobo.742.1">The Disagreement Deconvolution: Bringing Machine Learning Performance Metrics In Line With Reality</span></em><span class="koboSpan" id="kobo.743.1">. </span><span class="koboSpan" id="kobo.743.2">In </span><em class="italic"><span class="koboSpan" id="kobo.744.1">CHI Conference on Human Factors in Computing Systems (CHI ’21)</span></em><span class="koboSpan" id="kobo.745.1">, </span><em class="italic"><span class="koboSpan" id="kobo.746.1">May 8-13, 2021</span></em><span class="koboSpan" id="kobo.747.1">, </span><em class="italic"><span class="koboSpan" id="kobo.748.1">Yokohama</span></em><span class="koboSpan" id="kobo.749.1">, </span><em class="italic"><span class="koboSpan" id="kobo.750.1">Japan</span></em><span class="koboSpan" id="kobo.751.1">. </span><em class="italic"><span class="koboSpan" id="kobo.752.1">ACM</span></em><span class="koboSpan" id="kobo.753.1">, </span><em class="italic"><span class="koboSpan" id="kobo.754.1">New York</span></em><span class="koboSpan" id="kobo.755.1">, </span><em class="italic"><span class="koboSpan" id="kobo.756.1">NY</span></em><span class="koboSpan" id="kobo.757.1">, </span><em class="italic"><span class="koboSpan" id="kobo.758.1">USA</span></em><span class="koboSpan" id="kobo.759.1">, 14 </span><span class="No-Break"><span class="koboSpan" id="kobo.760.1">pages. </span></span><a href="https://doi.org/10.1145/3411764.3445423"><span class="No-Break"><span class="koboSpan" id="kobo.761.1">https://doi.org/10.1145/3411764.3445423</span></span></a></li>
<li><em class="italic"><span class="koboSpan" id="kobo.762.1">Shrout, P. </span><span class="koboSpan" id="kobo.762.2">E.</span></em><span class="koboSpan" id="kobo.763.1"> &amp; </span><em class="italic"><span class="koboSpan" id="kobo.764.1">Fleiss, J. </span><span class="koboSpan" id="kobo.764.2">L.</span></em><span class="koboSpan" id="kobo.765.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.766.1">1979</span></em><span class="koboSpan" id="kobo.767.1">), </span><em class="italic"><span class="koboSpan" id="kobo.768.1">Intraclass correlations: uses in assessing rater reliability</span></em><span class="koboSpan" id="kobo.769.1">, </span><em class="italic"><span class="koboSpan" id="kobo.770.1">Psychological Bulletin</span></em><span class="koboSpan" id="kobo.771.1">, </span><em class="italic"><span class="koboSpan" id="kobo.772.1">86(2)</span></em><span class="koboSpan" id="kobo.773.1">, </span><em class="italic"><span class="koboSpan" id="kobo.774.1">420.</span></em> <a href="https://psycnet.apa.org/doi/10.1037/0033-2909.86.2.420"><span class="koboSpan" id="kobo.775.1">https://psycnet.apa.org/doi/10.1037/0033-2909.86.2.420</span></a><span class="koboSpan" id="kobo.776.1">, viewed July </span><span class="No-Break"><span class="koboSpan" id="kobo.777.1">30, 2023.</span></span></li>
</ol>
</div>


<div class="Content" id="_idContainer032">
<h1 id="_idParaDest-70" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor069"/><span class="koboSpan" id="kobo.1.1">Part 3: Technical Approaches to Better Data</span></h1>
<p><span class="koboSpan" id="kobo.2.1">In this part, we explore technical approaches to enhance data quality and management in machine learning. </span><span class="koboSpan" id="kobo.2.2">We cover topics ranging from data cleaning, programmatic labeling, and synthetic data usage, to addressing bias and handling rare events. </span><span class="koboSpan" id="kobo.2.3">Each chapter gives you essential skills and knowledge to work efficiently with data in machine learning, highlighting how important good quality data is in building robust </span><span class="No-Break"><span class="koboSpan" id="kobo.3.1">ML systems.</span></span></p>
<p><span class="koboSpan" id="kobo.4.1">This part has the </span><span class="No-Break"><span class="koboSpan" id="kobo.5.1">following chapters:</span></span></p>
<ul>
<li><a href="B19297_05.xhtml#_idTextAnchor070"><em class="italic"><span class="koboSpan" id="kobo.6.1">Chapter 5</span></em></a><em class="italic"><span class="koboSpan" id="kobo.7.1">, Techniques for Data Cleaning</span></em></li>
<li><a href="B19297_06.xhtml#_idTextAnchor089"><em class="italic"><span class="koboSpan" id="kobo.8.1">Chapter 6</span></em></a><em class="italic"><span class="koboSpan" id="kobo.9.1">, Techniques for Programmatic Labeling in Machine Learning</span></em></li>
<li><a href="B19297_07.xhtml#_idTextAnchor111"><em class="italic"><span class="koboSpan" id="kobo.10.1">Chapter 7</span></em></a><em class="italic"><span class="koboSpan" id="kobo.11.1">, Using Synthetic Data in Data-Centric Machine Learning</span></em></li>
<li><a href="B19297_08.xhtml#_idTextAnchor125"><em class="italic"><span class="koboSpan" id="kobo.12.1">Chapter 8</span></em></a><em class="italic"><span class="koboSpan" id="kobo.13.1">, Techniques for Identifying and Removing Bias</span></em></li>
<li><a href="B19297_09.xhtml#_idTextAnchor141"><em class="italic"><span class="koboSpan" id="kobo.14.1">Chapter 9</span></em></a><em class="italic"><span class="koboSpan" id="kobo.15.1">, Dealing with Edge Cases and Rare Events in Machine Learning</span></em></li>
</ul>
</div>
<div>
<div id="_idContainer033">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer034">
</div>
</div>
</body></html>