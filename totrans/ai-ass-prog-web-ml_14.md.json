["```py\nimport numpy as np\nfrom tensorflow.keras.datasets import fashion_mnist\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\n# Step 1: Load Fashion-MNIST dataset\n(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n# Step 2: Normalize pixel values\ndef normalize_images(train_data, test_data):\n    # Convert to float32 to ensure division results in float\n    train_data = train_data.astype('float32')\n    test_data = test_data.astype('float32')\n    # Normalize pixel values to the range 0-1\n    train_data /= 255.0\n    test_data /= 255.0\n    return train_data, test_data\n# Step 3: Flatten images into vectors\ndef flatten_images(train_data, test_data):\n    # Reshape images to vectors\n    train_data = train_data.reshape(train_data.shape[0], -1)\n    test_data = test_data.reshape(test_data.shape[0], -1)\n    return train_data, test_data\n# Step 4: Encode categorical labels\ndef encode_labels(train_labels, test_labels):\n    # Initialize LabelEncoder\n    label_encoder = LabelEncoder()\n    # Fit LabelEncoder on training labels and transform both training and testing labels\n    train_labels = label_encoder.fit_transform(train_labels)\n    test_labels = label_encoder.transform(test_labels)\n    return train_labels, test_labels\n# Apply data preprocessing steps\nx_train, x_test = normalize_images(x_train, x_test)\nx_train, x_test = flatten_images(x_train, x_test)\ny_train, y_test = encode_labels(y_train, y_test)\n# Print the shapes of preprocessed data\nprint(\"x_train shape:\", x_train.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"x_test shape:\", x_test.shape)\nprint(\"y_test shape:\", y_test.shape) \n```", "```py\n# Step 5: Split dataset into training, validation, and testing sets\ndef split_dataset(train_data, train_labels, test_data, test_labels, validation_size=0.1, test_size=0.1, random_state=42):\n    # Split training set into training and validation sets\n    x_train, x_val, y_train, y_val = train_test_split(train_data, train_labels, test_size=validation_size, random_state=random_state)\n    # Split testing set into testing and validation sets\n    x_test, x_val, y_test, y_val = train_test_split(test_data, test_labels, test_size=test_size, random_state=random_state)\n    return x_train, y_train, x_val, y_val, x_test, y_test\n# Apply data splitting\nx_train, y_train, x_val, y_val, x_test, y_test = split_dataset(x_train, y_train, x_test, y_test)\n# Print the shapes of split datasets\nprint(\"Training set:\")\nprint(\"x_train shape:\", x_train.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"\\nValidation set:\")\nprint(\"x_val shape:\", x_val.shape)\nprint(\"y_val shape:\", y_val.shape)\nprint(\"\\nTesting set:\")\nprint(\"x_test shape:\", x_test.shape)\nprint(\"y_test shape:\", y_test.shape) \n```", "```py\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n# Step 6: Define a single-layer MLP model\ndef create_single_layer_mlp(input_shape, num_classes):\n    model = Sequential()\n    # Add a single hidden layer with ReLU activation\n    model.add(Dense(128, activation='relu', input_shape=input_shape))\n    # Add output layer with softmax activation for multi-class classification\n    model.add(Dense(num_classes, activation='softmax'))\n    return model\n# Define input shape and number of classes\ninput_shape = x_train.shape[1:]\nnum_classes = len(np.unique(y_train))\n# Create the single-layer MLP model\nmodel = create_single_layer_mlp(input_shape, num_classes)\n# Print model summary\nmodel.summary() \n```", "```py\nfrom tensorflow.keras.utils import to_categorical\n# Step 7: Model Training\ndef train_model(model, x_train, y_train, x_val, y_val, batch_size=32, epochs=10):\n    # Convert class vectors to binary class matrices (one-hot encoding)\n    y_train = to_categorical(y_train)\n    y_val = to_categorical(y_val)\n    # Compile the model with categorical cross-entropy loss and Adam optimizer\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    # Train the model on the training data\n    history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_val, y_val))\n    return history\n# Train the model\nhistory = train_model(model, x_train, y_train, x_val, y_val, batch_size=32, epochs=10) \n```", "```py\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n# Step 8: Model Evaluation\ndef evaluate_model(model, x_test, y_test):\n    # Predict labels for testing data\n    y_pred = model.predict(x_test)\n    # Convert predicted labels to class labels\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    # Convert true labels to class labels\n    y_true_classes = np.argmax(y_test, axis=1)\n    # Calculate accuracy\n    accuracy = accuracy_score(y_true_classes, y_pred_classes)\n    # Calculate precision\n    precision = precision_score(y_true_classes, y_pred_classes, average='weighted')\n    # Calculate recall\n    recall = recall_score(y_true_classes, y_pred_classes, average='weighted')\n    # Calculate confusion matrix\n    cm = confusion_matrix(y_true_classes, y_pred_classes)\n    return accuracy, precision, recall, cm\n# Evaluate the model\naccuracy, precision, recall, cm = evaluate_model(model, x_test, y_test)\n# Print evaluation metrics\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"Confusion Matrix:\\n\", cm)\n# Visualize confusion matrix\nplt.imshow(cm, cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.colorbar()\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show() \n```", "```py\n------282/282 [==============================] - 2s 5ms/step --------------------------------------------------------------------------- AxisError Traceback (most recent call last) <ipython-input-5-02193580c831> in <cell line: 23>() 21 22 # Evaluate the model ---> 23 accuracy, precision, recall, cm = evaluate_model(model, x_test, y_test) 24 25 # Print evaluation metrics 2 frames /usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py in _wrapfunc(obj, method, *args, **kwds) 57 58 try: ---> 59 return bound(*args, **kwds) 60 except TypeError: 61 # A TypeError occurs if the object does have such a method in its AxisError: axis 1 is out of bounds for array of dimension 1 \n```", "```py\nprint(\"Shape of y_test:\", y_test.shape) \n```", "```py\nShape of y_test: (9000,) \n```", "```py\n# Step 8: Model Evaluation\ndef evaluate_model(model, x_test, y_test):\n    # Predict labels for testing data\n    y_pred = model.predict(x_test)\n    # Convert predicted labels to class labels\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    # Calculate accuracy\n    accuracy = accuracy_score(y_test, y_pred_classes)\n    # Calculate precision\n    precision = precision_score(y_test, y_pred_classes, average='weighted')\n    # Calculate recall\n    recall = recall_score(y_test, y_pred_classes, average='weighted')\n    # Calculate confusion matrix\n    cm = confusion_matrix(y_test, y_pred_classes)\n    return accuracy, precision, recall, cm\n# Evaluate the model\naccuracy, precision, recall, cm = evaluate_model(model, x_test, y_test)\n# Print evaluation metrics\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"Confusion Matrix:\\n\", cm)\n# Visualize confusion matrix\nplt.imshow(cm, cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.colorbar()\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show() \n```", "```py\n# Define a two-layer MLP model\ndef create_two_layer_mlp(input_shape, num_classes):\n    model = Sequential()\n    # Add first hidden layer with ReLU activation\n    model.add(Dense(128, activation='relu', input_shape=input_shape))\n    # Add second hidden layer with ReLU activation\n    model.add(Dense(64, activation='relu'))\n    # Add output layer with softmax activation for multi-class classification\n    model.add(Dense(num_classes, activation='softmax'))\n    return model\n# Define a three-layer MLP model\ndef create_three_layer_mlp(input_shape, num_classes):\n    model = Sequential()\n    # Add first hidden layer with ReLU activation\n    model.add(Dense(128, activation='relu', input_shape=input_shape))\n    # Add second hidden layer with ReLU activation\n    model.add(Dense(64, activation='relu'))\n    # Add third hidden layer with ReLU activation\n    model.add(Dense(32, activation='relu'))\n    # Add output layer with softmax activation for multi-class classification\n    model.add(Dense(num_classes, activation='softmax'))\n    return model\n# Create two-layer MLP model\ntwo_layer_model = create_two_layer_mlp(input_shape, num_classes)\n# Create three-layer MLP model\nthree_layer_model = create_three_layer_mlp(input_shape, num_classes)\n# Train two-layer MLP model\ntwo_layer_history = train_model(two_layer_model, x_train, y_train, x_val, y_val)\n# Train three-layer MLP model\nthree_layer_history = train_model(three_layer_model, x_train, y_train, x_val, y_val) \n```", "```py\n# Evaluate two-layer MLP model\ntwo_layer_accuracy, _, _, _ = evaluate_model(two_layer_model, x_test, y_test)\n# Evaluate three-layer MLP model\nthree_layer_accuracy, _, _, _ = evaluate_model(three_layer_model, x_test, y_test)\n# Print model comparison\nprint(\"Single-layer MLP Accuracy:\", accuracy)\nprint(\"Two-layer MLP Accuracy:\", two_layer_accuracy)\nprint(\"Three-layer MLP Accuracy:\", three_layer_accuracy) \n```", "```py\n# Define and train model with batch size = 32\nmodel_batch_32 = create_single_layer_mlp(input_shape, num_classes)\nhistory_batch_32 = train_model(model_batch_32, x_train, y_train, x_val, y_val, batch_size=32)\n# Define and train model with batch size = 64\nmodel_batch_64 = create_single_layer_mlp(input_shape, num_classes)\nhistory_batch_64 = train_model(model_batch_64, x_train, y_train, x_val, y_val, batch_size=64)\n# Define and train model with batch size = 128\nmodel_batch_128 = create_single_layer_mlp(input_shape, num_classes)\nhistory_batch_128 = train_model(model_batch_128, x_train, y_train, x_val, y_val, batch_size=128) \n```", "```py\n# Evaluate models with different batch sizes\naccuracy_batch_32, _, _, _ = evaluate_model(model_batch_32, x_test, y_test)\naccuracy_batch_64, _, _, _ = evaluate_model(model_batch_64, x_test, y_test)\naccuracy_batch_128, _, _, _ = evaluate_model(model_batch_128, x_test, y_test)\n# Print model comparison\nprint(\"Single-layer MLP Accuracy (Batch Size = 32):\", accuracy_batch_32)\nprint(\"Single-layer MLP Accuracy (Batch Size = 64):\", accuracy_batch_64)\nprint(\"Single-layer MLP Accuracy (Batch Size = 128):\", accuracy_batch_128) \n```", "```py\n282/282 [==============================] - 1s 3ms/step 282/282 [==============================] - 1s 4ms/step 282/282 [==============================] - 1s 3ms/step \nSingle-layer MLP Accuracy (Batch Size = 32): 0.88 \nSingle-layer MLP Accuracy (Batch Size = 64): 0.8723333333333333 \nSingle-layer MLP Accuracy (Batch Size = 128): 0.8797777777777778 \n```", "```py\n# Define and train model with 64 neurons in the hidden layer\nmodel_neurons_64 = create_single_layer_mlp(input_shape, num_classes)\nhistory_neurons_64 = train_model(model_neurons_64, x_train, y_train, x_val, y_val)\n# Define and train model with 128 neurons in the hidden layer\nmodel_neurons_128 = create_single_layer_mlp(input_shape, num_classes)\nhistory_neurons_128 = train_model(model_neurons_128, x_train, y_train, x_val, y_val)\n# Define and train model with 256 neurons in the hidden layer\nmodel_neurons_256 = create_single_layer_mlp(input_shape, num_classes)\nhistory_neurons_256 = train_model(model_neurons_256, x_train, y_train, x_val, y_val) \n```", "```py\n# Evaluate models with different number of neurons\naccuracy_neurons_64, _, _, _ = evaluate_model(model_neurons_64, x_test, y_test)\naccuracy_neurons_128, _, _, _ = evaluate_model(model_neurons_128, x_test, y_test)\naccuracy_neurons_256, _, _, _ = evaluate_model(model_neurons_256, x_test, y_test)\n# Print model comparison\nprint(\"Single-layer MLP Accuracy (Neurons = 64):\", accuracy_neurons_64)\nprint(\"Single-layer MLP Accuracy (Neurons = 128):\", accuracy_neurons_128)\nprint(\"Single-layer MLP Accuracy (Neurons = 256):\", accuracy_neurons_256) \n```", "```py\nfrom tensorflow.keras.optimizers import SGD, Adam, RMSprop\n# Define and train model with SGD optimizer\ndef train_model_sgd(model, x_train, y_train, x_val, y_val, batch_size=32, epochs=10):\n    # One-hot encode the class labels\n    y_train_encoded = to_categorical(y_train)\n    y_val_encoded = to_categorical(y_val)\n    # Compile the model with SGD optimizer\n    model.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['accuracy'])\n    # Train the model on the training data\n    history = model.fit(x_train, y_train_encoded, batch_size=batch_size, epochs=epochs, validation_data=(x_val, y_val_encoded))\n    return history\n# Define and train model with Adam optimizer\ndef train_model_adam(model, x_train, y_train, x_val, y_val, batch_size=32, epochs=10):\n    # One-hot encode the class labels\n    y_train_encoded = to_categorical(y_train)\n    y_val_encoded = to_categorical(y_val)\n    # Compile the model with Adam optimizer\n    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n    # Train the model on the training data\n    history = model.fit(x_train, y_train_encoded, batch_size=batch_size, epochs=epochs, validation_data=(x_val, y_val_encoded))\n    return history\n# Define and train model with RMSprop optimizer\ndef train_model_rmsprop(model, x_train, y_train, x_val, y_val, batch_size=32, epochs=10):\n    # One-hot encode the class labels\n    y_train_encoded = to_categorical(y_train)\n    y_val_encoded = to_categorical(y_val)\n    # Compile the model with RMSprop optimizer\n    model.compile(loss='categorical_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])\n    # Train the model on the training data\n    history = model.fit(x_train, y_train_encoded, batch_size=batch_size, epochs=epochs, validation_data=(x_val, y_val_encoded))\n    return history\n# Train model with SGD optimizer\nmodel_sgd = create_single_layer_mlp(input_shape, num_classes)\nhistory_sgd = train_model_sgd(model_sgd, x_train, y_train, x_val, y_val)\n# Train model with Adam optimizer\nmodel_adam = create_single_layer_mlp(input_shape, num_classes)\nhistory_adam = train_model_adam(model_adam, x_train, y_train, x_val, y_val)\n# Train model with RMSprop optimizer\nmodel_rmsprop = create_single_layer_mlp(input_shape, num_classes)\nhistory_rmsprop = train_model_rmsprop(model_rmsprop, x_train, y_train, x_val, y_val) \n```", "```py\n# Evaluate models with different optimizers\naccuracy_sgd, _, _, _ = evaluate_model(model_sgd, x_test, y_test)\naccuracy_adam, _, _, _ = evaluate_model(model_adam, x_test, y_test)\naccuracy_rmsprop, _, _, _ = evaluate_model(model_rmsprop, x_test, y_test)\n# Print model comparison\nprint(\"Single-layer MLP Accuracy (SGD):\", accuracy_sgd)\nprint(\"Single-layer MLP Accuracy (Adam):\", accuracy_adam)\nprint(\"Single-layer MLP Accuracy (RMSprop):\", accuracy_rmsprop) \n```"]