- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Leveraging Amazon Redshift ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we discussed the overall benefits of **machine learning**
    (**ML**) and how it fits into your data warehouse.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will focus specifically on how to leverage **Amazon Redshift
    ML** to solve various use cases. These examples are designed to give you the foundation
    you need as you get hands-on training models, beginning in [*Chapter 5*](B19071_05.xhtml#_idTextAnchor068).
    We will show the benefits of Redshift ML, such as eliminating data movement, being
    able to create models using simple SQL, and drastically reducing the time it takes
    to train a new model and make it available for inference. Additionally, you will
    learn how Amazon Redshift ML leverages **Amazon SageMaker** behind the scenes
    to automatically train your models as we guide you through the following main
    topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Why Amazon Redshift ML?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An introduction to Amazon Redshift ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `CREATE` `MODEL` overview
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why Amazon Redshift ML?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon Redshift ML gives you the ability to create and train ML models with
    simple SQL commands, without the need to build specialized skills. This means
    your data analysts, data engineers, and BI analysts can now leverage their SQL
    skills to do ML, which increases agility, since they no longer need to wait for
    an ML expert to train their model.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, since you use your model in the data warehouse, you no longer
    need to export data to be trained or import it back into the warehouse after your
    model is used to make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: You do not have to worry about managing the governance of data. Data never leaves
    your VPC when you export data for training.
  prefs: []
  type: TYPE_NORMAL
- en: You can control who can create models and who can run inference queries on those
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Redshift ML provides a very cost-effective solution for training and
    using models. The cost for Amazon SageMaker resources is based on the number of
    cells in your training dataset, which is the product of the number of rows times
    the number of columns in the training set.
  prefs: []
  type: TYPE_NORMAL
- en: The costs for running prediction queries using Amazon Redshift Serverless are
    based on the compute capacity used by your queries.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about Amazon Redshift Serverless costs refer here [https://docs.aws.amazon.com/redshift/latest/mgmt/serverless-billing.html](https://docs.aws.amazon.com/redshift/latest/mgmt/serverless-billing.html).
  prefs: []
  type: TYPE_NORMAL
- en: You have the ability to control the costs of model training by limiting how
    much data is used to train the model, and by controlling the time for training.
    We will show you examples of this later in the *A CREATE MODEL* *overview* section.
  prefs: []
  type: TYPE_NORMAL
- en: When you run a prediction query, all predictions are computed locally in your
    Redshift data warehouse. This enables you to achieve very high throughput and
    low latency.
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to Amazon Redshift ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By leveraging Amazon Redshift ML, your organization can achieve many benefits.
    First of all, you eliminate unnecessary data movement, users can use familiar
    SQL commands, and integration with Amazon SageMaker is transparent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s define some of the terms that you will see throughout the remaining chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**CREATE MODEL**: This is a command that will contain the SQL that will export
    data to be used to train your model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Features**: These are the attributes in your dataset that will be used as
    input to train your model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Target**: This is the attribute in your dataset that you want to predict.
    This is also sometimes referred to as a **label**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inference**: This is also referred to as **prediction**. In Amazon Redshift
    ML, this is the process of executing a query against a trained model to get the
    predicted value generated by your model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To be able to create and access your ML models in Amazon Redshift to run prediction
    queries, you need to grant permissions on the model object, just like you would
    on other database objects such as tables, views, or functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s assume you have created the following role to allow a set of users to
    create models, called `analyst_cm_role`. A superuser can grant permissions to
    this role as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Users/groups/roles with the `CREATE MODEL` privilege can create a model in
    any schema in your serverless endpoint or Redshift cluster if the user has the
    `CREATE` permission on the Schema. A Redshift ML model is part of the schema hierarchy,
    similar to tables, views, stored procedures, and user-defined functions. Let’s
    assume we have a schema called `demo_ml`. You can grant `CREATE` and `USAGE` privileges
    on the `demo_ml` schema to the analyst role using the following `GRANT` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s assume we have another role to allow a set of users access to run
    prediction queries called `analyst_prediction_role`. You can grant access to run
    predictions on models using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The source data to create a model can be in Redshift or any other source that
    you can access from Redshift, including your **Amazon Simple Storage Service**
    (**Amazon S3**) S3 data lake via Spectrum or other sources using the Redshift
    federated query capability. At the time of writing, Amazon Aurora and Amazon RDS
    for PostgreSQL and MySQL are supported. More details are available here: [https://docs.aws.amazon.com/redshift/latest/dg/federated-overview.html](https://docs.aws.amazon.com/redshift/latest/dg/federated-overview.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Redshift ML and Amazon SageMaker manage all data conversions, permissions,
    and resource usage. The trained model is then compiled by SageMaker Neo and made
    available as a user-defined function in Amazon Redshift so that users can make
    predictions using simple SQL.
  prefs: []
  type: TYPE_NORMAL
- en: Once your model is trained and available as a function in Amazon Redshift, you
    can run prediction queries at scale and efficiently, locally in Amazon Redshift.
  prefs: []
  type: TYPE_NORMAL
- en: 'See the process flow here in *Figure 4**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – The Redshift ML CREATE MODEL process flow](img/B19071_04_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – The Redshift ML CREATE MODEL process flow
  prefs: []
  type: TYPE_NORMAL
- en: Now, let us go into more detail on how you can use the `CREATE` `MODEL` statement.
  prefs: []
  type: TYPE_NORMAL
- en: A CREATE MODEL overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `CREATE MODEL` statement allows for flexibility when addressing the various
    use cases you may need. There are four main types of `CREATE` `MODEL` statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '`AUTO` everything'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AUTO` with user guidance, where a user can provide the problem type'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AUTO OFF`, with customized options provided by the user'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bring your own** **model** (**BYOM**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 4**.2* illustrates the flexibility available when training models with
    Amazon Redshift ML:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Amazon Redshift ML flexibility](img/B19071_04_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – Amazon Redshift ML flexibility
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will provide an overview of the various types of `CREATE
    MODEL` statements. Subsequent chapters will provide in-depth examples of how to
    create all the different types of models, load the data to Redshift, and split
    your data into training and testing datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will walk you through the options available to create models
    and the optional parameters available that you can specify. All of the examples
    in this chapter are informational to prepare you for the remaining chapters. You
    will create your first model in [*Chapter 5*](B19071_05.xhtml#_idTextAnchor068).
  prefs: []
  type: TYPE_NORMAL
- en: AUTO everything
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you execute a `CREATE MODEL` command to solve a supervised learning problem
    using `AUTO` everything, Amazon Redshift ML and Amazon SageMaker manage all the
    data preprocessing, model training, and model tuning for you. Data will be exported
    from Amazon Redshift to Amazon S3, where SageMaker will train and tune up to 100
    models. **SageMaker Autopilot** will automatically determine the algorithm and
    problem type. The best-trained model is then compiled by SageMaker Neo and made
    available as a user-defined function in Amazon Redshift so that users can make
    predictions using simple SQL.
  prefs: []
  type: TYPE_NORMAL
- en: 'See the following syntax for an `AUTO` everything model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: You simply supply a table name or SQL statement for the data you want to use
    in training, along with the `TARGET` column that you are trying to predict.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s apply this to a simple example. Let’s assume we have a table called `reservation_history`
    that contains hotel reservation data, and we want to determine whether guests
    are likely to cancel an upcoming reservation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `CREATE MODEL` statement would look like this (note that this is informational;
    you do not need to run this):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In this `CREATE MODEL` statement, we only provided the minimum required parameters,
    which are `IAM_ROLE` and `S3_BUCKET`. The `TARGET` parameter is `cancelled`, which
    is what we will try to predict, based on the input we send to the `CREATE MODEL`
    statement. In this example, we send everything from the `reservation_history`
    table. The `FUNCTION` name is a description of the function that will be used
    later for predictions. The `IAM_ROLE` parameter will be attached to your serverless
    endpoint and provides access to SageMaker and an `CREATE MODEL` statement. Refer
    to [*Chapter 2*](B19071_02.xhtml#_idTextAnchor027), where we showed how to set
    up an IAM role.
  prefs: []
  type: TYPE_NORMAL
- en: 'Amazon SageMaker will automatically determine that this is a binary classification
    model, since our `TARGET` can only be one of two possible values. Amazon SageMaker
    will also choose the best model type. At the time of writing, the supported model
    types for supervised learning are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`XGBoost`: Based on the gradient-boosted trees algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Linear Learner`: Provides an increase in speed to solve either classification
    or regression problems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MLP`: A deep learning algorithm using a multilayer perceptron'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will create models using each of these models in subsequent chapters.
  prefs: []
  type: TYPE_NORMAL
- en: AUTO with user guidance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: More advanced users with a good understanding of ML may wish to provide more
    inputs to a model, such as `model _type`, `problem_type`, `preprocesors`, and
    `objective`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using our reservation example, we will build on the `AUTO` capabilities and
    specify a few more parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`MODEL_TYPE`: `XGBoost`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PROBLEM_TYPE`: `binary_classification`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Objective`: `F1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`S3_GARBAGE_COLLECT` – `OFF`: If set to `OFF`, the resulting datasets used
    to train the models remain in Amazon S3 and can be used for other purposes, such
    as troubleshooting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MAX_RUNTIME` – `1800`: This is one way to control the costs of model training
    by limiting the training time to `1800` seconds; the default is `5400` seconds'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By specifying `MODEL_TYPE` and/or `PROBLEM_TYPE` along with the `Objective`
    parameters, you can shorten the amount of time needed to train a model, since
    SageMaker does not have to determine these. Here is an example of the `CREATE`
    `MODEL` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Increasing `MAX_RUNTIME` and `MAX_CELLS` often improves model quality by allowing
    SageMaker to explore more candidates. If you want faster iteration or exploration
    of your dataset, reduce `MAX_RUNTIME` and `MAX_CELLS`. If you want improved accuracy
    of models, increase `MAX_RUNTIME` and `MAX_CELLS`.
  prefs: []
  type: TYPE_NORMAL
- en: It is a good practice to specify the problem type and objective, if known, to
    shorten training time. To improve model accuracy, provide more data if possible
    and include any features (input) that can influence the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, you can add your own preprocessors by specifying transformers.
    At the time of writing, Amazon Redshift ML supports 10 transformers including
    `OneHotEncoder`, `Ordinal Encoder`, and `StandardScaler`. You can find the complete
    list here: [https://docs.aws.amazon.com/redshift/latest/dg/r_create_model_use_cases.html#r_user_guidance_create_model](https://docs.aws.amazon.com/redshift/latest/dg/r_create_model_use_cases.html#r_user_guidance_create_model).'
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Redshift ML stores the trained transformers and automatically applies
    them as part of the prediction query. You don’t need to specify them when generating
    predictions from your model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take, as an example, using `OneHotEncoder`, which is used to convert
    a categorical value such as `country` or `gender` into a numeric value (binary
    vector) so that ML algorithms can better do predictions. Let’s create a model
    using one-hot encoding for our input columns, `marital_status` and `loyalty_program`.
    Note that this model is an example, and you do not need to run this statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: So far, all the `CREATE MODEL` examples we showed use `AUTO ON`. This is the
    default if you do not specify this parameter. Now, let’s move on to how you can
    do your own model tuning using `AUTO OFF` with XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost (AUTO OFF)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As an ML expert, you have the option to do hyperparameter tuning by using the
    `AUTO OFF` option with the `CREATE MODEL` statement. This gives you full control
    and Amazon Redshift ML does not attempt to discover the optimal preprocessors,
    algorithms, and hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see what the `CREATE MODEL` syntax looks like using our example reservation
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will specify the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`AUTO OFF`: Turns off the automatic discovery of a preprocessor, an algorithm,
    and hyperparameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MODEL_TYPE`:- `xgboost`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OBJECTIVE`: `''binary:logistic''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PREPROCESSORS`: `''none''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HYPERPARAMETERS`: `DEFAULT` `EXCEPT(NUM_ROUND ''100''/)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer here for a list of hyperparameters for XGBoost: [https://docs.amazonaws.cn/en_us/redshift/latest/dg/](https://docs.amazonaws.cn/en_us/redshift/latest/dg/):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'As of this writing, `''none''` is the only available option to specify for
    `PREPROCESSORS` when using `AUTO OFF`. Since we cannot specify one-hot encoding,
    we can use a case statement with our SQL to apply this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In [*Chapter 10*](B19071_10.xhtml#_idTextAnchor178), you will build an XGBoost
    model using `AUTO OFF` and gain a better understanding of this option.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s take a look at another `AUTO OFF` option using the K-means algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: K-means (AUTO OFF)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The K-means algorithm is used to group data together that isn’t labeled. Since
    this algorithm discovers groupings in your data, it solves an “*unsupervised*”
    learning problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what a sample `CREATE MODEL` looks like if we want to group our `reservation_history`
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '`AUTO OFF`: Turns off the automatic discovery of a preprocessor, an algorithm,
    and hyperparameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MODEL_TYPE`: `KMEANS`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PREPROCESSORS`: `OPTIONAL` (at the time of writing, Amazon Redshift supports
    `StandScaler`, `MinMax`, and `NumericPassthrough` for `KMEANS`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HYPERPARAMETERS`: `DEFAULT EXCEPT (K ''N'')`, where `N` is the number of clusters
    you want to create'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is an example of a `CREATE MODE``L` statement. Note that you will not
    run this statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Note that we are creating five clusters with this model. With the K-means algorithm,
    it is important to experiment with a different number of clusters. In [*Chapter
    8*](B19071_08.xhtml#_idTextAnchor139), you will get to dive deep into creating
    K-means models and determining how to validate the optimal clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s take a look at how you can run prediction queries using models built
    outside of Amazon Redshift ML.
  prefs: []
  type: TYPE_NORMAL
- en: BYOM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Additionally, you can use a model trained outside of Amazon Redshift with Amazon
    SageMaker for either local or remote inference in Amazon Redshift.
  prefs: []
  type: TYPE_NORMAL
- en: Local inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Local inference is used when models are trained outside of Redshift in Amazon
    SageMaker. This allows you to run inference queries inside of Amazon Redshift
    without having to retrain a model.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s suppose our previous example of building a model to predict whether a
    customer will cancel a reservation was trained outside of Amazon Redshift. We
    can bring that model to Redshift and then run inference queries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our `CREATE MODEL` sample will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`model_name`: This is the name you wish to give the local model in Redshift'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FROM`: This is `job_name` from Amazon SageMaker – you can find this in Amazon
    SageMaker under **Training Jobs**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FUNCTION`: The name of the function to be created along with the input data
    types'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RETURNS`: The data type of the value returned by the function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the data types in `FUNCTION` match the data types from our `reservation_history`
    table, and `RETURNS` matches the data type of our `TARGET` variable, which is
    `cancelled`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can derive the SageMaker `JobName` by navigating to the AWS Management
    Console and going to SageMaker:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Console Home](img/B19071_04_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Console Home
  prefs: []
  type: TYPE_NORMAL
- en: After clicking on **Amazon SageMaker**, click on **Training jobs**, as shown
    in *Figure 4**.4:*
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Training jobs](img/B19071_04_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Training jobs
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, note the job name of the model you wish to use for local inference, which
    is what you will put in your `CREATE MODEL` statement (see *Figure 4**.5*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – The training job name](img/B19071_04_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – The training job name
  prefs: []
  type: TYPE_NORMAL
- en: Remote inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Remote inference is useful if you have a model created in SageMaker for an algorithm
    that is not available natively in Amazon Redshift ML. For example, anomaly detection
    can be done using the Random Cut Forest algorithm from SageMaker. You can create
    a model that references the endpoint of the SageMaker model and then be able to
    run anomaly detection in Amazon Redshift.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our `CREATE MODEL` sample will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`model_name`: The name you wish to give the local model in Redshift'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FUNCTION`: The name of the function to be created along with the input data
    types'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RETURNS`: The data type of the value returned by the function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SAGEMAKER`: The name of the Amazon SageMaker endpoint:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Note that the data types in `FUNCTION` are for the input we send, and `RETURNS`
    is the data type of the data we receive when invoking the function.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can derive the SageMaker endpoint by navigating to the AWS Management Console,
    going to SageMaker, and then clicking on **Endpoints**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Endpoints](img/B19071_04_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – Endpoints
  prefs: []
  type: TYPE_NORMAL
- en: 'After you click on **Endpoints**, as shown in *Figure 4**.6*, you can see the
    endpoint names, as shown in *Figure 4**.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – The endpoint names](img/B19071_04_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – The endpoint names
  prefs: []
  type: TYPE_NORMAL
- en: Then, note the name of the endpoint for the model you wish to use for remote
    inference and put it in your `CREATE` `MODEL` statement.
  prefs: []
  type: TYPE_NORMAL
- en: You will dive deep into BYOM in [*Chapter 11*](B19071_11.xhtml#_idTextAnchor192)
    and get hands-on experience creating models for both local and remote inference.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed why Amazon Redshift ML is a good choice to use
    data in your data warehouse to make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: By bringing ML to your data warehouse, Amazon Redshift ML enables you to greatly
    shorten the amount of time to create and train models by putting the power of
    ML directly in the hands of your developers, data analysts, and BI professionals.
  prefs: []
  type: TYPE_NORMAL
- en: Your data remains secure; it never leaves your VPC. Plus, you can easily control
    access to create and use models.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we showed you different methods of creating models in Redshift ML,
    such as using `AUTO`, how to guide model training, and an advanced method to supply
    hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Now, you understand how ML fits into your data warehouse, how to use proper
    security and configuration guidelines with Redshift ML, and how a model is trained
    in Amazon SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will get hands-on and create your first model using
    Amazon Redshift ML, learn how to validate the model, and learn how to run an inference
    query.
  prefs: []
  type: TYPE_NORMAL
