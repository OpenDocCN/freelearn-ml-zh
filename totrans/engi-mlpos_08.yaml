- en: 'Chapter 6: Key Principles for Deploying Your ML System'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn the fundamental principles for deploying **machine
    learning** (**ML**) models in production and implement the hands-on deployment
    of ML models for the business problem we have been working on. To get a comprehensive
    understanding and first-hand experience, we will deploy ML models that were trained
    and packaged previously (in [*Chapter 4*](B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074),
    *Machine Learning Pipelines*, and [*Chapter 5*](B16572_05_Final_JM_ePub.xhtml#_idTextAnchor093),
    *Model Evaluation and Packaging*) using the Azure ML service on two different
    deployment targets: an Azure container instance and a Kubernetes cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: We will also learn how to deploy ML models using an open source framework called
    MLflow that we have already worked with. This will enable you to get an understanding
    of deploying ML models as REST API endpoints on diverse deployment targets using
    two different tools (the Azure ML service and MLflow). This will equip you with
    the skills required to deploy ML models for any given scenario on the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we start by looking at how ML is different in research and
    production and continue exploring the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: ML in research versus production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the types of ML inference in production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going through the mapping infrastructure for your solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hands-on deployment (for the business problem)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the need for continuous integration and continuous deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML in research versus production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML in research is implemented with specific goals and priorities to improve
    the state of the art in the field, whereas the aim of ML in production is to optimize,
    automate, or augment a scenario or a business.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to understand the deployment of ML models, let''s start by comparing
    how ML is implemented in research versus production (in the industry). Multiple
    factors, such as performance, priority, data, fairness, and interpretability,
    as listed in *Table 6.1*, depict how deployments and ML work differently in research
    and production:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 6.1 – ML in research and production'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Table_6.1.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.1 – ML in research and production
  prefs: []
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In general, data in research projects is static because data scientists or
    statisticians are working on a set dataset and trying to beat the current state-of-the-art
    models. For example, recently, many breakthroughs in natural language processing
    models have been witnessed, for instance, with BERT from Google or XLNet from
    Baidu. To train these models, data was scraped and compiled into a static dataset.
    In the research world, to evaluate or benchmark the performance of the models,
    static datasets are used to evaluate the performance, as shown in *Table 6.2*
    (source: [https://arxiv.org/abs/1906.08237](https://arxiv.org/abs/1906.08237)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 6.2 – BERT versus XLNet performance (in research)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Table_6.2.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.2 – BERT versus XLNet performance (in research)
  prefs: []
  type: TYPE_NORMAL
- en: For instance, we can compare the performance of two models by comparing their
    performance on a popular dataset called SQUAD (10,000+ QnA) version 1.1, on which
    BERT performs with 92.8% accuracy and XLNET with 94.0% accuracy. Likewise, data
    used in research for training and evaluating models is static, whereas data in
    production or in industrial use cases is dynamic and constantly changing as per
    the environment, operations, business, or users.
  prefs: []
  type: TYPE_NORMAL
- en: Fairness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In real life, biased models can be costly. Unfair or biased decisions will
    lead to poor choices for business and operations. For ML models in production,
    it is important that decisions made are as fair as possible. It can be costly
    for the business if the models in production are not fair. For example, recently,
    Amazon made HR screening software that screens applicants based on their suitability
    for the job they applied for. ML specialists at Amazon discovered that male candidates
    were favored over female candidates (source: [https://www.businessinsider.com/amazon-ai-biased-against-women-no-surprise-sandra-wachter-2018-10](https://www.businessinsider.com/amazon-ai-biased-against-women-no-surprise-sandra-wachter-2018-10)).
    This kind of system bias can be costly because, in Amazon''s case, you can miss
    out on some amazing talent as a result of bias. Hence having fair models in production
    is critical and should be monitored constantly. In research, fair models are important
    as well but not as critical as in production or real life, and fairness is not
    critically monitored as in production. The goal in research is to beat the state
    of the art, and model fairness is a secondary goal.'
  prefs: []
  type: TYPE_NORMAL
- en: Interpretability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Model interpretability is critical in production in order to understand the
    correlation or causality between the ML model's decisions and its impact on the
    operations or business to optimize, augment, or automate a business or task at
    hand. This is not the case in research, where the goal is to challenge or beat
    the state-of-the-art results, and here the priority is better performance (such
    as accuracy, or other metrics). In the case of research, ML model interpretability
    is good to have but not mandatory. Typically, ML projects are more concerned with
    predicting outcomes than with understanding causality. ML models are great at
    finding correlations in data, but not causation. We strive not to fall into the
    pit of equating association with the cause in our ventures. Our ability to rely
    on ML is severely hampered as a result of this issue. This problem severely limits
    our ability to use ML to make decisions. We need resources that can understand
    the causal relationships between data and build ML solutions that can generalize
    well from a business viewpoint. Having the right model interpretability mechanisms
    can enhance our understanding of causality and enable us to craft ML solutions
    that generalize well and are able to handle previously unseen data. As a result,
    we can make more reliable and transparent decisions using ML.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of production (in a business use case), a lack of interpretability
    is not recommended at all. Let us look at a hypothetical case. Let's assume you
    have cancer and have to choose a surgeon to perform your surgery. Two surgeons
    are available, one is human (with an 80% cure rate) and another is an AI black-box
    model (with a 90% cure rate) that cannot be interpreted or explain how it works,
    but it has a high cure rate. What would you choose? AI or a surgeon to cure cancer?
    It would be easier to replace the surgeon with AI if the model was not a black-box
    model. Though the AI is better than the surgeon, without understanding the model,
    decision, trust and compliance is an issue. Model interpretability is essential
    to make legal decisions. Hence, it is vital to have model interpretability for
    ML in production. We will learn more about this in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When it comes to the performance of the ML models, the focus in research is
    to improve on the state-of-the-art models, whereas in production the focus is
    to build better models than simpler models that serve the business needs (**state-of-the-art**
    models are not the focus).
  prefs: []
  type: TYPE_NORMAL
- en: Priority
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In research, training the models faster and better is the priority, whereas
    in production faster inference is the priority as the focus is to make decisions
    and serve the business needs in real time.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the types of ML inference in production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we saw the priorities of ML in research and production.
    To serve the business needs in production, ML models are inferred using various
    deployment targets, depending on the need. Predicting or making a decision using
    an ML model is called ML model inference. Let's explore ways of deploying ML models
    on different deployment targets to facilitate ML inference as per the business
    needs.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment targets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will look at different types of deployment targets and why
    and how we serve ML models for inference in these deployment targets. Let's start
    by looking at a virtual machine or an on-premises server.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual machines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Virtual machines can be on the cloud or on-premises, depending on the IT setup
    of a business or an organization. Serving ML models on virtual machines is quite
    common. ML models are served on virtual machines in the form of web services.
    The web service running on a virtual machine receives a user request (as an HTTP
    request) containing the input data. The web service, upon receiving the input
    data, preprocesses it in the required format to infer the ML model, which is part
    of the web service. After the ML model makes the prediction or performs the task,
    the output is transformed and presented in a user-readable format. Commonly into
    **JavaScript Object Notation** (**JSON**) or **Extensible Markup language string**
    (**XML**). Usually, a web service is served in the form of a REST API. REST API
    web services can be developed using multiple tools; for instance, FLASK or FAST
    API web application tools can be used to develop REST API web services using Python
    or Spring Boot in Java, or Plumber in R, depending on the need. A combination
    of virtual machines is used in parallel to scale and maintain the robustness of
    the web services.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to orchestrate the traffic and to scale the machines, a load balancer
    is used to dispatch incoming requests to the virtual machines for ML model inference.
    This way, ML models are deployed on virtual machines on the cloud or on-premises
    to serve the business needs, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Deployment on virtual machines'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_06_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.1 – Deployment on virtual machines
  prefs: []
  type: TYPE_NORMAL
- en: Containers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Containers are a reliable way to run applications using the Linux OS with customized
    settings. A container is an application running with a custom setting orchestrated
    by the developer. Containers are an alternative and more resource-efficient way
    of serving models than virtual machines. They operate like virtual machines as
    they have their own runtime environment, which is isolated and confined to memory,
    the filesystem, and processes.
  prefs: []
  type: TYPE_NORMAL
- en: Containers can be customized by developers to confine them to required resources
    such as memory, the filesystem, and processes, and the virtual machines are limited
    to such customizations. They are more flexible and operate in a modular way and
    hence provide more resource efficiency and optimization. They allow the possibility
    to scale to zero, as containers can be reduced to zero replicas and run a backup
    on request. This way, lower computation power consumption is possible compared
    to running web services on virtual machines. As a result of this lower computation
    power consumption, cost-saving on the cloud is possible.
  prefs: []
  type: TYPE_NORMAL
- en: Containers present many advantages; however, one disadvantage can be the complexity
    required to work with containers, as it requires expertise.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some differences in the way containers and virtual machines operate.
    For example, there can be multiple containers running inside a virtual machine
    that share the operating system and resources with the virtual machine, but the
    virtual machine runs its own resources and operating system. Containers can operate
    modularly, but virtual machines operate as single units. Docker is used to build
    and deploy containers; however, there are alternatives, such as Mesos and CoreOS
    rkt. A container is typically packaged with the ML model and web service to facilitate
    the ML inference, similar to how we serve the ML model wrapped in a web service
    in the virtual machine. Containers need to be orchestrated to be consumed by users.
    The orchestration of containers means the automation of the deployment, management,
    scaling, and networking of containers. Containers are orchestrated using a container
    orchestration system such as Kubernetes. In the following diagram, we can see
    container orchestration with auto-scaling (based on the traffic of requests):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Deployment on containers'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_06_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.2 – Deployment on containers
  prefs: []
  type: TYPE_NORMAL
- en: Serverless
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Serverless computing, as the name suggests, does not involve a virtual machine
    or container. It eliminates infrastructure management tasks such as OS management,
    server management, capacity provisioning, and disk management. Serverless computing
    enables developers and organizations to focus on their core product instead of
    mundane tasks such as managing and operating servers, either on the cloud or on-premises.
    Serverless computing is facilitated by using cloud-native services.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, Microsoft Azure uses Azure Functions, and AWS uses Lambda functions
    to deploy serverless applications. The deployment for serverless applications
    involves submitting a collection of files (in the form of `.zip` files) to run
    ML applications. The .zip archive typically has a file with a particular function
    or method to execute. The zip archive is uploaded to the cloud platform using
    cloud services and deployed as a serverless application. The deployed application
    serves as an API endpoint to submit input to the serverless application serving
    the ML model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deploying ML models using serverless applications can have many advantages:
    there''s no need to install or upgrade dependencies, or maintain or upgrade systems.
    Serverless applications auto-scale on demand and are robust in overall performance.
    Synchronous (execution happens one after another in a single series, A->B->C->D)
    and asynchronous (execution happens in parallel or on a priority basis, not in
    order: A->C->D->B or A and B together in parallel and C and D in parallel) operations
    are both supported by serverless functions. However, there are some disadvantages,
    such as cloud resource availability such as RAM or disk space or GPU unavailability,
    which can be crucial requirements for running heavy models such as deep learning
    or reinforcement learning models. For example, we can hit the wall of resource
    limitation if we have deployed a model without using serverless operations. The
    model or application deployed will not auto-scale and thus limit the available
    computation power. If more users infer the model or application than the limit,
    we will hit the resource unavailability blocker. In the following diagram, we
    can see how traditional applications and serverless applications are developed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3: Traditional versus serverless deployments'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_06_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.3: Traditional versus serverless deployments'
  prefs: []
  type: TYPE_NORMAL
- en: To develop serverless applications, the developer only has to focus on the application's
    logic and not worry about backend or security code, which is taken care of by
    the cloud services upon deploying serverless applications.
  prefs: []
  type: TYPE_NORMAL
- en: Model streaming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Model streaming is a method of serving models for handling streaming data. There
    is no beginning or end of streaming data. Every second, data is produced from
    thousands of sources and must be processed and analyzed as soon as possible. For
    example, Google Search results must be processed in real time. Model streaming
    is another way of deploying ML models. It has two main advantages over other model
    serving techniques, such as REST APIs or batch processing approaches. The first
    advantage is asynchronicity (serving multiple requests at a time). REST API ML
    applications are robust and scalable but have the limitation of being synchronous
    (they process requests from the client on a first come, first serve basis), which
    can lead to high latency and resource utilization. To cope with this limitation,
    stream processing is available. It is inherently asynchronous as the user or client
    does not have to coordinate or wait for the system to process the request.
  prefs: []
  type: TYPE_NORMAL
- en: Stream processing is able to process asynchronously and serve the users on the
    go. In order to do so, stream processing uses a message broker to receive messages
    from the users or clients. The message broker allows the data as it comes and
    spreads the processing over time. The message broker decouples the incoming requests
    and facilitates communication between the users or clients and the service without
    being aware of each other's operations, as shown in figure 5.4\. There are a couple
    of options for message streaming brokers, such as Apace Storm, Apache Kafka, Apache
    Spark, Apache Flint, Amazon Kinesis, and StreamSQL. The tool you choose is dependent
    on the IT setup and architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Model streaming process'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_06_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.4 – Model streaming process
  prefs: []
  type: TYPE_NORMAL
- en: The second advantage of stream processing is when multiple models are being
    inferred in an ML system. REST APIs are great for single-model or dual-model processing,
    but they manage to produce latency and use high amounts of computation of resources
    when multiple models need to be inferred, and on top of this they are limited
    to synchronous inference.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of multiple models, stream processing is a good option as all models
    and artifacts (code and files) needed to run the ML system can be packaged together
    and deployed on a stream processing engine (it runs on its own cluster of machines
    and manages resource allocation for distributing data processing).
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s look at the use case of an intelligent email assistant
    tasked to automate customer service, as shown in *Figure 5.4*. In order to automate
    replies to serve its users, the email assistant system performs multiple predictions
    using multiple models:'
  prefs: []
  type: TYPE_NORMAL
- en: Predict the class of the email, such as spam or accounts or renewal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Intent recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answer/text generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These four models deployed on REST API endpoints will generate high latency
    and maintenance costs, whereas a streaming service is a good alternative as it
    can package and serve multiple models as one process and continuously serve user
    requests in the form of a stream. Hence in such cases, streaming is recommended
    over REST API endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Mapping the infrastructure for our solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we map infrastructural needs and deployment targets needed
    to address diverse business needs, as seen in *Table 6.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 6.3 – Mapping the infrastructure for ML solutions'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Table_6.3.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.3 – Mapping the infrastructure for ML solutions
  prefs: []
  type: TYPE_NORMAL
- en: Depending on your use case, it is recommended to select suitable infrastructure
    and deployment targets to serve ML models to generate business or operational
    impact.
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on deployment (for the business problem)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn how to deploy solutions for the business problem
    we have been working on. So far, we have done data processing, ML model training,
    serialized models, and registered them to the Azure ML workspace. In this section,
    we will explore how inference is performed on the serialized model on a container
    and an auto-scaling cluster. These deployments will give you a broad understanding
    and will prepare you well for your future assignments.
  prefs: []
  type: TYPE_NORMAL
- en: We will use Python as the primary programming language, and Docker and Kubernetes
    for building and deploying containers. We will start with deploying a REST API
    service on an Azure container instance using Azure ML. Next, we will deploy a
    REST API service on an auto-scaling cluster using Kubernetes (for container orchestration)
    using Azure ML, and lastly, we will deploy on an Azure container instance using
    MLflow and an open source ML framework; this way, we will learn how to use multiple
    tools and deploy ML models on the cloud (Azure). Let's get started with deployment
    on **Azure Container Instances** (**ACI**).
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the model on ACI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To get started with deployment, go to the GitHub repository cloned on Azure
    DevOps previously (in [*Chapter 3*](B16572_03_Final_JM_ePub.xhtml#_idTextAnchor053),
    *Code Meets Data*), access the folder named `06_ModelDeployment`, and follow the
    implementation steps in the `01_Deploy_model_ACI.ipynb` notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing the required packages and check for the version of the
    Azure ML SDK, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding code will print the Azure ML SDK version (for example, `1.10.0`;
    your version may be different).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, using the `workspace` function from the Azure ML SDK, we connect to the
    ML workspace and download the required serialized files and model trained earlier
    using the `Model` function from the workspace. The serialized `scaler` and `model`
    are used to perform inference or prediction. `Scaler` will be used to shrink the
    input data to the same scale of data that was used for model training, and the
    `model` file is used to make predictions on the incoming data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After the `scaler` and the `model` files are downloaded, the next step is to
    prepare the `scoring` file. The `scoring` file is used to infer the ML models
    in the containers deployed with the ML service in the Azure container instance
    and Kubernetes cluster. The `scoring` script takes input passed by the user and
    infers the ML model for prediction and then serves the output with the prediction
    to the user. It contains two primary functions, `init()` and `run()`. We start
    by importing the required libraries and then define the `init()` and `run()` functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`%%writefile score.py` writes this code into a file named `score.py`, which
    is later packed as part of the ML service in the container for performing ML model
    inference.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We define the `init()` function; it downloads the required models and deserializes
    them into variables to be used for the predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In a nutshell, the `init()` function loads files (`model` and `scaler`) and
    deserializes and serves the model and artifact files needed for making predictions,
    which are used by the `run()` function as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will proceed to the crucial part of deploying the service on an Azure
    container instance. For this, we define a deployment environment by creating an
    environment `myenv.yml`, as shown in the following code. Using the `CondaDependencies()`
    function, we mention all the `pip` packages that need to be installed inside the
    Docker container that will be deployed as the ML service. Packages such as `numpy`,
    `onnxruntime`, `joblib`, `azureml-core`, `azureml-defaults`, and `scikit-learn`
    are installed inside the container upon triggering the environment file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we define the inference configuration by using the `InferenceConfig()`
    function, which takes `score.py` and the environment file as the arguments upon
    being called. Next, we call the `AciWebservice()` function to initiate the compute
    configuration ( `cpu_cores` and `memory`) in the `aciconfig` variable as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we are all set to deploy the ML or web service on the ACI. We will use
    `score.py`, the environment file (`myenv.yml`), `inference_config`, and `aci_config`
    to deploy the ML or web service. We will need to point to the models or artifacts
    to deploy. For this, we use the `Model()` function to load the `scaler` and `model`
    files from the workspace and get them ready for deployment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After the models are mounted into variables, `model1` and `model2`, we proceed
    with deploying them as a web service. We use the `deploy()` function to deploy
    the mounted models as a web service on the ACI, as shown in the preceding code.
    This process will take around 8 minutes, so grab your popcorn and enjoy the service
    being deployed. You will see a message like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Congratulations! You have successfully deployed your first ML service using
    MLOps.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s check out the workings and robustness of the deployed service. Check
    out the service URL and Swagger URL, as shown in the following code. You can use
    these URLs to perform ML model inference for input data of your choice in real
    time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Check for the deployed service in the Azure ML workspace.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we can test the service using the Azure ML SDK `service.run()` function
    by passing some input data as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The service we have deployed is a REST API web service that we can infer with
    an HTTP request as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When a `POST` request is made by passing input data, the service returns the
    model prediction in the form of `0` or `1`. When you get such a prediction, your
    service is working and is robust enough to serve production needs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, we will deploy the service on an auto-scaling cluster; this is ideal for
    production scenarios as the deployed service can auto-scale and serve user needs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Deploying the model on Azure Kubernetes Service (AKS)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To get started with the deployment, go to the Git repository cloned on Azure
    DevOps in [*Chapter 3*](B16572_03_Final_JM_ePub.xhtml#_idTextAnchor053), *Code
    Meets Data*, access the `06_ModelDeployment` folder, and follow the implementation
    steps in the `02_Deploy_model_AKS.ipynb` notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we did in the previous section, start by importing the required packages,
    such as `matplotlib`, `numpy`, and `azureml.core`, and the required functions,
    such as `Workspace` and `Model`, from `azureml.core`, as shown in the following
    code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the version of the Azure ML SDK and check for the version (it will print,
    for example, `1.10.0`; your version may be different). Use the config file and
    `Workspace` function connect to your workspace, as shown in the following code
    block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Download the `model` and `scaler` files as we did previously. After the `model`
    and the `scaler` files are downloaded, the next step is to prepare the `scoring`
    file, which is used to infer the ML models in the containers deployed with the
    ML service. The `scoring` script takes an input passed by the user, infers the
    ML model for prediction, and then serves the output with the prediction to the
    user. We will start by importing the required libraries, as shown in the following
    code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As we made `score.py` previously for ACI deployment, we will use the same file.
    It contains two primary functions, `init()` and `run()`. We define the `init()`
    function; it downloads the required models and deserializes them into variables
    to be used for predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As we did in the previous section on ACI deployment, by using `onnxruntime`
    package functions we can deserialize the support vector classifier model.The `InferenceSession()`
    function is used to deserialize and serve the model for inference, and the `input_name`
    and `label_name` variables are loaded from the deserialized model. In a nutshell,
    the `init()` function loads files (`model` and `scaler`), and deserializes and
    serves the model and artifact files needed for making predictions that are used
    by the `run()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We will use the same `run()` function previously used in the section *Deploying
    the model on ACI* for the AKS deployment. With this we can proceed to deploying
    the service on AKS.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we will proceed to the crucial part of deploying the service on `CondaDependencies()`
    function. We will mention all the required `pip` and `conda` packages to be installed
    inside the Docker container that will be deployed as the ML service. Packages
    such as `numpy`, `onnxruntime`, `joblib`, `azureml-core`, `azureml-defaults`,
    and `scikit-learn` are installed inside the container upon triggering the `environment`
    file. Next, use the publicly available container in the Microsoft Container Registry
    without any authentication. This container will install your environment and will
    be configured for deployment to your target AKS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, define the inference configuration by using the `InferenceConfig()` function,
    which takes `score.py` and the environment variable as the arguments upon being
    called:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we are all set to deploy the ML or web service on Azure Kubernetes Service
    (auto-scaling cluster). In order to do so, we will need to create an AKS cluster
    and attach it to the Azure ML workspace. Choose a name for your cluster and check
    if it exists using the `ComputeTarget()` function. If not, a cluster will be created
    or provisioned using the `ComputeTarget.create()` function. It takes a workspace
    object, `ws`; a service name; and a provisioning config to create the cluster.
    We use the default parameters for the provisioning config to create a default
    cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we proceed to the critical task of deploying the ML service in the Kubernetes
    cluster. In order to deploy, we need some prerequisites, such as mounting the
    models to deploy. We mount the models using the `Model()` function to load the
    `scaler` and `model` files from the workspace and get them ready for deployment,
    as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we are all set to deploy the service on AKS. We deploy the service with
    the help of the `Model.deploy()` function from the Azure ML SDK, which takes the
    workspace object, `ws`; `service_name`; `models`; `inference_config`; `deployment_config`;
    and `deployment_target` as arguments upon being called:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Deploying the service will take approximately around 10 mins. After deploying
    the ML service, you will get a message like the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Congratulations! Now you have deployed an ML service on AKS. Let's test it using
    the Azure ML SDK.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We use the `service.run()` function to pass data to the service and get the
    predictions, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The deployed service is a REST API web service that can be accessed with an
    HTTP request as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When a `POST` request is made by passing input data, the service returns the
    model prediction in the form of `0` or `1`. When you get such a prediction, your
    service is working and is robust to serve production needs. The service scales
    from `0` to the needed number of container replicas based on the user's request
    traffic.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Deploying the service using MLflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Lastly, let''s do the deployment of an ML service on the deployment target
    (ACI) using MLflow to get hands-on experience with an open source framework. To
    get started, go to the Git repository cloned on Azure DevOps previously (in [*Chapter
    3*](B16572_03_Final_JM_ePub.xhtml#_idTextAnchor053), *Code Meets Data*), access
    the folder named `06_ModelDeployment`, and follow the implementation steps in
    the `02_Deploy_model_MLflow.ipynb` notebook. Before implementing, it is recommended
    to read this documentation to understand the concepts behind the `mlflow.azureml`
    SDK: [https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-mlflow#deploy-and-register-mlflow-models](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-mlflow#deploy-and-register-mlflow-models).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing the required packages and check for the version of the
    Azure ML SDK, as shown in the following code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, using the `workspace` function from the Azure ML SDK, we connect to the
    ML workspace and set the tracking URI for the workspace using `set_tracking_uri`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now go to the workspace and fetch the path to the `mlflow` model from the `models`
    or `experiments` section and set the path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we are all set to deploy to the ACI using `mlflow` and the `azureml` SDK.
    Configure the ACI deployment target using the `deploy_configuration` function
    and deploy to the ACI using the `mlflow.azureml.deploy` function. The `deploy`
    function takes `model_uri`, `workspace`, `model_name`, `service_name`, `deployment_config`,
    and custom tags as arguments upon being called:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will get a deployment succeeded message upon successful deployment. For
    more clarity on MLflow deployment, follow these examples: [https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-mlflow#deploy-and-register-mlflow-models](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-mlflow#deploy-and-register-mlflow-models).'
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have deployed ML models on diverse deployment targets such
    as ACI and AKS using `azureml` and `mlflow`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will focus on bringing the full capabilities of MLOps to the table
    using continuous integration and continuous deployment to have a robust and dynamically
    developing system in production.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the need for continuous integration and continuous deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Continuous integration** (**CI**) and **continuous deployment** (**CD**)
    enable continuous delivery to the ML service. The goal is to maintain and version
    the source code used for model training, enable triggers to perform necessary
    jobs in parallel, build artifacts, and release them for deployment to the ML service.
    Several cloud vendors enable DevOps services that can be used for monitoring ML
    services, ML models in production, and orchestration with other services in the
    cloud.'
  prefs: []
  type: TYPE_NORMAL
- en: Using CI and CD, we can enable continuous learning, which is critical for the
    success of an ML system. Without continuous learning, an ML system is destined
    to end up as a failed **PoC** (**Proof of Concept**). We will delve into the concepts
    of CI/CD and implement hands-on CI and CD pipelines to see MLOps in play in the
    next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned the key principles of deploying ML models in
    production. We explored the various deployment methods and targets and their needs.
    For a comprehensive understanding and hands-on experience, we implemented the
    deployment to learn how ML models are deployed on a diverse range of deployment
    targets such as virtual machines, containers, and in an auto-scaling cluster.
    With this, you are ready to handle any type of deployment challenge that comes
    your way.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will delve into the secrets to building, deploying,
    and maintaining robust ML services enabled by CI and CD. This will enable the
    potential of MLOps! Let's delve into it.
  prefs: []
  type: TYPE_NORMAL
