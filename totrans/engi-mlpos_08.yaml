- en: 'Chapter 6: Key Principles for Deploying Your ML System'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 6 章：部署您的机器学习系统的主要原则
- en: 'In this chapter, you will learn the fundamental principles for deploying **machine
    learning** (**ML**) models in production and implement the hands-on deployment
    of ML models for the business problem we have been working on. To get a comprehensive
    understanding and first-hand experience, we will deploy ML models that were trained
    and packaged previously (in [*Chapter 4*](B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074),
    *Machine Learning Pipelines*, and [*Chapter 5*](B16572_05_Final_JM_ePub.xhtml#_idTextAnchor093),
    *Model Evaluation and Packaging*) using the Azure ML service on two different
    deployment targets: an Azure container instance and a Kubernetes cluster.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习在生产中部署 **机器学习**（**ML**）模型的基本原则，并实施针对我们一直在研究的业务问题的机器学习模型的实际部署。为了获得全面的理解和第一手经验，我们将使用
    Azure ML 服务在两个不同的部署目标上部署之前训练和打包的机器学习模型（在 [*第 4 章*](B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074)，*机器学习管道*，和
    [*第 5 章*](B16572_05_Final_JM_ePub.xhtml#_idTextAnchor093)，*模型评估和打包*)。
- en: We will also learn how to deploy ML models using an open source framework called
    MLflow that we have already worked with. This will enable you to get an understanding
    of deploying ML models as REST API endpoints on diverse deployment targets using
    two different tools (the Azure ML service and MLflow). This will equip you with
    the skills required to deploy ML models for any given scenario on the cloud.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将学习如何使用我们之前已经使用过的开源框架 MLflow 来部署机器学习模型。这将使您能够了解如何使用两种不同的工具（Azure ML 服务和 MLflow）将机器学习模型作为
    REST API 端点部署到不同的部署目标。这将使您具备在云上部署任何给定场景的机器学习模型所需的技能。
- en: 'In this chapter, we start by looking at how ML is different in research and
    production and continue exploring the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先探讨研究生产和实施机器学习（ML）的不同之处，并继续探讨以下主题：
- en: ML in research versus production
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 研究与生产中的机器学习
- en: Understanding the types of ML inference in production
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解生产中机器学习推理的类型
- en: Going through the mapping infrastructure for your solution
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过映射您的解决方案的基础设施
- en: Hands-on deployment (for the business problem)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实际部署（针对业务问题）
- en: Understanding the need for continuous integration and continuous deployment
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解持续集成和持续部署的需求
- en: ML in research versus production
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 研究与生产中的机器学习
- en: ML in research is implemented with specific goals and priorities to improve
    the state of the art in the field, whereas the aim of ML in production is to optimize,
    automate, or augment a scenario or a business.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 研究中的机器学习具有特定的目标和优先级，旨在提高该领域的最先进水平，而生产中的机器学习的目的是优化、自动化或增强某个场景或业务。
- en: 'In order to understand the deployment of ML models, let''s start by comparing
    how ML is implemented in research versus production (in the industry). Multiple
    factors, such as performance, priority, data, fairness, and interpretability,
    as listed in *Table 6.1*, depict how deployments and ML work differently in research
    and production:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解机器学习模型的部署，让我们首先比较研究生产和实施（在工业界）中机器学习的实现方式。如 *表 6.1* 所列出的多个因素，例如性能、优先级、数据、公平性和可解释性，描述了部署和机器学习在研究和生产中的不同之处：
- en: '![Table 6.1 – ML in research and production'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '![表 6.1 – 研究与生产中的机器学习'
- en: '](img/Table_6.1.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Table_6.1.jpg)'
- en: Table 6.1 – ML in research and production
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.1 – 研究与生产中的机器学习
- en: Data
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据
- en: 'In general, data in research projects is static because data scientists or
    statisticians are working on a set dataset and trying to beat the current state-of-the-art
    models. For example, recently, many breakthroughs in natural language processing
    models have been witnessed, for instance, with BERT from Google or XLNet from
    Baidu. To train these models, data was scraped and compiled into a static dataset.
    In the research world, to evaluate or benchmark the performance of the models,
    static datasets are used to evaluate the performance, as shown in *Table 6.2*
    (source: [https://arxiv.org/abs/1906.08237](https://arxiv.org/abs/1906.08237)):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，研究项目中的数据是静态的，因为数据科学家或统计学家正在处理一个固定的数据集，并试图超越当前的最先进模型。例如，最近在自然语言处理模型方面取得了许多突破，例如谷歌的
    BERT 或百度的 XLNet。为了训练这些模型，数据被抓取并编译成静态数据集。在研究界，为了评估或基准测试模型的性能，使用静态数据集来评估性能，如 *表
    6.2* 所示（来源：[https://arxiv.org/abs/1906.08237](https://arxiv.org/abs/1906.08237))：
- en: '![Table 6.2 – BERT versus XLNet performance (in research)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '![表 6.2 – BERT 与 XLNet 性能对比（研究）'
- en: '](img/Table_6.2.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Table_6.2.jpg)'
- en: Table 6.2 – BERT versus XLNet performance (in research)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.2 – BERT与XLNet在研究中的性能比较
- en: For instance, we can compare the performance of two models by comparing their
    performance on a popular dataset called SQUAD (10,000+ QnA) version 1.1, on which
    BERT performs with 92.8% accuracy and XLNET with 94.0% accuracy. Likewise, data
    used in research for training and evaluating models is static, whereas data in
    production or in industrial use cases is dynamic and constantly changing as per
    the environment, operations, business, or users.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以通过比较两个模型在名为SQUAD（10,000+ QnA）版本1.1的流行数据集上的性能来比较它们的性能，在这个数据集上BERT的准确率为92.8%，XLNET的准确率为94.0%。同样，用于研究和训练、评估模型的数据是静态的，而生产或工业用例中的数据是动态的，并且根据环境、操作、业务或用户不断变化。
- en: Fairness
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 公平性
- en: 'In real life, biased models can be costly. Unfair or biased decisions will
    lead to poor choices for business and operations. For ML models in production,
    it is important that decisions made are as fair as possible. It can be costly
    for the business if the models in production are not fair. For example, recently,
    Amazon made HR screening software that screens applicants based on their suitability
    for the job they applied for. ML specialists at Amazon discovered that male candidates
    were favored over female candidates (source: [https://www.businessinsider.com/amazon-ai-biased-against-women-no-surprise-sandra-wachter-2018-10](https://www.businessinsider.com/amazon-ai-biased-against-women-no-surprise-sandra-wachter-2018-10)).
    This kind of system bias can be costly because, in Amazon''s case, you can miss
    out on some amazing talent as a result of bias. Hence having fair models in production
    is critical and should be monitored constantly. In research, fair models are important
    as well but not as critical as in production or real life, and fairness is not
    critically monitored as in production. The goal in research is to beat the state
    of the art, and model fairness is a secondary goal.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实生活中，有偏见的模型可能会造成高昂的代价。不公平或偏见的决策会导致商业和运营的糟糕选择。对于生产中的机器学习模型，确保做出的决策尽可能公平是非常重要的。如果生产中的模型不公平，这可能会给企业带来高昂的成本。例如，最近，亚马逊推出了一款基于求职者适合其申请工作的程度进行筛选的人力资源筛选软件。亚马逊的机器学习专家发现，男性候选人比女性候选人更受青睐（来源：[https://www.businessinsider.com/amazon-ai-biased-against-women-no-surprise-sandra-wachter-2018-10](https://www.businessinsider.com/amazon-ai-biased-against-women-no-surprise-sandra-wachter-2018-10)）。这种系统偏见可能会造成高昂的代价，因为在亚马逊的案例中，由于偏见可能会错失一些杰出的人才。因此，在生产中拥有公平的模型至关重要，并且应该持续监控。在研究中，公平的模型也很重要，但不如生产或现实生活中那么关键，而且公平性不像在生产中那样受到严格的监控。研究的目标是超越现有技术，而模型公平性是一个次要目标。
- en: Interpretability
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可解释性
- en: Model interpretability is critical in production in order to understand the
    correlation or causality between the ML model's decisions and its impact on the
    operations or business to optimize, augment, or automate a business or task at
    hand. This is not the case in research, where the goal is to challenge or beat
    the state-of-the-art results, and here the priority is better performance (such
    as accuracy, or other metrics). In the case of research, ML model interpretability
    is good to have but not mandatory. Typically, ML projects are more concerned with
    predicting outcomes than with understanding causality. ML models are great at
    finding correlations in data, but not causation. We strive not to fall into the
    pit of equating association with the cause in our ventures. Our ability to rely
    on ML is severely hampered as a result of this issue. This problem severely limits
    our ability to use ML to make decisions. We need resources that can understand
    the causal relationships between data and build ML solutions that can generalize
    well from a business viewpoint. Having the right model interpretability mechanisms
    can enhance our understanding of causality and enable us to craft ML solutions
    that generalize well and are able to handle previously unseen data. As a result,
    we can make more reliable and transparent decisions using ML.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产中，模型的可解释性至关重要，以便理解机器学习模型的决策与其对运营或业务影响之间的相关性或因果关系，从而优化、增强或自动化业务或手头的任务。在研究中并非如此，研究的目标是挑战或超越最先进的结果，这里的重点是更好的性能（如准确度或其他指标）。在研究的情况下，机器学习模型的可解释性是好的，但不是强制性的。通常，机器学习项目更关注预测结果，而不是理解因果关系。机器学习模型擅长在数据中找到相关性，但不是因果关系。我们努力避免在我们的项目中将关联等同于原因。由于这个问题，我们依赖机器学习的能力受到了严重阻碍。这个问题严重限制了我们的能力，使我们无法使用机器学习来做出决策。我们需要能够理解数据之间因果关系的资源，并构建可以从业务角度很好地泛化的机器学习解决方案。拥有正确的模型可解释性机制可以增强我们对因果关系的理解，并使我们能够制定出泛化良好且能够处理以前未见数据的机器学习解决方案。因此，我们可以使用机器学习做出更可靠和透明的决策。
- en: In the case of production (in a business use case), a lack of interpretability
    is not recommended at all. Let us look at a hypothetical case. Let's assume you
    have cancer and have to choose a surgeon to perform your surgery. Two surgeons
    are available, one is human (with an 80% cure rate) and another is an AI black-box
    model (with a 90% cure rate) that cannot be interpreted or explain how it works,
    but it has a high cure rate. What would you choose? AI or a surgeon to cure cancer?
    It would be easier to replace the surgeon with AI if the model was not a black-box
    model. Though the AI is better than the surgeon, without understanding the model,
    decision, trust and compliance is an issue. Model interpretability is essential
    to make legal decisions. Hence, it is vital to have model interpretability for
    ML in production. We will learn more about this in later chapters.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产（在商业用例中）的情况下，完全不推荐缺乏可解释性。让我们看看一个假设的案例。假设你患有癌症，需要选择一位外科医生进行手术。有两位外科医生可供选择，一位是人类（治愈率为80%），另一位是AI黑盒模型（治愈率为90%），该模型无法被解释或解释其工作原理，但它有很高的治愈率。你会选择哪个？AI还是外科医生来治愈癌症？如果模型不是黑盒模型，用AI替换外科医生会更容易。尽管AI比外科医生更好，但没有理解模型，决策、信任和合规就会成为问题。模型的可解释性对于做出法律决策至关重要。因此，对于生产中的机器学习来说，拥有模型的可解释性至关重要。我们将在后面的章节中了解更多关于这方面的内容。
- en: Performance
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能
- en: When it comes to the performance of the ML models, the focus in research is
    to improve on the state-of-the-art models, whereas in production the focus is
    to build better models than simpler models that serve the business needs (**state-of-the-art**
    models are not the focus).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到机器学习模型的性能时，研究中的重点是改进最先进的模型，而在生产中，重点是构建比简单模型更好的模型，以满足业务需求（**最先进的**模型不是重点）。
- en: Priority
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优先级
- en: In research, training the models faster and better is the priority, whereas
    in production faster inference is the priority as the focus is to make decisions
    and serve the business needs in real time.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在研究中，优先级是更快、更好地训练模型，而在生产中，优先级是更快地进行推理，因为重点是实时做出决策并满足业务需求。
- en: Understanding the types of ML inference in production
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解生产中机器学习推理的类型
- en: In the previous section, we saw the priorities of ML in research and production.
    To serve the business needs in production, ML models are inferred using various
    deployment targets, depending on the need. Predicting or making a decision using
    an ML model is called ML model inference. Let's explore ways of deploying ML models
    on different deployment targets to facilitate ML inference as per the business
    needs.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到了机器学习在研究和生产中的优先级。为了在生产中满足业务需求，根据需要使用各种部署目标来推断机器学习模型。使用机器学习模型进行预测或做出决策称为机器学习模型推理。让我们探讨在不同部署目标上部署机器学习模型的方法，以根据业务需求促进机器学习推理。
- en: Deployment targets
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署目标
- en: In this section, we will look at different types of deployment targets and why
    and how we serve ML models for inference in these deployment targets. Let's start
    by looking at a virtual machine or an on-premises server.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨不同的部署目标类型，以及为什么以及如何在这些部署目标上为推理服务机器学习模型。让我们首先看看虚拟机或本地服务器。
- en: Virtual machines
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 虚拟机
- en: Virtual machines can be on the cloud or on-premises, depending on the IT setup
    of a business or an organization. Serving ML models on virtual machines is quite
    common. ML models are served on virtual machines in the form of web services.
    The web service running on a virtual machine receives a user request (as an HTTP
    request) containing the input data. The web service, upon receiving the input
    data, preprocesses it in the required format to infer the ML model, which is part
    of the web service. After the ML model makes the prediction or performs the task,
    the output is transformed and presented in a user-readable format. Commonly into
    **JavaScript Object Notation** (**JSON**) or **Extensible Markup language string**
    (**XML**). Usually, a web service is served in the form of a REST API. REST API
    web services can be developed using multiple tools; for instance, FLASK or FAST
    API web application tools can be used to develop REST API web services using Python
    or Spring Boot in Java, or Plumber in R, depending on the need. A combination
    of virtual machines is used in parallel to scale and maintain the robustness of
    the web services.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟机可以是云上的，也可以是本地的，这取决于企业或组织的IT配置。在虚拟机上服务机器学习模型相当常见。机器学习模型以网络服务的形式在虚拟机上提供服务。运行在虚拟机上的网络服务接收包含输入数据的用户请求（作为HTTP请求）。在接收到输入数据后，网络服务将其预处理为所需的格式，以推断网络服务中的机器学习模型。在机器学习模型做出预测或执行任务后，输出被转换并以用户可读的格式呈现。通常为**JavaScript对象表示法**（**JSON**）或**可扩展标记语言字符串**（**XML**）。通常，网络服务以REST
    API的形式提供服务。可以使用多种工具开发REST API网络服务；例如，可以使用Python或Java中的Spring Boot或R中的Plumber开发REST
    API网络服务，具体取决于需求。使用虚拟机的组合并行使用以扩展和维持网络服务的健壮性。
- en: 'In order to orchestrate the traffic and to scale the machines, a load balancer
    is used to dispatch incoming requests to the virtual machines for ML model inference.
    This way, ML models are deployed on virtual machines on the cloud or on-premises
    to serve the business needs, as shown in the following diagram:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了编排流量并扩展机器，使用负载均衡器将传入的请求调度到虚拟机以进行机器学习模型推理。这样，机器学习模型被部署在云或本地的虚拟机上，以满足业务需求，如下面的图所示：
- en: '![Figure 6.1 – Deployment on virtual machines'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.1 – 虚拟机上的部署'
- en: '](img/B16572_06_01.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.1 – 虚拟机上的部署'
- en: Figure 6.1 – Deployment on virtual machines
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – 虚拟机上的部署
- en: Containers
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 容器
- en: Containers are a reliable way to run applications using the Linux OS with customized
    settings. A container is an application running with a custom setting orchestrated
    by the developer. Containers are an alternative and more resource-efficient way
    of serving models than virtual machines. They operate like virtual machines as
    they have their own runtime environment, which is isolated and confined to memory,
    the filesystem, and processes.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 容器是使用Linux操作系统并带有自定义设置来运行应用程序的可靠方式。容器是由开发者编排的应用程序，运行时具有自定义设置。与虚拟机相比，容器是服务模型的一种替代方案，并且更节省资源。容器像虚拟机一样运行，因为它们拥有自己的运行时环境，该环境是隔离的，并且限制在内存、文件系统和进程内。
- en: Containers can be customized by developers to confine them to required resources
    such as memory, the filesystem, and processes, and the virtual machines are limited
    to such customizations. They are more flexible and operate in a modular way and
    hence provide more resource efficiency and optimization. They allow the possibility
    to scale to zero, as containers can be reduced to zero replicas and run a backup
    on request. This way, lower computation power consumption is possible compared
    to running web services on virtual machines. As a result of this lower computation
    power consumption, cost-saving on the cloud is possible.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者可以自定义容器，将它们限制在所需的资源上，如内存、文件系统和处理过程，而虚拟机则限于这些自定义。它们更灵活，以模块化方式操作，因此提供更高的资源效率和优化。它们允许扩展到零，因为容器可以减少到零副本，并在请求时运行备份。这样，与在虚拟机上运行
    Web 服务相比，可以实现更低的计算能力消耗。由于这种较低的计算能力消耗，在云上可以实现成本节约。
- en: Containers present many advantages; however, one disadvantage can be the complexity
    required to work with containers, as it requires expertise.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 容器具有许多优点；然而，一个缺点可能是与容器一起工作的复杂性，因为它需要专业知识。
- en: 'There are some differences in the way containers and virtual machines operate.
    For example, there can be multiple containers running inside a virtual machine
    that share the operating system and resources with the virtual machine, but the
    virtual machine runs its own resources and operating system. Containers can operate
    modularly, but virtual machines operate as single units. Docker is used to build
    and deploy containers; however, there are alternatives, such as Mesos and CoreOS
    rkt. A container is typically packaged with the ML model and web service to facilitate
    the ML inference, similar to how we serve the ML model wrapped in a web service
    in the virtual machine. Containers need to be orchestrated to be consumed by users.
    The orchestration of containers means the automation of the deployment, management,
    scaling, and networking of containers. Containers are orchestrated using a container
    orchestration system such as Kubernetes. In the following diagram, we can see
    container orchestration with auto-scaling (based on the traffic of requests):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 容器和虚拟机在操作方式上存在一些差异。例如，虚拟机内部可以运行多个容器，这些容器与虚拟机共享操作系统和资源，但虚拟机运行自己的资源和操作系统。容器可以模块化操作，而虚拟机则以单一单元操作。Docker
    用于构建和部署容器；然而，还有其他替代方案，如 Mesos 和 CoreOS rkt。容器通常与 ML 模型和 Web 服务打包在一起，以方便 ML 推理，类似于我们在虚拟机中用
    Web 服务包装 ML 模型来提供服务。容器需要被编排才能被用户使用。容器的编排意味着部署、管理、扩展和网络自动化的自动化。容器使用容器编排系统（如 Kubernetes）进行编排。在以下图中，我们可以看到具有自动扩展（基于请求流量）的容器编排：
- en: '![Figure 6.2 – Deployment on containers'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.2 – 容器部署'
- en: '](img/B16572_06_02.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16572_06_02.jpg)'
- en: Figure 6.2 – Deployment on containers
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 – 容器部署
- en: Serverless
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无服务器
- en: Serverless computing, as the name suggests, does not involve a virtual machine
    or container. It eliminates infrastructure management tasks such as OS management,
    server management, capacity provisioning, and disk management. Serverless computing
    enables developers and organizations to focus on their core product instead of
    mundane tasks such as managing and operating servers, either on the cloud or on-premises.
    Serverless computing is facilitated by using cloud-native services.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 无服务器计算，正如其名所示，不涉及虚拟机或容器。它消除了基础设施管理任务，如操作系统管理、服务器管理、容量配置和磁盘管理。无服务器计算使开发者和组织能够专注于核心产品，而不是管理服务器等日常任务，无论是在云上还是在本地。无服务器计算通过使用云原生服务来实现。
- en: For instance, Microsoft Azure uses Azure Functions, and AWS uses Lambda functions
    to deploy serverless applications. The deployment for serverless applications
    involves submitting a collection of files (in the form of `.zip` files) to run
    ML applications. The .zip archive typically has a file with a particular function
    or method to execute. The zip archive is uploaded to the cloud platform using
    cloud services and deployed as a serverless application. The deployed application
    serves as an API endpoint to submit input to the serverless application serving
    the ML model.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Microsoft Azure使用Azure Functions，AWS使用Lambda函数来部署无服务器应用程序。无服务器应用程序的部署涉及提交一组文件（以`.zip`文件的形式）以运行机器学习应用程序。通常，.zip存档包含一个特定的函数或方法来执行。该存档通过云服务上传到云平台，并作为无服务器应用程序部署。部署的应用程序作为API端点，用于向提供机器学习模型的无服务器应用程序提交输入。
- en: 'Deploying ML models using serverless applications can have many advantages:
    there''s no need to install or upgrade dependencies, or maintain or upgrade systems.
    Serverless applications auto-scale on demand and are robust in overall performance.
    Synchronous (execution happens one after another in a single series, A->B->C->D)
    and asynchronous (execution happens in parallel or on a priority basis, not in
    order: A->C->D->B or A and B together in parallel and C and D in parallel) operations
    are both supported by serverless functions. However, there are some disadvantages,
    such as cloud resource availability such as RAM or disk space or GPU unavailability,
    which can be crucial requirements for running heavy models such as deep learning
    or reinforcement learning models. For example, we can hit the wall of resource
    limitation if we have deployed a model without using serverless operations. The
    model or application deployed will not auto-scale and thus limit the available
    computation power. If more users infer the model or application than the limit,
    we will hit the resource unavailability blocker. In the following diagram, we
    can see how traditional applications and serverless applications are developed:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 使用无服务器应用程序部署机器学习模型可以带来许多优势：无需安装或升级依赖项，也无需维护或升级系统。无服务器应用程序按需自动扩展，并且在整体性能上非常稳健。无服务器函数同时支持同步操作（执行在一个序列中依次发生，A->B->C->D）和异步操作（并行或基于优先级执行，不按顺序：A->C->D->B
    或 A 和 B 并行，C 和 D 并行）。然而，也存在一些缺点，例如云资源可用性，如RAM或磁盘空间或GPU不可用，这对于运行深度学习或强化学习等重型模型可能是关键要求。例如，如果我们没有使用无服务器操作部署模型，我们可能会遇到资源限制的瓶颈。部署的模型或应用程序不会自动扩展，从而限制可用的计算能力。如果用户对模型或应用程序的推理超过了限制，我们将遇到资源不可用的障碍。在以下图中，我们可以看到传统应用程序和无服务器应用程序的开发方式：
- en: '![Figure 6.3: Traditional versus serverless deployments'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.3：传统与无服务器部署对比'
- en: '](img/B16572_06_03.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16572_06_03.jpg)'
- en: 'Figure 6.3: Traditional versus serverless deployments'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3：传统与无服务器部署对比
- en: To develop serverless applications, the developer only has to focus on the application's
    logic and not worry about backend or security code, which is taken care of by
    the cloud services upon deploying serverless applications.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 要开发无服务器应用程序，开发者只需关注应用程序的逻辑，无需担心后端或安全代码，这些在部署无服务器应用程序时由云服务负责处理。
- en: Model streaming
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型流
- en: Model streaming is a method of serving models for handling streaming data. There
    is no beginning or end of streaming data. Every second, data is produced from
    thousands of sources and must be processed and analyzed as soon as possible. For
    example, Google Search results must be processed in real time. Model streaming
    is another way of deploying ML models. It has two main advantages over other model
    serving techniques, such as REST APIs or batch processing approaches. The first
    advantage is asynchronicity (serving multiple requests at a time). REST API ML
    applications are robust and scalable but have the limitation of being synchronous
    (they process requests from the client on a first come, first serve basis), which
    can lead to high latency and resource utilization. To cope with this limitation,
    stream processing is available. It is inherently asynchronous as the user or client
    does not have to coordinate or wait for the system to process the request.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 模型流是一种用于处理流数据的模型服务方法。流数据没有开始或结束。每秒钟，都有来自数千个来源的数据产生，并且必须尽快进行处理和分析。例如，谷歌搜索结果必须实时处理。模型流是部署机器学习模型（ML）的另一种方式。它相较于其他模型服务技术，如REST
    API或批量处理方法，有两个主要优势。第一个优势是异步性（一次处理多个请求）。REST API机器学习应用稳健且可扩展，但它们有一个限制，即同步性（它们根据客户请求的先后顺序处理请求），这可能导致高延迟和资源利用率。为了应对这一限制，流处理是可用的。它本质上是异步的，因为用户或客户不需要协调或等待系统处理请求。
- en: Stream processing is able to process asynchronously and serve the users on the
    go. In order to do so, stream processing uses a message broker to receive messages
    from the users or clients. The message broker allows the data as it comes and
    spreads the processing over time. The message broker decouples the incoming requests
    and facilitates communication between the users or clients and the service without
    being aware of each other's operations, as shown in figure 5.4\. There are a couple
    of options for message streaming brokers, such as Apace Storm, Apache Kafka, Apache
    Spark, Apache Flint, Amazon Kinesis, and StreamSQL. The tool you choose is dependent
    on the IT setup and architecture.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理能够异步处理并即时为用户提供服务。为了做到这一点，流处理使用消息代理来接收来自用户或客户端的消息。消息代理允许数据即时到达，并将处理分散到时间上。消息代理解耦了传入的请求，并促进了用户或客户端与服务之间的通信，而无需了解彼此的操作，如图5.4所示。对于消息流代理有几个选项，例如Apache
    Storm、Apache Kafka、Apache Spark、Apache Flint、Amazon Kinesis和StreamSQL。您选择的工具取决于IT设置和架构。
- en: '![Figure 6.4 – Model streaming process'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.4 – 模型流过程'
- en: '](img/B16572_06_04.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ']'
- en: Figure 6.4 – Model streaming process
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 – 模型流过程
- en: The second advantage of stream processing is when multiple models are being
    inferred in an ML system. REST APIs are great for single-model or dual-model processing,
    but they manage to produce latency and use high amounts of computation of resources
    when multiple models need to be inferred, and on top of this they are limited
    to synchronous inference.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理的第二个优势在于，当在机器学习系统中进行多个模型推断时。对于单模型或双模型处理，REST API非常出色，但当需要推断多个模型时，它们会产生延迟并使用大量的计算资源，而且它们还限于同步推断。
- en: In the case of multiple models, stream processing is a good option as all models
    and artifacts (code and files) needed to run the ML system can be packaged together
    and deployed on a stream processing engine (it runs on its own cluster of machines
    and manages resource allocation for distributing data processing).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个模型的情况下，流处理是一个好的选择，因为所有运行ML系统所需的模型和工件（代码和文件）都可以打包在一起，并部署在流处理引擎上（它在自己的机器集群上运行并管理数据处理的资源分配）。
- en: 'For example, let''s look at the use case of an intelligent email assistant
    tasked to automate customer service, as shown in *Figure 5.4*. In order to automate
    replies to serve its users, the email assistant system performs multiple predictions
    using multiple models:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们看看智能电子邮件助手的使用案例，该助手的任务是自动化客户服务，如图5.4所示。为了自动化回复以服务其用户，电子邮件助手系统使用多个模型进行多次预测：
- en: Predict the class of the email, such as spam or accounts or renewal
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测电子邮件的类别，例如垃圾邮件或账户或续订
- en: Intent recognition
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 意图识别
- en: Sentiment prediction
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感预测
- en: Answer/text generation
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回答/文本生成
- en: These four models deployed on REST API endpoints will generate high latency
    and maintenance costs, whereas a streaming service is a good alternative as it
    can package and serve multiple models as one process and continuously serve user
    requests in the form of a stream. Hence in such cases, streaming is recommended
    over REST API endpoints.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在REST API端点部署的这四个模型将产生高延迟和维护成本，而流式服务是一个很好的替代方案，因为它可以将多个模型打包为一个进程，并以流的形式持续服务用户请求。因此，在这种情况下，建议使用流式服务而不是REST
    API端点。
- en: Mapping the infrastructure for our solution
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 映射我们的解决方案的基础设施
- en: 'In this section, we map infrastructural needs and deployment targets needed
    to address diverse business needs, as seen in *Table 6.3*:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们映射基础设施需求以及部署目标，以解决多样化的业务需求，如*表6.3*所示：
- en: '![Table 6.3 – Mapping the infrastructure for ML solutions'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '![Table 6.3 – Mapping the infrastructure for ML solutions'
- en: '](img/Table_6.3.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/Table_6.3.jpg]'
- en: Table 6.3 – Mapping the infrastructure for ML solutions
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.3 – 映射ML解决方案的基础设施
- en: Depending on your use case, it is recommended to select suitable infrastructure
    and deployment targets to serve ML models to generate business or operational
    impact.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的用例，建议选择合适的基础设施和部署目标，以服务于ML模型，以产生业务或运营影响。
- en: Hands-on deployment (for the business problem)
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手动部署（针对业务问题）
- en: In this section, we will learn how to deploy solutions for the business problem
    we have been working on. So far, we have done data processing, ML model training,
    serialized models, and registered them to the Azure ML workspace. In this section,
    we will explore how inference is performed on the serialized model on a container
    and an auto-scaling cluster. These deployments will give you a broad understanding
    and will prepare you well for your future assignments.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何部署我们一直在工作的业务问题的解决方案。到目前为止，我们已经完成了数据处理、ML模型训练、序列化模型并将它们注册到Azure ML工作区。在本节中，我们将探讨如何在容器和自动扩展集群上对序列化模型进行推理。这些部署将为您提供广泛的理解，并为您未来的任务做好准备。
- en: We will use Python as the primary programming language, and Docker and Kubernetes
    for building and deploying containers. We will start with deploying a REST API
    service on an Azure container instance using Azure ML. Next, we will deploy a
    REST API service on an auto-scaling cluster using Kubernetes (for container orchestration)
    using Azure ML, and lastly, we will deploy on an Azure container instance using
    MLflow and an open source ML framework; this way, we will learn how to use multiple
    tools and deploy ML models on the cloud (Azure). Let's get started with deployment
    on **Azure Container Instances** (**ACI**).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Python作为主要的编程语言，并使用Docker和Kubernetes来构建和部署容器。我们将从在Azure容器实例上部署REST API服务开始，使用Azure
    ML。接下来，我们将使用Kubernetes（用于容器编排）在自动扩展集群上部署REST API服务，并使用Azure ML，最后，我们将使用MLflow和开源ML框架在Azure容器实例上部署；这样，我们将学习如何使用多个工具并在云（Azure）上部署ML模型。让我们从**Azure容器实例**（**ACI**）的部署开始。
- en: Deploying the model on ACI
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在ACI上部署模型
- en: 'To get started with deployment, go to the GitHub repository cloned on Azure
    DevOps previously (in [*Chapter 3*](B16572_03_Final_JM_ePub.xhtml#_idTextAnchor053),
    *Code Meets Data*), access the folder named `06_ModelDeployment`, and follow the
    implementation steps in the `01_Deploy_model_ACI.ipynb` notebook:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始部署，请访问之前在Azure DevOps上克隆的GitHub存储库（在[*第3章*](B16572_03_Final_JM_ePub.xhtml#_idTextAnchor053)，*代码与数据*），访问名为`06_ModelDeployment`的文件夹，并按照`01_Deploy_model_ACI.ipynb`笔记本中的实现步骤进行操作：
- en: 'We start by importing the required packages and check for the version of the
    Azure ML SDK, as shown in the following code:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先导入所需的包，并检查Azure ML SDK的版本，如下面的代码所示：
- en: '[PRE0]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The preceding code will print the Azure ML SDK version (for example, `1.10.0`;
    your version may be different).
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码将打印出Azure ML SDK的版本（例如，`1.10.0`；您的版本可能不同）。
- en: 'Next, using the `workspace` function from the Azure ML SDK, we connect to the
    ML workspace and download the required serialized files and model trained earlier
    using the `Model` function from the workspace. The serialized `scaler` and `model`
    are used to perform inference or prediction. `Scaler` will be used to shrink the
    input data to the same scale of data that was used for model training, and the
    `model` file is used to make predictions on the incoming data:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用Azure ML SDK中的`workspace`函数，我们连接到ML工作区，并使用工作区的`Model`函数下载之前训练的所需序列化文件和模型。序列化的`scaler`和`model`用于执行推理或预测。`Scaler`将用于将输入数据缩小到与模型训练中使用的数据相同的规模，而`model`文件用于对传入的数据进行预测：
- en: '[PRE1]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After the `scaler` and the `model` files are downloaded, the next step is to
    prepare the `scoring` file. The `scoring` file is used to infer the ML models
    in the containers deployed with the ML service in the Azure container instance
    and Kubernetes cluster. The `scoring` script takes input passed by the user and
    infers the ML model for prediction and then serves the output with the prediction
    to the user. It contains two primary functions, `init()` and `run()`. We start
    by importing the required libraries and then define the `init()` and `run()` functions:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下载了 `scaler` 和 `model` 文件之后，下一步是准备 `scoring` 文件。`scoring` 文件用于在 Azure 容器实例和
    Kubernetes 集群中部署的 ML 服务中推断 ML 模型。`scoring` 脚本接受用户传入的输入，推断预测用的 ML 模型，然后将预测结果以服务的形式提供给用户。它包含两个主要函数，`init()`
    和 `run()`。我们首先导入所需的库，然后定义 `init()` 和 `run()` 函数：
- en: '[PRE2]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`%%writefile score.py` writes this code into a file named `score.py`, which
    is later packed as part of the ML service in the container for performing ML model
    inference.'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`%%writefile score.py` 将此代码写入名为 `score.py` 的文件中，该文件随后作为 ML 服务的一部分打包到容器中，以执行
    ML 模型推理。'
- en: 'We define the `init()` function; it downloads the required models and deserializes
    them into variables to be used for the predictions:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了 `init()` 函数；它下载所需的模型并将它们反序列化到变量中，以便用于预测：
- en: '[PRE3]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In a nutshell, the `init()` function loads files (`model` and `scaler`) and
    deserializes and serves the model and artifact files needed for making predictions,
    which are used by the `run()` function as follows:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 简而言之，`init()` 函数加载文件（`model` 和 `scaler`），反序列化并提供用于预测所需的模型和工件文件，这些文件由 `run()`
    函数如下使用：
- en: '[PRE4]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we will proceed to the crucial part of deploying the service on an Azure
    container instance. For this, we define a deployment environment by creating an
    environment `myenv.yml`, as shown in the following code. Using the `CondaDependencies()`
    function, we mention all the `pip` packages that need to be installed inside the
    Docker container that will be deployed as the ML service. Packages such as `numpy`,
    `onnxruntime`, `joblib`, `azureml-core`, `azureml-defaults`, and `scikit-learn`
    are installed inside the container upon triggering the environment file:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将继续部署服务到 Azure 容器实例的关键部分。为此，我们通过创建环境 `myenv.yml` 来定义部署环境，如下所示。使用 `CondaDependencies()`
    函数，我们列出需要在作为 ML 服务部署的 Docker 容器中安装的所有 `pip` 包。例如，`numpy`、`onnxruntime`、`joblib`、`azureml-core`、`azureml-defaults`
    和 `scikit-learn` 等包在触发环境文件时会在容器中安装：
- en: '[PRE5]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next, we define the inference configuration by using the `InferenceConfig()`
    function, which takes `score.py` and the environment file as the arguments upon
    being called. Next, we call the `AciWebservice()` function to initiate the compute
    configuration ( `cpu_cores` and `memory`) in the `aciconfig` variable as follows:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用 `InferenceConfig()` 函数定义推理配置，该函数在被调用时以 `score.py` 和环境文件作为参数。然后，我们调用
    `AciWebservice()` 函数在 `aciconfig` 变量中初始化计算配置（`cpu_cores` 和 `memory`），如下所示：
- en: '[PRE6]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now we are all set to deploy the ML or web service on the ACI. We will use
    `score.py`, the environment file (`myenv.yml`), `inference_config`, and `aci_config`
    to deploy the ML or web service. We will need to point to the models or artifacts
    to deploy. For this, we use the `Model()` function to load the `scaler` and `model`
    files from the workspace and get them ready for deployment:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经准备好在 ACI 上部署 ML 或 Web 服务了。我们将使用 `score.py`、环境文件（`myenv.yml`）、`inference_config`
    和 `aci_config` 来部署 ML 或 Web 服务。我们需要指向要部署的模型或工件。为此，我们使用 `Model()` 函数从工作区加载 `scaler`
    和 `model` 文件，并准备好它们以供部署：
- en: '[PRE7]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'After the models are mounted into variables, `model1` and `model2`, we proceed
    with deploying them as a web service. We use the `deploy()` function to deploy
    the mounted models as a web service on the ACI, as shown in the preceding code.
    This process will take around 8 minutes, so grab your popcorn and enjoy the service
    being deployed. You will see a message like this:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在将模型挂载到变量 `model1` 和 `model2` 之后，我们继续将它们作为 Web 服务部署。我们使用 `deploy()` 函数在 ACI
    上部署挂载的模型，如前述代码所示。这个过程大约需要 8 分钟，所以拿好爆米花，享受服务部署的过程吧。你会看到如下信息：
- en: '[PRE8]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Congratulations! You have successfully deployed your first ML service using
    MLOps.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 恭喜！你已经成功使用 MLOps 部署了你的第一个 ML 服务。
- en: 'Let''s check out the workings and robustness of the deployed service. Check
    out the service URL and Swagger URL, as shown in the following code. You can use
    these URLs to perform ML model inference for input data of your choice in real
    time:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查已部署服务的运行机制和鲁棒性。查看服务URL和Swagger URL，如下面的代码所示。您可以使用这些URL实时对您选择的输入数据进行ML模型推断：
- en: '[PRE9]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Check for the deployed service in the Azure ML workspace.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Azure ML工作区中检查已部署的服务。
- en: 'Now, we can test the service using the Azure ML SDK `service.run()` function
    by passing some input data as follows:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以通过使用Azure ML SDK的`service.run()`函数并传递一些输入数据来测试该服务，如下所示：
- en: '[PRE10]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The service we have deployed is a REST API web service that we can infer with
    an HTTP request as follows:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们部署的服务是一个REST API网络服务，我们可以通过以下HTTP请求进行推断：
- en: '[PRE11]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: When a `POST` request is made by passing input data, the service returns the
    model prediction in the form of `0` or `1`. When you get such a prediction, your
    service is working and is robust enough to serve production needs.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当通过传递输入数据发出`POST`请求时，服务以`0`或`1`的形式返回模型预测。当你得到这样的预测时，你的服务正在运行，并且足够鲁棒，足以满足生产需求。
- en: Next, we will deploy the service on an auto-scaling cluster; this is ideal for
    production scenarios as the deployed service can auto-scale and serve user needs.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，我们将该服务部署在自动扩展的集群上；这对于生产场景来说非常理想，因为部署的服务可以自动扩展并满足用户需求。
- en: Deploying the model on Azure Kubernetes Service (AKS)
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Azure Kubernetes Service (AKS)上部署模型
- en: 'To get started with the deployment, go to the Git repository cloned on Azure
    DevOps in [*Chapter 3*](B16572_03_Final_JM_ePub.xhtml#_idTextAnchor053), *Code
    Meets Data*, access the `06_ModelDeployment` folder, and follow the implementation
    steps in the `02_Deploy_model_AKS.ipynb` notebook:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始部署，请转到Azure DevOps上克隆的Git仓库中的[*第3章*](B16572_03_Final_JM_ePub.xhtml#_idTextAnchor053)，*代码与数据相遇*，访问`06_ModelDeployment`文件夹，并按照`02_Deploy_model_AKS.ipynb`笔记本中的实现步骤进行操作：
- en: 'As we did in the previous section, start by importing the required packages,
    such as `matplotlib`, `numpy`, and `azureml.core`, and the required functions,
    such as `Workspace` and `Model`, from `azureml.core`, as shown in the following
    code block:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前一小节所述，首先导入所需的包，例如`matplotlib`、`numpy`和`azureml.core`，以及从`azureml.core`导入的所需函数，例如`Workspace`和`Model`，如下面的代码块所示：
- en: '[PRE12]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Print the version of the Azure ML SDK and check for the version (it will print,
    for example, `1.10.0`; your version may be different). Use the config file and
    `Workspace` function connect to your workspace, as shown in the following code
    block:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印Azure ML SDK的版本并检查版本（例如，它将打印`1.10.0`；您的版本可能不同）。使用配置文件和`Workspace`函数连接到您的空间，如下面的代码块所示：
- en: '[PRE13]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Download the `model` and `scaler` files as we did previously. After the `model`
    and the `scaler` files are downloaded, the next step is to prepare the `scoring`
    file, which is used to infer the ML models in the containers deployed with the
    ML service. The `scoring` script takes an input passed by the user, infers the
    ML model for prediction, and then serves the output with the prediction to the
    user. We will start by importing the required libraries, as shown in the following
    code block:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载`model`和`scaler`文件，就像我们之前做的那样。在下载了`model`和`scaler`文件之后，下一步是准备`scoring`文件，该文件用于推断与ML服务一起部署的容器中的ML模型。`scoring`脚本接受用户传递的输入，推断用于预测的ML模型，然后将预测结果以输出形式提供给用户。我们将首先导入所需的库，如下面的代码块所示：
- en: '[PRE14]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As we made `score.py` previously for ACI deployment, we will use the same file.
    It contains two primary functions, `init()` and `run()`. We define the `init()`
    function; it downloads the required models and deserializes them into variables
    to be used for predictions:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如我们之前为ACI部署创建`score.py`一样，我们将使用相同的文件。它包含两个主要函数，`init()`和`run()`。我们定义`init()`函数；它下载所需的模型并将它们反序列化为用于预测的变量：
- en: '[PRE15]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'As we did in the previous section on ACI deployment, by using `onnxruntime`
    package functions we can deserialize the support vector classifier model.The `InferenceSession()`
    function is used to deserialize and serve the model for inference, and the `input_name`
    and `label_name` variables are loaded from the deserialized model. In a nutshell,
    the `init()` function loads files (`model` and `scaler`), and deserializes and
    serves the model and artifact files needed for making predictions that are used
    by the `run()` function:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如我们在上一节关于ACI部署中做的那样，通过使用`onnxruntime`包函数，我们可以反序列化支持向量机分类器模型。《InferenceSession()`函数用于反序列化和为推理提供模型，`input_name`和`label_name`变量从反序列化的模型中加载。简而言之，`init()`函数加载文件（`model`和`scaler`），并反序列化和提供用于预测的模型和工件文件，这些文件由`run()`函数使用：
- en: '[PRE16]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We will use the same `run()` function previously used in the section *Deploying
    the model on ACI* for the AKS deployment. With this we can proceed to deploying
    the service on AKS.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将使用之前在*在ACI上部署模型*部分中使用的相同的`run()`函数进行AKS部署。这样我们就可以继续在AKS上部署服务。
- en: 'Next, we will proceed to the crucial part of deploying the service on `CondaDependencies()`
    function. We will mention all the required `pip` and `conda` packages to be installed
    inside the Docker container that will be deployed as the ML service. Packages
    such as `numpy`, `onnxruntime`, `joblib`, `azureml-core`, `azureml-defaults`,
    and `scikit-learn` are installed inside the container upon triggering the `environment`
    file. Next, use the publicly available container in the Microsoft Container Registry
    without any authentication. This container will install your environment and will
    be configured for deployment to your target AKS:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将进行部署服务在`CondaDependencies()`函数中的关键部分。我们将提及需要在部署为机器学习服务的Docker容器中安装的所有`pip`和`conda`包。例如`numpy`、`onnxruntime`、`joblib`、`azureml-core`、`azureml-defaults`和`scikit-learn`在触发`environment`文件时在容器中安装。接下来，使用Microsoft
    Container Registry中公开可用的容器，无需任何身份验证。该容器将安装您的环境，并配置为部署到您的目标AKS：
- en: '[PRE17]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, define the inference configuration by using the `InferenceConfig()` function,
    which takes `score.py` and the environment variable as the arguments upon being
    called:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，通过使用`InferenceConfig()`函数定义推理配置，该函数在被调用时接受`score.py`和环境变量作为参数：
- en: '[PRE18]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now we are all set to deploy the ML or web service on Azure Kubernetes Service
    (auto-scaling cluster). In order to do so, we will need to create an AKS cluster
    and attach it to the Azure ML workspace. Choose a name for your cluster and check
    if it exists using the `ComputeTarget()` function. If not, a cluster will be created
    or provisioned using the `ComputeTarget.create()` function. It takes a workspace
    object, `ws`; a service name; and a provisioning config to create the cluster.
    We use the default parameters for the provisioning config to create a default
    cluster:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经准备好在Azure Kubernetes Service（自动扩展集群）上部署机器学习或Web服务。为了做到这一点，我们需要创建一个AKS集群并将其附加到Azure
    ML工作区。为您的集群选择一个名称，并使用`ComputeTarget()`函数检查它是否存在。如果不存在，将使用`ComputeTarget.create()`函数创建或配置一个集群。创建集群需要以下参数：工作区对象`ws`；服务名称；以及用于创建集群的配置文件。我们使用默认参数创建默认集群：
- en: '[PRE19]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, we proceed to the critical task of deploying the ML service in the Kubernetes
    cluster. In order to deploy, we need some prerequisites, such as mounting the
    models to deploy. We mount the models using the `Model()` function to load the
    `scaler` and `model` files from the workspace and get them ready for deployment,
    as shown in the following code:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们进行部署机器学习服务在Kubernetes集群中的关键任务。为了部署，我们需要一些先决条件，例如将模型挂载到部署中。我们使用`Model()`函数将`scaler`和`model`文件从工作区加载，并准备好它们以供部署，如下面的代码所示：
- en: '[PRE20]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now we are all set to deploy the service on AKS. We deploy the service with
    the help of the `Model.deploy()` function from the Azure ML SDK, which takes the
    workspace object, `ws`; `service_name`; `models`; `inference_config`; `deployment_config`;
    and `deployment_target` as arguments upon being called:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经准备好在AKS上部署服务。我们使用Azure ML SDK中的`Model.deploy()`函数来部署服务，该函数在被调用时接受工作区对象`ws`；`service_name`；`models`；`inference_config`；`deployment_config`；和`deployment_target`作为参数：
- en: '[PRE21]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Deploying the service will take approximately around 10 mins. After deploying
    the ML service, you will get a message like the following:'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部署该服务大约需要10分钟左右。部署机器学习服务后，你将收到如下消息：
- en: '[PRE22]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Congratulations! Now you have deployed an ML service on AKS. Let's test it using
    the Azure ML SDK.
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 恭喜！现在您已经在AKS上部署了一个机器学习服务。让我们使用Azure ML SDK来测试它。
- en: 'We use the `service.run()` function to pass data to the service and get the
    predictions, as follows:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用 `service.run()` 函数将数据传递到服务并获取预测，如下所示：
- en: '[PRE23]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The deployed service is a REST API web service that can be accessed with an
    HTTP request as follows:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署的服务是一个 REST API 网络服务，可以通过以下 HTTP 请求进行访问：
- en: '[PRE24]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: When a `POST` request is made by passing input data, the service returns the
    model prediction in the form of `0` or `1`. When you get such a prediction, your
    service is working and is robust to serve production needs. The service scales
    from `0` to the needed number of container replicas based on the user's request
    traffic.
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当通过传递输入数据发出 `POST` 请求时，服务以 `0` 或 `1` 的形式返回模型预测。当您获得这样的预测时，您的服务正在运行，并且能够稳健地满足生产需求。服务根据用户的请求流量从
    `0` 缩放到所需的容器副本数。
- en: Deploying the service using MLflow
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 MLflow 部署服务
- en: 'Lastly, let''s do the deployment of an ML service on the deployment target
    (ACI) using MLflow to get hands-on experience with an open source framework. To
    get started, go to the Git repository cloned on Azure DevOps previously (in [*Chapter
    3*](B16572_03_Final_JM_ePub.xhtml#_idTextAnchor053), *Code Meets Data*), access
    the folder named `06_ModelDeployment`, and follow the implementation steps in
    the `02_Deploy_model_MLflow.ipynb` notebook. Before implementing, it is recommended
    to read this documentation to understand the concepts behind the `mlflow.azureml`
    SDK: [https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-mlflow#deploy-and-register-mlflow-models](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-mlflow#deploy-and-register-mlflow-models).'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们使用 MLflow 在部署目标（ACI）上部署一个机器学习服务，以获得使用开源框架的实际操作经验。要开始，请访问之前在 Azure DevOps
    上克隆的 Git 仓库（在 [*第 3 章*](B16572_03_Final_JM_ePub.xhtml#_idTextAnchor053)，*代码遇见数据*），访问名为
    `06_ModelDeployment` 的文件夹，并按照 `02_Deploy_model_MLflow.ipynb` 笔记本中的实现步骤进行操作。在实施之前，建议阅读此文档以了解
    `mlflow.azureml` SDK 背后的概念：[https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-mlflow#deploy-and-register-mlflow-models](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-mlflow#deploy-and-register-mlflow-models)。
- en: 'We start by importing the required packages and check for the version of the
    Azure ML SDK, as shown in the following code block:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先导入所需的包，并检查 Azure ML SDK 的版本，如下面的代码块所示：
- en: '[PRE25]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, using the `workspace` function from the Azure ML SDK, we connect to the
    ML workspace and set the tracking URI for the workspace using `set_tracking_uri`:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用 Azure ML SDK 中的 `workspace` 函数，我们连接到机器学习工作区，并使用 `set_tracking_uri` 设置工作区的跟踪
    URI：
- en: '[PRE26]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now go to the workspace and fetch the path to the `mlflow` model from the `models`
    or `experiments` section and set the path:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，请转到工作区，从 `models` 或 `experiments` 部分获取 `mlflow` 模型的路径并设置该路径：
- en: '[PRE27]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now we are all set to deploy to the ACI using `mlflow` and the `azureml` SDK.
    Configure the ACI deployment target using the `deploy_configuration` function
    and deploy to the ACI using the `mlflow.azureml.deploy` function. The `deploy`
    function takes `model_uri`, `workspace`, `model_name`, `service_name`, `deployment_config`,
    and custom tags as arguments upon being called:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好使用 `mlflow` 和 `azureml` SDK 将模型部署到 ACI。使用 `deploy_configuration` 函数配置
    ACI 部署目标，并使用 `mlflow.azureml.deploy` 函数将模型部署到 ACI。`deploy` 函数在被调用时接受 `model_uri`、`workspace`、`model_name`、`service_name`、`deployment_config`
    和自定义标签作为参数：
- en: '[PRE28]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'You will get a deployment succeeded message upon successful deployment. For
    more clarity on MLflow deployment, follow these examples: [https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-mlflow#deploy-and-register-mlflow-models](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-mlflow#deploy-and-register-mlflow-models).'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 部署成功后，您将收到一条部署成功的消息。有关 MLflow 部署的更多详细信息，请参考以下示例：[https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-mlflow#deploy-and-register-mlflow-models](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-mlflow#deploy-and-register-mlflow-models)。
- en: Congratulations! You have deployed ML models on diverse deployment targets such
    as ACI and AKS using `azureml` and `mlflow`.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您已使用 `azureml` 和 `mlflow` 在多种部署目标上部署了机器学习模型，例如 ACI 和 AKS。
- en: Next, we will focus on bringing the full capabilities of MLOps to the table
    using continuous integration and continuous deployment to have a robust and dynamically
    developing system in production.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将专注于通过持续集成和持续部署将 MLOps 的全部功能呈现出来，以便在生产中拥有一个强大且动态发展的系统。
- en: Understanding the need for continuous integration and continuous deployment
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解持续集成和持续部署的需求
- en: '**Continuous integration** (**CI**) and **continuous deployment** (**CD**)
    enable continuous delivery to the ML service. The goal is to maintain and version
    the source code used for model training, enable triggers to perform necessary
    jobs in parallel, build artifacts, and release them for deployment to the ML service.
    Several cloud vendors enable DevOps services that can be used for monitoring ML
    services, ML models in production, and orchestration with other services in the
    cloud.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**持续集成**（**CI**）和**持续部署**（**CD**）使ML服务能够实现持续交付。目标是维护和版本控制用于模型训练的源代码，启用触发器以并行执行必要的作业，构建工件，并将它们发布以部署到ML服务。几个云服务提供商提供了DevOps服务，可用于监控ML服务、生产中的ML模型，以及与云中其他服务的编排。'
- en: Using CI and CD, we can enable continuous learning, which is critical for the
    success of an ML system. Without continuous learning, an ML system is destined
    to end up as a failed **PoC** (**Proof of Concept**). We will delve into the concepts
    of CI/CD and implement hands-on CI and CD pipelines to see MLOps in play in the
    next chapter.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 使用CI和CD，我们可以实现持续学习，这对于ML系统的成功至关重要。没有持续学习，ML系统注定会以失败的**PoC**（**概念验证**）告终。我们将在下一章深入探讨CI/CD的概念，并实施动手的CI和CD管道，以看到MLOps的实际应用。
- en: Summary
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have learned the key principles of deploying ML models in
    production. We explored the various deployment methods and targets and their needs.
    For a comprehensive understanding and hands-on experience, we implemented the
    deployment to learn how ML models are deployed on a diverse range of deployment
    targets such as virtual machines, containers, and in an auto-scaling cluster.
    With this, you are ready to handle any type of deployment challenge that comes
    your way.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了在生产中部署ML模型的关键原则。我们探讨了各种部署方法和目标及其需求。为了全面理解和动手实践，我们实现了部署，以了解ML模型是如何在多种部署目标上部署的，例如虚拟机、容器和自动扩展集群。有了这些，你就可以应对任何类型的部署挑战。
- en: In the next chapter, we will delve into the secrets to building, deploying,
    and maintaining robust ML services enabled by CI and CD. This will enable the
    potential of MLOps! Let's delve into it.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨利用CI（持续集成）和CD（持续部署）构建、部署和维护健壮的ML（机器学习）服务的秘诀。这将实现MLOps（机器学习运维）的潜力！让我们深入探讨。
