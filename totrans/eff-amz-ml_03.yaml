- en: Overview of an Amazon Machine Learning Workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter offers an overview of the workflow of a simple Amazon Machine
    Learning (Amazon ML) project, which comprises three main phases:'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training and selecting the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Making predictions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The reader will learn how to get started on the Amazon Machine Learning platform,
    how to set up an account, and how to secure it. In the second part, we go through
    a simple numeric prediction problem based on a classic dataset. We describe each
    of the three steps mentioned above, what happens, what to expect, and how to interpret
    the final result.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will study the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Opening an Amazon Web Services (AWS) account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up the account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of a standard Amazon Machine Learning workflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Opening an Amazon Web Services Account
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Signing up for an AWS account is straightforward. Go to [http://aws.amazon.com/](http://aws.amazon.com/),
    and choose Create a AWS Account. If you don't have an AWS account yet, take advantage
    of the free tier access. New free-tier accounts enjoy free resources up to a certain
    limit for up to 12 months. These free resources are available for many AWS services,
    such as EC2, S3, RDS or Redshift, and so forth. Unfortunately, Amazon Machine
    Learning is not included in the AWS Free Tier. You will be billed for your Amazon
    ML usage. However, since Amazon ML requires your data to be stored on S3 or another
    AWS source such as RedShift, which are included in the Free Tier offer, it will
    still be advantageous to start with a free tier account. Follow the instructions
    to open a free tier account. You will be asked for your name, e-mail, address,
    phone number, and payment information.
  prefs: []
  type: TYPE_NORMAL
- en: If you are already an Amazon retail customer, it will be easier if you separate
    your AWS account from your personal retail amazon account by using a different
    e-mail address for your AWS account. This is especially true if you plan to recover
    the costs of working on Amazon ML for professional purposes. Using the same e-mail
    for both your personal retail account (Amazon Prime, echo, and so on) and your
    AWS account could become confusing.
  prefs: []
  type: TYPE_NORMAL
- en: Security
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The e-mail and password you have used to open an AWS account are called your
    root credentials. They give you root access to every AWS service, which means
    unlimited access to unlimited resources. Someone obtaining your root credentials
    without your knowledge could rack up a heavy bill and they could carry out all
    types of activities through your account in your name. It is highly recommended
    not to use this root access in your everyday operations with AWS and to set up
    your account with the highest security level possible.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, AWS offers many ways to control and compartmentalize access to
    your AWS account through users, groups, roles, policies, passwords, and multi-factor
    authentication to reduce risks of unlawful access and fraud. Configuring and managing
    access to your account is done via the **AWS Identity and Access Management (IAM)**
    service. The IAM service is free of charge.
  prefs: []
  type: TYPE_NORMAL
- en: The IAM service allows you to create and configure all the access points to
    the different AWS services you plan to use. Having this level of granularity is
    important. You can restrict access by user, by service, by role, or even enable
    temporary access through tokens, which are limited in time. Enabling multi-factor
    authentication is another strongly recommended feature you should enable in order
    to prevent unauthorized access to your AWS account.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of this book, we will create a single user with unlimited access
    to only two services: Amazon ML and S3\. We will extend this user''s access to
    other AWS services as we need them in following chapters.'
  prefs: []
  type: TYPE_NORMAL
- en: We won't go through all the features offered by IAM here, but it's strongly
    recommended that you familiarize yourself with the IAM documentation and best
    practices ([http://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html](http://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html)).
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the account
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you set up your account for the first time, you are given access to your
    root public and secret access keys. These keys will be useful as you manage data
    on S3 and models in Amazon ML via the command line interface (AWS CLI). These
    two keys will only be available for you to view and copy at the time of creation.
    Once that page in your browser is closed, you can no longer access them and will
    need to create new ones. Creating new root keys is done by accessing My Account
    | Security Credentials. It's worth noting that no one can have access to your
    keys in AWS, not even the administrator of your account.
  prefs: []
  type: TYPE_NORMAL
- en: We won't go through all the possible actions you can take in IAM to configure,
    manage, and secure access to your account as a full presentation of IAM is beyond
    the scope of this book. Your access management needs and policies will depend
    on the size of your organization and security constraints. We assume here that
    you are a unique individual user of the account and that you do not need to set
    up password policies for other users, groups, or roles. However, we strongly recommend
    you familiarize yourself with IAM documentation and implement the IAM best practice
    ([https://aws.amazon.com/documentation/iam](https://aws.amazon.com/documentation/iam)).
  prefs: []
  type: TYPE_NORMAL
- en: We will create a new user whose username is `AML@Packt` and will use this access
    for the rest of the book. The only time when we use the root access (with the
    password you used to create the AWS account in the first place) is when we need
    to add or remove services to and from the `AML@Packt` user, for instance, when
    we want the user to use Amazon Athena for data processing or **Amazon Lambda**
    for scripting.
  prefs: []
  type: TYPE_NORMAL
- en: 'The IAM dashboard is available at [https://console.aws.amazon.com/iam](https://console.aws.amazon.com/iam/home#/home).
    It displays how many IAM assets you have created (users, roles, groups, and so
    on) as well as your security status as shown by this screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This screenshot shows that we have implemented the following three items:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Delete your root access keys**: These keys were given to you when you created
    your account. Since they provide unlimited access to your account, you should
    delete them and use only user-based access keys to access your account.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Activate Multi Factor Authentication on your root account:** After you have
    logged in with your login and password, Multi Factor Authentication (MFA) requires
    you to input a six digit code. This code can either be sent to you via text or
    e-mail or made available via an authenticator app installed on your mobile phone.
    MFA is a easy-to-implement and efficient means to secure access to your account.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Create individual IAM users:** By creating individual users you can restrict,
    manage their access level, and deactivate their account easily.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You could also create groups to assign permissions to your users and define
    a password policy.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a user
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start by creating your user. Go to the IAM dashboard at [https://console.aws.amazon.com/iam/](https://console.aws.amazon.com/iam/)
    and click on Users on the left sidebar. The user creation process is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: Click Create New Users.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter your user name. Keep the Generate an access key for each user checkbox
    selected.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'At that point, your user is created and you can choose to view and download
    the user security credentials. One of these two keys, the **Secret Access Key**,
    will no longer be available once you move away from this page. Be sure to copy
    or download the two keys now. If you lose the keys, you can always recreate a
    pair of new keys for that user from the IAM > Users dashboard. This screenshot
    is an example of the two user keys for the `AML@Packt` user we just created. Your
    keys will obviously be different than these ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_002.png)'
  prefs: []
  type: TYPE_IMG
- en: And that's it! Your `AML@Packt` user has been created. You can use `AML@Packt`'s
    access keys to access and manage AWS services via APIs and the command line. At
    this point, the `AML@Packt` user has unlimited access to all AWS services. In
    order to restrict the access scope of that user, you will need to attach policies
    to the user.
  prefs: []
  type: TYPE_NORMAL
- en: Defining policies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Policies declare what services a user or a group can access and the level of
    access (read-only, full access, and so forth). You can define global policies
    that take care of several services at the same time and attach them to groups
    of users, or you can attach specific mono-service policies to your user. This
    is what we'll do now.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have created your user and downloaded its credentials, you end up
    on the IAM user dashboard with a list of all your created users. Select the `AML@Packt`
    user and the permissions tab. Check all the services you know that the user will
    need to access. In our case, we select two services each with full access, which
    will be sufficient to explore the Amazon Machine Learning service:'
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Machine Learning Full Access
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon S3 Full Access
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will add other policies to these users to enable other services (Athena,
    RedShift, RDS, and so on) as needed later on.
  prefs: []
  type: TYPE_NORMAL
- en: Creating login credentials
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Last but not least if we want to use the `AML@Packt` user to log in to the
    AWS console, we must create login credentials for that user. As shown in the next
    screenshot, the Security Credentials tab is where you manage the user access keys,
    sign in credentials, and SSH keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_003.png)'
  prefs: []
  type: TYPE_IMG
- en: SSH keys are not the same as access keys. SSH keys will let you SSH into certain
    assets, such as *EC2* servers. Many services machine learning included have no
    use for SSH keys. Access keys, on the other hand, are used to programmatically
    manage AWS services. Access keys are necessary for setting the credentials needed
    to use the command line interface (AWS CLI).
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on Manage Password and set a password for the user. This is what the
    permission for user `AML@Packt` looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At this point, our IAM dashboard looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This previous screenshot shows the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We have deleted the root access keys. We can no longer programmatically access
    all AWS services in an unlimited fashion via the command line or APIs. We can
    still log in as root to the AWS console to create and manage access for people,
    but these will depend on the policies and access level we provide them with.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have activated **Multi Factor Authentication (MFA)**, a simple and very efficient
    way to secure access to AWS Services from your root access credentials.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have created the `AML@Packt` user, which we will use to log in to AWS and
    when using the APIs or the command line.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have not created groups or password policies that would further constrain
    the root access as we intend to only access Amazon ML through the `AML@Packt`
    user.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a summary of the different ways you can access and use AWS services:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in on the AWS Console with your root password and login using **Multi Factor
    Authentication (MFA)**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Log in with the `AML@Packt` user with that user's login and password using MFA.
    The `AML@Packt` user can only use S3 and Amazon ML services. This is quite restrictive,
    but just the right amount of access for that user, nothing more.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Programmatically access S3 and Amazon ML via the S3 and Amazon ML access keys
    using the command line interface or AWS SDKs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We had started with one user, the root user able to access everything AWS can
    offer programmatically and via the console. The new setup is much more secure
    and worth the time it took to set it up. Our newly gained understanding of IAM
    roles and policies will also be helpful later on when we start using different
    AWS services in tandem as the services will need to have appropriate access to
    one another.
  prefs: []
  type: TYPE_NORMAL
- en: Before we dive into the presentation of a standard Amazon ML workflow, we need
    a brief word on regions.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a region
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AWS currently operates data centers in 14 regions across the globe. More regions
    are being frequently opened across the globe ([https://aws.amazon.com/about-aws/global-infrastructure/](https://aws.amazon.com/about-aws/global-infrastructure/)).
    Most AWS services require you to choose a region of operation. The rule of thumb
    is to choose the region that is closest to you or the end-users accessing your
    resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'Choosing a region may depend on other factors, which may vary across regions,
    including:'
  prefs: []
  type: TYPE_NORMAL
- en: Latency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pricing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security and compliance rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Availability of the service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SLA–Service Level Agreements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use of renewable energy. At the time of writing, AWS offers two carbon neutral
    regions ([https://aws.amazon.com/about-aws/sustainability/](https://aws.amazon.com/about-aws/sustainability/))
    and is actively creating others .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon ML is only available in Northern Virginia and Ireland. You can check
    the following page for AWS availability per region: [https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/](https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/).
  prefs: []
  type: TYPE_NORMAL
- en: '**GovCloud:** From the AWS documentation: The AWS GovCloud (US) is an isolated
    AWS region designed to host sensitive data and regulated workloads in the cloud,
    helping customers support their US government compliance requirements. IAM is
    region free, with the following exception: IAM is available in the US East (N.
    Virginia) region and in the GovCloud region. Users and roles created in the US
    East region can be used in all other regions except the GovCloud region. And GovCloud
    IAM users cannot be used outside of the GovCloud region.'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of a standard Amazon Machine Learning workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Amazon Machine Learning service is available at [https://console.aws.amazon.com/machinelearning/](https://console.aws.amazon.com/machinelearning/).
    The Amazon ML workflow closely follows a standard Data Science workflow with steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Extract the data and clean it up. Make it available to the algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the data into a training and validation set, typically a 70/30 split with
    equal distribution of the predictors in each part.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the best model by training several models on the training dataset and
    comparing their performances on the validation dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the best model for predictions on new data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As shown in the following Amazon ML menu, the service is built around four
    objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_006.png)'
  prefs: []
  type: TYPE_IMG
- en: Datasource
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Datasource and Model can also be configured and set up in the same flow
    by creating a new Datasource and ML model. Let us take a closer look at each one
    of these steps.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the rest of the chapter, we will use the simple `Predicting Weight by Height
    and Age` dataset (from *Lewis Taylor (1967)*) with 237 samples of children's age,
    weight, height, and gender, which is available at [https://v8doc.sas.com/sashtml/stat/chap55/sect51.htm](https://v8doc.sas.com/sashtml/stat/chap55/sect51.htm).
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset is composed of 237 rows. Each row has the following predictors:
    sex (F, M), age (in *months*), height (in *inches*), and we are trying to predict
    the weight (in *lbs*) of these children. There are no missing values and no outliers.
    The variables are close enough in range and normalization is not required. In
    short, we do not need to carry out any preprocessing or cleaning on the original
    dataset. Age, height, and weight are numerical variables (real-valued), and sex
    is a categorical variable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will randomly select 20% of the rows as the held-out subset to use for prediction
    on previously unseen data and keep the other 80% as training and evaluation data.
    This data split can be done in Excel or any other spreadsheet editor:'
  prefs: []
  type: TYPE_NORMAL
- en: By creating a new column with randomly generated numbers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sorting the spreadsheet by that column
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting 190 rows for training and 47 rows for prediction (roughly a 80/20
    split)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let us name the training set `LT67_training.csv` and the held-out set that we will
    use for prediction `LT67_heldout.csv`, where *LT67* stands for *Lewis and Taylor,*
    the creator of this dataset in 1967.
  prefs: []
  type: TYPE_NORMAL
- en: As with all datasets, scripts, and resources mentioned in this book, the training
    and holdout files are available in the GitHub repository at [https://github.com/alexperrier/packt-aml](https://github.com/alexperrier/packt-aml).
  prefs: []
  type: TYPE_NORMAL
- en: Note that it is important for the distribution in age, sex, height, and weight
    to be similar in both subsets. We want the data on which we will make predictions
    to show patterns that are similar to the data on which we will train and optimize
    our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the rest of this section, we will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the data on S3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let Amazon ML infer the schema and transform the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the model's performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make a prediction on the held-out dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Loading the data on S3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Follow these stepsto load the training and held-out datasets on S3:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to your s3 console at [https://console.aws.amazon.com/s3](https://console.aws.amazon.com/s3).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a bucket if you haven't done so already. Buckets are basically folders
    that are uniquely named across all S3\. We created a bucket named `aml.packt`.
    Since that name has now been taken, you will have to choose another bucket name
    if you are following along with this demonstration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the bucket name you created and upload both the `LT67_training.csv`
    and `LT67_heldout.csv` files by selecting Upload from the Actions drop-down menu:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/image_03_007.png)'
  prefs: []
  type: TYPE_IMG
- en: Both files are small, only a few KB, and hosting costs should remain negligible
    for that exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Note that for each file, by selecting the Properties tab on the right, you can
    specify how your files are accessed, what user, role, group or AWS service may
    download, read, write, and delete the files, and whether or not they should be
    accessible from the Open Web. When creating the datasource in Amazon ML, you will
    be prompted to grant Amazon ML access to your input data. You can specify the
    access rules to these files now in S3 or simply grant access later on.
  prefs: []
  type: TYPE_NORMAL
- en: Our data is now in the cloud in an S3 bucket. We need to tell Amazon ML where
    to find that input data by creating a datasource. We will first create the datasource
    for the training file `ST67_training.csv`.
  prefs: []
  type: TYPE_NORMAL
- en: Declaring a datasource
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Go to the Amazon ML dashboard, and click on Create new... | Datasource and
    ML model. We will use the faster flow available by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_008.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As shown in the following screenshot, you are asked to specify the path to
    the `LT67_training.csv` file `{S3://bucket}{path}{file}`. Note that the S3 location
    field automatically populates with the bucket names and file names that are available
    to your user:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Specifying a Datasource name is useful to organize your Amazon ML assets. By
    clicking on Verify, Amazon ML will make sure that it has the proper rights to
    access the file. In case it needs to be granted access to the file, you will be
    prompted to do so as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_010.png)'
  prefs: []
  type: TYPE_IMG
- en: Just click on Yes to grant access. At this point, Amazon ML will validate the
    datasource and analyze its contents.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the datasource
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An Amazon ML datasource is composed of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The location of the data file: The data file is not duplicated or cloned in
    Amazon ML but accessed from S3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The schema that contains information onthe type of the variables contained
    in the CSV file:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categorical
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Text
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Numeric (real-valued)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Binary
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: As we will see in [Chapter 4](08d9b49a-a25c-4706-8846-36be9538b087.xhtml), *Loading
    and Preparing the Dataset*, it is possible to supply Amazon ML with your own schema
    or modify the one created by Amazon ML.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, Amazon ML has a pretty good idea of the type of data in your
    training dataset. It has identified the different types of variables and knows
    how many rows it has:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Move on to the next step by clicking on Continue, and see what schema Amazon
    ML has inferred from the dataset as shown in the next screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_012.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Amazon ML needs to know at that point which is the variable you are trying
    to predict. Be sure to tell Amazon ML the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The first line in the CSV file contains te column name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The target is the `weight`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We see here that Amazon ML has correctly inferred the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sex` is categorical'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`age`, `height`, and `weight` are numeric (continuous real values)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since we chose a numeric variable as the target Amazon ML, will use Linear
    Regression as the predictive model. For binary or categorical values, we would
    have used Logistic Regression. This means that Amazon ML will try to find the
    best *a*, *b*, and *c* coefficients so that the weight predicted by the following
    equation is as close as possible to the observed real weight present in the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '*predicted weight = a * age + b * height + c * sex*'
  prefs: []
  type: TYPE_NORMAL
- en: Amazon ML will then ask you if your data contains a row identifier. In our present
    case, it does not. Row identifiers are useful when you want to understand the
    prediction obtained for each row or add an extra column to your dataset later
    on in your project. Row identifiers are for reference purposes only and are not
    used by the service to build the model.
  prefs: []
  type: TYPE_NORMAL
- en: You will be asked to review the datasource. You can go back to each one of the
    previous steps and edit the parameters for the schema, the target and the input
    data. Now that the data is known to Amazon ML, the next step is to set up the
    parameters of the algorithm that will train the model.
  prefs: []
  type: TYPE_NORMAL
- en: The model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We select the default parameters for the training and evaluation settings.
    Amazon ML will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a recipe for data transformation based on the statistical properties
    it has inferred from the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Split the dataset (`ST67_training.csv`) into a training part and a validation
    part, with a 70/30 split. The split strategy assumes the data has already been
    shuffled and can be split sequentially.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In [Chapter 4](08d9b49a-a25c-4706-8846-36be9538b087.xhtml), *Loading and Preparing
    the Dataset*, we will take the longer road and work directly on the recipes, the
    schemas, and the validation split. On the next page, you are asked to review the
    model you just created.
  prefs: []
  type: TYPE_NORMAL
- en: The recipe will be used to transform the data in a similar way for the training
    and the validation datasets. The only transformation suggested by Amazon ML is to
    transform the categorical variable `sex` into a binary variable, where `m = 0`
    and `f = 1` for instance. No other transformation is needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The default advanced settings for the model are shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_013.png)'
  prefs: []
  type: TYPE_IMG
- en: We see that Amazon ML will pass over the data 10 times, shuffle splitting the
    data each time. It will use an L2 regularization strategy based on the sum of
    the square of the coefficients of the regression to prevent overfitting. We will
    evaluate the predictive power of the model using our `LT67_heldout.csv` dataset
    later on.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization comes in 3 levels with a *mild* (10^(^-6)), *medium* (10^(^-4)),
    or *aggressive* (10^(^-02)) setting, each value stronger than the previous one.
    The default setting is mild*,* the lowest, with a regularization constant of *0.00001*
    (10^(^-6)) implying that Amazon ML does not anticipate much overfitting on this
    dataset. This makes sense when the number of predictors, three in our case, is
    much smaller than the number of samples (190 for the training set).
  prefs: []
  type: TYPE_NORMAL
- en: Clicking on the Create ML model button will launch the model creation. This
    takes a few minutes to resolve, depending on the size and complexity of your dataset.
    You can check its status by refreshing the model page. In the meantime, the model
    status remains pending.
  prefs: []
  type: TYPE_NORMAL
- en: 'At that point, Amazon ML will split our training dataset into two subsets:
    a training and a validation set. It will use the training portion of the data
    to train several settings of the algorithm and select the best one based on its
    performance on the training data. It will then apply the associated model to the
    validation set and return an evaluation score for that model. By default, Amazon
    ML will sequentially take the first 70% of the samples for training and the remaining
    30% for validation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s worth noting that Amazon ML will not create two extra files and store
    them on S3, but instead create two new datasources out of the initial datasource
    we have previously defined. Each new datasource is obtained from the original
    one via a `Data rearrangement` JSON recipe such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see these two new datasources in the Datasource dashboard. Three datasources
    are now available where there was initially only one, as shown by the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_014.png)'
  prefs: []
  type: TYPE_IMG
- en: 'While the model is being trained, Amazon ML runs the Stochastic Gradient algorithm
    several times on the training data with different parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Varying the learning rate in increments of powers of 10: 0.01, 0.1, 1, 10,
    and 100.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making several passes over the training data while shuffling the samples before
    each path.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At each pass, calculating the prediction error, the **Root Mean Squared Error
    (RMSE)**, to estimate how much of an improvement over the last pass was obtained.
    If the decrease in RMSE is not really significant, the algorithm is considered
    to have converged, and no further pass shall be made.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of the passes, the setting that ends up with the lowest RMSE wins,
    and the associated model (the weights of the regression) is selected as the best
    version.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the model has finished training, Amazon ML evaluates its performance on
    the validation datasource. Once the evaluation itself is also ready, you have
    access to the model's evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: The evaluation of the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Amazon ML uses the standard metric RMSE for linear regression. RMSE is defined
    as the sum of the squares of the difference between the real values and the predicted
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_015.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *ŷ* is the predicted values, and *y* the real values we want to predict
    (the weight of the children in our case). The closer the predictions are to the
    real values, the lower the RMSE is. A lower RMSE means a better, more accurate
    prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing with a baseline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The RMSE is a relative quantity and not an absolute one. An RMSE of 100 may
    be good for a certain context, while a RMSE of 10 may be a sign of poor predictions
    in another context. It is, therefore, important to have a baseline that we can
    compare our model to. Each Amazon ML evaluation provides a baseline, which is
    calculated differently depending on the nature of the problem (regression, binary
    or multiclass classification). The baseline is the score we would obtain using
    the most simple and obvious model:'
  prefs: []
  type: TYPE_NORMAL
- en: The baseline for regression is given by a model that always predicts the mean
    of the target
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The baseline for binary classification is an AUC of *0.5*, which is the score
    for a model that randomly assigns 0 or 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The baseline for a multiclass classification problem is the macro average *F1*
    score for a model that would always predict the most common class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In our current case, when predicting the weight of students from the height,
    age, and sex, our training dataset has a baseline RMSE of *18.71*, while our model
    gives an RMSE of *14.44*. Our model does 22.8% better than simply predicting the
    average of the weights of all the students.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another thing to look at in the evaluation is the distribution of the residuals,
    which are defined as the difference between *ŷ* and *y*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_020.png)'
  prefs: []
  type: TYPE_IMG
- en: As we've seen in [Chapter 1](767f8b14-c3f2-4e45-bfcd-bb45ae2b9e65.xhtml), *Introduction
    to Machine Learning and Predictive Analytics*, one of the conditions for a linear
    regression to be considered valid is that the residuals are independent, identically
    distributed, and their distribution follows a **Gaussian distribution** or **Bell-shaped
    curve**. Amazon ML shows a histogram of the residuals that can help us visually
    assess the Gaussian nature of the residuals distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the shape bends to the right (left), this means that more predictions
    are greater (lower) than their targets. In both cases, the conclusion is that
    there is still some signal in the data that has not been captured by the model.
    When the distribution of residuals is centered on 0, the linear regression is
    considered valid. The following diagram shows the distributions of residuals for
    our current regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_021.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that our predictions are often larger than the targets, which indicates
    that our model could still be improved. There is information in the data that
    has not been exploited by that model.
  prefs: []
  type: TYPE_NORMAL
- en: Note that it may not be possible to extract the information in these patterns
    with a linear regression model. Other more complex models may be more adapted
    to this particular dataset. Transforming the data to create new variables may
    also be the key to a better model. The histogram of residuals is a good, simple
    diagnostic with regard to the quality of our model as it shows us that it could
    be improved in some way.
  prefs: []
  type: TYPE_NORMAL
- en: Making batch predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now have a model that has been properly trained and selected among other
    models. We can use it to make predictions on new data.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that, at the beginning of this chapter, under the section *Loading
    the Data on S3*, we uploaded two datasets to S3, the training dataset and the
    held-out dataset. We've used the training dataset to create the best model possible.
    We will now apply that model on the held-out dataset.
  prefs: []
  type: TYPE_NORMAL
- en: A batch prediction consists in applying a model to a datasource in order to
    make predictions on that datasource. We need to tell Amazon ML which model we
    want to apply on which data.
  prefs: []
  type: TYPE_NORMAL
- en: Batch predictions are different from streaming predictions. With batch predictions,
    all the data is already made available as a datasource, while for streaming predictions,
    the data will be fed to the model as it becomes available. The dataset is not
    available beforehand in its entirety.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Main Menu select Batch Predictions to access the dashboard predictions
    and click on Create a New Prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_022.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The first step is to select one of the models available in your model dashboard.
    You should choose the one that has the lowest RMSE:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_023.png)'
  prefs: []
  type: TYPE_IMG
- en: The next step is to associate a datasource to the model you just selected. We
    had uploaded the held-out dataset to S3 at the beginning of this chapter (under
    the *Loading the data on S3* section) but had not used it to create a datasource.
    We will do so now.
  prefs: []
  type: TYPE_NORMAL
- en: 'When asked for a datasource in the next screen, make sure to check My data
    is in S3, and I need to create a datasource, and then select the held-out dataset
    that should already be present in your S3 bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_024.png)'
  prefs: []
  type: TYPE_IMG
- en: Don't forget to tell Amazon ML that the first line of the file contains columns.
  prefs: []
  type: TYPE_NORMAL
- en: In our current project, our held-out dataset also contains the true values for
    the weight of the students. This would not be the case for "real" data in a real-world
    project where the real values are truly unknown. However, in our case, this will
    allow us to calculate the RMSE score of our predictions and assess the quality
    of these predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final step is to click on the Verify button and wait for a few minutes:'
  prefs: []
  type: TYPE_NORMAL
- en: Amazon ML will run the model on the new datasource and will generate predictions
    in the form of a CSV file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contrary to the evaluation and model-building phase, we now have real predictions.
    We are also no longer given a score associated with these predictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After a few minutes, you will notice a new batch-prediction folder in your
    S3 bucket. This folder contains a `manifest` file and a results folder. The manifest
    file is a JSON file with the path to the initial datasource and the path to the
    results file. The results folder contains a gzipped CSV file:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/image_03_025.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Uncompressed, the CSV file contains two columns, `trueLabel`, the initial target
    from the held-out set, and `score`, which corresponds to the predicted values.
    We can easily calculate the RMSE for those results directly in the spreadsheet
    through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a new column that holds the square of the difference of the two columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summing all the rows.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Taking the square root of the result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following illustration shows how we create a third column `C`, as the squared
    difference between the `trueLabel` column A and the score (or predicted value)
    column B:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_026.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As shown in the following screenshot, averaging column `C` and taking the square
    root gives an RMSE of 11.96, which is even significantly better than the RMSE
    we obtained during the evaluation phase (*RMSE 14.4*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_027.png)'
  prefs: []
  type: TYPE_IMG
- en: The fact that the RMSE on the held-out set is better than the RMSE on the validation
    set means that our model did not overfit the training data, since it performed
    even better on new data than expected. Our model is robust.
  prefs: []
  type: TYPE_NORMAL
- en: 'The left side of the following graph shows the True (*Triangle*) and Predicted
    (*Circle*) `Weight` values for all the samples in the held-out set. The right
    side shows the histogram of the residuals. Similar to the histogram of residuals
    we had observed on the validation set, we observe that the residuals are not centered
    on *0*. Our model has a tendency to overestimate the weight of the students:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_028.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first part of this chapter, we went through the Amazon account creation
    and how to properly set up and secure access to your AWS account. Using a combination
    of Multi Factor Authentication and User creation, we were able to quickly reach
    a satisfactory level of safety. AWS is a powerful platform with powerful tools
    and it's important to implement the best access protection possible.
  prefs: []
  type: TYPE_NORMAL
- en: In the second part, we went through the different steps involved in a simple
    linear regression prediction, from loading the data into S3, making that data
    accessible to Amazon ML via datasources, creating models, interpreting evaluations,
    and making predictions on new data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Amazon ML flow is smooth and facilitates the inherent data science loop:
    data, model, evaluation, and prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, we will dive further into data preparation and data
    transformation. This time we will use a classic binary classification problem,
    namely, *survival on the Titanic*, which is based on a very interesting dataset.
  prefs: []
  type: TYPE_NORMAL
