<html><head></head><body>
<div id="_idContainer012">
<h1 class="chapter-number" id="_idParaDest-14"><a id="_idTextAnchor031"/><a id="_idTextAnchor032"/><a id="_idTextAnchor033"/><a id="_idTextAnchor034"/><span class="koboSpan" id="kobo.1.1">1</span></h1>
<h1 id="_idParaDest-15"><a id="_idTextAnchor035"/><span class="koboSpan" id="kobo.2.1">The History and Development of Time Series Forecasting</span></h1>
<p><strong class="bold"><span class="koboSpan" id="kobo.3.1">Prophet</span></strong><span class="koboSpan" id="kobo.4.1"> is a </span><a id="_idTextAnchor036"/><a id="_idIndexMarker000"/><span class="koboSpan" id="kobo.5.1">powerful tool for creating, visualizing, and optimizing your forecasts! </span><span class="koboSpan" id="kobo.5.2">With Prophet, you’ll be able to understand what factors will drive your future results, which will enable you to make more confident decisions. </span><span class="koboSpan" id="kobo.5.3">You accomplish these tasks and goals through an intuitive but very flexible programming interface that is designed for both the beginner and </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">expert alike.</span></span></p>
<p><span class="koboSpan" id="kobo.7.1">You don’t need a deep knowledge of the math or statistics behind time series forecasting techniques to leverage the power of Prophet, although if you do possess this knowledge, Prophet includes a rich feature set that allows you to deploy your experience to great effect. </span><span class="koboSpan" id="kobo.7.2">You’ll be working in a structured paradigm where each problem follows the same pattern, allowing you to spend less time figuring out how to optimize your forecasts and more time discovering key insights to supercharge </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">your decisions.</span></span></p>
<p><span class="koboSpan" id="kobo.9.1">This chapter introduces the foundational ideas behind time series forecasting and discusses some of the key model iterations that eventually led to the development of Prophet. </span><span class="koboSpan" id="kobo.9.2">In this chapter, you’ll learn what time series data is and why it must be handled differently than non-time series data, and then you’ll discover the most powerful innovations, of which Prophet is one of the latest. </span><span class="koboSpan" id="kobo.9.3">Specifically, we will cover the </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.11.1">Understanding time </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">series forecasting</span></span></li>
<li><span class="koboSpan" id="kobo.13.1">Moving averages and </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">exponential smoothing</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.15.1">ARIMA</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.16.1">ARCH/GARCH</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.17.1">Neural networks</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.18.1">Prophet</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.19.1">Recent developments</span></span><a id="_idTextAnchor037"/><a id="_idTextAnchor038"/></li>
</ul>
<h1 id="_idParaDest-16"><a id="_idTextAnchor039"/><span class="koboSpan" id="kobo.20.1">Understanding time series forecasting</span></h1>
<p><span class="koboSpan" id="kobo.21.1">A </span><strong class="bold"><span class="koboSpan" id="kobo.22.1">time series</span></strong><span class="koboSpan" id="kobo.23.1"> is</span><a id="_idIndexMarker001"/><span class="koboSpan" id="kobo.24.1"> a se</span><a id="_idTextAnchor040"/><span class="koboSpan" id="kobo.25.1">t of data collected sequentially over time. </span><span class="koboSpan" id="kobo.25.2">For example, think of any chart where the </span><em class="italic"><span class="koboSpan" id="kobo.26.1">x a</span></em><span class="koboSpan" id="kobo.27.1">xis is some measurement of time—anything from the number of stars in the universe since the Big Bang until today or the amount of energy released each nanosecond from a nuclear reaction. </span><span class="koboSpan" id="kobo.27.2">The data behind both is time series. </span><span class="koboSpan" id="kobo.27.3">The chart in the weather app on your phone showing the expected temperature for the next 7 days? </span><span class="koboSpan" id="kobo.27.4">That’s also the plot of a </span><span class="No-Break"><span class="koboSpan" id="kobo.28.1">time series.</span></span></p>
<p><span class="koboSpan" id="kobo.29.1">In this book, we are mostly concerned with events on the human scales of years, months, days, and hours, but all of this is time series data. </span><span class="koboSpan" id="kobo.29.2">Predicting future values is the act </span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">of forecasting.</span></span></p>
<p><span class="koboSpan" id="kobo.31.1">Forecasting the weather ha</span><a id="_idTextAnchor041"/><span class="koboSpan" id="kobo.32.1">s</span><a id="_idIndexMarker002"/><span class="koboSpan" id="kobo.33.1"> obviously been important to humans for millennia, particularly since the advent of agriculture. </span><span class="koboSpan" id="kobo.33.2">In fact, over 2,300 years ago, the Greek philosopher Aristotle wrote a treatise called </span><em class="italic"><span class="koboSpan" id="kobo.34.1">Meteorology</span></em><span class="koboSpan" id="kobo.35.1"> that contained a discussion of early weather forecasting. </span><span class="koboSpan" id="kobo.35.2">The very word </span><em class="italic"><span class="koboSpan" id="kobo.36.1">forecast</span></em><span class="koboSpan" id="kobo.37.1"> was coined by an English meteorologist in the 1850s, Robert FitzRoy, who achieved fame as the captain of the </span><em class="italic"><span class="koboSpan" id="kobo.38.1">HMS Beagle</span></em><span class="koboSpan" id="kobo.39.1"> during Charles Darwin’s </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">pioneering voyage.</span></span></p>
<p><span class="koboSpan" id="kobo.41.1">However, time series data is not unique to weather. </span><span class="koboSpan" id="kobo.41.2">The field of medicine adopted time series analysis techniques with the 1901 invention of the first practical </span><strong class="bold"><span class="koboSpan" id="kobo.42.1">electrocardiogram</span></strong><span class="koboSpan" id="kobo.43.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.44.1">ECG</span></strong><span class="koboSpan" id="kobo.45.1">) by the Dutch physician Willem Einthoven. </span><span class="koboSpan" id="kobo.45.2">The ECG produces the familiar pattern of heartbeats</span><a id="_idIndexMarker003"/><span class="koboSpan" id="kobo.46.1"> we now see on the machine next to a patient’s bed in every </span><span class="No-Break"><span class="koboSpan" id="kobo.47.1">medical drama.</span></span></p>
<p><span class="koboSpan" id="kobo.48.1">Today, one of the most discussed fields of forecasting is economics. </span><span class="koboSpan" id="kobo.48.2">There are entire television channels dedicated to analyzing trends in the stock market. </span><span class="koboSpan" id="kobo.48.3">Governments use economic forecasting to advise central bank policy, politicians use economic forecasting to develop their platforms, and </span><a id="_idIndexMarker004"/><span class="koboSpan" id="kobo.49.1">business leaders use economic forecasting to guide </span><span class="No-Break"><span class="koboSpan" id="kobo.50.1">their decisions.</span></span></p>
<p><span class="koboSpan" id="kobo.51.1">In this book, we will be forecasting topics as varied as carbon dioxide levels in the atmosphere, the number of riders on Chicago’s public bike share program, the growth of the wolf population in Yellowstone, the solar sunspot cycles, local rainfall, and even Instagram likes on some </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">popular accou</span><a id="_idTextAnchor042"/><a id="_idTextAnchor043"/><span class="koboSpan" id="kobo.53.1">nts.</span></span></p>
<h2 id="_idParaDest-17"><a id="_idTextAnchor044"/><span class="koboSpan" id="kobo.54.1">The problem with dependent data</span></h2>
<p><span class="koboSpan" id="kobo.55.1">So, why does</span><a id="_idIndexMarker005"/><span class="koboSpan" id="kobo.56.1"> time series foreca</span><a id="_idTextAnchor045"/><span class="koboSpan" id="kobo.57.1">sting require its own unique approach? </span><span class="koboSpan" id="kobo.57.2">From a statistical perspective, you might see a scatter plot of time series with a relatively clear trend and attempt to fit a line using standard regression—the technique for fitting a straight line to data. </span><span class="koboSpan" id="kobo.57.3">The problem is that this violates the assumption of independence that linear </span><span class="No-Break"><span class="koboSpan" id="kobo.58.1">regression demands.</span></span></p>
<p><span class="koboSpan" id="kobo.59.1">To illustrate time series dependence with an example, let’s say that a gambler is rolling an unbiased die. </span><span class="koboSpan" id="kobo.59.2">I tell you that they just rolled a 2 and ask what the next value will be. </span><span class="koboSpan" id="kobo.59.3">This data is independent; previous rolls have no effect on future rolls, so knowing that the previous roll was a 2 does not provide any information about the </span><span class="No-Break"><span class="koboSpan" id="kobo.60.1">next roll.</span></span></p>
<p><span class="koboSpan" id="kobo.61.1">However, in a different situation, let’s say that I call you from an undisclosed location somewhere on Earth and ask you to guess the temperature at my location. </span><span class="koboSpan" id="kobo.61.2">Your best bet would be to guess some average global temperature for that day. </span><span class="koboSpan" id="kobo.61.3">Now, imagine that I tell you that yesterday’s temperature at my location was 90°F. </span><span class="koboSpan" id="kobo.61.4">That provides a great deal of information to you because you intuitively know that yesterday’s temperature and today’s temperature are linked in some way; they are </span><span class="No-Break"><span class="koboSpan" id="kobo.62.1">not independent.</span></span></p>
<p><span class="koboSpan" id="kobo.63.1">With time series data, you cannot randomly shuffle the order of data without disturbing the trends, within a reasonable margin of error. </span><span class="koboSpan" id="kobo.63.2">The order of the data matters; it is not independent. </span><span class="koboSpan" id="kobo.63.3">When data is dependent like this, a regression model can show statistical significance by random chance, even when there is no true correlation, much more often than your chosen confidence level </span><span class="No-Break"><span class="koboSpan" id="kobo.64.1">would suggest.</span></span></p>
<p><span class="koboSpan" id="kobo.65.1">Because high values tend to follow high values and low values tend to follow low values, a time series dataset is more likely to show more clusters of high or low values than would otherwise be present, and this, in turn, can lead to the appearance of more correlations than would otherwise </span><span class="No-Break"><span class="koboSpan" id="kobo.66.1">be present.</span></span></p>
<p><span class="koboSpan" id="kobo.67.1">The website </span><em class="italic"><span class="koboSpan" id="kobo.68.1">Spurious Correlations</span></em><span class="koboSpan" id="kobo.69.1"> by Tyler Vigen specializes in pointing out examples of seemingly significant, but utterly ridiculous, time series correlations. </span><span class="koboSpan" id="kobo.69.2">Here is </span><span class="No-Break"><span class="koboSpan" id="kobo.70.1">one exam</span><a id="_idTextAnchor046"/><span class="koboSpan" id="kobo.71.1">ple:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer007">
<span class="koboSpan" id="kobo.72.1"><img alt="Figure 1.1 – A spurious time series correlation (https://www.tylervigen.com/spurious-correlations)" src="image/Fig_1.1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.73.1">Figure 1.1 – A spurious time series correlation (https://www.tylervigen.com/spurious-correlations)</span></p>
<p><span class="koboSpan" id="kobo.74.1">Obviously, the n</span><a id="_idTextAnchor047"/><span class="koboSpan" id="kobo.75.1">umber of people who drown in pools each year is completely independent of the number of films Nicolas Cage appears in. </span><span class="koboSpan" id="kobo.75.2">They simply have no effect on each other at all. </span><span class="koboSpan" id="kobo.75.3">However, by making the fallacy of treating time series data as if it were independent, Vigen has shown that by pure random chance, the two series of data do, in fact, correlate significantly. </span><span class="koboSpan" id="kobo.75.4">These types of random chances are much more likely to happen when</span><a id="_idIndexMarker006"/><span class="koboSpan" id="kobo.76.1"> ignoring dependence in time </span><span class="No-Break"><span class="koboSpan" id="kobo.77.1">series data.</span></span></p>
<p><span class="koboSpan" id="kobo.78.1">Now that you understand what exactly time series data is and what sets it apart from other datasets, let’s look at a few milestones in the development of models, from the earliest models up </span><span class="No-Break"><span class="koboSpan" id="kobo.79.1">to Pro</span><a id="_idTextAnchor048"/><a id="_idTextAnchor049"/><span class="koboSpan" id="kobo.80.1">phet.</span></span></p>
<h1 id="_idParaDest-18"><a id="_idTextAnchor050"/><span class="koboSpan" id="kobo.81.1">Moving averages and exponential smoothing</span></h1>
<p><span class="koboSpan" id="kobo.82.1">Possibly the simplest form of foreca</span><a id="_idTextAnchor051"/><span class="koboSpan" id="kobo.83.1">sting is</span><a id="_idIndexMarker007"/><span class="koboSpan" id="kobo.84.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.85.1">moving average</span></strong><span class="koboSpan" id="kobo.86.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.87.1">MA</span></strong><span class="koboSpan" id="kobo.88.1">). </span><span class="koboSpan" id="kobo.88.2">Often, an MA </span><a id="_idTextAnchor052"/><span class="koboSpan" id="kobo.89.1">is used </span><a id="_idIndexMarker008"/><span class="koboSpan" id="kobo.90.1">as a </span><strong class="bold"><span class="koboSpan" id="kobo.91.1">smoothing technique</span></strong><span class="koboSpan" id="kobo.92.1"> to find a straighter line through data with a lot of variation. </span><span class="koboSpan" id="kobo.92.2">Each data point is adjusted to the value of the average of </span><em class="italic"><span class="koboSpan" id="kobo.93.1">n</span></em><span class="koboSpan" id="kobo.94.1"> surrounding data points, with </span><em class="italic"><span class="koboSpan" id="kobo.95.1">n</span></em><span class="koboSpan" id="kobo.96.1"> being referred to as the window size. </span><span class="koboSpan" id="kobo.96.2">With a window size of 10, for example, we would adjust a data point to be the average of the 5 values before and the 5 values after. </span><span class="koboSpan" id="kobo.96.3">In a forecasting setting, the future values are calculated as the average of the </span><em class="italic"><span class="koboSpan" id="kobo.97.1">n</span></em><span class="koboSpan" id="kobo.98.1"> previous values, so again, with a window size of 10, this means the average of the 10 </span><span class="No-Break"><span class="koboSpan" id="kobo.99.1">previous values.</span></span></p>
<p><span class="koboSpan" id="kobo.100.1">The balancing act with an MA is that you want a large window size in order to smooth out the noise and capture the actual trend, but with a larger window size, your forecasts are going to lag the trend significantly as you reach back further and further to calculate the average. </span><a id="_idTextAnchor053"/><span class="koboSpan" id="kobo.101.1">The idea </span><a id="_idIndexMarker009"/><span class="koboSpan" id="kobo.102.1">behind </span><strong class="bold"><span class="koboSpan" id="kobo.103.1">exponential smoothing</span></strong><span class="koboSpan" id="kobo.104.1"> is to apply exponentially decreasing weights to the values being averaged over time, giving recent values more weight and older values less weight. </span><span class="koboSpan" id="kobo.104.2">This allows the forecast to be more reactive to changes while still ignoring a good deal </span><span class="No-Break"><span class="koboSpan" id="kobo.105.1">of noise.</span></span></p>
<p><span class="koboSpan" id="kobo.106.1">As you can see in the following plot of simulated data, the MA line exhibits much rougher behavior than the exponential smoothing line, but both lines still adjust to trend changes at the </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">sam</span><a id="_idTextAnchor054"/><span class="koboSpan" id="kobo.108.1">e time:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer008">
<span class="koboSpan" id="kobo.109.1"><img alt="Figure 1.2 – MA versus exponential smoothing" src="image/Fig_1.2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.110.1">Figure 1.2 – MA versus exponential smoothing</span></p>
<p><span class="koboSpan" id="kobo.111.1">Exponential smoothing</span><a id="_idIndexMarker010"/><span class="koboSpan" id="kobo.112.1"> originat</span><a id="_idTextAnchor055"/><span class="koboSpan" id="kobo.113.1">ed in the 1950s with </span><strong class="bold"><span class="koboSpan" id="kobo.114.1">simple exponential smoothing</span></strong><span class="koboSpan" id="kobo.115.1">, which does not allow for trends or seasonality. </span><span class="koboSpan" id="kobo.115.2">Charles Holt advanced the technique in 1957 to allow for a trend with what he called </span><strong class="bold"><span class="koboSpan" id="kobo.116.1">double exponential smoothin</span><a id="_idTextAnchor056"/><span class="koboSpan" id="kobo.117.1">g</span></strong><span class="koboSpan" id="kobo.118.1">; and </span><a id="_idIndexMarker011"/><span class="koboSpan" id="kobo.119.1">in collaboration with Peter Winters, Holt added seasonality support in 1960, in what i</span><a id="_idTextAnchor057"/><span class="koboSpan" id="kobo.120.1">s commonly </span><a id="_idIndexMarker012"/><span class="koboSpan" id="kobo.121.1">called </span><strong class="bold"><span class="koboSpan" id="kobo.122.1">Holt-Winters </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.123.1">exponential smoothing</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.124.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.125.1">The downside to these methods of forecasting is that they can be slow to adjust to new trends and so forecasted values lag behind reality—they do not hold up well to longer forecasting timeframes, and there are many hyperparameters to tune, which can be a difficult and very </span><span class="No-Break"><span class="koboSpan" id="kobo.126.1">time-consumin</span><a id="_idTextAnchor058"/><a id="_idTextAnchor059"/><span class="koboSpan" id="kobo.127.1">g process.</span></span></p>
<h1 id="_idParaDest-19"><a id="_idTextAnchor060"/><span class="koboSpan" id="kobo.128.1">ARIMA</span></h1>
<p><span class="koboSpan" id="kobo.129.1">In 1970, the mathematicians George Box and Gwilym Jenkins published </span><em class="italic"><span class="koboSpan" id="kobo.130.1">Time Series: Forecasting and Control</span></em><span class="koboSpan" id="kobo.131.1">, which</span><a id="_idIndexMarker013"/><span class="koboSpan" id="kobo.132.1"> described what is now known as the </span><strong class="bold"><span class="koboSpan" id="kobo.133.1">Box-Jenkins </span><a id="_idTextAnchor061"/><span class="koboSpan" id="kobo.134.1">model</span></strong><span class="koboSpan" id="kobo.135.1">. </span><span class="koboSpan" id="kobo.135.2">This methodology took the idea of the MA further with the development of </span><strong class="bold"><span class="koboSpan" id="kobo.136.1">ARIMA</span></strong><span class="koboSpan" id="kobo.137.1">. </span><span class="koboSpan" id="kobo.137.2">As a</span><a id="_idTextAnchor062"/><span class="koboSpan" id="kobo.138.1"> term, ARIMA</span><a id="_idIndexMarker014"/><span class="koboSpan" id="kobo.139.1"> is often used interchangeably with Box-Jenkins, although technically, Box-Jenkins refers to a method of parameter optimization for an </span><span class="No-Break"><span class="koboSpan" id="kobo.140.1">ARIMA model.</span></span></p>
<p><span class="koboSpan" id="kobo.141.1">ARIMA is an acronym that refers to three concepts: </span><strong class="bold"><span class="koboSpan" id="kobo.142.1">Autoregressive</span></strong><span class="koboSpan" id="kobo.143.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.144.1">AR</span></strong><span class="koboSpan" id="kobo.145.1">), </span><strong class="bold"><span class="koboSpan" id="kobo.146.1">Integrated</span></strong><span class="koboSpan" id="kobo.147.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.148.1">I</span></strong><span class="koboSpan" id="kobo.149.1">), and </span><strong class="bold"><span class="koboSpan" id="kobo.150.1">MA</span></strong><span class="koboSpan" id="kobo.151.1">. </span><span class="koboSpan" id="kobo.151.2">We already understand the MA part. </span><span class="koboSpan" id="kobo.151.3">AR means that the model uses the dependent relationship between a data point and a certain number of lagged data points. </span><span class="koboSpan" id="kobo.151.4">That is, the model predicts upcoming values based on previous values. </span><span class="koboSpan" id="kobo.151.5">This is similar to predicting that it will be warm tomorrow because it’s been warm all week </span><span class="No-Break"><span class="koboSpan" id="kobo.152.1">so far.</span></span></p>
<p><span class="koboSpan" id="kobo.153.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.154.1">integrated</span></strong><span class="koboSpan" id="kobo.155.1"> part means that instead of using any raw data point, the difference between that data point and a previous data point is used. </span><span class="koboSpan" id="kobo.155.2">Essentially, this means that we convert a series of values into a series of changes in values. </span><span class="koboSpan" id="kobo.155.3">Intuitively, this suggests that tomorrow will be more or less the same temperature as today because the temperature all week hasn’t varied </span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">too much.</span></span></p>
<p><span class="koboSpan" id="kobo.157.1">Each of the AR, I, and MA components of an ARIMA model are explicitly specified as a parameter in the model. </span><span class="koboSpan" id="kobo.157.2">Traditionally, </span><em class="italic"><span class="koboSpan" id="kobo.158.1">p</span></em><span class="koboSpan" id="kobo.159.1"> is used for the number of lag observations to use, a</span><a id="_idTextAnchor063"/><span class="koboSpan" id="kobo.160.1">lso known as</span><a id="_idIndexMarker015"/><span class="koboSpan" id="kobo.161.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.162.1">lag order</span></strong><span class="koboSpan" id="kobo.163.1">. </span><span class="koboSpan" id="kobo.163.2">The number of times that a raw observation is differenced, or the degree of differencing, is known as </span><em class="italic"><span class="koboSpan" id="kobo.164.1">d</span></em><span class="koboSpan" id="kobo.165.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.166.1">q</span></em><span class="koboSpan" id="kobo.167.1"> represents the size of the MA window. </span><span class="koboSpan" id="kobo.167.2">Thus arises the standard notation for an ARIMA model of </span><em class="italic"><span class="koboSpan" id="kobo.168.1">ARIMA(p, d, q)</span></em><span class="koboSpan" id="kobo.169.1">, where </span><em class="italic"><span class="koboSpan" id="kobo.170.1">p</span></em><span class="koboSpan" id="kobo.171.1">, </span><em class="italic"><span class="koboSpan" id="kobo.172.1">d</span></em><span class="koboSpan" id="kobo.173.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.174.1">q</span></em><span class="koboSpan" id="kobo.175.1"> are all </span><span class="No-Break"><span class="koboSpan" id="kobo.176.1">non-negative integers.</span></span></p>
<p><span class="koboSpan" id="kobo.177.1">A problem with ARIMA models is that they do not support seasonality, or data with repeating cycles, such as temperature rising in the day and falling at night or rising in summer and falling in winter. </span><strong class="bold"><span class="koboSpan" id="kobo.178.1">Seasonal ARIMA</span><a id="_idTextAnchor064"/></strong><span class="koboSpan" id="kobo.179.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.180.1">SARIMA</span></strong><span class="koboSpan" id="kobo.181.1">) was</span><a id="_idIndexMarker016"/><span class="koboSpan" id="kobo.182.1"> developed to overcome this drawback. </span><span class="koboSpan" id="kobo.182.2">Similar to the ARIMA notation, the notation for a SARIMA model is </span><em class="italic"><span class="koboSpan" id="kobo.183.1">SARIMA(p, d, q)(P, D, Q)m</span></em><span class="koboSpan" id="kobo.184.1">, with </span><em class="italic"><span class="koboSpan" id="kobo.185.1">P</span></em><span class="koboSpan" id="kobo.186.1"> being the seasonal AR order, </span><em class="italic"><span class="koboSpan" id="kobo.187.1">D</span></em><span class="koboSpan" id="kobo.188.1"> the seasonal difference order, </span><em class="italic"><span class="koboSpan" id="kobo.189.1">Q</span></em><span class="koboSpan" id="kobo.190.1"> the seasonal MA order, and </span><em class="italic"><span class="koboSpan" id="kobo.191.1">m</span></em><span class="koboSpan" id="kobo.192.1"> the number of time steps for a single </span><span class="No-Break"><span class="koboSpan" id="kobo.193.1">seasonal period.</span></span></p>
<p><span class="koboSpan" id="kobo.194.1">You may also</span><a id="_idIndexMarker017"/><span class="koboSpan" id="kobo.195.1"> come across</span><a id="_idIndexMarker018"/><span class="koboSpan" id="kobo.196.1"> other variations of ARIMA models, including </span><strong class="bold"><span class="koboSpan" id="kobo.197.1">Vector A</span><a id="_idTextAnchor065"/><span class="koboSpan" id="kobo.198.1">RIMA</span></strong><span class="koboSpan" id="kobo.199.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.200.1">VARIMA</span></strong><span class="koboSpan" id="kobo.201.1">) for cases with multiple</span><a id="_idIndexMarker019"/><span class="koboSpan" id="kobo.202.1"> time series as vectors; </span><strong class="bold"><span class="koboSpan" id="kobo.203.1">Fractional ARIMA</span></strong><span lang="en-US" xml:lang="en-US"> </span><span class="koboSpan" id="kobo.204.1">(</span><strong class="bold"><span class="koboSpan" id="kobo.205.1">FARIMA</span></strong><span class="koboSpan" id="kobo.206.1">) or </span><strong class="bold"><span class="koboSpan" id="kobo.207.1">Autoregressive Fractionally Integrated </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.208.1">Moving Average</span></strong></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.209.1">PD: Style as P-Keyword</span></strong><span class="koboSpan" id="kobo.210.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.211.1">ARFIM</span><a id="_idTextAnchor066"/><span class="koboSpan" id="kobo.212.1">A</span></strong><span class="koboSpan" id="kobo.213.1">), both of which include a fractional d</span><a id="_idTextAnchor067"/><span class="koboSpan" id="kobo.214.1">ifferencing degree, allowing for long memory in the sense </span><a id="_idIndexMarker020"/><span class="koboSpan" id="kobo.215.1">that observations far apart in time can have non-negligible dependencies; and </span><strong class="bold"><span class="koboSpan" id="kobo.216.1">SARIMAX</span></strong><span class="koboSpan" id="kobo.217.1">, a </span><strong class="bold"><span class="koboSpan" id="kobo.218.1">seasonal ARIMA</span></strong><span class="koboSpan" id="kobo.219.1"> model where the </span><em class="italic"><span class="koboSpan" id="kobo.220.1">X</span></em><span class="koboSpan" id="kobo.221.1"> stands for exogenous or additional variables added to the model, such as adding a rain forecast to a </span><span class="No-Break"><span class="koboSpan" id="kobo.222.1">temperature model.</span></span></p>
<p><span class="koboSpan" id="kobo.223.1">ARIMA</span><a id="_idIndexMarker021"/><span class="koboSpan" id="kobo.224.1"> does typically exhibit very good results, but the downside is its complexity. </span><span class="koboSpan" id="kobo.224.2">Tuning and optimizing ARIMA models is often computationally expensive and successful results can depend upon the skill and experience of the forecaster. </span><span class="koboSpan" id="kobo.224.3">It is not a scalable process, but better suited to ad hoc analyses by </span><span class="No-Break"><span class="koboSpan" id="kobo.225.1">ski</span><a id="_idTextAnchor068"/><a id="_idTextAnchor069"/><span class="koboSpan" id="kobo.226.1">lled practitioners.</span></span></p>
<h1 id="_idParaDest-20"><a id="_idTextAnchor070"/><span class="koboSpan" id="kobo.227.1">ARCH/GARCH</span></h1>
<p><span class="koboSpan" id="kobo.228.1">When the variance of a dataset is not constant over time, ARIMA models face problems with modeling it. </span><span class="koboSpan" id="kobo.228.2">In economics and finance, in particular, this is common. </span><span class="koboSpan" id="kobo.228.3">In a financial time series, large returns tend to be followed by large returns and small returns tend to be followed by</span><a id="_idTextAnchor071"/><span class="koboSpan" id="kobo.229.1"> small returns. </span><span class="koboSpan" id="kobo.229.2">The former is </span><a id="_idIndexMarker022"/><span class="koboSpan" id="kobo.230.1">called </span><strong class="bold"><span class="koboSpan" id="kobo.231.1">high</span><a id="_idTextAnchor072"/><span class="koboSpan" id="kobo.232.1"> volatility</span></strong><span class="koboSpan" id="kobo.233.1">, and the latter </span><a id="_idIndexMarker023"/><span class="koboSpan" id="kobo.234.1">is </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.235.1">low volatility</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.236.1">.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.237.1">Autoregressive Conditional Heteroscedastic</span><a id="_idTextAnchor073"/><span class="koboSpan" id="kobo.238.1">ity</span></strong><span class="koboSpan" id="kobo.239.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.240.1">ARCH</span></strong><span class="koboSpan" id="kobo.241.1">) models</span><a id="_idIndexMarker024"/><span class="koboSpan" id="kobo.242.1"> were developed to solve this problem. </span><a id="_idTextAnchor074"/><strong class="bold"><span class="koboSpan" id="kobo.243.1">Heteroscedasticity</span></strong><span class="koboSpan" id="kobo.244.1"> is a </span><a id="_idIndexMarker025"/><span class="koboSpan" id="kobo.245.1">fancy way of saying that the variance or spread of the data is not constant throughout</span><a id="_idTextAnchor075"/><span class="koboSpan" id="kobo.246.1">, with the opposite term</span><a id="_idIndexMarker026"/><span class="koboSpan" id="kobo.247.1"> being </span><strong class="bold"><span class="koboSpan" id="kobo.248.1">homoscedasticity</span></strong><span class="koboSpan" id="kobo.249.1">. </span><span class="koboSpan" id="kobo.249.2">The differe</span><a id="_idTextAnchor076"/><span class="koboSpan" id="kobo.250.1">nce is </span><span class="No-Break"><span class="koboSpan" id="kobo.251.1">visualized here:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer009">
<span class="koboSpan" id="kobo.252.1"><img alt="Figure 1.3 – Scedasticity" src="image/Fig_1.3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.253.1">Figure 1.3 – Scedasticity</span></p>
<p><span class="koboSpan" id="kobo.254.1">Robert Engle introduced the first ARCH model in 1982 by describing </span><strong class="bold"><span class="koboSpan" id="kobo.255.1">c</span><a id="_idTextAnchor077"/><span class="koboSpan" id="kobo.256.1">onditional variance</span></strong><span class="koboSpan" id="kobo.257.1"> as a</span><a id="_idIndexMarker027"/><span class="koboSpan" id="kobo.258.1"> function of previous values. </span><span class="koboSpan" id="kobo.258.2">For example, there is a lot more uncertainty about daytime electricity usage than there is about nighttime usage. </span><span class="koboSpan" id="kobo.258.3">In a model of electricity usage, then, we might assume that the daytime hours have a particular variance, and usage during the night would have a </span><span class="No-Break"><span class="koboSpan" id="kobo.259.1">lower vari</span><a id="_idTextAnchor078"/><span class="koboSpan" id="kobo.260.1">ance.</span></span></p>
<p><span class="koboSpan" id="kobo.261.1">Tim Bollerslev and Stephen Taylor </span><a id="_idIndexMarker028"/><span class="koboSpan" id="kobo.262.1">introduced a moving average component to the model in 1986 with their </span><strong class="bold"><span class="koboSpan" id="kobo.263.1">Generalized ARCH</span></strong><span class="koboSpan" id="kobo.264.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.265.1">GARCH</span></strong><span class="koboSpan" id="kobo.266.1">) model. </span><span class="koboSpan" id="kobo.266.2">In the electricity example, the variance in usage was a function of the time of day, but perhaps the swings in volatility don’t necessarily occur at specific times of the day, and the swings themselves are random. </span><span class="koboSpan" id="kobo.266.3">This is when GARCH </span><span class="No-Break"><span class="koboSpan" id="kobo.267.1">is useful.</span></span></p>
<p><span class="koboSpan" id="kobo.268.1">Both ARCH and </span><a id="_idIndexMarker029"/><span class="koboSpan" id="kobo.269.1">GARCH models can handle neither trend nor seasonality though, so often, in practice, an ARIMA model may be built first to extract out the seasonal variation and trend of a time series, and then an ARCH model may be used to mo</span><a id="_idTextAnchor079"/><a id="_idTextAnchor080"/><span class="koboSpan" id="kobo.270.1">del the </span><span class="No-Break"><span class="koboSpan" id="kobo.271.1">expected </span><a id="_idTextAnchor081"/><span class="koboSpan" id="kobo.272.1">variance.</span></span></p>
<h1 id="_idParaDest-21"><a id="_idTextAnchor082"/><span class="koboSpan" id="kobo.273.1">Neural networks</span></h1>
<p><span class="koboSpan" id="kobo.274.1">A relatively recent development in time</span><a id="_idIndexMarker030"/><span class="koboSpan" id="kobo.275.1"> series forecasting is the use of </span><strong class="bold"><span class="koboSpan" id="kobo.276.1">Recurrent Neura</span><a id="_idTextAnchor083"/><span class="koboSpan" id="kobo.277.1">l Networks</span></strong><span class="koboSpan" id="kobo.278.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.279.1">RNNs</span></strong><span class="koboSpan" id="kobo.280.1">). </span><span class="koboSpan" id="kobo.280.2">This was made possible with the development of </span><a id="_idIndexMarker031"/><span class="koboSpan" id="kobo.281.1">the </span><strong class="bold"><span class="koboSpan" id="kobo.282.1">Long Short-Ter</span><a id="_idTextAnchor084"/><span class="koboSpan" id="kobo.283.1">m Memory</span></strong><span class="koboSpan" id="kobo.284.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.285.1">LSTM</span></strong><span class="koboSpan" id="kobo.286.1">) unit by Sepp Hochreiter and Jürgen Schmidhuber in 1997. </span><span class="koboSpan" id="kobo.286.2">Essentially, an LSTM unit allows a neural network to process a sequence of data, such as speech or video, instead of a single data point, such as </span><span class="No-Break"><span class="koboSpan" id="kobo.287.1">an image.</span></span></p>
<p><span class="koboSpan" id="kobo.288.1">A standard RNN is called </span><em class="italic"><span class="koboSpan" id="kobo.289.1">recurrent</span></em><span class="koboSpan" id="kobo.290.1"> because it has loops built into it, which is what gives it memory, that is, gives it access to previous information. </span><span class="koboSpan" id="kobo.290.2">A basic neural network</span><a id="_idIndexMarker032"/><span class="koboSpan" id="kobo.291.1"> can be trained to recognize an image of a pedestrian on a street by learning what a pedestrian looks like from previous images, but it cannot be trained to identify that a pedestrian in a video will soon be crossing the street based upon the pedestrian’s approach observed in previous frames of the video. </span><span class="koboSpan" id="kobo.291.2">It has no knowledge of the sequence of images that leads to the pedestrian stepping out into the road. </span><span class="koboSpan" id="kobo.291.3">Short-term memory is what the network needs temporarily to provide context, but that memory </span><span class="No-Break"><span class="koboSpan" id="kobo.292.1">degrades quickly.</span></span></p>
<p><span class="koboSpan" id="kobo.293.1">Early RNNs had a memory problem: it just wasn’t very long. </span><span class="koboSpan" id="kobo.293.2">In the sentence “</span><em class="italic"><span class="koboSpan" id="kobo.294.1">airplanes fly in the …</span></em><span class="koboSpan" id="kobo.295.1">,” a simple RNN may be able to guess the next word will be </span><em class="italic"><span class="koboSpan" id="kobo.296.1">sky</span></em><span class="koboSpan" id="kobo.297.1">, but with “</span><em class="italic"><span class="koboSpan" id="kobo.298.1">I went to France for vacation last summer. </span><span class="koboSpan" id="kobo.298.2">That’s why I spent my spring learning to speak …</span></em><span class="koboSpan" id="kobo.299.1">,” it’s not so easy for the RNN to guess that </span><em class="italic"><span class="koboSpan" id="kobo.300.1">French</span></em><span class="koboSpan" id="kobo.301.1"> comes next; it understands that the word for a language should come next but has forgotten that the phrase started by mentioning France. </span><span class="koboSpan" id="kobo.301.2">An LSTM, though, has this necessary context. </span><span class="koboSpan" id="kobo.301.3">It gives the network’s short-term memory more longevity. </span><span class="koboSpan" id="kobo.301.4">In the case of time series data, where patterns can reoccur over long </span><a id="_idIndexMarker033"/><span class="koboSpan" id="kobo.302.1">time scales, LSTMs can perform </span><span class="No-Break"><span class="koboSpan" id="kobo.303.1">very well.</span></span></p>
<p><span class="koboSpan" id="kobo.304.1">Time series forecasting with </span><a id="_idIndexMarker034"/><span class="koboSpan" id="kobo.305.1">LSTMs is still in its infancy when compared to the other forecasting methods discussed here; however, it shows promise. </span><span class="koboSpan" id="kobo.305.2">One strong advantage over other forecasting techniques is the ability of neural networks to capture non-linear relationships, but as with any deep learning problem, LSTM forecasting requires a great deal of data and computing power and a long </span><span class="No-Break"><span class="koboSpan" id="kobo.306.1">processing time.</span></span></p>
<p><span class="koboSpan" id="kobo.307.1">Additionally, there are many decisions to be made regarding the architecture of the model and the hyperparameters to be used, which necessitate a very experienced forecaster. </span><span class="koboSpan" id="kobo.307.2">In most practical problems, where budget and deadlines must be considered, an ARIMA mo</span><a id="_idTextAnchor085"/><a id="_idTextAnchor086"/><span class="koboSpan" id="kobo.308.1">del is often the</span><a id="_idTextAnchor087"/> <span class="No-Break"><span class="koboSpan" id="kobo.309.1">better choice.</span></span></p>
<h1 id="_idParaDest-22"><a id="_idTextAnchor088"/><span class="koboSpan" id="kobo.310.1">Prophet</span></h1>
<p><span class="koboSpan" id="kobo.311.1">Prophet</span><a id="_idIndexMarker035"/><span class="koboSpan" id="kobo.312.1"> was developed internally at Facebook (now known as </span><strong class="bold"><span class="koboSpan" id="kobo.313.1">Meta</span></strong><span class="koboSpan" id="kobo.314.1">) by Sean J. </span><span class="koboSpan" id="kobo.314.2">Taylor and Ben Letham in order to overcome two issues often encountered with other forecasting methodologies: the more automatic forecasting tools available tended to be too inflexible and unable to accommodate additional assumptions, and the more robust forecasting tools required an experienced analyst with specialized data science skills. </span><span class="koboSpan" id="kobo.314.3">Facebook experienced too much demand for high-quality business forecasts than their analysts were able to provide. </span><span class="koboSpan" id="kobo.314.4">In 2017, Facebook released Prophet to the public as open </span><span class="No-Break"><span class="koboSpan" id="kobo.315.1">source software.</span></span></p>
<p><span class="koboSpan" id="kobo.316.1">Prophet was designed to optimally handle business forecasting tasks, which typically feature any of </span><span class="No-Break"><span class="koboSpan" id="kobo.317.1">these attributes:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.318.1">Time series data captured at the hourly, daily, or weekly level with ideally at least a full year of </span><span class="No-Break"><span class="koboSpan" id="kobo.319.1">historical data</span></span></li>
<li><span class="koboSpan" id="kobo.320.1">Strong seasonality effects occurring daily, weekly, </span><span class="No-Break"><span class="koboSpan" id="kobo.321.1">and/or yearly</span></span></li>
<li><span class="koboSpan" id="kobo.322.1">Holidays and other special one-time events that don’t necessarily follow the seasonality patterns but </span><span class="No-Break"><span class="koboSpan" id="kobo.323.1">occur irregularly</span></span></li>
<li><span class="koboSpan" id="kobo.324.1">Missing data </span><span class="No-Break"><span class="koboSpan" id="kobo.325.1">and outliers</span></span></li>
<li><span class="koboSpan" id="kobo.326.1">Significant trend changes that may occur with the launch of new features or products, </span><span class="No-Break"><span class="koboSpan" id="kobo.327.1">for example</span></span></li>
<li><span class="koboSpan" id="kobo.328.1">Trends that asymptotically approach an upper or </span><span class="No-Break"><span class="koboSpan" id="kobo.329.1">lower bound</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.330.1">Out of the box, Prophet </span><a id="_idIndexMarker036"/><span class="koboSpan" id="kobo.331.1">typically produces very high-quality forecasts, but it is also very customizable and approachable for data analysts with no prior expertise in time series data. </span><span class="koboSpan" id="kobo.331.2">As you’ll see in later chapters, tuning a Prophet model is </span><span class="No-Break"><span class="koboSpan" id="kobo.332.1">very </span><a id="_idTextAnchor089"/><span class="koboSpan" id="kobo.333.1">intuitive.</span></span></p>
<p><span class="koboSpan" id="kobo.334.1">Essentially, Prophet is</span><a id="_idIndexMarker037"/><span class="koboSpan" id="kobo.335.1"> an </span><strong class="bold"><span class="koboSpan" id="kobo.336.1">additive reg</span><a id="_idTextAnchor090"/><span class="koboSpan" id="kobo.337.1">ression model</span></strong><span class="koboSpan" id="kobo.338.1">. </span><span class="koboSpan" id="kobo.338.2">This means that the model is simply the sum of several (optional) components, such as </span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.340.1">A linear or logistic growth </span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">trend curve</span></span></li>
<li><span class="koboSpan" id="kobo.342.1">An annual </span><span class="No-Break"><span class="koboSpan" id="kobo.343.1">seasonality curve</span></span></li>
<li><span class="koboSpan" id="kobo.344.1">A weekly </span><span class="No-Break"><span class="koboSpan" id="kobo.345.1">seasonality curve</span></span></li>
<li><span class="koboSpan" id="kobo.346.1">A daily </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">seasonality curve</span></span></li>
<li><span class="koboSpan" id="kobo.348.1">Holidays and other </span><span class="No-Break"><span class="koboSpan" id="kobo.349.1">special events</span></span></li>
<li><span class="koboSpan" id="kobo.350.1">Additional user-specified seasonality curves, such as hourly or quarterly, </span><span class="No-Break"><span class="koboSpan" id="kobo.351.1">for example</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.352.1">To take a concrete example, let’s say we are modeling the sales of a small online retail store over 4 years, from January 1, 2000, to the end of 2003. </span><span class="koboSpan" id="kobo.352.2">We observe that the overall trend is </span><a id="_idIndexMarker038"/><span class="koboSpan" id="kobo.353.1">constantly increasing over time from </span><strong class="bold"><span class="koboSpan" id="kobo.354.1">1,000</span></strong><span class="koboSpan" id="kobo.355.1"> sales per day to around </span><strong class="bold"><span class="koboSpan" id="kobo.356.1">1,800</span></strong><span class="koboSpan" id="kobo.357.1"> at the end of the time period. </span><span class="koboSpan" id="kobo.357.2">We also see that sales in spring are about </span><strong class="bold"><span class="koboSpan" id="kobo.358.1">50</span></strong><span class="koboSpan" id="kobo.359.1"> units above average and sales in autumn are about </span><strong class="bold"><span class="koboSpan" id="kobo.360.1">50</span></strong><span class="koboSpan" id="kobo.361.1"> units below average. </span><span class="koboSpan" id="kobo.361.2">Weekly, sales tend to be lowest on </span><strong class="bold"><span class="koboSpan" id="kobo.362.1">Tuesday</span></strong><span class="koboSpan" id="kobo.363.1"> and increase throughout the week, peaking on </span><strong class="bold"><span class="koboSpan" id="kobo.364.1">Saturday</span></strong><span class="koboSpan" id="kobo.365.1">. </span><span class="koboSpan" id="kobo.365.2">Finally, throughout the hours of the day, sales peak at noon and smoothly fall to their lowest at midnight. </span><span class="koboSpan" id="kobo.365.3">This is what those individual curves would look </span><a id="_idIndexMarker039"/><span class="koboSpan" id="kobo.366.1">like (note the diff</span><a id="_idTextAnchor091"/><span class="koboSpan" id="kobo.367.1">erent </span><em class="italic"><span class="koboSpan" id="kobo.368.1">x</span></em><span class="koboSpan" id="kobo.369.1">-axis scales on </span><span class="No-Break"><span class="koboSpan" id="kobo.370.1">each chart):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer010">
<span class="koboSpan" id="kobo.371.1"><img alt="Figure 1.4 ﻿– Model components" src="image/Fig_1.4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.372.1">Figure 1.4 </span><a id="_idTextAnchor092"/><span class="koboSpan" id="kobo.373.1">– Model components</span></p>
<p><span class="koboSpan" id="kobo.374.1">An additive model</span><a id="_idIndexMarker040"/><span class="koboSpan" id="kobo.375.1"> would take those four curves and simply add them to each other to arrive at the final model for sales throughout the years. </span><span class="koboSpan" id="kobo.375.2">The final curve gets more and more complex </span><a id="_idTextAnchor093"/><span class="koboSpan" id="kobo.376.1">as the sub-components are </span><span class="No-Break"><span class="koboSpan" id="kobo.377.1">added up:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer011">
<span class="koboSpan" id="kobo.378.1"><img alt="Figure 1.5 – Additive model" src="image/Fig_1.5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.379.1">Figure 1.5 – Additive model</span></p>
<p><span class="koboSpan" id="kobo.380.1">This preceding plot displays just the first year to see the weekly and daily variations better, but the full curve extends for </span><span class="No-Break"><span class="koboSpan" id="kobo.381.1">4 years.</span></span></p>
<p><span class="koboSpan" id="kobo.382.1">Under the </span><a id="_idTextAnchor094"/><span class="koboSpan" id="kobo.383.1">hood, Prophet is</span><a id="_idIndexMarker041"/><span class="koboSpan" id="kobo.384.1"> written </span><a id="_idIndexMarker042"/><span class="koboSpan" id="kobo.385.1">in </span><strong class="bold"><span class="koboSpan" id="kobo.386.1">Stan</span></strong><span class="koboSpan" id="kobo.387.1">, a probabilistic programming </span><a id="_idIndexMarker043"/><span class="koboSpan" id="kobo.388.1">language (see the home page at </span><a href="https://mc-stan.org/"><span class="koboSpan" id="kobo.389.1">https://mc-stan.org/</span></a><span class="koboSpan" id="kobo.390.1"> for more information about Stan). </span><span class="koboSpan" id="kobo.390.2">This has several advantages. </span><span class="koboSpan" id="kobo.390.3">It allows Prophet to optimize the fit process so that it typically completes in under a second. </span><span class="koboSpan" id="kobo.390.4">Stan is also compatible with both Python and R, so the Prophet team is able to share the same core fitting procedure between both language implementations. </span><span class="koboSpan" id="kobo.390.5">Also, by using Bayesian statistics, Stan allows Prophet to create uncertainty intervals for future predictions to add a data-driven estimate of </span><span class="No-Break"><span class="koboSpan" id="kobo.391.1">forecasting risk.</span></span></p>
<p><span class="koboSpan" id="kobo.392.1">Prophet</span><a id="_idIndexMarker044"/><span class="koboSpan" id="kobo.393.1"> manages to achieve typical results just as well as more complicated forecasting techniques but with just a fraction of the effort. </span><span class="koboSpan" id="kobo.393.2">It has something for everyone. </span><span class="koboSpan" id="kobo.393.3">A beginner can build a highly accurate model in just a few lines of code without necessarily understanding the details of how everything works, while an expert can dig deep into the model, adding more features and tweaking hyperparameters to eke out incrementally </span><span class="No-Break"><span class="koboSpan" id="kobo.394.1">better performance.</span></span></p>
<h1 id="_idParaDest-23"><a id="_idTextAnchor095"/><span class="koboSpan" id="kobo.395.1">Recent developments</span></h1>
<p><span class="koboSpan" id="kobo.396.1">The public release of Prophet has inspired a lot of open source activity around forecasting packages. </span><span class="koboSpan" id="kobo.396.2">Although Prophet remains the most widely used tool, there are several competing packages to keep an </span><span class="No-Break"><span class="koboSpan" id="kobo.397.1">eye on.</span></span></p>
<h2 id="_idParaDest-24"><a id="_idTextAnchor096"/><span class="koboSpan" id="kobo.398.1">NeuralProphet</span></h2>
<p><span class="koboSpan" id="kobo.399.1">Prophet has become so popular due to its ease of learning, quick predictions from data, and customizability. </span><span class="koboSpan" id="kobo.399.2">However, it does have some shortcomings; the key one among these is that it is a linear model. </span><span class="koboSpan" id="kobo.399.3">As discussed earlier in this chapter, neural networks are often used when forecasting tasks require a non-linear model, although an analyst must be very knowledgeable about both time series </span><a id="_idIndexMarker045"/><span class="koboSpan" id="kobo.400.1">and applied machine learning to apply these</span><a id="_idIndexMarker046"/><span class="koboSpan" id="kobo.401.1"> models effectively. </span><strong class="bold"><span class="koboSpan" id="kobo.402.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.403.1"> (</span><a href="https://github.com/ourownstory/neural_prophet"><span class="koboSpan" id="kobo.404.1">https://github.com/ourownstory/neural_prophet</span></a><span class="koboSpan" id="kobo.405.1">) aims to bridge this gap and allows an analyst with expertise only in time series to build a very strong </span><span class="No-Break"><span class="koboSpan" id="kobo.406.1">neural model.</span></span></p>
<p><span class="koboSpan" id="kobo.407.1">Oskar Triebe at Stanford University has built and optimized NeuralProphet for several years with the help of the open source community, but at the time of writing, NeuralProphet is still in the beta phase. </span><span class="koboSpan" id="kobo.407.2">It switches out Prophet’s dependency on the Stan language with PyTorch, thus enabling deep learning methods. </span><span class="koboSpan" id="kobo.407.3">NeuralProphet models time series autocorrelation with </span><a id="_idIndexMarker047"/><span class="koboSpan" id="kobo.408.1">an </span><strong class="bold"><span class="koboSpan" id="kobo.409.1">Autoregressive Network</span></strong><span class="koboSpan" id="kobo.410.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.411.1">AR-Net</span></strong><span class="koboSpan" id="kobo.412.1">) and models </span><a id="_idIndexMarker048"/><span class="koboSpan" id="kobo.413.1">lagged regressors with a </span><strong class="bold"><span class="koboSpan" id="kobo.414.1">Feed-Forward Neural Network</span></strong><span class="koboSpan" id="kobo.415.1">. </span><span class="koboSpan" id="kobo.415.2">The programming interface has been designed to be nearly identical to Prophet’s, so learning how to build models in NeuralProphet will be quite familiar to anyone already familiar </span><span class="No-Break"><span class="koboSpan" id="kobo.416.1">with Prophet.</span></span></p>
<h2 id="_idParaDest-25"><a id="_idTextAnchor097"/><span class="koboSpan" id="kobo.417.1">Google’s “robust time series forecasting at scale”</span></h2>
<p><span class="koboSpan" id="kobo.418.1">Not to be </span><a id="_idIndexMarker049"/><span class="koboSpan" id="kobo.419.1">outdone, in April 2017, just 2 months after Facebook announced Prophet was being made open source, Google described their solut</span><a id="_idTextAnchor098"/><span class="koboSpan" id="kobo.420.1">ion to the forecasting problem in their blog post </span><em class="italic"><span class="koboSpan" id="kobo.421.1">Our quest for a robust time series forecasting at scale </span></em><span class="koboSpan" id="kobo.422.1">(</span><a href="https://www.unofficialgoogledatascience.com/2017/04/our-quest-for-robust-time-series.html"><span class="koboSpan" id="kobo.423.1">https://www.unofficialgoogledatascience.com/2017/04/our-quest-for-robust-time-series.html</span></a><span class="koboSpan" id="kobo.424.1">). </span><span class="koboSpan" id="kobo.424.2">Unlike Prophet, Google’s package is not open source, so few details are publicly available. </span><span class="koboSpan" id="kobo.424.3">A key difference between Prophet’s and Google’s approaches is that Google’s forecasting package uses an ensemble method to forecast growth trends. </span><span class="koboSpan" id="kobo.424.4">In the time series context, this means that Google fits multiple forecast models, removes any outliers, and takes the weighted average of each individual model to arrive at a final model. </span><span class="koboSpan" id="kobo.424.5">At the time of writing, Google has not announced any plans to release its forecasting package to the open </span><span class="No-Break"><span class="koboSpan" id="kobo.425.1">source community.</span></span></p>
<h2 id="_idParaDest-26"><a id="_idTextAnchor099"/><span class="koboSpan" id="kobo.426.1">LinkedIn’s Silverkite/Greykite</span></h2>
<p><span class="koboSpan" id="kobo.427.1">Compared to Facebook and Google, LinkedIn is a relative newcomer to the open source forecasting community. </span><span class="koboSpan" id="kobo.427.2">In May 2021, LinkedIn announced</span><a id="_idIndexMarker050"/><span class="koboSpan" id="kobo.428.1"> their </span><strong class="bold"><span class="koboSpan" id="kobo.429.1">Greykite</span></strong><span class="koboSpan" id="kobo.430.1"> forecasting library for Python (</span><a href="https://github.com/linkedin/greykite"><span class="koboSpan" id="kobo.431.1">https://github.com/linkedin/greykite</span></a><span class="koboSpan" id="kobo.432.1">), which uses their </span><a id="_idIndexMarker051"/><span class="koboSpan" id="kobo.433.1">own </span><strong class="bold"><span class="koboSpan" id="kobo.434.1">Silverkite</span></strong><span class="koboSpan" id="kobo.435.1"> algorithm (the Prophet algorithms are also options within Greykite’s modeling framework). </span><span class="koboSpan" id="kobo.435.2">Greykite was developed to provide some key benefits to forecasting at LinkedIn: the solution must flexible, intuitive, and fast. </span><span class="koboSpan" id="kobo.435.3">If that sounds familiar, it’s because those are the very same qualities Facebook targeted when </span><span class="No-Break"><span class="koboSpan" id="kobo.436.1">developing Prophet.</span></span></p>
<p><span class="koboSpan" id="kobo.437.1">Whereas Prophet uses a Bayesian approach to fit a model, Silverkite uses more traditional models such as a ridge, elastic net, and boosted trees. </span><span class="koboSpan" id="kobo.437.2">Both Prophet and Silverkite can model linear growth, but only Silverkite</span><a id="_idIndexMarker052"/><span class="koboSpan" id="kobo.438.1"> can handle square root and quadratic growth. </span><span class="koboSpan" id="kobo.438.2">Prophet, however, can model logistic growth, something that Silverkite cannot do. </span><span class="koboSpan" id="kobo.438.3">Possibly the most exciting aspect of Silverkite from an analyst’s point of view is that domain expertise can easily be added to a model via external variables. </span><span class="koboSpan" id="kobo.438.4">Silverkite uses </span><strong class="source-inline"><span class="koboSpan" id="kobo.439.1">sklearn</span></strong><span class="koboSpan" id="kobo.440.1"> for its API, so any user familiar with that library should have no trouble ramping up </span><span class="No-Break"><span class="koboSpan" id="kobo.441.1">with Silverkite.</span></span></p>
<h2 id="_idParaDest-27"><a id="_idTextAnchor100"/><span class="koboSpan" id="kobo.442.1">Uber’s Orbit</span></h2>
<p><span class="koboSpan" id="kobo.443.1">At the same time that LinkedIn </span><a id="_idIndexMarker053"/><span class="koboSpan" id="kobo.444.1">announced the Greykite library, Uber announced their own forecasting </span><a id="_idIndexMarker054"/><span class="koboSpan" id="kobo.445.1">package, </span><strong class="bold"><span class="koboSpan" id="kobo.446.1">Object-Oriented Bayesian Time Series</span></strong><span class="koboSpan" id="kobo.447.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.448.1">Orbit</span></strong><span class="koboSpan" id="kobo.449.1">) (</span><a href="https://github.com/uber/orbit"><span class="koboSpan" id="kobo.450.1">https://github.com/uber/orbit</span></a><span class="koboSpan" id="kobo.451.1">). </span><span class="koboSpan" id="kobo.451.2">As the name suggests, Orbit is Bayesian just like Prophet. </span><span class="koboSpan" id="kobo.451.3">Orbit, however, was designed to be more generalizable than Prophet, bridging the gap between typical business problems and more complex </span><span class="No-Break"><span class="koboSpan" id="kobo.452.1">statistical solutions.</span></span></p>
<p><span class="koboSpan" id="kobo.453.1">Although Uber’s benchmarking indicates that Orbit performs well on all types of forecasting problems, its bread-and-butter use case is in marketing mix models, a technique to quantify the impact of several marketing inputs on sales. </span><span class="koboSpan" id="kobo.453.2">Orbit was implemented with </span><a id="_idIndexMarker055"/><span class="koboSpan" id="kobo.454.1">two main types of Bayesian structural time series: </span><strong class="bold"><span class="koboSpan" id="kobo.455.1">Local Global Trend</span></strong><span class="koboSpan" id="kobo.456.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.457.1">LGT</span></strong><span class="koboSpan" id="kobo.458.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.459.1">Damped Local Trend</span></strong><span class="koboSpan" id="kobo.460.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.461.1">DLT</span></strong><span class="koboSpan" id="kobo.462.1">) models. </span><span class="koboSpan" id="kobo.462.2">Uber </span><a id="_idIndexMarker056"/><span class="koboSpan" id="kobo.463.1">claims this approach shows competitive results compared to other models, including Prophet, with improved metrics ranging from 12% to 60%. </span><span class="koboSpan" id="kobo.463.2">Like Greykite, Orbit u</span><a id="_idTextAnchor101"/><a id="_idTextAnchor102"/><span class="koboSpan" id="kobo.464.1">ses the </span><strong class="source-inline"><span class="koboSpan" id="kobo.465.1">sklearn</span></strong><span class="koboSpan" id="kobo.466.1"> paradigm to help new </span><span class="No-Break"><span class="koboSpan" id="kobo.467.1">users onboard.</span></span></p>
<h1 id="_idParaDest-28"><a id="_idTextAnchor103"/><span class="koboSpan" id="kobo.468.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.469.1">Through this brief survey of time series, you learned why time series data can be problematic if not analyzed with specialized techniques. </span><span class="koboSpan" id="kobo.469.2">You followed the developments of mathematicians and statisticians as they created new techniques to achieve higher forecasting accuracy or greater ease of use. </span><span class="koboSpan" id="kobo.469.3">You also learned what motivated the Prophet team to add their own contributions to this legacy and what decisions they made in their approach, and you learned how the open source community has reacted and begun work on </span><span class="No-Break"><span class="koboSpan" id="kobo.470.1">different approaches.</span></span></p>
<p><span class="koboSpan" id="kobo.471.1">In the next chapter, you’ll learn how to get Prophet running on your machine and build your first model. </span><span class="koboSpan" id="kobo.471.2">By the end of this book, you’ll understand every feature, no matter how small, and have them all in your toolbox to supercharge your </span><span class="No-Break"><span class="koboSpan" id="kobo.472.1">own forecasts.</span></span></p>
</div>
<div>
<div class="IMG---Figure" id="_idContainer013">
</div>
</div>
</body></html>