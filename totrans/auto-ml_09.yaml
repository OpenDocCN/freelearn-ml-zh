- en: '*Chapter 7*: Doing Automated Machine Learning with Amazon SageMaker Autopilot'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"One of the holy grails of machine learning is to automate more and more of
    the feature engineering process."'
  prefs: []
  type: TYPE_NORMAL
- en: – Pedro Domingos
  prefs: []
  type: TYPE_NORMAL
- en: '"Automated machine learning, the best thing since sliced bread!"'
  prefs: []
  type: TYPE_NORMAL
- en: – Anonymous
  prefs: []
  type: TYPE_NORMAL
- en: '**Automated Machine Learning** (**AutoML**) via hyperscalers – that is, via
    cloud providers – has the potential to bring AI democratization to the masses.
    In the previous chapter, you created a **Machine Learning** (**ML**) workflow
    in SageMaker, and also learned about the internals of SageMaker Autopilot.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will look at a couple of examples explaining how Amazon
    SageMaker Autopilot can be used in a visual, as well as in notebook, format.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating an Amazon SageMaker Autopilot limited experiment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an AutoML experiment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running the SageMaker Autopilot experiment and deploying the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Invoking and testing the SageMaker Autopilot model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building and running SageMaker Autopilot experiments from the notebook
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need access to an Amazon SageMaker Studio instance on your machine.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an Amazon SageMaker Autopilot limited experiment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's gets a hands-on introduction to applying AutoML using SageMaker Autopilot.
    We will download and apply AutoML to an open source dataset. Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: From Amazon SageMaker Studio, start a data science notebook by clicking on the
    `bank-additional-full.csv`, along with all examples (complete data), ordered by
    date (from May 2008 to November 2010)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`bank-additional.csv`, with 10% of the examples (4,119) randomly selected from
    `bank-additional-full.csv`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`bank-additional-names.txt`, which contains the field information described
    in the preceding screenshot'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As shown in the following screenshot, you can view the contents of the files
    using pandas once you''ve loaded the CSV file into the pandas DataFrame:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Amazon SageMaker Studio Jupyter Notebook – loading the dataset
    in a pandas DataFrame and visualizing it'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.6_B16890.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.6 – Amazon SageMaker Studio Jupyter Notebook – loading the dataset
    in a pandas DataFrame and visualizing it
  prefs: []
  type: TYPE_NORMAL
- en: 'Using NumPy, split the dataset into training and testing segments. In this
    case, we will use 95% of the data for training and 5% of the data for testing,
    as shown in the following screenshot. You will store this data in two files: one
    for training and another for testing.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 - Amazon SageMaker Studio Jupyter Notebook – splitting the dataset
    into training/test and saving the files in S3'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.7_B16890.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.7 - Amazon SageMaker Studio Jupyter Notebook – splitting the dataset
    into training/test and saving the files in S3
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the SageMaker API, create a session and upload the training data we created
    in the previous step to S3:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Amazon SageMaker Studio Jupyter Notebook – uploading the dataset
    to S3'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_Preface_1_B16890.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.8 – Amazon SageMaker Studio Jupyter Notebook – uploading the dataset
    to S3
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous chapter, we learned how to create an AutoML experiment using
    the notebook. Now, let''s create an experiment via the SageMaker UI. Click on
    the experiment icon in the left pane and create an experiment by providing the
    experiment''s name and S3 bucket address, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – Amazon SageMaker Studio UI – creating an experiment'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.9_B16890.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.9 – Amazon SageMaker Studio UI – creating an experiment
  prefs: []
  type: TYPE_NORMAL
- en: 'Set the target attribute to `y`. The target attribute is described in the dataset
    as Output variable (desired target): `y` – has the client subscribed a term deposit?
    `(binary: "yes","no")`:![Figure 7.10 – Amazon SageMaker Studio UI – creating an
    experiment'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_7.10_B16890.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.10 – Amazon SageMaker Studio UI – creating an experiment
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As shown in the preceding screenshot, you can define the ML problem by yourself
    – it's binary classification in this case – or let the SageMaker AutoML engine
    decide this on its own. In this case, we will leave it as **Auto**, and you will
    see that the SageMaker will recognize this as a binary classification problem.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can either run the full experiment – that is, data analysis, feature engineering,
    and modeling tuning – or create a notebook to view the candidate definitions.
    We will do both with this dataset to demonstrate the benefits of each approach:![Figure
    7.11 – Amazon SageMaker Studio UI – complete experiment versus pilot
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: for candidate definitions
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/Figure_7.11_B16890.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.11 – Amazon SageMaker Studio UI – complete experiment versus pilot
    for candidate definitions
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Lastly, you can set some advanced optional parameters, such as a custom SageMaker
    role, encryption key (if your S3 data is encrypted), and VPC information, if you
    are working with a virtual private cloud:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.12 – Amazon SageMaker Studio UI – Advanced Settings'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/Figure_7.12_B16890.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.12 – Amazon SageMaker Studio UI – Advanced Settings
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'With that, we have entered all the required information and can run the experiment.
    Upon submitting the job, you will see the following screen, which contains two
    steps (analyzing data and candidate definitions generation). This is because we
    have chosen not to run the entire experiment; we have only chosen to generate
    candidate definitions:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.13 – Amazon SageMaker Studio experiment creation UI – Analyzing
    Data screen'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/Figure_7.13_B16890.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.13 – Amazon SageMaker Studio experiment creation UI – Analyzing Data
    screen
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once this partial experiment is completed, you will see the following screen,
    which shows the completed job information, trials, and job profile. Since we only
    generated the candidates in this case, the experiment didn''t take too long. The
    **Open candidate generation notebook** and **Open data exploration notebook**
    buttons can be found at the top-right of the page. Both these buttons will open
    the respective notebooks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.14 – Amazon SageMaker AutoML experiment completion view'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.14_B16890.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.14 – Amazon SageMaker AutoML experiment completion view
  prefs: []
  type: TYPE_NORMAL
- en: 'The SageMaker Autopilot candidate definition notebook helps the data scientist
    take a deeper look at the dataset, its features, its classification problem, and
    the quality metric of the trained model. This is essentially an in-depth view
    of what happens behind the scenes in the SageMaker Autopilot pipeline and gives
    the data scientist a chance to run this manually and fine-tune or make changes
    as they deem necessary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.15 – Amazon SageMaker Autopilot candidate definition notebook'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.15_B16890.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.15 – Amazon SageMaker Autopilot candidate definition notebook
  prefs: []
  type: TYPE_NORMAL
- en: 'The candidate definition notebook is a fairly large file and contains a table
    of contents, as shown in the preceding screenshot. Similarly, the data exploration
    notebook provides you with insights into the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.16 – Amazon SageMaker Autopilot data exploration notebook'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.16_B16890.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.16 – Amazon SageMaker Autopilot data exploration notebook
  prefs: []
  type: TYPE_NORMAL
- en: 'These insights include what you would typically expect from a data scientist
    – a data scientist looks for features and their data types, range, mean, median,
    descriptive statistics, missing data, and more. Even if you are skeptical about
    the AutoML capabilities that are available in general, this is an excellent place
    for a data scientist to just explore the dataset and its respective candidates:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.17 – Amazon SageMaker Autopilot data exploration notebook – descriptive
    statistics'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.17_B16890.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.17 – Amazon SageMaker Autopilot data exploration notebook – descriptive
    statistics
  prefs: []
  type: TYPE_NORMAL
- en: The Amazon SageMaker Autopilot data exploration and candidate definition notebooks
    provide a transparent view for users to analyze data and conduct experiments.
    As notebooks go, these are executable pieces of code where you can see the preprocessors,
    hyperparameters, algorithms, ranges of hyperparameters, and all the prescribed
    preprocessing steps that are used to identify the best candidates.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will build and run a full Autopilot experiment.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an AutoML experiment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since the Autopilot data exploration and candidate definition notebooks provide
    an in-depth overview of the dataset, the complete experiment actually runs these
    steps and give you a final, tuned model based on the steps described in these
    notebooks. Now, let''s create a full experiment using the same UI as looked at
    earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: 'From Amazon SageMaker Studio, start a data science experiment. Click on the
    experiment icon in the left-hand pane and create an experiment by providing the
    experiment name and S3 bucket address, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.18 – Amazon SageMaker Autopilot – creating the experiment'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.18_B16890.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.18 – Amazon SageMaker Autopilot – creating the experiment
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous *Creating an Amazon SageMaker Autopilot limited experiment
    section*, we did the limited run. In this section, we will use the complete experiment
    feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.19 – Amazon SageMaker Autopilot – creating the complete experiment'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.19_B16890.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.19 – Amazon SageMaker Autopilot – creating the complete experiment
  prefs: []
  type: TYPE_NORMAL
- en: 'When you start the experiment, it will behave very similar to our earlier candidate
    experiment, aside from the fact that this complete experiment will take longer
    and will build and execute the entire pipeline. You will see the following screen
    in the meantime while you wait for the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Figure_7.20_B16890.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.20 – Amazon SageMaker Autopilot – running the full experiment
  prefs: []
  type: TYPE_NORMAL
- en: 'While the experiment is running, you can track its progress by looking at the
    individual experiments and getting valuable insights from the **Trials** tab.
    You may also notice that the problem type here is correctly classified as binary
    classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.21 – Amazon SageMaker Autopilot – running the full experiment'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.21_B16890.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.21 – Amazon SageMaker Autopilot – running the full experiment
  prefs: []
  type: TYPE_NORMAL
- en: 'The detailed summary of the experiment shown in the following screenshot shows
    the inference containers that were used, the model data URI, and the environments
    that were utilized, along with their respective **Amazon Resource Names** (**ARNs**),
    which uniquely identify AWS resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.22 – Amazon SageMaker Autopilot inference container information'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.22_B16890.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.22 – Amazon SageMaker Autopilot inference container information
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Trials** tab shows the different trials and tuning jobs that run, as
    well as the objective function (F1 score), which demonstrates how it improves
    over time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.23 – Amazon SageMaker Autopilot experiment run trials – best model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.23_B16890.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.23 – Amazon SageMaker Autopilot experiment run trials – best model
  prefs: []
  type: TYPE_NORMAL
- en: 'You have seen this exact iteration in previous chapters; it is déjà vu all
    over again. We have seen this process unfolding in the OSS tools, but it''s just
    different here, in that it''s done in a more organized end-to-end manner. You
    have the entire pipeline built into one; that is, the strategy, data analysis,
    feature engineering, model tuning, and hyperparameter optimization processes.
    You can see the tuning job''s details in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.24 – Amazon SageMaker Autopilot tuning job details showing Bayesian
    strategy'
  prefs: []
  type: TYPE_NORMAL
- en: and resource information
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.24_B16890.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.24 – Amazon SageMaker Autopilot tuning job details showing Bayesian
    strategy and resource information
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've run the entire experiment and the process is completed, let's
    deploy the best model.
  prefs: []
  type: TYPE_NORMAL
- en: Running the SageMaker Autopilot experiment and deploying the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Amazon SageMaker Studio makes it easy for us to build, train, and deploy machine
    learning models; that is, it enables the data science life cycle. To deploy the
    model we built in the preceding section, we will need to set certain parameters.
    For this, you must provide the endpoint name, instance type, how many instances
    (count), and if you''d like to capture the request and response information. Let''s
    get started:'
  prefs: []
  type: TYPE_NORMAL
- en: If you select the **Data capture** option, you will need an S3 bucket for storage,
    as shown in the following screenshot:![Figure 7.25 – Amazon SageMaker endpoint
    deployment
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_7.25_B16890.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.25 – Amazon SageMaker endpoint deployment
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once you've clicked on **Deploy**, you will see the following screen, which
    shows the progress of the new endpoint being created:![Figure 7.26 – Amazon SageMaker
    endpoint deployment in progress
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_7.26_B16890.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.26 – Amazon SageMaker endpoint deployment in progress
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once the deployment is completed, you will see the following status of InService:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.27 – Amazon SageMaker endpoint deployment completed'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/Figure_7.27_B16890.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.27 – Amazon SageMaker endpoint deployment completed
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The model endpoint is an important resource for ensuring the quality of the
    model. By enabling the model monitor, you can detect data drift and monitor the
    quality of any models in production. This proactive detection of the model''s
    quality helps ensure that your machine learning service does not end up providing
    the wrong results in production. You can click on the `Enable monitoring` button
    to engage the Amazon SageMaker Model Monitor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.28 – Amazon SageMaker Autopilot Model Monitor startup screen'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.28_B16890.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.28 – Amazon SageMaker Autopilot Model Monitor startup screen
  prefs: []
  type: TYPE_NORMAL
- en: 'Model monitoring is an important area of the machine learning life cycle. As
    shown in the following screenshot, the Amazon SageMaker Model Monitor addresses
    this by capturing data, creating a baseline, scheduling monitoring jobs, and then
    allowing SMEs to interpret the results in the case of outliers and violations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.29 – Amazon SageMaker Autopilot Model Monitor enablement notebook'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.29_B16890.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.29 – Amazon SageMaker Autopilot Model Monitor enablement notebook
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have created and the deployed the model, it is time to test it out
    by invoking it. This operation of invoking a machine learning model that's been
    exposed via a web service is typically called inferencing or evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Invoking the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the model built and deployed using Amazon SageMaker Autopilot, we can
    test it out. Remember the test data we saved earlier? Now, it''s time to use it.
    Here, you can see that we are iterating through the `automl-test.csv` file and
    invoking the endpoint by passing the line of data as a request:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.30 – Amazon SageMaker Autopilot – model invocation from the notebook'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.30_B16890.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.30 – Amazon SageMaker Autopilot – model invocation from the notebook
  prefs: []
  type: TYPE_NORMAL
- en: 'The request contains information about the person applying for the loan. We
    have removed the outcome (label) from the request, and then compared it as we
    wish to print the value out. You can see the request, the label, and the corresponding
    response from the web service in the preceding screenshot. You can use this information
    to calculate the accuracy of the service results; they are fairly accurate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.31 – Amazon SageMaker Autopilot – model invocation responses'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.31_B16890.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.31 – Amazon SageMaker Autopilot – model invocation responses
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have learned how to set up an AutoML experiment from the Amazon
    SageMaker Autopilot UI, in the next section, we will use notebooks to do the same.
  prefs: []
  type: TYPE_NORMAL
- en: Building and running SageMaker Autopilot experiments from the notebook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Customer churn is a real problem for businesses and in this example, we will
    use our knowledge of completing AutoML in Amazon SageMaker Autopilot to build
    a customer churn prediction experiment using the notebook. In this experiment,
    we will use a publicly available dataset of US mobile customers provided by Daniel
    T. Larose in his book *Discovering Knowledge in Data*. To demonstrate running
    the full gamut, the sample notebook executes the Autopilot experiment by performing
    feature engineering, building a model pipeline (along with any optimal hyperparameters),
    and deploying the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The evolution of the UI/API/CLI paradigm has helped us utilize the same interface
    in multiple formats; in this case, we will be utilizing the capabilities of Amazon
    SageMaker Autopilot directly from the notebook. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the `autopilot_customer_churn` notebook from the `amazon-sagemaker-examples/autopilot`
    folder, as shown in the following screenshot:![Figure 7.32 – Amazon SageMaker
    Autopilot – customer churn prediction Autopilot notebook
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_7.32_B16890.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.32 – Amazon SageMaker Autopilot – customer churn prediction Autopilot
    notebook
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Run the setup by specifying the S3 bucket and the **Identity and Access Management**
    (**IAM**) role, as we did in the previous *Creating an AutoML experiment* section.
    Download the dataset, as shown in the following screenshot:![Figure 7.33 – Amazon
    SageMaker Autopilot – running the notebook to set up a default bucket and creating
    the session
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_7.33_B16890.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.33 – Amazon SageMaker Autopilot – running the notebook to set up a
    default bucket and creating the session
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: At this point, you will need to install the prerequisites, and download the
    dataset, as shown in the following screenshot:![Figure 7.34 – Amazon SageMaker
    Autopilot – downloading the dataset and unzipping the file
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_7.34_B16890.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.34 – Amazon SageMaker Autopilot – downloading the dataset and unzipping
    the file
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once the dataset has been downloaded and uncompressed, you can add it to a pandas
    DataFrame and view it. It shows information about the customer, such as their
    calling attributes, as shown in the following screenshot:![Figure 7.35 – Amazon
    SageMaker notebook showing the dataset's information
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_7.35_B16890.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.35 – Amazon SageMaker notebook showing the dataset's information
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can now sample the dataset as test and training buckets, and then upload
    these files to S3 for future use. Once they've been uploaded, you will get the
    S3 buckets' names, as shown in the following screenshot:![Figure 7.36 – Amazon
    SageMaker Autopilot – sample dataset for test and training, and uploading the
    files to the S3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_7.36_B16890.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.36 – Amazon SageMaker Autopilot – sample dataset for test and training,
    and uploading the files to the S3
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: So far, everything we have done is traditional notebook work. Now, we will set
    up the Autopilot job.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's define the configuration, as shown in the following screenshot:![Figure
    7.37 – Amazon SageMaker Autopilot – configuring the Autopilot job config
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_7.37_B16890.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.37 – Amazon SageMaker Autopilot – configuring the Autopilot job config
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, let's launch the SageMaker Autopilot job by invoking the `create_auto_ml_job`
    API call, like so:![Figure 7.38 – Amazon SageMaker Autopilot – configuring the
    Autopilot job
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_7.38_B16890.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.38 – Amazon SageMaker Autopilot – configuring the Autopilot job
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The job runs with multiple trials, including the components of each experiment,
    as shown in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.39 – Amazon SageMaker Autopilot – trial components in the Autopilot
    job notebook'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/Figure_7.39_B16890.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.39 – Amazon SageMaker Autopilot – trial components in the Autopilot
    job notebook
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'While tracking the Amazon SageMaker Autopilot job''s progress, you can print
    its status, along with any delays, as shown in the following screenshot. However,
    to view the details of individual trial runs in a meaningful manner visually,
    you can use the user interface:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.40 – Amazon SageMaker Autopilot – trial components in the Autopilot
    job notebook'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/Figure_7.40_B16890.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.40 – Amazon SageMaker Autopilot – trial components in the Autopilot
    job notebook
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once the feature engineering and model tuning jobs in the trials are complete,
    you can run `describe_auto_ml_job` to get the best candidate information. Then,
    you can traverse the `best_candidate` object to get information about the underlying
    score and metric, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.41 – Amazon SageMaker Autopilot – trial components in the Autopilot
    job notebook'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.41_B16890.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.41 – Amazon SageMaker Autopilot – trial components in the Autopilot
    job notebook
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the job is completed, you will see the candidate model, the final metric
    (the F1 score in this case), and any associated values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.42 – Amazon SageMaker Autopilot job results'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.42_B16890.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.42 – Amazon SageMaker Autopilot job results
  prefs: []
  type: TYPE_NORMAL
- en: We will deploy and invoke the best candidate model, which has a 93% F1 score,
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Hosting and invoking the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Similar to how we invoked the model we built using the Experiment UI earlier,
    we will now host and invoke the model we built in the notebook. The difference
    is that in the first instance, we were low-code, while here we are building it
    using code:'
  prefs: []
  type: TYPE_NORMAL
- en: To host the service, you will need to create a model object, endpoint configuration,
    and eventually an endpoint. Previously, this was done using the UI, but here,
    we will use the Amazon SageMaker Python instance to accomplish the same. This
    can be seen in the following screenshot:![Figure 7.43 – Amazon SageMaker notebook
    – hosting the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_7.43_B16890.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.43 – Amazon SageMaker notebook – hosting the model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The `get_waiter` method is part of Boto3, which is the Python SDK for AWS.
    Like other waiters, it polls until a successful state is reached. An error is
    typically returned after 60 failed checks. You can read about the methods by looking
    at the API documentation for it, which can be found here: [https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that the endpoint has been created and the model has been hosted, we can
    invoke the service. To evaluate the model, you will need to create a predictor
    instance and pass it the endpoint''s information, along with the parameters for
    prediction. Instead of calling the endpoint line by line, we can perform bulk
    predictions by passing in the entire test data CSV file and comparing the results
    against the ground truth. You can see the accuracy numbers in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.44 – Amazon SageMaker model evaluation for accuracy'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/Figure_7.44_B16890.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.44 – Amazon SageMaker model evaluation for accuracy
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once you have finished testing the endpoint, we must clean up. In cloud environments,
    you must clean up after yourself, so make this a priority checklist item. If you
    don't do this, you won't like the billing statement from leaving a server running.
    Virtual or not, it all adds up.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When you''re cleaning up a UI, turn off and delete the compute instances and
    the endpoints. Since we are doing a manual cleanup, you must delete the endpoint,
    endpoint config, and the model:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.45 – Amazon SageMaker Autopilot cleanup with resulting response
    codes'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.45_B16890.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.45 – Amazon SageMaker Autopilot cleanup with resulting response codes
  prefs: []
  type: TYPE_NORMAL
- en: Even though these examples have shown you how AWS AutoML enables you to perform
    feature engineering, model tuning, and hyperparameter optimization, you don't
    have to limit yourself to the algorithms provided by AWS. You can bring your own
    data processing code to SageMaker Autopilot, as shown at [https://github.com/aws/amazon-sagemaker-examples/blob/master/autopilot/custom-feature-selection/Feature_selection_autopilot.ipynb](https://github.com/aws/amazon-sagemaker-examples/blob/master/autopilot/custom-feature-selection/Feature_selection_autopilot.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building AutoML systems to democratize AI from scratch is a considerable effort.
    Therefore, cloud hyperscalers act as enablers and accelerators to jumpstart this
    journey. In this chapter, you learned how to use Amazon SageMaker Autopilot, both
    via notebooks and via the experimentation user interface. You were also exposed
    to the larger AWS machine learning ecosystem and SageMaker's capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will study another major cloud computing platform, Google
    Cloud Platform, and the AutoML offerings provided it. Happy coding!
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information on the topics that were covered in this chapter, please
    refer to the following links and resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Mastering Machine Learning on AWS: Advanced machine learning in Python using
    SageMaker, Apache Spark, and TensorFlow*, by Dr. Saket S.R. Mengle , Maximo Gurmendez,
    Packt Publishing: [https://www.amazon.com/Mastering-Machine-Learning-AWS-TensorFlow/dp/1789349796](https://www.amazon.com/Mastering-Machine-Learning-AWS-TensorFlow/dp/1789349796)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Learn Amazon SageMaker: A guide to building, training, and deploying machine
    learning models for developers and data scientists*, by Julien Simon and Francesco
    Pochetti, Packt Publishing: [https://www.amazon.com/Learn-Amazon-SageMaker-developers-scientists/dp/180020891X](https://www.amazon.com/Learn-Amazon-SageMaker-developers-scientists/dp/180020891X)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
